{"home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.a3ps.ActorCritic.__init__": [[31, 57], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["    ", "def", "__init__", "(", "self", ",", "action_size", ")", ":", "\n", "        ", "super", "(", "ActorCritic", ",", "self", ")", ".", "__init__", "(", ")", "\n", "#self.state_size = state_size", "\n", "self", ".", "action_size", "=", "action_size", "\n", "self", ".", "hidden_size", "=", "512", "\n", "self", ".", "embed_size", "=", "512", "\n", "self", ".", "n_layers", "=", "2", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "1", ",", "16", ",", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "16", ",", "32", ",", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "3", ",", "stride", "=", "1", ")", "\n", "self", ".", "fc_size", "=", "1600", "\n", "goal_state_size", "=", "3", "\n", "# self.conv6 = nn.Conv2d(64,64,3,stride=1)", "\n", "# self.conv7 = nn.Conv2d(64,128,3,stride=1)", "\n", "\n", "# self.drop1 = nn.Dropout2d(0.3)", "\n", "# self.drop2 = nn.Dropout2d(0.3)", "\n", "# self.lstm = nn.LSTM(300,self.num_hid,batch_first=True)", "\n", "self", ".", "linear1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size", ",", "self", ".", "embed_size", ")", "\n", "self", ".", "linear2_actor", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_size", "*", "2", ",", "256", ")", "\n", "self", ".", "linear2_critic", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_size", "*", "2", ",", "256", ")", "\n", "# self.linear3 = nn.Linear (512,128)", "\n", "self", ".", "linear_actor", "=", "nn", ".", "Linear", "(", "256", "+", "goal_state_size", ",", "self", ".", "action_size", ")", "\n", "self", ".", "linear_critic", "=", "nn", ".", "Linear", "(", "256", "+", "goal_state_size", ",", "1", ")", "\n", "self", ".", "actor_lstm", "=", "nn", ".", "LSTM", "(", "self", ".", "embed_size", ",", "self", ".", "hidden_size", ",", "self", ".", "n_layers", ",", "batch_first", "=", "True", ",", "dropout", "=", "0.3", ")", "\n", "self", ".", "critic_lstm", "=", "nn", ".", "LSTM", "(", "self", ".", "embed_size", ",", "self", ".", "hidden_size", ",", "self", ".", "n_layers", ",", "batch_first", "=", "True", ",", "dropout", "=", "0.3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.a3ps.ActorCritic.init_hidden": [[58, 67], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "h0.cuda.cuda.cuda", "c0.cuda.cuda.cuda"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "h0", "=", "torch", ".", "zeros", "(", "1", "*", "self", ".", "n_layers", ",", "batch_size", ",", "self", ".", "hidden_size", ")", "\n", "c0", "=", "torch", ".", "zeros", "(", "1", "*", "self", ".", "n_layers", ",", "batch_size", ",", "self", ".", "hidden_size", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "h0", "=", "h0", ".", "cuda", "(", ")", "\n", "c0", "=", "c0", ".", "cuda", "(", ")", "\n", "\n", "", "return", "(", "h0", ",", "c0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.a3ps.ActorCritic.forward": [[68, 106], ["a3ps.ActorCritic.init_hidden", "a3ps.ActorCritic.init_hidden", "state.view.view.view", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "a3ps.ActorCritic.linear1", "a3ps.ActorCritic.actor_lstm", "actor_h.view.view.view", "a3ps.ActorCritic.linear2_actor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "a3ps.ActorCritic.critic_lstm", "critic_h.view.view.view", "a3ps.ActorCritic.linear2_actor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "a3ps.ActorCritic.linear_actor", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "a3ps.ActorCritic.linear_critic", "a3ps.ActorCritic.conv1", "a3ps.ActorCritic.conv2", "a3ps.ActorCritic.conv3"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden"], ["", "def", "forward", "(", "self", ",", "state", ",", "goal_state", ")", ":", "\n", "        ", "batch_size", "=", "state", ".", "shape", "[", "0", "]", "\n", "(", "h0", ",", "c0", ")", "=", "self", ".", "init_hidden", "(", "batch_size", ")", "\n", "(", "_h0", ",", "_c0", ")", "=", "self", ".", "init_hidden", "(", "batch_size", ")", "\n", "state", "=", "state", ".", "view", "(", "-", "1", ",", "1", ",", "IMAGE_WIDTH", ",", "IMAGE_HEIGHT", ")", "\n", "output1", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "state", ")", ")", "\n", "output2", "=", "F", ".", "relu", "(", "self", ".", "conv2", "(", "output1", ")", ")", "\n", "output2", "=", "F", ".", "max_pool2d", "(", "output2", ",", "2", ",", "stride", "=", "2", ")", "\n", "\n", "# output2 = self.drop1(output2)", "\n", "\n", "output3", "=", "F", ".", "relu", "(", "self", ".", "conv3", "(", "output2", ")", ")", "\n", "output3", "=", "F", ".", "max_pool2d", "(", "output3", ",", "2", ",", "stride", "=", "2", ")", "\n", "\n", "# output5 = F.relu(self.conv5(output4))", "\n", "# output6 = F.relu(self.conv6(output5))", "\n", "# output7 = F.relu(self.conv7(output6))", "\n", "# output7 = F.max_pool2d(output7,2,stride=2)", "\n", "img_emb", "=", "torch", ".", "reshape", "(", "output3", ",", "(", "-", "1", ",", "STACK_SIZE", ",", "self", ".", "fc_size", ")", ")", "\n", "img_emb", "=", "self", ".", "linear1", "(", "img_emb", ")", "\n", "\n", "output", ",", "(", "actor_h", ",", "actor_c", ")", "=", "self", ".", "actor_lstm", "(", "img_emb", ",", "(", "h0", ",", "c0", ")", ")", "\n", "actor_h", "=", "actor_h", ".", "view", "(", "-", "1", ",", "self", ".", "hidden_size", "*", "2", ")", "\n", "fc_actor", "=", "self", ".", "linear2_actor", "(", "actor_h", ")", "\n", "fc_actor", "=", "torch", ".", "cat", "(", "(", "fc_actor", ",", "goal_state", ")", ",", "1", ")", "\n", "\n", "output", ",", "(", "critic_h", ",", "critic_c", ")", "=", "self", ".", "critic_lstm", "(", "img_emb", ",", "(", "_h0", ",", "_c0", ")", ")", "\n", "critic_h", "=", "critic_h", ".", "view", "(", "-", "1", ",", "self", ".", "hidden_size", "*", "2", ")", "\n", "fc_critic", "=", "self", ".", "linear2_actor", "(", "critic_h", ")", "\n", "fc_critic", "=", "torch", ".", "cat", "(", "(", "fc_critic", ",", "goal_state", ")", ",", "1", ")", "\n", "\n", "dist_actor", "=", "self", ".", "linear_actor", "(", "fc_actor", ")", "\n", "prob_actor", "=", "F", ".", "softmax", "(", "dist_actor", ",", "dim", "=", "-", "1", ")", "\n", "#dist_policy  = Categorical(prob_actor)", "\n", "# dist = Categorical(F.softmax(dist_actor, dim=-1))", "\n", "value_critic", "=", "self", ".", "linear_critic", "(", "fc_critic", ")", "\n", "\n", "return", "prob_actor", ",", "value_critic", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.a3ps.plot": [[107, 114], ["matplotlib.figure", "matplotlib.title", "matplotlib.plot", "matplotlib.show"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.plot"], ["", "", "def", "plot", "(", "frame_idx", ",", "rewards", ")", ":", "\n", "# clear_output(True)", "\n", "    ", "plt", ".", "figure", "(", "figsize", "=", "(", "16", ",", "8", ")", ")", "\n", "# plt.subplot(131)", "\n", "plt", ".", "title", "(", "'frame %s. reward: %s'", "%", "(", "frame_idx", ",", "rewards", "[", "-", "1", "]", ")", ")", "\n", "plt", ".", "plot", "(", "rewards", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.a3ps.test_env": [[115, 128], ["env.reset", "env.render", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "model", "env.step", "dist.sample().cpu().numpy", "env.render", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "dist.sample().cpu", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "dist.sample"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.reset"], ["", "def", "test_env", "(", "vis", "=", "False", ")", ":", "\n", "    ", "state", "=", "env", ".", "reset", "(", ")", "\n", "if", "vis", ":", "env", ".", "render", "(", ")", "\n", "done", "=", "False", "\n", "total_reward", "=", "0", "\n", "while", "not", "done", ":", "\n", "        ", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "dist", ",", "_", "=", "model", "(", "state", ")", "\n", "next_state", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "dist", ".", "sample", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "state", "=", "next_state", "\n", "if", "vis", ":", "env", ".", "render", "(", ")", "\n", "total_reward", "+=", "reward", "\n", "", "return", "total_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.a3ps.compute_gae": [[130, 145], ["reversed", "range", "returns.insert", "len"], "function", ["None"], ["", "def", "compute_gae", "(", "next_value", ",", "rewards", ",", "masks", ",", "values", ",", "gamma", "=", "0.99", ",", "tau", "=", "0.95", ")", ":", "\n", "    ", "values", "=", "values", "+", "[", "next_value", "]", "\n", "gae", "=", "0", "\n", "returns", "=", "[", "]", "\n", "for", "step", "in", "reversed", "(", "range", "(", "len", "(", "rewards", ")", ")", ")", ":", "\n", "# _v0 = values[step]", "\n", "# _v1 = values[step + 1]", "\n", "# _item1 = gamma * _v1 ", "\n", "# _item1 = _item1 * masks[step]", "\n", "# _item2 = _item1 - _v0", "\n", "        ", "delta", "=", "rewards", "[", "step", "]", "+", "gamma", "*", "values", "[", "step", "+", "1", "]", "*", "masks", "[", "step", "]", "-", "values", "[", "step", "]", "*", "masks", "[", "step", "]", "\n", "gae", "=", "delta", "+", "gamma", "*", "tau", "*", "masks", "[", "step", "]", "*", "gae", "\n", "returns", ".", "insert", "(", "0", ",", "gae", "+", "values", "[", "step", "]", ")", "\n", "# returns.insert(0, gae)", "\n", "", "return", "returns", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.a3ps.ppo_iter": [[146, 156], ["states.size", "range", "numpy.random.randint"], "function", ["None"], ["", "def", "ppo_iter", "(", "mini_batch_size", ",", "states", ",", "goal_states", ",", "actions", ",", "log_probs", ",", "returns", ",", "advantage", ")", ":", "\n", "    ", "batch_size", "=", "states", ".", "size", "(", "0", ")", "\n", "for", "_", "in", "range", "(", "batch_size", "//", "mini_batch_size", ")", ":", "\n", "        ", "rand_ids", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "batch_size", ",", "mini_batch_size", ")", "\n", "# _states = states[rand_ids, :]", "\n", "# _actions = actions[rand_ids]", "\n", "# _log_probs = log_probs[rand_ids, :]", "\n", "# _returns = returns[rand_ids, :]", "\n", "# _advantage = advantage[rand_ids, :]", "\n", "yield", "states", "[", "rand_ids", ",", ":", "]", ",", "goal_states", "[", "rand_ids", ",", ":", "]", ",", "actions", "[", "rand_ids", "]", ",", "log_probs", "[", "rand_ids", ",", ":", "]", ",", "returns", "[", "rand_ids", ",", ":", "]", ",", "advantage", "[", "rand_ids", ",", ":", "]", "\n", "# yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.a3ps.ppo_update": [[159, 183], ["model", "torch.distributions.Categorical", "torch.distributions.Categorical.entropy().mean", "torch.distributions.Categorical.log_prob", "new_log_probs.unsqueeze.unsqueeze", "optimizer.zero_grad", "loss.backward", "optimizer.step", "open", "open.write", "open.close", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.distributions.Categorical.entropy", "torch.min", "torch.min", "torch.min", "torch.min"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.close"], ["", "", "def", "ppo_update", "(", "ppo_epochs", ",", "mini_batch_size", ",", "states", ",", "goal_states", ",", "actions", ",", "log_probs", ",", "returns", ",", "advantages", ",", "clip_param", "=", "0.2", ")", ":", "\n", "    ", "dist_policy", ",", "values", "=", "model", "(", "states", ",", "goal_states", ")", "\n", "dist", "=", "Categorical", "(", "dist_policy", ")", "\n", "entropy", "=", "dist", ".", "entropy", "(", ")", ".", "mean", "(", ")", "\n", "new_log_probs", "=", "dist", ".", "log_prob", "(", "actions", ")", "\n", "new_log_probs", "=", "new_log_probs", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "ratio", "=", "(", "new_log_probs", "-", "log_probs", ")", ".", "exp", "(", ")", "\n", "surr1", "=", "ratio", "*", "advantages", "\n", "surr2", "=", "torch", ".", "clamp", "(", "ratio", ",", "1.0", "-", "clip_param", ",", "1.0", "+", "clip_param", ")", "*", "advantages", "\n", "\n", "actor_loss", "=", "-", "torch", ".", "min", "(", "surr1", ",", "surr2", ")", ".", "mean", "(", ")", "\n", "critic_loss", "=", "(", "returns", "-", "values", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "loss", "=", "0.5", "*", "critic_loss", "+", "actor_loss", "-", "0.001", "*", "entropy", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "# nn.utils.clip_grad_norm(model.parameters(), 40)", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "loss_log_file", "=", "open", "(", "train_log_loss_file_name", ",", "\"a+\"", ")", "\n", "loss_log_file", ".", "write", "(", "'%d %d %d\\n'", "%", "(", "actor_loss", ",", "critic_loss", ",", "loss", ")", ")", "\n", "loss_log_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.a3ps.a2c": [[236, 380], ["env.perform_action", "numpy.array", "pickle.dump", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "range", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "model", "a3ps.compute_gae", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "a3ps.ppo_update", "open", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "model", "torch.softmax", "torch.distributions.Categorical", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "env.perform_action", "print", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.entropy().mean", "torch.cat().detach.append", "torch.cat().detach.append", "rewards.append", "masks.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "len", "state_img.unsqueeze().to.unsqueeze().to", "advice_gen_img_encoder", "advice_gen_decoder.inference", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "advice_ids.view.view", "advice_driven_img_encoder", "advice_driven_encoder", "action_driven_decoder", "torch.softmax", "env.reset", "open", "open.write", "open.close", "training_rewards.append", "print", "env.perform_action", "numpy.array", "action_prob.log_prob.unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "print", "torch.save", "torch.save", "torch.save", "torch.save", "print", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "F.softmax.detach", "torch.argmax.cpu().numpy", "torch.argmax.cpu().numpy", "torch.distributions.Categorical.entropy", "state_img.unsqueeze().to.unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "str", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.argmax.cpu", "torch.argmax.cpu"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.perform_action", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.plot", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.compute_gae", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.ppo_update", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.perform_action", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.DecoderRNN.inference", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.reset", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.close", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.perform_action"], ["def", "a2c", "(", "env", ",", "vocab", ",", "advice_gen_img_encoder", ",", "advice_gen_decoder", ",", "advice_driven_img_encoder", ",", "advice_driven_encoder", ",", "action_driven_decoder", ",", "train_log_file", ")", ":", "\n", "\n", "    ", "frame_idx", "=", "0", "\n", "action", "=", "0", "\n", "alpha", "=", "0.5", "\n", "training_rewards", "=", "[", "]", "\n", "total_episode_reward", "=", "0", "\n", "reward", ",", "state", ",", "adv_state", ",", "done", "=", "env", ".", "perform_action", "(", "action", ",", "IMAGE_HEIGHT", ",", "IMAGE_WIDTH", ",", "image_height1", ",", "image_width1", ",", "STACK_SIZE", ")", "\n", "init_goal_state", "=", "np", ".", "array", "(", "[", "0", ",", "0", ",", "0", "]", ")", "\n", "num_episode", "=", "0", "\n", "num_moves", "=", "0", "\n", "while", "frame_idx", "<", "max_frames", "and", "not", "early_stop", ":", "\n", "\n", "        ", "log_probs", "=", "[", "]", "\n", "values", "=", "[", "]", "\n", "states", "=", "[", "]", "\n", "goal_states", "=", "[", "]", "\n", "actions", "=", "[", "]", "\n", "rewards", "=", "[", "]", "\n", "masks", "=", "[", "]", "\n", "entropy", "=", "0", "\n", "\n", "for", "_i_step", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "num_moves", "+=", "1", "\n", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "adv_state", "=", "torch", ".", "FloatTensor", "(", "adv_state", ")", ".", "to", "(", "device", ")", "\n", "goal_state", "=", "torch", ".", "FloatTensor", "(", "init_goal_state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "dist", ",", "value", "=", "model", "(", "state", ",", "goal_state", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "number_of_frames", "=", "len", "(", "adv_state", ")", "\n", "state_img", "=", "adv_state", "[", "number_of_frames", "-", "1", "]", "\n", "state_img", "=", "state_img", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "image_feature", "=", "advice_gen_img_encoder", "(", "state_img", ")", "\n", "advice_ids", "=", "advice_gen_decoder", ".", "inference", "(", "image_feature", ",", "vocab", ")", "\n", "advice_ids", "=", "torch", ".", "tensor", "(", "advice_ids", ")", ".", "to", "(", "device", ")", "\n", "advice_ids", "=", "advice_ids", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "# embedded_word = word_emb(advice_ids)", "\n", "img_features", "=", "advice_driven_img_encoder", "(", "state_img", ")", "\n", "embedded_adv", ",", "hidden", "=", "advice_driven_encoder", "(", "advice_ids", ")", "\n", "action_dist", "=", "action_driven_decoder", "(", "img_features", ",", "hidden", ")", "\n", "action_dist_prob", "=", "F", ".", "softmax", "(", "action_dist", ",", "dim", "=", "-", "1", ")", "\n", "\n", "#a2c_dist_prob = F.softmax(dist, dim=-1)", "\n", "# a2c_categorical_prob = Categorical(a2c_dist_prob)", "\n", "# _test_action = a2c_categorical_prob.sample()", "\n", "", "combined_prob_dist", "=", "(", "alpha", "*", "dist", ")", "+", "(", "(", "1", "-", "alpha", ")", "*", "action_dist_prob", ".", "detach", "(", ")", ")", "\n", "combined_prob", "=", "F", ".", "softmax", "(", "combined_prob_dist", ",", "dim", "=", "-", "1", ")", "\n", "action_prob", "=", "Categorical", "(", "combined_prob", ")", "\n", "action", "=", "torch", ".", "argmax", "(", "combined_prob", ",", "dim", "=", "1", ")", "\n", "\n", "#action = action_prob.sample()", "\n", "#action = dist.sample()", "\n", "\n", "reward", ",", "next_state", ",", "next_adv_state", ",", "done", "=", "env", ".", "perform_action", "(", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", ",", "IMAGE_HEIGHT", ",", "IMAGE_WIDTH", ",", "image_height1", ",", "image_width1", ",", "STACK_SIZE", ")", "\n", "# if done:", "\n", "#     reward = -2", "\n", "if", "reward", "==", "50", "and", "init_goal_state", "[", "0", "]", "==", "0", ":", "\n", "                ", "init_goal_state", "[", "0", "]", "=", "1", "\n", "", "elif", "reward", "==", "50", "and", "init_goal_state", "[", "0", "]", "==", "1", "and", "init_goal_state", "[", "1", "]", "==", "0", ":", "\n", "                ", "init_goal_state", "[", "1", "]", "=", "1", "\n", "", "elif", "reward", "==", "100", "and", "init_goal_state", "[", "0", "]", "==", "1", "and", "init_goal_state", "[", "1", "]", "==", "1", "and", "init_goal_state", "[", "2", "]", "==", "0", ":", "\n", "                ", "init_goal_state", "[", "2", "]", "=", "1", "\n", "\n", "", "print", "(", "\"reward: \"", ",", "reward", ",", "\" action: \"", ",", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", ")", "\n", "# if reward < -1:", "\n", "#     reward = -1", "\n", "# elif reward > 1:", "\n", "#     reward = 1", "\n", "total_episode_reward", "+=", "reward", "\n", "if", "done", ":", "\n", "                ", "env", ".", "reset", "(", ")", "\n", "# write the stats", "\n", "train_log_file", "=", "open", "(", "train_log_file_name", ",", "\"a+\"", ")", "\n", "train_log_file", ".", "write", "(", "'%d %d %d\\n'", "%", "(", "num_episode", ",", "num_moves", ",", "total_episode_reward", ")", ")", "\n", "train_log_file", ".", "close", "(", ")", "\n", "\n", "training_rewards", ".", "append", "(", "total_episode_reward", ")", "\n", "\n", "print", "(", "\"episode no: \"", ",", "num_episode", ",", "\" episode_reward: \"", ",", "total_episode_reward", ",", "\" frame_id: \"", ",", "frame_idx", ")", "\n", "total_episode_reward", "=", "0", "\n", "num_episode", "+=", "1", "\n", "no_action", "=", "0", "\n", "_", ",", "next_state", ",", "next_adv_state", ",", "_", "=", "env", ".", "perform_action", "(", "no_action", ",", "IMAGE_HEIGHT", ",", "IMAGE_WIDTH", ",", "image_height1", ",", "image_width1", ",", "STACK_SIZE", ")", "\n", "init_goal_state", "=", "np", ".", "array", "(", "[", "0", ",", "0", ",", "0", "]", ")", "\n", "num_moves", "=", "0", "\n", "\n", "# next_state, reward, done, _ = env.step(action.unsqueeze(0).cpu().numpy())", "\n", "", "log_prob", "=", "action_prob", ".", "log_prob", "(", "action", ")", "\n", "#combined_prob = combined_prob.squeeze(0)", "\n", "#log_prob = torch.log(combined_prob[action])", "\n", "entropy", "=", "action_prob", ".", "entropy", "(", ")", ".", "mean", "(", ")", "\n", "#entropy += combined_prob.entropy().mean()", "\n", "\n", "log_probs", ".", "append", "(", "log_prob", ".", "unsqueeze", "(", "0", ")", ")", "\n", "values", ".", "append", "(", "value", ")", "\n", "rewards", ".", "append", "(", "torch", ".", "tensor", "(", "[", "reward", "]", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", ")", "\n", "masks", ".", "append", "(", "torch", ".", "tensor", "(", "[", "1", "-", "done", "]", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", ")", "\n", "# rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))", "\n", "# masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))", "\n", "\n", "states", ".", "append", "(", "state", ")", "\n", "goal_states", ".", "append", "(", "goal_state", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "\n", "state", "=", "next_state", "\n", "adv_state", "=", "next_adv_state", "\n", "frame_idx", "+=", "1", "\n", "\n", "if", "frame_idx", "%", "5000", "==", "0", ":", "\n", "                ", "print", "(", "\"frame: \"", ",", "frame_idx", ")", "\n", "model_path", "=", "'frogger_model'", "+", "'/ppo+adv_gen_git_repo_max_'", "+", "str", "(", "frame_idx", ")", "+", "'.pkl'", "\n", "torch", ".", "save", "(", "model", ",", "model_path", ")", "\n", "print", "(", "\"model saved ..........\"", ")", "\n", "#     test_reward = np.mean([test_env(True) for _ in range(10)])", "\n", "#     test_rewards.append(test_reward)", "\n", "#     plot(frame_idx, test_rewards)", "\n", "#     if test_reward > threshold_reward: early_stop = True", "\n", "\n", "", "if", "frame_idx", "%", "50000", "==", "0", ":", "\n", "                ", "alpha", "=", "alpha", "+", "0.1", "\n", "\n", "\n", "", "", "next_state", "=", "torch", ".", "FloatTensor", "(", "next_state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "goal_state", "=", "torch", ".", "FloatTensor", "(", "init_goal_state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "_", ",", "next_value", "=", "model", "(", "next_state", ",", "goal_state", ")", "\n", "returns", "=", "compute_gae", "(", "next_value", ",", "rewards", ",", "masks", ",", "values", ")", "\n", "\n", "returns", "=", "torch", ".", "cat", "(", "returns", ")", ".", "detach", "(", ")", "\n", "log_probs", "=", "torch", ".", "cat", "(", "log_probs", ")", ".", "detach", "(", ")", "\n", "values", "=", "torch", ".", "cat", "(", "values", ")", ".", "detach", "(", ")", "\n", "states", "=", "torch", ".", "cat", "(", "states", ")", "\n", "goal_states", "=", "torch", ".", "cat", "(", "goal_states", ")", "\n", "actions", "=", "torch", ".", "cat", "(", "actions", ")", "\n", "advantage", "=", "returns", "-", "values", "\n", "\n", "ppo_update", "(", "ppo_epochs", ",", "mini_batch_size", ",", "states", ",", "goal_states", ",", "actions", ",", "log_probs", ",", "returns", ",", "advantage", ")", "\n", "\n", "", "pickle", ".", "dump", "(", "training_rewards", ",", "open", "(", "\"results/rewards_ppo+adv_argmax_gen_git_repo.p\"", ",", "\"wb\"", ")", ")", "\n", "fig1", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "12", ",", "9", ")", ")", "\n", "plt", ".", "plot", "(", "training_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'no_episode'", ",", "fontsize", "=", "13", ")", "\n", "plt", ".", "ylabel", "(", "'reward'", ",", "fontsize", "=", "13", ")", "\n", "plt", ".", "show", "(", "fig1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.models_pretrained_we.Encoder.__init__": [[13, 27], ["torch.nn.Module.__init__", "torchvision.models.resnet101", "torch.nn.Sequential", "torch.nn.AdaptiveAvgPool2d", "models_pretrained_we.Encoder.fine_tune", "list", "torchvision.models.resnet101.children"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.Encoder.fine_tune"], ["def", "__init__", "(", "self", ",", "encoded_image_size", "=", "14", ")", ":", "\n", "        ", "super", "(", "Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "enc_image_size", "=", "encoded_image_size", "\n", "\n", "resnet", "=", "torchvision", ".", "models", ".", "resnet101", "(", "pretrained", "=", "True", ")", "# pretrained ImageNet ResNet-101", "\n", "\n", "# Remove linear and pool layers (since we're not doing classification)", "\n", "modules", "=", "list", "(", "resnet", ".", "children", "(", ")", ")", "[", ":", "-", "2", "]", "\n", "self", ".", "resnet", "=", "nn", ".", "Sequential", "(", "*", "modules", ")", "\n", "\n", "# Resize image to fixed size to allow input images of variable size", "\n", "self", ".", "adaptive_pool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "(", "encoded_image_size", ",", "encoded_image_size", ")", ")", "\n", "\n", "self", ".", "fine_tune", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.models_pretrained_we.Encoder.forward": [[28, 39], ["models_pretrained_we.Encoder.resnet", "models_pretrained_we.Encoder.adaptive_pool", "out.permute.permute.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "images", ")", ":", "\n", "        ", "\"\"\"\n        Forward propagation.\n\n        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n        :return: encoded images\n        \"\"\"", "\n", "out", "=", "self", ".", "resnet", "(", "images", ")", "# (batch_size, 2048, image_size/32, image_size/32)", "\n", "out", "=", "self", ".", "adaptive_pool", "(", "out", ")", "# (batch_size, 2048, encoded_image_size, encoded_image_size)", "\n", "out", "=", "out", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "# (batch_size, encoded_image_size, encoded_image_size, 2048)", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.models_pretrained_we.Encoder.fine_tune": [[40, 52], ["models_pretrained_we.Encoder.resnet.parameters", "list", "c.parameters", "models_pretrained_we.Encoder.resnet.children"], "methods", ["None"], ["", "def", "fine_tune", "(", "self", ",", "fine_tune", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n\n        :param fine_tune: Allow?\n        \"\"\"", "\n", "for", "p", "in", "self", ".", "resnet", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "# If fine-tuning, only fine-tune convolutional blocks 2 through 4", "\n", "", "for", "c", "in", "list", "(", "self", ".", "resnet", ".", "children", "(", ")", ")", "[", "5", ":", "]", ":", "\n", "            ", "for", "p", "in", "c", ".", "parameters", "(", ")", ":", "\n", "                ", "p", ".", "requires_grad", "=", "fine_tune", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.models_pretrained_we.Attention.__init__": [[59, 71], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Softmax"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["def", "__init__", "(", "self", ",", "encoder_dim", ",", "decoder_dim", ",", "attention_dim", ")", ":", "\n", "        ", "\"\"\"\n        :param encoder_dim: feature size of encoded images\n        :param decoder_dim: size of decoder's RNN\n        :param attention_dim: size of the attention network\n        \"\"\"", "\n", "super", "(", "Attention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder_att", "=", "nn", ".", "Linear", "(", "encoder_dim", ",", "attention_dim", ")", "# linear layer to transform encoded image", "\n", "self", ".", "decoder_att", "=", "nn", ".", "Linear", "(", "decoder_dim", ",", "attention_dim", ")", "# linear layer to transform decoder's output", "\n", "self", ".", "full_att", "=", "nn", ".", "Linear", "(", "attention_dim", ",", "1", ")", "# linear layer to calculate values to be softmax-ed", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "# softmax layer to calculate weights", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.models_pretrained_we.Attention.forward": [[72, 87], ["models_pretrained_we.Attention.encoder_att", "models_pretrained_we.Attention.decoder_att", "models_pretrained_we.Attention.full_att().squeeze", "models_pretrained_we.Attention.softmax", "models_pretrained_we.Attention.full_att", "models_pretrained_we.Attention.relu", "models_pretrained_we.Attention.unsqueeze", "models_pretrained_we.Attention.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "encoder_out", ",", "decoder_hidden", ")", ":", "\n", "        ", "\"\"\"\n        Forward propagation.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n        :return: attention weighted encoding, weights\n        \"\"\"", "\n", "att1", "=", "self", ".", "encoder_att", "(", "encoder_out", ")", "# (batch_size, num_pixels, attention_dim)", "\n", "att2", "=", "self", ".", "decoder_att", "(", "decoder_hidden", ")", "# (batch_size, attention_dim)", "\n", "att", "=", "self", ".", "full_att", "(", "self", ".", "relu", "(", "att1", "+", "att2", ".", "unsqueeze", "(", "1", ")", ")", ")", ".", "squeeze", "(", "2", ")", "# (batch_size, num_pixels)", "\n", "alpha", "=", "self", ".", "softmax", "(", "att", ")", "# (batch_size, num_pixels)", "\n", "attention_weighted_encoding", "=", "(", "encoder_out", "*", "alpha", ".", "unsqueeze", "(", "2", ")", ")", ".", "sum", "(", "dim", "=", "1", ")", "# (batch_size, encoder_dim)", "\n", "\n", "return", "attention_weighted_encoding", ",", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.models_pretrained_we.DecoderWithAttention.__init__": [[94, 123], ["torch.nn.Module.__init__", "models_pretrained_we.Attention", "torch.nn.Embedding", "torch.nn.Dropout", "torch.nn.LSTMCell", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Sigmoid", "torch.nn.Linear", "models_pretrained_we.DecoderWithAttention.init_weights"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_weights"], ["def", "__init__", "(", "self", ",", "attention_dim", ",", "embed_dim", ",", "decoder_dim", ",", "vocab_size", ",", "encoder_dim", "=", "2048", ",", "dropout", "=", "0.5", ")", ":", "\n", "        ", "\"\"\"\n        :param attention_dim: size of attention network\n        :param embed_dim: embedding size\n        :param decoder_dim: size of decoder's RNN\n        :param vocab_size: size of vocabulary\n        :param encoder_dim: feature size of encoded images\n        :param dropout: dropout\n        \"\"\"", "\n", "super", "(", "DecoderWithAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "encoder_dim", "=", "encoder_dim", "\n", "self", ".", "attention_dim", "=", "attention_dim", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "decoder_dim", "=", "decoder_dim", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n", "self", ".", "attention", "=", "Attention", "(", "encoder_dim", ",", "decoder_dim", ",", "attention_dim", ")", "# attention network", "\n", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embed_dim", ")", "# embedding layer", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout", ")", "\n", "self", ".", "decode_step", "=", "nn", ".", "LSTMCell", "(", "embed_dim", "+", "encoder_dim", ",", "decoder_dim", ",", "bias", "=", "True", ")", "# decoding LSTMCell", "\n", "self", ".", "init_h", "=", "nn", ".", "Linear", "(", "encoder_dim", ",", "decoder_dim", ")", "# linear layer to find initial hidden state of LSTMCell", "\n", "self", ".", "init_c", "=", "nn", ".", "Linear", "(", "encoder_dim", ",", "decoder_dim", ")", "# linear layer to find initial cell state of LSTMCell", "\n", "self", ".", "f_beta", "=", "nn", ".", "Linear", "(", "decoder_dim", ",", "encoder_dim", ")", "# linear layer to create a sigmoid-activated gate", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "decoder_dim", ",", "vocab_size", ")", "# linear layer to find scores over vocabulary", "\n", "self", ".", "init_weights", "(", ")", "# initialize some layers with the uniform distribution", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.models_pretrained_we.DecoderWithAttention.init_weights": [[124, 131], ["models_pretrained_we.DecoderWithAttention.embedding.weight.data.uniform_", "models_pretrained_we.DecoderWithAttention.fc.bias.data.fill_", "models_pretrained_we.DecoderWithAttention.fc.weight.data.uniform_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initializes some parameters with values from the uniform distribution, for easier convergence.\n        \"\"\"", "\n", "self", ".", "embedding", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "self", ".", "fc", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "self", ".", "fc", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.models_pretrained_we.DecoderWithAttention.load_pretrained_embeddings": [[132, 139], ["torch.nn.Parameter"], "methods", ["None"], ["", "def", "load_pretrained_embeddings", "(", "self", ",", "embeddings", ")", ":", "\n", "        ", "\"\"\"\n        Loads embedding layer with pre-trained embeddings.\n\n        :param embeddings: pre-trained embeddings\n        \"\"\"", "\n", "self", ".", "embedding", ".", "weight", "=", "nn", ".", "Parameter", "(", "embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.models_pretrained_we.DecoderWithAttention.init_word_embedding": [[140, 143], ["None"], "methods", ["None"], ["", "def", "init_word_embedding", "(", "self", ",", "weight_init", ")", ":", "\n", "        ", "assert", "weight_init", ".", "shape", "==", "(", "self", ".", "vocab_size", ",", "self", ".", "embed_dim", ")", "\n", "self", ".", "embedding", ".", "weight", ".", "data", "[", ":", "self", ".", "vocab_size", "]", "=", "weight_init", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.models_pretrained_we.DecoderWithAttention.fine_tune_embeddings": [[144, 152], ["models_pretrained_we.DecoderWithAttention.embedding.parameters"], "methods", ["None"], ["", "def", "fine_tune_embeddings", "(", "self", ",", "fine_tune", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n\n        :param fine_tune: Allow?\n        \"\"\"", "\n", "for", "p", "in", "self", ".", "embedding", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "fine_tune", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.models_pretrained_we.DecoderWithAttention.init_hidden_state": [[153, 164], ["encoder_out.mean", "models_pretrained_we.DecoderWithAttention.init_h", "models_pretrained_we.DecoderWithAttention.init_c"], "methods", ["None"], ["", "", "def", "init_hidden_state", "(", "self", ",", "encoder_out", ")", ":", "\n", "        ", "\"\"\"\n        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :return: hidden state, cell state\n        \"\"\"", "\n", "mean_encoder_out", "=", "encoder_out", ".", "mean", "(", "dim", "=", "1", ")", "\n", "h", "=", "self", ".", "init_h", "(", "mean_encoder_out", ")", "# (batch_size, decoder_dim)", "\n", "c", "=", "self", ".", "init_c", "(", "mean_encoder_out", ")", "\n", "return", "h", ",", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.models_pretrained_we.DecoderWithAttention.forward": [[165, 220], ["encoder_out.view.view.size", "encoder_out.view.view.size", "encoder_out.view.view.view", "encoder_out.view.view.size", "caption_lengths.squeeze().sort", "models_pretrained_we.DecoderWithAttention.embedding", "models_pretrained_we.DecoderWithAttention.init_hidden_state", "torch.zeros().to", "torch.zeros().to", "range", "max", "sum", "models_pretrained_we.DecoderWithAttention.attention", "models_pretrained_we.DecoderWithAttention.sigmoid", "models_pretrained_we.DecoderWithAttention.decode_step", "models_pretrained_we.DecoderWithAttention.fc", "caption_lengths.squeeze", "torch.zeros", "torch.zeros", "models_pretrained_we.DecoderWithAttention.f_beta", "torch.cat", "models_pretrained_we.DecoderWithAttention.dropout", "max", "max"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.DecoderWithAttention.init_hidden_state"], ["", "def", "forward", "(", "self", ",", "encoder_out", ",", "encoded_captions", ",", "caption_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Forward propagation.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"", "\n", "\n", "batch_size", "=", "encoder_out", ".", "size", "(", "0", ")", "\n", "encoder_dim", "=", "encoder_out", ".", "size", "(", "-", "1", ")", "\n", "vocab_size", "=", "self", ".", "vocab_size", "\n", "\n", "# Flatten image", "\n", "encoder_out", "=", "encoder_out", ".", "view", "(", "batch_size", ",", "-", "1", ",", "encoder_dim", ")", "# (batch_size, num_pixels, encoder_dim)", "\n", "num_pixels", "=", "encoder_out", ".", "size", "(", "1", ")", "\n", "\n", "# Sort input data by decreasing lengths; why? apparent below", "\n", "\n", "caption_lengths", ",", "sort_ind", "=", "caption_lengths", ".", "squeeze", "(", "1", ")", ".", "sort", "(", "dim", "=", "0", ",", "descending", "=", "True", ")", "\n", "encoder_out", "=", "encoder_out", "[", "sort_ind", "]", "\n", "encoded_captions", "=", "encoded_captions", "[", "sort_ind", "]", "\n", "\n", "# Embedding", "\n", "embeddings", "=", "self", ".", "embedding", "(", "encoded_captions", ")", "# (batch_size, max_caption_length, embed_dim)", "\n", "\n", "# Initialize LSTM state", "\n", "h", ",", "c", "=", "self", ".", "init_hidden_state", "(", "encoder_out", ")", "# (batch_size, decoder_dim)", "\n", "\n", "# We won't decode at the <end> position, since we've finished generating as soon as we generate <end>", "\n", "# So, decoding lengths are actual lengths - 1", "\n", "decode_lengths", "=", "(", "caption_lengths", "-", "1", ")", ".", "tolist", "(", ")", "\n", "\n", "# Create tensors to hold word predicion scores and alphas", "\n", "predictions", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max", "(", "decode_lengths", ")", ",", "vocab_size", ")", ".", "to", "(", "device", ")", "\n", "alphas", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max", "(", "decode_lengths", ")", ",", "num_pixels", ")", ".", "to", "(", "device", ")", "\n", "\n", "# At each time-step, decode by", "\n", "# attention-weighing the encoder's output based on the decoder's previous hidden state output", "\n", "# then generate a new word in the decoder with the previous word and the attention weighted encoding", "\n", "for", "t", "in", "range", "(", "max", "(", "decode_lengths", ")", ")", ":", "\n", "            ", "batch_size_t", "=", "sum", "(", "[", "l", ">", "t", "for", "l", "in", "decode_lengths", "]", ")", "\n", "attention_weighted_encoding", ",", "alpha", "=", "self", ".", "attention", "(", "encoder_out", "[", ":", "batch_size_t", "]", ",", "\n", "h", "[", ":", "batch_size_t", "]", ")", "\n", "gate", "=", "self", ".", "sigmoid", "(", "self", ".", "f_beta", "(", "h", "[", ":", "batch_size_t", "]", ")", ")", "# gating scalar, (batch_size_t, encoder_dim)", "\n", "attention_weighted_encoding", "=", "gate", "*", "attention_weighted_encoding", "\n", "h", ",", "c", "=", "self", ".", "decode_step", "(", "\n", "torch", ".", "cat", "(", "[", "embeddings", "[", ":", "batch_size_t", ",", "t", ",", ":", "]", ",", "attention_weighted_encoding", "]", ",", "dim", "=", "1", ")", ",", "\n", "(", "h", "[", ":", "batch_size_t", "]", ",", "c", "[", ":", "batch_size_t", "]", ")", ")", "# (batch_size_t, decoder_dim)", "\n", "preds", "=", "self", ".", "fc", "(", "self", ".", "dropout", "(", "h", ")", ")", "# (batch_size_t, vocab_size)", "\n", "predictions", "[", ":", "batch_size_t", ",", "t", ",", ":", "]", "=", "preds", "\n", "alphas", "[", ":", "batch_size_t", ",", "t", ",", ":", "]", "=", "alpha", "\n", "\n", "", "return", "predictions", ",", "encoded_captions", ",", "decode_lengths", ",", "alphas", ",", "sort_ind", "\n", "", "", ""]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.game_access_t.Game.__init__": [[163, 166], ["game_access_t.Game.load_env"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.load_env"], ["def", "__init__", "(", "self", ",", "game_location", ")", ":", "\n", "        ", "self", ".", "ENV_LOCATION", "=", "game_location", "\n", "self", ".", "load_env", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.game_access_t.Game.load_env": [[171, 180], ["mlagents.envs.UnityEnvironment", "game_access_t.Game.env.reset"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.reset"], ["def", "load_env", "(", "self", ",", "wid", ")", ":", "\n", "# load env", "\n", "        ", "env_name", "=", "self", ".", "ENV_LOCATION", "\n", "self", ".", "env", "=", "UnityEnvironment", "(", "env_name", ",", "worker_id", "=", "wid", ")", "\n", "# Set the default brain to work with", "\n", "self", ".", "default_brain", "=", "self", ".", "env", ".", "brain_names", "[", "0", "]", "\n", "self", ".", "brain", "=", "self", ".", "env", ".", "brains", "[", "self", ".", "default_brain", "]", "\n", "# Reset the environment - train mode enabled", "\n", "env_info", "=", "self", ".", "env", ".", "reset", "(", "train_mode", "=", "True", ")", "[", "self", ".", "default_brain", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.game_access_t.Game.perform_action": [[195, 259], ["numpy.zeros", "numpy.zeros", "round", "skimage.color.rgb2gray", "skimage.transform.resize", "numpy.reshape", "skimage.transform.resize", "numpy.reshape", "range", "game_access_t.Game.env.step", "skimage.color.rgb2gray", "skimage.transform.resize", "numpy.reshape", "skimage.transform.resize", "numpy.reshape", "range", "game_access_t.Game.env.step", "round"], "methods", ["None"], ["def", "perform_action", "(", "self", ",", "action_value", ",", "image_height", ",", "image_width", ",", "image_height1", ",", "image_width1", ",", "number_of_frames", "=", "4", ")", ":", "\n", "#print(\"action_value: \",action_value)", "\n", "        ", "action", "=", "[", "[", "0", "]", "]", "\n", "#print(\"action: \",action)", "\n", "action", "[", "0", "]", "=", "action_value", "\n", "#print(\"action[0]: \",action[0])", "\n", "terminal", "=", "False", "# indication of terminal state", "\n", "# 3 - R, G, B", "\n", "#size = (image_height, image_width, 3, number_of_frames)  # create list to keep frames", "\n", "size", "=", "(", "number_of_frames", ",", "1", ",", "image_height", ",", "image_width", ")", "\n", "adv_size", "=", "(", "number_of_frames", ",", "3", ",", "image_height1", ",", "image_width1", ")", "\n", "\n", "stack", "=", "np", ".", "zeros", "(", "size", ")", "\n", "adv_stack", "=", "np", ".", "zeros", "(", "adv_size", ")", "\n", "\n", "#stack = torch.zeros(size)", "\n", "# to store data to sym rep 32 data is send from game", "\n", "observation_data", "=", "[", "[", "0", "]", "*", "34", "for", "i", "in", "range", "(", "number_of_frames", ")", "]", "\n", "\n", "# first frame after action", "\n", "env_info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "[", "self", ".", "default_brain", "]", "# send action to brain", "\n", "reward", "=", "round", "(", "env_info", ".", "rewards", "[", "0", "]", ",", "5", ")", "# get reward", "\n", "new_state", "=", "env_info", ".", "visual_observations", "[", "0", "]", "[", "0", "]", "# get state visual observation", "\n", "observations", "=", "env_info", ".", "vector_observations", "# get vector observations", "\n", "observation_data", "[", "0", "]", "=", "observations", "[", "0", "]", "\n", "new_state_gray", "=", "skimage", ".", "color", ".", "rgb2gray", "(", "new_state", ")", "# covert to gray scale", "\n", "new_state_gray", "=", "skimage", ".", "transform", ".", "resize", "(", "new_state_gray", ",", "(", "image_height", ",", "image_width", ")", ")", "# resize", "\n", "new_state_reshape", "=", "np", ".", "reshape", "(", "new_state_gray", ",", "(", "1", ",", "image_height", ",", "image_width", ")", ")", "\n", "new_state_adv", "=", "skimage", ".", "transform", ".", "resize", "(", "new_state", ",", "(", "image_height1", ",", "image_width1", ")", ")", "\n", "state_adv_reshape", "=", "np", ".", "reshape", "(", "new_state_adv", ",", "(", "3", ",", "image_height1", ",", "image_width1", ")", ")", "\n", "\n", "# check terminal reached", "\n", "if", "env_info", ".", "local_done", "[", "0", "]", ":", "\n", "            ", "terminal", "=", "True", "\n", "\n", "# add the state to the 0 th position of stack", "\n", "", "stack", "[", "0", ",", ":", ",", ":", ",", ":", "]", "=", "new_state_reshape", "\n", "#a2c_stack [0,:,:,:] = state_a2c_reshape", "\n", "\n", "# get stack of frames after the action", "\n", "for", "i", "in", "range", "(", "1", ",", "number_of_frames", ")", ":", "\n", "            ", "env_info", "=", "self", ".", "env", ".", "step", "(", ")", "[", "self", ".", "default_brain", "]", "# change environment to next step without action", "\n", "st", "=", "env_info", ".", "visual_observations", "[", "0", "]", "[", "0", "]", "\n", "\n", "observations", "=", "env_info", ".", "vector_observations", "# get vector observations", "\n", "observation_data", "[", "i", "]", "=", "observations", "[", "0", "]", "\n", "st_gray", "=", "skimage", ".", "color", ".", "rgb2gray", "(", "st", ")", "\n", "st_gray", "=", "skimage", ".", "transform", ".", "resize", "(", "st_gray", ",", "(", "image_height", ",", "image_width", ")", ")", "\n", "st_gray_reshape", "=", "np", ".", "reshape", "(", "st_gray", ",", "(", "1", ",", "image_height", ",", "image_width", ")", ")", "\n", "adv_st", "=", "skimage", ".", "transform", ".", "resize", "(", "st", ",", "(", "image_height1", ",", "image_width1", ")", ")", "\n", "adv_st_reshape", "=", "np", ".", "reshape", "(", "adv_st", ",", "(", "3", ",", "image_height1", ",", "image_width1", ")", ")", "\n", "stack", "[", "i", ",", ":", ",", ":", ",", ":", "]", "=", "st_gray_reshape", "\n", "adv_stack", "[", "i", ",", ":", ",", ":", ",", ":", "]", "=", "adv_st_reshape", "\n", "# if terminal only consider the reward for terminal", "\n", "if", "env_info", ".", "local_done", "[", "0", "]", ":", "\n", "                ", "terminal", "=", "True", "\n", "reward", "=", "round", "(", "env_info", ".", "rewards", "[", "0", "]", ",", "5", ")", "\n", "\n", "# reshape for Keras", "\n", "# noinspection PyArgumentList", "\n", "#stack = stack.reshape(1, stack.shape[0], stack.shape[1], stack.shape[2], stack.shape[3])", "\n", "\n", "#return reward, stack, terminal, observation_data", "\n", "", "", "return", "reward", ",", "stack", ",", "adv_stack", ",", "terminal", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.game_access_t.Game.close": [[264, 266], ["game_access_t.Game.env.close"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.close"], ["def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.game_access_t.Game.reload": [[270, 273], ["game_access_t.Game.close", "game_access_t.Game.load_env"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.close", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.load_env"], ["def", "reload", "(", "self", ")", ":", "\n", "        ", "self", ".", "close", "(", ")", "\n", "self", ".", "load_env", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.game_access_t.Game.reset": [[274, 276], ["game_access_t.Game.env.reset"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.reset"], ["", "def", "reset", "(", "self", ",", "train_mode", "=", "True", ")", ":", "\n", "        ", "self", ".", "env", ".", "reset", "(", "train_mode", "=", "train_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.game_access_t.create_sym_rep": [[29, 156], ["int", "int", "int", "int", "range", "range", "range", "range", "range", "range", "range", "range", "range", "int", "int", "int", "range", "range", "range", "range", "range", "range", "range"], "function", ["None"], ["", "def", "create_sym_rep", "(", "observation_data", ")", ":", "\n", "# frog in goal and witch ine is it ?", "\n", "    ", "frog_in_goal", "=", "int", "(", "observation_data", "[", "33", "]", ")", "# 0- if none , 1 - for right most one and 2 for left one", "\n", "# is car in tunnel", "\n", "car_in_tunnel", "=", "int", "(", "observation_data", "[", "32", "]", ")", "# 1 if car is inside the tunnel", "\n", "# frog position send them as the index for the state array", "\n", "# (12,8) will be the start position", "\n", "# to calculate x position (original x value - 50) / 25 and round this to nearest int (coz of the logs)", "\n", "# y should be fixed and when it go up or down +1 or - 1", "\n", "frog_x", "=", "int", "(", "observation_data", "[", "0", "]", ")", "\n", "frog_y", "=", "int", "(", "observation_data", "[", "1", "]", ")", "\n", "\n", "current_state", "=", "[", "[", "0", "]", "*", "Constant", ".", "STATE_WIDTH", "for", "i", "in", "range", "(", "Constant", ".", "STATE_HEIGHT", ")", "]", "\n", "\n", "# reset state part", "\n", "\n", "# init all to empty", "\n", "# only need to do this from 5 - 11 index (road part)", "\n", "\n", "for", "i", "in", "range", "(", "5", ",", "12", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "0", ",", "Constant", ".", "STATE_WIDTH", ")", ":", "\n", "            ", "current_state", "[", "i", "]", "[", "j", "]", "=", "StateKey", ".", "EMPTY", ".", "value", "\n", "\n", "# add platform and start pos", "\n", "# 13 - start , 12,4 platform", "\n", "", "", "for", "j", "in", "range", "(", "0", ",", "Constant", ".", "STATE_WIDTH", ")", ":", "\n", "        ", "current_state", "[", "13", "]", "[", "j", "]", "=", "StateKey", ".", "START", ".", "value", "\n", "current_state", "[", "12", "]", "[", "j", "]", "=", "StateKey", ".", "PLATFORM", ".", "value", "\n", "current_state", "[", "4", "]", "[", "j", "]", "=", "StateKey", ".", "PLATFORM", ".", "value", "\n", "\n", "# fill with hazard zone", "\n", "# rows 0 - 3 and col - all", "\n", "", "for", "i", "in", "range", "(", "0", ",", "4", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "0", ",", "Constant", ".", "STATE_WIDTH", ")", ":", "\n", "            ", "current_state", "[", "i", "]", "[", "j", "]", "=", "StateKey", ".", "HAZARD_ZONE", ".", "value", "\n", "\n", "# add goal positions", "\n", "# if frog is in the goal mark it as hazard ???", "\n", "# goal pos 1 is in 5 - 6 col in 0 th row", "\n", "", "", "for", "j", "in", "range", "(", "5", ",", "7", ")", ":", "\n", "# frog is in 1st goal pos", "\n", "        ", "if", "frog_in_goal", "==", "1", ":", "\n", "            ", "current_state", "[", "0", "]", "[", "j", "]", "=", "StateKey", ".", "HAZARD_ZONE", ".", "value", "\n", "", "else", ":", "\n", "            ", "current_state", "[", "0", "]", "[", "j", "]", "=", "StateKey", ".", "WIN", ".", "value", "\n", "\n", "# goal pos 2 is in 10 - 11 col in 0 th row", "\n", "", "", "for", "j", "in", "range", "(", "10", ",", "12", ")", ":", "\n", "# frog is in 2nd goal pos", "\n", "        ", "if", "frog_in_goal", "==", "2", ":", "\n", "            ", "current_state", "[", "0", "]", "[", "j", "]", "=", "StateKey", ".", "HAZARD_ZONE", ".", "value", "\n", "", "else", ":", "\n", "            ", "current_state", "[", "0", "]", "[", "j", "]", "=", "StateKey", ".", "WIN", ".", "value", "\n", "\n", "# mark tunnel", "\n", "# if car in tunnel mark it as lit  ow as a wall", "\n", "# its in row 8 col [6-12]", "\n", "", "", "for", "j", "in", "range", "(", "6", ",", "13", ")", ":", "\n", "# frog is in 2nd goal pos", "\n", "        ", "if", "car_in_tunnel", "==", "1", ":", "\n", "            ", "current_state", "[", "8", "]", "[", "j", "]", "=", "StateKey", ".", "TUNNEL_ON_KEY", ".", "value", "\n", "", "else", ":", "\n", "            ", "current_state", "[", "8", "]", "[", "j", "]", "=", "StateKey", ".", "WALL", ".", "value", "\n", "\n", "# frog position", "\n", "# from game I can directly send frog x , y as a index for this array", "\n", "", "", "current_state", "[", "frog_y", "]", "[", "frog_x", "]", "=", "StateKey", ".", "FROG", ".", "value", "\n", "\n", "# Add cars / logs", "\n", "# array  observations [2 - 20]", "\n", "# s , x , z", "\n", "\n", "index", "=", "1", "# easy to travel the array 1st car y data at 4 th index", "\n", "for", "i", "in", "range", "(", "0", ",", "10", ")", ":", "\n", "        ", "index", "+=", "3", "\n", "x_pos", "=", "int", "(", "observation_data", "[", "index", "-", "1", "]", ")", "\n", "y_pos", "=", "int", "(", "observation_data", "[", "index", "]", ")", "\n", "size", "=", "int", "(", "observation_data", "[", "index", "-", "2", "]", ")", "\n", "\n", "# out of view car /log", "\n", "if", "x_pos", "+", "size", "<=", "0", "or", "x_pos", ">", "16", ":", "\n", "            ", "continue", "\n", "", "elif", "x_pos", "<", "0", "<", "x_pos", "+", "size", ":", "\n", "            ", "size", "=", "size", "+", "x_pos", "\n", "x_pos", "=", "0", "\n", "# car in tunnel lane", "\n", "", "if", "y_pos", "==", "8", ":", "\n", "# car is fully in tunnel", "\n", "            ", "if", "x_pos", ">=", "6", "and", "x_pos", "+", "size", "<=", "12", ":", "\n", "                ", "continue", "\n", "# car is left and in tunnel", "\n", "", "elif", "x_pos", "<", "6", "<=", "x_pos", "+", "size", ":", "\n", "                ", "for", "j", "in", "range", "(", "x_pos", ",", "6", ")", ":", "\n", "                    ", "current_state", "[", "y_pos", "]", "[", "j", "]", "=", "StateKey", ".", "RED_CAR", ".", "value", "\n", "", "", "elif", "x_pos", "<=", "12", "<", "x_pos", "+", "size", ":", "\n", "                ", "for", "j", "in", "range", "(", "13", ",", "x_pos", "+", "size", ")", ":", "\n", "                    ", "current_state", "[", "y_pos", "]", "[", "j", "]", "=", "StateKey", ".", "RED_CAR", ".", "value", "\n", "# out of tunnel", "\n", "", "", "else", ":", "\n", "                ", "for", "j", "in", "range", "(", "x_pos", ",", "x_pos", "+", "size", ")", ":", "\n", "                    ", "if", "j", ">", "16", ":", "\n", "                        ", "break", "\n", "", "current_state", "[", "y_pos", "]", "[", "j", "]", "=", "StateKey", ".", "RED_CAR", ".", "value", "\n", "\n", "# all other cars /logs", "\n", "", "", "", "elif", "index", "<=", "32", ":", "\n", "# todo : get car color from game", "\n", "# for blue cars", "\n", "            ", "if", "y_pos", "==", "5", "or", "y_pos", "==", "7", "or", "y_pos", "==", "9", "or", "y_pos", "==", "10", ":", "\n", "                ", "for", "j", "in", "range", "(", "x_pos", ",", "x_pos", "+", "size", ")", ":", "\n", "                    ", "if", "j", ">", "16", ":", "\n", "                        ", "break", "\n", "", "current_state", "[", "y_pos", "]", "[", "j", "]", "=", "StateKey", ".", "BLUE_CAR", ".", "value", "\n", "# for red cars", "\n", "", "", "elif", "y_pos", "==", "6", "or", "y_pos", "==", "8", "or", "y_pos", "==", "11", ":", "\n", "                ", "for", "j", "in", "range", "(", "x_pos", ",", "x_pos", "+", "size", ")", ":", "\n", "                    ", "if", "j", ">", "16", ":", "\n", "                        ", "break", "\n", "", "current_state", "[", "y_pos", "]", "[", "j", "]", "=", "StateKey", ".", "RED_CAR", ".", "value", "\n", "# logs", "\n", "", "", "else", ":", "\n", "                ", "for", "j", "in", "range", "(", "x_pos", ",", "x_pos", "+", "size", ")", ":", "\n", "                    ", "if", "j", ">", "16", ":", "\n", "                        ", "break", "\n", "", "current_state", "[", "y_pos", "]", "[", "j", "]", "=", "StateKey", ".", "LOG", ".", "value", "\n", "\n", "", "", "", "", "return", "current_state", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.ActorCritic.__init__": [[26, 53], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["    ", "def", "__init__", "(", "self", ",", "action_size", ")", ":", "\n", "        ", "super", "(", "ActorCritic", ",", "self", ")", ".", "__init__", "(", ")", "\n", "#self.state_size = state_size", "\n", "self", ".", "action_size", "=", "action_size", "\n", "self", ".", "hidden_size", "=", "512", "\n", "self", ".", "embed_size", "=", "512", "\n", "self", ".", "n_layers", "=", "2", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "1", ",", "16", ",", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "16", ",", "32", ",", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "3", ",", "stride", "=", "1", ")", "\n", "self", ".", "fc_size", "=", "1600", "\n", "goal_state_size", "=", "3", "\n", "# self.conv6 = nn.Conv2d(64,64,3,stride=1)", "\n", "# self.conv7 = nn.Conv2d(64,128,3,stride=1)", "\n", "\n", "# self.drop1 = nn.Dropout2d(0.3)", "\n", "# self.drop2 = nn.Dropout2d(0.3)", "\n", "# self.lstm = nn.LSTM(300,self.num_hid,batch_first=True)", "\n", "\n", "self", ".", "linear1", "=", "nn", ".", "Linear", "(", "self", ".", "fc_size", ",", "self", ".", "embed_size", ")", "\n", "self", ".", "linear2_actor", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_size", "*", "2", ",", "256", ")", "\n", "self", ".", "linear2_critic", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_size", "*", "2", ",", "256", ")", "\n", "# self.linear3 = nn.Linear (512,128)", "\n", "self", ".", "linear_actor", "=", "nn", ".", "Linear", "(", "256", "+", "goal_state_size", ",", "self", ".", "action_size", ")", "\n", "self", ".", "linear_critic", "=", "nn", ".", "Linear", "(", "256", "+", "goal_state_size", ",", "1", ")", "\n", "self", ".", "actor_lstm", "=", "nn", ".", "LSTM", "(", "self", ".", "embed_size", ",", "self", ".", "hidden_size", ",", "self", ".", "n_layers", ",", "batch_first", "=", "True", ",", "dropout", "=", "0.3", ")", "\n", "self", ".", "critic_lstm", "=", "nn", ".", "LSTM", "(", "self", ".", "embed_size", ",", "self", ".", "hidden_size", ",", "self", ".", "n_layers", ",", "batch_first", "=", "True", ",", "dropout", "=", "0.3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.ActorCritic.init_hidden": [[54, 63], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "h0.cuda.cuda.cuda", "c0.cuda.cuda.cuda"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "h0", "=", "torch", ".", "zeros", "(", "1", "*", "self", ".", "n_layers", ",", "batch_size", ",", "self", ".", "hidden_size", ")", "\n", "c0", "=", "torch", ".", "zeros", "(", "1", "*", "self", ".", "n_layers", ",", "batch_size", ",", "self", ".", "hidden_size", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "h0", "=", "h0", ".", "cuda", "(", ")", "\n", "c0", "=", "c0", ".", "cuda", "(", ")", "\n", "\n", "", "return", "(", "h0", ",", "c0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.ActorCritic.forward": [[64, 105], ["experience_driven_agent.ActorCritic.init_hidden", "experience_driven_agent.ActorCritic.init_hidden", "state.view.view.view", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "experience_driven_agent.ActorCritic.linear1", "experience_driven_agent.ActorCritic.actor_lstm", "actor_h.view.view.view", "experience_driven_agent.ActorCritic.linear2_actor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "experience_driven_agent.ActorCritic.critic_lstm", "critic_h.view.view.view", "experience_driven_agent.ActorCritic.linear2_actor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "experience_driven_agent.ActorCritic.linear_actor", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "experience_driven_agent.ActorCritic.linear_critic", "experience_driven_agent.ActorCritic.conv1", "experience_driven_agent.ActorCritic.conv2", "experience_driven_agent.ActorCritic.conv3"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden"], ["", "def", "forward", "(", "self", ",", "state", ",", "goal_state", ")", ":", "\n", "        ", "batch_size", "=", "state", ".", "shape", "[", "0", "]", "\n", "(", "h0", ",", "c0", ")", "=", "self", ".", "init_hidden", "(", "batch_size", ")", "\n", "(", "_h0", ",", "_c0", ")", "=", "self", ".", "init_hidden", "(", "batch_size", ")", "\n", "state", "=", "state", ".", "view", "(", "-", "1", ",", "1", ",", "IMAGE_WIDTH", ",", "IMAGE_HEIGHT", ")", "\n", "output1", "=", "F", ".", "relu", "(", "self", ".", "conv1", "(", "state", ")", ")", "\n", "output2", "=", "F", ".", "relu", "(", "self", ".", "conv2", "(", "output1", ")", ")", "\n", "output2", "=", "F", ".", "max_pool2d", "(", "output2", ",", "2", ",", "stride", "=", "2", ")", "\n", "\n", "# output2 = self.drop1(output2)", "\n", "\n", "output3", "=", "F", ".", "relu", "(", "self", ".", "conv3", "(", "output2", ")", ")", "\n", "output3", "=", "F", ".", "max_pool2d", "(", "output3", ",", "2", ",", "stride", "=", "2", ")", "\n", "\n", "# output5 = F.relu(self.conv5(output4))", "\n", "# output6 = F.relu(self.conv6(output5))", "\n", "# output7 = F.relu(self.conv7(output6))", "\n", "# output7 = F.max_pool2d(output7,2,stride=2)", "\n", "\n", "\n", "img_emb", "=", "torch", ".", "reshape", "(", "output3", ",", "(", "-", "1", ",", "STACK_SIZE", ",", "self", ".", "fc_size", ")", ")", "\n", "img_emb", "=", "self", ".", "linear1", "(", "img_emb", ")", "\n", "\n", "output", ",", "(", "actor_h", ",", "actor_c", ")", "=", "self", ".", "actor_lstm", "(", "img_emb", ",", "(", "h0", ",", "c0", ")", ")", "\n", "actor_h", "=", "actor_h", ".", "view", "(", "-", "1", ",", "self", ".", "hidden_size", "*", "2", ")", "\n", "fc_actor", "=", "self", ".", "linear2_actor", "(", "actor_h", ")", "\n", "fc_actor", "=", "torch", ".", "cat", "(", "(", "fc_actor", ",", "goal_state", ")", ",", "1", ")", "\n", "\n", "output", ",", "(", "critic_h", ",", "critic_c", ")", "=", "self", ".", "critic_lstm", "(", "img_emb", ",", "(", "_h0", ",", "_c0", ")", ")", "\n", "critic_h", "=", "critic_h", ".", "view", "(", "-", "1", ",", "self", ".", "hidden_size", "*", "2", ")", "\n", "fc_critic", "=", "self", ".", "linear2_actor", "(", "critic_h", ")", "\n", "fc_critic", "=", "torch", ".", "cat", "(", "(", "fc_critic", ",", "goal_state", ")", ",", "1", ")", "\n", "\n", "\n", "dist_actor", "=", "self", ".", "linear_actor", "(", "fc_actor", ")", "\n", "prob_actor", "=", "F", ".", "softmax", "(", "dist_actor", ",", "dim", "=", "-", "1", ")", "\n", "dist_policy", "=", "Categorical", "(", "prob_actor", ")", "\n", "# dist = Categorical(F.softmax(dist_actor, dim=-1))", "\n", "value_critic", "=", "self", ".", "linear_critic", "(", "fc_critic", ")", "\n", "\n", "return", "dist_policy", ",", "value_critic", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.plot": [[106, 113], ["matplotlib.figure", "matplotlib.title", "matplotlib.plot", "matplotlib.show"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.plot"], ["", "", "def", "plot", "(", "frame_idx", ",", "rewards", ")", ":", "\n", "# clear_output(True)", "\n", "    ", "plt", ".", "figure", "(", "figsize", "=", "(", "16", ",", "8", ")", ")", "\n", "# plt.subplot(131)", "\n", "plt", ".", "title", "(", "'frame %s. reward: %s'", "%", "(", "frame_idx", ",", "rewards", "[", "-", "1", "]", ")", ")", "\n", "plt", ".", "plot", "(", "rewards", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.test_env": [[114, 127], ["env.reset", "env.render", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "model", "env.step", "dist.sample().cpu().numpy", "env.render", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "dist.sample().cpu", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "dist.sample"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.reset"], ["", "def", "test_env", "(", "vis", "=", "False", ")", ":", "\n", "    ", "state", "=", "env", ".", "reset", "(", ")", "\n", "if", "vis", ":", "env", ".", "render", "(", ")", "\n", "done", "=", "False", "\n", "total_reward", "=", "0", "\n", "while", "not", "done", ":", "\n", "        ", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "dist", ",", "_", "=", "model", "(", "state", ")", "\n", "next_state", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "dist", ".", "sample", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "state", "=", "next_state", "\n", "if", "vis", ":", "env", ".", "render", "(", ")", "\n", "total_reward", "+=", "reward", "\n", "", "return", "total_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.compute_gae": [[129, 144], ["reversed", "range", "returns.insert", "len"], "function", ["None"], ["", "def", "compute_gae", "(", "next_value", ",", "rewards", ",", "masks", ",", "values", ",", "gamma", "=", "0.99", ",", "tau", "=", "0.95", ")", ":", "\n", "    ", "values", "=", "values", "+", "[", "next_value", "]", "\n", "gae", "=", "0", "\n", "returns", "=", "[", "]", "\n", "for", "step", "in", "reversed", "(", "range", "(", "len", "(", "rewards", ")", ")", ")", ":", "\n", "# _v0 = values[step]", "\n", "# _v1 = values[step + 1]", "\n", "# _item1 = gamma * _v1 ", "\n", "# _item1 = _item1 * masks[step]", "\n", "# _item2 = _item1 - _v0", "\n", "        ", "delta", "=", "rewards", "[", "step", "]", "+", "gamma", "*", "values", "[", "step", "+", "1", "]", "*", "masks", "[", "step", "]", "-", "values", "[", "step", "]", "*", "masks", "[", "step", "]", "\n", "gae", "=", "delta", "+", "gamma", "*", "tau", "*", "masks", "[", "step", "]", "*", "gae", "\n", "returns", ".", "insert", "(", "0", ",", "gae", "+", "values", "[", "step", "]", ")", "\n", "# returns.insert(0, gae)", "\n", "", "return", "returns", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.ppo_iter": [[145, 155], ["states.size", "range", "numpy.random.randint"], "function", ["None"], ["", "def", "ppo_iter", "(", "mini_batch_size", ",", "states", ",", "goal_states", ",", "actions", ",", "log_probs", ",", "returns", ",", "advantage", ")", ":", "\n", "    ", "batch_size", "=", "states", ".", "size", "(", "0", ")", "\n", "for", "_", "in", "range", "(", "batch_size", "//", "mini_batch_size", ")", ":", "\n", "        ", "rand_ids", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "batch_size", ",", "mini_batch_size", ")", "\n", "# _states = states[rand_ids, :]", "\n", "# _actions = actions[rand_ids]", "\n", "# _log_probs = log_probs[rand_ids, :]", "\n", "# _returns = returns[rand_ids, :]", "\n", "# _advantage = advantage[rand_ids, :]", "\n", "yield", "states", "[", "rand_ids", ",", ":", "]", ",", "goal_states", "[", "rand_ids", ",", ":", "]", ",", "actions", "[", "rand_ids", "]", ",", "log_probs", "[", "rand_ids", ",", ":", "]", ",", "returns", "[", "rand_ids", ",", ":", "]", ",", "advantage", "[", "rand_ids", ",", ":", "]", "\n", "# yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.ppo_update": [[158, 181], ["model", "dist.entropy().mean", "dist.log_prob", "new_log_probs.unsqueeze.unsqueeze", "optimizer.zero_grad", "loss.backward", "optimizer.step", "open", "open.write", "open.close", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "dist.entropy", "torch.min", "torch.min", "torch.min", "torch.min"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.close"], ["", "", "def", "ppo_update", "(", "ppo_epochs", ",", "mini_batch_size", ",", "states", ",", "goal_states", ",", "actions", ",", "log_probs", ",", "returns", ",", "advantages", ",", "clip_param", "=", "0.2", ")", ":", "\n", "    ", "dist", ",", "values", "=", "model", "(", "states", ",", "goal_states", ")", "\n", "entropy", "=", "dist", ".", "entropy", "(", ")", ".", "mean", "(", ")", "\n", "new_log_probs", "=", "dist", ".", "log_prob", "(", "actions", ")", "\n", "new_log_probs", "=", "new_log_probs", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "ratio", "=", "(", "new_log_probs", "-", "log_probs", ")", ".", "exp", "(", ")", "\n", "surr1", "=", "ratio", "*", "advantages", "\n", "surr2", "=", "torch", ".", "clamp", "(", "ratio", ",", "1.0", "-", "clip_param", ",", "1.0", "+", "clip_param", ")", "*", "advantages", "\n", "\n", "actor_loss", "=", "-", "torch", ".", "min", "(", "surr1", ",", "surr2", ")", ".", "mean", "(", ")", "\n", "critic_loss", "=", "(", "returns", "-", "values", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "loss", "=", "0.5", "*", "critic_loss", "+", "actor_loss", "-", "0.001", "*", "entropy", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "# nn.utils.clip_grad_norm(model.parameters(), 40)", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "loss_log_file", "=", "open", "(", "train_log_loss_file_name", ",", "\"a+\"", ")", "\n", "loss_log_file", ".", "write", "(", "'%d %d %d\\n'", "%", "(", "actor_loss", ",", "critic_loss", ",", "loss", ")", ")", "\n", "loss_log_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.a2c": [[233, 348], ["env.perform_action", "numpy.array", "pickle.dump", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "range", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "model", "experience_driven_agent.compute_gae", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat().detach", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "experience_driven_agent.ppo_update", "open", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "model", "dist.sample", "env.perform_action", "print", "dist.log_prob", "dist.entropy().mean", "torch.cat().detach.append", "torch.cat().detach.append", "rewards.append", "masks.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "env.reset", "open", "open.write", "open.close", "training_rewards.append", "print", "env.perform_action", "numpy.array", "dist.log_prob.unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "print", "torch.save", "torch.save", "torch.save", "torch.save", "print", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "dist.sample.cpu().numpy", "dist.sample.cpu().numpy", "dist.entropy", "str", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "dist.sample.cpu", "dist.sample.cpu"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.perform_action", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.plot", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.compute_gae", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.ml-agents-0.8.1.experience_driven_agent.ppo_update", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.perform_action", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.reset", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.close", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.perform_action"], ["def", "a2c", "(", "env", ")", ":", "\n", "\n", "    ", "frame_idx", "=", "150001", "\n", "action", "=", "0", "\n", "training_rewards", "=", "[", "]", "\n", "total_episode_reward", "=", "0", "\n", "reward", ",", "state", ",", "done", "=", "env", ".", "perform_action", "(", "action", ",", "IMAGE_HEIGHT", ",", "IMAGE_WIDTH", ",", "STACK_SIZE", ")", "\n", "init_goal_state", "=", "np", ".", "array", "(", "[", "0", ",", "0", ",", "0", "]", ")", "\n", "num_episode", "=", "3092", "\n", "num_moves", "=", "0", "\n", "while", "frame_idx", "<", "max_frames", "and", "not", "early_stop", ":", "\n", "\n", "        ", "log_probs", "=", "[", "]", "\n", "values", "=", "[", "]", "\n", "states", "=", "[", "]", "\n", "goal_states", "=", "[", "]", "\n", "actions", "=", "[", "]", "\n", "rewards", "=", "[", "]", "\n", "masks", "=", "[", "]", "\n", "entropy", "=", "0", "\n", "\n", "for", "_i_step", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "num_moves", "+=", "1", "\n", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "goal_state", "=", "torch", ".", "FloatTensor", "(", "init_goal_state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "dist", ",", "value", "=", "model", "(", "state", ",", "goal_state", ")", "\n", "\n", "action", "=", "dist", ".", "sample", "(", ")", "\n", "reward", ",", "next_state", ",", "done", "=", "env", ".", "perform_action", "(", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", ",", "IMAGE_HEIGHT", ",", "IMAGE_WIDTH", ",", "STACK_SIZE", ")", "\n", "# if done:", "\n", "#     reward = -2", "\n", "if", "reward", "==", "10", "and", "init_goal_state", "[", "0", "]", "==", "0", ":", "\n", "                ", "init_goal_state", "[", "0", "]", "=", "1", "\n", "", "elif", "reward", "==", "100", "and", "init_goal_state", "[", "0", "]", "==", "1", "and", "init_goal_state", "[", "1", "]", "==", "0", ":", "\n", "                ", "init_goal_state", "[", "1", "]", "=", "1", "\n", "", "elif", "reward", "==", "400", "and", "init_goal_state", "[", "0", "]", "==", "1", "and", "init_goal_state", "[", "1", "]", "==", "1", "and", "init_goal_state", "[", "2", "]", "==", "0", ":", "\n", "                ", "init_goal_state", "[", "2", "]", "=", "1", "\n", "\n", "", "print", "(", "\"reward: \"", ",", "reward", ",", "\" action: \"", ",", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", ")", "\n", "# if reward < -1:", "\n", "#     reward = -1", "\n", "# elif reward > 1:", "\n", "#     reward = 1", "\n", "total_episode_reward", "+=", "reward", "\n", "if", "done", ":", "\n", "                ", "env", ".", "reset", "(", ")", "\n", "\n", "# write the stats", "\n", "train_log_file", "=", "open", "(", "train_log_file_name", ",", "\"a+\"", ")", "\n", "train_log_file", ".", "write", "(", "'%d %d %d\\n'", "%", "(", "num_episode", ",", "num_moves", ",", "total_episode_reward", ")", ")", "\n", "train_log_file", ".", "close", "(", ")", "\n", "\n", "training_rewards", ".", "append", "(", "total_episode_reward", ")", "\n", "\n", "print", "(", "\"episode no: \"", ",", "num_episode", ",", "\" episode_reward: \"", ",", "total_episode_reward", ",", "\" frame_id: \"", ",", "frame_idx", ")", "\n", "total_episode_reward", "=", "0", "\n", "num_episode", "+=", "1", "\n", "no_action", "=", "0", "\n", "_", ",", "next_state", ",", "_", "=", "env", ".", "perform_action", "(", "no_action", ",", "IMAGE_HEIGHT", ",", "IMAGE_WIDTH", ",", "STACK_SIZE", ")", "\n", "init_goal_state", "=", "np", ".", "array", "(", "[", "0", ",", "0", ",", "0", "]", ")", "\n", "num_moves", "=", "0", "\n", "\n", "\n", "# next_state, reward, done, _ = env.step(action.unsqueeze(0).cpu().numpy())", "\n", "\n", "", "log_prob", "=", "dist", ".", "log_prob", "(", "action", ")", "\n", "entropy", "+=", "dist", ".", "entropy", "(", ")", ".", "mean", "(", ")", "\n", "\n", "log_probs", ".", "append", "(", "log_prob", ".", "unsqueeze", "(", "0", ")", ")", "\n", "values", ".", "append", "(", "value", ")", "\n", "rewards", ".", "append", "(", "torch", ".", "tensor", "(", "[", "reward", "]", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", ")", "\n", "masks", ".", "append", "(", "torch", ".", "tensor", "(", "[", "1", "-", "done", "]", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", ")", "\n", "# rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))", "\n", "# masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))", "\n", "\n", "states", ".", "append", "(", "state", ")", "\n", "goal_states", ".", "append", "(", "goal_state", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "\n", "\n", "state", "=", "next_state", "\n", "frame_idx", "+=", "1", "\n", "\n", "if", "frame_idx", "%", "5000", "==", "0", ":", "\n", "                ", "print", "(", "\"frame: \"", ",", "frame_idx", ")", "\n", "model_path", "=", "'frogger_model'", "+", "'/ppo_spr_rew'", "+", "str", "(", "frame_idx", ")", "+", "'.pkl'", "\n", "torch", ".", "save", "(", "model", ",", "model_path", ")", "\n", "print", "(", "\"model saved ..........\"", ")", "\n", "#     test_reward = np.mean([test_env(True) for _ in range(10)])", "\n", "#     test_rewards.append(test_reward)", "\n", "#     plot(frame_idx, test_rewards)", "\n", "#     if test_reward > threshold_reward: early_stop = True", "\n", "\n", "\n", "", "", "next_state", "=", "torch", ".", "FloatTensor", "(", "next_state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "goal_state", "=", "torch", ".", "FloatTensor", "(", "init_goal_state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "_", ",", "next_value", "=", "model", "(", "next_state", ",", "goal_state", ")", "\n", "returns", "=", "compute_gae", "(", "next_value", ",", "rewards", ",", "masks", ",", "values", ")", "\n", "\n", "returns", "=", "torch", ".", "cat", "(", "returns", ")", ".", "detach", "(", ")", "\n", "log_probs", "=", "torch", ".", "cat", "(", "log_probs", ")", ".", "detach", "(", ")", "\n", "values", "=", "torch", ".", "cat", "(", "values", ")", ".", "detach", "(", ")", "\n", "states", "=", "torch", ".", "cat", "(", "states", ")", "\n", "goal_states", "=", "torch", ".", "cat", "(", "goal_states", ")", "\n", "actions", "=", "torch", ".", "cat", "(", "actions", ")", "\n", "advantage", "=", "returns", "-", "values", "\n", "\n", "ppo_update", "(", "ppo_epochs", ",", "mini_batch_size", ",", "states", ",", "goal_states", ",", "actions", ",", "log_probs", ",", "returns", ",", "advantage", ")", "\n", "\n", "", "pickle", ".", "dump", "(", "training_rewards", ",", "open", "(", "\"results/spr_rewards_ppo.p\"", ",", "\"wb\"", ")", ")", "\n", "fig1", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "12", ",", "9", ")", ")", "\n", "plt", ".", "plot", "(", "training_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'no_episode'", ",", "fontsize", "=", "13", ")", "\n", "plt", ".", "ylabel", "(", "'reward'", ",", "fontsize", "=", "13", ")", "\n", "plt", ".", "show", "(", "fig1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.EncoderCNN.__init__": [[26, 36], ["torch.Module.__init__", "torchvision.resnet50", "torchvision.resnet50.parameters", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "param.requires_grad_", "list", "torchvision.resnet50.children"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embed_size", ")", ":", "\n", "        ", "super", "(", "EncoderCNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "resnet", "=", "models", ".", "resnet50", "(", "pretrained", "=", "True", ")", "\n", "for", "param", "in", "resnet", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad_", "(", "False", ")", "\n", "\n", "", "modules", "=", "list", "(", "resnet", ".", "children", "(", ")", ")", "[", ":", "-", "1", "]", "\n", "self", ".", "resnet", "=", "nn", ".", "Sequential", "(", "*", "modules", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout2d", "(", "p", "=", "0.2", ")", "\n", "self", ".", "embed", "=", "nn", ".", "Linear", "(", "resnet", ".", "fc", ".", "in_features", ",", "embed_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.EncoderCNN.forward": [[37, 48], ["advice_models.EncoderCNN.resnet", "advice_models.EncoderCNN.view", "advice_models.EncoderCNN.dropout", "advice_models.EncoderCNN.embed", "advice_models.EncoderCNN.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "images", ")", ":", "\n", "#print(\"images shape: \", images.shape)", "\n", "        ", "features", "=", "self", ".", "resnet", "(", "images", ")", "\n", "#print(\"features shape: \", features.shape)", "\n", "features", "=", "features", ".", "view", "(", "features", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "#print(\"features shape after view: \", features.shape)", "\n", "features", "=", "self", ".", "dropout", "(", "features", ")", "\n", "#print(\"features shape after dropout: \", features.shape)", "\n", "features", "=", "self", ".", "embed", "(", "features", ")", "\n", "# print(\"features shape after embed: \", features.shape)", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.DecoderRNN.__init__": [[52, 69], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "advice_models.DecoderRNN.init_weights"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "word_embed_size", ",", "image_emb_size", ",", "hidden_size", ",", "vocab_size", ",", "num_layers", "=", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding_layer", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "word_embed_size", ")", "\n", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "input_size", "=", "word_embed_size", "+", "image_emb_size", ",", "hidden_size", "=", "hidden_size", ",", "\n", "num_layers", "=", "num_layers", ",", "batch_first", "=", "True", ")", "\n", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "vocab_size", ")", "\n", "# self.hid_linear1 = nn.Linear(image_emb_size, hidden_size)", "\n", "# self.hid_linear2 = nn.Linear(image_emb_size, hidden_size)", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout2d", "(", "p", "=", "0.5", ")", "\n", "self", ".", "n_layers", "=", "num_layers", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "word_embed_size", "=", "word_embed_size", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "image_emb_size", "=", "image_emb_size", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.DecoderRNN.init_weights": [[70, 73], ["advice_models.DecoderRNN.linear.weight.data.normal_", "advice_models.DecoderRNN.linear.bias.data.fill_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "self", ".", "linear", ".", "weight", ".", "data", ".", "normal_", "(", "0.0", ",", "0.02", ")", "\n", "self", ".", "linear", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "# self.hid_linear1.weight.data.normal_(0.0, 0.02)", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.DecoderRNN.init_word_embedding": [[78, 81], ["None"], "methods", ["None"], ["", "def", "init_word_embedding", "(", "self", ",", "weight_init", ")", ":", "\n", "        ", "assert", "weight_init", ".", "shape", "==", "(", "self", ".", "vocab_size", ",", "self", ".", "word_embed_size", ")", "\n", "self", ".", "embedding_layer", ".", "weight", ".", "data", "[", ":", "self", ".", "vocab_size", "]", "=", "weight_init", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.DecoderRNN.init_hidden": [[82, 96], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "h0.cuda.cuda.cuda", "c0.cuda.cuda.cuda"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "features", ")", ":", "\n", "        ", "batch_size", "=", "features", ".", "shape", "[", "0", "]", "\n", "# _h = self.hid_linear1(features)", "\n", "# _c = self.hid_linear2(features)", "\n", "# h0 = _h.view(1 * self.n_layers, batch_size, self.hidden_size)", "\n", "# c0 = _c.view(1 * self.n_layers, batch_size, self.hidden_size)", "\n", "h0", "=", "torch", ".", "zeros", "(", "1", "*", "self", ".", "n_layers", ",", "batch_size", ",", "self", ".", "hidden_size", ")", "\n", "c0", "=", "torch", ".", "zeros", "(", "1", "*", "self", ".", "n_layers", ",", "batch_size", ",", "self", ".", "hidden_size", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "h0", "=", "h0", ".", "cuda", "(", ")", "\n", "c0", "=", "c0", ".", "cuda", "(", ")", "\n", "\n", "", "return", "(", "h0", ",", "c0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.DecoderRNN.forward": [[97, 115], ["advice_models.DecoderRNN.embedding_layer", "advice_models.DecoderRNN.init_hidden", "features.expand.expand.unsqueeze", "features.expand.expand.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "advice_models.DecoderRNN.lstm", "advice_models.DecoderRNN.dropout", "advice_models.DecoderRNN.linear", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden"], ["", "def", "forward", "(", "self", ",", "features", ",", "captions", ")", ":", "\n", "\n", "# captions = captions[:, :-1]", "\n", "        ", "caption_shape", "=", "captions", ".", "shape", "\n", "embeddings", "=", "self", ".", "embedding_layer", "(", "captions", ")", "\n", "(", "h", ",", "c", ")", "=", "self", ".", "init_hidden", "(", "features", ")", "\n", "features", "=", "features", ".", "unsqueeze", "(", "1", ")", "\n", "features", "=", "features", ".", "expand", "(", "-", "1", ",", "caption_shape", "[", "1", "]", ",", "-", "1", ")", "\n", "lstm_input", "=", "torch", ".", "cat", "(", "(", "features", ",", "embeddings", ")", ",", "2", ")", "\n", "\n", "# embed = torch.cat((features.unsqueeze(1), embed), dim = 1)", "\n", "\n", "\n", "lstm_outputs", ",", "_", "=", "self", ".", "lstm", "(", "lstm_input", ",", "(", "h", ",", "c", ")", ")", "\n", "lstm_outputs", "=", "self", ".", "dropout", "(", "lstm_outputs", ")", "\n", "out", "=", "self", ".", "linear", "(", "lstm_outputs", ")", "\n", "predicted", "=", "torch", ".", "argmax", "(", "out", ",", "dim", "=", "2", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.DecoderRNN.inference": [[116, 140], ["advice_models.DecoderRNN.init_hidden", "features.unsqueeze.unsqueeze.unsqueeze", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "advice_models.DecoderRNN.embedding_layer", "advice_models.DecoderRNN.unsqueeze", "advice_models.DecoderRNN.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "advice_models.DecoderRNN.lstm", "advice_models.DecoderRNN.linear", "output.squeeze.squeeze.squeeze", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "sampled_ids.append", "advice_models.DecoderRNN.embedding_layer().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.log_softmax.max", "advice_models.DecoderRNN.embedding_layer"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden"], ["", "def", "inference", "(", "self", ",", "features", ")", ":", "\n", "        ", "batch_size", "=", "features", ".", "shape", "[", "0", "]", "\n", "(", "hn", ",", "cn", ")", "=", "self", ".", "init_hidden", "(", "features", ")", "\n", "\n", "sampled_ids", "=", "[", "]", "\n", "features", "=", "features", ".", "unsqueeze", "(", "1", ")", "\n", "predicted", "=", "torch", ".", "tensor", "(", "[", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "embed", "=", "self", ".", "embedding_layer", "(", "predicted", ")", "\n", "embed", "=", "embed", ".", "unsqueeze", "(", "1", ")", "\n", "embed", "=", "embed", ".", "expand", "(", "batch_size", ",", "-", "1", ",", "-", "1", ")", "\n", "lstm_input", "=", "torch", ".", "cat", "(", "(", "features", ",", "embed", ")", ",", "2", ")", "\n", "for", "i", "in", "range", "(", "10", ")", ":", "\n", "            ", "lstm_outputs", ",", "(", "hn", ",", "cn", ")", "=", "self", ".", "lstm", "(", "lstm_input", ",", "(", "hn", ",", "cn", ")", ")", "\n", "# lstm_outputs = self.dropout(lstm_outputs)", "\n", "output", "=", "self", ".", "linear", "(", "lstm_outputs", ")", "\n", "output", "=", "output", ".", "squeeze", "(", "1", ")", "\n", "scores", "=", "F", ".", "log_softmax", "(", "output", ",", "dim", "=", "1", ")", "\n", "_predicted", "=", "torch", ".", "argmax", "(", "output", ",", "dim", "=", "1", ")", "\n", "predicted", "=", "scores", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "sampled_ids", ".", "append", "(", "predicted", ")", "\n", "embed", "=", "self", ".", "embedding_layer", "(", "predicted", ")", ".", "unsqueeze", "(", "1", ")", "\n", "lstm_input", "=", "torch", ".", "cat", "(", "(", "features", ",", "embed", ")", ",", "2", ")", "\n", "\n", "", "return", "sampled_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.image_EncoderCNN.__init__": [[142, 158], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "advice_models.image_EncoderCNN.init_weights"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "image_embedding_size", ")", ":", "\n", "        ", "super", "(", "image_EncoderCNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "3", ",", "16", ",", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "max1", "=", "nn", ".", "MaxPool2d", "(", "2", ",", "2", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "16", ",", "32", ",", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "max2", "=", "nn", ".", "MaxPool2d", "(", "2", ",", "2", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "3", ",", "stride", "=", "1", ")", "\n", "self", ".", "max3", "=", "nn", ".", "MaxPool2d", "(", "2", ",", "1", ")", "\n", "#self.flatten = torch.flatten()", "\n", "\n", "#self.resnet = nn.Sequential(*modules)", "\n", "#self.dropout = nn.Dropout2d(p=0.2)", "\n", "fc1_size", "=", "1024", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "6400", ",", "fc1_size", ")", "\n", "self", ".", "embed", "=", "nn", ".", "Linear", "(", "fc1_size", ",", "image_embedding_size", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.image_EncoderCNN.init_weights": [[159, 162], ["advice_models.image_EncoderCNN.fc1.weight.data.normal_", "advice_models.image_EncoderCNN.fc1.bias.data.fill_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "self", ".", "fc1", ".", "weight", ".", "data", ".", "normal_", "(", "0.0", ",", "0.02", ")", "\n", "self", ".", "fc1", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.image_EncoderCNN.forward": [[163, 179], ["advice_models.image_EncoderCNN.conv1", "advice_models.image_EncoderCNN.max1", "advice_models.image_EncoderCNN.conv2", "advice_models.image_EncoderCNN.max2", "advice_models.image_EncoderCNN.conv3", "advice_models.image_EncoderCNN.max3", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "advice_models.image_EncoderCNN.fc1", "advice_models.image_EncoderCNN.embed"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "images", ")", ":", "\n", "#print(\"images shape: \", images.shape)", "\n", "        ", "conv1", "=", "self", ".", "conv1", "(", "images", ")", "\n", "features1", "=", "self", ".", "max1", "(", "conv1", ")", "\n", "conv2", "=", "self", ".", "conv2", "(", "features1", ")", "\n", "features2", "=", "self", ".", "max2", "(", "conv2", ")", "\n", "conv3", "=", "self", ".", "conv3", "(", "features2", ")", "\n", "features3", "=", "self", ".", "max3", "(", "conv3", ")", "\n", "batch", "=", "features3", ".", "shape", "[", "0", "]", "\n", "#print(\"features3_shape: \",features3.shape)", "\n", "flat_f", "=", "torch", ".", "reshape", "(", "features3", ",", "(", "batch", ",", "-", "1", ")", ")", "\n", "#print(\"flat_f_shape: \",flat_f.shape)", "\n", "fc", "=", "self", ".", "fc1", "(", "flat_f", ")", "\n", "features", "=", "self", ".", "embed", "(", "fc", ")", "\n", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.WordEmbedding.__init__": [[186, 192], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["def", "__init__", "(", "self", ",", "ntoken", ",", "advice_dim", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "WordEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "emb", "=", "nn", ".", "Embedding", "(", "ntoken", "+", "1", ",", "advice_dim", ",", "padding_idx", "=", "ntoken", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "ntoken", "=", "ntoken", "\n", "self", ".", "emb_dim", "=", "advice_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.WordEmbedding.init_embedding": [[193, 197], ["torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.load"], "methods", ["None"], ["", "def", "init_embedding", "(", "self", ",", "np_file", ")", ":", "\n", "        ", "weight_init", "=", "torch", ".", "from_numpy", "(", "np", ".", "load", "(", "np_file", ")", ")", "\n", "assert", "weight_init", ".", "shape", "==", "(", "self", ".", "ntoken", ",", "self", ".", "emb_dim", ")", "\n", "self", ".", "emb", ".", "weight", ".", "data", "[", ":", "self", ".", "ntoken", "]", "=", "weight_init", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.WordEmbedding.forward": [[198, 207], ["advice_models.WordEmbedding.emb", "advice_models.WordEmbedding.dropout", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "emb", "=", "self", ".", "emb", "(", "x", ")", "\n", "#print(\"emb_shape: \",emb.shape)", "\n", "emb", "=", "self", ".", "dropout", "(", "emb", ")", "\n", "#print(\"emb_drop_shape: \",emb.shape)", "\n", "batch", "=", "emb", ".", "shape", "[", "0", "]", "\n", "emb", "=", "torch", ".", "reshape", "(", "emb", ",", "(", "batch", ",", "-", "1", ",", "self", ".", "emb_dim", ")", ")", "\n", "#print(\"emb_reshape_shape: \",emb.shape)", "\n", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.Advice_Encoder.__init__": [[209, 223], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "advice_models.Advice_Encoder.init_weights"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "word_embed_size", ",", "hidden_size", ",", "num_layers", ",", "drop_prob", "=", "0.2", ")", ":", "\n", "        ", "super", "(", "Advice_Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding_layer", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "word_embed_size", ")", "\n", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "input_size", "=", "word_embed_size", ",", "hidden_size", "=", "hidden_size", ",", "\n", "num_layers", "=", "num_layers", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "word_embed_size", "=", "word_embed_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "n_layers", "=", "num_layers", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "vocab_size", ")", "\n", "\n", "#self.relu = nn.ReLU()", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.Advice_Encoder.init_weights": [[224, 227], ["advice_models.Advice_Encoder.linear.weight.data.normal_", "advice_models.Advice_Encoder.linear.bias.data.fill_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "self", ".", "linear", ".", "weight", ".", "data", ".", "normal_", "(", "0.0", ",", "0.02", ")", "\n", "self", ".", "linear", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.Advice_Encoder.init_word_embedding": [[229, 232], ["None"], "methods", ["None"], ["", "def", "init_word_embedding", "(", "self", ",", "weight_init", ")", ":", "\n", "        ", "assert", "weight_init", ".", "shape", "==", "(", "self", ".", "vocab_size", ",", "self", ".", "word_embed_size", ")", "\n", "self", ".", "embedding_layer", ".", "weight", ".", "data", "[", ":", "self", ".", "vocab_size", "]", "=", "weight_init", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.Advice_Encoder.forward": [[233, 242], ["advice_models.Advice_Encoder.init_hidden", "advice_models.Advice_Encoder.init_hidden", "advice_models.Advice_Encoder.embedding_layer", "advice_models.Advice_Encoder.lstm", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden"], ["", "def", "forward", "(", "self", ",", "advice_words", ")", ":", "\n", "        ", "batch", "=", "advice_words", ".", "shape", "[", "0", "]", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "batch", ")", "\n", "context", "=", "self", ".", "init_hidden", "(", "batch", ")", "\n", "advice_embed", "=", "self", ".", "embedding_layer", "(", "advice_words", ")", "\n", "out", ",", "(", "h", ",", "_", ")", "=", "self", ".", "lstm", "(", "advice_embed", ",", "(", "hidden", ",", "context", ")", ")", "\n", "#print(\"out GRU_shape: \",out.shape)", "\n", "h", "=", "torch", ".", "reshape", "(", "h", ",", "(", "batch", ",", "-", "1", ")", ")", "\n", "return", "out", ",", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.Advice_Encoder.init_hidden": [[243, 247], ["weight.new().zero_", "next", "advice_models.Advice_Encoder.parameters", "weight.new"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "weight", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "data", "\n", "hidden", "=", "weight", ".", "new", "(", "self", ".", "n_layers", ",", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "zero_", "(", ")", "\n", "return", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.Sentence_Encoder.__init__": [[250, 258], ["torch.Module.__init__", "torch.GRU", "torch.GRU", "torch.GRU", "torch.GRU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "n_layers", ",", "drop_prob", "=", "0.2", ")", ":", "\n", "        ", "super", "(", "Sentence_Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "n_layers", "=", "n_layers", "\n", "\n", "self", ".", "gru", "=", "nn", ".", "GRU", "(", "input_dim", ",", "hidden_dim", ",", "n_layers", ",", "batch_first", "=", "True", ",", "dropout", "=", "drop_prob", ")", "\n", "#self.fc = nn.Linear(hidden_dim, output_dim)", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.Sentence_Encoder.forward": [[259, 269], ["advice_models.Sentence_Encoder.init_hidden", "advice_models.Sentence_Encoder.gru", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch", "=", "x", ".", "shape", "[", "0", "]", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "batch", ")", "\n", "#print(\"hidden initialized_shape: \",hidden.shape)", "\n", "out", ",", "h", "=", "self", ".", "gru", "(", "x", ",", "hidden", ")", "\n", "#print(\"out GRU_shape: \",out.shape)", "\n", "h", "=", "torch", ".", "reshape", "(", "h", ",", "(", "batch", ",", "-", "1", ")", ")", "\n", "#print(\"h_GRU_shape: \",h.shape)", "\n", "#out = self.fc(self.relu(out[:,-1]))", "\n", "return", "out", ",", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.Sentence_Encoder.init_hidden": [[270, 274], ["weight.new().zero_", "next", "advice_models.Sentence_Encoder.parameters", "weight.new"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "weight", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "data", "\n", "hidden", "=", "weight", ".", "new", "(", "self", ".", "n_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ".", "zero_", "(", ")", "\n", "return", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.Action_Decoder.__init__": [[276, 285], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["    ", "def", "__init__", "(", "self", ",", "img_emb_size", ",", "sent_emb_size", ",", "output_dim", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "Action_Decoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "#comb_embed = img_emb_size+sent_emb_size", "\n", "self", ".", "output", "=", "output_dim", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "out_emb1", "=", "nn", ".", "Linear", "(", "img_emb_size", "+", "sent_emb_size", ",", "512", ")", "\n", "self", ".", "out_emb2", "=", "nn", ".", "Linear", "(", "512", ",", "128", ")", "\n", "self", ".", "out_emb3", "=", "nn", ".", "Linear", "(", "128", ",", "output_dim", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.advice_models.Action_Decoder.forward": [[286, 300], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "advice_models.Action_Decoder.out_emb1", "advice_models.Action_Decoder.dropout", "advice_models.Action_Decoder.out_emb2", "advice_models.Action_Decoder.dropout", "advice_models.Action_Decoder.out_emb3", "advice_models.Action_Decoder.relu", "advice_models.Action_Decoder.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "concat_vec", "=", "torch", ".", "cat", "(", "(", "x", ",", "y", ")", ",", "dim", "=", "1", ")", "\n", "#print(\"concatenated_vector: \",concat_vec)", "\n", "decoded1", "=", "self", ".", "out_emb1", "(", "concat_vec", ")", "\n", "#print(\"decoded1_vector: \",decoded1)", "\n", "drop_vec1", "=", "self", ".", "dropout", "(", "self", ".", "relu", "(", "decoded1", ")", ")", "\n", "#drop_vec1= self.dropout(decoded1)", "\n", "decoded2", "=", "self", ".", "out_emb2", "(", "drop_vec1", ")", "\n", "drop_vec2", "=", "self", ".", "dropout", "(", "self", ".", "relu", "(", "decoded2", ")", ")", "\n", "#decoded2 = self.out_emb2(drop_vec2)", "\n", "out", "=", "self", ".", "out_emb3", "(", "drop_vec2", ")", "\n", "#print(\"out_shape: \", out.shape)", "\n", "\n", "return", "out", "", "", "", ""]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.data_loader.FroggerDataLoader.__init__": [[50, 60], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab", ",", "rationalizations", ",", "images", ",", "cur_image_dir", ",", "action", ")", ":", "\n", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "rationalization", "=", "rationalizations", "\n", "#print(\"self.advices: \",(self.rationalization))", "\n", "\n", "self", ".", "images", "=", "images", "\n", "#print(\"self.images: \",(self.images))", "\n", "self", ".", "image_dir", "=", "cur_image_dir", "\n", "self", ".", "action", "=", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.data_loader.FroggerDataLoader.__getitem__": [[62, 90], ["os.path.join", "cv2.imread", "cv2.resize", "numpy.reshape", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "data_loader.create_rationalization_matrix", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.reshape.type", "torch.reshape.type", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.frogger_dataset.FroggerDataset.create_rationalization_matrix"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "\n", "        ", "image_name", "=", "self", ".", "images", "[", "index", "]", "\n", "action", "=", "self", ".", "action", "[", "index", "]", "\n", "advice", "=", "self", ".", "rationalization", "[", "index", "]", "\n", "#print(\"advice: \",advice, \"image: \",image_name, \"action: \",action)", "\n", "\n", "img_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "image_dir", ",", "image_name", ")", "\n", "img_arr", "=", "cv2", ".", "imread", "(", "img_path", ")", "\n", "resized_img", "=", "cv2", ".", "resize", "(", "img_arr", ",", "(", "224", ",", "224", ")", ")", "\n", "x", "=", "np", ".", "reshape", "(", "resized_img", ",", "(", "3", ",", "224", ",", "224", ")", ")", "\n", "image_arr", "=", "torch", ".", "Tensor", "(", "x", ")", "\n", "\n", "advice_ids", "=", "create_rationalization_matrix", "(", "advice", ",", "self", ".", "vocab", ")", "\n", "#print(\"advice_testing: \", advice_ids)", "\n", "advice_text", "=", "torch", ".", "tensor", "(", "advice_ids", ")", "\n", "advice_text", "=", "advice_text", ".", "type", "(", "torch", ".", "LongTensor", ")", "\n", "#print(\"advice_text_shape: \",advice_text.shape)", "\n", "advice_text", "=", "torch", ".", "reshape", "(", "advice_text", ",", "(", "-", "1", ",", ")", ")", "\n", "#advice_text = torch.from_numpy(advice_ids)", "\n", "\n", "\n", "#action_arr = torch.zeros(5)", "\n", "#action_arr[action] = 1", "\n", "action_arr", "=", "torch", ".", "tensor", "(", "action", ")", "\n", "#action_arr = action_arr.type(torch.LongTensor)", "\n", "\n", "return", "advice_text", ",", "image_arr", ",", "action_arr", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.data_loader.FroggerDataLoader.__len__": [[91, 93], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "rationalization", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.data_loader.create_rationalization_matrix": [[17, 47], ["rationalization.replace", "sent.strip.strip", "sent.strip.lower().split", "numpy.zeros", "enumerate", "np.array.append", "sequence_lenght_arr.append", "numpy.array", "sent.strip.lower"], "function", ["None"], ["def", "create_rationalization_matrix", "(", "rationalization", ",", "vocab", ")", ":", "\n", "    ", "max_rationalization_len", "=", "40", "\n", "rationalization_matrix", "=", "[", "]", "\n", "sequence_lenght_arr", "=", "[", "]", "\n", "#sent = rationalization", "\n", "sent", "=", "rationalization", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", "\n", "sent", "=", "sent", ".", "strip", "(", ")", "\n", "words", "=", "sent", ".", "lower", "(", ")", ".", "split", "(", "' '", ")", "\n", "\n", "ration_sent", "=", "np", ".", "zeros", "(", "[", "max_rationalization_len", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "ration_sent", "[", "0", "]", "=", "1", "\n", "idx", "=", "1", "\n", "\n", "for", "k", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "if", "idx", "==", "max_rationalization_len", ":", "\n", "            ", "break", "\n", "", "if", "word", "in", "vocab", ".", "word2idx", ":", "\n", "            ", "ration_sent", "[", "idx", "]", "=", "vocab", ".", "word2idx", "[", "word", "]", "\n", "", "else", ":", "\n", "            ", "ration_sent", "[", "idx", "]", "=", "3", "\n", "", "idx", "+=", "1", "\n", "\n", "", "if", "idx", "<", "max_rationalization_len", ":", "\n", "        ", "ration_sent", "[", "idx", "]", "=", "2", "\n", "idx", "+=", "1", "\n", "", "rationalization_matrix", ".", "append", "(", "ration_sent", ")", "\n", "sequence_lenght_arr", ".", "append", "(", "idx", ")", "\n", "\n", "rationalization_matrix", "=", "np", ".", "array", "(", "rationalization_matrix", ")", "\n", "return", "rationalization_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.data_loader.get_loader": [[94, 99], ["data_loader.FroggerDataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "function", ["None"], ["", "", "def", "get_loader", "(", "vocab", ",", "advices", ",", "images", ",", "cur_image_dir", ",", "act", ",", "batch_size", ",", "transform", ",", "shuffle", ",", "num_workers", ")", ":", "\n", "    ", "frogger", "=", "FroggerDataLoader", "(", "vocab", ",", "advices", ",", "images", ",", "cur_image_dir", ",", "act", ")", "\n", "\n", "data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", "=", "frogger", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "shuffle", ",", "num_workers", "=", "num_workers", ")", "\n", "return", "data_loader", "\n", "", ""]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.models_pretrained_we.Encoder.__init__": [[13, 27], ["torch.nn.Module.__init__", "torchvision.models.resnet101", "torch.nn.Sequential", "torch.nn.AdaptiveAvgPool2d", "models_pretrained_we.Encoder.fine_tune", "list", "torchvision.models.resnet101.children"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.Encoder.fine_tune"], ["def", "__init__", "(", "self", ",", "encoded_image_size", "=", "14", ")", ":", "\n", "        ", "super", "(", "Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "enc_image_size", "=", "encoded_image_size", "\n", "\n", "resnet", "=", "torchvision", ".", "models", ".", "resnet101", "(", "pretrained", "=", "True", ")", "# pretrained ImageNet ResNet-101", "\n", "\n", "# Remove linear and pool layers (since we're not doing classification)", "\n", "modules", "=", "list", "(", "resnet", ".", "children", "(", ")", ")", "[", ":", "-", "2", "]", "\n", "self", ".", "resnet", "=", "nn", ".", "Sequential", "(", "*", "modules", ")", "\n", "\n", "# Resize image to fixed size to allow input images of variable size", "\n", "self", ".", "adaptive_pool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "(", "encoded_image_size", ",", "encoded_image_size", ")", ")", "\n", "\n", "self", ".", "fine_tune", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.models_pretrained_we.Encoder.forward": [[28, 39], ["models_pretrained_we.Encoder.resnet", "models_pretrained_we.Encoder.adaptive_pool", "out.permute.permute.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "images", ")", ":", "\n", "        ", "\"\"\"\n        Forward propagation.\n\n        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n        :return: encoded images\n        \"\"\"", "\n", "out", "=", "self", ".", "resnet", "(", "images", ")", "# (batch_size, 2048, image_size/32, image_size/32)", "\n", "out", "=", "self", ".", "adaptive_pool", "(", "out", ")", "# (batch_size, 2048, encoded_image_size, encoded_image_size)", "\n", "out", "=", "out", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "# (batch_size, encoded_image_size, encoded_image_size, 2048)", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.models_pretrained_we.Encoder.fine_tune": [[40, 52], ["models_pretrained_we.Encoder.resnet.parameters", "list", "c.parameters", "models_pretrained_we.Encoder.resnet.children"], "methods", ["None"], ["", "def", "fine_tune", "(", "self", ",", "fine_tune", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n\n        :param fine_tune: Allow?\n        \"\"\"", "\n", "for", "p", "in", "self", ".", "resnet", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "# If fine-tuning, only fine-tune convolutional blocks 2 through 4", "\n", "", "for", "c", "in", "list", "(", "self", ".", "resnet", ".", "children", "(", ")", ")", "[", "5", ":", "]", ":", "\n", "            ", "for", "p", "in", "c", ".", "parameters", "(", ")", ":", "\n", "                ", "p", ".", "requires_grad", "=", "fine_tune", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.models_pretrained_we.Attention.__init__": [[59, 71], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Softmax"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["def", "__init__", "(", "self", ",", "encoder_dim", ",", "decoder_dim", ",", "attention_dim", ")", ":", "\n", "        ", "\"\"\"\n        :param encoder_dim: feature size of encoded images\n        :param decoder_dim: size of decoder's RNN\n        :param attention_dim: size of the attention network\n        \"\"\"", "\n", "super", "(", "Attention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder_att", "=", "nn", ".", "Linear", "(", "encoder_dim", ",", "attention_dim", ")", "# linear layer to transform encoded image", "\n", "self", ".", "decoder_att", "=", "nn", ".", "Linear", "(", "decoder_dim", ",", "attention_dim", ")", "# linear layer to transform decoder's output", "\n", "self", ".", "full_att", "=", "nn", ".", "Linear", "(", "attention_dim", ",", "1", ")", "# linear layer to calculate values to be softmax-ed", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "# softmax layer to calculate weights", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.models_pretrained_we.Attention.forward": [[72, 87], ["models_pretrained_we.Attention.encoder_att", "models_pretrained_we.Attention.decoder_att", "models_pretrained_we.Attention.full_att().squeeze", "models_pretrained_we.Attention.softmax", "models_pretrained_we.Attention.full_att", "models_pretrained_we.Attention.relu", "models_pretrained_we.Attention.unsqueeze", "models_pretrained_we.Attention.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "encoder_out", ",", "decoder_hidden", ")", ":", "\n", "        ", "\"\"\"\n        Forward propagation.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n        :return: attention weighted encoding, weights\n        \"\"\"", "\n", "att1", "=", "self", ".", "encoder_att", "(", "encoder_out", ")", "# (batch_size, num_pixels, attention_dim)", "\n", "att2", "=", "self", ".", "decoder_att", "(", "decoder_hidden", ")", "# (batch_size, attention_dim)", "\n", "att", "=", "self", ".", "full_att", "(", "self", ".", "relu", "(", "att1", "+", "att2", ".", "unsqueeze", "(", "1", ")", ")", ")", ".", "squeeze", "(", "2", ")", "# (batch_size, num_pixels)", "\n", "alpha", "=", "self", ".", "softmax", "(", "att", ")", "# (batch_size, num_pixels)", "\n", "attention_weighted_encoding", "=", "(", "encoder_out", "*", "alpha", ".", "unsqueeze", "(", "2", ")", ")", ".", "sum", "(", "dim", "=", "1", ")", "# (batch_size, encoder_dim)", "\n", "\n", "return", "attention_weighted_encoding", ",", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.models_pretrained_we.DecoderWithAttention.__init__": [[94, 123], ["torch.nn.Module.__init__", "models_pretrained_we.Attention", "torch.nn.Embedding", "torch.nn.Dropout", "torch.nn.LSTMCell", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Sigmoid", "torch.nn.Linear", "models_pretrained_we.DecoderWithAttention.init_weights"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_weights"], ["def", "__init__", "(", "self", ",", "attention_dim", ",", "embed_dim", ",", "decoder_dim", ",", "vocab_size", ",", "encoder_dim", "=", "2048", ",", "dropout", "=", "0.5", ")", ":", "\n", "        ", "\"\"\"\n        :param attention_dim: size of attention network\n        :param embed_dim: embedding size\n        :param decoder_dim: size of decoder's RNN\n        :param vocab_size: size of vocabulary\n        :param encoder_dim: feature size of encoded images\n        :param dropout: dropout\n        \"\"\"", "\n", "super", "(", "DecoderWithAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "encoder_dim", "=", "encoder_dim", "\n", "self", ".", "attention_dim", "=", "attention_dim", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "decoder_dim", "=", "decoder_dim", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n", "self", ".", "attention", "=", "Attention", "(", "encoder_dim", ",", "decoder_dim", ",", "attention_dim", ")", "# attention network", "\n", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embed_dim", ")", "# embedding layer", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout", ")", "\n", "self", ".", "decode_step", "=", "nn", ".", "LSTMCell", "(", "embed_dim", "+", "encoder_dim", ",", "decoder_dim", ",", "bias", "=", "True", ")", "# decoding LSTMCell", "\n", "self", ".", "init_h", "=", "nn", ".", "Linear", "(", "encoder_dim", ",", "decoder_dim", ")", "# linear layer to find initial hidden state of LSTMCell", "\n", "self", ".", "init_c", "=", "nn", ".", "Linear", "(", "encoder_dim", ",", "decoder_dim", ")", "# linear layer to find initial cell state of LSTMCell", "\n", "self", ".", "f_beta", "=", "nn", ".", "Linear", "(", "decoder_dim", ",", "encoder_dim", ")", "# linear layer to create a sigmoid-activated gate", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "decoder_dim", ",", "vocab_size", ")", "# linear layer to find scores over vocabulary", "\n", "self", ".", "init_weights", "(", ")", "# initialize some layers with the uniform distribution", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.models_pretrained_we.DecoderWithAttention.init_weights": [[124, 131], ["models_pretrained_we.DecoderWithAttention.embedding.weight.data.uniform_", "models_pretrained_we.DecoderWithAttention.fc.bias.data.fill_", "models_pretrained_we.DecoderWithAttention.fc.weight.data.uniform_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initializes some parameters with values from the uniform distribution, for easier convergence.\n        \"\"\"", "\n", "self", ".", "embedding", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "self", ".", "fc", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "self", ".", "fc", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.models_pretrained_we.DecoderWithAttention.load_pretrained_embeddings": [[132, 139], ["torch.nn.Parameter"], "methods", ["None"], ["", "def", "load_pretrained_embeddings", "(", "self", ",", "embeddings", ")", ":", "\n", "        ", "\"\"\"\n        Loads embedding layer with pre-trained embeddings.\n\n        :param embeddings: pre-trained embeddings\n        \"\"\"", "\n", "self", ".", "embedding", ".", "weight", "=", "nn", ".", "Parameter", "(", "embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.models_pretrained_we.DecoderWithAttention.init_word_embedding": [[140, 143], ["None"], "methods", ["None"], ["", "def", "init_word_embedding", "(", "self", ",", "weight_init", ")", ":", "\n", "        ", "assert", "weight_init", ".", "shape", "==", "(", "self", ".", "vocab_size", ",", "self", ".", "embed_dim", ")", "\n", "self", ".", "embedding", ".", "weight", ".", "data", "[", ":", "self", ".", "vocab_size", "]", "=", "weight_init", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.models_pretrained_we.DecoderWithAttention.fine_tune_embeddings": [[144, 152], ["models_pretrained_we.DecoderWithAttention.embedding.parameters"], "methods", ["None"], ["", "def", "fine_tune_embeddings", "(", "self", ",", "fine_tune", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n\n        :param fine_tune: Allow?\n        \"\"\"", "\n", "for", "p", "in", "self", ".", "embedding", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "fine_tune", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.models_pretrained_we.DecoderWithAttention.init_hidden_state": [[153, 164], ["encoder_out.mean", "models_pretrained_we.DecoderWithAttention.init_h", "models_pretrained_we.DecoderWithAttention.init_c"], "methods", ["None"], ["", "", "def", "init_hidden_state", "(", "self", ",", "encoder_out", ")", ":", "\n", "        ", "\"\"\"\n        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :return: hidden state, cell state\n        \"\"\"", "\n", "mean_encoder_out", "=", "encoder_out", ".", "mean", "(", "dim", "=", "1", ")", "\n", "h", "=", "self", ".", "init_h", "(", "mean_encoder_out", ")", "# (batch_size, decoder_dim)", "\n", "c", "=", "self", ".", "init_c", "(", "mean_encoder_out", ")", "\n", "return", "h", ",", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.models_pretrained_we.DecoderWithAttention.forward": [[165, 220], ["encoder_out.view.view.size", "encoder_out.view.view.size", "encoder_out.view.view.view", "encoder_out.view.view.size", "caption_lengths.squeeze().sort", "models_pretrained_we.DecoderWithAttention.embedding", "models_pretrained_we.DecoderWithAttention.init_hidden_state", "torch.zeros().to", "torch.zeros().to", "range", "max", "sum", "models_pretrained_we.DecoderWithAttention.attention", "models_pretrained_we.DecoderWithAttention.sigmoid", "models_pretrained_we.DecoderWithAttention.decode_step", "models_pretrained_we.DecoderWithAttention.fc", "caption_lengths.squeeze", "torch.zeros", "torch.zeros", "models_pretrained_we.DecoderWithAttention.f_beta", "torch.cat", "models_pretrained_we.DecoderWithAttention.dropout", "max", "max"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.DecoderWithAttention.init_hidden_state"], ["", "def", "forward", "(", "self", ",", "encoder_out", ",", "encoded_captions", ",", "caption_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Forward propagation.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"", "\n", "\n", "batch_size", "=", "encoder_out", ".", "size", "(", "0", ")", "\n", "encoder_dim", "=", "encoder_out", ".", "size", "(", "-", "1", ")", "\n", "vocab_size", "=", "self", ".", "vocab_size", "\n", "\n", "# Flatten image", "\n", "encoder_out", "=", "encoder_out", ".", "view", "(", "batch_size", ",", "-", "1", ",", "encoder_dim", ")", "# (batch_size, num_pixels, encoder_dim)", "\n", "num_pixels", "=", "encoder_out", ".", "size", "(", "1", ")", "\n", "\n", "# Sort input data by decreasing lengths; why? apparent below", "\n", "\n", "caption_lengths", ",", "sort_ind", "=", "caption_lengths", ".", "squeeze", "(", "1", ")", ".", "sort", "(", "dim", "=", "0", ",", "descending", "=", "True", ")", "\n", "encoder_out", "=", "encoder_out", "[", "sort_ind", "]", "\n", "encoded_captions", "=", "encoded_captions", "[", "sort_ind", "]", "\n", "\n", "# Embedding", "\n", "embeddings", "=", "self", ".", "embedding", "(", "encoded_captions", ")", "# (batch_size, max_caption_length, embed_dim)", "\n", "\n", "# Initialize LSTM state", "\n", "h", ",", "c", "=", "self", ".", "init_hidden_state", "(", "encoder_out", ")", "# (batch_size, decoder_dim)", "\n", "\n", "# We won't decode at the <end> position, since we've finished generating as soon as we generate <end>", "\n", "# So, decoding lengths are actual lengths - 1", "\n", "decode_lengths", "=", "(", "caption_lengths", "-", "1", ")", ".", "tolist", "(", ")", "\n", "\n", "# Create tensors to hold word predicion scores and alphas", "\n", "predictions", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max", "(", "decode_lengths", ")", ",", "vocab_size", ")", ".", "to", "(", "device", ")", "\n", "alphas", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max", "(", "decode_lengths", ")", ",", "num_pixels", ")", ".", "to", "(", "device", ")", "\n", "\n", "# At each time-step, decode by", "\n", "# attention-weighing the encoder's output based on the decoder's previous hidden state output", "\n", "# then generate a new word in the decoder with the previous word and the attention weighted encoding", "\n", "for", "t", "in", "range", "(", "max", "(", "decode_lengths", ")", ")", ":", "\n", "            ", "batch_size_t", "=", "sum", "(", "[", "l", ">", "t", "for", "l", "in", "decode_lengths", "]", ")", "\n", "attention_weighted_encoding", ",", "alpha", "=", "self", ".", "attention", "(", "encoder_out", "[", ":", "batch_size_t", "]", ",", "\n", "h", "[", ":", "batch_size_t", "]", ")", "\n", "gate", "=", "self", ".", "sigmoid", "(", "self", ".", "f_beta", "(", "h", "[", ":", "batch_size_t", "]", ")", ")", "# gating scalar, (batch_size_t, encoder_dim)", "\n", "attention_weighted_encoding", "=", "gate", "*", "attention_weighted_encoding", "\n", "h", ",", "c", "=", "self", ".", "decode_step", "(", "\n", "torch", ".", "cat", "(", "[", "embeddings", "[", ":", "batch_size_t", ",", "t", ",", ":", "]", ",", "attention_weighted_encoding", "]", ",", "dim", "=", "1", ")", ",", "\n", "(", "h", "[", ":", "batch_size_t", "]", ",", "c", "[", ":", "batch_size_t", "]", ")", ")", "# (batch_size_t, decoder_dim)", "\n", "preds", "=", "self", ".", "fc", "(", "self", ".", "dropout", "(", "h", ")", ")", "# (batch_size_t, vocab_size)", "\n", "predictions", "[", ":", "batch_size_t", ",", "t", ",", ":", "]", "=", "preds", "\n", "alphas", "[", ":", "batch_size_t", ",", "t", ",", ":", "]", "=", "alpha", "\n", "\n", "", "return", "predictions", ",", "encoded_captions", ",", "decode_lengths", ",", "alphas", ",", "sort_ind", "\n", "", "", ""]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.Encoder.__init__": [[14, 28], ["torch.nn.Module.__init__", "torchvision.models.resnet101", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.AdaptiveAvgPool2d", "torch.nn.AdaptiveAvgPool2d", "pytorch_img_cap_models.Encoder.fine_tune", "list", "torchvision.models.resnet101.children"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.Encoder.fine_tune"], ["def", "__init__", "(", "self", ",", "encoded_image_size", "=", "14", ")", ":", "\n", "        ", "super", "(", "Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "enc_image_size", "=", "encoded_image_size", "\n", "\n", "resnet", "=", "torchvision", ".", "models", ".", "resnet101", "(", "pretrained", "=", "True", ")", "# pretrained ImageNet ResNet-101", "\n", "\n", "# Remove linear and pool layers (since we're not doing classification)", "\n", "modules", "=", "list", "(", "resnet", ".", "children", "(", ")", ")", "[", ":", "-", "2", "]", "\n", "self", ".", "resnet", "=", "nn", ".", "Sequential", "(", "*", "modules", ")", "\n", "\n", "# Resize image to fixed size to allow input images of variable size", "\n", "self", ".", "adaptive_pool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "(", "encoded_image_size", ",", "encoded_image_size", ")", ")", "\n", "\n", "self", ".", "fine_tune", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.Encoder.forward": [[29, 40], ["pytorch_img_cap_models.Encoder.resnet", "pytorch_img_cap_models.Encoder.adaptive_pool", "out.permute.permute.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "images", ")", ":", "\n", "        ", "\"\"\"\n        Forward propagation.\n\n        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n        :return: encoded images\n        \"\"\"", "\n", "out", "=", "self", ".", "resnet", "(", "images", ")", "# (batch_size, 2048, image_size/32, image_size/32)", "\n", "out", "=", "self", ".", "adaptive_pool", "(", "out", ")", "# (batch_size, 2048, encoded_image_size, encoded_image_size)", "\n", "out", "=", "out", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "# (batch_size, encoded_image_size, encoded_image_size, 2048)", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.Encoder.fine_tune": [[41, 53], ["pytorch_img_cap_models.Encoder.resnet.parameters", "list", "c.parameters", "pytorch_img_cap_models.Encoder.resnet.children"], "methods", ["None"], ["", "def", "fine_tune", "(", "self", ",", "fine_tune", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n\n        :param fine_tune: Allow?\n        \"\"\"", "\n", "for", "p", "in", "self", ".", "resnet", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "# If fine-tuning, only fine-tune convolutional blocks 2 through 4", "\n", "", "for", "c", "in", "list", "(", "self", ".", "resnet", ".", "children", "(", ")", ")", "[", "5", ":", "]", ":", "\n", "            ", "for", "p", "in", "c", ".", "parameters", "(", ")", ":", "\n", "                ", "p", ".", "requires_grad", "=", "fine_tune", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.Attention.__init__": [[60, 72], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Softmax", "torch.nn.Softmax"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["def", "__init__", "(", "self", ",", "encoder_dim", ",", "decoder_dim", ",", "attention_dim", ")", ":", "\n", "        ", "\"\"\"\n        :param encoder_dim: feature size of encoded images\n        :param decoder_dim: size of decoder's RNN\n        :param attention_dim: size of the attention network\n        \"\"\"", "\n", "super", "(", "Attention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder_att", "=", "nn", ".", "Linear", "(", "encoder_dim", ",", "attention_dim", ")", "# linear layer to transform encoded image", "\n", "self", ".", "decoder_att", "=", "nn", ".", "Linear", "(", "decoder_dim", ",", "attention_dim", ")", "# linear layer to transform decoder's output", "\n", "self", ".", "full_att", "=", "nn", ".", "Linear", "(", "attention_dim", ",", "1", ")", "# linear layer to calculate values to be softmax-ed", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "# softmax layer to calculate weights", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.Attention.forward": [[73, 88], ["pytorch_img_cap_models.Attention.encoder_att", "pytorch_img_cap_models.Attention.decoder_att", "pytorch_img_cap_models.Attention.full_att().squeeze", "pytorch_img_cap_models.Attention.softmax", "pytorch_img_cap_models.Attention.full_att", "pytorch_img_cap_models.Attention.relu", "pytorch_img_cap_models.Attention.unsqueeze", "pytorch_img_cap_models.Attention.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "encoder_out", ",", "decoder_hidden", ")", ":", "\n", "        ", "\"\"\"\n        Forward propagation.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n        :return: attention weighted encoding, weights\n        \"\"\"", "\n", "att1", "=", "self", ".", "encoder_att", "(", "encoder_out", ")", "# (batch_size, num_pixels, attention_dim)", "\n", "att2", "=", "self", ".", "decoder_att", "(", "decoder_hidden", ")", "# (batch_size, attention_dim)", "\n", "att", "=", "self", ".", "full_att", "(", "self", ".", "relu", "(", "att1", "+", "att2", ".", "unsqueeze", "(", "1", ")", ")", ")", ".", "squeeze", "(", "2", ")", "# (batch_size, num_pixels)", "\n", "alpha", "=", "self", ".", "softmax", "(", "att", ")", "# (batch_size, num_pixels)", "\n", "attention_weighted_encoding", "=", "(", "encoder_out", "*", "alpha", ".", "unsqueeze", "(", "2", ")", ")", ".", "sum", "(", "dim", "=", "1", ")", "# (batch_size, encoder_dim)", "\n", "\n", "return", "attention_weighted_encoding", ",", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.DecoderWithAttention.__init__": [[95, 124], ["torch.nn.Module.__init__", "pytorch_img_cap_models.Attention", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Sigmoid", "torch.nn.Sigmoid", "torch.nn.Linear", "torch.nn.Linear", "pytorch_img_cap_models.DecoderWithAttention.init_weights"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_weights"], ["def", "__init__", "(", "self", ",", "attention_dim", ",", "embed_dim", ",", "decoder_dim", ",", "vocab_size", ",", "encoder_dim", "=", "2048", ",", "dropout", "=", "0.5", ")", ":", "\n", "        ", "\"\"\"\n        :param attention_dim: size of attention network\n        :param embed_dim: embedding size\n        :param decoder_dim: size of decoder's RNN\n        :param vocab_size: size of vocabulary\n        :param encoder_dim: feature size of encoded images\n        :param dropout: dropout\n        \"\"\"", "\n", "super", "(", "DecoderWithAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "encoder_dim", "=", "encoder_dim", "\n", "self", ".", "attention_dim", "=", "attention_dim", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "decoder_dim", "=", "decoder_dim", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n", "self", ".", "attention", "=", "Attention", "(", "encoder_dim", ",", "decoder_dim", ",", "attention_dim", ")", "# attention network", "\n", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embed_dim", ")", "# embedding layer", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout", ")", "\n", "self", ".", "decode_step", "=", "nn", ".", "LSTMCell", "(", "embed_dim", "+", "encoder_dim", ",", "decoder_dim", ",", "bias", "=", "True", ")", "# decoding LSTMCell", "\n", "self", ".", "init_h", "=", "nn", ".", "Linear", "(", "encoder_dim", ",", "decoder_dim", ")", "# linear layer to find initial hidden state of LSTMCell", "\n", "self", ".", "init_c", "=", "nn", ".", "Linear", "(", "encoder_dim", ",", "decoder_dim", ")", "# linear layer to find initial cell state of LSTMCell", "\n", "self", ".", "f_beta", "=", "nn", ".", "Linear", "(", "decoder_dim", ",", "encoder_dim", ")", "# linear layer to create a sigmoid-activated gate", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "decoder_dim", ",", "vocab_size", ")", "# linear layer to find scores over vocabulary", "\n", "self", ".", "init_weights", "(", ")", "# initialize some layers with the uniform distribution", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.DecoderWithAttention.init_weights": [[125, 132], ["pytorch_img_cap_models.DecoderWithAttention.embedding.weight.data.uniform_", "pytorch_img_cap_models.DecoderWithAttention.fc.bias.data.fill_", "pytorch_img_cap_models.DecoderWithAttention.fc.weight.data.uniform_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initializes some parameters with values from the uniform distribution, for easier convergence.\n        \"\"\"", "\n", "self", ".", "embedding", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "self", ".", "fc", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "self", ".", "fc", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.DecoderWithAttention.load_pretrained_embeddings": [[133, 140], ["torch.nn.Parameter", "torch.nn.Parameter"], "methods", ["None"], ["", "def", "load_pretrained_embeddings", "(", "self", ",", "embeddings", ")", ":", "\n", "        ", "\"\"\"\n        Loads embedding layer with pre-trained embeddings.\n\n        :param embeddings: pre-trained embeddings\n        \"\"\"", "\n", "self", ".", "embedding", ".", "weight", "=", "nn", ".", "Parameter", "(", "embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.DecoderWithAttention.fine_tune_embeddings": [[141, 149], ["pytorch_img_cap_models.DecoderWithAttention.embedding.parameters"], "methods", ["None"], ["", "def", "fine_tune_embeddings", "(", "self", ",", "fine_tune", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n\n        :param fine_tune: Allow?\n        \"\"\"", "\n", "for", "p", "in", "self", ".", "embedding", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "fine_tune", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.DecoderWithAttention.init_hidden_state": [[150, 161], ["encoder_out.mean", "pytorch_img_cap_models.DecoderWithAttention.init_h", "pytorch_img_cap_models.DecoderWithAttention.init_c"], "methods", ["None"], ["", "", "def", "init_hidden_state", "(", "self", ",", "encoder_out", ")", ":", "\n", "        ", "\"\"\"\n        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :return: hidden state, cell state\n        \"\"\"", "\n", "mean_encoder_out", "=", "encoder_out", ".", "mean", "(", "dim", "=", "1", ")", "\n", "h", "=", "self", ".", "init_h", "(", "mean_encoder_out", ")", "# (batch_size, decoder_dim)", "\n", "c", "=", "self", ".", "init_c", "(", "mean_encoder_out", ")", "\n", "return", "h", ",", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.DecoderWithAttention.forward": [[162, 217], ["encoder_out.view.view.size", "encoder_out.view.view.size", "encoder_out.view.view.view", "encoder_out.view.view.size", "caption_lengths.squeeze().sort", "pytorch_img_cap_models.DecoderWithAttention.embedding", "pytorch_img_cap_models.DecoderWithAttention.init_hidden_state", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "range", "max", "sum", "pytorch_img_cap_models.DecoderWithAttention.attention", "pytorch_img_cap_models.DecoderWithAttention.sigmoid", "pytorch_img_cap_models.DecoderWithAttention.decode_step", "pytorch_img_cap_models.DecoderWithAttention.fc", "caption_lengths.squeeze", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "pytorch_img_cap_models.DecoderWithAttention.f_beta", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pytorch_img_cap_models.DecoderWithAttention.dropout", "max", "max"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.DecoderWithAttention.init_hidden_state"], ["", "def", "forward", "(", "self", ",", "encoder_out", ",", "encoded_captions", ",", "caption_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Forward propagation.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"", "\n", "\n", "batch_size", "=", "encoder_out", ".", "size", "(", "0", ")", "\n", "encoder_dim", "=", "encoder_out", ".", "size", "(", "-", "1", ")", "\n", "vocab_size", "=", "self", ".", "vocab_size", "\n", "\n", "# Flatten image", "\n", "encoder_out", "=", "encoder_out", ".", "view", "(", "batch_size", ",", "-", "1", ",", "encoder_dim", ")", "# (batch_size, num_pixels, encoder_dim)", "\n", "num_pixels", "=", "encoder_out", ".", "size", "(", "1", ")", "\n", "\n", "# Sort input data by decreasing lengths; why? apparent below", "\n", "\n", "caption_lengths", ",", "sort_ind", "=", "caption_lengths", ".", "squeeze", "(", "1", ")", ".", "sort", "(", "dim", "=", "0", ",", "descending", "=", "True", ")", "\n", "encoder_out", "=", "encoder_out", "[", "sort_ind", "]", "\n", "encoded_captions", "=", "encoded_captions", "[", "sort_ind", "]", "\n", "\n", "# Embedding", "\n", "embeddings", "=", "self", ".", "embedding", "(", "encoded_captions", ")", "# (batch_size, max_caption_length, embed_dim)", "\n", "\n", "# Initialize LSTM state", "\n", "h", ",", "c", "=", "self", ".", "init_hidden_state", "(", "encoder_out", ")", "# (batch_size, decoder_dim)", "\n", "\n", "# We won't decode at the <end> position, since we've finished generating as soon as we generate <end>", "\n", "# So, decoding lengths are actual lengths - 1", "\n", "decode_lengths", "=", "(", "caption_lengths", "-", "1", ")", ".", "tolist", "(", ")", "\n", "\n", "# Create tensors to hold word predicion scores and alphas", "\n", "predictions", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max", "(", "decode_lengths", ")", ",", "vocab_size", ")", ".", "to", "(", "device", ")", "\n", "alphas", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max", "(", "decode_lengths", ")", ",", "num_pixels", ")", ".", "to", "(", "device", ")", "\n", "\n", "# At each time-step, decode by", "\n", "# attention-weighing the encoder's output based on the decoder's previous hidden state output", "\n", "# then generate a new word in the decoder with the previous word and the attention weighted encoding", "\n", "for", "t", "in", "range", "(", "max", "(", "decode_lengths", ")", ")", ":", "\n", "            ", "batch_size_t", "=", "sum", "(", "[", "l", ">", "t", "for", "l", "in", "decode_lengths", "]", ")", "\n", "attention_weighted_encoding", ",", "alpha", "=", "self", ".", "attention", "(", "encoder_out", "[", ":", "batch_size_t", "]", ",", "\n", "h", "[", ":", "batch_size_t", "]", ")", "\n", "gate", "=", "self", ".", "sigmoid", "(", "self", ".", "f_beta", "(", "h", "[", ":", "batch_size_t", "]", ")", ")", "# gating scalar, (batch_size_t, encoder_dim)", "\n", "attention_weighted_encoding", "=", "gate", "*", "attention_weighted_encoding", "\n", "h", ",", "c", "=", "self", ".", "decode_step", "(", "\n", "torch", ".", "cat", "(", "[", "embeddings", "[", ":", "batch_size_t", ",", "t", ",", ":", "]", ",", "attention_weighted_encoding", "]", ",", "dim", "=", "1", ")", ",", "\n", "(", "h", "[", ":", "batch_size_t", "]", ",", "c", "[", ":", "batch_size_t", "]", ")", ")", "# (batch_size_t, decoder_dim)", "\n", "preds", "=", "self", ".", "fc", "(", "self", ".", "dropout", "(", "h", ")", ")", "# (batch_size_t, vocab_size)", "\n", "predictions", "[", ":", "batch_size_t", ",", "t", ",", ":", "]", "=", "preds", "\n", "alphas", "[", ":", "batch_size_t", ",", "t", ",", ":", "]", "=", "alpha", "\n", "\n", "", "return", "predictions", ",", "encoded_captions", ",", "decode_lengths", ",", "alphas", ",", "sort_ind", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.DecoderWithAttention.inference": [[218, 317], ["len", "encoder_out.expand.expand.size", "encoder_out.expand.expand.size", "encoder_out.expand.expand.contiguous", "encoder_out.expand.expand.view", "encoder_out.expand.expand.size", "encoder_out.expand.expand.expand", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "list", "list", "pytorch_img_cap_models.DecoderWithAttention.init_hidden_state", "hypotheses.append", "pytorch_img_cap_models.DecoderWithAttention.embedding().squeeze", "pytorch_img_cap_models.DecoderWithAttention.attention", "pytorch_img_cap_models.DecoderWithAttention.sigmoid", "pytorch_img_cap_models.DecoderWithAttention.decode_step", "pytorch_img_cap_models.DecoderWithAttention.fc", "torch.log_softmax", "torch.log_softmax", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "list", "len", "top_k_scores[].unsqueeze", "next_word_inds[].unsqueeze", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "pytorch_img_cap_models.DecoderWithAttention.f_beta", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "top_k_scores[].unsqueeze.expand_as", "scores[].topk", "torch.log_softmax.view().topk", "len", "list.extend", "list.extend", "pytorch_img_cap_models.DecoderWithAttention.embedding", "next_word_inds.unsqueeze", "enumerate", "set", "set", "seqs[].tolist", "torch.log_softmax.view", "range", "len"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.pytorch_img_cap_models.DecoderWithAttention.init_hidden_state"], ["", "def", "inference", "(", "self", ",", "encoder_out", ",", "word_map", ")", ":", "\n", "\n", "        ", "hypotheses", "=", "[", "]", "\n", "vocab_size", "=", "len", "(", "word_map", ")", "\n", "k", "=", "1", "\n", "batch_size", "=", "encoder_out", ".", "shape", "[", "0", "]", "\n", "enc_image_size", "=", "encoder_out", ".", "size", "(", "1", ")", "\n", "#encoder_dim = encoder_out.size(2)", "\n", "encoder_dim", "=", "encoder_out", ".", "size", "(", "3", ")", "\n", "\n", "# Flatten encoding", "\n", "encoder_out", "=", "encoder_out", ".", "contiguous", "(", ")", "\n", "encoder_out", "=", "encoder_out", ".", "view", "(", "k", ",", "-", "1", ",", "encoder_dim", ")", "# (1, num_pixels, encoder_dim)", "\n", "num_pixels", "=", "encoder_out", ".", "size", "(", "1", ")", "\n", "\n", "# We'll treat the problem as having a batch size of k", "\n", "encoder_out", "=", "encoder_out", ".", "expand", "(", "k", ",", "num_pixels", ",", "encoder_dim", ")", "# (k, num_pixels, encoder_dim)", "\n", "\n", "# Tensor to store top k previous words at each step; now they're just <start>", "\n", "k_prev_words", "=", "torch", ".", "LongTensor", "(", "[", "[", "word_map", ".", "word2idx", "[", "'<start>'", "]", "]", "]", "*", "k", ")", ".", "to", "(", "device", ")", "# (k, 1)", "\n", "\n", "# Tensor to store top k sequences; now they're just <start>", "\n", "seqs", "=", "k_prev_words", "# (k, 1)", "\n", "\n", "# Tensor to store top k sequences' scores; now they're just 0", "\n", "top_k_scores", "=", "torch", ".", "zeros", "(", "k", ",", "1", ")", ".", "to", "(", "device", ")", "# (k, 1)", "\n", "\n", "# Lists to store completed sequences and scores", "\n", "complete_seqs", "=", "list", "(", ")", "\n", "complete_seqs_scores", "=", "list", "(", ")", "\n", "\n", "# Start decoding", "\n", "step", "=", "1", "\n", "h", ",", "c", "=", "self", ".", "init_hidden_state", "(", "encoder_out", ")", "\n", "\n", "# s is a number less than or equal to k, because sequences are removed from this process once they hit <end>", "\n", "while", "True", ":", "\n", "\n", "            ", "embeddings", "=", "self", ".", "embedding", "(", "k_prev_words", ")", ".", "squeeze", "(", "1", ")", "# (s, embed_dim)", "\n", "\n", "awe", ",", "_", "=", "self", ".", "attention", "(", "encoder_out", ",", "h", ")", "# (s, encoder_dim), (s, num_pixels)", "\n", "\n", "gate", "=", "self", ".", "sigmoid", "(", "self", ".", "f_beta", "(", "h", ")", ")", "# gating scalar, (s, encoder_dim)", "\n", "awe", "=", "gate", "*", "awe", "\n", "\n", "h", ",", "c", "=", "self", ".", "decode_step", "(", "torch", ".", "cat", "(", "[", "embeddings", ",", "awe", "]", ",", "dim", "=", "1", ")", ",", "(", "h", ",", "c", ")", ")", "# (s, decoder_dim)", "\n", "\n", "scores", "=", "self", ".", "fc", "(", "h", ")", "# (s, vocab_size)", "\n", "scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "1", ")", "\n", "\n", "# Add", "\n", "scores", "=", "top_k_scores", ".", "expand_as", "(", "scores", ")", "+", "scores", "# (s, vocab_size)", "\n", "\n", "# For the first step, all k points will have the same scores (since same k previous words, h, c)", "\n", "if", "step", "==", "1", ":", "\n", "                ", "top_k_scores", ",", "top_k_words", "=", "scores", "[", "0", "]", ".", "topk", "(", "k", ",", "0", ",", "True", ",", "True", ")", "# (s)", "\n", "", "else", ":", "\n", "# Unroll and find top scores, and their unrolled indices", "\n", "                ", "top_k_scores", ",", "top_k_words", "=", "scores", ".", "view", "(", "-", "1", ")", ".", "topk", "(", "k", ",", "0", ",", "True", ",", "True", ")", "# (s)", "\n", "\n", "# Convert unrolled indices to actual indices of scores", "\n", "", "prev_word_inds", "=", "top_k_words", "/", "vocab_size", "# (s)", "\n", "next_word_inds", "=", "top_k_words", "%", "vocab_size", "# (s)", "\n", "\n", "# Add new words to sequences", "\n", "seqs", "=", "torch", ".", "cat", "(", "[", "seqs", "[", "prev_word_inds", "]", ",", "next_word_inds", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "1", ")", "# (s, step+1)", "\n", "\n", "# Which sequences are incomplete (didn't reach <end>)?", "\n", "incomplete_inds", "=", "[", "ind", "for", "ind", ",", "next_word", "in", "enumerate", "(", "next_word_inds", ")", "if", "\n", "next_word", "!=", "word_map", ".", "word2idx", "[", "'<end>'", "]", "]", "\n", "complete_inds", "=", "list", "(", "set", "(", "range", "(", "len", "(", "next_word_inds", ")", ")", ")", "-", "set", "(", "incomplete_inds", ")", ")", "\n", "\n", "# Set aside complete sequences", "\n", "if", "len", "(", "complete_inds", ")", ">", "0", ":", "\n", "                ", "complete_seqs", ".", "extend", "(", "seqs", "[", "complete_inds", "]", ".", "tolist", "(", ")", ")", "\n", "complete_seqs_scores", ".", "extend", "(", "top_k_scores", "[", "complete_inds", "]", ")", "\n", "", "k", "-=", "len", "(", "complete_inds", ")", "# reduce beam length accordingly", "\n", "\n", "# Proceed with incomplete sequences", "\n", "if", "k", "==", "0", ":", "\n", "                ", "break", "\n", "", "seqs", "=", "seqs", "[", "incomplete_inds", "]", "\n", "h", "=", "h", "[", "prev_word_inds", "[", "incomplete_inds", "]", "]", "\n", "c", "=", "c", "[", "prev_word_inds", "[", "incomplete_inds", "]", "]", "\n", "encoder_out", "=", "encoder_out", "[", "prev_word_inds", "[", "incomplete_inds", "]", "]", "\n", "top_k_scores", "=", "top_k_scores", "[", "incomplete_inds", "]", ".", "unsqueeze", "(", "1", ")", "\n", "k_prev_words", "=", "next_word_inds", "[", "incomplete_inds", "]", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Break if things have been going on too long", "\n", "if", "step", ">", "10", ":", "\n", "                ", "break", "\n", "", "step", "+=", "1", "\n", "\n", "#i = complete_seqs_scores.index(max(complete_seqs_scores))", "\n", "#seq = complete_seqs[i]", "\n", "", "seq", "=", "seqs", "[", "0", "]", "\n", "hypotheses", ".", "append", "(", "[", "w", "for", "w", "in", "seq", "if", "w", "not", "in", "{", "word_map", ".", "word2idx", "[", "'<start>'", "]", ",", "word_map", ".", "word2idx", "[", "'<end>'", "]", ",", "word_map", ".", "word2idx", "[", "'<pad>'", "]", "}", "]", ")", "\n", "\n", "return", "hypotheses", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.build_vocab_v2.Vocabulary.__init__": [[16, 20], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "word2idx", "=", "{", "}", "\n", "self", ".", "idx2word", "=", "{", "}", "\n", "self", ".", "idx", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.build_vocab_v2.Vocabulary.add_word": [[21, 26], ["None"], "methods", ["None"], ["", "def", "add_word", "(", "self", ",", "word", ")", ":", "\n", "        ", "if", "not", "word", "in", "self", ".", "word2idx", ":", "\n", "            ", "self", ".", "word2idx", "[", "word", "]", "=", "self", ".", "idx", "\n", "self", ".", "idx2word", "[", "self", ".", "idx", "]", "=", "word", "\n", "self", ".", "idx", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.build_vocab_v2.Vocabulary.__call__": [[27, 31], ["None"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "word", ")", ":", "\n", "        ", "if", "not", "word", "in", "self", ".", "word2idx", ":", "\n", "            ", "return", "self", ".", "word2idx", "[", "'<unk>'", "]", "\n", "", "return", "self", ".", "word2idx", "[", "word", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.build_vocab_v2.Vocabulary.__len__": [[32, 34], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "word2idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.build_vocab_v2.build_vocab": [[35, 62], ["COCO", "collections.Counter", "COCO.anns.keys", "enumerate", "build_vocab_v2.Vocabulary", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "enumerate", "str", "nltk.tokenize.word_tokenize", "collections.Counter.update", "build_vocab_v2.Vocabulary.add_word", "str.lower", "print", "collections.Counter.items", "len"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word"], ["", "", "def", "build_vocab", "(", "json", ",", "threshold", ")", ":", "\n", "    ", "\"\"\"Build a simple vocabulary wrapper.\"\"\"", "\n", "coco", "=", "COCO", "(", "json", ")", "\n", "counter", "=", "Counter", "(", ")", "\n", "ids", "=", "coco", ".", "anns", ".", "keys", "(", ")", "\n", "for", "i", ",", "id", "in", "enumerate", "(", "ids", ")", ":", "\n", "        ", "caption", "=", "str", "(", "coco", ".", "anns", "[", "id", "]", "[", "'caption'", "]", ")", "\n", "tokens", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "caption", ".", "lower", "(", ")", ")", "\n", "counter", ".", "update", "(", "tokens", ")", "\n", "\n", "if", "i", "%", "1000", "==", "0", ":", "\n", "            ", "print", "(", "\"[%d/%d] Tokenized the captions.\"", "%", "(", "i", ",", "len", "(", "ids", ")", ")", ")", "\n", "\n", "# If the word frequency is less than 'threshold', then the word is discarded.", "\n", "", "", "words", "=", "[", "word", "for", "word", ",", "cnt", "in", "counter", ".", "items", "(", ")", "if", "cnt", ">=", "threshold", "]", "\n", "\n", "# Creates a vocab wrapper and add some special tokens.", "\n", "vocab", "=", "Vocabulary", "(", ")", "\n", "vocab", ".", "add_word", "(", "'<pad>'", ")", "\n", "vocab", ".", "add_word", "(", "'<start>'", ")", "\n", "vocab", ".", "add_word", "(", "'<end>'", ")", "\n", "vocab", ".", "add_word", "(", "'<unk>'", ")", "\n", "\n", "# Adds the words to the vocabulary.", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "vocab", ".", "add_word", "(", "word", ")", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.build_vocab_v2.build_vocab_frogger": [[63, 90], ["open", "open.read().lower", "open.close", "re.sub.lower", "re.sub", "list", "build_vocab_v2.Vocabulary", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "enumerate", "re.sub.split", "build_vocab_v2.Vocabulary.add_word", "open.read"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.close", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word"], ["", "def", "build_vocab_frogger", "(", "json", ",", "threshold", ")", ":", "\n", "\n", "# Creates a vocab wrapper and add some special tokens.", "\n", "    ", "file", "=", "open", "(", "'Sample_Training_Set/Rationalizations.txt'", ",", "'r'", ")", "\n", "# .lower() returns a version with all upper case characters replaced with lower case characters.", "\n", "text", "=", "file", ".", "read", "(", ")", ".", "lower", "(", ")", "\n", "file", ".", "close", "(", ")", "\n", "# replaces anything that is not a lowercase letter, a space, or an apostrophe with a space:", "\n", "text", "=", "text", ".", "lower", "(", ")", "\n", "# print(type(text))", "\n", "# exit(0)", "\n", "text", "=", "re", ".", "sub", "(", "'[^a-z\\ \\']+'", ",", "\" \"", ",", "text", ")", "\n", "words", "=", "list", "(", "text", ".", "split", "(", ")", ")", "\n", "# print words", "\n", "# exit(0)", "\n", "vocab", "=", "Vocabulary", "(", ")", "\n", "vocab", ".", "add_word", "(", "'<pad>'", ")", "\n", "vocab", ".", "add_word", "(", "'<start>'", ")", "\n", "vocab", ".", "add_word", "(", "'<end>'", ")", "\n", "vocab", ".", "add_word", "(", "'<unk>'", ")", "\n", "\n", "\n", "\n", "# Adds the words to the vocabulary.", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "vocab", ".", "add_word", "(", "word", ")", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.build_vocab_v2.build_vocab_frogger_turk": [[91, 134], ["xlrd.open_workbook", "xlrd.open_workbook.sheets", "build_vocab_v2.Vocabulary", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "enumerate", "enumerate", "print", "collections.Counter", "range", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "actions.append", "nltk.tokenize.word_tokenize", "collections.Counter.update", "collections.Counter.items", "sheet.cell", "line.lower", "sheet.cell"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word"], ["", "def", "build_vocab_frogger_turk", "(", "json", ",", "threshold", ")", ":", "\n", "\n", "#open excel file and create a list of all the words in the rationalizations", "\n", "    ", "wb", "=", "open_workbook", "(", "args", ".", "caption_path", ")", "\n", "text", "=", "\"\"", "\n", "for", "sheet", "in", "wb", ".", "sheets", "(", ")", ":", "\n", "        ", "number_of_rows", "=", "sheet", ".", "nrows", "\n", "number_of_columns", "=", "sheet", ".", "ncols", "\n", "rationalizations", "=", "[", "]", "\n", "items", "=", "[", "]", "\n", "rows", "=", "[", "]", "\n", "lengths", "=", "[", "]", "\n", "max_length", "=", "0", "\n", "counter", "=", "Counter", "(", ")", "\n", "for", "row", "in", "range", "(", "1", ",", "number_of_rows", ")", ":", "\n", "            ", "values", "=", "[", "]", "\n", "line", "=", "sheet", ".", "cell", "(", "row", ",", "4", ")", ".", "value", "\n", "actions", ".", "append", "(", "sheet", ".", "cell", "(", "row", ",", "2", ")", ".", "value", ")", "\n", "tokens", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "line", ".", "lower", "(", ")", ")", "\n", "counter", ".", "update", "(", "tokens", ")", "\n", "", "", "words", "=", "[", "word", "for", "word", ",", "cnt", "in", "counter", ".", "items", "(", ")", "if", "cnt", ">=", "threshold", "]", "\n", "vocab", "=", "Vocabulary", "(", ")", "\n", "vocab", ".", "add_word", "(", "'<pad>'", ")", "\n", "vocab", ".", "add_word", "(", "'<start>'", ")", "\n", "vocab", ".", "add_word", "(", "'<end>'", ")", "\n", "vocab", ".", "add_word", "(", "'<unk>'", ")", "\n", "\n", "\n", "input_vocab", "=", "Vocabulary", "(", ")", "\n", "\n", "input_vocab", ".", "add_word", "(", "'<pad>'", ")", "\n", "input_vocab", ".", "add_word", "(", "'<start>'", ")", "\n", "input_vocab", ".", "add_word", "(", "'<end>'", ")", "\n", "input_vocab", ".", "add_word", "(", "'<unk>'", ")", "\n", "\n", "# Adds the words to the vocabulary", "\n", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "actions", ")", ":", "\n", "        ", "input_vocab", ".", "add_word", "(", "word", ")", "\n", "", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "vocab", ".", "add_word", "(", "word", ")", "\n", "", "print", "(", "vocab", ".", "word2idx", "[", "'again'", "]", ")", "\n", "return", "vocab", ",", "input_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.build_vocab_v2.build_vocab_input": [[135, 156], ["os.walk", "os.walk", "enumerate", "fnmatch.filter", "fnmatch.filter", "input_vocab.add_word", "open", "f.readlines", "enumerate", "open", "f.readlines", "enumerate", "os.path.join", "line.split", "enumerate", "os.path.join", "line.split", "enumerate", "words.append", "words.append", "str", "str"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word"], ["", "def", "build_vocab_input", "(", "input_vocab", ")", ":", "\n", "    ", "words", "=", "[", "]", "\n", "for", "dirpath", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "'./data/SymbRep(2)/Current'", ")", ":", "\n", "        ", "for", "filename", "in", "fnmatch", ".", "filter", "(", "files", ",", "'*.txt'", ")", ":", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "dirpath", ",", "filename", ")", ")", "as", "f", ":", "\n", "                ", "content", "=", "f", ".", "readlines", "(", ")", "\n", "for", "k", ",", "line", "in", "enumerate", "(", "content", ")", ":", "\n", "                    ", "nums", "=", "line", ".", "split", "(", ")", "\n", "for", "i", ",", "num", "in", "enumerate", "(", "nums", ")", ":", "\n", "                        ", "words", ".", "append", "(", "str", "(", "num", ")", ")", "\n", "", "", "", "", "", "for", "dirpath", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "'./data/SymbRep(2)/Next'", ")", ":", "\n", "        ", "for", "filename", "in", "fnmatch", ".", "filter", "(", "files", ",", "'*.txt'", ")", ":", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "dirpath", ",", "filename", ")", ")", "as", "f", ":", "\n", "                ", "content", "=", "f", ".", "readlines", "(", ")", "\n", "for", "k", ",", "line", "in", "enumerate", "(", "content", ")", ":", "\n", "                    ", "nums", "=", "line", ".", "split", "(", ")", "\n", "for", "i", ",", "num", "in", "enumerate", "(", "nums", ")", ":", "\n", "                        ", "words", ".", "append", "(", "str", "(", "num", ")", ")", "\n", "", "", "", "", "", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "input_vocab", ".", "add_word", "(", "word", ")", "\n", "", "return", "input_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.build_vocab_v2.main": [[157, 172], ["build_vocab_v2.build_vocab_frogger_turk", "build_vocab_v2.build_vocab_input", "print", "print", "print", "print", "open", "pickle.dump", "open", "pickle.dump", "len"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.build_vocab_frogger_turk", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.build_vocab_input"], ["", "def", "main", "(", "args", ")", ":", "\n", "# vocab = build_vocab(json=args.caption_path,", "\n", "#                     threshold=args.threshold)", "\n", "    ", "vocab", ",", "input_vocab", "=", "build_vocab_frogger_turk", "(", "json", "=", "args", ".", "caption_path", ",", "\n", "threshold", "=", "args", ".", "threshold", ")", "\n", "input_vocab", "=", "build_vocab_input", "(", "input_vocab", ")", "\n", "print", "(", "input_vocab", ")", "\n", "vocab_path", "=", "args", ".", "vocab_path", "\n", "print", "(", "input_vocab", ".", "word2idx", ")", "\n", "with", "open", "(", "vocab_path", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "vocab", ",", "f", ")", "\n", "", "with", "open", "(", "args", ".", "input_vocab_path", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "input_vocab", ",", "f", ")", "\n", "", "print", "(", "\"Total vocabulary size: %d\"", "%", "len", "(", "vocab", ")", ")", "\n", "print", "(", "\"Saved the vocabulary wrapper to '%s'\"", "%", "vocab_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.frogger_dataset_preprocessed.FroggerDataset.__init__": [[19, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data_file", ",", "vocab", ")", ":", "\n", "        ", "self", ".", "data_file", "=", "data_file", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.frogger_dataset_preprocessed.FroggerDataset.create_rationalization_matrix": [[23, 46], ["sent.strip.strip.replace", "sent.strip.strip.strip", "sent.strip.strip.lower().split", "len", "numpy.zeros", "enumerate", "rationalization_matrix.append", "sequence_lenght_arr.append", "sent.strip.strip.lower"], "methods", ["None"], ["", "def", "create_rationalization_matrix", "(", "self", ",", "rationalizations", ",", "vocab", ")", ":", "\n", "        ", "rationalization_matrix", "=", "[", "]", "\n", "sequence_lenght_arr", "=", "[", "]", "\n", "for", "sent", "in", "rationalizations", ":", "\n", "            ", "sent", "=", "sent", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", "\n", "sent", "=", "sent", ".", "strip", "(", ")", "\n", "words", "=", "sent", ".", "lower", "(", ")", ".", "split", "(", "' '", ")", "\n", "max_rationalization_len", "=", "len", "(", "words", ")", "\n", "max_rationalization_len", "=", "max_rationalization_len", "+", "2", "\n", "ration_sent", "=", "np", ".", "zeros", "(", "[", "max_rationalization_len", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "ration_sent", "[", "0", "]", "=", "1", "\n", "\n", "for", "k", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "                ", "if", "word", "in", "vocab", ".", "word2idx", ":", "\n", "                    ", "ration_sent", "[", "k", "+", "1", "]", "=", "vocab", ".", "word2idx", "[", "word", "]", "\n", "", "else", ":", "\n", "                    ", "ration_sent", "[", "k", "+", "1", "]", "=", "3", "\n", "\n", "", "", "ration_sent", "[", "max_rationalization_len", "-", "1", "]", "=", "2", "\n", "rationalization_matrix", ".", "append", "(", "ration_sent", ")", "\n", "sequence_lenght_arr", ".", "append", "(", "max_rationalization_len", "+", "2", ")", "\n", "\n", "", "return", "rationalization_matrix", ",", "sequence_lenght_arr", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.frogger_dataset_preprocessed.FroggerDataset.preprocessing": [[47, 61], ["advice.lower", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace", "line.replace.replace.replace"], "methods", ["None"], ["", "def", "preprocessing", "(", "advice", ")", ":", "\n", "#rationalization = []", "\n", "        ", "line", "=", "advice", ".", "lower", "(", ")", "\n", "line", "=", "line", ".", "replace", "(", "'\\''", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'.'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "','", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'``'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'\"'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'\\\\n'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "':'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'('", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "')'", ",", "''", ")", "\n", "\n", "return", "line", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.frogger_dataset_preprocessed.FroggerDataset.load_data": [[62, 172], ["xlrd.open_workbook", "set", "xlrd.open_workbook.sheets", "print", "int", "print", "print", "slice", "print", "slice", "print", "print", "print", "nltk.corpus.stopwords.words", "collections.Counter", "range", "open", "pickle.dump", "open", "pickle.dump", "math.floor", "len", "len", "len", "len", "len", "len", "len", "numpy.array", "numpy.array", "len", "sheet.cell", "good_ids.append", "re.sub.lower", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "nltk.tokenize.word_tokenize", "good_rationalizations.append", "re.sub", "re.sub.split", "len", "lengths.append", "new_id_list.append", "enumerate", "len", "lengths_unk.append", "rationalizations.append", "rationalization_tokens.append", "len", "sheet.cell", "action.lower.lower.lower", "word_token.append", "print", "sheet.cell", "new_id_list.append"], "methods", ["None"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "data_file", "=", "self", ".", "data_file", "\n", "vocab", "=", "self", ".", "vocab", "\n", "wb", "=", "open_workbook", "(", "data_file", ")", "\n", "stop_words", "=", "set", "(", "stopwords", ".", "words", "(", "'english'", ")", ")", "\n", "\n", "#read the rationalizations from the excel file and create a list of training/testing rationalizations. ", "\n", "for", "sheet", "in", "wb", ".", "sheets", "(", ")", ":", "\n", "            ", "number_of_rows", "=", "sheet", ".", "nrows", "\n", "number_of_columns", "=", "sheet", ".", "ncols", "\n", "rationalizations", "=", "[", "]", "\n", "rationalization_tokens", "=", "[", "]", "\n", "items", "=", "[", "]", "\n", "rows", "=", "[", "]", "\n", "lengths", "=", "[", "]", "\n", "lengths_unk", "=", "[", "]", "\n", "\n", "max_length", "=", "0", "\n", "\n", "bad_worker_ids", "=", "[", "'A2CNSIECB9UP05'", ",", "'A23782O23HSPLA'", ",", "'A2F9ZBSR6AXXND'", ",", "'A3GI86L18Z71XY'", ",", "'AIXTI8PKSX1D2'", ",", "'A2QWHXMFQI18GQ'", ",", "'A3SB7QYI84HYJT'", ",", "\n", "'A2Q2A7AB6MMFLI'", ",", "'A2P1KI42CJVNIA'", ",", "'A1IJXPKZTJV809'", ",", "'A2WZ0RZMKQ2WGJ'", ",", "'A3EKETMVGU2PM9'", ",", "'A1OCEC1TBE3CWA'", ",", "'AE1RYK54MH11G'", ",", "'A2ADEPVGNNXNPA'", ",", "\n", "'A15QGLWS8CNJFU'", ",", "'A18O3DEA5Z4MJD'", ",", "'AAAL4RENVAPML'", ",", "'A3TZBZ92CQKQLG'", ",", "'ABO9F0JD9NN54'", ",", "'A8F6JFG0WSELT'", ",", "'ARN9ET3E608LJ'", ",", "'A2TCYNRAZWK8CC'", ",", "\n", "'A32BK0E1IPDUAF'", ",", "'ANNV3E6CIVCW4'", ",", "'AXMQBHHU22TSP'", ",", "'AKATSYE8XLYNL'", ",", "'A355PGLV2ID2SX'", ",", "'A55CXM7QR7R0N'", ",", "'A111ZFNLXK1TCO'", "]", "\n", "\n", "good_ids", "=", "[", "]", "\n", "good_rationalizations", "=", "[", "]", "\n", "counter", "=", "Counter", "(", ")", "\n", "for", "row", "in", "range", "(", "1", ",", "number_of_rows", ")", ":", "\n", "                ", "values", "=", "[", "]", "\n", "worker_id", "=", "sheet", ".", "cell", "(", "row", ",", "0", ")", ".", "value", "\n", "if", "worker_id", "not", "in", "bad_worker_ids", ":", "\n", "                    ", "good_ids", ".", "append", "(", "row", "-", "1", ")", "\n", "line", "=", "sheet", ".", "cell", "(", "row", ",", "4", ")", ".", "value", "\n", "line", "=", "line", ".", "lower", "(", ")", "\n", "line", "=", "line", ".", "replace", "(", "'\\''", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'.'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "','", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'``'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'\"'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'\\\\n'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "':'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'('", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "')'", ",", "''", ")", "\n", "#line = self.preprocessing(line)", "\n", "tokens", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "line", ")", "\n", "word_token", "=", "[", "w", "for", "w", "in", "tokens", "if", "not", "w", "in", "stop_words", "]", "\n", "#line = line.lower()", "\n", "good_rationalizations", ".", "append", "(", "line", ")", "\n", "#line = re.sub('[^a-z\\ ]+', \" \", line)", "\n", "line", "=", "re", ".", "sub", "(", "r'[^\\w\\s]'", ",", "' '", ",", "line", ")", "\n", "#word_token= [w for w in tokens if not w in stop_words]", "\n", "words", "=", "line", ".", "split", "(", ")", "\n", "\n", "length", "=", "len", "(", "word_token", ")", "\n", "if", "length", "==", "0", ":", "\n", "                        ", "action", "=", "sheet", ".", "cell", "(", "row", ",", "2", ")", ".", "value", "\n", "action", "=", "action", ".", "lower", "(", ")", "\n", "word_token", ".", "append", "(", "action", ")", "\n", "print", "(", "\"short sentence: \"", ",", "word_token", ")", "\n", "", "lengths", ".", "append", "(", "length", ")", "\n", "if", "length", ">", "max_length", ":", "\n", "                        ", "max_length", "=", "length", "\n", "", "new_id_list", "=", "[", "]", "\n", "new_id_list", ".", "append", "(", "vocab", ".", "word2idx", "[", "'<start>'", "]", ")", "\n", "for", "index", ",", "word", "in", "enumerate", "(", "word_token", ")", ":", "\n", "                        ", "if", "word", "in", "vocab", ".", "word2idx", ":", "\n", "                            ", "id_word", "=", "vocab", ".", "word2idx", "[", "word", "]", "\n", "new_id_list", ".", "append", "(", "id_word", ")", "\n", "\n", "#else:", "\n", "#word_token[index] = vocab.word2idx['<unk>']", "\n", "", "", "length_unk", "=", "len", "(", "new_id_list", ")", "\n", "lengths_unk", ".", "append", "(", "length_unk", ")", "\n", "rationalizations", ".", "append", "(", "words", ")", "\n", "rationalization_tokens", ".", "append", "(", "new_id_list", ")", "\n", "", "", "rationalizations", "=", "[", "np", ".", "array", "(", "xi", ")", "for", "xi", "in", "rationalizations", "]", "\n", "rationalization_tokens", "=", "[", "np", ".", "array", "(", "xi", ")", "for", "xi", "in", "rationalization_tokens", "]", "\n", "\n", "#good_ids_split = int(floor((90.0/100)*len(good_ids)))", "\n", "", "with", "open", "(", "\"sentence_len.pkl\"", ",", "\"wb\"", ")", "as", "output_file", ":", "\n", "            ", "pkl", ".", "dump", "(", "lengths", ",", "output_file", ")", "\n", "", "with", "open", "(", "\"sent_len_no_unk.pkl\"", ",", "\"wb\"", ")", "as", "output_file", ":", "\n", "            ", "pkl", ".", "dump", "(", "lengths_unk", ",", "output_file", ")", "\n", "\n", "", "print", "(", "\"max_length: \"", ",", "max_length", ")", "\n", "split", "=", "int", "(", "floor", "(", "(", "95.0", "/", "100", ")", "*", "len", "(", "rationalizations", ")", ")", ")", "\n", "print", "(", "\"good_rationalizations: \"", ",", "len", "(", "good_rationalizations", ")", ")", "\n", "#print(\"good_rationalizations: \",good_rationalizations)\t\t", "\n", "print", "(", "\"rationalizations_tokens: \"", ",", "len", "(", "rationalization_tokens", ")", ")", "\n", "#print(\"rationalizations: \",rationalizations)        ", "\n", "tr", "=", "slice", "(", "0", ",", "split", ")", "\n", "tr_good_ids", "=", "good_ids", "[", "tr", "]", "\n", "print", "(", "\"tr_good_ids: \"", ",", "len", "(", "tr_good_ids", ")", ")", "\n", "\n", "tr_indices", "=", "[", "0", ",", "split", "-", "1", "]", "\n", "#print(\"tr_contents: \",tr)", "\n", "te_indices", "=", "[", "split", ",", "len", "(", "rationalizations", ")", "-", "1", "]", "\n", "\n", "te", "=", "slice", "(", "split", ",", "len", "(", "rationalizations", ")", ")", "\n", "te_good_ids", "=", "good_ids", "[", "te", "]", "\n", "print", "(", "\"te_good_ids: \"", ",", "len", "(", "te_good_ids", ")", ")", "\n", "#print(\"te_contents: \",te)", "\n", "#training_rationalizations = good_rationalizations[tr]", "\n", "#testing_rationalizations = good_rationalizations[te]", "\n", "training_rationalizations", "=", "rationalization_tokens", "[", "tr", "]", "\n", "print", "(", "\"train rationalizations_tokens: \"", ",", "len", "(", "training_rationalizations", ")", ")", "\n", "testing_rationalizations", "=", "rationalization_tokens", "[", "te", "]", "\n", "print", "(", "\"test rationalizations_tokens: \"", ",", "len", "(", "testing_rationalizations", ")", ")", "\n", "\n", "return", "tr_good_ids", ",", "te_good_ids", ",", "tr_indices", ",", "te_indices", ",", "training_rationalizations", ",", "testing_rationalizations", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.frogger_dataset_preprocessed.FroggerDataset.load_images": [[173, 199], ["os.listdir", "sorted", "len", "print", "enumerate", "cur_training_images.append", "cur_test_images.append"], "methods", ["None"], ["", "def", "load_images", "(", "self", ",", "current_image_dir", ",", "tr_good_ids", ",", "te_good_ids", ",", "tr_indices", ")", ":", "\n", "        ", "current_images", "=", "os", ".", "listdir", "(", "current_image_dir", ")", "\n", "current_images", "=", "sorted", "(", "current_images", ",", "key", "=", "numericalSort", ")", "\n", "\n", "#print(good_ids[tr_indices[0]]", "\n", "\n", "#next_images = os.listdir(next_image_dir)", "\n", "#next_images = sorted(next_images ,key = numericalSort)", "\n", "num_images", "=", "len", "(", "current_images", ")", "\n", "print", "(", "\"number of images: \"", ",", "num_images", ")", "\n", "\n", "cur_training_images", "=", "[", "]", "\n", "#next_training_images = []", "\n", "cur_test_images", "=", "[", "]", "\n", "#next_test_images = []", "\n", "for", "i", ",", "file", "in", "enumerate", "(", "current_images", ")", ":", "\n", "            ", "if", "i", "in", "tr_good_ids", ":", "\n", "                ", "cur_training_images", ".", "append", "(", "file", ")", "\n", "#if good_ids[tr_indices[0]]<=i and i<=good_ids[tr_indices[1]]:", "\n", "#cur_training_images.append(file)", "\n", "#next_training_images.append(next_images[i])", "\n", "", "elif", "i", "in", "te_good_ids", ":", "\n", "                ", "cur_test_images", ".", "append", "(", "file", ")", "\n", "#next_test_images.append(next_images[i])", "\n", "#print(\"number of test images: \",len(cur_test_images))       ", "\n", "", "", "return", "cur_training_images", ",", "cur_test_images", "\n", "", "", ""]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice_support_codes.frogger_dataset_preprocessed.numericalSort": [[13, 17], ["numbers.split", "map"], "function", ["None"], ["def", "numericalSort", "(", "value", ")", ":", "\n", "    ", "parts", "=", "numbers", ".", "split", "(", "value", ")", "\n", "parts", "[", "1", ":", ":", "2", "]", "=", "map", "(", "int", ",", "parts", "[", "1", ":", ":", "2", "]", ")", "\n", "return", "parts", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.__init__": [[163, 166], ["game_access.Game.load_env"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.load_env"], ["def", "__init__", "(", "self", ",", "game_location", ")", ":", "\n", "        ", "self", ".", "ENV_LOCATION", "=", "game_location", "\n", "self", ".", "load_env", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.load_env": [[171, 180], ["mlagents.envs.UnityEnvironment", "game_access.Game.env.reset"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.reset"], ["def", "load_env", "(", "self", ",", "wid", ")", ":", "\n", "# load env", "\n", "        ", "env_name", "=", "self", ".", "ENV_LOCATION", "\n", "self", ".", "env", "=", "UnityEnvironment", "(", "env_name", ",", "worker_id", "=", "wid", ")", "\n", "# Set the default brain to work with", "\n", "self", ".", "default_brain", "=", "self", ".", "env", ".", "brain_names", "[", "0", "]", "\n", "self", ".", "brain", "=", "self", ".", "env", ".", "brains", "[", "self", ".", "default_brain", "]", "\n", "# Reset the environment - train mode enabled", "\n", "env_info", "=", "self", ".", "env", ".", "reset", "(", "train_mode", "=", "True", ")", "[", "self", ".", "default_brain", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.perform_action": [[195, 251], ["numpy.zeros", "round", "skimage.transform.resize", "numpy.reshape", "range", "game_access.Game.env.step", "skimage.transform.resize", "numpy.reshape", "range", "game_access.Game.env.step", "round"], "methods", ["None"], ["def", "perform_action", "(", "self", ",", "action_value", ",", "image_height", ",", "image_width", ",", "number_of_frames", "=", "4", ")", ":", "\n", "#print(\"action_value: \",action_value)", "\n", "        ", "action", "=", "[", "[", "0", "]", "]", "\n", "#print(\"action: \",action)", "\n", "action", "[", "0", "]", "=", "action_value", "\n", "#print(\"action[0]: \",action[0])", "\n", "terminal", "=", "False", "# indication of terminal state", "\n", "# 3 - R, G, B", "\n", "#size = (image_height, image_width, 3, number_of_frames)  # create list to keep frames", "\n", "size", "=", "(", "number_of_frames", ",", "3", ",", "image_height", ",", "image_width", ")", "\n", "stack", "=", "np", ".", "zeros", "(", "size", ")", "\n", "# to store data to sym rep 32 data is send from game", "\n", "observation_data", "=", "[", "[", "0", "]", "*", "34", "for", "i", "in", "range", "(", "number_of_frames", ")", "]", "\n", "\n", "# first frame after action", "\n", "env_info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "[", "self", ".", "default_brain", "]", "# send action to brain", "\n", "reward", "=", "round", "(", "env_info", ".", "rewards", "[", "0", "]", ",", "5", ")", "# get reward", "\n", "new_state", "=", "env_info", ".", "visual_observations", "[", "0", "]", "[", "0", "]", "# get state visual observation", "\n", "#print(\"new_state keras shape: \", new_state_gray.shape)", "\n", "#print(\"new_state keras shape: \", new_state.shape)", "\n", "observations", "=", "env_info", ".", "vector_observations", "# get vector observations", "\n", "observation_data", "[", "0", "]", "=", "observations", "[", "0", "]", "\n", "# new_state_gray = skimage.color.rgb2gray(new_state)  # covert to gray scale", "\n", "new_state_gray", "=", "skimage", ".", "transform", ".", "resize", "(", "new_state", ",", "(", "image_height", ",", "image_width", ")", ")", "# resize", "\n", "#print(\"new_state_gray shape: \",new_state_gray.shape)", "\n", "new_state_reshape", "=", "np", ".", "reshape", "(", "new_state_gray", ",", "(", "3", ",", "100", ",", "100", ")", ")", "\n", "#print(\"new_state keras shape: \", new_state_reshape.shape)", "\n", "# check terminal reached", "\n", "if", "env_info", ".", "local_done", "[", "0", "]", ":", "\n", "            ", "terminal", "=", "True", "\n", "\n", "# add the state to the 0 th position of stack", "\n", "", "stack", "[", "0", ",", ":", ",", ":", ",", ":", "]", "=", "new_state_reshape", "\n", "\n", "# get stack of frames after the action", "\n", "for", "i", "in", "range", "(", "1", ",", "number_of_frames", ")", ":", "\n", "            ", "env_info", "=", "self", ".", "env", ".", "step", "(", ")", "[", "self", ".", "default_brain", "]", "# change environment to next step without action", "\n", "st", "=", "env_info", ".", "visual_observations", "[", "0", "]", "[", "0", "]", "\n", "\n", "observations", "=", "env_info", ".", "vector_observations", "# get vector observations", "\n", "observation_data", "[", "i", "]", "=", "observations", "[", "0", "]", "\n", "#st_gray = skimage.color.rgb2gray(st)", "\n", "st_gray", "=", "skimage", ".", "transform", ".", "resize", "(", "st", ",", "(", "image_height", ",", "image_width", ")", ")", "\n", "st_gray_reshape", "=", "np", ".", "reshape", "(", "new_state_gray", ",", "(", "3", ",", "100", ",", "100", ")", ")", "\n", "stack", "[", "i", ",", ":", ",", ":", ",", ":", "]", "=", "st_gray_reshape", "\n", "# if terminal only consider the reward for terminal", "\n", "if", "env_info", ".", "local_done", "[", "0", "]", ":", "\n", "                ", "terminal", "=", "True", "\n", "reward", "=", "round", "(", "env_info", ".", "rewards", "[", "0", "]", ",", "5", ")", "\n", "\n", "# reshape for Keras", "\n", "# noinspection PyArgumentList", "\n", "#stack = stack.reshape(1, stack.shape[0], stack.shape[1], stack.shape[2], stack.shape[3])", "\n", "\n", "#return reward, stack, terminal, observation_data", "\n", "", "", "return", "reward", ",", "stack", ",", "terminal", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.close": [[256, 258], ["game_access.Game.env.close"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.close"], ["def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.reset": [[262, 265], ["game_access.Game.close", "game_access.Game.load_env"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.close", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.load_env"], ["def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "close", "(", ")", "\n", "self", ".", "load_env", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.create_sym_rep": [[29, 156], ["int", "int", "int", "int", "range", "range", "range", "range", "range", "range", "range", "range", "range", "int", "int", "int", "range", "range", "range", "range", "range", "range", "range"], "function", ["None"], ["", "def", "create_sym_rep", "(", "observation_data", ")", ":", "\n", "# frog in goal and witch ine is it ?", "\n", "    ", "frog_in_goal", "=", "int", "(", "observation_data", "[", "33", "]", ")", "# 0- if none , 1 - for right most one and 2 for left one", "\n", "# is car in tunnel", "\n", "car_in_tunnel", "=", "int", "(", "observation_data", "[", "32", "]", ")", "# 1 if car is inside the tunnel", "\n", "# frog position send them as the index for the state array", "\n", "# (12,8) will be the start position", "\n", "# to calculate x position (original x value - 50) / 25 and round this to nearest int (coz of the logs)", "\n", "# y should be fixed and when it go up or down +1 or - 1", "\n", "frog_x", "=", "int", "(", "observation_data", "[", "0", "]", ")", "\n", "frog_y", "=", "int", "(", "observation_data", "[", "1", "]", ")", "\n", "\n", "current_state", "=", "[", "[", "0", "]", "*", "Constant", ".", "STATE_WIDTH", "for", "i", "in", "range", "(", "Constant", ".", "STATE_HEIGHT", ")", "]", "\n", "\n", "# reset state part", "\n", "\n", "# init all to empty", "\n", "# only need to do this from 5 - 11 index (road part)", "\n", "\n", "for", "i", "in", "range", "(", "5", ",", "12", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "0", ",", "Constant", ".", "STATE_WIDTH", ")", ":", "\n", "            ", "current_state", "[", "i", "]", "[", "j", "]", "=", "StateKey", ".", "EMPTY", ".", "value", "\n", "\n", "# add platform and start pos", "\n", "# 13 - start , 12,4 platform", "\n", "", "", "for", "j", "in", "range", "(", "0", ",", "Constant", ".", "STATE_WIDTH", ")", ":", "\n", "        ", "current_state", "[", "13", "]", "[", "j", "]", "=", "StateKey", ".", "START", ".", "value", "\n", "current_state", "[", "12", "]", "[", "j", "]", "=", "StateKey", ".", "PLATFORM", ".", "value", "\n", "current_state", "[", "4", "]", "[", "j", "]", "=", "StateKey", ".", "PLATFORM", ".", "value", "\n", "\n", "# fill with hazard zone", "\n", "# rows 0 - 3 and col - all", "\n", "", "for", "i", "in", "range", "(", "0", ",", "4", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "0", ",", "Constant", ".", "STATE_WIDTH", ")", ":", "\n", "            ", "current_state", "[", "i", "]", "[", "j", "]", "=", "StateKey", ".", "HAZARD_ZONE", ".", "value", "\n", "\n", "# add goal positions", "\n", "# if frog is in the goal mark it as hazard ???", "\n", "# goal pos 1 is in 5 - 6 col in 0 th row", "\n", "", "", "for", "j", "in", "range", "(", "5", ",", "7", ")", ":", "\n", "# frog is in 1st goal pos", "\n", "        ", "if", "frog_in_goal", "==", "1", ":", "\n", "            ", "current_state", "[", "0", "]", "[", "j", "]", "=", "StateKey", ".", "HAZARD_ZONE", ".", "value", "\n", "", "else", ":", "\n", "            ", "current_state", "[", "0", "]", "[", "j", "]", "=", "StateKey", ".", "WIN", ".", "value", "\n", "\n", "# goal pos 2 is in 10 - 11 col in 0 th row", "\n", "", "", "for", "j", "in", "range", "(", "10", ",", "12", ")", ":", "\n", "# frog is in 2nd goal pos", "\n", "        ", "if", "frog_in_goal", "==", "2", ":", "\n", "            ", "current_state", "[", "0", "]", "[", "j", "]", "=", "StateKey", ".", "HAZARD_ZONE", ".", "value", "\n", "", "else", ":", "\n", "            ", "current_state", "[", "0", "]", "[", "j", "]", "=", "StateKey", ".", "WIN", ".", "value", "\n", "\n", "# mark tunnel", "\n", "# if car in tunnel mark it as lit  ow as a wall", "\n", "# its in row 8 col [6-12]", "\n", "", "", "for", "j", "in", "range", "(", "6", ",", "13", ")", ":", "\n", "# frog is in 2nd goal pos", "\n", "        ", "if", "car_in_tunnel", "==", "1", ":", "\n", "            ", "current_state", "[", "8", "]", "[", "j", "]", "=", "StateKey", ".", "TUNNEL_ON_KEY", ".", "value", "\n", "", "else", ":", "\n", "            ", "current_state", "[", "8", "]", "[", "j", "]", "=", "StateKey", ".", "WALL", ".", "value", "\n", "\n", "# frog position", "\n", "# from game I can directly send frog x , y as a index for this array", "\n", "", "", "current_state", "[", "frog_y", "]", "[", "frog_x", "]", "=", "StateKey", ".", "FROG", ".", "value", "\n", "\n", "# Add cars / logs", "\n", "# array  observations [2 - 20]", "\n", "# s , x , z", "\n", "\n", "index", "=", "1", "# easy to travel the array 1st car y data at 4 th index", "\n", "for", "i", "in", "range", "(", "0", ",", "10", ")", ":", "\n", "        ", "index", "+=", "3", "\n", "x_pos", "=", "int", "(", "observation_data", "[", "index", "-", "1", "]", ")", "\n", "y_pos", "=", "int", "(", "observation_data", "[", "index", "]", ")", "\n", "size", "=", "int", "(", "observation_data", "[", "index", "-", "2", "]", ")", "\n", "\n", "# out of view car /log", "\n", "if", "x_pos", "+", "size", "<=", "0", "or", "x_pos", ">", "16", ":", "\n", "            ", "continue", "\n", "", "elif", "x_pos", "<", "0", "<", "x_pos", "+", "size", ":", "\n", "            ", "size", "=", "size", "+", "x_pos", "\n", "x_pos", "=", "0", "\n", "# car in tunnel lane", "\n", "", "if", "y_pos", "==", "8", ":", "\n", "# car is fully in tunnel", "\n", "            ", "if", "x_pos", ">=", "6", "and", "x_pos", "+", "size", "<=", "12", ":", "\n", "                ", "continue", "\n", "# car is left and in tunnel", "\n", "", "elif", "x_pos", "<", "6", "<=", "x_pos", "+", "size", ":", "\n", "                ", "for", "j", "in", "range", "(", "x_pos", ",", "6", ")", ":", "\n", "                    ", "current_state", "[", "y_pos", "]", "[", "j", "]", "=", "StateKey", ".", "RED_CAR", ".", "value", "\n", "", "", "elif", "x_pos", "<=", "12", "<", "x_pos", "+", "size", ":", "\n", "                ", "for", "j", "in", "range", "(", "13", ",", "x_pos", "+", "size", ")", ":", "\n", "                    ", "current_state", "[", "y_pos", "]", "[", "j", "]", "=", "StateKey", ".", "RED_CAR", ".", "value", "\n", "# out of tunnel", "\n", "", "", "else", ":", "\n", "                ", "for", "j", "in", "range", "(", "x_pos", ",", "x_pos", "+", "size", ")", ":", "\n", "                    ", "if", "j", ">", "16", ":", "\n", "                        ", "break", "\n", "", "current_state", "[", "y_pos", "]", "[", "j", "]", "=", "StateKey", ".", "RED_CAR", ".", "value", "\n", "\n", "# all other cars /logs", "\n", "", "", "", "elif", "index", "<=", "32", ":", "\n", "# todo : get car color from game", "\n", "# for blue cars", "\n", "            ", "if", "y_pos", "==", "5", "or", "y_pos", "==", "7", "or", "y_pos", "==", "9", "or", "y_pos", "==", "10", ":", "\n", "                ", "for", "j", "in", "range", "(", "x_pos", ",", "x_pos", "+", "size", ")", ":", "\n", "                    ", "if", "j", ">", "16", ":", "\n", "                        ", "break", "\n", "", "current_state", "[", "y_pos", "]", "[", "j", "]", "=", "StateKey", ".", "BLUE_CAR", ".", "value", "\n", "# for red cars", "\n", "", "", "elif", "y_pos", "==", "6", "or", "y_pos", "==", "8", "or", "y_pos", "==", "11", ":", "\n", "                ", "for", "j", "in", "range", "(", "x_pos", ",", "x_pos", "+", "size", ")", ":", "\n", "                    ", "if", "j", ">", "16", ":", "\n", "                        ", "break", "\n", "", "current_state", "[", "y_pos", "]", "[", "j", "]", "=", "StateKey", ".", "RED_CAR", ".", "value", "\n", "# logs", "\n", "", "", "else", ":", "\n", "                ", "for", "j", "in", "range", "(", "x_pos", ",", "x_pos", "+", "size", ")", ":", "\n", "                    ", "if", "j", ">", "16", ":", "\n", "                        ", "break", "\n", "", "current_state", "[", "y_pos", "]", "[", "j", "]", "=", "StateKey", ".", "LOG", ".", "value", "\n", "\n", "", "", "", "", "return", "current_state", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.EncoderCNN.__init__": [[27, 37], ["torch.Module.__init__", "torchvision.resnet50", "torchvision.resnet50.parameters", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "param.requires_grad_", "list", "torchvision.resnet50.children"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["        ", "super", "(", "EncoderCNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "resnet", "=", "models", ".", "resnet50", "(", "pretrained", "=", "True", ")", "\n", "for", "param", "in", "resnet", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad_", "(", "False", ")", "\n", "\n", "", "modules", "=", "list", "(", "resnet", ".", "children", "(", ")", ")", "[", ":", "-", "1", "]", "\n", "self", ".", "resnet", "=", "nn", ".", "Sequential", "(", "*", "modules", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout2d", "(", "p", "=", "0.2", ")", "\n", "self", ".", "embed", "=", "nn", ".", "Linear", "(", "resnet", ".", "fc", ".", "in_features", ",", "embed_size", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "images", ")", ":", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.EncoderCNN.forward": [[38, 49], ["advice_models.EncoderCNN.resnet", "advice_models.EncoderCNN.view", "advice_models.EncoderCNN.dropout", "advice_models.EncoderCNN.embed", "advice_models.EncoderCNN.size"], "methods", ["None"], ["#print(\"images shape: \", images.shape)", "\n", "        ", "features", "=", "self", ".", "resnet", "(", "images", ")", "\n", "#print(\"features shape: \", features.shape)", "\n", "features", "=", "features", ".", "view", "(", "features", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "#print(\"features shape after view: \", features.shape)", "\n", "features", "=", "self", ".", "dropout", "(", "features", ")", "\n", "#print(\"features shape after dropout: \", features.shape)", "\n", "features", "=", "self", ".", "embed", "(", "features", ")", "\n", "# print(\"features shape after embed: \", features.shape)", "\n", "return", "features", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.DecoderRNN.__init__": [[51, 68], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "advice_models.DecoderRNN.init_weights"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_weights"], ["", "", "class", "DecoderRNN", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "word_embed_size", ",", "image_emb_size", ",", "hidden_size", ",", "vocab_size", ",", "num_layers", "=", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding_layer", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "word_embed_size", ")", "\n", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "input_size", "=", "word_embed_size", "+", "image_emb_size", ",", "hidden_size", "=", "hidden_size", ",", "\n", "num_layers", "=", "num_layers", ",", "batch_first", "=", "True", ")", "\n", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "vocab_size", ")", "\n", "# self.hid_linear1 = nn.Linear(image_emb_size, hidden_size)", "\n", "# self.hid_linear2 = nn.Linear(image_emb_size, hidden_size)", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout2d", "(", "p", "=", "0.5", ")", "\n", "self", ".", "n_layers", "=", "num_layers", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "word_embed_size", "=", "word_embed_size", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "image_emb_size", "=", "image_emb_size", "\n", "self", ".", "init_weights", "(", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.DecoderRNN.init_weights": [[69, 72], ["advice_models.DecoderRNN.linear.weight.data.normal_", "advice_models.DecoderRNN.linear.bias.data.fill_"], "methods", ["None"], ["\n", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "self", ".", "linear", ".", "weight", ".", "data", ".", "normal_", "(", "0.0", ",", "0.02", ")", "\n", "self", ".", "linear", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.DecoderRNN.init_word_embedding": [[77, 80], ["None"], "methods", ["None"], ["\n", "", "def", "init_word_embedding", "(", "self", ",", "weight_init", ")", ":", "\n", "        ", "assert", "weight_init", ".", "shape", "==", "(", "self", ".", "vocab_size", ",", "self", ".", "word_embed_size", ")", "\n", "self", ".", "embedding_layer", ".", "weight", ".", "data", "[", ":", "self", ".", "vocab_size", "]", "=", "weight_init", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.DecoderRNN.init_hidden": [[81, 95], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "h0.cuda.cuda.cuda", "c0.cuda.cuda.cuda"], "methods", ["None"], ["\n", "", "def", "init_hidden", "(", "self", ",", "features", ")", ":", "\n", "        ", "batch_size", "=", "features", ".", "shape", "[", "0", "]", "\n", "# _h = self.hid_linear1(features)", "\n", "# _c = self.hid_linear2(features)", "\n", "# h0 = _h.view(1 * self.n_layers, batch_size, self.hidden_size)", "\n", "# c0 = _c.view(1 * self.n_layers, batch_size, self.hidden_size)", "\n", "h0", "=", "torch", ".", "zeros", "(", "1", "*", "self", ".", "n_layers", ",", "batch_size", ",", "self", ".", "hidden_size", ")", "\n", "c0", "=", "torch", ".", "zeros", "(", "1", "*", "self", ".", "n_layers", ",", "batch_size", ",", "self", ".", "hidden_size", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "h0", "=", "h0", ".", "cuda", "(", ")", "\n", "c0", "=", "c0", ".", "cuda", "(", ")", "\n", "\n", "", "return", "(", "h0", ",", "c0", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.DecoderRNN.forward": [[96, 114], ["advice_models.DecoderRNN.embedding_layer", "advice_models.DecoderRNN.init_hidden", "features.expand.expand.unsqueeze", "features.expand.expand.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "advice_models.DecoderRNN.lstm", "advice_models.DecoderRNN.dropout", "advice_models.DecoderRNN.linear", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden"], ["\n", "", "def", "forward", "(", "self", ",", "features", ",", "captions", ")", ":", "\n", "\n", "# captions = captions[:, :-1]", "\n", "        ", "caption_shape", "=", "captions", ".", "shape", "\n", "embeddings", "=", "self", ".", "embedding_layer", "(", "captions", ")", "\n", "(", "h", ",", "c", ")", "=", "self", ".", "init_hidden", "(", "features", ")", "\n", "features", "=", "features", ".", "unsqueeze", "(", "1", ")", "\n", "features", "=", "features", ".", "expand", "(", "-", "1", ",", "caption_shape", "[", "1", "]", ",", "-", "1", ")", "\n", "lstm_input", "=", "torch", ".", "cat", "(", "(", "features", ",", "embeddings", ")", ",", "2", ")", "\n", "\n", "# embed = torch.cat((features.unsqueeze(1), embed), dim = 1)", "\n", "\n", "\n", "lstm_outputs", ",", "_", "=", "self", ".", "lstm", "(", "lstm_input", ",", "(", "h", ",", "c", ")", ")", "\n", "lstm_outputs", "=", "self", ".", "dropout", "(", "lstm_outputs", ")", "\n", "out", "=", "self", ".", "linear", "(", "lstm_outputs", ")", "\n", "predicted", "=", "torch", ".", "argmax", "(", "out", ",", "dim", "=", "2", ")", "\n", "return", "out", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.DecoderRNN.inference": [[115, 139], ["advice_models.DecoderRNN.init_hidden", "features.unsqueeze.unsqueeze.unsqueeze", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "advice_models.DecoderRNN.embedding_layer", "advice_models.DecoderRNN.unsqueeze", "advice_models.DecoderRNN.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "advice_models.DecoderRNN.lstm", "advice_models.DecoderRNN.linear", "output.squeeze.squeeze.squeeze", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "sampled_ids.append", "advice_models.DecoderRNN.embedding_layer().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.log_softmax.max", "advice_models.DecoderRNN.embedding_layer"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden"], ["\n", "", "def", "inference", "(", "self", ",", "features", ")", ":", "\n", "        ", "batch_size", "=", "features", ".", "shape", "[", "0", "]", "\n", "(", "hn", ",", "cn", ")", "=", "self", ".", "init_hidden", "(", "features", ")", "\n", "\n", "sampled_ids", "=", "[", "]", "\n", "features", "=", "features", ".", "unsqueeze", "(", "1", ")", "\n", "predicted", "=", "torch", ".", "tensor", "(", "[", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "embed", "=", "self", ".", "embedding_layer", "(", "predicted", ")", "\n", "embed", "=", "embed", ".", "unsqueeze", "(", "1", ")", "\n", "embed", "=", "embed", ".", "expand", "(", "batch_size", ",", "-", "1", ",", "-", "1", ")", "\n", "lstm_input", "=", "torch", ".", "cat", "(", "(", "features", ",", "embed", ")", ",", "2", ")", "\n", "for", "i", "in", "range", "(", "10", ")", ":", "\n", "            ", "lstm_outputs", ",", "(", "hn", ",", "cn", ")", "=", "self", ".", "lstm", "(", "lstm_input", ",", "(", "hn", ",", "cn", ")", ")", "\n", "# lstm_outputs = self.dropout(lstm_outputs)", "\n", "output", "=", "self", ".", "linear", "(", "lstm_outputs", ")", "\n", "output", "=", "output", ".", "squeeze", "(", "1", ")", "\n", "scores", "=", "F", ".", "log_softmax", "(", "output", ",", "dim", "=", "1", ")", "\n", "_predicted", "=", "torch", ".", "argmax", "(", "output", ",", "dim", "=", "1", ")", "\n", "predicted", "=", "scores", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "sampled_ids", ".", "append", "(", "predicted", ")", "\n", "embed", "=", "self", ".", "embedding_layer", "(", "predicted", ")", ".", "unsqueeze", "(", "1", ")", "\n", "lstm_input", "=", "torch", ".", "cat", "(", "(", "features", ",", "embed", ")", ",", "2", ")", "\n", "\n", "", "return", "sampled_ids", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.image_EncoderCNN.__init__": [[141, 156], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["", "", "class", "image_EncoderCNN", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "image_embedding_size", ")", ":", "\n", "        ", "super", "(", "image_EncoderCNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "3", ",", "16", ",", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "max1", "=", "nn", ".", "MaxPool2d", "(", "2", ",", "2", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "16", ",", "32", ",", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "max2", "=", "nn", ".", "MaxPool2d", "(", "2", ",", "2", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "3", ",", "stride", "=", "1", ")", "\n", "self", ".", "max3", "=", "nn", ".", "MaxPool2d", "(", "2", ",", "1", ")", "\n", "#self.flatten = torch.flatten()", "\n", "\n", "#self.resnet = nn.Sequential(*modules)", "\n", "#self.dropout = nn.Dropout2d(p=0.2)", "\n", "fc1_size", "=", "1024", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "6400", ",", "fc1_size", ")", "\n", "self", ".", "embed", "=", "nn", ".", "Linear", "(", "fc1_size", ",", "image_embedding_size", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.image_EncoderCNN.init_weights": [[158, 161], ["advice_models.image_EncoderCNN.fc1.weight.data.normal_", "advice_models.image_EncoderCNN.fc1.bias.data.fill_"], "methods", ["None"], ["\n", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "self", ".", "fc1", ".", "weight", ".", "data", ".", "normal_", "(", "0.0", ",", "0.02", ")", "\n", "self", ".", "fc1", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.image_EncoderCNN.forward": [[162, 182], ["advice_models.image_EncoderCNN.conv1", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "advice_models.image_EncoderCNN.conv2", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "advice_models.image_EncoderCNN.conv3", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "advice_models.image_EncoderCNN.fc1", "advice_models.image_EncoderCNN.embed"], "methods", ["None"], ["\n", "", "def", "forward", "(", "self", ",", "images", ")", ":", "\n", "#print(\"images shape: \", images.shape)", "\n", "        ", "conv1", "=", "self", ".", "conv1", "(", "images", ")", "\n", "features1", "=", "self", ".", "max1", "(", "conv1", ")", "\n", "conv2", "=", "self", ".", "conv2", "(", "features1", ")", "\n", "features2", "=", "self", ".", "max2", "(", "conv2", ")", "\n", "conv3", "=", "self", ".", "conv3", "(", "features2", ")", "\n", "features3", "=", "self", ".", "max3", "(", "conv3", ")", "\n", "batch", "=", "features3", ".", "shape", "[", "0", "]", "\n", "#print(\"features3_shape: \",features3.shape)", "\n", "flat_f", "=", "torch", ".", "reshape", "(", "features3", ",", "(", "batch", ",", "-", "1", ")", ")", "\n", "#print(\"flat_f_shape: \",flat_f.shape)", "\n", "fc", "=", "self", ".", "fc1", "(", "flat_f", ")", "\n", "features", "=", "self", ".", "embed", "(", "fc", ")", "\n", "\n", "return", "features", "\n", "\n", "\n", "", "", "class", "WordEmbedding", "(", "nn", ".", "Module", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.WordEmbedding.__init__": [[189, 195], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "ntoken", "=", "ntoken", "\n", "self", ".", "emb_dim", "=", "advice_dim", "\n", "\n", "", "def", "init_embedding", "(", "self", ",", "np_file", ")", ":", "\n", "        ", "weight_init", "=", "torch", ".", "from_numpy", "(", "np", ".", "load", "(", "np_file", ")", ")", "\n", "assert", "weight_init", ".", "shape", "==", "(", "self", ".", "ntoken", ",", "self", ".", "emb_dim", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.WordEmbedding.init_embedding": [[196, 200], ["torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.load"], "methods", ["None"], ["self", ".", "emb", ".", "weight", ".", "data", "[", ":", "self", ".", "ntoken", "]", "=", "weight_init", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "emb", "=", "self", ".", "emb", "(", "x", ")", "\n", "#print(\"emb_shape: \",emb.shape)", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.WordEmbedding.forward": [[201, 210], ["advice_models.WordEmbedding.emb", "advice_models.WordEmbedding.dropout", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape"], "methods", ["None"], ["emb", "=", "self", ".", "dropout", "(", "emb", ")", "\n", "#print(\"emb_drop_shape: \",emb.shape)", "\n", "batch", "=", "emb", ".", "shape", "[", "0", "]", "\n", "emb", "=", "torch", ".", "reshape", "(", "emb", ",", "(", "batch", ",", "-", "1", ",", "self", ".", "emb_dim", ")", ")", "\n", "#print(\"emb_reshape_shape: \",emb.shape)", "\n", "return", "emb", "\n", "\n", "", "", "class", "Advice_Encoder", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "word_embed_size", ",", "hidden_size", ",", "num_layers", ",", "drop_prob", "=", "0.2", ")", ":", "\n", "        ", "super", "(", "Advice_Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.__init__": [[212, 226], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "advice_models.Advice_Encoder.init_weights"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_weights"], ["\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "input_size", "=", "word_embed_size", ",", "hidden_size", "=", "hidden_size", ",", "\n", "num_layers", "=", "num_layers", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "word_embed_size", "=", "word_embed_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "n_layers", "=", "num_layers", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "vocab_size", ")", "\n", "\n", "#self.relu = nn.ReLU()", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "self", ".", "linear", ".", "weight", ".", "data", ".", "normal_", "(", "0.0", ",", "0.02", ")", "\n", "self", ".", "linear", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_weights": [[227, 230], ["advice_models.Advice_Encoder.linear.weight.data.normal_", "advice_models.Advice_Encoder.linear.bias.data.fill_"], "methods", ["None"], ["\n", "\n", "", "def", "init_word_embedding", "(", "self", ",", "weight_init", ")", ":", "\n", "        ", "assert", "weight_init", ".", "shape", "==", "(", "self", ".", "vocab_size", ",", "self", ".", "word_embed_size", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_word_embedding": [[232, 235], ["None"], "methods", ["None"], ["\n", "", "def", "forward", "(", "self", ",", "advice_words", ")", ":", "\n", "        ", "batch", "=", "advice_words", ".", "shape", "[", "0", "]", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "batch", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.forward": [[236, 245], ["advice_models.Advice_Encoder.init_hidden", "advice_models.Advice_Encoder.init_hidden", "advice_models.Advice_Encoder.embedding_layer", "advice_models.Advice_Encoder.lstm", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden"], ["context", "=", "self", ".", "init_hidden", "(", "batch", ")", "\n", "advice_embed", "=", "self", ".", "embedding_layer", "(", "advice_words", ")", "\n", "out", ",", "(", "h", ",", "_", ")", "=", "self", ".", "lstm", "(", "advice_embed", ",", "(", "hidden", ",", "context", ")", ")", "\n", "#print(\"out GRU_shape: \",out.shape)", "\n", "h", "=", "torch", ".", "reshape", "(", "h", ",", "(", "batch", ",", "-", "1", ")", ")", "\n", "return", "out", ",", "h", "\n", "\n", "", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "weight", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "data", "\n", "hidden", "=", "weight", ".", "new", "(", "self", ".", "n_layers", ",", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "zero_", "(", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_hidden": [[246, 250], ["weight.new().zero_", "next", "advice_models.Advice_Encoder.parameters", "weight.new"], "methods", ["None"], ["return", "hidden", "\n", "\n", "\n", "", "", "class", "Sentence_Encoder", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "n_layers", ",", "drop_prob", "=", "0.2", ")", ":", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.__init__": [[253, 261], ["torch.Module.__init__", "torch.GRU", "torch.GRU", "torch.GRU", "torch.GRU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["self", ".", "n_layers", "=", "n_layers", "\n", "\n", "self", ".", "gru", "=", "nn", ".", "GRU", "(", "input_dim", ",", "hidden_dim", ",", "n_layers", ",", "batch_first", "=", "True", ",", "dropout", "=", "drop_prob", ")", "\n", "#self.fc = nn.Linear(hidden_dim, output_dim)", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch", "=", "x", ".", "shape", "[", "0", "]", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "batch", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.forward": [[262, 272], ["advice_models.Sentence_Encoder.init_hidden", "advice_models.Sentence_Encoder.gru", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden"], ["#print(\"hidden initialized_shape: \",hidden.shape)", "\n", "out", ",", "h", "=", "self", ".", "gru", "(", "x", ",", "hidden", ")", "\n", "#print(\"out GRU_shape: \",out.shape)", "\n", "h", "=", "torch", ".", "reshape", "(", "h", ",", "(", "batch", ",", "-", "1", ")", ")", "\n", "#print(\"h_GRU_shape: \",h.shape)", "\n", "#out = self.fc(self.relu(out[:,-1]))", "\n", "return", "out", ",", "h", "\n", "\n", "", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "weight", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "data", "\n", "hidden", "=", "weight", ".", "new", "(", "self", ".", "n_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ".", "zero_", "(", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Sentence_Encoder.init_hidden": [[273, 277], ["weight.new().zero_", "next", "advice_models.Sentence_Encoder.parameters", "weight.new"], "methods", ["None"], ["return", "hidden", "\n", "\n", "", "", "class", "Action_Decoder", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "img_emb_size", ",", "sent_emb_size", ",", "output_dim", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "Action_Decoder", ",", "self", ")", ".", "__init__", "(", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Action_Decoder.__init__": [[279, 288], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__"], ["self", ".", "output", "=", "output_dim", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "out_emb1", "=", "nn", ".", "Linear", "(", "img_emb_size", "+", "sent_emb_size", ",", "512", ")", "\n", "self", ".", "out_emb2", "=", "nn", ".", "Linear", "(", "512", ",", "128", ")", "\n", "self", ".", "out_emb3", "=", "nn", ".", "Linear", "(", "128", ",", "output_dim", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "concat_vec", "=", "torch", ".", "cat", "(", "(", "x", ",", "y", ")", ",", "dim", "=", "1", ")", "\n", "#print(\"concatenated_vector: \",concat_vec)", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Action_Decoder.forward": [[289, 303], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "advice_models.Action_Decoder.out_emb1", "advice_models.Action_Decoder.dropout", "advice_models.Action_Decoder.out_emb2", "advice_models.Action_Decoder.dropout", "advice_models.Action_Decoder.out_emb3", "advice_models.Action_Decoder.relu", "advice_models.Action_Decoder.relu"], "methods", ["None"], ["decoded1", "=", "self", ".", "out_emb1", "(", "concat_vec", ")", "\n", "#print(\"decoded1_vector: \",decoded1)", "\n", "drop_vec1", "=", "self", ".", "dropout", "(", "self", ".", "relu", "(", "decoded1", ")", ")", "\n", "#drop_vec1= self.dropout(decoded1)", "\n", "decoded2", "=", "self", ".", "out_emb2", "(", "drop_vec1", ")", "\n", "drop_vec2", "=", "self", ".", "dropout", "(", "self", ".", "relu", "(", "decoded2", ")", ")", "\n", "#decoded2 = self.out_emb2(drop_vec2)", "\n", "out", "=", "self", ".", "out_emb3", "(", "drop_vec2", ")", "\n", "#print(\"out_shape: \", out.shape)", "\n", "\n", "return", "out", "", "", "", ""]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader.FroggerDataLoader.__init__": [[75, 85], ["None"], "methods", ["None"], ["advice_ids", "=", "create_rationalization_matrix", "(", "advice", ",", "self", ".", "vocab", ")", "\n", "#print(\"advice_testing: \", advice_ids)", "\n", "advice_text", "=", "torch", ".", "tensor", "(", "advice_ids", ")", "\n", "advice_text", "=", "advice_text", ".", "type", "(", "torch", ".", "LongTensor", ")", "\n", "#print(\"advice_text_shape: \",advice_text.shape)", "\n", "advice_text", "=", "torch", ".", "reshape", "(", "advice_text", ",", "(", "-", "1", ",", ")", ")", "\n", "#advice_text = torch.from_numpy(advice_ids)", "\n", "\n", "\n", "#action_arr = torch.zeros(5)", "\n", "#action_arr[action] = 1", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader.FroggerDataLoader.__getitem__": [[87, 113], ["os.path.join", "cv2.imread", "cv2.resize", "numpy.reshape", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "data_loader.create_rationalization_matrix", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.reshape.type", "torch.reshape.type", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.frogger_dataset.FroggerDataset.create_rationalization_matrix"], ["#action_arr = action_arr.type(torch.LongTensor)", "\n", "\n", "return", "advice_text", ",", "image_arr", ",", "action_arr", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "rationalization", ")", "\n", "\n", "", "", "def", "get_loader", "(", "vocab", ",", "advices", ",", "images", ",", "cur_image_dir", ",", "act", ",", "batch_size", ",", "transform", ",", "shuffle", ",", "num_workers", ")", ":", "\n", "    ", "frogger", "=", "FroggerDataLoader", "(", "vocab", ",", "advices", ",", "images", ",", "cur_image_dir", ",", "act", ")", "\n", "\n", "data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", "=", "frogger", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "shuffle", ",", "num_workers", "=", "num_workers", ")", "\n", "return", "data_loader", "\n", "", ""]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader.FroggerDataLoader.__len__": [[114, 116], ["len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader.create_rationalization_matrix": [[41, 71], ["rationalization.replace", "sent.strip.strip", "sent.strip.lower().split", "numpy.zeros", "enumerate", "np.array.append", "sequence_lenght_arr.append", "numpy.array", "sent.strip.lower"], "function", ["None"], ["idx", "+=", "1", "\n", "", "rationalization_matrix", ".", "append", "(", "ration_sent", ")", "\n", "sequence_lenght_arr", ".", "append", "(", "idx", ")", "\n", "\n", "rationalization_matrix", "=", "np", ".", "array", "(", "rationalization_matrix", ")", "\n", "return", "rationalization_matrix", "\n", "\n", "\n", "", "class", "FroggerDataLoader", "(", "data", ".", "Dataset", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "vocab", ",", "rationalizations", ",", "images", ",", "cur_image_dir", ",", "action", ")", ":", "\n", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "rationalization", "=", "rationalizations", "\n", "#print(\"self.advices: \",(self.rationalization))", "\n", "\n", "self", ".", "images", "=", "images", "\n", "#print(\"self.images: \",(self.images))", "\n", "self", ".", "image_dir", "=", "cur_image_dir", "\n", "self", ".", "action", "=", "action", "\n", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "\n", "        ", "image_name", "=", "self", ".", "images", "[", "index", "]", "\n", "action", "=", "self", ".", "action", "[", "index", "]", "\n", "advice", "=", "self", ".", "rationalization", "[", "index", "]", "\n", "#print(\"advice: \",advice, \"image: \",image_name, \"action: \",action)", "\n", "\n", "img_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "image_dir", ",", "image_name", ")", "\n", "img_arr", "=", "cv2", ".", "imread", "(", "img_path", ")", "\n", "resized_img", "=", "cv2", ".", "resize", "(", "img_arr", ",", "(", "224", ",", "224", ")", ")", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader.get_loader": [[117, 122], ["data_loader.FroggerDataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "function", ["None"], []], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader_advice_driven_train.FroggerDataLoader.__init__": [[48, 59], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab", ",", "rationalizations", ",", "images", ",", "cur_image_dir", ",", "action", ")", ":", "\n", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "rationalization", "=", "rationalizations", "\n", "#print(\"self.advices: \",(self.rationalization))", "\n", "self", ".", "height", "=", "100", "\n", "self", ".", "width", "=", "100", "\n", "self", ".", "images", "=", "images", "\n", "#print(\"self.images: \",(self.images))", "\n", "self", ".", "image_dir", "=", "cur_image_dir", "\n", "self", ".", "action", "=", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader_advice_driven_train.FroggerDataLoader.__getitem__": [[61, 86], ["os.path.join", "cv2.imread", "cv2.resize", "numpy.reshape", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "data_loader_advice_driven_train.create_rationalization_matrix", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.reshape.type", "torch.reshape.type", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.frogger_dataset.FroggerDataset.create_rationalization_matrix"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "\n", "        ", "image_name", "=", "self", ".", "images", "[", "index", "]", "\n", "action", "=", "self", ".", "action", "[", "index", "]", "\n", "advice", "=", "self", ".", "rationalization", "[", "index", "]", "\n", "\n", "\n", "img_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "image_dir", ",", "image_name", ")", "\n", "img_arr", "=", "cv2", ".", "imread", "(", "img_path", ")", "\n", "resized_img", "=", "cv2", ".", "resize", "(", "img_arr", ",", "(", "self", ".", "height", ",", "self", ".", "width", ")", ")", "\n", "x", "=", "np", ".", "reshape", "(", "resized_img", ",", "(", "3", ",", "self", ".", "width", ",", "self", ".", "width", ")", ")", "\n", "image_arr", "=", "torch", ".", "Tensor", "(", "x", ")", "\n", "\n", "#advice_ids = create_rationalization_matrix(advice, self.vocab)", "\n", "advice_ids", "=", "create_rationalization_matrix", "(", "advice", ",", "self", ".", "vocab", ")", "\n", "\n", "advice_text", "=", "torch", ".", "tensor", "(", "advice_ids", ")", "\n", "advice_text", "=", "advice_text", ".", "type", "(", "torch", ".", "LongTensor", ")", "\n", "\n", "advice_text", "=", "torch", ".", "reshape", "(", "advice_text", ",", "(", "-", "1", ",", ")", ")", "\n", "\n", "action_arr", "=", "torch", ".", "tensor", "(", "action", ")", "\n", "#action_arr = action_arr.type(torch.LongTensor)", "\n", "\n", "return", "advice_text", ",", "image_arr", ",", "action_arr", ",", "image_name", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader_advice_driven_train.FroggerDataLoader.__len__": [[87, 89], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "rationalization", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader_advice_driven_train.create_rationalization_matrix": [[15, 45], ["rationalization.replace", "sent.strip.strip", "sent.strip.lower().split", "numpy.zeros", "enumerate", "np.array.append", "sequence_lenght_arr.append", "numpy.array", "sent.strip.lower"], "function", ["None"], ["def", "create_rationalization_matrix", "(", "rationalization", ",", "vocab", ")", ":", "\n", "    ", "max_rationalization_len", "=", "17", "\n", "rationalization_matrix", "=", "[", "]", "\n", "sequence_lenght_arr", "=", "[", "]", "\n", "#sent = rationalization", "\n", "sent", "=", "rationalization", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", "\n", "sent", "=", "sent", ".", "strip", "(", ")", "\n", "words", "=", "sent", ".", "lower", "(", ")", ".", "split", "(", "' '", ")", "\n", "\n", "ration_sent", "=", "np", ".", "zeros", "(", "[", "max_rationalization_len", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "ration_sent", "[", "0", "]", "=", "1", "\n", "idx", "=", "1", "\n", "\n", "for", "k", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "if", "idx", "==", "max_rationalization_len", ":", "\n", "            ", "break", "\n", "", "if", "word", "in", "vocab", ".", "word2idx", ":", "\n", "            ", "ration_sent", "[", "idx", "]", "=", "vocab", ".", "word2idx", "[", "word", "]", "\n", "#else:", "\n", "#ration_sent[idx] = 3", "\n", "", "idx", "+=", "1", "\n", "\n", "", "if", "idx", "<", "max_rationalization_len", ":", "\n", "        ", "ration_sent", "[", "idx", "]", "=", "2", "\n", "idx", "+=", "1", "\n", "", "rationalization_matrix", ".", "append", "(", "ration_sent", ")", "\n", "sequence_lenght_arr", ".", "append", "(", "idx", ")", "\n", "\n", "rationalization_matrix", "=", "np", ".", "array", "(", "rationalization_matrix", ")", "\n", "return", "rationalization_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader_advice_driven_train.get_loader": [[90, 95], ["data_loader_advice_driven_train.FroggerDataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "function", ["None"], ["", "", "def", "get_loader", "(", "vocab", ",", "advices", ",", "images", ",", "cur_image_dir", ",", "act", ",", "batch_size", ",", "transform", ",", "shuffle", ",", "num_workers", ")", ":", "\n", "    ", "frogger", "=", "FroggerDataLoader", "(", "vocab", ",", "advices", ",", "images", ",", "cur_image_dir", ",", "act", ")", "\n", "\n", "data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", "=", "frogger", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "shuffle", ",", "num_workers", "=", "num_workers", ")", "\n", "return", "data_loader", "\n", "", ""]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.numericalSort": [[29, 33], ["numbers.split", "map"], "function", ["None"], ["def", "numericalSort", "(", "value", ")", ":", "\n", "    ", "parts", "=", "numbers", ".", "split", "(", "value", ")", "\n", "parts", "[", "1", ":", ":", "2", "]", "=", "map", "(", "int", ",", "parts", "[", "1", ":", ":", "2", "]", ")", "\n", "return", "parts", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.calculate_stat": [[42, 74], ["dict", "len", "range", "action[].item", "action[].item", "action[].item", "action[].item", "action[].item", "action[].item"], "function", ["None"], ["def", "calculate_stat", "(", "action", ")", ":", "\n", "    ", "action_stat", "=", "dict", "(", ")", "\n", "act_down", "=", "0", "\n", "act_left", "=", "0", "\n", "act_right", "=", "0", "\n", "act_up", "=", "0", "\n", "act_wait", "=", "0", "\n", "\n", "#i = 0", "\n", "length", "=", "len", "(", "action", ")", "\n", "#action = action.unsqueeze(0)", "\n", "\n", "for", "i", "in", "range", "(", "length", ")", ":", "\n", "        ", "j", "=", "action", "[", "i", "]", ".", "item", "(", ")", "\n", "if", "(", "action", "[", "i", "]", ".", "item", "(", ")", "==", "0", ")", ":", "\n", "            ", "act_down", "+=", "1", "\n", "", "elif", "(", "action", "[", "i", "]", ".", "item", "(", ")", "==", "1", ")", ":", "\n", "            ", "act_left", "+=", "1", "\n", "", "elif", "(", "action", "[", "i", "]", ".", "item", "(", ")", "==", "2", ")", ":", "\n", "            ", "act_right", "+=", "1", "\n", "", "elif", "(", "action", "[", "i", "]", ".", "item", "(", ")", "==", "3", ")", ":", "\n", "            ", "act_up", "+=", "1", "\n", "", "elif", "(", "action", "[", "i", "]", ".", "item", "(", ")", "==", "4", ")", ":", "\n", "            ", "act_wait", "+=", "1", "\n", "\n", "", "", "action_stat", "[", "'Down'", "]", "=", "act_down", "\n", "action_stat", "[", "'Left'", "]", "=", "act_left", "\n", "action_stat", "[", "'Right'", "]", "=", "act_right", "\n", "action_stat", "[", "'Up'", "]", "=", "act_up", "\n", "action_stat", "[", "'Wait'", "]", "=", "act_wait", "\n", "\n", "return", "action_stat", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.weight_gloVe": [[77, 96], ["pickle.load", "pickle.load", "len", "numpy.zeros", "range", "print", "open", "numpy.random.normal"], "function", ["None"], ["", "def", "weight_gloVe", "(", "target_vocab", ")", ":", "\n", "    ", "glove", "=", "pickle", ".", "load", "(", "open", "(", "f'glove/6B_300_words_emb.pkl'", ",", "'rb'", ")", ")", "\n", "\n", "matrix_len", "=", "len", "(", "target_vocab", ".", "word2idx", ")", "\n", "weights_matrix", "=", "np", ".", "zeros", "(", "(", "matrix_len", ",", "300", ")", ")", "\n", "words_found", "=", "0", "\n", "emb_dim", "=", "300", "\n", "\n", "for", "i", "in", "range", "(", "matrix_len", ")", ":", "\n", "        ", "word", "=", "target_vocab", ".", "idx2word", "[", "i", "]", "\n", "try", ":", "\n", "            ", "weights_matrix", "[", "i", "]", "=", "glove", "[", "word", "]", "\n", "words_found", "+=", "1", "\n", "", "except", "KeyError", ":", "\n", "            ", "weights_matrix", "[", "i", "]", "=", "np", ".", "random", ".", "normal", "(", "scale", "=", "0.6", ",", "size", "=", "(", "emb_dim", ",", ")", ")", "\n", "\n", "", "", "print", "(", "\"words_found: \"", ",", "words_found", ")", "\n", "\n", "return", "weights_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.load_data_preprocess": [[97, 220], ["xlrd.open_workbook", "set", "xlrd.open_workbook.sheets", "print", "int", "print", "print", "slice", "print", "slice", "print", "open", "pickle.load", "pickle.load", "nltk.corpus.stopwords.words", "collections.Counter", "range", "open", "pickle.dump", "pickle.dump", "open", "pickle.dump", "pickle.dump", "math.floor", "len", "len", "len", "len", "len", "numpy.array", "len", "sheet.cell", "good_ids.append", "re.sub.lower", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "nltk.tokenize.word_tokenize", "re.sub", "re.sub.split", "actions.append", "len", "good_rationalizations.append", "lengths.append", "new_id_list.append", "enumerate", "len", "lengths_unk.append", "rationalizations.append", "rationalization_tokens.append", "len", "sheet.cell", "sheet.cell", "_action.lower", "print", "word_list.append", "new_id_list.append"], "function", ["None"], ["", "def", "load_data_preprocess", "(", "data_file", ",", "vocab_path", ")", ":", "\n", "#data_file = data_file", "\n", "#vocab = self.vocab", "\n", "    ", "wb", "=", "open_workbook", "(", "data_file", ")", "\n", "with", "open", "(", "vocab_path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "vocab", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "stop_words", "=", "set", "(", "stopwords", ".", "words", "(", "'english'", ")", ")", "\n", "\n", "#read the rationalizations from the excel file and create a list of training/testing rationalizations. ", "\n", "for", "sheet", "in", "wb", ".", "sheets", "(", ")", ":", "\n", "        ", "number_of_rows", "=", "sheet", ".", "nrows", "\n", "number_of_columns", "=", "sheet", ".", "ncols", "\n", "rationalizations", "=", "[", "]", "\n", "rationalization_tokens", "=", "[", "]", "\n", "items", "=", "[", "]", "\n", "rows", "=", "[", "]", "\n", "lengths", "=", "[", "]", "\n", "lengths_unk", "=", "[", "]", "\n", "\n", "max_length", "=", "0", "\n", "\n", "bad_worker_ids", "=", "[", "'A2CNSIECB9UP05'", ",", "'A23782O23HSPLA'", ",", "'A2F9ZBSR6AXXND'", ",", "'A3GI86L18Z71XY'", ",", "'AIXTI8PKSX1D2'", ",", "'A2QWHXMFQI18GQ'", ",", "'A3SB7QYI84HYJT'", ",", "\n", "'A2Q2A7AB6MMFLI'", ",", "'A2P1KI42CJVNIA'", ",", "'A1IJXPKZTJV809'", ",", "'A2WZ0RZMKQ2WGJ'", ",", "'A3EKETMVGU2PM9'", ",", "'A1OCEC1TBE3CWA'", ",", "'AE1RYK54MH11G'", ",", "'A2ADEPVGNNXNPA'", ",", "\n", "'A15QGLWS8CNJFU'", ",", "'A18O3DEA5Z4MJD'", ",", "'AAAL4RENVAPML'", ",", "'A3TZBZ92CQKQLG'", ",", "'ABO9F0JD9NN54'", ",", "'A8F6JFG0WSELT'", ",", "'ARN9ET3E608LJ'", ",", "'A2TCYNRAZWK8CC'", ",", "\n", "'A32BK0E1IPDUAF'", ",", "'ANNV3E6CIVCW4'", ",", "'AXMQBHHU22TSP'", ",", "'AKATSYE8XLYNL'", ",", "'A355PGLV2ID2SX'", ",", "'A55CXM7QR7R0N'", ",", "'A111ZFNLXK1TCO'", "]", "\n", "\n", "good_ids", "=", "[", "]", "\n", "good_rationalizations", "=", "[", "]", "\n", "actions", "=", "[", "]", "\n", "counter", "=", "Counter", "(", ")", "\n", "for", "row", "in", "range", "(", "1", ",", "number_of_rows", ")", ":", "\n", "            ", "values", "=", "[", "]", "\n", "word_list", "=", "[", "]", "\n", "worker_id", "=", "sheet", ".", "cell", "(", "row", ",", "0", ")", ".", "value", "\n", "if", "worker_id", "not", "in", "bad_worker_ids", ":", "\n", "                ", "good_ids", ".", "append", "(", "row", "-", "1", ")", "\n", "line", "=", "sheet", ".", "cell", "(", "row", ",", "4", ")", ".", "value", "\n", "line", "=", "line", ".", "lower", "(", ")", "\n", "line", "=", "line", ".", "replace", "(", "'\\''", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'.'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "','", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'``'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'\"'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'\\\\n'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "':'", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "'('", ",", "''", ")", "\n", "line", "=", "line", ".", "replace", "(", "')'", ",", "''", ")", "\n", "#line = self.preprocessing(line)", "\n", "tokens", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "line", ")", "\n", "sentence", "=", "\"\"", "\n", "#word_token= [w for w in tokens if not w in stop_words]", "\n", "for", "w", "in", "tokens", ":", "\n", "                    ", "if", "not", "w", "in", "stop_words", ":", "\n", "                        ", "sentence", "=", "sentence", "+", "\" \"", "+", "w", "\n", "word_list", ".", "append", "(", "w", ")", "\n", "\n", "#good_rationalizations.append(sentence)", "\n", "#line = re.sub('[^a-z\\ ]+', \" \", line)", "\n", "", "", "line", "=", "re", ".", "sub", "(", "r'[^\\w\\s]'", ",", "' '", ",", "line", ")", "\n", "words", "=", "line", ".", "split", "(", ")", "\n", "_action", "=", "sheet", ".", "cell", "(", "row", ",", "2", ")", ".", "value", "\n", "actions", ".", "append", "(", "actions_map", "[", "_action", "]", ")", "\n", "\n", "length", "=", "len", "(", "word_list", ")", "\n", "if", "length", "==", "0", ":", "\n", "#action = sheet.cell(row,2).value                        ", "\n", "                    ", "action", "=", "_action", ".", "lower", "(", ")", "\n", "sentence", "=", "sentence", "+", "\" \"", "+", "action", "\n", "#word_token.append(action)", "\n", "print", "(", "\"short sentence: \"", ",", "sentence", ")", "\n", "", "good_rationalizations", ".", "append", "(", "sentence", ")", "\n", "lengths", ".", "append", "(", "length", ")", "\n", "if", "length", ">", "max_length", ":", "\n", "                    ", "max_length", "=", "length", "\n", "", "new_id_list", "=", "[", "]", "\n", "new_id_list", ".", "append", "(", "vocab", ".", "word2idx", "[", "'<start>'", "]", ")", "\n", "for", "index", ",", "word", "in", "enumerate", "(", "word_list", ")", ":", "\n", "                    ", "if", "word", "in", "vocab", ".", "word2idx", ":", "\n", "                        ", "id_word", "=", "vocab", ".", "word2idx", "[", "word", "]", "\n", "new_id_list", ".", "append", "(", "id_word", ")", "\n", "\n", "#else:", "\n", "#word_token[index] = vocab.word2idx['<unk>']", "\n", "", "", "length_unk", "=", "len", "(", "new_id_list", ")", "\n", "lengths_unk", ".", "append", "(", "length_unk", ")", "\n", "rationalizations", ".", "append", "(", "words", ")", "\n", "rationalization_tokens", ".", "append", "(", "new_id_list", ")", "\n", "", "", "rationalizations", "=", "[", "np", ".", "array", "(", "xi", ")", "for", "xi", "in", "rationalizations", "]", "\n", "#rationalization_tokens = [np.array(xi) for xi in rationalization_tokens]", "\n", "\n", "#good_ids_split = int(floor((90.0/100)*len(good_ids)))", "\n", "", "with", "open", "(", "\"sentence_len.pkl\"", ",", "\"wb\"", ")", "as", "output_file", ":", "\n", "        ", "pickle", ".", "dump", "(", "lengths", ",", "output_file", ")", "\n", "", "with", "open", "(", "\"sent_len_no_unk.pkl\"", ",", "\"wb\"", ")", "as", "output_file", ":", "\n", "        ", "pickle", ".", "dump", "(", "lengths_unk", ",", "output_file", ")", "\n", "\n", "", "print", "(", "\"max_length: \"", ",", "max_length", ")", "\n", "split", "=", "int", "(", "floor", "(", "(", "90.0", "/", "100", ")", "*", "len", "(", "rationalizations", ")", ")", ")", "\n", "print", "(", "\"good_rationalizations: \"", ",", "len", "(", "good_rationalizations", ")", ")", "\n", "#print(\"good_rationalizations: \",good_rationalizations)\t\t", "\n", "print", "(", "\"rationalizations_tokens: \"", ",", "len", "(", "rationalization_tokens", ")", ")", "\n", "#print(\"rationalizations: \",rationalizations)        ", "\n", "tr", "=", "slice", "(", "0", ",", "split", ")", "\n", "tr_good_ids", "=", "good_ids", "[", "tr", "]", "\n", "print", "(", "\"tr_good_ids: \"", ",", "len", "(", "tr_good_ids", ")", ")", "\n", "\n", "tr_indices", "=", "[", "0", ",", "split", "-", "1", "]", "\n", "#print(\"tr_contents: \",tr)", "\n", "te_indices", "=", "[", "split", ",", "len", "(", "rationalizations", ")", "-", "1", "]", "\n", "\n", "te", "=", "slice", "(", "split", ",", "len", "(", "rationalizations", ")", ")", "\n", "te_good_ids", "=", "good_ids", "[", "te", "]", "\n", "print", "(", "\"te_good_ids: \"", ",", "len", "(", "te_good_ids", ")", ")", "\n", "\n", "training_rationalizations", "=", "good_rationalizations", "[", "tr", "]", "\n", "#print(\"train rationalizations_tokens: \",len(training_rationalizations))", "\n", "testing_rationalizations", "=", "good_rationalizations", "[", "te", "]", "\n", "training_actions", "=", "actions", "[", "tr", "]", "\n", "testing_actions", "=", "actions", "[", "te", "]", "\n", "#print(\"test rationalizations_tokens: \",len(testing_rationalizations))", "\n", "\n", "return", "tr_good_ids", ",", "te_good_ids", ",", "tr_indices", ",", "te_indices", ",", "training_rationalizations", ",", "testing_rationalizations", ",", "training_actions", ",", "testing_actions", ",", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.load_data": [[221, 293], ["xlrd.open_workbook", "xlrd.open_workbook.sheets", "int", "slice", "print", "slice", "print", "open", "pickle.load", "pickle.load", "collections.Counter", "range", "math.floor", "type", "len", "len", "numpy.array", "len", "sheet.cell", "good_ids.append", "nltk.tokenize.word_tokenize", "actions.append", "re.sub.lower", "good_rationalizations.append", "re.sub", "re.sub.split", "len", "lengths.append", "enumerate", "rationalizations.append", "len", "sheet.cell", "re.sub.lower", "sheet.cell"], "function", ["None"], ["", "def", "load_data", "(", "data_file", ",", "vocab_path", ")", ":", "\n", "    ", "wb", "=", "open_workbook", "(", "data_file", ")", "\n", "with", "open", "(", "vocab_path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "vocab", "=", "pickle", ".", "load", "(", "f", ")", "\n", "#vocab = vocab", "\n", "#read the rationalizations from the excel file and create a list of training/testing rationalizations. ", "\n", "", "for", "sheet", "in", "wb", ".", "sheets", "(", ")", ":", "\n", "        ", "number_of_rows", "=", "sheet", ".", "nrows", "\n", "number_of_columns", "=", "sheet", ".", "ncols", "\n", "rationalizations", "=", "[", "]", "\n", "items", "=", "[", "]", "\n", "rows", "=", "[", "]", "\n", "lengths", "=", "[", "]", "\n", "max_length", "=", "0", "\n", "\n", "bad_worker_ids", "=", "[", "'A2CNSIECB9UP05'", ",", "'A23782O23HSPLA'", ",", "'A2F9ZBSR6AXXND'", ",", "'A3GI86L18Z71XY'", ",", "'AIXTI8PKSX1D2'", ",", "'A2QWHXMFQI18GQ'", ",", "'A3SB7QYI84HYJT'", ",", "\n", "'A2Q2A7AB6MMFLI'", ",", "'A2P1KI42CJVNIA'", ",", "'A1IJXPKZTJV809'", ",", "'A2WZ0RZMKQ2WGJ'", ",", "'A3EKETMVGU2PM9'", ",", "'A1OCEC1TBE3CWA'", ",", "'AE1RYK54MH11G'", ",", "'A2ADEPVGNNXNPA'", ",", "\n", "'A15QGLWS8CNJFU'", ",", "'A18O3DEA5Z4MJD'", ",", "'AAAL4RENVAPML'", ",", "'A3TZBZ92CQKQLG'", ",", "'ABO9F0JD9NN54'", ",", "'A8F6JFG0WSELT'", ",", "'ARN9ET3E608LJ'", ",", "'A2TCYNRAZWK8CC'", ",", "\n", "'A32BK0E1IPDUAF'", ",", "'ANNV3E6CIVCW4'", ",", "'AXMQBHHU22TSP'", ",", "'AKATSYE8XLYNL'", ",", "'A355PGLV2ID2SX'", ",", "'A55CXM7QR7R0N'", ",", "'A111ZFNLXK1TCO'", "]", "\n", "\n", "good_ids", "=", "[", "]", "\n", "good_rationalizations", "=", "[", "]", "\n", "actions", "=", "[", "]", "\n", "counter", "=", "Counter", "(", ")", "\n", "for", "row", "in", "range", "(", "1", ",", "number_of_rows", ")", ":", "\n", "            ", "values", "=", "[", "]", "\n", "worker_id", "=", "sheet", ".", "cell", "(", "row", ",", "0", ")", ".", "value", "\n", "if", "worker_id", "not", "in", "bad_worker_ids", ":", "\n", "                ", "good_ids", ".", "append", "(", "row", "-", "1", ")", "\n", "line", "=", "sheet", ".", "cell", "(", "row", ",", "4", ")", ".", "value", "\n", "tokens", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "line", ".", "lower", "(", ")", ")", "\n", "# if tokens!=[]:", "\n", "_action", "=", "sheet", ".", "cell", "(", "row", ",", "2", ")", ".", "value", "\n", "actions", ".", "append", "(", "actions_map", "[", "_action", "]", ")", "\n", "line", "=", "line", ".", "lower", "(", ")", "\n", "good_rationalizations", ".", "append", "(", "line", ")", "\n", "#line = re.sub('[^a-z\\ ]+', \" \", line)", "\n", "line", "=", "re", ".", "sub", "(", "r'[^\\w\\s]'", ",", "' '", ",", "line", ")", "\n", "words", "=", "line", ".", "split", "(", ")", "\n", "length", "=", "len", "(", "tokens", ")", "\n", "lengths", ".", "append", "(", "length", ")", "\n", "if", "length", ">", "max_length", ":", "\n", "                    ", "max_length", "=", "length", "\n", "", "for", "index", ",", "word", "in", "enumerate", "(", "tokens", ")", ":", "\n", "                    ", "if", "word", "in", "vocab", ".", "word2idx", ":", "\n", "                        ", "tokens", "[", "index", "]", "=", "vocab", ".", "word2idx", "[", "word", "]", "\n", "", "else", ":", "\n", "                        ", "tokens", "[", "index", "]", "=", "vocab", ".", "word2idx", "[", "'<unk>'", "]", "\n", "\n", "", "", "rationalizations", ".", "append", "(", "words", ")", "\n", "", "", "rationalizations", "=", "[", "np", ".", "array", "(", "xi", ")", "for", "xi", "in", "rationalizations", "]", "\n", "\n", "", "split", "=", "int", "(", "floor", "(", "(", "90.0", "/", "100", ")", "*", "len", "(", "rationalizations", ")", ")", ")", "\n", "\n", "tr", "=", "slice", "(", "0", ",", "split", ")", "\n", "tr_good_ids", "=", "good_ids", "[", "tr", "]", "\n", "print", "(", "\"tr_good_ids: \"", ",", "type", "(", "tr_good_ids", ")", ")", "\n", "tr_indices", "=", "[", "0", ",", "split", "-", "1", "]", "\n", "te_indices", "=", "[", "split", ",", "len", "(", "rationalizations", ")", "-", "1", "]", "\n", "te", "=", "slice", "(", "split", ",", "len", "(", "rationalizations", ")", ")", "\n", "te_good_ids", "=", "good_ids", "[", "te", "]", "\n", "print", "(", "\"te_good_ids: \"", ",", "len", "(", "te_good_ids", ")", ")", "\n", "training_rationalizations", "=", "good_rationalizations", "[", "tr", "]", "\n", "testing_rationalizations", "=", "good_rationalizations", "[", "te", "]", "\n", "training_actions", "=", "actions", "[", "tr", "]", "\n", "testing_actions", "=", "actions", "[", "te", "]", "\n", "\n", "training_rationalizations_text", "=", "good_rationalizations", "[", "tr", "]", "\n", "testing_rationalizations_text", "=", "good_rationalizations", "[", "te", "]", "\n", "\n", "\n", "return", "tr_good_ids", ",", "te_good_ids", ",", "tr_indices", ",", "te_indices", ",", "training_rationalizations", ",", "testing_rationalizations", ",", "training_actions", ",", "testing_actions", ",", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.load_images": [[295, 319], ["os.listdir", "sorted", "len", "os.listdir", "sorted", "len", "print", "enumerate", "type", "cur_training_images.append", "next_training_images.append", "cur_test_images.append", "next_test_images.append"], "function", ["None"], ["", "def", "load_images", "(", "current_image_dir", ",", "next_image_dir", ",", "tr_good_ids", ",", "tr_indices", ")", ":", "\n", "    ", "current_images", "=", "os", ".", "listdir", "(", "current_image_dir", ")", "\n", "current_images", "=", "sorted", "(", "current_images", ",", "key", "=", "numericalSort", ")", "\n", "num_images", "=", "len", "(", "current_images", ")", "\n", "#print(\"num_next_images: \",num_images)", "\n", "next_images", "=", "os", ".", "listdir", "(", "next_image_dir", ")", "\n", "next_images", "=", "sorted", "(", "next_images", ",", "key", "=", "numericalSort", ")", "\n", "num_next_images", "=", "len", "(", "next_images", ")", "\n", "print", "(", "\"_next_images: \"", ",", "type", "(", "next_images", ")", ")", "\n", "\n", "cur_training_images", "=", "[", "]", "\n", "next_training_images", "=", "[", "]", "\n", "cur_test_images", "=", "[", "]", "\n", "next_test_images", "=", "[", "]", "\n", "for", "i", ",", "file", "in", "enumerate", "(", "current_images", ")", ":", "\n", "        ", "if", "i", "in", "tr_good_ids", ":", "\n", "            ", "cur_training_images", ".", "append", "(", "file", ")", "\n", "next_training_images", ".", "append", "(", "next_images", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "            ", "cur_test_images", ".", "append", "(", "file", ")", "\n", "next_test_images", ".", "append", "(", "next_images", "[", "i", "]", ")", "\n", "\n", "\n", "", "", "return", "cur_training_images", ",", "next_training_images", ",", "cur_test_images", ",", "next_test_images", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.train": [[339, 484], ["torchvision.transforms.Compose", "torchvision.transforms.Compose", "advice_driven_agent.load_data_preprocess", "advice_driven_agent.load_images", "data_loader_advice_driven_train.get_loader", "data_loader_advice_driven_train.get_loader", "torch.device", "torch.device", "torch.device", "torch.device", "len", "advice_driven_agent.weight_gloVe", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "advice_models.EncoderCNN().to", "advice_models.Advice_Encoder().to", "Advice_Encoder().to.init_word_embedding", "advice_models.Action_Decoder().to", "torch.CrossEntropyLoss", "torch.optim.Adamax", "torch.optim.Adamax", "torch.optim.Adamax", "torch.optim.Adamax", "len", "len", "open", "range", "open.close", "list", "print", "EncoderCNN().to.train", "Advice_Encoder().to.train", "Action_Decoder().to.train", "enumerate", "print", "train_losses.append", "val_losses.append", "open.write", "torchvision.transforms.RandomCrop", "torchvision.transforms.RandomHorizontalFlip", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.Resize", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "advice_models.EncoderCNN", "advice_models.Advice_Encoder", "advice_models.Action_Decoder", "list", "list", "Action_Decoder().to.parameters", "img.to.to", "adv.to.to", "act.to.to", "EncoderCNN().to.", "Advice_Encoder().to.", "Action_Decoder().to.", "nn.CrossEntropyLoss.", "torch.optim.Adamax.zero_grad", "criterion.backward", "torch.optim.Adamax.step", "criterion.item", "EncoderCNN().to.eval", "Advice_Encoder().to.eval", "Action_Decoder().to.eval", "print", "print", "print", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "EncoderCNN().to.parameters", "Advice_Encoder().to.parameters", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "print", "open.write", "EncoderCNN().to.state_dict", "os.path.join", "Advice_Encoder().to.state_dict", "os.path.join", "Action_Decoder().to.state_dict", "os.path.join", "img.to.to", "adv.to.to", "act.to.to", "EncoderCNN().to.", "Advice_Encoder().to.", "Action_Decoder().to.", "nn.CrossEntropyLoss.", "act.to.size", "criterion.item", "torch.max", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.load_data_preprocess", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.frogger_dataset.FroggerDataset.load_images", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader_advice_driven_train.get_loader", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader_advice_driven_train.get_loader", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_gen.weight_gloVe", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_word_embedding", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.close", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.train", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.train", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.train"], ["def", "train", "(", "cur_image_dir", ",", "next_image_dir", ")", ":", "\n", "\n", "    ", "train_transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomCrop", "(", "image_size", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "(", "0.485", ",", "0.456", ",", "0.406", ")", ",", "\n", "(", "0.229", ",", "0.224", ",", "0.225", ")", ")", "]", ")", "\n", "\n", "val_transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Resize", "(", "image_size", ",", "interpolation", "=", "Image", ".", "LANCZOS", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "(", "0.485", ",", "0.456", ",", "0.406", ")", ",", "\n", "(", "0.229", ",", "0.224", ",", "0.225", ")", ")", "]", ")", "\n", "\n", "tr_good_ids", ",", "te_good_ids", ",", "training_indices", ",", "testing_indices", ",", "training_rationalizations", ",", "testing_rationalizations", ",", "trn_act", ",", "tst_act", ",", "vocab", "=", "load_data_preprocess", "(", "\"Turk_Master_File_sorted.xlsx\"", ",", "'data/vocab_frogger_preprocessed.pkl'", ")", "\n", "cur_training_images", ",", "next_training_images", ",", "cur_test_images", ",", "next_test_images", "=", "load_images", "(", "current_image_dir", ",", "next_image_dir", ",", "tr_good_ids", ",", "training_indices", ")", "\n", "#print(\"training_rationalizations: \", training_rationalizations)", "\n", "\n", "train_data_loader", "=", "get_loader", "(", "vocab", ",", "training_rationalizations", ",", "cur_training_images", ",", "\n", "current_image_dir", ",", "trn_act", ",", "batch_size", ",", "train_transform", ",", "\n", "shuffle", "=", "True", ",", "num_workers", "=", "0", ")", "\n", "\n", "\n", "\n", "val_data_loader", "=", "get_loader", "(", "vocab", ",", "testing_rationalizations", ",", "cur_test_images", ",", "\n", "current_image_dir", ",", "tst_act", ",", "batch_size", ",", "val_transform", ",", "\n", "shuffle", "=", "True", ",", "num_workers", "=", "0", ")", "\n", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "vocab_size", "=", "len", "(", "train_data_loader", ".", "dataset", ".", "vocab", ")", "\n", "weights_", "=", "weight_gloVe", "(", "vocab", ")", "\n", "weights_", "=", "torch", ".", "from_numpy", "(", "weights_", ")", ".", "float", "(", ")", "\n", "vocab_size", ",", "word_embedding_size", "=", "weights_", ".", "shape", "\n", "image_encoder", "=", "EncoderCNN", "(", "image_embedding_size", ")", ".", "to", "(", "device", ")", "\n", "#word_emb = WordEmbedding(vocab_size,embedding_dim, 0.0)", "\n", "#word_emb.init_embedding(weights_)", "\n", "#advice_encoder = Sentence_Encoder(embedding_dim,dim_hidden,1,0.0)", "\n", "advice_encoder", "=", "Advice_Encoder", "(", "vocab_size", ",", "word_embedding_size", ",", "hidden_size", ",", "1", ",", "0.0", ")", ".", "to", "(", "device", ")", "\n", "advice_encoder", ".", "init_word_embedding", "(", "weights_", ")", "\n", "decoded_action", "=", "Action_Decoder", "(", "image_embedding_size", ",", "sentence_embedding_size", ",", "num_output", ",", "0.2", ")", ".", "to", "(", "device", ")", "\n", "\n", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "params", "=", "list", "(", "image_encoder", ".", "parameters", "(", ")", ")", "+", "list", "(", "advice_encoder", ".", "parameters", "(", ")", ")", "+", "list", "(", "decoded_action", ".", "parameters", "(", ")", ")", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adamax", "(", "params", ",", "lr", "=", "0.001", ",", "weight_decay", "=", "1e-5", ")", "\n", "\n", "train_step", "=", "len", "(", "train_data_loader", ")", "\n", "val_step", "=", "len", "(", "val_data_loader", ")", "\n", "\n", "train_losses", "=", "[", "]", "\n", "val_losses", "=", "[", "]", "\n", "\n", "file1", "=", "open", "(", "\"models/advice_driven_agent_loss_100.txt\"", ",", "\"a+\"", ")", "\n", "\n", "#max_epoch = 2", "\n", "for", "epoch", "in", "range", "(", "0", ",", "max_epoch", ")", ":", "\n", "        ", "print", "(", "\"Epoch\"", ",", "epoch", ",", "\"is starting....\"", ")", "\n", "train_loss", "=", "0.0", "\n", "\n", "image_encoder", ".", "train", "(", ")", "\n", "#word_emb.train()", "\n", "advice_encoder", ".", "train", "(", ")", "\n", "decoded_action", ".", "train", "(", ")", "\n", "\n", "for", "i", ",", "(", "adv", ",", "img", ",", "act", ",", "img_name", ")", "in", "enumerate", "(", "train_data_loader", ")", ":", "\n", "            ", "img", "=", "img", ".", "to", "(", "device", ")", "\n", "adv", "=", "adv", ".", "to", "(", "device", ")", "\n", "act", "=", "act", ".", "to", "(", "device", ")", "\n", "encoded_image", "=", "image_encoder", "(", "img", ")", "\n", "embedded_adv", ",", "hidden", "=", "advice_encoder", "(", "adv", ")", "\n", "predicted_action", "=", "decoded_action", "(", "encoded_image", ",", "hidden", ")", "\n", "\n", "loss", "=", "criterion", "(", "predicted_action", ",", "act", ")", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "train_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "# Testing the models    ", "\n", "", "if", "(", "epoch", "%", "5", "==", "0", ")", ":", "\n", "            ", "image_encoder", ".", "eval", "(", ")", "\n", "advice_encoder", ".", "eval", "(", ")", "\n", "decoded_action", ".", "eval", "(", ")", "\n", "\n", "total_val_loss", "=", "0.0", "\n", "true_action_score", "=", "0", "\n", "pred_act_score", "=", "0", "\n", "true_act_list", "=", "{", "}", "\n", "predict_act_list", "=", "{", "}", "\n", "\n", "print", "(", "\"Validation process is starting....\"", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "for", "j", ",", "(", "adv", ",", "img", ",", "act", ",", "img_name", ")", "in", "enumerate", "(", "val_data_loader", ")", ":", "\n", "                    ", "img", "=", "img", ".", "to", "(", "device", ")", "\n", "adv", "=", "adv", ".", "to", "(", "device", ")", "\n", "act", "=", "act", ".", "to", "(", "device", ")", "\n", "#true_act_stat = calculate_stat(act)", "\n", "encoded_image", "=", "image_encoder", "(", "img", ")", "\n", "embedded_adv", ",", "hidden", "=", "advice_encoder", "(", "adv", ")", "\n", "predicted_action", "=", "decoded_action", "(", "encoded_image", ",", "hidden", ")", "\n", "#true_act_list.append(act)", "\n", "\n", "val_loss", "=", "criterion", "(", "predicted_action", ",", "act", ")", "\n", "#print(\"real action: \",act)", "\n", "true_action_score", "+=", "act", ".", "size", "(", "0", ")", "\n", "pred_act", "=", "torch", ".", "max", "(", "predicted_action", ",", "1", ")", "[", "1", "]", ".", "data", "\n", "#predict_act_list.append(pred_act)", "\n", "#predicted_act_stat = calculate_stat(pred_act)", "\n", "\n", "pred_act_score", "+=", "(", "pred_act", "==", "act", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "val_accuracy", "=", "100", "*", "pred_act_score", "/", "true_action_score", "\n", "img_test", "=", "img_name", "\n", "\n", "total_val_loss", "+=", "val_loss", ".", "item", "(", ")", "\n", "\n", "\n", "#print(\"true_act_stat: \", true_act_stat)", "\n", "#print(\"predicted_act_stat: \", predicted_act_stat)", "\n", "", "val_avg_loss", "=", "total_val_loss", "/", "val_step", "\n", "print", "(", "\"Avg Validation loss after epoch: \"", ",", "epoch", ",", "\"is: \"", ",", "val_avg_loss", ")", "\n", "file1", ".", "write", "(", "\"for epoch:%d, Average_Validation_loss:%.3f\\n\"", "%", "(", "epoch", ",", "val_avg_loss", ")", ")", "\n", "", "print", "(", "\"Test accuracy score after epoch: \"", ",", "epoch", ",", "\"is: \"", ",", "val_accuracy", ")", "\n", "\n", "", "train_loss_avg", "=", "train_loss", "/", "train_step", "\n", "\n", "print", "(", "\"Average_Train_loss: \"", ",", "train_loss_avg", ")", "\n", "#print(\"Average_Validation_loss: \", val_loss_avg)", "\n", "train_losses", ".", "append", "(", "train_loss_avg", ")", "\n", "val_losses", ".", "append", "(", "val_avg_loss", ")", "\n", "#write mode ", "\n", "file1", ".", "write", "(", "\"Average_Train_loss:%.3f \\n\"", "%", "(", "train_loss_avg", ")", ")", "\n", "\n", "\n", "if", "(", "epoch", "%", "10", "==", "0", ")", ":", "\n", "\n", "            ", "print", "(", "\"\\nSaving the models\"", ")", "\n", "torch", ".", "save", "(", "image_encoder", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "'models/'", ",", "'image_encoder_pre_we300_im2048-%d.pth'", "%", "epoch", ")", ")", "\n", "torch", ".", "save", "(", "advice_encoder", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "'models/'", ",", "'advice_encoder_pre_we300_im2048-%d.pth'", "%", "epoch", ")", ")", "\n", "torch", ".", "save", "(", "decoded_action", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "'models/'", ",", "'decoder_pre_we300_im2048-%d.pth'", "%", "epoch", ")", ")", "\n", "", "", "file1", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.test_agent": [[485, 508], ["data_loader_advice_driven_train.create_rationalization_matrix", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.reshape.type", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load.", "torch.load.", "torch.load.", "open", "pickle.load", "pickle.load", "torch.max", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.frogger_dataset.FroggerDataset.create_rationalization_matrix"], ["", "def", "test_agent", "(", "encoded_img", ",", "advice_text", ")", ":", "\n", "    ", "vocab_path", "=", "'data/vocab_frogger.pkl'", "\n", "with", "open", "(", "vocab_path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "vocab", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "advice_ids", "=", "create_rationalization_matrix", "(", "advice", ",", "vocab", ")", "\n", "#print(\"advice_testing: \", advice_ids)", "\n", "advice_text", "=", "torch", ".", "tensor", "(", "advice_ids", ")", "\n", "advice_text", "=", "advice_text", ".", "type", "(", "torch", ".", "LongTensor", ")", "\n", "#print(\"advice_text_shape: \",advice_text.shape)", "\n", "advice_text", "=", "torch", ".", "reshape", "(", "advice_text", ",", "(", "-", "1", ",", ")", ")", "\n", "\n", "word_emb", "=", "torch", ".", "load", "(", "\"word_emb-100.pth\"", ")", "\n", "sentence_emb", "=", "torch", ".", "load", "(", "\"sentence_encoder-100.pth\"", ")", "\n", "action_decode", "=", "torch", ".", "load", "(", "\"decoder-100.pth\"", ")", "\n", "\n", "words", "=", "word_emb", "(", "advice_text", ")", "\n", "sentences", "=", "sentence_emb", "(", "words", ")", "\n", "actions", "=", "action_decode", "(", "encoded_img", ",", "sentences", ")", "\n", "\n", "pred_act", "=", "torch", ".", "max", "(", "actions", ",", "1", ")", "[", "1", "]", ".", "data", "\n", "\n", "return", "pred_act", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.frogger_dataset.FroggerDataset.__init__": [[17, 20], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data_file", ",", "vocab", ")", ":", "\n", "        ", "self", ".", "data_file", "=", "data_file", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.frogger_dataset.FroggerDataset.create_rationalization_matrix": [[21, 44], ["sent.strip.strip.replace", "sent.strip.strip.strip", "sent.strip.strip.lower().split", "len", "numpy.zeros", "enumerate", "rationalization_matrix.append", "sequence_lenght_arr.append", "sent.strip.strip.lower"], "methods", ["None"], ["", "def", "create_rationalization_matrix", "(", "self", ",", "rationalizations", ",", "vocab", ")", ":", "\n", "        ", "rationalization_matrix", "=", "[", "]", "\n", "sequence_lenght_arr", "=", "[", "]", "\n", "for", "sent", "in", "rationalizations", ":", "\n", "            ", "sent", "=", "sent", ".", "replace", "(", "\"'\"", ",", "\"\"", ")", "\n", "sent", "=", "sent", ".", "strip", "(", ")", "\n", "words", "=", "sent", ".", "lower", "(", ")", ".", "split", "(", "' '", ")", "\n", "max_rationalization_len", "=", "len", "(", "words", ")", "\n", "max_rationalization_len", "=", "max_rationalization_len", "+", "2", "\n", "ration_sent", "=", "np", ".", "zeros", "(", "[", "max_rationalization_len", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "ration_sent", "[", "0", "]", "=", "1", "\n", "\n", "for", "k", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "                ", "if", "word", "in", "vocab", ".", "word2idx", ":", "\n", "                    ", "ration_sent", "[", "k", "+", "1", "]", "=", "vocab", ".", "word2idx", "[", "word", "]", "\n", "", "else", ":", "\n", "                    ", "ration_sent", "[", "k", "+", "1", "]", "=", "3", "\n", "\n", "", "", "ration_sent", "[", "max_rationalization_len", "-", "1", "]", "=", "2", "\n", "rationalization_matrix", ".", "append", "(", "ration_sent", ")", "\n", "sequence_lenght_arr", ".", "append", "(", "max_rationalization_len", "+", "2", ")", "\n", "\n", "", "return", "rationalization_matrix", ",", "sequence_lenght_arr", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.frogger_dataset.FroggerDataset.load_data": [[45, 122], ["xlrd.open_workbook", "xlrd.open_workbook.sheets", "int", "print", "print", "slice", "print", "slice", "print", "print", "print", "collections.Counter", "range", "math.floor", "len", "len", "len", "len", "len", "len", "len", "numpy.array", "numpy.array", "len", "sheet.cell", "good_ids.append", "nltk.tokenize.word_tokenize", "re.sub.lower", "good_rationalizations.append", "re.sub", "re.sub.split", "len", "lengths.append", "enumerate", "rationalizations.append", "rationalization_tokens.append", "len", "sheet.cell", "re.sub.lower"], "methods", ["None"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "data_file", "=", "self", ".", "data_file", "\n", "vocab", "=", "self", ".", "vocab", "\n", "wb", "=", "open_workbook", "(", "data_file", ")", "\n", "\n", "#read the rationalizations from the excel file and create a list of training/testing rationalizations. ", "\n", "for", "sheet", "in", "wb", ".", "sheets", "(", ")", ":", "\n", "            ", "number_of_rows", "=", "sheet", ".", "nrows", "\n", "number_of_columns", "=", "sheet", ".", "ncols", "\n", "rationalizations", "=", "[", "]", "\n", "rationalization_tokens", "=", "[", "]", "\n", "items", "=", "[", "]", "\n", "rows", "=", "[", "]", "\n", "lengths", "=", "[", "]", "\n", "max_length", "=", "0", "\n", "\n", "bad_worker_ids", "=", "[", "'A2CNSIECB9UP05'", ",", "'A23782O23HSPLA'", ",", "'A2F9ZBSR6AXXND'", ",", "'A3GI86L18Z71XY'", ",", "'AIXTI8PKSX1D2'", ",", "'A2QWHXMFQI18GQ'", ",", "'A3SB7QYI84HYJT'", ",", "\n", "'A2Q2A7AB6MMFLI'", ",", "'A2P1KI42CJVNIA'", ",", "'A1IJXPKZTJV809'", ",", "'A2WZ0RZMKQ2WGJ'", ",", "'A3EKETMVGU2PM9'", ",", "'A1OCEC1TBE3CWA'", ",", "'AE1RYK54MH11G'", ",", "'A2ADEPVGNNXNPA'", ",", "\n", "'A15QGLWS8CNJFU'", ",", "'A18O3DEA5Z4MJD'", ",", "'AAAL4RENVAPML'", ",", "'A3TZBZ92CQKQLG'", ",", "'ABO9F0JD9NN54'", ",", "'A8F6JFG0WSELT'", ",", "'ARN9ET3E608LJ'", ",", "'A2TCYNRAZWK8CC'", ",", "\n", "'A32BK0E1IPDUAF'", ",", "'ANNV3E6CIVCW4'", ",", "'AXMQBHHU22TSP'", ",", "'AKATSYE8XLYNL'", ",", "'A355PGLV2ID2SX'", ",", "'A55CXM7QR7R0N'", ",", "'A111ZFNLXK1TCO'", "]", "\n", "\n", "good_ids", "=", "[", "]", "\n", "good_rationalizations", "=", "[", "]", "\n", "counter", "=", "Counter", "(", ")", "\n", "for", "row", "in", "range", "(", "1", ",", "number_of_rows", ")", ":", "\n", "                ", "values", "=", "[", "]", "\n", "worker_id", "=", "sheet", ".", "cell", "(", "row", ",", "0", ")", ".", "value", "\n", "if", "worker_id", "not", "in", "bad_worker_ids", ":", "\n", "                    ", "good_ids", ".", "append", "(", "row", "-", "1", ")", "\n", "line", "=", "sheet", ".", "cell", "(", "row", ",", "4", ")", ".", "value", "\n", "tokens", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "line", ".", "lower", "(", ")", ")", "\n", "\n", "line", "=", "line", ".", "lower", "(", ")", "\n", "good_rationalizations", ".", "append", "(", "line", ")", "\n", "#line = re.sub('[^a-z\\ ]+', \" \", line)", "\n", "line", "=", "re", ".", "sub", "(", "r'[^\\w\\s]'", ",", "' '", ",", "line", ")", "\n", "words", "=", "line", ".", "split", "(", ")", "\n", "length", "=", "len", "(", "tokens", ")", "\n", "lengths", ".", "append", "(", "length", ")", "\n", "if", "length", ">", "max_length", ":", "\n", "                        ", "max_length", "=", "length", "\n", "", "for", "index", ",", "word", "in", "enumerate", "(", "tokens", ")", ":", "\n", "                        ", "if", "word", "in", "vocab", ".", "word2idx", ":", "\n", "                            ", "tokens", "[", "index", "]", "=", "vocab", ".", "word2idx", "[", "word", "]", "\n", "", "else", ":", "\n", "                            ", "tokens", "[", "index", "]", "=", "vocab", ".", "word2idx", "[", "'<unk>'", "]", "\n", "", "", "rationalizations", ".", "append", "(", "words", ")", "\n", "rationalization_tokens", ".", "append", "(", "tokens", ")", "\n", "", "", "rationalizations", "=", "[", "np", ".", "array", "(", "xi", ")", "for", "xi", "in", "rationalizations", "]", "\n", "rationalization_tokens", "=", "[", "np", ".", "array", "(", "xi", ")", "for", "xi", "in", "rationalization_tokens", "]", "\n", "\n", "#good_ids_split = int(floor((90.0/100)*len(good_ids)))", "\n", "", "split", "=", "int", "(", "floor", "(", "(", "90.0", "/", "100", ")", "*", "len", "(", "rationalizations", ")", ")", ")", "\n", "print", "(", "\"good_rationalizations: \"", ",", "len", "(", "good_rationalizations", ")", ")", "\n", "#print(\"good_rationalizations: \",good_rationalizations)\t\t", "\n", "print", "(", "\"rationalizations_tokens: \"", ",", "len", "(", "rationalization_tokens", ")", ")", "\n", "#print(\"rationalizations: \",rationalizations)        ", "\n", "tr", "=", "slice", "(", "0", ",", "split", ")", "\n", "tr_good_ids", "=", "good_ids", "[", "tr", "]", "\n", "print", "(", "\"tr_good_ids: \"", ",", "len", "(", "tr_good_ids", ")", ")", "\n", "\n", "tr_indices", "=", "[", "0", ",", "split", "-", "1", "]", "\n", "#print(\"tr_contents: \",tr)", "\n", "te_indices", "=", "[", "split", ",", "len", "(", "rationalizations", ")", "-", "1", "]", "\n", "\n", "te", "=", "slice", "(", "split", ",", "len", "(", "rationalizations", ")", ")", "\n", "te_good_ids", "=", "good_ids", "[", "te", "]", "\n", "print", "(", "\"te_good_ids: \"", ",", "len", "(", "te_good_ids", ")", ")", "\n", "#print(\"te_contents: \",te)", "\n", "#training_rationalizations = good_rationalizations[tr]", "\n", "#testing_rationalizations = good_rationalizations[te]", "\n", "training_rationalizations", "=", "rationalization_tokens", "[", "tr", "]", "\n", "print", "(", "\"train rationalizations_tokens: \"", ",", "len", "(", "training_rationalizations", ")", ")", "\n", "testing_rationalizations", "=", "rationalization_tokens", "[", "te", "]", "\n", "print", "(", "\"test rationalizations_tokens: \"", ",", "len", "(", "testing_rationalizations", ")", ")", "\n", "\n", "return", "tr_good_ids", ",", "te_good_ids", ",", "tr_indices", ",", "te_indices", ",", "training_rationalizations", ",", "testing_rationalizations", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.frogger_dataset.FroggerDataset.load_images": [[123, 149], ["os.listdir", "sorted", "len", "print", "enumerate", "cur_training_images.append", "cur_test_images.append"], "methods", ["None"], ["", "def", "load_images", "(", "self", ",", "current_image_dir", ",", "tr_good_ids", ",", "te_good_ids", ",", "tr_indices", ")", ":", "\n", "        ", "current_images", "=", "os", ".", "listdir", "(", "current_image_dir", ")", "\n", "current_images", "=", "sorted", "(", "current_images", ",", "key", "=", "numericalSort", ")", "\n", "\n", "#print(good_ids[tr_indices[0]]", "\n", "\n", "#next_images = os.listdir(next_image_dir)", "\n", "#next_images = sorted(next_images ,key = numericalSort)", "\n", "num_images", "=", "len", "(", "current_images", ")", "\n", "print", "(", "\"number of images: \"", ",", "num_images", ")", "\n", "\n", "cur_training_images", "=", "[", "]", "\n", "#next_training_images = []", "\n", "cur_test_images", "=", "[", "]", "\n", "#next_test_images = []", "\n", "for", "i", ",", "file", "in", "enumerate", "(", "current_images", ")", ":", "\n", "            ", "if", "i", "in", "tr_good_ids", ":", "\n", "                ", "cur_training_images", ".", "append", "(", "file", ")", "\n", "#if good_ids[tr_indices[0]]<=i and i<=good_ids[tr_indices[1]]:", "\n", "#cur_training_images.append(file)", "\n", "#next_training_images.append(next_images[i])", "\n", "", "elif", "i", "in", "te_good_ids", ":", "\n", "                ", "cur_test_images", ".", "append", "(", "file", ")", "\n", "#next_test_images.append(next_images[i])", "\n", "#print(\"number of test images: \",len(cur_test_images))       ", "\n", "", "", "return", "cur_training_images", ",", "cur_test_images", "\n", "", "", ""]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.frogger_dataset.numericalSort": [[11, 15], ["numbers.split", "map"], "function", ["None"], ["def", "numericalSort", "(", "value", ")", ":", "\n", "    ", "parts", "=", "numbers", ".", "split", "(", "value", ")", "\n", "parts", "[", "1", ":", ":", "2", "]", "=", "map", "(", "int", ",", "parts", "[", "1", ":", ":", "2", "]", ")", "\n", "return", "parts", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_gen.get_length": [[37, 44], ["len", "list().index", "len", "list"], "function", ["None"], ["def", "get_length", "(", "source_list", ")", ":", "\n", "    ", "length", "=", "len", "(", "source_list", ")", "\n", "try", ":", "\n", "        ", "length", "=", "list", "(", "source_list", ")", ".", "index", "(", "0", ")", "\n", "", "except", ":", "\n", "        ", "length", "=", "len", "(", "source_list", ")", "\n", "", "return", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_gen.get_sentences": [[45, 59], ["len", "len", "range", "range", "sentence.strip.strip", "sentences.append", "int"], "function", ["None"], ["", "def", "get_sentences", "(", "word_ids", ")", ":", "\n", "    ", "batch", "=", "len", "(", "word_ids", "[", "0", "]", ")", "\n", "sent_len", "=", "len", "(", "word_ids", ")", "\n", "sentences", "=", "[", "]", "\n", "for", "col", "in", "range", "(", "batch", ")", ":", "\n", "        ", "sentence", "=", "\"\"", "\n", "for", "row", "in", "range", "(", "sent_len", ")", ":", "\n", "            ", "word", "=", "in_vocab", ".", "idx2word", "[", "int", "(", "word_ids", "[", "row", "]", "[", "col", "]", ")", "]", "\n", "if", "word", "!=", "'<end>'", "and", "word", "!=", "'<pad>'", "and", "word", "!=", "'<start>'", ":", "\n", "                ", "sentence", "=", "sentence", "+", "\" \"", "+", "word", "\n", "\n", "", "", "sentence", "=", "sentence", ".", "strip", "(", ")", "\n", "sentences", ".", "append", "(", "sentence", ")", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_gen.weight_gloVe": [[60, 82], ["pickle.load", "len", "numpy.zeros", "range", "print", "open", "numpy.random.normal"], "function", ["None"], ["", "def", "weight_gloVe", "(", "target_vocab", ")", ":", "\n", "# words = pickle.load(open(f'glove.6B/6B.300_words.pkl', 'rb'))", "\n", "# word2idx = pickle.load(open(f'glove.6B/6B.300_idx.pkl', 'rb'))", "\n", "# vectors = bcolz.open(f'glove.6B/6B.300.dat')[:]", "\n", "    ", "glove", "=", "pickle", ".", "load", "(", "open", "(", "f'glove/6B_300_words_emb.pkl'", ",", "'rb'", ")", ")", "\n", "\n", "matrix_len", "=", "len", "(", "target_vocab", ".", "word2idx", ")", "\n", "weights_matrix", "=", "np", ".", "zeros", "(", "(", "matrix_len", ",", "300", ")", ")", "\n", "words_found", "=", "0", "\n", "emb_dim", "=", "300", "\n", "\n", "for", "i", "in", "range", "(", "matrix_len", ")", ":", "\n", "        ", "word", "=", "target_vocab", ".", "idx2word", "[", "i", "]", "\n", "try", ":", "\n", "            ", "weights_matrix", "[", "i", "]", "=", "glove", "[", "word", "]", "\n", "words_found", "+=", "1", "\n", "", "except", "KeyError", ":", "\n", "            ", "weights_matrix", "[", "i", "]", "=", "np", ".", "random", ".", "normal", "(", "scale", "=", "0.6", ",", "size", "=", "(", "emb_dim", ",", ")", ")", "\n", "\n", "", "", "print", "(", "\"words_found: \"", ",", "words_found", ")", "\n", "\n", "return", "weights_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_gen.main": [[83, 272], ["torchvision.transforms.Compose", "torchvision.transforms.Compose", "frogger_dataset_preprocessed.FroggerDataset", "frogger_dataset_preprocessed.FroggerDataset.load_data", "frogger_dataset_preprocessed.FroggerDataset.load_images", "print", "print", "data_loader_advice_gen.get_loader", "data_loader_advice_gen.get_loader", "torch.device", "torch.device", "torch.device", "torch.device", "len", "advice_gen.weight_gloVe", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "pretrained_word_weight.to.to", "advice_models.EncoderCNN().to", "advice_models.DecoderRNN().to", "DecoderRNN().to.init_word_embedding", "torch.CrossEntropyLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "len", "print", "len", "print", "range", "numpy.save", "numpy.save", "len", "len", "list", "list", "EncoderCNN().to.train", "DecoderRNN().to.train", "range", "train_losses.append", "print", "numpy.array", "numpy.array", "torchvision.transforms.RandomCrop", "torchvision.transforms.RandomHorizontalFlip", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.Resize", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "advice_models.EncoderCNN", "advice_models.DecoderRNN", "DecoderRNN().to.parameters", "EncoderCNN().to.parameters", "DecoderRNN().to.zero_grad", "EncoderCNN().to.zero_grad", "next", "captions[].to", "captions[].to", "images.to.to", "EncoderCNN().to.", "DecoderRNN().to.", "enumerate", "loss.backward", "torch.optim.Adam.step", "loss.item", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "EncoderCNN().to.eval", "DecoderRNN().to.eval", "range", "val_losses.append", "print", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "list", "list", "json.dump", "json.dump", "iter", "zip", "advice_gen.get_length", "nn.CrossEntropyLoss.", "next", "val_captions[].to", "val_captions[].to", "val_images.to.to", "EncoderCNN().to.", "DecoderRNN().to.", "DecoderRNN().to.inference", "decoder.cpu().numpy", "advice_gen.get_sentences", "range", "nn.CrossEntropyLoss.", "criterion.item", "DecoderRNN().to.state_dict", "os.path.join", "EncoderCNN().to.state_dict", "os.path.join", "open", "open", "iter", "range", "range", "results.append", "inference_results.append", "decoder.view", "val_captions[].to.contiguous().view", "decoder.cpu", "len", "numpy.argmax", "int", "len", "int", "val_captions[].to.contiguous"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.frogger_dataset.FroggerDataset.load_data", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.frogger_dataset.FroggerDataset.load_images", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader_advice_driven_train.get_loader", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.data_loader_advice_driven_train.get_loader", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_gen.weight_gloVe", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.Advice_Encoder.init_word_embedding", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.train", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_driven_agent.train", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_gen.get_length", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_models.DecoderRNN.inference", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.advice_gen.get_sentences"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "train_transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomCrop", "(", "args", ".", "image_size", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "(", "0.485", ",", "0.456", ",", "0.406", ")", ",", "\n", "(", "0.229", ",", "0.224", ",", "0.225", ")", ")", "]", ")", "\n", "\n", "val_transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Resize", "(", "args", ".", "image_size", ",", "interpolation", "=", "Image", ".", "LANCZOS", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "(", "0.485", ",", "0.456", ",", "0.406", ")", ",", "\n", "(", "0.229", ",", "0.224", ",", "0.225", ")", ")", "]", ")", "\n", "\n", "frogger_dataset_ob", "=", "FroggerDataset", "(", "'Turk_Master_File_sorted.xlsx'", ",", "in_vocab", ")", "\n", "tr_good_ids", ",", "te_good_ids", ",", "tr_indices", ",", "te_indices", ",", "training_rationalizations", ",", "testing_rationalizations", "=", "frogger_dataset_ob", ".", "load_data", "(", ")", "\n", "cur_training_images", ",", "cur_test_images", "=", "frogger_dataset_ob", ".", "load_images", "(", "current_image_dir", ",", "tr_good_ids", ",", "te_good_ids", ",", "tr_indices", ")", "\n", "print", "(", "\"numbers of Train images: \"", ",", "len", "(", "cur_training_images", ")", ")", "\n", "print", "(", "\"numbers of test images: \"", ",", "len", "(", "cur_test_images", ")", ")", "\n", "\n", "train_data_loader", "=", "get_loader", "(", "in_vocab", ",", "training_rationalizations", ",", "cur_training_images", ",", "\n", "current_image_dir", ",", "args", ".", "batch_size", ",", "train_transform", ",", "\n", "shuffle", "=", "True", ",", "num_workers", "=", "0", ")", "\n", "val_data_loader", "=", "get_loader", "(", "in_vocab", ",", "testing_rationalizations", ",", "cur_test_images", ",", "\n", "current_image_dir", ",", "args", ".", "batch_size", ",", "val_transform", ",", "\n", "shuffle", "=", "True", ",", "num_workers", "=", "0", ")", "\n", "\n", "# val_losses = []", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "vocab_size", "=", "len", "(", "train_data_loader", ".", "dataset", ".", "vocab", ")", "\n", "pretrained_word_weight", "=", "weight_gloVe", "(", "in_vocab", ")", "\n", "pretrained_word_weight", "=", "torch", ".", "from_numpy", "(", "pretrained_word_weight", ")", ".", "float", "(", ")", "\n", "pretrained_word_weight", "=", "pretrained_word_weight", ".", "to", "(", "device", ")", "\n", "#encoder = image_EncoderCNN(args.img_feature_size).to(device)", "\n", "encoder", "=", "EncoderCNN", "(", "args", ".", "img_feature_size", ")", ".", "to", "(", "device", ")", "\n", "decoder", "=", "DecoderRNN", "(", "args", ".", "embed_size", ",", "args", ".", "img_feature_size", ",", "args", ".", "hidden_size", ",", "vocab_size", ")", ".", "to", "(", "device", ")", "\n", "decoder", ".", "init_word_embedding", "(", "pretrained_word_weight", ")", "\n", "\n", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "params", "=", "list", "(", "decoder", ".", "parameters", "(", ")", ")", "+", "list", "(", "encoder", ".", "parameters", "(", ")", ")", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "params", ",", "lr", "=", "0.001", ",", "weight_decay", "=", "1e-5", ")", "\n", "\n", "total_train_step", "=", "len", "(", "train_data_loader", ")", "\n", "print", "(", "\"numbers of train images: \"", ",", "total_train_step", ")", "\n", "total_val_step", "=", "len", "(", "val_data_loader", ")", "\n", "print", "(", "\"numbers of val images: \"", ",", "total_val_step", ")", "\n", "\n", "\n", "\n", "#if os.path.exists('models/decoder-90.pth'):", "\n", "#decoder.load_state_dict(torch.load('models/decoder-90.pth'))", "\n", "#print('decoder Model loaded')", "\n", "#if os.path.exists('models/encoder-90.pth'):", "\n", "#encoder.load_state_dict(torch.load('models/encoder-90.pth'))       ", "\n", "#print('encoder Model loaded')", "\n", "\n", "#num_epochs = 1", "\n", "train_losses", "=", "[", "]", "\n", "val_losses", "=", "[", "]", "\n", "for", "epoch", "in", "range", "(", "0", ",", "args", ".", "num_epochs", ")", ":", "\n", "        ", "results", "=", "[", "]", "\n", "inference_results", "=", "[", "]", "\n", "total_train_loss", "=", "0", "\n", "\n", "# set decoder and encoder into train mode", "\n", "encoder", ".", "train", "(", ")", "\n", "decoder", ".", "train", "(", ")", "\n", "for", "i_step", "in", "range", "(", "0", ",", "total_train_step", ")", ":", "\n", "# zero the gradients", "\n", "            ", "decoder", ".", "zero_grad", "(", ")", "\n", "encoder", ".", "zero_grad", "(", ")", "\n", "\n", "\n", "# Obtain the batch.", "\n", "img_name", ",", "images", ",", "captions", "=", "next", "(", "iter", "(", "train_data_loader", ")", ")", "\n", "\n", "# make the captions for targets and teacher forcer", "\n", "captions_target", "=", "captions", "[", ":", ",", "1", ":", "]", ".", "to", "(", "device", ")", "\n", "captions_train", "=", "captions", "[", ":", ",", ":", "captions", ".", "shape", "[", "1", "]", "]", ".", "to", "(", "device", ")", "\n", "\n", "# Move batch of images and captions to GPU if CUDA is available.", "\n", "images", "=", "images", ".", "to", "(", "device", ")", "\n", "\n", "# Pass the inputs through the CNN-RNN model.", "\n", "features", "=", "encoder", "(", "images", ")", "\n", "outputs", "=", "decoder", "(", "features", ",", "captions_train", ")", "\n", "\n", "# Calculate the batch loss", "\n", "loss", "=", "0.0", "\n", "for", "sj", ",", "output_result", "in", "enumerate", "(", "zip", "(", "outputs", ",", "captions_target", ")", ")", ":", "\n", "                ", "length", "=", "get_length", "(", "output_result", "[", "1", "]", ")", "\n", "x", "=", "output_result", "[", "0", "]", "\n", "y", "=", "output_result", "[", "1", "]", "\n", "loss", "+=", "criterion", "(", "x", "[", ":", "length", ",", "]", ",", "y", "[", ":", "length", "]", ")", "\n", "#loss = criterion(outputs.view(-1, vocab_size), captions_target.contiguous().view(-1))", "\n", "\n", "# Backward pass", "\n", "# optimizer.zero_grad()", "\n", "", "loss", ".", "backward", "(", ")", "\n", "\n", "# Update the parameters in the optimizer", "\n", "optimizer", ".", "step", "(", ")", "\n", "total_train_loss", "+=", "loss", ".", "item", "(", ")", "\n", "", "avg_train_loss", "=", "total_train_loss", "/", "total_train_step", "\n", "train_losses", ".", "append", "(", "avg_train_loss", ")", "\n", "# - - - Validate - - -", "\n", "# turn the evaluation mode on", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# set the evaluation mode", "\n", "            ", "encoder", ".", "eval", "(", ")", "\n", "decoder", ".", "eval", "(", ")", "\n", "total_val_loss", "=", "0", "\n", "for", "val_step", "in", "range", "(", "0", ",", "total_val_step", ")", ":", "\n", "# get the validation images and captions", "\n", "                ", "img_name", ",", "val_images", ",", "val_captions", "=", "next", "(", "iter", "(", "val_data_loader", ")", ")", "\n", "\n", "# define the captions", "\n", "captions_target", "=", "val_captions", "[", ":", ",", "1", ":", "]", ".", "to", "(", "device", ")", "\n", "captions_train", "=", "val_captions", "[", ":", ",", ":", "val_captions", ".", "shape", "[", "1", "]", "-", "1", "]", ".", "to", "(", "device", ")", "\n", "\n", "# Move batch of images and captions to GPU if CUDA is available.", "\n", "val_images", "=", "val_images", ".", "to", "(", "device", ")", "\n", "\n", "# Pass the inputs through the CNN-RNN model.", "\n", "features", "=", "encoder", "(", "val_images", ")", "\n", "outputs", "=", "decoder", "(", "features", ",", "captions_train", ")", "\n", "output_ids", "=", "decoder", ".", "inference", "(", "features", ")", "\n", "cap_predicted", "=", "outputs", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predicted_sentences", "=", "get_sentences", "(", "output_ids", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "args", ".", "batch_size", ")", ":", "\n", "                    ", "caption_str", "=", "\"\"", "\n", "caption_grndtrth", "=", "\"\"", "\n", "for", "j", "in", "range", "(", "0", ",", "len", "(", "cap_predicted", "[", "i", "]", ")", ")", ":", "\n", "                        ", "out_str", "=", "np", ".", "argmax", "(", "cap_predicted", "[", "i", "]", "[", "j", "]", ")", "\n", "#print(\"out_str: \",out_str)", "\n", "out_", "=", "int", "(", "out_str", ")", "\n", "#out_ = str(out_)", "\n", "out_cap", "=", "in_vocab", ".", "idx2word", "[", "out_str", "]", "\n", "caption_str", "=", "caption_str", "+", "\" \"", "+", "out_cap", "\n", "\n", "", "for", "k", "in", "range", "(", "0", ",", "len", "(", "outputs", "[", "i", "]", ")", ")", ":", "\n", "                        ", "grnd_str", "=", "captions_target", "[", "i", "]", "[", "k", "]", "\n", "#print(\"grnd_str: \", grnd_str)", "\n", "grnd_", "=", "int", "(", "grnd_str", ")", "\n", "#print(\"grnd_: \", grnd_)", "\n", "#grnd_ = str(grnd_)", "\n", "grnd_cap", "=", "in_vocab", ".", "idx2word", "[", "grnd_", "]", "\n", "caption_grndtrth", "=", "caption_grndtrth", "+", "\" \"", "+", "grnd_cap", "\n", "\n", "", "image_name", "=", "img_name", "[", "i", "]", "\n", "\n", "\n", "results", ".", "append", "(", "{", "u'image name'", ":", "image_name", ",", "u'generated caption'", ":", "caption_str", ",", "u'ground_truth_cap'", ":", "caption_grndtrth", "}", ")", "\n", "inference_results", ".", "append", "(", "{", "u'image name'", ":", "image_name", ",", "u'generated caption'", ":", "predicted_sentences", "[", "i", "]", ",", "u'ground_truth_cap'", ":", "caption_grndtrth", "}", ")", "\n", "\n", "#print(\"generated caption: \", caption_str, \" ground_truth_cap: \", caption_grndtrth)", "\n", "#\" ground_ans: \", true_ans)", "\n", "\n", "\n", "# Calculate the batch loss.", "\n", "", "val_loss", "=", "criterion", "(", "outputs", ".", "view", "(", "-", "1", ",", "vocab_size", ")", ",", "captions_target", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ")", "\n", "total_val_loss", "+=", "val_loss", ".", "item", "(", ")", "\n", "\n", "", "avg_val_loss", "=", "total_val_loss", "/", "total_val_step", "\n", "val_losses", ".", "append", "(", "avg_val_loss", ")", "\n", "\n", "# save the losses", "\n", "\n", "\n", "# Get training statistics.", "\n", "", "stats", "=", "'Epoch [%d/%d], Training Loss: %.4f, Val Loss: %.4f'", "%", "(", "epoch", ",", "args", ".", "num_epochs", ",", "avg_train_loss", ",", "avg_val_loss", ")", "\n", "\n", "# Print training statistics (on same line).", "\n", "print", "(", "'\\r'", "+", "stats", ")", "\n", "#sys.stdout.flush()", "\n", "\n", "# Save the weights.", "\n", "if", "epoch", "%", "20", "==", "0", ":", "\n", "            ", "print", "(", "\"\\nSaving the model\"", ")", "\n", "torch", ".", "save", "(", "decoder", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "'models/'", ",", "'decoder_prep95_dl-%d.pth'", "%", "epoch", ")", ")", "\n", "torch", ".", "save", "(", "encoder", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "'models/'", ",", "'encoder_prep95_dl-%d.pth'", "%", "epoch", ")", ")", "\n", "my_advice", "=", "list", "(", "results", ")", "\n", "inference_advice", "=", "list", "(", "inference_results", ")", "\n", "json", ".", "dump", "(", "my_advice", ",", "open", "(", "'results/advices_img_prep95-epoch-%d.json'", "%", "epoch", ",", "'w'", ")", ")", "\n", "json", ".", "dump", "(", "inference_advice", ",", "open", "(", "'results/inference_advice_img_prep95-epoch-%d.json'", "%", "epoch", ",", "'w'", ")", ")", "\n", "\n", "", "", "np", ".", "save", "(", "'results/train_losses'", ",", "np", ".", "array", "(", "train_losses", ")", ")", "\n", "np", ".", "save", "(", "'results/val_losses'", ",", "np", ".", "array", "(", "val_losses", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__init__": [[16, 20], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "word2idx", "=", "{", "}", "\n", "self", ".", "idx2word", "=", "{", "}", "\n", "self", ".", "idx", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word": [[21, 26], ["None"], "methods", ["None"], ["", "def", "add_word", "(", "self", ",", "word", ")", ":", "\n", "        ", "if", "not", "word", "in", "self", ".", "word2idx", ":", "\n", "            ", "self", ".", "word2idx", "[", "word", "]", "=", "self", ".", "idx", "\n", "self", ".", "idx2word", "[", "self", ".", "idx", "]", "=", "word", "\n", "self", ".", "idx", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__call__": [[27, 31], ["None"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "word", ")", ":", "\n", "        ", "if", "not", "word", "in", "self", ".", "word2idx", ":", "\n", "            ", "return", "self", ".", "word2idx", "[", "'<unk>'", "]", "\n", "", "return", "self", ".", "word2idx", "[", "word", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.__len__": [[32, 34], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "word2idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.build_vocab": [[35, 62], ["COCO", "collections.Counter", "COCO.anns.keys", "enumerate", "build_vocab_v2.Vocabulary", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "enumerate", "str", "nltk.tokenize.word_tokenize", "collections.Counter.update", "build_vocab_v2.Vocabulary.add_word", "str.lower", "print", "collections.Counter.items", "len"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word"], ["", "", "def", "build_vocab", "(", "json", ",", "threshold", ")", ":", "\n", "    ", "\"\"\"Build a simple vocabulary wrapper.\"\"\"", "\n", "coco", "=", "COCO", "(", "json", ")", "\n", "counter", "=", "Counter", "(", ")", "\n", "ids", "=", "coco", ".", "anns", ".", "keys", "(", ")", "\n", "for", "i", ",", "id", "in", "enumerate", "(", "ids", ")", ":", "\n", "        ", "caption", "=", "str", "(", "coco", ".", "anns", "[", "id", "]", "[", "'caption'", "]", ")", "\n", "tokens", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "caption", ".", "lower", "(", ")", ")", "\n", "counter", ".", "update", "(", "tokens", ")", "\n", "\n", "if", "i", "%", "1000", "==", "0", ":", "\n", "            ", "print", "(", "\"[%d/%d] Tokenized the captions.\"", "%", "(", "i", ",", "len", "(", "ids", ")", ")", ")", "\n", "\n", "# If the word frequency is less than 'threshold', then the word is discarded.", "\n", "", "", "words", "=", "[", "word", "for", "word", ",", "cnt", "in", "counter", ".", "items", "(", ")", "if", "cnt", ">=", "threshold", "]", "\n", "\n", "# Creates a vocab wrapper and add some special tokens.", "\n", "vocab", "=", "Vocabulary", "(", ")", "\n", "vocab", ".", "add_word", "(", "'<pad>'", ")", "\n", "vocab", ".", "add_word", "(", "'<start>'", ")", "\n", "vocab", ".", "add_word", "(", "'<end>'", ")", "\n", "vocab", ".", "add_word", "(", "'<unk>'", ")", "\n", "\n", "# Adds the words to the vocabulary.", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "vocab", ".", "add_word", "(", "word", ")", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.build_vocab_frogger": [[63, 90], ["open", "open.read().lower", "open.close", "re.sub.lower", "re.sub", "list", "build_vocab_v2.Vocabulary", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "enumerate", "re.sub.split", "build_vocab_v2.Vocabulary.add_word", "open.read"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.game_access.Game.close", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word"], ["", "def", "build_vocab_frogger", "(", "json", ",", "threshold", ")", ":", "\n", "\n", "# Creates a vocab wrapper and add some special tokens.", "\n", "    ", "file", "=", "open", "(", "'Sample_Training_Set/Rationalizations.txt'", ",", "'r'", ")", "\n", "# .lower() returns a version with all upper case characters replaced with lower case characters.", "\n", "text", "=", "file", ".", "read", "(", ")", ".", "lower", "(", ")", "\n", "file", ".", "close", "(", ")", "\n", "# replaces anything that is not a lowercase letter, a space, or an apostrophe with a space:", "\n", "text", "=", "text", ".", "lower", "(", ")", "\n", "# print(type(text))", "\n", "# exit(0)", "\n", "text", "=", "re", ".", "sub", "(", "'[^a-z\\ \\']+'", ",", "\" \"", ",", "text", ")", "\n", "words", "=", "list", "(", "text", ".", "split", "(", ")", ")", "\n", "# print words", "\n", "# exit(0)", "\n", "vocab", "=", "Vocabulary", "(", ")", "\n", "vocab", ".", "add_word", "(", "'<pad>'", ")", "\n", "vocab", ".", "add_word", "(", "'<start>'", ")", "\n", "vocab", ".", "add_word", "(", "'<end>'", ")", "\n", "vocab", ".", "add_word", "(", "'<unk>'", ")", "\n", "\n", "\n", "\n", "# Adds the words to the vocabulary.", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "vocab", ".", "add_word", "(", "word", ")", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.build_vocab_frogger_turk": [[91, 134], ["xlrd.open_workbook", "xlrd.open_workbook.sheets", "build_vocab_v2.Vocabulary", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "enumerate", "enumerate", "print", "collections.Counter", "range", "build_vocab_v2.Vocabulary.add_word", "build_vocab_v2.Vocabulary.add_word", "actions.append", "nltk.tokenize.word_tokenize", "collections.Counter.update", "collections.Counter.items", "sheet.cell", "line.lower", "sheet.cell"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word"], ["", "def", "build_vocab_frogger_turk", "(", "json", ",", "threshold", ")", ":", "\n", "\n", "#open excel file and create a list of all the words in the rationalizations", "\n", "    ", "wb", "=", "open_workbook", "(", "args", ".", "caption_path", ")", "\n", "text", "=", "\"\"", "\n", "for", "sheet", "in", "wb", ".", "sheets", "(", ")", ":", "\n", "        ", "number_of_rows", "=", "sheet", ".", "nrows", "\n", "number_of_columns", "=", "sheet", ".", "ncols", "\n", "rationalizations", "=", "[", "]", "\n", "items", "=", "[", "]", "\n", "rows", "=", "[", "]", "\n", "lengths", "=", "[", "]", "\n", "max_length", "=", "0", "\n", "counter", "=", "Counter", "(", ")", "\n", "for", "row", "in", "range", "(", "1", ",", "number_of_rows", ")", ":", "\n", "            ", "values", "=", "[", "]", "\n", "line", "=", "sheet", ".", "cell", "(", "row", ",", "4", ")", ".", "value", "\n", "actions", ".", "append", "(", "sheet", ".", "cell", "(", "row", ",", "2", ")", ".", "value", ")", "\n", "tokens", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "line", ".", "lower", "(", ")", ")", "\n", "counter", ".", "update", "(", "tokens", ")", "\n", "", "", "words", "=", "[", "word", "for", "word", ",", "cnt", "in", "counter", ".", "items", "(", ")", "if", "cnt", ">=", "threshold", "]", "\n", "vocab", "=", "Vocabulary", "(", ")", "\n", "vocab", ".", "add_word", "(", "'<pad>'", ")", "\n", "vocab", ".", "add_word", "(", "'<start>'", ")", "\n", "vocab", ".", "add_word", "(", "'<end>'", ")", "\n", "vocab", ".", "add_word", "(", "'<unk>'", ")", "\n", "\n", "\n", "input_vocab", "=", "Vocabulary", "(", ")", "\n", "\n", "input_vocab", ".", "add_word", "(", "'<pad>'", ")", "\n", "input_vocab", ".", "add_word", "(", "'<start>'", ")", "\n", "input_vocab", ".", "add_word", "(", "'<end>'", ")", "\n", "input_vocab", ".", "add_word", "(", "'<unk>'", ")", "\n", "\n", "# Adds the words to the vocabulary", "\n", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "actions", ")", ":", "\n", "        ", "input_vocab", ".", "add_word", "(", "word", ")", "\n", "", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "vocab", ".", "add_word", "(", "word", ")", "\n", "", "print", "(", "vocab", ".", "word2idx", "[", "'again'", "]", ")", "\n", "return", "vocab", ",", "input_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.build_vocab_input": [[135, 156], ["os.walk", "os.walk", "enumerate", "fnmatch.filter", "fnmatch.filter", "input_vocab.add_word", "open", "f.readlines", "enumerate", "open", "f.readlines", "enumerate", "os.path.join", "line.split", "enumerate", "os.path.join", "line.split", "enumerate", "words.append", "words.append", "str", "str"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.Vocabulary.add_word"], ["", "def", "build_vocab_input", "(", "input_vocab", ")", ":", "\n", "    ", "words", "=", "[", "]", "\n", "for", "dirpath", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "'./data/SymbRep(2)/Current'", ")", ":", "\n", "        ", "for", "filename", "in", "fnmatch", ".", "filter", "(", "files", ",", "'*.txt'", ")", ":", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "dirpath", ",", "filename", ")", ")", "as", "f", ":", "\n", "                ", "content", "=", "f", ".", "readlines", "(", ")", "\n", "for", "k", ",", "line", "in", "enumerate", "(", "content", ")", ":", "\n", "                    ", "nums", "=", "line", ".", "split", "(", ")", "\n", "for", "i", ",", "num", "in", "enumerate", "(", "nums", ")", ":", "\n", "                        ", "words", ".", "append", "(", "str", "(", "num", ")", ")", "\n", "", "", "", "", "", "for", "dirpath", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "'./data/SymbRep(2)/Next'", ")", ":", "\n", "        ", "for", "filename", "in", "fnmatch", ".", "filter", "(", "files", ",", "'*.txt'", ")", ":", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "dirpath", ",", "filename", ")", ")", "as", "f", ":", "\n", "                ", "content", "=", "f", ".", "readlines", "(", ")", "\n", "for", "k", ",", "line", "in", "enumerate", "(", "content", ")", ":", "\n", "                    ", "nums", "=", "line", ".", "split", "(", ")", "\n", "for", "i", ",", "num", "in", "enumerate", "(", "nums", ")", ":", "\n", "                        ", "words", ".", "append", "(", "str", "(", "num", ")", ")", "\n", "", "", "", "", "", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "input_vocab", ".", "add_word", "(", "word", ")", "\n", "", "return", "input_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.main": [[157, 172], ["build_vocab_v2.build_vocab_frogger_turk", "build_vocab_v2.build_vocab_input", "print", "print", "print", "print", "open", "pickle.dump", "open", "pickle.dump", "len"], "function", ["home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.build_vocab_frogger_turk", "home.repos.pwc.inspect_result.sultanalnahian_Reverse-Frogger-A3PS.advice generator.build_vocab_v2.build_vocab_input"], ["", "def", "main", "(", "args", ")", ":", "\n", "# vocab = build_vocab(json=args.caption_path,", "\n", "#                     threshold=args.threshold)", "\n", "    ", "vocab", ",", "input_vocab", "=", "build_vocab_frogger_turk", "(", "json", "=", "args", ".", "caption_path", ",", "\n", "threshold", "=", "args", ".", "threshold", ")", "\n", "input_vocab", "=", "build_vocab_input", "(", "input_vocab", ")", "\n", "print", "(", "input_vocab", ")", "\n", "vocab_path", "=", "args", ".", "vocab_path", "\n", "print", "(", "input_vocab", ".", "word2idx", ")", "\n", "with", "open", "(", "vocab_path", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "vocab", ",", "f", ")", "\n", "", "with", "open", "(", "args", ".", "input_vocab_path", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "input_vocab", ",", "f", ")", "\n", "", "print", "(", "\"Total vocabulary size: %d\"", "%", "len", "(", "vocab", ")", ")", "\n", "print", "(", "\"Saved the vocabulary wrapper to '%s'\"", "%", "vocab_path", ")", "\n", "\n"]]}