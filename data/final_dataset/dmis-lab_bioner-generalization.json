{"home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.__init__": [[20, 48], ["re.compile", "evaluate.TextPreprocess.punctuation.replace", "nltk.stem.PorterStemmer", "evaluate.TextPreprocess.load_typo2correction", "re.escape"], "methods", ["home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.load_typo2correction"], ["def", "__init__", "(", "self", ",", "\n", "lowercase", "=", "True", ",", "\n", "remove_punctuation", "=", "True", ",", "\n", "ignore_punctuations", "=", "\"\"", ",", "\n", "stemming", "=", "False", ",", "\n", "typo_path", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ==========\n        typo_path : str\n            path of known typo dictionary\n        \"\"\"", "\n", "self", ".", "lowercase", "=", "lowercase", "\n", "self", ".", "typo_path", "=", "typo_path", "\n", "self", ".", "rmv_puncts", "=", "remove_punctuation", "\n", "self", ".", "punctuation", "=", "punctuation", "\n", "for", "ig_punc", "in", "ignore_punctuations", ":", "\n", "            ", "self", ".", "punctuation", "=", "self", ".", "punctuation", ".", "replace", "(", "ig_punc", ",", "\"\"", ")", "\n", "", "self", ".", "rmv_puncts_regex", "=", "re", ".", "compile", "(", "r'[\\s{}]+'", ".", "format", "(", "re", ".", "escape", "(", "self", ".", "punctuation", ")", ")", ")", "\n", "\n", "self", ".", "stemming", "=", "stemming", "\n", "if", "self", ".", "stemming", ":", "\n", "            ", "self", ".", "stemmer", "=", "PorterStemmer", "(", ")", "\n", "\n", "", "if", "typo_path", ":", "\n", "            ", "self", ".", "typo2correction", "=", "self", ".", "load_typo2correction", "(", "typo_path", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "typo2correction", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.load_typo2correction": [[49, 60], ["open", "f.readlines", "line.strip", "line.strip.split", "len"], "methods", ["None"], ["", "", "def", "load_typo2correction", "(", "self", ",", "typo_path", ")", ":", "\n", "        ", "typo2correction", "=", "{", "}", "\n", "with", "open", "(", "typo_path", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "s", "=", "line", ".", "strip", "(", ")", "\n", "tokens", "=", "s", ".", "split", "(", "\"||\"", ")", "\n", "value", "=", "\"\"", "if", "len", "(", "tokens", ")", "==", "1", "else", "tokens", "[", "1", "]", "\n", "typo2correction", "[", "tokens", "[", "0", "]", "]", "=", "value", "\n", "\n", "", "", "return", "typo2correction", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.remove_punctuation": [[61, 66], ["evaluate.TextPreprocess.rmv_puncts_regex.split"], "methods", ["None"], ["", "def", "remove_punctuation", "(", "self", ",", "phrase", ")", ":", "\n", "        ", "phrase", "=", "self", ".", "rmv_puncts_regex", ".", "split", "(", "phrase", ")", "\n", "phrase", "=", "' '", ".", "join", "(", "phrase", ")", ".", "strip", "(", ")", "\n", "\n", "return", "phrase", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.correct_spelling": [[67, 78], ["phrase.strip.strip.split", "phrase.strip.strip.strip", "evaluate.TextPreprocess.typo2correction.keys"], "methods", ["None"], ["", "def", "correct_spelling", "(", "self", ",", "phrase", ")", ":", "\n", "        ", "phrase_tokens", "=", "phrase", ".", "split", "(", ")", "\n", "phrase", "=", "\"\"", "\n", "\n", "for", "phrase_token", "in", "phrase_tokens", ":", "\n", "            ", "if", "phrase_token", "in", "self", ".", "typo2correction", ".", "keys", "(", ")", ":", "\n", "                ", "phrase_token", "=", "self", ".", "typo2correction", "[", "phrase_token", "]", "\n", "", "phrase", "+=", "phrase_token", "+", "\" \"", "\n", "\n", "", "phrase", "=", "phrase", ".", "strip", "(", ")", "\n", "return", "phrase", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.stem_tokens": [[80, 88], ["nltk.tokenize.word_tokenize", "out.append", "evaluate.TextPreprocess.stemmer.stem"], "methods", ["None"], ["", "def", "stem_tokens", "(", "self", ",", "text", ")", ":", "\n", "        ", "words", "=", "word_tokenize", "(", "text", ")", "\n", "\n", "out", "=", "[", "]", "\n", "for", "w", "in", "words", ":", "\n", "            ", "out", ".", "append", "(", "self", ".", "stemmer", ".", "stem", "(", "w", ")", ")", "\n", "", "out", "=", "\" \"", ".", "join", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.run": [[89, 105], ["evaluate.TextPreprocess.strip", "evaluate.TextPreprocess.lower", "evaluate.TextPreprocess.correct_spelling", "evaluate.TextPreprocess.remove_punctuation", "evaluate.TextPreprocess.stem_tokens"], "methods", ["home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.correct_spelling", "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.remove_punctuation", "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.stem_tokens"], ["", "def", "run", "(", "self", ",", "text", ")", ":", "\n", "        ", "if", "self", ".", "lowercase", ":", "\n", "            ", "text", "=", "text", ".", "lower", "(", ")", "\n", "\n", "", "if", "self", ".", "typo_path", ":", "\n", "            ", "text", "=", "self", ".", "correct_spelling", "(", "text", ")", "\n", "\n", "", "if", "self", ".", "rmv_puncts", ":", "\n", "            ", "text", "=", "self", ".", "remove_punctuation", "(", "text", ")", "\n", "\n", "", "if", "self", ".", "stemming", ":", "\n", "            ", "text", "=", "self", ".", "stem_tokens", "(", "text", ")", "\n", "\n", "", "text", "=", "text", ".", "strip", "(", ")", "\n", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.add_space": [[106, 123], ["result.split"], "methods", ["None"], ["", "def", "add_space", "(", "self", ",", "s", ")", ":", "\n", "        ", "if", "not", "s", ":", "return", "\"\"", "\n", "result", "=", "s", "[", "0", "]", "\n", "prev_c", "=", "s", "[", "0", "]", "\n", "for", "c", "in", "s", "[", "1", ":", "]", ":", "\n", "            ", "if", "prev_c", "in", "punctuation", ":", "\n", "                ", "result", "+=", "\" \"", "\n", "result", "+=", "c", "\n", "", "else", ":", "\n", "                ", "if", "c", "in", "punctuation", ":", "\n", "                    ", "result", "+=", "\" \"", "\n", "result", "+=", "c", "\n", "", "else", ":", "\n", "                    ", "result", "+=", "c", "\n", "", "", "prev_c", "=", "c", "\n", "", "result", "=", "\" \"", ".", "join", "(", "result", ".", "split", "(", ")", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.get_single_cuis": [[124, 141], ["enumerate", "type", "type", "cc.split", "cui_list.append", "cc.split", "cui_list.append", "cui_list.append"], "function", ["None"], ["", "", "def", "get_single_cuis", "(", "cuis", ")", ":", "\n", "    ", "if", "type", "(", "cuis", ")", "==", "str", ":", "cuis", "=", "[", "cuis", "]", "\n", "elif", "type", "(", "cuis", ")", "==", "list", ":", "cuis", "=", "cuis", "\n", "\n", "cui_list", "=", "[", "]", "\n", "for", "c_i", ",", "cc", "in", "enumerate", "(", "cuis", ")", ":", "\n", "        ", "if", "'|'", "in", "cc", ":", "\n", "            ", "cs", "=", "cc", ".", "split", "(", "'|'", ")", "\n", "for", "c", "in", "cs", ":", "\n", "                ", "cui_list", ".", "append", "(", "c", ")", "\n", "", "", "elif", "'+'", "in", "cc", ":", "# For NCBI", "\n", "            ", "cs", "=", "cc", ".", "split", "(", "'+'", ")", "\n", "for", "c", "in", "cs", ":", "\n", "                ", "cui_list", ".", "append", "(", "c", ")", "\n", "", "", "else", ":", "\n", "            ", "cui_list", ".", "append", "(", "cc", ")", "\n", "", "", "return", "cui_list", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.update": [[143, 168], ["preprocessor.run", "ValueError", "preprocessor.run", "sum", "preprocessor.run", "sum", "evaluate.get_single_cuis", "evaluate.get_single_cuis"], "function", ["home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.run", "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.run", "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.run", "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.get_single_cuis", "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.get_single_cuis"], ["", "def", "update", "(", "spl", ",", "preprocessor", ",", "entity_str", ",", "index_tmp", ",", "num_mem", ",", "num_syn", ",", "num_con", ",", "mention_dictionary", ",", "cui_dictionary", ",", "test_splits", ",", "cuis", ")", ":", "\n", "    ", "if", "spl", "==", "'Mem'", ":", "\n", "        ", "if", "(", "preprocessor", ".", "run", "(", "entity_str", ")", "in", "mention_dictionary", ")", ":", "\n", "            ", "num_mem", "+=", "1", "\n", "", "else", ":", "\n", "            ", "for", "j", "in", "index_tmp", ":", "\n", "                ", "test_splits", "[", "spl", "]", "[", "j", "]", "=", "'O'", "\n", "", "", "", "elif", "spl", "==", "'Syn'", ":", "\n", "        ", "if", "(", "preprocessor", ".", "run", "(", "entity_str", ")", "not", "in", "mention_dictionary", ")", "and", "sum", "(", "[", "1", "if", "c", "in", "cui_dictionary", "else", "0", "for", "c", "in", "get_single_cuis", "(", "cuis", "[", "index_tmp", "[", "0", "]", "]", ")", "]", ")", ">", "0", ":", "\n", "            ", "num_syn", "+=", "1", "\n", "", "else", ":", "\n", "            ", "for", "j", "in", "index_tmp", ":", "\n", "                ", "test_splits", "[", "spl", "]", "[", "j", "]", "=", "'O'", "\n", "", "", "", "elif", "spl", "==", "'Con'", ":", "\n", "        ", "if", "(", "(", "preprocessor", ".", "run", "(", "entity_str", ")", "not", "in", "mention_dictionary", ")", "and", "sum", "(", "[", "1", "if", "c", "in", "cui_dictionary", "else", "0", "for", "c", "in", "get_single_cuis", "(", "cuis", "[", "index_tmp", "[", "0", "]", "]", ")", "]", ")", "==", "0", ")", ":", "\n", "            ", "num_con", "+=", "1", "\n", "", "else", ":", "\n", "            ", "for", "j", "in", "index_tmp", ":", "\n", "                ", "test_splits", "[", "spl", "]", "[", "j", "]", "=", "'O'", "\n", "", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid name: {}.\"", ".", "format", "(", "spl", ")", ")", "\n", "\n", "", "return", "num_mem", ",", "num_syn", ",", "num_con", ",", "test_splits", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.partition": [[169, 219], ["list", "print", "test_splits.keys", "tqdm.tqdm", "evaluate.update", "entity_tmp.append", "index_tmp.append", "entity_tmp.append", "index_tmp.append", "entity_tmp.append", "index_tmp.append", "evaluate.update"], "function", ["home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.update", "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.update"], ["", "def", "partition", "(", "preprocessor", ",", "mention_dictionary", ",", "cui_dictionary", ",", "test_splits", ",", "tokens", ",", "cuis", ")", ":", "\n", "    ", "num_mem", "=", "num_syn", "=", "num_con", "=", "0", "\n", "for", "spl", "in", "list", "(", "test_splits", ".", "keys", "(", ")", ")", ":", "\n", "        ", "if", "spl", "==", "'Overall'", ":", "\n", "            ", "continue", "\n", "\n", "# init", "\n", "", "entity_tmp", "=", "[", "]", "\n", "index_tmp", "=", "[", "]", "\n", "inside_entity", "=", "False", "\n", "i", "=", "-", "1", "\n", "for", "pred", "in", "tqdm", "(", "test_splits", "[", "spl", "]", ")", ":", "\n", "            ", "i", "+=", "1", "\n", "\n", "if", "pred", "[", "0", "]", "==", "'B'", ":", "\n", "                ", "if", "inside_entity", ":", "\n", "                    ", "assert", "cuis", "[", "index_tmp", "[", "0", "]", "]", "!=", "'-'", "\n", "entity_str", "=", "' '", ".", "join", "(", "entity_tmp", ")", "\n", "\n", "num_mem", ",", "num_syn", ",", "num_con", ",", "test_splits", "=", "update", "(", "spl", ",", "preprocessor", ",", "entity_str", ",", "index_tmp", ",", "num_mem", ",", "num_syn", ",", "num_con", ",", "mention_dictionary", ",", "cui_dictionary", ",", "test_splits", ",", "cuis", ")", "\n", "# init", "\n", "inside_entity", "=", "False", "\n", "entity_tmp", "=", "[", "]", "\n", "index_tmp", "=", "[", "]", "\n", "\n", "inside_entity", "=", "True", "\n", "entity_tmp", ".", "append", "(", "tokens", "[", "i", "]", ")", "\n", "index_tmp", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "                    ", "inside_entity", "=", "True", "\n", "entity_tmp", ".", "append", "(", "tokens", "[", "i", "]", ")", "\n", "index_tmp", ".", "append", "(", "i", ")", "\n", "", "", "elif", "pred", "[", "0", "]", "==", "'I'", ":", "\n", "                ", "inside_entity", "=", "True", "\n", "entity_tmp", ".", "append", "(", "tokens", "[", "i", "]", ")", "\n", "index_tmp", ".", "append", "(", "i", ")", "\n", "", "elif", "pred", "[", "0", "]", "==", "'O'", ":", "\n", "                ", "if", "inside_entity", ":", "\n", "                    ", "assert", "cuis", "[", "index_tmp", "[", "0", "]", "]", "!=", "'-'", "\n", "entity_str", "=", "' '", ".", "join", "(", "entity_tmp", ")", "\n", "\n", "num_mem", ",", "num_syn", ",", "num_con", ",", "test_splits", "=", "update", "(", "spl", ",", "preprocessor", ",", "entity_str", ",", "index_tmp", ",", "num_mem", ",", "num_syn", ",", "num_con", ",", "mention_dictionary", ",", "cui_dictionary", ",", "test_splits", ",", "cuis", ")", "\n", "# init", "\n", "inside_entity", "=", "False", "\n", "entity_tmp", "=", "[", "]", "\n", "index_tmp", "=", "[", "]", "\n", "", "", "", "", "print", "(", "\"Splits | Mem: {}, Syn: {}, Zero: {}\"", ".", "format", "(", "num_mem", ",", "num_syn", ",", "num_con", ")", ")", "\n", "return", "test_splits", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.print_score": [[220, 232], ["print", "seqeval.metrics.precision_score", "seqeval.metrics.recall_score", "seqeval.metrics.f1_score", "print", "seqeval.metrics.recall_score", "print"], "function", ["None"], ["", "def", "print_score", "(", "SPLITS", ",", "test_splits", ",", "predictions", ")", ":", "\n", "    ", "print", "(", "\"\\n--Evaluation--\"", ")", "\n", "\n", "for", "spl", "in", "SPLITS", ":", "\n", "        ", "if", "spl", "==", "'Overall'", ":", "\n", "            ", "p", "=", "precision_score", "(", "test_splits", "[", "'Overall'", "]", ",", "predictions", ")", "\n", "r", "=", "recall_score", "(", "test_splits", "[", "'Overall'", "]", ",", "predictions", ")", "\n", "f1", "=", "f1_score", "(", "test_splits", "[", "'Overall'", "]", ",", "predictions", ")", "\n", "print", "(", "\"{} {:2.1f}\\t{:2.1f}\\t{:2.1f}\"", ".", "format", "(", "spl", ",", "p", "*", "100", ",", "r", "*", "100", ",", "f1", "*", "100", ")", ")", "\n", "", "else", ":", "\n", "            ", "r", "=", "recall_score", "(", "test_splits", "[", "spl", "]", ",", "predictions", ")", "\n", "print", "(", "\"{} {:2.1f}\"", ".", "format", "(", "spl", ",", "r", "*", "100", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.main": [[233, 305], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "evaluate.TextPreprocess", "print", "open", "open.readlines", "open", "open.readlines", "open", "open.readlines", "enumerate", "evaluate.partition", "evaluate.print_score", "open", "open", "len", "len", "zip", "cuis.append", "predictions.append", "tokens.append", "evaluate.TextPreprocess.run", "line.rstrip", "set", "pred.split", "pred.split", "pred.split", "label.split", "gold_cuis[].split", "label.split", "test_splits[].append", "line.rstrip", "open.readlines", "open.readlines", "gold_cuis[].split"], "function", ["home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.partition", "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.print_score", "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.None.evaluate.TextPreprocess.run"], ["", "", "", "def", "main", "(", ")", ":", "\n", "# arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--mention_dictionary'", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--cui_dictionary'", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--gold_labels'", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--gold_cuis'", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--predictions'", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "preprocessor", "=", "TextPreprocess", "(", ")", "\n", "\n", "# Load dictionaries", "\n", "with", "open", "(", "args", ".", "mention_dictionary", ")", "as", "g", ":", "\n", "        ", "mention_dictionary", "=", "[", "preprocessor", ".", "run", "(", "line", ".", "rstrip", "(", "'\\n'", ")", ")", "for", "line", "in", "g", ".", "readlines", "(", ")", "]", "\n", "", "with", "open", "(", "args", ".", "cui_dictionary", ")", "as", "g", ":", "\n", "        ", "cui_dictionary", "=", "[", "line", ".", "rstrip", "(", "'\\n'", ")", "for", "line", "in", "g", ".", "readlines", "(", ")", "]", "\n", "\n", "", "print", "(", "len", "(", "mention_dictionary", ")", ",", "len", "(", "set", "(", "mention_dictionary", ")", ")", ")", "\n", "\n", "# Load model predictions", "\n", "f", "=", "open", "(", "args", ".", "predictions", ")", "\n", "preds", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "# Load test data", "\n", "g", "=", "open", "(", "args", ".", "gold_labels", ")", "\n", "gold_labels", "=", "g", ".", "readlines", "(", ")", "\n", "g", "=", "open", "(", "args", ".", "gold_cuis", ")", "\n", "gold_cuis", "=", "g", ".", "readlines", "(", ")", "\n", "\n", "# Initialize", "\n", "SPLITS", "=", "[", "'Overall'", ",", "'Mem'", ",", "'Syn'", ",", "'Con'", "]", "\n", "test_splits", "=", "{", "}", "\n", "for", "l", "in", "SPLITS", ":", "\n", "        ", "test_splits", "[", "l", "]", "=", "[", "]", "\n", "\n", "#", "\n", "", "predictions", "=", "[", "]", "\n", "tokens", "=", "[", "]", "\n", "cuis", "=", "[", "]", "\n", "for", "i", ",", "(", "pred", ",", "label", ")", "in", "enumerate", "(", "zip", "(", "preds", ",", "gold_labels", ")", ")", ":", "\n", "        ", "if", "not", "pred", ".", "split", "(", ")", ":", "continue", "\n", "\n", "p_token", "=", "pred", ".", "split", "(", ")", "[", "0", "]", "\n", "p_label", "=", "pred", ".", "split", "(", ")", "[", "1", "]", "\n", "l_token", "=", "label", ".", "split", "(", ")", "[", "0", "]", "\n", "c_token", "=", "gold_cuis", "[", "i", "]", ".", "split", "(", ")", "[", "0", "]", "\n", "assert", "p_token", "==", "l_token", "\n", "assert", "c_token", "==", "l_token", "\n", "\n", "l_label", "=", "label", ".", "split", "(", ")", "[", "1", "]", "\n", "\n", "# The seqeval framework requires entity types for entity-level NER evaluation.", "\n", "# Assign 'MISC' to all annotations if the data is of a single type and the annotations do not specify an entity type.", "\n", "if", "p_label", "==", "'B'", "or", "p_label", "==", "'I'", ":", "\n", "            ", "p_label", "=", "p_label", "+", "'-MISC'", "\n", "", "if", "l_label", "==", "'B'", "or", "l_label", "==", "'I'", ":", "\n", "            ", "l_label", "=", "l_label", "+", "'-MISC'", "\n", "\n", "", "cuis", ".", "append", "(", "gold_cuis", "[", "i", "]", ".", "split", "(", ")", "[", "1", "]", ")", "\n", "predictions", ".", "append", "(", "p_label", ")", "\n", "tokens", ".", "append", "(", "p_token", ")", "\n", "for", "spl", "in", "SPLITS", ":", "\n", "            ", "test_splits", "[", "spl", "]", ".", "append", "(", "l_label", ")", "\n", "\n", "# Partition benchmarks", "\n", "", "", "test_splits", "=", "partition", "(", "preprocessor", ",", "mention_dictionary", ",", "cui_dictionary", ",", "test_splits", ",", "tokens", ",", "cuis", ")", "\n", "\n", "# Evaluation", "\n", "print_score", "(", "SPLITS", ",", "test_splits", ",", "predictions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.resources.convert.convert": [[5, 14], ["open", "f.readlines", "open", "g.write", "os.path.join", "os.path.join", "line.split"], "function", ["None"], ["def", "convert", "(", "data", ",", "file_name", ",", "save_name", ")", ":", "\n", "    ", "new", "=", "[", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data", ",", "file_name", ")", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "new", "=", "[", "line", ".", "split", "(", "'\\t'", ")", "[", "0", "]", "for", "line", "in", "lines", "]", "\n", "\n", "", "new", "=", "'\\n'", ".", "join", "(", "new", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data", ",", "save_name", ")", ",", "'w'", ")", "as", "g", ":", "\n", "        ", "g", ".", "write", "(", "new", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.training.run_ner.main": [[96, 320], ["transformers.HfArgumentParser", "logging.basicConfig", "logger.warning", "logger.info", "transformers.set_seed", "utils_ner.get_labels", "len", "transformers.AutoConfig.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModelForTokenClassification.from_pretrained", "transformers.Trainer", "sys.argv[].endswith", "transformers.HfArgumentParser.parse_json_file", "transformers.HfArgumentParser.parse_args_into_dataclasses", "os.path.exists", "os.listdir", "ValueError", "bool", "utils_ner.NerDataset", "utils_ner.NerDataset", "numpy.argmax", "range", "run_ner.main.align_predictions"], "function", ["home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.training.utils_ner.get_labels"], ["", "def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ")", ")", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "2", "and", "sys", ".", "argv", "[", "1", "]", ".", "endswith", "(", "\".json\"", ")", ":", "\n", "# If we pass only one argument to the script and it's the path to a json file,", "\n", "# let's parse it to get our arguments.", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_json_file", "(", "json_file", "=", "os", ".", "path", ".", "abspath", "(", "sys", ".", "argv", "[", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "", "if", "(", "\n", "os", ".", "path", ".", "exists", "(", "training_args", ".", "output_dir", ")", "\n", "and", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", "\n", "and", "training_args", ".", "do_train", "\n", "and", "not", "training_args", ".", "overwrite_output_dir", "\n", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "\n", "# Setup logging", "\n", "", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "level", "=", "logging", ".", "INFO", "if", "training_args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "else", "logging", ".", "WARN", ",", "\n", ")", "\n", "logger", ".", "warning", "(", "\n", "\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\"", ",", "\n", "training_args", ".", "local_rank", ",", "\n", "training_args", ".", "device", ",", "\n", "training_args", ".", "n_gpu", ",", "\n", "bool", "(", "training_args", ".", "local_rank", "!=", "-", "1", ")", ",", "\n", "training_args", ".", "fp16", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "# Set seed", "\n", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# Prepare CONLL-2003 task", "\n", "labels", "=", "get_labels", "(", "data_args", ".", "labels", ")", "\n", "label_map", ":", "Dict", "[", "int", ",", "str", "]", "=", "{", "i", ":", "label", "for", "i", ",", "label", "in", "enumerate", "(", "labels", ")", "}", "\n", "num_labels", "=", "len", "(", "labels", ")", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "model_args", ".", "config_name", "if", "model_args", ".", "config_name", "else", "model_args", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "id2label", "=", "label_map", ",", "\n", "label2id", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "labels", ")", "}", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "model_args", ".", "tokenizer_name", "if", "model_args", ".", "tokenizer_name", "else", "model_args", ".", "model_name_or_path", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "use_fast", "=", "model_args", ".", "use_fast", ",", "\n", ")", "\n", "model", "=", "AutoModelForTokenClassification", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "'''\n    model_to_save = AutoModel.from_pretrained(\n        model_args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n        config=config,\n        cache_dir=model_args.cache_dir,\n    )\n    model_to_save.save_pretrained(training_args.output_dir)\n    tokenizer.save_pretrained(training_args.output_dir)\n    import pdb; pdb.set_trace()\n    '''", "\n", "\n", "# Get datasets", "\n", "train_dataset", "=", "(", "\n", "NerDataset", "(", "\n", "data_dir", "=", "data_args", ".", "data_dir", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "labels", "=", "labels", ",", "\n", "model_type", "=", "config", ".", "model_type", ",", "\n", "max_seq_length", "=", "data_args", ".", "max_seq_length", ",", "\n", "overwrite_cache", "=", "data_args", ".", "overwrite_cache", ",", "\n", "mode", "=", "Split", ".", "train", ",", "\n", ")", "\n", "if", "training_args", ".", "do_train", "\n", "else", "None", "\n", ")", "\n", "eval_dataset", "=", "(", "\n", "NerDataset", "(", "\n", "data_dir", "=", "data_args", ".", "data_dir", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "labels", "=", "labels", ",", "\n", "model_type", "=", "config", ".", "model_type", ",", "\n", "max_seq_length", "=", "data_args", ".", "max_seq_length", ",", "\n", "overwrite_cache", "=", "data_args", ".", "overwrite_cache", ",", "\n", "mode", "=", "Split", ".", "dev", ",", "\n", ")", "\n", "if", "training_args", ".", "do_eval", "\n", "else", "None", "\n", ")", "\n", "\n", "def", "align_predictions", "(", "predictions", ":", "np", ".", "ndarray", ",", "label_ids", ":", "np", ".", "ndarray", ")", "->", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "preds", "=", "np", ".", "argmax", "(", "predictions", ",", "axis", "=", "2", ")", "\n", "\n", "batch_size", ",", "seq_len", "=", "preds", ".", "shape", "\n", "\n", "out_label_list", "=", "[", "[", "]", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "preds_list", "=", "[", "[", "]", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "seq_len", ")", ":", "\n", "                ", "if", "label_ids", "[", "i", ",", "j", "]", "!=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "ignore_index", ":", "\n", "                    ", "out_label_list", "[", "i", "]", ".", "append", "(", "label_map", "[", "label_ids", "[", "i", "]", "[", "j", "]", "]", ")", "\n", "preds_list", "[", "i", "]", ".", "append", "(", "label_map", "[", "preds", "[", "i", "]", "[", "j", "]", "]", ")", "\n", "\n", "", "", "", "return", "preds_list", ",", "out_label_list", "\n", "\n", "", "def", "compute_metrics", "(", "p", ":", "EvalPrediction", ")", "->", "Dict", ":", "\n", "        ", "preds_list", ",", "out_label_list", "=", "align_predictions", "(", "p", ".", "predictions", ",", "p", ".", "label_ids", ")", "\n", "\n", "return", "{", "\n", "\"precision\"", ":", "precision_score", "(", "out_label_list", ",", "preds_list", ")", ",", "\n", "\"recall\"", ":", "recall_score", "(", "out_label_list", ",", "preds_list", ")", ",", "\n", "\"f1\"", ":", "f1_score", "(", "out_label_list", ",", "preds_list", ")", ",", "\n", "}", "\n", "\n", "# Initialize our Trainer", "\n", "", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", ",", "\n", "eval_dataset", "=", "eval_dataset", ",", "\n", "compute_metrics", "=", "compute_metrics", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "trainer", ".", "train", "(", "\n", "model_path", "=", "model_args", ".", "model_name_or_path", "if", "os", ".", "path", ".", "isdir", "(", "model_args", ".", "model_name_or_path", ")", "else", "None", "\n", ")", "\n", "trainer", ".", "save_model", "(", ")", "\n", "# For convenience, we also re-save the tokenizer to the same directory,", "\n", "# so that you can share your model easily on huggingface.co/models =)", "\n", "if", "trainer", ".", "is_world_master", "(", ")", ":", "\n", "            ", "tokenizer", ".", "save_pretrained", "(", "training_args", ".", "output_dir", ")", "\n", "\n", "# Evaluation", "\n", "", "", "results", "=", "{", "}", "\n", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "result", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"eval_results.txt\"", ")", "\n", "if", "trainer", ".", "is_world_master", "(", ")", ":", "\n", "            ", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", ",", "value", "in", "result", ".", "items", "(", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "value", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "value", ")", ")", "\n", "\n", "", "", "results", ".", "update", "(", "result", ")", "\n", "\n", "\n", "# Predict", "\n", "", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "test_dataset", "=", "NerDataset", "(", "\n", "data_dir", "=", "data_args", ".", "data_dir", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "labels", "=", "labels", ",", "\n", "model_type", "=", "config", ".", "model_type", ",", "\n", "max_seq_length", "=", "data_args", ".", "max_seq_length", ",", "\n", "overwrite_cache", "=", "data_args", ".", "overwrite_cache", ",", "\n", "mode", "=", "Split", ".", "test", ",", "\n", ")", "\n", "\n", "predictions", ",", "label_ids", ",", "metrics", "=", "trainer", ".", "predict", "(", "test_dataset", ")", "\n", "preds_list", ",", "_", "=", "align_predictions", "(", "predictions", ",", "label_ids", ")", "\n", "\n", "# Save predictions", "\n", "output_test_results_file", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"test_results.txt\"", ")", "\n", "if", "trainer", ".", "is_world_master", "(", ")", ":", "\n", "            ", "with", "open", "(", "output_test_results_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                ", "logger", ".", "info", "(", "\"***** Test results *****\"", ")", "\n", "for", "key", ",", "value", "in", "metrics", ".", "items", "(", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "value", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "value", ")", ")", "\n", "\n", "\n", "", "", "", "output_test_predictions_file", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"test_predictions.txt\"", ")", "\n", "if", "trainer", ".", "is_world_master", "(", ")", ":", "\n", "            ", "with", "open", "(", "output_test_predictions_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_args", ".", "data_dir", ",", "\"test.txt\"", ")", ",", "\"r\"", ")", "as", "f", ":", "\n", "                    ", "example_id", "=", "0", "\n", "for", "line", "in", "f", ":", "\n", "                        ", "if", "line", ".", "startswith", "(", "\"-DOCSTART-\"", ")", "or", "line", "==", "\"\"", "or", "line", "==", "\"\\n\"", ":", "\n", "                            ", "writer", ".", "write", "(", "line", ")", "\n", "if", "not", "preds_list", "[", "example_id", "]", ":", "\n", "                                ", "example_id", "+=", "1", "\n", "", "", "elif", "preds_list", "[", "example_id", "]", ":", "\n", "                            ", "entity_label", "=", "preds_list", "[", "example_id", "]", ".", "pop", "(", "0", ")", "\n", "if", "entity_label", "==", "'O'", ":", "\n", "                                ", "output_line", "=", "line", ".", "split", "(", ")", "[", "0", "]", "+", "\" \"", "+", "entity_label", "+", "\"\\n\"", "\n", "", "else", ":", "\n", "                                ", "output_line", "=", "line", ".", "split", "(", ")", "[", "0", "]", "+", "\" \"", "+", "entity_label", "[", "0", "]", "+", "\"\\n\"", "\n", "# output_line = line.split()[0] + \" \" + preds_list[example_id].pop(0) + \"\\n\"", "\n", "", "writer", ".", "write", "(", "output_line", ")", "\n", "", "else", ":", "\n", "                            ", "logger", ".", "warning", "(", "\n", "\"Maximum sequence length exceeded: No prediction for '%s'.\"", ",", "line", ".", "split", "(", ")", "[", "0", "]", "\n", ")", "\n", "\n", "\n", "", "", "", "", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.training.run_ner._mp_fn": [[322, 325], ["run_ner.main"], "function", ["home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.training.run_ner.main"], ["", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "    ", "main", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.training.utils_ner.read_examples_from_file": [[234, 265], ["isinstance", "os.path.join", "open", "examples.append", "line.startswith", "line.split", "words.append", "utils_ner.InputExample", "examples.append", "len", "splits[].replace", "labels.append", "utils_ner.InputExample", "labels.append", "labels.append"], "function", ["None"], ["", "", "", "def", "read_examples_from_file", "(", "data_dir", ",", "mode", ":", "Union", "[", "Split", ",", "str", "]", ")", "->", "List", "[", "InputExample", "]", ":", "\n", "    ", "if", "isinstance", "(", "mode", ",", "Split", ")", ":", "\n", "        ", "mode", "=", "mode", ".", "value", "\n", "", "file_path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "f\"{mode}.txt\"", ")", "\n", "guid_index", "=", "1", "\n", "examples", "=", "[", "]", "\n", "with", "open", "(", "file_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "words", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "            ", "if", "line", ".", "startswith", "(", "\"-DOCSTART-\"", ")", "or", "line", "==", "\"\"", "or", "line", "==", "\"\\n\"", ":", "\n", "                ", "if", "words", ":", "\n", "                    ", "examples", ".", "append", "(", "InputExample", "(", "guid", "=", "f\"{mode}-{guid_index}\"", ",", "words", "=", "words", ",", "labels", "=", "labels", ")", ")", "\n", "guid_index", "+=", "1", "\n", "words", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "", "", "else", ":", "\n", "                ", "splits", "=", "line", ".", "split", "(", "\" \"", ")", "\n", "words", ".", "append", "(", "splits", "[", "0", "]", ")", "\n", "if", "len", "(", "splits", ")", ">", "1", ":", "\n", "                    ", "splits_replace", "=", "splits", "[", "-", "1", "]", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", "\n", "if", "splits_replace", "==", "'O'", ":", "\n", "                        ", "labels", ".", "append", "(", "splits_replace", ")", "\n", "", "else", ":", "\n", "                        ", "labels", ".", "append", "(", "splits_replace", "+", "\"-bio\"", ")", "\n", "", "", "else", ":", "\n", "# Examples could have no label for mode = \"test\"", "\n", "                    ", "labels", ".", "append", "(", "\"O\"", ")", "\n", "", "", "", "if", "words", ":", "\n", "            ", "examples", ".", "append", "(", "InputExample", "(", "guid", "=", "f\"{mode}-{guid_index}\"", ",", "words", "=", "words", ",", "labels", "=", "labels", ")", ")", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.training.utils_ner.convert_examples_to_features": [[267, 392], ["enumerate", "zip", "tokenizer.num_special_tokens_to_add", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "logger.info", "tokenizer.tokenize", "len", "len", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "utils_ner.InputFeatures", "len", "len", "tokens.extend", "label_ids.extend", "str", "str", "str", "str", "str", "len"], "function", ["None"], ["", "def", "convert_examples_to_features", "(", "\n", "examples", ":", "List", "[", "InputExample", "]", ",", "\n", "label_list", ":", "List", "[", "str", "]", ",", "\n", "max_seq_length", ":", "int", ",", "\n", "tokenizer", ":", "PreTrainedTokenizer", ",", "\n", "cls_token_at_end", "=", "False", ",", "\n", "cls_token", "=", "\"[CLS]\"", ",", "\n", "cls_token_segment_id", "=", "1", ",", "\n", "sep_token", "=", "\"[SEP]\"", ",", "\n", "sep_token_extra", "=", "False", ",", "\n", "pad_on_left", "=", "False", ",", "\n", "pad_token", "=", "0", ",", "\n", "pad_token_segment_id", "=", "0", ",", "\n", "pad_token_label_id", "=", "-", "100", ",", "\n", "sequence_a_segment_id", "=", "0", ",", "\n", "mask_padding_with_zero", "=", "True", ",", "\n", ")", "->", "List", "[", "InputFeatures", "]", ":", "\n", "    ", "\"\"\" Loads a data file into a list of `InputFeatures`\n        `cls_token_at_end` define the location of the CLS token:\n            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n    \"\"\"", "\n", "# TODO clean up all this to leverage built-in features of tokenizers", "\n", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "if", "ex_index", "%", "10_000", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Writing example %d of %d\"", ",", "ex_index", ",", "len", "(", "examples", ")", ")", "\n", "\n", "", "tokens", "=", "[", "]", "\n", "label_ids", "=", "[", "]", "\n", "for", "word", ",", "label", "in", "zip", "(", "example", ".", "words", ",", "example", ".", "labels", ")", ":", "\n", "            ", "word_tokens", "=", "tokenizer", ".", "tokenize", "(", "word", ")", "\n", "\n", "# bert-base-multilingual-cased sometimes output \"nothing ([]) when calling tokenize with just a space.", "\n", "if", "len", "(", "word_tokens", ")", ">", "0", ":", "\n", "                ", "tokens", ".", "extend", "(", "word_tokens", ")", "\n", "# Use the real label id for the first token of the word, and padding ids for the remaining tokens", "\n", "label_ids", ".", "extend", "(", "[", "label_map", "[", "label", "]", "]", "+", "[", "pad_token_label_id", "]", "*", "(", "len", "(", "word_tokens", ")", "-", "1", ")", ")", "\n", "\n", "# Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.", "\n", "", "", "special_tokens_count", "=", "tokenizer", ".", "num_special_tokens_to_add", "(", ")", "\n", "if", "len", "(", "tokens", ")", ">", "max_seq_length", "-", "special_tokens_count", ":", "\n", "            ", "tokens", "=", "tokens", "[", ":", "(", "max_seq_length", "-", "special_tokens_count", ")", "]", "\n", "label_ids", "=", "label_ids", "[", ":", "(", "max_seq_length", "-", "special_tokens_count", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids:   0   0   0   0  0     0   0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambiguously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "tokens", "+=", "[", "sep_token", "]", "\n", "label_ids", "+=", "[", "pad_token_label_id", "]", "\n", "if", "sep_token_extra", ":", "\n", "# roberta uses an extra separator b/w pairs of sentences", "\n", "            ", "tokens", "+=", "[", "sep_token", "]", "\n", "label_ids", "+=", "[", "pad_token_label_id", "]", "\n", "", "segment_ids", "=", "[", "sequence_a_segment_id", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "if", "cls_token_at_end", ":", "\n", "            ", "tokens", "+=", "[", "cls_token", "]", "\n", "label_ids", "+=", "[", "pad_token_label_id", "]", "\n", "segment_ids", "+=", "[", "cls_token_segment_id", "]", "\n", "", "else", ":", "\n", "            ", "tokens", "=", "[", "cls_token", "]", "+", "tokens", "\n", "label_ids", "=", "[", "pad_token_label_id", "]", "+", "label_ids", "\n", "segment_ids", "=", "[", "cls_token_segment_id", "]", "+", "segment_ids", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "if", "mask_padding_with_zero", "else", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding_length", "=", "max_seq_length", "-", "len", "(", "input_ids", ")", "\n", "if", "pad_on_left", ":", "\n", "            ", "input_ids", "=", "(", "[", "pad_token", "]", "*", "padding_length", ")", "+", "input_ids", "\n", "input_mask", "=", "(", "[", "0", "if", "mask_padding_with_zero", "else", "1", "]", "*", "padding_length", ")", "+", "input_mask", "\n", "segment_ids", "=", "(", "[", "pad_token_segment_id", "]", "*", "padding_length", ")", "+", "segment_ids", "\n", "label_ids", "=", "(", "[", "pad_token_label_id", "]", "*", "padding_length", ")", "+", "label_ids", "\n", "", "else", ":", "\n", "            ", "input_ids", "+=", "[", "pad_token", "]", "*", "padding_length", "\n", "input_mask", "+=", "[", "0", "if", "mask_padding_with_zero", "else", "1", "]", "*", "padding_length", "\n", "segment_ids", "+=", "[", "pad_token_segment_id", "]", "*", "padding_length", "\n", "label_ids", "+=", "[", "pad_token_label_id", "]", "*", "padding_length", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "label_ids", ")", "==", "max_seq_length", "\n", "\n", "if", "ex_index", "<", "5", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", ",", "example", ".", "guid", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"segment_ids: %s\"", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label_ids: %s\"", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "label_ids", "]", ")", ")", "\n", "\n", "", "if", "\"token_type_ids\"", "not", "in", "tokenizer", ".", "model_input_names", ":", "\n", "            ", "segment_ids", "=", "None", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "\n", "input_ids", "=", "input_ids", ",", "attention_mask", "=", "input_mask", ",", "token_type_ids", "=", "segment_ids", ",", "label_ids", "=", "label_ids", "\n", ")", "\n", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.dmis-lab_bioner-generalization.training.utils_ner.get_labels": [[394, 405], ["open", "f.read().splitlines", "f.read"], "function", ["None"], ["", "def", "get_labels", "(", "path", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "if", "path", ":", "\n", "        ", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "labels", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "labels", "=", "[", "i", "+", "'-bio'", "if", "i", "!=", "'O'", "else", "'O'", "for", "i", "in", "labels", "]", "\n", "", "if", "\"O\"", "not", "in", "labels", ":", "\n", "            ", "labels", "=", "[", "\"O\"", "]", "+", "labels", "\n", "", "return", "labels", "\n", "", "else", ":", "\n", "# return [\"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]", "\n", "        ", "return", "[", "\"O\"", ",", "\"B-bio\"", ",", "\"I-bio\"", "]", "\n", "", "", ""]]}