{"home.repos.pwc.inspect_result.amnonattali_dsaa.None.dsaa.train": [[20, 274], ["torch_models.Abstraction", "torch.optim.Adam", "torch_models.SuccessorRepresentation", "torch.optim.Adam", "torch_models.MonolithicPolicy", "torch_models.MonolithicPolicy", "torch_models.MonolithicPolicy.network.load_state_dict", "utils.ReplayBuffer", "range", "torch_models.Abstraction.parameters", "torch_models.SuccessorRepresentation.parameters", "torch_models.MonolithicPolicy.network.state_dict", "torch_models.Abstraction.load_state_dict", "torch_models.SuccessorRepresentation.load_state_dict", "torch_models.MonolithicPolicy.network.load_state_dict", "pickle.load", "print", "torch.eye", "torch.zeros", "numpy.copy", "torch.zeros", "torch.zeros", "set", "numpy.zeros", "print", "range", "print", "update_models.update_abstraction", "print", "update_models.train_option_policies", "torch.save", "utils.ReplayBuffer.sample", "zip", "torch.FloatTensor", "torch.FloatTensor", "torch.eye", "print", "pickle.dump", "torch.load", "torch.load", "torch.load", "open", "env.step", "phi.tolist", "utils.ReplayBuffer.add", "torch.save", "torch.save", "torch.save", "pickle.dump", "pickle.dump", "torch_models.MonolithicPolicy.network.state_dict", "len", "torch.no_grad", "torch_models.Abstraction.", "torch_models.Abstraction.", "torch_models.Abstraction.to_num().flatten", "torch_models.Abstraction.to_num().flatten", "open", "all_episode_success.append", "env.reset", "utils.get_new_skill", "phi.tolist", "random.randrange", "torch_models.MonolithicPolicy.choose_action", "torch.no_grad", "torch_models.Abstraction.", "[].item", "update_models.train_option_policies", "numpy.copy", "option_success[].mean().item", "print", "torch_models.MonolithicPolicy.network.state_dict", "torch_models.Abstraction.state_dict", "torch_models.SuccessorRepresentation.state_dict", "open", "open", "torch.no_grad", "torch_models.Abstraction.", "[].item", "utils.get_new_skill", "torch.FloatTensor().view", "float", "print", "len", "utils.delete_edge", "torch.zeros.sum", "torch_models.Abstraction.to_num", "torch_models.Abstraction.to_num", "len", "numpy.mean", "torch.FloatTensor().view", "option_success[].mean", "torch.FloatTensor", "torch_models.Abstraction.to_num", "utils.get_nbrs", "torch.FloatTensor", "torch_models.Abstraction.to_num", "torch.diag().mean().item", "torch.eye.nonzero", "torch.eye.nonzero", "torch.diag().mean", "new_visits.sum", "torch.diag"], "function", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.update_models.update_abstraction", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.update_models.train_option_policies", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.add", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reset", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.get_new_skill", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.SoftQNetwork.choose_action", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.update_models.train_option_policies", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.get_new_skill", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.delete_edge", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.to_num", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.to_num", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.to_num", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.get_nbrs", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.to_num"], ["def", "train", "(", "config", ")", ":", "\n", "# ------------- INIT -------------", "\n", "    ", "env", "=", "env_types", "[", "config", "[", "\"env_type\"", "]", "]", "(", "config", ")", "\n", "save_path", "=", "config", "[", "\"save_path\"", "]", "\n", "\n", "obs_size", "=", "env", ".", "observation_size", "\n", "num_actions", "=", "env", ".", "action_size", "\n", "num_abstract_states", "=", "config", "[", "\"num_abstract_states\"", "]", "\n", "num_skills", "=", "num_abstract_states", "# one skill per possible next abstract state", "\n", "learning_rate", "=", "config", "[", "\"learning_rate\"", "]", "\n", "\n", "# initialize abstraction model", "\n", "phi", "=", "Abstraction", "(", "obs_size", ",", "num_abstract_states", ")", "\n", "phi_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "phi", ".", "parameters", "(", ")", ",", "lr", "=", "learning_rate", ")", "\n", "# initialize successor representation", "\n", "psi", "=", "SuccessorRepresentation", "(", "num_abstract_states", ")", "\n", "psi_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "psi", ".", "parameters", "(", ")", ",", "lr", "=", "learning_rate", ")", "\n", "\n", "# initialize option policies (online and target models)", "\n", "online_Q", "=", "MonolithicPolicy", "(", "num_abstract_states", ",", "num_skills", ",", "obs_size", ",", "num_actions", ",", "config", ")", "\n", "target_Q", "=", "MonolithicPolicy", "(", "num_abstract_states", ",", "num_skills", ",", "obs_size", ",", "num_actions", ",", "config", ")", "\n", "option_optimizer", "=", "online_Q", ".", "option_optimizer", "\n", "target_Q", ".", "network", ".", "load_state_dict", "(", "online_Q", ".", "network", ".", "state_dict", "(", ")", ")", "\n", "\n", "# initialize replay buffer", "\n", "replay_buffer_size", "=", "config", "[", "\"option_replay_buffer_size\"", "]", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "replay_buffer_size", ")", "\n", "\n", "first_iteration", "=", "True", "# in the first iteration we get pure random exploration", "\n", "if", "config", "[", "\"load_saved_abstraction\"", "]", ":", "\n", "        ", "phi", ".", "load_state_dict", "(", "torch", ".", "load", "(", "f\"{save_path}/phi.torch\"", ")", ")", "\n", "psi", ".", "load_state_dict", "(", "torch", ".", "load", "(", "f\"{save_path}/psi.torch\"", ")", ")", "\n", "\n", "online_Q", ".", "network", ".", "load_state_dict", "(", "torch", ".", "load", "(", "f\"{save_path}/mono_Q.torch\"", ")", ")", "\n", "abstract_adjacency", "=", "pickle", ".", "load", "(", "open", "(", "f\"{save_path}/abstract_adjacency.pickle\"", ",", "\"rb\"", ")", ")", "\n", "\n", "print", "(", "\"Loaded saved abstraction and abstract graph:\\n\"", ",", "abstract_adjacency", ")", "\n", "# if we load a saved model it is not the first iteration", "\n", "first_iteration", "=", "False", "\n", "", "else", ":", "\n", "# if we don't load a saved model we initialize the adjacency to the identity", "\n", "        ", "abstract_adjacency", "=", "torch", ".", "eye", "(", "num_abstract_states", ")", "\n", "\n", "# option training params", "\n", "", "num_exploration_iters", "=", "config", "[", "\"option_replay_buffer_size\"", "]", "# we just fill up the buffer once", "\n", "# initialize env_done to True so we trigger a reset in the first iteration", "\n", "env_done", "=", "True", "\n", "option_done", "=", "True", "\n", "all_episode_success", "=", "[", "]", "# keep track of episode successes (environment task)", "\n", "for", "iteration", "in", "range", "(", "config", "[", "\"max_iter\"", "]", ")", ":", "\n", "# ------------- OPTION EXPLORATION PHASE -------------", "\n", "\n", "# visit counts represents the number of times each abstract state is transitioned into", "\n", "# this is useful for biasing exploration towards low visit states", "\n", "#   i.e., \"intrinsic motivation\" in the abstract state space", "\n", "        ", "visit_counts", "=", "torch", ".", "zeros", "(", "(", "num_abstract_states", ")", ")", "\n", "old_visit_counts", "=", "np", ".", "copy", "(", "visit_counts", ")", "# used to keep track of new visits each iteration", "\n", "\n", "# rewarding_options keeps track of which options, when executed, provide the most reward ", "\n", "# NOTE: this is a heuristic replacement for training a proper abstract policy", "\n", "rewarding_options", "=", "torch", ".", "zeros", "(", "(", "num_abstract_states", ",", "num_abstract_states", ")", ")", "\n", "\n", "# option_success is a running exponential average of the success of pi_(s,z)", "\n", "option_success", "=", "torch", ".", "zeros", "(", "(", "num_abstract_states", ",", "num_skills", ")", ")", "\n", "avg_success", "=", "0", "# avg success keeps track of recent success rate of options which were called", "\n", "\n", "# To avoid wasting time on option policies that keep fail we sometimes delete edges", "\n", "bad_edges", "=", "set", "(", "[", "]", ")", "\n", "num_times_stuck", "=", "np", ".", "zeros", "(", "(", "num_abstract_states", ",", "num_skills", ")", ")", "\n", "\n", "# some more statistics", "\n", "running_episode_reward", "=", "0", "\n", "episode_reward", "=", "0", "\n", "print", "(", "f\"**Beginning Exploration Phase for {num_exploration_iters} Steps**\"", ")", "\n", "for", "option_iter", "in", "range", "(", "num_exploration_iters", ")", ":", "\n", "# if the episode is over reset", "\n", "            ", "if", "env_done", ":", "\n", "                ", "running_episode_reward", "=", "running_episode_reward", "*", "0.95", "+", "(", "1.0", "-", "0.95", ")", "*", "(", "episode_reward", ">", "0", ")", "\n", "all_episode_success", ".", "append", "(", "episode_reward", ">", "0", ")", "\n", "# NOTE: in the unsupervised case the loop runs for max_iter iterations", "\n", "if", "len", "(", "all_episode_success", ")", ">", "10", "and", "np", ".", "mean", "(", "all_episode_success", "[", "-", "10", ":", "]", ")", ">", "config", "[", "\"success_threshold\"", "]", ":", "\n", "                    ", "return", "all_episode_success", "\n", "\n", "", "env_done", "=", "False", "\n", "option_done", "=", "False", "\n", "steps_in_current_state", "=", "0", "# this is how long we've been running the option for", "\n", "episode_reward", "=", "0", "# this is the episode return", "\n", "current_option_reward", "=", "0", "# this is how much reward was accumulated during the current option", "\n", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "# we compute the current abstract state", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# TODO: fix ugly code", "\n", "                    ", "abstract_state", "=", "phi", "(", "torch", ".", "FloatTensor", "(", "state", ")", ".", "view", "(", "1", ",", "-", "1", ")", ")", "\n", "a_num", "=", "phi", ".", "to_num", "(", "abstract_state", ")", "[", "0", "]", ".", "item", "(", ")", "\n", "abstract_state", "=", "abstract_state", "[", "0", "]", "\n", "# In principle this new skill is just a random neighbor in the abstract graph", "\n", "#   when we find an option that solves the environment task we plan a shortest path to it", "\n", "", "skill", "=", "get_new_skill", "(", "first_iteration", ",", "a_num", ",", "abstract_adjacency", ",", "\n", "visit_counts", ",", "rewarding_options", ",", "episode_reward", ",", "config", "[", "\"use_env_reward\"", "]", ")", "\n", "\n", "# augment the primitive state with the abstract state", "\n", "state", "+=", "abstract_state", ".", "tolist", "(", ")", "\n", "# the starting abstract state counts as being transitioned into", "\n", "visit_counts", "[", "a_num", "]", "+=", "1", "\n", "\n", "# if the option has terminated (but the episode has not) we need a new skill", "\n", "", "elif", "option_done", ":", "\n", "                ", "option_done", "=", "False", "\n", "steps_in_current_state", "=", "0", "\n", "current_option_reward", "=", "0", "\n", "skill", "=", "get_new_skill", "(", "first_iteration", ",", "a_num", ",", "abstract_adjacency", ",", "\n", "visit_counts", ",", "rewarding_options", ",", "episode_reward", ",", "config", "[", "\"use_env_reward\"", "]", ")", "\n", "\n", "", "steps_in_current_state", "+=", "1", "\n", "if", "first_iteration", ":", "\n", "                ", "action", "=", "random", ".", "randrange", "(", "num_actions", ")", "\n", "", "else", ":", "\n", "                ", "action", "=", "online_Q", ".", "choose_action", "(", "state", ",", "skill", ")", "\n", "\n", "# step in environment... env takes care of structuring state correctly", "\n", "", "next_state", ",", "env_reward", ",", "env_done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "# get the next abstract state", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_abstract_state", "=", "phi", "(", "torch", ".", "FloatTensor", "(", "next_state", ")", ".", "view", "(", "1", ",", "-", "1", ")", ")", "\n", "next_a_num", "=", "phi", ".", "to_num", "(", "next_abstract_state", ")", "[", "0", "]", ".", "item", "(", ")", "\n", "next_abstract_state", "=", "next_abstract_state", "[", "0", "]", "\n", "\n", "", "next_state", "+=", "next_abstract_state", ".", "tolist", "(", ")", "# again, we augment the state with the abstract state     ", "\n", "\n", "# keep track of environment reward", "\n", "episode_reward", "+=", "env_reward", "\n", "current_option_reward", "+=", "env_reward", "\n", "rewarding_options", "[", "a_num", ",", "skill", "]", "+=", "env_reward", "\n", "\n", "# option ends when an abstract transition occurs", "\n", "option_done", "=", "(", "a_num", "!=", "next_a_num", ")", "\n", "# get reward if transition to the correct abstract state", "\n", "option_reward", "=", "0.0", "\n", "if", "option_done", ":", "\n", "                ", "visit_counts", "[", "next_a_num", "]", "+=", "1", "\n", "# ---- reward ----", "\n", "# NOTE: in the softQlearning setting where we reward the agent with entropy, ", "\n", "#   we need to make sure that the agent gets more reward for transitioning than for high entropy", "\n", "option_reward", "+=", "config", "[", "\"option_success_reward\"", "]", "*", "(", "float", "(", "next_a_num", "==", "skill", ")", ")", "\n", "\n", "# ---- adding edges ----", "\n", "# whenever a transition occurs we mark it", "\n", "# if the transition hadn't been seen before add edge to graph (meaning add skill)", "\n", "if", "not", "next_a_num", "in", "get_nbrs", "(", "abstract_adjacency", ",", "a_num", ")", "and", "not", "(", "a_num", ",", "next_a_num", ")", "in", "bad_edges", ":", "\n", "                    ", "abstract_adjacency", "[", "a_num", ",", "next_a_num", "]", "=", "1.0", "# TODO: eventually we may want to actually use this weight", "\n", "print", "(", "f\"Adding edge {a_num} --> {next_a_num}\"", ")", "\n", "\n", "# ---- track option success statistics ----", "\n", "", "if", "a_num", "!=", "skill", ":", "\n", "# if the option received some reward we consider this option successful", "\n", "                    ", "cur_suc", "=", "(", "(", "option_reward", ">", "0", ")", "or", "(", "config", "[", "\"use_env_reward\"", "]", "and", "current_option_reward", ">", "0", ")", ")", "\n", "option_success", "[", "a_num", ",", "skill", "]", "=", "option_success", "[", "a_num", ",", "skill", "]", "*", "0.95", "+", "0.05", "*", "cur_suc", "\n", "avg_success", "=", "avg_success", "*", "0.99", "+", "0.01", "*", "cur_suc", "\n", "", "else", ":", "# self loop edge is not supposed to transition", "\n", "                    ", "option_success", "[", "a_num", ",", "a_num", "]", "=", "option_success", "[", "a_num", ",", "a_num", "]", "*", "0.99", "\n", "\n", "\n", "# add experience to replay buffers", "\n", "# NOTE because during the update phi-based reward is recomputed, we only store the environment reward", "\n", "#   this is clearly inefficient to recompute the abstract states/reward, but prepares us for future work ", "\n", "#       in which the abstraction update and option updates are more interleaved ", "\n", "", "", "if", "config", "[", "\"use_env_reward\"", "]", ":", "\n", "                ", "option_env_reward", "=", "2", "*", "config", "[", "\"option_success_reward\"", "]", "*", "env_reward", "\n", "", "else", ":", "\n", "                ", "option_env_reward", "=", "0", "\n", "", "replay_buffer", ".", "add", "(", "(", "state", "[", ":", "obs_size", "]", ",", "next_state", "[", ":", "obs_size", "]", ",", "action", ",", "option_env_reward", ",", "option_done", ")", ")", "\n", "\n", "# we mark option as done if it hasn't transitioned for too long", "\n", "if", "not", "first_iteration", "and", "steps_in_current_state", ">", "config", "[", "\"max_steps_in_state\"", "]", ":", "\n", "                ", "visit_counts", "[", "a_num", "]", "+=", "1", "# it counts as having visited here again (since we were here a while)", "\n", "option_done", "=", "True", "\n", "# if we didn't transition it counts as a failure (assuming not self loop)", "\n", "if", "a_num", "!=", "skill", ":", "\n", "# in the supervised case it counts as success if we got environment reward", "\n", "                    ", "if", "not", "(", "config", "[", "\"use_env_reward\"", "]", "and", "current_option_reward", ">", "0", ")", ":", "\n", "                        ", "num_times_stuck", "[", "a_num", ",", "skill", "]", "+=", "1", "\n", "", "", "else", ":", "# self loop edge should get here", "\n", "                    ", "option_success", "[", "a_num", ",", "a_num", "]", "=", "option_success", "[", "a_num", ",", "a_num", "]", "*", "0.99", "+", "0.01", "\n", "\n", "# prepare next iteration", "\n", "", "", "state", "=", "next_state", "\n", "abstract_state", "=", "next_abstract_state", "\n", "a_num", "=", "next_a_num", "\n", "\n", "# Now update the option policy for this abstract state", "\n", "if", "not", "first_iteration", "and", "option_iter", "%", "config", "[", "\"option_update_freq\"", "]", "==", "0", "and", "len", "(", "replay_buffer", ")", ">", "config", "[", "\"option_batch_size\"", "]", ":", "\n", "                ", "train_option_policies", "(", "online_Q", ",", "target_Q", ",", "option_optimizer", ",", "\n", "phi", ",", "replay_buffer", ",", "config", ",", "num_updates", "=", "1", ")", "\n", "\n", "# log results", "\n", "", "if", "option_iter", "%", "config", "[", "\"log_freq\"", "]", "==", "config", "[", "\"log_freq\"", "]", "-", "1", ":", "\n", "                ", "new_visits", "=", "visit_counts", "-", "old_visit_counts", "\n", "old_visit_counts", "=", "np", ".", "copy", "(", "visit_counts", ")", "\n", "\n", "if", "config", "[", "\"delete_bad_edges\"", "]", ":", "\n", "                    ", "delete_edge", "(", "option_success", ",", "abstract_adjacency", ",", "num_times_stuck", ",", "visit_counts", ",", "bad_edges", ")", "\n", "\n", "", "all_avg_suc", "=", "option_success", "[", "abstract_adjacency", ".", "nonzero", "(", ")", "[", ":", ",", "0", "]", ",", "abstract_adjacency", ".", "nonzero", "(", ")", "[", ":", ",", "1", "]", "]", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "print", "(", "f\"Stats (iter {iteration}, option_iter: {option_iter}):\"", "+", "\n", "f\"\\n\\tVisit counts          {visit_counts}\"", "+", "\n", "f\"\\n\\tNew visits:           {new_visits.sum()}\"", "+", "\n", "f\"\\n\\tAvg success (recent): {avg_success:1.3f}\"", "+", "\n", "f\"\\n\\tAvg Success (all):    {all_avg_suc:1.3f}\"", "+", "\n", "f\"\\n\\tSelf loop success:    {torch.diag(option_success).mean().item():1.3f}\"", "+", "\n", "f\"\\n\\tRunning reward:       {running_episode_reward:4.3f}\"", "+", "\n", "f\"\\n\\tBad edges:            {bad_edges}\"", "\n", ")", "\n", "", "", "print", "(", "f\"Exploration phase finished, total visits {visit_counts.sum()}\"", ")", "\n", "\n", "if", "not", "first_iteration", ":", "\n", "            ", "torch", ".", "save", "(", "online_Q", ".", "network", ".", "state_dict", "(", ")", ",", "f\"{save_path}/mono_Q.torch\"", ")", "\n", "torch", ".", "save", "(", "phi", ".", "state_dict", "(", ")", ",", "f\"{save_path}/prev_phi.torch\"", ")", "\n", "torch", ".", "save", "(", "psi", ".", "state_dict", "(", ")", ",", "f\"{save_path}/prev_psi.torch\"", ")", "\n", "pickle", ".", "dump", "(", "abstract_adjacency", ",", "open", "(", "f\"{save_path}/abstract_adjacency.pickle\"", ",", "\"wb\"", ")", ")", "\n", "pickle", ".", "dump", "(", "option_success", ",", "open", "(", "f\"{save_path}/option_success.pickle\"", ",", "\"wb\"", ")", ")", "\n", "\n", "# ------------- ABSTRACTION UPDATE PHASE -------------", "\n", "", "update_abstraction", "(", "phi", ",", "phi_optimizer", ",", "psi", ",", "psi_optimizer", ",", "replay_buffer", ",", "config", ")", "\n", "\n", "# optimizations ---------------------------------------------------------------------------------------------", "\n", "print", "(", "\"Finished abstraction update, now initializing option policies with new abstraction and old data.\"", ")", "\n", "# ------------- OPTION INITIALIZATION WITH NEW ABSTRACTION -------------", "\n", "train_option_policies", "(", "online_Q", ",", "target_Q", ",", "option_optimizer", ",", "\n", "phi", ",", "replay_buffer", ",", "config", ",", "num_updates", "=", "config", "[", "\"post_abstraction_option_updates\"", "]", ")", "\n", "torch", ".", "save", "(", "online_Q", ".", "network", ".", "state_dict", "(", ")", ",", "f\"{save_path}/mono_Q_post_abstraction.torch\"", ")", "\n", "\n", "# ------------- INITIALIZE ABSTRACT GRAPH WITH KNOWN TRANSITIONS -------------", "\n", "# TODO: this is inefficient, should do this during the previous step", "\n", "batch", "=", "replay_buffer", ".", "sample", "(", "len", "(", "replay_buffer", ")", ")", "# NOTE: needs to fit in memory...", "\n", "batch_state", ",", "batch_next_state", ",", "_", ",", "_", ",", "_", "=", "zip", "(", "*", "batch", ")", "\n", "batch_state", "=", "torch", ".", "FloatTensor", "(", "batch_state", ")", "\n", "batch_next_state", "=", "torch", ".", "FloatTensor", "(", "batch_next_state", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "abstract_state", "=", "phi", "(", "batch_state", ")", "\n", "next_abstract_state", "=", "phi", "(", "batch_next_state", ")", "\n", "\n", "abstract_state_nums", "=", "phi", ".", "to_num", "(", "abstract_state", ")", ".", "flatten", "(", ")", "\n", "next_abstract_state_nums", "=", "phi", ".", "to_num", "(", "next_abstract_state", ")", ".", "flatten", "(", ")", "\n", "", "abstract_adjacency", "=", "torch", ".", "eye", "(", "num_abstract_states", ")", "\n", "abstract_adjacency", "[", "abstract_state_nums", ",", "next_abstract_state_nums", "]", "=", "1", "\n", "print", "(", "\"Initialized adjacency matrix:\\n\"", ",", "abstract_adjacency", ")", "\n", "pickle", ".", "dump", "(", "abstract_adjacency", ",", "open", "(", "f\"{save_path}/abstract_adjacency_post_abstraction.pickle\"", ",", "\"wb\"", ")", ")", "\n", "\n", "first_iteration", "=", "False", "# first iteration is over", "\n", "env_done", "=", "True", "# we want to reset at the beginning of each exploration phase", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.__init__": [[19, 31], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.DrawArm.__init__"], ["    ", "def", "__init__", "(", "self", ",", "obs_size", ",", "num_abstract_states", ")", ":", "\n", "        ", "super", "(", "Abstraction", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "obs_size", "=", "obs_size", "\n", "self", ".", "num_abstract_states", "=", "num_abstract_states", "\n", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "obs_size", ",", "128", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "128", ",", "256", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "256", ",", "self", ".", "num_abstract_states", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.to_num": [[34, 36], ["torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["None"], ["", "def", "to_num", "(", "self", ",", "abstract_state", ")", ":", "\n", "        ", "return", "torch", ".", "argmax", "(", "abstract_state", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.phi": [[37, 39], ["torch_models.Abstraction.features"], "methods", ["None"], ["", "def", "phi", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "self", ".", "features", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.sample": [[41, 44], ["torch_models.Abstraction.phi", "torch.nn.functional.gumbel_softmax", "torch.nn.functional.gumbel_softmax", "torch.nn.functional.gumbel_softmax", "torch.nn.functional.gumbel_softmax"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.phi"], ["", "def", "sample", "(", "self", ",", "obs", ",", "tau", "=", "1.0", ",", "hard", "=", "False", ")", ":", "\n", "        ", "abstract_state", "=", "self", ".", "phi", "(", "obs", ")", "\n", "return", "torch", ".", "nn", ".", "functional", ".", "gumbel_softmax", "(", "abstract_state", ",", "tau", "=", "tau", ",", "hard", "=", "hard", ",", "dim", "=", "-", "1", ")", "+", "0.1", "**", "16", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.forward": [[46, 49], ["torch_models.Abstraction.phi", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.phi"], ["", "def", "forward", "(", "self", ",", "obs", ")", ":", "\n", "        ", "abstract_state", "=", "self", ".", "phi", "(", "obs", ")", "\n", "return", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "abstract_state", ",", "dim", "=", "-", "1", ")", "+", "0.1", "**", "16", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.SuccessorRepresentation.__init__": [[52, 63], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.DrawArm.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_abstract_states", ")", ":", "\n", "        ", "super", "(", "SuccessorRepresentation", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_abstract_states", "=", "num_abstract_states", "\n", "\n", "self", ".", "psi", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "num_abstract_states", ",", "64", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "64", ",", "128", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "128", ",", "self", ".", "num_abstract_states", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.SuccessorRepresentation.forward": [[65, 68], ["torch_models.SuccessorRepresentation.psi"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "abstract_state", ")", ":", "\n", "        ", "successor_representation", "=", "self", ".", "psi", "(", "abstract_state", ")", "\n", "return", "successor_representation", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.MonolithicPolicy.__init__": [[79, 90], ["torch_models.MonolithicPolicy.reset"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reset"], ["    ", "def", "__init__", "(", "self", ",", "num_abstract_states", ",", "num_skills", ",", "obs_size", ",", "num_actions", ",", "config", ")", ":", "\n", "        ", "self", ".", "num_abstract_states", "=", "num_abstract_states", "\n", "self", ".", "num_skills", "=", "num_skills", "\n", "self", ".", "obs_size", "=", "obs_size", "\n", "self", ".", "num_actions", "=", "num_actions", "\n", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "model_input_size", "=", "num_abstract_states", "+", "obs_size", "\n", "self", ".", "model_output_size", "=", "num_actions", "*", "num_skills", "\n", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.MonolithicPolicy.reset": [[91, 95], ["torch_models.SoftQNetwork", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch_models.MonolithicPolicy.network.parameters"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "network", "=", "SoftQNetwork", "(", "self", ".", "model_input_size", ",", "self", ".", "model_output_size", ",", "self", ".", "config", "[", "\"option_entropy_coef\"", "]", ")", "\n", "self", ".", "learn_steps", "=", "0", "\n", "self", ".", "option_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "config", "[", "\"learning_rate\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.MonolithicPolicy.__call__": [[96, 98], ["torch_models.MonolithicPolicy.network"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "state", ",", "skill", ")", ":", "\n", "        ", "return", "self", ".", "network", "(", "state", ")", "[", ":", ",", "skill", "*", "self", ".", "num_actions", ":", "(", "skill", "+", "1", ")", "*", "self", ".", "num_actions", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.MonolithicPolicy.choose_action": [[100, 113], ["torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.distributions.Categorical.sample.item", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch_models.MonolithicPolicy.network", "torch.distributions.Categorical", "torch.distributions.Categorical", "print"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.sample"], ["", "def", "choose_action", "(", "self", ",", "state", ",", "skill", ")", ":", "\n", "        ", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "unsqueeze", "(", "0", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "q", "=", "self", ".", "network", "(", "state", ")", "[", ":", ",", "skill", "*", "self", ".", "num_actions", ":", "(", "skill", "+", "1", ")", "*", "self", ".", "num_actions", "]", "\n", "dist", "=", "torch", ".", "softmax", "(", "q", ",", "dim", "=", "1", ")", "\n", "\n", "try", ":", "\n", "                ", "c", "=", "Categorical", "(", "dist", ")", "\n", "", "except", "ValueError", ":", "\n", "                ", "print", "(", "\"ERROR\"", ",", "dist", ")", "\n", "\n", "", "a", "=", "c", ".", "sample", "(", ")", "\n", "", "return", "a", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.SoftQNetwork.__init__": [[115, 129], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.DrawArm.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inputs", ",", "outputs", ",", "entropy_coef", "=", "0.01", ")", ":", "\n", "        ", "super", "(", "SoftQNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "alpha", "=", "4", "\n", "self", ".", "entropy_coef", "=", "entropy_coef", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "inputs", ",", "64", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "64", ",", "128", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "128", ",", "256", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "256", ",", "512", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "512", ",", "outputs", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.SoftQNetwork.forward": [[131, 133], ["torch_models.SoftQNetwork.features"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "features", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.SoftQNetwork.getV": [[136, 140], ["torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.clip", "torch.clip", "torch.clip", "torch.clip", "torch.log", "torch.log", "torch.log", "torch.log"], "methods", ["None"], ["", "def", "getV", "(", "self", ",", "q_value", ")", ":", "\n", "        ", "probs", "=", "torch", ".", "softmax", "(", "q_value", ",", "dim", "=", "1", ")", "+", "0.1", "**", "16", "\n", "v", "=", "(", "probs", "*", "(", "q_value", "-", "self", ".", "entropy_coef", "*", "torch", ".", "clip", "(", "torch", ".", "log", "(", "probs", ")", ",", "min", "=", "-", "5", ",", "max", "=", "0", ")", ")", ")", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.SoftQNetwork.choose_action": [[141, 150], ["torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.distributions.Categorical.sample.item", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch_models.SoftQNetwork.forward", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.SoftQNetwork.forward", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.sample"], ["", "def", "choose_action", "(", "self", ",", "state", ")", ":", "\n", "        ", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "unsqueeze", "(", "0", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "q", "=", "self", ".", "forward", "(", "state", ")", "\n", "dist", "=", "torch", ".", "softmax", "(", "q", ",", "dim", "=", "1", ")", "\n", "c", "=", "Categorical", "(", "dist", ")", "\n", "a", "=", "c", ".", "sample", "(", ")", "\n", "# a = torch.argmax(dist)", "\n", "", "return", "a", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.baseline.train_baseline_policy": [[9, 105], ["environments.env_wrappers.Manipulator2D", "torch_models.SoftQNetwork", "torch_models.SoftQNetwork", "torch_models.SoftQNetwork.load_state_dict", "torch.optim.Adam", "torch.optim.Adam", "utils.ReplayBuffer", "range", "torch_models.SoftQNetwork.state_dict", "torch_models.SoftQNetwork.parameters", "environments.env_wrappers.Manipulator2D.reset", "environments.env_wrappers.Manipulator2D.step", "utils.ReplayBuffer.add", "episode_successes.append", "print", "len", "utils.ReplayBuffer.sample", "zip", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.mse_loss", "torch.optim.Adam.zero_grad", "F.mse_loss.backward", "torch.optim.Adam.step", "torch.no_grad", "torch.no_grad", "len", "torch_models.SoftQNetwork.load_state_dict", "torch.no_grad", "torch.no_grad", "torch_models.SoftQNetwork.", "torch_models.SoftQNetwork.getV", "torch_models.SoftQNetwork.gather", "random.random", "torch_models.SoftQNetwork.choose_action", "random.randrange", "numpy.mean", "torch_models.SoftQNetwork.state_dict", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor().unsqueeze.long", "torch_models.SoftQNetwork.", "numpy.array"], "function", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reset", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.add", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.SoftQNetwork.getV", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.SoftQNetwork.choose_action"], ["def", "train_baseline_policy", "(", ")", ":", "\n", "# baseline configured for hard task", "\n", "    ", "config", "=", "{", "\"max_steps\"", ":", "5000", ",", "\"num_arm_joints\"", ":", "3", ",", "\"arm_joint_lengths\"", ":", "10", ",", "\"ball_goal_height\"", ":", "11", "}", "\n", "\n", "env", "=", "Manipulator2D", "(", "config", ")", "\n", "input_size", "=", "env", ".", "observation_size", "\n", "num_actions", "=", "env", ".", "action_size", "\n", "\n", "# Params", "\n", "option_batch_size", "=", "512", "\n", "learn_steps", "=", "0", "\n", "target_update_steps", "=", "20", "\n", "gamma", "=", "0.95", "\n", "num_epochs", "=", "1000", "\n", "learning_rate", "=", "0.01", "\n", "softQ_entropy_coeff", "=", "0.01", "\n", "\n", "online_policy", "=", "SoftQNetwork", "(", "inputs", "=", "input_size", ",", "\n", "outputs", "=", "num_actions", ",", "\n", "entropy_coef", "=", "softQ_entropy_coeff", ")", "\n", "\n", "target_policy", "=", "SoftQNetwork", "(", "inputs", "=", "input_size", ",", "\n", "outputs", "=", "num_actions", ",", "\n", "entropy_coef", "=", "softQ_entropy_coeff", ")", "\n", "target_policy", ".", "load_state_dict", "(", "online_policy", ".", "state_dict", "(", ")", ")", "\n", "option_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "online_policy", ".", "parameters", "(", ")", ",", "lr", "=", "learning_rate", ")", "\n", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "1000000", ")", "\n", "\n", "# the first epoch is skipped...", "\n", "env_done", "=", "True", "\n", "running_reward", "=", "0", "\n", "epoch_reward", "=", "0", "\n", "episode_successes", "=", "[", "]", "\n", "for", "epoch", "in", "range", "(", "num_epochs", ")", ":", "\n", "\n", "        ", "while", "not", "env_done", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Get the primitive action from the option_policy for the current abstract_state and skill", "\n", "                ", "if", "random", ".", "random", "(", ")", "<", "0.7", ":", "\n", "                    ", "action", "=", "online_policy", ".", "choose_action", "(", "state", ")", "\n", "", "else", ":", "\n", "                    ", "action", "=", "random", ".", "randrange", "(", "num_actions", ")", "\n", "\n", "# Step in the environment", "\n", "", "", "next_state", ",", "env_reward", ",", "env_done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "epoch_reward", "+=", "env_reward", "\n", "\n", "ep_done", "=", "False", "# infinite horizon", "\n", "replay_buffer", ".", "add", "(", "(", "state", ",", "next_state", ",", "action", ",", "env_reward", "*", "100.0", ",", "ep_done", ")", ")", "\n", "\n", "state", "=", "next_state", "\n", "\n", "", "if", "epoch", ">", "0", ":", "\n", "            ", "running_reward", "=", "running_reward", "*", "0.95", "+", "(", "epoch_reward", ">", "0", ")", "*", "0.05", "\n", "episode_successes", ".", "append", "(", "epoch_reward", ">", "0", ")", "\n", "if", "len", "(", "episode_successes", ")", ">", "10", ":", "\n", "                ", "if", "np", ".", "mean", "(", "episode_successes", "[", "-", "10", ":", "]", ")", ">", "0.75", ":", "\n", "                    ", "return", "episode_successes", "\n", "", "", "print", "(", "f\"Epoch {epoch}, Success: {epoch_reward>0}, Epoch reward {epoch_reward}, \"", "+", "\n", "f\"Running reward {running_reward:4.3f}, final state {np.array(state)}\"", ")", "\n", "# arm_visit_entropy(replay_buffer)", "\n", "\n", "# Here env_done = True, reset everything", "\n", "", "env_done", "=", "False", "\n", "epoch_reward", "=", "0", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "\n", "# Technically this code can come after every time we add to the buffer...", "\n", "if", "len", "(", "replay_buffer", ")", ">", "option_batch_size", ":", "\n", "            ", "learn_steps", "+=", "1", "\n", "if", "learn_steps", "%", "target_update_steps", "==", "0", ":", "\n", "                ", "target_policy", ".", "load_state_dict", "(", "online_policy", ".", "state_dict", "(", ")", ")", "\n", "\n", "", "batch", "=", "replay_buffer", ".", "sample", "(", "option_batch_size", ")", "\n", "batch_state", ",", "batch_next_state", ",", "batch_action", ",", "batch_reward", ",", "batch_done", "=", "zip", "(", "*", "batch", ")", "\n", "\n", "batch_state", "=", "torch", ".", "FloatTensor", "(", "batch_state", ")", "\n", "batch_next_state", "=", "torch", ".", "FloatTensor", "(", "batch_next_state", ")", "\n", "batch_action", "=", "torch", ".", "FloatTensor", "(", "batch_action", ")", ".", "unsqueeze", "(", "1", ")", "\n", "batch_reward", "=", "torch", ".", "FloatTensor", "(", "batch_reward", ")", ".", "unsqueeze", "(", "1", ")", "\n", "batch_done", "=", "torch", ".", "FloatTensor", "(", "batch_done", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_q", "=", "target_policy", "(", "batch_next_state", ")", "\n", "next_v", "=", "target_policy", ".", "getV", "(", "next_q", ")", "\n", "y", "=", "batch_reward", "+", "(", "1", "-", "batch_done", ")", "*", "gamma", "*", "next_v", "\n", "\n", "", "loss", "=", "F", ".", "mse_loss", "(", "online_policy", "(", "batch_state", ")", ".", "gather", "(", "1", ",", "batch_action", ".", "long", "(", ")", ")", ",", "y", ")", "\n", "\n", "option_optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "option_optimizer", ".", "step", "(", ")", "\n", "# The end", "\n", "", "", "return", "episode_successes", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.update_models.train_option_policies": [[7, 95], ["range", "replay_buffer.sample", "zip", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "range", "option_optimizer.zero_grad", "loss.backward", "option_optimizer.step", "torch.no_grad", "torch.no_grad", "phi", "phi", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "phi.to_num", "phi.to_num", "online_Q().gather", "torch.smooth_l1_loss", "reward.sum().item", "print", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "target_Q.network.load_state_dict", "batch_env_reward.view.view", "torch.FloatTensor().unsqueeze.long", "torch.no_grad", "torch.no_grad", "target_Q", "q_target.sum().item", "loss.item", "online_Q.network.state_dict", "online_Q", "reward.sum", "torch.softmax", "torch.softmax", "torch.max", "torch.max", "q_target.sum", "len", "set", "phi.to_num.view().numpy", "torch.clip", "torch.clip", "torch.log", "torch.log", "phi.to_num.view"], "function", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.phi", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.phi", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.to_num", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.to_num"], ["def", "train_option_policies", "(", "online_Q", ",", "target_Q", ",", "option_optimizer", ",", "\n", "phi", ",", "replay_buffer", ",", "config", ",", "num_updates", "=", "None", ")", ":", "\n", "    ", "running_loss", "=", "0", "\n", "for", "update_idx", "in", "range", "(", "num_updates", ")", ":", "\n", "        ", "batch", "=", "replay_buffer", ".", "sample", "(", "config", "[", "\"option_batch_size\"", "]", ")", "\n", "# When we sample from the buffer we ignore the environment \"done\", and compute our own", "\n", "batch_state", ",", "batch_next_state", ",", "batch_action", ",", "batch_env_reward", ",", "_", "=", "zip", "(", "*", "batch", ")", "\n", "batch_state", "=", "torch", ".", "FloatTensor", "(", "batch_state", ")", "\n", "batch_next_state", "=", "torch", ".", "FloatTensor", "(", "batch_next_state", ")", "\n", "batch_action", "=", "torch", ".", "FloatTensor", "(", "batch_action", ")", ".", "unsqueeze", "(", "1", ")", "\n", "batch_env_reward", "=", "torch", ".", "FloatTensor", "(", "batch_env_reward", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# NOTE: when we train option policies we use Softmax not Gumbel so the abstraction is consistent", "\n", "            ", "abstract_state", "=", "phi", "(", "batch_state", ")", "\n", "next_abstract_state", "=", "phi", "(", "batch_next_state", ")", "\n", "\n", "# We append the full abstract state encoding, not just the index", "\n", "#   this gives the model more info about abstract state boundaries", "\n", "batch_state", "=", "torch", ".", "cat", "(", "(", "batch_state", ",", "abstract_state", ")", ",", "dim", "=", "1", ")", "\n", "batch_next_state", "=", "torch", ".", "cat", "(", "(", "batch_next_state", ",", "next_abstract_state", ")", ",", "dim", "=", "1", ")", "\n", "\n", "# We need the abstract state indices in order to check where abstract state transitions occur", "\n", "abstract_state_nums", "=", "phi", ".", "to_num", "(", "abstract_state", ")", "\n", "next_abstract_state_nums", "=", "phi", ".", "to_num", "(", "next_abstract_state", ")", "\n", "\n", "# Our option policy terminates when an abstract state transition occurs", "\n", "batch_done", "=", "1.0", "*", "(", "next_abstract_state_nums", "!=", "abstract_state_nums", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "# some stats", "\n", "", "loss", "=", "0", "\n", "total_r", "=", "0", "\n", "q_mean", "=", "0", "\n", "# We loop over each possible next_abstract_state goal - this determines the intrinsic reward", "\n", "for", "skill", "in", "range", "(", "config", "[", "\"num_abstract_states\"", "]", ")", ":", "\n", "# In Double DQN the target model lags behind the online one for stability", "\n", "            ", "online_Q", ".", "learn_steps", "+=", "1", "\n", "if", "online_Q", ".", "learn_steps", "%", "config", "[", "\"ddqn_target_update_steps\"", "]", "==", "0", ":", "\n", "                ", "target_Q", ".", "network", ".", "load_state_dict", "(", "online_Q", ".", "network", ".", "state_dict", "(", ")", ")", "\n", "\n", "# We reward our option policy when there is a transition into the correct next abstract state", "\n", "#   note that we don't reward the agent if it is already in the goal", "\n", "", "reward", "=", "config", "[", "\"option_success_reward\"", "]", "*", "(", "(", "abstract_state_nums", "!=", "skill", ")", "*", "(", "next_abstract_state_nums", "==", "skill", ")", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "# We disensentivize self-loop edges from transitioning outside the current state", "\n", "# TODO: not clear this is necessary (since soft-Q learning implies avoid ending the episode in the absence of reward)", "\n", "reward", "=", "reward", "-", "10.0", "*", "(", "(", "abstract_state_nums", "==", "skill", ")", "*", "(", "next_abstract_state_nums", "!=", "abstract_state_nums", ")", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "# In the supervised setting we provide the policy with the environment reward for the transition", "\n", "if", "config", "[", "\"reward_self\"", "]", ":", "# only reward the self loop option", "\n", "                ", "batch_env_reward", "=", "(", "batch_env_reward", "*", "(", "abstract_state_nums", "==", "skill", ")", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "", "else", ":", "# reward all options", "\n", "                ", "batch_env_reward", "=", "batch_env_reward", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "# Now that we have computed the reward we are finally ready for the update", "\n", "", "current_q", "=", "online_Q", "(", "batch_state", ",", "skill", ")", ".", "gather", "(", "1", ",", "batch_action", ".", "long", "(", ")", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_q", "=", "target_Q", "(", "batch_next_state", ",", "skill", ")", "\n", "if", "config", "[", "\"soft_Q_update\"", "]", ":", "\n", "# In soft Q learning we sample actions according to the softmax of the Q values (probs*next_q)", "\n", "# We add some reward based on the entropy (-probs*log(probs))", "\n", "                    ", "probs", "=", "torch", ".", "softmax", "(", "next_q", ",", "dim", "=", "1", ")", "+", "0.1", "**", "16", "\n", "next_v", "=", "(", "probs", "*", "(", "next_q", "-", "config", "[", "\"option_entropy_coef\"", "]", "*", "torch", ".", "clip", "(", "torch", ".", "log", "(", "probs", ")", ",", "min", "=", "-", "5", ",", "max", "=", "0", ")", ")", "\n", ")", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "next_v", "=", "torch", ".", "max", "(", "next_q", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", "\n", "# The Temporal-Difference target is r + gamma * next_v * (1-done)", "\n", "", "q_target", "=", "reward", "+", "batch_env_reward", "+", "config", "[", "\"option_gamma\"", "]", "*", "next_v", "*", "(", "1", "-", "batch_done", ")", "\n", "\n", "", "loss", "+=", "F", ".", "smooth_l1_loss", "(", "current_q", ",", "q_target", ")", "\n", "total_r", "+=", "reward", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "q_mean", "+=", "q_target", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "config", "[", "\"option_batch_size\"", "]", "\n", "\n", "", "q_mean", "/=", "config", "[", "\"num_abstract_states\"", "]", "\n", "loss", "/=", "config", "[", "\"num_abstract_states\"", "]", "\n", "option_optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "option_optimizer", ".", "step", "(", ")", "\n", "running_loss", "=", "running_loss", "*", "0.99", "+", "0.01", "*", "loss", ".", "item", "(", ")", "\n", "\n", "# print debug...", "\n", "# NOTE: (Mean Q, Total R, and unique states) are all from the current iteration. ", "\n", "# running_loss is an exponential average over all iterations", "\n", "if", "update_idx", "%", "100", "==", "99", ":", "\n", "            ", "print", "(", "f\"Loss {running_loss:2.4f}, Mean Q: {q_mean:4.3f}, \"", "+", "\n", "f\"Total R: {total_r}, unique states {len(set(abstract_state_nums.view(-1,).numpy()))}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.update_models.update_abstraction": [[101, 169], ["range", "len", "phi_optimizer.zero_grad", "psi_optimizer.zero_grad", "replay_buffer.sample", "zip", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "phi.mean", "psi", "abstraction_loss.backward", "phi_optimizer.step", "psi_optimizer.step", "sr_td_loss.item", "ent_loss.item", "phi.sample", "phi", "phi", "torch.no_grad", "torch.no_grad", "psi", "print", "print", "torch.save", "torch.save", "torch.save", "torch.save", "torch.no_grad", "torch.no_grad", "phi.sample", "abstract_state.mean.detach", "phi.state_dict", "psi.state_dict", "torch.log", "torch.log", "phi.detach"], "function", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.phi", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.phi", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.sample"], ["", "", "", "def", "update_abstraction", "(", "phi", ",", "phi_optimizer", ",", "psi", ",", "psi_optimizer", ",", "replay_buffer", ",", "config", ")", ":", "\n", "    ", "total_sr_loss", "=", "0", "\n", "total_entropy_loss", "=", "0", "\n", "for", "abstraction_iter", "in", "range", "(", "config", "[", "\"num_abstraction_updates\"", "]", ")", ":", "\n", "        ", "if", "len", "(", "replay_buffer", ")", ">", "config", "[", "\"abstraction_batch_size\"", "]", ":", "\n", "            ", "phi_optimizer", ".", "zero_grad", "(", ")", "\n", "psi_optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "batch", "=", "replay_buffer", ".", "sample", "(", "config", "[", "\"abstraction_batch_size\"", "]", ")", "\n", "batch_state", ",", "batch_next_state", ",", "_", ",", "_", ",", "_", "=", "zip", "(", "*", "batch", ")", "\n", "\n", "batch_state", "=", "torch", ".", "FloatTensor", "(", "batch_state", ")", "\n", "batch_next_state", "=", "torch", ".", "FloatTensor", "(", "batch_next_state", ")", "\n", "\n", "if", "config", "[", "\"use_gumbel\"", "]", ":", "\n", "# compute abstract states", "\n", "                ", "abstract_state", "=", "phi", ".", "sample", "(", "batch_state", ",", "tau", "=", "config", "[", "\"gumbel_tau\"", "]", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "next_abstract_state", "=", "phi", ".", "sample", "(", "batch_next_state", ",", "tau", "=", "config", "[", "\"gumbel_tau\"", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "abstract_state", "=", "phi", "(", "batch_state", ")", "\n", "next_abstract_state", "=", "phi", "(", "batch_next_state", ")", "\n", "\n", "# compute the entropy of the distribution of abstract states", "\n", "", "mean_abstract_state_probs", "=", "abstract_state", ".", "mean", "(", "dim", "=", "0", ")", "\n", "avg_entropy", "=", "(", "-", "mean_abstract_state_probs", "*", "torch", ".", "log", "(", "mean_abstract_state_probs", ")", ")", ".", "sum", "(", ")", "\n", "\n", "# compute the successor representation", "\n", "successor_representation", "=", "psi", "(", "abstract_state", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_successor_representation", "=", "psi", "(", "next_abstract_state", ")", "\n", "\n", "", "sr_td_loss", "=", "(", "(", "successor_representation", "-", "\n", "(", "abstract_state", ".", "detach", "(", ")", "+", "config", "[", "\"sr_gamma\"", "]", "*", "next_successor_representation", ")", ")", "**", "2", ")", ".", "sum", "(", "dim", "=", "1", ")", ".", "mean", "(", ")", "\n", "\n", "# NOTE: we can use a one hot representation of the abstract state... doesn't seem to improve things", "\n", "# one_hot_abstract_state = torch.zeros((abstraction_batch_size, num_abstract_states)).scatter_(1, abstract_state_nums, 1.)", "\n", "# sr_td_loss = ((successor_representation -", "\n", "#     (one_hot_abstract_state + sr_gamma * next_successor_representation))**2).sum(dim=1).mean()", "\n", "\n", "# NOTE: we can apply the loss only to transitions... doesn't seem to improve things", "\n", "# abstract_state_nums = phi.to_num(abstract_state).detach()", "\n", "# next_abstract_state_nums = phi.to_num(next_abstract_state).detach()", "\n", "# sr_td_loss = torch.masked_select(sr_td_loss, abstract_state_nums != next_abstract_state_nums).mean()", "\n", "\n", "# NOTE: we can add some more losses... doesn't seem to improve things", "\n", "#   such as a contrastive one which encourages transitions to occur where SR changes maximally", "\n", "# contrastive_loss = (successor_representation*next_successor_representation).sum(dim=1)", "\n", "# contrastive_loss = torch.masked_select(contrastive_loss, abstract_state_nums != next_abstract_state_nums).mean()", "\n", "\n", "ent_loss", "=", "-", "config", "[", "\"abstraction_entropy_coef\"", "]", "*", "avg_entropy", "\n", "abstraction_loss", "=", "sr_td_loss", "+", "ent_loss", "\n", "\n", "abstraction_loss", ".", "backward", "(", ")", "\n", "phi_optimizer", ".", "step", "(", ")", "\n", "psi_optimizer", ".", "step", "(", ")", "\n", "\n", "total_sr_loss", "+=", "sr_td_loss", ".", "item", "(", ")", "\n", "total_entropy_loss", "+=", "ent_loss", ".", "item", "(", ")", "\n", "\n", "if", "abstraction_iter", "%", "1000", "==", "999", ":", "\n", "                ", "print", "(", "\"Mean abstract state:\"", ",", "mean_abstract_state_probs", ".", "detach", "(", ")", ")", "\n", "print", "(", "f\"Abstraction iters {abstraction_iter}, \"", "+", "\n", "f\"sr_loss {total_sr_loss / abstraction_iter:3.3f}, \"", "+", "\n", "f\"entropy_loss {total_entropy_loss / abstraction_iter:3.3f}\"", ")", "\n", "torch", ".", "save", "(", "phi", ".", "state_dict", "(", ")", ",", "\"{}/phi.torch\"", ".", "format", "(", "config", "[", "\"save_path\"", "]", ")", ")", "\n", "torch", ".", "save", "(", "psi", ".", "state_dict", "(", ")", ",", "\"{}/psi.torch\"", ".", "format", "(", "config", "[", "\"save_path\"", "]", ")", ")", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.__init__": [[7, 10], ["collections.deque"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "capacity", ")", ":", "\n", "        ", "self", ".", "buffer", "=", "deque", "(", "maxlen", "=", "capacity", ")", "\n", "self", ".", "capacity", "=", "capacity", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.reset": [[11, 13], ["utils.ReplayBuffer.buffer.clear"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "buffer", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.add": [[14, 16], ["utils.ReplayBuffer.buffer.append"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "experience", ")", ":", "\n", "        ", "self", ".", "buffer", ".", "append", "(", "experience", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.sample": [[17, 19], ["random.sample"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.sample"], ["", "def", "sample", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "return", "random", ".", "sample", "(", "self", ".", "buffer", ",", "batch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.__len__": [[20, 22], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.get_nbrs": [[24, 26], ["adj[].nonzero().flatten().numpy", "adj[].nonzero().flatten", "adj[].nonzero"], "function", ["None"], ["", "", "def", "get_nbrs", "(", "adj", ",", "a_num", ")", ":", "\n", "    ", "return", "adj", "[", "a_num", "]", ".", "nonzero", "(", ")", ".", "flatten", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.get_new_skill": [[31, 53], ["random.random", "rewarding_options.sum().item", "utils.plan_abstract_path", "get_nbrs().tolist", "numpy.random.choice", "rewarding_options.sum", "utils.get_nbrs", "torch.argmin().item", "utils.get_nbrs", "torch.max", "torch.argmin"], "function", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.plan_abstract_path", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.get_nbrs", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.get_nbrs"], ["", "def", "get_new_skill", "(", "first_iteration", ",", "a_num", ",", "abstract_adjacency", ",", "visit_counts", ",", "rewarding_options", ",", "episode_reward", ",", "use_env_reward", ")", ":", "\n", "    ", "if", "first_iteration", ":", "\n", "        ", "skill", "=", "-", "1", "# in the first iteration we will just take random actions (might not be necessary, but simpler)", "\n", "", "else", ":", "\n", "        ", "if", "use_env_reward", "and", "rewarding_options", ".", "sum", "(", ")", ".", "item", "(", ")", ">", "0", "and", "episode_reward", "<=", "0.01", ":", "\n", "            ", "max_reward_pair", "=", "(", "rewarding_options", "==", "torch", ".", "max", "(", "rewarding_options", ")", ")", ".", "nonzero", "(", ")", "[", "0", "]", "\n", "if", "a_num", "==", "max_reward_pair", "[", "0", "]", ":", "\n", "                ", "skill", "=", "max_reward_pair", "[", "1", "]", "\n", "", "else", ":", "\n", "                ", "max_reward_path", "=", "plan_abstract_path", "(", "a_num", ",", "max_reward_pair", "[", "0", "]", ",", "abstract_adjacency", ")", "\n", "skill", "=", "max_reward_path", "[", "0", "]", "[", "1", "]", "\n", "", "", "else", ":", "\n", "            ", "random_decision", "=", "random", ".", "random", "(", ")", "\n", "if", "random_decision", "<", "0.25", ":", "\n", "# We find that sometimes choosing the neighbor with fewest visits is helpful for diversifying the data", "\n", "                ", "candidates", "=", "get_nbrs", "(", "abstract_adjacency", ",", "a_num", ")", ".", "tolist", "(", ")", "\n", "skill", "=", "candidates", "[", "torch", ".", "argmin", "(", "visit_counts", "[", "candidates", "]", ")", ".", "item", "(", ")", "]", "\n", "", "elif", "random_decision", "<", "0.5", ":", "\n", "                ", "skill", "=", "a_num", "# lazy random walk", "\n", "", "else", ":", "# random neighbor from the adjacency graph", "\n", "                ", "skill", "=", "np", ".", "random", ".", "choice", "(", "get_nbrs", "(", "abstract_adjacency", ",", "a_num", ")", ")", "\n", "", "", "", "return", "skill", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.plan_abstract_path": [[56, 91], ["heapq.heappush", "path.reverse", "heapq.heappop", "utils.get_nbrs", "path.append", "len", "utils.plan_abstract_path.backtrack"], "function", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.get_nbrs"], ["", "def", "plan_abstract_path", "(", "start", ",", "abstract_state_goal", ",", "adjacency", ")", ":", "\n", "    ", "def", "backtrack", "(", "visited_states", ",", "current_state", ")", ":", "\n", "        ", "path", "=", "[", "]", "\n", "skill", "=", "None", "\n", "while", "current_state", "is", "not", "None", ":", "\n", "            ", "path", ".", "append", "(", "(", "current_state", ",", "skill", ")", ")", "\n", "skill", "=", "visited_states", "[", "current_state", "]", "[", "2", "]", "\n", "current_state", "=", "visited_states", "[", "current_state", "]", "[", "0", "]", "\n", "", "path", ".", "reverse", "(", ")", "\n", "if", "len", "(", "path", ")", "==", "1", ":", "\n", "            ", "path", "[", "0", "]", "=", "(", "path", "[", "0", "]", "[", "0", "]", ",", "path", "[", "0", "]", "[", "0", "]", ")", "# the skill is self", "\n", "", "return", "path", "\n", "\n", "", "visited_states", "=", "{", "start", ":", "(", "None", ",", "0", ",", "None", ")", "}", "\n", "frontier", "=", "[", "]", "\n", "heapq", ".", "heappush", "(", "frontier", ",", "(", "0", ",", "start", ")", ")", "\n", "while", "frontier", ":", "\n", "        ", "dist", ",", "current_state", "=", "heapq", ".", "heappop", "(", "frontier", ")", "\n", "if", "current_state", "==", "abstract_state_goal", ":", "\n", "            ", "return", "backtrack", "(", "visited_states", ",", "current_state", ")", "\n", "\n", "", "for", "n", "in", "get_nbrs", "(", "adjacency", ",", "current_state", ")", ":", "\n", "            ", "skill", "=", "int", "(", "n", ")", "# skill is just next abstract state", "\n", "edge_weight", "=", "adjacency", "[", "current_state", ",", "skill", "]", "\n", "\n", "if", "skill", "==", "current_state", "or", "edge_weight", "<", "0.01", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "skill", "not", "in", "visited_states", "or", "dist", "+", "edge_weight", "<", "visited_states", "[", "skill", "]", "[", "1", "]", ":", "\n", "                ", "new_dist", "=", "(", "dist", "+", "edge_weight", ")", "\n", "heapq", ".", "heappush", "(", "frontier", ",", "(", "new_dist", ",", "skill", ")", ")", "\n", "visited_states", "[", "skill", "]", "=", "(", "current_state", ",", "new_dist", ",", "skill", ")", "\n", "\n", "", "", "", "rand_act", "=", "start", "\n", "return", "[", "(", "start", ",", "rand_act", ")", ",", "(", "rand_act", ",", "None", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.delete_edge": [[95, 125], ["numpy.array", "len", "numpy.zeros", "numpy.argsort", "int", "int", "torch.clone", "utils.plan_abstract_path", "print", "print", "bad_edges.add", "print", "option_success[].item", "abstract_adjacency.nonzero().numpy", "len", "abstract_adjacency.nonzero"], "function", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.plan_abstract_path", "home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.ReplayBuffer.add"], ["", "def", "delete_edge", "(", "option_success", ",", "abstract_adjacency", ",", "num_times_stuck", ",", "visit_counts", ",", "bad_edges", ")", ":", "\n", "# sort the options in order of success rate", "\n", "    ", "rel_suc", "=", "np", ".", "array", "(", "[", "[", "option_success", "[", "tmp_s", ",", "tmp_z", "]", ".", "item", "(", ")", ",", "tmp_s", ",", "tmp_z", "]", "for", "tmp_s", ",", "tmp_z", "in", "abstract_adjacency", ".", "nonzero", "(", ")", ".", "numpy", "(", ")", "if", "tmp_s", "!=", "tmp_z", "]", ")", "\n", "# we want to avoid deleting things that were just added... use num_stuck", "\n", "if", "len", "(", "rel_suc", ")", "==", "0", ":", "\n", "        ", "rel_suc", "=", "np", ".", "zeros", "(", "(", "1", ",", "3", ")", ")", "\n", "", "sorted_suc", "=", "np", ".", "argsort", "(", "rel_suc", ",", "axis", "=", "0", ")", "[", ":", ",", "0", "]", "\n", "for", "min_suc", "in", "sorted_suc", ":", "\n", "        ", "edge_start", "=", "int", "(", "rel_suc", "[", "min_suc", ",", "1", "]", ")", "\n", "edge_end", "=", "int", "(", "rel_suc", "[", "min_suc", ",", "2", "]", ")", "\n", "# don't delete an edge if we haven't tried it enough, it is very successful, or the target is rarely visited", "\n", "if", "num_times_stuck", "[", "edge_start", ",", "edge_end", "]", "<", "10", "or", "rel_suc", "[", "min_suc", ",", "0", "]", ">", "0.5", "or", "visit_counts", "[", "edge_end", "]", "<", "50", ":", "\n", "            ", "continue", "# TODO: some magic numbers here...", "\n", "\n", "", "tmp_adj", "=", "torch", ".", "clone", "(", "abstract_adjacency", ")", "\n", "tmp_adj", "[", "edge_start", ",", "edge_end", "]", "=", "0", "\n", "\n", "can_remove_path", "=", "plan_abstract_path", "(", "edge_start", ",", "edge_end", ",", "tmp_adj", ")", "\n", "can_remove", "=", "(", "can_remove_path", "[", "-", "1", "]", "[", "0", "]", "==", "edge_end", ")", "and", "len", "(", "can_remove_path", ")", "<", "5", "\n", "if", "can_remove", ":", "\n", "            ", "print", "(", "f\"Removed edge {edge_start}-->{edge_end}, success was {rel_suc[min_suc, 0]:0.3f}, num_stuck is {num_times_stuck[edge_start, edge_end]}\"", ")", "\n", "print", "(", "f\"Alternative path is {[tmpp[0] for tmpp in can_remove_path]}\"", ")", "\n", "# NOTE: both pytorch arrays and python sets are passed by reference so this will affect inputs", "\n", "abstract_adjacency", "[", "edge_start", ",", "edge_end", "]", "=", "0", "\n", "bad_edges", ".", "add", "(", "(", "edge_start", ",", "edge_end", ")", ")", "\n", "# only delete one edge", "\n", "return", "(", "edge_start", ",", "edge_end", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "f\"Cannot remove {edge_start}-->{edge_end}\"", ")", "\n", "", "", "return", "None", "", "", ""]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.vis.draw_abstract_mdp": [[8, 27], ["matplotlib.clf", "nx.MultiDiGraph", "range", "nx.spring_layout", "nx.draw", "matplotlib.savefig", "len", "utils.get_nbrs", "len", "nx.MultiDiGraph.add_edge"], "function", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.utils.get_nbrs"], ["def", "draw_abstract_mdp", "(", "adjacency", ",", "save_path", "=", "\"\"", ")", ":", "\n", "    ", "import", "networkx", "as", "nx", "\n", "plt", ".", "clf", "(", ")", "\n", "G", "=", "nx", ".", "MultiDiGraph", "(", ")", "\n", "for", "s", "in", "range", "(", "len", "(", "adjacency", ")", ")", ":", "\n", "        ", "nbrs", "=", "get_nbrs", "(", "adjacency", ",", "s", ")", "\n", "if", "len", "(", "nbrs", ")", "==", "1", ":", "\n", "            ", "continue", "\n", "", "for", "s_prime", "in", "nbrs", ":", "\n", "            ", "e", "=", "(", "s", ",", "s_prime", ")", "\n", "G", ".", "add_edge", "(", "e", "[", "0", "]", ",", "e", "[", "1", "]", ")", "\n", "\n", "", "", "pos", "=", "nx", ".", "spring_layout", "(", "G", ")", "\n", "nx", ".", "draw", "(", "G", ",", "pos", ",", "\n", "with_labels", "=", "True", ",", "\n", "connectionstyle", "=", "'arc3, rad = 0.1'", ",", "\n", "node_color", "=", "'lightgreen'", ")", "\n", "\n", "plt", ".", "savefig", "(", "\"{}/abstract_mdp.png\"", ".", "format", "(", "save_path", ")", ",", "format", "=", "\"PNG\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.vis.cspace_video": [[28, 53], ["animation.FuncAnimation", "animation.FuncAnimation.save", "print", "matplotlib.clf", "numpy.meshgrid", "numpy.concatenate", "matplotlib.imshow", "matplotlib.colorbar", "matplotlib.figure", "numpy.arange", "numpy.arange", "numpy.ones", "numpy.concatenate", "torch.no_grad", "phi().argmax().view", "phi().argmax().view.numpy", "dim1.reshape", "dim2.reshape", "dim3.reshape", "numpy.zeros", "phi().argmax", "phi", "torch.FloatTensor"], "function", ["home.repos.pwc.inspect_result.amnonattali_dsaa.None.torch_models.Abstraction.phi"], ["", "def", "cspace_video", "(", "save_path", ",", "phi", ",", "num_abstract_states", ",", "yes_obj", "=", "False", ")", ":", "\n", "    ", "import", "matplotlib", ".", "animation", "as", "animation", "\n", "\n", "def", "plot_stuff", "(", "frame", ")", ":", "\n", "        ", "print", "(", "f\"\\r{frame}\"", ",", "end", "=", "\"\"", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "xvalues", "=", "np", ".", "arange", "(", "180", ")", "-", "90", "\n", "yvalues", "=", "np", ".", "arange", "(", "180", ")", "-", "90", "\n", "dim2", ",", "dim3", "=", "np", ".", "meshgrid", "(", "xvalues", ",", "yvalues", ")", "\n", "dim1", "=", "np", ".", "ones", "(", "180", "*", "180", ")", "*", "(", "frame", "*", "3", "-", "90", ")", "\n", "pts", "=", "np", ".", "concatenate", "(", "(", "dim1", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "dim2", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "dim3", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ",", "axis", "=", "1", ")", "\n", "\n", "if", "yes_obj", ":", "\n", "            ", "obj_pose", "=", "np", ".", "zeros", "(", "(", "180", "*", "180", ",", "2", ")", ")", "+", "13.0", "# obj starts at (13,13)", "\n", "pts", "=", "np", ".", "concatenate", "(", "(", "pts", ",", "obj_pose", ")", ",", "axis", "=", "1", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "abstract_state", "=", "phi", "(", "torch", ".", "FloatTensor", "(", "pts", ")", ")", ".", "argmax", "(", "dim", "=", "1", ")", ".", "view", "(", "180", ",", "180", ")", "\n", "\n", "", "plt", ".", "imshow", "(", "abstract_state", ".", "numpy", "(", ")", ",", "vmin", "=", "0", ",", "vmax", "=", "num_abstract_states", "-", "1.0", ")", "\n", "plt", ".", "colorbar", "(", ")", "\n", "return", "[", "]", "\n", "\n", "", "ani", "=", "animation", ".", "FuncAnimation", "(", "plt", ".", "figure", "(", ")", ",", "plot_stuff", ",", "frames", "=", "180", "//", "3", ",", "interval", "=", "100", ",", "blit", "=", "True", ")", "\n", "ani", ".", "save", "(", "\"{}/cspace.mp4\"", ".", "format", "(", "save_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.vis.make_sr_vis": [[54, 133], ["matplotlib.clf", "TwoRoomsViz", "TwoRoomsViz.reset", "numpy.argmax", "numpy.zeros", "range", "print", "range", "print", "matplotlib.subplots", "numpy.zeros", "enumerate", "range", "fig.add_axes", "matplotlib.colorbar", "plt.colorbar.set_ticks", "plt.colorbar.set_ticklabels", "fig.suptitle", "matplotlib.savefig", "matplotlib.close", "random.randrange", "TwoRoomsViz.step", "numpy.argmax", "state_pairs.append", "numpy.random.permutation", "enumerate", "print", "ax[].imshow", "ax[].set_axis_off", "dir_to_wall.append", "len", "len", "vis.make_sr_vis.wall_hugging_option"], "function", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reset", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.close", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step"], ["", "def", "make_sr_vis", "(", "save_path", ",", "option_type", "=", "\"uniform\"", ")", ":", "\n", "    ", "plt", ".", "clf", "(", ")", "\n", "from", "environments", ".", "env_wrappers", "import", "TwoRoomsViz", "\n", "def", "wall_hugging_option", "(", "env", ")", ":", "\n", "        ", "dir_to_vec", "=", "{", "0", ":", "[", "-", "1", ",", "0", "]", ",", "1", ":", "[", "0", ",", "1", "]", ",", "2", ":", "[", "1", ",", "0", "]", ",", "3", ":", "[", "0", ",", "-", "1", "]", "}", "\n", "dir_to_wall", "=", "[", "]", "\n", "for", "action", "in", "dir_to_vec", ":", "\n", "            ", "tmp_pos", "=", "env", ".", "agent_pos", "[", ":", "2", "]", "+", "dir_to_vec", "[", "action", "]", "\n", "dir_to_wall", ".", "append", "(", "not", "env", ".", "is_free", "(", "tmp_pos", ")", ")", "\n", "\n", "", "action", "=", "random", ".", "randrange", "(", "4", ")", "\n", "if", "dir_to_wall", "[", "0", "]", ":", "\n", "            ", "action", "=", "3", "\n", "", "if", "dir_to_wall", "[", "3", "]", ":", "\n", "            ", "action", "=", "2", "\n", "", "if", "dir_to_wall", "[", "2", "]", ":", "\n", "            ", "action", "=", "1", "\n", "", "if", "dir_to_wall", "[", "1", "]", "and", "not", "dir_to_wall", "[", "0", "]", ":", "\n", "            ", "action", "=", "0", "\n", "\n", "", "if", "dir_to_wall", "[", "0", "]", "and", "dir_to_wall", "[", "2", "]", ":", "\n", "            ", "action", "=", "1", "\n", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                ", "action", "=", "3", "\n", "", "", "return", "action", "\n", "\n", "", "env", "=", "TwoRoomsViz", "(", ")", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "i", "=", "np", ".", "argmax", "(", "state", ")", "\n", "psi", "=", "np", ".", "zeros", "(", "(", "len", "(", "state", ")", ",", "len", "(", "state", ")", ")", ")", "\n", "state_pairs", "=", "[", "]", "\n", "for", "cur_idx", "in", "range", "(", "300000", ")", ":", "\n", "        ", "if", "option_type", "==", "\"hugwall\"", ":", "\n", "            ", "action", "=", "wall_hugging_option", "(", "env", ".", "env", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "random", ".", "randrange", "(", "env", ".", "action_size", ")", "\n", "\n", "", "next_state", ",", "_", ",", "_", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "next_i", "=", "np", ".", "argmax", "(", "next_state", ")", "\n", "state_pairs", ".", "append", "(", "[", "state", ",", "i", ",", "next_i", "]", ")", "\n", "\n", "i", "=", "next_i", "\n", "state", "=", "next_state", "\n", "if", "cur_idx", "%", "1000", "==", "0", ":", "\n", "            ", "print", "(", "cur_idx", ",", "end", "=", "\"\\r\"", ")", "\n", "", "", "print", "(", "\"Done exploration\"", ")", "\n", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "        ", "perm", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "state_pairs", ")", ")", "\n", "for", "cur_idx", ",", "p_i", "in", "enumerate", "(", "perm", ")", ":", "\n", "            ", "state", ",", "i", ",", "next_i", "=", "state_pairs", "[", "p_i", "]", "\n", "psi", "[", "i", "]", "=", "psi", "[", "i", "]", "+", "0.1", "*", "(", "(", "state", "+", "0.99", "*", "psi", "[", "next_i", "]", ")", "-", "psi", "[", "i", "]", ")", "\n", "if", "cur_idx", "%", "5000", "==", "0", ":", "\n", "                ", "print", "(", "cur_idx", ",", "end", "=", "\"\\r\"", ")", "\n", "", "", "", "print", "(", "\"Done Update\"", ")", "\n", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "nrows", "=", "1", ",", "ncols", "=", "3", ")", "\n", "diffs", "=", "np", ".", "zeros", "(", "(", "3", ",", "10", ",", "19", ")", ")", "\n", "ref_squares", "=", "[", "[", "4", ",", "4", "]", ",", "[", "4", ",", "9", "]", ",", "[", "4", ",", "14", "]", "]", "\n", "for", "ax_idx", ",", "ref_square", "in", "enumerate", "(", "ref_squares", ")", ":", "\n", "        ", "idx", "=", "19", "*", "ref_square", "[", "0", "]", "+", "ref_square", "[", "1", "]", "\n", "print", "(", "idx", ")", "\n", "diffs", "[", "ax_idx", "]", "+=", "np", ".", "abs", "(", "psi", "-", "psi", "[", "idx", "]", ")", ".", "sum", "(", "axis", "=", "1", ")", ".", "reshape", "(", "19", ",", "19", ")", "[", ":", "10", ",", ":", "19", "]", "\n", "walls", "=", "psi", ".", "sum", "(", "axis", "=", "1", ")", ".", "reshape", "(", "19", ",", "19", ")", "[", ":", "10", ",", ":", "19", "]", "<", "0.01", "\n", "diffs", "[", "ax_idx", "]", "[", "walls", ".", "nonzero", "(", ")", "]", "=", "-", "1.0", "\n", "\n", "", "for", "ax_idx", "in", "range", "(", "3", ")", ":", "\n", "        ", "im", "=", "ax", "[", "ax_idx", "]", ".", "imshow", "(", "diffs", "[", "ax_idx", "]", ",", "vmin", "=", "-", "1.0", ",", "vmax", "=", "np", ".", "max", "(", "diffs", ")", ",", "cmap", "=", "\"hot\"", ")", "\n", "ax", "[", "ax_idx", "]", ".", "set_axis_off", "(", ")", "\n", "\n", "", "cax", "=", "fig", ".", "add_axes", "(", "[", "ax", "[", "2", "]", ".", "get_position", "(", ")", ".", "x1", "+", "0.01", ",", "ax", "[", "2", "]", ".", "get_position", "(", ")", ".", "y0", ",", "0.02", ",", "ax", "[", "2", "]", ".", "get_position", "(", ")", ".", "height", "]", ")", "\n", "cbar", "=", "plt", ".", "colorbar", "(", "im", ",", "cax", "=", "cax", ")", "\n", "\n", "cbar", ".", "set_ticks", "(", "[", "0", ",", "int", "(", "np", ".", "max", "(", "diffs", ")", ")", "]", ")", "\n", "cbar", ".", "set_ticklabels", "(", "[", "0", ",", "int", "(", "np", ".", "max", "(", "diffs", ")", ")", "]", ")", "\n", "\n", "fig", ".", "suptitle", "(", "f\"Relative SR distance under {option_type} policy\"", ")", "\n", "plt", ".", "savefig", "(", "f\"{save_path}/SR_{option_type}.png\"", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.None.vis.plot_returns_from_paper": [[134, 186], ["matplotlib.clf", "pickle.load", "pickle.load", "pickle.load", "pickle.load", "max", "vis.plot_returns_from_paper.make_np"], "function", ["None"], ["", "def", "plot_returns_from_paper", "(", ")", ":", "\n", "    ", "plt", ".", "clf", "(", ")", "\n", "def", "make_np", "(", "ar", ",", "longest", ")", ":", "\n", "        ", "new_ar", "=", "np", ".", "ones", "(", "(", "len", "(", "ar", ")", ",", "longest", ")", ")", "\n", "for", "i", ",", "j", "in", "enumerate", "(", "ar", ")", ":", "\n", "            ", "new_ar", "[", "i", "]", "[", "0", ":", "len", "(", "j", ")", "]", "=", "j", "\n", "", "return", "new_ar", "\n", "\n", "", "dsaa_easy", "=", "pickle", ".", "load", "(", "open", "(", "\"saved_data/dsaa_easy.pickle\"", ",", "\"rb\"", ")", ")", "\n", "baseline_easy", "=", "pickle", ".", "load", "(", "open", "(", "\"saved_data/baseline_easy.pickle\"", ",", "\"rb\"", ")", ")", "\n", "baseline_hard", "=", "pickle", ".", "load", "(", "open", "(", "\"saved_data/baseline_hard.pickle\"", ",", "\"rb\"", ")", ")", "\n", "dsaa_hard", "=", "pickle", ".", "load", "(", "open", "(", "\"saved_data/dsaa_hard.pickle\"", ",", "\"rb\"", ")", ")", "\n", "\n", "longest", "=", "max", "(", "[", "len", "(", "a", ")", "for", "a", "in", "dsaa_easy", "+", "baseline_easy", "+", "baseline_hard", "+", "dsaa_hard", "]", ")", "\n", "d_easy", "=", "make_np", "(", "dsaa_easy", ",", "longest", ")", "\n", "d_hard", "=", "make_np", "(", "dsaa_hard", ",", "longest", ")", "\n", "b_easy", "=", "make_np", "(", "baseline_easy", ",", "longest", ")", "\n", "b_hard", "=", "make_np", "(", "baseline_hard", ",", "longest", ")", "\n", "\n", "gamma", "=", "0.9", "\n", "for", "col", "in", "range", "(", "1", ",", "longest", ")", ":", "\n", "        ", "d_easy", "[", ":", ",", "col", "]", "=", "d_easy", "[", ":", ",", "col", "-", "1", "]", "*", "gamma", "+", "d_easy", "[", ":", ",", "col", "]", "*", "(", "1", "-", "gamma", ")", "\n", "d_hard", "[", ":", ",", "col", "]", "=", "d_hard", "[", ":", ",", "col", "-", "1", "]", "*", "gamma", "+", "d_hard", "[", ":", ",", "col", "]", "*", "(", "1", "-", "gamma", ")", "\n", "\n", "b_easy", "[", ":", ",", "col", "]", "=", "b_easy", "[", ":", ",", "col", "-", "1", "]", "*", "gamma", "+", "b_easy", "[", ":", ",", "col", "]", "*", "(", "1", "-", "gamma", ")", "\n", "b_hard", "[", ":", ",", "col", "]", "=", "b_hard", "[", ":", ",", "col", "-", "1", "]", "*", "gamma", "+", "b_hard", "[", ":", ",", "col", "]", "*", "(", "1", "-", "gamma", ")", "\n", "\n", "", "means_d_easy", "=", "np", ".", "mean", "(", "d_easy", ",", "axis", "=", "0", ")", "\n", "stds_d_easy", "=", "np", ".", "std", "(", "d_easy", ",", "axis", "=", "0", ")", "\n", "means_d_hard", "=", "np", ".", "mean", "(", "d_hard", ",", "axis", "=", "0", ")", "\n", "stds_d_hard", "=", "np", ".", "std", "(", "d_hard", ",", "axis", "=", "0", ")", "\n", "\n", "means_b_easy", "=", "np", ".", "mean", "(", "b_easy", ",", "axis", "=", "0", ")", "\n", "stds_b_easy", "=", "np", ".", "std", "(", "b_easy", ",", "axis", "=", "0", ")", "\n", "means_b_hard", "=", "np", ".", "mean", "(", "b_hard", ",", "axis", "=", "0", ")", "\n", "stds_b_hard", "=", "np", ".", "std", "(", "b_hard", ",", "axis", "=", "0", ")", "\n", "\n", "x", "=", "np", ".", "arange", "(", "longest", ")", "\n", "plt", ".", "plot", "(", "x", ",", "means_d_easy", ",", "label", "=", "\"dsaa_easy\"", ")", "\n", "plt", ".", "plot", "(", "x", ",", "means_d_hard", ",", "label", "=", "\"dsaa_hard\"", ")", "\n", "plt", ".", "plot", "(", "x", ",", "means_b_easy", ",", "label", "=", "\"base_easy\"", ")", "\n", "plt", ".", "plot", "(", "x", ",", "means_b_hard", ",", "label", "=", "\"base_hard\"", ")", "\n", "plt", ".", "xlabel", "(", "\"Number of Episodes\"", ",", "fontsize", "=", "13", ")", "\n", "plt", ".", "xticks", "(", "fontsize", "=", "13", ")", "\n", "plt", ".", "yticks", "(", "fontsize", "=", "13", ")", "\n", "plt", ".", "legend", "(", "fontsize", "=", "14", ")", "\n", "plt", ".", "ylabel", "(", "\"Average Return\"", ",", "fontsize", "=", "13", ")", "\n", "plt", ".", "fill_between", "(", "x", ",", "(", "means_d_easy", "-", "stds_d_easy", ")", ".", "clip", "(", "0", ",", "1", ")", ",", "(", "means_d_easy", "+", "stds_d_easy", ")", ".", "clip", "(", "0", ",", "1", ")", ",", "color", "=", "\"blue\"", ",", "alpha", "=", "0.2", ")", "\n", "# plt.fill_between(x, (means_d_hard-stds_d_hard).clip(0,1), (means_d_hard+stds_d_hard).clip(0,1), color=\"orange\", alpha=0.2)", "\n", "plt", ".", "fill_between", "(", "x", ",", "(", "means_b_easy", "-", "stds_b_easy", ")", ".", "clip", "(", "0", ",", "1", ")", ",", "(", "means_b_easy", "+", "stds_b_easy", ")", ".", "clip", "(", "0", ",", "1", ")", ",", "color", "=", "\"green\"", ",", "alpha", "=", "0.2", ")", "\n", "# plt.fill_between(x, (means_b_hard-stds_b_hard).clip(0,1), (means_b_hard+stds_b_hard).clip(0,1), color=\"red\", alpha=0.2)", "\n", "plt", ".", "savefig", "(", "\"saved_data/paper_avg_returns.png\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.BaseFourRooms.__init__": [[15, 25], ["gym.make", "gym.Wrapper.__init__", "gym.make._make_obs"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.DrawArm.__init__", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._make_obs"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "max_steps", "=", "config", "[", "\"max_steps\"", "]", "\n", "env", "=", "gym", ".", "make", "(", "'dsaa_envs:fourrooms-v0'", ",", "max_steps", "=", "max_steps", ")", "\n", "super", "(", "BaseFourRooms", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "example_obs", "=", "env", ".", "_make_obs", "(", ")", "\n", "\n", "self", ".", "observation_size", "=", "2", "\n", "self", ".", "action_size", "=", "4", "\n", "self", ".", "preprocessors", "=", "[", "obs_to_loc", "]", "\n", "self", ".", "name", "=", "\"four_rooms\"", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.BaseFourRooms.reset": [[26, 29], ["env_wrappers.BaseFourRooms.env.reset", "env_wrappers.obs_to_loc"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reset", "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.obs_to_loc"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "return", "obs_to_loc", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.BaseFourRooms.step": [[30, 33], ["env_wrappers.BaseFourRooms.env.step", "env_wrappers.obs_to_loc"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step", "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.obs_to_loc"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "return", "obs_to_loc", "(", "obs", ")", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.BaseFourRooms.close": [[34, 36], ["env_wrappers.BaseFourRooms.env.close"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.FourRoomsRandomNoise.__init__": [[39, 49], ["gym.make", "gym.Wrapper.__init__", "gym.make._make_obs"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.DrawArm.__init__", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._make_obs"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "max_steps", "=", "config", "[", "\"max_steps\"", "]", "\n", "env", "=", "gym", ".", "make", "(", "'dsaa_envs:fourrooms-v0'", ",", "max_steps", "=", "max_steps", ")", "\n", "super", "(", "FourRoomsRandomNoise", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "example_obs", "=", "env", ".", "_make_obs", "(", ")", "\n", "\n", "self", ".", "observation_size", "=", "3", "\n", "self", ".", "action_size", "=", "4", "\n", "self", ".", "preprocessors", "=", "[", "self", ".", "add_bit", "]", "\n", "self", ".", "name", "=", "\"four_rooms_random_bit\"", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.FourRoomsRandomNoise.add_bit": [[50, 52], ["env_wrappers.obs_to_loc", "random.random"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.obs_to_loc"], ["", "def", "add_bit", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "obs_to_loc", "(", "x", ")", "+", "[", "100", "*", "random", ".", "random", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.FourRoomsRandomNoise.reset": [[53, 56], ["env_wrappers.FourRoomsRandomNoise.env.reset", "env_wrappers.FourRoomsRandomNoise.add_bit"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reset", "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.FourRoomsRandomNoise.add_bit"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "return", "self", ".", "add_bit", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.FourRoomsRandomNoise.step": [[57, 60], ["env_wrappers.FourRoomsRandomNoise.env.step", "env_wrappers.FourRoomsRandomNoise.add_bit"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step", "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.FourRoomsRandomNoise.add_bit"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "return", "self", ".", "add_bit", "(", "obs", ")", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.FourRoomsRandomNoise.close": [[61, 63], ["env_wrappers.FourRoomsRandomNoise.env.close"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.TwoRoomsViz.__init__": [[65, 72], ["gym.make", "gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.DrawArm.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "env", "=", "gym", ".", "make", "(", "'dsaa_envs:fourrooms-v0'", ",", "max_steps", "=", "10000", ")", "\n", "super", "(", "TwoRoomsViz", ",", "self", ")", ".", "__init__", "(", "self", ".", "env", ")", "\n", "\n", "self", ".", "observation_size", "=", "2", "\n", "self", ".", "action_size", "=", "4", "\n", "self", ".", "name", "=", "\"make_vis\"", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.TwoRoomsViz.make_state": [[73, 77], ["torch.tensor().view().flatten", "list", "torch.tensor().view", "torch.tensor"], "methods", ["None"], ["", "def", "make_state", "(", "self", ",", "obs", ")", ":", "\n", "# Make one_hot", "\n", "        ", "obs", "=", "torch", ".", "tensor", "(", "obs", ",", "dtype", "=", "torch", ".", "float", ")", ".", "view", "(", "obs", ".", "shape", "[", "-", "2", "]", ",", "obs", ".", "shape", "[", "-", "1", "]", ")", ".", "flatten", "(", ")", "\n", "return", "list", "(", "1.0", "*", "(", "obs", "==", "2", ")", ".", "numpy", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.TwoRoomsViz.reset": [[78, 84], ["env_wrappers.TwoRoomsViz.env.reset", "env_wrappers.TwoRoomsViz.make_state"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reset", "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.TwoRoomsViz.make_state"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "# block off the bottom two rooms", "\n", "self", ".", "env", ".", "grid", "[", "9", ",", "4", "]", "=", "1", "\n", "self", ".", "env", ".", "grid", "[", "9", ",", "14", "]", "=", "1", "\n", "return", "self", ".", "make_state", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.TwoRoomsViz.step": [[85, 89], ["env_wrappers.TwoRoomsViz.env.step", "env_wrappers.TwoRoomsViz.make_state"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step", "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.TwoRoomsViz.make_state"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "state", "=", "self", ".", "make_state", "(", "obs", ")", "\n", "return", "state", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2D.__init__": [[94, 109], ["gym.make", "gym.Wrapper.__init__", "env_wrappers.Manipulator2D.make_obs", "numpy.zeros", "len"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.DrawArm.__init__", "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2DNoOBJ.make_obs"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "self", ".", "num_joints", "=", "config", "[", "\"num_arm_joints\"", "]", "\n", "self", ".", "arm_lengths", "=", "np", ".", "zeros", "(", "self", ".", "num_joints", ")", "+", "config", "[", "\"arm_joint_lengths\"", "]", "\n", "self", ".", "task_height", "=", "config", "[", "\"ball_goal_height\"", "]", "\n", "env", "=", "gym", ".", "make", "(", "'dsaa_envs:manipulator2d-v0'", ",", "num_joints", "=", "self", ".", "num_joints", ",", "\n", "arm_lengths", "=", "self", ".", "arm_lengths", ",", "max_steps", "=", "config", "[", "\"max_steps\"", "]", ")", "\n", "\n", "super", "(", "Manipulator2D", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "\n", "self", ".", "example_obs", "=", "self", ".", "make_obs", "(", "env", ".", "config", ")", "\n", "\n", "# there is one movable object", "\n", "self", ".", "observation_size", "=", "self", ".", "num_joints", "+", "len", "(", "self", ".", "env", ".", "movable_objects", "[", "0", "]", ")", "-", "1", "\n", "self", ".", "action_size", "=", "self", ".", "num_joints", "*", "2", "# for each joint you can move angle +-1", "\n", "self", ".", "name", "=", "\"manipulator_2d_yes_obj\"", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2D.make_obs": [[110, 112], ["list", "numpy.concatenate"], "methods", ["None"], ["", "def", "make_obs", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "list", "(", "np", ".", "concatenate", "(", "(", "obs", ",", "self", ".", "env", ".", "movable_objects", "[", "0", "]", "[", ":", "2", "]", ")", ",", "axis", "=", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2D.reset": [[113, 115], ["env_wrappers.Manipulator2D.make_obs", "env_wrappers.Manipulator2D.env.reset"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2DNoOBJ.make_obs", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "make_obs", "(", "self", ".", "env", ".", "reset", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2D.step": [[116, 120], ["env_wrappers.Manipulator2D.env.step", "env_wrappers.Manipulator2D.make_obs"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step", "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2DNoOBJ.make_obs"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "env_reward", "=", "1", "if", "self", ".", "env", ".", "movable_objects", "[", "0", "]", "[", "1", "]", "<", "self", ".", "task_height", "else", "0", "\n", "return", "self", ".", "make_obs", "(", "obs", ")", ",", "env_reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2D.close": [[121, 123], ["env_wrappers.Manipulator2D.env.close"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2DNoOBJ.__init__": [[126, 140], ["gym.make", "gym.Wrapper.__init__", "env_wrappers.Manipulator2DNoOBJ.make_obs", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.DrawArm.__init__", "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2DNoOBJ.make_obs"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "self", ".", "num_joints", "=", "config", "[", "\"num_arm_joints\"", "]", "\n", "self", ".", "arm_lengths", "=", "np", ".", "zeros", "(", "self", ".", "num_joints", ")", "+", "config", "[", "\"arm_joint_lengths\"", "]", "\n", "self", ".", "task_height", "=", "config", "[", "\"ball_goal_height\"", "]", "\n", "env", "=", "gym", ".", "make", "(", "'dsaa_envs:manipulator2d-v0'", ",", "num_joints", "=", "self", ".", "num_joints", ",", "\n", "arm_lengths", "=", "self", ".", "arm_lengths", ",", "max_steps", "=", "config", "[", "\"max_steps\"", "]", ")", "\n", "\n", "super", "(", "Manipulator2DNoOBJ", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "\n", "self", ".", "example_obs", "=", "self", ".", "make_obs", "(", "env", ".", "config", ")", "\n", "\n", "self", ".", "observation_size", "=", "self", ".", "num_joints", "\n", "self", ".", "action_size", "=", "self", ".", "num_joints", "*", "2", "\n", "self", ".", "name", "=", "\"manipulator_2d_no_obj\"", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2DNoOBJ.make_obs": [[141, 143], ["list"], "methods", ["None"], ["", "def", "make_obs", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "list", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2DNoOBJ.reset": [[144, 146], ["env_wrappers.Manipulator2DNoOBJ.make_obs", "env_wrappers.Manipulator2DNoOBJ.env.reset"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2DNoOBJ.make_obs", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "make_obs", "(", "self", ".", "env", ".", "reset", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2DNoOBJ.step": [[147, 151], ["env_wrappers.Manipulator2DNoOBJ.env.step", "env_wrappers.Manipulator2DNoOBJ.make_obs"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step", "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2DNoOBJ.make_obs"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "env_reward", "=", "1", "if", "self", ".", "env", ".", "movable_objects", "[", "0", "]", "[", "1", "]", "<", "self", ".", "task_height", "else", "0", "\n", "return", "self", ".", "make_obs", "(", "obs", ")", ",", "env_reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.Manipulator2DNoOBJ.close": [[152, 154], ["env_wrappers.Manipulator2DNoOBJ.env.close"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "close", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.amnonattali_dsaa.environments.env_wrappers.obs_to_loc": [[6, 10], ["torch.tensor().view", "torch.tensor().view.view().argmax", "torch.tensor", "torch.tensor().view.view", "obs.view().argmax.item", "obs.view().argmax.item", "len"], "function", ["None"], ["def", "obs_to_loc", "(", "obs", ")", ":", "\n", "    ", "obs", "=", "torch", ".", "tensor", "(", "obs", ",", "dtype", "=", "torch", ".", "float", ")", ".", "view", "(", "1", ",", "1", ",", "obs", ".", "shape", "[", "-", "2", "]", ",", "obs", ".", "shape", "[", "-", "1", "]", ")", "\n", "idx", "=", "obs", ".", "view", "(", "len", "(", "obs", ")", ",", "-", "1", ")", ".", "argmax", "(", "dim", "=", "1", ")", "\n", "return", "[", "idx", ".", "item", "(", ")", "//", "obs", ".", "shape", "[", "2", "]", ",", "idx", ".", "item", "(", ")", "%", "obs", ".", "shape", "[", "3", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.__init__": [[19, 67], ["print", "fourrooms.FourRoomsEnv._make_grid", "gym.spaces.Discrete", "gym.spaces.Box", "fourrooms.FourRoomsEnv.reset", "print", "print", "print"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._make_grid", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reset"], ["def", "__init__", "(", "self", ",", "size", "=", "19", ",", "max_steps", "=", "100", ",", "goal", "=", "None", ",", "no_env_reward", "=", "False", ")", ":", "\n", "# print(size, max_steps, goal)", "\n", "        ", "self", ".", "max_steps", "=", "max_steps", "\n", "self", ".", "loc_goal", "=", "goal", "\n", "self", ".", "no_env_reward", "=", "no_env_reward", "\n", "if", "self", ".", "no_env_reward", ":", "\n", "            ", "print", "(", "\"Unsupervised environment\"", ")", "\n", "", "elif", "self", ".", "loc_goal", "is", "None", ":", "\n", "            ", "print", "(", "\"Goal is fourth room\"", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Goal is:\"", ",", "self", ".", "loc_goal", ")", "\n", "", "print", "(", "\"Max steps:\"", ",", "self", ".", "max_steps", ")", "\n", "\n", "self", ".", "width", "=", "size", "\n", "self", ".", "height", "=", "size", "\n", "\n", "# make the grid", "\n", "self", ".", "_make_grid", "(", ")", "\n", "\n", "# turn left, turn right, forward", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "4", ")", "\n", "# free/wall/agent", "\n", "'''\n        self.grid_space = spaces.Box(\n            low=0,\n            high=2,\n            shape=(self.height, self.width, 1),\n            dtype='uint8'\n        )\n        self.observation_space = spaces.Dict({\n            'image': self.grid_space,\n            #'direction': spaces.Discrete(4)\n        })\n        '''", "\n", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "0", ",", "\n", "high", "=", "255", ",", "\n", "shape", "=", "(", "1", ",", "self", ".", "height", ",", "self", ".", "width", ")", ",", "\n", "dtype", "=", "'uint8'", "\n", ")", "\n", "\n", "#self.observation_space = spaces.Box(low=0, high=19, shape=(2,))", "\n", "\n", "# Initialize the state", "\n", "self", ".", "reset", "(", ")", "\n", "\n", "self", ".", "viewer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._make_grid": [[68, 87], ["numpy.zeros"], "methods", ["None"], ["", "def", "_make_grid", "(", "self", ")", ":", "\n", "# zero means free space", "\n", "        ", "self", ".", "grid", "=", "np", ".", "zeros", "(", "shape", "=", "(", "self", ".", "height", ",", "self", ".", "width", ")", ")", "\n", "\n", "# surrounding walls", "\n", "self", ".", "grid", "[", "0", ",", ":", "]", "=", "1", "\n", "self", ".", "grid", "[", "self", ".", "height", "-", "1", ",", ":", "]", "=", "1", "\n", "self", ".", "grid", "[", ":", ",", "0", "]", "=", "1", "\n", "self", ".", "grid", "[", ":", ",", "self", ".", "width", "-", "1", "]", "=", "1", "\n", "\n", "# middle walls", "\n", "self", ".", "grid", "[", "self", ".", "height", "//", "2", ",", ":", "]", "=", "1", "\n", "self", ".", "grid", "[", ":", ",", "self", ".", "width", "//", "2", "]", "=", "1", "\n", "\n", "# entry-ways", "\n", "self", ".", "grid", "[", "self", ".", "height", "//", "2", ",", "self", ".", "width", "//", "4", "]", "=", "0", "\n", "self", ".", "grid", "[", "self", ".", "height", "//", "2", ",", "3", "*", "self", ".", "width", "//", "4", "]", "=", "0", "\n", "self", ".", "grid", "[", "self", ".", "height", "//", "4", ",", "self", ".", "width", "//", "2", "]", "=", "0", "\n", "self", ".", "grid", "[", "3", "*", "self", ".", "height", "//", "4", ",", "self", ".", "width", "//", "2", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._is_free": [[88, 90], ["None"], "methods", ["None"], ["", "def", "_is_free", "(", "self", ",", "pos", ")", ":", "\n", "        ", "return", "self", ".", "grid", "[", "pos", "[", "0", "]", ",", "pos", "[", "1", "]", "]", "==", "0", "\n", "", "def", "is_free", "(", "self", ",", "pos", ")", ":", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.is_free": [[90, 92], ["None"], "methods", ["None"], ["", "def", "is_free", "(", "self", ",", "pos", ")", ":", "\n", "        ", "return", "self", ".", "grid", "[", "pos", "[", "0", "]", ",", "pos", "[", "1", "]", "]", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._is_goal": [[93, 108], ["None"], "methods", ["None"], ["", "def", "_is_goal", "(", "self", ",", "pos", ")", ":", "\n", "        ", "if", "not", "self", ".", "loc_goal", "is", "None", ":", "\n", "            ", "if", "pos", "[", "0", "]", "==", "self", ".", "loc_goal", "[", "0", "]", "and", "pos", "[", "1", "]", "==", "self", ".", "loc_goal", "[", "1", "]", ":", "\n", "                ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "if", "pos", "[", "0", "]", "<", "self", ".", "height", "//", "2", "and", "pos", "[", "1", "]", "<", "self", ".", "width", "//", "2", "and", "self", ".", "goal", "==", "0", ":", "\n", "            ", "return", "True", "\n", "", "if", "pos", "[", "0", "]", "<", "self", ".", "height", "//", "2", "and", "pos", "[", "1", "]", ">", "self", ".", "width", "//", "2", "and", "self", ".", "goal", "==", "1", ":", "\n", "            ", "return", "True", "\n", "", "if", "pos", "[", "0", "]", ">", "self", ".", "height", "//", "2", "and", "pos", "[", "1", "]", "<", "self", ".", "width", "//", "2", "and", "self", ".", "goal", "==", "2", ":", "\n", "            ", "return", "True", "\n", "", "if", "pos", "[", "0", "]", ">", "self", ".", "height", "//", "2", "and", "pos", "[", "1", "]", ">", "self", ".", "width", "//", "2", "and", "self", ".", "goal", "==", "3", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.step": [[109, 147], ["fourrooms.FourRoomsEnv._is_free", "fourrooms.FourRoomsEnv._is_goal", "fourrooms.FourRoomsEnv._make_obs", "numpy.copy"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._is_free", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._is_goal", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._make_obs"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "# never eat sour wheat", "\n", "        ", "dir_to_vec", "=", "{", "0", ":", "[", "-", "1", ",", "0", "]", ",", "1", ":", "[", "0", ",", "1", "]", ",", "2", ":", "[", "1", ",", "0", "]", ",", "3", ":", "[", "0", ",", "-", "1", "]", "}", "\n", "\n", "self", ".", "step_count", "+=", "1", "\n", "\n", "reward", "=", "0", "\n", "done", "=", "False", "\n", "\n", "tmp_pos", "=", "self", ".", "agent_pos", "[", ":", "2", "]", "+", "dir_to_vec", "[", "action", "]", "\n", "# this while loop means always move", "\n", "# while not self._is_free(tmp_pos):", "\n", "#     rand_act = random.randint(0,3)", "\n", "#     tmp_pos = self.agent_pos[:2] + dir_to_vec[rand_act]", "\n", "\n", "if", "self", ".", "_is_free", "(", "tmp_pos", ")", ":", "\n", "            ", "self", ".", "agent_pos", "[", ":", "2", "]", "=", "np", ".", "copy", "(", "tmp_pos", ")", "\n", "reward", "-=", "0.0", "\n", "", "else", ":", "\n", "            ", "done", "=", "False", "\n", "#reward -= 1", "\n", "\n", "\n", "", "if", "self", ".", "_is_goal", "(", "self", ".", "agent_pos", ")", ":", "\n", "            ", "done", "=", "True", "\n", "reward", "+=", "1", "\n", "\n", "", "obs", "=", "self", ".", "_make_obs", "(", ")", "\n", "\n", "#reward += 0.1*(self.agent_pos[0] + self.agent_pos[1])", "\n", "if", "self", ".", "no_env_reward", ":", "\n", "            ", "done", "=", "False", "\n", "reward", "=", "0", "\n", "\n", "", "if", "self", ".", "step_count", ">=", "self", ".", "max_steps", ":", "\n", "            ", "done", "=", "True", "\n", "\n", "", "return", "obs", ",", "reward", ",", "done", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._make_obs_2": [[148, 152], ["numpy.copy().reshape", "numpy.copy"], "methods", ["None"], ["", "def", "_make_obs_2", "(", "self", ")", ":", "\n", "        ", "grid_copy", "=", "np", ".", "copy", "(", "self", ".", "grid", ")", ".", "reshape", "(", "self", ".", "height", ",", "self", ".", "width", ",", "1", ")", "\n", "grid_copy", "[", "self", ".", "agent_pos", "[", "0", "]", ",", "self", ".", "agent_pos", "[", "1", "]", "]", "=", "2", "\n", "return", "{", "'image'", ":", "grid_copy", "}", "#, 'direction': self.agent_pos[2]}", "\n", "", "def", "_make_obs", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._make_obs": [[152, 156], ["numpy.copy().reshape", "numpy.copy"], "methods", ["None"], ["", "def", "_make_obs", "(", "self", ")", ":", "\n", "        ", "grid_copy", "=", "np", ".", "copy", "(", "self", ".", "grid", ")", ".", "reshape", "(", "1", ",", "self", ".", "height", ",", "self", ".", "width", ")", "\n", "grid_copy", "[", "0", ",", "self", ".", "agent_pos", "[", "0", "]", ",", "self", ".", "agent_pos", "[", "1", "]", "]", "=", "2", "\n", "return", "grid_copy", "\n", "", "def", "_make_obs_3", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._make_obs_3": [[156, 158], ["None"], "methods", ["None"], ["", "def", "_make_obs_3", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "agent_pos", "[", ":", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.reset": [[161, 176], ["numpy.random.randint", "numpy.array", "fourrooms.FourRoomsEnv._make_obs", "fourrooms.FourRoomsEnv._is_goal", "numpy.random.randint", "fourrooms.FourRoomsEnv._is_free"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._make_obs", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._is_goal", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv._is_free"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\n", "#self.goal = np.random.randint(4)", "\n", "# hard code goal for now", "\n", "        ", "self", ".", "goal", "=", "3", "\n", "\n", "pos", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "(", "0", ",", "0", ",", "0", ")", ",", "high", "=", "(", "self", ".", "height", ",", "self", ".", "width", ",", "4", ")", ",", "size", "=", "3", ")", "\n", "while", "not", "self", ".", "_is_free", "(", "pos", ")", "or", "self", ".", "_is_goal", "(", "pos", ")", ":", "\n", "            ", "pos", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "(", "0", ",", "0", ",", "0", ")", ",", "high", "=", "(", "self", ".", "height", ",", "self", ".", "width", ",", "4", ")", ",", "size", "=", "3", ")", "\n", "", "pos", "=", "np", ".", "array", "(", "[", "2", ",", "2", ",", "0", "]", ")", "\n", "\n", "self", ".", "agent_pos", "=", "pos", "\n", "self", ".", "step_count", "=", "0", "\n", "\n", "return", "self", ".", "_make_obs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.render": [[177, 223], ["numpy.zeros", "range", "range", "fourrooms.FourRoomsEnv.viewer.imshow", "rendering.SimpleImageViewer", "print", "print"], "methods", ["None"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ")", ":", "\n", "# Compute the total grid size", "\n", "        ", "width_px", "=", "self", ".", "width", "*", "TILE_SIZE", "\n", "height_px", "=", "self", ".", "height", "*", "TILE_SIZE", "\n", "\n", "img", "=", "np", ".", "zeros", "(", "shape", "=", "(", "height_px", ",", "width_px", ",", "3", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "\n", "# Render the grid", "\n", "for", "j", "in", "range", "(", "0", ",", "self", ".", "height", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "0", ",", "self", ".", "width", ")", ":", "\n", "                ", "ymin", "=", "j", "*", "TILE_SIZE", "\n", "ymax", "=", "(", "j", "+", "1", ")", "*", "TILE_SIZE", "\n", "xmin", "=", "i", "*", "TILE_SIZE", "\n", "xmax", "=", "(", "i", "+", "1", ")", "*", "TILE_SIZE", "\n", "\n", "if", "self", ".", "grid", "[", "j", ",", "i", "]", "==", "0", ":", "# free:", "\n", "                    ", "img", "[", "ymin", ":", "ymax", ",", "xmin", ":", "xmax", ",", ":", "]", "=", "FREE_IMG", "\n", "", "elif", "self", ".", "grid", "[", "j", ",", "i", "]", "==", "1", ":", "# wall:", "\n", "                    ", "img", "[", "ymin", ":", "ymax", ",", "xmin", ":", "xmax", ",", ":", "]", "=", "WALL_IMG", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"ERROR, grid should only have 0 and 1\"", ")", "\n", "# agent", "\n", "", "", "", "ymin", "=", "self", ".", "agent_pos", "[", "0", "]", "*", "TILE_SIZE", "\n", "ymax", "=", "(", "self", ".", "agent_pos", "[", "0", "]", "+", "1", ")", "*", "TILE_SIZE", "\n", "xmin", "=", "self", ".", "agent_pos", "[", "1", "]", "*", "TILE_SIZE", "\n", "xmax", "=", "(", "self", ".", "agent_pos", "[", "1", "]", "+", "1", ")", "*", "TILE_SIZE", "\n", "\n", "if", "self", ".", "agent_pos", "[", "2", "]", "==", "0", ":", "\n", "            ", "img", "[", "ymin", ":", "ymax", ",", "xmin", ":", "xmax", ",", ":", "]", "=", "AGENT_IMG_0", "\n", "", "elif", "self", ".", "agent_pos", "[", "2", "]", "==", "1", ":", "\n", "            ", "img", "[", "ymin", ":", "ymax", ",", "xmin", ":", "xmax", ",", ":", "]", "=", "AGENT_IMG_1", "\n", "", "elif", "self", ".", "agent_pos", "[", "2", "]", "==", "2", ":", "\n", "            ", "img", "[", "ymin", ":", "ymax", ",", "xmin", ":", "xmax", ",", ":", "]", "=", "AGENT_IMG_2", "\n", "", "elif", "self", ".", "agent_pos", "[", "2", "]", "==", "3", ":", "\n", "            ", "img", "[", "ymin", ":", "ymax", ",", "xmin", ":", "xmax", ",", ":", "]", "=", "AGENT_IMG_3", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"ERROR AGENT DIR\"", ")", "\n", "\n", "", "if", "mode", "==", "'rgb_array'", ":", "\n", "            ", "return", "img", "\n", "", "elif", "mode", "==", "'human'", ":", "\n", "            ", "from", "gym", ".", "envs", ".", "classic_control", "import", "rendering", "\n", "if", "self", ".", "viewer", "is", "None", ":", "\n", "                ", "self", ".", "viewer", "=", "rendering", ".", "SimpleImageViewer", "(", ")", "\n", "", "self", ".", "viewer", ".", "imshow", "(", "img", ")", "\n", "return", "self", ".", "viewer", ".", "isopen", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.get_keys_to_action": [[224, 231], ["ord", "ord", "ord"], "methods", ["None"], ["", "", "def", "get_keys_to_action", "(", "self", ")", ":", "\n", "        ", "keys_to_action", "=", "{", "}", "\n", "keys_to_action", "[", "ord", "(", "'a'", ")", "]", "=", "0", "\n", "keys_to_action", "[", "ord", "(", "'d'", ")", "]", "=", "1", "\n", "keys_to_action", "[", "ord", "(", "'w'", ")", "]", "=", "2", "\n", "\n", "return", "keys_to_action", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.get_action_meanings": [[232, 234], ["None"], "methods", ["None"], ["", "def", "get_action_meanings", "(", "self", ")", ":", "\n", "        ", "return", "[", "\"LEFT\"", ",", "\"RIGHT\"", ",", "\"FORWARD\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.close": [[235, 239], ["fourrooms.FourRoomsEnv.viewer.close"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.fourrooms.FourRoomsEnv.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "viewer", "is", "not", "None", ":", "\n", "            ", "self", ".", "viewer", ".", "close", "(", ")", "\n", "self", ".", "viewer", "=", "None", "", "", "", "", ""]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.__init__": [[12, 55], ["numpy.zeros", "manipulator2d.Manipulator2DEnv.update_workspace_config", "numpy.array", "numpy.copy", "manipulator2d.DrawArm", "gym.spaces.Discrete", "gym.spaces.Box", "manipulator2d.Manipulator2DEnv.reset", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.update_workspace_config", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reset"], ["    ", "def", "__init__", "(", "self", ",", "num_joints", ",", "arm_lengths", ",", "max_steps", "=", "5000", ",", "config", "=", "None", ")", ":", "\n", "        ", "self", ".", "num_joints", "=", "num_joints", "\n", "# each joint is limited in the angles it can take, we set this from -90 to 90 degrees", "\n", "self", ".", "min_angle", "=", "np", ".", "zeros", "(", "self", ".", "num_joints", ")", "-", "90", "\n", "self", ".", "max_angle", "=", "np", ".", "zeros", "(", "self", ".", "num_joints", ")", "+", "90", "\n", "# start_point is the starting (x,y) coordinate of the first limb/joint", "\n", "self", ".", "start_point", "=", "[", "0", ",", "0", "]", "\n", "# arm_lengths is the length of each arm limb", "\n", "self", ".", "arm_lengths", "=", "arm_lengths", "\n", "# config is the current angle each joint is at", "\n", "# the angle is relative to the previous joint", "\n", "if", "config", "is", "not", "None", ":", "self", ".", "config", "=", "config", "\n", "else", ":", "self", ".", "config", "=", "np", ".", "zeros", "(", "self", ".", "num_joints", ")", "\n", "\n", "# workspace_config is the (x,y) coordinate of each limb end", "\n", "self", ".", "workspace_config", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_joints", "+", "1", ",", "2", ")", ")", "\n", "self", ".", "update_workspace_config", "(", ")", "\n", "self", ".", "tip_position", "=", "self", ".", "workspace_config", "[", "-", "1", "]", "\n", "\n", "# self.goal = (10,-5,2)", "\n", "self", ".", "obstacles", "=", "[", "]", "#np.array([7.0, 3.0, 2.0])]", "\n", "\n", "self", ".", "starting_movable_objects", "=", "np", ".", "array", "(", "[", "[", "13.0", ",", "13.0", ",", "2.0", "]", "]", ")", "\n", "# self.starting_movable_objects = np.array([[10.0,3.0,2.0]])", "\n", "self", ".", "movable_objects", "=", "np", ".", "copy", "(", "self", ".", "starting_movable_objects", ")", "\n", "\n", "self", ".", "artist", "=", "DrawArm", "(", "self", ")", "\n", "\n", "# GYM STUFF", "\n", "self", ".", "max_steps", "=", "max_steps", "\n", "# for each joint we can either increase or decrease the angle (by 1 degree)", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "2", "*", "self", ".", "num_joints", ")", "\n", "# our observation is the angle for each joint ", "\n", "# (TODO: along with the position of all movable objects)", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "-", "90", ",", "\n", "high", "=", "90", ",", "\n", "shape", "=", "(", "self", ".", "num_joints", ",", ")", ",", "\n", "dtype", "=", "'int8'", "\n", ")", "\n", "\n", "# Initialize the state", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.to_list": [[56, 58], ["list"], "methods", ["None"], ["", "def", "to_list", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "self", ".", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reset": [[59, 65], ["numpy.zeros", "manipulator2d.Manipulator2DEnv.update_workspace_config", "numpy.copy", "manipulator2d.Manipulator2DEnv.to_list"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.update_workspace_config", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.to_list"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "config", "=", "np", ".", "zeros", "(", "self", ".", "num_joints", ")", "\n", "self", ".", "update_workspace_config", "(", ")", "# do we care about workspace config?", "\n", "self", ".", "step_count", "=", "0", "\n", "self", ".", "movable_objects", "=", "np", ".", "copy", "(", "self", ".", "starting_movable_objects", ")", "# should make this general", "\n", "return", "self", ".", "to_list", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.step": [[66, 111], ["numpy.copy", "numpy.copy", "max", "min", "manipulator2d.Manipulator2DEnv.update_workspace_config", "manipulator2d.Manipulator2DEnv.detect_collision", "manipulator2d.Manipulator2DEnv.detect_collision", "manipulator2d.Manipulator2DEnv.to_list", "manipulator2d.Manipulator2DEnv.detect_collision"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.update_workspace_config", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.detect_collision", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.detect_collision", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.to_list", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.detect_collision"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "self", ".", "step_count", "+=", "1", "\n", "old_config", "=", "np", ".", "copy", "(", "self", ".", "config", ")", "\n", "old_w_config", "=", "np", ".", "copy", "(", "self", ".", "workspace_config", ")", "\n", "\n", "# if action is even we subtract from angle, if odd we add", "\n", "sign", "=", "2", "*", "(", "action", "%", "2", ")", "-", "1", "\n", "# if self.config[action // 2] >= 90 or self.config[action // 2] <= -90:", "\n", "#     sign *= -1", "\n", "self", ".", "config", "[", "action", "//", "2", "]", "+=", "sign", "\n", "\n", "# angle needs to be between -90 and 90", "\n", "self", ".", "config", "[", "action", "//", "2", "]", "=", "max", "(", "-", "90", ",", "self", ".", "config", "[", "action", "//", "2", "]", ")", "\n", "self", ".", "config", "[", "action", "//", "2", "]", "=", "min", "(", "90", ",", "self", ".", "config", "[", "action", "//", "2", "]", ")", "\n", "\n", "# update workspace config", "\n", "self", ".", "update_workspace_config", "(", ")", "\n", "\n", "# check obstacle collision, go back to previous location if collision", "\n", "col", ",", "_", ",", "_", "=", "self", ".", "detect_collision", "(", "self", ".", "obstacles", ")", "\n", "reward", "=", "0", "\n", "old_movable_object_height", "=", "self", ".", "movable_objects", "[", "0", "]", "[", "1", "]", "\n", "if", "col", ":", "\n", "            ", "self", ".", "config", "=", "old_config", "\n", "self", ".", "workspace_config", "=", "old_w_config", "\n", "", "else", ":", "\n", "# update location of movable objects", "\n", "            ", "col", ",", "vec", ",", "ob_idx", "=", "self", ".", "detect_collision", "(", "self", ".", "movable_objects", ")", "\n", "# print(\"HELLO\")", "\n", "while", "col", ":", "# ideally this should be one and done, no loop", "\n", "# print(\"Hello again\")", "\n", "# exit()", "\n", "                ", "self", ".", "movable_objects", "[", "ob_idx", "]", "[", ":", "2", "]", "+=", "vec", "*", "0.1", "# should remove this magic constant but ok", "\n", "reward", "-=", "vec", "[", "1", "]", "\n", "col", ",", "vec", ",", "ob_idx", "=", "self", ".", "detect_collision", "(", "self", ".", "movable_objects", ")", "\n", "\n", "", "", "done", "=", "False", "\n", "if", "self", ".", "step_count", ">=", "self", ".", "max_steps", ":", "\n", "            ", "done", "=", "True", "\n", "\n", "", "reward", "=", "self", ".", "movable_objects", "[", "0", "]", "[", "1", "]", "-", "self", ".", "starting_movable_objects", "[", "0", "]", "[", "1", "]", "# move the object up from start", "\n", "# reward = (self.movable_objects[0][1] - old_movable_object_height)", "\n", "\n", "# self.artist.update_draw()", "\n", "return", "self", ".", "to_list", "(", ")", ",", "reward", ",", "done", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.update_config": [[112, 115], ["manipulator2d.Manipulator2DEnv.update_workspace_config"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.update_workspace_config"], ["", "def", "update_config", "(", "self", ",", "new_config", ")", ":", "\n", "        ", "self", ".", "config", "=", "new_config", "\n", "self", ".", "update_workspace_config", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.update_workspace_config": [[118, 130], ["enumerate", "zip", "math.cos", "math.sin", "math.radians", "math.radians"], "methods", ["None"], ["", "def", "update_workspace_config", "(", "self", ")", ":", "\n", "        ", "x", "=", "self", ".", "start_point", "[", "0", "]", "\n", "y", "=", "self", ".", "start_point", "[", "1", "]", "\n", "self", ".", "workspace_config", "[", "0", "]", "=", "[", "x", ",", "y", "]", "\n", "rel_angle", "=", "0", "\n", "for", "index", ",", "(", "angle", ",", "length", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "config", ",", "self", ".", "arm_lengths", ")", ")", ":", "\n", "            ", "rel_angle", "+=", "angle", "\n", "x", "+=", "length", "*", "math", ".", "cos", "(", "math", ".", "radians", "(", "rel_angle", ")", ")", "\n", "y", "-=", "length", "*", "math", ".", "sin", "(", "math", ".", "radians", "(", "rel_angle", ")", ")", "\n", "self", ".", "workspace_config", "[", "index", "+", "1", "]", "=", "[", "x", ",", "y", "]", "\n", "", "self", ".", "tip_position", "=", "self", ".", "workspace_config", "[", "-", "1", "]", "\n", "return", "self", ".", "workspace_config", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.in_boundary": [[132, 138], ["zip"], "methods", ["None"], ["", "def", "in_boundary", "(", "self", ")", ":", "\n", "        ", "for", "angle", "in", "self", ".", "config", ":", "\n", "            ", "for", "min_a", ",", "max_a", "in", "zip", "(", "self", ".", "min_angle", ",", "self", ".", "max_angle", ")", ":", "\n", "                ", "if", "angle", "<", "min_a", "or", "angle", ">", "max_a", ":", "\n", "                    ", "return", "False", "\n", "", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.detect_collision": [[141, 179], ["enumerate", "numpy.zeros", "range", "numpy.dot", "numpy.linalg.norm", "numpy.array", "numpy.dot", "numpy.linalg.norm", "numpy.linalg.norm", "numpy.linalg.norm", "numpy.dot", "numpy.linalg.norm", "numpy.linalg.norm", "numpy.cross"], "methods", ["None"], ["", "def", "detect_collision", "(", "self", ",", "obstacles", ")", ":", "\n", "        ", "for", "obidx", ",", "ob", "in", "enumerate", "(", "obstacles", ")", ":", "\n", "            ", "center", "=", "ob", "[", ":", "2", "]", "\n", "radius", "=", "ob", "[", "2", "]", "\n", "total_vec", "=", "np", ".", "zeros", "(", "2", ")", "\n", "collision", "=", "False", "\n", "for", "i", "in", "range", "(", "self", ".", "num_joints", ")", ":", "\n", "                ", "start", "=", "self", ".", "workspace_config", "[", "i", "]", "\n", "end", "=", "self", ".", "workspace_config", "[", "i", "+", "1", "]", "\n", "\n", "# check distance between a line segment (the arm link) and a point (center)", "\n", "s_e", "=", "end", "-", "start", "\n", "e_c", "=", "center", "-", "end", "\n", "s_c", "=", "center", "-", "start", "\n", "\n", "# if the center is below the line", "\n", "if", "np", ".", "dot", "(", "s_c", ",", "s_e", ")", "<", "0", ":", "\n", "                    ", "distance", "=", "np", ".", "linalg", ".", "norm", "(", "s_c", ")", "\n", "", "elif", "np", ".", "dot", "(", "e_c", ",", "s_e", ")", ">", "0", ":", "\n", "                    ", "distance", "=", "np", ".", "linalg", ".", "norm", "(", "e_c", ")", "\n", "", "else", ":", "\n", "                    ", "cross", "=", "np", ".", "linalg", ".", "norm", "(", "np", ".", "cross", "(", "s_e", ",", "s_c", ")", ")", "\n", "# this should be the same as self.arm_lengths[i]", "\n", "joint_length", "=", "np", ".", "linalg", ".", "norm", "(", "s_e", ")", "\n", "distance", "=", "cross", "/", "joint_length", "\n", "# the distance between the center and the link should be greater than radius", "\n", "", "if", "distance", "<=", "radius", ":", "\n", "                    ", "vec", "=", "e_c", "+", "s_c", "\n", "normal_vec", "=", "np", ".", "array", "(", "[", "start", "[", "1", "]", "-", "end", "[", "1", "]", ",", "end", "[", "0", "]", "-", "start", "[", "0", "]", "]", ")", "\n", "if", "np", ".", "dot", "(", "normal_vec", ",", "vec", ")", "<", "0", ":", "\n", "                        ", "normal_vec", "*=", "-", "1", "\n", "\n", "# total_vec += vec / np.linalg.norm(vec)", "\n", "", "total_vec", "+=", "normal_vec", "/", "np", ".", "linalg", ".", "norm", "(", "normal_vec", ")", "\n", "collision", "=", "True", "\n", "", "", "if", "collision", ":", "\n", "                ", "return", "True", ",", "total_vec", "/", "np", ".", "linalg", ".", "norm", "(", "total_vec", ")", ",", "0", "# -1#obidx", "\n", "", "", "return", "False", ",", "-", "1", ",", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.reached_goal": [[182, 184], ["numpy.linalg.norm", "numpy.array"], "methods", ["None"], ["", "def", "reached_goal", "(", "self", ",", "goal", ")", ":", "\n", "        ", "return", "np", ".", "linalg", ".", "norm", "(", "self", ".", "tip_position", "-", "np", ".", "array", "(", "goal", "[", ":", "2", "]", ")", ")", "<=", "goal", "[", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.DrawArm.__init__": [[186, 220], ["matplotlib.figure", "matplotlib.axis", "arm.update_workspace_config", "matplotlib.plot", "matplotlib.axhline", "matplotlib.legend", "matplotlib.axis", "matplotlib.xlim", "matplotlib.Circle", "matplotlib.gcf().gca().add_artist", "matplotlib.Circle", "matplotlib.gcf().gca().add_artist", "manipulator2d.DrawArm.mov_ob.append", "matplotlib.gcf().gca", "matplotlib.gcf().gca", "matplotlib.gcf", "matplotlib.gcf"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.update_workspace_config"], ["    ", "def", "__init__", "(", "self", ",", "arm", ")", ":", "\n", "        ", "self", ".", "arm", "=", "arm", "\n", "\n", "self", ".", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "4", ",", "4", ")", ")", "\n", "self", ".", "axis", "=", "plt", ".", "axis", "(", "(", "-", "35", ",", "35", ",", "-", "35", ",", "35", ")", ")", "\n", "\n", "# draw arm", "\n", "arm", ".", "config", "-=", "15", "\n", "arm", ".", "config", "[", "0", "]", "+=", "30", "\n", "arm", ".", "update_workspace_config", "(", ")", "\n", "self", ".", "line", ",", "=", "plt", ".", "plot", "(", "arm", ".", "workspace_config", "[", ":", ",", "0", "]", ",", "arm", ".", "workspace_config", "[", ":", ",", "1", "]", ")", "\n", "# self.line.set_animated(True)", "\n", "\n", "# add obstacles", "\n", "for", "ob", "in", "self", ".", "arm", ".", "obstacles", ":", "\n", "            ", "circle", "=", "plt", ".", "Circle", "(", "ob", "[", ":", "2", "]", ",", "ob", "[", "2", "]", ",", "color", "=", "\"r\"", ")", "\n", "plt", ".", "gcf", "(", ")", ".", "gca", "(", ")", ".", "add_artist", "(", "circle", ")", "\n", "", "self", ".", "mov_ob", "=", "[", "]", "\n", "for", "mov_ob", "in", "self", ".", "arm", ".", "movable_objects", ":", "\n", "            ", "circle", "=", "plt", ".", "Circle", "(", "mov_ob", "[", ":", "2", "]", ",", "mov_ob", "[", "2", "]", ",", "color", "=", "\"g\"", ")", "\n", "\n", "plt", ".", "gcf", "(", ")", ".", "gca", "(", ")", ".", "add_artist", "(", "circle", ")", "\n", "self", ".", "mov_ob", ".", "append", "(", "circle", ")", "\n", "\n", "# TEMP -----------", "\n", "# plt.plot(arm.workspace_config[0,0],arm.workspace_config[0,1],c='black',marker=\"+\",markersize=10)", "\n", "# plt.plot(arm.workspace_config[1,0],arm.workspace_config[1,1],c='black',marker=\"+\",markersize=10)", "\n", "# plt.plot(arm.workspace_config[2,0],arm.workspace_config[2,1],c='black',marker=\"+\",markersize=10) ", "\n", "", "plt", ".", "axhline", "(", "y", "=", "9", ",", "color", "=", "'r'", ",", "linestyle", "=", "'--'", ",", "label", "=", "\"Hard\"", ")", "\n", "# plt.axhline(y=11, color='g', linestyle='--', label=\"Easy\")", "\n", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "axis", "(", "\"equal\"", ")", "\n", "# plt.ylim(-5,20)", "\n", "plt", ".", "xlim", "(", "-", "20", ",", "35", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.DrawArm.update_draw": [[227, 268], ["manipulator2d.DrawArm.line.set_xdata", "manipulator2d.DrawArm.line.set_ydata", "enumerate", "matplotlib.title", "manipulator2d.DrawArm.arm.detect_collision", "m.set", "numpy.array", "manipulator2d.DrawArm.goal_circle.set", "manipulator2d.DrawArm.arm.detect_collision", "numpy.round"], "methods", ["home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.detect_collision", "home.repos.pwc.inspect_result.amnonattali_dsaa.envs.manipulator2d.Manipulator2DEnv.detect_collision"], ["", "def", "update_draw", "(", "self", ")", ":", "\n", "# if joint is not None:", "\n", "#     self.arm.config[joint] = angle", "\n", "#     self.arm.update_workspace_config()", "\n", "\n", "        ", "if", "False", ":", "# this is now done in step...", "\n", "            ", "col", ",", "vec", ",", "ob_idx", "=", "self", ".", "arm", ".", "detect_collision", "(", "[", "self", ".", "goal", "]", ")", "\n", "while", "col", ":", "\n", "# self.goal_circle.set_radius(1)", "\n", "                ", "np_goal", "=", "np", ".", "array", "(", "self", ".", "goal", "[", ":", "2", "]", ")", "\n", "#vec = np_goal - self.arm.tip_position", "\n", "#vec = vec / np.linalg.norm(vec)", "\n", "\n", "new_center", "=", "np_goal", "+", "vec", "*", "0.1", "\n", "self", ".", "goal_circle", ".", "set", "(", "center", "=", "new_center", ")", "\n", "self", ".", "goal", "=", "(", "new_center", "[", "0", "]", ",", "new_center", "[", "1", "]", ",", "self", ".", "goal", "[", "2", "]", ")", "\n", "\n", "col", ",", "vec", ",", "ob_idx", "=", "self", ".", "arm", ".", "detect_collision", "(", "[", "self", ".", "goal", "]", ")", "\n", "\n", "", "", "self", ".", "line", ".", "set_xdata", "(", "self", ".", "arm", ".", "workspace_config", "[", ":", ",", "0", "]", ")", "\n", "self", ".", "line", ".", "set_ydata", "(", "self", ".", "arm", ".", "workspace_config", "[", ":", ",", "1", "]", ")", "\n", "for", "i", ",", "m", "in", "enumerate", "(", "self", ".", "mov_ob", ")", ":", "\n", "            ", "m", ".", "set", "(", "center", "=", "self", ".", "arm", ".", "movable_objects", "[", "i", "]", "[", ":", "2", "]", ")", "\n", "# print(self.arm.movable_objects[i][:2])", "\n", "# self.text_goal.set_text(\"Reached Goal: {}\".format(self.arm.reached_goal(self.goal)))", "\n", "# self.text_collide.set_text(\"Collision: {}\".format(self.arm.detect_collision(self.arm.obstacles)[0]))", "\n", "", "plt", ".", "title", "(", "\"Current arm angles: {}\\nTip position: {}\"", ".", "format", "(", "self", ".", "arm", ".", "config", ",", "np", ".", "round", "(", "self", ".", "arm", ".", "tip_position", ")", ")", ")", "\n", "\n", "# plt.savefig(f\"tmp_{idx}.png\")", "\n", "# plt.draw()", "\n", "# print(plt.gcf())", "\n", "\n", "# io_buf = io.BytesIO()", "\n", "# self.fig.savefig(io_buf, format='raw', dpi=400)", "\n", "# io_buf.seek(0)", "\n", "# img_arr = np.reshape(np.frombuffer(io_buf.getvalue(), dtype=np.uint8),", "\n", "#                     newshape=(int(self.fig.bbox.bounds[3]), int(self.fig.bbox.bounds[2]), -1))", "\n", "# io_buf.close()", "\n", "# return img_arr", "\n", "# return plt.plot(self.arm.workspace_config[:,0], self.arm.workspace_config[:,1],c=\"black\")[0]", "\n", "return", "self", ".", "line", "\n", "\n"]]}