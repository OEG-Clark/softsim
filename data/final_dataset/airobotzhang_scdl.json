{"home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.set_seed": [[55, 61], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["def", "set_seed", "(", "args", ")", ":", "\n", "    ", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.initialize": [[62, 189], ["config_class.from_pretrained", "model_class.from_pretrained", "torch.nn.parallel.DistributedDataParallel.to", "config_class.from_pretrained", "model_class.from_pretrained", "torch.nn.parallel.DistributedDataParallel.to", "config_class.from_pretrained", "model_class.from_pretrained", "torch.nn.parallel.DistributedDataParallel.to", "config_class.from_pretrained", "model_class.from_pretrained", "torch.nn.parallel.DistributedDataParallel.to", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "torch.nn.parallel.DistributedDataParallel.zero_grad", "torch.nn.parallel.DistributedDataParallel.zero_grad", "torch.nn.parallel.DistributedDataParallel.zero_grad", "torch.nn.parallel.DistributedDataParallel.zero_grad", "torch.nn.parallel.DistributedDataParallel.parameters", "torch.nn.parallel.DistributedDataParallel.parameters", "amp.initialize", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "param.detach_", "param.detach_", "bool", "bool", "bool", "bool", "ImportError", "torch.nn.parallel.DistributedDataParallel.named_parameters", "torch.nn.parallel.DistributedDataParallel.named_parameters", "any", "torch.nn.parallel.DistributedDataParallel.named_parameters", "torch.nn.parallel.DistributedDataParallel.named_parameters", "any", "any", "any"], "function", ["home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.initialize"], ["", "", "def", "initialize", "(", "args", ",", "t_total", ",", "num_labels", ",", "epoch", ")", ":", "\n", "    ", "config_class", ",", "model_class", ",", "_", "=", "MODEL_CLASSES", "[", "\"student1\"", "]", "\n", "config_s1", "=", "config_class", ".", "from_pretrained", "(", "\n", "args", ".", "student1_config_name", "if", "args", ".", "student1_config_name", "else", "args", ".", "student1_model_name_or_path", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "None", ",", "\n", ")", "\n", "model_s1", "=", "model_class", ".", "from_pretrained", "(", "\n", "args", ".", "student1_model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "args", ".", "student1_model_name_or_path", ")", ",", "\n", "config", "=", "config_s1", ",", "\n", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "None", ",", "\n", ")", "\n", "model_s1", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "config_class", ",", "model_class", ",", "_", "=", "MODEL_CLASSES", "[", "\"student2\"", "]", "\n", "config_s2", "=", "config_class", ".", "from_pretrained", "(", "\n", "args", ".", "student2_config_name", "if", "args", ".", "student2_config_name", "else", "args", ".", "student2_model_name_or_path", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "None", ",", "\n", ")", "\n", "model_s2", "=", "model_class", ".", "from_pretrained", "(", "\n", "args", ".", "student2_model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "args", ".", "student2_model_name_or_path", ")", ",", "\n", "config", "=", "config_s2", ",", "\n", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "None", ",", "\n", ")", "\n", "model_s2", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "config_class", ",", "model_class", ",", "_", "=", "MODEL_CLASSES", "[", "\"student1\"", "]", "\n", "config_t1", "=", "config_class", ".", "from_pretrained", "(", "\n", "args", ".", "student1_config_name", "if", "args", ".", "student1_config_name", "else", "args", ".", "student1_model_name_or_path", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "None", ",", "\n", ")", "\n", "model_t1", "=", "model_class", ".", "from_pretrained", "(", "\n", "args", ".", "student1_model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "args", ".", "student1_model_name_or_path", ")", ",", "\n", "config", "=", "config_t1", ",", "\n", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "None", ",", "\n", ")", "\n", "model_t1", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "config_class", ",", "model_class", ",", "_", "=", "MODEL_CLASSES", "[", "\"student2\"", "]", "\n", "config_t2", "=", "config_class", ".", "from_pretrained", "(", "\n", "args", ".", "student2_config_name", "if", "args", ".", "student2_config_name", "else", "args", ".", "student2_model_name_or_path", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "None", ",", "\n", ")", "\n", "model_t2", "=", "model_class", ".", "from_pretrained", "(", "\n", "args", ".", "student2_model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "args", ".", "student2_model_name_or_path", ")", ",", "\n", "config", "=", "config_t2", ",", "\n", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "None", ",", "\n", ")", "\n", "model_t2", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "\n", "optimizer_grouped_parameters_1", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model_s1", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "args", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model_s1", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\"weight_decay\"", ":", "0.0", "}", ",", "\n", "]", "\n", "optimizer_s1", "=", "AdamW", "(", "optimizer_grouped_parameters_1", ",", "lr", "=", "args", ".", "learning_rate", ",", "eps", "=", "args", ".", "adam_epsilon", ",", "betas", "=", "(", "args", ".", "adam_beta1", ",", "args", ".", "adam_beta2", ")", ")", "\n", "scheduler_s1", "=", "get_linear_schedule_with_warmup", "(", "\n", "optimizer_s1", ",", "num_warmup_steps", "=", "args", ".", "warmup_steps", ",", "num_training_steps", "=", "t_total", "\n", ")", "\n", "\n", "optimizer_grouped_parameters_2", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model_s2", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "args", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model_s2", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\"weight_decay\"", ":", "0.0", "}", ",", "\n", "]", "\n", "optimizer_s2", "=", "AdamW", "(", "optimizer_grouped_parameters_2", ",", "lr", "=", "args", ".", "learning_rate", ",", "eps", "=", "args", ".", "adam_epsilon", ",", "betas", "=", "(", "args", ".", "adam_beta1", ",", "args", ".", "adam_beta2", ")", ")", "\n", "scheduler_s2", "=", "get_linear_schedule_with_warmup", "(", "\n", "optimizer_s2", ",", "num_warmup_steps", "=", "args", ".", "warmup_steps", ",", "num_training_steps", "=", "t_total", "\n", ")", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", "import", "amp", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"", ")", "\n", "", "[", "model_s1", ",", "model_s2", ",", "model_t1", ",", "model_t2", "]", ",", "[", "optimizer_s1", ",", "optimizer_s2", "]", "=", "amp", ".", "initialize", "(", "\n", "[", "model_s1", ",", "model_s2", ",", "model_t1", ",", "model_t2", "]", ",", "[", "optimizer_s1", ",", "optimizer_s2", "]", ",", "opt_level", "=", "args", ".", "fp16_opt_level", ")", "\n", "\n", "# Multi-gpu training (should be after apex fp16 initialization)", "\n", "", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "# model_t = torch.nn.DataParallel(model_t)", "\n", "        ", "model_s1", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model_s1", ")", "\n", "model_s2", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model_s2", ")", "\n", "model_t1", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model_t1", ")", "\n", "model_t2", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model_t2", ")", "\n", "\n", "# Distributed training (should be after apex fp16 initialization)", "\n", "", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "model_s1", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "\n", "model_s1", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "output_device", "=", "args", ".", "local_rank", ",", "find_unused_parameters", "=", "True", "\n", ")", "\n", "model_s2", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "\n", "model_s2", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "output_device", "=", "args", ".", "local_rank", ",", "find_unused_parameters", "=", "True", "\n", ")", "\n", "model_t1", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "\n", "model_t1", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "output_device", "=", "args", ".", "local_rank", ",", "find_unused_parameters", "=", "True", "\n", ")", "\n", "model_t2", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "\n", "model_t2", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "output_device", "=", "args", ".", "local_rank", ",", "find_unused_parameters", "=", "True", "\n", ")", "\n", "\n", "", "model_s1", ".", "zero_grad", "(", ")", "\n", "model_s2", ".", "zero_grad", "(", ")", "\n", "model_t1", ".", "zero_grad", "(", ")", "\n", "model_t2", ".", "zero_grad", "(", ")", "\n", "\n", "for", "param", "in", "model_t1", ".", "parameters", "(", ")", ":", "\n", "        ", "param", ".", "detach_", "(", ")", "\n", "", "for", "param", "in", "model_t2", ".", "parameters", "(", ")", ":", "\n", "        ", "param", ".", "detach_", "(", ")", "\n", "\n", "", "return", "model_s1", ",", "model_s2", ",", "model_t1", ",", "model_t2", ",", "optimizer_s1", ",", "scheduler_s1", ",", "optimizer_s2", ",", "scheduler_s2", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.validation": [[190, 227], ["MODEL_NAMES[].lower", "utils.eval.evaluate", "utils.eval.evaluate", "os.path.join", "logger.info", "model_to_save.save_pretrained", "tokenizer.save_pretrained", "os.path.join", "logger.info", "model_to_save.save_pretrained", "tokenizer.save_pretrained", "os.path.exists", "os.makedirs", "hasattr", "os.path.exists", "os.makedirs", "hasattr"], "function", ["home.repos.pwc.inspect_result.airobotzhang_scdl.utils.eval.evaluate", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.eval.evaluate"], ["", "def", "validation", "(", "args", ",", "model", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "best_dev", ",", "best_test", ",", "\n", "global_step", ",", "t_total", ",", "epoch", ",", "tors", ")", ":", "\n", "\n", "    ", "model_type", "=", "MODEL_NAMES", "[", "tors", "]", ".", "lower", "(", ")", "\n", "\n", "results", ",", "_", ",", "best_dev", ",", "is_updated1", "=", "evaluate", "(", "args", ",", "model", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "best_dev", ",", "mode", "=", "\"dev\"", ",", "logger", "=", "logger", ",", "prefix", "=", "'dev [Step {}/{} | Epoch {}/{}]'", ".", "format", "(", "global_step", ",", "t_total", ",", "epoch", ",", "args", ".", "num_train_epochs", ")", ",", "verbose", "=", "False", ")", "\n", "\n", "results", ",", "_", ",", "best_test", ",", "is_updated2", "=", "evaluate", "(", "args", ",", "model", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "best_test", ",", "mode", "=", "\"test\"", ",", "logger", "=", "logger", ",", "prefix", "=", "'test [Step {}/{} | Epoch {}/{}]'", ".", "format", "(", "global_step", ",", "t_total", ",", "epoch", ",", "args", ".", "num_train_epochs", ")", ",", "verbose", "=", "False", ")", "\n", "\n", "# output_dirs = []", "\n", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "and", "is_updated1", ":", "\n", "# updated_self_training_teacher = True", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", "+", "tors", ",", "\"checkpoint-best-1\"", ")", "\n", "logger", ".", "info", "(", "\"Saving model checkpoint to %s\"", ",", "path", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "path", ")", "\n", "", "model_to_save", "=", "(", "\n", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "\n", ")", "# Take care of distributed/parallel training", "\n", "model_to_save", ".", "save_pretrained", "(", "path", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "path", ")", "\n", "# output_dirs = []", "\n", "", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "and", "is_updated2", ":", "\n", "# updated_self_training_teacher = True", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", "+", "tors", ",", "\"checkpoint-best-2\"", ")", "\n", "logger", ".", "info", "(", "\"Saving model checkpoint to %s\"", ",", "path", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "path", ")", "\n", "", "model_to_save", "=", "(", "\n", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "\n", ")", "# Take care of distributed/parallel training", "\n", "model_to_save", ".", "save_pretrained", "(", "path", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "path", ")", "\n", "\n", "", "return", "best_dev", ",", "best_test", ",", "is_updated1", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.random_sampler": [[228, 253], ["copy.deepcopy", "non_entity.size", "int", "torch.multinomial", "copy.deepcopy", "torch.nn.Softmax", "torch.nn.Softmax.", "torch.rand().to", "prob_[].max", "torch.rand"], "function", ["None"], ["", "def", "random_sampler", "(", "args", ",", "label_", ",", "prob", "=", "None", ")", ":", "\n", "# label: batch, seq_len", "\n", "    ", "label", "=", "copy", ".", "deepcopy", "(", "label_", ")", "\n", "mask", "=", "(", "label", "==", "0", ")", "\n", "non_entity", "=", "label", "[", "mask", "]", "\n", "size", "=", "non_entity", ".", "size", "(", "0", ")", "\n", "if", "prob", "is", "not", "None", ":", "\n", "        ", "prob_", "=", "copy", ".", "deepcopy", "(", "prob", ")", "\n", "softmax", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "prob_", "=", "softmax", "(", "prob_", ")", "\n", "prob_", "=", "prob_", "[", "mask", "]", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", "\n", "prob_", "=", "1", "-", "prob_", "\n", "", "else", ":", "\n", "        ", "prob_", "=", "torch", ".", "rand", "(", "size", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "", "num_samples", "=", "int", "(", "0.2", "*", "size", ")", "\n", "if", "num_samples", "<=", "0", ":", "\n", "        ", "return", "label", "!=", "-", "100", "\n", "\n", "# print(prob_)", "\n", "", "select_ids", "=", "torch", ".", "multinomial", "(", "prob_", ",", "num_samples", ")", "\n", "non_entity", "[", "select_ids", "]", "=", "-", "100", "\n", "label", "[", "label", "==", "0", "]", "=", "non_entity", "\n", "label_mask", "=", "(", "label", "!=", "-", "100", ")", "\n", "\n", "return", "label_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.initial_mask": [[254, 261], ["run_script.random_sampler", "run_script.random_sampler"], "function", ["home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.random_sampler", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.random_sampler"], ["", "def", "initial_mask", "(", "args", ",", "batch", ")", ":", "\n", "    ", "if", "args", ".", "dataset", "in", "[", "]", ":", "\n", "        ", "return", "None", ",", "None", "\n", "", "else", ":", "\n", "        ", "label_mask1", "=", "random_sampler", "(", "args", ",", "batch", ",", "prob", "=", "None", ")", "\n", "label_mask2", "=", "random_sampler", "(", "args", ",", "batch", ",", "prob", "=", "None", ")", "\n", "return", "label_mask1", ",", "label_mask2", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.get_teacher": [[262, 273], ["copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy"], "function", ["None"], ["", "", "def", "get_teacher", "(", "args", ",", "model_t1", ",", "model_t2", ",", "t_model1", ",", "t_model2", ",", "dev_is_updated1", ",", "dev_is_updated2", ",", "batch", "=", "True", ")", ":", "\n", "    ", "if", "args", ".", "dataset", "in", "[", "\"conll03\"", ",", "\"wikigold\"", "]", "and", "batch", ":", "\n", "        ", "if", "dev_is_updated1", ":", "\n", "            ", "t_model1", "=", "copy", ".", "deepcopy", "(", "model_t1", ")", "\n", "", "if", "dev_is_updated2", ":", "\n", "            ", "t_model2", "=", "copy", ".", "deepcopy", "(", "model_t2", ")", "\n", "", "", "else", ":", "\n", "        ", "t_model1", "=", "copy", ".", "deepcopy", "(", "model_t1", ")", "\n", "t_model2", "=", "copy", ".", "deepcopy", "(", "model_t2", ")", "\n", "\n", "", "return", "t_model1", ",", "t_model2", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.train": [[274, 520], ["len", "torch.utils.data.DataLoader", "run_script.initialize", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "tqdm.trange", "run_script.set_seed", "torch.nn.Softmax", "copy.deepcopy", "copy.deepcopy", "utils.loss_utils.NegEntropy", "max", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "len", "int", "tqdm.tqdm", "enumerate", "logger.info", "logger.info", "run_script.validation", "logger.info", "run_script.validation", "logger.info", "run_script.validation", "logger.info", "run_script.validation", "run_script.get_teacher", "len", "model_s1.train", "model_s2.train", "model_t1.train", "model_t2.train", "tuple", "model_s1", "model_s2", "loss_dict1.keys", "loss_dict2.keys", "tqdm.trange.close", "len", "torch.distributed.get_world_size", "utils.model_utils.soft_frequency", "utils.model_utils.soft_frequency", "run_script.initial_mask", "loss1.mean.mean", "loss2.mean.mean", "loss1.mean.backward", "loss2.mean.backward", "loss1.mean.item", "loss2.mean.item", "optimizer_s1.step", "scheduler_s1.step", "optimizer_s2.step", "scheduler_s2.step", "model_s1.zero_grad", "model_s2.zero_grad", "utils.model_utils._update_mean_model_variables", "utils.model_utils._update_mean_model_variables", "tqdm.tqdm.close", "len", "t.to", "torch.argmax", "torch.argmax", "torch.no_grad", "copy.deepcopy.", "copy.deepcopy.", "torch.argmax", "torch.argmax", "utils.model_utils.mask_tokens", "utils.model_utils.mask_tokens", "amp.scale_loss", "scaled_loss1.backward", "amp.scale_loss", "scaled_loss2.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "copy.deepcopy", "copy.deepcopy.eval", "copy.deepcopy", "copy.deepcopy.eval", "torch.no_grad", "copy.deepcopy.", "copy.deepcopy.", "utils.model_utils.mask_tokens", "utils.model_utils.mask_tokens", "amp.master_params", "amp.master_params", "model_s1.parameters", "model_s2.parameters", "logger.info", "logger.info", "run_script.validation", "logger.info", "run_script.validation", "logger.info", "logger.info", "run_script.validation", "logger.info", "run_script.validation", "run_script.get_teacher", "loss1.mean.item", "loss2.mean.item"], "function", ["home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.initialize", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.set_seed", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.validation", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.validation", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.validation", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.validation", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.get_teacher", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.train", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.train", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.train", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.train", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils.soft_frequency", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils.soft_frequency", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.initial_mask", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils._update_mean_model_variables", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils._update_mean_model_variables", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils.mask_tokens", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils.mask_tokens", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils.mask_tokens", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils.mask_tokens", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.validation", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.validation", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.validation", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.validation", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.get_teacher"], ["", "def", "train", "(", "args", ",", "train_dataset", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ")", ":", "\n", "    ", "\"\"\" Train the model \"\"\"", "\n", "num_labels", "=", "len", "(", "labels", ")", "\n", "args", ".", "train_batch_size", "=", "args", ".", "per_gpu_train_batch_size", "*", "max", "(", "1", ",", "args", ".", "n_gpu", ")", "\n", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ")", "if", "args", ".", "local_rank", "==", "-", "1", "else", "DistributedSampler", "(", "train_dataset", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "train_dataset", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "# train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size)", "\n", "\n", "if", "args", ".", "max_steps", ">", "0", ":", "\n", "        ", "t_total", "=", "args", ".", "max_steps", "\n", "args", ".", "num_train_epochs", "=", "args", ".", "max_steps", "//", "(", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", ")", "+", "1", "\n", "", "else", ":", "\n", "        ", "t_total", "=", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", "\n", "\n", "", "model_s1", ",", "model_s2", ",", "model_t1", ",", "model_t2", ",", "optimizer_s1", ",", "scheduler_s1", ",", "optimizer_s2", ",", "scheduler_s2", "=", "initialize", "(", "args", ",", "t_total", ",", "num_labels", ",", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_dataset", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num Epochs = %d\"", ",", "args", ".", "num_train_epochs", ")", "\n", "logger", ".", "info", "(", "\"  Instantaneous batch size per GPU = %d\"", ",", "args", ".", "per_gpu_train_batch_size", ")", "\n", "logger", ".", "info", "(", "\n", "\"  Total train batch size (w. parallel, distributed & accumulation) = %d\"", ",", "\n", "args", ".", "train_batch_size", "\n", "*", "args", ".", "gradient_accumulation_steps", "\n", "*", "(", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "if", "args", ".", "local_rank", "!=", "-", "1", "else", "1", ")", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"  Gradient Accumulation steps = %d\"", ",", "args", ".", "gradient_accumulation_steps", ")", "\n", "logger", ".", "info", "(", "\"  Total optimization steps = %d\"", ",", "t_total", ")", "\n", "\n", "global_step", "=", "0", "\n", "epochs_trained", "=", "0", "\n", "\n", "tr_loss", ",", "logging_loss", "=", "0.0", ",", "0.0", "\n", "train_iterator", "=", "trange", "(", "\n", "epochs_trained", ",", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", "\n", ")", "\n", "set_seed", "(", "args", ")", "# Added here for reproductibility", "\n", "s1_best_dev", ",", "s1_best_test", "=", "[", "0", ",", "0", ",", "0", "]", ",", "[", "0", ",", "0", ",", "0", "]", "\n", "s2_best_dev", ",", "s2_best_test", "=", "[", "0", ",", "0", ",", "0", "]", ",", "[", "0", ",", "0", ",", "0", "]", "\n", "t1_best_dev", ",", "t1_best_test", "=", "[", "0", ",", "0", ",", "0", "]", ",", "[", "0", ",", "0", ",", "0", "]", "\n", "t2_best_dev", ",", "t2_best_test", "=", "[", "0", ",", "0", ",", "0", "]", ",", "[", "0", ",", "0", ",", "0", "]", "\n", "\n", "self_learning_teacher_model1", "=", "model_s1", "\n", "self_learning_teacher_model2", "=", "model_s2", "\n", "\n", "softmax", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "t_model1", "=", "copy", ".", "deepcopy", "(", "model_s1", ")", "\n", "t_model2", "=", "copy", ".", "deepcopy", "(", "model_s2", ")", "\n", "\n", "loss_regular", "=", "NegEntropy", "(", ")", "\n", "\n", "begin_global_step", "=", "len", "(", "train_dataloader", ")", "*", "args", ".", "begin_epoch", "//", "args", ".", "gradient_accumulation_steps", "\n", "for", "epoch", "in", "train_iterator", ":", "\n", "        ", "epoch_iterator", "=", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ",", "disable", "=", "args", ".", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", ")", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "epoch_iterator", ")", ":", "\n", "            ", "model_s1", ".", "train", "(", ")", "\n", "model_s2", ".", "train", "(", ")", "\n", "model_t1", ".", "train", "(", ")", "\n", "model_t2", ".", "train", "(", ")", "\n", "\n", "batch", "=", "tuple", "(", "t", ".", "to", "(", "args", ".", "device", ")", "for", "t", "in", "batch", ")", "\n", "if", "epoch", ">=", "args", ".", "begin_epoch", ":", "\n", "                ", "delta", "=", "global_step", "-", "begin_global_step", "\n", "if", "delta", "//", "args", ".", "self_learning_period", ">", "0", ":", "\n", "                    ", "if", "delta", "%", "args", ".", "self_learning_period", "==", "0", ":", "\n", "                        ", "self_learning_teacher_model1", "=", "copy", ".", "deepcopy", "(", "t_model1", ")", "\n", "self_learning_teacher_model1", ".", "eval", "(", ")", "\n", "self_learning_teacher_model2", "=", "copy", ".", "deepcopy", "(", "t_model2", ")", "\n", "self_learning_teacher_model2", ".", "eval", "(", ")", "\n", "", "inputs", "=", "{", "\"input_ids\"", ":", "batch", "[", "0", "]", ",", "\"attention_mask\"", ":", "batch", "[", "1", "]", "}", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                        ", "outputs1", "=", "self_learning_teacher_model1", "(", "**", "inputs", ")", "\n", "outputs2", "=", "self_learning_teacher_model2", "(", "**", "inputs", ")", "\n", "", "pseudo_labels1", "=", "torch", ".", "argmax", "(", "outputs2", "[", "0", "]", ",", "axis", "=", "2", ")", "\n", "pseudo_labels2", "=", "torch", ".", "argmax", "(", "outputs1", "[", "0", "]", ",", "axis", "=", "2", ")", "\n", "", "else", ":", "\n", "                    ", "pseudo_labels1", "=", "batch", "[", "3", "]", "\n", "pseudo_labels2", "=", "batch", "[", "3", "]", "\n", "# model1 = copy.deepcopy(model_s1)", "\n", "# model1.eval()", "\n", "# model2 = copy.deepcopy(model_s2)", "\n", "# model2.eval()", "\n", "# inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}", "\n", "# with torch.no_grad():", "\n", "#     outputs1 = model1(**inputs)", "\n", "#     outputs2 = model2(**inputs)", "\n", "# pseudo_labels1 = torch.argmax(outputs1[0], axis=2)", "\n", "# pseudo_labels2 = torch.argmax(outputs2[0], axis=2)", "\n", "\n", "", "inputs", "=", "{", "\"input_ids\"", ":", "batch", "[", "0", "]", ",", "\"attention_mask\"", ":", "batch", "[", "1", "]", "}", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "outputs1", "=", "t_model1", "(", "**", "inputs", ")", "\n", "outputs2", "=", "t_model2", "(", "**", "inputs", ")", "\n", "logits1", "=", "outputs1", "[", "0", "]", "\n", "logits2", "=", "outputs2", "[", "0", "]", "\n", "pred_labels1", "=", "torch", ".", "argmax", "(", "logits1", ",", "dim", "=", "-", "1", ")", "\n", "pred_labels2", "=", "torch", ".", "argmax", "(", "logits2", ",", "dim", "=", "-", "1", ")", "\n", "label_mask1", "=", "(", "pred_labels1", "==", "pseudo_labels1", ")", "\n", "label_mask2", "=", "(", "pred_labels2", "==", "pseudo_labels2", ")", "\n", "\n", "", "logits1", "=", "soft_frequency", "(", "logits", "=", "logits1", ",", "power", "=", "2", ")", "\n", "logits2", "=", "soft_frequency", "(", "logits", "=", "logits2", ",", "power", "=", "2", ")", "\n", "\n", "if", "args", ".", "self_learning_label_mode", "==", "\"hard\"", ":", "\n", "                    ", "pred_labels1", ",", "label_mask1_", "=", "mask_tokens", "(", "args", ",", "batch", "[", "3", "]", ",", "pred_labels1", ",", "pad_token_label_id", ",", "pred_logits", "=", "logits1", ")", "\n", "pred_labels2", ",", "label_mask2_", "=", "mask_tokens", "(", "args", ",", "batch", "[", "3", "]", ",", "pred_labels2", ",", "pad_token_label_id", ",", "pred_logits", "=", "logits2", ")", "\n", "", "elif", "args", ".", "self_learning_label_mode", "==", "\"soft\"", ":", "\n", "# pred_labels1 = soft_frequency(logits=logits1, power=2)", "\n", "# pred_labels2 = soft_frequency(logits=logits2, power=2)", "\n", "# print(\"pred_labels1\")", "\n", "# print(pred_labels1)", "\n", "\n", "                    ", "pred_labels1", ",", "label_mask1_", "=", "mask_tokens", "(", "args", ",", "batch", "[", "3", "]", ",", "logits1", ",", "pad_token_label_id", ")", "\n", "pred_labels2", ",", "label_mask2_", "=", "mask_tokens", "(", "args", ",", "batch", "[", "3", "]", ",", "logits2", ",", "pad_token_label_id", ")", "\n", "# print(\"label_mask1_\")", "\n", "# print(label_mask1_)", "\n", "\n", "", "if", "label_mask1_", "is", "not", "None", ":", "\n", "                    ", "label_mask1", "=", "label_mask1", "&", "label_mask1_", "\n", "# label_mask1_ = random_sampler(args, pseudo_labels1, prob=logits1)", "\n", "# label_mask1 = label_mask1&label_mask1_ ", "\n", "# print(\"label_mask1\")", "\n", "# print(label_mask1)", "\n", "", "if", "label_mask2_", "is", "not", "None", ":", "\n", "                    ", "label_mask2", "=", "label_mask2", "&", "label_mask2_", "\n", "# label_mask2_ = random_sampler(args, pseudo_labels2, prob=logits2)", "\n", "# label_mask2 = label_mask2&label_mask2_ ", "\n", "", "", "else", ":", "\n", "# label_mask1 = random_sampler(args, batch[3], prob=None)", "\n", "# print(batch[3])", "\n", "# print(\"label_mask1\")", "\n", "# print(label_mask1)", "\n", "# label_mask1 = None", "\n", "                ", "pred_labels1", "=", "batch", "[", "3", "]", "\n", "# label_mask2 = random_sampler(args, batch[3], prob=None)", "\n", "# print(\"label_mask2\")", "\n", "# print(label_mask2)", "\n", "# exit()", "\n", "# label_mask2 = None", "\n", "pred_labels2", "=", "batch", "[", "3", "]", "\n", "pseudo_labels1", "=", "batch", "[", "3", "]", "\n", "pseudo_labels2", "=", "batch", "[", "3", "]", "\n", "label_mask1", ",", "label_mask2", "=", "initial_mask", "(", "args", ",", "batch", "[", "3", "]", ")", "\n", "\n", "", "inputs1", "=", "{", "\"input_ids\"", ":", "batch", "[", "0", "]", ",", "\"attention_mask\"", ":", "batch", "[", "1", "]", ",", "\"labels\"", ":", "{", "\"pseudo\"", ":", "pred_labels1", "}", ",", "\"label_mask\"", ":", "label_mask1", "}", "\n", "outputs1", "=", "model_s1", "(", "**", "inputs1", ")", "\n", "\n", "inputs2", "=", "{", "\"input_ids\"", ":", "batch", "[", "0", "]", ",", "\"attention_mask\"", ":", "batch", "[", "1", "]", ",", "\"labels\"", ":", "{", "\"pseudo\"", ":", "pred_labels2", "}", ",", "\"label_mask\"", ":", "label_mask2", "}", "\n", "outputs2", "=", "model_s2", "(", "**", "inputs2", ")", "\n", "\n", "loss1", "=", "0.0", "\n", "loss_dict1", "=", "outputs1", "[", "0", "]", "\n", "keys", "=", "loss_dict1", ".", "keys", "(", ")", "\n", "for", "key", "in", "keys", ":", "\n", "                ", "loss1", "+=", "LOSS_WEIGHTS", "[", "key", "]", "*", "loss_dict1", "[", "key", "]", "\n", "# if epoch < args.begin_epoch:", "\n", "# loss1 += loss_regular(outputs1[1].view(-1, num_labels))", "\n", "", "loss2", "=", "0.0", "\n", "loss_dict2", "=", "outputs2", "[", "0", "]", "\n", "keys", "=", "loss_dict2", ".", "keys", "(", ")", "\n", "for", "key", "in", "keys", ":", "\n", "                ", "loss2", "+=", "LOSS_WEIGHTS", "[", "key", "]", "*", "loss_dict2", "[", "key", "]", "\n", "# if epoch < args.begin_epoch:", "\n", "# loss2 += loss_regular(outputs2[1].view(-1, num_labels))", "\n", "\n", "", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "loss1", "=", "loss1", ".", "mean", "(", ")", "# mean() to average on multi-gpu parallel training", "\n", "loss2", "=", "loss2", ".", "mean", "(", ")", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss1", "=", "loss1", "/", "args", ".", "gradient_accumulation_steps", "\n", "loss2", "=", "loss2", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                ", "with", "amp", ".", "scale_loss", "(", "loss1", ",", "optimizer_s1", ")", "as", "scaled_loss1", ":", "\n", "                    ", "scaled_loss1", ".", "backward", "(", ")", "\n", "", "with", "amp", ".", "scale_loss", "(", "loss2", ",", "optimizer_s2", ")", "as", "scaled_loss2", ":", "\n", "                    ", "scaled_loss2", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "loss1", ".", "backward", "(", ")", "\n", "loss2", ".", "backward", "(", ")", "\n", "\n", "", "tr_loss", "+=", "loss1", ".", "item", "(", ")", "+", "loss2", ".", "item", "(", ")", "\n", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "if", "args", ".", "fp16", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "amp", ".", "master_params", "(", "optimizer_s1", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "amp", ".", "master_params", "(", "optimizer_s2", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "", "else", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model_s1", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model_s2", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "\n", "", "optimizer_s1", ".", "step", "(", ")", "\n", "scheduler_s1", ".", "step", "(", ")", "# Update learning rate schedule", "\n", "optimizer_s2", ".", "step", "(", ")", "\n", "scheduler_s2", ".", "step", "(", ")", "# Update learning rate schedule", "\n", "model_s1", ".", "zero_grad", "(", ")", "\n", "model_s2", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "_update_mean_model_variables", "(", "model_s1", ",", "model_t1", ",", "args", ".", "mean_alpha", ",", "global_step", ")", "\n", "_update_mean_model_variables", "(", "model_s2", ",", "model_t2", ",", "args", ".", "mean_alpha", ",", "global_step", ")", "\n", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "and", "args", ".", "logging_steps", ">", "0", "and", "global_step", "%", "args", ".", "logging_steps", "==", "0", ":", "\n", "# Log metrics", "\n", "                    ", "if", "args", ".", "evaluate_during_training", ":", "\n", "                        ", "logger", ".", "info", "(", "\"***** Student1 combined Entropy loss : %.4f *****\"", ",", "loss1", ".", "item", "(", ")", ")", "\n", "logger", ".", "info", "(", "\"##### Student1 #####\"", ")", "\n", "s1_best_dev", ",", "s1_best_test", ",", "_", "=", "validation", "(", "args", ",", "model_s1", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "s1_best_dev", ",", "s1_best_test", ",", "global_step", ",", "t_total", ",", "epoch", ",", "\"student1\"", ")", "\n", "logger", ".", "info", "(", "\"##### Teacher1 #####\"", ")", "\n", "t1_best_dev", ",", "t1_best_test", ",", "dev_is_updated1", "=", "validation", "(", "args", ",", "model_t1", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "t1_best_dev", ",", "t1_best_test", ",", "global_step", ",", "t_total", ",", "epoch", ",", "\"teacher1\"", ")", "\n", "logger", ".", "info", "(", "\"***** Student2 combined Entropy loss : %.4f *****\"", ",", "loss2", ".", "item", "(", ")", ")", "\n", "logger", ".", "info", "(", "\"##### Student2 #####\"", ")", "\n", "s2_best_dev", ",", "s2_best_test", ",", "_", "=", "validation", "(", "args", ",", "model_s2", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "s2_best_dev", ",", "s2_best_test", ",", "global_step", ",", "t_total", ",", "epoch", ",", "\"student2\"", ")", "\n", "logger", ".", "info", "(", "\"##### Teacher2 #####\"", ")", "\n", "t2_best_dev", ",", "t2_best_test", ",", "dev_is_updated2", "=", "validation", "(", "args", ",", "model_t2", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "t2_best_dev", ",", "t2_best_test", ",", "global_step", ",", "t_total", ",", "epoch", ",", "\"teacher2\"", ")", "\n", "t_model1", ",", "t_model2", "=", "get_teacher", "(", "args", ",", "model_t1", ",", "model_t2", ",", "t_model1", ",", "t_model2", ",", "dev_is_updated1", ",", "dev_is_updated2", ")", "\n", "\n", "", "", "", "if", "args", ".", "max_steps", ">", "0", "and", "global_step", ">", "args", ".", "max_steps", ":", "\n", "                ", "epoch_iterator", ".", "close", "(", ")", "\n", "break", "\n", "\n", "", "", "logger", ".", "info", "(", "\"***** Epoch : %d *****\"", ",", "epoch", ")", "\n", "logger", ".", "info", "(", "\"##### Student1 #####\"", ")", "\n", "s1_best_dev", ",", "s1_best_test", ",", "_", "=", "validation", "(", "args", ",", "model_s1", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "s1_best_dev", ",", "s1_best_test", ",", "global_step", ",", "t_total", ",", "epoch", ",", "\"student1\"", ")", "\n", "logger", ".", "info", "(", "\"##### Teacher1 #####\"", ")", "\n", "t1_best_dev", ",", "t1_best_test", ",", "dev_is_updated1", "=", "validation", "(", "args", ",", "model_t1", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "t1_best_dev", ",", "t1_best_test", ",", "global_step", ",", "t_total", ",", "epoch", ",", "\"teacher1\"", ")", "\n", "logger", ".", "info", "(", "\"##### Student2 #####\"", ")", "\n", "s2_best_dev", ",", "s2_best_test", ",", "_", "=", "validation", "(", "args", ",", "model_s2", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "s2_best_dev", ",", "s2_best_test", ",", "global_step", ",", "t_total", ",", "epoch", ",", "\"student2\"", ")", "\n", "logger", ".", "info", "(", "\"##### Teacher2 #####\"", ")", "\n", "t2_best_dev", ",", "t2_best_test", ",", "dev_is_updated2", "=", "validation", "(", "args", ",", "model_t2", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "t2_best_dev", ",", "t2_best_test", ",", "global_step", ",", "t_total", ",", "epoch", ",", "\"teacher2\"", ")", "\n", "t_model1", ",", "t_model2", "=", "get_teacher", "(", "args", ",", "model_t1", ",", "model_t2", ",", "t_model1", ",", "t_model2", ",", "dev_is_updated1", ",", "dev_is_updated2", ",", "True", ")", "\n", "\n", "if", "args", ".", "max_steps", ">", "0", "and", "global_step", ">", "args", ".", "max_steps", ":", "\n", "            ", "train_iterator", ".", "close", "(", ")", "\n", "break", "\n", "\n", "", "", "results", "=", "(", "t1_best_dev", ",", "t1_best_test", ",", "t2_best_dev", ",", "t2_best_test", ")", "\n", "\n", "return", "global_step", ",", "tr_loss", "/", "global_step", ",", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.main": [[521, 599], ["utils.config.config", "logging.basicConfig", "logging.Formatter", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.FileHandler.setFormatter", "logger.addHandler", "logger.warning", "run_script.set_seed", "utils.data_utils.get_labels", "len", "transformers.RobertaTokenizer.from_pretrained", "logger.info", "os.path.exists", "os.listdir", "ValueError", "os.makedirs", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "os.path.join", "bool", "torch.nn.CrossEntropyLoss", "torch.distributed.barrier", "torch.distributed.barrier", "utils.data_utils.load_and_cache_examples", "run_script.train", "logger.info", "os.path.exists", "torch.cuda.is_available"], "function", ["home.repos.pwc.inspect_result.airobotzhang_scdl.utils.config.config", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.set_seed", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.get_labels", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.load_and_cache_examples", "home.repos.pwc.inspect_result.airobotzhang_scdl.None.run_script.train"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "config", "(", ")", "\n", "# args.do_train = args.do_train.lower()", "\n", "# args.do_test = args.do_test.lower()", "\n", "\n", "if", "(", "\n", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "\n", "and", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "\n", "and", "args", ".", "do_train", "\n", "and", "not", "args", ".", "overwrite_output_dir", "\n", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"", ".", "format", "(", "\n", "args", ".", "output_dir", "\n", ")", "\n", ")", "\n", "\n", "# Create output directory if needed", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "and", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "args", ".", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "\"nccl\"", ")", "\n", "args", ".", "n_gpu", "=", "1", "\n", "", "args", ".", "device", "=", "device", "\n", "\n", "# Setup logging", "\n", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "level", "=", "logging", ".", "INFO", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "else", "logging", ".", "WARN", ",", "\n", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\"%m/%d/%Y %H:%M:%S\"", ")", "\n", "logging_fh", "=", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'log.txt'", ")", ")", "\n", "logging_fh", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "logging_fh", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "logging_fh", ")", "\n", "logger", ".", "warning", "(", "\n", "\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\"", ",", "\n", "args", ".", "local_rank", ",", "\n", "device", ",", "\n", "args", ".", "n_gpu", ",", "\n", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "\n", "args", ".", "fp16", ",", "\n", ")", "\n", "\n", "# Set seed", "\n", "set_seed", "(", "args", ")", "\n", "labels", "=", "get_labels", "(", "args", ".", "data_dir", ",", "args", ".", "dataset", ")", "\n", "num_labels", "=", "len", "(", "labels", ")", "\n", "# Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later", "\n", "pad_token_label_id", "=", "CrossEntropyLoss", "(", ")", ".", "ignore_index", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "if", "args", ".", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "torch", ".", "distributed", ".", "barrier", "(", ")", "# Make sure only the first process in distributed training will download model & vocab", "\n", "\n", "", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "tokenizer_name", ",", "\n", "do_lower_case", "=", "args", ".", "do_lower_case", ",", "\n", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "None", ",", "\n", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "torch", ".", "distributed", ".", "barrier", "(", ")", "# Make sure only the first process in distributed training will download model & vocab", "\n", "\n", "", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "args", ")", "\n", "\n", "# Training", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "train_dataset", "=", "load_and_cache_examples", "(", "args", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "mode", "=", "\"train\"", ")", "\n", "global_step", ",", "tr_loss", ",", "best_results", "=", "train", "(", "args", ",", "train_dataset", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ")", "\n", "logger", ".", "info", "(", "\" global_step = %s, average loss = %s\"", ",", "global_step", ",", "tr_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.config.config": [[4, 153], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "config", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the training files for the CoNLL-2003 NER task.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dataset\"", ",", "\n", "default", "=", "\"conll03\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The dataset name.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--student1_model_name_or_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Path to pre-trained model or shortcut name\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--student2_model_name_or_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Path to pre-trained model or shortcut name\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ",", "\n", ")", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--student1_config_name\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "help", "=", "\"Pretrained config name or path if not the same as model_name\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--student2_config_name\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "help", "=", "\"Pretrained config name or path if not the same as model_name\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--tokenizer_name\"", ",", "\n", "default", "=", "\"bert-base-uncased\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Pretrained tokenizer name or path if not the same as model_name\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--model_type\"", ",", "\n", "default", "=", "\"bert\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Pretrained tokenizer name or path if not the same as model_name\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--cache_dir\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Where do you want to store the pre-trained models downloaded from s3\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after tokenization. Sequences longer \"", "\n", "\"than this will be truncated, sequences shorter will be padded.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Whether to run training.\"", ")", "\n", "# # parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")", "\n", "# parser.add_argument(\"--do_test\", default=\"false\", type=str, help=\"Whether to run predictions on the test set.\")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--evaluate_during_training\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Whether to run evaluation during training at each logging step.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--do_lower_case\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Set this flag if you are using an uncased model.\"", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--per_gpu_train_batch_size\"", ",", "default", "=", "8", ",", "type", "=", "int", ",", "help", "=", "\"Batch size per GPU/CPU for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--per_gpu_eval_batch_size\"", ",", "default", "=", "8", ",", "type", "=", "int", ",", "help", "=", "\"Batch size per GPU/CPU for evaluation.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--gradient_accumulation_steps\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "help", "=", "\"Weight decay if we apply some.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_epsilon\"", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "help", "=", "\"Epsilon for Adam optimizer.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_beta1\"", ",", "default", "=", "0.9", ",", "type", "=", "float", ",", "help", "=", "\"BETA1 for Adam optimizer.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_beta2\"", ",", "default", "=", "0.999", ",", "type", "=", "float", ",", "help", "=", "\"BETA2 for Adam optimizer.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_grad_norm\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ",", "help", "=", "\"Max gradient norm.\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--num_train_epochs\"", ",", "default", "=", "3.0", ",", "type", "=", "float", ",", "help", "=", "\"Total number of training epochs to perform.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--max_steps\"", ",", "\n", "default", "=", "-", "1", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"If > 0: set total number of training steps to perform. Override num_train_epochs.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "help", "=", "\"Linear warmup over warmup_steps.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--logging_steps\"", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "\"Log every X updates steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_steps\"", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "\"Save checkpoint every X updates steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--eval_all_checkpoints\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Avoid using CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--overwrite_output_dir\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Overwrite the content of the output directory\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--overwrite_cache\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Overwrite the cached training and evaluation sets\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "42", ",", "help", "=", "\"random seed for initialization\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fp16\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fp16_opt_level\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"O1\"", ",", "\n", "help", "=", "\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"", "\n", "\"See details at https://nvidia.github.io/apex/amp.html\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"For distributed training: local_rank\"", ")", "\n", "parser", ".", "add_argument", "(", "'--begin_epoch'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'the begin step (usually after the first epoch) to start students mutual learning.'", ")", "\n", "parser", ".", "add_argument", "(", "'--self_learning_begin_step'", ",", "type", "=", "int", ",", "default", "=", "900", ",", "help", "=", "'the begin step (usually after the first epoch) to start students mutual learning.'", ")", "\n", "parser", ".", "add_argument", "(", "'--self_learning_label_mode'", ",", "type", "=", "str", ",", "default", "=", "\"hard\"", ",", "help", "=", "'pseudo label type. choices:[hard(default), soft].'", ")", "\n", "parser", ".", "add_argument", "(", "'--self_learning_period'", ",", "type", "=", "int", ",", "default", "=", "878", ",", "help", "=", "'the self-training period.'", ")", "\n", "parser", ".", "add_argument", "(", "'--mean_alpha'", ",", "default", "=", "0.995", ",", "type", "=", "float", ",", "help", "=", "\"moving average parameter of mean student1 and student2 (for the exponential moving average).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--threshold'", ",", "default", "=", "0.9", ",", "type", "=", "float", ",", "help", "=", "\"confidence threshold.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "", ""]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils.soft_frequency": [[11, 33], ["torch.sum", "torch.sum", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax.view", "torch.sum", "torch.sum", "torch.nn.Softmax.", "logits.view"], "function", ["None"], ["def", "soft_frequency", "(", "logits", ",", "power", "=", "2", ",", "probs", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Unsupervised Deep Embedding for Clustering Analysiszaodian\n    https://arxiv.org/abs/1511.06335\n    \"\"\"", "\n", "if", "not", "probs", ":", "\n", "        ", "softmax", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "y", "=", "softmax", "(", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", ")", ".", "view", "(", "logits", ".", "shape", ")", "\n", "", "else", ":", "\n", "        ", "y", "=", "logits", "\n", "", "f", "=", "torch", ".", "sum", "(", "y", ",", "dim", "=", "(", "0", ",", "1", ")", ")", "\n", "t", "=", "y", "**", "power", "/", "f", "\n", "p", "=", "t", "/", "torch", ".", "sum", "(", "t", ",", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "# m = torch.argmax(y, dim=2, keepdim=True)", "\n", "# m = (m==0)", "\n", "# m = m.repeat(1,1,y.size(2))", "\n", "# p = p.masked_fill(mask=m,value=torch.tensor(0))", "\n", "# m = ~m", "\n", "# y = y.masked_fill(mask=m,value=torch.tensor(0))", "\n", "# p = p+y", "\n", "\n", "return", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils.get_hard_label": [[34, 38], ["None"], "function", ["None"], ["", "def", "get_hard_label", "(", "args", ",", "combined_labels", ",", "pred_labels", ",", "pad_token_label_id", ",", "pred_logits", "=", "None", ")", ":", "\n", "    ", "pred_labels", "[", "combined_labels", "==", "pad_token_label_id", "]", "=", "pad_token_label_id", "\n", "\n", "return", "pred_labels", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils.mask_tokens": [[39, 55], ["torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax.view", "torch.nn.Softmax.", "pred_logits.view", "pred_labels.max", "softmax().view.max"], "function", ["None"], ["", "def", "mask_tokens", "(", "args", ",", "combined_labels", ",", "pred_labels", ",", "pad_token_label_id", ",", "pred_logits", "=", "None", ")", ":", "\n", "\n", "    ", "if", "args", ".", "self_learning_label_mode", "==", "\"hard\"", ":", "\n", "        ", "softmax", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "y", "=", "softmax", "(", "pred_logits", ".", "view", "(", "-", "1", ",", "pred_logits", ".", "shape", "[", "-", "1", "]", ")", ")", ".", "view", "(", "pred_logits", ".", "shape", ")", "\n", "_threshold", "=", "args", ".", "threshold", "\n", "pred_labels", "[", "y", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", ">", "_threshold", "]", "=", "pad_token_label_id", "\n", "# if args.self_training_hp_label < 5:", "\n", "#     pred_labels[combined_labels==pad_token_label_id] = pad_token_label_id", "\n", "# pred_labels[combined_labels==pad_token_label_id] = pad_token_label_id", "\n", "return", "pred_labels", ",", "None", "\n", "\n", "", "elif", "args", ".", "self_learning_label_mode", "==", "\"soft\"", ":", "\n", "        ", "label_mask", "=", "(", "pred_labels", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", ">", "args", ".", "threshold", ")", "\n", "\n", "return", "pred_labels", ",", "label_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils.opt_grad": [[56, 61], ["hasattr", "torch.autograd.grad", "torch.autograd.grad"], "function", ["None"], ["", "", "def", "opt_grad", "(", "loss", ",", "in_var", ",", "optimizer", ")", ":", "\n", "\n", "    ", "if", "hasattr", "(", "optimizer", ",", "'scalar'", ")", ":", "\n", "        ", "loss", "=", "loss", "*", "optimizer", ".", "scaler", ".", "loss_scale", "\n", "", "return", "torch", ".", "autograd", ".", "grad", "(", "loss", ",", "in_var", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils._update_mean_model_variables": [[62, 66], ["min", "zip", "m_model.parameters", "model.parameters", "m_param.data.mul_().add_", "m_param.data.mul_"], "function", ["None"], ["", "def", "_update_mean_model_variables", "(", "model", ",", "m_model", ",", "alpha", ",", "global_step", ")", ":", "\n", "    ", "alpha", "=", "min", "(", "1", "-", "1", "/", "(", "global_step", "+", "1", ")", ",", "alpha", ")", "\n", "for", "m_param", ",", "param", "in", "zip", "(", "m_model", ".", "parameters", "(", ")", ",", "model", ".", "parameters", "(", ")", ")", ":", "\n", "        ", "m_param", ".", "data", ".", "mul_", "(", "alpha", ")", ".", "add_", "(", "1", "-", "alpha", ",", "param", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.model_utils._update_mean_prediction_variables": [[67, 71], ["min", "m_prediction.data.mul_().add_", "m_prediction.data.mul_"], "function", ["None"], ["", "", "def", "_update_mean_prediction_variables", "(", "prediction", ",", "m_prediction", ",", "alpha", ",", "global_step", ")", ":", "\n", "    ", "alpha", "=", "min", "(", "1", "-", "1", "/", "(", "global_step", "+", "1", ")", ",", "alpha", ")", "\n", "# for m_param, param in zip(m_model.parameters(), model.parameters()):", "\n", "m_prediction", ".", "data", ".", "mul_", "(", "alpha", ")", ".", "add_", "(", "1", "-", "alpha", ",", "prediction", ".", "data", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.InputExample.__init__": [[14, 27], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "words", ",", "labels", ",", "hp_labels", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            words: list. The words of the sequence.\n            labels: (Optional) list. The labels for each word of the sequence. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "words", "=", "words", "\n", "self", ".", "labels", "=", "labels", "\n", "self", ".", "hp_labels", "=", "hp_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.InputFeatures.__init__": [[31, 39], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", ",", "full_label_ids", ",", "hp_label_ids", ")", ":", "\n", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_ids", "=", "label_ids", "\n", "self", ".", "full_label_ids", "=", "full_label_ids", "\n", "self", ".", "hp_label_ids", "=", "hp_label_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.read_examples_from_file": [[40, 62], ["os.path.join", "open", "json.load", "examples.append", "data_utils.InputExample", "len"], "function", ["None"], ["", "", "def", "read_examples_from_file", "(", "args", ",", "data_dir", ",", "mode", ")", ":", "\n", "\n", "# if mode == \"train\":", "\n", "#     mode = str(args.noise_ratio)+\"-\"+mode", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"{}_{}.json\"", ".", "format", "(", "args", ".", "dataset", ",", "mode", ")", ")", "\n", "guid_index", "=", "1", "\n", "examples", "=", "[", "]", "\n", "\n", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "for", "item", "in", "data", ":", "\n", "            ", "words", "=", "item", "[", "\"str_words\"", "]", "\n", "labels", "=", "item", "[", "\"tags\"", "]", "\n", "if", "\"tags_hp\"", "in", "labels", ":", "\n", "                ", "hp_labels", "=", "item", "[", "\"tags_hp\"", "]", "\n", "", "else", ":", "\n", "                ", "hp_labels", "=", "[", "None", "]", "*", "len", "(", "labels", ")", "\n", "", "examples", ".", "append", "(", "InputExample", "(", "guid", "=", "\"%s-%d\"", ".", "format", "(", "mode", ",", "guid_index", ")", ",", "words", "=", "words", ",", "labels", "=", "labels", ",", "hp_labels", "=", "hp_labels", ")", ")", "\n", "guid_index", "+=", "1", "\n", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.convert_examples_to_features": [[64, 204], ["enumerate", "logger.info", "zip", "tokenizer.convert_tokens_to_ids", "features.append", "len", "logger.info", "tokenizer.tokenize", "tokens.extend", "label_ids.extend", "hp_label_ids.extend", "full_label_ids.extend", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "data_utils.InputFeatures", "len", "len", "str", "str", "str", "str", "str", "str", "str", "len", "len"], "function", ["None"], ["", "def", "convert_examples_to_features", "(", "\n", "examples", ",", "\n", "label_list", ",", "\n", "max_seq_length", ",", "\n", "tokenizer", ",", "\n", "cls_token_at_end", "=", "False", ",", "\n", "cls_token", "=", "\"[CLS]\"", ",", "\n", "cls_token_segment_id", "=", "1", ",", "\n", "sep_token", "=", "\"[SEP]\"", ",", "\n", "sep_token_extra", "=", "False", ",", "\n", "pad_on_left", "=", "False", ",", "\n", "pad_token", "=", "0", ",", "\n", "pad_token_segment_id", "=", "0", ",", "\n", "pad_token_label_id", "=", "-", "100", ",", "\n", "sequence_a_segment_id", "=", "0", ",", "\n", "mask_padding_with_zero", "=", "True", ",", "\n", "show_exnum", "=", "-", "1", ",", "\n", ")", ":", "\n", "    ", "\"\"\" Loads a data file into a list of `InputBatch`s\n        `cls_token_at_end` define the location of the CLS token:\n            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n    \"\"\"", "\n", "features", "=", "[", "]", "\n", "extra_long_samples", "=", "0", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "if", "ex_index", "%", "10000", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Writing example %d of %d\"", ",", "ex_index", ",", "len", "(", "examples", ")", ")", "\n", "\n", "", "tokens", "=", "[", "]", "\n", "label_ids", "=", "[", "]", "\n", "full_label_ids", "=", "[", "]", "\n", "hp_label_ids", "=", "[", "]", "\n", "for", "word", ",", "label", ",", "hp_label", "in", "zip", "(", "example", ".", "words", ",", "example", ".", "labels", ",", "example", ".", "hp_labels", ")", ":", "\n", "            ", "word_tokens", "=", "tokenizer", ".", "tokenize", "(", "word", ")", "\n", "tokens", ".", "extend", "(", "word_tokens", ")", "\n", "# Use the real label id for the first token of the word, and padding ids for the remaining tokens", "\n", "label_ids", ".", "extend", "(", "[", "label", "]", "+", "[", "pad_token_label_id", "]", "*", "(", "len", "(", "word_tokens", ")", "-", "1", ")", ")", "\n", "hp_label_ids", ".", "extend", "(", "[", "hp_label", "if", "hp_label", "is", "not", "None", "else", "pad_token_label_id", "]", "+", "[", "pad_token_label_id", "]", "*", "(", "len", "(", "word_tokens", ")", "-", "1", ")", ")", "\n", "full_label_ids", ".", "extend", "(", "[", "label", "]", "*", "len", "(", "word_tokens", ")", ")", "\n", "\n", "# Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.", "\n", "", "special_tokens_count", "=", "3", "if", "sep_token_extra", "else", "2", "\n", "if", "len", "(", "tokens", ")", ">", "max_seq_length", "-", "special_tokens_count", ":", "\n", "            ", "tokens", "=", "tokens", "[", ":", "(", "max_seq_length", "-", "special_tokens_count", ")", "]", "\n", "label_ids", "=", "label_ids", "[", ":", "(", "max_seq_length", "-", "special_tokens_count", ")", "]", "\n", "hp_label_ids", "=", "hp_label_ids", "[", ":", "(", "max_seq_length", "-", "special_tokens_count", ")", "]", "\n", "full_label_ids", "=", "full_label_ids", "[", ":", "(", "max_seq_length", "-", "special_tokens_count", ")", "]", "\n", "extra_long_samples", "+=", "1", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids:   0   0   0   0  0     0   0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambiguously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "tokens", "+=", "[", "sep_token", "]", "\n", "label_ids", "+=", "[", "pad_token_label_id", "]", "\n", "hp_label_ids", "+=", "[", "pad_token_label_id", "]", "\n", "full_label_ids", "+=", "[", "pad_token_label_id", "]", "\n", "if", "sep_token_extra", ":", "\n", "# roberta uses an extra separator b/w pairs of sentences", "\n", "            ", "tokens", "+=", "[", "sep_token", "]", "\n", "label_ids", "+=", "[", "pad_token_label_id", "]", "\n", "hp_label_ids", "+=", "[", "pad_token_label_id", "]", "\n", "full_label_ids", "+=", "[", "pad_token_label_id", "]", "\n", "", "segment_ids", "=", "[", "sequence_a_segment_id", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "if", "cls_token_at_end", ":", "\n", "            ", "tokens", "+=", "[", "cls_token", "]", "\n", "label_ids", "+=", "[", "pad_token_label_id", "]", "\n", "hp_label_ids", "+=", "[", "pad_token_label_id", "]", "\n", "full_label_ids", "+=", "[", "pad_token_label_id", "]", "\n", "segment_ids", "+=", "[", "cls_token_segment_id", "]", "\n", "", "else", ":", "\n", "            ", "tokens", "=", "[", "cls_token", "]", "+", "tokens", "\n", "label_ids", "=", "[", "pad_token_label_id", "]", "+", "label_ids", "\n", "hp_label_ids", "=", "[", "pad_token_label_id", "]", "+", "hp_label_ids", "\n", "full_label_ids", "=", "[", "pad_token_label_id", "]", "+", "full_label_ids", "\n", "segment_ids", "=", "[", "cls_token_segment_id", "]", "+", "segment_ids", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "if", "mask_padding_with_zero", "else", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding_length", "=", "max_seq_length", "-", "len", "(", "input_ids", ")", "\n", "if", "pad_on_left", ":", "\n", "            ", "input_ids", "=", "(", "[", "pad_token", "]", "*", "padding_length", ")", "+", "input_ids", "\n", "input_mask", "=", "(", "[", "0", "if", "mask_padding_with_zero", "else", "1", "]", "*", "padding_length", ")", "+", "input_mask", "\n", "segment_ids", "=", "(", "[", "pad_token_segment_id", "]", "*", "padding_length", ")", "+", "segment_ids", "\n", "label_ids", "=", "(", "[", "pad_token_label_id", "]", "*", "padding_length", ")", "+", "label_ids", "\n", "hp_label_ids", "=", "(", "[", "pad_token_label_id", "]", "*", "padding_length", ")", "+", "hp_label_ids", "\n", "full_label_ids", "=", "(", "[", "pad_token_label_id", "]", "*", "padding_length", ")", "+", "full_label_ids", "\n", "", "else", ":", "\n", "            ", "input_ids", "+=", "[", "pad_token", "]", "*", "padding_length", "\n", "input_mask", "+=", "[", "0", "if", "mask_padding_with_zero", "else", "1", "]", "*", "padding_length", "\n", "segment_ids", "+=", "[", "pad_token_segment_id", "]", "*", "padding_length", "\n", "label_ids", "+=", "[", "pad_token_label_id", "]", "*", "padding_length", "\n", "hp_label_ids", "+=", "[", "pad_token_label_id", "]", "*", "padding_length", "\n", "full_label_ids", "+=", "[", "pad_token_label_id", "]", "*", "padding_length", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "label_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "hp_label_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "full_label_ids", ")", "==", "max_seq_length", "\n", "\n", "if", "ex_index", "<", "show_exnum", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", ",", "example", ".", "guid", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"segment_ids: %s\"", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label_ids: %s\"", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "label_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"hp_label_ids: %s\"", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "hp_label_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"full_label_ids: %s\"", ",", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "full_label_ids", "]", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "input_mask", "=", "input_mask", ",", "segment_ids", "=", "segment_ids", ",", "label_ids", "=", "label_ids", ",", "full_label_ids", "=", "full_label_ids", ",", "hp_label_ids", "=", "hp_label_ids", ")", "\n", ")", "\n", "", "logger", ".", "info", "(", "\"Extra long example %d of %d\"", ",", "extra_long_samples", ",", "len", "(", "examples", ")", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.load_and_cache_examples": [[206, 261], ["os.path.join", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.distributed.barrier", "torch.distributed.barrier", "os.path.exists", "logger.info", "torch.load", "torch.load", "logger.info", "data_utils.read_examples_from_file", "data_utils.convert_examples_to_features", "torch.distributed.barrier", "torch.distributed.barrier", "logger.info", "torch.save", "torch.save", "bool", "bool", "bool", "range", "tokenizer.convert_tokens_to_ids", "len"], "function", ["home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.read_examples_from_file", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.convert_examples_to_features"], ["", "def", "load_and_cache_examples", "(", "args", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "mode", ")", ":", "\n", "\n", "    ", "if", "args", ".", "local_rank", "not", "in", "[", "-", "1", ",", "0", "]", "and", "not", "evaluate", ":", "\n", "        ", "torch", ".", "distributed", ".", "barrier", "(", ")", "# Make sure only the first process in distributed training process the dataset, and the others will use the cache", "\n", "\n", "# Load data features from cache or dataset file", "\n", "", "cached_features_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "\n", "\"{}_{}.pt\"", ".", "format", "(", "\n", "args", ".", "dataset", ",", "mode", "\n", ")", ",", "\n", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "cached_features_file", ")", "and", "not", "args", ".", "overwrite_cache", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading features from cached file %s\"", ",", "cached_features_file", ")", "\n", "features", "=", "torch", ".", "load", "(", "cached_features_file", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Creating features from dataset file at %s\"", ",", "args", ".", "data_dir", ")", "\n", "examples", "=", "read_examples_from_file", "(", "args", ",", "args", ".", "data_dir", ",", "mode", ")", "\n", "features", "=", "convert_examples_to_features", "(", "\n", "examples", ",", "\n", "labels", ",", "\n", "args", ".", "max_seq_length", ",", "\n", "tokenizer", ",", "\n", "cls_token_at_end", "=", "bool", "(", "args", ".", "model_type", "in", "[", "\"xlnet\"", "]", ")", ",", "\n", "# xlnet has a cls token at the end", "\n", "cls_token", "=", "tokenizer", ".", "cls_token", ",", "\n", "cls_token_segment_id", "=", "2", "if", "args", ".", "model_type", "in", "[", "\"xlnet\"", "]", "else", "0", ",", "\n", "sep_token", "=", "tokenizer", ".", "sep_token", ",", "\n", "sep_token_extra", "=", "bool", "(", "args", ".", "model_type", "in", "[", "\"roberta\"", "]", ")", ",", "\n", "# roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805", "\n", "pad_on_left", "=", "bool", "(", "args", ".", "model_type", "in", "[", "\"xlnet\"", "]", ")", ",", "\n", "# pad on the left for xlnet", "\n", "pad_token", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "tokenizer", ".", "pad_token", "]", ")", "[", "0", "]", ",", "\n", "pad_token_segment_id", "=", "4", "if", "args", ".", "model_type", "in", "[", "\"xlnet\"", "]", "else", "0", ",", "\n", "pad_token_label_id", "=", "pad_token_label_id", ",", "\n", ")", "\n", "if", "args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "logger", ".", "info", "(", "\"Saving features into cached file %s\"", ",", "cached_features_file", ")", "\n", "torch", ".", "save", "(", "features", ",", "cached_features_file", ")", "\n", "\n", "", "", "if", "args", ".", "local_rank", "==", "0", "and", "not", "evaluate", ":", "\n", "        ", "torch", ".", "distributed", ".", "barrier", "(", ")", "# Make sure only the first process in distributed training process the dataset, and the others will use the cache", "\n", "\n", "# Convert to Tensors and build dataset", "\n", "", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_full_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "full_label_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_hp_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "hp_label_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ids", "=", "torch", ".", "tensor", "(", "[", "f", "for", "f", "in", "range", "(", "len", "(", "features", ")", ")", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "dataset", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_label_ids", ",", "all_full_label_ids", ",", "all_hp_label_ids", ",", "all_ids", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.get_labels": [[262, 274], ["os.path.exists", "open", "json.load", "json.load.items", "labels.append"], "function", ["None"], ["", "def", "get_labels", "(", "path", "=", "None", ",", "dataset", "=", "None", ")", ":", "\n", "    ", "if", "path", "and", "os", ".", "path", ".", "exists", "(", "path", "+", "dataset", "+", "\"_tag_to_id.json\"", ")", ":", "\n", "        ", "labels", "=", "[", "]", "\n", "with", "open", "(", "path", "+", "dataset", "+", "\"_tag_to_id.json\"", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "for", "l", ",", "_", "in", "data", ".", "items", "(", ")", ":", "\n", "                ", "labels", ".", "append", "(", "l", ")", "\n", "", "", "if", "\"O\"", "not", "in", "labels", ":", "\n", "            ", "labels", "=", "[", "\"O\"", "]", "+", "labels", "\n", "", "return", "labels", "\n", "", "else", ":", "\n", "        ", "return", "[", "\"O\"", ",", "\"B-LOC\"", ",", "\"B-ORG\"", ",", "\"B-PER\"", ",", "\"B-MISC\"", ",", "\"I-PER\"", ",", "\"I-MISC\"", ",", "\"I-ORG\"", ",", "\"I-LOC\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.tag_to_id": [[275, 282], ["os.path.exists", "open", "json.load"], "function", ["None"], ["", "", "def", "tag_to_id", "(", "path", "=", "None", ",", "dataset", "=", "None", ")", ":", "\n", "    ", "if", "path", "and", "os", ".", "path", ".", "exists", "(", "path", "+", "dataset", "+", "\"_tag_to_id.json\"", ")", ":", "\n", "        ", "with", "open", "(", "path", "+", "dataset", "+", "\"_tag_to_id.json\"", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "data", "\n", "", "else", ":", "\n", "        ", "return", "{", "\"O\"", ":", "0", ",", "\"B-LOC\"", ":", "1", ",", "\"B-ORG\"", ":", "2", ",", "\"B-PER\"", ":", "3", ",", "\"B-MISC\"", ":", "4", ",", "\"I-PER\"", ":", "5", ",", "\"I-MISC\"", ":", "6", ",", "\"I-ORG\"", ":", "7", ",", "\"I-LOC\"", ":", "8", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.get_chunk_type": [[283, 300], ["tag_name.split", "tag_name.split"], "function", ["None"], ["", "", "def", "get_chunk_type", "(", "tok", ",", "idx_to_tag", ")", ":", "\n", "    ", "\"\"\"\n    The function takes in a chunk (\"B-PER\") and then splits it into the tag (PER) and its class (B)\n    as defined in BIOES\n\n    Args:\n        tok: id of token, ex 4\n        idx_to_tag: dictionary {4: \"B-PER\", ...}\n\n    Returns:\n        tuple: \"B\", \"PER\"\n\n    \"\"\"", "\n", "tag_name", "=", "idx_to_tag", "[", "tok", "]", "\n", "tag_class", "=", "tag_name", ".", "split", "(", "'-'", ")", "[", "0", "]", "\n", "tag_type", "=", "tag_name", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", "\n", "return", "tag_class", ",", "tag_type", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.get_chunks": [[301, 350], ["enumerate", "chunks.append", "tags.items", "chunks.append", "len", "data_utils.get_chunk_type", "chunks.append"], "function", ["home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.get_chunk_type"], ["", "def", "get_chunks", "(", "seq", ",", "tags", ")", ":", "\n", "    ", "\"\"\"Given a sequence of tags, group entities and their position\n\n    Args:\n        seq: [4, 4, 0, 0, ...] sequence of labels\n        tags: dict[\"O\"] = 4\n\n    Returns:\n        list of (chunk_type, chunk_start, chunk_end)\n\n    Example:\n        seq = [4, 5, 0, 3]\n        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n\n    \"\"\"", "\n", "default", "=", "tags", "[", "\"O\"", "]", "\n", "idx_to_tag", "=", "{", "idx", ":", "tag", "for", "tag", ",", "idx", "in", "tags", ".", "items", "(", ")", "}", "\n", "chunks", "=", "[", "]", "\n", "\n", "chunk_type", ",", "chunk_start", "=", "None", ",", "None", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "seq", ")", ":", "\n", "        ", "if", "tok", "==", "default", "and", "chunk_type", "is", "not", "None", ":", "\n", "            ", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "i", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "chunk_type", ",", "chunk_start", "=", "None", ",", "None", "\n", "\n", "", "elif", "tok", "!=", "default", ":", "\n", "            ", "tok_chunk_class", ",", "tok_chunk_type", "=", "get_chunk_type", "(", "tok", ",", "idx_to_tag", ")", "\n", "if", "chunk_type", "is", "None", ":", "\n", "                ", "if", "tok_chunk_class", "!=", "\"I\"", ":", "\n", "                    ", "chunk_type", ",", "chunk_start", "=", "tok_chunk_type", ",", "i", "\n", "", "else", ":", "\n", "                    ", "pass", "\n", "", "", "elif", "tok_chunk_type", "!=", "chunk_type", ":", "\n", "                ", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "i", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "if", "tok_chunk_class", "!=", "\"I\"", ":", "\n", "                    ", "chunk_type", ",", "chunk_start", "=", "tok_chunk_type", ",", "i", "\n", "", "else", ":", "\n", "                    ", "chunk_type", ",", "chunk_start", "=", "None", ",", "None", "\n", "\n", "", "", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "", "if", "chunk_type", "is", "not", "None", ":", "\n", "        ", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "len", "(", "seq", ")", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "", "return", "chunks", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.get_chunks_token": [[351, 384], ["enumerate", "tags.items", "chunks.append"], "function", ["None"], ["", "def", "get_chunks_token", "(", "seq", ",", "tags", ")", ":", "\n", "    ", "\"\"\"Given a sequence of tags, group entities and their position\n\n    Args:\n        seq: [4, 4, 0, 0, ...] sequence of labels\n        tags: dict[\"O\"] = 4\n\n    Returns:\n        list of (chunk_type, chunk_start, chunk_end)\n\n    Example:\n        seq = [4, 5, 0, 3]\n        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n\n    \"\"\"", "\n", "default", "=", "tags", "[", "\"O\"", "]", "\n", "idx_to_tag", "=", "{", "idx", ":", "tag", "for", "tag", ",", "idx", "in", "tags", ".", "items", "(", ")", "}", "\n", "chunks", "=", "[", "]", "\n", "\n", "chunk_type", ",", "chunk_start", "=", "None", ",", "None", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "seq", ")", ":", "\n", "# if tok == default and chunk_type is not None:", "\n", "#     chunk = (chunk_type, chunk_start, i)", "\n", "#     chunks.append(chunk)", "\n", "#     chunk_type, chunk_start = None, None", "\n", "        ", "if", "tok", "!=", "default", ":", "\n", "            ", "chunk", "=", "(", "idx_to_tag", "[", "tok", "]", ",", "i", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "", "return", "chunks", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.eval.evaluate": [[28, 126], ["utils.data_utils.load_and_cache_examples", "torch.utils.data.DataLoader", "logger.info", "model.eval", "tqdm.tqdm", "numpy.argmax", "range", "zip", "logger.info", "sorted", "max", "torch.utils.data.SequentialSampler", "torch.utils.data.distributed.DistributedSampler", "logger.info", "logger.info", "tuple", "range", "set", "set", "len", "len", "len", "results.keys", "logger.info", "len", "torch.no_grad", "model", "tmp_eval_loss.mean.item", "logits.detach().cpu().numpy", "[].detach().cpu().numpy", "numpy.append", "numpy.append", "enumerate", "range", "range", "range", "utils.data_utils.get_chunks", "utils.data_utils.get_chunks", "str", "t.to", "tmp_eval_loss.mean.mean", "logits.detach().cpu().numpy", "[].detach().cpu().numpy", "preds_list[].append", "out_id_list[].append", "preds_id_list[].append", "utils.data_utils.tag_to_id", "utils.data_utils.tag_to_id", "logits.detach().cpu", "[].detach().cpu", "logits.detach().cpu", "[].detach().cpu", "logits.detach", "[].detach", "logits.detach", "[].detach"], "function", ["home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.load_and_cache_examples", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.get_chunks", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.get_chunks", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.tag_to_id", "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.data_utils.tag_to_id"], ["def", "evaluate", "(", "args", ",", "model", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "best", ",", "mode", ",", "logger", ",", "prefix", "=", "\"\"", ",", "verbose", "=", "True", ")", ":", "\n", "\n", "    ", "eval_dataset", "=", "load_and_cache_examples", "(", "args", ",", "tokenizer", ",", "labels", ",", "pad_token_label_id", ",", "mode", "=", "mode", ")", "\n", "\n", "args", ".", "eval_batch_size", "=", "args", ".", "per_gpu_eval_batch_size", "*", "max", "(", "1", ",", "args", ".", "n_gpu", ")", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_dataset", ")", "if", "args", ".", "local_rank", "==", "-", "1", "else", "DistributedSampler", "(", "eval_dataset", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_dataset", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "# multi-gpu evaluate", "\n", "# if args.n_gpu > 1:", "\n", "#     model = torch.nn.DataParallel(model)", "\n", "\n", "logger", ".", "info", "(", "\"***** Running evaluation %s *****\"", ",", "prefix", ")", "\n", "if", "verbose", ":", "\n", "        ", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_dataset", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "", "eval_loss", "=", "0.0", "\n", "nb_eval_steps", "=", "0", "\n", "preds", "=", "None", "\n", "out_label_ids", "=", "None", "\n", "model", ".", "eval", "(", ")", "\n", "for", "batch", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ":", "\n", "        ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "args", ".", "device", ")", "for", "t", "in", "batch", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "inputs", "=", "{", "\"input_ids\"", ":", "batch", "[", "0", "]", ",", "\"attention_mask\"", ":", "batch", "[", "1", "]", ",", "\"labels\"", ":", "{", "\"pseudo\"", ":", "batch", "[", "3", "]", "}", "}", "\n", "if", "args", ".", "model_type", "!=", "\"distilbert\"", ":", "\n", "                ", "inputs", "[", "\"token_type_ids\"", "]", "=", "(", "\n", "batch", "[", "2", "]", "if", "args", ".", "model_type", "in", "[", "\"bert\"", ",", "\"mobilebert\"", "]", "else", "None", "\n", ")", "# XLM and RoBERTa don\"t use segment_ids", "\n", "", "outputs", "=", "model", "(", "**", "inputs", ")", "\n", "tmp_eval_loss_dict", ",", "logits", "=", "outputs", "[", ":", "2", "]", "\n", "tmp_eval_loss", "=", "tmp_eval_loss_dict", "[", "\"pseudo\"", "]", "\n", "\n", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "tmp_eval_loss", "=", "tmp_eval_loss", ".", "mean", "(", ")", "\n", "\n", "", "eval_loss", "+=", "tmp_eval_loss", ".", "item", "(", ")", "\n", "", "nb_eval_steps", "+=", "1", "\n", "if", "preds", "is", "None", ":", "\n", "            ", "preds", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "out_label_ids", "=", "inputs", "[", "\"labels\"", "]", "[", "\"pseudo\"", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "            ", "preds", "=", "np", ".", "append", "(", "preds", ",", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "0", ")", "\n", "out_label_ids", "=", "np", ".", "append", "(", "out_label_ids", ",", "inputs", "[", "\"labels\"", "]", "[", "\"pseudo\"", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "0", ")", "\n", "\n", "", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "# print(preds)", "\n", "preds", "=", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "2", ")", "\n", "\n", "label_map", "=", "{", "i", ":", "label", "for", "i", ",", "label", "in", "enumerate", "(", "labels", ")", "}", "\n", "preds_list", "=", "[", "[", "]", "for", "_", "in", "range", "(", "out_label_ids", ".", "shape", "[", "0", "]", ")", "]", "\n", "out_id_list", "=", "[", "[", "]", "for", "_", "in", "range", "(", "out_label_ids", ".", "shape", "[", "0", "]", ")", "]", "\n", "preds_id_list", "=", "[", "[", "]", "for", "_", "in", "range", "(", "out_label_ids", ".", "shape", "[", "0", "]", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "out_label_ids", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "out_label_ids", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "if", "out_label_ids", "[", "i", ",", "j", "]", "!=", "pad_token_label_id", ":", "\n", "                ", "preds_list", "[", "i", "]", ".", "append", "(", "label_map", "[", "preds", "[", "i", "]", "[", "j", "]", "]", ")", "\n", "out_id_list", "[", "i", "]", ".", "append", "(", "out_label_ids", "[", "i", "]", "[", "j", "]", ")", "\n", "preds_id_list", "[", "i", "]", ".", "append", "(", "preds", "[", "i", "]", "[", "j", "]", ")", "\n", "\n", "", "", "", "correct_preds", ",", "total_correct", ",", "total_preds", "=", "0.", ",", "0.", ",", "0.", "# i variables", "\n", "for", "ground_truth_id", ",", "predicted_id", "in", "zip", "(", "out_id_list", ",", "preds_id_list", ")", ":", "\n", "# We use the get chunks function defined above to get the true chunks", "\n", "# and the predicted chunks from true labels and predicted labels respectively", "\n", "        ", "lab_chunks", "=", "set", "(", "get_chunks", "(", "ground_truth_id", ",", "tag_to_id", "(", "args", ".", "data_dir", ",", "args", ".", "dataset", ")", ")", ")", "\n", "lab_pred_chunks", "=", "set", "(", "get_chunks", "(", "predicted_id", ",", "tag_to_id", "(", "args", ".", "data_dir", ",", "args", ".", "dataset", ")", ")", ")", "\n", "\n", "# Updating the i variables", "\n", "correct_preds", "+=", "len", "(", "lab_chunks", "&", "lab_pred_chunks", ")", "\n", "total_preds", "+=", "len", "(", "lab_pred_chunks", ")", "\n", "total_correct", "+=", "len", "(", "lab_chunks", ")", "\n", "\n", "", "p", "=", "correct_preds", "/", "total_preds", "if", "correct_preds", ">", "0", "else", "0", "\n", "r", "=", "correct_preds", "/", "total_correct", "if", "correct_preds", ">", "0", "else", "0", "\n", "new_F", "=", "2", "*", "p", "*", "r", "/", "(", "p", "+", "r", ")", "if", "correct_preds", ">", "0", "else", "0", "\n", "\n", "is_updated", "=", "False", "\n", "if", "new_F", ">", "best", "[", "-", "1", "]", ":", "\n", "        ", "best", "=", "[", "p", ",", "r", ",", "new_F", "]", "\n", "is_updated", "=", "True", "\n", "\n", "", "results", "=", "{", "\n", "\"loss\"", ":", "eval_loss", ",", "\n", "\"precision\"", ":", "p", ",", "\n", "\"recall\"", ":", "r", ",", "\n", "\"f1\"", ":", "new_F", ",", "\n", "\"best_precision\"", ":", "best", "[", "0", "]", ",", "\n", "\"best_recall\"", ":", "best", "[", "1", "]", ",", "\n", "\"best_f1\"", ":", "best", "[", "-", "1", "]", "\n", "}", "\n", "\n", "logger", ".", "info", "(", "\"***** Eval results %s *****\"", ",", "prefix", ")", "\n", "for", "key", "in", "sorted", "(", "results", ".", "keys", "(", ")", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "results", "[", "key", "]", ")", ")", "\n", "\n", "", "return", "results", ",", "preds_list", ",", "best", ",", "is_updated", "\n", "", ""]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.loss_utils.CrossEntropyLabelSmooth.__init__": [[19, 24], ["torch.Module.__init__", "torch.LogSoftmax().cuda", "torch.LogSoftmax().cuda", "torch.LogSoftmax().cuda", "torch.LogSoftmax", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.airobotzhang_scdl.models.modeling_roberta.RobertaForTokenClassification_Modified.__init__"], ["def", "__init__", "(", "self", ",", "num_classes", ",", "epsilon", "=", "0.1", ")", ":", "\n", "\t\t", "super", "(", "CrossEntropyLabelSmooth", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.loss_utils.CrossEntropyLabelSmooth.forward": [[25, 36], ["loss_utils.CrossEntropyLabelSmooth.logsoftmax", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_.unsqueeze", "torch.zeros_like().scatter_.unsqueeze", "torch.zeros_like().scatter_.unsqueeze", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "targets", ")", ":", "\n", "\t\t", "\"\"\"\n\t\tArgs:\n\t\t\tinputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\n\t\t\ttargets: ground truth labels with shape (num_classes)\n\t\t\"\"\"", "\n", "log_probs", "=", "self", ".", "logsoftmax", "(", "inputs", ")", "\n", "targets", "=", "torch", ".", "zeros_like", "(", "log_probs", ")", ".", "scatter_", "(", "1", ",", "targets", ".", "unsqueeze", "(", "1", ")", ",", "1", ")", "\n", "targets", "=", "(", "1", "-", "self", ".", "epsilon", ")", "*", "targets", "+", "self", ".", "epsilon", "/", "self", ".", "num_classes", "\n", "loss", "=", "(", "-", "targets", "*", "log_probs", ")", ".", "mean", "(", "0", ")", ".", "sum", "(", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.loss_utils.SoftEntropy.__init__": [[38, 41], ["torch.Module.__init__", "torch.LogSoftmax().cuda", "torch.LogSoftmax().cuda", "torch.LogSoftmax().cuda", "torch.LogSoftmax", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.airobotzhang_scdl.models.modeling_roberta.RobertaForTokenClassification_Modified.__init__"], ["\t", "def", "__init__", "(", "self", ")", ":", "\n", "\t\t", "super", "(", "SoftEntropy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.loss_utils.SoftEntropy.forward": [[42, 46], ["loss_utils.SoftEntropy.logsoftmax", "torch.softmax().detach", "torch.softmax().detach", "torch.softmax().detach", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "targets", ")", ":", "\n", "\t\t", "log_probs", "=", "self", ".", "logsoftmax", "(", "inputs", ")", "\n", "loss", "=", "(", "-", "F", ".", "softmax", "(", "targets", ",", "dim", "=", "1", ")", ".", "detach", "(", ")", "*", "log_probs", ")", ".", "mean", "(", "0", ")", ".", "sum", "(", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.loss_utils.NegEntropy.__init__": [[48, 51], ["torch.Module.__init__", "torch.LogSoftmax().cuda", "torch.LogSoftmax().cuda", "torch.LogSoftmax().cuda", "torch.LogSoftmax", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.airobotzhang_scdl.models.modeling_roberta.RobertaForTokenClassification_Modified.__init__"], ["\t", "def", "__init__", "(", "self", ")", ":", "\n", "\t\t", "super", "(", "NegEntropy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.loss_utils.NegEntropy.forward": [[52, 56], ["loss_utils.NegEntropy.logsoftmax", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "\t\t", "log_probs", "=", "self", ".", "logsoftmax", "(", "inputs", ")", "\n", "loss", "=", "(", "F", ".", "softmax", "(", "inputs", ",", "dim", "=", "1", ")", "*", "log_probs", ")", ".", "mean", "(", "0", ")", ".", "sum", "(", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.loss_utils.FocalLoss.__init__": [[58, 63], ["torch.Module.__init__", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.airobotzhang_scdl.models.modeling_roberta.RobertaForTokenClassification_Modified.__init__"], ["\t", "def", "__init__", "(", "self", ",", "weight", "=", "None", ",", "reduction", "=", "'mean'", ",", "gamma", "=", "0", ",", "eps", "=", "1e-7", ")", ":", "\n", "\t\t", "super", "(", "FocalLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "ce", "=", "nn", ".", "CrossEntropyLoss", "(", "weight", "=", "weight", ",", "reduction", "=", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.loss_utils.FocalLoss.forward": [[64, 69], ["loss_utils.FocalLoss.ce", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "targets", ")", ":", "\n", "\t\t", "log_probs", "=", "self", ".", "ce", "(", "inputs", ",", "targets", ")", "\n", "probs", "=", "torch", ".", "exp", "(", "-", "log_probs", ")", "\n", "loss", "=", "(", "1", "-", "probs", ")", "**", "self", ".", "gamma", "*", "log_probs", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.loss_utils.SoftFocalLoss.__init__": [[71, 77], ["torch.Module.__init__", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.LogSoftmax().cuda", "torch.LogSoftmax().cuda", "torch.LogSoftmax().cuda", "torch.LogSoftmax", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.airobotzhang_scdl.models.modeling_roberta.RobertaForTokenClassification_Modified.__init__"], ["\t", "def", "__init__", "(", "self", ",", "weight", "=", "None", ",", "reduction", "=", "'mean'", ",", "gamma", "=", "0", ",", "eps", "=", "1e-7", ")", ":", "\n", "\t\t", "super", "(", "SoftFocalLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "ce", "=", "nn", ".", "CrossEntropyLoss", "(", "weight", "=", "weight", ",", "reduction", "=", "reduction", ")", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.utils.loss_utils.SoftFocalLoss.forward": [[78, 89], ["loss_utils.SoftFocalLoss.logsoftmax", "torch.softmax().detach", "torch.softmax().detach", "torch.softmax().detach", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "targets", ")", ":", "\n", "# print(\"+++data+++\")", "\n", "# print(inputs)", "\n", "\t\t", "log_probs", "=", "self", ".", "logsoftmax", "(", "inputs", ")", "\n", "# print(\"+++++++\")", "\n", "# for log_prob in log_probs:", "\n", "# \tprint(log_prob)", "\n", "loss", "=", "(", "-", "F", ".", "softmax", "(", "targets", ",", "dim", "=", "1", ")", ".", "detach", "(", ")", "*", "log_probs", "*", "(", "(", "1", "-", "F", ".", "softmax", "(", "inputs", ",", "dim", "=", "1", ")", ")", "**", "self", ".", "gamma", ")", ")", ".", "mean", "(", "0", ")", ".", "sum", "(", ")", "\n", "# print(\"loss\")", "\n", "# print(loss)", "\n", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.airobotzhang_scdl.models.modeling_roberta.RobertaForTokenClassification_Modified.__init__": [[46, 58], ["transformers.BertPreTrainedModel.__init__", "transformers.RobertaModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax", "modeling_roberta.RobertaForTokenClassification_Modified.init_weights"], "methods", ["home.repos.pwc.inspect_result.airobotzhang_scdl.models.modeling_roberta.RobertaForTokenClassification_Modified.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "# self.focalloss = FocalLoss(gamma=2)", "\n", "# self.softfocalloss = SoftFocalLoss(gamma=2)", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.airobotzhang_scdl.models.modeling_roberta.RobertaForTokenClassification_Modified.forward": [[59, 134], ["modeling_roberta.RobertaForTokenClassification_Modified.roberta", "modeling_roberta.RobertaForTokenClassification_Modified.dropout", "modeling_roberta.RobertaForTokenClassification_Modified.classifier", "attention_mask.view", "modeling_roberta.RobertaForTokenClassification_Modified.view", "torch.nn.KLDivLoss", "torch.nn.KLDivLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "label_mask.view", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "label.view", "label.view", "modeling_roberta.RobertaForTokenClassification_Modified.view", "label.view"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "label_mask", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "outputs", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", ")", "\n", "\n", "final_embedding", "=", "outputs", "[", "0", "]", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "final_embedding", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "outputs", "=", "(", "logits", ",", "final_embedding", ",", ")", "+", "outputs", "[", "2", ":", "]", "# add hidden states and attention if they are here", "\n", "\n", "loss_dict", "=", "{", "}", "\n", "if", "labels", "is", "not", "None", ":", "\n", "# logits = self.logsoftmax(logits)", "\n", "# Only keep active parts of the loss", "\n", "            ", "active_loss", "=", "True", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "# active_loss = True", "\n", "# if attention_mask is not None:", "\n", "#     active_loss = attention_mask.view(-1) == 1", "\n", "# if label_mask is not None:", "\n", "#     active_loss = active_loss & label_mask.view(-1)", "\n", "# active_logits = logits.view(-1, self.num_labels)[active_loss]", "\n", "\n", "", "for", "key", "in", "labels", ":", "\n", "                ", "label", "=", "labels", "[", "key", "]", "\n", "if", "label", "is", "None", ":", "\n", "                    ", "continue", "\n", "# if key==\"pseudo\" and label_mask is not None:", "\n", "", "if", "label_mask", "is", "not", "None", ":", "\n", "                    ", "all_active_loss", "=", "active_loss", "&", "label_mask", ".", "view", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "                    ", "all_active_loss", "=", "active_loss", "\n", "", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "all_active_loss", "]", "\n", "\n", "if", "label", ".", "shape", "==", "logits", ".", "shape", ":", "\n", "                    ", "loss_fct", "=", "KLDivLoss", "(", ")", "\n", "# loss_fct = SoftFocalLoss(gamma=2)", "\n", "if", "attention_mask", "is", "not", "None", "or", "label_mask", "is", "not", "None", ":", "\n", "                        ", "active_labels", "=", "label", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "all_active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                        ", "loss", "=", "loss_fct", "(", "logits", ",", "label", ")", "\n", "", "", "else", ":", "\n", "                    ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# loss_fct = FocalLoss(gamma=2)", "\n", "# loss_fct = NLLLoss()", "\n", "if", "attention_mask", "is", "not", "None", "or", "label_mask", "is", "not", "None", ":", "\n", "                        ", "active_labels", "=", "label", ".", "view", "(", "-", "1", ")", "[", "all_active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                        ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "label", ".", "view", "(", "-", "1", ")", ")", "\n", "", "", "loss_dict", "[", "key", "]", "=", "loss", "\n", "\n", "\n", "", "outputs", "=", "(", "loss_dict", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "# (loss dict), scores, final_embedding, (hidden_states), (attentions)", "\n", "", "", ""]]}