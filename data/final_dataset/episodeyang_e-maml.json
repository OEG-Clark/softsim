{"home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.config.config_run": [[23, 28], ["G.update", "datetime.datetime.now", "DIR_TEMPLATE.format"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update"], ["", "def", "config_run", "(", "**", "_G", ")", ":", "\n", "    ", "G", ".", "update", "(", "_G", ")", "\n", "from", "datetime", "import", "datetime", "\n", "now", "=", "datetime", ".", "now", "(", ")", "\n", "RUN", ".", "log_prefix", "=", "DIR_TEMPLATE", ".", "format", "(", "now", "=", "now", ",", "G", "=", "G", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_policies.MlpPolicy.__repr__": [[26, 28], ["None"], "methods", ["None"], ["def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "f\"{self.__class__} {self.name}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_policies.MlpPolicy.__init__": [[30, 133], ["isinstance", "isinstance", "tensorflow.variable_scope", "range", "ge_policies.fc", "isinstance", "tensorflow.variable_scope", "baselines.common.distributions.make_pdtype", "ge_policies.MlpPolicy.pdtype.pdfromflat", "ge_policies.MlpPolicy.pd.sample", "ge_policies.MlpPolicy.pd.mode", "ge_policies.MlpPolicy.pd.neglogp", "TypeError", "ge_policies.fc", "tensorflow.concat", "NotImplemented", "ge_policies.fc", "tensorflow.shape", "tensorflow.tile", "numpy.sqrt", "tensorflow.ones", "tensorflow.get_variable", "tensorflow.tile", "ge_policies.fc", "tensorflow.constant_initializer"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc", "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.Controls.sample", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.mode", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.neglogp", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc"], ["", "def", "__init__", "(", "self", ",", "ac_space", ",", "X", ",", "hidden_size", ",", "n_layers", "=", "2", ",", "activation", "=", "\"tanh\"", ",", "value_baseline", "=", "False", ",", "\n", "scope", "=", "'MlpPolicy'", ",", "reuse", "=", "False", ",", "X_placeholder", "=", "None", ",", "fix_variance", "=", "False", ",", "init_logstd", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Gaussian Policy. The variance is learned as parameters. You can also pass in the logstd from the outside.\n\n            __init__: Construct the graph for the MLP policy.\n\n        :param ac_space: action space, one of `gym.spaces.Box`\n        :param X: Tensor or input placeholder for the observation\n            :param hidden_size: size of hidden layers in network\n        :param activation: one of 'reLU', 'tanh'\n        :param scope: str, name of variable scope.\n        :param reuse:\n        :param value_baseline: bool flag whether compute a value baseline\n        :param X_placeholder:\n        :param fix_variance:\n        :param init_logstd:\n        \"\"\"", "\n", "assert", "n_layers", ">=", "2", ",", "f\"hey, what's going on with this puny {n_layers}-layer network? \"", "f\"--Ge (your friendly lab-mate)\"", "\n", "if", "isinstance", "(", "scope", ",", "tf", ".", "VariableScope", ")", ":", "\n", "            ", "self", ".", "scope_name", "=", "scope", ".", "name", "\n", "", "else", ":", "\n", "            ", "self", ".", "scope_name", "=", "scope", "\n", "", "self", ".", "name", "=", "(", "self", ".", "scope_name", "+", "\"_reuse\"", ")", "if", "reuse", "else", "self", ".", "scope_name", "\n", "\n", "self", ".", "X_ph", "=", "X", "if", "X_placeholder", "is", "None", "else", "X_placeholder", "\n", "\n", "# done: this only applies to Discrete action space. Need to make more general.", "\n", "# now it works for both discrete action and gaussian policies.", "\n", "if", "isinstance", "(", "ac_space", ",", "spaces", ".", "Discrete", ")", ":", "\n", "            ", "act_dim", "=", "ac_space", ".", "n", "\n", "", "else", ":", "\n", "            ", "act_dim", ",", "*", "_", "=", "ac_space", ".", "shape", "\n", "\n", "", "if", "activation", "==", "'tanh'", ":", "\n", "            ", "act", "=", "tf", ".", "tanh", "\n", "", "elif", "activation", "==", "\"relu\"", ":", "\n", "            ", "act", "=", "tf", ".", "nn", ".", "relu", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "f\"{activation} is not available in this MLP.\"", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", ",", "reuse", "=", "reuse", ")", ":", "\n", "            ", "h_", "=", "X", "\n", "for", "i", "in", "range", "(", "1", ",", "n_layers", "+", "1", ")", ":", "# there is no off-by-one error here --Ge.", "\n", "                ", "h_", "=", "fc", "(", "h_", ",", "f'pi_fc_{i}'", ",", "nh", "=", "hidden_size", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ",", "act", "=", "act", ")", "\n", "# a_ = fc(h_, f'pi_attn_{i}', nh=h_.shape[1], init_scale=np.sqrt(2), act=tf.math.sigmoid)", "\n", "# h_ = fc(h_ * a_, f'pi_fc_{i}', nh=hidden_size, init_scale=np.sqrt(2), act=act)", "\n", "", "mu", "=", "fc", "(", "h_", ",", "'pi'", ",", "act_dim", ",", "act", "=", "lambda", "x", ":", "x", ",", "init_scale", "=", "0.01", ")", "\n", "# _ = fc(h2, 'pi', act_dim, act=tf.tanh, init_scale=0.01)", "\n", "# mu = ac_space.low + 0.5 * (ac_space.high - ac_space.low) * (_ + 1)", "\n", "\n", "self", ".", "h_", "=", "h_", "# used for learned loss", "\n", "\n", "# assert (not G.vf_coef) ^ (G.baseline == \"critic\"), \"These two can not be true or false at the same time.\"", "\n", "if", "value_baseline", ":", "\n", "# todo: conditionally declare these only when used", "\n", "# h1 = fc(X, 'vf_fc1', nh=hidden_size, init_scale=np.sqrt(2), act=act)", "\n", "# h2 = fc(h1, 'vf_fc2', nh=hidden_size, init_scale=np.sqrt(2), act=act)", "\n", "                ", "self", ".", "vf", "=", "fc", "(", "self", ".", "h_", ",", "'vf'", ",", "1", ",", "act", "=", "lambda", "x", ":", "x", ")", "[", ":", ",", "0", "]", "\n", "\n", "", "if", "isinstance", "(", "ac_space", ",", "spaces", ".", "Box", ")", ":", "# gaussian policy requires logstd", "\n", "                ", "shape", "=", "tf", ".", "shape", "(", "mu", ")", "[", "0", "]", "\n", "if", "fix_variance", ":", "\n", "                    ", "_", "=", "tf", ".", "ones", "(", "shape", "=", "[", "1", ",", "act_dim", "]", ",", "name", "=", "\"unit_logstd\"", ")", "*", "(", "init_logstd", "or", "0", ")", "\n", "logstd", "=", "tf", ".", "tile", "(", "_", ",", "[", "shape", ",", "1", "]", ")", "\n", "", "elif", "init_logstd", "is", "not", "None", ":", "\n", "                    ", "_", "=", "tf", ".", "get_variable", "(", "name", "=", "\"logstd\"", ",", "shape", "=", "[", "1", ",", "act_dim", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "init_logstd", ")", ")", "\n", "# todo: clip logstd to limit the range.", "\n", "logstd", "=", "tf", ".", "tile", "(", "_", ",", "[", "shape", ",", "1", "]", ")", "\n", "", "else", ":", "\n", "# use variance network when no initial logstd is given.", "\n", "# _ = fc(X, 'logstd_fc1', nh=hidden_size, init_scale=np.sqrt(2), act=act)", "\n", "# _ = fc(_, 'logstd_fc2', nh=hidden_size, init_scale=np.sqrt(2), act=act)", "\n", "\n", "# note: this doesn't work. Really need to bound the variance.", "\n", "# logstd = 1 + fc(self.h_, 'logstd', act_dim, act=lambda x: x, init_scale=0.01)", "\n", "                    ", "logstd", "=", "fc", "(", "self", ".", "h_", ",", "'logstd'", ",", "act_dim", ",", "act", "=", "lambda", "x", ":", "x", ",", "init_scale", "=", "0.01", ")", "\n", "# logstd = fc(self.h2, 'logstd', act_dim, act=tf.tanh, init_scale=0.01)", "\n", "# logstd = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (logstd + 1)", "\n", "\n", "# GaussianPd takes 2 * [act_length] b/c of the logstd concatenation.", "\n", "", "ac", "=", "tf", ".", "concat", "(", "[", "mu", ",", "logstd", "]", ",", "axis", "=", "1", ")", "\n", "# A much simpler way is to multiply _logstd with a zero tensor shaped as mu.", "\n", "# [mu, mu * 0 + _logstd]", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplemented", "(", "'Discrete action space is not implemented!'", ")", "\n", "\n", "# list of parameters is fixed at graph time.", "\n", "# todo: Only gets trainables that are newly created by the current policy function.", "\n", "# self.trainables = tf.trainable_variables()", "\n", "\n", "# placeholders = placeholders_from_variables(self.trainables)", "\n", "# self._assign_placeholder_dict = {t.name: p for t, p in zip(self.trainables, placeholders)}", "\n", "# self._assign_op = tf.group(*[v.assign(p) for v, p in zip(self.trainables, placeholders)])", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"Gaussian_Action\"", ")", ":", "\n", "            ", "self", ".", "pdtype", "=", "make_pdtype", "(", "ac_space", ")", "\n", "self", ".", "pd", "=", "self", ".", "pdtype", ".", "pdfromflat", "(", "ac", ")", "\n", "\n", "self", ".", "a", "=", "a", "=", "self", ".", "pd", ".", "sample", "(", ")", "\n", "self", ".", "mu", "=", "self", ".", "pd", ".", "mode", "(", ")", "\n", "self", ".", "neglogpac", "=", "self", ".", "pd", ".", "neglogp", "(", "a", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_policies.MlpPolicy.trainables": [[134, 137], ["DeprecationWarning"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "trainables", "(", "self", ")", ":", "\n", "        ", "raise", "DeprecationWarning", "(", "\"deprecated b/c bias transform.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_policies.MlpPolicy.state_dict": [[138, 142], ["zip", "tensorflow.get_default_session().run", "tensorflow.get_default_session"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_dict", "(", "self", ")", ":", "\n", "# todo: should make the tensor names scoped locally.", "\n", "        ", "return", "{", "t", ".", "name", ":", "v", "for", "t", ",", "v", "in", "zip", "(", "self", ".", "trainables", ",", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "self", ".", "trainables", ")", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_policies.MlpPolicy.step": [[148, 160], ["tensorflow.get_default_session", "feed_dict.update", "tensorflow.get_default_session.run", "tensorflow.get_default_session.run"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update"], ["", "def", "step", "(", "self", ",", "ob", ",", "soft", ",", "feed_dict", "=", "None", ")", ":", "\n", "        ", "if", "feed_dict", ":", "\n", "            ", "feed_dict", ".", "update", "(", "{", "self", ".", "X_ph", ":", "ob", "}", ")", "\n", "", "else", ":", "\n", "            ", "feed_dict", "=", "{", "self", ".", "X_ph", ":", "ob", "}", "\n", "", "sess", "=", "tf", ".", "get_default_session", "(", ")", "\n", "if", "self", ".", "vf", "is", "None", ":", "\n", "            ", "ts", "=", "[", "self", ".", "a", "if", "soft", "else", "self", ".", "mu", ",", "self", ".", "neglogpac", "]", "\n", "return", "sess", ".", "run", "(", "ts", ",", "feed_dict", "=", "feed_dict", ")", "\n", "", "else", ":", "\n", "            ", "ts", "=", "[", "self", ".", "a", "if", "soft", "else", "self", ".", "mu", ",", "self", ".", "vf", ",", "self", ".", "neglogpac", "]", "\n", "return", "sess", ".", "run", "(", "ts", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_policies.MlpPolicy.value": [[163, 170], ["tensorflow.get_default_session", "tensorflow.get_default_session.run", "feed_dict.update"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update"], ["def", "value", "(", "self", ",", "ob", ",", "feed_dict", "=", "None", ")", ":", "\n", "        ", "if", "feed_dict", ":", "\n", "            ", "feed_dict", ".", "update", "(", "{", "self", ".", "X_ph", ":", "ob", "}", ")", "\n", "", "else", ":", "\n", "            ", "feed_dict", "=", "{", "self", ".", "X_ph", ":", "ob", "}", "\n", "", "sess", "=", "tf", ".", "get_default_session", "(", ")", "\n", "return", "sess", ".", "run", "(", "self", ".", "vf", ",", "feed_dict", "=", "feed_dict", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_policies.fc": [[9, 17], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "act", "tensorflow.matmul", "x.get_shape", "baselines.a2c.utils.ortho_init", "tensorflow.constant_initializer"], "function", ["None"], ["def", "fc", "(", "x", ",", "scope", ",", "nh", ",", "act", "=", "tf", ".", "nn", ".", "relu", ",", "init_scale", "=", "1.0", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", ")", ":", "\n", "        ", "nin", "=", "x", ".", "get_shape", "(", ")", "[", "-", "1", "]", ".", "value", "# can take batched or individual tensors.", "\n", "w", "=", "tf", ".", "get_variable", "(", "\"w\"", ",", "[", "nin", ",", "nh", "]", ",", "initializer", "=", "ortho_init", "(", "init_scale", ")", ")", "\n", "b", "=", "tf", ".", "get_variable", "(", "\"b\"", ",", "[", "nh", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ")", "\n", "z", "=", "tf", ".", "matmul", "(", "x", ",", "w", ")", "+", "b", "\n", "h", "=", "act", "(", "z", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.e_maml_ge.Meta.__init__": [[30, 112], ["tensorflow.variable_scope", "tensorflow.placeholder", "tensorflow.get_variable", "tensorflow.tile", "ge_policies.MlpPolicy", "e_maml_tf.algos.ppo2.Inputs", "tensorflow.shape", "tensorflow.trainable_variables", "callable", "add_loss", "e_maml_tf.algos.ppo2.PPO", "e_maml_tf.algos.vpg.Optimize", "e_maml_tf.algos.vpg.Inputs", "tensorflow.zeros_initializer", "tensorflow.concat", "e_maml_tf.algos.vpg.VPG", "tensorflow.placeholder", "e_maml_tf.algos.cpi.Inputs", "e_maml_tf.algos.cpi.CPI", "tensorflow.placeholder", "e_maml_tf.algos.bc.Inputs", "e_maml_tf.algos.bc.BC", "e_maml_tf.algos.bc_learned_loss.Inputs", "NotImplementedError", "e_maml_tf.algos.bc_learned_loss.BCLearnedLoss", "tensorflow.placeholder", "NotImplementedError", "len", "e_maml_tf.ge_utils.stem"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.stem"], ["def", "__init__", "(", "self", ",", "*", ",", "scope_name", ",", "act_space", ",", "ob_shape", ",", "algo", ",", "reuse", ":", "Any", "=", "False", ",", "trainables", "=", "None", ",", "optimizer", "=", "None", ",", "\n", "add_loss", "=", "None", ",", "loss_only", "=", "False", ",", "lr_rank", "=", "None", ",", "max_grad_norm", "=", "None", ",", "max_grad_clip", "=", "None", ",", "\n", "fix_variance", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Meta Graph Constructor\n\n        :param scope_name:\n        :param act_space:\n        :param ob_shape:\n        :param algo:\n        :param reuse:\n        :param trainables:\n        :param optimizer:\n        :param lr_rank: One of [None, 0, 1, 2] corresponding to [(), 'scalar', 'simple', \"full\"] learned learning rate.\n        :param max_grad_norm:\n        :param max_grad_clip:\n        :param fix_variance:\n        \"\"\"", "\n", "assert", "algo", "in", "ALLOWED_ALGS", ",", "\"model algorithm need to be one of {}\"", ".", "format", "(", "ALLOWED_ALGS", ")", "\n", "with", "tf", ".", "variable_scope", "(", "scope_name", ",", "reuse", "=", "reuse", ")", ":", "\n", "            ", "obs", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "ob_shape", ",", "name", "=", "'obs'", ")", "# obs", "\n", "if", "algo", "==", "\"PPO\"", ":", "\n", "                ", "self", ".", "inputs", "=", "inputs", "=", "PPOInputs", "(", "action_space", "=", "act_space", ",", "value_baseline", "=", "(", "G", ".", "baseline", "==", "\"critic\"", ")", ")", "\n", "Optimize", "=", "PPO_Optimize", "\n", "", "elif", "algo", "==", "\"VPG\"", ":", "\n", "                ", "self", ".", "inputs", "=", "inputs", "=", "VPGInputs", "(", "action_space", "=", "act_space", ",", "value_baseline", "=", "(", "G", ".", "baseline", "==", "\"critic\"", ")", ")", "\n", "Optimize", "=", "VPG_Optimize", "\n", "", "elif", "algo", "==", "\"CPI\"", ":", "\n", "                ", "self", ".", "inputs", "=", "inputs", "=", "CPIInputs", "(", "action_space", "=", "act_space", ",", "value_baseline", "=", "(", "G", ".", "baseline", "==", "\"critic\"", ")", ")", "\n", "Optimize", "=", "CPI_Optimize", "\n", "", "elif", "algo", "==", "\"BC\"", ":", "\n", "                ", "self", ".", "inputs", "=", "inputs", "=", "BCInputs", "(", "action_space", "=", "act_space", ")", "\n", "Optimize", "=", "BC_Optimize", "\n", "", "elif", "algo", "==", "\"BCLearnedLoss\"", ":", "\n", "                ", "self", ".", "inputs", "=", "inputs", "=", "BCLearnedLossInputs", "(", "action_space", "=", "act_space", ",", "type", "=", "G", ".", "learned_loss_type", ")", "\n", "Optimize", "=", "BC_Optimize", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "\n", "'Only supports PPO, VPG, CPI, BC and BC with Learned Loss (BCLearnedLoss)'", ")", "\n", "", "inputs", ".", "X", "=", "obs", "# https://github.com/tianheyu927/mil/blob/master/mil.py#L218", "\n", "bias_transformation", "=", "tf", ".", "get_variable", "(", "'input_bias'", ",", "[", "1", ",", "G", ".", "bias_dim", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "batch_n", "=", "tf", ".", "shape", "(", "obs", ")", "[", "0", "]", "\n", "trans_input", "=", "tf", ".", "tile", "(", "bias_transformation", ",", "[", "batch_n", ",", "1", "]", ")", "\n", "self", ".", "policy", "=", "policy", "=", "MlpPolicy", "(", "\n", "ac_space", "=", "act_space", ",", "hidden_size", "=", "G", ".", "hidden_size", ",", "n_layers", "=", "G", ".", "n_layers", ",", "\n", "activation", "=", "G", ".", "activation", ",", "value_baseline", "=", "(", "G", ".", "baseline", "==", "\"critic\"", ")", ",", "\n", "reuse", "=", "reuse", ",", "X", "=", "tf", ".", "concat", "(", "values", "=", "(", "obs", ",", "trans_input", ")", ",", "axis", "=", "1", ")", ",", "X_placeholder", "=", "obs", ",", "\n", "init_logstd", "=", "G", ".", "init_logstd", ",", "fix_variance", "=", "fix_variance", ")", "\n", "\n", "# note that policy.trainables are the original trainable parameters, not the mocked variables.", "\n", "# todo: concatenate policy.trainable with local trainable (bias_transformation)", "\n", "self", ".", "trainables", "=", "tf", ".", "trainable_variables", "(", ")", "if", "trainables", "is", "None", "else", "trainables", "\n", "\n", "ext_loss", "=", "add_loss", "(", "inputs", ".", "ADV", ")", "if", "callable", "(", "add_loss", ")", "else", "None", "\n", "if", "algo", "==", "\"PPO\"", ":", "\n", "                ", "self", ".", "model", "=", "PPO", "(", "inputs", "=", "inputs", ",", "policy", "=", "policy", ",", "vf_coef", "=", "G", ".", "vf_coef", ",", "ent_coef", "=", "G", ".", "ent_coef", ")", "\n", "", "elif", "algo", "==", "\"VPG\"", ":", "\n", "                ", "self", ".", "model", "=", "VPG", "(", "inputs", "=", "inputs", ",", "policy", "=", "policy", ",", "vf_coef", "=", "G", ".", "vf_coef", ")", "\n", "", "elif", "algo", "==", "\"CPI\"", ":", "\n", "                ", "self", ".", "model", "=", "CPI", "(", "inputs", "=", "inputs", ",", "policy", "=", "policy", ",", "vf_coef", "=", "G", ".", "vf_coef", ",", "ent_coef", "=", "G", ".", "ent_coef", ")", "\n", "", "elif", "algo", "==", "\"BC\"", ":", "\n", "                ", "self", ".", "model", "=", "BC", "(", "inputs", "=", "inputs", ",", "policy", "=", "policy", ")", "\n", "", "elif", "algo", "==", "\"BCLearnedLoss\"", ":", "\n", "                ", "self", ".", "model", "=", "BCLearnedLoss", "(", "inputs", "=", "inputs", ",", "policy", "=", "policy", ",", "type", "=", "G", ".", "learned_loss_type", ")", "\n", "\n", "", "self", ".", "loss", "=", "self", ".", "model", ".", "loss", "if", "ext_loss", "is", "None", "else", "(", "self", ".", "model", ".", "loss", "+", "ext_loss", ")", "\n", "\n", "if", "not", "loss_only", ":", "\n", "                ", "if", "lr_rank", "==", "0", ":", "\n", "                    ", "inputs", ".", "LR", "=", "lr", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "[", "]", ",", "name", "=", "\"LR\"", ")", "\n", "", "elif", "lr_rank", "==", "1", ":", "\n", "                    ", "inputs", ".", "LR", "=", "lr", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", "len", "(", "self", ".", "trainables", ")", ",", ")", ",", "name", "=", "\"LR\"", ")", "\n", "", "elif", "lr_rank", "==", "2", ":", "\n", "                    ", "inputs", ".", "LR", "=", "lr", "=", "[", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "t", ".", "shape", ",", "name", "=", "f\"LR_{stem(t, 2)}\"", ")", "\n", "for", "t", "in", "self", ".", "trainables", "]", "\n", "", "elif", "lr_rank", "is", "None", ":", "\n", "                    ", "lr", "=", "None", "\n", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "(", "f\"lr_rank = {lr_rank} is not supported. Check for programming error.\"", ")", "\n", "", "self", ".", "optim", "=", "Optimize", "(", "lr", "=", "lr", ",", "loss", "=", "self", ".", "loss", ",", "reports", "=", "self", ".", "model", ".", "reports", ",", "\n", "trainables", "=", "self", ".", "trainables", ",", "max_grad_norm", "=", "max_grad_norm", ",", "\n", "max_grad_clip", "=", "max_grad_clip", ",", "optimizer", "=", "optimizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.e_maml_ge.SingleTask.__init__": [[126, 184], ["ge_utils.defaultlist", "ge_utils.defaultlist", "ge_utils.defaultlist", "meta_trainable_map.copy", "params[].update", "range", "type", "ge_utils.make_with_custom_variables", "ge_utils.make_with_custom_variables", "tensorflow.variable_scope", "meta_trainable_map.copy", "e_maml_ge.Meta", "params[].update", "params[].update", "e_maml_ge.cmaml_loss", "e_maml_ge.Meta", "e_maml_tf.ge_utils.get_scope_name", "isinstance", "len", "hasattr", "len", "len", "e_maml_tf.ge_utils.get_scope_name", "list", "ge_utils.make_with_custom_variables.optim.apply_grad", "ge_utils.make_with_custom_variables.optim.apply_grad", "params[].values", "zip", "zip", "tensorflow.stop_gradient", "params[].items", "params[].items"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.make_with_custom_variables", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.make_with_custom_variables", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.e_maml_ge.cmaml_loss", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.get_scope_name", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.get_scope_name"], ["    ", "def", "__init__", "(", "self", ",", "act_space", ",", "ob_shape", ",", "trainable_map", ",", "meta_trainable_map", "=", "None", ",", "lr", "=", "None", ")", ":", "\n", "# no need to go beyond despite of large G.eval_grad_steps, b/c RL samples using runner policy.", "\n", "\n", "        ", "if", "meta_trainable_map", "is", "None", ":", "\n", "            ", "meta_trainable_map", "=", "trainable_map", "\n", "\n", "", "self", ".", "workers", "=", "defaultlist", "(", "None", ")", "\n", "self", ".", "metas", "=", "defaultlist", "(", "None", ")", "\n", "\n", "params", "=", "defaultlist", "(", "None", ")", "\n", "params", "[", "0", "]", "=", "meta_trainable_map", ".", "copy", "(", ")", "\n", "params", "[", "0", "]", ".", "update", "(", "trainable_map", ")", "\n", "\n", "import", "gym", "\n", "assert", "type", "(", "act_space", ")", "is", "gym", ".", "spaces", ".", "Box", "\n", "act_dim", ",", "*", "_", "=", "act_space", ".", "shape", "\n", "\n", "for", "k", "in", "range", "(", "G", ".", "n_grad_steps", "+", "1", ")", ":", "\n", "            ", "if", "k", "<", "G", ".", "n_grad_steps", ":", "# 0 - 9,", "\n", "\n", "                ", "self", ".", "workers", "[", "k", "]", "=", "worker", "=", "make_with_custom_variables", "(", "\n", "lambda", ":", "Meta", "(", "scope_name", "=", "f'inner_{k}_grad_network'", ",", "\n", "act_space", "=", "act_space", ",", "ob_shape", "=", "ob_shape", ",", "algo", "=", "G", ".", "inner_alg", ",", "\n", "# do NOT pass in learning rate to inhibit the Meta.optimize operator.", "\n", "optimizer", "=", "G", ".", "inner_optimizer", ",", "reuse", "=", "True", ",", "trainables", "=", "list", "(", "params", "[", "k", "]", ".", "values", "(", ")", ")", ",", "\n", "max_grad_norm", "=", "G", ".", "inner_max_grad_norm", ",", "max_grad_clip", "=", "G", ".", "inner_max_grad_clip", ",", "\n", "fix_variance", "=", "True", "\n", ")", ",", "# pass in the trainable_map for proper gradient", "\n", "params", "[", "k", "]", ",", "f'{get_scope_name()}/inner_{k}_grad_network/'", "\n", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "f'SGD_grad_{k}'", ")", ":", "\n", "                    ", "if", "(", "isinstance", "(", "lr", ",", "Sequence", ")", "and", "len", "(", "lr", ")", ")", "or", "(", "hasattr", "(", "lr", ",", "'shape'", ")", "and", "len", "(", "lr", ".", "shape", ")", ")", ":", "\n", "                        ", "learn_rates", "=", "lr", "[", "k", "]", "\n", "", "else", ":", "\n", "                        ", "worker", ".", "inputs", ".", "LR", "=", "lr", "# this is important because this is needed by the feed_dict", "\n", "learn_rates", "=", "[", "lr", "]", "*", "len", "(", "worker", ".", "optim", ".", "grads", ")", "\n", "", "params", "[", "k", "+", "1", "]", "=", "meta_trainable_map", ".", "copy", "(", ")", "\n", "if", "G", ".", "first_order", ":", "\n", "                        ", "params", "[", "k", "+", "1", "]", ".", "update", "(", "{", "k", ":", "worker", ".", "optim", ".", "apply_grad", "(", "lr", "=", "lr", ",", "grad", "=", "tf", ".", "stop_gradient", "(", "g", ")", ",", "var", "=", "v", ")", "\n", "for", "g", ",", "lr", ",", "(", "k", ",", "v", ")", "in", "\n", "zip", "(", "learn_rates", ",", "worker", ".", "optim", ".", "grads", ",", "params", "[", "k", "]", ".", "items", "(", ")", ")", "}", ")", "\n", "", "else", ":", "\n", "                        ", "params", "[", "k", "+", "1", "]", ".", "update", "(", "{", "k", ":", "worker", ".", "optim", ".", "apply_grad", "(", "lr", "=", "lr", ",", "grad", "=", "g", ",", "var", "=", "v", ")", "\n", "for", "g", ",", "lr", ",", "(", "k", ",", "v", ")", "in", "\n", "zip", "(", "learn_rates", ",", "worker", ".", "optim", ".", "grads", ",", "params", "[", "k", "]", ".", "items", "(", ")", ")", "}", ")", "\n", "\n", "", "", "", "if", "k", "==", "G", ".", "n_grad_steps", ":", "# 10 or 1.", "\n", "                ", "add_loss", "=", "None", "if", "G", ".", "run_mode", "!=", "'e-maml'", "else", "lambda", "ADV", ":", "cmaml_loss", "(", "[", "w", ".", "model", ".", "neglogpac", "for", "w", "in", "self", ".", "workers", "]", ",", "ADV", ")", "\n", "self", ".", "meta", "=", "make_with_custom_variables", "(", "\n", "lambda", ":", "Meta", "(", "scope_name", "=", "\"meta_network\"", ",", "act_space", "=", "act_space", ",", "ob_shape", "=", "ob_shape", ",", "\n", "algo", "=", "G", ".", "meta_alg", ",", "reuse", "=", "True", ",", "add_loss", "=", "add_loss", ",", "loss_only", "=", "True", ",", ")", "\n", ",", "params", "[", "k", "]", ",", "f'{get_scope_name()}/meta_network/'", "\n", ")", "\n", "\n", "# Expose as non-public API for debugging purposes", "\n", "", "", "self", ".", "_params", "=", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.e_maml_ge.E_MAML.__init__": [[205, 315], ["logger.upload_file", "e_maml_ge.Meta", "ge_utils.var_map", "e_maml_ge.Meta", "ge_utils.var_map", "tensorflow.placeholder", "print", "tqdm.trange", "tensorflow.trainable_variables", "type", "isinstance", "range", "tensorflow.variable_scope", "ge_utils.Cache", "baselines.function", "baselines.function", "tensorflow.variable_scope", "tensorflow.gradients", "ge_utils.defaultlist", "ge_utils.defaultlist", "range", "e_maml_ge.E_MAML.graphs[].meta.model.reports.keys", "tensorflow.variable_scope", "e_maml_ge.SingleTask", "e_maml_ge.E_MAML.graphs.append", "tensorflow.reduce_mean", "ge_utils.GradientSum", "Optim", "e_maml_ge.E_MAML.meta_optimizers[].apply_gradients", "e_maml_ge.E_MAML.graphs[].meta.model.reports.values", "tensorflow.variable_scope", "e_maml_ge.E_MAML.alpha.append", "zip", "tensorflow.reduce_mean", "dict", "NotImplemented", "zip", "tensorflow.get_variable", "tensorflow.stop_gradient", "tensorflow.constant_initializer", "tensorflow.maximum", "graph.meta.model.reports.values", "e_maml_tf.ge_utils.stem", "tensorflow.norm"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.var_map", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.var_map", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.stem"], ["def", "__init__", "(", "self", ",", "ob_space", ",", "act_space", ")", ":", "\n", "        ", "\"\"\"\n        Usage:\n            self.env = env\n            ob_shape = (None,) + self.env.observation_space.shape\n        \"\"\"", "\n", "from", "ml_logger", "import", "logger", "\n", "logger", ".", "upload_file", "(", "__file__", ")", "\n", "\n", "ob_shape", "=", "(", "None", ",", ")", "+", "ob_space", ".", "shape", "\n", "\n", "import", "gym", "\n", "assert", "type", "(", "act_space", ")", "is", "gym", ".", "spaces", ".", "Box", "\n", "act_dim", ",", "*", "_", "=", "act_space", ".", "shape", "\n", "\n", "if", "G", ".", "meta_sgd", "==", "'full'", ":", "\n", "            ", "lr_rank", "=", "2", "\n", "", "elif", "G", ".", "meta_sgd", ":", "\n", "            ", "lr_rank", "=", "1", "\n", "", "else", ":", "\n", "            ", "lr_rank", "=", "0", "\n", "# Meta holds policy, inner optimizer. Also creates an input.LR placeholder.", "\n", "", "self", ".", "runner", "=", "Meta", "(", "scope_name", "=", "'runner'", ",", "act_space", "=", "act_space", ",", "ob_shape", "=", "ob_shape", ",", "algo", "=", "G", ".", "inner_alg", ",", "\n", "lr_rank", "=", "lr_rank", ",", "optimizer", "=", "G", ".", "inner_optimizer", ",", "max_grad_norm", "=", "G", ".", "inner_max_grad_norm", ",", "\n", "max_grad_clip", "=", "G", ".", "inner_max_grad_clip", ",", "fix_variance", "=", "G", ".", "control_variance", ")", "\n", "\n", "trainables", "=", "self", ".", "runner", ".", "trainables", "\n", "runner_var_map", "=", "var_map", "(", "trainables", ",", "'runner/'", ")", "\n", "# note: the point of AUTO_REUSE is:", "\n", "# note:            if reuse=True, gives error when no prior is available. Otherwise always creates new.", "\n", "# note:            This yaw, only creates new when old is not available.", "\n", "self", ".", "meta_runner", "=", "Meta", "(", "scope_name", "=", "\"runner\"", ",", "act_space", "=", "act_space", ",", "ob_shape", "=", "ob_shape", ",", "algo", "=", "G", ".", "meta_alg", ",", "\n", "reuse", "=", "tf", ".", "AUTO_REUSE", ",", "loss_only", "=", "True", ",", "fix_variance", "=", "G", ".", "fix_meta_variance", ")", "\n", "meta_trainables", "=", "self", ".", "meta_runner", ".", "trainables", "\n", "meta_runner_var_map", "=", "var_map", "(", "meta_trainables", ",", "'runner/'", ")", "\n", "# meta_trainables = assert_match(trainables, meta_trainables)", "\n", "\n", "self", ".", "beta", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "]", ",", "name", "=", "\"beta\"", ")", "\n", "\n", "print", "(", "\">>>>>>>>>>> Constructing Meta Graph <<<<<<<<<<<\"", ")", "\n", "# todo: we can do multi-GPU placement of the graph here.", "\n", "self", ".", "graphs", "=", "[", "]", "\n", "assert", "G", ".", "n_graphs", "==", "1", "or", "G", ".", "n_graphs", "==", "G", ".", "n_tasks", ",", "\"graph number is 1 or equal to the number of tasks\"", "\n", "\n", "if", "G", ".", "meta_sgd", ":", "\n", "            ", "assert", "isinstance", "(", "G", ".", "alpha", ",", "Number", ")", ",", "\"alpha need to be a scalar.\"", "\n", "self", ".", "alpha", "=", "[", "]", "# has to be per-layer per-block. Bias and weights require different scales.", "\n", "for", "k", "in", "range", "(", "G", ".", "n_grad_steps", ")", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "f'learned_alpha_{k}'", ")", ":", "\n", "                    ", "self", ".", "alpha", ".", "append", "(", "[", "\n", "tf", ".", "get_variable", "(", "f'alpha_{stem(t.name, 2)}'", ",", "shape", "=", "t", ".", "shape", "if", "G", ".", "meta_sgd", "==", "\"full\"", "else", "(", ")", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "G", ".", "alpha", ")", ")", "\n", "for", "t", "in", "trainables", "\n", "]", ")", "\n", "", "", "", "else", ":", "\n", "            ", "self", ".", "alpha", "=", "self", ".", "runner", ".", "inputs", ".", "LR", "\n", "", "for", "t", "in", "trange", "(", "G", ".", "n_graphs", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "f\"graph_{t}\"", ")", ":", "\n", "# note: should use different learning rate for each gradient step", "\n", "                ", "task_graph", "=", "SingleTask", "(", "act_space", "=", "act_space", ",", "ob_shape", "=", "ob_shape", ",", "trainable_map", "=", "runner_var_map", ",", "\n", "meta_trainable_map", "=", "meta_runner_var_map", ",", "lr", "=", "self", ".", "alpha", ")", "\n", "self", ".", "graphs", ".", "append", "(", "task_graph", ")", "\n", "\n", "", "", "all_trainables", "=", "tf", ".", "trainable_variables", "(", ")", "# might be controlled variables in the meta loop", "\n", "\n", "# Only do this after the meta graph has finished using policy.trainables", "\n", "# Note: stateful operators for saving to a cache and loading from it. Only used to reset runner", "\n", "# Note: Slots are not supported. Only weights.", "\n", "# fixit: all_variables might not be needed. Only that of the runner need to be cached.", "\n", "with", "tf", ".", "variable_scope", "(", "\"weight_cache\"", ")", ":", "\n", "            ", "self", ".", "cache", "=", "Cache", "(", "all_trainables", ")", "\n", "self", ".", "save_weight_cache", "=", "U", ".", "function", "(", "[", "]", ",", "[", "self", ".", "cache", ".", "save", "]", ")", "\n", "self", ".", "load_weight_cache", "=", "U", ".", "function", "(", "[", "]", ",", "[", "self", ".", "cache", ".", "load", "]", ")", "\n", "\n", "# Now construct the meta optimizers", "\n", "", "with", "tf", ".", "variable_scope", "(", "'meta_optimizer'", ")", ":", "\n", "# call gradient_sum.set_op first, then add_op. Call k times in-total.", "\n", "            ", "self", ".", "meta_grads", "=", "tf", ".", "gradients", "(", "tf", ".", "reduce_mean", "(", "[", "task_graph", ".", "meta", ".", "loss", "for", "task_graph", "in", "self", ".", "graphs", "]", ")", ",", "\n", "all_trainables", ")", "\n", "if", "G", ".", "n_graphs", "==", "1", ":", "\n", "                ", "self", ".", "gradient_sum", "=", "GradientSum", "(", "all_trainables", ",", "self", ".", "meta_grads", ")", "\n", "grads", "=", "[", "c", "/", "G", ".", "n_tasks", "for", "c", "in", "self", ".", "gradient_sum", ".", "cache", "]", "\n", "", "else", ":", "\n", "                ", "grads", "=", "self", ".", "meta_grads", "\n", "\n", "", "if", "G", ".", "meta_max_grad_norm", ":", "# allow 0 to be by-pass", "\n", "                ", "grads", "=", "[", "None", "if", "g", "is", "None", "else", "\n", "g", "*", "tf", ".", "stop_gradient", "(", "G", ".", "meta_max_grad_norm", "/", "tf", ".", "maximum", "(", "G", ".", "meta_max_grad_norm", ",", "tf", ".", "norm", "(", "g", ")", ")", ")", "\n", "for", "g", "in", "grads", "]", "\n", "\n", "# do NOT apply gradient norm here.", "\n", "", "if", "G", ".", "meta_optimizer", "==", "\"Adam\"", ":", "\n", "                ", "Optim", ",", "kwargs", "=", "tf", ".", "train", ".", "AdamOptimizer", ",", "{", "}", "\n", "", "elif", "G", ".", "meta_optimizer", "==", "\"AdamW\"", ":", "\n", "                ", "Optim", ",", "kwargs", "=", "tf", ".", "contrib", ".", "opt", ".", "AdamWOptimizer", ",", "dict", "(", "weight_decay", "=", "0.0001", ")", "\n", "", "elif", "G", ".", "meta_optimizer", "==", "\"SGD\"", ":", "\n", "                ", "Optim", ",", "kwargs", "=", "tf", ".", "train", ".", "GradientDescentOptimizer", ",", "{", "}", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplemented", "(", "f\"{G.meta_optimizer} as a meta optimizer is not implemented.\"", ")", "\n", "\n", "# Uses a different optimizer (with slots) for each step in the meta update.", "\n", "", "self", ".", "meta_update_ops", "=", "defaultlist", "(", "None", ")", "\n", "self", ".", "meta_optimizers", "=", "defaultlist", "(", "None", ")", "\n", "for", "i", "in", "range", "(", "1", "if", "G", ".", "reuse_meta_optimizer", "else", "G", ".", "meta_n_grad_steps", ")", ":", "\n", "                ", "self", ".", "meta_optimizers", "[", "i", "]", "=", "Optim", "(", "learning_rate", "=", "self", ".", "beta", ",", "**", "kwargs", ")", "\n", "self", ".", "meta_update_ops", "[", "i", "]", "=", "self", ".", "meta_optimizers", "[", "i", "]", ".", "apply_gradients", "(", "zip", "(", "grads", ",", "all_trainables", ")", ")", "\n", "\n", "", "self", ".", "meta_reporting_keys", "=", "self", ".", "graphs", "[", "0", "]", ".", "meta", ".", "model", ".", "reports", ".", "keys", "(", ")", "\n", "self", ".", "meta_reporting", "=", "self", ".", "graphs", "[", "0", "]", ".", "meta", ".", "model", ".", "reports", ".", "values", "(", ")", "if", "G", ".", "n_graphs", "==", "1", "else", "[", "tf", ".", "reduce_mean", "(", "_", ")", "for", "_", "in", "zip", "(", "*", "[", "graph", ".", "meta", ".", "model", ".", "reports", ".", "values", "(", ")", "for", "graph", "in", "self", ".", "graphs", "]", ")", "]", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.e_maml_ge._mean": [[114, 116], ["tensorflow.reduce_mean"], "function", ["None"], ["", "", "", "", "def", "_mean", "(", "x", ",", "axis", "=", "None", ",", "keepdims", "=", "False", ")", ":", "\n", "    ", "return", "tf", ".", "reduce_mean", "(", "x", ",", "reduction_indices", "=", "None", "if", "axis", "is", "None", "else", "[", "axis", "]", ",", "keep_dims", "=", "keepdims", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.e_maml_ge.cmaml_loss": [[118, 123], ["e_maml_ge._mean", "e_maml_ge._mean"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.e_maml_ge._mean", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.e_maml_ge._mean"], ["", "def", "cmaml_loss", "(", "neglogpacs", ",", "advantage", ")", ":", "\n", "    ", "mean_adv", "=", "_mean", "(", "advantage", ")", "\n", "# we attribute adv to all workers in the style of DICE", "\n", "exploration_term", "=", "_mean", "(", "neglogpacs", ")", "*", "mean_adv", "\n", "return", "exploration_term", "*", "G", ".", "e_maml_lambda", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.e_maml_ge.assert_match": [[186, 191], ["enumerate", "len", "zip", "len", "len"], "function", ["None"], ["", "", "def", "assert_match", "(", "l1", ",", "l2", ")", ":", "\n", "    ", "assert", "len", "(", "l1", ")", ">", "0", "\n", "for", "i", ",", "(", "a", ",", "b", ")", "in", "enumerate", "(", "zip", "(", "l1", ",", "l2", ")", ")", ":", "\n", "        ", "assert", "a", "==", "b", ",", "\"existing items has to be the same.\"", "\n", "", "return", "l1", "[", "i", "+", "1", ":", "]", "if", "len", "(", "l1", ")", ">", "len", "(", "l2", ")", "else", "l2", "[", "i", "+", "1", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.trainer.train_supervised_maml": [[16, 184], ["tensorflow.get_default_session", "ml_logger.logger.flush", "ml_logger.logger.split", "ml_logger.logger.log", "collections.defaultdict", "maml.save_weight_cache", "range", "tensorflow.get_default_session().run", "ml_logger.logger.split", "ml_logger.logger.log_line", "ml_logger.logger.log", "collections.defaultdict.items", "tf.get_default_session.run", "ml_logger.logger.log", "ml_logger.logger.log", "isinstance", "config.G.beta.send", "numpy.array", "range", "tensorflow.get_default_session().run", "range", "feed_dict.clear", "numpy.array().mean", "ml_logger.logger.log_key_value", "isinstance", "config.G.alpha.send", "numpy.array", "ml_logger.metrify", "tensorflow.get_default_session().run", "zip", "tensorflow.get_default_session", "ml_logger.metrify", "dict", "trainer.path_to_feed_dict", "maml.runner.optim.run_optimize", "path_to_feed_dict.clear", "zip", "feed_dict.update", "tensorflow.get_default_session", "ml_logger.logger.log_key_value", "numpy.array", "dict", "RuntimeError", "e_maml_tf.algos.bc.sample_demonstration_data", "e_maml_tf.algos.bc.sample_demonstration_data", "Exception", "bc.sample_demonstration_data.items", "maml.runner.model.reports.keys", "batch_data[].append", "ml_logger.logger.log_key_value", "ml_logger.logger.log_line", "RuntimeError", "trainer.path_to_feed_dict", "dict", "numpy.fromiter().any", "feed_dict.update", "tensorflow.get_default_session", "enumerate", "zip", "numpy.random.randint", "termcolor.colored", "trainer.path_to_feed_dict", "tensorflow.get_default_session().run", "feed_dict.clear", "tensorflow.get_default_session().run", "e_maml_tf.ge_utils.stem", "numpy.fromiter", "tensorflow.get_default_session().run", "tensorflow.get_default_session().run", "dict.values", "tensorflow.get_default_session", "tensorflow.get_default_session", "tensorflow.get_default_session", "tensorflow.get_default_session"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.path_to_feed_dict", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc.sample_demonstration_data", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc.sample_demonstration_data", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.path_to_feed_dict", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.path_to_feed_dict", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.stem"], ["def", "train_supervised_maml", "(", "*", ",", "k_tasks", "=", "1", ",", "maml", ":", "E_MAML", ")", ":", "\n", "# env used for evaluation purposes only.", "\n", "    ", "if", "G", ".", "meta_sgd", ":", "\n", "        ", "assert", "maml", ".", "alpha", "is", "not", "None", ",", "\"Coding Mistake if meta_sgd is trueful but maml.alpha is None.\"", "\n", "\n", "", "assert", "G", ".", "n_tasks", ">=", "k_tasks", ",", "f\"Is this intended? You probably want to have \"", "f\"meta-batch({G.n_tasks}) >= k_tasks({k_tasks}).\"", "\n", "\n", "sess", "=", "tf", ".", "get_default_session", "(", ")", "\n", "\n", "epoch_ind", ",", "pref", "=", "-", "1", ",", "\"\"", "\n", "while", "epoch_ind", "<", "G", ".", "n_epochs", ":", "\n", "# for epoch_ind in range(G.n_epochs + 1):", "\n", "        ", "logger", ".", "flush", "(", ")", "\n", "logger", ".", "split", "(", ")", "\n", "\n", "is_bc_test", "=", "(", "pref", "!=", "\"test/\"", "and", "G", ".", "eval_interval", "and", "epoch_ind", "%", "G", ".", "eval_interval", "==", "0", ")", "\n", "pref", "=", "\"test/\"", "if", "is_bc_test", "else", "\"\"", "\n", "epoch_ind", "+=", "0", "if", "is_bc_test", "else", "1", "\n", "\n", "if", "G", ".", "meta_sgd", ":", "\n", "            ", "alpha_lr", "=", "sess", ".", "run", "(", "maml", ".", "alpha", ")", "# only used in the runner.", "\n", "logger", ".", "log", "(", "metrics", "=", "{", "f\"alpha_{i}/{stem(t.name, 2)}\"", ":", "a", "\n", "for", "i", ",", "a_", "in", "enumerate", "(", "alpha_lr", ")", "\n", "for", "t", ",", "a", "in", "zip", "(", "maml", ".", "runner", ".", "trainables", ",", "a_", ")", "}", ",", "silent", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "alpha_lr", "=", "G", ".", "alpha", ".", "send", "(", "epoch_ind", ")", "if", "isinstance", "(", "G", ".", "alpha", ",", "Schedule", ")", "else", "np", ".", "array", "(", "G", ".", "alpha", ")", "\n", "logger", ".", "log", "(", "alpha", "=", "metrify", "(", "alpha_lr", ")", ",", "epoch", "=", "epoch_ind", ",", "silent", "=", "True", ")", "\n", "\n", "", "beta_lr", "=", "G", ".", "beta", ".", "send", "(", "epoch_ind", ")", "if", "isinstance", "(", "G", ".", "beta", ",", "Schedule", ")", "else", "np", ".", "array", "(", "G", ".", "beta", ")", "\n", "logger", ".", "log", "(", "beta", "=", "metrify", "(", "beta_lr", ")", ",", "epoch", "=", "epoch_ind", ",", "silent", "=", "True", ")", "\n", "\n", "if", "G", ".", "checkpoint_interval", "and", "epoch_ind", "%", "G", ".", "checkpoint_interval", "==", "0", ":", "\n", "            ", "yield", "\"pre-update-checkpoint\"", ",", "epoch_ind", "\n", "\n", "# Compute updates for each task in the batch", "\n", "# 0. save value of variables", "\n", "# 1. sample", "\n", "# 2. gradient descent", "\n", "# 3. repeat step 1., 2. until all gradient steps are exhausted.", "\n", "", "batch_data", "=", "defaultdict", "(", "list", ")", "\n", "\n", "maml", ".", "save_weight_cache", "(", ")", "\n", "load_ops", "=", "[", "]", "if", "DEBUG", ".", "no_weight_reset", "else", "[", "maml", ".", "cache", ".", "load", "]", "\n", "\n", "feed_dict", "=", "{", "}", "\n", "for", "task_ind", "in", "range", "(", "k_tasks", "if", "is_bc_test", "else", "G", ".", "n_tasks", ")", ":", "\n", "            ", "graph_branch", "=", "maml", ".", "graphs", "[", "0", "]", "if", "G", ".", "n_graphs", "==", "1", "else", "maml", ".", "graphs", "[", "task_ind", "]", "\n", "if", "G", ".", "n_graphs", "==", "1", ":", "\n", "                ", "gradient_sum_op", "=", "maml", ".", "gradient_sum", ".", "set_op", "if", "task_ind", "==", "0", "else", "maml", ".", "gradient_sum", ".", "add_op", "\n", "\n", "", "\"\"\"\n            In BC mode, we don't have an environment. The sampling is handled here then fed to the sampler.\n            > task_spec = dict(index=0)\n            \n            Here we make the testing more efficient.\n            \"\"\"", "\n", "if", "not", "DEBUG", ".", "no_task_resample", ":", "\n", "                ", "if", "not", "is_bc_test", ":", "\n", "                    ", "task_spec", "=", "dict", "(", "index", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "k_tasks", ")", ")", "\n", "", "elif", "task_ind", "<", "k_tasks", ":", "\n", "                    ", "task_spec", "=", "dict", "(", "index", "=", "task_ind", "%", "k_tasks", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'should never hit here.'", ")", "\n", "\n", "", "", "for", "k", "in", "range", "(", "G", ".", "n_grad_steps", "+", "1", ")", ":", "# 0 - 10 <== last one being the maml policy.", "\n", "\n", "# for imitation inner loss, we still sample trajectory for evaluation purposes, but", "\n", "# replace it with the demonstration data for learning", "\n", "                ", "if", "k", "<", "G", ".", "n_grad_steps", ":", "\n", "                    ", "p", "=", "p", "if", "G", ".", "single_sampling", "and", "k", ">", "0", "else", "bc", ".", "sample_demonstration_data", "(", "task_spec", ",", "key", "=", "(", "\"eval\"", "if", "is_bc_test", "else", "None", ")", ")", "\n", "", "elif", "k", "==", "G", ".", "n_grad_steps", ":", "\n", "# note: use meta bc samples.", "\n", "                    ", "p", "=", "bc", ".", "sample_demonstration_data", "(", "task_spec", ",", "key", "=", "\"meta\"", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "'Implementation error. Should never reach this line.'", ")", "\n", "\n", "", "_p", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "p", ".", "items", "(", ")", "if", "k", "!=", "\"ep_info\"", "}", "\n", "\n", "if", "k", "<", "G", ".", "n_grad_steps", ":", "\n", "# note: under meta-SGD mode, the runner needs the k^th learning rate.", "\n", "                    ", "_lr", "=", "alpha_lr", "[", "k", "]", "if", "G", ".", "meta_sgd", "else", "alpha_lr", "\n", "\n", "runner_feed_dict", "=", "path_to_feed_dict", "(", "inputs", "=", "maml", ".", "runner", ".", "inputs", ",", "paths", "=", "_p", ",", "lr", "=", "_lr", ")", "\n", "# todo: optimize `maml.meta_runner` if k >= G.n_grad_steps.", "\n", "loss", ",", "*", "_", ",", "__", "=", "maml", ".", "runner", ".", "optim", ".", "run_optimize", "(", "feed_dict", "=", "runner_feed_dict", ")", "\n", "runner_feed_dict", ".", "clear", "(", ")", "\n", "\n", "for", "key", ",", "value", "in", "zip", "(", "maml", ".", "runner", ".", "model", ".", "reports", ".", "keys", "(", ")", ",", "[", "loss", ",", "*", "_", "]", ")", ":", "\n", "                        ", "batch_data", "[", "pref", "+", "f\"grad_{k}_step_{key}\"", "]", ".", "append", "(", "value", ")", "\n", "logger", ".", "log_key_value", "(", "pref", "+", "f\"task_{task_ind}_grad_{k}_{key}\"", ",", "value", ",", "silent", "=", "True", ")", "\n", "\n", "", "if", "loss", ">", "G", ".", "term_loss_threshold", ":", "# todo: make this batch-based instead of on single episode", "\n", "                        ", "err", "=", "pref", "+", "\"episode loss blew up:\"", ",", "loss", ",", "\"terminating training.\"", "\n", "logger", ".", "log_line", "(", "colored", "(", "err", ",", "\"red\"", ")", ",", "flush", "=", "True", ")", "\n", "raise", "RuntimeError", "(", "'loss is TOO HIGH. Terminating the experiment.'", ")", "\n", "\n", "# fixit: has bug when using fixed learning rate. Still needs to get learning rate from placeholder", "\n", "", "feed_dict", ".", "update", "(", "path_to_feed_dict", "(", "inputs", "=", "graph_branch", ".", "workers", "[", "k", "]", ".", "inputs", ",", "paths", "=", "_p", ")", ")", "\n", "", "elif", "k", "==", "G", ".", "n_grad_steps", ":", "\n", "                    ", "yield_keys", "=", "dict", "(", "\n", "movie", "=", "G", ".", "record_movie_interval", "and", "epoch_ind", ">=", "G", ".", "start_movie_after_epoch", "and", "\n", "epoch_ind", "%", "G", ".", "record_movie_interval", "==", "0", ",", "\n", "eval", "=", "is_bc_test", "\n", ")", "\n", "if", "np", ".", "fromiter", "(", "yield_keys", ".", "values", "(", ")", ",", "bool", ")", ".", "any", "(", ")", ":", "\n", "                        ", "yield", "yield_keys", ",", "epoch_ind", ",", "task_spec", "\n", "", "if", "is_bc_test", ":", "\n", "                        ", "if", "load_ops", ":", "\n", "                            ", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "load_ops", ")", "\n", "", "continue", "# do NOT meta learn from test samples.", "\n", "\n", "# we don't treat the meta_input the same way even though we could. This is more clear to read.", "\n", "# note: feed in the learning rate only later.", "\n", "", "feed_dict", ".", "update", "(", "path_to_feed_dict", "(", "inputs", "=", "graph_branch", ".", "meta", ".", "inputs", ",", "paths", "=", "_p", ")", ")", "\n", "\n", "if", "G", ".", "n_graphs", "==", "1", ":", "\n", "# load from checkpoint before computing the meta gradient\\nrun gradient sum operation", "\n", "                        ", "if", "load_ops", ":", "\n", "                            ", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "load_ops", ")", "\n", "# note: meta reporting should be run here. Not supported for simplicity. (need to reduce across", "\n", "# note: tasks, and can not be done outside individual task graphs.", "\n", "", "if", "G", ".", "meta_sgd", "is", "None", ":", "\n", "                            ", "feed_dict", "[", "maml", ".", "alpha", "]", "=", "alpha_lr", "\n", "", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "gradient_sum_op", ",", "feed_dict", ")", "\n", "feed_dict", ".", "clear", "(", ")", "\n", "\n", "", "if", "load_ops", ":", "\n", "                        ", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "load_ops", ")", "\n", "\n", "", "", "", "", "if", "is_bc_test", ":", "\n", "            ", "continue", "# do NOT meta learn from test samples.", "\n", "\n", "", "if", "G", ".", "meta_sgd", "is", "None", ":", "\n", "            ", "feed_dict", "[", "maml", ".", "alpha", "]", "=", "alpha_lr", "\n", "\n", "", "if", "G", ".", "n_graphs", "==", "1", ":", "\n", "            ", "assert", "G", ".", "meta_n_grad_steps", "==", "1", ",", "\"ERROR: Can only run 1 meta gradient step with a single graph.\"", "\n", "# note: remove meta reporting b/c meta report should be in each task in this case.", "\n", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "maml", ".", "meta_update_ops", "[", "0", "]", ",", "{", "maml", ".", "beta", ":", "beta_lr", "}", ")", "\n", "", "else", ":", "\n", "            ", "assert", "feed_dict", ",", "\"ERROR: It is likely that you jumped here from L:178.\"", "\n", "feed_dict", "[", "maml", ".", "beta", "]", "=", "beta_lr", "\n", "for", "i", "in", "range", "(", "G", ".", "meta_n_grad_steps", ")", ":", "\n", "                ", "update_op", "=", "maml", ".", "meta_update_ops", "[", "0", "if", "G", ".", "reuse_meta_optimizer", "else", "i", "]", "\n", "*", "reports", ",", "_", "=", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "maml", ".", "meta_reporting", "+", "[", "update_op", "]", ",", "feed_dict", ")", "\n", "if", "i", "not", "in", "(", "0", ",", "G", ".", "meta_n_grad_steps", "-", "1", ")", ":", "\n", "                    ", "continue", "\n", "", "for", "key", ",", "v", "in", "zip", "(", "maml", ".", "meta_reporting_keys", ",", "reports", ")", ":", "\n", "                    ", "logger", ".", "log_key_value", "(", "pref", "+", "f\"grad_{G.n_grad_steps + i}_step_{key}\"", ",", "v", ",", "silent", "=", "True", ")", "\n", "\n", "", "", "feed_dict", ".", "clear", "(", ")", "\n", "\n", "", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "maml", ".", "cache", ".", "save", ")", "\n", "\n", "# Now compute the meta gradients.", "\n", "# note: runner shares variables with the MAML graph. Reload from state_dict", "\n", "# note: if max_grad_step is the same as n_grad_steps then no need here.", "\n", "\n", "dt", "=", "logger", ".", "split", "(", ")", "\n", "logger", ".", "log_line", "(", "'Timer Starts...'", "if", "dt", "is", "None", "else", "f'{dt:0.2f} sec/epoch'", ")", "\n", "logger", ".", "log", "(", "dt_epoch", "=", "dt", "or", "np", ".", "nan", ",", "epoch", "=", "epoch_ind", ")", "\n", "\n", "for", "key", ",", "arr", "in", "batch_data", ".", "items", "(", ")", ":", "\n", "            ", "reduced", "=", "np", ".", "array", "(", "arr", ")", ".", "mean", "(", ")", "\n", "logger", ".", "log_key_value", "(", "key", ",", "reduced", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.trainer.train_maml": [[186, 403], ["e_maml_tf.sampler.path_gen_fn", "next", "tensorflow.get_default_session", "config.G.inner_alg.startswith", "e_maml_tf.sampler.path_gen_fn", "next", "ml_logger.logger.load_variables", "ml_logger.logger.flush", "ml_logger.logger.split", "ml_logger.logger.log", "collections.defaultdict", "maml.save_weight_cache", "range", "tensorflow.get_default_session().run", "ml_logger.logger.split", "ml_logger.logger.log_line", "ml_logger.logger.log", "collections.defaultdict.items", "ml_logger.logger.flush", "tf.get_default_session.run", "ml_logger.logger.log", "ml_logger.logger.log", "isinstance", "config.G.beta.send", "numpy.array", "isinstance", "config.G.clip_range.send", "numpy.array", "isinstance", "config.G.batch_timesteps.send", "ml_logger.logger.log_line", "ml_logger.logger.save_variables", "print", "range", "tensorflow.get_default_session().run", "range", "feed_dict.clear", "numpy.array().mean", "ml_logger.logger.log_key_value", "isinstance", "config.G.alpha.send", "numpy.array", "ml_logger.metrify", "ml_logger.metrify", "tensorflow.trainable_variables", "tensorflow.get_default_session().run", "zip", "tensorflow.get_default_session", "ml_logger.metrify", "print", "tasks.sample", "config.G.inner_alg.startswith", "batch_data[].append", "trainer.path_to_feed_dict", "maml.runner.optim.run_optimize", "path_to_feed_dict.clear", "zip", "feed_dict.update", "tensorflow.get_default_session", "ml_logger.logger.log_key_value", "numpy.array", "dict", "print", "tasks.sample", "RuntimeError", "config.G.meta_alg.startswith", "Exception", "_.send", "numpy.mean", "ml_logger.logger.log_line", "RuntimeError", "ml_logger.logger.log_key_value", "bc.sample_demonstration_data.items", "maml.runner.model.reports.keys", "batch_data[].append", "ml_logger.logger.log_key_value", "ml_logger.logger.log_line", "RuntimeError", "trainer.path_to_feed_dict", "dict", "numpy.fromiter().any", "feed_dict.update", "tensorflow.get_default_session", "enumerate", "zip", "e_maml_tf.algos.bc.sample_demonstration_data", "e_maml_tf.sampler.path_gen_fn.send", "e_maml_tf.algos.bc.sample_demonstration_data", "trainer.path_to_feed_dict", "tensorflow.get_default_session().run", "feed_dict.clear", "tensorflow.get_default_session().run", "e_maml_tf.ge_utils.stem", "e_maml_tf.sampler.path_gen_fn.send", "numpy.fromiter", "tensorflow.get_default_session().run", "tensorflow.get_default_session().run", "dict.values", "tensorflow.get_default_session", "tensorflow.get_default_session", "tensorflow.get_default_session", "tensorflow.get_default_session"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.path_gen_fn", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.path_gen_fn", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.Controls.sample", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.path_to_feed_dict", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update", "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.Controls.sample", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.path_to_feed_dict", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc.sample_demonstration_data", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc.sample_demonstration_data", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.path_to_feed_dict", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.stem", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send"], ["", "", "", "def", "train_maml", "(", "*", ",", "n_tasks", ":", "int", ",", "tasks", ":", "MetaRLTasks", ",", "maml", ":", "E_MAML", ")", ":", "\n", "    ", "if", "not", "G", ".", "inner_alg", ".", "startswith", "(", "\"BC\"", ")", ":", "\n", "        ", "path_gen", "=", "path_gen_fn", "(", "env", "=", "tasks", ".", "envs", ",", "policy", "=", "maml", ".", "runner", ".", "policy", ",", "start_reset", "=", "G", ".", "reset_on_start", ")", "\n", "next", "(", "path_gen", ")", "\n", "\n", "", "meta_path_gen", "=", "path_gen_fn", "(", "env", "=", "tasks", ".", "envs", ",", "policy", "=", "maml", ".", "meta_runner", ".", "policy", ",", "start_reset", "=", "G", ".", "reset_on_start", ")", "\n", "next", "(", "meta_path_gen", ")", "\n", "\n", "if", "G", ".", "load_from_checkpoint", ":", "\n", "# todo: add variable to checkpoint", "\n", "# todo: set the epoch_ind starting point here.", "\n", "        ", "logger", ".", "load_variables", "(", "G", ".", "load_from_checkpoint", ")", "\n", "\n", "", "if", "G", ".", "meta_sgd", ":", "\n", "        ", "assert", "maml", ".", "alpha", "is", "not", "None", ",", "\"Coding Mistake if meta_sgd is trueful but maml.alpha is None.\"", "\n", "\n", "", "max_episode_length", "=", "tasks", ".", "spec", ".", "max_episode_steps", "\n", "\n", "sess", "=", "tf", ".", "get_default_session", "(", ")", "\n", "epoch_ind", ",", "prefix", "=", "G", ".", "epoch_init", "-", "1", ",", "\"\"", "\n", "while", "epoch_ind", "<", "G", ".", "epoch_init", "+", "G", ".", "n_epochs", ":", "\n", "        ", "logger", ".", "flush", "(", ")", "\n", "logger", ".", "split", "(", ")", "\n", "\n", "is_bc_test", "=", "(", "prefix", "!=", "\"test/\"", "and", "G", ".", "eval_interval", "and", "epoch_ind", "%", "G", ".", "eval_interval", "==", "0", ")", "\n", "prefix", "=", "\"test/\"", "if", "is_bc_test", "else", "\"\"", "\n", "epoch_ind", "+=", "0", "if", "is_bc_test", "else", "1", "\n", "\n", "if", "G", ".", "meta_sgd", ":", "\n", "            ", "alpha_lr", "=", "sess", ".", "run", "(", "maml", ".", "alpha", ")", "# only used in the runner.", "\n", "logger", ".", "log", "(", "metrics", "=", "{", "f\"alpha_{i}/{stem(t.name, 2)}\"", ":", "a", "\n", "for", "i", ",", "a_", "in", "enumerate", "(", "alpha_lr", ")", "\n", "for", "t", ",", "a", "in", "zip", "(", "maml", ".", "runner", ".", "trainables", ",", "a_", ")", "}", ",", "silent", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "alpha_lr", "=", "G", ".", "alpha", ".", "send", "(", "epoch_ind", ")", "if", "isinstance", "(", "G", ".", "alpha", ",", "Schedule", ")", "else", "np", ".", "array", "(", "G", ".", "alpha", ")", "\n", "logger", ".", "log", "(", "alpha", "=", "metrify", "(", "alpha_lr", ")", ",", "epoch", "=", "epoch_ind", ",", "silent", "=", "True", ")", "\n", "\n", "", "beta_lr", "=", "G", ".", "beta", ".", "send", "(", "epoch_ind", ")", "if", "isinstance", "(", "G", ".", "beta", ",", "Schedule", ")", "else", "np", ".", "array", "(", "G", ".", "beta", ")", "\n", "clip_range", "=", "G", ".", "clip_range", ".", "send", "(", "epoch_ind", ")", "if", "isinstance", "(", "G", ".", "clip_range", ",", "Schedule", ")", "else", "np", ".", "array", "(", "G", ".", "clip_range", ")", "\n", "logger", ".", "log", "(", "beta", "=", "metrify", "(", "beta_lr", ")", ",", "clip_range", "=", "metrify", "(", "clip_range", ")", ",", "epoch", "=", "epoch_ind", ",", "silent", "=", "True", ")", "\n", "\n", "batch_timesteps", "=", "G", ".", "batch_timesteps", ".", "send", "(", "epoch_ind", ")", "if", "isinstance", "(", "G", ".", "batch_timesteps", ",", "Schedule", ")", "else", "G", ".", "batch_timesteps", "\n", "\n", "# Compute updates for each task in the batch", "\n", "# 0. save value of variables", "\n", "# 1. sample", "\n", "# 2. gradient descent", "\n", "# 3. repeat step 1., 2. until all gradient steps are exhausted.", "\n", "batch_data", "=", "defaultdict", "(", "list", ")", "\n", "\n", "maml", ".", "save_weight_cache", "(", ")", "\n", "load_ops", "=", "[", "]", "if", "DEBUG", ".", "no_weight_reset", "else", "[", "maml", ".", "cache", ".", "load", "]", "\n", "\n", "if", "G", ".", "checkpoint_interval", "and", "epoch_ind", "%", "G", ".", "checkpoint_interval", "==", "0", "and", "not", "is_bc_test", "and", "epoch_ind", ">=", "G", ".", "start_checkpoint_after_epoch", ":", "\n", "            ", "cp_path", "=", "f\"checkpoints/variables_{epoch_ind:04d}.pkl\"", "\n", "logger", ".", "log_line", "(", "f'saving checkpoint {cp_path}'", ")", "\n", "# note: of course I don't know that are all of the trainables at the moment.", "\n", "logger", ".", "save_variables", "(", "tf", ".", "trainable_variables", "(", ")", ",", "path", "=", "cp_path", ")", "\n", "\n", "", "feed_dict", "=", "{", "}", "\n", "for", "task_ind", "in", "range", "(", "n_tasks", "if", "is_bc_test", "else", "G", ".", "n_tasks", ")", ":", "\n", "            ", "graph_branch", "=", "maml", ".", "graphs", "[", "0", "]", "if", "G", ".", "n_graphs", "==", "1", "else", "maml", ".", "graphs", "[", "task_ind", "]", "\n", "if", "G", ".", "n_graphs", "==", "1", ":", "\n", "                ", "gradient_sum_op", "=", "maml", ".", "gradient_sum", ".", "set_op", "if", "task_ind", "==", "0", "else", "maml", ".", "gradient_sum", ".", "add_op", "\n", "\n", "", "print", "(", "f\"task_ind {task_ind}...\"", ")", "\n", "if", "not", "DEBUG", ".", "no_task_resample", ":", "\n", "                ", "if", "not", "is_bc_test", ":", "\n", "                    ", "print", "(", "f'L250: sampling task'", ")", "\n", "tasks", ".", "sample", "(", ")", "\n", "", "elif", "task_ind", "<", "n_tasks", ":", "\n", "                    ", "task_spec", "=", "dict", "(", "index", "=", "task_ind", "%", "n_tasks", ")", "\n", "print", "(", "f'L254: sampling task {task_spec}'", ")", "\n", "tasks", ".", "sample", "(", "**", "task_spec", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'should never hit here.'", ")", "\n", "\n", "", "", "for", "k", "in", "range", "(", "G", ".", "n_grad_steps", "+", "1", ")", ":", "# 0 - 10 <== last one being the maml policy.", "\n", "                ", "_is_new", "=", "False", "\n", "# for imitation inner loss, we still sample trajectory for evaluation purposes, but", "\n", "# replace it with the demonstration data for learning", "\n", "if", "k", "<", "G", ".", "n_grad_steps", ":", "\n", "                    ", "if", "G", ".", "inner_alg", ".", "startswith", "(", "\"BC\"", ")", ":", "\n", "                        ", "p", "=", "p", "if", "G", ".", "single_sampling", "and", "k", ">", "0", "else", "bc", ".", "sample_demonstration_data", "(", "tasks", ".", "task_spec", ",", "key", "=", "(", "\"eval\"", "if", "is_bc_test", "else", "None", ")", ")", "\n", "", "else", ":", "\n", "                        ", "p", ",", "_is_new", "=", "path_gen", ".", "send", "(", "batch_timesteps", ")", ",", "True", "\n", "", "", "elif", "k", "==", "G", ".", "n_grad_steps", ":", "\n", "                    ", "if", "G", ".", "meta_alg", ".", "startswith", "(", "\"BC\"", ")", ":", "\n", "# note: use meta bc samples.", "\n", "                        ", "p", "=", "bc", ".", "sample_demonstration_data", "(", "tasks", ".", "task_spec", ",", "key", "=", "\"meta\"", ")", "\n", "", "else", ":", "\n", "                        ", "p", ",", "_is_new", "=", "meta_path_gen", ".", "send", "(", "batch_timesteps", ")", ",", "True", "\n", "", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "'Implementation error. Should never reach this line.'", ")", "\n", "\n", "", "if", "k", "in", "G", ".", "eval_grad_steps", ":", "\n", "                    ", "_", "=", "path_gen", "if", "k", "<", "G", ".", "n_grad_steps", "else", "meta_path_gen", "\n", "p_eval", "=", "p", "if", "_is_new", "else", "_", ".", "send", "(", "G", ".", "eval_timesteps", ")", "\n", "# reporting on new trajectory samples", "\n", "avg_r", "=", "p_eval", "[", "'ep_info'", "]", "[", "'reward'", "]", "if", "G", ".", "normalize_env", "else", "np", ".", "mean", "(", "p_eval", "[", "'rewards'", "]", ")", "\n", "episode_r", "=", "avg_r", "*", "max_episode_length", "# default horizon for HalfCheetah", "\n", "\n", "if", "episode_r", "<", "G", ".", "term_reward_threshold", ":", "# todo: make this batch-based instead of on single episode", "\n", "                        ", "logger", ".", "log_line", "(", "\"episode reward is too low: \"", ",", "episode_r", ",", "\"terminating training.\"", ",", "flush", "=", "True", ")", "\n", "raise", "RuntimeError", "(", "'AVERAGE REWARD TOO LOW. Terminating the experiment.'", ")", "\n", "\n", "", "batch_data", "[", "prefix", "+", "f\"grad_{k}_step_reward\"", "]", ".", "append", "(", "avg_r", "if", "Reporting", ".", "report_mean", "else", "episode_r", ")", "\n", "if", "k", "in", "G", ".", "eval_grad_steps", ":", "\n", "                        ", "logger", ".", "log_key_value", "(", "prefix", "+", "f\"task_{task_ind}_grad_{k}_reward\"", ",", "episode_r", ",", "silent", "=", "True", ")", "\n", "\n", "", "", "_p", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "p", ".", "items", "(", ")", "if", "k", "!=", "\"ep_info\"", "}", "\n", "\n", "if", "k", "<", "G", ".", "n_grad_steps", ":", "\n", "# note: under meta-SGD mode, the runner needs the k^th learning rate.", "\n", "                    ", "_lr", "=", "alpha_lr", "[", "k", "]", "if", "G", ".", "meta_sgd", "else", "alpha_lr", "\n", "\n", "# clip_range is not used in BC mode. but still passed in.", "\n", "runner_feed_dict", "=", "path_to_feed_dict", "(", "inputs", "=", "maml", ".", "runner", ".", "inputs", ",", "paths", "=", "_p", ",", "lr", "=", "_lr", ",", "\n", "baseline", "=", "G", ".", "baseline", ",", "gamma", "=", "G", ".", "gamma", ",", "use_gae", "=", "G", ".", "use_gae", ",", "lam", "=", "G", ".", "lam", ",", "\n", "horizon", "=", "max_episode_length", ",", "clip_range", "=", "clip_range", ")", "\n", "# todo: optimize `maml.meta_runner` if k >= G.n_grad_steps.", "\n", "loss", ",", "*", "_", ",", "__", "=", "maml", ".", "runner", ".", "optim", ".", "run_optimize", "(", "feed_dict", "=", "runner_feed_dict", ")", "\n", "runner_feed_dict", ".", "clear", "(", ")", "\n", "\n", "for", "key", ",", "value", "in", "zip", "(", "maml", ".", "runner", ".", "model", ".", "reports", ".", "keys", "(", ")", ",", "[", "loss", ",", "*", "_", "]", ")", ":", "\n", "                        ", "batch_data", "[", "prefix", "+", "f\"grad_{k}_step_{key}\"", "]", ".", "append", "(", "value", ")", "\n", "logger", ".", "log_key_value", "(", "prefix", "+", "f\"task_{task_ind}_grad_{k}_{key}\"", ",", "value", ",", "silent", "=", "True", ")", "\n", "\n", "", "if", "loss", ">", "G", ".", "term_loss_threshold", ":", "# todo: make this batch-based instead of on single episode", "\n", "                        ", "logger", ".", "log_line", "(", "prefix", "+", "\"episode loss blew up:\"", ",", "loss", ",", "\"terminating training.\"", ",", "flush", "=", "True", ")", "\n", "raise", "RuntimeError", "(", "'loss is TOO HIGH. Terminating the experiment.'", ")", "\n", "\n", "# done: has bug when using fixed learning rate. Needs the learning rate as input.", "\n", "", "feed_dict", ".", "update", "(", "# do NOT pass in the learning rate because the graph already includes those.", "\n", "path_to_feed_dict", "(", "inputs", "=", "graph_branch", ".", "workers", "[", "k", "]", ".", "inputs", ",", "paths", "=", "_p", ",", "\n", "lr", "=", "None", "if", "G", ".", "meta_sgd", "else", "alpha_lr", ",", "# but do with fixed alpha", "\n", "horizon", "=", "max_episode_length", ",", "\n", "baseline", "=", "G", ".", "baseline", ",", "gamma", "=", "G", ".", "gamma", ",", "use_gae", "=", "G", ".", "use_gae", ",", "lam", "=", "G", ".", "lam", ",", "\n", "clip_range", "=", "clip_range", ")", ")", "\n", "\n", "", "elif", "k", "==", "G", ".", "n_grad_steps", ":", "\n", "                    ", "yield_keys", "=", "dict", "(", "\n", "movie", "=", "epoch_ind", ">=", "G", ".", "start_movie_after_epoch", "and", "epoch_ind", "%", "G", ".", "record_movie_interval", "==", "0", ",", "\n", "eval", "=", "is_bc_test", "\n", ")", "\n", "if", "np", ".", "fromiter", "(", "yield_keys", ".", "values", "(", ")", ",", "bool", ")", ".", "any", "(", ")", ":", "\n", "                        ", "yield", "yield_keys", ",", "epoch_ind", ",", "tasks", ".", "task_spec", "\n", "", "if", "is_bc_test", ":", "\n", "                        ", "if", "load_ops", ":", "# we need to reset the weights. Otherwise the world would be on fire.", "\n", "                            ", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "load_ops", ")", "\n", "", "continue", "# do NOT meta learn from test samples.", "\n", "\n", "# we don't treat the meta_input the same way even though we could. This is more clear to read.", "\n", "# note: feed in the learning rate only later.", "\n", "", "feed_dict", ".", "update", "(", "# do NOT need learning rate", "\n", "path_to_feed_dict", "(", "inputs", "=", "graph_branch", ".", "meta", ".", "inputs", ",", "paths", "=", "_p", ",", "\n", "horizon", "=", "max_episode_length", ",", "\n", "baseline", "=", "G", ".", "baseline", ",", "gamma", "=", "G", ".", "gamma", ",", "use_gae", "=", "G", ".", "use_gae", ",", "lam", "=", "G", ".", "lam", ",", "\n", "clip_range", "=", "clip_range", ")", ")", "\n", "\n", "if", "G", ".", "n_graphs", "==", "1", ":", "\n", "# load from checkpoint before computing the meta gradient\\nrun gradient sum operation", "\n", "                        ", "if", "load_ops", ":", "\n", "                            ", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "load_ops", ")", "\n", "# note: meta reporting should be run here. Not supported for simplicity. (need to reduce across", "\n", "# note: tasks, and can not be done outside individual task graphs.", "\n", "", "if", "G", ".", "meta_sgd", "is", "None", ":", "# note: copied from train_supervised_maml, not tested", "\n", "                            ", "feed_dict", "[", "maml", ".", "alpha", "]", "=", "alpha_lr", "\n", "", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "gradient_sum_op", ",", "feed_dict", ")", "\n", "feed_dict", ".", "clear", "(", ")", "\n", "\n", "", "if", "load_ops", ":", "\n", "                        ", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "load_ops", ")", "\n", "\n", "", "", "", "", "if", "is_bc_test", ":", "\n", "            ", "continue", "# do NOT meta learn from test samples.", "\n", "\n", "# note: copied from train_supervised_maml, not tested", "\n", "", "if", "G", ".", "meta_sgd", "is", "None", ":", "\n", "            ", "feed_dict", "[", "maml", ".", "alpha", "]", "=", "alpha_lr", "\n", "\n", "", "if", "G", ".", "n_graphs", "==", "1", ":", "\n", "            ", "assert", "G", ".", "meta_n_grad_steps", "==", "1", ",", "\"ERROR: Can only run 1 meta gradient step with a single graph.\"", "\n", "# note: remove meta reporting b/c meta report should be in each task in this case.", "\n", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "maml", ".", "meta_update_ops", "[", "0", "]", ",", "{", "maml", ".", "beta", ":", "beta_lr", "}", ")", "\n", "", "else", ":", "\n", "            ", "assert", "feed_dict", ",", "\"ERROR: It is likely that you jumped here from L:178.\"", "\n", "feed_dict", "[", "maml", ".", "beta", "]", "=", "beta_lr", "\n", "for", "i", "in", "range", "(", "G", ".", "meta_n_grad_steps", ")", ":", "\n", "                ", "update_op", "=", "maml", ".", "meta_update_ops", "[", "0", "if", "G", ".", "reuse_meta_optimizer", "else", "i", "]", "\n", "*", "reports", ",", "_", "=", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "maml", ".", "meta_reporting", "+", "[", "update_op", "]", ",", "feed_dict", ")", "\n", "if", "i", "not", "in", "(", "0", ",", "G", ".", "meta_n_grad_steps", "-", "1", ")", ":", "\n", "                    ", "continue", "\n", "", "for", "key", ",", "v", "in", "zip", "(", "maml", ".", "meta_reporting_keys", ",", "reports", ")", ":", "\n", "                    ", "logger", ".", "log_key_value", "(", "prefix", "+", "f\"grad_{G.n_grad_steps + i}_step_{key}\"", ",", "v", ",", "silent", "=", "True", ")", "\n", "\n", "", "", "feed_dict", ".", "clear", "(", ")", "\n", "\n", "", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "maml", ".", "cache", ".", "save", ")", "\n", "\n", "# Now compute the meta gradients.", "\n", "# note: runner shares variables with the MAML graph. Reload from state_dict", "\n", "# note: if max_grad_step is the same as n_grad_steps then no need here.", "\n", "\n", "dt", "=", "logger", ".", "split", "(", ")", "\n", "logger", ".", "log_line", "(", "'Timer Starts...'", "if", "dt", "is", "None", "else", "f'{dt:0.2f} sec/epoch'", ")", "\n", "logger", ".", "log", "(", "dt_epoch", "=", "dt", "or", "np", ".", "nan", ",", "epoch", "=", "epoch_ind", ")", "\n", "\n", "for", "key", ",", "arr", "in", "batch_data", ".", "items", "(", ")", ":", "\n", "            ", "reduced", "=", "np", ".", "array", "(", "arr", ")", ".", "mean", "(", ")", "\n", "logger", ".", "log_key_value", "(", "key", ",", "reduced", ")", "\n", "\n", "", "logger", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.trainer.path_to_feed_dict": [[405, 422], ["isinstance", "e_maml_tf.sampler.paths_process", "e_maml_tf.algos.vpg.path_to_feed_dict", "isinstance", "e_maml_tf.sampler.paths_process", "e_maml_tf.algos.ppo2.path_to_feed_dict", "isinstance", "e_maml_tf.sampler.paths_process", "e_maml_tf.algos.cpi.path_to_feed_dict", "isinstance", "e_maml_tf.algos.bc.path_to_feed_dict", "isinstance", "e_maml_tf.algos.bc_learned_loss.path_to_feed_dict", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.paths_process", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.path_to_feed_dict", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.paths_process", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.path_to_feed_dict", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.paths_process", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.path_to_feed_dict", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.path_to_feed_dict", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.path_to_feed_dict"], ["", "", "def", "path_to_feed_dict", "(", "*", ",", "inputs", ",", "paths", ",", "lr", "=", "None", ",", "**", "rest", ")", ":", "\n", "    ", "from", "e_maml_tf", ".", "sampler", "import", "paths_process", "\n", "if", "isinstance", "(", "inputs", ",", "vpg", ".", "Inputs", ")", ":", "\n", "        ", "paths", "=", "paths_process", "(", "paths", ",", "**", "rest", ")", "\n", "return", "vpg", ".", "path_to_feed_dict", "(", "inputs", "=", "inputs", ",", "paths", "=", "paths", ",", "lr", "=", "lr", ")", "# kl limit etc", "\n", "", "elif", "isinstance", "(", "inputs", ",", "ppo2", ".", "Inputs", ")", ":", "\n", "        ", "paths", "=", "paths_process", "(", "paths", ",", "**", "rest", ")", "\n", "return", "ppo2", ".", "path_to_feed_dict", "(", "inputs", "=", "inputs", ",", "paths", "=", "paths", ",", "lr", "=", "lr", ",", "**", "rest", ")", "# kl limit etc", "\n", "", "elif", "isinstance", "(", "inputs", ",", "cpi", ".", "Inputs", ")", ":", "\n", "        ", "paths", "=", "paths_process", "(", "paths", ",", "**", "rest", ")", "\n", "return", "cpi", ".", "path_to_feed_dict", "(", "inputs", "=", "inputs", ",", "paths", "=", "paths", ",", "lr", "=", "lr", ",", "**", "rest", ")", "# kl limit etc", "\n", "", "elif", "isinstance", "(", "inputs", ",", "bc", ".", "Inputs", ")", ":", "\n", "        ", "return", "bc", ".", "path_to_feed_dict", "(", "inputs", "=", "inputs", ",", "paths", "=", "paths", ",", "lr", "=", "lr", ",", "**", "rest", ")", "# kl limit etc", "\n", "", "elif", "isinstance", "(", "inputs", ",", "bc_learned_loss", ".", "Inputs", ")", ":", "\n", "        ", "return", "bc_learned_loss", ".", "path_to_feed_dict", "(", "inputs", "=", "inputs", ",", "paths", "=", "paths", ",", "lr", "=", "lr", ",", "**", "rest", ")", "# kl limit etc", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Input type is not recognised\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.trainer.eval_tensors": [[425, 427], ["tensorflow.get_default_session().run", "tensorflow.get_default_session"], "function", ["None"], ["", "", "def", "eval_tensors", "(", "*", ",", "variable", ",", "feed_dict", ")", ":", "\n", "    ", "return", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "variable", ",", "feed_dict", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.train.run_e_maml": [[10, 94], ["ml_logger.logger.log_params", "ml_logger.logger.upload_file", "e_maml_tf.meta_rl_tasks.MetaRLTasks", "tensorflow.Graph", "tensorflow.reset_default_graph", "e_maml_tf.config.G.update", "e_maml_tf.config.DEBUG.update", "tf.Graph.as_default", "U.make_session", "e_maml_tf.e_maml_ge.E_MAML", "U.initialize", "gym.make", "unbatch_policy", "render_gen_fn", "e_maml_tf.trainer.train_maml", "ml_logger.logger.flush", "vars", "vars", "vars", "k_index", "next", "_hook_cache.clear", "status.startswith", "status.endswith", "k_index.sample_task", "ml_logger.logger.log_video", "k_index.sample_task", "ml_logger.logger.log_video", "next", "next", "range", "range"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.trainer.train_maml", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.k_index.k_index", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.sample_task", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.sample_task"], ["def", "run_e_maml", "(", "_G", "=", "None", ",", "_DEBUG", "=", "None", ")", ":", "\n", "    ", "import", "baselines", ".", "common", ".", "tf_util", "as", "U", "\n", "if", "_G", "is", "not", "None", ":", "\n", "        ", "config", ".", "G", ".", "update", "(", "_G", ")", "\n", "", "if", "_DEBUG", "is", "not", "None", ":", "\n", "        ", "config", ".", "DEBUG", ".", "update", "(", "_DEBUG", ")", "\n", "\n", "# todo: let's take the control of the log director away from the train script. It should all be set from outside.", "\n", "# done: now this is set in the runner thunk.", "\n", "# logger.configure(log_directory=config.RUN.log_dir, prefix=config.RUN.log_prefix)", "\n", "", "logger", ".", "log_params", "(", "\n", "G", "=", "vars", "(", "config", ".", "G", ")", ",", "\n", "Reporting", "=", "vars", "(", "config", ".", "Reporting", ")", ",", "\n", "DEBUG", "=", "vars", "(", "config", ".", "DEBUG", ")", "\n", ")", "\n", "logger", ".", "upload_file", "(", "__file__", ")", "\n", "\n", "tasks", "=", "MetaRLTasks", "(", "env_name", "=", "config", ".", "G", ".", "env_name", ",", "batch_size", "=", "config", ".", "G", ".", "n_parallel_envs", ",", "\n", "start_seed", "=", "config", ".", "G", ".", "start_seed", ",", "\n", "log_directory", "=", "(", "config", ".", "RUN", ".", "log_directory", "+", "\"/{seed}\"", ")", "if", "config", ".", "G", ".", "render", "else", "None", ",", "\n", "max_steps", "=", "config", ".", "G", ".", "env_max_timesteps", ")", "\n", "\n", "# sess_config = tf.ConfigProto(log_device_placement=config.Reporting.log_device_placement)", "\n", "# with tf.Session(config=sess_config), tf.device('/gpu:0'), tasks:", "\n", "graph", "=", "tf", ".", "Graph", "(", ")", "\n", "with", "graph", ".", "as_default", "(", ")", ",", "U", ".", "make_session", "(", "num_cpu", "=", "config", ".", "G", ".", "n_cpu", ")", ",", "tasks", ":", "\n", "        ", "maml", "=", "E_MAML", "(", "ob_space", "=", "tasks", ".", "envs", ".", "observation_space", ",", "act_space", "=", "tasks", ".", "envs", ".", "action_space", ")", "\n", "\n", "U", ".", "initialize", "(", ")", "\n", "\n", "import", "gym", "\n", "from", "rl", ".", "helpers", "import", "unbatch_policy", ",", "render_gen_fn", "\n", "\n", "eval_env", "=", "gym", ".", "make", "(", "config", ".", "G", ".", "env_name", ")", "\n", "\n", "if", "config", ".", "G", ".", "use_k_index", ":", "\n", "            ", "from", "e_maml_tf", ".", "wrappers", ".", "k_index", "import", "k_index", "\n", "eval_env", "=", "k_index", "(", "eval_env", ")", "\n", "\n", "", "_policy", "=", "unbatch_policy", "(", "maml", ".", "runner", ".", "policy", ")", "\n", "\n", "# todo: use batch-mode to accelerate rendering.", "\n", "rend_gen", "=", "render_gen_fn", "(", "_policy", ",", "eval_env", ",", "stochastic", "=", "False", ",", "width", "=", "640", ",", "height", "=", "480", ",", "reset_on_done", "=", "True", ")", "\n", "\n", "_ep_ind", ",", "_hook_cache", "=", "None", ",", "{", "}", "\n", "train_iter", "=", "train_maml", "(", "n_tasks", "=", "config", ".", "G", ".", "n_tasks", ",", "tasks", "=", "tasks", ",", "maml", "=", "maml", ")", "\n", "while", "True", ":", "\n", "            ", "try", ":", "\n", "                ", "status", ",", "epoch", ",", "task_spec", ",", "*", "_", "=", "next", "(", "train_iter", ")", "\n", "\n", "t_id", "=", "task_spec", "[", "'index'", "]", "\n", "if", "epoch", "!=", "_ep_ind", ":", "\n", "                    ", "_hook_cache", ".", "clear", "(", ")", "\n", "", "_ep_ind", "=", "epoch", "\n", "\n", "if", "status", ".", "startswith", "(", "'grad-'", ")", "and", "status", ".", "endswith", "(", "'movie'", ")", ":", "\n", "                    ", "k", ",", "=", "_", "\n", "hook", "=", "f\"{config.G.env_name}_{epoch:04d}_k({k})_t({t_id})\"", "\n", "if", "hook", "in", "_hook_cache", ":", "\n", "                        ", "continue", "\n", "", "_hook_cache", "[", "hook", "]", "=", "True", "\n", "\n", "eval_env", ".", "sample_task", "(", "**", "task_spec", ")", "\n", "movie", "=", "[", "next", "(", "rend_gen", ")", "for", "_", "in", "range", "(", "config", ".", "G", ".", "movie_timesteps", ")", "]", "\n", "logger", ".", "log_video", "(", "movie", ",", "\"videos/\"", "+", "hook", "+", "\".mp4\"", ",", "fps", "=", "30", ")", "\n", "del", "movie", "\n", "# samples = [next(sample_gen) for _ in range(config.G.movie_timesteps)]", "\n", "# logger.log_data(samples, hook + \".pkl\")", "\n", "\n", "", "hook", "=", "f\"{config.G.env_name}_{epoch:04d}_t({t_id})\"", "\n", "if", "status", "==", "'post-update-movie'", "and", "hook", "not", "in", "_hook_cache", ":", "\n", "                    ", "_hook_cache", "[", "hook", "]", "=", "True", "\n", "eval_env", ".", "sample_task", "(", "**", "task_spec", ")", "\n", "movie", "=", "[", "next", "(", "rend_gen", ")", "for", "_", "in", "range", "(", "config", ".", "G", ".", "movie_timesteps", ")", "]", "\n", "logger", ".", "log_video", "(", "movie", ",", "\"videos/\"", "+", "hook", "+", "\".mp4\"", ",", "fps", "=", "30", ")", "\n", "del", "movie", "\n", "# samples = [next(sample_gen) for _ in range(config.G.movie_timesteps)]", "\n", "# logger.log_data(samples, hook + \".pkl\")", "\n", "\n", "", "", "except", "StopIteration", ":", "\n", "                ", "break", "\n", "", "", "logger", ".", "flush", "(", ")", "\n", "\n", "", "tf", ".", "reset_default_graph", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.train.launch": [[96, 108], ["str", "e_maml_tf.config.config_run", "train.run_e_maml", "traceback.format_exc", "ml_logger.logger.log_line"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.config.config_run", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.train.run_e_maml"], ["", "def", "launch", "(", "**", "_G", ")", ":", "\n", "    ", "import", "traceback", "\n", "import", "os", "\n", "os", ".", "environ", "[", "'CUDA_VISIBLE_DEVICES'", "]", "=", "str", "(", "3", ")", "\n", "\n", "try", ":", "\n", "        ", "config", ".", "config_run", "(", "**", "_G", ")", "\n", "run_e_maml", "(", "_G", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "tb", "=", "traceback", ".", "format_exc", "(", ")", "\n", "logger", ".", "log_line", "(", "tb", ")", "\n", "raise", "e", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.path_gen_fn": [[9, 73], ["collections.defaultdict", "env.reset", "collections.defaultdict.clear", "range", "paths[].append", "paths[].append", "paths[].append", "env.step", "paths[].append", "paths[].append", "done_mask.all", "env.reset", "obs.copy", "policy.act", "policy.act", "paths[].append", "actions.copy", "paths[].append", "paths[].append", "policy.value", "numpy.array", "collections.defaultdict.items"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.reset", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.step", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.reset", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_policies.MlpPolicy.value"], ["def", "path_gen_fn", "(", "env", ":", "SubprocVecEnv", ",", "policy", ":", "MlpPolicy", ",", "start_reset", "=", "False", ",", "soft", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Generator function for the path data. This one outputs the log-likelihood, value and baseline.\n\n    Usage:\n\n    |   s = path_gen_fn(...)\n    |   timesteps = 100\n    |   paths = s.send(timesteps)\n    |\n    |   assert \"acs\" in paths\n    |   assert \"obs\" in paths\n\n    :param env: A parallel env, with first index being the batch\n    :param policy: has the signature `act`, returns batched actions for each observation (in the batch)\n    :param gamma: the gamma parameter for the GAE\n    :param lam: the lambda parameter for the GAE\n    :param start_reset: boolean flag for resting on each generator start.\n    :param soft:\n    :param _render:\n    :return: dimension is Size(timesteps, n_envs, feature_size)\n    \"\"\"", "\n", "# todo: use a default dict for these data collection. Much cleaner.", "\n", "\n", "timesteps", "=", "yield", "\n", "obs", ",", "dones", "=", "env", ".", "reset", "(", ")", ",", "[", "False", "]", "*", "env", ".", "num_envs", "\n", "paths", "=", "defaultdict", "(", "list", ")", "\n", "while", "True", ":", "\n", "        ", "paths", ".", "clear", "(", ")", "\n", "# do NOT use this if environment is parallel env.", "\n", "if", "start_reset", ":", "# note: mostly useless.", "\n", "            ", "obs", ",", "dones", "=", "env", ".", "reset", "(", ")", ",", "[", "False", "]", "*", "env", ".", "num_envs", "\n", "", "for", "_", "in", "range", "(", "timesteps", ")", ":", "\n", "            ", "paths", "[", "'obs'", "]", ".", "append", "(", "obs", ".", "copy", "(", ")", ")", "\n", "if", "policy", ".", "vf", "is", "None", ":", "\n", "                ", "actions", ",", "neglogpacs", "=", "policy", ".", "act", "(", "obs", ",", "soft", ")", "\n", "", "else", ":", "\n", "                ", "actions", ",", "values", ",", "neglogpacs", "=", "policy", ".", "act", "(", "obs", ",", "soft", ")", "\n", "paths", "[", "'values'", "]", ".", "append", "(", "values", ")", "\n", "", "paths", "[", "'acs'", "]", ".", "append", "(", "actions", ".", "copy", "(", ")", ")", "\n", "paths", "[", "'neglogpacs'", "]", ".", "append", "(", "neglogpacs", ")", "\n", "obs", ",", "rewards", ",", "dones", ",", "info", "=", "env", ".", "step", "(", "actions", ")", "\n", "\n", "paths", "[", "'rewards'", "]", ".", "append", "(", "rewards", ")", "\n", "paths", "[", "'dones'", "]", ".", "append", "(", "dones", ")", "\n", "\n", "# In multiworld, `info` contains the entire observation. Processing these", "\n", "# will take way too much time. So we don't do that.", "\n", "_suc", "=", "[", "_", "[", "'success'", "]", "for", "_", "in", "info", "if", "'success'", "in", "_", "]", "\n", "if", "_suc", ":", "\n", "                ", "paths", "[", "'info.successes'", "]", ".", "append", "(", "_suc", ")", "\n", "", "_dist", "=", "[", "_", "[", "'dist'", "]", "for", "_", "in", "info", "if", "'dist'", "in", "_", "]", "\n", "if", "_dist", ":", "\n", "                ", "paths", "[", "'info.dists'", "]", ".", "append", "(", "_dist", ")", "\n", "\n", "\n", "# The TimeLimit env wrapper \"dones\" the env when time limit", "\n", "# has been reached. This is technically not correct.", "\n", "# if has vf and not done. Discounted infinite horizon.", "\n", "", "", "done_mask", "=", "1", "-", "dones", "\n", "if", "policy", ".", "vf", "is", "not", "None", "and", "done_mask", ".", "all", "(", ")", ":", "# bootstrap from the (k + 1)th value", "\n", "            ", "paths", "[", "'last_values'", "]", "=", "policy", ".", "value", "(", "obs", ")", "*", "done_mask", "\n", "\n", "", "timesteps", "=", "yield", "{", "k", ":", "np", ".", "array", "(", "v", ")", "for", "k", ",", "v", "in", "paths", ".", "items", "(", ")", "}", "\n", "# now, this is missing bunch of stuff, return for example.", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.paths_reshape": [[76, 97], ["paths.copy", "paths.copy.items", "d.swapaxes().reshape().swapaxes", "isinstance", "len", "d.swapaxes().reshape", "d.swapaxes"], "function", ["None"], ["", "", "def", "paths_reshape", "(", "paths", ",", "horizon", ")", ":", "\n", "    ", "\"\"\"\n    reshapes the trajectories in the path. Used to split paths data with multiple\n    rollouts in a single env into k independent rollout vectors. This is needed\n    for fitting the linear feature baseline.\n\n    | n -> timesteps, k -> rollouts, c -> features.\n    \n    :param paths: dict('acs', 'obs', ...)\n    :param horizon: int, the horizon we want to chop the paths dict into\n    :return:\n    \"\"\"", "\n", "_", "=", "paths", ".", "copy", "(", ")", "\n", "for", "key", ",", "d", "in", "_", ".", "items", "(", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "d", ",", "np", ".", "ndarray", ")", "or", "len", "(", "d", ".", "shape", ")", "<", "2", ":", "\n", "            ", "continue", "# I prefer explicitness, but this requires less maintenance", "\n", "", "n", ",", "k", ",", "*", "c", "=", "d", ".", "shape", "# *c accommodate rank-2 rewards/returns tensors", "\n", "_", "[", "key", "]", "=", "d", ".", "swapaxes", "(", "0", ",", "1", ")", ".", "reshape", "(", "n", "*", "k", "//", "horizon", ",", "horizon", ",", "*", "c", ")", ".", "swapaxes", "(", "0", ",", "1", ")", "\n", "", "return", "_", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.mc": [[99, 109], ["numpy.zeros_like", "range", "numpy.zeros_like", "len"], "function", ["None"], ["", "def", "mc", "(", "paths", ",", "gamma", "=", "None", ")", ":", "\n", "    ", "rewards", "=", "paths", "[", "'rewards'", "]", "\n", "dones", "=", "paths", "[", "'dones'", "]", "# not used", "\n", "returns", "=", "np", ".", "zeros_like", "(", "rewards", ")", "\n", "value_so_far", "=", "paths", "[", "'last_values'", "]", "if", "'last_values'", "in", "paths", "else", "np", ".", "zeros_like", "(", "rewards", "[", "-", "1", "]", ")", "\n", "for", "step", "in", "range", "(", "len", "(", "returns", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "        ", "done_mask", "=", "1", "-", "dones", "[", "step", "]", "\n", "value_so_far", "=", "rewards", "[", "step", "]", "+", "gamma", "*", "value_so_far", "*", "done_mask", "\n", "returns", "[", "step", "]", "=", "value_so_far", "\n", "", "return", "returns", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.value_baseline": [[114, 118], ["m.fit", "m.predict", "e_maml_tf.value_baselines.linear_feature_baseline.LinearFeatureBaseline"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.fit", "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.predict"], ["def", "value_baseline", "(", "paths", ",", "m", ":", "LinearFeatureBaseline", "=", "None", ")", ":", "\n", "    ", "m", "=", "m", "or", "LinearFeatureBaseline", "(", ")", "\n", "m", ".", "fit", "(", "paths", "[", "'obs'", "]", ",", "paths", "[", "'rewards'", "]", ",", "paths", "[", "'returns'", "]", ")", "\n", "return", "m", ".", "predict", "(", "paths", "[", "'obs'", "]", ",", "paths", "[", "'rewards'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.gae": [[120, 136], ["numpy.zeros_like", "len", "range", "numpy.zeros_like"], "function", ["None"], ["", "def", "gae", "(", "paths", ",", "gamma", ",", "lam", ")", ":", "\n", "    ", "assert", "'values'", "in", "paths", ",", "'paths data need to contain value estimates.'", "\n", "gl", "=", "gamma", "*", "lam", "\n", "rewards", "=", "paths", "[", "'rewards'", "]", "\n", "dones", "=", "paths", "[", "'dones'", "]", "\n", "values", "=", "paths", "[", "'values'", "]", "\n", "last_values", "=", "paths", "[", "'last_values'", "]", "if", "'last_values'", "in", "paths", "else", "np", ".", "zeros_like", "(", "rewards", "[", "-", "1", "]", ")", "\n", "gae", "=", "np", ".", "zeros_like", "(", "rewards", ")", "\n", "last_gae", "=", "0", "\n", "l", "=", "len", "(", "rewards", ")", "\n", "for", "step", "in", "range", "(", "l", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "        ", "done_mask", "=", "1", "-", "dones", "[", "step", "]", "\n", "delta", "=", "rewards", "[", "step", "]", "+", "gamma", "*", "(", "last_values", "if", "step", "==", "l", "-", "1", "else", "values", "[", "step", "]", ")", "*", "done_mask", "-", "values", "[", "step", "]", "\n", "last_gae", "=", "delta", "+", "gl", "*", "last_gae", "*", "done_mask", "\n", "gae", "[", "step", "]", "=", "last_gae", "\n", "", "return", "gae", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.paths_process": [[141, 165], ["paths.copy", "sampler.mc", "sampler.paths_reshape", "sampler.value_baseline", "sampler.gae"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.mc", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.paths_reshape", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.value_baseline", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler.gae"], ["def", "paths_process", "(", "paths", ",", "baseline", ",", "horizon", ",", "gamma", "=", "None", ",", "use_gae", "=", "None", ",", "lam", "=", "None", ",", "**", "_", ")", ":", "\n", "    ", "\"\"\"\n    Master RL sample Processor, with GAE configurations and value baseline.\n\n    :param paths:\n    :param baseline:\n    :param use_gae:\n    :param gamma:\n    :param lam:\n    :return:\n    \"\"\"", "\n", "_", "=", "paths", ".", "copy", "(", ")", "\n", "_", "[", "'returns'", "]", "=", "mc", "(", "_", ",", "gamma", "=", "gamma", ")", "\n", "# fixit: this is wrong. Need to fix", "\n", "if", "horizon", ":", "\n", "        ", "_", "=", "paths_reshape", "(", "_", ",", "horizon", ")", "# Need to reshape by rollout for the fitted linearFeatureBaseline.", "\n", "", "if", "baseline", "==", "'linear'", ":", "\n", "        ", "assert", "'values'", "not", "in", "_", ",", "'_ should not contain value estimates when '", "'using the linear feature baseline. LFB Overwrites original estimate.'", "\n", "# todo: use a single baseline model instance to save on speed.", "\n", "_", "[", "'values'", "]", "=", "value_baseline", "(", "_", ",", "m", "=", "linear_baseline_model", ")", "\n", "", "if", "use_gae", ":", "\n", "        ", "_", "[", "'advs'", "]", "=", "gae", "(", "_", ",", "gamma", "=", "gamma", ",", "lam", "=", "lam", ")", "\n", "", "return", "_", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler._deprecated_ppo2_gae": [[210, 240], ["numpy.zeros_like", "numpy.zeros_like", "len", "reversed", "range", "dict"], "function", ["None"], ["", "def", "_deprecated_ppo2_gae", "(", ")", ":", "\n", "# from OpenAI.baselines.ppo2", "\n", "# compute returns from path.", "\n", "# 0. compute rewards", "\n", "# 1. compute adv (GAE)", "\n", "# 2. compute regular adv (no GAE)", "\n", "    ", "\"\"\"\n    rewards = r + \\gamma * V(s_{t + 1})\n    \"\"\"", "\n", "advs", "=", "np", ".", "zeros_like", "(", "paths", "[", "'rewards'", "]", ")", "\n", "\n", "# discount/bootstrap off value fn", "\n", "_advs", "=", "np", ".", "zeros_like", "(", "paths", "[", "'rewards'", "]", ")", "\n", "last_gae_lam", "=", "0", "\n", "n_rollouts", "=", "len", "(", "_obs", ")", "\n", "for", "t", "in", "reversed", "(", "range", "(", "n_rollouts", ")", ")", ":", "\n", "        ", "if", "t", "==", "n_rollouts", "-", "1", ":", "\n", "            ", "next_non_terminal", "=", "1.0", "-", "dones", "\n", "next_values", "=", "last_values", "\n", "", "else", ":", "\n", "            ", "next_non_terminal", "=", "1.0", "-", "_dones", "[", "t", "+", "1", "]", "\n", "next_values", "=", "_values", "[", "t", "+", "1", "]", "\n", "", "delta", "=", "_rewards", "[", "t", "]", "+", "gamma", "*", "next_values", "*", "next_non_terminal", "-", "_values", "[", "t", "]", "\n", "_advs", "[", "t", "]", "=", "last_gae_lam", "=", "delta", "+", "gamma", "*", "lam", "*", "next_non_terminal", "*", "last_gae_lam", "\n", "", "_returns", "=", "_advs", "+", "_values", "\n", "\n", "# return dimension is Size(timesteps, n_envs, feature_size)", "\n", "timesteps", "=", "yield", "dict", "(", "obs", "=", "_obs", ",", "acs", "=", "_actions", ",", "rewards", "=", "_rewards", ",", "dones", "=", "_dones", ",", "\n", "returns", "=", "_returns", ",", "\n", "values", "=", "_values", ",", "neglogpacs", "=", "_neglogpacs", ",", "ep_info", "=", "ep_info", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampler._deprecated_gae_old": [[242, 259], ["numpy.append", "numpy.append", "len", "numpy.empty", "reversed", "range"], "function", ["None"], ["", "def", "_deprecated_gae_old", "(", "paths", ",", "gamma", ",", "lam", ")", ":", "\n", "    ", "\"\"\"\n    Compute advantage with GAE(lambda)\n    \"\"\"", "\n", "# last element is only used for last vtarg, but we already zeroed it if last new = 1", "\n", "new", "=", "np", ".", "append", "(", "paths", "[", "\"new\"", "]", ",", "0", ")", "\n", "vpred", "=", "np", ".", "append", "(", "paths", "[", "\"vpred\"", "]", ",", "paths", "[", "\"nextvpred\"", "]", ")", "\n", "T", "=", "len", "(", "paths", "[", "\"rew\"", "]", ")", "\n", "paths", "[", "\"adv\"", "]", "=", "gaelam", "=", "np", ".", "empty", "(", "T", ",", "'float32'", ")", "\n", "rew", "=", "paths", "[", "\"rew\"", "]", "\n", "lastgaelam", "=", "0", "\n", "for", "t", "in", "reversed", "(", "range", "(", "T", ")", ")", ":", "\n", "        ", "nonterminal", "=", "1", "-", "new", "[", "t", "+", "1", "]", "\n", "delta", "=", "rew", "[", "t", "]", "+", "gamma", "*", "vpred", "[", "t", "+", "1", "]", "*", "nonterminal", "-", "vpred", "[", "t", "]", "\n", "gaelam", "[", "t", "]", "=", "lastgaelam", "=", "delta", "+", "gamma", "*", "lam", "*", "nonterminal", "*", "lastgaelam", "\n", "\n", "", "paths", "[", "\"tdlamret\"", "]", "=", "paths", "[", "\"adv\"", "]", "+", "paths", "[", "\"vpred\"", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.defaultlist.__init__": [[136, 139], ["list", "callable"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "default_factory", ")", ":", "\n", "        ", "self", ".", "data", "=", "list", "(", ")", "\n", "self", ".", "default_factory", "=", "default_factory", "if", "callable", "(", "default_factory", ")", "else", "lambda", ":", "default_factory", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.defaultlist.__setitem__": [[140, 146], ["ge_utils.defaultlist.data.extend", "ge_utils.defaultlist.default_factory", "len"], "methods", ["None"], ["", "def", "__setitem__", "(", "self", ",", "key", ",", "value", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "self", ".", "data", "[", "key", "]", "=", "value", "\n", "", "except", "IndexError", ":", "\n", "            ", "self", ".", "data", ".", "extend", "(", "[", "self", ".", "default_factory", "(", ")", "]", "*", "(", "key", "+", "1", "-", "len", "(", "self", ".", "data", ")", ")", ")", "\n", "self", ".", "data", "[", "key", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.defaultlist.__getitem__": [[147, 149], ["None"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "return", "self", ".", "data", "[", "item", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.defaultlist.__setstate__": [[150, 152], ["NotImplementedError"], "methods", ["None"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'need to be implemented for remote execution.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.defaultlist.__getstate__": [[153, 155], ["NotImplementedError"], "methods", ["None"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'need to be implemented for remote execution.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.Cache.__init__": [[158, 168], ["tensorflow.group", "tensorflow.group", "ge_utils.var_like", "c.assign", "v.assign", "tensorflow.stop_gradient", "zip", "tensorflow.stop_gradient", "zip"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.var_like"], ["    ", "def", "__init__", "(", "self", ",", "variables", ")", ":", "\n", "        ", "\"\"\"\n        creates a variable flip-flop in-memory.\n\n        :param variables:\n        :return: save_op, load_op, cache array\n        \"\"\"", "\n", "self", ".", "cache", "=", "[", "var_like", "(", "v", ")", "for", "v", "in", "variables", "]", "\n", "self", ".", "save", "=", "tf", ".", "group", "(", "*", "[", "c", ".", "assign", "(", "tf", ".", "stop_gradient", "(", "v", ")", ")", "for", "c", ",", "v", "in", "zip", "(", "self", ".", "cache", ",", "variables", ")", "]", ")", "\n", "self", ".", "load", "=", "tf", ".", "group", "(", "*", "[", "v", ".", "assign", "(", "tf", ".", "stop_gradient", "(", "c", ")", ")", "for", "c", ",", "v", "in", "zip", "(", "self", ".", "cache", ",", "variables", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.GradientSum.__init__": [[172, 179], ["tensorflow.group", "tensorflow.group", "ge_utils.var_like", "c.assign", "c.assign_add", "tensorflow.stop_gradient", "zip", "tensorflow.stop_gradient", "zip"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.var_like"], ["    ", "def", "__init__", "(", "self", ",", "variables", ",", "grad_inputs", ")", ":", "\n", "        ", "\"\"\"k is the number of gradients you want to sum.\n        zero this gradient op once every meta iteration. \"\"\"", "\n", "self", ".", "cache", "=", "[", "var_like", "(", "v", ")", "for", "v", "in", "variables", "]", "\n", "# call set before calling add op, faster than zeroing out the cache.", "\n", "self", ".", "set_op", "=", "tf", ".", "group", "(", "*", "[", "c", ".", "assign", "(", "tf", ".", "stop_gradient", "(", "g", ")", ")", "for", "c", ",", "g", "in", "zip", "(", "self", ".", "cache", ",", "grad_inputs", ")", "]", ")", "\n", "self", ".", "add_op", "=", "tf", ".", "group", "(", "*", "[", "c", ".", "assign_add", "(", "tf", ".", "stop_gradient", "(", "g", ")", ")", "for", "c", ",", "g", "in", "zip", "(", "self", ".", "cache", ",", "grad_inputs", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.probe_var": [[6, 8], ["tensorflow.get_default_session().run", "tensorflow.get_default_session"], "function", ["None"], ["def", "probe_var", "(", "*", "variables", ")", ":", "\n", "    ", "return", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "variables", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.as_dict": [[10, 12], ["vars().items", "vars"], "function", ["None"], ["", "def", "as_dict", "(", "c", ")", ":", "\n", "    ", "return", "{", "k", ":", "v", "for", "k", ",", "v", "in", "vars", "(", "c", ")", ".", "items", "(", ")", "if", "k", "[", "0", "]", "!=", "\"_\"", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.var_like": [[14, 21], ["tensorflow.Variable", "tuple", "name.split", "var.get_shape().as_list", "tensorflow.zeros", "var.get_shape"], "function", ["None"], ["", "def", "var_like", "(", "var", ",", "trainable", "=", "False", ")", ":", "\n", "    ", "name", ",", "dtype", ",", "shape", "=", "var", ".", "name", ",", "var", ".", "dtype", ",", "tuple", "(", "var", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", ")", "\n", "new_name", "=", "name", ".", "split", "(", "':'", ")", "[", "0", "]", "\n", "# note: assuming that you are using a variable scope for this declaration.", "\n", "new_var", "=", "tf", ".", "Variable", "(", "initial_value", "=", "tf", ".", "zeros", "(", "shape", ",", "dtype", ")", ",", "name", "=", "new_name", ")", "\n", "# print(f\"declaring variable like {name} w/ new name: {new_var.name}\")", "\n", "return", "new_var", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.placeholders_from_variables": [[23, 42], ["isinstance", "isinstance", "isinstance", "tensorflow.placeholder", "ge_utils.placeholders_from_variables", "tuple", "tuple", "var.get_shape().as_list", "var.get_shape"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.placeholders_from_variables"], ["", "def", "placeholders_from_variables", "(", "var", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Returns a nested collection of TensorFlow placeholders that match shapes\n    and dtypes of the given nested collection of variables.\n    Arguments:\n    ----------\n        var: Nested collection of variables.\n        name: Placeholder name.\n    Returns:\n    --------\n        Nested collection (same structure as `var`) of TensorFlow placeholders.\n    \"\"\"", "\n", "if", "isinstance", "(", "var", ",", "list", ")", "or", "isinstance", "(", "var", ",", "tuple", ")", ":", "\n", "        ", "result", "=", "[", "placeholders_from_variables", "(", "v", ",", "name", ")", "for", "v", "in", "var", "]", "\n", "if", "isinstance", "(", "var", ",", "tuple", ")", ":", "\n", "            ", "return", "tuple", "(", "result", ")", "\n", "", "return", "result", "\n", "", "else", ":", "\n", "        ", "dtype", ",", "shape", "=", "var", ".", "dtype", ",", "tuple", "(", "var", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", ")", "\n", "return", "tf", ".", "placeholder", "(", "dtype", "=", "dtype", ",", "shape", "=", "shape", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.wrap_variable_creation": [[44, 56], ["hasattr", "original_get_variable", "mock.patch", "func", "AttributeError"], "function", ["None"], ["", "", "def", "wrap_variable_creation", "(", "func", ",", "custom_getter", ")", ":", "\n", "    ", "\"\"\"Provides a custom getter for all variable creations.\"\"\"", "\n", "original_get_variable", "=", "tf", ".", "get_variable", "\n", "\n", "def", "custom_get_variable", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "hasattr", "(", "kwargs", ",", "\"custom_getter\"", ")", ":", "\n", "            ", "raise", "AttributeError", "(", "\"Custom getters are not supported for optimizee variables.\"", ")", "\n", "", "return", "original_get_variable", "(", "*", "args", ",", "custom_getter", "=", "custom_getter", ",", "**", "kwargs", ")", "\n", "\n", "# Mock the get_variable method.", "\n", "", "with", "mock", ".", "patch", "(", "\"tensorflow.get_variable\"", ",", "custom_get_variable", ")", ":", "\n", "        ", "return", "func", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.get_var_name": [[58, 60], ["string.split"], "function", ["None"], ["", "", "def", "get_var_name", "(", "string", ")", ":", "\n", "    ", "return", "string", ".", "split", "(", "':'", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.var_map": [[62, 71], ["ge_utils.get_var_name", "v.name.startswith", "len"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.get_var_name"], ["", "def", "var_map", "(", "variables", ",", "root_scope_name", ")", ":", "\n", "    ", "\"\"\"\n    only returns those that starts with the root_scope_name.\n\n    :param variables:\n    :param root_scope_name:\n    :return:\n    \"\"\"", "\n", "return", "{", "get_var_name", "(", "v", ".", "name", ")", "[", "len", "(", "root_scope_name", ")", ":", "]", ":", "v", "for", "v", "in", "variables", "if", "v", ".", "name", ".", "startswith", "(", "root_scope_name", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.get_scope_name": [[73, 75], ["tensorflow.get_default_graph().get_name_scope", "tensorflow.get_default_graph"], "function", ["None"], ["", "def", "get_scope_name", "(", ")", ":", "\n", "    ", "return", "tf", ".", "get_default_graph", "(", ")", ".", "get_name_scope", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.stem": [[77, 104], ["[].split", "n.split"], "function", ["None"], ["", "def", "stem", "(", "n", ",", "k", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    Allow using k > 1 to leave a longer segment of the bread crum\n\n    Example Variable(output Tensor) Names:\n    ```\n        runner/input_bias:0\n        runner/MlpPolicy/pi_fc1/w:0\n        runner/MlpPolicy/pi_fc1/b:0\n        runner/MlpPolicy/pi_fc2/w:0\n    ```\n\n    stem(tensor.name, 2) should give us\n\n    ```\n        runner/input_bias\n        pi_fc1/w\n        pi_fc1/b\n        pi_fc2/w\n    ```\n\n\n    :param n:\n    :param k:\n    :return:\n    \"\"\"", "\n", "return", "\"/\"", ".", "join", "(", "n", ".", "split", "(", "\":\"", ")", "[", "0", "]", ".", "split", "(", "'/'", ")", "[", "-", "k", ":", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.make_with_custom_variables": [[109, 130], ["ge_utils.wrap_variable_creation", "len"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.wrap_variable_creation"], ["def", "make_with_custom_variables", "(", "func", ":", "Callable", "[", "[", "Any", "]", ",", "T", "]", ",", "variable_map", ",", "root_name_space", "=", "\"\"", ")", "->", "T", ":", "\n", "    ", "\"\"\"Calls func and replaces any trainable variables.\n    This returns the output of func, but whenever `get_variable` is called it\n    will replace any trainable variables with the tensors in `variables`, in the\n    same order. Non-trainable variables will re-use any variables already\n    created.\n    Arguments:\n    ----------\n        func: Function to be called.\n        variables: A list of tensors replacing the trainable variables.\n    Returns:\n    --------\n        The return value of func is returned.\n    \"\"\"", "\n", "\n", "def", "custom_getter", "(", "getter", ",", "name", ",", "**", "kwargs", ")", ":", "\n", "        ", "nonlocal", "variable_map", "\n", "postfix", "=", "name", "[", "len", "(", "root_name_space", ")", ":", "]", "\n", "return", "variable_map", "[", "postfix", "]", "\n", "\n", "", "return", "wrap_variable_creation", "(", "func", ",", "custom_getter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.flatten": [[181, 185], ["arr.swapaxes().reshape", "arr.swapaxes"], "function", ["None"], ["", "", "def", "flatten", "(", "arr", ")", ":", "\n", "    ", "\"\"\"swap and then flatten axes 0 and 1\"\"\"", "\n", "n_steps", ",", "n_envs", ",", "*", "_", "=", "arr", ".", "shape", "\n", "return", "arr", ".", "swapaxes", "(", "0", ",", "1", ")", ".", "reshape", "(", "n_steps", "*", "n_envs", ",", "*", "_", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.__init__": [[7, 13], ["tensorflow.split", "tensorflow.tanh", "tensorflow.exp", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "flat", ")", ":", "\n", "        ", "self", ".", "flat", "=", "flat", "\n", "mean", ",", "logstd", "=", "tf", ".", "split", "(", "axis", "=", "len", "(", "flat", ".", "shape", ")", "-", "1", ",", "num_or_size_splits", "=", "2", ",", "value", "=", "flat", ")", "\n", "self", ".", "mean", "=", "tf", ".", "tanh", "(", "mean", ")", "\n", "self", ".", "logstd", "=", "logstd", "\n", "self", ".", "std", "=", "tf", ".", "exp", "(", "logstd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.flatparam": [[14, 16], ["None"], "methods", ["None"], ["", "def", "flatparam", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "flat", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.mode": [[17, 19], ["None"], "methods", ["None"], ["", "def", "mode", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "mean", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.neglogp": [[20, 26], ["tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.log", "tensorflow.reduce_sum", "tensorflow.to_float", "clip_but_pass_gradient", "tensorflow.square", "numpy.log", "tensorflow.shape", "distributions.SquashedDiagGaussianPd.sample"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.Controls.sample"], ["", "def", "neglogp", "(", "self", ",", "x", ")", ":", "\n", "        ", "_", "=", "0.5", "*", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "(", "x", "-", "self", ".", "mean", ")", "/", "self", ".", "std", ")", ",", "axis", "=", "-", "1", ")", "+", "0.5", "*", "np", ".", "log", "(", "2.0", "*", "np", ".", "pi", ")", "*", "tf", ".", "to_float", "(", "tf", ".", "shape", "(", "x", ")", "[", "-", "1", "]", ")", "+", "tf", ".", "reduce_sum", "(", "self", ".", "logstd", ",", "axis", "=", "-", "1", ")", "\n", "_", "+=", "tf", ".", "reduce_sum", "(", "tf", ".", "log", "(", "clip_but_pass_gradient", "(", "1", "-", "self", ".", "sample", "(", ")", "**", "2", ",", "l", "=", "0", ",", "u", "=", "1", ")", "+", "1e-6", ")", ",", "axis", "=", "1", ")", "\n", "return", "_", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.kl": [[27, 31], ["isinstance", "tensorflow.reduce_sum", "tensorflow.square", "tensorflow.square", "tensorflow.square"], "methods", ["None"], ["", "def", "kl", "(", "self", ",", "other", ")", ":", "\n", "        ", "assert", "isinstance", "(", "other", ",", "DiagGaussianPd", ")", "\n", "return", "tf", ".", "reduce_sum", "(", "other", ".", "logstd", "-", "self", ".", "logstd", "+", "(", "tf", ".", "square", "(", "self", ".", "std", ")", "+", "tf", ".", "square", "(", "self", ".", "mean", "-", "other", ".", "mean", ")", ")", "/", "(", "\n", "2.0", "*", "tf", ".", "square", "(", "other", ".", "std", ")", ")", "-", "0.5", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.entropy": [[32, 34], ["tensorflow.reduce_sum", "numpy.log"], "methods", ["None"], ["", "def", "entropy", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "reduce_sum", "(", "self", ".", "logstd", "+", ".5", "*", "np", ".", "log", "(", "2.0", "*", "np", ".", "pi", "*", "np", ".", "e", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.sample": [[35, 37], ["tensorflow.tanh", "tensorflow.random_normal", "tensorflow.shape"], "methods", ["None"], ["", "def", "sample", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "tanh", "(", "self", ".", "mean", "+", "self", ".", "std", "*", "tf", ".", "random_normal", "(", "tf", ".", "shape", "(", "self", ".", "mean", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.fromflat": [[38, 41], ["cls"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "fromflat", "(", "cls", ",", "flat", ")", ":", "\n", "        ", "return", "cls", "(", "flat", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.meta_rl_tasks.MetaRLTasks.__enter__": [[36, 38], ["None"], "methods", ["None"], ["    ", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.meta_rl_tasks.MetaRLTasks.__exit__": [[39, 41], ["meta_rl_tasks.MetaRLTasks.envs.close"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.close"], ["", "def", "__exit__", "(", "self", ",", "exc_type", ",", "exc_val", ",", "exc_tb", ")", ":", "\n", "        ", "self", ".", "envs", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.meta_rl_tasks.MetaRLTasks.k_tasks": [[42, 45], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "k_tasks", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "spec", ".", "_kwargs", "[", "'k_tasks'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.meta_rl_tasks.MetaRLTasks.__init__": [[46, 101], ["gym.logger.set_level", "e_maml_tf.wrappers.subproc_vec_env.SubprocVecEnv", "gym.envs.registry.spec", "vec_normalize", "gym.make", "wrap.seed", "numpy.random.seed", "meta_rl_tasks.MetaRLTasks.__init__.make_env"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.vec_normalize", "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.seed", "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.seed"], ["", "def", "__init__", "(", "self", ",", "*", ",", "env_name", ",", "batch_size", ",", "start_seed", ",", "log_directory", "=", "None", ",", "max_steps", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        use log_directory/{seed}/ to dynamically generate movies with individual seeds.\n        \"\"\"", "\n", "import", "gym", "\n", "gym", ".", "logger", ".", "set_level", "(", "40", ")", "# set logging level to avoid annoying warning.", "\n", "\n", "assert", "env_name", "in", "ALLOWED_ENVS", ",", "\"environment {} is not supported. Need to be one of {}\"", ".", "format", "(", "env_name", ",", "ALLOWED_ENVS", ")", "\n", "\n", "# keep the env_name for sampling logic. Can be removed if made more general.", "\n", "self", ".", "env_name", "=", "env_name", "\n", "\n", "def", "make_env", "(", "env_seed", ",", "env_name", ",", "monitor_log_directory", "=", "None", ",", "wrap", "=", "None", ")", ":", "\n", "            ", "def", "_f", "(", ")", ":", "\n", "                ", "nonlocal", "max_steps", "\n", "env", "=", "gym", ".", "make", "(", "env_name", ")", "\n", "# Note: gym seed does not allow task_seed. Use constructor instead.", "\n", "# if self.env_name in GRID_WORLD_KEYS:", "\n", "#     env.seed(seed=(seed, task_seed))", "\n", "# else:", "\n", "env", ".", "seed", "(", "seed", "=", "env_seed", ")", "\n", "# fixit: this seems a bit counter-intuitive. Should probably remove.", "\n", "if", "max_steps", ":", "# 0, None, False are null values.", "\n", "# see issue #410: https://github.com/openai/gym/issues/410 the TimeLimit wrapper is now used as a", "\n", "# standard wrapper, and the _max_episode_steps is used inside TimeLimit wrapper for episode step-out", "\n", "# limit.", "\n", "# Note: should not override the default when reporting.", "\n", "                    ", "env", ".", "_max_episode_steps", "=", "max_steps", "\n", "\n", "", "numpy", ".", "random", ".", "seed", "(", "env_seed", ")", "\n", "# deprecation: we can remove this code", "\n", "if", "monitor_log_directory", "is", "not", "None", ":", "\n", "                    ", "env", "=", "gym", ".", "wrappers", ".", "Monitor", "(", "env", ",", "monitor_log_directory", ".", "format", "(", "seed", "=", "env_seed", ")", ",", "force", "=", "True", ")", "\n", "# todo: use bench Montior", "\n", "# from rl_algs.bench import Monitor", "\n", "# env = Monitor(env, monitor_log_directory.format(seed=seed), force=True)", "\n", "", "if", "wrap", ":", "\n", "                    ", "env", "=", "wrap", "(", "env", ")", "\n", "", "return", "env", "\n", "\n", "", "return", "_f", "\n", "\n", "", "from", "e_maml_tf", ".", "wrappers", ".", "k_index", "import", "k_index", "\n", "self", ".", "envs", "=", "SubprocVecEnv", "(", "\n", "[", "make_env", "(", "env_seed", "=", "start_seed", "+", "s", ",", "env_name", "=", "env_name", ",", "monitor_log_directory", "=", "log_directory", ",", "\n", "wrap", "=", "k_index", "if", "config", ".", "G", ".", "use_k_index", "else", "None", ")", "for", "s", "in", "\n", "range", "(", "batch_size", ")", "]", ")", "\n", "\n", "if", "config", ".", "G", ".", "normalize_env", ":", "\n", "            ", "from", "e_maml_tf", ".", "wrappers", ".", "vec_env_normalize", "import", "vec_normalize", "\n", "self", ".", "envs", "=", "vec_normalize", "(", "self", ".", "envs", ")", "\n", "\n", "# This is used in the reporting logic, to respect the standard reporting for episode length etc..", "\n", "", "self", ".", "spec", "=", "gym", ".", "envs", ".", "registry", ".", "spec", "(", "env_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.meta_rl_tasks.MetaRLTasks.sample": [[102, 122], ["envs.call_sync", "numpy.random.uniform", "envs.call_sync", "envs.call_sync", "envs.call_sync", "numpy.random.randint", "numpy.random.rand"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.call_sync", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.call_sync", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.call_sync", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.call_sync"], ["", "def", "sample", "(", "self", ",", "index", "=", "None", ",", "identical_batch", "=", "True", ")", ":", "\n", "        ", "\"\"\"has to set the goals by batch at least once. Otherwise the initial goals are different depending on the\n        random seed.\"\"\"", "\n", "envs", ":", "SubprocVecEnv", "=", "self", ".", "envs", "\n", "if", "self", ".", "env_name", "==", "\"HalfCheetahGoalVel-v0\"", ":", "\n", "            ", "new_goal", "=", "index", "or", "numpy", ".", "random", ".", "uniform", "(", "0", ",", "2.0", ")", "\n", "# print('New Goal Velocity: ', new_goal)", "\n", "envs", ".", "call_sync", "(", "\"set_goal_velocity\"", ",", "new_goal", "if", "identical_batch", "else", "None", ")", "\n", "", "elif", "self", ".", "env_name", "==", "\"HalfCheetahGoalDir-v0\"", ":", "\n", "            ", "new_direction", "=", "index", "or", "(", "1", "if", "numpy", ".", "random", ".", "rand", "(", ")", ">", "0.5", "else", "-", "1", ")", "\n", "envs", ".", "call_sync", "(", "\"set_goal_direction\"", ",", "new_direction", "if", "identical_batch", "else", "None", ")", "\n", "", "elif", "index", "is", "None", ":", "\n", "            ", "new_obj_index", "=", "numpy", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "k_tasks", ")", "if", "identical_batch", "else", "None", "\n", "envs", ".", "call_sync", "(", "\"sample_task\"", ",", "index", "=", "new_obj_index", ")", "\n", "", "else", ":", "\n", "            ", "envs", ".", "call_sync", "(", "\"sample_task\"", ",", "index", "=", "index", ")", "\n", "\n", "", "self", ".", "_task_spec", "=", "None", "\n", "# algorithm always resets, so no need to reset here.", "\n", "return", "envs", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.meta_rl_tasks.MetaRLTasks.task_spec": [[125, 151], ["meta_rl_tasks.MetaRLTasks.env_name.startswith", "meta_rl_tasks.MetaRLTasks.env_name.startswith", "meta_rl_tasks.MetaRLTasks.envs.call_sync", "dict"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.call_sync"], ["@", "property", "\n", "def", "task_spec", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "env_name", ".", "startswith", "(", "\"ReacherMultitask\"", ")", "or", "self", ".", "env_name", ".", "startswith", "(", "\"PointMassMultitask\"", ")", "or", "self", ".", "env_name", "==", "\"ReacherSingleTask-v1\"", "or", "self", ".", "env_name", "==", "\"PointMass-v0\"", "or", "self", ".", "env_name", "==", "\"SawyerPointMultitaskSimple-v0\"", "or", "self", ".", "env_name", "==", "\"SawyerPointMultitask-v0\"", "or", "self", ".", "env_name", "==", "\"PointMassQuadrangle-v0\"", "or", "self", ".", "env_name", "==", "\"SawyerPickLiftMultitaskSimple-v0\"", "or", "self", ".", "env_name", "==", "\"SawyerPickLiftMultitask-v0\"", "or", "self", ".", "env_name", "==", "\"SawyerPickReachMultitaskSimple-v0\"", "or", "self", ".", "env_name", "==", "\"SawyerPickReachMultitask-v0\"", "or", "self", ".", "env_name", "==", "\"SawyerPickPlaceMultitaskSimple-v0\"", "or", "self", ".", "env_name", "==", "\"SawyerPickPlaceMultitask-v0\"", "or", "self", ".", "env_name", "==", "'SawyerDoorFixedMultitask-v0'", "or", "self", ".", "env_name", "==", "'SawyerDoorMultitask-v0'", "or", "self", ".", "env_name", "==", "'SawyerMixedMultitask-v0'", ":", "\n", "\n", "            ", "if", "self", ".", "_task_spec", ":", "\n", "                ", "return", "self", ".", "_task_spec", "\n", "# take just the index from the first env.", "\n", "", "index", ",", "*", "_", "=", "self", ".", "envs", ".", "call_sync", "(", "\"get_goal_index\"", ")", "\n", "self", ".", "_task_spec", "=", "dict", "(", "index", "=", "index", ")", "\n", "return", "self", ".", "_task_spec", "\n", "", "raise", "NotImplemented", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampling_utils.is_scalar": [[4, 6], ["isinstance", "hasattr", "len"], "function", ["None"], ["def", "is_scalar", "(", "n", ")", ":", "\n", "    ", "return", "(", "hasattr", "(", "n", ",", "'shape'", ")", "and", "len", "(", "n", ".", "shape", ")", "<", "1", ")", "or", "isinstance", "(", "n", ",", "Number", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampling_utils.batchify": [[8, 25], ["np.random.randn().argsort", "range", "np.random.randn", "sampling_utils.is_scalar", "paths.items", "range"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.sampling_utils.is_scalar"], ["", "def", "batchify", "(", "paths", ",", "batch_size", ",", "n", ",", "shuffle", ")", ":", "\n", "    ", "\"\"\"\n\n    :param paths:\n    :param batch_size:\n    :param n: length of the\n    :param shuffle: boolean flag to shuffle the batch.\n    :return:\n    \"\"\"", "\n", "import", "numpy", "as", "np", "\n", "shuffled_inds", "=", "np", ".", "random", ".", "randn", "(", "n", ")", ".", "argsort", "(", ")", "\n", "for", "i", "in", "range", "(", "n", "//", "batch_size", ")", ":", "\n", "        ", "start", "=", "i", "*", "batch_size", "\n", "end", "=", "start", "+", "batch_size", "\n", "yield", "{", "\n", "k", ":", "v", "if", "is_scalar", "(", "v", ")", "else", "v", "[", "shuffled_inds", "[", "start", ":", "end", "]", "if", "shuffle", "else", "range", "(", "start", ",", "end", ")", "]", "\n", "for", "k", ",", "v", "in", "paths", ".", "items", "(", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc.Inputs.__init__": [[25, 30], ["isinstance", "tensorflow.placeholder", "tensorflow.placeholder", "list"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "*", ",", "action_space", ",", "A", "=", "None", ")", ":", "\n", "        ", "if", "isinstance", "(", "action_space", ",", "spaces", ".", "Discrete", ")", ":", "\n", "            ", "self", ".", "A", "=", "A", "or", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "name", "=", "\"A\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "A", "=", "A", "or", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", "+", "list", "(", "action_space", ".", "shape", ")", ",", "name", "=", "\"A\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc.BC.__init__": [[40, 50], ["tensorflow.variable_scope", "tensorflow.reduce_mean", "bc.Reports", "policy.pd.neglogp", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "policy.pd.entropy"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.neglogp", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.entropy"], ["    ", "def", "__init__", "(", "self", ",", "*", ",", "inputs", ":", "Inputs", ",", "policy", ")", ":", "\n", "        ", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "policy", "=", "policy", "\n", "with", "tf", ".", "variable_scope", "(", "'BC'", ")", ":", "\n", "            ", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "neglogp", "(", "inputs", ".", "A", ")", ")", "# equivalent to L2 loss", "\n", "self", ".", "reports", "=", "Reports", "(", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "act_norm", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "mean", ")", ",", "\n", "targ_act_norm", "=", "tf", ".", "reduce_mean", "(", "inputs", ".", "A", ")", ",", "\n", "entropy", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "entropy", "(", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc.Optimize.__init__": [[57, 107], ["tensorflow.variable_scope", "tensorflow.gradients", "tensorflow.get_default_session().run", "hasattr", "v.assign", "tensorflow.get_default_session().run", "tensorflow.get_default_session", "tensorflow.zeros_like", "zip", "tensorflow.stop_gradient", "tensorflow.clip_by_value", "hasattr", "len", "isinstance", "len", "bc.Optimize.apply_grad", "enumerate", "zip", "vars().values", "tensorflow.get_default_session", "tensorflow.maximum", "tensorflow.norm", "vars"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "*", ",", "loss", ",", "trainables", ",", "lr", "=", "None", ",", "max_grad_norm", "=", "None", ",", "max_grad_clip", "=", "None", ",", "strict", "=", "False", ",", "\n", "reports", "=", "None", ",", "**", "_", ")", ":", "\n", "        ", "\"\"\"\n        Graph constructor for the optmizer\n\n        :param lr: The learning rate, usually a placeholder but can be a float. Not needed if using external optimizer,\n                    Needed here for the SGD update in the inner-step.\n                    If set to None, then does not construct the self.optimize operator and the self.run_optimize\n                    function.\n        :param loss:\n        :param trainables: Optional array used for the gradient calculation\n        :param max_grad_norm:\n        :param optimizer:\n        :param _:\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'BC_Optimize'", ")", ":", "\n", "# optimizer.gradients is just a wrapper around tf.gradients, with extra assertions. This is why it raises", "\n", "# errors on non-trainables.", "\n", "            ", "_grads", "=", "tf", ".", "gradients", "(", "loss", ",", "trainables", ")", "\n", "if", "strict", ":", "\n", "                ", "for", "g", "in", "_grads", ":", "\n", "                    ", "assert", "g", "is", "not", "None", ",", "f'Some Grads are not defined: {_grads}'", "\n", "", "", "else", ":", "\n", "                ", "_grads", "=", "[", "tf", ".", "zeros_like", "(", "p", ")", "if", "g", "is", "None", "else", "g", "for", "g", ",", "p", "in", "zip", "(", "_grads", ",", "trainables", ")", "]", "\n", "\n", "", "assert", "(", "not", "max_grad_norm", "or", "not", "max_grad_clip", ")", ",", "f'max_grad_norm({max_grad_clip}) and max_grad_norm({max_grad_clip}) can not be trueful at the same time.'", "\n", "if", "max_grad_norm", ":", "# allow 0 to be by-pass", "\n", "# print('setting max-grad-norm to', max_grad_norm)", "\n", "# tf.clip_by_global_norm is just fine. No need to use my own.", "\n", "                ", "_grads", "=", "[", "g", "*", "tf", ".", "stop_gradient", "(", "max_grad_norm", "/", "tf", ".", "maximum", "(", "max_grad_norm", ",", "tf", ".", "norm", "(", "g", ")", ")", ")", "for", "g", "in", "_grads", "]", "\n", "# _grads, grad_norm = tf.clip_by_global_norm(_grads, max_grad_norm)", "\n", "", "elif", "max_grad_clip", ":", "\n", "                ", "_grads", "=", "[", "tf", ".", "clip_by_value", "(", "g", ",", "-", "max_grad_clip", ",", "max_grad_clip", ")", "for", "g", "in", "_grads", "]", "\n", "\n", "", "self", ".", "grads", "=", "_grads", "\n", "\n", "# graph operator for updating the parameter. used by maml with the SGD inner step", "\n", "self", ".", "apply_grad", "=", "lambda", "*", ",", "lr", ",", "grad", ",", "var", ":", "var", "-", "lr", "*", "grad", "\n", "\n", "if", "lr", "is", "not", "None", ":", "\n", "                ", "assert", "hasattr", "(", "trainables", "[", "0", "]", ",", "'_variable'", ")", ",", "\"trainables have to have the _variable attribute\"", "\n", "lr_not_scalar", "=", "(", "hasattr", "(", "lr", ",", "'shape'", ")", "and", "len", "(", "lr", ".", "shape", ")", ")", "or", "(", "isinstance", "(", "lr", ",", "Sequence", ")", "and", "len", "(", "lr", ")", ")", "\n", "self", ".", "optimize", "=", "[", "v", ".", "assign", "(", "self", ".", "apply_grad", "(", "lr", "=", "lr", "[", "i", "]", "if", "lr_not_scalar", "else", "lr", ",", "grad", "=", "g", ",", "var", "=", "v", ")", ")", "\n", "for", "i", ",", "(", "v", ",", "g", ")", "in", "enumerate", "(", "zip", "(", "trainables", ",", "self", ".", "grads", ")", ")", "]", "\n", "_", "=", "self", ".", "optimize", "if", "reports", "is", "None", "else", "[", "*", "vars", "(", "reports", ")", ".", "values", "(", ")", ",", "*", "self", ".", "optimize", "]", "\n", "self", ".", "run_optimize", "=", "lambda", "feed_dict", ":", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "_", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n", "# Function to compute the CPI gradients", "\n", "", "", "self", ".", "run_grads", "=", "lambda", "*", ",", "feed_dict", ":", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "[", "_grads", "]", ",", "feed_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc.sample_generator": [[114, 187], ["len", "dict", "np.random.rand().argsort", "range", "hasattr", "v.reshape().swapaxes().reshape", "_.items", "np.random.rand", "isinstance", "v[].reshape", "_.items", "isinstance", "v[].reshape", "selected_paths.items", "augment_fn", "v.reshape().swapaxes", "v.reshape"], "function", ["None"], ["def", "sample_generator", "(", "*", ",", "paths_list", ":", "Union", "[", "list", "]", ",", "batch_size", "=", "None", ",", "augment_fn", ":", "Union", "[", "None", ",", "Callable", "]", "=", "None", ",", "\n", "episodic_subsample_interval", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    mode == \"timestep\":\n        The sampler samples each timestep individually. Different rollouts are always sampled individually.\n\n    mode == \"episode\":\n        Each episode (index = 1) are sampled individually (rollout). Timesteps are not shuffled.\n\n    Episodic Subsampling:\n        Only applies under mode == \"episode\".\n        \n        The episodic subsample occurs at fixed interval. The starting point of this subsampling is randomly sampled.\n\n\n    :param paths: dict['obs', 'acs'], values are tensors of the shape\n\n                Size(timesteps, n_envs, feat_n).\n\n            This makes it easier to manipulate shuffling and slicing timestep wise.\n\n    :param batch_size: size for the mini-batches.\n    :param augment_fn:  A function (*, obs, acs, *task_spec) => augmented path{obs, acs}\n            Note: This augment_fn is called every mini-batch. It is task-specific. (takes in task_spec)\n    :return: dict(\n                 obs = Size(batch_size, 1, feat_n),\n                 acs = Size(batch_size, 1, feat_n)\n                 ...\n             )\n    \"\"\"", "\n", "import", "numpy", "as", "np", "\n", "\n", "# assert mode is 'multitask', \"Only multitask mode is supported now.\"", "\n", "# assert augment_fn is None, \"The augmentation function is not called under this mode.\"", "\n", "# Now allow augment_fn in multitask mode.", "\n", "\n", "p0", "=", "paths_list", "[", "0", "]", "# assume that all data are identical shape.", "\n", "assert", "p0", "[", "'obs'", "]", ".", "shape", "[", "0", "]", "==", "p0", "[", "'acs'", "]", ".", "shape", "[", "0", "]", ",", "\"observation and actions need to have the same length.\"", "\n", "assert", "len", "(", "p0", "[", "'obs'", "]", ".", "shape", ")", "==", "3", ",", "\"observation (and action) are rank 3 tensors ~ Size(k, horizon, feat_n).\"", "\n", "\n", "timesteps", ",", "k_rollouts", ",", "_", "=", "p0", "[", "'obs'", "]", ".", "shape", "\n", "batch_size", "=", "batch_size", "or", "timesteps", "\n", "batch_n", "=", "timesteps", "*", "k_rollouts", "//", "batch_size", "\n", "assert", "timesteps", "%", "(", "episodic_subsample_interval", "*", "batch_size", ")", "==", "0", ",", "f's.t. that shuffling works. '", "f'{timesteps} % ({episodic_subsample_interval} * {batch_size}) != 0'", "\n", "\n", "# the first next returns the number of batch :)", "\n", "task_spec", "=", "yield", "dict", "(", "batch_n", "=", "batch_n", ")", "\n", "\n", "assert", "timesteps", "%", "episodic_subsample_interval", "==", "0", ",", "\"has to be the right shape\"", "\n", "new_shape", "=", "[", "episodic_subsample_interval", ",", "timesteps", "//", "episodic_subsample_interval", ",", "k_rollouts", ",", "-", "1", "]", "\n", "final_shape", "=", "[", "k_rollouts", "*", "episodic_subsample_interval", ",", "timesteps", "//", "episodic_subsample_interval", ",", "-", "1", "]", "\n", "paths", "=", "[", "{", "k", ":", "v", ".", "reshape", "(", "new_shape", ")", ".", "swapaxes", "(", "1", ",", "2", ")", ".", "reshape", "(", "final_shape", ")", "if", "hasattr", "(", "v", ",", "'shape'", ")", "else", "v", "\n", "for", "k", ",", "v", "in", "_", ".", "items", "(", ")", "}", "for", "_", "in", "paths_list", "]", "\n", "while", "True", ":", "\n", "        ", "shuffled_inds", "=", "np", ".", "random", ".", "rand", "(", "episodic_subsample_interval", "*", "k_rollouts", ")", ".", "argsort", "(", ")", "\n", "# do all of the copying here.", "\n", "shuffled_paths", "=", "[", "{", "\n", "k", ":", "v", "[", "shuffled_inds", "]", ".", "reshape", "(", "timesteps", "*", "k_rollouts", ",", "-", "1", ")", "if", "isinstance", "(", "v", ",", "np", ".", "ndarray", ")", "\n", "else", "v", "for", "k", ",", "v", "in", "_", ".", "items", "(", ")", "}", "for", "_", "in", "paths", "]", "\n", "for", "i", "in", "range", "(", "batch_n", ")", ":", "\n", "            ", "task_index", "=", "task_spec", "[", "'index'", "]", "if", "task_spec", "else", "0", "\n", "selected_paths", "=", "shuffled_paths", "[", "task_index", "]", "\n", "\n", "start", "=", "i", "*", "batch_size", "\n", "# no copy involved", "\n", "batch_paths", "=", "{", "\n", "k", ":", "v", "[", "start", ":", "start", "+", "batch_size", "]", ".", "reshape", "(", "batch_size", ",", "1", ",", "-", "1", ")", "if", "isinstance", "(", "v", ",", "np", ".", "ndarray", ")", "\n", "else", "v", "for", "k", ",", "v", "in", "selected_paths", ".", "items", "(", ")", "\n", "}", "\n", "# obs_augment occurs here", "\n", "# note: pass in index=task_index explicitly, b/c task_spec can be None.", "\n", "task_spec", "=", "yield", "augment_fn", "(", "**", "batch_paths", ",", "index", "=", "task_index", ")", "if", "augment_fn", "else", "batch_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc.use_samples": [[189, 194], ["bc.sample_generator", "next"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc.sample_generator"], ["", "", "", "def", "use_samples", "(", "key", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "global", "SAMPLE_GENS", "\n", "key", "=", "'default'", "if", "key", "is", "None", "else", "key", "\n", "SAMPLE_GENS", "[", "key", "]", "=", "sample_generator", "(", "**", "kwargs", ")", "\n", "return", "next", "(", "SAMPLE_GENS", "[", "key", "]", ")", "# start the generator, return information of the generator.", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc.sample_demonstration_data": [[200, 218], ["SAMPLE_GENS.get", "next", "SAMPLE_GENS.get.send", "np.random.choice", "SAMPLE_GENS.keys", "SAMPLE_GENS.keys", "len", "k.startswith"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.get", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send"], ["def", "sample_demonstration_data", "(", "task_spec", "=", "None", ",", "key", "=", "None", ")", ":", "\n", "    ", "global", "SAMPLE_GENS", "\n", "import", "numpy", "as", "np", "\n", "# add logic here to support multi-mode.", "\n", "if", "DATA_MODE", "==", "\"multi-mode\"", ":", "\n", "        ", "if", "key", "is", "None", ":", "\n", "            ", "keys", "=", "[", "k", "for", "k", "in", "SAMPLE_GENS", ".", "keys", "(", ")", "if", "\"/\"", "not", "in", "k", "]", "\n", "", "else", ":", "\n", "            ", "keys", "=", "[", "k", "for", "k", "in", "SAMPLE_GENS", ".", "keys", "(", ")", "if", "k", "==", "key", "or", "k", ".", "startswith", "(", "key", "+", "\"/\"", ")", "]", "\n", "", "key", "=", "keys", "[", "np", ".", "random", ".", "choice", "(", "len", "(", "keys", ")", ")", "]", "\n", "", "elif", "DATA_MODE", "==", "\"simple\"", ":", "\n", "        ", "key", "=", "'default'", "if", "key", "is", "None", "else", "key", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "g", "=", "SAMPLE_GENS", ".", "get", "(", "key", ",", "None", ")", "\n", "assert", "g", "is", "not", "None", ",", "f'sample key {key} does NOT exist. First call use_samples to setup this sample gen.'", "\n", "return", "next", "(", "g", ")", "if", "task_spec", "is", "None", "else", "g", ".", "send", "(", "task_spec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc.path_to_feed_dict": [[222, 251], ["paths[].reshape", "paths[].reshape"], "function", ["None"], ["", "def", "path_to_feed_dict", "(", "*", ",", "inputs", ":", "Inputs", ",", "paths", ",", "lr", "=", "None", ",", "**", "_rest", ")", ":", "\n", "    ", "\"\"\"\n    convert path objects to feed_dict for the tensorflow graph.\n\n    :param inputs:  Input object\n    :param paths: dict['obs', 'acs']: Size(n_timesteps, n_envs, feat_n)\n    :param lr: placeholder or floating point number\n    :param _rest:\n    :return: feed_dict, keyed by the input placeholders.\n    \"\"\"", "\n", "# reshaping the path, need to debug", "\n", "n_timesteps", ",", "n_envs", ",", "*", "_", "=", "paths", "[", "'obs'", "]", ".", "shape", "\n", "n", "=", "n_timesteps", "*", "n_envs", "\n", "\n", "feed_dict", "=", "{", "\n", "inputs", ".", "X", ":", "paths", "[", "'obs'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", ",", "\n", "inputs", ".", "A", ":", "paths", "[", "'acs'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", ",", "\n", "# all of these are gone.", "\n", "# inputs.OLD_NEG_LOG_P_AC: paths['neglogpacs'].reshape(-1),", "\n", "# inputs.OLD_V_PRED: paths['values'].reshape(-1),", "\n", "# These are useful if the agent receives the reward.", "\n", "# inputs.ADV: advs_normalized.reshape(-1),", "\n", "# inputs.R: paths['returns'].reshape(-1),", "\n", "# inputs.CLIP_RANGE: clip_range", "\n", "}", "\n", "if", "lr", "is", "not", "None", ":", "\n", "        ", "assert", "inputs", ".", "LR", "is", "not", "None", ",", "f'Input should have LR attribute if a learning rate is passed.'", "\n", "feed_dict", "[", "inputs", ".", "LR", "]", "=", "lr", "\n", "", "return", "feed_dict", "\n", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.cpi.Inputs.__init__": [[20, 32], ["isinstance", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "list"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "*", ",", "action_space", ",", "value_baseline", "=", "False", ")", ":", "\n", "        ", "if", "isinstance", "(", "action_space", ",", "spaces", ".", "Discrete", ")", ":", "\n", "            ", "self", ".", "A", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "name", "=", "\"A\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "A", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", "+", "list", "(", "action_space", ".", "shape", ")", ",", "name", "=", "\"A\"", ")", "\n", "", "self", ".", "ADV", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"ADV\"", ")", "\n", "self", ".", "OLD_NEG_LOG_P_AC", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"OLD_NEG_LOG_P_AC\"", ")", "\n", "self", ".", "CLIP_RANGE", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "]", ",", "name", "=", "\"CLIP_RANGE\"", ")", "\n", "\n", "if", "value_baseline", ":", "\n", "            ", "self", ".", "R", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"R\"", ")", "\n", "self", ".", "OLD_V_PRED", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"OLD_V_PRED\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.cpi.CPI.__init__": [[43, 69], ["tensorflow.variable_scope", "policy.pd.neglogp", "tensorflow.reduce_mean", "tensorflow.exp", "tensorflow.reduce_mean", "cpi.Reports", "policy.pd.entropy", "tensorflow.square", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.to_float", "tensorflow.square", "tensorflow.greater", "tensorflow.abs"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.neglogp", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.entropy"], ["def", "__init__", "(", "self", ",", "*", ",", "inputs", ":", "Inputs", ",", "policy", ",", "vf_coef", "=", "None", ",", "ent_coef", "=", "None", ")", ":", "\n", "        ", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "policy", "=", "policy", "\n", "with", "tf", ".", "variable_scope", "(", "'CPI'", ")", ":", "\n", "            ", "self", ".", "neglogpac", "=", "policy", ".", "pd", ".", "neglogp", "(", "inputs", ".", "A", ")", "\n", "entropy", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "entropy", "(", ")", ")", "\n", "\n", "ratio", "=", "tf", ".", "exp", "(", "inputs", ".", "OLD_NEG_LOG_P_AC", "-", "self", ".", "neglogpac", ")", "\n", "pg_loss", "=", "tf", ".", "reduce_mean", "(", "-", "inputs", ".", "ADV", "*", "ratio", ")", "\n", "self", ".", "loss", "=", "pg_loss", "-", "entropy", "*", "ent_coef", "\n", "\n", "if", "policy", ".", "vf", "is", "not", "None", ":", "\n", "                ", "assert", "vf_coef", "is", "not", "None", ",", "\"vf_coef can not be None when policy has value function.\"", "\n", "vpred", "=", "policy", ".", "vf", "\n", "vf_loss", "=", "tf", ".", "square", "(", "vpred", "-", "inputs", ".", "R", ")", "\n", "self", ".", "loss", "+=", "vf_loss", "*", "vf_coef", "\n", "\n", "", "self", ".", "reports", "=", "Reports", "(", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "entropy", "=", "entropy", ",", "\n", "approx_kl", "=", ".5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "self", ".", "neglogpac", "-", "inputs", ".", "OLD_NEG_LOG_P_AC", ")", ")", ",", "\n", "clip_frac", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "to_float", "(", "tf", ".", "greater", "(", "tf", ".", "abs", "(", "ratio", "-", "1.0", ")", ",", "inputs", ".", "CLIP_RANGE", ")", ")", ")", "\n", ")", "\n", "if", "policy", ".", "vf", "is", "not", "None", ":", "\n", "                ", "self", ".", "reports", ".", "vf_loss", "=", "vf_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.cpi.Optimize.__init__": [[75, 116], ["tensorflow.variable_scope", "tensorflow.gradients", "tensorflow.get_default_session().run", "hasattr", "v.assign", "tensorflow.get_default_session().run", "tensorflow.get_default_session", "tensorflow.zeros_like", "zip", "tensorflow.stop_gradient", "tensorflow.clip_by_value", "hasattr", "len", "isinstance", "len", "cpi.Optimize.apply_grad", "enumerate", "zip", "vars().values", "tensorflow.get_default_session", "tensorflow.maximum", "tensorflow.norm", "vars"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "*", ",", "loss", ",", "trainables", ",", "lr", "=", "None", ",", "max_grad_norm", "=", "None", ",", "max_grad_clip", "=", "None", ",", "\n", "strict", "=", "None", ",", "reports", "=", "None", ",", "**", "_", ")", ":", "\n", "        ", "\"\"\"\n        :param trainables: Optional array used for the gradient calculation\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'CPI_Optimize'", ")", ":", "\n", "# grad_placeholders = placeholders_from_variables(trainables)", "\n", "# optimizer.gradients is just a wrapper around tf.gradients, with extra assertions. This is why it raises", "\n", "# errors on non-trainables.", "\n", "            ", "_grads", "=", "tf", ".", "gradients", "(", "loss", ",", "trainables", ")", "\n", "if", "strict", ":", "\n", "                ", "for", "g", "in", "_grads", ":", "\n", "                    ", "assert", "g", "is", "not", "None", ",", "f'Some Grads are not defined: {_grads}'", "\n", "", "", "else", ":", "\n", "                ", "_grads", "=", "[", "tf", ".", "zeros_like", "(", "p", ")", "if", "g", "is", "None", "else", "g", "for", "g", ",", "p", "in", "zip", "(", "_grads", ",", "trainables", ")", "]", "\n", "\n", "", "assert", "(", "not", "max_grad_norm", "or", "not", "max_grad_clip", ")", ",", "f'max_grad_norm({max_grad_clip}) and max_grad_norm({max_grad_clip}) can not be trueful at the same time.'", "\n", "if", "max_grad_norm", ":", "# allow 0 to be by-pass", "\n", "# print('setting max-grad-norm to', max_grad_norm)", "\n", "# tf.clip_by_global_norm is just fine. No need to use my own.", "\n", "                ", "_grads", "=", "[", "g", "*", "tf", ".", "stop_gradient", "(", "max_grad_norm", "/", "tf", ".", "maximum", "(", "max_grad_norm", ",", "tf", ".", "norm", "(", "g", ")", ")", ")", "for", "g", "in", "_grads", "]", "\n", "# _grads, grad_norm = tf.clip_by_global_norm(_grads, max_grad_norm)", "\n", "", "elif", "max_grad_clip", ":", "\n", "                ", "_grads", "=", "[", "tf", ".", "clip_by_value", "(", "g", ",", "-", "max_grad_clip", ",", "max_grad_clip", ")", "for", "g", "in", "_grads", "]", "\n", "\n", "", "self", ".", "grads", "=", "_grads", "\n", "\n", "# graph operator for updating the parameter. used by maml with the SGD inner step", "\n", "self", ".", "apply_grad", "=", "lambda", "*", ",", "lr", ",", "grad", ",", "var", ":", "var", "-", "lr", "*", "grad", "\n", "\n", "if", "lr", "is", "not", "None", ":", "\n", "                ", "assert", "hasattr", "(", "trainables", "[", "0", "]", ",", "'_variable'", ")", ",", "\"trainables have to have the _variable attribute\"", "\n", "lr_not_scalar", "=", "(", "hasattr", "(", "lr", ",", "'shape'", ")", "and", "len", "(", "lr", ".", "shape", ")", ")", "or", "(", "isinstance", "(", "lr", ",", "Sequence", ")", "and", "len", "(", "lr", ")", ")", "\n", "self", ".", "optimize", "=", "[", "v", ".", "assign", "(", "self", ".", "apply_grad", "(", "lr", "=", "lr", "[", "i", "]", "if", "lr_not_scalar", "else", "lr", ",", "grad", "=", "g", ",", "var", "=", "v", ")", ")", "\n", "for", "i", ",", "(", "v", ",", "g", ")", "in", "enumerate", "(", "zip", "(", "trainables", ",", "self", ".", "grads", ")", ")", "]", "\n", "_", "=", "self", ".", "optimize", "if", "reports", "is", "None", "else", "[", "*", "vars", "(", "reports", ")", ".", "values", "(", ")", ",", "*", "self", ".", "optimize", "]", "\n", "self", ".", "run_optimize", "=", "lambda", "feed_dict", ":", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "_", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n", "# Function to compute the CPI gradients", "\n", "", "", "self", ".", "run_grads", "=", "lambda", "*", ",", "feed_dict", ":", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "[", "_grads", "]", ",", "feed_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.cpi.path_to_feed_dict": [[118, 144], ["hasattr", "paths[].reshape", "paths[].reshape", "phi.reshape", "paths[].reshape", "paths[].reshape", "paths[].reshape"], "function", ["None"], ["", "", "def", "path_to_feed_dict", "(", "*", ",", "inputs", ":", "Inputs", ",", "paths", ",", "lr", "=", "None", ",", "clip_range", ",", "**", "_r", ")", ":", "\n", "    ", "if", "'adv'", "in", "paths", ":", "\n", "        ", "phi", "=", "paths", "[", "'advs'", "]", "\n", "", "elif", "'values'", "in", "paths", ":", "\n", "        ", "phi", "=", "paths", "[", "'returns'", "]", "-", "paths", "[", "'values'", "]", "\n", "", "else", ":", "\n", "        ", "phi", "=", "paths", "[", "'returns'", "]", "\n", "# advs_normalized = (advs - advs.mean()) / (advs.std() + 1e-8)", "\n", "\n", "", "n_timesteps", ",", "n_envs", ",", "*", "_", "=", "paths", "[", "'obs'", "]", ".", "shape", "\n", "n", "=", "n_timesteps", "*", "n_envs", "\n", "\n", "feed_dict", "=", "{", "\n", "inputs", ".", "X", ":", "paths", "[", "'obs'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", ",", "\n", "inputs", ".", "A", ":", "paths", "[", "'acs'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", ",", "\n", "inputs", ".", "ADV", ":", "phi", ".", "reshape", "(", "-", "1", ")", ",", "\n", "inputs", ".", "OLD_NEG_LOG_P_AC", ":", "paths", "[", "'neglogpacs'", "]", ".", "reshape", "(", "-", "1", ")", ",", "\n", "inputs", ".", "CLIP_RANGE", ":", "clip_range", "\n", "}", "\n", "if", "hasattr", "(", "inputs", ",", "'OLD_V_PRED'", ")", ":", "\n", "        ", "feed_dict", "[", "inputs", ".", "OLD_V_PRED", "]", "=", "paths", "[", "'values'", "]", ".", "reshape", "(", "-", "1", ")", "\n", "feed_dict", "[", "inputs", ".", "R", "]", "=", "paths", "[", "'returns'", "]", ".", "reshape", "(", "-", "1", ")", "\n", "", "if", "lr", "is", "not", "None", ":", "\n", "        ", "assert", "inputs", ".", "LR", "is", "not", "None", ",", "f'Input should have LR attribute if a learning rate is passed.'", "\n", "feed_dict", "[", "inputs", ".", "LR", "]", "=", "lr", "\n", "", "return", "feed_dict", "\n", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.ppo2.Inputs.__init__": [[16, 29], ["isinstance", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "list"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "*", ",", "action_space", ",", "value_baseline", "=", "False", ")", ":", "\n", "        ", "if", "isinstance", "(", "action_space", ",", "spaces", ".", "Discrete", ")", ":", "\n", "            ", "self", ".", "A", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "name", "=", "\"A\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "A", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", "+", "list", "(", "action_space", ".", "shape", ")", ",", "name", "=", "\"A\"", ")", "\n", "\n", "", "self", ".", "ADV", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"ADV\"", ")", "\n", "self", ".", "OLD_NEG_LOG_P_AC", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"OLD_NEG_LOG_P_AC\"", ")", "\n", "self", ".", "CLIP_RANGE", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "]", ",", "name", "=", "\"CLIP_RANGE\"", ")", "\n", "\n", "if", "value_baseline", ":", "\n", "            ", "self", ".", "R", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"R\"", ")", "\n", "self", ".", "OLD_V_PRED", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"OLD_V_PRED\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.ppo2.PPO.__init__": [[44, 78], ["tensorflow.variable_scope", "policy.pd.neglogp", "tensorflow.reduce_mean", "tensorflow.exp", "tensorflow.reduce_mean", "ppo2.Reports", "policy.pd.entropy", "tensorflow.clip_by_value", "tensorflow.maximum", "tensorflow.square", "tensorflow.square", "tensorflow.clip_by_value", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.maximum", "tensorflow.reduce_mean", "tensorflow.to_float", "tensorflow.square", "tensorflow.greater", "tensorflow.abs"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.neglogp", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.entropy"], ["def", "__init__", "(", "self", ",", "*", ",", "inputs", ":", "Inputs", ",", "policy", ",", "vf_coef", "=", "None", ",", "ent_coef", "=", "None", ")", ":", "\n", "        ", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "policy", "=", "policy", "\n", "with", "tf", ".", "variable_scope", "(", "'PPO'", ")", ":", "\n", "            ", "self", ".", "neglogpac", "=", "policy", ".", "pd", ".", "neglogp", "(", "inputs", ".", "A", ")", "\n", "entropy", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "entropy", "(", ")", ")", "\n", "\n", "ratio", "=", "tf", ".", "exp", "(", "inputs", ".", "OLD_NEG_LOG_P_AC", "-", "self", ".", "neglogpac", ")", "\n", "pg_losses", "=", "-", "inputs", ".", "ADV", "*", "ratio", "\n", "pg_losses2", "=", "-", "inputs", ".", "ADV", "*", "tf", ".", "clip_by_value", "(", "ratio", ",", "1.0", "-", "inputs", ".", "CLIP_RANGE", ",", "1.0", "+", "inputs", ".", "CLIP_RANGE", ")", "\n", "pg_loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "maximum", "(", "pg_losses", ",", "pg_losses2", ")", ")", "\n", "self", ".", "loss", "=", "pg_loss", "-", "entropy", "*", "ent_coef", "\n", "\n", "if", "policy", ".", "vf", "is", "not", "None", ":", "\n", "                ", "assert", "vf_coef", "is", "not", "None", ",", "\"vf_coef can not be None when policy has value function.\"", "\n", "vpred", "=", "policy", ".", "vf", "\n", "vpred_clipped", "=", "inputs", ".", "OLD_V_PRED", "+", "tf", ".", "clip_by_value", "(", "policy", ".", "vf", "-", "inputs", ".", "OLD_V_PRED", ",", "-", "inputs", ".", "CLIP_RANGE", ",", "inputs", ".", "CLIP_RANGE", ")", "\n", "vf_losses1", "=", "tf", ".", "square", "(", "vpred", "-", "inputs", ".", "R", ")", "\n", "vf_losses2", "=", "tf", ".", "square", "(", "vpred_clipped", "-", "inputs", ".", "R", ")", "\n", "vf_loss", "=", ".5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "maximum", "(", "vf_losses1", ",", "vf_losses2", ")", ")", "\n", "self", ".", "loss", "+=", "vf_loss", "*", "vf_coef", "\n", "\n", "", "self", ".", "reports", "=", "Reports", "(", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "entropy", "=", "entropy", ",", "\n", "act_norm", "=", "tf", ".", "reduce_mean", "(", "inputs", ".", "A", ")", ",", "\n", "pg_loss", "=", "pg_loss", ",", "\n", "approx_kl", "=", ".5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "self", ".", "neglogpac", "-", "inputs", ".", "OLD_NEG_LOG_P_AC", ")", ")", ",", "\n", "clip_frac", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "to_float", "(", "tf", ".", "greater", "(", "tf", ".", "abs", "(", "ratio", "-", "1.0", ")", ",", "inputs", ".", "CLIP_RANGE", ")", ")", ")", "\n", ")", "\n", "if", "policy", ".", "vf", "is", "not", "None", ":", "\n", "                ", "self", ".", "reports", ".", "vf_loss", "=", "vf_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.ppo2.Optimize.__init__": [[84, 125], ["tensorflow.variable_scope", "tensorflow.gradients", "tensorflow.get_default_session().run", "hasattr", "v.assign", "tensorflow.get_default_session().run", "tensorflow.get_default_session", "tensorflow.zeros_like", "zip", "tensorflow.stop_gradient", "tensorflow.clip_by_value", "hasattr", "len", "isinstance", "len", "ppo2.Optimize.apply_grad", "enumerate", "zip", "vars().values", "tensorflow.get_default_session", "tensorflow.maximum", "tensorflow.norm", "vars"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "*", ",", "loss", ",", "trainables", ",", "lr", "=", "None", ",", "max_grad_norm", "=", "None", ",", "max_grad_clip", "=", "None", ",", "\n", "strict", "=", "False", ",", "reports", "=", "None", ",", "**", "_", ")", ":", "\n", "        ", "\"\"\"\n        :param trainables: Optional array used for the gradient calculation\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'PPO_Optimize'", ")", ":", "\n", "# grad_placeholders = placeholders_from_variables(trainables)", "\n", "# optimizer.gradients is just a wrapper around tf.gradients, with extra assertions. This is why it raises", "\n", "# errors on non-trainables.", "\n", "            ", "_grads", "=", "tf", ".", "gradients", "(", "loss", ",", "trainables", ")", "\n", "if", "strict", ":", "\n", "                ", "for", "g", "in", "_grads", ":", "\n", "                    ", "assert", "g", "is", "not", "None", ",", "f'Some Grads are not defined: {_grads}'", "\n", "", "", "else", ":", "\n", "                ", "_grads", "=", "[", "tf", ".", "zeros_like", "(", "p", ")", "if", "g", "is", "None", "else", "g", "for", "g", ",", "p", "in", "zip", "(", "_grads", ",", "trainables", ")", "]", "\n", "\n", "", "assert", "(", "not", "max_grad_norm", "or", "not", "max_grad_clip", ")", ",", "f'max_grad_norm({max_grad_clip}) and max_grad_norm({max_grad_clip}) can not be trueful at the same time.'", "\n", "if", "max_grad_norm", ":", "# allow 0 to be by-pass", "\n", "# print('setting max-grad-norm to', max_grad_norm)", "\n", "# tf.clip_by_global_norm is just fine. No need to use my own.", "\n", "                ", "_grads", "=", "[", "g", "*", "tf", ".", "stop_gradient", "(", "max_grad_norm", "/", "tf", ".", "maximum", "(", "max_grad_norm", ",", "tf", ".", "norm", "(", "g", ")", ")", ")", "for", "g", "in", "_grads", "]", "\n", "# _grads, grad_norm = tf.clip_by_global_norm(_grads, max_grad_norm)", "\n", "", "elif", "max_grad_clip", ":", "\n", "                ", "_grads", "=", "[", "tf", ".", "clip_by_value", "(", "g", ",", "-", "max_grad_clip", ",", "max_grad_clip", ")", "for", "g", "in", "_grads", "]", "\n", "\n", "", "self", ".", "grads", "=", "_grads", "\n", "\n", "# graph operator for updating the parameter. used by maml with the SGD inner step", "\n", "self", ".", "apply_grad", "=", "lambda", "*", ",", "lr", ",", "grad", ",", "var", ":", "var", "-", "lr", "*", "grad", "\n", "\n", "if", "lr", "is", "not", "None", ":", "\n", "                ", "assert", "hasattr", "(", "trainables", "[", "0", "]", ",", "'_variable'", ")", ",", "\"trainables have to have the _variable attribute\"", "\n", "lr_not_scalar", "=", "(", "hasattr", "(", "lr", ",", "'shape'", ")", "and", "len", "(", "lr", ".", "shape", ")", ")", "or", "(", "isinstance", "(", "lr", ",", "Sequence", ")", "and", "len", "(", "lr", ")", ")", "\n", "self", ".", "optimize", "=", "[", "v", ".", "assign", "(", "self", ".", "apply_grad", "(", "lr", "=", "lr", "[", "i", "]", "if", "lr_not_scalar", "else", "lr", ",", "grad", "=", "g", ",", "var", "=", "v", ")", ")", "\n", "for", "i", ",", "(", "v", ",", "g", ")", "in", "enumerate", "(", "zip", "(", "trainables", ",", "self", ".", "grads", ")", ")", "]", "\n", "_", "=", "self", ".", "optimize", "if", "reports", "is", "None", "else", "[", "*", "vars", "(", "reports", ")", ".", "values", "(", ")", ",", "*", "self", ".", "optimize", "]", "\n", "self", ".", "run_optimize", "=", "lambda", "feed_dict", ":", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "_", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n", "# Function to compute the PPO gradients", "\n", "", "", "self", ".", "run_grads", "=", "lambda", "*", ",", "feed_dict", ":", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "[", "_grads", "]", ",", "feed_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.ppo2.path_to_feed_dict": [[127, 153], ["hasattr", "paths[].reshape", "paths[].reshape", "phi.reshape", "paths[].reshape", "paths[].reshape", "paths[].reshape"], "function", ["None"], ["", "", "def", "path_to_feed_dict", "(", "*", ",", "inputs", ":", "Inputs", ",", "paths", ",", "lr", "=", "None", ",", "clip_range", ",", "**", "_r", ")", ":", "\n", "    ", "if", "'adv'", "in", "paths", ":", "\n", "        ", "phi", "=", "paths", "[", "'advs'", "]", "\n", "", "elif", "'values'", "in", "paths", ":", "\n", "        ", "phi", "=", "paths", "[", "'returns'", "]", "-", "paths", "[", "'values'", "]", "\n", "", "else", ":", "\n", "        ", "phi", "=", "paths", "[", "'returns'", "]", "\n", "# advs_normalized = (advs - advs.mean()) / (advs.std() + 1e-8)", "\n", "\n", "", "n_timesteps", ",", "n_envs", ",", "*", "_", "=", "paths", "[", "'obs'", "]", ".", "shape", "\n", "n", "=", "n_timesteps", "*", "n_envs", "\n", "\n", "feed_dict", "=", "{", "\n", "inputs", ".", "X", ":", "paths", "[", "'obs'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", ",", "\n", "inputs", ".", "A", ":", "paths", "[", "'acs'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", ",", "\n", "inputs", ".", "ADV", ":", "phi", ".", "reshape", "(", "-", "1", ")", ",", "\n", "inputs", ".", "OLD_NEG_LOG_P_AC", ":", "paths", "[", "'neglogpacs'", "]", ".", "reshape", "(", "-", "1", ")", ",", "\n", "inputs", ".", "CLIP_RANGE", ":", "clip_range", "\n", "}", "\n", "if", "hasattr", "(", "inputs", ",", "'OLD_V_PRED'", ")", ":", "\n", "        ", "feed_dict", "[", "inputs", ".", "OLD_V_PRED", "]", "=", "paths", "[", "'values'", "]", ".", "reshape", "(", "-", "1", ")", "\n", "feed_dict", "[", "inputs", ".", "R", "]", "=", "paths", "[", "'returns'", "]", ".", "reshape", "(", "-", "1", ")", "\n", "", "if", "lr", "is", "not", "None", ":", "\n", "        ", "assert", "inputs", ".", "LR", "is", "not", "None", ",", "f'Input should have LR attribute if a learning rate is passed.'", "\n", "feed_dict", "[", "inputs", ".", "LR", "]", "=", "lr", "\n", "", "return", "feed_dict", "\n", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.ReplayBuffer.__init__": [[19, 27], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "size", ")", ":", "\n", "        ", "import", "numpy", "as", "np", "\n", "self", ".", "obs1_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "obs_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "obs2_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "obs_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "acts_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "act_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "rews_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "ptr", ",", "self", ".", "size", ",", "self", ".", "max_size", "=", "0", ",", "0", ",", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.ReplayBuffer.store": [[28, 36], ["min"], "methods", ["None"], ["", "def", "store", "(", "self", ",", "obs", ",", "act", ",", "rew", ",", "next_obs", ",", "done", ")", ":", "\n", "        ", "self", ".", "obs1_buf", "[", "self", ".", "ptr", "]", "=", "obs", "\n", "self", ".", "obs2_buf", "[", "self", ".", "ptr", "]", "=", "next_obs", "\n", "self", ".", "acts_buf", "[", "self", ".", "ptr", "]", "=", "act", "\n", "self", ".", "rews_buf", "[", "self", ".", "ptr", "]", "=", "rew", "\n", "self", ".", "done_buf", "[", "self", ".", "ptr", "]", "=", "done", "\n", "self", ".", "ptr", "=", "(", "self", ".", "ptr", "+", "1", ")", "%", "self", ".", "max_size", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "max_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.ReplayBuffer.sample_batch": [[37, 44], ["numpy.random.randint", "dict"], "methods", ["None"], ["", "def", "sample_batch", "(", "self", ",", "batch_size", "=", "32", ")", ":", "\n", "        ", "idxs", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "size", ",", "size", "=", "batch_size", ")", "\n", "return", "dict", "(", "obs", "=", "self", ".", "obs1_buf", "[", "idxs", "]", ",", "\n", "obs_next", "=", "self", ".", "obs2_buf", "[", "idxs", "]", ",", "\n", "acs", "=", "self", ".", "acts_buf", "[", "idxs", "]", ",", "\n", "rews", "=", "self", ".", "rews_buf", "[", "idxs", "]", ",", "\n", "dones", "=", "self", ".", "done_buf", "[", "idxs", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.Inputs.__init__": [[51, 62], ["tensorflow.placeholder", "tensorflow.placeholder", "isinstance", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "list"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "*", ",", "ob_shape", ",", "action_space", ",", ")", ":", "\n", "        ", "self", ".", "X", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "ob_shape", ",", "name", "=", "'obs'", ")", "\n", "self", ".", "X_NEXT", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "ob_shape", ",", "name", "=", "'obs'", ")", "\n", "\n", "if", "isinstance", "(", "action_space", ",", "spaces", ".", "Discrete", ")", ":", "\n", "            ", "self", ".", "A", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "name", "=", "\"A\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "A", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", "+", "list", "(", "action_space", ".", "shape", ")", ",", "name", "=", "\"A\"", ")", "\n", "\n", "", "self", ".", "R", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"R\"", ")", "\n", "self", ".", "DONE", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"DONE\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.Critic.__init__": [[71, 119], ["sac.mlp", "tensorflow.squeeze", "tensorflow.variable_scope", "tensorflow.concat", "tensorflow.concat", "tensorflow.trainable_variables", "tensorflow.trainable_variables", "TypeError", "tensorflow.variable_scope", "sac.Critic.__init__.vf_mlp"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.mlp"], ["    ", "def", "__init__", "(", "self", ",", "inputs", ",", "pi", ",", "hidden_sizes", "=", "(", "400", ",", "200", ")", ",", "activation", "=", "'relu'", ",", "scope", "=", "'Critic'", ",", "reuse", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n\n        :param X: placehodler for th\n        :param A: placeholder for the sampled actions\n        :param pi: the reparameterized action\n        :param hidden_sizes:\n        :param activation:\n        :param scope:\n        :param reuse:\n        \"\"\"", "\n", "if", "activation", "==", "'tanh'", ":", "\n", "            ", "act", "=", "tf", ".", "tanh", "\n", "", "elif", "activation", "==", "\"relu\"", ":", "\n", "            ", "act", "=", "tf", ".", "nn", ".", "relu", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "f\"{activation} is not available in this MLP.\"", ")", "\n", "\n", "", "def", "vf_mlp", "(", "x", ")", ":", "\n", "            ", "_", "=", "mlp", "(", "x", ",", "[", "*", "hidden_sizes", ",", "1", "]", ",", "act", ",", "None", ")", "\n", "return", "tf", ".", "squeeze", "(", "_", ",", "1", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", ")", ":", "\n", "# note: allow passing in trainables.", "\n", "            ", "_old_trainables", "=", "{", "*", "tf", ".", "trainable_variables", "(", ")", "}", "\n", "\n", "x_a_", "=", "tf", ".", "concat", "(", "[", "inputs", ".", "X", ",", "inputs", ".", "A", "]", ",", "-", "1", ")", "\n", "x_pi_", "=", "tf", ".", "concat", "(", "[", "inputs", ".", "X", ",", "pi", "]", ",", "-", "1", ")", "\n", "with", "tf", ".", "variable_scope", "(", "'Q_0'", ",", "reuse", "=", "reuse", ")", ":", "\n", "                ", "self", ".", "q_0", "=", "vf_mlp", "(", "x_a_", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'Q_0'", ",", "reuse", "=", "True", ")", ":", "\n", "                ", "self", ".", "q_0_pi", "=", "vf_mlp", "(", "x_pi_", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'Q_1'", ",", "reuse", "=", "reuse", ")", ":", "\n", "                ", "self", ".", "q_1", "=", "vf_mlp", "(", "x_a_", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'Q_1'", ",", "reuse", "=", "True", ")", ":", "\n", "                ", "self", ".", "q_1_pi", "=", "vf_mlp", "(", "x_pi_", ")", "\n", "\n", "", "_", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "with", "tf", ".", "variable_scope", "(", "'v'", ",", "reuse", "=", "reuse", ")", ":", "\n", "                ", "self", ".", "v", "=", "vf_mlp", "(", "inputs", ".", "X", ")", "\n", "", "self", ".", "v_trainables", "=", "[", "v", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "if", "v", "not", "in", "_", "]", "\n", "\n", "self", ".", "trainables", "=", "[", "v", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "if", "v", "not", "in", "_old_trainables", "]", "\n", "\n", "_", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "with", "tf", ".", "variable_scope", "(", "'v_target'", ",", "reuse", "=", "reuse", ")", ":", "\n", "                ", "self", ".", "v_targ", "=", "vf_mlp", "(", "inputs", ".", "X_NEXT", ")", "\n", "", "self", ".", "v_targ_trainables", "=", "[", "v", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "if", "v", "not", "in", "_", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.SAC.__init__": [[133, 162], ["tensorflow.variable_scope", "tensorflow.minimum", "tensorflow.stop_gradient", "tensorflow.stop_gradient", "tensorflow.reduce_mean", "sac.Reports", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.assign", "tensorflow.square", "tensorflow.square", "tensorflow.square", "zip", "tensorflow.reduce_mean"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "*", ",", "inputs", ":", "Inputs", ",", "policy", ":", "MlpPolicy", ",", "critic", ":", "Critic", ",", "polyak", ",", "ent_coef", ",", "gamma", ")", ":", "\n", "        ", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "policy", "=", "policy", "\n", "self", ".", "critic", "=", "critic", "\n", "with", "tf", ".", "variable_scope", "(", "'SAC'", ")", ":", "\n", "            ", "min_q_pi", "=", "tf", ".", "minimum", "(", "critic", ".", "q_0_pi", ",", "critic", ".", "q_0_pi", ")", "\n", "q_backup", "=", "tf", ".", "stop_gradient", "(", "inputs", ".", "R", "+", "gamma", "*", "(", "1", "-", "inputs", ".", "DONE", ")", "*", "critic", ".", "v_targ", ")", "\n", "v_backup", "=", "tf", ".", "stop_gradient", "(", "min_q_pi", "-", "ent_coef", "*", "policy", ".", "logpac", ")", "\n", "\n", "# this first term is using the Q function as an energy model to compute the KL divergence between", "\n", "# the policy distribution and the distribution from the Q function (critic)", "\n", "self", ".", "pi_loss", "=", "tf", ".", "reduce_mean", "(", "ent_coef", "*", "policy", ".", "logpac", "-", "critic", ".", "q_0_pi", ")", "\n", "q0_loss", "=", "0.5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "q_backup", "-", "critic", ".", "q_0", ")", ")", "\n", "q1_loss", "=", "0.5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "q_backup", "-", "critic", ".", "q_1", ")", ")", "\n", "v_loss", "=", "0.5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "v_backup", "-", "critic", ".", "v", ")", ")", "\n", "self", ".", "value_loss", "=", "q0_loss", "+", "q1_loss", "+", "v_loss", "\n", "\n", "self", ".", "update_v_targ_ops", "=", "[", "tf", ".", "assign", "(", "v_targ", ",", "polyak", "*", "v_targ", "+", "(", "1", "-", "polyak", ")", "*", "v_main", ")", "\n", "for", "v_main", ",", "v_targ", "in", "zip", "(", "critic", ".", "v_trainables", ",", "critic", ".", "v_targ_trainables", ")", "]", "\n", "\n", "# entropy = tf.reduce_mean(policy.pd.entropy())", "\n", "self", ".", "reports", "=", "Reports", "(", "\n", "value_loss", "=", "self", ".", "value_loss", ",", "\n", "pi_kl", "=", "self", ".", "pi_loss", ",", "\n", "q0_loss", "=", "q0_loss", ",", "\n", "q1_loss", "=", "q1_loss", ",", "\n", "v_loss", "=", "v_loss", ",", "\n", "entropy", "=", "policy", ".", "entropy", ",", "\n", "act_norm", "=", "tf", ".", "reduce_mean", "(", "inputs", ".", "A", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.Optimize.__init__": [[169, 195], ["tensorflow.variable_scope", "tensorflow.gradients", "tensorflow.gradients", "hasattr", "v.assign", "tensorflow.control_dependencies", "tensorflow.get_default_session().run", "hasattr", "len", "isinstance", "len", "sac.Optimize.apply_grad", "enumerate", "v.assign", "zip", "sac.Optimize.apply_grad", "enumerate", "vars().values", "tensorflow.get_default_session", "zip", "vars"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "policy_loss", ",", "policy_trainables", ",", "critic_loss", ",", "critic_trainables", ",", "lr", "=", "None", ",", "reports", "=", "None", ",", "**", "_", ")", ":", "\n", "        ", "\"\"\"\n        :param trainables: Optional array used for the gradient calculation\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'SAC_Optimize'", ")", ":", "\n", "# Note: optimizer.gradients is just a wrapper around tf.gradients, with extra assertions. This is why it", "\n", "#  raises errors on non-trainables.", "\n", "            ", "self", ".", "policy_grads", "=", "tf", ".", "gradients", "(", "policy_loss", ",", "policy_trainables", ")", "\n", "self", ".", "critic_grads", "=", "tf", ".", "gradients", "(", "critic_loss", ",", "critic_trainables", ")", "\n", "\n", "# graph operator for updating the parameter. used by maml with the SGD inner step", "\n", "self", ".", "apply_grad", "=", "lambda", "*", ",", "lr", ",", "grad", ",", "var", ":", "var", "-", "lr", "*", "grad", "\n", "\n", "if", "lr", "is", "not", "None", ":", "# this is only called when we use this algo inside MAML, with SGD inner step.", "\n", "# todo: not used, not tested, but should be correct.", "\n", "                ", "assert", "hasattr", "(", "policy_trainables", "[", "0", "]", ",", "'_variable'", ")", ",", "\"trainables have to have the _variable attribute\"", "\n", "lr_not_scalar", "=", "(", "hasattr", "(", "lr", ",", "'shape'", ")", "and", "len", "(", "lr", ".", "shape", ")", ")", "or", "(", "isinstance", "(", "lr", ",", "Sequence", ")", "and", "len", "(", "lr", ")", ")", "\n", "pi_opt_op", "=", "[", "v", ".", "assign", "(", "self", ".", "apply_grad", "(", "lr", "=", "lr", "[", "i", "]", "if", "lr_not_scalar", "else", "lr", ",", "grad", "=", "g", ",", "var", "=", "v", ")", ")", "\n", "for", "i", ",", "(", "v", ",", "g", ")", "in", "enumerate", "(", "zip", "(", "policy_trainables", ",", "self", ".", "policy_grads", ")", ")", "]", "\n", "\n", "with", "tf", ".", "control_dependencies", "(", "pi_opt_op", ")", ":", "\n", "                    ", "self", ".", "optimize", "=", "[", "v", ".", "assign", "(", "self", ".", "apply_grad", "(", "lr", "=", "lr", "[", "i", "]", "if", "lr_not_scalar", "else", "lr", ",", "grad", "=", "g", ",", "var", "=", "v", ")", ")", "\n", "for", "i", ",", "(", "v", ",", "g", ")", "in", "enumerate", "(", "zip", "(", "critic_trainables", ",", "self", ".", "critic_grads", ")", ")", "]", "\n", "\n", "", "_", "=", "self", ".", "optimize", "if", "reports", "is", "None", "else", "[", "*", "vars", "(", "reports", ")", ".", "values", "(", ")", ",", "*", "self", ".", "optimize", "]", "\n", "self", ".", "run_optimize", "=", "lambda", "feed_dict", ":", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "_", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.mlp": [[64, 68], ["tensorflow.layers.dense", "tensorflow.layers.dense"], "function", ["None"], ["", "", "def", "mlp", "(", "x", ",", "hidden_sizes", "=", "(", "32", ",", ")", ",", "activation", "=", "tf", ".", "tanh", ",", "output_activation", "=", "None", ")", ":", "\n", "    ", "for", "h", "in", "hidden_sizes", "[", ":", "-", "1", "]", ":", "\n", "        ", "x", "=", "tf", ".", "layers", ".", "dense", "(", "x", ",", "units", "=", "h", ",", "activation", "=", "activation", ")", "\n", "", "return", "tf", ".", "layers", ".", "dense", "(", "x", ",", "units", "=", "hidden_sizes", "[", "-", "1", "]", ",", "activation", "=", "output_activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.use_replay_buffer": [[200, 203], ["None"], "function", ["None"], ["def", "use_replay_buffer", "(", "buffer", ")", ":", "\n", "    ", "global", "BUFFER", "\n", "BUFFER", "=", "buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.path_to_feed_dict": [[205, 246], ["paths[].reshape", "paths[].reshape", "paths[].reshape", "paths[].reshape", "range", "buffer.sample_batch", "buffer.store"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.ReplayBuffer.sample_batch", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.sac.ReplayBuffer.store"], ["", "def", "path_to_feed_dict", "(", "*", ",", "inputs", ":", "Inputs", ",", "paths", ",", "lr", "=", "None", ",", "**", "_r", ")", ":", "\n", "    ", "\"\"\"\n    In SAC (and other value-based, non-policy gradient methods, where the policy gradient is provided\n    by the true critic), the path_to_feed_dict function is stateful and contains a\n    replay buffer.\n\n    :param inputs:\n    :param paths:\n    :param lr:\n    :param clip_range:\n    :param _r:\n    :return:\n    \"\"\"", "\n", "assert", "BUFFER", "is", "not", "None", ",", "\"BUFFER is None. You need to first setup the replay buffer\"", "\n", "\n", "buffer", "=", "BUFFER", "\n", "\n", "n_timesteps", ",", "n_envs", ",", "*", "_", "=", "paths", "[", "'obs'", "]", ".", "shape", "\n", "n", "=", "n_timesteps", "*", "n_envs", "\n", "\n", "obs", "=", "paths", "[", "'obs'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", "\n", "acs", "=", "paths", "[", "'acs'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", "\n", "rewards", "=", "paths", "[", "'rewards'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", "\n", "dones", "=", "paths", "[", "'dones'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", "\n", "\n", "for", "step", "in", "range", "(", "1", ",", "n_timesteps", ")", ":", "\n", "        ", "buffer", ".", "store", "(", "obs", "[", "step", "-", "1", "]", ",", "acs", "[", "step", "]", ",", "rewards", "[", "step", "]", ",", "obs", "[", "step", "]", ",", "dones", "[", "step", "]", ")", "\n", "\n", "", "_", "=", "buffer", ".", "sample_batch", "(", "batch_size", "=", "n", ")", "\n", "\n", "feed_dict", "=", "{", "\n", "inputs", ".", "X", ":", "_", "[", "'obs'", "]", ",", "\n", "inputs", ".", "X_NEXT", ":", "_", "[", "'obs_next'", "]", ",", "\n", "inputs", ".", "A", ":", "_", "[", "'acs'", "]", ",", "\n", "inputs", ".", "R", ":", "_", "[", "'rews'", "]", ",", "\n", "inputs", ".", "DONE", ":", "_", "[", "'dones'", "]", "\n", "}", "\n", "if", "lr", "is", "not", "None", ":", "\n", "        ", "assert", "inputs", ".", "LR", "is", "not", "None", ",", "f'Input should have LR attribute if a learning rate is passed.'", "\n", "feed_dict", "[", "inputs", ".", "LR", "]", "=", "lr", "\n", "", "return", "feed_dict", "\n", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.vpg.Inputs.__init__": [[14, 24], ["isinstance", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "list"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "*", ",", "action_space", ",", "value_baseline", "=", "False", ",", ")", ":", "\n", "# self.X = X or tf.placeholder(tf.float32, [None], name=\"obs\")", "\n", "        ", "if", "isinstance", "(", "action_space", ",", "spaces", ".", "Discrete", ")", ":", "\n", "            ", "self", ".", "A", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "name", "=", "\"A\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "A", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", "+", "list", "(", "action_space", ".", "shape", ")", ",", "name", "=", "\"A\"", ")", "\n", "", "self", ".", "ADV", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"ADV\"", ")", "\n", "\n", "if", "value_baseline", ":", "\n", "            ", "self", ".", "R", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"R\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.vpg.VPG.__init__": [[34, 54], ["tensorflow.variable_scope", "policy.pd.neglogp", "tensorflow.reduce_mean", "vpg.Reports", "tensorflow.square", "tensorflow.reduce_mean", "policy.pd.entropy"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.neglogp", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.entropy"], ["def", "__init__", "(", "self", ",", "*", ",", "inputs", ",", "policy", ",", "vf_coef", "=", "None", ")", ":", "\n", "        ", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "policy", "=", "policy", "\n", "with", "tf", ".", "variable_scope", "(", "\"VPG\"", ")", ":", "\n", "            ", "self", ".", "neglogpac", "=", "policy", ".", "pd", ".", "neglogp", "(", "inputs", ".", "A", ")", "\n", "\n", "self", ".", "vpg_loss", "=", "tf", ".", "reduce_mean", "(", "inputs", ".", "ADV", "*", "self", ".", "neglogpac", ")", "\n", "self", ".", "loss", "=", "self", ".", "vpg_loss", "# <== this is the value function loss ratio.", "\n", "\n", "if", "policy", ".", "vf", "is", "not", "None", ":", "\n", "                ", "self", ".", "vf_loss", "=", "tf", ".", "square", "(", "policy", ".", "vf", "-", "inputs", ".", "R", ")", "\n", "self", ".", "loss", "+=", "self", ".", "vf_loss", "*", "vf_coef", "\n", "# used for reporting", "\n", "", "self", ".", "reports", "=", "Reports", "(", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "entropy", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "entropy", "(", ")", ")", ",", "\n", "# approx_kl=.5 * tf.reduce_mean(tf.square(self.neglogpac - inputs.OLD_NEG_LOG_P_AC))", "\n", ")", "\n", "if", "policy", ".", "vf", "is", "not", "None", ":", "\n", "                ", "self", ".", "reports", ".", "vf_loss", "=", "self", ".", "vf_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.vpg.Optimize.__init__": [[60, 114], ["tensorflow.variable_scope", "tensorflow.gradients", "tensorflow.get_default_session().run", "hasattr", "v.assign", "tensorflow.get_default_session().run", "tensorflow.get_default_session", "tensorflow.zeros_like", "zip", "tensorflow.stop_gradient", "tensorflow.clip_by_value", "hasattr", "len", "isinstance", "len", "vpg.Optimize.apply_grad", "enumerate", "zip", "vars().values", "tensorflow.get_default_session", "tensorflow.maximum", "tensorflow.norm", "vars"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "*", ",", "loss", ",", "trainables", ",", "lr", "=", "None", ",", "max_grad_norm", "=", "None", ",", "max_grad_clip", "=", "None", ",", "optimizer", "=", "\"SGD\"", ",", "\n", "strict", "=", "None", ",", "\n", "reports", "=", "None", ",", "**", "_", ")", ":", "\n", "        ", "\"\"\"\n        If lr is None, do not create the self.optimize operator.\n\n        :param loss:\n        :param trainables:\n        :param lr:\n        :param max_grad_norm:\n        :param max_grad_clip:\n        :param optimizer:\n        :param strict:\n        :param reports:\n        :param _:\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'VPG_Optimize'", ")", ":", "\n", "# optimizer.gradients is just a wrapper around tf.gradients, with extra assertions. This is why it raises", "\n", "# errors on non-trainables.", "\n", "            ", "_grads", "=", "tf", ".", "gradients", "(", "loss", ",", "trainables", ")", "\n", "if", "strict", ":", "\n", "                ", "for", "g", "in", "_grads", ":", "\n", "                    ", "assert", "g", "is", "not", "None", ",", "f'Some Grads are not defined: {_grads}'", "\n", "", "", "else", ":", "\n", "                ", "_grads", "=", "[", "tf", ".", "zeros_like", "(", "p", ")", "if", "g", "is", "None", "else", "g", "for", "g", ",", "p", "in", "zip", "(", "_grads", ",", "trainables", ")", "]", "\n", "\n", "", "assert", "(", "not", "max_grad_norm", "or", "not", "max_grad_clip", ")", ",", "f'max_grad_norm({max_grad_clip}) and max_grad_norm({max_grad_clip}) can not be trueful at the same time.'", "\n", "if", "max_grad_norm", ":", "# allow 0 to be by-pass", "\n", "# print('setting max-grad-norm to', max_grad_norm)", "\n", "# tf.clip_by_global_norm is just fine. No need to use my own.", "\n", "                ", "_grads", "=", "[", "g", "*", "tf", ".", "stop_gradient", "(", "max_grad_norm", "/", "tf", ".", "maximum", "(", "max_grad_norm", ",", "tf", ".", "norm", "(", "g", ")", ")", ")", "for", "g", "in", "_grads", "]", "\n", "# _grads, grad_norm = tf.clip_by_global_norm(_grads, max_grad_norm)", "\n", "", "elif", "max_grad_clip", ":", "\n", "                ", "_grads", "=", "[", "tf", ".", "clip_by_value", "(", "g", ",", "-", "max_grad_clip", ",", "max_grad_clip", ")", "for", "g", "in", "_grads", "]", "\n", "\n", "", "self", ".", "grads", "=", "_grads", "\n", "\n", "# beta = tf.get_variable('RMSProp_beta')", "\n", "# avg_grad = tf.get_variable('RMSProp_avg_g')", "\n", "# avg_grad = beta * avg_grad + (1 - beta) * grad", "\n", "# graph operator for updating the parameter. used by maml with the SGD inner step", "\n", "self", ".", "apply_grad", "=", "lambda", "*", ",", "lr", ",", "grad", ",", "var", ":", "var", "-", "lr", "*", "grad", "\n", "\n", "if", "lr", "is", "not", "None", ":", "\n", "                ", "assert", "hasattr", "(", "trainables", "[", "0", "]", ",", "'_variable'", ")", ",", "\"trainables have to have the _variable attribute\"", "\n", "lr_not_scalar", "=", "(", "hasattr", "(", "lr", ",", "'shape'", ")", "and", "len", "(", "lr", ".", "shape", ")", ")", "or", "(", "isinstance", "(", "lr", ",", "Sequence", ")", "and", "len", "(", "lr", ")", ")", "\n", "self", ".", "optimize", "=", "[", "v", ".", "assign", "(", "self", ".", "apply_grad", "(", "lr", "=", "lr", "[", "i", "]", "if", "lr_not_scalar", "else", "lr", ",", "grad", "=", "g", ",", "var", "=", "v", ")", ")", "\n", "for", "i", ",", "(", "v", ",", "g", ")", "in", "enumerate", "(", "zip", "(", "trainables", ",", "self", ".", "grads", ")", ")", "]", "\n", "_", "=", "self", ".", "optimize", "if", "reports", "is", "None", "else", "[", "*", "vars", "(", "reports", ")", ".", "values", "(", ")", ",", "*", "self", ".", "optimize", "]", "\n", "self", ".", "run_optimize", "=", "lambda", "feed_dict", ":", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "_", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n", "# Function to compute the PPO gradients", "\n", "", "", "self", ".", "run_grads", "=", "lambda", "*", ",", "feed_dict", ":", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "[", "_grads", "]", ",", "feed_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.vpg.path_to_feed_dict": [[116, 140], ["hasattr", "paths[].reshape", "paths[].reshape", "phi.reshape", "paths[].reshape"], "function", ["None"], ["", "", "def", "path_to_feed_dict", "(", "*", ",", "inputs", ":", "Inputs", ",", "paths", ",", "lr", "=", "None", ",", "**", "_r", ")", ":", "\n", "    ", "if", "'adv'", "in", "paths", ":", "\n", "        ", "phi", "=", "paths", "[", "'advs'", "]", "\n", "", "elif", "'values'", "in", "paths", ":", "\n", "        ", "phi", "=", "paths", "[", "'returns'", "]", "-", "paths", "[", "'values'", "]", "\n", "", "else", ":", "\n", "        ", "phi", "=", "paths", "[", "'returns'", "]", "\n", "# advs_normalized = (advs - advs.mean()) / (advs.std() + 1e-8)", "\n", "\n", "", "n_timesteps", ",", "n_envs", ",", "*", "_", "=", "paths", "[", "'obs'", "]", ".", "shape", "\n", "n", "=", "n_timesteps", "*", "n_envs", "\n", "\n", "feed_dict", "=", "{", "\n", "inputs", ".", "X", ":", "paths", "[", "'obs'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", ",", "\n", "inputs", ".", "A", ":", "paths", "[", "'acs'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", ",", "\n", "inputs", ".", "ADV", ":", "phi", ".", "reshape", "(", "-", "1", ")", ",", "\n", "}", "\n", "\n", "if", "hasattr", "(", "inputs", ",", "'R'", ")", ":", "\n", "        ", "feed_dict", "[", "inputs", ".", "R", "]", "=", "paths", "[", "'returns'", "]", ".", "reshape", "(", "-", "1", ")", "\n", "", "if", "lr", "is", "not", "None", ":", "\n", "        ", "assert", "inputs", ".", "LR", "is", "not", "None", ",", "f'Input should have LR attribute if a learning rate is passed.'", "\n", "feed_dict", "[", "inputs", ".", "LR", "]", "=", "lr", "\n", "", "return", "feed_dict", "\n", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.Inputs.__init__": [[22, 29], ["isinstance", "tensorflow.placeholder", "tensorflow.placeholder", "list"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "*", ",", "action_space", ",", "type", "=", "None", ")", ":", "\n", "        ", "if", "type", "in", "[", "LOSS_TYPES", ".", "two_headed_BC", ",", "LOSS_TYPES", ".", "learned_loss_exp_act", ",", "LOSS_TYPES", ".", "learned_loss_deep", ",", "\n", "LOSS_TYPES", ".", "learned_loss_exp_act_deep", "]", ":", "\n", "            ", "if", "isinstance", "(", "action_space", ",", "spaces", ".", "Discrete", ")", ":", "\n", "                ", "self", ".", "A", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "name", "=", "\"A\"", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "A", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", "+", "list", "(", "action_space", ".", "shape", ")", ",", "name", "=", "\"A\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.BCLearnedLoss.__init__": [[59, 143], ["tensorflow.variable_scope", "bc_learned_loss.fc", "tensorflow.reduce_mean", "bc_learned_loss.Reports", "policy.pd.neglogp", "bc_learned_loss.fc", "tensorflow.reduce_mean", "bc_learned_loss.Reports", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.concat", "bc_learned_loss.fc", "tensorflow.reduce_mean", "bc_learned_loss.Reports", "policy.pd.entropy", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.concat", "tensorflow.concat", "bc_learned_loss.fc", "tensorflow.reduce_mean", "bc_learned_loss.Reports", "policy.pd.entropy", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "bc_learned_loss.Reports", "policy.pd.entropy", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.variable_scope", "tensorflow.concat", "bc_learned_loss.fc", "bc_learned_loss.fc", "tensorflow.reduce_mean", "bc_learned_loss.Reports", "policy.pd.entropy", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.variable_scope", "tensorflow.concat", "bc_learned_loss.fc", "tensorflow.reduce_mean", "bc_learned_loss.Reports", "policy.pd.entropy", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.variable_scope", "tensorflow.concat", "bc_learned_loss.fc", "bc_learned_loss.fc", "tensorflow.reduce_mean", "policy.pd.entropy", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "policy.pd.entropy"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.neglogp", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.entropy", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.entropy", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.entropy", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.entropy", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.entropy", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc", "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.entropy", "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.distributions.SquashedDiagGaussianPd.entropy"], ["    ", "def", "__init__", "(", "self", ",", "*", ",", "inputs", ":", "Inputs", ",", "policy", ",", "type", ":", "str", ")", ":", "\n", "        ", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "policy", "=", "policy", "\n", "with", "tf", ".", "variable_scope", "(", "'BCLearnedLoss'", ")", ":", "\n", "            ", "act_dim", "=", "policy", ".", "pd", ".", "mean", ".", "shape", "[", "-", "1", "]", "\n", "if", "type", "==", "LOSS_TYPES", ".", "surrogate_target", ":", "\n", "# learned loss. Use identity function as the activation", "\n", "                ", "surrogate_action", "=", "fc", "(", "policy", ".", "h_", ",", "'surrogate_action_target'", ",", "nh", "=", "act_dim", ",", "act", "=", "lambda", "x", ":", "x", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "neglogp", "(", "surrogate_action", ")", ")", "# equivalent to L2 loss", "\n", "self", ".", "reports", "=", "Reports", "(", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "act_norm", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "mean", ")", ",", "\n", "surrogate_act_norm", "=", "tf", ".", "reduce_mean", "(", "surrogate_action", ")", ",", "\n", "entropy", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "entropy", "(", ")", ")", "\n", ")", "\n", "", "elif", "type", "==", "LOSS_TYPES", ".", "a2_target", ":", "\n", "# two headed architecture. The policy head is not BC trained.", "\n", "                ", "action", "=", "fc", "(", "tf", ".", "concat", "(", "[", "policy", ".", "h_", ",", "policy", ".", "pd", ".", "mean", "]", ",", "-", "1", ")", ",", "'bc-surrogate-head'", ",", "nh", "=", "act_dim", ",", "\n", "act", "=", "lambda", "x", ":", "x", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "action", "**", "2", ")", "\n", "self", ".", "reports", "=", "Reports", "(", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "act_norm", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "mean", ")", ",", "\n", "surrogate_act_norm", "=", "tf", ".", "reduce_mean", "(", "action", ")", ",", "\n", "entropy", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "entropy", "(", ")", ")", "\n", ")", "\n", "", "elif", "type", "==", "LOSS_TYPES", ".", "two_headed_BC", ":", "\n", "# two headed architecture. The policy head is not BC trained.", "\n", "# Requires a BC action input", "\n", "                ", "surrogate_action", "=", "fc", "(", "tf", ".", "concat", "(", "[", "policy", ".", "h_", ",", "policy", ".", "pd", ".", "mean", "]", ",", "-", "1", ")", ",", "'surrogate_loss'", ",", "nh", "=", "1", ",", "\n", "act", "=", "lambda", "x", ":", "x", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "surrogate_action", "-", "inputs", ".", "A", ")", "\n", "self", ".", "reports", "=", "Reports", "(", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "act_norm", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "mean", ")", ",", "\n", "entropy", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "entropy", "(", ")", ")", ",", "\n", "surrogate_act_norm", "=", "tf", ".", "reduce_mean", "(", "surrogate_action", ")", ",", "\n", "expert_act_norm", "=", "tf", ".", "reduce_mean", "(", "inputs", ".", "A", ")", ",", "\n", ")", "\n", "", "elif", "type", "==", "LOSS_TYPES", ".", "learned_loss", ":", "\n", "                ", "_", "=", "tf", ".", "concat", "(", "[", "inputs", ".", "X", ",", "policy", ".", "pd", ".", "mean", "]", ",", "-", "1", ")", "\n", "_", "=", "fc", "(", "_", ",", "'learned_loss'", ",", "nh", "=", "1", ",", "act", "=", "lambda", "x", ":", "x", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "_", "**", "2", ")", "# effectively", "\n", "self", ".", "reports", "=", "Reports", "(", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "act_norm", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "mean", ")", ",", "\n", "entropy", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "entropy", "(", ")", ")", "\n", ")", "\n", "", "elif", "type", "==", "LOSS_TYPES", ".", "learned_loss_deep", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "'learned_loss'", ")", ":", "\n", "                    ", "_", "=", "tf", ".", "concat", "(", "[", "inputs", ".", "X", ",", "policy", ".", "pd", ".", "mean", "]", ",", "-", "1", ")", "\n", "_", "=", "fc", "(", "_", ",", "'layer_1'", ",", "nh", "=", "64", ")", "\n", "_", "=", "fc", "(", "_", ",", "'layer_2'", ",", "nh", "=", "1", ",", "act", "=", "lambda", "x", ":", "x", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "_", "**", "2", ")", "# effectively", "\n", "", "self", ".", "reports", "=", "Reports", "(", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "act_norm", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "mean", ")", ",", "\n", "entropy", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "entropy", "(", ")", ")", "\n", ")", "\n", "", "elif", "type", "==", "LOSS_TYPES", ".", "learned_loss_exp_act", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "'learned_loss'", ")", ":", "\n", "                    ", "_", "=", "tf", ".", "concat", "(", "[", "inputs", ".", "X", ",", "inputs", ".", "A", ",", "policy", ".", "pd", ".", "mean", "]", ",", "-", "1", ")", "\n", "_", "=", "fc", "(", "_", ",", "'learned_loss'", ",", "nh", "=", "1", ",", "act", "=", "lambda", "x", ":", "x", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "_", "**", "2", ")", "# effectively", "\n", "", "self", ".", "reports", "=", "Reports", "(", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "expert_act_norm", "=", "tf", ".", "reduce_mean", "(", "inputs", ".", "A", ")", ",", "\n", "act_norm", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "mean", ")", ",", "\n", "entropy", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "entropy", "(", ")", ")", "\n", ")", "\n", "", "elif", "type", "==", "LOSS_TYPES", ".", "learned_loss_exp_act_deep", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "'learned_loss'", ")", ":", "\n", "                    ", "_", "=", "tf", ".", "concat", "(", "[", "inputs", ".", "X", ",", "inputs", ".", "A", ",", "policy", ".", "pd", ".", "mean", "]", ",", "-", "1", ")", "\n", "_", "=", "fc", "(", "_", ",", "'layer_1'", ",", "nh", "=", "64", ")", "\n", "_", "=", "fc", "(", "_", ",", "'layer_2'", ",", "nh", "=", "1", ",", "act", "=", "lambda", "x", ":", "x", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "_", "**", "2", ")", "# effectively", "\n", "", "self", ".", "reports", "=", "Reports", "(", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "expert_act_norm", "=", "tf", ".", "reduce_mean", "(", "inputs", ".", "A", ")", ",", "\n", "act_norm", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "mean", ")", ",", "\n", "entropy", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "pd", ".", "entropy", "(", ")", ")", "\n", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplemented", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc": [[48, 56], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "act", "tensorflow.matmul", "x.get_shape", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.constant_initializer"], "function", ["None"], ["", "def", "fc", "(", "x", ",", "scope", ",", "nh", ",", "act", "=", "tf", ".", "nn", ".", "relu", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "scope", ")", ":", "\n", "        ", "nin", "=", "x", ".", "get_shape", "(", ")", "[", "-", "1", "]", ".", "value", "# can take batched or individual tensors.", "\n", "w", "=", "tf", ".", "get_variable", "(", "\"w\"", ",", "[", "nin", ",", "nh", "]", ",", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "b", "=", "tf", ".", "get_variable", "(", "\"b\"", ",", "[", "nh", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ")", "\n", "z", "=", "tf", ".", "matmul", "(", "x", ",", "w", ")", "+", "b", "\n", "h", "=", "act", "(", "z", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.path_to_feed_dict": [[147, 170], ["paths[].reshape", "hasattr", "paths[].reshape"], "function", ["None"], ["", "", "", "", "def", "path_to_feed_dict", "(", "*", ",", "inputs", ":", "Inputs", ",", "paths", ",", "lr", "=", "None", ",", "**", "_rest", ")", ":", "\n", "    ", "\"\"\"\n    convert path objects to feed_dict for the tensorflow graph.\n\n    :param inputs:  Input object\n    :param paths: dict['obs', 'acs']: Size(n_timesteps, n_envs, feat_n)\n    :param lr: placeholder or floating point number\n    :param _rest:\n    :return: feed_dict, keyed by the input placeholders.\n    \"\"\"", "\n", "# reshaping the path, need to debug", "\n", "n_timesteps", ",", "n_envs", ",", "*", "_", "=", "paths", "[", "'obs'", "]", ".", "shape", "\n", "n", "=", "n_timesteps", "*", "n_envs", "\n", "\n", "feed_dict", "=", "{", "\n", "inputs", ".", "X", ":", "paths", "[", "'obs'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", ",", "\n", "}", "\n", "if", "hasattr", "(", "inputs", ",", "'A'", ")", "and", "inputs", ".", "A", "is", "not", "None", ":", "\n", "        ", "feed_dict", "[", "inputs", ".", "A", "]", "=", "paths", "[", "'acs'", "]", ".", "reshape", "(", "n", ",", "-", "1", ")", "\n", "", "if", "lr", "is", "not", "None", ":", "\n", "        ", "assert", "inputs", ".", "LR", "is", "not", "None", ",", "f'Input should have LR attribute if a learning rate is passed.'", "\n", "feed_dict", "[", "inputs", ".", "LR", "]", "=", "lr", "\n", "", "return", "feed_dict", "\n", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.patches.__getattribute__": [[5, 10], ["object.__getattribute__", "object.__getattribute__"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.patches.__getattribute__", "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.patches.__getattribute__"], ["def", "__getattribute__", "(", "self", ",", "attr_name", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "return", "object", ".", "__getattribute__", "(", "self", ",", "attr_name", ")", "\n", "", "except", "AttributeError", ":", "\n", "        ", "return", "object", ".", "__getattribute__", "(", "self", ".", "env", ",", "attr_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.patches.__getattr__": [[19, 29], ["dir", "object.__getattribute__", "numpy.stack", "remote.send", "remote.recv", "dict"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.patches.__getattribute__", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send"], ["def", "__getattr__", "(", "self", ",", "attr_name", ")", ":", "\n", "    ", "if", "attr_name", "in", "dir", "(", "self", ")", ":", "\n", "        ", "return", "object", ".", "__getattribute__", "(", "self", ",", "attr_name", ")", "\n", "", "else", ":", "\n", "        ", "def", "remote_exec", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "                ", "remote", ".", "send", "(", "(", "attr_name", ",", "dict", "(", "args", "=", "args", ",", "kwargs", "=", "kwargs", ")", ")", ")", "\n", "", "return", "np", ".", "stack", "(", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "self", ".", "remotes", "]", ")", "\n", "\n", "", "return", "remote_exec", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.patches.worker": [[34, 58], ["parent_remote.close", "env_fn_wrapper.x", "remote.recv", "env_fn_wrapper.x.step", "remote.send", "env_fn_wrapper.x.reset", "env_fn_wrapper.x.reset", "remote.send", "env_fn_wrapper.x.reset_task", "remote.send", "remote.close", "remote.send", "remote.send", "getattr"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.close", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.step", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.reset", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.reset", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.reset_task", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.close", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send"], ["def", "worker", "(", "remote", ",", "parent_remote", ",", "env_fn_wrapper", ")", ":", "\n", "    ", "parent_remote", ".", "close", "(", ")", "\n", "env", "=", "env_fn_wrapper", ".", "x", "(", ")", "\n", "while", "True", ":", "\n", "        ", "cmd", ",", "data", "=", "remote", ".", "recv", "(", ")", "\n", "if", "cmd", "==", "'step'", ":", "\n", "            ", "ob", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "data", ")", "\n", "if", "done", ":", "\n", "                ", "ob", "=", "env", ".", "reset", "(", ")", "\n", "", "remote", ".", "send", "(", "(", "ob", ",", "reward", ",", "done", ",", "info", ")", ")", "\n", "", "elif", "cmd", "==", "'reset'", ":", "\n", "            ", "ob", "=", "env", ".", "reset", "(", ")", "\n", "remote", ".", "send", "(", "ob", ")", "\n", "", "elif", "cmd", "==", "'reset_task'", ":", "\n", "            ", "ob", "=", "env", ".", "reset_task", "(", ")", "\n", "remote", ".", "send", "(", "ob", ")", "\n", "", "elif", "cmd", "==", "'close'", ":", "\n", "            ", "remote", ".", "close", "(", ")", "\n", "break", "\n", "", "elif", "cmd", "==", "'get_spaces'", ":", "\n", "            ", "remote", ".", "send", "(", "(", "env", ".", "observation_space", ",", "env", ".", "action_space", ")", ")", "\n", "", "else", ":", "\n", "# todo: to distinguish between a functional call and a getitem, this needs some more thought", "\n", "            ", "remote", ".", "send", "(", "getattr", "(", "env", ",", "cmd", ")", "(", "*", "data", "[", "'args'", "]", ",", "**", "data", "[", "'kwargs'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.patches.close": [[65, 72], ["patches..viewer.finish"], "function", ["None"], ["def", "close", "(", "self", ")", ":", "\n", "    ", "if", "self", ".", "viewer", "is", "not", "None", ":", "\n", "        ", "try", ":", "\n", "            ", "self", ".", "viewer", ".", "finish", "(", ")", "\n", "", "except", ":", "\n", "            ", "pass", "\n", "", "self", ".", "viewer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.patches.pdfromlatent": [[79, 88], ["fc", "tf.get_variable", "tf.concat", "patches..pdfromflat", "tf.zeros_initializer"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.algos.bc_learned_loss.fc"], ["def", "pdfromlatent", "(", "self", ",", "latent_vector", ",", "init_scale", "=", "1.0", ",", "init_bias", "=", "0.0", ")", ":", "\n", "    ", "import", "tensorflow", "as", "tf", "\n", "import", "numpy", "as", "np", "\n", "import", "baselines", ".", "common", ".", "tf_util", "as", "U", "\n", "from", "baselines", ".", "a2c", ".", "utils", "import", "fc", "\n", "mean", "=", "fc", "(", "latent_vector", ",", "'pi'", ",", "self", ".", "size", ",", "init_scale", "=", "init_scale", ",", "init_bias", "=", "init_bias", ")", "\n", "logstd", "=", "tf", ".", "get_variable", "(", "name", "=", "'logstd'", ",", "shape", "=", "[", "1", ",", "self", ".", "size", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "pdparam", "=", "tf", ".", "concat", "(", "[", "mean", ",", "mean", "*", "0.0", "+", "logstd", "]", ",", "axis", "=", "1", ")", "\n", "return", "self", ".", "pdfromflat", "(", "pdparam", ")", ",", "mean", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_velocity.Controls.sample": [[13, 18], ["numpy.random.uniform"], "methods", ["None"], ["def", "sample", "(", "self", ",", "goal_velocity", "=", "None", ")", ":", "\n", "        ", "if", "goal_velocity", ":", "\n", "            ", "self", ".", "goal_velocity", "=", "goal_velocity", "\n", "", "else", ":", "\n", "            ", "self", ".", "goal_velocity", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "self", ".", "low", ",", "high", "=", "self", ".", "high", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.__init__": [[25, 30], ["half_cheetah_goal_velocity.Controls", "gym.envs.mujoco.mujoco_env.MujocoEnv.__init__", "gym.utils.EzPickle.__init__"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.__init__", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "controls", "=", "Controls", "(", ")", "\n", "# call super init after initializing the variables.", "\n", "mujoco_env", ".", "MujocoEnv", ".", "__init__", "(", "self", ",", "'half_cheetah.xml'", ",", "5", ")", "\n", "utils", ".", "EzPickle", ".", "__init__", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.seed": [[31, 34], ["gym.utils.seeding.np_random"], "methods", ["None"], ["", "def", "seed", "(", "self", ",", "seed", "=", "None", ")", ":", "\n", "        ", "self", ".", "np_random", ",", "seed", "=", "seeding", ".", "np_random", "(", "seed", ")", "\n", "return", "[", "seed", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.step": [[35, 49], ["half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.do_simulation", "half_cheetah_goal_velocity.HalfCheetahGoalVelEnv._get_obs", "abs", "numpy.square().sum", "dict", "numpy.square"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._get_obs"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "# xposbefore = self.model.data.qpos[0, 0]", "\n", "# change model api to work with Mujoco1.5", "\n", "        ", "xposbefore", "=", "self", ".", "data", ".", "qpos", "[", "0", "]", "\n", "self", ".", "do_simulation", "(", "action", ",", "self", ".", "frame_skip", ")", "\n", "# xposafter = self.model.data.qpos[0, 0]", "\n", "xposafter", "=", "self", ".", "data", ".", "qpos", "[", "0", "]", "\n", "ob", "=", "self", ".", "_get_obs", "(", ")", "\n", "reward_ctrl", "=", "-", "1e-1", "*", "0.5", "*", "np", ".", "square", "(", "action", ")", ".", "sum", "(", ")", "# add factor of 0.5, ref cbfinn.", "\n", "velocity", "=", "(", "xposafter", "-", "xposbefore", ")", "/", "self", ".", "dt", "\n", "cost_run", "=", "abs", "(", "velocity", "-", "self", ".", "controls", ".", "goal_velocity", ")", "\n", "reward", "=", "reward_ctrl", "-", "cost_run", "\n", "done", "=", "False", "\n", "return", "ob", ",", "reward", ",", "done", ",", "dict", "(", "cost_run", "=", "cost_run", ",", "reward_ctrl", "=", "reward_ctrl", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_velocity.HalfCheetahGoalVelEnv._get_obs": [[50, 56], ["numpy.concatenate"], "methods", ["None"], ["", "def", "_get_obs", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "[", "\n", "# self.model.data.qpos.flat[1:],", "\n", "self", ".", "data", ".", "qpos", "[", "1", ":", "]", ",", "\n", "# self.model.data.qvel.flat,", "\n", "self", ".", "data", ".", "qvel", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.set_goal_velocity": [[58, 61], ["half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.controls.sample"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.Controls.sample"], ["", "def", "set_goal_velocity", "(", "self", ",", "goal_velocity", "=", "None", ")", ":", "\n", "# print('***** goal velocity **********>>', goal_velocity)", "\n", "        ", "self", ".", "controls", ".", "sample", "(", "goal_velocity", "=", "goal_velocity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.get_goal_velocity": [[62, 64], ["None"], "methods", ["None"], ["", "def", "get_goal_velocity", "(", "self", ")", ":", "# only for debugging", "\n", "        ", "return", "self", ".", "controls", ".", "goal_velocity", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.reset_model": [[65, 70], ["half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.set_state", "half_cheetah_goal_velocity.HalfCheetahGoalVelEnv._get_obs", "half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.np_random.uniform", "half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.np_random.randn"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._get_obs"], ["", "def", "reset_model", "(", "self", ")", ":", "\n", "        ", "qpos", "=", "self", ".", "init_qpos", "+", "self", ".", "np_random", ".", "uniform", "(", "low", "=", "-", ".1", ",", "high", "=", ".1", ",", "size", "=", "self", ".", "model", ".", "nq", ")", "\n", "qvel", "=", "self", ".", "init_qvel", "+", "self", ".", "np_random", ".", "randn", "(", "self", ".", "model", ".", "nv", ")", "*", ".1", "\n", "self", ".", "set_state", "(", "qpos", ",", "qvel", ")", "\n", "return", "self", ".", "_get_obs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_velocity.HalfCheetahGoalVelEnv.viewer_setup": [[71, 73], ["None"], "methods", ["None"], ["", "def", "viewer_setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "viewer", ".", "cam", ".", "distance", "=", "self", ".", "model", ".", "stat", ".", "extent", "*", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.Controls.sample": [[10, 15], ["numpy.random.rand"], "methods", ["None"], ["def", "sample", "(", "self", ",", "goal_direction", "=", "None", ")", ":", "\n", "        ", "if", "goal_direction", "is", "not", "None", ":", "\n", "            ", "self", ".", "goal_direction", "=", "goal_direction", "\n", "", "else", ":", "\n", "            ", "self", ".", "goal_direction", "=", "1", "if", "np", ".", "random", ".", "rand", "(", ")", ">", "0.5", "else", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.HalfCheetahGoalDirEnv.__init__": [[22, 27], ["half_cheetah_goal_direction.Controls", "gym.envs.mujoco.mujoco_env.MujocoEnv.__init__", "gym.utils.EzPickle.__init__"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.__init__", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "controls", "=", "Controls", "(", ")", "\n", "# call super init after initializing the variables.", "\n", "mujoco_env", ".", "MujocoEnv", ".", "__init__", "(", "self", ",", "'half_cheetah.xml'", ",", "5", ")", "\n", "utils", ".", "EzPickle", ".", "__init__", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.HalfCheetahGoalDirEnv.step": [[28, 42], ["half_cheetah_goal_direction.HalfCheetahGoalDirEnv.do_simulation", "half_cheetah_goal_direction.HalfCheetahGoalDirEnv._get_obs", "numpy.square().sum", "dict", "numpy.square"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._get_obs"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "# xposbefore = self.model.data.qpos[0, 0]", "\n", "# change model api to work with Mujoco1.5", "\n", "        ", "xposbefore", "=", "self", ".", "data", ".", "qpos", "[", "0", "]", "\n", "self", ".", "do_simulation", "(", "action", ",", "self", ".", "frame_skip", ")", "\n", "# xposafter = self.model.data.qpos[0, 0]", "\n", "xposafter", "=", "self", ".", "data", ".", "qpos", "[", "0", "]", "\n", "ob", "=", "self", ".", "_get_obs", "(", ")", "\n", "reward_ctrl", "=", "-", "1e-1", "*", "0.5", "*", "np", ".", "square", "(", "action", ")", ".", "sum", "(", ")", "# add factor of 0.5, ref cbfinn.", "\n", "velocity", "=", "(", "xposafter", "-", "xposbefore", ")", "/", "self", ".", "dt", "\n", "cost_run", "=", "self", ".", "controls", ".", "goal_direction", "*", "velocity", "\n", "reward", "=", "reward_ctrl", "-", "cost_run", "\n", "done", "=", "False", "\n", "return", "ob", ",", "reward", ",", "done", ",", "dict", "(", "cost_run", "=", "cost_run", ",", "reward_ctrl", "=", "reward_ctrl", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.HalfCheetahGoalDirEnv._get_obs": [[43, 49], ["numpy.concatenate"], "methods", ["None"], ["", "def", "_get_obs", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "[", "\n", "# self.model.data.qpos.flat[1:],", "\n", "self", ".", "data", ".", "qpos", "[", "1", ":", "]", ",", "\n", "# self.model.data.qvel.flat,", "\n", "self", ".", "data", ".", "qvel", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.HalfCheetahGoalDirEnv.set_goal_direction": [[51, 53], ["half_cheetah_goal_direction.HalfCheetahGoalDirEnv.controls.sample"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.Controls.sample"], ["", "def", "set_goal_direction", "(", "self", ",", "goal_direction", "=", "None", ")", ":", "\n", "        ", "self", ".", "controls", ".", "sample", "(", "goal_direction", "=", "goal_direction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.HalfCheetahGoalDirEnv.get_goal_direction": [[54, 56], ["None"], "methods", ["None"], ["", "def", "get_goal_direction", "(", "self", ")", ":", "# only for debugging", "\n", "        ", "return", "self", ".", "controls", ".", "goal_direction", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.HalfCheetahGoalDirEnv.reset_model": [[57, 62], ["half_cheetah_goal_direction.HalfCheetahGoalDirEnv.set_state", "half_cheetah_goal_direction.HalfCheetahGoalDirEnv._get_obs", "half_cheetah_goal_direction.HalfCheetahGoalDirEnv.np_random.uniform", "half_cheetah_goal_direction.HalfCheetahGoalDirEnv.np_random.randn"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._get_obs"], ["", "def", "reset_model", "(", "self", ")", ":", "\n", "        ", "qpos", "=", "self", ".", "init_qpos", "+", "self", ".", "np_random", ".", "uniform", "(", "low", "=", "-", ".1", ",", "high", "=", ".1", ",", "size", "=", "self", ".", "model", ".", "nq", ")", "\n", "qvel", "=", "self", ".", "init_qvel", "+", "self", ".", "np_random", ".", "randn", "(", "self", ".", "model", ".", "nv", ")", "*", ".1", "\n", "self", ".", "set_state", "(", "qpos", ",", "qvel", ")", "\n", "return", "self", ".", "_get_obs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.half_cheetah_goal_direction.HalfCheetahGoalDirEnv.viewer_setup": [[63, 65], ["None"], "methods", ["None"], ["", "def", "viewer_setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "viewer", ".", "cam", ".", "distance", "=", "self", ".", "model", ".", "stat", ".", "extent", "*", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._seed": [[13, 15], ["numpy.random.RandomState"], "methods", ["None"], ["def", "_seed", "(", "self", ",", "seed", ")", ":", "\n", "        ", "self", ".", "task_rng", "=", "np", ".", "random", ".", "RandomState", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv.__init__": [[16, 34], ["gym.spaces.Discrete", "gym.spaces.Box", "dict", "numpy.zeros", "copy.deepcopy", "maze_env.MazeEnv._seed", "maze_env.MazeEnv.reset_task", "gym.utils.EzPickle.__init__", "numpy.load"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._seed", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.reset_task", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.__init__"], ["", "def", "__init__", "(", "self", ",", "batch_size", ",", "path", ",", "n_episodes", "=", "5", ",", "episode_horizon", "=", "12", ",", "seed", "=", "69", ",", "num_envs", "=", "None", ")", ":", "\n", "        ", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "4", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "-", "1", ",", "1", ",", "15", ")", "# new(1), rew(1), onehot act(4), obs(9)", "\n", "self", ".", "_mazes", "=", "np", ".", "load", "(", "path", ")", "[", "'arr_0'", "]", "\n", "self", ".", "num_envs", "=", "num_envs", "\n", "self", ".", "reward_range", "=", "[", "-", "100", ",", "100", "]", "\n", "self", ".", "metadata", "=", "dict", "(", ")", "\n", "\n", "# maze id (1), ep count (1), time count (1), cur loc (2), goal(2), start(2)", "\n", "self", ".", "_state", "=", "np", ".", "zeros", "(", "[", "batch_size", ",", "9", "]", ",", "np", ".", "int32", ")", "\n", "self", ".", "_alive_cost", "=", "1.0", "/", "episode_horizon", "\n", "self", ".", "batch_size", ",", "self", ".", "n_episodes", ",", "self", ".", "episode_horizon", "=", "batch_size", ",", "n_episodes", ",", "episode_horizon", "\n", "self", ".", "_state_for_reset", "=", "copy", ".", "deepcopy", "(", "self", ".", "_state", ")", "\n", "# self.init_pos_rng = random.Random(init_pos_seed)", "\n", "self", ".", "_seed", "(", "seed", ")", "\n", "self", ".", "reset_task", "(", ")", "\n", "\n", "utils", ".", "EzPickle", ".", "__init__", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._close": [[35, 37], ["None"], "methods", ["None"], ["", "def", "_close", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._reset": [[38, 48], ["numpy.sum", "copy.deepcopy", "numpy.zeros", "numpy.zeros.squeeze", "numpy.ones", "maze_env.MazeEnv._get_obs"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._get_obs"], ["", "def", "_reset", "(", "self", ",", "dones", "=", "None", ")", ":", "\n", "        ", "if", "dones", "is", "None", ":", "\n", "            ", "dones", "=", "np", ".", "ones", "(", "self", ".", "batch_size", ",", "np", ".", "bool", ")", "\n", "\n", "", "batch_size", "=", "np", ".", "sum", "(", "dones", ")", "\n", "self", ".", "_state", "=", "copy", ".", "deepcopy", "(", "self", ".", "_state_for_reset", ")", "\n", "obs", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", ")", "+", "self", ".", "observation_space", ".", "shape", ",", "np", ".", "float32", ")", "\n", "obs", "[", ":", ",", "0", "]", "=", "1", "\n", "obs", "[", ":", ",", "6", ":", "]", "=", "self", ".", "_get_obs", "(", ")", "[", "dones", "]", "\n", "return", "obs", ".", "squeeze", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv.reset_task": [[49, 75], ["numpy.sum", "maze_env.MazeEnv.task_rng.randint", "numpy.array", "numpy.array", "numpy.array", "copy.deepcopy", "numpy.zeros", "numpy.zeros.squeeze", "numpy.ones", "len", "list", "starts.append", "goals.append", "maze_env.MazeEnv._get_obs", "zip", "numpy.where", "maze_env.MazeEnv.task_rng.randint", "maze_env.MazeEnv.task_rng.randint", "len", "len"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._get_obs"], ["", "def", "reset_task", "(", "self", ",", "dones", "=", "None", ")", ":", "\n", "        ", "if", "dones", "is", "None", ":", "\n", "            ", "dones", "=", "np", ".", "ones", "(", "self", ".", "batch_size", ",", "np", ".", "bool", ")", "\n", "\n", "", "batch_size", "=", "np", ".", "sum", "(", "dones", ")", "\n", "maze_idx", "=", "self", ".", "task_rng", ".", "randint", "(", "len", "(", "self", ".", "_mazes", ")", ",", "size", "=", "batch_size", ")", "\n", "# maze_idx = self.mazes_fixed", "\n", "starts", ",", "goals", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "maze_idx", ":", "\n", "            ", "locs", "=", "list", "(", "zip", "(", "*", "np", ".", "where", "(", "~", "self", ".", "_mazes", "[", "i", "]", ")", ")", ")", "\n", "# starts.append(self.starts_fixed)", "\n", "# goals.append(self.goals_fixed)", "\n", "starts", ".", "append", "(", "locs", "[", "self", ".", "task_rng", ".", "randint", "(", "len", "(", "locs", ")", ")", "]", ")", "\n", "goals", ".", "append", "(", "locs", "[", "self", ".", "task_rng", ".", "randint", "(", "len", "(", "locs", ")", ")", "]", ")", "\n", "\n", "", "self", ".", "_state", "[", "dones", ",", "0", "]", "=", "maze_idx", "\n", "self", ".", "_state", "[", "dones", ",", "1", ":", "3", "]", "=", "0", "\n", "self", ".", "_state", "[", "dones", ",", "3", ":", "5", "]", "=", "np", ".", "array", "(", "starts", ")", "\n", "self", ".", "_state", "[", "dones", ",", "5", ":", "7", "]", "=", "np", ".", "array", "(", "goals", ")", "\n", "self", ".", "_state", "[", "dones", ",", "7", ":", "9", "]", "=", "np", ".", "array", "(", "starts", ")", "\n", "self", ".", "_state_for_reset", "=", "copy", ".", "deepcopy", "(", "self", ".", "_state", ")", "\n", "\n", "obs", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", ")", "+", "self", ".", "observation_space", ".", "shape", ",", "np", ".", "float32", ")", "\n", "obs", "[", ":", ",", "0", "]", "=", "1", "\n", "obs", "[", ":", ",", "6", ":", "]", "=", "self", ".", "_get_obs", "(", ")", "[", "dones", "]", "\n", "return", "obs", ".", "squeeze", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._step": [[76, 99], ["numpy.equal().all", "numpy.equal", "numpy.zeros", "maze_env.MazeEnv._get_obs", "numpy.equal", "numpy.zeros.squeeze", "rewards.squeeze", "numpy.equal.squeeze", "dict", "numpy.equal", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._get_obs"], ["", "def", "_step", "(", "self", ",", "actions", ")", ":", "\n", "        ", "t", "=", "self", ".", "_state", "[", ":", ",", "2", "]", "\n", "next_loc", "=", "self", ".", "_state", "[", ":", ",", "3", ":", "5", "]", "+", "self", ".", "_action_set", "[", "actions", "]", "\n", "hit_wall", "=", "self", ".", "_mazes", "[", "self", ".", "_state", "[", ":", ",", "0", "]", ",", "next_loc", "[", ":", ",", "0", "]", ",", "next_loc", "[", ":", ",", "1", "]", "]", "\n", "\n", "self", ".", "_state", "[", "~", "hit_wall", ",", "3", ":", "5", "]", "=", "next_loc", "[", "~", "hit_wall", "]", "\n", "t", "[", ":", "]", "+=", "1", "\n", "\n", "at_goal", "=", "np", ".", "equal", "(", "self", ".", "_state", "[", ":", ",", "3", ":", "5", "]", ",", "self", ".", "_state", "[", ":", ",", "5", ":", "7", "]", ")", ".", "all", "(", "1", ")", "\n", "finished_episode", "=", "np", ".", "equal", "(", "t", ",", "self", ".", "episode_horizon", ")", "|", "at_goal", "\n", "t", "[", "finished_episode", "]", "=", "0", "\n", "self", ".", "_state", "[", "finished_episode", ",", "1", "]", "+=", "1", "\n", "self", ".", "_state", "[", "finished_episode", ",", "3", ":", "5", "]", "=", "self", ".", "_state", "[", "finished_episode", ",", "7", ":", "9", "]", "\n", "\n", "rewards", "=", "(", "1", "+", "self", ".", "_alive_cost", ")", "*", "at_goal", "-", "1e-3", "*", "hit_wall", "-", "self", ".", "_alive_cost", "\n", "dones", "=", "np", ".", "equal", "(", "self", ".", "_state", "[", ":", ",", "1", "]", ",", "self", ".", "n_episodes", ")", "\n", "\n", "obs", "=", "np", ".", "zeros", "(", "(", "self", ".", "batch_size", ",", ")", "+", "self", ".", "observation_space", ".", "shape", ",", "np", ".", "float32", ")", "\n", "obs", "[", ":", ",", "0", "]", "=", "finished_episode", "\n", "obs", "[", ":", ",", "1", "]", "=", "rewards", "\n", "obs", "[", "np", ".", "arange", "(", "1", ")", ",", "2", "+", "actions", "]", "=", "1.0", "\n", "obs", "[", ":", ",", "6", ":", "]", "=", "self", ".", "_get_obs", "(", ")", "\n", "return", "obs", ".", "squeeze", "(", ")", ",", "rewards", ".", "squeeze", "(", ")", ",", "dones", ".", "squeeze", "(", ")", ",", "dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._get_obs": [[100, 109], ["numpy.meshgrid", "maze_env.MazeEnv._mazes[].reshape", "obs[].any", "numpy.arange", "numpy.arange"], "methods", ["None"], ["", "def", "_get_obs", "(", "self", ")", ":", "\n", "        ", "x", ",", "y", "=", "self", ".", "_state", "[", ":", ",", "3", ":", "5", "]", ".", "T", "\n", "dx", ",", "dy", "=", "np", ".", "meshgrid", "(", "np", ".", "arange", "(", "-", "1", ",", "2", ")", ",", "np", ".", "arange", "(", "-", "1", ",", "2", ")", ",", "indexing", "=", "'ij'", ")", "\n", "xi", ",", "yi", "=", "x", "[", ":", ",", "None", ",", "None", "]", "+", "dx", ",", "y", "[", ":", ",", "None", ",", "None", "]", "+", "dy", "\n", "mi", "=", "self", ".", "_state", "[", ":", ",", ":", "1", ",", "None", "]", "\n", "obs", "=", "self", ".", "_mazes", "[", "mi", ",", "xi", ",", "yi", "]", ".", "reshape", "(", "(", "-", "1", ",", "9", ")", ")", "\n", "if", "obs", "[", ":", ",", "4", "]", ".", "any", "(", ")", ":", "\n", "            ", "raise", "ValueError", "\n", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.__init__": [[7, 12], ["callable", "inspect.getsource", "inspect.getsource.rstrip", "len", "inspect.getsource.rstrip", "inspect.getsource.split"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "schedule_fn", ":", "Callable", ")", ":", "\n", "        ", "assert", "callable", "(", "schedule_fn", ")", ",", "'need to pass in a real callable.'", "\n", "self", ".", "schedule_fn", "=", "schedule_fn", "\n", "source", "=", "inspect", ".", "getsource", "(", "self", ".", "schedule_fn", ")", "\n", "self", ".", "repr", "=", "\"Schedule:\\n\"", "+", "source", ".", "rstrip", "(", ")", "if", "len", "(", "source", ".", "split", "(", "'\\n'", ")", "[", "1", "]", ")", ">", "1", "else", "source", ".", "rstrip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send": [[13, 15], ["schedules.Schedule.schedule_fn"], "methods", ["None"], ["", "def", "send", "(", "self", ",", "epoch_ind", ")", ":", "\n", "        ", "return", "self", ".", "schedule_fn", "(", "epoch_ind", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.throw": [[16, 18], ["StopIteration"], "methods", ["None"], ["", "def", "throw", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "raise", "StopIteration", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.__repr__": [[19, 21], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "repr", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.__str__": [[22, 24], ["schedules.Schedule.__repr__"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.DeltaAneal.__init__": [[86, 100], ["schedules.dilated_delta", "schedules.Schedule.__init__", "dilated_delta."], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.dilated_delta", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "min", ",", "max", ",", "n", ",", "k", ")", ":", "\n", "        ", "\"\"\"Delta Anneal Scheduler,\n\n        Starting from max, goes down linearly to min, then repeat with \n\n        :param min: minimum of the parameter, to which the schedule converges to\n        :param max: maximum of the parameter, that the schedule starts with\n        :param n: the total number of epochs for this schedule\n        :param k: the number of dilated cycles.\n        :return: A dilated delta annealing schedule generator g, call g.send(ep) for the parameter value.\n        \"\"\"", "\n", "delta_fn", "=", "dilated_delta", "(", "n", ",", "k", ")", "\n", "super", "(", ")", ".", "__init__", "(", "lambda", "ep", ":", "max", "-", "(", "max", "-", "min", ")", "*", "delta_fn", "(", "ep", ")", ")", "\n", "self", ".", "repr", "=", "f\"DeltaAnneal(min={min}, max={max}, n={n}, k={k})\"", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.CosineAnneal.__init__": [[115, 130], ["schedules.dilated_delta", "schedules.Schedule.__init__", "np.cos", "dilated_delta."], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.dilated_delta", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "min", ",", "max", ",", "n", ",", "k", ")", ":", "\n", "        ", "\"\"\"Cosine Anneal Scheduler,\n\n        Starting from max, goes down as a cosine function to min, then repeat with\n\n        :param min: minimum of the parameter, to which the schedule converges to\n        :param max: maximum of the parameter, that the schedule starts with\n        :param n: the total number of epochs for this schedule\n        :param k: the number of dilated cycles.\n        :return: A dilated delta annealing schedule generator g, call g.send(ep) for the parameter value.\n        \"\"\"", "\n", "import", "numpy", "as", "np", "\n", "delta_fn", "=", "dilated_delta", "(", "n", ",", "k", ")", "\n", "super", "(", ")", ".", "__init__", "(", "lambda", "ep", ":", "min", "+", "(", "max", "-", "min", ")", "*", "0.5", "*", "(", "1", "+", "np", ".", "cos", "(", "np", ".", "pi", "*", "delta_fn", "(", "ep", ")", ")", ")", ")", "\n", "self", ".", "repr", "=", "f\"CosineAnneal(min={min}, max={max}, n={n}, k={k})\"", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.dilated_delta": [[47, 71], ["np.concatenate", "np.arange", "len", "math.floor", "zip"], "function", ["None"], ["", "def", "dilated_delta", "(", "n", ",", "k", ")", ":", "\n", "    ", "\"\"\"Dilated Delta Schedule function\n\n    returns a dilated delta function, starting with 0 and increasing, with double\n    of the duty cycle after each cycle.\n\n    :param n: total number of steps\n    :param k: number of cycles\n    :return: value between 0 and 1, in floats\n    \"\"\"", "\n", "import", "numpy", "as", "np", "\n", "import", "math", "\n", "\n", "ints", "=", "2", "**", "np", ".", "arange", "(", "k", ")", "\n", "ends", "=", "ints", "*", "2", "-", "1", "\n", "si", "=", "np", ".", "concatenate", "(", "[", "[", "(", "e", "-", "i", ",", "i", ")", "]", "*", "i", "for", "i", ",", "e", "in", "zip", "(", "ints", ",", "ends", ")", "]", ")", "\n", "schedule_ratio", "=", "n", "/", "len", "(", "si", ")", "\n", "\n", "def", "dilated_delta_fn", "(", "ep", ")", ":", "\n", "        ", "i", "=", "math", ".", "floor", "(", "ep", "/", "schedule_ratio", ")", "\n", "s", ",", "i", "=", "si", "[", "i", "]", "\n", "return", "(", "ep", "-", "schedule_ratio", "*", "s", ")", "/", "(", "schedule_ratio", "*", "i", ")", "\n", "\n", "", "return", "dilated_delta_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.zero_baseline.ZeroBaseline.get_param_values": [[6, 8], ["None"], "methods", ["None"], ["    ", "def", "get_param_values", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.zero_baseline.ZeroBaseline.set_param_values": [[9, 11], ["None"], "methods", ["None"], ["", "def", "set_param_values", "(", "self", ",", "val", ",", "**", "kwargs", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.zero_baseline.ZeroBaseline.fit": [[12, 14], ["None"], "methods", ["None"], ["", "def", "fit", "(", "self", ",", "paths", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.zero_baseline.ZeroBaseline.predict": [[15, 17], ["numpy.zeros_like"], "methods", ["None"], ["", "def", "predict", "(", "self", ",", "path", ")", ":", "\n", "        ", "return", "np", ".", "zeros_like", "(", "path", "[", "\"rewards\"", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.base.Baseline.algorithm_parallelized": [[2, 5], ["None"], "methods", ["None"], ["    ", "@", "property", "\n", "def", "algorithm_parallelized", "(", "self", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.base.Baseline.get_param_values": [[6, 8], ["None"], "methods", ["None"], ["", "def", "get_param_values", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.base.Baseline.set_param_values": [[9, 11], ["None"], "methods", ["None"], ["", "def", "set_param_values", "(", "self", ",", "val", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.base.Baseline.fit": [[12, 14], ["None"], "methods", ["None"], ["", "def", "fit", "(", "self", ",", "obs", ",", "rewards", ",", "returns", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.base.Baseline.predict": [[15, 17], ["None"], "methods", ["None"], ["", "def", "predict", "(", "self", ",", "obs", ",", "rewards", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.base.Baseline.add_args": [[18, 21], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "add_args", "(", "cls", ",", "parser", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.base.Baseline.new_from_args": [[22, 25], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "new_from_args", "(", "cls", ",", "args", ",", "mdp", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.base.Baseline.log_diagnostics": [[26, 31], ["None"], "methods", ["None"], ["", "def", "log_diagnostics", "(", "self", ",", "paths", ")", ":", "\n", "        ", "\"\"\"\n        Log extra information per iteration based on the collected paths\n        \"\"\"", "\n", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.linear_feature_baseline.LinearFeatureBaseline.__init__": [[6, 9], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "reg_coeff", "=", "1e-5", ")", ":", "\n", "        ", "self", ".", "_coeffs", "=", "None", "\n", "self", ".", "_reg_coeff", "=", "reg_coeff", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.linear_feature_baseline.LinearFeatureBaseline.get_param_values": [[10, 12], ["None"], "methods", ["None"], ["", "def", "get_param_values", "(", "self", ",", "**", "tags", ")", ":", "\n", "        ", "return", "self", ".", "_coeffs", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.linear_feature_baseline.LinearFeatureBaseline.set_param_values": [[13, 15], ["None"], "methods", ["None"], ["", "def", "set_param_values", "(", "self", ",", "val", ",", "**", "tags", ")", ":", "\n", "        ", "self", ".", "_coeffs", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.linear_feature_baseline.LinearFeatureBaseline.features": [[16, 22], ["numpy.clip", "len", "numpy.concatenate", "numpy.arange().reshape", "numpy.ones", "numpy.arange"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "features", "(", "obs", ",", "rewards", ")", ":", "\n", "        ", "o", "=", "np", ".", "clip", "(", "obs", ",", "-", "10", ",", "10", ")", "# hidden defaults are evil. -- Ge", "\n", "l", "=", "len", "(", "rewards", ")", "\n", "al", "=", "np", ".", "arange", "(", "l", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "/", "100.0", "\n", "return", "np", ".", "concatenate", "(", "[", "o", ",", "o", "**", "2", ",", "al", ",", "al", "**", "2", ",", "al", "**", "3", ",", "np", ".", "ones", "(", "(", "l", ",", "1", ")", ")", "]", ",", "axis", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.linear_feature_baseline.LinearFeatureBaseline.fit": [[23, 49], ["obs.swapaxes.swapaxes.swapaxes", "rewards.swapaxes.swapaxes.swapaxes", "numpy.concatenate", "returns.swapaxes().reshape.swapaxes().reshape.swapaxes().reshape", "range", "numpy.linalg.lstsq", "linear_feature_baseline.LinearFeatureBaseline.features", "returns.swapaxes().reshape.swapaxes().reshape.swapaxes", "numpy.concatenate.T.dot", "numpy.any", "zip", "numpy.concatenate.T.dot", "numpy.isnan", "numpy.identity"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.linear_feature_baseline.LinearFeatureBaseline.features"], ["", "def", "fit", "(", "self", ",", "obs", ",", "rewards", ",", "returns", ")", ":", "\n", "        ", "\"\"\"\n        Fits each path separately, from the state, the rewards, to the returns.\n\n        Note: The signature of this function is questionable. Why pass in the opaque paths object,\n        when we use the return, the reward, and the observation as feature?\n\n        n -> timesteps, k -> rollouts, c -> features.\n\n        :param obs: the observation with size(n, k, c)\n        :param rewards: the rewards with size(n, k)\n        :param returns: the returns with size(n, k)\n        :return: The fitted\n        \"\"\"", "\n", "obs", "=", "obs", ".", "swapaxes", "(", "0", ",", "1", ")", "\n", "rewards", "=", "rewards", ".", "swapaxes", "(", "0", ",", "1", ")", "\n", "featmat", "=", "np", ".", "concatenate", "(", "[", "self", ".", "features", "(", "ob", ",", "r", ")", "for", "ob", ",", "r", "in", "zip", "(", "obs", ",", "rewards", ")", "]", ")", "\n", "returns", "=", "returns", ".", "swapaxes", "(", "0", ",", "1", ")", ".", "reshape", "(", "-", "1", ")", "\n", "reg_coeff", "=", "self", ".", "_reg_coeff", "\n", "for", "_", "in", "range", "(", "10", ")", ":", "\n", "            ", "self", ".", "_coeffs", ",", "*", "_", "=", "np", ".", "linalg", ".", "lstsq", "(", "\n", "featmat", ".", "T", ".", "dot", "(", "featmat", ")", "+", "reg_coeff", "*", "np", ".", "identity", "(", "featmat", ".", "shape", "[", "1", "]", ")", ",", "\n", "featmat", ".", "T", ".", "dot", "(", "returns", ")", ",", "rcond", "=", "None", ",", ")", "\n", "if", "not", "np", ".", "any", "(", "np", ".", "isnan", "(", "self", ".", "_coeffs", ")", ")", ":", "\n", "                ", "break", "\n", "", "reg_coeff", "*=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.linear_feature_baseline.LinearFeatureBaseline.predict": [[50, 57], ["obs.swapaxes.swapaxes.swapaxes", "rewards.swapaxes.swapaxes.swapaxes", "numpy.concatenate", "numpy.concatenate.dot().reshape().swapaxes", "linear_feature_baseline.LinearFeatureBaseline.features", "numpy.concatenate.dot().reshape", "zip", "numpy.concatenate.dot"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.linear_feature_baseline.LinearFeatureBaseline.features"], ["", "", "def", "predict", "(", "self", ",", "obs", ",", "rewards", ")", ":", "\n", "        ", "assert", "self", ".", "_coeffs", "is", "not", "None", ",", "\"need to fit the observation and rewards first.\"", "\n", "n_timesteps", ",", "n_envs", ",", "*", "_", "=", "rewards", ".", "shape", "\n", "obs", "=", "obs", ".", "swapaxes", "(", "0", ",", "1", ")", "\n", "rewards", "=", "rewards", ".", "swapaxes", "(", "0", ",", "1", ")", "\n", "featmat", "=", "np", ".", "concatenate", "(", "[", "self", ".", "features", "(", "ob", ",", "r", ")", "for", "ob", ",", "r", "in", "zip", "(", "obs", ",", "rewards", ")", "]", ")", "\n", "return", "featmat", ".", "dot", "(", "self", ".", "_coeffs", ")", ".", "reshape", "(", "n_envs", ",", "n_timesteps", ")", ".", "swapaxes", "(", "0", ",", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_conv_baseline.GaussianConvBaseline.__init__": [[8, 18], ["base.Baseline.__init__", "rllab.regressors.gaussian_conv_regressor.GaussianConvRegressor", "dict"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env_spec", ",", "regressor_args", "=", "None", ",", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "env_spec", ")", "\n", "if", "regressor_args", "is", "None", ":", "\n", "            ", "regressor_args", "=", "dict", "(", ")", "\n", "\n", "", "self", ".", "_regressor", "=", "GaussianConvRegressor", "(", "\n", "input_shape", "=", "env_spec", ".", "observation_space", ".", "shape", ",", "\n", "output_dim", "=", "1", ",", "\n", "name", "=", "\"vf\"", ",", "\n", "**", "regressor_args", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_conv_baseline.GaussianConvBaseline.fit": [[20, 24], ["numpy.concatenate", "numpy.concatenate", "gaussian_conv_baseline.GaussianConvBaseline._regressor.fit", "numpy.concatenate.reshape"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.fit"], ["", "def", "fit", "(", "self", ",", "paths", ")", ":", "\n", "        ", "observations", "=", "np", ".", "concatenate", "(", "[", "p", "[", "\"obs\"", "]", "for", "p", "in", "paths", "]", ")", "\n", "returns", "=", "np", ".", "concatenate", "(", "[", "p", "[", "\"returns\"", "]", "for", "p", "in", "paths", "]", ")", "\n", "self", ".", "_regressor", ".", "fit", "(", "observations", ",", "returns", ".", "reshape", "(", "(", "-", "1", ",", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_conv_baseline.GaussianConvBaseline.fit_by_samples_data": [[25, 29], ["gaussian_conv_baseline.GaussianConvBaseline._regressor.fit", "returns.reshape"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.fit"], ["", "def", "fit_by_samples_data", "(", "self", ",", "samples_data", ")", ":", "\n", "        ", "observations", "=", "samples_data", "[", "\"obs\"", "]", "\n", "returns", "=", "samples_data", "[", "\"returns\"", "]", "\n", "self", ".", "_regressor", ".", "fit", "(", "observations", ",", "returns", ".", "reshape", "(", "(", "-", "1", ",", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_conv_baseline.GaussianConvBaseline.predict": [[30, 32], ["gaussian_conv_baseline.GaussianConvBaseline._regressor.predict().flatten", "gaussian_conv_baseline.GaussianConvBaseline._regressor.predict"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.flatten", "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.predict"], ["", "def", "predict", "(", "self", ",", "path", ")", ":", "\n", "        ", "return", "self", ".", "_regressor", ".", "predict", "(", "path", "[", "\"obs\"", "]", ")", ".", "flatten", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_conv_baseline.GaussianConvBaseline.get_param_values": [[33, 35], ["gaussian_conv_baseline.GaussianConvBaseline._regressor.get_param_values"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.get_param_values"], ["", "def", "get_param_values", "(", "self", ",", "**", "tags", ")", ":", "\n", "        ", "return", "self", ".", "_regressor", ".", "get_param_values", "(", "**", "tags", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_conv_baseline.GaussianConvBaseline.set_param_values": [[36, 38], ["gaussian_conv_baseline.GaussianConvBaseline._regressor.set_param_values"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.set_param_values"], ["", "def", "set_param_values", "(", "self", ",", "flattened_params", ",", "**", "tags", ")", ":", "\n", "        ", "self", ".", "_regressor", ".", "set_param_values", "(", "flattened_params", ",", "**", "tags", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.__init__": [[8, 18], ["rllab.regressors.gaussian_mlp_regressor.GaussianMLPRegressor", "dict"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "env_spec", ",", "subsample_factor", "=", "1.", ",", "num_seq_inputs", "=", "1", ",", "regressor_args", "=", "None", ",", ")", ":", "\n", "        ", "self", ".", "_subsample_factor", "=", "subsample_factor", "\n", "if", "regressor_args", "is", "None", ":", "\n", "            ", "regressor_args", "=", "dict", "(", ")", "\n", "\n", "", "self", ".", "_regressor", "=", "GaussianMLPRegressor", "(", "\n", "input_shape", "=", "(", "env_spec", ".", "observation_space", ".", "flat_dim", "*", "num_seq_inputs", ",", ")", ",", "\n", "output_dim", "=", "1", ",", "\n", "name", "=", "\"vf\"", ",", "\n", "**", "regressor_args", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.fit": [[20, 37], ["gaussian_mlp_baseline.GaussianMLPBaseline._regressor.fit", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate.reshape", "len", "numpy.random.choice", "lst_rnd_idx.append", "int", "numpy.ceil", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.fit"], ["", "def", "fit", "(", "self", ",", "paths", ")", ":", "\n", "# --", "\n", "# Subsample before fitting.", "\n", "        ", "if", "self", ".", "_subsample_factor", "<", "1", ":", "\n", "            ", "lst_rnd_idx", "=", "[", "]", "\n", "for", "path", "in", "paths", ":", "\n", "# Subsample index", "\n", "                ", "path_len", "=", "len", "(", "path", "[", "'returns'", "]", ")", "\n", "rnd_idx", "=", "np", ".", "random", ".", "choice", "(", "path_len", ",", "int", "(", "np", ".", "ceil", "(", "path_len", "*", "self", ".", "_subsample_factor", ")", ")", ",", "\n", "replace", "=", "False", ")", "\n", "lst_rnd_idx", ".", "append", "(", "rnd_idx", ")", "\n", "", "observations", "=", "np", ".", "concatenate", "(", "[", "p", "[", "\"obs\"", "]", "[", "idx", "]", "for", "p", ",", "idx", "in", "zip", "(", "paths", ",", "lst_rnd_idx", ")", "]", ")", "\n", "returns", "=", "np", ".", "concatenate", "(", "[", "p", "[", "\"returns\"", "]", "[", "idx", "]", "for", "p", ",", "idx", "in", "zip", "(", "paths", ",", "lst_rnd_idx", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "observations", "=", "np", ".", "concatenate", "(", "[", "p", "[", "\"obs\"", "]", "for", "p", "in", "paths", "]", ")", "\n", "returns", "=", "np", ".", "concatenate", "(", "[", "p", "[", "\"returns\"", "]", "for", "p", "in", "paths", "]", ")", "\n", "", "self", ".", "_regressor", ".", "fit", "(", "observations", ",", "returns", ".", "reshape", "(", "(", "-", "1", ",", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.predict": [[38, 40], ["gaussian_mlp_baseline.GaussianMLPBaseline._regressor.predict().flatten", "gaussian_mlp_baseline.GaussianMLPBaseline._regressor.predict"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.flatten", "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.predict"], ["", "def", "predict", "(", "self", ",", "path", ")", ":", "\n", "        ", "return", "self", ".", "_regressor", ".", "predict", "(", "path", "[", "\"obs\"", "]", ")", ".", "flatten", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.get_param_values": [[41, 43], ["gaussian_mlp_baseline.GaussianMLPBaseline._regressor.get_param_values"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.get_param_values"], ["", "def", "get_param_values", "(", "self", ",", "**", "tags", ")", ":", "\n", "        ", "return", "self", ".", "_regressor", ".", "get_param_values", "(", "**", "tags", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.set_param_values": [[44, 46], ["gaussian_mlp_baseline.GaussianMLPBaseline._regressor.set_param_values"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.value_baselines.gaussian_mlp_baseline.GaussianMLPBaseline.set_param_values"], ["", "def", "set_param_values", "(", "self", ",", "flattened_params", ",", "**", "tags", ")", ":", "\n", "        ", "self", ".", "_regressor", ".", "set_param_values", "(", "flattened_params", ",", "**", "tags", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.__init__": [[54, 58], ["numpy.zeros", "numpy.zeros"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "epsilon", "=", "1e-4", ",", "shape", "=", "(", ")", ")", ":", "\n", "        ", "self", ".", "mean", "=", "np", ".", "zeros", "(", "shape", ",", "'float64'", ")", "\n", "self", ".", "var", "=", "np", ".", "zeros", "(", "shape", ",", "'float64'", ")", "\n", "self", ".", "count", "=", "epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update": [[59, 78], ["numpy.mean", "numpy.var", "numpy.square"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch_mean", "=", "np", ".", "mean", "(", "x", ",", "axis", "=", "0", ")", "\n", "batch_var", "=", "np", ".", "var", "(", "x", ",", "axis", "=", "0", ")", "\n", "batch_count", "=", "x", ".", "shape", "[", "0", "]", "\n", "\n", "delta", "=", "batch_mean", "-", "self", ".", "mean", "\n", "tot_count", "=", "self", ".", "count", "+", "batch_count", "\n", "\n", "new_mean", "=", "self", ".", "mean", "+", "delta", "*", "batch_count", "/", "tot_count", "\n", "m_a", "=", "self", ".", "var", "*", "(", "self", ".", "count", ")", "\n", "m_b", "=", "batch_var", "*", "(", "batch_count", ")", "\n", "M2", "=", "m_a", "+", "m_b", "+", "np", ".", "square", "(", "delta", ")", "*", "self", ".", "count", "*", "batch_count", "/", "(", "self", ".", "count", "+", "batch_count", ")", "\n", "new_var", "=", "M2", "/", "(", "self", ".", "count", "+", "batch_count", ")", "\n", "\n", "new_count", "=", "batch_count", "+", "self", ".", "count", "\n", "\n", "self", ".", "mean", "=", "new_mean", "\n", "self", ".", "var", "=", "new_var", "\n", "self", ".", "count", "=", "new_count", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.vec_normalize": [[4, 50], ["numpy.zeros", "vec_env_normalize.RunningMeanStd", "vec_env_normalize.RunningMeanStd", "_step", "dict", "vec_env_normalize.vec_normalize._obfilt"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._step"], ["def", "vec_normalize", "(", "envs", ",", "ob", "=", "True", ",", "ret", "=", "True", ",", "clipob", "=", "10.", ",", "cliprew", "=", "10.", ",", "gamma", "=", "0.99", ")", ":", "\n", "    ", "ob_rms", "=", "RunningMeanStd", "(", "shape", "=", "envs", ".", "observation_space", ".", "shape", ")", "if", "ob", "else", "None", "\n", "ret_rms", "=", "RunningMeanStd", "(", "shape", "=", "(", ")", ")", "if", "ret", "else", "None", "\n", "ret", "=", "np", ".", "zeros", "(", "envs", ".", "num_envs", ")", "\n", "gamma", "=", "gamma", "\n", "\n", "_step", "=", "envs", ".", "step", "\n", "_reset", "=", "envs", ".", "reset", "\n", "\n", "def", "step", "(", "vac", ")", ":", "\n", "        ", "\"\"\"\n        Apply sequence of actions to sequence of environments\n        actions -> (observations, rewards, news)\n\n        where 'news' is a boolean vector indicating whether each element is new.\n        \"\"\"", "\n", "nonlocal", "ret", ",", "ret_rms", ",", "cliprew", "\n", "obs", ",", "rewards", ",", "news", ",", "infos", "=", "_step", "(", "vac", ")", "\n", "_info", "=", "dict", "(", "reward_mean", "=", "rewards", ".", "mean", "(", ")", ",", "reward_std", "=", "rewards", ".", "std", "(", ")", ")", "\n", "ret", "=", "ret", "*", "gamma", "+", "rewards", "\n", "obs", "=", "_obfilt", "(", "obs", ")", "\n", "if", "ret_rms", ":", "\n", "            ", "ret_rms", ".", "update", "(", "ret", ")", "\n", "rewards", "=", "np", ".", "clip", "(", "rewards", "/", "np", ".", "sqrt", "(", "ret_rms", ".", "var", ")", ",", "-", "cliprew", ",", "cliprew", ")", "\n", "", "return", "obs", ",", "rewards", ",", "news", ",", "_info", "\n", "\n", "", "def", "_obfilt", "(", "obs", ")", ":", "\n", "        ", "nonlocal", "clipob", "\n", "if", "ob_rms", ":", "\n", "            ", "ob_rms", ".", "update", "(", "obs", ")", "\n", "obs", "=", "np", ".", "clip", "(", "(", "obs", "-", "ob_rms", ".", "mean", ")", "/", "np", ".", "sqrt", "(", "ob_rms", ".", "var", ")", ",", "-", "clipob", ",", "clipob", ")", "\n", "return", "obs", "\n", "", "else", ":", "\n", "            ", "return", "obs", "\n", "\n", "", "", "def", "reset", "(", ")", ":", "\n", "        ", "\"\"\"\n        Reset all environments\n        \"\"\"", "\n", "obs", "=", "_reset", "(", ")", "\n", "return", "_obfilt", "(", "obs", ")", "\n", "\n", "", "envs", ".", "step", "=", "step", "\n", "envs", ".", "reset", "=", "reset", "\n", "\n", "return", "envs", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.test_runningmeanstd": [[80, 95], ["vec_env_normalize.RunningMeanStd", "numpy.concatenate", "vec_env_normalize.RunningMeanStd.update", "vec_env_normalize.RunningMeanStd.update", "vec_env_normalize.RunningMeanStd.update", "numpy.allclose", "numpy.random.randn", "numpy.random.randn", "numpy.random.randn", "numpy.random.randn", "numpy.random.randn", "numpy.random.randn", "np.concatenate.mean", "np.concatenate.var"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update"], ["", "", "def", "test_runningmeanstd", "(", ")", ":", "\n", "    ", "for", "(", "x1", ",", "x2", ",", "x3", ")", "in", "[", "\n", "(", "np", ".", "random", ".", "randn", "(", "3", ")", ",", "np", ".", "random", ".", "randn", "(", "4", ")", ",", "np", ".", "random", ".", "randn", "(", "5", ")", ")", ",", "\n", "(", "np", ".", "random", ".", "randn", "(", "3", ",", "2", ")", ",", "np", ".", "random", ".", "randn", "(", "4", ",", "2", ")", ",", "np", ".", "random", ".", "randn", "(", "5", ",", "2", ")", ")", ",", "\n", "]", ":", "\n", "        ", "rms", "=", "RunningMeanStd", "(", "epsilon", "=", "0.0", ",", "shape", "=", "x1", ".", "shape", "[", "1", ":", "]", ")", "\n", "\n", "x", "=", "np", ".", "concatenate", "(", "[", "x1", ",", "x2", ",", "x3", "]", ",", "axis", "=", "0", ")", "\n", "ms1", "=", "[", "x", ".", "mean", "(", "axis", "=", "0", ")", ",", "x", ".", "var", "(", "axis", "=", "0", ")", "]", "\n", "rms", ".", "update", "(", "x1", ")", "\n", "rms", ".", "update", "(", "x2", ")", "\n", "rms", ".", "update", "(", "x3", ")", "\n", "ms2", "=", "[", "rms", ".", "mean", ",", "rms", ".", "var", "]", "\n", "\n", "assert", "np", ".", "allclose", "(", "ms1", ",", "ms2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.k_index.k_index": [[4, 56], ["gym.logger.set_level", "isinstance", "gym.spaces.box.Box", "numpy.concatenate", "_step", "k_index.k_index.obfilt"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.custom_vendor.maze_env.MazeEnv._step"], ["def", "k_index", "(", "env", ")", ":", "\n", "    ", "\"\"\"\n    add a k timestep index to the observation. Making value prediction and other\n    modeling significantly easier. Should work with both single and batched environments, but not tested\n    on the latter.\n\n    :param env:\n    :return:\n    \"\"\"", "\n", "import", "gym", "\n", "gym", ".", "logger", ".", "set_level", "(", "40", ")", "\n", "_step", "=", "env", ".", "step", "\n", "_reset", "=", "env", ".", "reset", "\n", "\n", "assert", "isinstance", "(", "env", ".", "observation_space", ",", "gym", ".", "spaces", ".", "box", ".", "Box", ")", ",", "'we only support the box observation atm.'", "\n", "ks", "=", "None", "# the step counter", "\n", "\n", "def", "obfilt", "(", "obs", ")", ":", "\n", "        ", "nonlocal", "ks", "\n", "if", "ks", "is", "None", ":", "\n", "            ", "ks", "=", "np", ".", "zeros", "(", "*", "obs", ".", "shape", "[", ":", "-", "1", "]", ",", "1", ")", "\n", "", "return", "np", ".", "concatenate", "(", "[", "obs", ",", "ks", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "", "def", "step", "(", "vac", ")", ":", "\n", "        ", "\"\"\"\n        Apply sequence of actions to sequence of environments\n        actions -> (observations, rewards, news)\n\n        where 'news' is a boolean vector indicating whether each element is new.\n        \"\"\"", "\n", "nonlocal", "ks", "\n", "obs", ",", "rewards", ",", "news", ",", "infos", "=", "_step", "(", "vac", ")", "\n", "_obs", "=", "obfilt", "(", "obs", ")", "\n", "ks", "=", "(", "1", "-", "news", ")", "+", "ks", "*", "(", "1", "-", "news", ")", "\n", "return", "_obs", ",", "rewards", ",", "news", ",", "infos", "\n", "\n", "", "def", "reset", "(", ")", ":", "\n", "        ", "nonlocal", "ks", "\n", "obs", "=", "_reset", "(", ")", "\n", "ks", "=", "None", "# clear the step counter", "\n", "return", "obfilt", "(", "obs", ")", "\n", "\n", "", "env", ".", "step", "=", "step", "\n", "env", ".", "reset", "=", "reset", "\n", "\n", "obs_space", "=", "env", ".", "observation_space", "\n", "env", ".", "observation_space", "=", "gym", ".", "spaces", ".", "box", ".", "Box", "(", "\n", "np", ".", "concatenate", "(", "[", "obs_space", ".", "low", ",", "[", "0", "]", "]", ")", ",", "\n", "np", ".", "concatenate", "(", "[", "obs_space", ".", "high", ",", "[", "env", ".", "spec", ".", "max_episode_steps", "or", "None", "]", "]", ")", ",", "# note: magic numbers are evil --Ge", "\n", ")", "\n", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.__init__": [[41, 65], ["len", "zip", "subproc_vec_env.SubprocVecEnv.first.send", "subproc_vec_env.SubprocVecEnv.first.recv", "subproc_vec_env.SubprocVecEnv.first.send", "subproc_vec_env.SubprocVecEnv.first.recv", "subproc_vec_env.SubprocVecEnv.first.send", "subproc_vec_env.SubprocVecEnv.first.recv", "multiprocessing.Process", "p.start", "remote.close", "zip", "multiprocessing.Pipe", "range", "baselines.common.vec_env.CloudpickleWrapper"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.close"], ["def", "__init__", "(", "self", ",", "env_fns", ")", ":", "\n", "        ", "\"\"\"\n        envs: list of gym environments to run in subprocesses\n        \"\"\"", "\n", "self", ".", "waiting", "=", "False", "\n", "self", ".", "closed", "=", "False", "\n", "self", ".", "num_envs", "=", "len", "(", "env_fns", ")", "\n", "\n", "self", ".", "remotes", ",", "self", ".", "work_remotes", "=", "zip", "(", "*", "[", "Pipe", "(", ")", "for", "_", "in", "range", "(", "self", ".", "num_envs", ")", "]", ")", "\n", "self", ".", "ps", "=", "[", "Process", "(", "target", "=", "worker", ",", "args", "=", "(", "work_remote", ",", "remote", ",", "CloudpickleWrapper", "(", "env_fn", ")", ")", ")", "\n", "for", "(", "work_remote", ",", "remote", ",", "env_fn", ")", "in", "zip", "(", "self", ".", "work_remotes", ",", "self", ".", "remotes", ",", "env_fns", ")", "]", "\n", "for", "p", "in", "self", ".", "ps", ":", "\n", "            ", "p", ".", "daemon", "=", "True", "# if the main process crashes, we should not cause things to hang", "\n", "p", ".", "start", "(", ")", "\n", "", "for", "remote", "in", "self", ".", "work_remotes", ":", "\n", "            ", "remote", ".", "close", "(", ")", "\n", "\n", "", "self", ".", "first", "=", "self", ".", "remotes", "[", "0", "]", "\n", "self", ".", "first", ".", "send", "(", "(", "'get'", ",", "'action_space'", ")", ")", "\n", "self", ".", "action_space", "=", "self", ".", "first", ".", "recv", "(", ")", "\n", "self", ".", "first", ".", "send", "(", "(", "'get'", ",", "'observation_space'", ")", ")", "\n", "self", ".", "observation_space", "=", "self", ".", "first", ".", "recv", "(", ")", "\n", "self", ".", "first", ".", "send", "(", "(", "'get'", ",", "'spec'", ")", ")", "\n", "self", ".", "spec", "=", "self", ".", "first", ".", "recv", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.fork": [[66, 71], ["copy"], "methods", ["None"], ["", "def", "fork", "(", "self", ",", "n", ")", ":", "\n", "        ", "from", "copy", "import", "copy", "\n", "_self", "=", "copy", "(", "self", ")", "\n", "_self", ".", "remotes", "=", "_self", ".", "remotes", "[", ":", "n", "]", "\n", "return", "_self", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.call_sync": [[72, 80], ["dict", "remote.send", "numpy.stack", "RuntimeError", "remote.recv"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send"], ["", "def", "call_sync", "(", "self", ",", "fn_name", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "_", "=", "fn_name", ",", "dict", "(", "args", "=", "args", ",", "kwargs", "=", "kwargs", ")", "\n", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "            ", "remote", ".", "send", "(", "_", ")", "\n", "", "try", ":", "\n", "            ", "return", "np", ".", "stack", "(", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "self", ".", "remotes", "]", ")", "\n", "", "except", "EOFError", "as", "e", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Unknown Error has occurred with the environment.'", ")", "from", "e", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.get": [[81, 83], ["NotImplementedError"], "methods", ["None"], ["", "", "def", "get", "(", "self", ",", "key", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'need to decide for self.first or all.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.step": [[84, 89], ["zip", "zip", "remote.send", "numpy.stack", "numpy.stack", "numpy.stack", "remote.recv"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "        ", "for", "remote", ",", "action", "in", "zip", "(", "self", ".", "remotes", ",", "actions", ")", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'step'", ",", "action", ")", ")", "\n", "", "obs", ",", "rews", ",", "dones", ",", "infos", "=", "zip", "(", "*", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "self", ".", "remotes", "]", ")", "\n", "return", "np", ".", "stack", "(", "obs", ")", ",", "np", ".", "stack", "(", "rews", ")", ",", "np", ".", "stack", "(", "dones", ")", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.render": [[90, 92], ["subproc_vec_env.SubprocVecEnv.call_sync"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.call_sync"], ["", "def", "render", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "call_sync", "(", "'render'", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.step_async": [[93, 97], ["zip", "remote.send"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send"], ["", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "for", "remote", ",", "action", "in", "zip", "(", "self", ".", "remotes", ",", "actions", ")", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'step'", ",", "action", ")", ")", "\n", "", "self", ".", "waiting", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.step_wait": [[98, 103], ["zip", "remote.recv", "numpy.stack", "numpy.stack", "numpy.stack"], "methods", ["None"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "results", "=", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "self", ".", "remotes", "]", "\n", "self", ".", "waiting", "=", "False", "\n", "obs", ",", "rews", ",", "dones", ",", "infos", "=", "zip", "(", "*", "results", ")", "\n", "return", "np", ".", "stack", "(", "obs", ")", ",", "np", ".", "stack", "(", "rews", ")", ",", "np", ".", "stack", "(", "dones", ")", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.reset": [[104, 106], ["subproc_vec_env.SubprocVecEnv.call_sync"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.call_sync"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "call_sync", "(", "'reset'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.sample_task": [[107, 109], ["subproc_vec_env.SubprocVecEnv.call_sync"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.call_sync"], ["", "def", "sample_task", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "call_sync", "(", "'sample_task'", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.reset_task": [[110, 112], ["subproc_vec_env.SubprocVecEnv.call_sync"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.call_sync"], ["", "def", "reset_task", "(", "self", ")", ":", "\n", "        ", "self", ".", "call_sync", "(", "'reset_task'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.close": [[113, 125], ["remote.send", "p.join", "remote.recv"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "\"\"\"looks bad: mix sync and async handling.\"\"\"", "\n", "if", "self", ".", "closed", ":", "\n", "            ", "return", "\n", "", "if", "self", ".", "waiting", ":", "\n", "            ", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "                ", "remote", ".", "recv", "(", ")", "\n", "", "", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'close'", ",", "None", ")", ")", "\n", "", "for", "p", "in", "self", ".", "ps", ":", "\n", "            ", "p", ".", "join", "(", ")", "\n", "", "self", ".", "closed", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.first_call_sync": [[126, 129], ["subproc_vec_env.SubprocVecEnv.first.send", "subproc_vec_env.SubprocVecEnv.first.recv", "dict"], "methods", ["home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send"], ["", "def", "first_call_sync", "(", "self", ",", "fn_name", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "first", ".", "send", "(", "(", "fn_name", ",", "dict", "(", "args", "=", "args", ",", "kwargs", "=", "kwargs", ")", ")", ")", "\n", "return", "self", ".", "first", ".", "recv", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.worker": [[8, 36], ["parent_remote.close", "hasattr", "env.x", "env", "remote.recv", "env.step", "remote.send", "print", "env.reset", "remote.send", "getattr", "remote.close", "data.get", "data.get", "remote.send", "dict", "tuple", "dict", "getattr"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.close", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.step", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.reset", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.close", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.get", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.subproc_vec_env.SubprocVecEnv.get", "home.repos.pwc.inspect_result.episodeyang_e-maml.packages.schedules.Schedule.send"], ["def", "worker", "(", "remote", ",", "parent_remote", ",", "env", ")", ":", "\n", "    ", "parent_remote", ".", "close", "(", ")", "\n", "env", "=", "env", ".", "x", "(", ")", "if", "hasattr", "(", "env", ",", "'x'", ")", "else", "env", "(", ")", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "cmd", ",", "data", "=", "remote", ".", "recv", "(", ")", "\n", "if", "cmd", "==", "'step'", ":", "\n", "                ", "ob", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "data", ")", "\n", "if", "done", ":", "\n", "                    ", "ob", "=", "env", ".", "reset", "(", ")", "\n", "", "remote", ".", "send", "(", "(", "ob", ",", "reward", ",", "done", ",", "info", ")", ")", "\n", "", "elif", "cmd", "==", "'get'", ":", "\n", "                ", "remote", ".", "send", "(", "getattr", "(", "env", ",", "data", ")", ")", "\n", "", "elif", "cmd", "==", "'close'", ":", "\n", "                ", "remote", ".", "close", "(", ")", "\n", "break", "# this terminates the process.", "\n", "", "else", ":", "\n", "                ", "data", "=", "data", "or", "dict", "(", ")", "\n", "args", "=", "data", ".", "get", "(", "'args'", ",", "tuple", "(", ")", ")", "\n", "kwargs", "=", "data", ".", "get", "(", "'kwargs'", ",", "dict", "(", ")", ")", "\n", "_", "=", "getattr", "(", "env", ",", "cmd", ")", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "remote", ".", "send", "(", "_", ")", "\n", "\n", "", "", "except", "EOFError", "as", "e", ":", "# process has ended from inside", "\n", "            ", "break", "# this terminates the process", "\n", "", "except", "BaseException", "as", "e", ":", "\n", "            ", "print", "(", "e", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_experiments.jaynes_demo.train_fn": [[5, 11], ["print", "print", "print"], "function", ["None"], ["def", "train_fn", "(", "some_variable", "=", "0", ")", ":", "\n", "    ", "import", "tensorflow", "as", "tf", "\n", "print", "(", "f\"tensorflow version: {tf.__version__}\"", ")", "\n", "\n", "print", "(", "'training is happening!'", ")", "\n", "print", "(", "\"some_variable is\"", ",", "some_variable", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_experiments.__init__.dir_prefix": [[20, 28], ["os.path.abspath", "logger.truncate", "os.path.join", "functools.reduce", "range", "inspect.getmodule", "os.path.dirname", "len", "__file__.split", "inspect.stack"], "function", ["None"], []], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_experiments.__init__.config_charts": [[30, 49], ["logger.log_text", "os.path.abspath", "dedent", "inspect.getmodule", "logger.stem", "open", "s.read", "termcolor.cprint", "termcolor.cprint", "os.path.join", "open", "s.read", "os.path.dirname", "os.path.join", "inspect.stack", "os.path.dirname"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.stem"], []], "home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_experiments.__init__.thunk": [[51, 124], ["os.path.abspath", "logger.truncate", "os.path.join", "logger.configure", "logger.log_params", "logger.log_params", "logger.diff", "logger.stem", "termcolor.cprint", "logger.configure", "logger.log_params", "time.sleep", "inspect.getmodule", "logger.now", "logger.run_info", "logger.rev_info", "logger.fn_info", "os.path.join.replace", "KWARGS.copy", "KWARGS.copy.update", "fn", "logger.log_line", "logger.log_params", "len", "dict", "dict", "time.sleep", "traceback.format_exc", "time.sleep", "__file__.split", "dict", "logger.SyncContext", "logger.log_text", "logger.log_params", "logger.log_line", "logger.flush", "inspect.stack", "logger.now", "logger.now", "dict", "logger.now"], "function", ["home.repos.pwc.inspect_result.episodeyang_e-maml.e_maml_tf.ge_utils.stem", "home.repos.pwc.inspect_result.episodeyang_e-maml.wrappers.vec_env_normalize.RunningMeanStd.update"], []]}