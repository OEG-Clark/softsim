{"home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ppo_inference.A3CFFGaussian.__init__": [[33, 49], ["chainer.Chain.__init__", "ppo_inference.A3CFFGaussian.init_scope", "chainerrl.policies.FCGaussianPolicyWithStateIndependentCovariance", "chainerrl.links.MLP"], "methods", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.DDQN_Adversary.__init__"], ["def", "__init__", "(", "self", ",", "obs_size", ",", "action_space", ",", "\n", "n_hidden_layers", "=", "3", ",", "n_hidden_channels", "=", "64", ",", "\n", "bound_mean", "=", "True", ")", ":", "\n", "        ", "assert", "bound_mean", "in", "[", "False", ",", "True", "]", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "hidden_sizes", "=", "(", "n_hidden_channels", ",", ")", "*", "n_hidden_layers", "\n", "# hidden_sizes = (128, 64) Run4", "\n", "with", "self", ".", "init_scope", "(", ")", ":", "\n", "            ", "self", ".", "pi", "=", "policies", ".", "FCGaussianPolicyWithStateIndependentCovariance", "(", "\n", "obs_size", ",", "action_space", ".", "low", ".", "size", ",", "\n", "n_hidden_layers", ",", "n_hidden_channels", ",", "\n", "var_type", "=", "'diagonal'", ",", "nonlinearity", "=", "F", ".", "tanh", ",", "\n", "bound_mean", "=", "bound_mean", ",", "\n", "min_action", "=", "action_space", ".", "low", ",", "max_action", "=", "action_space", ".", "high", ",", "\n", "mean_wscale", "=", "1e-2", ")", "\n", "self", ".", "v", "=", "links", ".", "MLP", "(", "obs_size", ",", "1", ",", "hidden_sizes", "=", "hidden_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ppo_inference.A3CFFGaussian.pi_and_v": [[50, 52], ["ppo_inference.A3CFFGaussian.pi", "ppo_inference.A3CFFGaussian.v"], "methods", ["None"], ["", "", "def", "pi_and_v", "(", "self", ",", "state", ")", ":", "\n", "        ", "return", "self", ".", "pi", "(", "state", ")", ",", "self", ".", "v", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ppo_inference.main": [[54, 470], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "chainerrl.misc.set_random_seed", "ppo_inference.main.make_env"], "function", ["None"], ["", "", "def", "main", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--env_id'", ",", "type", "=", "str", ",", "default", "=", "'LL'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "# default 0", "\n", "help", "=", "'Random seed [0, 2 ** 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--outdir'", ",", "type", "=", "str", ",", "default", "=", "'results'", ",", "\n", "help", "=", "'Directory path to save output files.'", "\n", "' If it does not exist, it will be created.'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_episodes'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--render'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--num-envs'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Number of envs run in parallel.'", ")", "\n", "parser", ".", "add_argument", "(", "'--monitor'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--rollout'", ",", "type", "=", "str", ",", "default", "=", "'Nominal'", ",", "\n", "choices", "=", "(", "'Nominal'", ",", "'Random'", ",", "'MAS'", ",", "'LAS'", ")", ")", "\n", "parser", ".", "add_argument", "(", "'--start_atk'", ",", "type", "=", "int", ",", "default", "=", "'1'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "\n", "help", "=", "'If set to False, actions will be projected '", "\n", "'based on unclipped nominal action '", ")", "\n", "parser", ".", "add_argument", "(", "'--epsilon'", ",", "type", "=", "float", ",", "default", "=", "0.1", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "3", ")", "\n", "parser", ".", "add_argument", "(", "'--save_experiments'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--horizon'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--budget'", ",", "type", "=", "float", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--s'", ",", "type", "=", "str", ",", "default", "=", "'l2'", ")", "\n", "parser", ".", "add_argument", "(", "'--t'", ",", "type", "=", "str", ",", "default", "=", "'l2'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "env_id", "==", "'LL'", ":", "\n", "        ", "env_name", "=", "'LunarLanderContinuous-v2'", "\n", "load", "=", "'PPOLunarLanderContinuous-v2'", "\n", "n_hidden_channels", "=", "128", "\n", "n_hidden_layers", "=", "3", "\n", "\n", "", "elif", "args", ".", "env_id", "==", "'BW'", ":", "\n", "        ", "env_name", "=", "'BipedalWalker-v2'", "\n", "load", "=", "'PPOBipedalWalker-v2'", "\n", "n_hidden_channels", "=", "64", "\n", "n_hidden_layers", "=", "3", "\n", "\n", "", "elif", "args", ".", "env_id", "==", "'Hopper'", ":", "\n", "        ", "env_name", "=", "'Hopper-v2'", "\n", "load", "=", "'hopper/3177/2000000_finish'", "\n", "n_hidden_channels", "=", "64", "\n", "n_hidden_layers", "=", "3", "\n", "\n", "", "elif", "args", ".", "env_id", "==", "'Walker'", ":", "\n", "        ", "env_name", "=", "'Walker2d-v2'", "\n", "load", "=", "'walker2D/3204/1600000_finish'", "\n", "n_hidden_channels", "=", "64", "\n", "n_hidden_layers", "=", "3", "\n", "\n", "", "elif", "args", ".", "env_id", "==", "'HalfCheetah'", ":", "\n", "        ", "env_name", "=", "'HalfCheetah-v2'", "\n", "load", "=", "'halfcheetah/2798/2500000_finish'", "\n", "args", ".", "seed", "=", "99", "\n", "n_hidden_channels", "=", "64", "\n", "n_hidden_layers", "=", "3", "\n", "", "else", ":", "\n", "        ", "print", "(", "'No model found'", ")", "\n", "\n", "", "def", "clip_action_filter", "(", "a", ")", ":", "\n", "        ", "return", "np", ".", "clip", "(", "a", ",", "action_space", ".", "low", ",", "action_space", ".", "high", ")", "\n", "\n", "", "misc", ".", "set_random_seed", "(", "args", ".", "seed", ",", "gpus", "=", "(", "args", ".", "gpu", ",", ")", ")", "\n", "process_seeds", "=", "np", ".", "arange", "(", "args", ".", "num_envs", ")", "+", "args", ".", "seed", "*", "args", ".", "num_envs", "\n", "\n", "def", "make_env", "(", "env_name", ")", ":", "\n", "        ", "env", "=", "gym", ".", "make", "(", "env_name", ")", "\n", "# Use different random seeds for train and test envs", "\n", "# process_seed = int(process_seeds[process_idx])", "\n", "#env_seed = 2 ** 32 - 1 - process_seed if test else process_seed", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "\n", "# Cast observations to float32 because our model uses float32", "\n", "env", "=", "chainerrl", ".", "wrappers", ".", "CastObservationToFloat32", "(", "env", ")", "\n", "if", "args", ".", "monitor", ":", "\n", "            ", "env", "=", "gym", ".", "wrappers", ".", "Monitor", "(", "env", ",", "args", ".", "outdir", ")", "\n", "", "if", "isinstance", "(", "env", ".", "action_space", ",", "spaces", ".", "Box", ")", ":", "\n", "            ", "misc", ".", "env_modifiers", ".", "make_action_filtered", "(", "env", ",", "clip_action_filter", ")", "\n", "", "if", "args", ".", "render", ":", "\n", "            ", "env", "=", "chainerrl", ".", "wrappers", ".", "Render", "(", "env", ")", "\n", "", "return", "env", "\n", "\n", "", "env", "=", "make_env", "(", "env_name", ")", "\n", "spy_env", "=", "make_env", "(", "env_name", ")", "\n", "timestep_limit", "=", "env", ".", "spec", ".", "tags", ".", "get", "(", "'wrapper_config.TimeLimit.max_episode_steps'", ")", "\n", "obs_space", "=", "env", ".", "observation_space", "\n", "action_space", "=", "env", ".", "action_space", "\n", "\n", "# Normalize observations based on their empirical mean and variance", "\n", "obs_normalizer", "=", "chainerrl", ".", "links", ".", "EmpiricalNormalization", "(", "\n", "obs_space", ".", "low", ".", "size", ",", "clip_threshold", "=", "5", ")", "\n", "\n", "# Switch policy types accordingly to action space types", "\n", "model", "=", "A3CFFGaussian", "(", "obs_space", ".", "low", ".", "size", ",", "action_space", ",", "\n", "n_hidden_layers", "=", "n_hidden_layers", ",", "\n", "n_hidden_channels", "=", "n_hidden_channels", ",", "\n", "bound_mean", "=", "True", ")", "\n", "\n", "#For Mujoco Envs", "\n", "#################################################################", "\n", "if", "args", ".", "env_id", "==", "'Hopper'", "or", "args", ".", "env_id", "==", "'Walker'", "or", "args", ".", "env_id", "==", "'HalfCheetah'", ":", "\n", "        ", "winit", "=", "chainerrl", ".", "initializers", ".", "Orthogonal", "(", "1.", ")", "\n", "winit_last", "=", "chainerrl", ".", "initializers", ".", "Orthogonal", "(", "1e-2", ")", "\n", "action_size", "=", "action_space", ".", "low", ".", "size", "\n", "\n", "if", "args", ".", "env_id", "==", "'Hopper'", ":", "\n", "            ", "policy", "=", "chainer", ".", "Sequential", "(", "\n", "L", ".", "Linear", "(", "None", ",", "64", ",", "initialW", "=", "winit", ")", ",", "\n", "F", ".", "tanh", ",", "\n", "L", ".", "Linear", "(", "None", ",", "64", ",", "initialW", "=", "winit", ")", ",", "\n", "F", ".", "tanh", ",", "\n", "L", ".", "Linear", "(", "None", ",", "action_size", ",", "initialW", "=", "winit_last", ")", ",", "\n", "chainerrl", ".", "policies", ".", "GaussianHeadWithStateIndependentCovariance", "(", "\n", "action_size", "=", "action_size", ",", "\n", "var_type", "=", "'diagonal'", ",", "\n", "var_func", "=", "lambda", "x", ":", "F", ".", "exp", "(", "2", "*", "x", ")", ",", "# Parameterize log std", "\n", "var_param_init", "=", "0", ",", "# log std = 0 => std = 1", "\n", ")", ",", "\n", ")", "\n", "vf", "=", "chainer", ".", "Sequential", "(", "\n", "L", ".", "Linear", "(", "None", ",", "64", ",", "initialW", "=", "winit", ")", ",", "\n", "F", ".", "tanh", ",", "\n", "L", ".", "Linear", "(", "None", ",", "64", ",", "initialW", "=", "winit", ")", ",", "\n", "F", ".", "tanh", ",", "\n", "L", ".", "Linear", "(", "None", ",", "1", ",", "initialW", "=", "winit", ")", ",", "\n", ")", "\n", "\n", "\n", "", "elif", "args", ".", "env_id", "==", "'Walker'", "or", "args", ".", "env_id", "==", "'HalfCheetah'", ":", "\n", "            ", "policy", "=", "chainer", ".", "Sequential", "(", "\n", "L", ".", "Linear", "(", "None", ",", "64", ",", "initialW", "=", "winit", ")", ",", "\n", "F", ".", "tanh", ",", "\n", "L", ".", "Linear", "(", "None", ",", "64", ",", "initialW", "=", "winit", ")", ",", "\n", "F", ".", "tanh", ",", "\n", "L", ".", "Linear", "(", "None", ",", "action_size", ",", "initialW", "=", "winit_last", ")", ",", "\n", "chainerrl", ".", "policies", ".", "GaussianHeadWithStateIndependentCovariance", "(", "\n", "action_size", "=", "action_size", ",", "\n", "var_type", "=", "'diagonal'", ",", "\n", "var_func", "=", "lambda", "x", ":", "F", ".", "exp", "(", "2", "*", "x", ")", ",", "# Parameterize log std", "\n", "var_param_init", "=", "0", ",", "# log std = 0 => std = 1", "\n", ")", ",", "\n", ")", "\n", "\n", "vf", "=", "chainer", ".", "Sequential", "(", "\n", "L", ".", "Linear", "(", "None", ",", "128", ",", "initialW", "=", "winit", ")", ",", "\n", "F", ".", "tanh", ",", "\n", "L", ".", "Linear", "(", "None", ",", "64", ",", "initialW", "=", "winit", ")", ",", "\n", "F", ".", "tanh", ",", "\n", "L", ".", "Linear", "(", "None", ",", "1", ",", "initialW", "=", "winit", ")", ",", "\n", ")", "\n", "\n", "", "model", "=", "chainerrl", ".", "links", ".", "Branched", "(", "policy", ",", "vf", ")", "\n", "\n", "", "opt", "=", "chainer", ".", "optimizers", ".", "Adam", "(", "alpha", "=", "3e-4", ",", "eps", "=", "1e-5", ")", "\n", "opt", ".", "setup", "(", "model", ")", "\n", "\n", "agent", "=", "PPO_Adversary", "(", "model", ",", "opt", ",", "\n", "obs_normalizer", "=", "obs_normalizer", ",", "\n", "gpu", "=", "args", ".", "gpu", ",", "update_interval", "=", "2048", ",", "\n", "minibatch_size", "=", "64", ",", "epochs", "=", "10", ",", "\n", "clip_eps_vf", "=", "None", ",", "entropy_coef", "=", "0.0", ",", "\n", "standardize_advantages", "=", "True", ",", "\n", ")", "\n", "\n", "spy", "=", "PPO_Adversary", "(", "model", ",", "opt", ",", "\n", "obs_normalizer", "=", "obs_normalizer", ",", "\n", "gpu", "=", "args", ".", "gpu", ",", "update_interval", "=", "2048", ",", "\n", "minibatch_size", "=", "64", ",", "epochs", "=", "10", ",", "\n", "clip_eps_vf", "=", "None", ",", "entropy_coef", "=", "0.0", ",", "\n", "standardize_advantages", "=", "True", ",", "\n", ")", "\n", "\n", "agent", ".", "load", "(", "load", ")", "\n", "spy", ".", "load", "(", "load", ")", "\n", "\n", "def", "compute_grad", "(", "action", ",", "means", ",", "std_devs", ")", ":", "\n", "# compute analytical gradient", "\n", "        ", "coeff", "=", "-", "(", "action", "-", "means", ")", "/", "(", "(", "np", ".", "power", "(", "std_devs", ",", "3", ")", "*", "(", "np", ".", "sqrt", "(", "2", "*", "np", ".", "pi", ")", ")", ")", ")", "\n", "power", "=", "-", "(", "np", ".", "power", "(", "(", "action", "-", "means", ")", ",", "2", ")", ")", "/", "(", "2", "*", "np", ".", "power", "(", "std_devs", ",", "2", ")", ")", "\n", "exp", "=", "np", ".", "exp", "(", "power", ")", "\n", "grad_a", "=", "coeff", "*", "exp", "\n", "return", "grad_a", "\n", "\n", "", "epsilon", "=", "args", ".", "epsilon", "\n", "lr", "=", "args", ".", "lr", "\n", "budget", "=", "args", ".", "budget", "\n", "\n", "\n", "if", "args", ".", "rollout", "==", "'Nominal'", ":", "\n", "        ", "print", "(", "'Running nominal inference'", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "n_episodes", ")", ":", "\n", "            ", "done", "=", "False", "\n", "obs", "=", "env", ".", "reset", "(", ")", "\n", "R", "=", "0", "\n", "t", "=", "0", "\n", "while", "not", "done", "and", "t", "<", "timestep_limit", ":", "\n", "# env.render()", "\n", "                ", "action", ",", "action_dist", ",", "vs_pred", "=", "agent", ".", "act_forward", "(", "obs", ")", "\n", "obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "if", "args", ".", "clip", "is", "True", ":", "\n", "                    ", "action", "=", "np", ".", "clip", "(", "action", ",", "-", "1", ",", "1", ")", "\n", "", "R", "+=", "r", "\n", "t", "+=", "1", "\n", "", "agent", ".", "stop_episode", "(", ")", "\n", "print", "(", "'test episode:'", ",", "i", ",", "'R:'", ",", "R", ")", "\n", "\n", "", "", "elif", "args", ".", "rollout", "==", "'Random'", ":", "\n", "        ", "print", "(", "'Running random attacks'", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "n_episodes", ")", ":", "\n", "            ", "done", "=", "False", "\n", "obs", "=", "env", ".", "reset", "(", ")", "\n", "R", "=", "0", "\n", "t", "=", "0", "\n", "while", "not", "done", "and", "t", "<", "timestep_limit", ":", "\n", "# env.render()", "\n", "                ", "action", ",", "action_dist", ",", "vs_pred", "=", "agent", ".", "act_forward", "(", "obs", ")", "\n", "if", "t", "%", "1", "==", "0", ":", "\n", "                    ", "delta", "=", "norms", ".", "random_delta", "(", "n_dim", "=", "action_space", ".", "low", ".", "size", ",", "budget", "=", "args", ".", "budget", ")", "\n", "action", "=", "action", "+", "delta", "\n", "", "obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "R", "+=", "r", "\n", "t", "+=", "1", "\n", "", "agent", ".", "stop_episode", "(", ")", "\n", "print", "(", "'test episode:'", ",", "i", ",", "'R:'", ",", "R", ")", "\n", "\n", "", "", "elif", "args", ".", "rollout", "==", "'MAS'", ":", "\n", "        ", "print", "(", "'Running MAS'", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "n_episodes", ")", ":", "\n", "            ", "obs", "=", "env", ".", "reset", "(", ")", "\n", "done", "=", "False", "\n", "R", "=", "0", "\n", "t", "=", "0", "\n", "while", "not", "done", "and", "t", "<", "timestep_limit", ":", "\n", "                ", "if", "t", "<", "args", ".", "start_atk", ":", "\n", "                    ", "print", "(", "t", ")", "\n", "#env.render()", "\n", "action", ",", "action_dist", ",", "vs_pred", "=", "agent", ".", "act_forward", "(", "obs", ")", "\n", "if", "args", ".", "clip", "is", "True", ":", "\n", "                        ", "action", "=", "np", ".", "clip", "(", "action", ",", "-", "1", ",", "1", ")", "\n", "", "obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "R", "+=", "r", "\n", "t", "+=", "1", "\n", "", "else", ":", "\n", "#env.render()", "\n", "                    ", "action", ",", "action_dist", ",", "vs_pred", "=", "spy", ".", "act_forward", "(", "obs", ")", "\n", "if", "args", ".", "clip", "is", "True", ":", "\n", "                        ", "action", "=", "np", ".", "clip", "(", "action", ",", "-", "1", ",", "1", ")", "\n", "", "means", "=", "[", "]", "\n", "std_devs", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "len", "(", "action_dist", ".", "mean", ".", "data", "[", "0", "]", ")", ")", ":", "\n", "                        ", "means", ".", "append", "(", "cp", ".", "asnumpy", "(", "action_dist", ".", "mean", "[", "0", "]", "[", "k", "]", ".", "data", ")", ")", "\n", "var", "=", "np", ".", "exp", "(", "cp", ".", "asnumpy", "(", "action_dist", ".", "ln_var", "[", "0", "]", "[", "k", "]", ".", "data", ")", ")", "\n", "std_devs", ".", "append", "(", "np", ".", "sqrt", "(", "var", ")", ")", "\n", "\n", "", "grad_a", "=", "compute_grad", "(", "action", ",", "means", ",", "std_devs", ")", "\n", "adv_action", "=", "action", "-", "(", "lr", "*", "grad_a", ")", "\n", "\n", "grad_a", "=", "compute_grad", "(", "adv_action", ",", "means", ",", "std_devs", ")", "\n", "adv_action_new", "=", "adv_action", "-", "(", "lr", "*", "grad_a", ")", "\n", "\n", "counter", "=", "0", "\n", "while", "np", ".", "absolute", "(", "adv_action", "-", "adv_action_new", ")", ".", "any", "(", ")", ">", "epsilon", "and", "counter", "<", "25", ":", "\n", "# print('Optimizing')", "\n", "                        ", "adv_action", "=", "adv_action_new", "\n", "grad_a", "=", "compute_grad", "(", "adv_action", ",", "means", ",", "std_devs", ")", "\n", "adv_action_new", "=", "adv_action", "-", "(", "lr", "*", "grad_a", ")", "\n", "counter", "+=", "1", "\n", "\n", "", "delta", "=", "adv_action_new", "-", "action", "\n", "if", "args", ".", "s", "==", "'l2'", ":", "\n", "                        ", "proj_spatial_delta", "=", "norms", ".", "l2_spatial_project", "(", "delta", ",", "budget", ")", "\n", "", "elif", "args", ".", "s", "==", "'l1'", ":", "\n", "                        ", "proj_spatial_delta", "=", "norms", ".", "l1_spatial_project2", "(", "delta", ",", "budget", ")", "\n", "\n", "", "proj_action", "=", "action", "+", "proj_spatial_delta", "\n", "proj_action", "=", "np", ".", "clip", "(", "proj_action", ",", "-", "1", ",", "1", ")", "\n", "obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "proj_action", ")", "\n", "R", "+=", "r", "\n", "t", "+=", "1", "\n", "\n", "", "", "print", "(", "'test episode:'", ",", "i", ",", "'R:'", ",", "R", ")", "\n", "agent", ".", "stop_episode", "(", ")", "\n", "\n", "", "", "elif", "args", ".", "rollout", "==", "'LAS'", ":", "\n", "        ", "print", "(", "'Running LAS'", ")", "\n", "global_budget", "=", "args", ".", "budget", "\n", "epsilon", "=", "args", ".", "epsilon", "\n", "for", "i", "in", "range", "(", "args", ".", "n_episodes", ")", ":", "\n", "            ", "print", "(", "'Episode:'", ",", "i", ")", "\n", "exp_v", "=", "[", "]", "\n", "total_R", "=", "[", "]", "\n", "states", "=", "[", "]", "\n", "actions", "=", "[", "]", "\n", "adv_actions", "=", "[", "]", "\n", "\n", "obs", "=", "env", ".", "reset", "(", ")", "\n", "done", "=", "False", "\n", "R", "=", "0", "\n", "t", "=", "0", "\n", "\n", "while", "not", "done", "and", "t", "<", "timestep_limit", ":", "\n", "#-----------virtual planning loop begins here-------------------------#", "\n", "                ", "v_actions", "=", "[", "]", "\n", "v_values", "=", "[", "]", "\n", "v_states", "=", "[", "]", "\n", "spy_rewards", "=", "[", "]", "\n", "deltas", "=", "[", "]", "\n", "\n", "spy_done", "=", "False", "\n", "spy_R", "=", "0", "\n", "spy_t", "=", "0", "\n", "spy_env", ".", "seed", "(", "args", ".", "seed", ")", "\n", "spy_obs", "=", "spy_env", ".", "reset", "(", ")", "\n", "\n", "#bring virtual environment up to state s_t", "\n", "for", "l", "in", "range", "(", "t", ")", ":", "\n", "#spy_env.render()", "\n", "                    ", "spy_obs", ",", "spy_r", ",", "spy_done", ",", "_", "=", "spy_env", ".", "step", "(", "adv_actions", "[", "l", "]", ")", "\n", "", "spy_done", "=", "False", "\n", "\n", "if", "t", "%", "args", ".", "horizon", "==", "0", ":", "\n", "                    ", "horizon", "=", "args", ".", "horizon", "\n", "global_budget", "=", "args", ".", "budget", "\n", "\n", "", "while", "not", "spy_done", "and", "spy_t", "<", "horizon", ":", "\n", "#spy_env.render()", "\n", "\n", "#forcing spy to act on real observation", "\n", "                    ", "if", "spy_t", "==", "0", ":", "\n", "                        ", "action", ",", "action_dist", ",", "vs_pred", "=", "spy", ".", "act_forward", "(", "obs", ")", "\n", "", "else", ":", "\n", "                        ", "action", ",", "action_dist", ",", "vs_pred", "=", "spy", ".", "act_forward", "(", "spy_obs", ")", "\n", "\n", "", "if", "args", ".", "clip", "is", "True", ":", "\n", "                        ", "action", "=", "np", ".", "clip", "(", "action", ",", "-", "1", ",", "1", ")", "\n", "\n", "", "means", "=", "[", "]", "\n", "std_devs", "=", "[", "]", "\n", "dims", "=", "len", "(", "action_dist", ".", "mean", ".", "data", "[", "0", "]", ")", "\n", "for", "j", "in", "range", "(", "dims", ")", ":", "\n", "                        ", "means", ".", "append", "(", "cp", ".", "asnumpy", "(", "action_dist", ".", "mean", "[", "0", "]", "[", "j", "]", ".", "data", ")", ")", "\n", "var", "=", "np", ".", "exp", "(", "cp", ".", "asnumpy", "(", "action_dist", ".", "ln_var", "[", "0", "]", "[", "j", "]", ".", "data", ")", ")", "\n", "std_devs", ".", "append", "(", "np", ".", "sqrt", "(", "var", ")", ")", "\n", "\n", "", "grad_a", "=", "compute_grad", "(", "action", ",", "means", ",", "std_devs", ")", "\n", "adv_action", "=", "action", "-", "(", "lr", "*", "grad_a", ")", "\n", "\n", "grad_a", "=", "compute_grad", "(", "adv_action", ",", "means", ",", "std_devs", ")", "\n", "adv_action_new", "=", "adv_action", "-", "(", "lr", "*", "grad_a", ")", "\n", "\n", "counter", "=", "0", "\n", "while", "np", ".", "absolute", "(", "adv_action", "-", "adv_action_new", ")", ".", "any", "(", ")", ">", "epsilon", "and", "counter", "<", "25", ":", "\n", "# print('Optimizing')", "\n", "                        ", "adv_action", "=", "adv_action_new", "\n", "grad_a", "=", "compute_grad", "(", "adv_action", ",", "means", ",", "std_devs", ")", "\n", "adv_action_new", "=", "adv_action", "-", "(", "lr", "*", "grad_a", ")", "\n", "counter", "+=", "1", "\n", "\n", "", "v_actions", ".", "append", "(", "list", "(", "map", "(", "float", ",", "action", ")", ")", ")", "\n", "delta", "=", "adv_action_new", "-", "action", "\n", "deltas", ".", "append", "(", "list", "(", "map", "(", "float", ",", "delta", ")", ")", ")", "\n", "\n", "spy_obs", ",", "spy_r", ",", "spy_done", ",", "_", "=", "spy_env", ".", "step", "(", "action", ")", "\n", "spy_R", "+=", "spy_r", "\n", "spy_t", "+=", "1", "\n", "\n", "v_values", ".", "append", "(", "float", "(", "vs_pred", ".", "data", "[", "0", "]", "[", "0", "]", ")", ")", "\n", "spy_rewards", ".", "append", "(", "spy_R", ")", "\n", "v_states", ".", "append", "(", "list", "(", "map", "(", "float", ",", "spy_obs", ")", ")", ")", "\n", "\n", "#-----------virtual planning loop ends here-------------------------#", "\n", "", "if", "args", ".", "s", "==", "'l1'", ":", "\n", "                    ", "delta_norms", "=", "[", "norms", ".", "l1_spatial_norm", "(", "delta", ")", "for", "delta", "in", "deltas", "]", "\n", "", "elif", "args", ".", "s", "==", "'l2'", ":", "\n", "                    ", "delta_norms", "=", "[", "norms", ".", "l2_spatial_norm", "(", "delta", ")", "for", "delta", "in", "deltas", "]", "\n", "\n", "", "if", "args", ".", "t", "==", "'l1'", ":", "\n", "                    ", "temporal_deltas", "=", "norms", ".", "l1_time_project2", "(", "delta_norms", ",", "global_budget", ")", "\n", "", "elif", "args", ".", "t", "==", "'l2'", ":", "\n", "                    ", "temporal_deltas", "=", "norms", ".", "l2_time_project", "(", "delta_norms", ",", "global_budget", ")", "\n", "\n", "", "spatial_deltas", "=", "[", "]", "\n", "for", "a", "in", "range", "(", "len", "(", "temporal_deltas", ")", ")", ":", "\n", "                    ", "if", "args", ".", "s", "==", "'l1'", ":", "\n", "                        ", "spatial_deltas", ".", "append", "(", "list", "(", "map", "(", "float", ",", "norms", ".", "l1_spatial_project2", "(", "deltas", "[", "a", "]", ",", "temporal_deltas", "[", "a", "]", ")", ")", ")", ")", "\n", "", "elif", "args", ".", "s", "==", "'l2'", ":", "\n", "                        ", "spatial_deltas", ".", "append", "(", "list", "(", "map", "(", "float", ",", "norms", ".", "l2_spatial_project", "(", "deltas", "[", "a", "]", ",", "temporal_deltas", "[", "a", "]", ")", ")", ")", ")", "\n", "\n", "", "", "proj_actions", "=", "list", "(", "map", "(", "add", ",", "v_actions", "[", "0", "]", ",", "spatial_deltas", "[", "0", "]", ")", ")", "\n", "proj_actions", "=", "np", ".", "clip", "(", "proj_actions", ",", "-", "1", ",", "1", ")", "\n", "\n", "obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "proj_actions", ")", "\n", "R", "+=", "r", "\n", "t", "+=", "1", "\n", "\n", "total_R", ".", "append", "(", "R", ")", "\n", "exp_v", ".", "append", "(", "float", "(", "vs_pred", "[", "0", "]", "[", "0", "]", ".", "data", ")", ")", "\n", "states", ".", "append", "(", "list", "(", "map", "(", "float", ",", "obs", ")", ")", ")", "\n", "actions", ".", "append", "(", "list", "(", "map", "(", "float", ",", "(", "np", ".", "clip", "(", "v_actions", "[", "0", "]", ",", "-", "1", ",", "1", ")", ")", ")", ")", ")", "\n", "adv_actions", ".", "append", "(", "list", "(", "map", "(", "float", ",", "np", ".", "clip", "(", "proj_actions", ",", "-", "1", ",", "1", ")", ")", ")", ")", "\n", "\n", "#adaptive reduce budget and planning horiozn here", "\n", "horizon", "=", "horizon", "-", "1", "\n", "if", "args", ".", "t", "==", "'l1'", ":", "\n", "                    ", "used", "=", "temporal_deltas", "[", "0", "]", "\n", "global_budget", "=", "norms", ".", "reducebudget_l1", "(", "used", ",", "global_budget", ")", "\n", "", "elif", "args", ".", "t", "==", "'l2'", ":", "\n", "                    ", "used", "=", "temporal_deltas", "[", "0", "]", "\n", "global_budget", "=", "norms", ".", "reducebudget_l2", "(", "used", ",", "global_budget", ")", "\n", "\n", "", "", "print", "(", "'test episode:'", ",", "i", ",", "'R:'", ",", "R", ")", "\n", "agent", ".", "stop_episode", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.A3CFFSoftmax.__init__": [[41, 46], ["chainerrl.policies.SoftmaxPolicy", "chainer.links.MLP", "chainer.ChainList.__init__", "chainer.links.MLP"], "methods", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.DDQN_Adversary.__init__"], ["def", "__init__", "(", "self", ",", "ndim_obs", ",", "n_actions", ",", "hidden_sizes", "=", "(", "200", ",", "200", ")", ")", ":", "\n", "        ", "self", ".", "pi", "=", "policies", ".", "SoftmaxPolicy", "(", "\n", "model", "=", "links", ".", "MLP", "(", "ndim_obs", ",", "n_actions", ",", "hidden_sizes", ")", ")", "\n", "self", ".", "v", "=", "links", ".", "MLP", "(", "ndim_obs", ",", "1", ",", "hidden_sizes", "=", "hidden_sizes", ")", "\n", "super", "(", ")", ".", "__init__", "(", "self", ".", "pi", ",", "self", ".", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.A3CFFSoftmax.pi_and_v": [[47, 49], ["simply_ppo.A3CFFSoftmax.pi", "simply_ppo.A3CFFSoftmax.v"], "methods", ["None"], ["", "def", "pi_and_v", "(", "self", ",", "state", ")", ":", "\n", "        ", "return", "self", ".", "pi", "(", "state", ")", ",", "self", ".", "v", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.A3CFFMellowmax.__init__": [[53, 58], ["chainerrl.policies.MellowmaxPolicy", "chainer.links.MLP", "chainer.ChainList.__init__", "chainer.links.MLP"], "methods", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.DDQN_Adversary.__init__"], ["def", "__init__", "(", "self", ",", "ndim_obs", ",", "n_actions", ",", "hidden_sizes", "=", "(", "200", ",", "200", ")", ")", ":", "\n", "        ", "self", ".", "pi", "=", "policies", ".", "MellowmaxPolicy", "(", "\n", "model", "=", "links", ".", "MLP", "(", "ndim_obs", ",", "n_actions", ",", "hidden_sizes", ")", ")", "\n", "self", ".", "v", "=", "links", ".", "MLP", "(", "ndim_obs", ",", "1", ",", "hidden_sizes", "=", "hidden_sizes", ")", "\n", "super", "(", ")", ".", "__init__", "(", "self", ".", "pi", ",", "self", ".", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.A3CFFMellowmax.pi_and_v": [[59, 61], ["simply_ppo.A3CFFMellowmax.pi", "simply_ppo.A3CFFMellowmax.v"], "methods", ["None"], ["", "def", "pi_and_v", "(", "self", ",", "state", ")", ":", "\n", "        ", "return", "self", ".", "pi", "(", "state", ")", ",", "self", ".", "v", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.A3CFFGaussian.__init__": [[65, 80], ["chainer.Chain.__init__", "simply_ppo.A3CFFGaussian.init_scope", "chainerrl.policies.FCGaussianPolicyWithStateIndependentCovariance", "chainer.links.MLP"], "methods", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.DDQN_Adversary.__init__"], ["def", "__init__", "(", "self", ",", "obs_size", ",", "action_space", ",", "\n", "n_hidden_layers", "=", "3", ",", "n_hidden_channels", "=", "512", ",", "\n", "bound_mean", "=", "True", ")", ":", "\n", "        ", "assert", "bound_mean", "in", "[", "False", ",", "True", "]", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "hidden_sizes", "=", "(", "n_hidden_channels", ",", ")", "*", "n_hidden_layers", "\n", "with", "self", ".", "init_scope", "(", ")", ":", "\n", "            ", "self", ".", "pi", "=", "policies", ".", "FCGaussianPolicyWithStateIndependentCovariance", "(", "\n", "obs_size", ",", "action_space", ".", "low", ".", "size", ",", "\n", "n_hidden_layers", ",", "n_hidden_channels", ",", "\n", "var_type", "=", "'diagonal'", ",", "nonlinearity", "=", "F", ".", "tanh", ",", "\n", "bound_mean", "=", "bound_mean", ",", "\n", "min_action", "=", "action_space", ".", "low", ",", "max_action", "=", "action_space", ".", "high", ",", "\n", "mean_wscale", "=", "1e-2", ")", "\n", "self", ".", "v", "=", "links", ".", "MLP", "(", "obs_size", ",", "1", ",", "hidden_sizes", "=", "hidden_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.A3CFFGaussian.pi_and_v": [[81, 83], ["simply_ppo.A3CFFGaussian.pi", "simply_ppo.A3CFFGaussian.v"], "methods", ["None"], ["", "", "def", "pi_and_v", "(", "self", ",", "state", ")", ":", "\n", "        ", "return", "self", ".", "pi", "(", "state", ")", ",", "self", ".", "v", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.ConvFCGaussianPolicyWithStateIndependentCovariance.__init__": [[104, 149], ["super().__init__", "layers.append", "simply_ppo.ConvFCGaussianPolicyWithStateIndependentCovariance.init_scope", "chainer.Sequential", "chainer.Parameter", "chainer.links.Convolution2D", "functools.partial", "chainer.links.Convolution2D", "functools.partial", "chainer.links.Convolution2D", "chainer.links.Linear", "chainer.links.Linear", "bound_by_tanh", "chainerrl.initializers.LeCunNormal"], "methods", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.DDQN_Adversary.__init__"], ["def", "__init__", "(", "self", ",", "n_input_channels", ",", "action_size", ",", "\n", "min_action", "=", "None", ",", "max_action", "=", "None", ",", "bound_mean", "=", "False", ",", "\n", "var_type", "=", "'spherical'", ",", "\n", "nonlinearity", "=", "F", ".", "relu", ",", "\n", "pooling", "=", "F", ".", "max_pooling_2d", ",", "\n", "mean_wscale", "=", "1", ",", "\n", "var_func", "=", "F", ".", "softplus", ",", "\n", "var_param_init", "=", "0", ",", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "n_input_channels", "=", "n_input_channels", "\n", "self", ".", "action_size", "=", "action_size", "\n", "self", ".", "min_action", "=", "min_action", "\n", "self", ".", "max_action", "=", "max_action", "\n", "self", ".", "bound_mean", "=", "bound_mean", "\n", "self", ".", "nonlinearity", "=", "nonlinearity", "\n", "self", ".", "var_func", "=", "var_func", "\n", "self", ".", "pooling", "=", "pooling", "\n", "var_size", "=", "{", "'spherical'", ":", "1", ",", "'diagonal'", ":", "action_size", "}", "[", "var_type", "]", "\n", "\n", "if", "self", ".", "bound_mean", ":", "\n", "            ", "layers", ".", "append", "(", "lambda", "x", ":", "bound_by_tanh", "(", "\n", "x", ",", "self", ".", "min_action", ",", "self", ".", "max_action", ")", ")", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", ")", "\n", "with", "self", ".", "init_scope", "(", ")", ":", "\n", "            ", "self", ".", "hidden_layers", "=", "Sequential", "(", "\n", "L", ".", "Convolution2D", "(", "3", ",", "8", ",", "ksize", "=", "4", ")", ",", "\n", "self", ".", "nonlinearity", ",", "\n", "partial", "(", "F", ".", "max_pooling_2d", ",", "ksize", "=", "2", ")", ",", "\n", "\n", "L", ".", "Convolution2D", "(", "8", ",", "16", ",", "ksize", "=", "3", ")", ",", "\n", "self", ".", "nonlinearity", ",", "\n", "partial", "(", "F", ".", "max_pooling_2d", ",", "ksize", "=", "2", ")", ",", "\n", "\n", "L", ".", "Convolution2D", "(", "16", ",", "64", ",", "ksize", "=", "3", ")", ",", "\n", "self", ".", "nonlinearity", ",", "\n", "L", ".", "Linear", "(", "None", ",", "256", ")", ",", "\n", "# self.nonlinearity,", "\n", "# L.Linear(512, 256),", "\n", "self", ".", "nonlinearity", ",", "\n", "L", ".", "Linear", "(", "256", ",", "action_size", ",", "initialW", "=", "LeCunNormal", "(", "mean_wscale", ")", ")", "\n", ")", "\n", "self", ".", "var_param", "=", "chainer", ".", "Parameter", "(", "\n", "initializer", "=", "var_param_init", ",", "shape", "=", "(", "var_size", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.ConvFCGaussianPolicyWithStateIndependentCovariance.__call__": [[150, 155], ["cupy.rollaxis", "simply_ppo.ConvFCGaussianPolicyWithStateIndependentCovariance.hidden_layers", "chainer.functions.broadcast_to", "chainerrl.distribution.GaussianDistribution", "simply_ppo.ConvFCGaussianPolicyWithStateIndependentCovariance.var_func"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "cp", ".", "rollaxis", "(", "x", ",", "3", ",", "1", ")", "\n", "mean", "=", "self", ".", "hidden_layers", "(", "x", ")", "\n", "var", "=", "F", ".", "broadcast_to", "(", "self", ".", "var_func", "(", "self", ".", "var_param", ")", ",", "mean", ".", "shape", ")", "\n", "return", "distribution", ".", "GaussianDistribution", "(", "mean", ",", "var", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.ConvCritic.__init__": [[158, 169], ["chainer.Chain.__init__", "simply_ppo.ConvCritic.init_scope", "chainer.links.Convolution2D", "chainer.links.BatchNormalization", "chainer.links.Convolution2D", "chainer.links.BatchNormalization", "chainer.links.Convolution2D", "chainer.links.BatchNormalization", "chainer.links.Linear", "chainer.links.Linear"], "methods", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.DDQN_Adversary.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "with", "self", ".", "init_scope", "(", ")", ":", "\n", "            ", "self", ".", "l1", "=", "L", ".", "Convolution2D", "(", "3", ",", "32", ",", "ksize", "=", "8", ",", "stride", "=", "2", ",", "pad", "=", "0", ")", "\n", "self", ".", "b1", "=", "L", ".", "BatchNormalization", "(", "32", ")", "\n", "self", ".", "l2", "=", "L", ".", "Convolution2D", "(", "32", ",", "32", ",", "ksize", "=", "4", ",", "stride", "=", "1", ",", "pad", "=", "0", ")", "\n", "self", ".", "b2", "=", "L", ".", "BatchNormalization", "(", "32", ")", "\n", "self", ".", "l3", "=", "L", ".", "Convolution2D", "(", "32", ",", "32", ",", "ksize", "=", "2", ",", "stride", "=", "1", ",", "pad", "=", "0", ")", "\n", "self", ".", "b3", "=", "L", ".", "BatchNormalization", "(", "32", ")", "\n", "self", ".", "fc1", "=", "L", ".", "Linear", "(", "None", ",", "128", ")", "\n", "self", ".", "fc2", "=", "L", ".", "Linear", "(", "128", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.ConvCritic.__call__": [[170, 191], ["cupy.rollaxis", "simply_ppo.ConvCritic.l1", "chainer.functions.max_pooling_2d", "chainer.functions.tanh", "simply_ppo.ConvCritic.l2", "chainer.functions.max_pooling_2d", "chainer.functions.tanh", "chainer.functions.tanh", "chainer.functions.tanh", "simply_ppo.ConvCritic.fc1", "simply_ppo.ConvCritic.fc2"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "x", ",", "test", "=", "False", ")", ":", "\n", "\n", "        ", "x", "=", "cp", ".", "rollaxis", "(", "x", ",", "3", ",", "1", ")", "\n", "\n", "h", "=", "self", ".", "l1", "(", "x", ")", "\n", "h", "=", "F", ".", "max_pooling_2d", "(", "h", ",", "ksize", "=", "2", ",", "stride", "=", "1", ")", "\n", "#h = self.b1(h)", "\n", "h", "=", "F", ".", "tanh", "(", "h", ")", "\n", "\n", "h", "=", "self", ".", "l2", "(", "h", ")", "\n", "h", "=", "F", ".", "max_pooling_2d", "(", "h", ",", "ksize", "=", "2", ",", "stride", "=", "1", ")", "\n", "#h = self.b2(h)", "\n", "h", "=", "F", ".", "tanh", "(", "h", ")", "\n", "\n", "# h = self.l3(h)", "\n", "# h = self.b3(h)", "\n", "# h = F.tanh(h)", "\n", "\n", "h", "=", "F", ".", "tanh", "(", "self", ".", "fc1", "(", "h", ")", ")", "\n", "h", "=", "F", ".", "tanh", "(", "self", ".", "fc2", "(", "h", ")", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.A3CFFConvGaussian.__init__": [[194, 206], ["chainer.Chain.__init__", "simply_ppo.A3CFFConvGaussian.init_scope", "simply_ppo.ConvFCGaussianPolicyWithStateIndependentCovariance", "simply_ppo.ConvCritic"], "methods", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.DDQN_Adversary.__init__"], ["    ", "def", "__init__", "(", "self", ",", "obs_size", ",", "action_space", ",", "\n", "bound_mean", "=", "None", ")", ":", "\n", "        ", "assert", "bound_mean", "in", "[", "False", ",", "True", "]", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "with", "self", ".", "init_scope", "(", ")", ":", "\n", "            ", "self", ".", "pi", "=", "ConvFCGaussianPolicyWithStateIndependentCovariance", "(", "\n", "n_input_channels", "=", "obs_size", ",", "action_size", "=", "action_space", ".", "low", ".", "size", ",", "\n", "var_type", "=", "'diagonal'", ",", "nonlinearity", "=", "F", ".", "tanh", ",", "\n", "bound_mean", "=", "bound_mean", ",", "\n", "min_action", "=", "action_space", ".", "low", ",", "max_action", "=", "action_space", ".", "high", ",", "\n", "mean_wscale", "=", "1e-2", ")", "\n", "self", ".", "v", "=", "ConvCritic", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.A3CFFConvGaussian.pi_and_v": [[207, 209], ["simply_ppo.A3CFFConvGaussian.pi", "simply_ppo.A3CFFConvGaussian.v"], "methods", ["None"], ["", "", "def", "pi_and_v", "(", "self", ",", "state", ")", ":", "\n", "        ", "return", "self", ".", "pi", "(", "state", ")", ",", "self", ".", "v", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ppo.main": [[211, 342], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logging.basicConfig", "chainerrl.misc.set_random_seed", "simply_ppo.main.make_env"], "function", ["None"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "import", "logging", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "type", "=", "str", ",", "default", "=", "'BipedalWalkerHardcore-v2'", ")", "#MountainCarContinuous-v0, LunarLanderContinuous-v2, BipedalWalker-v2, BipedalWalkerHardcore-v2, CarRacing-v0", "\n", "parser", ".", "add_argument", "(", "'--arch'", ",", "type", "=", "str", ",", "default", "=", "'FFGaussian'", ",", "\n", "choices", "=", "(", "'FFSoftmax'", ",", "'FFMellowmax'", ",", "\n", "'FFGaussian'", ",", "'FFConvGaussian'", ")", ")", "\n", "parser", ".", "add_argument", "(", "'--bound-mean'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Random seed [0, 2 ** 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--outdir'", ",", "type", "=", "str", ",", "default", "=", "'results'", ",", "\n", "help", "=", "'Directory path to save output files.'", "\n", "' If it does not exist, it will be created.'", ")", "\n", "parser", ".", "add_argument", "(", "'--steps'", ",", "type", "=", "int", ",", "default", "=", "10", "**", "6", ")", "\n", "parser", ".", "add_argument", "(", "'--eval-interval'", ",", "type", "=", "int", ",", "default", "=", "10000", ")", "\n", "parser", ".", "add_argument", "(", "'--eval-n-runs'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--n_episodes'", ",", "type", "=", "int", ",", "default", "=", "5000", ")", "\n", "parser", ".", "add_argument", "(", "'--reward-scale-factor'", ",", "type", "=", "float", ",", "default", "=", "1e-2", ")", "#default is 1e-2", "\n", "parser", ".", "add_argument", "(", "'--standardize-advantages'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--render'", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "1e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "type", "=", "float", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--load'", ",", "type", "=", "str", ",", "default", "=", "''", ")", "#PPOBipedalWalker-v2_Run2", "\n", "parser", ".", "add_argument", "(", "'--logger-level'", ",", "type", "=", "int", ",", "default", "=", "logging", ".", "DEBUG", ")", "\n", "parser", ".", "add_argument", "(", "'--monitor'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--update-interval'", ",", "type", "=", "int", ",", "default", "=", "200", ")", "\n", "parser", ".", "add_argument", "(", "'--batchsize'", ",", "type", "=", "int", ",", "default", "=", "256", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--entropy-coef'", ",", "type", "=", "float", ",", "default", "=", "0.0275", ")", "\n", "parser", ".", "add_argument", "(", "'--run_id'", ",", "type", "=", "str", ",", "default", "=", "'_Run1'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "logging", ".", "basicConfig", "(", "level", "=", "args", ".", "logger_level", ")", "\n", "misc", ".", "set_random_seed", "(", "args", ".", "seed", ",", "gpus", "=", "(", "args", ".", "gpu", ",", ")", ")", "\n", "\n", "def", "make_env", "(", "args_env", ",", "test", ")", ":", "\n", "        ", "env", "=", "gym", ".", "make", "(", "args", ".", "env", ")", "\n", "# Use different random seeds for train and test envs", "\n", "env_seed", "=", "2", "**", "32", "-", "1", "-", "args", ".", "seed", "if", "test", "else", "args", ".", "seed", "\n", "env", ".", "seed", "(", "env_seed", ")", "\n", "# Cast observations to float32 because our model uses float32", "\n", "env", "=", "chainerrl", ".", "wrappers", ".", "CastObservationToFloat32", "(", "env", ")", "\n", "if", "args", ".", "monitor", ":", "\n", "            ", "env", "=", "gym", ".", "wrappers", ".", "Monitor", "(", "env", ",", "args", ".", "outdir", ")", "\n", "", "if", "not", "test", ":", "\n", "# Scale rewards (and thus returns) to a reasonable range so that training is easier", "\n", "            ", "env", "=", "chainerrl", ".", "wrappers", ".", "ScaleReward", "(", "env", ",", "args", ".", "reward_scale_factor", ")", "\n", "# if args.render:", "\n", "#     env = chainerrl.wrappers.Render(env)", "\n", "", "return", "env", "\n", "\n", "", "env", "=", "make_env", "(", "args", ".", "env", ",", "test", "=", "False", ")", "\n", "timestep_limit", "=", "env", ".", "spec", ".", "tags", ".", "get", "(", "\n", "'wrapper_config.TimeLimit.max_episode_steps'", ")", "\n", "obs_space", "=", "env", ".", "observation_space", "\n", "action_space", "=", "env", ".", "action_space", "\n", "action_size", "=", "action_space", ".", "low", ".", "size", "\n", "\n", "# Normalize observations based on their empirical mean and variance", "\n", "obs_normalizer", "=", "chainerrl", ".", "links", ".", "EmpiricalNormalization", "(", "\n", "obs_space", ".", "low", ".", "size", ",", "clip_threshold", "=", "5", ")", "\n", "\n", "# Switch policy types accordingly to action space types", "\n", "if", "args", ".", "arch", "==", "'FFSoftmax'", ":", "\n", "        ", "model", "=", "A3CFFSoftmax", "(", "obs_space", ".", "low", ".", "size", ",", "action_space", ".", "n", ")", "\n", "", "elif", "args", ".", "arch", "==", "'FFMellowmax'", ":", "\n", "        ", "model", "=", "A3CFFMellowmax", "(", "obs_space", ".", "low", ".", "size", ",", "action_space", ".", "n", ")", "\n", "", "elif", "args", ".", "arch", "==", "'FFGaussian'", ":", "\n", "        ", "model", "=", "A3CFFGaussian", "(", "obs_space", ".", "low", ".", "size", ",", "action_space", ",", "\n", "bound_mean", "=", "args", ".", "bound_mean", ")", "\n", "", "elif", "args", ".", "arch", "==", "'FFConvGaussian'", ":", "\n", "        ", "model", "=", "A3CFFConvGaussian", "(", "obs_space", ".", "low", ".", "size", ",", "action_space", ",", "\n", "bound_mean", "=", "args", ".", "bound_mean", ")", "\n", "obs_normalizer", "=", "None", "\n", "\n", "", "if", "args", ".", "gpu", ">", "-", "1", ":", "\n", "        ", "model", ".", "to_gpu", "(", "args", ".", "gpu", ")", "\n", "\n", "", "opt", "=", "chainer", ".", "optimizers", ".", "Adam", "(", "alpha", "=", "args", ".", "lr", ",", "eps", "=", "1e-5", ")", "\n", "opt", ".", "setup", "(", "model", ")", "\n", "\n", "if", "args", ".", "weight_decay", ">", "0", ":", "\n", "        ", "opt", ".", "add_hook", "(", "NonbiasWeightDecay", "(", "args", ".", "weight_decay", ")", ")", "\n", "\n", "", "agent", "=", "PPO", "(", "model", ",", "opt", ",", "\n", "obs_normalizer", "=", "obs_normalizer", ",", "gpu", "=", "args", ".", "gpu", ",", "\n", "update_interval", "=", "args", ".", "update_interval", ",", "\n", "minibatch_size", "=", "args", ".", "batchsize", ",", "epochs", "=", "args", ".", "epochs", ",", "\n", "clip_eps_vf", "=", "None", ",", "entropy_coef", "=", "args", ".", "entropy_coef", ",", "\n", "standardize_advantages", "=", "args", ".", "standardize_advantages", ",", "\n", ")", "\n", "if", "len", "(", "args", ".", "load", ")", ">", "0", ":", "\n", "        ", "agent", ".", "load", "(", "args", ".", "load", ")", "\n", "\n", "", "print", "(", "'Environment Time Step Limit'", ",", "timestep_limit", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "args", ".", "n_episodes", "+", "1", ")", ":", "\n", "        ", "obs", "=", "env", ".", "reset", "(", ")", "\n", "reward", "=", "0", "\n", "done", "=", "False", "\n", "R", "=", "0", "\n", "t", "=", "0", "\n", "while", "not", "done", "and", "t", "<", "timestep_limit", ":", "\n", "#env.render()", "\n", "            ", "action", "=", "agent", ".", "act_and_train", "(", "obs", ",", "reward", ")", "\n", "obs", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "R", "+=", "reward", "\n", "t", "+=", "1", "\n", "", "if", "i", "%", "10", "==", "0", ":", "\n", "            ", "print", "(", "'episode:'", ",", "i", ",", "\n", "'R:'", ",", "R", ",", "\n", "'statistics:'", ",", "agent", ".", "get_statistics", "(", ")", ")", "\n", "", "agent", ".", "stop_episode_and_train", "(", "obs", ",", "reward", ",", "done", ")", "\n", "", "print", "(", "'Finished.'", ")", "\n", "\n", "for", "i", "in", "range", "(", "10", ")", ":", "\n", "        ", "obs", "=", "env", ".", "reset", "(", ")", "\n", "done", "=", "False", "\n", "R", "=", "0", "\n", "t", "=", "0", "\n", "while", "not", "done", "and", "t", "<", "timestep_limit", ":", "\n", "            ", "env", ".", "render", "(", ")", "\n", "action", "=", "agent", ".", "act", "(", "obs", ")", "\n", "obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "R", "+=", "r", "\n", "t", "+=", "1", "\n", "", "print", "(", "'test episode:'", ",", "i", ",", "'R:'", ",", "R", ")", "\n", "agent", ".", "stop_episode", "(", ")", "\n", "\n", "", "agent", ".", "save", "(", "'PPO'", "+", "args", ".", "env", "+", "args", ".", "run_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.DecayAdditiveOU.__init__": [[223, 236], ["logging.getLogger"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mu", "=", "0.0", ",", "theta", "=", "0.15", ",", "sigma", "=", "0.3", ",", "start_with_mu", "=", "False", ",", "\n", "end_sigma", "=", "0.2", ",", "decay_steps", "=", "1e3", ",", "logger", "=", "getLogger", "(", "__name__", ")", ")", ":", "\n", "\n", "        ", "self", ".", "mu", "=", "mu", "\n", "self", ".", "theta", "=", "theta", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "start_with_mu", "=", "start_with_mu", "\n", "self", ".", "logger", "=", "logger", "\n", "self", ".", "ou_state", "=", "None", "\n", "\n", "self", ".", "steps", "=", "0", "\n", "self", ".", "decay_steps", "=", "decay_steps", "\n", "self", ".", "delta", "=", "(", "self", ".", "sigma", "-", "end_sigma", ")", "/", "decay_steps", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.DecayAdditiveOU.decay_sigma": [[237, 240], ["None"], "methods", ["None"], ["", "def", "decay_sigma", "(", "self", ")", ":", "\n", "        ", "self", ".", "sigma", "=", "self", ".", "sigma", "-", "self", ".", "delta", "\n", "return", "self", ".", "sigma", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.DecayAdditiveOU.select_action": [[241, 260], ["greedy_action_func", "norms.DecayAdditiveOU.logger.debug", "norms.DecayAdditiveOU.decay_sigma", "norms.DecayAdditiveOU.evolve", "numpy.full", "numpy.random.normal().astype", "numpy.sqrt", "numpy.random.normal"], "methods", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.DecayAdditiveOU.decay_sigma"], ["", "def", "select_action", "(", "self", ",", "t", ",", "greedy_action_func", ",", "action_value", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "steps", "<", "self", ".", "decay_steps", ":", "\n", "            ", "self", ".", "sigma", "=", "self", ".", "decay_sigma", "(", ")", "\n", "self", ".", "steps", "+=", "1", "\n", "\n", "", "a", "=", "greedy_action_func", "(", ")", "\n", "if", "self", ".", "ou_state", "is", "None", ":", "\n", "            ", "if", "self", ".", "start_with_mu", ":", "\n", "                ", "self", ".", "ou_state", "=", "np", ".", "full", "(", "a", ".", "shape", ",", "self", ".", "mu", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "                ", "sigma_stable", "=", "(", "self", ".", "sigma", "\n", "/", "np", ".", "sqrt", "(", "2", "*", "self", ".", "theta", "-", "self", ".", "theta", "**", "2", ")", ")", "\n", "self", ".", "ou_state", "=", "np", ".", "random", ".", "normal", "(", "\n", "size", "=", "a", ".", "shape", ",", "loc", "=", "self", ".", "mu", ",", "scale", "=", "sigma_stable", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "evolve", "(", ")", "\n", "", "noise", "=", "self", ".", "ou_state", "\n", "self", ".", "logger", ".", "debug", "(", "'t:%s noise:%s'", ",", "t", ",", "noise", ")", "\n", "return", "a", "+", "noise", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.reducebudget_l1": [[22, 25], ["max"], "function", ["None"], ["def", "reducebudget_l1", "(", "used", ",", "budget", ")", ":", "\n", "    ", "new_budget", "=", "max", "(", "0", ",", "budget", "-", "used", ")", "\n", "return", "new_budget", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.reducebudget_l2": [[27, 33], ["math.sqrt", "max"], "function", ["None"], ["", "def", "reducebudget_l2", "(", "used", ",", "budget", ")", ":", "\n", "    ", "new_budget", "=", "math", ".", "sqrt", "(", "budget", "**", "2", "-", "used", "**", "2", ")", "\n", "new_budget", "=", "max", "(", "0", ",", "new_budget", ")", "\n", "# print('used:',used)", "\n", "# print('remainder',new_budget)", "\n", "return", "new_budget", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.random_delta": [[35, 43], ["numpy.random.uniform", "numpy.random.randint", "sum", "numpy.abs"], "function", ["None"], ["", "def", "random_delta", "(", "n_dim", ",", "budget", ")", ":", "\n", "    ", "real_budget", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "0", ",", "high", "=", "budget", ",", "size", "=", "1", ")", "\n", "x", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "-", "100", ",", "high", "=", "100", ",", "size", "=", "n_dim", ")", "\n", "t", "=", "sum", "(", "np", ".", "abs", "(", "item", ")", "for", "item", "in", "x", ")", "\n", "x", "=", "[", "item", "/", "t", "for", "item", "in", "x", "]", "\n", "x", "=", "[", "item", "*", "real_budget", "for", "item", "in", "x", "]", "\n", "x", "=", "[", "item", "[", "0", "]", "for", "item", "in", "x", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l1_time_project2": [[45, 67], ["list", "sorted", "range", "numpy.linalg.norm", "range", "map", "len", "sum", "max", "len", "len", "numpy.argmax", "sum", "numpy.sign", "sum", "range", "range"], "function", ["None"], ["", "def", "l1_time_project2", "(", "y_orig", ",", "budget", ")", ":", "\n", "\n", "    ", "y_abs", "=", "list", "(", "map", "(", "abs", ",", "y_orig", ")", ")", "\n", "u", "=", "sorted", "(", "y_abs", ",", "reverse", "=", "True", ")", "\n", "binK", "=", "1", "\n", "K", "=", "1", "\n", "bin_list", "=", "[", "0", "]", "*", "len", "(", "u", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "u", ")", "+", "1", ")", ":", "\n", "        ", "if", "(", "sum", "(", "[", "u", "[", "r", "]", "for", "r", "in", "range", "(", "i", ")", "]", ")", "-", "budget", ")", "/", "i", "<", "u", "[", "i", "-", "1", "]", ":", "\n", "            ", "bin_list", "[", "i", "-", "1", "]", "=", "binK", "\n", "binK", "+=", "1", "\n", "\n", "", "", "if", "sum", "(", "bin_list", ")", ">", "0", ":", "\n", "        ", "K", "=", "np", ".", "argmax", "(", "bin_list", ")", "+", "1", "\n", "\n", "", "tau", "=", "(", "sum", "(", "[", "u", "[", "i", "]", "for", "i", "in", "range", "(", "K", ")", "]", ")", "-", "budget", ")", "/", "K", "\n", "xn", "=", "[", "max", "(", "item", "-", "tau", ",", "0", ")", "for", "item", "in", "y_abs", "]", "\n", "l1_norm_y", "=", "np", ".", "linalg", ".", "norm", "(", "y_orig", ",", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "y_orig", ")", ")", ":", "\n", "        ", "if", "l1_norm_y", ">", "budget", ":", "\n", "            ", "y_orig", "[", "i", "]", "=", "np", ".", "sign", "(", "y_orig", "[", "i", "]", ")", "*", "xn", "[", "i", "]", "\n", "", "", "return", "y_orig", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l1_spatial_project2": [[69, 91], ["list", "sorted", "range", "numpy.linalg.norm", "range", "map", "len", "sum", "max", "len", "len", "numpy.argmax", "sum", "numpy.sign", "sum", "range", "range"], "function", ["None"], ["", "def", "l1_spatial_project2", "(", "y_orig", ",", "budget", ")", ":", "\n", "\n", "    ", "y_abs", "=", "list", "(", "map", "(", "abs", ",", "y_orig", ")", ")", "\n", "u", "=", "sorted", "(", "y_abs", ",", "reverse", "=", "True", ")", "\n", "binK", "=", "1", "\n", "K", "=", "1", "\n", "bin_list", "=", "[", "0", "]", "*", "len", "(", "u", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "u", ")", "+", "1", ")", ":", "\n", "        ", "if", "(", "sum", "(", "[", "u", "[", "r", "]", "for", "r", "in", "range", "(", "i", ")", "]", ")", "-", "budget", ")", "/", "i", "<", "u", "[", "i", "-", "1", "]", ":", "\n", "            ", "bin_list", "[", "i", "-", "1", "]", "=", "binK", "\n", "binK", "+=", "1", "\n", "\n", "", "", "if", "sum", "(", "bin_list", ")", ">", "0", ":", "\n", "        ", "K", "=", "np", ".", "argmax", "(", "bin_list", ")", "+", "1", "\n", "\n", "", "tau", "=", "(", "sum", "(", "[", "u", "[", "i", "]", "for", "i", "in", "range", "(", "K", ")", "]", ")", "-", "budget", ")", "/", "K", "\n", "xn", "=", "[", "max", "(", "item", "-", "tau", ",", "0", ")", "for", "item", "in", "y_abs", "]", "\n", "l1_norm_y", "=", "np", ".", "linalg", ".", "norm", "(", "y_orig", ",", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "y_orig", ")", ")", ":", "\n", "        ", "if", "l1_norm_y", ">", "budget", ":", "\n", "            ", "y_orig", "[", "i", "]", "=", "np", ".", "sign", "(", "y_orig", "[", "i", "]", ")", "*", "xn", "[", "i", "]", "\n", "", "", "return", "y_orig", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l1_time_project": [[93, 129], ["list", "range", "numpy.linalg.norm", "range", "map", "len", "len", "numpy.linalg.norm", "enumerate", "max", "len", "numpy.linalg.norm", "v.append", "v.append", "v.pop", "numpy.sign", "numpy.linalg.norm", "numpy.linalg.norm", "numpy.linalg.norm"], "function", ["None"], ["", "def", "l1_time_project", "(", "y_orig", ",", "delta", ")", ":", "\n", "    ", "y", "=", "list", "(", "map", "(", "abs", ",", "y_orig", ")", ")", "\n", "v", "=", "[", "y", "[", "0", "]", "]", "\n", "vb", "=", "[", "]", "\n", "rho", "=", "y", "[", "0", "]", "-", "delta", "\n", "for", "i", "in", "range", "(", "2", ",", "len", "(", "y", ")", ")", ":", "\n", "        ", "if", "y", "[", "i", "]", ">", "rho", ":", "\n", "            ", "rho", "=", "rho", "+", "(", "y", "[", "i", "]", "-", "rho", ")", "/", "(", "np", ".", "linalg", ".", "norm", "(", "v", ",", "1", ")", "+", "1", ")", "\n", "if", "rho", ">", "y", "[", "i", "]", "-", "delta", ":", "\n", "                ", "v", ".", "append", "(", "y", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                ", "vb", "=", "vb", "+", "v", "\n", "v", "=", "[", "y", "[", "i", "]", "]", "\n", "rho", "=", "y", "[", "i", "]", "-", "delta", "\n", "\n", "", "", "", "if", "len", "(", "vb", ")", ">", "0", ":", "\n", "        ", "for", "item", "in", "vb", ":", "\n", "            ", "if", "item", ">", "rho", ":", "\n", "                ", "v", ".", "append", "(", "item", ")", "\n", "rho", "=", "rho", "+", "(", "item", "-", "rho", ")", "/", "np", ".", "linalg", ".", "norm", "(", "v", ",", "1", ")", "\n", "\n", "", "", "", "v_proj", "=", "0", "\n", "while", "not", "v_proj", "==", "np", ".", "linalg", ".", "norm", "(", "v", ",", "1", ")", ":", "\n", "        ", "v_proj", "=", "np", ".", "linalg", ".", "norm", "(", "v", ",", "1", ")", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "v", ")", ":", "\n", "            ", "if", "item", "<=", "rho", ":", "\n", "                ", "y_loc", "=", "v", ".", "pop", "(", "i", ")", "\n", "rho", "=", "rho", "+", "(", "rho", "-", "y_loc", ")", "/", "np", ".", "linalg", ".", "norm", "(", "v", ",", "1", ")", "\n", "\n", "", "", "", "tau", "=", "rho", "\n", "xn", "=", "[", "max", "(", "item", "-", "tau", ",", "0", ")", "for", "item", "in", "y", "]", "\n", "l1_norm_y", "=", "np", ".", "linalg", ".", "norm", "(", "y_orig", ",", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "y_orig", ")", ")", ":", "\n", "        ", "if", "l1_norm_y", ">", "delta", ":", "\n", "            ", "y_orig", "[", "i", "]", "=", "np", ".", "sign", "(", "y_orig", "[", "i", "]", ")", "*", "xn", "[", "i", "]", "\n", "", "", "return", "y_orig", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l2_spatial_norm": [[131, 133], ["numpy.linalg.norm"], "function", ["None"], ["", "def", "l2_spatial_norm", "(", "x", ")", ":", "\n", "    ", "return", "LA", ".", "norm", "(", "x", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l1_spatial_norm": [[135, 137], ["numpy.linalg.norm"], "function", ["None"], ["", "def", "l1_spatial_norm", "(", "x", ")", ":", "\n", "    ", "return", "LA", ".", "norm", "(", "x", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l2_spatial_norm_gpu": [[139, 141], ["cupy.linalg.norm"], "function", ["None"], ["", "def", "l2_spatial_norm_gpu", "(", "x", ")", ":", "\n", "    ", "return", "LA_GPU", ".", "norm", "(", "x", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l2_time_norm": [[143, 145], ["numpy.linalg.norm"], "function", ["None"], ["", "def", "l2_time_norm", "(", "x", ")", ":", "\n", "    ", "return", "LA", ".", "norm", "(", "x", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.linf_spatial_norm": [[147, 149], ["numpy.linalg.norm"], "function", ["None"], ["", "def", "linf_spatial_norm", "(", "x", ")", ":", "\n", "    ", "return", "LA", ".", "norm", "(", "x", ",", "np", ".", "inf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.linf_time_norm": [[151, 153], ["numpy.linalg.norm"], "function", ["None"], ["", "def", "linf_time_norm", "(", "x", ")", ":", "\n", "    ", "return", "LA", ".", "norm", "(", "x", ",", "np", ".", "inf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l2_spatial_project": [[155, 165], ["norms.l2_spatial_norm"], "function", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l2_spatial_norm"], ["", "def", "l2_spatial_project", "(", "x", ",", "distance", ")", ":", "\n", "    ", "norm", "=", "l2_spatial_norm", "(", "x", ")", "\n", "# print('x',x)", "\n", "# print('l2 norm', diff)", "\n", "# print('dist',distance)", "\n", "if", "norm", "<=", "distance", ":", "\n", "        ", "delta", "=", "x", "\n", "", "else", ":", "\n", "        ", "delta", "=", "(", "x", "/", "norm", ")", "*", "distance", "\n", "", "return", "delta", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l2_spatial_project_gpu": [[167, 176], ["norms.l2_spatial_norm_gpu"], "function", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l2_spatial_norm_gpu"], ["", "def", "l2_spatial_project_gpu", "(", "x", ",", "distance", ")", ":", "\n", "    ", "norm", "=", "l2_spatial_norm_gpu", "(", "x", ")", "\n", "# print('x',x)", "\n", "# print('l2 norm', diff)", "\n", "if", "norm", "<=", "distance", ":", "\n", "        ", "delta", "=", "x", "\n", "", "else", ":", "\n", "        ", "delta", "=", "(", "x", "/", "norm", ")", "*", "distance", "\n", "", "return", "delta", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l2_time_project": [[178, 188], ["norms.l2_time_norm"], "function", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.l2_time_norm"], ["", "def", "l2_time_project", "(", "x", ",", "distance", ")", ":", "\n", "    ", "norm", "=", "l2_time_norm", "(", "x", ")", "\n", "# print('x',x)", "\n", "# print('l2 norm', diff)", "\n", "# print('dist',distance)", "\n", "if", "norm", "<=", "distance", ":", "\n", "        ", "delta", "=", "x", "\n", "", "else", ":", "\n", "        ", "delta", "=", "(", "x", "/", "norm", ")", "*", "distance", "\n", "", "return", "delta", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.linf_spatial_project": [[190, 200], ["norms.linf_spatial_norm", "numpy.clip"], "function", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.linf_spatial_norm"], ["", "def", "linf_spatial_project", "(", "x", ",", "distance", ")", ":", "\n", "# print('x', x)", "\n", "    ", "norm", "=", "linf_spatial_norm", "(", "x", ")", "\n", "# print('norm', norm)", "\n", "if", "norm", "<=", "distance", ":", "\n", "        ", "x_hat", "=", "x", "\n", "", "else", ":", "\n", "        ", "x_hat", "=", "np", ".", "clip", "(", "x", ",", "-", "distance", ",", "distance", ")", "\n", "# print('xhat',x_hat)", "\n", "", "return", "x_hat", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.linf_time_project": [[202, 211], ["norms.linf_time_norm", "numpy.clip"], "function", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.norms.linf_time_norm"], ["", "def", "linf_time_project", "(", "x", ",", "distance", ")", ":", "\n", "# print('x', x)", "\n", "    ", "norm", "=", "linf_time_norm", "(", "x", ")", "\n", "if", "norm", "<=", "distance", ":", "\n", "        ", "x_hat", "=", "x", "\n", "", "else", ":", "\n", "        ", "x_hat", "=", "np", ".", "clip", "(", "x", ",", "-", "distance", ",", "distance", ")", "\n", "# print('xhat',x_hat)", "\n", "", "return", "x_hat", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_inference.main": [[36, 413], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "chainerrl.misc.set_random_seed", "chainerrl.experiments.prepare_output_dir", "print", "ddqn_inference.main.make_env"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--outdir'", ",", "type", "=", "str", ",", "default", "=", "'results'", ",", "\n", "help", "=", "'Directory path to save output files.'", "\n", "' If it does not exist, it will be created.'", ")", "\n", "parser", ".", "add_argument", "(", "'--env_id'", ",", "type", "=", "str", ",", "default", "=", "'MC'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Random seed [0, 2 ** 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--n_episodes'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--episodic-replay'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--prioritized-replay'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--render'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--minibatch-size'", ",", "type", "=", "int", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--monitor'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--reward-scale-factor'", ",", "type", "=", "float", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--rollout'", ",", "type", "=", "str", ",", "default", "=", "'Random'", ",", "\n", "choices", "=", "(", "'Nominal'", ",", "'Random'", ",", "'MAS'", ",", "'LAS'", ")", ")", "\n", "parser", ".", "add_argument", "(", "'--start_atk'", ",", "type", "=", "float", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--clip'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "\n", "help", "=", "'If set to False, actions will be projected '", "\n", "'based on unclipped nominal action '", ")", "\n", "parser", ".", "add_argument", "(", "'--epsilon'", ",", "type", "=", "float", ",", "default", "=", "0.1", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "3", ")", "\n", "parser", ".", "add_argument", "(", "'--save_experiments'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--horizon'", ",", "type", "=", "int", ",", "default", "=", "25", ")", "\n", "parser", ".", "add_argument", "(", "'--budget'", ",", "type", "=", "float", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--s'", ",", "type", "=", "str", ",", "default", "=", "'l2'", ")", "\n", "parser", ".", "add_argument", "(", "'--t'", ",", "type", "=", "str", ",", "default", "=", "'l2'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "misc", ".", "set_random_seed", "(", "args", ".", "seed", ",", "gpus", "=", "(", "args", ".", "gpu", ",", ")", ")", "\n", "args", ".", "outdir", "=", "experiments", ".", "prepare_output_dir", "(", "\n", "args", ",", "args", ".", "outdir", ",", "argv", "=", "sys", ".", "argv", ")", "\n", "print", "(", "'Output files are saved in {}'", ".", "format", "(", "args", ".", "outdir", ")", ")", "\n", "\n", "if", "args", ".", "env_id", "==", "'LL'", ":", "\n", "        ", "env_name", "=", "'LunarLanderContinuous-v2'", "\n", "load", "=", "'DDQNLunarLanderContinuous-v2_Run2'", "\n", "n_hidden_channels", "=", "128", "\n", "n_hidden_layers", "=", "4", "\n", "", "elif", "args", ".", "env_id", "==", "'BW'", ":", "\n", "# Not robust walker", "\n", "        ", "env_name", "=", "'BipedalWalker-v2'", "\n", "load", "=", "'DDQNBipedalWalker-v2_Run2'", "\n", "n_hidden_channels", "=", "256", "\n", "n_hidden_layers", "=", "3", "\n", "", "else", ":", "\n", "        ", "print", "(", "'No model found'", ")", "\n", "\n", "", "def", "clip_action_filter", "(", "a", ")", ":", "\n", "        ", "return", "np", ".", "clip", "(", "a", ",", "action_space", ".", "low", ",", "action_space", ".", "high", ")", "\n", "\n", "", "def", "make_env", "(", "env_name", ")", ":", "\n", "        ", "env", "=", "gym", ".", "make", "(", "env_name", ")", "\n", "# Use different random seeds for train and test envs", "\n", "env_seed", "=", "args", ".", "seed", "\n", "env", ".", "seed", "(", "env_seed", ")", "\n", "# Cast observations to float32 because our model uses float32", "\n", "env", "=", "chainerrl", ".", "wrappers", ".", "CastObservationToFloat32", "(", "env", ")", "\n", "if", "args", ".", "monitor", ":", "\n", "            ", "env", "=", "gym", ".", "wrappers", ".", "Monitor", "(", "env", ",", "args", ".", "outdir", ")", "\n", "", "if", "isinstance", "(", "env", ".", "action_space", ",", "spaces", ".", "Box", ")", ":", "\n", "            ", "misc", ".", "env_modifiers", ".", "make_action_filtered", "(", "env", ",", "clip_action_filter", ")", "\n", "", "if", "args", ".", "render", ":", "\n", "            ", "env", "=", "chainerrl", ".", "wrappers", ".", "Render", "(", "env", ")", "\n", "", "return", "env", "\n", "\n", "", "env", "=", "make_env", "(", "env_name", ")", "\n", "spy_env", "=", "make_env", "(", "env_name", ")", "\n", "timestep_limit", "=", "env", ".", "spec", ".", "tags", ".", "get", "(", "'wrapper_config.TimeLimit.max_episode_steps'", ")", "\n", "obs_space", "=", "env", ".", "observation_space", "\n", "obs_size", "=", "obs_space", ".", "low", ".", "size", "\n", "action_space", "=", "env", ".", "action_space", "\n", "action_size", "=", "action_space", ".", "low", ".", "size", "\n", "\n", "# Use NAF to apply DQN to continuous action spaces", "\n", "q_func", "=", "q_functions", ".", "FCQuadraticStateQFunction", "(", "\n", "obs_size", ",", "action_size", ",", "\n", "n_hidden_channels", "=", "n_hidden_channels", ",", "\n", "n_hidden_layers", "=", "n_hidden_layers", ",", "\n", "action_space", "=", "action_space", ")", "\n", "# Use the Ornstein-Uhlenbeck process for exploration", "\n", "ou_sigma", "=", "(", "action_space", ".", "high", "-", "action_space", ".", "low", ")", "*", "0.2", "\n", "explorer", "=", "explorers", ".", "AdditiveOU", "(", "sigma", "=", "ou_sigma", ")", "\n", "\n", "q_func", ".", "to_gpu", "(", "args", ".", "gpu", ")", "\n", "opt", "=", "optimizers", ".", "Adam", "(", ")", "\n", "opt", ".", "setup", "(", "q_func", ")", "\n", "\n", "rbuf_capacity", "=", "5", "*", "10", "**", "5", "\n", "if", "args", ".", "episodic_replay", ":", "\n", "        ", "if", "args", ".", "minibatch_size", "is", "None", ":", "\n", "            ", "args", ".", "minibatch_size", "=", "4", "\n", "", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "betasteps", "=", "(", "args", ".", "steps", "-", "args", ".", "replay_start_size", ")", "//", "args", ".", "update_interval", "\n", "rbuf", "=", "replay_buffer", ".", "PrioritizedEpisodicReplayBuffer", "(", "\n", "rbuf_capacity", ",", "betasteps", "=", "betasteps", ")", "\n", "", "else", ":", "\n", "            ", "rbuf", "=", "replay_buffer", ".", "EpisodicReplayBuffer", "(", "rbuf_capacity", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "args", ".", "minibatch_size", "is", "None", ":", "\n", "            ", "args", ".", "minibatch_size", "=", "32", "\n", "", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "betasteps", "=", "(", "args", ".", "steps", "-", "args", ".", "replay_start_size", ")", "//", "args", ".", "update_interval", "\n", "rbuf", "=", "replay_buffer", ".", "PrioritizedReplayBuffer", "(", "\n", "rbuf_capacity", ",", "betasteps", "=", "betasteps", ")", "\n", "", "else", ":", "\n", "            ", "rbuf", "=", "replay_buffer", ".", "ReplayBuffer", "(", "rbuf_capacity", ")", "\n", "\n", "", "", "agent", "=", "DDQN_Adversary", "(", "q_func", ",", "opt", ",", "rbuf", ",", "gpu", "=", "args", ".", "gpu", ",", "gamma", "=", "0.995", ",", "\n", "explorer", "=", "explorer", ",", "replay_start_size", "=", "1000", ",", "\n", "target_update_interval", "=", "100", ",", "\n", "update_interval", "=", "1", ",", "minibatch_size", "=", "64", ",", "\n", "target_update_method", "=", "'hard'", ",", "soft_update_tau", "=", "1e-2", ",", "\n", "episodic_update", "=", "True", ",", "episodic_update_len", "=", "16", ")", "\n", "\n", "spy", "=", "DDQN_Adversary", "(", "q_func", ",", "opt", ",", "rbuf", ",", "gpu", "=", "args", ".", "gpu", ",", "gamma", "=", "0.995", ",", "\n", "explorer", "=", "explorer", ",", "replay_start_size", "=", "1000", ",", "\n", "target_update_interval", "=", "100", ",", "\n", "update_interval", "=", "1", ",", "minibatch_size", "=", "64", ",", "\n", "target_update_method", "=", "'hard'", ",", "soft_update_tau", "=", "1e-2", ",", "\n", "episodic_update", "=", "True", ",", "episodic_update_len", "=", "16", ")", "\n", "\n", "agent", ".", "load", "(", "load", ")", "\n", "spy", ".", "load", "(", "load", ")", "\n", "\n", "def", "compute_grad", "(", "q", ",", "adv_q", ",", "action", ",", "adv_action", ")", ":", "\n", "        ", "action", "=", "chainer", ".", "as_variable", "(", "cp", ".", "asarray", "(", "action", ".", "data", ")", ")", "\n", "diff_q", "=", "(", "q", "-", "adv_q", ")", "\n", "diff_a", "=", "action", "-", "adv_action", "\n", "diff_a", "=", "diff_a", ".", "data", "[", "0", "]", "\n", "diff_a", "[", "diff_a", "==", "0", "]", "=", "1e-3", "\n", "diff_a", "=", "chainer", ".", "as_variable", "(", "diff_a", ")", "\n", "grad", "=", "diff_q", "/", "diff_a", "\n", "return", "grad", "\n", "\n", "", "def", "sample_adv_action", "(", ")", ":", "\n", "        ", "adv_action", "=", "cp", ".", "random", ".", "uniform", "(", "action_space", ".", "low", ",", "action_space", ".", "high", ",", "action_size", ")", "\n", "adv_action", "=", "chainer", ".", "as_variable", "(", "cp", ".", "expand_dims", "(", "adv_action", ".", "astype", "(", "cp", ".", "float32", ")", ",", "axis", "=", "0", ")", ")", "\n", "return", "adv_action", "\n", "\n", "", "print", "(", "'Environment Time Step Limit'", ",", "timestep_limit", ")", "\n", "\n", "epsilon", "=", "args", ".", "epsilon", "\n", "lr", "=", "args", ".", "lr", "\n", "budget", "=", "args", ".", "budget", "\n", "\n", "if", "args", ".", "rollout", "==", "'Nominal'", ":", "\n", "        ", "print", "(", "'Running nominal inference'", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "n_episodes", ")", ":", "\n", "            ", "done", "=", "False", "\n", "obs", "=", "env", ".", "reset", "(", ")", "\n", "R", "=", "0", "\n", "t", "=", "0", "\n", "\n", "while", "not", "done", "and", "t", "<", "timestep_limit", ":", "\n", "                ", "env", ".", "render", "(", ")", "\n", "adv_action", "=", "sample_adv_action", "(", ")", "\n", "action", ",", "q", ",", "adv_q", "=", "agent", ".", "act_forward", "(", "obs", ",", "adv_action", ")", "\n", "obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "R", "+=", "r", "\n", "t", "+=", "1", "\n", "", "agent", ".", "stop_episode", "(", ")", "\n", "print", "(", "'test episode:'", ",", "i", ",", "'R:'", ",", "R", ")", "\n", "\n", "", "", "elif", "args", ".", "rollout", "==", "'Random'", ":", "\n", "        ", "print", "(", "'Running random attacks'", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "n_episodes", ")", ":", "\n", "            ", "done", "=", "False", "\n", "obs", "=", "env", ".", "reset", "(", ")", "\n", "R", "=", "0", "\n", "t", "=", "0", "\n", "while", "not", "done", "and", "t", "<", "timestep_limit", ":", "\n", "                ", "env", ".", "render", "(", ")", "\n", "adv_action", "=", "sample_adv_action", "(", ")", "\n", "action", ",", "q", ",", "adv_q", "=", "agent", ".", "act_forward", "(", "obs", ",", "adv_action", ")", "\n", "if", "t", "%", "1", "==", "0", ":", "\n", "                    ", "delta", "=", "norms", ".", "random_delta", "(", "n_dim", "=", "action_space", ".", "low", ".", "size", ",", "budget", "=", "args", ".", "budget", ")", "\n", "action", "=", "action", "+", "delta", "\n", "action", "=", "cuda", ".", "to_cpu", "(", "action", ")", "\n", "", "obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "R", "+=", "r", "\n", "t", "+=", "1", "\n", "", "agent", ".", "stop_episode", "(", ")", "\n", "print", "(", "'test episode:'", ",", "i", ",", "'R:'", ",", "R", ")", "\n", "\n", "", "", "elif", "args", ".", "rollout", "==", "'MAS'", ":", "\n", "        ", "print", "(", "'Running MAS Attacks'", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "n_episodes", ")", ":", "\n", "            ", "obs", "=", "env", ".", "reset", "(", ")", "\n", "done", "=", "False", "\n", "R", "=", "0", "\n", "t", "=", "0", "\n", "while", "not", "done", "and", "t", "<", "timestep_limit", ":", "\n", "                ", "if", "t", "<", "args", ".", "start_atk", ":", "\n", "# env.render()", "\n", "                    ", "adv_action", "=", "sample_adv_action", "(", ")", "\n", "action", ",", "q", ",", "adv_q", "=", "agent", ".", "act_forward", "(", "obs", ",", "adv_action", ")", "\n", "if", "args", ".", "clip", "is", "True", ":", "\n", "                        ", "action", "=", "np", ".", "clip", "(", "action", ",", "-", "1", ",", "1", ")", "\n", "", "obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "R", "+=", "r", "\n", "t", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "env", ".", "render", "(", ")", "\n", "# optimization loop here", "\n", "adv_action", "=", "sample_adv_action", "(", ")", "\n", "action", ",", "q", ",", "adv_q", "=", "spy", ".", "act_forward", "(", "obs", ",", "adv_action", ")", "\n", "if", "args", ".", "clip", "is", "True", ":", "\n", "                        ", "action", "=", "np", ".", "clip", "(", "action", ",", "-", "1", ",", "1", ")", "\n", "\n", "", "grad_a", "=", "compute_grad", "(", "q", ",", "adv_q", ",", "action", ",", "adv_action", ")", "\n", "adv_action_new", "=", "adv_action", "-", "(", "lr", "*", "grad_a", ")", "\n", "\n", "action", ",", "q", ",", "adv_q_new", "=", "spy", ".", "act_forward", "(", "obs", ",", "adv_action_new", ")", "\n", "counter", "=", "0", "\n", "\n", "while", "np", ".", "absolute", "(", "adv_action", ".", "data", "-", "adv_action_new", ".", "data", ")", ".", "any", "(", ")", ">", "epsilon", "and", "counter", "<", "25", ":", "\n", "# print('Optimizing')", "\n", "                        ", "grad_a", "=", "compute_grad", "(", "adv_q", ",", "adv_q_new", ",", "adv_action", ",", "adv_action_new", ")", "\n", "adv_action", "=", "adv_action_new", "\n", "adv_action_new", "=", "adv_action", "-", "(", "lr", "*", "grad_a", ")", "\n", "counter", "+=", "1", "\n", "\n", "", "delta", "=", "cuda", ".", "to_cpu", "(", "adv_action_new", ".", "data", "[", "0", "]", ")", "-", "action", "\n", "if", "args", ".", "s", "==", "'l2'", ":", "\n", "                        ", "proj_spatial_delta", "=", "norms", ".", "l2_spatial_project", "(", "delta", ",", "budget", ")", "\n", "", "elif", "args", ".", "s", "==", "'l1'", ":", "\n", "                        ", "proj_spatial_delta", "=", "norms", ".", "l1_spatial_project2", "(", "delta", ",", "budget", ")", "\n", "\n", "", "proj_action", "=", "action", "+", "proj_spatial_delta", "\n", "proj_action", "=", "np", ".", "clip", "(", "proj_action", ",", "-", "1", ",", "1", ")", "\n", "proj_action_gpu", "=", "cuda", ".", "to_gpu", "(", "proj_action", ")", "\n", "\n", "action", ",", "q", ",", "adv_q_new", "=", "spy", ".", "act_forward", "(", "obs", ",", "proj_action_gpu", ")", "\n", "obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "proj_action", ")", "\n", "\n", "R", "+=", "r", "\n", "t", "+=", "1", "\n", "", "", "print", "(", "'test episode:'", ",", "i", ",", "'R:'", ",", "R", ")", "\n", "agent", ".", "stop_episode", "(", ")", "\n", "\n", "", "", "elif", "args", ".", "rollout", "==", "'LAS'", ":", "\n", "        ", "print", "(", "'LAS Attacks'", ")", "\n", "global_budget", "=", "args", ".", "budget", "\n", "epsilon", "=", "args", ".", "epsilon", "\n", "for", "i", "in", "range", "(", "args", ".", "n_episodes", ")", ":", "\n", "            ", "print", "(", "'Episode:'", ",", "i", ")", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "exp_v", "=", "[", "]", "\n", "total_R", "=", "[", "]", "\n", "states", "=", "[", "]", "\n", "actions", "=", "[", "]", "\n", "adv_actions", "=", "[", "]", "\n", "\n", "obs", "=", "env", ".", "reset", "(", ")", "\n", "spy_obs", "=", "spy_env", ".", "reset", "(", ")", "\n", "done", "=", "False", "\n", "R", "=", "0", "\n", "t", "=", "0", "\n", "\n", "while", "not", "done", "and", "t", "<", "timestep_limit", ":", "\n", "# env.render()", "\n", "#-----------virtual planning loop begins here-------------------------#", "\n", "                ", "v_actions", "=", "[", "]", "\n", "v_values", "=", "[", "]", "\n", "v_states", "=", "[", "]", "\n", "spy_rewards", "=", "[", "]", "\n", "deltas", "=", "[", "]", "\n", "\n", "spy_done", "=", "False", "\n", "spy_R", "=", "0", "\n", "spy_t", "=", "0", "\n", "spy_env", ".", "seed", "(", "args", ".", "seed", ")", "\n", "spy_obs", "=", "spy_env", ".", "reset", "(", ")", "\n", "\n", "#bring environment up to state s_t", "\n", "for", "l", "in", "range", "(", "t", ")", ":", "\n", "# spy_env.render()", "\n", "                    ", "spy_obs", ",", "spy_r", ",", "spy_done", ",", "_", "=", "spy_env", ".", "step", "(", "adv_actions", "[", "l", "]", ")", "\n", "\n", "", "spy_done", "=", "False", "\n", "\n", "#reset planning horizon is used up", "\n", "if", "t", "%", "args", ".", "horizon", "==", "0", ":", "\n", "                    ", "horizon", "=", "args", ".", "horizon", "\n", "global_budget", "=", "args", ".", "budget", "\n", "\n", "", "while", "not", "spy_done", "and", "spy_t", "<", "horizon", ":", "\n", "# spy_env.render()", "\n", "                    ", "adv_action", "=", "sample_adv_action", "(", ")", "\n", "if", "spy_t", "==", "0", ":", "\n", "                        ", "action", ",", "q", ",", "adv_q", "=", "spy", ".", "act_forward", "(", "obs", ",", "adv_action", ")", "\n", "", "else", ":", "\n", "                        ", "action", ",", "q", ",", "adv_q", "=", "spy", ".", "act_forward", "(", "spy_obs", ",", "adv_action", ")", "\n", "\n", "", "if", "args", ".", "clip", "is", "True", ":", "\n", "                        ", "action", "=", "np", ".", "clip", "(", "action", ",", "-", "1", ",", "1", ")", "\n", "\n", "", "grad_a", "=", "compute_grad", "(", "q", ",", "adv_q", ",", "action", ",", "adv_action", ")", "\n", "adv_action_new", "=", "adv_action", "-", "(", "lr", "*", "grad_a", ")", "\n", "\n", "action", ",", "q", ",", "adv_q_new", "=", "spy", ".", "act_forward", "(", "obs", ",", "adv_action_new", ")", "\n", "if", "args", ".", "clip", "is", "True", ":", "\n", "                        ", "action", "=", "np", ".", "clip", "(", "action", ",", "-", "1", ",", "1", ")", "\n", "\n", "", "counter", "=", "0", "\n", "while", "np", ".", "absolute", "(", "adv_action", ".", "data", "-", "adv_action_new", ".", "data", ")", ".", "any", "(", ")", ">", "epsilon", "and", "counter", "<", "25", ":", "\n", "# print('Optimizing')", "\n", "                        ", "adv_action", "=", "cuda", ".", "to_cpu", "(", "adv_action", ".", "data", "[", "0", "]", ")", "\n", "grad_a", "=", "compute_grad", "(", "adv_q", ",", "adv_q_new", ",", "adv_action", ",", "adv_action_new", ")", "\n", "adv_action", "=", "adv_action_new", "\n", "adv_action_new", "=", "adv_action", "-", "(", "lr", "*", "grad_a", ")", "\n", "counter", "+=", "1", "\n", "\n", "", "v_actions", ".", "append", "(", "list", "(", "map", "(", "float", ",", "action", ")", ")", ")", "\n", "delta", "=", "np", ".", "squeeze", "(", "cp", ".", "asnumpy", "(", "adv_action_new", ".", "data", ")", "-", "action", ")", "\n", "deltas", ".", "append", "(", "list", "(", "map", "(", "float", ",", "delta", ")", ")", ")", "\n", "\n", "spy_obs", ",", "spy_r", ",", "spy_done", ",", "_", "=", "spy_env", ".", "step", "(", "action", ")", "\n", "spy_R", "+=", "spy_r", "\n", "spy_t", "+=", "1", "\n", "\n", "v_values", ".", "append", "(", "q", ")", "\n", "spy_rewards", ".", "append", "(", "spy_R", ")", "\n", "v_states", ".", "append", "(", "list", "(", "map", "(", "float", ",", "spy_obs", ")", ")", ")", "\n", "\n", "# -----------virtual planning loop ends here-------------------------", "\n", "\n", "", "if", "args", ".", "s", "==", "'l1'", ":", "\n", "                    ", "delta_norms", "=", "[", "norms", ".", "l1_spatial_norm", "(", "delta", ")", "for", "delta", "in", "deltas", "]", "\n", "", "elif", "args", ".", "s", "==", "'l2'", ":", "\n", "                    ", "delta_norms", "=", "[", "norms", ".", "l2_spatial_norm", "(", "delta", ")", "for", "delta", "in", "deltas", "]", "\n", "\n", "", "if", "args", ".", "t", "==", "'l1'", ":", "\n", "                    ", "temporal_deltas", "=", "norms", ".", "l1_time_project2", "(", "delta_norms", ",", "global_budget", ")", "\n", "", "elif", "args", ".", "t", "==", "'l2'", ":", "\n", "                    ", "temporal_deltas", "=", "norms", ".", "l2_time_project", "(", "delta_norms", ",", "global_budget", ")", "\n", "\n", "", "spatial_deltas", "=", "[", "]", "\n", "for", "a", "in", "range", "(", "len", "(", "temporal_deltas", ")", ")", ":", "\n", "                    ", "if", "args", ".", "s", "==", "'l1'", ":", "\n", "                        ", "spatial_deltas", ".", "append", "(", "list", "(", "map", "(", "float", ",", "norms", ".", "l1_spatial_project2", "(", "deltas", "[", "a", "]", ",", "temporal_deltas", "[", "a", "]", ")", ")", ")", ")", "\n", "", "elif", "args", ".", "s", "==", "'l2'", ":", "\n", "                        ", "spatial_deltas", ".", "append", "(", "list", "(", "map", "(", "float", ",", "norms", ".", "l2_spatial_project", "(", "deltas", "[", "a", "]", ",", "temporal_deltas", "[", "a", "]", ")", ")", ")", ")", "\n", "\n", "", "", "proj_actions", "=", "list", "(", "map", "(", "add", ",", "v_actions", "[", "0", "]", ",", "spatial_deltas", "[", "0", "]", ")", ")", "\n", "proj_actions", "=", "np", ".", "clip", "(", "proj_actions", ",", "-", "1", ",", "1", ")", "\n", "\n", "obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "proj_actions", ")", "\n", "R", "+=", "r", "\n", "t", "+=", "1", "\n", "\n", "action", ",", "q", ",", "adv_q_new", "=", "spy", ".", "act_forward", "(", "obs", ",", "cp", ".", "asarray", "(", "proj_actions", ")", ")", "\n", "\n", "total_R", ".", "append", "(", "R", ")", "\n", "exp_v", ".", "append", "(", "(", "cp", ".", "asnumpy", "(", "adv_q_new", ")", ")", ".", "tolist", "(", ")", ")", "\n", "states", ".", "append", "(", "list", "(", "map", "(", "float", ",", "obs", ")", ")", ")", "\n", "actions", ".", "append", "(", "list", "(", "map", "(", "float", ",", "(", "np", ".", "clip", "(", "v_actions", "[", "0", "]", ",", "-", "1", ",", "1", ")", ")", ")", ")", ")", "\n", "adv_actions", ".", "append", "(", "list", "(", "map", "(", "float", ",", "np", ".", "clip", "(", "proj_actions", ",", "-", "1", ",", "1", ")", ")", ")", ")", "\n", "\n", "#adaptive reduce budget here", "\n", "horizon", "=", "horizon", "-", "1", "\n", "if", "args", ".", "t", "==", "'l1'", ":", "\n", "                    ", "used", "=", "temporal_deltas", "[", "0", "]", "\n", "global_budget", "=", "norms", ".", "reducebudget_l1", "(", "used", ",", "global_budget", ")", "\n", "", "elif", "args", ".", "t", "==", "'l2'", ":", "\n", "                    ", "used", "=", "temporal_deltas", "[", "0", "]", "\n", "global_budget", "=", "norms", ".", "reducebudget_l2", "(", "used", ",", "global_budget", ")", "\n", "\n", "", "", "print", "(", "'test episode:'", ",", "i", ",", "'R:'", ",", "R", ")", "\n", "agent", ".", "stop_episode", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ppo_adversary.PPO_Adversary.__init__": [[85, 151], ["collections.deque", "collections.deque", "collections.deque", "collections.deque", "chainer.cuda.get_device_from_id().use", "chainer.cuda.get_device_from_id().use", "ppo_adversary.PPO_Adversary.model.to_gpu", "ppo_adversary.PPO_Adversary.obs_normalizer.to_gpu", "chainer.cuda.get_device_from_id", "chainer.cuda.get_device_from_id"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "model", ",", "\n", "optimizer", ",", "\n", "obs_normalizer", "=", "None", ",", "\n", "gpu", "=", "None", ",", "\n", "gamma", "=", "0.99", ",", "\n", "lambd", "=", "0.95", ",", "\n", "phi", "=", "lambda", "x", ":", "x", ",", "\n", "value_func_coef", "=", "1.0", ",", "\n", "entropy_coef", "=", "0.01", ",", "\n", "update_interval", "=", "2048", ",", "\n", "minibatch_size", "=", "64", ",", "\n", "epochs", "=", "10", ",", "\n", "clip_eps", "=", "0.2", ",", "\n", "clip_eps_vf", "=", "None", ",", "\n", "standardize_advantages", "=", "True", ",", "\n", "batch_states", "=", "batch_states", ",", "\n", "value_stats_window", "=", "1000", ",", "\n", "entropy_stats_window", "=", "1000", ",", "\n", "value_loss_stats_window", "=", "100", ",", "\n", "policy_loss_stats_window", "=", "100", ",", "\n", ")", ":", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "obs_normalizer", "=", "obs_normalizer", "\n", "\n", "if", "gpu", "is", "not", "None", "and", "gpu", ">=", "0", ":", "\n", "            ", "cuda", ".", "get_device_from_id", "(", "gpu", ")", ".", "use", "(", ")", "\n", "self", ".", "model", ".", "to_gpu", "(", "device", "=", "gpu", ")", "\n", "if", "self", ".", "obs_normalizer", "is", "not", "None", ":", "\n", "                ", "self", ".", "obs_normalizer", ".", "to_gpu", "(", "device", "=", "gpu", ")", "\n", "\n", "", "", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "lambd", "=", "lambd", "\n", "self", ".", "phi", "=", "phi", "\n", "self", ".", "value_func_coef", "=", "value_func_coef", "\n", "self", ".", "entropy_coef", "=", "entropy_coef", "\n", "self", ".", "update_interval", "=", "update_interval", "\n", "self", ".", "minibatch_size", "=", "minibatch_size", "\n", "self", ".", "epochs", "=", "epochs", "\n", "self", ".", "clip_eps", "=", "clip_eps", "\n", "self", ".", "clip_eps_vf", "=", "clip_eps_vf", "\n", "self", ".", "standardize_advantages", "=", "standardize_advantages", "\n", "self", ".", "batch_states", "=", "batch_states", "\n", "\n", "self", ".", "xp", "=", "self", ".", "model", ".", "xp", "\n", "\n", "# Contains episodes used for next update iteration", "\n", "self", ".", "memory", "=", "[", "]", "\n", "\n", "# Contains transitions of the last episode not moved to self.memory yet", "\n", "self", ".", "last_episode", "=", "[", "]", "\n", "self", ".", "last_state", "=", "None", "\n", "self", ".", "last_action", "=", "None", "\n", "\n", "# Batch versions of last_episode, last_state, and last_action", "\n", "self", ".", "batch_last_episode", "=", "None", "\n", "self", ".", "batch_last_state", "=", "None", "\n", "self", ".", "batch_last_action", "=", "None", "\n", "\n", "self", ".", "value_record", "=", "collections", ".", "deque", "(", "maxlen", "=", "value_stats_window", ")", "\n", "self", ".", "entropy_record", "=", "collections", ".", "deque", "(", "maxlen", "=", "entropy_stats_window", ")", "\n", "self", ".", "value_loss_record", "=", "collections", ".", "deque", "(", "\n", "maxlen", "=", "value_loss_stats_window", ")", "\n", "self", ".", "policy_loss_record", "=", "collections", ".", "deque", "(", "\n", "maxlen", "=", "policy_loss_stats_window", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ppo_adversary.PPO_Adversary.act": [[152, 164], ["ppo_adversary.PPO_Adversary.batch_states", "ppo_adversary.PPO_Adversary.obs_normalizer", "chainer.using_config", "chainer.using_config", "chainer.using_config", "chainer.using_config", "chainer.no_backprop_mode", "chainer.no_backprop_mode", "chainer.no_backprop_mode", "chainer.no_backprop_mode", "ppo_adversary.PPO_Adversary.model", "chainer.cuda.to_cpu", "chainer.cuda.to_cpu", "chainer.cuda.to_cpu", "chainer.cuda.to_cpu", "action_distrib.sample"], "methods", ["None"], ["", "def", "act", "(", "self", ",", "obs", ")", ":", "\n", "        ", "xp", "=", "self", ".", "xp", "\n", "b_state", "=", "self", ".", "batch_states", "(", "[", "obs", "]", ",", "xp", ",", "self", ".", "phi", ")", "\n", "\n", "if", "self", ".", "obs_normalizer", ":", "\n", "            ", "b_state", "=", "self", ".", "obs_normalizer", "(", "b_state", ",", "update", "=", "False", ")", "\n", "\n", "", "with", "chainer", ".", "using_config", "(", "'train'", ",", "False", ")", ",", "chainer", ".", "no_backprop_mode", "(", ")", ":", "\n", "            ", "action_distrib", ",", "_", "=", "self", ".", "model", "(", "b_state", ")", "\n", "action", "=", "chainer", ".", "cuda", ".", "to_cpu", "(", "action_distrib", ".", "sample", "(", ")", ".", "data", ")", "[", "0", "]", "\n", "\n", "", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ppo_adversary.PPO_Adversary.act_forward": [[167, 194], ["ppo_adversary.PPO_Adversary.batch_states", "ppo_adversary.PPO_Adversary.obs_normalizer", "chainer.using_config", "chainer.using_config", "chainer.using_config", "chainer.using_config", "chainer.no_backprop_mode", "chainer.no_backprop_mode", "chainer.no_backprop_mode", "chainer.no_backprop_mode", "ppo_adversary.PPO_Adversary.model", "chainer.cuda.to_cpu", "chainer.cuda.to_cpu", "chainer.cuda.to_cpu", "chainer.cuda.to_cpu", "action_distrib.sample"], "methods", ["None"], ["", "def", "act_forward", "(", "self", ",", "obs", ")", ":", "\n", "        ", "xp", "=", "self", ".", "xp", "\n", "b_state", "=", "self", ".", "batch_states", "(", "[", "obs", "]", ",", "xp", ",", "self", ".", "phi", ")", "\n", "\n", "if", "self", ".", "obs_normalizer", ":", "\n", "            ", "b_state", "=", "self", ".", "obs_normalizer", "(", "b_state", ",", "update", "=", "False", ")", "\n", "\n", "", "with", "chainer", ".", "using_config", "(", "'train'", ",", "False", ")", ",", "chainer", ".", "no_backprop_mode", "(", ")", ":", "\n", "            ", "action_distrib", ",", "vs_pred", "=", "self", ".", "model", "(", "b_state", ")", "\n", "action", "=", "chainer", ".", "cuda", ".", "to_cpu", "(", "action_distrib", ".", "sample", "(", ")", ".", "data", ")", "[", "0", "]", "\n", "\n", "# means = []", "\n", "# dims = len(action_distrib.mean.data[0])", "\n", "# for j in range(dims):", "\n", "#     means.append(cp.asnumpy(action_distrib.mean[0][j].data))", "\n", "#     # var = np.exp(cp.asnumpy(action_dist.ln_var[0][j].data))", "\n", "#     # std_devs.append(np.sqrt(var))", "\n", "\n", "# means = list(float(elem) for elem in means)", "\n", "# action = means", "\n", "\n", "#original ppo is to max entropy (min(-ent))", "\n", "#entropy = action_distrib.entropy", "\n", "\n", "#loss = vs_pred + loss_prob_ratio + loss_entropy", "\n", "\n", "", "return", "action", ",", "action_distrib", ",", "vs_pred", ",", "#loss", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ppo_adversary.PPO_Adversary._lossfun": [[195, 229], ["chainer.exp", "chainer.exp", "ppo_adversary.PPO_Adversary.value_loss_record.append", "ppo_adversary.PPO_Adversary.policy_loss_record.append", "chainer.mean", "chainer.mean", "chainer.mean_squared_error", "chainer.mean_squared_error", "chainer.mean", "chainer.mean", "chainer.mean", "chainer.mean", "float", "float", "chainer.minimum", "chainer.minimum", "chainer.maximum", "chainer.maximum", "chainer.square", "chainer.square", "chainer.square", "chainer.square", "chainer.clip", "chainer.clip", "ppo_adversary._elementwise_clip"], "methods", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ppo_adversary._elementwise_clip"], ["", "def", "_lossfun", "(", "self", ",", "\n", "distribs", ",", "vs_pred", ",", "log_probs", ",", "\n", "vs_pred_old", ",", "target_log_probs", ",", "\n", "advs", ",", "vs_teacher", ")", ":", "\n", "\n", "        ", "prob_ratio", "=", "F", ".", "exp", "(", "log_probs", "-", "target_log_probs", ")", "\n", "ent", "=", "distribs", ".", "entropy", "\n", "\n", "loss_policy", "=", "-", "F", ".", "mean", "(", "F", ".", "minimum", "(", "\n", "prob_ratio", "*", "advs", ",", "\n", "F", ".", "clip", "(", "prob_ratio", ",", "1", "-", "self", ".", "clip_eps", ",", "1", "+", "self", ".", "clip_eps", ")", "*", "advs", ")", ")", "\n", "\n", "if", "self", ".", "clip_eps_vf", "is", "None", ":", "\n", "            ", "loss_value_func", "=", "F", ".", "mean_squared_error", "(", "vs_pred", ",", "vs_teacher", ")", "\n", "", "else", ":", "\n", "            ", "loss_value_func", "=", "F", ".", "mean", "(", "F", ".", "maximum", "(", "\n", "F", ".", "square", "(", "vs_pred", "-", "vs_teacher", ")", ",", "\n", "F", ".", "square", "(", "_elementwise_clip", "(", "vs_pred", ",", "\n", "vs_pred_old", "-", "self", ".", "clip_eps_vf", ",", "\n", "vs_pred_old", "+", "self", ".", "clip_eps_vf", ")", "\n", "-", "vs_teacher", ")", "\n", ")", ")", "\n", "", "loss_entropy", "=", "-", "F", ".", "mean", "(", "ent", ")", "\n", "\n", "self", ".", "value_loss_record", ".", "append", "(", "float", "(", "loss_value_func", ".", "array", ")", ")", "\n", "self", ".", "policy_loss_record", ".", "append", "(", "float", "(", "loss_policy", ".", "array", ")", ")", "\n", "\n", "loss", "=", "(", "\n", "loss_policy", "\n", "+", "self", ".", "value_func_coef", "*", "loss_value_func", "\n", "+", "self", ".", "entropy_coef", "*", "loss_entropy", "\n", ")", "\n", "\n", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ppo_adversary._mean_or_nan": [[26, 29], ["numpy.mean"], "function", ["None"], ["def", "_mean_or_nan", "(", "xs", ")", ":", "\n", "    ", "\"\"\"Return its mean a non-empty sequence, numpy.nan for a empty one.\"\"\"", "\n", "return", "np", ".", "mean", "(", "xs", ")", "if", "xs", "else", "np", ".", "nan", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ppo_adversary._elementwise_clip": [[31, 37], ["chainer.minimum", "chainer.maximum"], "function", ["None"], ["", "def", "_elementwise_clip", "(", "x", ",", "x_min", ",", "x_max", ")", ":", "\n", "    ", "\"\"\"Elementwise clipping\n\n    Note: chainer.functions.clip supports clipping to constant intervals\n    \"\"\"", "\n", "return", "F", ".", "minimum", "(", "F", ".", "maximum", "(", "x", ",", "x_min", ")", ",", "x_max", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ddqn.ConvNet.__init__": [[34, 42], ["chainer.Chain.__init__", "simply_ddqn.ConvNet.init_scope", "L.Convolution2D", "L.Convolution2D", "L.Convolution2D", "L.Linear", "L.Linear"], "methods", ["home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.DDQN_Adversary.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "with", "self", ".", "init_scope", "(", ")", ":", "\n", "            ", "self", ".", "l1", "=", "L", ".", "Convolution2D", "(", "3", ",", "32", ",", "ksize", "=", "8", ",", "stride", "=", "4", ")", "\n", "self", ".", "l2", "=", "L", ".", "Convolution2D", "(", "32", ",", "64", ",", "ksize", "=", "4", ",", "stride", "=", "2", ")", "\n", "self", ".", "l3", "=", "L", ".", "Convolution2D", "(", "64", ",", "64", ",", "ksize", "=", "3", ",", "stride", "=", "1", ")", "\n", "self", ".", "fc1", "=", "L", ".", "Linear", "(", "None", ",", "512", ")", "\n", "self", ".", "fc2", "=", "L", ".", "Linear", "(", "512", ",", "5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ddqn.ConvNet.__call__": [[43, 55], ["simply_ddqn.ConvNet.l1", "F.relu", "simply_ddqn.ConvNet.l2", "F.relu", "F.dropout", "simply_ddqn.ConvNet.l3", "F.relu", "F.relu", "F.relu", "simply_ddqn.ConvNet.fc1", "simply_ddqn.ConvNet.fc2"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "x", ",", "test", "=", "False", ")", ":", "\n", "\n", "        ", "h", "=", "self", ".", "l1", "(", "x", ")", "\n", "h", "=", "F", ".", "relu", "(", "h", ")", "\n", "h", "=", "self", ".", "l2", "(", "h", ")", "\n", "h", "=", "F", ".", "relu", "(", "h", ")", "\n", "h", "=", "F", ".", "dropout", "(", "h", ",", "ratio", "=", "0.5", ")", "\n", "h", "=", "self", ".", "l3", "(", "h", ")", "\n", "h", "=", "F", ".", "relu", "(", "h", ")", "\n", "h", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "h", ")", ")", "\n", "h", "=", "F", ".", "relu", "(", "self", ".", "fc2", "(", "h", ")", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.simply_ddqn.main": [[56, 235], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "chainerrl.misc.set_random_seed", "chainerrl.experiments.prepare_output_dir", "print", "simply_ddqn.main.make_env"], "function", ["None"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--outdir'", ",", "type", "=", "str", ",", "default", "=", "'results'", ",", "\n", "help", "=", "'Directory path to save output files.'", "\n", "' If it does not exist, it will be created.'", ")", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "type", "=", "str", ",", "default", "=", "'BipedalWalkerHardcore-v2'", ")", "#MountainCarContinuous-v0, BipedalWalker-v2, BipedalWalkerHardcore-v2,CarRacing-v0", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "5", ",", "\n", "help", "=", "'Random seed [0, 2 ** 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--final-exploration-steps'", ",", "\n", "type", "=", "int", ",", "default", "=", "10", "**", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--start-epsilon'", ",", "type", "=", "float", ",", "default", "=", "1.0", ")", "\n", "parser", ".", "add_argument", "(", "'--end-epsilon'", ",", "type", "=", "float", ",", "default", "=", "0.1", ")", "\n", "parser", ".", "add_argument", "(", "'--load'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "#DDQNBipedalWalker-v2_Run2", "\n", "parser", ".", "add_argument", "(", "'--n_episodes'", ",", "type", "=", "int", ",", "default", "=", "7000", ")", "\n", "parser", ".", "add_argument", "(", "'--prioritized-replay'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--episodic-replay'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--replay-start-size'", ",", "type", "=", "int", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'--target-update-interval'", ",", "type", "=", "int", ",", "default", "=", "500", ")", "#500 ", "\n", "parser", ".", "add_argument", "(", "'--target-update-method'", ",", "type", "=", "str", ",", "default", "=", "'hard'", ")", "\n", "parser", ".", "add_argument", "(", "'--soft-update-tau'", ",", "type", "=", "float", ",", "default", "=", "1e-2", ")", "\n", "parser", ".", "add_argument", "(", "'--update-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--n-hidden-channels'", ",", "type", "=", "int", ",", "default", "=", "200", ")", "#128", "\n", "parser", ".", "add_argument", "(", "'--n-hidden-layers'", ",", "type", "=", "int", ",", "default", "=", "3", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "1e-4", ")", "#1e-4", "\n", "parser", ".", "add_argument", "(", "'--minibatch-size'", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "'--render'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--monitor'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--reward-scale-factor'", ",", "type", "=", "float", ",", "default", "=", "1e-2", ")", "#1e-2", "\n", "parser", ".", "add_argument", "(", "'--run_id'", ",", "type", "=", "str", ",", "default", "=", "'_Run1'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# Set a random seed used in ChainerRL", "\n", "misc", ".", "set_random_seed", "(", "args", ".", "seed", ",", "gpus", "=", "(", "args", ".", "gpu", ",", ")", ")", "\n", "\n", "args", ".", "outdir", "=", "experiments", ".", "prepare_output_dir", "(", "\n", "args", ",", "args", ".", "outdir", ",", "argv", "=", "sys", ".", "argv", ")", "\n", "print", "(", "'Output files are saved in {}'", ".", "format", "(", "args", ".", "outdir", ")", ")", "\n", "\n", "def", "clip_action_filter", "(", "a", ")", ":", "\n", "        ", "return", "cp", ".", "clip", "(", "a", ",", "action_space", ".", "low", ",", "action_space", ".", "high", ")", "\n", "\n", "", "def", "make_env", "(", "test", ")", ":", "\n", "        ", "env", "=", "gym", ".", "make", "(", "args", ".", "env", ")", "\n", "# Use different random seeds for train and test envs", "\n", "env_seed", "=", "2", "**", "32", "-", "1", "-", "args", ".", "seed", "if", "test", "else", "args", ".", "seed", "\n", "env", ".", "seed", "(", "env_seed", ")", "\n", "# Cast observations to float32 because our model uses float32", "\n", "env", "=", "chainerrl", ".", "wrappers", ".", "CastObservationToFloat32", "(", "env", ")", "\n", "if", "args", ".", "monitor", ":", "\n", "            ", "env", "=", "gym", ".", "wrappers", ".", "Monitor", "(", "env", ",", "args", ".", "outdir", ")", "\n", "", "if", "isinstance", "(", "env", ".", "action_space", ",", "spaces", ".", "Box", ")", ":", "\n", "            ", "misc", ".", "env_modifiers", ".", "make_action_filtered", "(", "env", ",", "clip_action_filter", ")", "\n", "", "if", "not", "test", ":", "\n", "# Scale rewards (and thus returns) to a reasonable range so that", "\n", "# training is easier", "\n", "            ", "env", "=", "chainerrl", ".", "wrappers", ".", "ScaleReward", "(", "env", ",", "args", ".", "reward_scale_factor", ")", "\n", "# if args.render:", "\n", "#     env = chainerrl.wrappers.Render(env)", "\n", "", "return", "env", "\n", "\n", "", "env", "=", "make_env", "(", "test", "=", "False", ")", "\n", "eval_env", "=", "make_env", "(", "test", "=", "True", ")", "\n", "timestep_limit", "=", "env", ".", "spec", ".", "tags", ".", "get", "(", "\n", "'wrapper_config.TimeLimit.max_episode_steps'", ")", "\n", "obs_space", "=", "env", ".", "observation_space", "\n", "obs_size", "=", "obs_space", ".", "low", ".", "size", "\n", "action_space", "=", "env", ".", "action_space", "\n", "\n", "if", "isinstance", "(", "action_space", ",", "spaces", ".", "Box", ")", ":", "\n", "        ", "action_size", "=", "action_space", ".", "low", ".", "size", "\n", "# Use NAF to apply DQN to continuous action spaces", "\n", "# q_func = q_functions.FCQuadraticStateQFunction(", "\n", "#     obs_size, action_size,", "\n", "#     n_hidden_channels=args.n_hidden_channels,", "\n", "#     n_hidden_layers=args.n_hidden_layers,", "\n", "#     action_space=action_space)", "\n", "\n", "q_func", "=", "q_functions", ".", "FCBNQuadraticStateQFunction", "(", "\n", "obs_size", ",", "action_size", ",", "\n", "n_hidden_channels", "=", "args", ".", "n_hidden_channels", ",", "\n", "n_hidden_layers", "=", "args", ".", "n_hidden_layers", ",", "\n", "action_space", "=", "action_space", ")", "\n", "\n", "# q_func = q_functions.FCLSTMQuadraticStateQFunction(", "\n", "#     obs_size, action_size,", "\n", "#     n_hidden_channels=args.n_hidden_channels,", "\n", "#     n_hidden_layers=args.n_hidden_layers,", "\n", "#     action_space=action_space)", "\n", "# Use the Ornstein-Uhlenbeck process for exploration", "\n", "ou_sigma", "=", "(", "action_space", ".", "high", "-", "action_space", ".", "low", ")", "*", "0.0015", "\n", "explorer", "=", "explorers", ".", "AdditiveOU", "(", "sigma", "=", "ou_sigma", ")", "\n", "#explorer = norms.DecayAdditiveOU(sigma=0.5, end_sigma=ou_sigma, decay_steps=1e4)", "\n", "\n", "", "else", ":", "\n", "        ", "n_actions", "=", "action_space", ".", "n", "\n", "q_func", "=", "q_functions", ".", "FCStateQFunctionWithDiscreteAction", "(", "\n", "obs_size", ",", "n_actions", ",", "\n", "n_hidden_channels", "=", "args", ".", "n_hidden_channels", ",", "\n", "n_hidden_layers", "=", "args", ".", "n_hidden_layers", ")", "\n", "# Use epsilon-greedy for exploration", "\n", "explorer", "=", "explorers", ".", "LinearDecayEpsilonGreedy", "(", "\n", "args", ".", "start_epsilon", ",", "args", ".", "end_epsilon", ",", "args", ".", "final_exploration_steps", ",", "\n", "action_space", ".", "sample", ")", "\n", "\n", "", "q_func", ".", "to_gpu", "(", "args", ".", "gpu", ")", "\n", "opt", "=", "optimizers", ".", "Adam", "(", "args", ".", "lr", ")", "\n", "opt", ".", "setup", "(", "q_func", ")", "\n", "\n", "rbuf_capacity", "=", "5", "*", "10", "**", "5", "\n", "if", "args", ".", "episodic_replay", ":", "\n", "        ", "if", "args", ".", "minibatch_size", "is", "None", ":", "\n", "            ", "args", ".", "minibatch_size", "=", "4", "\n", "", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "betasteps", "=", "(", "args", ".", "steps", "-", "args", ".", "replay_start_size", ")", "//", "args", ".", "update_interval", "\n", "rbuf", "=", "replay_buffer", ".", "PrioritizedEpisodicReplayBuffer", "(", "\n", "rbuf_capacity", ",", "betasteps", "=", "betasteps", ")", "\n", "", "else", ":", "\n", "            ", "rbuf", "=", "replay_buffer", ".", "EpisodicReplayBuffer", "(", "rbuf_capacity", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "args", ".", "minibatch_size", "is", "None", ":", "\n", "            ", "args", ".", "minibatch_size", "=", "32", "\n", "", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "betasteps", "=", "(", "args", ".", "steps", "-", "args", ".", "replay_start_size", ")", "//", "args", ".", "update_interval", "\n", "rbuf", "=", "replay_buffer", ".", "PrioritizedReplayBuffer", "(", "\n", "rbuf_capacity", ",", "betasteps", "=", "betasteps", ")", "\n", "", "else", ":", "\n", "            ", "rbuf", "=", "replay_buffer", ".", "ReplayBuffer", "(", "rbuf_capacity", ")", "\n", "\n", "", "", "agent", "=", "DoubleDQN", "(", "q_func", ",", "opt", ",", "rbuf", ",", "gpu", "=", "args", ".", "gpu", ",", "gamma", "=", "args", ".", "gamma", ",", "\n", "explorer", "=", "explorer", ",", "replay_start_size", "=", "args", ".", "replay_start_size", ",", "\n", "target_update_interval", "=", "args", ".", "target_update_interval", ",", "\n", "update_interval", "=", "args", ".", "update_interval", ",", "\n", "minibatch_size", "=", "args", ".", "minibatch_size", ",", "\n", "target_update_method", "=", "args", ".", "target_update_method", ",", "\n", "soft_update_tau", "=", "args", ".", "soft_update_tau", ",", "\n", "episodic_update", "=", "args", ".", "episodic_replay", ",", "episodic_update_len", "=", "16", ")", "\n", "\n", "if", "args", ".", "load", "is", "not", "None", ":", "\n", "        ", "agent", ".", "load", "(", "args", ".", "load", ")", "\n", "\n", "", "print", "(", "'Environment Time Step Limit'", ",", "timestep_limit", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "args", ".", "n_episodes", "+", "1", ")", ":", "\n", "        ", "obs", "=", "env", ".", "reset", "(", ")", "\n", "reward", "=", "0", "\n", "done", "=", "False", "\n", "R", "=", "0", "\n", "t", "=", "0", "\n", "while", "not", "done", "and", "t", "<", "timestep_limit", ":", "\n", "#env.render()", "\n", "            ", "action", "=", "agent", ".", "act_and_train", "(", "obs", ",", "reward", ")", "\n", "obs", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "R", "+=", "reward", "\n", "t", "+=", "1", "\n", "", "if", "i", "%", "10", "==", "0", ":", "\n", "            ", "print", "(", "'episode:'", ",", "i", ",", "\n", "'R:'", ",", "R", ",", "\n", "'statistics:'", ",", "agent", ".", "get_statistics", "(", ")", ")", "\n", "", "agent", ".", "stop_episode_and_train", "(", "obs", ",", "reward", ",", "done", ")", "\n", "", "agent", ".", "save", "(", "'DDQN'", "+", "args", ".", "env", "+", "args", ".", "run_id", ")", "\n", "\n", "for", "i", "in", "range", "(", "10", ")", ":", "\n", "        ", "obs", "=", "env", ".", "reset", "(", ")", "\n", "done", "=", "False", "\n", "R", "=", "0", "\n", "t", "=", "0", "\n", "while", "not", "done", "and", "t", "<", "timestep_limit", ":", "\n", "            ", "env", ".", "render", "(", ")", "\n", "action", "=", "agent", ".", "act", "(", "obs", ")", "\n", "obs", ",", "r", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "R", "+=", "r", "\n", "t", "+=", "1", "\n", "", "print", "(", "'test episode:'", ",", "i", ",", "'R:'", ",", "R", ")", "\n", "agent", ".", "stop_episode", "(", ")", "\n", "\n", "", "print", "(", "\"Finished\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.DDQN_Adversary.__init__": [[79, 149], ["logging.getLogger", "chainerrl.replay_buffer.ReplayUpdater", "ddqn_adversary.DDQN_Adversary.sync_target_network", "chainer.cuda.get_device().use", "chainer.cuda.get_device().use", "ddqn_adversary.DDQN_Adversary.model.to_gpu", "ValueError", "chainer.cuda.get_device", "chainer.cuda.get_device"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "q_function", ",", "optimizer", ",", "replay_buffer", ",", "gamma", ",", "\n", "explorer", ",", "gpu", "=", "None", ",", "replay_start_size", "=", "50000", ",", "\n", "minibatch_size", "=", "32", ",", "update_interval", "=", "1", ",", "\n", "target_update_interval", "=", "10000", ",", "clip_delta", "=", "True", ",", "\n", "phi", "=", "lambda", "x", ":", "x", ",", "\n", "target_update_method", "=", "'hard'", ",", "\n", "soft_update_tau", "=", "1e-2", ",", "\n", "n_times_update", "=", "1", ",", "average_q_decay", "=", "0.999", ",", "\n", "average_loss_decay", "=", "0.99", ",", "\n", "batch_accumulator", "=", "'mean'", ",", "episodic_update", "=", "False", ",", "\n", "episodic_update_len", "=", "None", ",", "\n", "logger", "=", "getLogger", "(", "__name__", ")", ",", "\n", "batch_states", "=", "batch_states", ")", ":", "\n", "\n", "        ", "self", ".", "model", "=", "q_function", "\n", "self", ".", "adv_model", "=", "q_function", "# To load same weights", "\n", "self", ".", "q_function", "=", "q_function", "# For backward compatibility", "\n", "\n", "if", "gpu", "is", "not", "None", "and", "gpu", ">=", "0", ":", "\n", "            ", "cuda", ".", "get_device", "(", "gpu", ")", ".", "use", "(", ")", "\n", "self", ".", "model", ".", "to_gpu", "(", "device", "=", "gpu", ")", "\n", "\n", "", "self", ".", "xp", "=", "self", ".", "model", ".", "xp", "\n", "self", ".", "replay_buffer", "=", "replay_buffer", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "explorer", "=", "explorer", "\n", "self", ".", "gpu", "=", "gpu", "\n", "self", ".", "target_update_interval", "=", "target_update_interval", "\n", "self", ".", "clip_delta", "=", "clip_delta", "\n", "self", ".", "phi", "=", "phi", "\n", "self", ".", "target_update_method", "=", "target_update_method", "\n", "self", ".", "soft_update_tau", "=", "soft_update_tau", "\n", "self", ".", "batch_accumulator", "=", "batch_accumulator", "\n", "assert", "batch_accumulator", "in", "(", "'mean'", ",", "'sum'", ")", "\n", "self", ".", "logger", "=", "logger", "\n", "self", ".", "batch_states", "=", "batch_states", "\n", "if", "episodic_update", ":", "\n", "            ", "update_func", "=", "self", ".", "update_from_episodes", "\n", "", "else", ":", "\n", "            ", "update_func", "=", "self", ".", "update", "\n", "", "self", ".", "replay_updater", "=", "ReplayUpdater", "(", "\n", "replay_buffer", "=", "replay_buffer", ",", "\n", "update_func", "=", "update_func", ",", "\n", "batchsize", "=", "minibatch_size", ",", "\n", "episodic_update", "=", "episodic_update", ",", "\n", "episodic_update_len", "=", "episodic_update_len", ",", "\n", "n_times_update", "=", "n_times_update", ",", "\n", "replay_start_size", "=", "replay_start_size", ",", "\n", "update_interval", "=", "update_interval", ",", "\n", ")", "\n", "\n", "self", ".", "t", "=", "0", "\n", "self", ".", "last_state", "=", "None", "\n", "self", ".", "last_action", "=", "None", "\n", "self", ".", "target_model", "=", "None", "\n", "self", ".", "sync_target_network", "(", ")", "\n", "# For backward compatibility", "\n", "self", ".", "target_q_function", "=", "self", ".", "target_model", "\n", "self", ".", "average_q", "=", "0", "\n", "self", ".", "average_q_decay", "=", "average_q_decay", "\n", "self", ".", "average_loss", "=", "0", "\n", "self", ".", "average_loss_decay", "=", "average_loss_decay", "\n", "\n", "# Error checking", "\n", "if", "(", "self", ".", "replay_buffer", ".", "capacity", "is", "not", "None", "and", "\n", "self", ".", "replay_buffer", ".", "capacity", "<", "\n", "self", ".", "replay_updater", ".", "replay_start_size", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Replay start size cannot exceed '", "\n", "'replay buffer capacity.'", ")", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.DDQN_Adversary._compute_target_values": [[152, 169], ["ddqn_adversary.DDQN_Adversary.target_q_function", "ddqn_adversary.DDQN_Adversary.evaluate_actions", "chainer.using_config", "chainer.using_config", "chainer.using_config", "chainer.using_config", "chainerrl.recurrent.state_kept", "ddqn_adversary.DDQN_Adversary.q_function"], "methods", ["None"], ["", "", "def", "_compute_target_values", "(", "self", ",", "exp_batch", ")", ":", "\n", "\n", "        ", "batch_next_state", "=", "exp_batch", "[", "'next_state'", "]", "\n", "\n", "with", "chainer", ".", "using_config", "(", "'train'", ",", "False", ")", ",", "state_kept", "(", "self", ".", "q_function", ")", ":", "\n", "            ", "next_qout", "=", "self", ".", "q_function", "(", "batch_next_state", ")", "\n", "\n", "", "target_next_qout", "=", "self", ".", "target_q_function", "(", "batch_next_state", ")", "\n", "\n", "next_q_max", "=", "target_next_qout", ".", "evaluate_actions", "(", "\n", "next_qout", ".", "greedy_actions", ")", "\n", "\n", "batch_rewards", "=", "exp_batch", "[", "'reward'", "]", "\n", "batch_terminal", "=", "exp_batch", "[", "'is_state_terminal'", "]", "\n", "discount", "=", "exp_batch", "[", "'discount'", "]", "\n", "\n", "return", "batch_rewards", "+", "discount", "*", "(", "1.0", "-", "batch_terminal", ")", "*", "next_q_max", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.DDQN_Adversary.act": [[170, 183], ["ddqn_adversary.DDQN_Adversary.logger.debug", "chainer.using_config", "chainer.using_config", "chainer.using_config", "chainer.using_config", "chainer.no_backprop_mode", "chainer.no_backprop_mode", "chainer.no_backprop_mode", "chainer.no_backprop_mode", "ddqn_adversary.DDQN_Adversary.model", "float", "ddqn_adversary.DDQN_Adversary.batch_states", "chainer.cuda.to_cpu", "chainer.cuda.to_cpu"], "methods", ["None"], ["", "def", "act", "(", "self", ",", "obs", ")", ":", "\n", "        ", "with", "chainer", ".", "using_config", "(", "'train'", ",", "False", ")", ",", "chainer", ".", "no_backprop_mode", "(", ")", ":", "\n", "            ", "action_value", "=", "self", ".", "model", "(", "\n", "self", ".", "batch_states", "(", "[", "obs", "]", ",", "self", ".", "xp", ",", "self", ".", "phi", ")", ")", "\n", "q", "=", "float", "(", "action_value", ".", "max", ".", "array", ")", "\n", "action", "=", "cuda", ".", "to_cpu", "(", "action_value", ".", "greedy_actions", ".", "array", ")", "[", "0", "]", "\n", "\n", "# Update stats", "\n", "", "self", ".", "average_q", "*=", "self", ".", "average_q_decay", "\n", "self", ".", "average_q", "+=", "(", "1", "-", "self", ".", "average_q_decay", ")", "*", "q", "\n", "\n", "self", ".", "logger", ".", "debug", "(", "'t:%s q:%s action_value:%s'", ",", "self", ".", "t", ",", "q", ",", "action_value", ")", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.DDQN_Adversary.act_forward": [[184, 208], ["ddqn_adversary.DDQN_Adversary.logger.debug", "chainer.using_config", "chainer.using_config", "chainer.using_config", "chainer.using_config", "chainer.no_backprop_mode", "chainer.no_backprop_mode", "chainer.no_backprop_mode", "chainer.no_backprop_mode", "ddqn_adversary.DDQN_Adversary.model", "float", "chainerrl.action_value.QuadraticActionValue().evaluate_actions", "ddqn_adversary.DDQN_Adversary.batch_states", "chainer.cuda.to_cpu", "chainer.cuda.to_cpu", "chainerrl.action_value.QuadraticActionValue"], "methods", ["None"], ["", "def", "act_forward", "(", "self", ",", "obs", ",", "adv_action", ")", ":", "\n", "        ", "with", "chainer", ".", "using_config", "(", "'train'", ",", "False", ")", ",", "chainer", ".", "no_backprop_mode", "(", ")", ":", "\n", "            ", "action_value", "=", "self", ".", "model", "(", "self", ".", "batch_states", "(", "[", "obs", "]", ",", "self", ".", "xp", ",", "self", ".", "phi", ")", ")", "\n", "q", "=", "float", "(", "action_value", ".", "max", ".", "array", ")", "\n", "action", "=", "cuda", ".", "to_cpu", "(", "action_value", ".", "greedy_actions", ".", "array", ")", "[", "0", "]", "\n", "\n", "#Q(s,a|theta) =  A(s,a|theta) + V(s|theta) as a NAF", "\n", "#V(s|theta) is constant", "\n", "#A(s,a|theta) = -1/2 (a - mu(s))^T P(s) (a - mu(s))", "\n", "#Only u is changing in the equation above to compute advantage", "\n", "\n", "v", "=", "action_value", ".", "v", "\n", "mat", "=", "action_value", ".", "mat", "\n", "mu", "=", "action_value", ".", "mu", "\n", "q_adv", "=", "QuadraticActionValue", "(", "mu", ",", "mat", ",", "v", ",", "[", "-", "1", "-", "1", "]", ",", "[", "1", ",", "1", "]", ")", ".", "evaluate_actions", "(", "adv_action", ")", "\n", "q_adv", "=", "q_adv", ".", "data", "[", "0", "]", "\n", "\n", "# Update stats", "\n", "", "self", ".", "average_q", "*=", "self", ".", "average_q_decay", "\n", "self", ".", "average_q", "+=", "(", "1", "-", "self", ".", "average_q_decay", ")", "*", "q", "\n", "\n", "self", ".", "logger", ".", "debug", "(", "'t:%s q:%s action_value:%s'", ",", "self", ".", "t", ",", "q", ",", "action_value", ")", "\n", "\n", "return", "action", ",", "q", ",", "q_adv", "\n", "\n"]], "home.repos.pwc.inspect_result.xylee95_Spatiotemporal-Attack-On-Deep-RL-Agents.None.ddqn_adversary.scale_by_tanh": [[38, 45], ["chainer.cuda.get_array_module", "cuda.get_array_module.expand_dims", "cuda.get_array_module.expand_dims", "cuda.get_array_module.asarray", "cuda.get_array_module.asarray", "chainer.tanh"], "function", ["None"], ["def", "scale_by_tanh", "(", "x", ",", "low", ",", "high", ")", ":", "\n", "    ", "xp", "=", "cuda", ".", "get_array_module", "(", "x", ".", "array", ")", "\n", "scale", "=", "(", "high", "-", "low", ")", "/", "2", "\n", "scale", "=", "xp", ".", "expand_dims", "(", "xp", ".", "asarray", "(", "scale", ",", "dtype", "=", "np", ".", "float32", ")", ",", "axis", "=", "0", ")", "\n", "mean", "=", "(", "high", "+", "low", ")", "/", "2", "\n", "mean", "=", "xp", ".", "expand_dims", "(", "xp", ".", "asarray", "(", "mean", ",", "dtype", "=", "np", ".", "float32", ")", ",", "axis", "=", "0", ")", "\n", "return", "F", ".", "tanh", "(", "x", ")", "*", "scale", "+", "mean", "\n", "\n"]]}