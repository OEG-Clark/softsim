{"home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.read_file.read_raw_text": [[7, 18], ["time.time", "sum", "print", "open", "tqdm.tqdm", "line.split", "open", "any", "all_text.append", "time.time", "len", "line.strip"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["def", "read_raw_text", "(", "fname", ",", "input_keywords", ")", ":", "\n", "    ", "start", "=", "time", ".", "time", "(", ")", "\n", "all_text", "=", "[", "]", "\n", "num_lines", "=", "sum", "(", "1", "for", "line", "in", "open", "(", "fname", ",", "'r'", ")", ")", "\n", "with", "open", "(", "fname", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "fin", ",", "total", "=", "num_lines", ")", ":", "\n", "            ", "temp", "=", "line", ".", "split", "(", ")", "\n", "if", "any", "(", "ele", "in", "temp", "for", "ele", "in", "input_keywords", ")", "and", "len", "(", "line", ")", "<=", "150", ":", "\n", "                ", "all_text", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "", "", "print", "(", "'[read_data.py] Finish reading data using %.2fs'", "%", "(", "time", ".", "time", "(", ")", "-", "start", ")", ")", "\n", "return", "all_text", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.read_file.read_input_and_ground_truth": [[20, 38], ["collections.defaultdict", "sorted", "open", "list", "open", "[].strip().lower", "[].split", "set", "line.strip().split", "euphemism_answer[].append", "[].strip", "line.strip", "line.split", "collections.defaultdict.values", "i.strip", "line.split", "i.strip().lower", "i.strip"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["", "def", "read_input_and_ground_truth", "(", "target_category_name", ")", ":", "\n", "    ", "fname_euphemism_answer", "=", "'./data/euphemism_answer_'", "+", "target_category_name", "+", "'.txt'", "\n", "fname_target_keywords_name", "=", "'./data/target_keywords_'", "+", "target_category_name", "+", "'.txt'", "\n", "euphemism_answer", "=", "defaultdict", "(", "list", ")", "\n", "with", "open", "(", "fname_euphemism_answer", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "ans", "=", "line", ".", "split", "(", "':'", ")", "[", "0", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "for", "i", "in", "line", ".", "split", "(", "':'", ")", "[", "1", "]", ".", "split", "(", "';'", ")", ":", "\n", "                ", "euphemism_answer", "[", "i", ".", "strip", "(", ")", ".", "lower", "(", ")", "]", ".", "append", "(", "ans", ")", "\n", "", "", "", "input_keywords", "=", "sorted", "(", "list", "(", "set", "(", "[", "y", "for", "x", "in", "euphemism_answer", ".", "values", "(", ")", "for", "y", "in", "x", "]", ")", ")", ")", "\n", "target_name", "=", "{", "}", "\n", "count", "=", "0", "\n", "with", "open", "(", "fname_target_keywords_name", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "for", "i", "in", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", ":", "\n", "                ", "target_name", "[", "i", ".", "strip", "(", ")", "]", "=", "count", "\n", "", "count", "+=", "1", "\n", "", "", "return", "euphemism_answer", ",", "input_keywords", ",", "target_name", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.read_file.read_all_data": [[40, 48], ["print", "read_file.read_input_and_ground_truth", "read_file.read_raw_text"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.read_file.read_input_and_ground_truth", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.read_file.read_raw_text"], ["", "def", "read_all_data", "(", "dataset_name", ",", "target_category_name", ")", ":", "\n", "    ", "\"\"\" target_name is a dict (key: a target keyword, value: index). This is for later classification purpose, since\n        different target keyword can refer to the same concept (e.g., 'alprazolam' and 'xanax', 'ecstasy' and 'mdma').\n    \"\"\"", "\n", "print", "(", "'[read_data.py] Reading data...'", ")", "\n", "euphemism_answer", ",", "input_keywords", ",", "target_name", "=", "read_input_and_ground_truth", "(", "target_category_name", ")", "\n", "all_text", "=", "read_raw_text", "(", "'./data/text/'", "+", "dataset_name", "+", "'.txt'", ",", "input_keywords", ")", "\n", "return", "all_text", ",", "euphemism_answer", ",", "input_keywords", ",", "target_name", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.euphemism_identification": [[17, 57], ["print", "identification.get_final_test", "identification.get_train_test_data", "print", "print", "print", "print", "identification.train_LRT_classifier", "identification.get_filtered_final_out", "identification.train_initialization", "range", "identification.convert_final_test_output_to_final_out", "identification.get_filtered_final_out", "identification.train_LRT_classifier", "identification.train_initialization", "range", "identification.convert_final_test_output_to_final_out", "identification.train_model", "identification.eval_model", "print", "identification.eval_model", "torch.Tensor().long", "identification.train_model", "identification.eval_model", "print", "identification.eval_model", "torch.Tensor().long", "torch.Tensor", "torch.Tensor"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.get_final_test", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.get_train_test_data", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.train_LRT_classifier", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.get_filtered_final_out", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.train_initialization", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.convert_final_test_output_to_final_out", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.get_filtered_final_out", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.train_LRT_classifier", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.train_initialization", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.convert_final_test_output_to_final_out", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.train_model", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.eval_model", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.eval_model", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.train_model", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.eval_model", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.eval_model"], ["def", "euphemism_identification", "(", "top_words", ",", "all_text", ",", "euphemism_answer", ",", "input_keywords", ",", "target_name", ",", "args", ")", ":", "\n", "    ", "print", "(", "'\\n'", "+", "'*'", "*", "40", "+", "' [Euphemism Identification] '", "+", "'*'", "*", "40", ")", "\n", "''' Construct Training Dataset'''", "\n", "all_classifiers", "=", "[", "'LRT'", ",", "'LSTM'", ",", "'LSTMAtten'", ",", "'RNN'", ",", "'RCNN'", ",", "'SelfAttention'", "]", "\n", "classifier_1", "=", "all_classifiers", "[", "args", ".", "c1", "]", "\n", "NGRAMS", "=", "1", "\n", "final_test", "=", "get_final_test", "(", "euphemism_answer", ",", "top_words", ",", "input_keywords", ")", "\n", "train_data", ",", "test_data", ",", "final_test_data", ",", "train_data_pre", ",", "test_data_pre", ",", "unique_vocab_dict", ",", "unique_vocab_list", "=", "get_train_test_data", "(", "input_keywords", ",", "target_name", ",", "all_text", ",", "final_test", ",", "NGRAMS", ",", "train_perc", "=", "0.8", ")", "\n", "final_test_output", ",", "final_test_data_t", "=", "[", "]", ",", "[", "]", "# For faster computation", "\n", "\n", "if", "args", ".", "coarse", ":", "\n", "        ", "print", "(", "'-'", "*", "40", "+", "' [Coarse Binary Classifier] '", "+", "'-'", "*", "40", ")", "\n", "print", "(", "'Model: '", "+", "classifier_1", ")", "\n", "if", "classifier_1", "in", "[", "'LRT'", "]", ":", "\n", "            ", "model", ",", "final_out", ",", "final_test_output", ",", "final_test_data_t", "=", "train_LRT_classifier", "(", "train_data_pre", ",", "test_data_pre", ",", "final_test_data", ",", "final_test", ",", "unique_vocab_dict", ",", "unique_vocab_list", ",", "target_name", ",", "IsPre", "=", "1", ",", "has_coarse", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "train_iter", ",", "test_iter", ",", "Final_test_iter", ",", "lr", ",", "epoch_num", ",", "model", ",", "loss_fn", "=", "train_initialization", "(", "classifier_1", ",", "train_data_pre", ",", "test_data_pre", ",", "final_test_data", ",", "target_name", ",", "IsPre", "=", "1", ")", "\n", "for", "epoch", "in", "range", "(", "epoch_num", ")", ":", "\n", "                ", "train_loss", ",", "train_acc", "=", "train_model", "(", "model", ",", "train_iter", ",", "loss_fn", ",", "lr", ",", "epoch", ")", "\n", "test_loss", ",", "test_acc", ",", "_", "=", "eval_model", "(", "model", ",", "test_iter", ",", "loss_fn", ")", "\n", "print", "(", "f'Epoch: {epoch + 1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:3f}, Test Acc: {test_acc:.2f}%'", ")", "\n", "_", ",", "_", ",", "final_test_output", "=", "eval_model", "(", "model", ",", "Final_test_iter", ",", "loss_fn", ")", "\n", "", "convert_final_test_output_to_final_out", "(", "final_test_output", ",", "target_name", ",", "final_test", ",", "torch", ".", "Tensor", "(", "[", "i", "[", "1", "]", "for", "i", "in", "final_test_data", "]", ")", ".", "long", "(", ")", ",", "IsPre", "=", "1", ",", "has_coarse", "=", "0", ")", "\n", "\n", "", "", "print", "(", "'\\n'", "+", "'-'", "*", "40", "+", "' [Fine-grained Multi-class Classifer] '", "+", "'-'", "*", "40", ")", "\n", "classifier_2", "=", "all_classifiers", "[", "args", ".", "c2", "]", "\n", "print", "(", "'Model: '", "+", "classifier_2", ")", "\n", "if", "classifier_2", "in", "[", "'LRT'", "]", ":", "\n", "        ", "model", ",", "final_out", ",", "_", ",", "_", "=", "train_LRT_classifier", "(", "train_data", ",", "test_data", ",", "final_test_data", ",", "final_test", ",", "unique_vocab_dict", ",", "unique_vocab_list", ",", "target_name", ",", "0", ",", "args", ".", "coarse", ",", "final_test_output", ",", "final_test_data_t", ")", "\n", "get_filtered_final_out", "(", "final_out", ",", "final_test", ",", "input_keywords", ",", "target_name", ")", "\n", "", "else", ":", "\n", "        ", "train_iter", ",", "test_iter", ",", "Final_test_iter", ",", "lr", ",", "epoch_num", ",", "model", ",", "loss_fn", "=", "train_initialization", "(", "classifier_2", ",", "train_data", ",", "test_data", ",", "final_test_data", ",", "target_name", ")", "\n", "for", "epoch", "in", "range", "(", "epoch_num", ")", ":", "\n", "            ", "train_loss", ",", "train_acc", "=", "train_model", "(", "model", ",", "train_iter", ",", "loss_fn", ",", "lr", ",", "epoch", ")", "\n", "test_loss", ",", "test_acc", ",", "_", "=", "eval_model", "(", "model", ",", "test_iter", ",", "loss_fn", ")", "\n", "print", "(", "f'Epoch: {epoch + 1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:3f}, Test Acc: {test_acc:.2f}%'", ")", "\n", "_", ",", "_", ",", "final_test_output", "=", "eval_model", "(", "model", ",", "Final_test_iter", ",", "loss_fn", ")", "\n", "", "final_out", "=", "convert_final_test_output_to_final_out", "(", "final_test_output", ",", "target_name", ",", "final_test", ",", "torch", ".", "Tensor", "(", "[", "i", "[", "1", "]", "for", "i", "in", "final_test_data", "]", ")", ".", "long", "(", ")", ",", "0", ",", "args", ".", "coarse", ",", "final_test_output", ")", "\n", "get_filtered_final_out", "(", "final_out", ",", "final_test", ",", "input_keywords", ",", "target_name", ")", "\n", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.train_LRT_classifier": [[60, 123], ["print", "identification.train_LRT_classifier.transform_data"], "function", ["None"], ["def", "train_LRT_classifier", "(", "train_data", ",", "test_data", ",", "final_test_data", ",", "final_test", ",", "unique_vocab_dict", ",", "unique_vocab_list", ",", "target_name", ",", "IsPre", ",", "has_coarse", "=", "0", ",", "final_test_output_pre", "=", "[", "]", ",", "final_test_data_t", "=", "[", "]", ")", ":", "\n", "    ", "def", "transform_data", "(", "a_dataset", ",", "unique_vocab_dict", ",", "NGRAMS", ")", ":", "\n", "        ", "train_X", "=", "torch", ".", "zeros", "(", "len", "(", "a_dataset", ")", ",", "len", "(", "unique_vocab_dict", ")", ")", "\n", "for", "i", "in", "tqdm", "(", "range", "(", "len", "(", "a_dataset", ")", ")", ")", ":", "\n", "            ", "tokens", "=", "nltk", ".", "word_tokenize", "(", "a_dataset", "[", "i", "]", "[", "0", "]", ")", "\n", "tokens", "=", "tokens", "if", "NGRAMS", "==", "1", "else", "ngrams_iterator", "(", "tokens", ",", "NGRAMS", ")", "\n", "for", "j", "in", "tokens", ":", "\n", "                ", "if", "j", "in", "[", "string", ".", "punctuation", ",", "'to'", ",", "'and'", ",", "'the'", ",", "'be'", ",", "'a'", ",", "'is'", ",", "'that'", ",", "'of'", "]", ":", "\n", "                    ", "continue", "\n", "", "try", ":", "\n", "                    ", "train_X", "[", "i", "]", "[", "unique_vocab_dict", "[", "j", "]", "]", "+=", "0.5", "\n", "", "except", ":", "\n", "                    ", "pass", "\n", "", "", "", "train_Y", "=", "torch", ".", "Tensor", "(", "[", "i", "[", "1", "]", "for", "i", "in", "a_dataset", "]", ")", ".", "long", "(", ")", "\n", "return", "train_X", ",", "train_Y", "\n", "\n", "", "print", "(", "'[utils.py] Transforming datasets...'", ")", "\n", "train_X", ",", "train_Y", "=", "transform_data", "(", "train_data", ",", "unique_vocab_dict", ",", "NGRAMS", "=", "1", ")", "\n", "test_X", ",", "test_Y", "=", "transform_data", "(", "test_data", ",", "unique_vocab_dict", ",", "NGRAMS", "=", "1", ")", "\n", "final_test_X", ",", "final_test_Y", "=", "transform_data", "(", "final_test_data", ",", "unique_vocab_dict", ",", "NGRAMS", "=", "1", ")", "if", "final_test_data_t", "==", "[", "]", "else", "final_test_data_t", "\n", "N_EPOCHS", "=", "50", "\n", "num_class", "=", "2", "if", "IsPre", "else", "max", "(", "target_name", ".", "values", "(", ")", ")", "+", "1", "\n", "model", "=", "LR", "(", "unique_vocab_dict", ",", "unique_vocab_list", ",", "num_class", "=", "num_class", ")", ".", "to", "(", "device", ")", "\n", "learning_rate", "=", "5.0", "\n", "criterion", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "# criterion1 = torch.nn.MSELoss()", "\n", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "learning_rate", ")", "\n", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "StepLR", "(", "optimizer", ",", "1", ",", "gamma", "=", "0.99", ")", "\n", "BATCH_SIZE", "=", "32", "\n", "\n", "print", "(", "'[utils.py] Model Training...'", ")", "\n", "for", "epoch", "in", "range", "(", "N_EPOCHS", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "train_loss", "=", "0.0", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "train_X", ")", ",", "BATCH_SIZE", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "batch_X", "=", "train_X", "[", "i", ":", "i", "+", "BATCH_SIZE", "]", ".", "to", "(", "device", ")", "\n", "batch_Y", "=", "train_Y", "[", "i", ":", "i", "+", "BATCH_SIZE", "]", ".", "to", "(", "device", ")", "\n", "out_X", "=", "model", "(", "batch_X", ")", "\n", "", "except", ":", "\n", "                ", "batch_X", "=", "train_X", "[", "i", ":", "i", "+", "BATCH_SIZE", "]", ".", "to", "(", "device", ")", ".", "long", "(", ")", "\n", "batch_Y", "=", "train_Y", "[", "i", ":", "i", "+", "BATCH_SIZE", "]", ".", "to", "(", "device", ")", "\n", "out_X", "=", "model", "(", "batch_X", ")", "\n", "", "loss", "=", "criterion", "(", "out_X", ",", "batch_Y", ")", "\n", "# temp_Y = []", "\n", "# for j in batch_Y:", "\n", "#     temp_Y.append(np.eye(len(Labels))[j.item()])", "\n", "# temp_Y = torch.Tensor(temp_Y).to(device)", "\n", "# loss = criterion1(out_X, temp_Y)", "\n", "train_loss", "+=", "loss", ".", "item", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "", "scheduler", ".", "step", "(", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "_", ",", "train_acc", "=", "_test_result", "(", "train_X", ",", "train_Y", ",", "model", ",", "BATCH_SIZE", ",", "confusion", "=", "0", ",", "msg", "=", "'Training'", ")", "\n", "_", ",", "test_acc", "=", "_test_result", "(", "test_X", ",", "test_Y", ",", "model", ",", "BATCH_SIZE", ",", "confusion", "=", "0", ",", "msg", "=", "'Testing'", ")", "\n", "print", "(", "f'Epoch: {epoch + 1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%'", ")", "\n", "", "final_test_output", ",", "_", "=", "_test_result", "(", "final_test_X", ",", "final_test_Y", ",", "model", ",", "BATCH_SIZE", ",", "confusion", "=", "0", ")", "\n", "final_out", "=", "convert_final_test_output_to_final_out", "(", "final_test_output", ",", "target_name", ",", "final_test", ",", "final_test_Y", ",", "IsPre", ",", "has_coarse", ",", "final_test_output_pre", ")", "\n", "# torch.save(model, './classifier_' + dataset_name + '.pth')", "\n", "return", "model", ",", "final_out", ",", "final_test_output", ",", "(", "final_test_X", ",", "final_test_Y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification._test_result": [[125, 145], ["torch.no_grad", "range", "float", "test_Y.tolist", "numpy.array().argmax().tolist", "print", "print", "print", "len", "output.extend", "sum", "len", "sklearn.metrics.confusion_matrix", "test_X[].to", "model", "model.tolist", "numpy.array().argmax", "test_X[].to().long", "model", "numpy.argmax", "test_Y.tolist", "numpy.array", "test_X[].to"], "function", ["None"], ["", "def", "_test_result", "(", "test_X", ",", "test_Y", ",", "model", ",", "BATCH_SIZE", ",", "confusion", "=", "0", ",", "msg", "=", "''", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "test_X", ")", ",", "BATCH_SIZE", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "batch_X", "=", "test_X", "[", "i", ":", "i", "+", "BATCH_SIZE", "]", ".", "to", "(", "device", ")", "\n", "out_X", "=", "model", "(", "batch_X", ")", "\n", "", "except", ":", "\n", "                ", "batch_X", "=", "test_X", "[", "i", ":", "i", "+", "BATCH_SIZE", "]", ".", "to", "(", "device", ")", ".", "long", "(", ")", "\n", "out_X", "=", "model", "(", "batch_X", ")", "\n", "", "output", ".", "extend", "(", "out_X", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "acc", "=", "100", "*", "sum", "(", "np", ".", "argmax", "(", "output", ",", "1", ")", "==", "test_Y", ".", "tolist", "(", ")", ")", "/", "float", "(", "len", "(", "test_Y", ")", ")", "\n", "if", "confusion", "==", "1", ":", "\n", "        ", "GT", "=", "test_Y", ".", "tolist", "(", ")", "\n", "ours", "=", "np", ".", "array", "(", "output", ")", ".", "argmax", "(", "1", ")", ".", "tolist", "(", ")", "\n", "print", "(", "msg", ",", "end", "=", "' '", ")", "\n", "print", "(", "f'Accuracy: {acc:.2f}%'", ")", "\n", "print", "(", "confusion_matrix", "(", "GT", ",", "ours", ")", ")", "\n", "", "return", "output", ",", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.train_initialization": [[148, 222], ["identification.train_initialization.load_dataset"], "function", ["None"], ["def", "train_initialization", "(", "classifier_name", ",", "train_data", ",", "test_data", ",", "Final_test", ",", "target_name", ",", "IsPre", "=", "0", ")", ":", "\n", "    ", "def", "load_dataset", "(", "train_data", ",", "test_data", ",", "Final_test", ",", "embedding_length", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"\n        tokenizer : Breaks sentences into a list of words. If sequential=False, no tokenization is applied\n        Field : A class that stores information about the way of preprocessing\n        fix_length : An important property of TorchText is that we can let the input to be variable length, and TorchText will\n                     dynamically pad each sequence to the longest sequence in that \"batch\". But here we are using fi_length which\n                     will pad each sequence to have a fix length of 200.\n\n        build_vocab : It will first make a vocabulary or dictionary mapping all the unique words present in the train_data to an\n                      idx and then after it will use GloVe word embedding to map the index to the corresponding word embedding.\n\n        vocab.vectors : This returns a torch tensor of shape (vocab_size x embedding_dim) containing the pre-trained word embeddings.\n        BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n\n        \"\"\"", "\n", "\n", "def", "get_dataset", "(", "a_data", ",", "fields", ")", ":", "\n", "            ", "examples", "=", "[", "]", "\n", "for", "data_i", "in", "tqdm", "(", "a_data", ")", ":", "\n", "                ", "examples", ".", "append", "(", "torchtext", ".", "data", ".", "Example", ".", "fromlist", "(", "[", "data_i", "[", "0", "]", ",", "data_i", "[", "1", "]", "]", ",", "fields", ")", ")", "\n", "", "return", "examples", "\n", "\n", "", "tokenize", "=", "lambda", "x", ":", "x", ".", "split", "(", ")", "\n", "TEXT", "=", "torchtext", ".", "data", ".", "Field", "(", "sequential", "=", "True", ",", "tokenize", "=", "tokenize", ",", "lower", "=", "True", ",", "include_lengths", "=", "True", ",", "batch_first", "=", "True", ",", "fix_length", "=", "15", ")", "\n", "LABEL", "=", "torchtext", ".", "data", ".", "LabelField", "(", ")", "\n", "fields", "=", "[", "(", "\"text\"", ",", "TEXT", ")", ",", "(", "\"label\"", ",", "LABEL", ")", "]", "\n", "train_data", "=", "get_dataset", "(", "train_data", ",", "fields", ")", "\n", "test_data", "=", "get_dataset", "(", "test_data", ",", "fields", ")", "\n", "Final_test", "=", "get_dataset", "(", "Final_test", ",", "fields", ")", "\n", "train_data", "=", "torchtext", ".", "data", ".", "Dataset", "(", "train_data", ",", "fields", "=", "fields", ")", "\n", "test_data", "=", "torchtext", ".", "data", ".", "Dataset", "(", "test_data", ",", "fields", "=", "fields", ")", "\n", "Final_test", "=", "torchtext", ".", "data", ".", "Dataset", "(", "Final_test", ",", "fields", "=", "fields", ")", "\n", "\n", "TEXT", ".", "build_vocab", "(", "train_data", ",", "vectors", "=", "GloVe", "(", "name", "=", "'6B'", ",", "dim", "=", "embedding_length", ")", ")", "\n", "LABEL", ".", "build_vocab", "(", "train_data", ")", "\n", "word_embeddings", "=", "TEXT", ".", "vocab", ".", "vectors", "\n", "vocab_size", "=", "len", "(", "TEXT", ".", "vocab", ")", "\n", "print", "(", "\"Length of Text Vocabulary: \"", "+", "str", "(", "len", "(", "TEXT", ".", "vocab", ")", ")", ")", "\n", "print", "(", "\"Vector size of Text Vocabulary: \"", ",", "TEXT", ".", "vocab", ".", "vectors", ".", "size", "(", ")", ")", "\n", "print", "(", "\"Label Length: \"", "+", "str", "(", "len", "(", "LABEL", ".", "vocab", ")", ")", ")", "\n", "\n", "### If validation", "\n", "# train_data, valid_data = train_data.split()", "\n", "# train_iter, valid_iter, test_iter = torchtext.data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=32, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)", "\n", "train_iter", ",", "test_iter", ",", "Final_test_iter", "=", "torchtext", ".", "data", ".", "Iterator", ".", "splits", "(", "(", "train_data", ",", "test_data", ",", "Final_test", ")", ",", "batch_size", "=", "batch_size", ",", "sort", "=", "False", ",", "repeat", "=", "False", ")", "\n", "return", "TEXT", ",", "vocab_size", ",", "word_embeddings", ",", "train_iter", ",", "test_iter", ",", "Final_test_iter", "\n", "\n", "", "output_size", "=", "2", "if", "IsPre", "else", "max", "(", "target_name", ".", "values", "(", ")", ")", "+", "1", "\n", "learning_rate", "=", "0.002", "\n", "hidden_size", "=", "256", "\n", "embedding_length", "=", "100", "\n", "epoch_num", "=", "3", "if", "IsPre", "else", "10", "\n", "batch_size", "=", "32", "\n", "pre_train", "=", "True", "\n", "embedding_tune", "=", "False", "\n", "\n", "TEXT", ",", "vocab_size", ",", "word_embeddings", ",", "train_iter", ",", "test_iter", ",", "Final_test_iter", "=", "load_dataset", "(", "train_data", ",", "test_data", ",", "[", "[", "x", "[", "0", "]", ",", "0", "]", "for", "x", "in", "Final_test", "]", ",", "embedding_length", ",", "batch_size", ")", "\n", "if", "classifier_name", "==", "'LSTM'", ":", "\n", "        ", "model", "=", "LSTM", "(", "batch_size", ",", "output_size", ",", "hidden_size", ",", "vocab_size", ",", "embedding_length", ",", "word_embeddings", ",", "pre_train", ",", "embedding_tune", ")", "\n", "", "elif", "classifier_name", "==", "'LSTMAtten'", ":", "\n", "        ", "model", "=", "LSTM_AttentionModel", "(", "batch_size", ",", "output_size", ",", "hidden_size", ",", "vocab_size", ",", "embedding_length", ",", "word_embeddings", ",", "pre_train", ",", "embedding_tune", ")", "\n", "", "elif", "classifier_name", "==", "'RNN'", ":", "\n", "        ", "learning_rate", "=", "0.0005", "\n", "model", "=", "RNN", "(", "batch_size", ",", "output_size", ",", "hidden_size", ",", "vocab_size", ",", "embedding_length", ",", "word_embeddings", ",", "pre_train", ",", "embedding_tune", ")", "\n", "", "elif", "classifier_name", "==", "'RCNN'", ":", "\n", "        ", "model", "=", "RCNN", "(", "batch_size", ",", "output_size", ",", "hidden_size", ",", "vocab_size", ",", "embedding_length", ",", "word_embeddings", ",", "pre_train", ",", "embedding_tune", ")", "\n", "", "elif", "classifier_name", "==", "'SelfAttention'", ":", "\n", "        ", "model", "=", "SelfAttention", "(", "batch_size", ",", "output_size", ",", "hidden_size", ",", "vocab_size", ",", "embedding_length", ",", "word_embeddings", ",", "pre_train", ",", "embedding_tune", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Not a valid classifier_name!!!'", ")", "\n", "", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "loss_fn", "=", "F", ".", "cross_entropy", "\n", "return", "train_iter", ",", "test_iter", ",", "Final_test_iter", ",", "learning_rate", ",", "epoch_num", ",", "model", ",", "loss_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.train_model": [[224, 260], ["torch.optim.Adam", "model.train", "enumerate", "list", "filter", "torch.autograd.Variable().long", "text.to.to", "target.to.to", "torch.optim.Adam.zero_grad", "model", "loss_fn", "loss_fn.backward", "identification.train_model.clip_gradient"], "function", ["None"], ["", "def", "train_model", "(", "model", ",", "train_iter", ",", "loss_fn", ",", "learning_rate", ",", "epoch", ")", ":", "\n", "    ", "def", "clip_gradient", "(", "model", ",", "clip_value", ")", ":", "\n", "        ", "params", "=", "list", "(", "filter", "(", "lambda", "p", ":", "p", ".", "grad", "is", "not", "None", ",", "model", ".", "parameters", "(", ")", ")", ")", "\n", "for", "p", "in", "params", ":", "\n", "            ", "p", ".", "grad", ".", "data", ".", "clamp_", "(", "-", "clip_value", ",", "clip_value", ")", "\n", "\n", "", "", "total_epoch_loss", "=", "0", "\n", "total_epoch_acc", "=", "0", "\n", "optim", "=", "torch", ".", "optim", ".", "Adam", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", ",", "lr", "=", "learning_rate", ")", "\n", "steps", "=", "0", "\n", "model", ".", "train", "(", ")", "\n", "for", "idx", ",", "batch", "in", "enumerate", "(", "train_iter", ")", ":", "\n", "        ", "text", "=", "batch", ".", "text", "[", "0", "]", "\n", "target", "=", "batch", ".", "label", "\n", "target", "=", "torch", ".", "autograd", ".", "Variable", "(", "target", ")", ".", "long", "(", ")", "\n", "text", "=", "text", ".", "to", "(", "device", ")", "\n", "target", "=", "target", ".", "to", "(", "device", ")", "\n", "if", "(", "text", ".", "size", "(", ")", "[", "0", "]", "is", "not", "32", ")", ":", "# One of the batch returned by BucketIterator has length different than 32.", "\n", "            ", "continue", "\n", "", "optim", ".", "zero_grad", "(", ")", "\n", "prediction", "=", "model", "(", "text", ")", "\n", "loss", "=", "loss_fn", "(", "prediction", ",", "target", ")", "\n", "num_corrects", "=", "(", "torch", ".", "max", "(", "prediction", ",", "1", ")", "[", "1", "]", ".", "view", "(", "target", ".", "size", "(", ")", ")", ".", "data", "==", "target", ".", "data", ")", ".", "float", "(", ")", ".", "sum", "(", ")", "\n", "acc", "=", "100.0", "*", "num_corrects", "/", "len", "(", "batch", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "clip_gradient", "(", "model", ",", "1e-1", ")", "\n", "optim", ".", "step", "(", ")", "\n", "steps", "+=", "1", "\n", "\n", "if", "steps", "%", "1000", "==", "0", ":", "\n", "            ", "print", "(", "f'Epoch: {epoch + 1}, Idx: {idx + 1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%'", ")", "\n", "\n", "", "total_epoch_loss", "+=", "loss", ".", "item", "(", ")", "\n", "total_epoch_acc", "+=", "acc", ".", "item", "(", ")", "\n", "\n", "", "return", "total_epoch_loss", "/", "len", "(", "train_iter", ")", ",", "total_epoch_acc", "/", "len", "(", "train_iter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.eval_model": [[262, 286], ["model.eval", "torch.no_grad", "enumerate", "torch.autograd.Variable().long", "text.to.to", "target.to.to", "model", "all_prediction.extend", "all_target.extend", "loss_fn", "loss_fn.item", "acc.item", "len", "len", "model.tolist", "target.to.tolist", "len", "text.to.size", "torch.autograd.Variable", "[].view", "target.to.size", "torch.max"], "function", ["None"], ["", "def", "eval_model", "(", "model", ",", "val_iter", ",", "loss_fn", ")", ":", "\n", "    ", "total_epoch_loss", "=", "0", "\n", "total_epoch_acc", "=", "0", "\n", "all_prediction", "=", "[", "]", "\n", "all_target", "=", "[", "]", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "idx", ",", "batch", "in", "enumerate", "(", "val_iter", ")", ":", "\n", "            ", "text", "=", "batch", ".", "text", "[", "0", "]", "\n", "if", "(", "text", ".", "size", "(", ")", "[", "0", "]", "is", "not", "32", ")", ":", "\n", "                ", "continue", "\n", "", "target", "=", "batch", ".", "label", "\n", "target", "=", "torch", ".", "autograd", ".", "Variable", "(", "target", ")", ".", "long", "(", ")", "\n", "text", "=", "text", ".", "to", "(", "device", ")", "\n", "target", "=", "target", ".", "to", "(", "device", ")", "\n", "prediction", "=", "model", "(", "text", ")", "\n", "all_prediction", ".", "extend", "(", "prediction", ".", "tolist", "(", ")", ")", "\n", "all_target", ".", "extend", "(", "target", ".", "tolist", "(", ")", ")", "\n", "loss", "=", "loss_fn", "(", "prediction", ",", "target", ")", "\n", "num_corrects", "=", "(", "torch", ".", "max", "(", "prediction", ",", "1", ")", "[", "1", "]", ".", "view", "(", "target", ".", "size", "(", ")", ")", ".", "data", "==", "target", ".", "data", ")", ".", "sum", "(", ")", "\n", "acc", "=", "100.0", "*", "num_corrects", "/", "len", "(", "batch", ")", "\n", "total_epoch_loss", "+=", "loss", ".", "item", "(", ")", "\n", "total_epoch_acc", "+=", "acc", ".", "item", "(", ")", "\n", "", "", "return", "total_epoch_loss", "/", "len", "(", "val_iter", ")", ",", "total_epoch_acc", "/", "len", "(", "val_iter", ")", ",", "all_prediction", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.get_train_test_data": [[289, 335], ["print", "tqdm.tqdm", "identification.get_train_test_data._shuffle_and_balance"], "function", ["None"], ["def", "get_train_test_data", "(", "input_keywords", ",", "target_name", ",", "all_text", ",", "final_test", ",", "NGRAMS", ",", "train_perc", ")", ":", "\n", "    ", "print", "(", "'[utils.py] Constructing train and test data...'", ")", "\n", "all_data", "=", "[", "]", "\n", "all_data_pre", "=", "[", "]", "\n", "final_test_data", "=", "[", "]", "\n", "for", "i", "in", "tqdm", "(", "all_text", ")", ":", "\n", "        ", "temp", "=", "nltk", ".", "word_tokenize", "(", "i", ")", "\n", "for", "j", ",", "keyword", "in", "enumerate", "(", "input_keywords", ")", ":", "# Add positive labels that belong to input keywords.", "\n", "            ", "if", "keyword", "not", "in", "temp", ":", "\n", "                ", "continue", "\n", "", "temp_index", "=", "temp", ".", "index", "(", "keyword", ")", "\n", "masked_sentence", "=", "' '", ".", "join", "(", "temp", "[", ":", "temp_index", "]", ")", "+", "' [MASK] '", "+", "' '", ".", "join", "(", "temp", "[", "temp_index", "+", "1", ":", "]", ")", "\n", "all_data", ".", "append", "(", "[", "masked_sentence", ",", "target_name", "[", "keyword", "]", "]", ")", "\n", "all_data_pre", ".", "append", "(", "[", "masked_sentence", ",", "1", "]", ")", "# is one of the target keywords.", "\n", "", "temp_index", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "temp", ")", "-", "1", ")", "\n", "if", "temp", "[", "temp_index", "]", "not", "in", "input_keywords", ":", "# Add negative labels that NOT belong to input keywords.", "\n", "            ", "masked_sentence", "=", "' '", ".", "join", "(", "temp", "[", ":", "temp_index", "]", ")", "+", "' [MASK] '", "+", "' '", ".", "join", "(", "temp", "[", "temp_index", "+", "1", ":", "]", ")", "\n", "all_data_pre", ".", "append", "(", "[", "masked_sentence", ",", "0", "]", ")", "# is NOT one of the target keywords.", "\n", "", "for", "j", ",", "keyword", "in", "enumerate", "(", "final_test", ")", ":", "# Construct final_test_data", "\n", "            ", "if", "keyword", "not", "in", "temp", ":", "\n", "                ", "continue", "\n", "", "temp_index", "=", "temp", ".", "index", "(", "keyword", ")", "\n", "masked_sentence", "=", "' '", ".", "join", "(", "temp", "[", ":", "temp_index", "]", ")", "+", "' [MASK] '", "+", "' '", ".", "join", "(", "temp", "[", "temp_index", "+", "1", ":", "]", ")", "\n", "final_test_data", ".", "append", "(", "[", "masked_sentence", ",", "j", "]", ")", "# final_test_data's label is the id number of final_test. For later final_out construction", "\n", "\n", "", "", "def", "_shuffle_and_balance", "(", "all_data", ",", "max_len", ")", ":", "\n", "# for i in range(max(target_name.values())+2):", "\n", "#     print(sum([x[1] == i for x in all_data]), end=', ')", "\n", "# print()", "\n", "        ", "random", ".", "shuffle", "(", "all_data", ")", "\n", "data_len", "=", "defaultdict", "(", "int", ")", "\n", "all_data_balanced", "=", "[", "]", "\n", "for", "i", "in", "all_data", ":", "\n", "            ", "if", "data_len", "[", "i", "[", "1", "]", "]", "==", "max_len", ":", "\n", "                ", "continue", "\n", "", "data_len", "[", "i", "[", "1", "]", "]", "+=", "1", "\n", "all_data_balanced", ".", "append", "(", "i", ")", "\n", "", "random", ".", "shuffle", "(", "all_data_balanced", ")", "\n", "train_data", "=", "all_data_balanced", "[", ":", "int", "(", "train_perc", "*", "len", "(", "all_data_balanced", ")", ")", "]", "\n", "test_data", "=", "all_data_balanced", "[", "int", "(", "train_perc", "*", "len", "(", "all_data_balanced", ")", ")", ":", "]", "\n", "return", "train_data", ",", "test_data", "\n", "\n", "", "train_data", ",", "test_data", "=", "_shuffle_and_balance", "(", "all_data", ",", "max_len", "=", "2000", ")", "\n", "train_data_pre", ",", "test_data_pre", "=", "_shuffle_and_balance", "(", "all_data_pre", ",", "max_len", "=", "min", "(", "100000", ",", "sum", "(", "[", "x", "[", "1", "]", "==", "0", "for", "x", "in", "all_data_pre", "]", ")", ",", "sum", "(", "[", "x", "[", "1", "]", "==", "1", "for", "x", "in", "all_data_pre", "]", ")", ")", ")", "\n", "unique_vocab_dict", ",", "unique_vocab_list", "=", "build_vocab", "(", "train_data", ",", "NGRAMS", ",", "min_count", "=", "10", ")", "\n", "return", "train_data", ",", "test_data", ",", "final_test_data", ",", "train_data_pre", ",", "test_data_pre", ",", "unique_vocab_dict", ",", "unique_vocab_list", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.get_filtered_final_out": [[337, 351], ["print", "enumerate", "identification.print_final_out", "final_top_words.append", "filtered_final_out.append", "len"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.print_final_out", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["", "def", "get_filtered_final_out", "(", "final_out", ",", "final_test", ",", "input_keywords", ",", "target_name", ")", ":", "\n", "    ", "print", "(", "'\\n'", "+", "'-'", "*", "40", "+", "' [Final Results] '", "+", "'-'", "*", "40", ")", "\n", "final_top_words", "=", "[", "]", "\n", "filtered_final_out", "=", "[", "]", "\n", "filtered_final_test", "=", "{", "}", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "final_test", ")", ":", "\n", "        ", "if", "final_out", "[", "i", "]", "==", "[", "len", "(", "input_keywords", ")", "]", ":", "\n", "            ", "continue", "\n", "", "final_top_words", ".", "append", "(", "word", ")", "\n", "if", "final_test", "[", "word", "]", "!=", "[", "'None'", "]", ":", "\n", "            ", "filtered_final_out", ".", "append", "(", "final_out", "[", "i", "]", ")", "\n", "filtered_final_test", "[", "word", "]", "=", "final_test", "[", "word", "]", "\n", "", "", "print_final_out", "(", "filtered_final_out", ",", "filtered_final_test", ",", "target_name", ")", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.print_final_out": [[353, 377], ["range", "enumerate", "print", "print", "range", "print", "print", "range", "print", "target_name_list.append", "ranking_list.append", "len", "print", "len", "print", "max", "any", "len", "sum", "len", "range", "target_name.values", "sum", "len", "len"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["", "def", "print_final_out", "(", "final_out", ",", "final_test", ",", "target_name", ")", ":", "\n", "    ", "ranking_list", "=", "[", "]", "\n", "target_name_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "max", "(", "target_name", ".", "values", "(", ")", ")", "+", "1", ")", ":", "\n", "        ", "target_name_list", ".", "append", "(", "[", "x", "for", "x", "in", "target_name", "if", "target_name", "[", "x", "]", "==", "i", "]", ")", "\n", "", "for", "i", ",", "word", "in", "enumerate", "(", "final_test", ")", ":", "\n", "# print('{:12s}: \\t'.format(word), end='')", "\n", "        ", "position", "=", "0", "\n", "for", "j", "in", "final_out", "[", "i", "]", ":", "\n", "            ", "position", "+=", "1", "\n", "if", "any", "(", "ele", "in", "target_name_list", "[", "j", "]", "for", "ele", "in", "final_test", "[", "word", "]", ")", ":", "\n", "                ", "break", "\n", "", "", "ranking_list", ".", "append", "(", "position", ")", "\n", "", "print", "(", "'Average ranking is {:.2f} for {:d} euphemisms.'", ".", "format", "(", "sum", "(", "ranking_list", ")", "/", "len", "(", "ranking_list", ")", ",", "len", "(", "ranking_list", ")", ")", ")", "\n", "topk_acc", "=", "[", "sum", "(", "x", "<=", "k", "+", "1", "for", "x", "in", "ranking_list", ")", "/", "len", "(", "final_test", ")", "for", "k", "in", "range", "(", "len", "(", "target_name_list", ")", ")", "]", "\n", "print", "(", "'[Top-k Accuracy]: '", ",", "end", "=", "''", ")", "\n", "for", "k", "in", "range", "(", "len", "(", "target_name_list", ")", ")", ":", "\n", "        ", "print", "(", "'|  {:2d}  '", ".", "format", "(", "k", "+", "1", ")", ",", "end", "=", "''", ")", "\n", "", "print", "(", ")", "\n", "print", "(", "' '", "*", "18", ",", "end", "=", "''", ")", "\n", "for", "k", "in", "range", "(", "len", "(", "target_name_list", ")", ")", ":", "\n", "        ", "print", "(", "'| {:.2f} '", ".", "format", "(", "topk_acc", "[", "k", "]", ")", ",", "end", "=", "''", ")", "\n", "", "print", "(", ")", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.convert_final_test_output_to_final_out": [[379, 401], ["scipy.special.softmax().tolist", "enumerate", "range", "enumerate", "numpy.array", "len", "scipy.special.softmax", "numpy.array", "range", "sum", "[].tolist", "range", "len", "len", "len", "len", "range", "numpy.argsort", "max", "target_name.values"], "function", ["None"], ["", "def", "convert_final_test_output_to_final_out", "(", "final_test_output", ",", "target_name", ",", "final_test", ",", "final_test_Y", ",", "IsPre", ",", "has_coarse", ",", "final_test_output_pre", "=", "[", "]", ")", ":", "\n", "    ", "final_test_output", "=", "softmax", "(", "final_test_output", ",", "axis", "=", "1", ")", ".", "tolist", "(", ")", "\n", "if", "IsPre", ":", "\n", "        ", "final_out", "=", "[", "np", ".", "array", "(", "[", "0.0", ",", "0.0", "]", ")", "for", "x", "in", "range", "(", "len", "(", "final_test", ")", ")", "]", "\n", "for", "i", ",", "j", "in", "enumerate", "(", "final_test_output", ")", ":", "\n", "            ", "if", "j", "[", "0", "]", "<", "j", "[", "1", "]", ":", "\n", "                ", "final_out", "[", "final_test_Y", "[", "i", "]", "]", "+=", "j", "\n", "", "", "return", "final_out", "\n", "\n", "", "final_out", "=", "[", "np", ".", "array", "(", "[", "0.0", "for", "y", "in", "range", "(", "max", "(", "target_name", ".", "values", "(", ")", ")", "+", "1", ")", "]", ")", "for", "x", "in", "range", "(", "len", "(", "final_test", ")", ")", "]", "\n", "for", "i", ",", "j", "in", "enumerate", "(", "final_test_output", ")", ":", "\n", "        ", "if", "has_coarse", "and", "i", "<", "len", "(", "final_test_output_pre", ")", ":", "\n", "            ", "if", "final_test_output_pre", "[", "i", "]", "[", "0", "]", "<", "final_test_output_pre", "[", "i", "]", "[", "1", "]", ":", "\n", "                ", "final_out", "[", "final_test_Y", "[", "i", "]", "]", "+=", "j", "\n", "", "", "else", ":", "\n", "            ", "final_out", "[", "final_test_Y", "[", "i", "]", "]", "+=", "j", "\n", "", "", "for", "i", "in", "range", "(", "len", "(", "final_out", ")", ")", ":", "\n", "        ", "if", "sum", "(", "final_out", "[", "i", "]", ")", "==", "0", ":", "\n", "            ", "final_out", "[", "i", "]", "=", "[", "len", "(", "target_name", ")", "]", "\n", "", "else", ":", "\n", "            ", "final_out", "[", "i", "]", "=", "np", ".", "argsort", "(", "final_out", "[", "i", "]", ")", "[", ":", ":", "-", "1", "]", ".", "tolist", "(", ")", "\n", "", "", "return", "final_out", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.get_final_test": [[403, 414], ["any"], "function", ["None"], ["", "def", "get_final_test", "(", "euphemism_answer", ",", "top_words", ",", "input_keywords", ")", ":", "\n", "    ", "final_test", "=", "{", "}", "\n", "for", "x", "in", "top_words", ":", "\n", "        ", "if", "x", "in", "euphemism_answer", ":", "\n", "            ", "if", "any", "(", "ele", "in", "euphemism_answer", "[", "x", "]", "for", "ele", "in", "input_keywords", ")", ":", "\n", "                ", "final_test", "[", "x", "]", "=", "euphemism_answer", "[", "x", "]", "\n", "", "else", ":", "\n", "                ", "final_test", "[", "x", "]", "=", "[", "'None'", "]", "\n", "", "", "else", ":", "\n", "            ", "final_test", "[", "x", "]", "=", "[", "'None'", "]", "\n", "", "", "return", "final_test", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.build_vocab": [[416, 427], ["collections.defaultdict", "range", "len", "nltk.word_tokenize", "torchtext.data.utils.ngrams_iterator", "range", "len"], "function", ["None"], ["", "def", "build_vocab", "(", "xlist", ",", "NGRAMS", ",", "min_count", ")", ":", "\n", "    ", "vocabi2w", "=", "[", "'[SOS]'", ",", "'[EOS]'", ",", "'[PAD]'", ",", "'[UNK]'", "]", "# A list of unique words", "\n", "seen", "=", "defaultdict", "(", "int", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "xlist", ")", ")", ":", "\n", "        ", "tokens", "=", "nltk", ".", "word_tokenize", "(", "xlist", "[", "i", "]", "[", "0", "]", ")", "\n", "tokens", "=", "tokens", "if", "NGRAMS", "==", "1", "else", "ngrams_iterator", "(", "tokens", ",", "NGRAMS", ")", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "seen", "[", "token", "]", "+=", "1", "\n", "", "", "vocabi2w", "+=", "[", "x", "for", "x", "in", "seen", "if", "seen", "[", "x", "]", ">=", "min_count", "]", "\n", "vocabw2i", "=", "{", "vocabi2w", "[", "x", "]", ":", "x", "for", "x", "in", "range", "(", "len", "(", "vocabi2w", ")", ")", "}", "\n", "return", "vocabw2i", ",", "vocabi2w", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.LR.__init__": [[10, 17], ["torch.Module.__init__", "len", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.__init__"], ["    ", "def", "__init__", "(", "self", ",", "unique_vocab_dict", ",", "unique_vocab_list", ",", "num_class", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "unique_vocab_dict", "=", "unique_vocab_dict", "\n", "self", ".", "unique_vocab_list", "=", "unique_vocab_list", "\n", "self", ".", "vocab_size", "=", "len", "(", "unique_vocab_dict", ")", "\n", "self", ".", "num_class", "=", "num_class", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "vocab_size", ",", "num_class", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.LR.forward": [[18, 21], ["classification_model.LR.fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "text", ")", ":", "\n", "        ", "out", "=", "self", ".", "fc", "(", "text", ")", "\n", "return", "out", "\n", "# return torch.softmax(out, dim=1)", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.LR.get_features": [[23, 36], ["print", "range", "print", "range", "print", "print", "features.append", "classification_model.LR.fc.weight[].argsort().tolist", "min", "feature.append", "len", "classification_model.LR.fc.weight[].argsort"], "methods", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["", "def", "get_features", "(", "self", ",", "class_names", ",", "num", "=", "50", ")", ":", "\n", "        ", "print", "(", "'-----------------------'", ")", "\n", "features", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_class", ")", ":", "\n", "            ", "feature", "=", "[", "]", "\n", "sorted_weight_index", "=", "self", ".", "fc", ".", "weight", "[", "i", "]", ".", "argsort", "(", ")", ".", "tolist", "(", ")", "[", ":", ":", "-", "1", "]", "\n", "for", "j", "in", "range", "(", "min", "(", "num", ",", "len", "(", "sorted_weight_index", ")", ")", ")", ":", "\n", "                ", "feature", ".", "append", "(", "self", ".", "unique_vocab_list", "[", "sorted_weight_index", "[", "j", "]", "]", ")", "\n", "", "print", "(", "class_names", "[", "i", "]", ",", "end", "=", "': '", ")", "\n", "print", "(", "feature", ")", "\n", "features", ".", "append", "(", "feature", ")", "\n", "", "print", "(", "'-----------------------'", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.LR_embeddings.__init__": [[39, 44], ["torch.Module.__init__", "len", "torch.EmbeddingBag", "torch.EmbeddingBag", "torch.EmbeddingBag", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.__init__"], ["    ", "def", "__init__", "(", "self", ",", "unique_vocab_dict", ",", "embedding_length", ",", "num_class", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab_size", "=", "len", "(", "unique_vocab_dict", ")", "\n", "self", ".", "embedding", "=", "nn", ".", "EmbeddingBag", "(", "self", ".", "vocab_size", ",", "embedding_dim", "=", "embedding_length", ",", "sparse", "=", "True", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "embedding_length", ",", "num_class", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.LR_embeddings.forward": [[45, 49], ["classification_model.LR_embeddings.embedding", "classification_model.LR_embeddings.fc", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "text", ")", ":", "\n", "        ", "out", "=", "self", ".", "embedding", "(", "text", ")", "\n", "out", "=", "self", ".", "fc", "(", "out", ")", "\n", "return", "torch", ".", "softmax", "(", "out", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.CNN.__init__": [[77, 114], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "len"], "methods", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.__init__"], ["    ", "def", "__init__", "(", "self", ",", "batch_size", ",", "output_size", ",", "in_channels", ",", "out_channels", ",", "kernel_heights", ",", "stride", ",", "padding", ",", "keep_probab", ",", "\n", "vocab_size", ",", "embedding_length", ",", "weights", ",", "pre_train", ",", "embedding_tune", ")", ":", "\n", "        ", "super", "(", "CNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "\"\"\"\n        Arguments\n        ---------\n        batch_size : Size of each batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n        output_size : 2 = (pos, neg)\n        in_channels : Number of input channels. Here it is 1 as the input data has dimension = (batch_size, num_seq, embedding_length)\n        out_channels : Number of output channels after convolution operation performed on the input matrix\n        kernel_heights : A list consisting of 3 different kernel_heights. Convolution will be performed 3 times and finally results from each kernel_height will be concatenated.\n        keep_probab : Probability of retaining an activation node during dropout operation\n        vocab_size : Size of the vocabulary containing unique words\n        embedding_length : Embedding dimension of GloVe word embeddings\n        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n        --------\n\n        \"\"\"", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "output_size", "=", "output_size", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_heights", "=", "kernel_heights", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "embedding_length", "=", "embedding_length", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embedding_length", ")", "\n", "if", "pre_train", ":", "\n", "            ", "self", ".", "word_embeddings", ".", "weight", "=", "nn", ".", "Parameter", "(", "weights", ",", "requires_grad", "=", "embedding_tune", ")", "\n", "", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "(", "kernel_heights", "[", "0", "]", ",", "embedding_length", ")", ",", "stride", ",", "padding", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "(", "kernel_heights", "[", "1", "]", ",", "embedding_length", ")", ",", "stride", ",", "padding", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "(", "kernel_heights", "[", "2", "]", ",", "embedding_length", ")", ",", "stride", ",", "padding", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "keep_probab", ")", "\n", "self", ".", "label", "=", "nn", ".", "Linear", "(", "len", "(", "kernel_heights", ")", "*", "out_channels", ",", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.CNN.conv_block": [[115, 120], ["conv_layer", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "conv_layer.squeeze", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "torch.relu.size"], "methods", ["None"], ["", "def", "conv_block", "(", "self", ",", "input", ",", "conv_layer", ")", ":", "\n", "        ", "conv_out", "=", "conv_layer", "(", "input", ")", "# conv_out.size() = (batch_size, out_channels, dim, 1)", "\n", "activation", "=", "F", ".", "relu", "(", "conv_out", ".", "squeeze", "(", "3", ")", ")", "# activation.size() = (batch_size, out_channels, dim1)", "\n", "max_out", "=", "F", ".", "max_pool1d", "(", "activation", ",", "activation", ".", "size", "(", ")", "[", "2", "]", ")", ".", "squeeze", "(", "2", ")", "# maxpool_out.size() = (batch_size, out_channels)", "\n", "return", "max_out", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.CNN.forward": [[121, 155], ["classification_model.CNN.word_embeddings", "input.unsqueeze.unsqueeze.unsqueeze", "classification_model.CNN.conv_block", "classification_model.CNN.conv_block", "classification_model.CNN.conv_block", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "classification_model.CNN.dropout", "classification_model.CNN.label"], "methods", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.CNN.conv_block", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.CNN.conv_block", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.CNN.conv_block"], ["", "def", "forward", "(", "self", ",", "input_sentences", ",", "batch_size", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        The idea of the Convolutional Neural Netwok for Text Classification is very simple. We perform convolution operation on the embedding matrix\n        whose shape for each batch is (num_seq, embedding_length) with kernel of varying height but constant width which is same as the embedding_length.\n        We will be using ReLU activation after the convolution operation and then for each kernel height, we will use max_pool operation on each tensor\n        and will filter all the maximum activation for every channel and then we will concatenate the resulting tensors. This output is then fully connected\n        to the output layers consisting two units which basically gives us the logits for both positive and negative classes.\n\n        Parameters\n        ----------\n        input_sentences: input_sentences of shape = (batch_size, num_sequences)\n        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n\n        Returns\n        -------\n        Output of the linear layer containing logits for pos & neg class.\n        logits.size() = (batch_size, output_size)\n\n        \"\"\"", "\n", "\n", "input", "=", "self", ".", "word_embeddings", "(", "input_sentences", ")", "\n", "# input.size() = (batch_size, num_seq, embedding_length)", "\n", "input", "=", "input", ".", "unsqueeze", "(", "1", ")", "\n", "# input.size() = (batch_size, 1, num_seq, embedding_length)", "\n", "max_out1", "=", "self", ".", "conv_block", "(", "input", ",", "self", ".", "conv1", ")", "\n", "max_out2", "=", "self", ".", "conv_block", "(", "input", ",", "self", ".", "conv2", ")", "\n", "max_out3", "=", "self", ".", "conv_block", "(", "input", ",", "self", ".", "conv3", ")", "\n", "\n", "all_out", "=", "torch", ".", "cat", "(", "(", "max_out1", ",", "max_out2", ",", "max_out3", ")", ",", "1", ")", "\n", "# all_out.size() = (batch_size, num_kernels*out_channels)", "\n", "fc_in", "=", "self", ".", "dropout", "(", "all_out", ")", "\n", "# fc_in.size()) = (batch_size, num_kernels*out_channels)", "\n", "logits", "=", "self", ".", "label", "(", "fc_in", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.LSTM.__init__": [[158, 184], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter"], "methods", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.__init__"], ["    ", "def", "__init__", "(", "self", ",", "batch_size", ",", "output_size", ",", "hidden_size", ",", "vocab_size", ",", "embedding_length", ",", "weights", ",", "pre_train", ",", "embedding_tune", ")", ":", "\n", "        ", "super", "(", "LSTM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "\"\"\"\n        Arguments\n        ---------\n        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n        output_size : 2 = (pos, neg)\n        hidden_sie : Size of the hidden_state of the LSTM\n        vocab_size : Size of the vocabulary containing unique words\n        embedding_length : Embeddding dimension of GloVe word embeddings\n        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n\n        \"\"\"", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "output_size", "=", "output_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "embedding_length", "=", "embedding_length", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embedding_length", ")", "# Initializing the look-up table.", "\n", "if", "pre_train", ":", "\n", "            ", "self", ".", "word_embeddings", ".", "weight", "=", "nn", ".", "Parameter", "(", "weights", ",", "requires_grad", "=", "embedding_tune", ")", "# Assigning the look-up table to the pre-trained GloVe word embedding.", "\n", "", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "embedding_length", ",", "hidden_size", ")", "\n", "self", ".", "label", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.LSTM.forward": [[185, 212], ["classification_model.LSTM.word_embeddings", "input.permute.permute.permute", "classification_model.LSTM.lstm", "classification_model.LSTM.label", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_sentence", ",", "batch_size", "=", "None", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n\n        Returns\n        -------\n        Output of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n        final_output.shape = (batch_size, output_size)\n\n        \"\"\"", "\n", "\n", "''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''", "\n", "input", "=", "self", ".", "word_embeddings", "(", "input_sentence", ")", "# embedded input of shape = (batch_size, num_sequences, embedding_length)", "\n", "input", "=", "input", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# input.size() = (num_sequences, batch_size, embedding_length)", "\n", "if", "batch_size", "is", "None", ":", "\n", "            ", "h_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "self", ".", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "# Initial hidden state of the LSTM", "\n", "c_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "self", ".", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "# Initial cell state of the LSTM", "\n", "", "else", ":", "\n", "            ", "h_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "c_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "", "output", ",", "(", "final_hidden_state", ",", "final_cell_state", ")", "=", "self", ".", "lstm", "(", "input", ",", "(", "h_0", ",", "c_0", ")", ")", "\n", "final_output", "=", "self", ".", "label", "(", "final_hidden_state", "[", "-", "1", "]", ")", "# final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)", "\n", "return", "final_output", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.LSTM_AttentionModel.__init__": [[215, 243], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter"], "methods", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.__init__"], ["    ", "def", "__init__", "(", "self", ",", "batch_size", ",", "output_size", ",", "hidden_size", ",", "vocab_size", ",", "embedding_length", ",", "weights", ",", "pre_train", ",", "embedding_tune", ")", ":", "\n", "        ", "super", "(", "LSTM_AttentionModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "\"\"\"\n        Arguments\n        ---------\n        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n        output_size : 2 = (pos, neg)\n        hidden_sie : Size of the hidden_state of the LSTM\n        vocab_size : Size of the vocabulary containing unique words\n        embedding_length : Embeddding dimension of GloVe word embeddings\n        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n\n        --------\n\n        \"\"\"", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "output_size", "=", "output_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "embedding_length", "=", "embedding_length", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embedding_length", ")", "\n", "if", "pre_train", ":", "\n", "            ", "self", ".", "word_embeddings", ".", "weights", "=", "nn", ".", "Parameter", "(", "weights", ",", "requires_grad", "=", "embedding_tune", ")", "\n", "", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "embedding_length", ",", "hidden_size", ")", "\n", "self", ".", "label", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.LSTM_AttentionModel.attention_net": [[246, 277], ["final_state.squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.softmax", "torch.softmax", "torch.softmax", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "final_state.squeeze.unsqueeze", "lstm_output.transpose", "torch.softmax.unsqueeze"], "methods", ["None"], ["", "def", "attention_net", "(", "self", ",", "lstm_output", ",", "final_state", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Now we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention to compute soft alignment score corresponding\n        between each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm for the batch matrix multiplication.\n\n        Arguments\n        ---------\n\n        lstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n        final_state : Final time-step hidden state (h_n) of the LSTM\n\n        ---------\n\n        Returns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n                  new hidden state.\n\n        Tensor Size :\n                    hidden.size() = (batch_size, hidden_size)\n                    attn_weights.size() = (batch_size, num_seq)\n                    soft_attn_weights.size() = (batch_size, num_seq)\n                    new_hidden_state.size() = (batch_size, hidden_size)\n\n        \"\"\"", "\n", "\n", "hidden", "=", "final_state", ".", "squeeze", "(", "0", ")", "\n", "attn_weights", "=", "torch", ".", "bmm", "(", "lstm_output", ",", "hidden", ".", "unsqueeze", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "soft_attn_weights", "=", "F", ".", "softmax", "(", "attn_weights", ",", "1", ")", "\n", "new_hidden_state", "=", "torch", ".", "bmm", "(", "lstm_output", ".", "transpose", "(", "1", ",", "2", ")", ",", "soft_attn_weights", ".", "unsqueeze", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "\n", "return", "new_hidden_state", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.LSTM_AttentionModel.forward": [[278, 309], ["classification_model.LSTM_AttentionModel.word_embeddings", "input.permute.permute.permute", "classification_model.LSTM_AttentionModel.lstm", "output.permute.permute.permute", "classification_model.LSTM_AttentionModel.attention_net", "classification_model.LSTM_AttentionModel.label", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.SelfAttention.attention_net"], ["", "def", "forward", "(", "self", ",", "input_sentences", ",", "batch_size", "=", "None", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n\n        Returns\n        -------\n        Output of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\n        final_output.shape = (batch_size, output_size)\n\n        \"\"\"", "\n", "\n", "input", "=", "self", ".", "word_embeddings", "(", "input_sentences", ")", "\n", "input", "=", "input", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "if", "batch_size", "is", "None", ":", "\n", "            ", "h_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "self", ".", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "c_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "self", ".", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "", "else", ":", "\n", "            ", "h_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "c_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "\n", "", "output", ",", "(", "final_hidden_state", ",", "final_cell_state", ")", "=", "self", ".", "lstm", "(", "input", ",", "(", "h_0", ",", "c_0", ")", ")", "# final_hidden_state.size() = (1, batch_size, hidden_size)", "\n", "output", "=", "output", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# output.size() = (batch_size, num_seq, hidden_size)", "\n", "\n", "attn_output", "=", "self", ".", "attention_net", "(", "output", ",", "final_hidden_state", ")", "\n", "logits", "=", "self", ".", "label", "(", "attn_output", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.RCNN.__init__": [[312, 340], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter"], "methods", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.__init__"], ["    ", "def", "__init__", "(", "self", ",", "batch_size", ",", "output_size", ",", "hidden_size", ",", "vocab_size", ",", "embedding_length", ",", "weights", ",", "pre_train", ",", "embedding_tune", ")", ":", "\n", "        ", "super", "(", "RCNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "\"\"\"\n        Arguments\n        ---------\n        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n        output_size : 2 = (pos, neg)\n        hidden_sie : Size of the hidden_state of the LSTM\n        vocab_size : Size of the vocabulary containing unique words\n        embedding_length : Embedding dimension of GloVe word embeddings\n        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n\n        \"\"\"", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "output_size", "=", "output_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "embedding_length", "=", "embedding_length", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embedding_length", ")", "# Initializing the look-up table.", "\n", "if", "pre_train", ":", "\n", "            ", "self", ".", "word_embeddings", ".", "weight", "=", "nn", ".", "Parameter", "(", "weights", ",", "requires_grad", "=", "embedding_tune", ")", "# Assigning the look-up table to the pre-trained GloVe word embedding.", "\n", "", "self", ".", "dropout", "=", "0.8", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "embedding_length", ",", "hidden_size", ",", "dropout", "=", "self", ".", "dropout", ",", "bidirectional", "=", "True", ")", "\n", "self", ".", "W2", "=", "nn", ".", "Linear", "(", "2", "*", "hidden_size", "+", "embedding_length", ",", "hidden_size", ")", "\n", "self", ".", "label", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.RCNN.forward": [[341, 385], ["classification_model.RCNN.word_embeddings", "input.permute.permute.permute", "classification_model.RCNN.lstm", "torch.cat().permute", "torch.cat().permute", "torch.cat().permute", "torch.cat().permute", "torch.cat().permute", "torch.cat().permute", "torch.cat().permute", "torch.cat().permute", "torch.cat().permute", "classification_model.RCNN.W2", "y.squeeze.squeeze.permute", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "y.squeeze.squeeze.squeeze", "classification_model.RCNN.label", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "y.squeeze.squeeze.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_sentence", ",", "batch_size", "=", "None", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n\n        Returns\n        -------\n        Output of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n        final_output.shape = (batch_size, output_size)\n\n        \"\"\"", "\n", "\n", "\"\"\"\n\n        The idea of the paper \"Recurrent Convolutional Neural Networks for Text Classification\" is that we pass the embedding vector\n        of the text sequences through a bidirectional LSTM and then for each sequence, our final embedding vector is the concatenation of \n        its own GloVe embedding and the left and right contextual embedding which in bidirectional LSTM is same as the corresponding hidden\n        state. This final embedding is passed through a linear layer which maps this long concatenated encoding vector back to the hidden_size\n        vector. After this step, we use a max pooling layer across all sequences of texts. This converts any varying length text into a fixed\n        dimension tensor of size (batch_size, hidden_size) and finally we map this to the output layer.\n\n        \"\"\"", "\n", "input", "=", "self", ".", "word_embeddings", "(", "input_sentence", ")", "# embedded input of shape = (batch_size, num_sequences, embedding_length)", "\n", "input", "=", "input", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# input.size() = (num_sequences, batch_size, embedding_length)", "\n", "if", "batch_size", "is", "None", ":", "\n", "            ", "h_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", ",", "self", ".", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "# Initial hidden state of the LSTM", "\n", "c_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", ",", "self", ".", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "# Initial cell state of the LSTM", "\n", "", "else", ":", "\n", "            ", "h_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", ",", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "c_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", ",", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "\n", "", "output", ",", "(", "final_hidden_state", ",", "final_cell_state", ")", "=", "self", ".", "lstm", "(", "input", ",", "(", "h_0", ",", "c_0", ")", ")", "\n", "\n", "final_encoding", "=", "torch", ".", "cat", "(", "(", "output", ",", "input", ")", ",", "2", ")", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "y", "=", "self", ".", "W2", "(", "final_encoding", ")", "# y.size() = (batch_size, num_sequences, hidden_size)", "\n", "y", "=", "y", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# y.size() = (batch_size, hidden_size, num_sequences)", "\n", "y", "=", "F", ".", "max_pool1d", "(", "y", ",", "y", ".", "size", "(", ")", "[", "2", "]", ")", "# y.size() = (batch_size, hidden_size, 1)", "\n", "y", "=", "y", ".", "squeeze", "(", "2", ")", "\n", "logits", "=", "self", ".", "label", "(", "y", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.RNN.__init__": [[388, 414], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.RNN", "torch.RNN", "torch.RNN", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter"], "methods", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.__init__"], ["    ", "def", "__init__", "(", "self", ",", "batch_size", ",", "output_size", ",", "hidden_size", ",", "vocab_size", ",", "embedding_length", ",", "weights", ",", "pre_train", ",", "embedding_tune", ")", ":", "\n", "        ", "super", "(", "RNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "\"\"\"\n        Arguments\n        ---------\n        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n        output_size : 2 = (pos, neg)\n        hidden_sie : Size of the hidden_state of the LSTM\n        vocab_size : Size of the vocabulary containing unique words\n        embedding_length : Embeddding dimension of GloVe word embeddings\n        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n\n        \"\"\"", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "output_size", "=", "output_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "embedding_length", "=", "embedding_length", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embedding_length", ")", "\n", "if", "pre_train", ":", "\n", "            ", "self", ".", "word_embeddings", ".", "weight", "=", "nn", ".", "Parameter", "(", "weights", ",", "requires_grad", "=", "embedding_tune", ")", "\n", "", "self", ".", "rnn", "=", "nn", ".", "RNN", "(", "embedding_length", ",", "hidden_size", ",", "num_layers", "=", "2", ",", "bidirectional", "=", "True", ")", "\n", "self", ".", "label", "=", "nn", ".", "Linear", "(", "4", "*", "hidden_size", ",", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.RNN.forward": [[415, 444], ["classification_model.RNN.word_embeddings", "input.permute.permute.permute", "classification_model.RNN.rnn", "h_n.contiguous().view.contiguous().view.permute", "h_n.contiguous().view.contiguous().view.contiguous().view", "classification_model.RNN.label", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "h_n.contiguous().view.contiguous().view.contiguous", "h_n.contiguous().view.contiguous().view.size", "h_n.contiguous().view.contiguous().view.size", "h_n.contiguous().view.contiguous().view.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_sentences", ",", "batch_size", "=", "None", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n\n        Returns\n        -------\n        Output of the linear layer containing logits for pos & neg class which receives its input as the final_hidden_state of RNN.\n        logits.size() = (batch_size, output_size)\n\n        \"\"\"", "\n", "\n", "input", "=", "self", ".", "word_embeddings", "(", "input_sentences", ")", "\n", "input", "=", "input", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "if", "batch_size", "is", "None", ":", "\n", "            ", "h_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "4", ",", "self", ".", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "# 4 = num_layers*num_directions", "\n", "", "else", ":", "\n", "            ", "h_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "4", ",", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "", "output", ",", "h_n", "=", "self", ".", "rnn", "(", "input", ",", "h_0", ")", "\n", "# h_n.size() = (4, batch_size, hidden_size)", "\n", "h_n", "=", "h_n", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# h_n.size() = (batch_size, 4, hidden_size)", "\n", "h_n", "=", "h_n", ".", "contiguous", "(", ")", ".", "view", "(", "h_n", ".", "size", "(", ")", "[", "0", "]", ",", "h_n", ".", "size", "(", ")", "[", "1", "]", "*", "h_n", ".", "size", "(", ")", "[", "2", "]", ")", "\n", "# h_n.size() = (batch_size, 4*hidden_size)", "\n", "logits", "=", "self", ".", "label", "(", "h_n", ")", "# logits.size() = (batch_size, output_size)", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.SelfAttention.__init__": [[447, 481], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter"], "methods", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.__init__"], ["    ", "def", "__init__", "(", "self", ",", "batch_size", ",", "output_size", ",", "hidden_size", ",", "vocab_size", ",", "embedding_length", ",", "weights", ",", "pre_train", ",", "embedding_tune", ")", ":", "\n", "        ", "super", "(", "SelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "\"\"\"\n        Arguments\n        ---------\n        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n        output_size : 2 = (pos, neg)\n        hidden_sie : Size of the hidden_state of the LSTM\n        vocab_size : Size of the vocabulary containing unique words\n        embedding_length : Embeddding dimension of GloVe word embeddings\n        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n\n        --------\n\n        \"\"\"", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "output_size", "=", "output_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "embedding_length", "=", "embedding_length", "\n", "self", ".", "weights", "=", "weights", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embedding_length", ")", "\n", "if", "pre_train", ":", "\n", "            ", "self", ".", "word_embeddings", ".", "weights", "=", "nn", ".", "Parameter", "(", "weights", ",", "requires_grad", "=", "embedding_tune", ")", "\n", "", "self", ".", "dropout", "=", "0.8", "\n", "self", ".", "bilstm", "=", "nn", ".", "LSTM", "(", "embedding_length", ",", "hidden_size", ",", "dropout", "=", "self", ".", "dropout", ",", "bidirectional", "=", "True", ")", "\n", "# We will use da = 350, r = 30 & penalization_coeff = 1 as per given in the self-attention original ICLR paper", "\n", "self", ".", "W_s1", "=", "nn", ".", "Linear", "(", "2", "*", "hidden_size", ",", "350", ")", "\n", "self", ".", "W_s2", "=", "nn", ".", "Linear", "(", "350", ",", "30", ")", "\n", "self", ".", "fc_layer", "=", "nn", ".", "Linear", "(", "30", "*", "2", "*", "hidden_size", ",", "2000", ")", "\n", "self", ".", "label", "=", "nn", ".", "Linear", "(", "2000", ",", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.SelfAttention.attention_net": [[482, 509], ["classification_model.SelfAttention.W_s2", "torch.softmax.permute", "torch.softmax", "torch.softmax", "torch.softmax", "torch.tanh", "torch.tanh", "torch.tanh", "classification_model.SelfAttention.W_s1"], "methods", ["None"], ["", "def", "attention_net", "(", "self", ",", "lstm_output", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Now we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n        encoding of the inout sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of\n        the input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully\n        connected layer of size 2000 which will be connected to the output layer of size 2 returning logits for our two classes i.e.,\n        pos & neg.\n\n        Arguments\n        ---------\n\n        lstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n        ---------\n\n        Returns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n                  attention to different parts of the input sentence.\n\n        Tensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n                      attn_weight_matrix.size() = (batch_size, 30, num_seq)\n\n        \"\"\"", "\n", "attn_weight_matrix", "=", "self", ".", "W_s2", "(", "F", ".", "tanh", "(", "self", ".", "W_s1", "(", "lstm_output", ")", ")", ")", "\n", "attn_weight_matrix", "=", "attn_weight_matrix", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "attn_weight_matrix", "=", "F", ".", "softmax", "(", "attn_weight_matrix", ",", "dim", "=", "2", ")", "\n", "\n", "return", "attn_weight_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.SelfAttention.forward": [[510, 549], ["classification_model.SelfAttention.word_embeddings", "input.permute.permute.permute", "classification_model.SelfAttention.bilstm", "output.permute.permute.permute", "classification_model.SelfAttention.attention_net", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "classification_model.SelfAttention.fc_layer", "classification_model.SelfAttention.label", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.bmm.view", "torch.bmm.view", "torch.bmm.view", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.bmm.size", "torch.bmm.size", "torch.bmm.size", "torch.bmm.size", "torch.bmm.size", "torch.bmm.size"], "methods", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.classification_model.SelfAttention.attention_net"], ["", "def", "forward", "(", "self", ",", "input_sentences", ",", "batch_size", "=", "None", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n\n        Returns\n        -------\n        Output of the linear layer containing logits for pos & neg class.\n\n        \"\"\"", "\n", "\n", "input", "=", "self", ".", "word_embeddings", "(", "input_sentences", ")", "\n", "input", "=", "input", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "if", "batch_size", "is", "None", ":", "\n", "            ", "h_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", ",", "self", ".", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "c_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", ",", "self", ".", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "", "else", ":", "\n", "            ", "h_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", ",", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "c_0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", ",", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "device", ")", ")", "\n", "\n", "", "output", ",", "(", "h_n", ",", "c_n", ")", "=", "self", ".", "bilstm", "(", "input", ",", "(", "h_0", ",", "c_0", ")", ")", "\n", "output", "=", "output", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "# output.size() = (batch_size, num_seq, 2*hidden_size)", "\n", "# h_n.size() = (1, batch_size, hidden_size)", "\n", "# c_n.size() = (1, batch_size, hidden_size)", "\n", "attn_weight_matrix", "=", "self", ".", "attention_net", "(", "output", ")", "\n", "# attn_weight_matrix.size() = (batch_size, r, num_seq)", "\n", "# output.size() = (batch_size, num_seq, 2*hidden_size)", "\n", "hidden_matrix", "=", "torch", ".", "bmm", "(", "attn_weight_matrix", ",", "output", ")", "\n", "# hidden_matrix.size() = (batch_size, r, 2*hidden_size)", "\n", "# Let's now concatenate the hidden_matrix and connect it to the fully connected layer.", "\n", "fc_out", "=", "self", ".", "fc_layer", "(", "hidden_matrix", ".", "view", "(", "-", "1", ",", "hidden_matrix", ".", "size", "(", ")", "[", "1", "]", "*", "hidden_matrix", ".", "size", "(", ")", "[", "2", "]", ")", ")", "\n", "logits", "=", "self", ".", "label", "(", "fc_out", ")", "\n", "# logits.size() = (batch_size, output_size)", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.detection.color_print_top_words": [[33, 44], ["print", "set", "print", "print", "x.split", "print", "print"], "function", ["None"], ["", "def", "color_print_top_words", "(", "top_words", ",", "gt_euphemism", ")", ":", "\n", "    ", "print", "(", "'[Euphemism Candidates]: '", ")", "\n", "gt_euphemism_upper", "=", "set", "(", "[", "y", "for", "x", "in", "gt_euphemism", "for", "y", "in", "x", ".", "split", "(", ")", "]", ")", "\n", "for", "i", "in", "top_words", "[", ":", "100", "]", ":", "\n", "        ", "if", "i", "in", "gt_euphemism", ":", "\n", "            ", "print", "(", "print_color", ".", "BOLD", "+", "print_color", ".", "PURPLE", "+", "i", "+", "print_color", ".", "END", ",", "end", "=", "', '", ")", "\n", "", "elif", "i", "in", "gt_euphemism_upper", ":", "\n", "            ", "print", "(", "print_color", ".", "UNDERLINE", "+", "print_color", ".", "PURPLE", "+", "i", "+", "print_color", ".", "END", ",", "end", "=", "', '", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "i", ",", "end", "=", "', '", ")", "\n", "", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.detection.evaluate_detection": [[47, 70], ["detection.color_print_top_words", "set", "enumerate", "range", "correct_list.append", "correct_list_upper.append", "len", "topk_precision_list.append", "topk_precision_list_upper.append", "len", "print", "x.split"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.detection.color_print_top_words", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["def", "evaluate_detection", "(", "top_words", ",", "gt_euphemism", ")", ":", "\n", "    ", "color_print_top_words", "(", "top_words", ",", "gt_euphemism", ")", "\n", "correct_list", "=", "[", "]", "# appear in the ground truth", "\n", "correct_list_upper", "=", "[", "]", "# not appear in the ground truth but contain in a ground truth phase.", "\n", "gt_euphemism_upper", "=", "set", "(", "[", "y", "for", "x", "in", "gt_euphemism", "for", "y", "in", "x", ".", "split", "(", ")", "]", ")", "\n", "for", "i", ",", "x", "in", "enumerate", "(", "top_words", ")", ":", "\n", "        ", "correct_list", ".", "append", "(", "1", "if", "x", "in", "gt_euphemism", "else", "0", ")", "\n", "correct_list_upper", ".", "append", "(", "1", "if", "x", "in", "gt_euphemism_upper", "else", "0", ")", "\n", "\n", "", "topk_precision_list", "=", "[", "]", "\n", "cummulative_sum", "=", "0", "\n", "topk_precision_list_upper", "=", "[", "]", "\n", "cummulative_sum_upper", "=", "0", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "correct_list", ")", ")", ":", "\n", "        ", "cummulative_sum", "+=", "correct_list", "[", "i", "]", "\n", "topk_precision_list", ".", "append", "(", "cummulative_sum", "/", "(", "i", "+", "1", ")", ")", "\n", "cummulative_sum_upper", "+=", "correct_list_upper", "[", "i", "]", "\n", "topk_precision_list_upper", ".", "append", "(", "cummulative_sum_upper", "/", "(", "i", "+", "1", ")", ")", "\n", "\n", "", "for", "topk", "in", "[", "10", ",", "20", ",", "30", ",", "40", ",", "50", ",", "60", ",", "80", ",", "100", "]", ":", "\n", "        ", "if", "topk", "<", "len", "(", "topk_precision_list", ")", ":", "\n", "            ", "print", "(", "'Top-{:d} precision is ({:.2f}, {:.2f})'", ".", "format", "(", "topk", ",", "topk_precision_list", "[", "topk", "-", "1", "]", ",", "topk_precision_list_upper", "[", "topk", "-", "1", "]", ")", ")", "\n", "", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.detection.MLM": [[73, 139], ["collections.defaultdict", "sorted", "torch.tensor", "bert_tokenizer.tokenize.index", "bert_tokenizer.tokenize", "detection.MLM.to_bert_input"], "function", ["None"], ["def", "MLM", "(", "sgs", ",", "input_keywords", ",", "thres", "=", "1", ",", "filter_uninformative", "=", "1", ")", ":", "\n", "    ", "def", "to_bert_input", "(", "tokens", ",", "bert_tokenizer", ")", ":", "\n", "        ", "token_idx", "=", "torch", ".", "tensor", "(", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", ")", "\n", "sep_idx", "=", "tokens", ".", "index", "(", "'[SEP]'", ")", "\n", "segment_idx", "=", "token_idx", "*", "0", "\n", "segment_idx", "[", "(", "sep_idx", "+", "1", ")", ":", "]", "=", "1", "\n", "mask", "=", "(", "token_idx", "!=", "0", ")", "\n", "return", "token_idx", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", ",", "segment_idx", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", ",", "mask", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "def", "single_MLM", "(", "message", ")", ":", "\n", "        ", "MLM_k", "=", "50", "\n", "tokens", "=", "bert_tokenizer", ".", "tokenize", "(", "message", ")", "\n", "if", "len", "(", "tokens", ")", "==", "0", ":", "\n", "            ", "return", "[", "]", "\n", "", "if", "tokens", "[", "0", "]", "!=", "CLS", ":", "\n", "            ", "tokens", "=", "[", "CLS", "]", "+", "tokens", "\n", "", "if", "tokens", "[", "-", "1", "]", "!=", "SEP", ":", "\n", "            ", "tokens", ".", "append", "(", "SEP", ")", "\n", "", "token_idx", ",", "segment_idx", ",", "mask", "=", "to_bert_input", "(", "tokens", ",", "bert_tokenizer", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "logits", "=", "bert_model", "(", "token_idx", ",", "segment_idx", ",", "mask", ",", "masked_lm_labels", "=", "None", ")", "\n", "", "logits", "=", "logits", ".", "squeeze", "(", "0", ")", "\n", "probs", "=", "torch", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "for", "idx", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "if", "token", "==", "MASK", ":", "\n", "                ", "topk_prob", ",", "topk_indices", "=", "torch", ".", "topk", "(", "probs", "[", "idx", ",", ":", "]", ",", "MLM_k", ")", "\n", "topk_tokens", "=", "bert_tokenizer", ".", "convert_ids_to_tokens", "(", "topk_indices", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "", "out", "=", "[", "[", "topk_tokens", "[", "i", "]", ",", "float", "(", "topk_prob", "[", "i", "]", ")", "]", "for", "i", "in", "range", "(", "MLM_k", ")", "]", "\n", "return", "out", "\n", "\n", "", "PAD", ",", "MASK", ",", "CLS", ",", "SEP", "=", "'[PAD]'", ",", "'[MASK]'", ",", "'[CLS]'", ",", "'[SEP]'", "\n", "MLM_score", "=", "defaultdict", "(", "float", ")", "\n", "temp", "=", "sgs", "if", "len", "(", "sgs", ")", "<", "10", "else", "tqdm", "(", "sgs", ")", "\n", "skip_ms_num", "=", "0", "\n", "good_sgs", "=", "[", "]", "\n", "for", "sgs_i", "in", "temp", ":", "\n", "        ", "top_words", "=", "single_MLM", "(", "sgs_i", ")", "\n", "seen_input", "=", "0", "\n", "for", "input_i", "in", "input_keywords", ":", "\n", "            ", "if", "input_i", "in", "[", "x", "[", "0", "]", "for", "x", "in", "top_words", "[", ":", "thres", "]", "]", ":", "\n", "                ", "seen_input", "+=", "1", "\n", "", "", "if", "filter_uninformative", "==", "1", "and", "seen_input", "<", "2", ":", "\n", "            ", "skip_ms_num", "+=", "1", "\n", "continue", "\n", "", "good_sgs", ".", "append", "(", "sgs_i", ")", "\n", "for", "j", "in", "top_words", ":", "\n", "            ", "if", "j", "[", "0", "]", "in", "string", ".", "punctuation", ":", "\n", "                ", "continue", "\n", "", "if", "j", "[", "0", "]", "in", "stopwords", ".", "words", "(", "'english'", ")", ":", "\n", "                ", "continue", "\n", "", "if", "j", "[", "0", "]", "in", "input_keywords", ":", "\n", "                ", "continue", "\n", "", "if", "j", "[", "0", "]", "in", "[", "'drug'", ",", "'drugs'", "]", ":", "# exclude these two for the drug dataset.", "\n", "                ", "continue", "\n", "", "if", "j", "[", "0", "]", "[", ":", "2", "]", "==", "'##'", ":", "# the '##' by BERT indicates that is not a word.", "\n", "                ", "continue", "\n", "", "MLM_score", "[", "j", "[", "0", "]", "]", "+=", "j", "[", "1", "]", "\n", "# print(sgs_i)", "\n", "# print([x[0] for x in top_words[:20]])", "\n", "", "", "out", "=", "sorted", "(", "MLM_score", ",", "key", "=", "lambda", "x", ":", "MLM_score", "[", "x", "]", ",", "reverse", "=", "True", ")", "\n", "out_tuple", "=", "[", "[", "x", ",", "MLM_score", "[", "x", "]", "]", "for", "x", "in", "out", "]", "\n", "if", "len", "(", "sgs", ")", ">=", "10", ":", "\n", "        ", "print", "(", "'The percentage of uninformative masked sentences is {:d}/{:d} = {:.2f}%'", ".", "format", "(", "skip_ms_num", ",", "len", "(", "sgs", ")", ",", "float", "(", "skip_ms_num", ")", "/", "len", "(", "sgs", ")", "*", "100", ")", ")", "\n", "", "return", "out", ",", "out_tuple", ",", "good_sgs", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.detection.euphemism_detection": [[141, 159], ["print", "print", "print", "print", "tqdm.tqdm", "random.shuffle", "print", "detection.MLM", "nltk.word_tokenize", "nltk.word_tokenize.index"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.detection.MLM"], ["", "def", "euphemism_detection", "(", "input_keywords", ",", "all_text", ",", "ms_limit", ",", "filter_uninformative", ")", ":", "\n", "    ", "print", "(", "'\\n'", "+", "'*'", "*", "40", "+", "' [Euphemism Detection] '", "+", "'*'", "*", "40", ")", "\n", "print", "(", "'[util.py] Input Keyword: '", ",", "end", "=", "''", ")", "\n", "print", "(", "input_keywords", ")", "\n", "print", "(", "'[util.py] Extracting masked sentences for input keywords...'", ")", "\n", "masked_sentence", "=", "[", "]", "\n", "for", "sentence", "in", "tqdm", "(", "all_text", ")", ":", "\n", "        ", "temp", "=", "nltk", ".", "word_tokenize", "(", "sentence", ")", "\n", "for", "input_keyword_i", "in", "input_keywords", ":", "\n", "            ", "if", "input_keyword_i", "not", "in", "temp", ":", "\n", "                ", "continue", "\n", "", "temp_index", "=", "temp", ".", "index", "(", "input_keyword_i", ")", "\n", "masked_sentence", "+=", "[", "' '", ".", "join", "(", "temp", "[", ":", "temp_index", "]", ")", "+", "' [MASK] '", "+", "' '", ".", "join", "(", "temp", "[", "temp_index", "+", "1", ":", "]", ")", "]", "\n", "", "", "random", ".", "shuffle", "(", "masked_sentence", ")", "\n", "masked_sentence", "=", "masked_sentence", "[", ":", "ms_limit", "]", "\n", "print", "(", "'[util.py] Generating top candidates...'", ")", "\n", "top_words", ",", "_", ",", "_", "=", "MLM", "(", "masked_sentence", ",", "input_keywords", ",", "thres", "=", "5", ",", "filter_uninformative", "=", "filter_uninformative", ")", "\n", "return", "top_words", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.embed.embed_helper.train_char_embed": [[17, 33], ["gensim.models.word2vec.LineSentence", "print", "time.time", "gensim.models.FastText", "time.time", "print", "gensim.models.FastText.save", "print"], "function", ["None"], ["def", "train_char_embed", "(", "corpus_fn", ",", "model_fn", ",", "vec_dim", "=", "100", ",", "window", "=", "8", ")", ":", "\n", "# train embed", "\n", "    ", "sentences", "=", "LineSentence", "(", "corpus_fn", ")", "\n", "sent_cnt", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "sent_cnt", "+=", "1", "\n", "", "print", "(", "\"# of sents: {}\"", ".", "format", "(", "sent_cnt", ")", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "model", "=", "FastText", "(", "sentences", ",", "min_count", "=", "10", ",", "size", "=", "vec_dim", ",", "\n", "window", "=", "window", ",", "iter", "=", "8", ",", "workers", "=", "30", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\"embed train time: {}s\"", ".", "format", "(", "end", "-", "start", ")", ")", "\n", "\n", "# save embed model", "\n", "model", ".", "save", "(", "model_fn", ")", "\n", "print", "(", "\"Save FastText model to {}\"", ".", "format", "(", "model_fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.embed.embed_helper.train_word2vec_embed": [[35, 51], ["gensim.models.word2vec.LineSentence", "print", "time.time", "gensim.models.Word2Vec", "time.time", "print", "gensim.models.Word2Vec.wv.save_word2vec_format", "print"], "function", ["None"], ["", "def", "train_word2vec_embed", "(", "corpus_fn", ",", "embed_fn", ",", "ft", "=", "100", ",", "vec_dim", "=", "100", ",", "window", "=", "8", ")", ":", "\n", "# train embed", "\n", "    ", "sentences", "=", "LineSentence", "(", "corpus_fn", ")", "\n", "sent_cnt", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "sent_cnt", "+=", "1", "\n", "", "print", "(", "\"# of sents: {}\"", ".", "format", "(", "sent_cnt", ")", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "model", "=", "Word2Vec", "(", "sentences", ",", "min_count", "=", "ft", ",", "size", "=", "vec_dim", ",", "\n", "window", "=", "window", ",", "iter", "=", "10", ",", "workers", "=", "30", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\"embed train time: {}s\"", ".", "format", "(", "end", "-", "start", ")", ")", "\n", "\n", "# save embed", "\n", "model", ".", "wv", ".", "save_word2vec_format", "(", "embed_fn", ",", "binary", "=", "False", ")", "\n", "print", "(", "\"save embedding to {}\"", ".", "format", "(", "embed_fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_euphemisms.data_helper.read_realword_seeds": [[3, 15], ["open", "line.strip().split", "selected_seeds.append", "line.strip"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["def", "read_realword_seeds", "(", "target_word_fn", ",", "dictionary", ",", "limit", "=", "200", ")", ":", "\n", "    ", "selected_seeds", "=", "[", "]", "\n", "cnt", "=", "0", "\n", "with", "open", "(", "target_word_fn", ",", "\"r\"", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "word", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", "0", "]", "\n", "if", "word", "in", "dictionary", ":", "\n", "                ", "selected_seeds", ".", "append", "(", "word", ")", "\n", "cnt", "+=", "1", "\n", "if", "cnt", ">", "limit", ":", "\n", "                    ", "break", "\n", "", "", "", "", "return", "selected_seeds", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_euphemisms.data_helper.read_misspelled_seeds": [[17, 29], ["open", "line.strip().split", "selected_seeds.append", "line.strip"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["", "def", "read_misspelled_seeds", "(", "target_word_fn", ",", "dictionary", ",", "limit", "=", "200", ")", ":", "\n", "    ", "selected_seeds", "=", "[", "]", "\n", "cnt", "=", "0", "\n", "with", "open", "(", "target_word_fn", ",", "\"r\"", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "word", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", "0", "]", "\n", "if", "word", "not", "in", "dictionary", ":", "\n", "                ", "selected_seeds", ".", "append", "(", "word", ")", "\n", "cnt", "+=", "1", "\n", "if", "cnt", ">", "limit", ":", "\n", "                    ", "break", "\n", "", "", "", "", "return", "selected_seeds", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_euphemisms.char_eu_detection.eval_char_embed": [[21, 44], ["open", "open.write", "gensim.models.FastText.load", "open.close", "wv.similar_by_word", "open.write", "wv.similar_by_word", "print", "selected_nebs.append"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["def", "eval_char_embed", "(", "model_fn", ",", "seeds", ",", "res_fn", ")", ":", "\n", "    ", "fout", "=", "open", "(", "res_fn", ",", "\"w\"", ")", "\n", "fout", ".", "write", "(", "\",\"", ".", "join", "(", "[", "\"KeyWord\"", ",", "\"Neighbors\"", "]", ")", "+", "\"\\n\"", ")", "\n", "model", "=", "FastText", ".", "load", "(", "model_fn", ")", "\n", "wv", "=", "model", ".", "wv", "\n", "for", "seed", "in", "seeds", ":", "\n", "        ", "try", ":", "\n", "            ", "neb_scores", "=", "wv", ".", "similar_by_word", "(", "seed", ",", "topn", "=", "50", ")", "\n", "nebs", "=", "[", "ns", "[", "0", "]", "for", "ns", "in", "neb_scores", "]", "\n", "# save all neighbors", "\n", "fout", ".", "write", "(", "\",\"", ".", "join", "(", "[", "seed", "]", "+", "nebs", ")", "+", "\"\\n\"", ")", "\n", "selected_nebs", "=", "[", "]", "\n", "for", "neb_score", "in", "neb_scores", ":", "\n", "                ", "neb", ",", "score", "=", "neb_score", "\n", "double_neb_scores", "=", "wv", ".", "similar_by_word", "(", "neb", ",", "topn", "=", "50", ")", "\n", "neb_nebs", "=", "[", "tup", "[", "0", "]", "for", "tup", "in", "double_neb_scores", "]", "\n", "if", "seed", "in", "neb_nebs", ":", "\n", "                    ", "selected_nebs", ".", "append", "(", "neb", ")", "\n", "# save selected neighbors", "\n", "#fout.write(\",\".join([seed] + selected_nebs)+\"\\n\")", "\n", "", "", "", "except", ":", "\n", "            ", "print", "(", "\"{} not in the dictionary\"", ".", "format", "(", "seed", ")", ")", "\n", "", "", "fout", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_euphemisms.word2vec_eu_detection.eval_w2v_embed": [[15, 37], ["open", "open.write", "gensim.models.KeyedVectors.load_word2vec_format", "open.close", "print", "print", "KeyedVectors.load_word2vec_format.similar_by_word", "open.write", "KeyedVectors.load_word2vec_format.similar_by_word", "print", "set", "print"], "function", ["None"], ["def", "eval_w2v_embed", "(", "embed_fn", ",", "seeds", "=", "[", "]", ",", "res_fn", "=", "None", ")", ":", "\n", "    ", "fout", "=", "open", "(", "res_fn", ",", "\"w\"", ")", "\n", "fout", ".", "write", "(", "\",\"", ".", "join", "(", "[", "\"KeyWord\"", ",", "\"Neighbors\"", "]", ")", "+", "\"\\n\"", ")", "\n", "wv", "=", "KeyedVectors", ".", "load_word2vec_format", "(", "embed_fn", ",", "binary", "=", "False", ")", "\n", "for", "seed", "in", "seeds", ":", "\n", "        ", "print", "(", "\"seed: {}\"", ".", "format", "(", "seed", ")", ")", "\n", "try", ":", "\n", "            ", "neb_scores", "=", "wv", ".", "similar_by_word", "(", "seed", ",", "topn", "=", "100", ")", "\n", "#print(neb_scores)", "\n", "nebs", "=", "[", "ns", "[", "0", "]", "for", "ns", "in", "neb_scores", "]", "\n", "fout", ".", "write", "(", "\",\"", ".", "join", "(", "[", "seed", "]", "+", "nebs", ")", "+", "\"\\n\"", ")", "\n", "# select words", "\n", "for", "neb_score", "in", "neb_scores", ":", "\n", "                ", "neb", ",", "score", "=", "neb_score", "\n", "double_neb_scores", "=", "wv", ".", "similar_by_word", "(", "neb", ",", "topn", "=", "100", ")", "\n", "neb_nebs", "=", "[", "tup", "[", "0", "]", "for", "tup", "in", "double_neb_scores", "]", "\n", "if", "seed", "in", "set", "(", "neb_nebs", ")", ":", "\n", "                    ", "print", "(", "\"neb: {} for seed: {}\"", ".", "format", "(", "neb", ",", "seed", ")", ")", "\n", "", "", "", "except", ":", "\n", "            ", "print", "(", "\"not in the dictionary!\"", ")", "\n", "", "print", "(", "\"\\n\"", ")", "\n", "", "fout", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.bert_embedder.get_vector_target_word": [[15, 30], ["sent[].index", "numpy.add"], "function", ["None"], ["def", "get_vector_target_word", "(", "embedding", ",", "target_word", ")", ":", "\n", "# Go through each of the sentences and see if the target_word is present", "\n", "# Take the sum total of the vectors ", "\n", "# Take the avg of the vectors", "\n", "# return the position of the word in vector space", "\n", "    ", "vec_sum", "=", "0", "\n", "n", "=", "0", "\n", "for", "sent", "in", "embedding", ":", "\n", "        ", "if", "target_word", "in", "sent", "[", "0", "]", ":", "\n", "            ", "n", "+=", "1", "\n", "index_temp", "=", "sent", "[", "0", "]", ".", "index", "(", "target_word", ")", "\n", "vec_sum", "=", "np", ".", "add", "(", "vec_sum", ",", "sent", "[", "1", "]", "[", "index_temp", "]", ")", "\n", "\n", "\n", "", "", "return", "(", "vec_sum", ",", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.bert_embedder.get_sentence_vectors": [[31, 44], ["len", "sentences.append", "vectors.append", "sum", "len"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["", "def", "get_sentence_vectors", "(", "embeddings", ")", ":", "\n", "#return a dictinoary with sentences and the avg of the word vectors ", "\n", "# find the sentence embedding", "\n", "    ", "sentences", "=", "[", "]", "\n", "vectors", "=", "[", "]", "\n", "for", "sent", "in", "embeddings", ":", "\n", "        ", "if", "(", "len", "(", "sent", "[", "1", "]", ")", ">", "0", ")", ":", "\n", "            ", "complete_sent", "=", "\" \"", ".", "join", "(", "sent", "[", "0", "]", ")", "\n", "sent_vector", "=", "(", "sum", "(", "sent", "[", "1", "]", ")", "/", "len", "(", "sent", "[", "1", "]", ")", ")", "\n", "sentences", ".", "append", "(", "complete_sent", ")", "\n", "vectors", ".", "append", "(", "sent_vector", ")", "\n", "\n", "", "", "return", "(", "sentences", ",", "vectors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.bert_embedder.process_embeddibngs": [[45, 48], ["bert_embedder.get_vector_target_word"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.bert_embedder.get_vector_target_word"], ["", "def", "process_embeddibngs", "(", "embeddings", ",", "word", ")", ":", "\n", "    ", "vector_sum", ",", "n", "=", "get_vector_target_word", "(", "embeddings", ",", "word", ")", "\n", "return", "(", "vector_sum", ",", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.bert_embedder.save_word_vector": [[50, 54], ["numpy.divide", "open", "pickle.dump"], "function", ["None"], ["", "def", "save_word_vector", "(", "sum_vec", ",", "num", ",", "path", ")", ":", "\n", "    ", "vec", "=", "np", ".", "divide", "(", "sum_vec", ",", "num", ")", "\n", "with", "open", "(", "path", ",", "'wb'", ")", "as", "fout", ":", "\n", "        ", "pickle", ".", "dump", "(", "vec", ",", "fout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.bert_embedder.main": [[55, 92], ["con.cursor", "argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "open", "bert_embedder.save_word_vector", "con.close", "con.commit", "print", "pickle.load", "bert_embedder.process_embeddibngs", "numpy.add", "bert_embedder.get_sentence_vectors", "zip", "con.cursor.execute"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.bert_embedder.save_word_vector", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.bert_embedder.process_embeddibngs", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.bert_embedder.get_sentence_vectors"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "cur", "=", "con", ".", "cursor", "(", ")", "\n", "\n", "########## Uncomment to create a new table", "\n", "# cur.execute(\"create table Sentences (vectors array, sentences text)\")", "\n", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Pass the files.'", ")", "\n", "parser", ".", "add_argument", "(", "'--embeddings_in'", ",", "required", "=", "True", ",", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--word_vec_out'", ",", "required", "=", "True", ",", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "file_path_word_emb", "=", "args", ".", "word_vec_out", "\n", "embedding_path", "=", "args", ".", "embeddings_in", "\n", "N", "=", "0", "\n", "vec_sum", "=", "0", "\n", "i", "=", "0", "\n", "key_word", "=", "'cocaine'", "\n", "with", "open", "(", "embedding_path", ",", "'rb'", ")", "as", "temp_fin", ":", "\n", "        ", "while", "1", ":", "\n", "            ", "try", ":", "\n", "                ", "con", ".", "commit", "(", ")", "\n", "i", "+=", "1", "\n", "print", "(", "f\"Current batch is {i}\"", ")", "\n", "obj", "=", "(", "pickle", ".", "load", "(", "temp_fin", ")", ")", "\n", "new_sum", ",", "n", "=", "process_embeddibngs", "(", "obj", ",", "key_word", ")", "\n", "vec_sum", "=", "np", ".", "add", "(", "vec_sum", ",", "new_sum", ")", "\n", "N", "+=", "n", "\n", "sentences", ",", "sentences_vec", "=", "get_sentence_vectors", "(", "obj", ")", "\n", "for", "sent", ",", "sent_vec", "in", "zip", "(", "sentences", ",", "sentences_vec", ")", ":", "\n", "                    ", "cur", ".", "execute", "(", "\"INSERT INTO Sentences VALUES (?,?)\"", ",", "(", "sent_vec", ",", "sent", ",", ")", ")", "\n", "\n", "", "", "except", "EOFError", ":", "\n", "                ", "break", "\n", "\n", "", "", "save_word_vector", "(", "vec_sum", ",", "N", ",", "file_path_word_emb", ")", "\n", "con", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.neighbouring_sentences.get_sentences": [[24, 36], ["cur.execute", "cur.fetchall", "cur.execute", "cur.fetchone", "sents.append", "open", "csv.writer", "csv.writer.writerow"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["def", "get_sentences", "(", "indexes", ",", "file_path", ")", ":", "\n", "    ", "sents", "=", "[", "]", "\n", "cur", ".", "execute", "(", "\"SELECT vectors FROM Sentences\"", ")", "\n", "all_vecs", "=", "cur", ".", "fetchall", "(", ")", "\n", "for", "index", "in", "indexes", ":", "\n", "        ", "cur", ".", "execute", "(", "\"SELECT * FROM Sentences WHERE vectors=?\"", ",", "(", "all_vecs", "[", "index", "]", "[", "0", "]", ",", ")", ")", "\n", "arr_vec", ",", "sent", "=", "cur", ".", "fetchone", "(", ")", "\n", "sents", ".", "append", "(", "sent", ")", "\n", "\n", "", "with", "open", "(", "file_path", ",", "'w'", ")", "as", "output_file", ":", "\n", "        ", "wr", "=", "csv", ".", "writer", "(", "output_file", ")", "\n", "wr", ".", "writerow", "(", "sents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.neighbouring_sentences.get_query_vector": [[39, 44], ["open", "pickle.load"], "function", ["None"], ["", "", "def", "get_query_vector", "(", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "'rb'", ")", "as", "fin", ":", "\n", "        ", "a", "=", "pickle", ".", "load", "(", "fin", ")", "\n", "\n", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.neighbouring_sentences.main": [[45, 58], ["con.cursor", "neighbouring_sentences.get_query_vector", "con.cursor.execute", "print", "neighbouring_sentences.get_sentences", "cosine_sim.append", "len", "sorted", "scipy.spatial.distance.cosine", "range", "len"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.neighbouring_sentences.get_query_vector", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.neighbouring_sentences.get_sentences", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["", "def", "main", "(", ")", ":", "\n", "    ", "cur", "=", "con", ".", "cursor", "(", ")", "\n", "mj_vec", "=", "args", ".", "embeddings_in", "\n", "target_vec", "=", "get_query_vector", "(", "mj_vec", ")", "\n", "cosine_sim", "=", "[", "]", "\n", "for", "vec", "in", "cur", ".", "execute", "(", "\"SELECT  vectors FROM Sentences\"", ")", ":", "\n", "        ", "cosine_similarity", "=", "1", "-", "distance", ".", "cosine", "(", "vec", ",", "target_vec", ")", "\n", "cosine_sim", ".", "append", "(", "cosine_similarity", ")", "\n", "\n", "", "print", "(", "len", "(", "cosine_sim", ")", ")", "\n", "N", "=", "100", "\n", "cos_indexes", "=", "sorted", "(", "range", "(", "len", "(", "cosine_sim", ")", ")", ",", "key", "=", "lambda", "sub", ":", "cosine_sim", "[", "sub", "]", ")", "[", "-", "N", ":", "]", "\n", "get_sentences", "(", "cos_indexes", ",", "args", ".", "sent_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.bert_tuning.batch_create": [[9, 14], ["len", "range", "min"], "function", ["None"], ["def", "batch_create", "(", "iterable", ",", "n", "=", "1", ")", ":", "\n", "#Used to create batches of an iterable object(e.g-List)", "\n", "    ", "l", "=", "len", "(", "iterable", ")", "\n", "for", "ndx", "in", "range", "(", "0", ",", "l", ",", "n", ")", ":", "\n", "        ", "yield", "iterable", "[", "ndx", ":", "min", "(", "ndx", "+", "n", ",", "l", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.bert_tuning.main": [[16, 42], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "fin.read.split", "list", "bert_tuning.batch_create", "print", "tqdm.tqdm", "open", "fin.read", "filter", "bert_embedding", "open", "pickle.dump"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.bert_tuning.batch_create"], ["", "", "def", "main", "(", ")", ":", "\n", "# reddit_in_text_path = '/Volumes/My Passport for Mac/Rohan/Euphemism_Detection_2020/Wiki_data/wiki_whole.txt'", "\n", "# reddit_out_file='/Volumes/My Passport for Mac/Rohan/Euphemism_Detection_2020/Wiki_data/wiki_embeddings.pkl'", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Pass the files.'", ")", "\n", "parser", ".", "add_argument", "(", "'--corpus_in'", ",", "required", "=", "True", ",", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--vectors_out'", ",", "required", "=", "True", ",", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "#Read the input file", "\n", "with", "open", "(", "args", ".", "corpus_in", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "str_1", "=", "fin", ".", "read", "(", ")", "\n", "", "sentences", "=", "str_1", ".", "split", "(", "\"\\n\"", ")", "\n", "sentences", "=", "list", "(", "filter", "(", "None", ",", "sentences", ")", ")", "\n", "sentence_batches", "=", "batch_create", "(", "sentences", ",", "3000", ")", "\n", "#Create batches of the text file to avoid Memory Issues", "\n", "print", "(", "\"#Start training the vectors\"", ")", "\n", "for", "batch", "in", "tqdm", "(", "sentence_batches", ")", ":", "\n", "        ", "try", ":", "\n", "#Train the embeddings in batches", "\n", "            ", "reddit_bert_vector", "=", "bert_embedding", "(", "batch", ")", "\n", "# Dump the embeddings ", "\n", "with", "open", "(", "args", ".", "vectors_out", ",", "'ab+'", ")", "as", "fout", ":", "\n", "                ", "pickle", ".", "dump", "(", "reddit_bert_vector", ",", "fout", ")", "\n", "", "pass", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.__init__": [[25, 39], ["h5py.File", "h5f.create_dataset"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "datapath", ",", "dataset", ",", "shape", ",", "dtype", "=", "np", ".", "float32", ",", "compression", "=", "\"gzip\"", ",", "chunk_len", "=", "1", ")", ":", "\n", "        ", "self", ".", "datapath", "=", "datapath", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "shape", "=", "shape", "\n", "self", ".", "i", "=", "0", "\n", "\n", "with", "h5py", ".", "File", "(", "self", ".", "datapath", ",", "mode", "=", "'w'", ")", "as", "h5f", ":", "\n", "            ", "self", ".", "dset", "=", "h5f", ".", "create_dataset", "(", "\n", "dataset", ",", "\n", "shape", "=", "(", "0", ",", ")", "+", "shape", ",", "\n", "maxshape", "=", "(", "None", ",", ")", "+", "shape", ",", "\n", "dtype", "=", "dtype", ",", "\n", "compression", "=", "compression", ",", "\n", "chunks", "=", "(", "chunk_len", ",", ")", "+", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append": [[40, 47], ["h5py.File", "dset.resize", "h5f.flush"], "methods", ["None"], ["", "", "def", "append", "(", "self", ",", "values", ")", ":", "\n", "        ", "with", "h5py", ".", "File", "(", "self", ".", "datapath", ",", "mode", "=", "'a'", ")", "as", "h5f", ":", "\n", "            ", "dset", "=", "h5f", "[", "self", ".", "dataset", "]", "\n", "dset", ".", "resize", "(", "(", "self", ".", "i", "+", "1", ",", ")", "+", "self", ".", "shape", ")", "\n", "dset", "[", "self", ".", "i", "]", "=", "[", "values", "]", "\n", "self", ".", "i", "+=", "1", "\n", "h5f", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.adapt_array": [[51, 59], ["io.BytesIO", "numpy.save", "io.BytesIO.seek", "sqlite3.Binary", "io.BytesIO.read"], "function", ["None"], ["", "", "", "def", "adapt_array", "(", "arr", ")", ":", "\n", "    ", "\"\"\"\n    http://stackoverflow.com/a/31312102/190597\n    \"\"\"", "\n", "out", "=", "io", ".", "BytesIO", "(", ")", "\n", "np", ".", "save", "(", "out", ",", "arr", ")", "\n", "out", ".", "seek", "(", "0", ")", "\n", "return", "sqlite3", ".", "Binary", "(", "out", ".", "read", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.convert_array": [[60, 64], ["io.BytesIO", "io.BytesIO.seek", "numpy.load"], "function", ["None"], ["", "def", "convert_array", "(", "text", ")", ":", "\n", "    ", "out", "=", "io", ".", "BytesIO", "(", "text", ")", "\n", "out", ".", "seek", "(", "0", ")", "\n", "return", "np", ".", "load", "(", "out", ")", "", "", ""]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.preprocess.preprocess_gab.tokenize_str": [[9, 24], ["re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub.strip().lower", "re.sub.strip"], "function", ["None"], ["def", "tokenize_str", "(", "string", ")", ":", "\n", "  ", "string", "=", "re", ".", "sub", "(", "r\"[^A-Za-z0-9()#@$,!?\\'\\`]\"", ",", "\" \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'s\"", ",", "\" \\'s\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ve\"", ",", "\" \\'ve\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"n\\'t\"", ",", "\" n\\'t\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'re\"", ",", "\" \\'re\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'d\"", ",", "\" \\'d\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ll\"", ",", "\" \\'ll\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\",\"", ",", "\" , \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"!\"", ",", "\" ! \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\(\"", ",", "\" \\( \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\)\"", ",", "\" \\) \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\?\"", ",", "\" \\? \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\s{2,}\"", ",", "\" \"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.preprocess.preprocess_gab.tokenize_post": [[26, 29], ["p.tokenize"], "function", ["None"], ["", "def", "tokenize_post", "(", "string", ")", ":", "\n", "  ", "import", "preprocessor", "as", "p", "\n", "return", "p", ".", "tokenize", "(", "string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.preprocess.preprocess_gab.write_gab_corpus": [[31, 45], ["open", "os.listdir", "open.close", "print", "open", "eval", "preprocess_gab.tokenize_str", "open.write", "line.replace.replace", "line.replace.strip"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.corpus_helper.tokenize_str"], ["", "def", "write_gab_corpus", "(", "folder", ",", "save_fn", ")", ":", "\n", "    ", "fout", "=", "open", "(", "save_fn", ",", "\"w\"", ")", "\n", "for", "rel_fn", "in", "os", ".", "listdir", "(", "folder", ")", ":", "\n", "        ", "fn", "=", "folder", "+", "\"/\"", "+", "rel_fn", "\n", "with", "open", "(", "fn", ",", "\"r\"", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "for", "tok", "in", "tok_map", ":", "\n", "                    ", "line", "=", "line", ".", "replace", "(", "tok", ",", "tok_map", "[", "tok", "]", ")", "\n", "", "post_dict", "=", "eval", "(", "line", ".", "strip", "(", ")", ")", "\n", "post", "=", "post_dict", "[", "\"body\"", "]", "\n", "tok_post", "=", "tokenize_str", "(", "post", ")", "\n", "fout", ".", "write", "(", "tok_post", "+", "\"\\n\"", ")", "\n", "", "", "", "fout", ".", "close", "(", ")", "\n", "print", "(", "\"save tok gap posts to {}\"", ".", "format", "(", "save_fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.preprocess.preprocess_reddit.tokenize_str": [[6, 21], ["re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub.strip().lower", "re.sub.strip"], "function", ["None"], ["def", "tokenize_str", "(", "string", ")", ":", "\n", "    ", "string", "=", "re", ".", "sub", "(", "r\"[^A-Za-z0-9()#@$,!?\\'\\`]\"", ",", "\" \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'s\"", ",", "\" \\'s\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ve\"", ",", "\" \\'ve\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"n\\'t\"", ",", "\" n\\'t\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'re\"", ",", "\" \\'re\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'d\"", ",", "\" \\'d\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ll\"", ",", "\" \\'ll\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\",\"", ",", "\" , \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"!\"", ",", "\" ! \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\(\"", ",", "\" \\( \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\)\"", ",", "\" \\) \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\?\"", ",", "\" \\? \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\s{2,}\"", ",", "\" \"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.preprocess.preprocess_reddit.proc_reddit_corpus": [[23, 33], ["open", "open.close", "print", "open", "csv.reader", "next", "open.write", "preprocess_reddit.tokenize_str"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.corpus_helper.tokenize_str"], ["", "def", "proc_reddit_corpus", "(", "fn", "=", "\"data/reddit.csv\"", ",", "save_fn", "=", "\"reddit_corpus.txt\"", ")", ":", "\n", "    ", "fout", "=", "open", "(", "save_fn", ",", "\"w\"", ")", "\n", "with", "open", "(", "fn", ")", "as", "csvfile", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "csvfile", ",", "delimiter", "=", "\",\"", ")", "\n", "next", "(", "reader", ")", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "subreddit", ",", "title", ",", "text", "=", "row", "\n", "fout", ".", "write", "(", "tokenize_str", "(", "text", ")", "+", "\"\\n\"", ")", "\n", "", "", "fout", ".", "close", "(", ")", "\n", "print", "(", "\"Save reddit corpus to {}\"", ".", "format", "(", "save_fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.preprocess.preprocess_reddit.read_corpus_vocab": [[34, 46], ["collections.Counter", "print", "open", "preprocess_reddit.tokenize_str", "tokenize_str.split", "len", "line.strip"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.corpus_helper.tokenize_str"], ["", "def", "read_corpus_vocab", "(", "fn", ")", ":", "\n", "    ", "cnt", "=", "Counter", "(", ")", "\n", "with", "open", "(", "fn", ",", "\"r\"", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "if", "line", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "                ", "continue", "\n", "", "tok_line", "=", "tokenize_str", "(", "line", ")", "\n", "seq", "=", "tok_line", ".", "split", "(", ")", "\n", "for", "word", "in", "seq", ":", "\n", "                ", "cnt", "[", "word", "]", "+=", "1", "\n", "", "", "", "print", "(", "\"fn: {}, vocab size: {}\"", ".", "format", "(", "fn", ",", "len", "(", "cnt", ")", ")", ")", "\n", "return", "cnt", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.identify_words_tfidf.identify_words": [[7, 27], ["corpus_helper.read_vocab_from_wiki", "corpus_helper.read_vocab_from_corpus", "collections.defaultdict", "sorted", "collections.defaultdict.items", "open", "open.close", "print", "word.isalpha", "open.write", "str"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.corpus_helper.read_vocab_from_wiki", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.corpus_helper.read_vocab_from_corpus"], ["def", "identify_words", "(", "target_corpus", ",", "wiki_corpus", ",", "ft", ",", "save_path", ")", ":", "\n", "    ", "wiki_cnt", "=", "read_vocab_from_wiki", "(", "wiki_corpus", ")", "\n", "target_cnt", "=", "read_vocab_from_corpus", "(", "target_corpus", ")", "\n", "score_dict", "=", "defaultdict", "(", "float", ")", "\n", "for", "word", "in", "target_cnt", ":", "\n", "        ", "if", "target_cnt", "[", "word", "]", "<", "ft", ":", "\n", "            ", "continue", "\n", "", "if", "not", "word", ".", "isalpha", "(", ")", ":", "\n", "            ", "continue", "\n", "", "score", "=", "1.0", "*", "target_cnt", "[", "word", "]", "/", "(", "target_cnt", "[", "word", "]", "+", "wiki_cnt", "[", "word", "]", ")", "\n", "score_dict", "[", "word", "]", "=", "score", "\n", "", "sorted_dict", "=", "sorted", "(", "score_dict", ".", "items", "(", ")", ",", "key", "=", "lambda", "item", ":", "item", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "if", "save_path", "is", "not", "None", ":", "\n", "        ", "fout", "=", "open", "(", "save_path", ",", "\"w\"", ")", "\n", "for", "item", "in", "sorted_dict", ":", "\n", "            ", "word", ",", "score", "=", "item", "\n", "fout", ".", "write", "(", "word", "+", "\"\\t\"", "+", "str", "(", "score", ")", "+", "\"\\n\"", ")", "\n", "", "fout", ".", "close", "(", ")", "\n", "print", "(", "\"save words to {}\"", ".", "format", "(", "save_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.corpus_helper.tokenize_str": [[6, 21], ["re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub.strip().lower", "re.sub.strip"], "function", ["None"], ["def", "tokenize_str", "(", "string", ")", ":", "\n", "  ", "string", "=", "re", ".", "sub", "(", "r\"[^A-Za-z0-9()#@$,!?\\'\\`]\"", ",", "\" \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'s\"", ",", "\" \\'s\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ve\"", ",", "\" \\'ve\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"n\\'t\"", ",", "\" n\\'t\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'re\"", ",", "\" \\'re\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'d\"", ",", "\" \\'d\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ll\"", ",", "\" \\'ll\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\",\"", ",", "\" , \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"!\"", ",", "\" ! \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\(\"", ",", "\" \\( \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\)\"", ",", "\" \\) \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\?\"", ",", "\" \\? \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\s{2,}\"", ",", "\" \"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.corpus_helper.read_vocab_from_wiki": [[23, 34], ["collections.Counter", "open", "corpus_helper.tokenize_str", "tokenize_str.split", "line.strip"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.corpus_helper.tokenize_str"], ["", "def", "read_vocab_from_wiki", "(", "fn", ")", ":", "\n", "    ", "cnt", "=", "Counter", "(", ")", "\n", "with", "open", "(", "fn", ",", "\"r\"", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "if", "line", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "                ", "continue", "\n", "", "tok_line", "=", "tokenize_str", "(", "line", ")", "\n", "seq", "=", "tok_line", ".", "split", "(", ")", "\n", "for", "word", "in", "seq", ":", "\n", "                ", "cnt", "[", "word", "]", "+=", "1", "\n", "", "", "", "return", "cnt", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.corpus_helper.read_vocab_from_corpus": [[36, 46], ["collections.Counter", "open", "line.lower().split", "line.strip", "line.lower"], "function", ["None"], ["", "def", "read_vocab_from_corpus", "(", "fn", ")", ":", "\n", "    ", "cnt", "=", "Counter", "(", ")", "\n", "with", "open", "(", "fn", ",", "\"r\"", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "if", "line", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "                ", "continue", "\n", "", "seq", "=", "line", ".", "lower", "(", ")", ".", "split", "(", ")", "\n", "for", "word", "in", "seq", ":", "\n", "                ", "cnt", "[", "word", "]", "+=", "1", "\n", "", "", "", "return", "cnt", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.identify_words_dict.identify_words": [[6, 27], ["corpus_helper.read_vocab_from_corpus", "set", "nltk.corpus.words.words", "selected_words.append", "open", "open.close", "print", "word.isalpha", "open.write"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.corpus_helper.read_vocab_from_corpus", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["def", "identify_words", "(", "corpus", ",", "ft", ",", "save_path", ")", ":", "\n", "# get vocabulary of the corpus", "\n", "    ", "vocab_cnt", "=", "read_vocab_from_corpus", "(", "corpus", ")", "\n", "# load standard dictionary", "\n", "dictionary", "=", "set", "(", "words", ".", "words", "(", ")", ")", "\n", "# select words that are not in standard dictionary", "\n", "selected_words", "=", "[", "]", "\n", "for", "word", "in", "vocab_cnt", ":", "\n", "        ", "if", "vocab_cnt", "[", "word", "]", "<", "ft", ":", "\n", "            ", "continue", "\n", "", "if", "word", "in", "dictionary", ":", "\n", "            ", "continue", "\n", "", "if", "not", "word", ".", "isalpha", "(", ")", ":", "\n", "            ", "continue", "\n", "", "selected_words", ".", "append", "(", "word", ")", "\n", "", "if", "save_path", "!=", "None", ":", "\n", "        ", "fout", "=", "open", "(", "save_path", ",", "\"w\"", ")", "\n", "for", "word", "in", "selected_words", ":", "\n", "            ", "fout", ".", "write", "(", "word", "+", "\"\\n\"", ")", "\n", "", "fout", ".", "close", "(", ")", "\n", "print", "(", "\"save words to {}\"", ".", "format", "(", "save_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.identify_words_graph.load_emb_from_wiki": [[21, 24], ["gensim.models.KeyedVectors.load_word2vec_format"], "function", ["None"], ["def", "load_emb_from_wiki", "(", "embed_fn", ")", ":", "\n", "    ", "emb_dict", "=", "KeyedVectors", ".", "load_word2vec_format", "(", "embed_fn", ",", "binary", "=", "False", ",", "limit", "=", "20000", ")", "\n", "return", "emb_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.identify_words_graph.build_vocab_emb_from_corpus": [[26, 49], ["collections.Counter", "collections.Counter.most_common", "print", "open", "line.strip().split", "vocab.append", "emb_arr.append", "len", "len", "len", "list", "line.strip"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["", "def", "build_vocab_emb_from_corpus", "(", "corpus_fn", ",", "emb_dict", ",", "ft", "=", "5", ")", ":", "\n", "    ", "vocab_cnt", "=", "Counter", "(", ")", "\n", "with", "open", "(", "corpus_fn", ",", "\"r\"", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "seq", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "for", "tok", "in", "seq", ":", "\n", "                ", "vocab_cnt", "[", "tok", "]", "+=", "1", "\n", "", "", "", "sorted_vocab", "=", "vocab_cnt", ".", "most_common", "(", ")", "\n", "vocab", "=", "[", "]", "\n", "word2idx", "=", "{", "}", "\n", "emb_arr", "=", "[", "]", "\n", "for", "(", "word", ",", "cnt", ")", "in", "sorted_vocab", ":", "\n", "        ", "if", "cnt", "<=", "ft", ":", "\n", "            ", "continue", "\n", "", "try", ":", "\n", "            ", "vec", "=", "emb_dict", "[", "word", "]", "\n", "vocab", ".", "append", "(", "word", ")", "\n", "word2idx", "[", "word", "]", "=", "len", "(", "vocab", ")", "-", "1", "\n", "emb_arr", ".", "append", "(", "list", "(", "vec", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "", "", "print", "(", "\"vocab size: {}, emb_size: {}\"", ".", "format", "(", "len", "(", "vocab", ")", ",", "len", "(", "emb_arr", ")", ")", ")", "\n", "return", "vocab", ",", "word2idx", ",", "emb_arr", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.identify_words_graph.create_graph_with_pairwise_cosine": [[51, 64], ["networkx.Graph", "len", "range", "nx.Graph.add_node", "sklearn.metrics.pairwise.cosine_distances", "range", "nx.Graph.add_weighted_edges_from"], "function", ["None"], ["", "def", "create_graph_with_pairwise_cosine", "(", "emb_arr", ",", "vocab", ")", ":", "\n", "    ", "graph", "=", "nx", ".", "Graph", "(", ")", "\n", "for", "word", "in", "vocab", ":", "\n", "        ", "graph", ".", "add_node", "(", "word", ")", "\n", "", "vocab_size", "=", "len", "(", "vocab", ")", "\n", "for", "i", "in", "range", "(", "vocab_size", "-", "1", ")", ":", "\n", "# row: (1, dim)", "\n", "        ", "row", "=", "[", "emb_arr", "[", "i", "]", "]", "\n", "# cosine_dist_row: (1, vocab_size)", "\n", "cosine_dist_row", "=", "cosine_dist_func", "(", "row", ",", "emb_arr", "[", "i", "+", "1", ":", "]", ")", "\n", "for", "j", "in", "range", "(", "i", "+", "1", ",", "vocab_size", ")", ":", "\n", "            ", "graph", ".", "add_weighted_edges_from", "(", "[", "(", "vocab", "[", "i", "]", ",", "vocab", "[", "j", "]", ",", "cosine_dist_row", "[", "0", "]", "[", "j", "-", "i", "-", "1", "]", ")", "]", ")", "\n", "", "", "return", "graph", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.identify_words_graph.count_word_pairs": [[66, 87], ["collections.defaultdict", "open", "line.strip().lower().split", "range", "len", "range", "line.strip().lower", "tok_seq.append", "idx_seq.append", "min", "len", "line.strip", "min", "max"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["", "def", "count_word_pairs", "(", "word2idx", ",", "corpus_fn", ",", "context_window", "=", "5", ")", ":", "\n", "    ", "word_pair_cnt", "=", "defaultdict", "(", "int", ")", "\n", "with", "open", "(", "corpus_fn", ",", "\"r\"", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "all_tok_seq", "=", "line", ".", "strip", "(", ")", ".", "lower", "(", ")", ".", "split", "(", ")", "\n", "tok_seq", "=", "[", "]", "\n", "idx_seq", "=", "[", "]", "\n", "for", "tok", "in", "all_tok_seq", ":", "\n", "                ", "if", "tok", "in", "word2idx", ":", "\n", "                    ", "tok_seq", ".", "append", "(", "tok", ")", "\n", "idx_seq", ".", "append", "(", "word2idx", "[", "tok", "]", ")", "\n", "", "", "for", "pos", "in", "range", "(", "len", "(", "idx_seq", ")", ")", ":", "\n", "                ", "idx", "=", "idx_seq", "[", "pos", "]", "\n", "tok", "=", "tok_seq", "[", "pos", "]", "\n", "for", "next_pos", "in", "range", "(", "pos", "+", "1", ",", "min", "(", "pos", "+", "context_window", "+", "1", ",", "len", "(", "idx_seq", ")", ")", ")", ":", "\n", "                    ", "next_idx", "=", "idx_seq", "[", "next_pos", "]", "\n", "next_tok", "=", "tok_seq", "[", "next_pos", "]", "\n", "if", "idx", "==", "next_idx", ":", "\n", "                        ", "continue", "\n", "", "word_pair_cnt", "[", "(", "min", "(", "tok", ",", "next_tok", ")", ",", "max", "(", "tok", ",", "next_tok", ")", ")", "]", "+=", "1", "\n", "", "", "", "", "return", "word_pair_cnt", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.identify_words_graph.remove_edges": [[89, 108], ["len", "range", "print", "range", "min", "max", "graph.remove_edge"], "function", ["None"], ["", "def", "remove_edges", "(", "graph", ",", "vocab", ",", "word_pair_cnt", ")", ":", "\n", "    ", "\"\"\"\n    corpus with tokenized tokens\n    \"\"\"", "\n", "vocab_size", "=", "len", "(", "vocab", ")", "\n", "removed_edge_cnt", "=", "0", "\n", "for", "idx", "in", "range", "(", "vocab_size", ")", ":", "\n", "        ", "tok", "=", "vocab", "[", "idx", "]", "\n", "for", "next_idx", "in", "range", "(", "idx", "+", "1", ",", "vocab_size", ")", ":", "\n", "            ", "next_tok", "=", "vocab", "[", "next_idx", "]", "\n", "word_pair", "=", "(", "min", "(", "tok", ",", "next_tok", ")", ",", "max", "(", "tok", ",", "next_tok", ")", ")", "\n", "if", "word_pair_cnt", "[", "word_pair", "]", "==", "0", ":", "\n", "                ", "try", ":", "\n", "                    ", "graph", ".", "remove_edge", "(", "tok", ",", "next_tok", ")", "\n", "removed_edge_cnt", "+=", "1", "\n", "", "except", ":", "\n", "                    ", "continue", "\n", "", "", "", "", "print", "(", "\"# of nodes: {}, # of removed edges: {}\"", ".", "format", "(", "vocab_size", ",", "removed_edge_cnt", ")", ")", "\n", "return", "graph", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.identify_keywords.identify_words_graph.rank_words": [[110, 118], ["networkx.eigenvector_centrality", "sorted", "print", "nx.eigenvector_centrality.items", "open", "fout.write", "str"], "function", ["None"], ["", "def", "rank_words", "(", "graph", ",", "vocab", ",", "res_fn", ")", ":", "\n", "    ", "centrality", "=", "nx", ".", "eigenvector_centrality", "(", "graph", ")", "\n", "sorted_vocab", "=", "sorted", "(", "centrality", ".", "items", "(", ")", ",", "key", "=", "lambda", "kv", ":", "kv", "[", "1", "]", ",", "\n", "reverse", "=", "True", ")", "\n", "with", "open", "(", "res_fn", ",", "\"w\"", ")", "as", "fout", ":", "\n", "        ", "for", "(", "word", ",", "score", ")", "in", "sorted_vocab", ":", "\n", "            ", "fout", ".", "write", "(", "word", "+", "\"\\t\"", "+", "str", "(", "score", ")", "+", "\"\\n\"", ")", "\n", "", "", "print", "(", "\"saving ranked words to {}\"", ".", "format", "(", "res_fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.word2vec_sim.word2vec_sim.load_emb_from_wiki": [[14, 17], ["gensim.models.KeyedVectors.load_word2vec_format"], "function", ["None"], ["def", "load_emb_from_wiki", "(", "embed_fn", ")", ":", "\n", "    ", "emb_dict", "=", "KeyedVectors", ".", "load_word2vec_format", "(", "embed_fn", ",", "binary", "=", "False", ",", "limit", "=", "20000", ")", "\n", "return", "emb_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.word2vec_sim.word2vec_sim.train_word2vec_embed": [[19, 36], ["gensim.models.word2vec.LineSentence", "print", "time.time", "gensim.models.Word2Vec", "time.time", "print", "gensim.models.Word2Vec.wv.save_word2vec_format", "print"], "function", ["None"], ["", "def", "train_word2vec_embed", "(", "corpus_fn", ",", "embed_fn", ",", "ft", "=", "100", ",", "vec_dim", "=", "50", ",", "window", "=", "8", ")", ":", "\n", "# train embed", "\n", "    ", "sentences", "=", "LineSentence", "(", "corpus_fn", ")", "\n", "sent_cnt", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "sent_cnt", "+=", "1", "\n", "", "print", "(", "\"# of sents: {}\"", ".", "format", "(", "sent_cnt", ")", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "model", "=", "Word2Vec", "(", "sentences", ",", "min_count", "=", "ft", ",", "size", "=", "vec_dim", ",", "\n", "window", "=", "window", ",", "iter", "=", "10", ",", "workers", "=", "30", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\"embed train time: {}s\"", ".", "format", "(", "end", "-", "start", ")", ")", "\n", "\n", "# save embed", "\n", "model", ".", "wv", ".", "save_word2vec_format", "(", "embed_fn", ",", "binary", "=", "False", ")", "\n", "print", "(", "\"save embedding to {}\"", ".", "format", "(", "embed_fn", ")", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.word2vec_sim.word2vec_sim.read_basic_files": [[38, 55], ["collections.defaultdict", "sorted", "sorted", "open", "list", "list", "open", "[].strip().lower", "[].split", "set", "set", "line.strip().split", "euphemism_answer[].append", "[].strip", "line.strip", "line.split", "collections.defaultdict.items", "collections.defaultdict.items", "i.strip", "line.split", "i.strip().lower", "i.strip"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["", "def", "read_basic_files", "(", "dataset", ")", ":", "\n", "    ", "euphemism_answer", "=", "defaultdict", "(", "list", ")", "\n", "with", "open", "(", "'../../data/answer_'", "+", "dataset", "+", "'.txt'", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "ans", "=", "line", ".", "split", "(", "':'", ")", "[", "0", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "for", "i", "in", "line", ".", "split", "(", "':'", ")", "[", "1", "]", ".", "split", "(", "';'", ")", ":", "\n", "                ", "euphemism_answer", "[", "i", ".", "strip", "(", ")", ".", "lower", "(", ")", "]", ".", "append", "(", "ans", ")", "\n", "", "", "", "drug_euphemism", "=", "sorted", "(", "list", "(", "set", "(", "[", "x", "[", "0", "]", "for", "x", "in", "euphemism_answer", ".", "items", "(", ")", "]", ")", ")", ")", "\n", "drug_formal", "=", "sorted", "(", "list", "(", "set", "(", "[", "y", "for", "x", "in", "euphemism_answer", ".", "items", "(", ")", "for", "y", "in", "x", "[", "1", "]", "]", ")", ")", ")", "\n", "drug_name", "=", "{", "}", "\n", "count", "=", "0", "\n", "with", "open", "(", "'../../data/name_'", "+", "dataset", "+", "'.txt'", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "for", "i", "in", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", ":", "\n", "                ", "drug_name", "[", "i", ".", "strip", "(", ")", "]", "=", "count", "\n", "", "count", "+=", "1", "\n", "", "", "return", "euphemism_answer", ",", "drug_formal", ",", "drug_name", "\n", "\n"]], "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.word2vec_sim.word2vec_sim.word2vec_detection_identification": [[57, 109], ["word2vec_sim.train_word2vec_embed", "word2vec_sim.load_emb_from_wiki", "print", "enumerate", "numpy.array", "print", "word2vec_sim.read_basic_files", "identification.get_final_test", "print", "identification.print_final_out", "numpy.sum", "len", "open", "open", "final_answer.append", "np.array.append", "seq.append", "train_word2vec_embed.wv.similar_by_vector", "fout.write", "fout.write", "euphemism_candidates.append", "answer.append", "line.strip", "temp.append", "numpy.argsort().tolist", "numpy.argsort", "sklearn.metrics.pairwise.cosine_similarity"], "function", ["home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.word2vec_sim.word2vec_sim.train_word2vec_embed", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.word2vec_sim.word2vec_sim.load_emb_from_wiki", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.word2vec_sim.word2vec_sim.read_basic_files", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.get_final_test", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.None.identification.print_final_out", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append", "home.repos.pwc.inspect_result.WanzhengZhu_Euphemism.BERT.hdf5_save.HDF5Store.append"], ["", "def", "word2vec_detection_identification", "(", "prefix", ")", ":", "\n", "# train wiki embedding", "\n", "    ", "corpus", "=", "'../../data/text/'", "+", "prefix", "+", "'.txt'", "\n", "embed_file", "=", "'../../'", "+", "prefix", "+", "'_embeddings.txt'", "\n", "# embed_file = '../../results/baselines/' + prefix + '_embeddings.txt'", "\n", "word2vec_model", "=", "train_word2vec_embed", "(", "corpus", ",", "embed_file", ")", "\n", "\n", "# load wiki embedding", "\n", "emb_dict", "=", "load_emb_from_wiki", "(", "embed_file", ")", "\n", "print", "(", "\"Finished loading embedding...\"", ")", "\n", "real_seeds", "=", "[", "\"acetaminophen and oxycodone combination\"", ",", "\"adderall\"", ",", "\"alprazolam\"", ",", "\"amphetamine\"", ",", "\"amphetamine and dextroamphetamine combination\"", ",", "\"buprenorphine and naloxone combination\"", ",", "\"clonazepam\"", ",", "\"cocaine\"", ",", "\"concerta\"", ",", "\"crack cocaine\"", ",", "\"daytrana\"", ",", "\"dilaudid\"", ",", "\"ecstasy\"", ",", "\"fentanyl\"", ",", "\"flunitrazepam\"", ",", "\"gamma-hydroxybutyric acid\"", ",", "\"ghb\"", ",", "\"hash oil\"", ",", "\"heroin\"", ",", "\"hydrocodone\"", ",", "\"hydromorphone\"", ",", "\"ketalar\"", ",", "\"ketamine\"", ",", "\"khat\"", ",", "\"klonopin\"", ",", "\"lorcet\"", ",", "\"lsd\"", ",", "\"lysergic acid diethylamide\"", ",", "\"marijuana\"", ",", "\"marijuana concentrates\"", ",", "\"mdma\"", ",", "\"mescaline\"", ",", "\"methamphetamine\"", ",", "\"methylphenidate\"", ",", "\"molly\"", ",", "\"morphine\"", ",", "\"norco\"", ",", "\"opium\"", ",", "\"oxaydo\"", ",", "\"oxycodone\"", ",", "\"oxycontin\"", ",", "\"pcp\"", ",", "\"percocet\"", ",", "\"peyote\"", ",", "\"phencyclidine\"", ",", "\"promethazine\"", ",", "\"psilocybin mushrooms\"", ",", "\"ritalin\"", ",", "\"rohypnol\"", ",", "\"roxicodone\"", ",", "\"steroids\"", ",", "\"suboxone\"", ",", "\"synthetic cannabinoids\"", ",", "\"synthetic cathinones\"", ",", "\"u-47700\"", ",", "\"vicodin\"", ",", "\"xanax\"", "]", "\n", "# real_seeds = ['pistol', 'gun', 'rifles']", "\n", "# real_seeds = ['genitals', 'penis', 'nipple']", "\n", "\n", "''' Detection '''", "\n", "target_vector", "=", "[", "]", "\n", "seq", "=", "[", "]", "\n", "for", "i", ",", "seed", "in", "enumerate", "(", "real_seeds", ")", ":", "\n", "        ", "if", "seed", "in", "emb_dict", ":", "\n", "            ", "target_vector", ".", "append", "(", "emb_dict", "[", "seed", "]", ")", "\n", "seq", ".", "append", "(", "i", ")", "\n", "", "", "target_vector", "=", "np", ".", "array", "(", "target_vector", ")", "\n", "target_vector_ave", "=", "np", ".", "sum", "(", "target_vector", ",", "0", ")", "/", "len", "(", "target_vector", ")", "\n", "top_words", "=", "[", "x", "[", "0", "]", "for", "x", "in", "word2vec_model", ".", "wv", ".", "similar_by_vector", "(", "target_vector_ave", ",", "topn", "=", "2000", ")", "if", "x", "[", "0", "]", "not", "in", "real_seeds", "]", "\n", "print", "(", "top_words", ")", "\n", "with", "open", "(", "'../../euphemisms_word2vec_'", "+", "prefix", "+", "'.txt'", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "for", "i", "in", "top_words", ":", "\n", "            ", "fout", ".", "write", "(", "i", ")", "\n", "fout", ".", "write", "(", "'\\n'", ")", "\n", "\n", "", "", "''' Identification '''", "\n", "euphemism_candidates", "=", "[", "]", "\n", "with", "open", "(", "'../../results/top_words_reddit.txt'", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "euphemism_candidates", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "", "euphemism_answer", ",", "drug_formal", ",", "drug_name", "=", "read_basic_files", "(", "'drug'", ")", "\n", "final_test", "=", "get_final_test", "(", "euphemism_answer", ",", "euphemism_candidates", ",", "drug_formal", ")", "\n", "answer", "=", "[", "]", "\n", "filtered_final_test", "=", "{", "}", "\n", "for", "i", "in", "euphemism_candidates", ":", "\n", "        ", "if", "(", "i", "in", "emb_dict", ")", "and", "(", "final_test", "[", "i", "]", "!=", "[", "'None'", "]", ")", ":", "\n", "            ", "answer", ".", "append", "(", "[", "drug_name", "[", "real_seeds", "[", "seq", "[", "x", "]", "]", "]", "for", "x", "in", "np", ".", "argsort", "(", "cosine_similarity", "(", "[", "emb_dict", "[", "i", "]", "]", ",", "target_vector", ")", ")", ".", "tolist", "(", ")", "[", "0", "]", "[", ":", ":", "-", "1", "]", "]", ")", "\n", "filtered_final_test", "[", "i", "]", "=", "final_test", "[", "i", "]", "\n", "", "", "final_answer", "=", "[", "]", "\n", "for", "answer_i", "in", "answer", ":", "\n", "        ", "temp", "=", "[", "]", "\n", "for", "j", "in", "answer_i", ":", "\n", "            ", "if", "j", "not", "in", "temp", ":", "\n", "                ", "temp", ".", "append", "(", "j", ")", "\n", "", "", "final_answer", ".", "append", "(", "temp", ")", "\n", "", "print", "(", "final_answer", ")", "\n", "print_final_out", "(", "final_answer", ",", "filtered_final_test", ",", "drug_name", ")", "\n", "\n"]]}