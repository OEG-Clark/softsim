{"home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerTextEmbeddings.__init__": [[58, 66], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_position_embeddings", "=", "config", ".", "max_position_embeddings", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerTextEmbeddings.forward": [[67, 90], ["modeling_skim.SkimformerTextEmbeddings.token_type_embeddings", "modeling_skim.SkimformerTextEmbeddings.LayerNorm", "modeling_skim.SkimformerTextEmbeddings.dropout", "input_ids.size", "torch.zeros", "modeling_skim.SkimformerTextEmbeddings.word_embeddings", "modeling_skim.SkimformerTextEmbeddings.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "inputs_embeds", "=", "None", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "device", "=", "input_ids", ".", "device", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "device", "=", "inputs_embeds", ".", "device", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "\n", "", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "token_type_embeddings", "\n", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.Skimformer1DPositionEmbeddings.__init__": [[94, 101], ["torch.nn.Module.__init__", "torch.nn.Embedding", "getattr", "torch.nn.LayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "\n", "self", ".", "LayerNorm", "=", "LayerNorm", "(", "config", ".", "hidden_layout_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.Skimformer1DPositionEmbeddings.forward": [[102, 115], ["modeling_skim.Skimformer1DPositionEmbeddings.position_embeddings", "modeling_skim.Skimformer1DPositionEmbeddings.LayerNorm", "modeling_skim.Skimformer1DPositionEmbeddings.dropout", "torch.arange", "position_ids.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "position_ids.unsqueeze().expand.unsqueeze().expand.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_shape", ",", "device", ",", "position_ids", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "torch", ".", "arange", "(", "\n", "0", ",", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", "\n", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "input_shape", ")", "\n", "\n", "", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "\n", "position_embeddings", "=", "self", ".", "LayerNorm", "(", "position_embeddings", ")", "\n", "position_embeddings", "=", "self", ".", "dropout", "(", "position_embeddings", ")", "\n", "return", "position_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.Skimformer2DPositionEmbeddings.__init__": [[118, 128], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.Dropout", "hasattr"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "x_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_2d_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "self", ".", "y_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_2d_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "self", ".", "h_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_2d_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "self", ".", "w_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_2d_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "self", ".", "degrade_2d_positions", "=", "config", ".", "degrade_2d_positions", "if", "hasattr", "(", "config", ",", "\"degrade_2d_positions\"", ")", "else", "False", "\n", "\n", "self", ".", "LayerNorm", "=", "LayerNorm", "(", "config", ".", "hidden_layout_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.Skimformer2DPositionEmbeddings.forward": [[129, 163], ["modeling_skim.Skimformer2DPositionEmbeddings.LayerNorm", "modeling_skim.Skimformer2DPositionEmbeddings.dropout", "modeling_skim.Skimformer2DPositionEmbeddings.h_position_embeddings", "modeling_skim.Skimformer2DPositionEmbeddings.w_position_embeddings", "modeling_skim.Skimformer2DPositionEmbeddings.x_position_embeddings", "modeling_skim.Skimformer2DPositionEmbeddings.y_position_embeddings", "modeling_skim.Skimformer2DPositionEmbeddings.x_position_embeddings", "modeling_skim.Skimformer2DPositionEmbeddings.y_position_embeddings", "modeling_skim.Skimformer2DPositionEmbeddings.x_position_embeddings", "modeling_skim.Skimformer2DPositionEmbeddings.y_position_embeddings", "IndexError", "IndexError"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "bbox", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "degrade_2d_positions", ":", "\n", "            ", "try", ":", "\n", "                ", "x_center", "=", "(", "bbox", "[", ":", ",", ":", ",", "0", "]", "+", "bbox", "[", ":", ",", ":", ",", "2", "]", ")", "//", "2", "\n", "y_center", "=", "(", "bbox", "[", ":", ",", ":", ",", "1", "]", "+", "bbox", "[", ":", ",", ":", ",", "3", "]", ")", "//", "2", "\n", "x_center_position_embeddings", "=", "self", ".", "x_position_embeddings", "(", "x_center", ")", "\n", "y_center_position_embeddings", "=", "self", ".", "y_position_embeddings", "(", "y_center", ")", "\n", "", "except", "IndexError", "as", "e", ":", "\n", "                ", "raise", "IndexError", "(", "\"The :obj:`bbox` coordinate values should be within 0-1000 range.\"", ")", "from", "e", "\n", "\n", "", "embeddings", "=", "x_center_position_embeddings", "+", "y_center_position_embeddings", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "left_position_embeddings", "=", "self", ".", "x_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "upper_position_embeddings", "=", "self", ".", "y_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "1", "]", ")", "\n", "right_position_embeddings", "=", "self", ".", "x_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "2", "]", ")", "\n", "lower_position_embeddings", "=", "self", ".", "y_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "3", "]", ")", "\n", "", "except", "IndexError", "as", "e", ":", "\n", "                ", "raise", "IndexError", "(", "\"The :obj:`bbox` coordinate values should be within 0-1000 range.\"", ")", "from", "e", "\n", "\n", "", "h_position_embeddings", "=", "self", ".", "h_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "3", "]", "-", "bbox", "[", ":", ",", ":", ",", "1", "]", ")", "\n", "w_position_embeddings", "=", "self", ".", "w_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "2", "]", "-", "bbox", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "\n", "embeddings", "=", "(", "\n", "left_position_embeddings", "\n", "+", "upper_position_embeddings", "\n", "+", "right_position_embeddings", "\n", "+", "lower_position_embeddings", "\n", "+", "h_position_embeddings", "\n", "+", "w_position_embeddings", "\n", ")", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.BertWithSkimEmbedEmbeddings.__init__": [[167, 187], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.Dropout", "modeling_skim.BertWithSkimEmbedEmbeddings.register_buffer", "torch.arange().expand", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "x_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_2d_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "self", ".", "y_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_2d_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "self", ".", "h_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_2d_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "self", ".", "w_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_2d_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "\n", "self", ".", "x_position_projection", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_layout_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "y_position_projection", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_layout_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "h_position_projection", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_layout_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "w_position_projection", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_layout_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "LayerNorm", "=", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "register_buffer", "(", "\"position_ids\"", ",", "torch", ".", "arange", "(", "config", ".", "max_position_embeddings", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.BertWithSkimEmbedEmbeddings.forward": [[188, 256], ["modeling_skim.BertWithSkimEmbedEmbeddings.position_embeddings", "modeling_skim.BertWithSkimEmbedEmbeddings.h_position_embeddings", "modeling_skim.BertWithSkimEmbedEmbeddings.w_position_embeddings", "modeling_skim.BertWithSkimEmbedEmbeddings.x_position_projection", "modeling_skim.BertWithSkimEmbedEmbeddings.y_position_projection", "modeling_skim.BertWithSkimEmbedEmbeddings.x_position_projection", "modeling_skim.BertWithSkimEmbedEmbeddings.y_position_projection", "modeling_skim.BertWithSkimEmbedEmbeddings.h_position_projection", "modeling_skim.BertWithSkimEmbedEmbeddings.w_position_projection", "modeling_skim.BertWithSkimEmbedEmbeddings.token_type_embeddings", "modeling_skim.BertWithSkimEmbedEmbeddings.LayerNorm", "modeling_skim.BertWithSkimEmbedEmbeddings.dropout", "input_ids.size", "torch.zeros", "modeling_skim.BertWithSkimEmbedEmbeddings.word_embeddings", "modeling_skim.BertWithSkimEmbedEmbeddings.x_position_embeddings", "modeling_skim.BertWithSkimEmbedEmbeddings.y_position_embeddings", "modeling_skim.BertWithSkimEmbedEmbeddings.x_position_embeddings", "modeling_skim.BertWithSkimEmbedEmbeddings.y_position_embeddings", "modeling_skim.BertWithSkimEmbedEmbeddings.size", "IndexError"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "bbox", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "self", ".", "position_ids", "[", ":", ",", ":", "seq_length", "]", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "inputs_embeds", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "try", ":", "\n", "            ", "left_position_embeddings", "=", "self", ".", "x_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "upper_position_embeddings", "=", "self", ".", "y_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "1", "]", ")", "\n", "right_position_embeddings", "=", "self", ".", "x_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "2", "]", ")", "\n", "lower_position_embeddings", "=", "self", ".", "y_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "3", "]", ")", "\n", "", "except", "IndexError", "as", "e", ":", "\n", "            ", "raise", "IndexError", "(", "\"The :obj:`bbox`coordinate values should be within 0-1000 range.\"", ")", "from", "e", "\n", "\n", "", "h_position_embeddings", "=", "self", ".", "h_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "3", "]", "-", "bbox", "[", ":", ",", ":", ",", "1", "]", ")", "\n", "w_position_embeddings", "=", "self", ".", "w_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "2", "]", "-", "bbox", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "\n", "# project into same dimension as text embeddings", "\n", "left_position_embeddings", "=", "self", ".", "x_position_projection", "(", "left_position_embeddings", ")", "\n", "upper_position_embeddings", "=", "self", ".", "y_position_projection", "(", "upper_position_embeddings", ")", "\n", "right_position_embeddings", "=", "self", ".", "x_position_projection", "(", "right_position_embeddings", ")", "\n", "lower_position_embeddings", "=", "self", ".", "y_position_projection", "(", "lower_position_embeddings", ")", "\n", "\n", "h_position_embeddings", "=", "self", ".", "h_position_projection", "(", "h_position_embeddings", ")", "\n", "w_position_embeddings", "=", "self", ".", "w_position_projection", "(", "w_position_embeddings", ")", "\n", "\n", "two_dim_pos_embeddings", "=", "(", "\n", "left_position_embeddings", "\n", "+", "upper_position_embeddings", "\n", "+", "right_position_embeddings", "\n", "+", "lower_position_embeddings", "\n", "+", "h_position_embeddings", "\n", "+", "w_position_embeddings", "\n", ")", "\n", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "(", "\n", "words_embeddings", "\n", "+", "position_embeddings", "\n", "+", "two_dim_pos_embeddings", "\n", "+", "token_type_embeddings", "\n", ")", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimAttention.__init__": [[258, 270], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "getattr"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "config", ".", "skim_attention_head_size", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_layout_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_layout_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "\"absolute\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimAttention.transpose_for_scores": [[271, 275], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimAttention.forward": [[276, 302], ["modeling_skim.SkimAttention.transpose_for_scores", "modeling_skim.SkimAttention.transpose_for_scores", "torch.matmul", "modeling_skim.SkimAttention.dropout", "modeling_skim.SkimAttention.key", "modeling_skim.SkimAttention.query", "modeling_skim.SkimAttention.transpose", "math.sqrt", "torch.nn.Softmax"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_layout_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_layout_states", ")", ")", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "query", "(", "hidden_layout_states", ")", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "            ", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "# return the attention probabilities only: Softmax(QK^T/sqrt(d))", "\n", "return", "attention_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerSelfAttention.__init__": [[304, 312], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "config", ".", "attention_head_size", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerSelfAttention.transpose_for_scores": [[313, 317], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerSelfAttention.forward": [[318, 338], ["modeling_skim.SkimformerSelfAttention.transpose_for_scores", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling_skim.SkimformerSelfAttention.value", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_probs", ",", "\n", "head_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "\n", "# Mask heads if we want to", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "attention_probs", "=", "attention_probs", "*", "head_mask", "\n", "\n", "#\u00a0Softmax(QK^T/sqrt(d)) . V", "\n", "", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerSelfOutput.__init__": [[340, 346], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "all_head_size", "=", "config", ".", "num_attention_heads", "*", "config", ".", "attention_head_size", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "all_head_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerSelfOutput.forward": [[348, 353], ["modeling_skim.SkimformerSelfOutput.dense", "modeling_skim.SkimformerSelfOutput.dropout", "modeling_skim.SkimformerSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerAttention.__init__": [[356, 361], ["torch.nn.Module.__init__", "modeling_skim.SkimformerSelfAttention", "modeling_skim.SkimformerSelfOutput", "set"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "SkimformerSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "SkimformerSelfOutput", "(", "config", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerAttention.prune_heads": [[362, 377], ["transformers.modeling_utils.find_pruneable_heads_and_indices", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "modeling_skim.SkimformerAttention.pruned_heads.union", "len", "len"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "heads", ",", "index", "=", "find_pruneable_heads_and_indices", "(", "\n", "heads", ",", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ",", "self", ".", "pruned_heads", "\n", ")", "\n", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "\n", "# Update hyper params and store pruned heads", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerAttention.forward": [[379, 397], ["modeling_skim.SkimformerAttention.self", "modeling_skim.SkimformerAttention.output", "len", "torch.index_select", "range"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_probs", ",", "\n", "head_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "pruned_heads", ")", ">", "0", ":", "\n", "            ", "num_attention_heads", "=", "attention_probs", ".", "shape", "[", "1", "]", "\n", "indices", "=", "[", "idx", "for", "idx", "in", "range", "(", "num_attention_heads", ")", "if", "idx", "not", "in", "self", ".", "pruned_heads", "]", "\n", "attention_probs", "=", "torch", ".", "index_select", "(", "attention_probs", ",", "1", ",", "indices", ")", "\n", "\n", "", "self_output", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "\n", "attention_probs", ",", "\n", "head_mask", ",", "\n", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "hidden_states", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerLayer.__init__": [[399, 406], ["torch.nn.Module.__init__", "modeling_skim.SkimformerAttention", "transformers.models.bert.modeling_bert.BertIntermediate", "transformers.models.bert.modeling_bert.BertOutput"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "self", ".", "attention", "=", "SkimformerAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerLayer.forward": [[407, 424], ["modeling_skim.SkimformerLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_probs", ",", "\n", "head_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attention_probs", ",", "\n", "head_mask", ",", "\n", ")", "\n", "\n", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "feed_forward_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attention_output", "\n", ")", "\n", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerLayer.feed_forward_chunk": [[425, 429], ["modeling_skim.SkimformerLayer.intermediate", "modeling_skim.SkimformerLayer.output"], "methods", ["None"], ["", "def", "feed_forward_chunk", "(", "self", ",", "attention_output", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerEncoder.__init__": [[431, 435], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "modeling_skim.SkimformerLayer", "range"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "SkimformerLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerEncoder.forward": [[436, 489], ["enumerate", "SkimformerEncoderOutput", "getattr", "tuple", "torch.utils.checkpoint.checkpoint", "layer_module", "modeling_skim.SkimformerEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_probs", ",", "\n", "head_mask", "=", "None", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_head_mask", "=", "head_mask", "[", "i", "]", "if", "head_mask", "is", "not", "None", "else", "None", "\n", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", ":", "\n", "\n", "                ", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_output", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attention_probs", ",", "\n", "layer_head_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_output", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_probs", ",", "\n", "layer_head_mask", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "layer_output", "\n", "\n", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "all_hidden_states", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "SkimformerEncoderOutput", "(", "\n", "hidden_states", "=", "hidden_states", ",", "\n", "all_hidden_states", "=", "all_hidden_states", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerPreTrainedModel._init_weights": [[501, 512], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.BertWithSkimEmbedPreTrainedModel._init_weights": [[524, 535], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimmingMaskPreTrainedModel._init_weights": [[546, 557], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerModel.__init__": [[593, 629], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_skim.SkimformerTextEmbeddings", "modeling_skim.SkimAttention", "modeling_skim.SkimformerEncoder", "modeling_skim.SkimformerModel.init_weights", "modeling_skim.Skimformer1DPositionEmbeddings", "modeling_skim.Skimformer2DPositionEmbeddings", "transformers.models.bert.modeling_bert.BertEncoder", "transformers.models.bert.modeling_bert.BertPooler", "transformers.models.bert.modeling_bert.BertConfig"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "use_1d_positions", "=", "config", ".", "use_1d_positions", "\n", "\n", "self", ".", "text_embeddings", "=", "SkimformerTextEmbeddings", "(", "config", ")", "\n", "\n", "if", "self", ".", "use_1d_positions", ":", "\n", "            ", "self", ".", "one_dim_pos_embeddings", "=", "Skimformer1DPositionEmbeddings", "(", "config", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "two_dim_pos_embeddings", "=", "Skimformer2DPositionEmbeddings", "(", "config", ")", "\n", "\n", "", "self", ".", "contextualize_2d_positions", "=", "config", ".", "contextualize_2d_positions", "\n", "if", "self", ".", "contextualize_2d_positions", ":", "\n", "            ", "self", ".", "layout_encoder", "=", "BertEncoder", "(", "\n", "BertConfig", "(", "\n", "hidden_size", "=", "config", ".", "hidden_layout_size", ",", "\n", "num_hidden_layers", "=", "config", ".", "num_hidden_layers_layout_encoder", ",", "\n", "num_attention_heads", "=", "config", ".", "num_attention_heads_layout_encoder", ",", "\n", "intermediate_size", "=", "config", ".", "intermediate_size", ",", "\n", "hidden_act", "=", "config", ".", "hidden_act", ",", "\n", "hidden_dropout_prob", "=", "config", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "config", ".", "attention_probs_dropout_prob", ",", "\n", "max_position_embeddings", "=", "config", ".", "max_2d_position_embeddings", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "layer_norm_eps", "=", "config", ".", "layer_norm_eps", ",", "\n", "gradient_checkpointing", "=", "config", ".", "gradient_checkpointing", ",", "\n", ")", "\n", ")", "\n", "", "self", ".", "skim_attention", "=", "SkimAttention", "(", "config", ")", "\n", "self", ".", "encoder", "=", "SkimformerEncoder", "(", "config", ")", "\n", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerModel.get_input_embeddings": [[630, 632], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "text_embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerModel.set_input_embeddings": [[633, 635], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "text_embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerModel._prune_heads": [[636, 643], ["heads_to_prune.items", "modeling_skim.SkimformerModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerModel.forward": [[644, 753], ["modeling_skim.SkimformerModel.get_extended_attention_mask", "modeling_skim.SkimformerModel.get_head_mask", "modeling_skim.SkimformerModel.text_embeddings", "modeling_skim.SkimformerModel.skim_attention", "modeling_skim.SkimformerModel.encoder", "modeling_skim.SkimformerModelOutput", "ValueError", "len", "torch.zeros.size", "torch.ones", "torch.zeros", "torch.zeros", "modeling_skim.SkimformerModel.one_dim_pos_embeddings", "modeling_skim.SkimformerModel.two_dim_pos_embeddings", "modeling_skim.SkimformerModel.pooler", "input_ids.size", "len", "tuple", "ValueError", "modeling_skim.SkimformerModel.layout_encoder", "inputs_embeds.size", "list"], "methods", ["None"], ["", "", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "bbox", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "batch_size", ",", "seq_length", "=", "input_shape", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "assert", "(", "\n", "len", "(", "input_shape", ")", "==", "2", "\n", ")", ",", "\"`input_ids` has to be of shape `[batch_size, sequence_length]`, but got shape: {}\"", ".", "format", "(", "input_shape", ")", "\n", "\n", "if", "bbox", "is", "not", "None", ":", "\n", "            ", "bbox_shape", "=", "bbox", ".", "size", "(", ")", "\n", "assert", "(", "\n", "len", "(", "bbox_shape", ")", "==", "3", "\n", ")", ",", "\"`bbox` has to be of shape `[batch_size, sequence_length, 4]`, but got shape: {}\"", ".", "format", "(", "bbox_shape", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "(", "(", "batch_size", ",", "seq_length", ")", ")", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "", "if", "bbox", "is", "None", ":", "\n", "            ", "bbox", "=", "torch", ".", "zeros", "(", "tuple", "(", "list", "(", "input_shape", ")", "+", "[", "4", "]", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "text_embedding_output", "=", "self", ".", "text_embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", ")", "\n", "if", "self", ".", "use_1d_positions", ":", "\n", "            ", "pos_embedding_output", "=", "self", ".", "one_dim_pos_embeddings", "(", "\n", "input_shape", "=", "input_shape", ",", "\n", "device", "=", "device", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "pos_embedding_output", "=", "self", ".", "two_dim_pos_embeddings", "(", "\n", "bbox", "=", "bbox", ",", "\n", ")", "\n", "if", "self", ".", "contextualize_2d_positions", ":", "\n", "                ", "pos_embedding_output", "=", "self", ".", "layout_encoder", "(", "\n", "hidden_states", "=", "pos_embedding_output", ",", "\n", ")", "[", "0", "]", "\n", "\n", "", "", "skim_attention_output", "=", "self", ".", "skim_attention", "(", "\n", "pos_embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", ")", "\n", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "text_embedding_output", ",", "\n", "skim_attention_output", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "outputs", "=", "(", "sequence_output", ",", "pooled_output", ")", "\n", "if", "output_attentions", ":", "\n", "                ", "outputs", "=", "outputs", "+", "(", "skim_attention_output", ",", ")", "\n", "", "if", "output_hidden_states", ":", "\n", "                ", "outputs", "=", "outputs", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "", "return", "outputs", "\n", "\n", "", "return", "SkimformerModelOutput", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "attentions", "=", "skim_attention_output", "if", "output_attentions", "else", "None", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "all_hidden_states", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.BertWithSkimEmbedModel.__init__": [[757, 766], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_skim.BertWithSkimEmbedEmbeddings", "transformers.models.bert.modeling_bert.BertEncoder", "modeling_skim.BertWithSkimEmbedModel.init_weights", "transformers.models.bert.modeling_bert.BertPooler"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "embeddings", "=", "BertWithSkimEmbedEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.BertWithSkimEmbedModel.get_input_embeddings": [[767, 769], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.BertWithSkimEmbedModel.set_input_embeddings": [[770, 772], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.BertWithSkimEmbedModel._prune_heads": [[773, 780], ["heads_to_prune.items", "modeling_skim.BertWithSkimEmbedModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.BertWithSkimEmbedModel.forward": [[781, 861], ["torch.ones.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "modeling_skim.BertWithSkimEmbedModel.embeddings", "modeling_skim.BertWithSkimEmbedModel.encoder", "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions", "ValueError", "torch.ones", "torch.zeros", "torch.zeros", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.to", "modeling_skim.BertWithSkimEmbedModel.pooler", "input_ids.size", "tuple", "torch.ones.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.expand", "ValueError", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "inputs_embeds.size", "list", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "next", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "modeling_skim.BertWithSkimEmbedModel.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "bbox", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "input_shape", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "", "if", "bbox", "is", "None", ":", "\n", "            ", "bbox", "=", "torch", ".", "zeros", "(", "tuple", "(", "list", "(", "input_shape", ")", "+", "[", "4", "]", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "self", ".", "dtype", ")", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "if", "head_mask", ".", "dim", "(", ")", "==", "1", ":", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "head_mask", "=", "head_mask", ".", "expand", "(", "self", ".", "config", ".", "num_hidden_layers", ",", "-", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "", "elif", "head_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "head_mask", "=", "head_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "\n", "", "else", ":", "\n", "            ", "head_mask", "=", "[", "None", "]", "*", "self", ".", "config", ".", "num_hidden_layers", "\n", "\n", "", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "bbox", "=", "bbox", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "sequence_output", ",", "pooled_output", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "BaseModelOutputWithPoolingAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "encoder_outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimmingMaskModel.__init__": [[865, 898], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_skim.Skimformer2DPositionEmbeddings", "modeling_skim.SkimAttention", "transformers.models.bert.modeling_bert.BertEncoder", "modeling_skim.SkimmingMaskModel.init_weights", "transformers.models.bert.modeling_bert.BertEmbeddings", "transformers.models.layoutlm.modeling_layoutlm.LayoutLMEmbeddings", "transformers.models.bert.modeling_bert.BertEncoder", "transformers.models.bert.modeling_bert.BertPooler", "transformers.models.bert.modeling_bert.BertConfig"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "core_model_type", "=", "config", ".", "core_model_type", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "if", "self", ".", "core_model_type", "==", "\"bert\"", "else", "LayoutLMEmbeddings", "(", "config", ")", "\n", "self", ".", "two_dim_pos_embeddings", "=", "Skimformer2DPositionEmbeddings", "(", "config", ")", "\n", "self", ".", "contextualize_2d_positions", "=", "config", ".", "contextualize_2d_positions", "\n", "if", "config", ".", "contextualize_2d_positions", ":", "\n", "            ", "self", ".", "layout_encoder", "=", "BertEncoder", "(", "\n", "BertConfig", "(", "\n", "hidden_size", "=", "config", ".", "hidden_layout_size", ",", "\n", "num_hidden_layers", "=", "config", ".", "num_hidden_layers_layout_encoder", ",", "\n", "num_attention_heads", "=", "config", ".", "num_attention_heads_layout_encoder", ",", "\n", "intermediate_size", "=", "config", ".", "intermediate_size", ",", "\n", "hidden_act", "=", "config", ".", "hidden_act", ",", "\n", "hidden_dropout_prob", "=", "config", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "config", ".", "attention_probs_dropout_prob", ",", "\n", "max_position_embeddings", "=", "config", ".", "max_2d_position_embeddings", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "layer_norm_eps", "=", "config", ".", "layer_norm_eps", ",", "\n", "gradient_checkpointing", "=", "config", ".", "gradient_checkpointing", ",", "\n", ")", "\n", ")", "\n", "\n", "", "self", ".", "skim_attention", "=", "SkimAttention", "(", "config", ")", "\n", "self", ".", "top_k", "=", "config", ".", "top_k", "\n", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimmingMaskModel.get_input_embeddings": [[899, 901], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimmingMaskModel.set_input_embeddings": [[902, 904], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimmingMaskModel._prune_heads": [[905, 912], ["heads_to_prune.items", "modeling_skim.SkimmingMaskModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimmingMaskModel.forward": [[913, 1013], ["modeling_skim.SkimmingMaskModel.get_extended_attention_mask", "modeling_skim.SkimmingMaskModel.get_head_mask", "modeling_skim.SkimmingMaskModel.two_dim_pos_embeddings", "modeling_skim.SkimmingMaskModel.skim_attention", "torch.zeros", "skim_attention_mask.scatter.scatter.scatter", "modeling_skim.SkimmingMaskModel.encoder", "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions", "ValueError", "torch.ones", "torch.zeros", "torch.zeros", "modeling_skim.SkimmingMaskModel.embeddings", "modeling_skim.SkimmingMaskModel.embeddings", "torch.topk", "modeling_skim.SkimmingMaskModel.pooler", "input_ids.size", "tuple", "modeling_skim.SkimmingMaskModel.layout_encoder", "ValueError", "inputs_embeds.size", "list"], "methods", ["None"], ["", "", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "bbox", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "input_shape", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "", "if", "bbox", "is", "None", ":", "\n", "            ", "bbox", "=", "torch", ".", "zeros", "(", "tuple", "(", "list", "(", "input_shape", ")", "+", "[", "4", "]", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "if", "self", ".", "core_model_type", "==", "\"bert\"", ":", "\n", "            ", "text_embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "text_embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "bbox", "=", "bbox", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", ")", "\n", "", "spatial_pos_embedding_output", "=", "self", ".", "two_dim_pos_embeddings", "(", "bbox", "=", "bbox", ")", "\n", "\n", "if", "self", ".", "contextualize_2d_positions", ":", "\n", "            ", "spatial_pos_embedding_output", "=", "self", ".", "layout_encoder", "(", "hidden_states", "=", "spatial_pos_embedding_output", ")", "[", "0", "]", "\n", "\n", "", "skim_attention_output", "=", "self", ".", "skim_attention", "(", "\n", "spatial_pos_embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", ")", "\n", "\n", "topk_idx", "=", "torch", ".", "topk", "(", "skim_attention_output", ",", "self", ".", "top_k", ",", "-", "1", ")", ".", "indices", "\n", "skim_attention_mask", "=", "torch", ".", "zeros", "(", "skim_attention_output", ".", "shape", ",", "device", "=", "device", ")", "\n", "skim_attention_mask", "=", "skim_attention_mask", ".", "scatter", "(", "-", "1", ",", "topk_idx", ",", "1", ")", "\n", "skim_attention_mask", "=", "skim_attention_mask", "*", "attention_mask", "[", ":", ",", "None", ",", ":", ",", ":", "]", "\n", "\n", "skim_attention_mask", "=", "(", "1.0", "-", "skim_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "text_embedding_output", ",", "\n", "skim_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "sequence_output", ",", "pooled_output", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "BaseModelOutputWithPoolingAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "encoder_outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerForMaskedLM.__init__": [[1021, 1028], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_skim.SkimformerModel", "transformers.models.bert.modeling_bert.BertOnlyMLMHead", "modeling_skim.SkimformerForMaskedLM.init_weights"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "skimformer", "=", "SkimformerModel", "(", "config", ",", "add_pooling_layer", "=", "False", ")", "\n", "self", ".", "cls", "=", "BertOnlyMLMHead", "(", "config", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerForMaskedLM.get_output_embeddings": [[1029, 1031], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cls", ".", "predictions", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerForMaskedLM.set_output_embeddings": [[1032, 1034], ["None"], "methods", ["None"], ["", "def", "set_output_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "cls", ".", "predictions", ".", "decoder", "=", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerForMaskedLM.forward": [[1035, 1088], ["modeling_skim.SkimformerForMaskedLM.skimformer", "modeling_skim.SkimformerForMaskedLM.cls", "transformers.modeling_outputs.MaskedLMOutput", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "modeling_skim.SkimformerForMaskedLM.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "bbox", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        \"\"\"", "\n", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "skimformer", "(", "\n", "input_ids", ",", "\n", "bbox", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "prediction_scores", "=", "self", ".", "cls", "(", "sequence_output", ")", "\n", "\n", "masked_lm_loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "# -100 index = padding token", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "prediction_scores", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "masked_lm_loss", ",", ")", "+", "output", ")", "if", "masked_lm_loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "MaskedLMOutput", "(", "\n", "loss", "=", "masked_lm_loss", ",", "\n", "logits", "=", "prediction_scores", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerForTokenClassification.__init__": [[1094, 1103], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_skim.SkimformerModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling_skim.SkimformerForTokenClassification.init_weights"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "skimformer", "=", "SkimformerModel", "(", "config", ",", "add_pooling_layer", "=", "False", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimformerForTokenClassification.forward": [[1104, 1166], ["modeling_skim.SkimformerForTokenClassification.skimformer", "modeling_skim.SkimformerForTokenClassification.dropout", "modeling_skim.SkimformerForTokenClassification.classifier", "transformers.modeling_outputs.TokenClassifierOutput", "torch.nn.CrossEntropyLoss", "modeling_skim.SkimformerForTokenClassification.view", "torch.where", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "labels.view", "torch.tensor().type_as", "modeling_skim.SkimformerForTokenClassification.view", "labels.view", "torch.tensor"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "bbox", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n            1]``.\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "skimformer", "(", "\n", "input_ids", ",", "\n", "bbox", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "\n", "active_labels", "=", "torch", ".", "where", "(", "\n", "active_loss", ",", "labels", ".", "view", "(", "-", "1", ")", ",", "torch", ".", "tensor", "(", "loss_fct", ".", "ignore_index", ")", ".", "type_as", "(", "labels", ")", "\n", ")", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "TokenClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.BertWithSkimEmbedForTokenClassification.__init__": [[1172, 1181], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_skim.BertWithSkimEmbedModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling_skim.BertWithSkimEmbedForTokenClassification.init_weights"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "bert_with_skim_embed", "=", "BertWithSkimEmbedModel", "(", "config", ",", "add_pooling_layer", "=", "False", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.BertWithSkimEmbedForTokenClassification.get_input_embeddings": [[1182, 1184], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "bert_with_skim_embed", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.BertWithSkimEmbedForTokenClassification.set_input_embeddings": [[1185, 1187], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "bert_with_skim_embed", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.BertWithSkimEmbedForTokenClassification.forward": [[1188, 1243], ["modeling_skim.BertWithSkimEmbedForTokenClassification.bert_with_skim_embed", "modeling_skim.BertWithSkimEmbedForTokenClassification.dropout", "modeling_skim.BertWithSkimEmbedForTokenClassification.classifier", "transformers.modeling_outputs.TokenClassifierOutput", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "modeling_skim.BertWithSkimEmbedForTokenClassification.view", "labels.view", "modeling_skim.BertWithSkimEmbedForTokenClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "bbox", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "bert_with_skim_embed", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "bbox", "=", "bbox", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "TokenClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimmingMaskForTokenClassification.__init__": [[1248, 1257], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_skim.SkimmingMaskModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling_skim.SkimmingMaskForTokenClassification.init_weights"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "skimming_mask_model", "=", "SkimmingMaskModel", "(", "config", ",", "add_pooling_layer", "=", "False", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimmingMaskForTokenClassification.get_input_embeddings": [[1258, 1260], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "skimming_mask_model", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimmingMaskForTokenClassification.set_input_embeddings": [[1261, 1263], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "skimming_mask_model", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_skim.SkimmingMaskForTokenClassification.forward": [[1264, 1322], ["modeling_skim.SkimmingMaskForTokenClassification.skimming_mask_model", "modeling_skim.SkimmingMaskForTokenClassification.dropout", "modeling_skim.SkimmingMaskForTokenClassification.classifier", "transformers.modeling_outputs.TokenClassifierOutput", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.dim", "modeling_skim.SkimmingMaskForTokenClassification.view", "labels.view", "modeling_skim.SkimmingMaskForTokenClassification.view", "labels.view", "torch.sum().view", "attention_mask.view", "torch.sum"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "bbox", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "skimming_mask_model", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "bbox", "=", "bbox", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "if", "attention_mask", ".", "dim", "(", ")", "==", "3", ":", "\n", "                    ", "active_loss", "=", "(", "torch", ".", "sum", "(", "attention_mask", ",", "dim", "=", "-", "1", ")", ")", ".", "view", "(", "-", "1", ")", ">", "0", "\n", "", "else", ":", "\n", "                    ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "TokenClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", ""]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.configuration_longskim.LongSkimformerConfig.__init__": [[42, 69], ["configuration_skim.SkimformerConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "max_position_embeddings", ":", "int", "=", "4098", ",", "\n", "attention_window", ":", "Union", "[", "List", "[", "int", "]", ",", "int", "]", "=", "512", ",", "\n", "pad_token_id", ":", "int", "=", "1", ",", "\n", "sep_token_id", ":", "int", "=", "2", ",", "\n", "type_vocab_size", ":", "int", "=", "1", ",", "\n", "bos_token_id", ":", "int", "=", "0", ",", "\n", "eos_token_id", ":", "int", "=", "2", ",", "\n", "ignore_attention_mask", ":", "bool", "=", "False", ",", "\n", "layer_norm_eps", ":", "float", "=", "1e-05", ",", "\n", "pad_token_bbox_value", ":", "int", "=", "0", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "max_position_embeddings", "=", "max_position_embeddings", ",", "\n", "pad_token_id", "=", "pad_token_id", ",", "\n", "sep_token_id", "=", "sep_token_id", ",", "\n", "type_vocab_size", "=", "type_vocab_size", ",", "\n", "layer_norm_eps", "=", "layer_norm_eps", ",", "\n", "**", "kwargs", "\n", ")", "\n", "self", ".", "attention_window", "=", "attention_window", "\n", "self", ".", "bos_token_id", "=", "bos_token_id", "\n", "self", ".", "eos_token_id", "=", "eos_token_id", "\n", "self", ".", "ignore_attention_mask", "=", "ignore_attention_mask", "\n", "self", ".", "pad_token_bbox_value", "=", "pad_token_bbox_value", "\n", "", "", ""]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.configuration_skim.SkimformerConfig.__init__": [[10, 61], ["transformers.configuration_utils.PretrainedConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_size", "=", "30522", ",", "\n", "hidden_size", "=", "768", ",", "\n", "hidden_layout_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "attention_head_size", "=", "64", ",", "\n", "skim_attention_head_size", "=", "64", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "max_2d_position_embeddings", "=", "1024", ",", "\n", "use_1d_positions", "=", "False", ",", "\n", "degrade_2d_positions", "=", "False", ",", "\n", "contextualize_2d_positions", "=", "False", ",", "\n", "num_hidden_layers_layout_encoder", "=", "2", ",", "\n", "num_attention_heads_layout_encoder", "=", "12", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "layer_norm_eps", "=", "1e-12", ",", "\n", "pad_token_id", "=", "0", ",", "\n", "gradient_checkpointing", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "pad_token_id", "=", "pad_token_id", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "hidden_layout_size", "=", "hidden_layout_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "attention_head_size", "\n", "self", ".", "skim_attention_head_size", "=", "skim_attention_head_size", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "max_2d_position_embeddings", "=", "max_2d_position_embeddings", "\n", "self", ".", "use_1d_positions", "=", "use_1d_positions", "\n", "self", ".", "degrade_2d_positions", "=", "degrade_2d_positions", "\n", "self", ".", "contextualize_2d_positions", "=", "contextualize_2d_positions", "\n", "self", ".", "num_hidden_layers_layout_encoder", "=", "num_hidden_layers_layout_encoder", "\n", "self", ".", "num_attention_heads_layout_encoder", "=", "num_attention_heads_layout_encoder", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "self", ".", "layer_norm_eps", "=", "layer_norm_eps", "\n", "self", ".", "gradient_checkpointing", "=", "gradient_checkpointing", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.configuration_skim.BertWithSkimEmbedConfig.__init__": [[66, 109], ["transformers.BertConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_size", "=", "30522", ",", "\n", "hidden_size", "=", "768", ",", "\n", "hidden_layout_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "layer_norm_eps", "=", "1e-12", ",", "\n", "pad_token_id", "=", "0", ",", "\n", "gradient_checkpointing", "=", "False", ",", "\n", "max_2d_position_embeddings", "=", "1024", ",", "\n", "num_hidden_layers_layout_encoder", "=", "2", ",", "\n", "num_attention_heads_layout_encoder", "=", "12", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "vocab_size", "=", "vocab_size", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", "num_hidden_layers", "=", "num_hidden_layers", ",", "\n", "num_attention_heads", "=", "num_attention_heads", ",", "\n", "intermediate_size", "=", "intermediate_size", ",", "\n", "hidden_act", "=", "hidden_act", ",", "\n", "hidden_dropout_prob", "=", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", ",", "\n", "max_position_embeddings", "=", "max_position_embeddings", ",", "\n", "type_vocab_size", "=", "type_vocab_size", ",", "\n", "initializer_range", "=", "initializer_range", ",", "\n", "layer_norm_eps", "=", "layer_norm_eps", ",", "\n", "pad_token_id", "=", "pad_token_id", ",", "\n", "gradient_checkpointing", "=", "gradient_checkpointing", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "self", ".", "max_2d_position_embeddings", "=", "max_2d_position_embeddings", "\n", "self", ".", "hidden_layout_size", "=", "hidden_layout_size", "\n", "self", ".", "num_hidden_layers_layout_encoder", "=", "num_hidden_layers_layout_encoder", "\n", "self", ".", "num_attention_heads_layout_encoder", "=", "num_attention_heads_layout_encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.configuration_skim.SkimmingMaskConfig.__init__": [[113, 167], ["transformers.BertConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_size", "=", "30522", ",", "\n", "hidden_size", "=", "768", ",", "\n", "hidden_layout_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "num_skim_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "layer_norm_eps", "=", "1e-12", ",", "\n", "skim_attention_head_size", "=", "64", ",", "\n", "pad_token_id", "=", "0", ",", "\n", "gradient_checkpointing", "=", "False", ",", "\n", "max_2d_position_embeddings", "=", "1024", ",", "\n", "contextualize_2d_positions", "=", "False", ",", "\n", "num_hidden_layers_layout_encoder", "=", "2", ",", "\n", "num_attention_heads_layout_encoder", "=", "12", ",", "\n", "top_k", "=", "0", ",", "\n", "core_model_type", "=", "\"bert\"", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "vocab_size", "=", "vocab_size", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", "num_hidden_layers", "=", "num_hidden_layers", ",", "\n", "num_attention_heads", "=", "num_attention_heads", ",", "\n", "intermediate_size", "=", "intermediate_size", ",", "\n", "hidden_act", "=", "hidden_act", ",", "\n", "hidden_dropout_prob", "=", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", ",", "\n", "max_position_embeddings", "=", "max_position_embeddings", ",", "\n", "type_vocab_size", "=", "type_vocab_size", ",", "\n", "initializer_range", "=", "initializer_range", ",", "\n", "layer_norm_eps", "=", "layer_norm_eps", ",", "\n", "pad_token_id", "=", "pad_token_id", ",", "\n", "gradient_checkpointing", "=", "gradient_checkpointing", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "self", ".", "hidden_layout_size", "=", "hidden_layout_size", "\n", "self", ".", "num_skim_attention_heads", "=", "num_skim_attention_heads", "\n", "self", ".", "skim_attention_head_size", "=", "skim_attention_head_size", "\n", "self", ".", "max_2d_position_embeddings", "=", "max_2d_position_embeddings", "\n", "self", ".", "contextualize_2d_positions", "=", "contextualize_2d_positions", "\n", "self", ".", "num_hidden_layers_layout_encoder", "=", "num_hidden_layers_layout_encoder", "\n", "self", ".", "num_attention_heads_layout_encoder", "=", "num_attention_heads_layout_encoder", "\n", "\n", "self", ".", "top_k", "=", "top_k", "if", "top_k", "else", "0", "\n", "self", ".", "core_model_type", "=", "core_model_type", "", "", "", ""]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerTextEmbeddings.__init__": [[219, 230], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "padding_idx", "=", "config", ".", "pad_token_id", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerTextEmbeddings.forward": [[231, 250], ["modeling_longskim.LongSkimformerTextEmbeddings.token_type_embeddings", "modeling_longskim.LongSkimformerTextEmbeddings.LayerNorm", "modeling_longskim.LongSkimformerTextEmbeddings.dropout", "input_ids.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "modeling_longskim.LongSkimformerTextEmbeddings.word_embeddings", "modeling_longskim.LongSkimformerTextEmbeddings.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "inputs_embeds", "=", "None", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "device", "=", "input_ids", ".", "device", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "device", "=", "inputs_embeds", ".", "device", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformer2DPositionEmbeddings.__init__": [[254, 263], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "x_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_2d_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "self", ".", "y_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_2d_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "self", ".", "h_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_2d_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "self", ".", "w_position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_2d_position_embeddings", ",", "config", ".", "hidden_layout_size", ")", "\n", "\n", "self", ".", "LayerNorm", "=", "LayerNorm", "(", "config", ".", "hidden_layout_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformer2DPositionEmbeddings.forward": [[264, 287], ["modeling_longskim.LongSkimformer2DPositionEmbeddings.h_position_embeddings", "modeling_longskim.LongSkimformer2DPositionEmbeddings.w_position_embeddings", "modeling_longskim.LongSkimformer2DPositionEmbeddings.LayerNorm", "modeling_longskim.LongSkimformer2DPositionEmbeddings.dropout", "modeling_longskim.LongSkimformer2DPositionEmbeddings.x_position_embeddings", "modeling_longskim.LongSkimformer2DPositionEmbeddings.y_position_embeddings", "modeling_longskim.LongSkimformer2DPositionEmbeddings.x_position_embeddings", "modeling_longskim.LongSkimformer2DPositionEmbeddings.y_position_embeddings", "IndexError"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "bbox", "=", "None", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "left_position_embeddings", "=", "self", ".", "x_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "upper_position_embeddings", "=", "self", ".", "y_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "1", "]", ")", "\n", "right_position_embeddings", "=", "self", ".", "x_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "2", "]", ")", "\n", "lower_position_embeddings", "=", "self", ".", "y_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "3", "]", ")", "\n", "", "except", "IndexError", "as", "e", ":", "\n", "            ", "raise", "IndexError", "(", "\"The :obj:`bbox` coordinate values should be within 0-1000 range.\"", ")", "from", "e", "\n", "\n", "", "h_position_embeddings", "=", "self", ".", "h_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "3", "]", "-", "bbox", "[", ":", ",", ":", ",", "1", "]", ")", "\n", "w_position_embeddings", "=", "self", ".", "w_position_embeddings", "(", "bbox", "[", ":", ",", ":", ",", "2", "]", "-", "bbox", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "\n", "embeddings", "=", "(", "\n", "left_position_embeddings", "\n", "+", "upper_position_embeddings", "\n", "+", "right_position_embeddings", "\n", "+", "lower_position_embeddings", "\n", "+", "h_position_embeddings", "\n", "+", "w_position_embeddings", "\n", ")", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimAttention.__init__": [[290, 315], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "head_dim", "=", "config", ".", "skim_attention_head_size", "# int(config.hidden_size / config.num_attention_heads)", "\n", "self", ".", "embed_dim", "=", "self", ".", "num_heads", "*", "self", ".", "head_dim", "# hidden_layout_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_layout_size", ",", "self", ".", "embed_dim", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_layout_size", ",", "self", ".", "embed_dim", ")", "\n", "\n", "# separate projection layers for tokens with global attention", "\n", "self", ".", "query_global", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_layout_size", ",", "self", ".", "embed_dim", ")", "\n", "self", ".", "key_global", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_layout_size", ",", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "dropout", "=", "config", ".", "attention_probs_dropout_prob", "\n", "\n", "attention_window", "=", "config", ".", "attention_window", "\n", "assert", "(", "\n", "attention_window", "%", "2", "==", "0", "\n", ")", ",", "f\"`attention_window` has to be an even value. Given {attention_window}\"", "\n", "assert", "(", "\n", "attention_window", ">", "0", "\n", ")", ",", "f\"`attention_window` has to be positive. Given {attention_window}\"", "\n", "\n", "self", ".", "one_sided_attn_window_size", "=", "attention_window", "//", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimAttention.forward": [[316, 430], ["hidden_layout_states.transpose.transpose.transpose", "modeling_longskim.LongSkimAttention.query", "modeling_longskim.LongSkimAttention.key", "hidden_layout_states.transpose.transpose.size", "math.sqrt", "query_vectors.view().transpose.view().transpose.view().transpose", "key_vectors.view().transpose.view().transpose.view().transpose", "modeling_longskim.LongSkimAttention._sliding_chunks_query_key_matmul", "remove_from_windowed_attention_mask.type_as().masked_fill", "modeling_longskim.LongSkimAttention._sliding_chunks_query_key_matmul", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.masked_fill", "torch.masked_fill", "torch.masked_fill", "torch.masked_fill", "torch.masked_fill", "torch.masked_fill", "torch.masked_fill", "torch.masked_fill", "torch.masked_fill", "torch.nn.functional.dropout.type_as", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "remove_from_windowed_attention_mask.type_as().masked_fill.new_ones", "list", "modeling_longskim.LongSkimAttention._get_global_attn_indices", "modeling_longskim.LongSkimAttention._concat_with_global_key_attn_probs", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling_longskim.LongSkimAttention._compute_global_attn_probs_from_hidden", "query_vectors.view().transpose.view().transpose.view", "key_vectors.view().transpose.view().transpose.view", "remove_from_windowed_attention_mask.type_as", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "remove_from_windowed_attention_mask.type_as().masked_fill.size"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._sliding_chunks_query_key_matmul", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._sliding_chunks_query_key_matmul", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._get_global_attn_indices", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._concat_with_global_key_attn_probs", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimAttention._compute_global_attn_probs_from_hidden"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_layout_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "is_index_masked", "=", "None", ",", "\n", "is_index_global_attn", "=", "None", ",", "\n", "is_global_attn", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        :class:`LongformerSelfAttention` expects `len(hidden_states)` to be multiple of `attention_window`. Padding to\n        `attention_window` happens in :meth:`LongformerModel.forward` to avoid redoing the padding on each layer.\n\n        The `attention_mask` is changed in :meth:`LongformerModel.forward` from 0, 1, 2 to:\n\n            * -10000: no attention\n            * 0: local attention\n            * +10000: global attention\n        \"\"\"", "\n", "hidden_layout_states", "=", "hidden_layout_states", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# project hidden states", "\n", "query_vectors", "=", "self", ".", "query", "(", "hidden_layout_states", ")", "\n", "key_vectors", "=", "self", ".", "key", "(", "hidden_layout_states", ")", "\n", "\n", "seq_len", ",", "batch_size", ",", "embed_dim", "=", "hidden_layout_states", ".", "size", "(", ")", "\n", "assert", "(", "\n", "embed_dim", "==", "self", ".", "embed_dim", "\n", ")", ",", "f\"hidden_states should have embed_dim = {self.embed_dim}, but has {embed_dim}\"", "\n", "\n", "# normalize query", "\n", "query_vectors", "/=", "math", ".", "sqrt", "(", "self", ".", "head_dim", ")", "\n", "\n", "query_vectors", "=", "query_vectors", ".", "view", "(", "seq_len", ",", "batch_size", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "key_vectors", "=", "key_vectors", ".", "view", "(", "seq_len", ",", "batch_size", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "attn_scores", "=", "self", ".", "_sliding_chunks_query_key_matmul", "(", "\n", "query_vectors", ",", "key_vectors", ",", "self", ".", "one_sided_attn_window_size", "\n", ")", "\n", "\n", "# values to pad for attention probs", "\n", "remove_from_windowed_attention_mask", "=", "(", "attention_mask", "!=", "0", ")", "[", ":", ",", ":", ",", "None", ",", "None", "]", "\n", "\n", "# cast to fp32/fp16 then replace 1's with -inf", "\n", "float_mask", "=", "remove_from_windowed_attention_mask", ".", "type_as", "(", "query_vectors", ")", ".", "masked_fill", "(", "\n", "remove_from_windowed_attention_mask", ",", "-", "10000.0", "\n", ")", "\n", "# diagonal mask with zeros everywhere and -inf inplace of padding", "\n", "diagonal_mask", "=", "self", ".", "_sliding_chunks_query_key_matmul", "(", "\n", "float_mask", ".", "new_ones", "(", "size", "=", "float_mask", ".", "size", "(", ")", ")", ",", "float_mask", ",", "self", ".", "one_sided_attn_window_size", "\n", ")", "\n", "\n", "# pad local attention probs", "\n", "attn_scores", "+=", "diagonal_mask", "\n", "\n", "assert", "list", "(", "attn_scores", ".", "size", "(", ")", ")", "==", "[", "\n", "batch_size", ",", "\n", "seq_len", ",", "\n", "self", ".", "num_heads", ",", "\n", "self", ".", "one_sided_attn_window_size", "*", "2", "+", "1", ",", "\n", "]", ",", "f\"local_attn_probs should be of size ({batch_size}, {seq_len}, {self.num_heads}, {self.one_sided_attn_window_size * 2 + 1}), but is of size {attn_scores.size()}\"", "\n", "\n", "# compute local attention probs from global attention keys and contact over window dim", "\n", "if", "is_global_attn", ":", "\n", "# compute global attn indices required through out forward fn", "\n", "            ", "(", "\n", "max_num_global_attn_indices", ",", "\n", "is_index_global_attn_nonzero", ",", "\n", "is_local_index_global_attn_nonzero", ",", "\n", "is_local_index_no_global_attn_nonzero", ",", "\n", ")", "=", "self", ".", "_get_global_attn_indices", "(", "is_index_global_attn", ")", "\n", "# calculate global attn probs from global key", "\n", "\n", "global_key_attn_scores", "=", "self", ".", "_concat_with_global_key_attn_probs", "(", "\n", "query_vectors", "=", "query_vectors", ",", "\n", "key_vectors", "=", "key_vectors", ",", "\n", "max_num_global_attn_indices", "=", "max_num_global_attn_indices", ",", "\n", "is_index_global_attn_nonzero", "=", "is_index_global_attn_nonzero", ",", "\n", "is_local_index_global_attn_nonzero", "=", "is_local_index_global_attn_nonzero", ",", "\n", "is_local_index_no_global_attn_nonzero", "=", "is_local_index_no_global_attn_nonzero", ",", "\n", ")", "\n", "# concat to local_attn_probs", "\n", "# (batch_size, seq_len, num_heads, extra attention count + 2*window+1)", "\n", "attn_scores", "=", "torch", ".", "cat", "(", "(", "global_key_attn_scores", ",", "attn_scores", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# free memory", "\n", "del", "global_key_attn_scores", "\n", "\n", "", "attn_probs", "=", "F", ".", "softmax", "(", "attn_scores", ",", "dim", "=", "-", "1", ",", "dtype", "=", "torch", ".", "float32", ")", "# use fp32 for numerical stability", "\n", "\n", "# softmax sometimes inserts NaN if all positions are masked, replace them with 0", "\n", "attn_probs", "=", "torch", ".", "masked_fill", "(", "attn_probs", ",", "is_index_masked", "[", ":", ",", ":", ",", "None", ",", "None", "]", ",", "0.0", ")", "\n", "attn_probs", "=", "attn_probs", ".", "type_as", "(", "attn_scores", ")", "\n", "\n", "# free memory", "\n", "del", "attn_scores", "\n", "\n", "# apply dropout", "\n", "attn_probs", "=", "F", ".", "dropout", "(", "attn_probs", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "if", "is_global_attn", ":", "\n", "            ", "global_attn_probs", "=", "self", ".", "_compute_global_attn_probs_from_hidden", "(", "\n", "hidden_layout_states", "=", "hidden_layout_states", ",", "\n", "max_num_global_attn_indices", "=", "max_num_global_attn_indices", ",", "\n", "is_local_index_global_attn_nonzero", "=", "is_local_index_global_attn_nonzero", ",", "\n", "is_index_global_attn_nonzero", "=", "is_index_global_attn_nonzero", ",", "\n", "is_local_index_no_global_attn_nonzero", "=", "is_local_index_no_global_attn_nonzero", ",", "\n", "is_index_masked", "=", "is_index_masked", ",", "\n", ")", "\n", "# The attention weights for tokens with global attention are", "\n", "# just filler values, they are never used to compute the output.", "\n", "# Fill with 0 now, the correct values are in 'global_attn_probs'.", "\n", "attn_probs", "[", "is_index_global_attn_nonzero", "]", "=", "0", "\n", "\n", "", "return", "(", "attn_probs", ",", "global_attn_probs", ")", "if", "is_global_attn", "else", "attn_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimAttention._pad_and_transpose_last_two_dims": [[431, 441], ["torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "hidden_states_padded.view.view.view", "hidden_states_padded.view.view.size", "hidden_states_padded.view.view.size", "hidden_states_padded.view.view.size"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_pad_and_transpose_last_two_dims", "(", "hidden_states_padded", ",", "padding", ")", ":", "\n", "        ", "\"\"\"pads rows and then flips rows and columns\"\"\"", "\n", "hidden_states_padded", "=", "F", ".", "pad", "(", "\n", "hidden_states_padded", ",", "padding", "\n", ")", "# padding value is not important because it will be overwritten", "\n", "hidden_states_padded", "=", "hidden_states_padded", ".", "view", "(", "\n", "*", "hidden_states_padded", ".", "size", "(", ")", "[", ":", "-", "2", "]", ",", "hidden_states_padded", ".", "size", "(", "-", "1", ")", ",", "hidden_states_padded", ".", "size", "(", "-", "2", ")", "\n", ")", "\n", "return", "hidden_states_padded", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimAttention._pad_and_diagonalize": [[442, 475], ["chunked_hidden_states.view.view.size", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "chunked_hidden_states.view.view.view", "chunked_hidden_states.view.view.view"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_pad_and_diagonalize", "(", "chunked_hidden_states", ")", ":", "\n", "        ", "\"\"\"\n        shift every row 1 step right, converting columns into diagonals.\n\n        Example::\n\n              chunked_hidden_states: [ 0.4983,  2.6918, -0.0071,  1.0492,\n                                       -1.8348,  0.7672,  0.2986,  0.0285,\n                                       -0.7584,  0.4206, -0.0405,  0.1599,\n                                       2.0514, -1.1600,  0.5372,  0.2629 ]\n              window_overlap = num_rows = 4\n             (pad & diagonalize) =>\n             [ 0.4983,  2.6918, -0.0071,  1.0492, 0.0000,  0.0000,  0.0000\n               0.0000,  -1.8348,  0.7672,  0.2986,  0.0285, 0.0000,  0.0000\n               0.0000,  0.0000, -0.7584,  0.4206, -0.0405,  0.1599, 0.0000\n               0.0000,  0.0000,  0.0000, 2.0514, -1.1600,  0.5372,  0.2629 ]\n        \"\"\"", "\n", "total_num_heads", ",", "num_chunks", ",", "window_overlap", ",", "hidden_dim", "=", "chunked_hidden_states", ".", "size", "(", ")", "\n", "chunked_hidden_states", "=", "F", ".", "pad", "(", "\n", "chunked_hidden_states", ",", "(", "0", ",", "window_overlap", "+", "1", ")", "\n", ")", "# total_num_heads x num_chunks x window_overlap x (hidden_dim+window_overlap+1). Padding value is not important because it'll be overwritten", "\n", "chunked_hidden_states", "=", "chunked_hidden_states", ".", "view", "(", "\n", "total_num_heads", ",", "num_chunks", ",", "-", "1", "\n", ")", "# total_num_heads x num_chunks x window_overlap*window_overlap+window_overlap", "\n", "chunked_hidden_states", "=", "chunked_hidden_states", "[", "\n", ":", ",", ":", ",", ":", "-", "window_overlap", "\n", "]", "# total_num_heads x num_chunks x window_overlap*window_overlap", "\n", "chunked_hidden_states", "=", "chunked_hidden_states", ".", "view", "(", "\n", "total_num_heads", ",", "num_chunks", ",", "window_overlap", ",", "window_overlap", "+", "hidden_dim", "\n", ")", "\n", "chunked_hidden_states", "=", "chunked_hidden_states", "[", ":", ",", ":", ",", ":", ",", ":", "-", "1", "]", "\n", "return", "chunked_hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimAttention._chunk": [[476, 495], ["hidden_states.view.view.view", "list", "list", "hidden_states.view.view.as_strided", "hidden_states.view.view.size", "hidden_states.view.view.size", "hidden_states.view.view.size", "hidden_states.view.view.stride", "hidden_states.view.view.size"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_chunk", "(", "hidden_states", ",", "window_overlap", ")", ":", "\n", "        ", "\"\"\"convert into overlapping chunks. Chunk size = 2w, overlap size = w\"\"\"", "\n", "\n", "# non-overlapping chunks of size = 2w", "\n", "hidden_states", "=", "hidden_states", ".", "view", "(", "\n", "hidden_states", ".", "size", "(", "0", ")", ",", "\n", "hidden_states", ".", "size", "(", "1", ")", "//", "(", "window_overlap", "*", "2", ")", ",", "\n", "window_overlap", "*", "2", ",", "\n", "hidden_states", ".", "size", "(", "2", ")", ",", "\n", ")", "\n", "\n", "# use `as_strided` to make the chunks overlap with an overlap size = window_overlap", "\n", "chunk_size", "=", "list", "(", "hidden_states", ".", "size", "(", ")", ")", "\n", "chunk_size", "[", "1", "]", "=", "chunk_size", "[", "1", "]", "*", "2", "-", "1", "\n", "\n", "chunk_stride", "=", "list", "(", "hidden_states", ".", "stride", "(", ")", ")", "\n", "chunk_stride", "[", "1", "]", "=", "chunk_stride", "[", "1", "]", "//", "2", "\n", "return", "hidden_states", ".", "as_strided", "(", "size", "=", "chunk_size", ",", "stride", "=", "chunk_stride", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimAttention._mask_invalid_locations": [[496, 507], ["input_tensor.new_ones().tril().flip", "beginning_mask.expand.expand.flip", "beginning_mask.expand.expand.expand", "beginning_input.masked_fill_", "ending_mask.expand.expand.expand", "ending_input.masked_fill_", "beginning_input.size", "ending_input.size", "input_tensor.new_ones().tril", "float", "float", "input_tensor.new_ones"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_mask_invalid_locations", "(", "input_tensor", ",", "affected_seq_len", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "beginning_mask_2d", "=", "input_tensor", ".", "new_ones", "(", "affected_seq_len", ",", "affected_seq_len", "+", "1", ")", ".", "tril", "(", ")", ".", "flip", "(", "dims", "=", "[", "0", "]", ")", "\n", "beginning_mask", "=", "beginning_mask_2d", "[", "None", ",", ":", ",", "None", ",", ":", "]", "\n", "ending_mask", "=", "beginning_mask", ".", "flip", "(", "dims", "=", "(", "1", ",", "3", ")", ")", "\n", "beginning_input", "=", "input_tensor", "[", ":", ",", ":", "affected_seq_len", ",", ":", ",", ":", "affected_seq_len", "+", "1", "]", "\n", "beginning_mask", "=", "beginning_mask", ".", "expand", "(", "beginning_input", ".", "size", "(", ")", ")", "\n", "beginning_input", ".", "masked_fill_", "(", "beginning_mask", "==", "1", ",", "-", "float", "(", "\"inf\"", ")", ")", "# `== 1` converts to bool or uint8", "\n", "ending_input", "=", "input_tensor", "[", ":", ",", "-", "affected_seq_len", ":", ",", ":", ",", "-", "(", "affected_seq_len", "+", "1", ")", ":", "]", "\n", "ending_mask", "=", "ending_mask", ".", "expand", "(", "ending_input", ".", "size", "(", ")", ")", "\n", "ending_input", ".", "masked_fill_", "(", "ending_mask", "==", "1", ",", "-", "float", "(", "\"inf\"", ")", ")", "# `== 1` converts to bool or uint8", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimAttention._sliding_chunks_query_key_matmul": [[508, 573], ["modeling_longskim.LongSkimAttention.size", "modeling_longskim.LongSkimAttention.transpose().reshape", "modeling_longskim.LongSkimAttention.transpose().reshape", "modeling_longskim.LongSkimAttention._chunk", "modeling_longskim.LongSkimAttention._chunk", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "modeling_longskim.LongSkimAttention._pad_and_transpose_last_two_dims", "modeling_longskim.LongSkimAttention.new_empty", "diagonal_attention_scores.view().transpose.view().transpose.view().transpose", "modeling_longskim.LongSkimAttention._mask_invalid_locations", "modeling_longskim.LongSkimAttention.size", "modeling_longskim.LongSkimAttention.size", "modeling_longskim.LongSkimAttention.transpose", "modeling_longskim.LongSkimAttention.transpose", "diagonal_attention_scores.view().transpose.view().transpose.view"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._chunk", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._chunk", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._pad_and_transpose_last_two_dims", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._mask_invalid_locations"], ["", "def", "_sliding_chunks_query_key_matmul", "(", "self", ",", "query", ":", "torch", ".", "Tensor", ",", "key", ":", "torch", ".", "Tensor", ",", "window_overlap", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Matrix multiplication of query and key tensors using with a sliding window attention pattern. This\n        implementation splits the input into overlapping chunks of size 2w (e.g. 512 for pretrained Longformer) with an\n        overlap of size window_overlap\n        \"\"\"", "\n", "batch_size", ",", "seq_len", ",", "num_heads", ",", "head_dim", "=", "query", ".", "size", "(", ")", "\n", "assert", "(", "\n", "seq_len", "%", "(", "window_overlap", "*", "2", ")", "==", "0", "\n", ")", ",", "f\"Sequence length should be multiple of {window_overlap * 2}. Given {seq_len}\"", "\n", "assert", "query", ".", "size", "(", ")", "==", "key", ".", "size", "(", ")", "\n", "\n", "chunks_count", "=", "seq_len", "//", "window_overlap", "-", "1", "\n", "\n", "# group batch_size and num_heads dimensions into one, then chunk seq_len into chunks of size window_overlap * 2", "\n", "query", "=", "query", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "batch_size", "*", "num_heads", ",", "seq_len", ",", "head_dim", ")", "\n", "key", "=", "key", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "batch_size", "*", "num_heads", ",", "seq_len", ",", "head_dim", ")", "\n", "\n", "query", "=", "self", ".", "_chunk", "(", "query", ",", "window_overlap", ")", "\n", "key", "=", "self", ".", "_chunk", "(", "key", ",", "window_overlap", ")", "\n", "\n", "# matrix multiplication", "\n", "# bcxd: batch_size * num_heads x chunks x 2window_overlap x head_dim", "\n", "# bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim", "\n", "# bcxy: batch_size * num_heads x chunks x 2window_overlap x window_overlap", "\n", "diagonal_chunked_attention_scores", "=", "torch", ".", "einsum", "(", "\"bcxd,bcyd->bcxy\"", ",", "(", "query", ",", "key", ")", ")", "# multiply", "\n", "\n", "# convert diagonals into columns", "\n", "diagonal_chunked_attention_scores", "=", "self", ".", "_pad_and_transpose_last_two_dims", "(", "\n", "diagonal_chunked_attention_scores", ",", "padding", "=", "(", "0", ",", "0", ",", "0", ",", "1", ")", "\n", ")", "\n", "\n", "# allocate space for the overall attention matrix where the chunks are combined. The last dimension", "\n", "# has (window_overlap * 2 + 1) columns. The first (window_overlap) columns are the window_overlap lower triangles (attention from a word to", "\n", "# window_overlap previous words). The following column is attention score from each word to itself, then", "\n", "# followed by window_overlap columns for the upper triangle.", "\n", "\n", "diagonal_attention_scores", "=", "diagonal_chunked_attention_scores", ".", "new_empty", "(", "\n", "(", "batch_size", "*", "num_heads", ",", "chunks_count", "+", "1", ",", "window_overlap", ",", "window_overlap", "*", "2", "+", "1", ")", "\n", ")", "\n", "\n", "# copy parts from diagonal_chunked_attention_scores into the combined matrix of attentions", "\n", "# - copying the main diagonal and the upper triangle", "\n", "diagonal_attention_scores", "[", ":", ",", ":", "-", "1", ",", ":", ",", "window_overlap", ":", "]", "=", "diagonal_chunked_attention_scores", "[", "\n", ":", ",", ":", ",", ":", "window_overlap", ",", ":", "window_overlap", "+", "1", "\n", "]", "\n", "diagonal_attention_scores", "[", ":", ",", "-", "1", ",", ":", ",", "window_overlap", ":", "]", "=", "diagonal_chunked_attention_scores", "[", "\n", ":", ",", "-", "1", ",", "window_overlap", ":", ",", ":", "window_overlap", "+", "1", "\n", "]", "\n", "# - copying the lower triangle", "\n", "diagonal_attention_scores", "[", ":", ",", "1", ":", ",", ":", ",", ":", "window_overlap", "]", "=", "diagonal_chunked_attention_scores", "[", "\n", ":", ",", ":", ",", "-", "(", "window_overlap", "+", "1", ")", ":", "-", "1", ",", "window_overlap", "+", "1", ":", "\n", "]", "\n", "\n", "diagonal_attention_scores", "[", ":", ",", "0", ",", "1", ":", "window_overlap", ",", "1", ":", "window_overlap", "]", "=", "diagonal_chunked_attention_scores", "[", "\n", ":", ",", "0", ",", ":", "window_overlap", "-", "1", ",", "1", "-", "window_overlap", ":", "\n", "]", "\n", "\n", "# separate batch_size and num_heads dimensions again", "\n", "diagonal_attention_scores", "=", "diagonal_attention_scores", ".", "view", "(", "\n", "batch_size", ",", "num_heads", ",", "seq_len", ",", "2", "*", "window_overlap", "+", "1", "\n", ")", ".", "transpose", "(", "2", ",", "1", ")", "\n", "\n", "self", ".", "_mask_invalid_locations", "(", "diagonal_attention_scores", ",", "window_overlap", ")", "\n", "return", "diagonal_attention_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimAttention._get_global_attn_indices": [[574, 601], ["is_index_global_attn.long().sum", "is_index_global_attn.long().sum.max", "is_index_global_attn.nonzero", "is_local_index_global_attn.nonzero", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "is_index_global_attn.long().sum.unsqueeze", "is_index_global_attn.long"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_global_attn_indices", "(", "is_index_global_attn", ")", ":", "\n", "        ", "\"\"\" compute global attn indices required throughout forward pass \"\"\"", "\n", "# helper variable", "\n", "num_global_attn_indices", "=", "is_index_global_attn", ".", "long", "(", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "\n", "# max number of global attn indices in batch", "\n", "max_num_global_attn_indices", "=", "num_global_attn_indices", ".", "max", "(", ")", "\n", "\n", "# indices of global attn", "\n", "is_index_global_attn_nonzero", "=", "is_index_global_attn", ".", "nonzero", "(", "as_tuple", "=", "True", ")", "\n", "\n", "# helper variable", "\n", "is_local_index_global_attn", "=", "torch", ".", "arange", "(", "\n", "max_num_global_attn_indices", ",", "device", "=", "is_index_global_attn", ".", "device", "\n", ")", "<", "num_global_attn_indices", ".", "unsqueeze", "(", "dim", "=", "-", "1", ")", "\n", "\n", "# location of the non-padding values within global attention indices", "\n", "is_local_index_global_attn_nonzero", "=", "is_local_index_global_attn", ".", "nonzero", "(", "as_tuple", "=", "True", ")", "\n", "\n", "# location of the padding values within global attention indices", "\n", "is_local_index_no_global_attn_nonzero", "=", "(", "is_local_index_global_attn", "==", "0", ")", ".", "nonzero", "(", "as_tuple", "=", "True", ")", "\n", "return", "(", "\n", "max_num_global_attn_indices", ",", "\n", "is_index_global_attn_nonzero", ",", "\n", "is_local_index_global_attn_nonzero", ",", "\n", "is_local_index_no_global_attn_nonzero", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimAttention._concat_with_global_key_attn_probs": [[603, 629], ["key_vectors.new_zeros", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "_concat_with_global_key_attn_probs", "(", "\n", "self", ",", "\n", "key_vectors", ",", "\n", "query_vectors", ",", "\n", "max_num_global_attn_indices", ",", "\n", "is_index_global_attn_nonzero", ",", "\n", "is_local_index_global_attn_nonzero", ",", "\n", "is_local_index_no_global_attn_nonzero", ",", "\n", ")", ":", "\n", "        ", "batch_size", "=", "key_vectors", ".", "shape", "[", "0", "]", "\n", "\n", "# create only global key vectors", "\n", "key_vectors_only_global", "=", "key_vectors", ".", "new_zeros", "(", "\n", "batch_size", ",", "max_num_global_attn_indices", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", "\n", ")", "\n", "\n", "key_vectors_only_global", "[", "is_local_index_global_attn_nonzero", "]", "=", "key_vectors", "[", "is_index_global_attn_nonzero", "]", "\n", "\n", "# (batch_size, seq_len, num_heads, max_num_global_attn_indices)", "\n", "attn_probs_from_global_key", "=", "torch", ".", "einsum", "(", "\"blhd,bshd->blhs\"", ",", "(", "query_vectors", ",", "key_vectors_only_global", ")", ")", "\n", "\n", "attn_probs_from_global_key", "[", "\n", "is_local_index_no_global_attn_nonzero", "[", "0", "]", ",", ":", ",", ":", ",", "is_local_index_no_global_attn_nonzero", "[", "1", "]", "\n", "]", "=", "-", "10000.0", "\n", "\n", "return", "attn_probs_from_global_key", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimAttention._compute_global_attn_probs_from_hidden": [[630, 696], ["hidden_layout_states.new_zeros", "modeling_longskim.LongSkimAttention.query_global", "modeling_longskim.LongSkimAttention.key_global", "math.sqrt", "global_query_vectors_only_global.contiguous().view().transpose.contiguous().view().transpose.contiguous().view().transpose", "global_key_vectors.contiguous().view().transpose.contiguous().view().transpose.contiguous().view().transpose", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "global_attn_scores.view.view.view", "global_attn_scores.view.view.masked_fill", "global_attn_scores.view.view.view", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "global_key_vectors.contiguous().view().transpose.contiguous().view().transpose.transpose", "list", "torch.nn.functional.softmax.type_as", "global_query_vectors_only_global.contiguous().view().transpose.contiguous().view().transpose.contiguous().view", "global_key_vectors.contiguous().view().transpose.contiguous().view().transpose.contiguous().view", "global_attn_scores.view.view.size", "global_attn_scores.view.view.size", "global_query_vectors_only_global.contiguous().view().transpose.contiguous().view().transpose.contiguous", "global_key_vectors.contiguous().view().transpose.contiguous().view().transpose.contiguous"], "methods", ["None"], ["", "def", "_compute_global_attn_probs_from_hidden", "(", "\n", "self", ",", "\n", "hidden_layout_states", ",", "\n", "max_num_global_attn_indices", ",", "\n", "is_local_index_global_attn_nonzero", ",", "\n", "is_index_global_attn_nonzero", ",", "\n", "is_local_index_no_global_attn_nonzero", ",", "\n", "is_index_masked", ",", "\n", ")", ":", "\n", "        ", "seq_len", ",", "batch_size", "=", "hidden_layout_states", ".", "shape", "[", ":", "2", "]", "\n", "\n", "# prepare global hidden states", "\n", "global_attn_hidden_layout_states", "=", "hidden_layout_states", ".", "new_zeros", "(", "max_num_global_attn_indices", ",", "batch_size", ",", "self", ".", "embed_dim", ")", "\n", "global_attn_hidden_layout_states", "[", "is_local_index_global_attn_nonzero", "[", ":", ":", "-", "1", "]", "]", "=", "hidden_layout_states", "[", "\n", "is_index_global_attn_nonzero", "[", ":", ":", "-", "1", "]", "\n", "]", "\n", "\n", "# global key, query", "\n", "global_query_vectors_only_global", "=", "self", ".", "query_global", "(", "global_attn_hidden_layout_states", ")", "\n", "global_key_vectors", "=", "self", ".", "key_global", "(", "hidden_layout_states", ")", "\n", "\n", "# normalize", "\n", "global_query_vectors_only_global", "/=", "math", ".", "sqrt", "(", "self", ".", "head_dim", ")", "\n", "\n", "# reshape", "\n", "global_query_vectors_only_global", "=", "(", "\n", "global_query_vectors_only_global", ".", "contiguous", "(", ")", "\n", ".", "view", "(", "max_num_global_attn_indices", ",", "batch_size", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", "\n", ".", "transpose", "(", "0", ",", "1", ")", "\n", ")", "# (batch_size * self.num_heads, max_num_global_attn_indices, head_dim)", "\n", "global_key_vectors", "=", "(", "\n", "global_key_vectors", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "batch_size", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", ")", "# batch_size * self.num_heads, seq_len, head_dim)", "\n", "\n", "# compute attn scores", "\n", "global_attn_scores", "=", "torch", ".", "bmm", "(", "global_query_vectors_only_global", ",", "global_key_vectors", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "assert", "list", "(", "global_attn_scores", ".", "size", "(", ")", ")", "==", "[", "\n", "batch_size", "*", "self", ".", "num_heads", ",", "\n", "max_num_global_attn_indices", ",", "\n", "seq_len", ",", "\n", "]", ",", "f\"global_attn_scores have the wrong size. Size should be {(batch_size * self.num_heads, max_num_global_attn_indices, seq_len)}, but is {global_attn_scores.size()}.\"", "\n", "\n", "global_attn_scores", "=", "global_attn_scores", ".", "view", "(", "batch_size", ",", "self", ".", "num_heads", ",", "max_num_global_attn_indices", ",", "seq_len", ")", "\n", "\n", "global_attn_scores", "[", "\n", "is_local_index_no_global_attn_nonzero", "[", "0", "]", ",", ":", ",", "is_local_index_no_global_attn_nonzero", "[", "1", "]", ",", ":", "\n", "]", "=", "-", "10000.0", "\n", "\n", "global_attn_scores", "=", "global_attn_scores", ".", "masked_fill", "(", "\n", "is_index_masked", "[", ":", ",", "None", ",", "None", ",", ":", "]", ",", "\n", "-", "10000.0", ",", "\n", ")", "\n", "\n", "global_attn_scores", "=", "global_attn_scores", ".", "view", "(", "batch_size", "*", "self", ".", "num_heads", ",", "max_num_global_attn_indices", ",", "seq_len", ")", "\n", "\n", "# compute global attn probs", "\n", "global_attn_probs_float", "=", "F", ".", "softmax", "(", "\n", "global_attn_scores", ",", "dim", "=", "-", "1", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", "# use fp32 for numerical stability", "\n", "\n", "global_attn_probs", "=", "F", ".", "dropout", "(", "\n", "global_attn_probs_float", ".", "type_as", "(", "global_attn_scores", ")", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", "\n", ")", "\n", "\n", "return", "global_attn_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention.__init__": [[699, 720], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "head_dim", "=", "config", ".", "attention_head_size", "# int(config.hidden_size / config.num_attention_heads)", "\n", "self", ".", "embed_dim", "=", "self", ".", "num_heads", "*", "self", ".", "head_dim", "# config.hidden_size", "\n", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "embed_dim", ")", "\n", "self", ".", "value_global", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "dropout", "=", "config", ".", "attention_probs_dropout_prob", "\n", "\n", "attention_window", "=", "config", ".", "attention_window", "\n", "assert", "(", "\n", "attention_window", "%", "2", "==", "0", "\n", ")", ",", "f\"`attention_window` has to be an even value. Given {attention_window}\"", "\n", "assert", "(", "\n", "attention_window", ">", "0", "\n", ")", ",", "f\"`attention_window` has to be positive. Given {attention_window}\"", "\n", "\n", "self", ".", "one_sided_attn_window_size", "=", "attention_window", "//", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention.forward": [[721, 813], ["hidden_states.transpose.transpose.transpose", "modeling_longskim.LongSkimformerSelfAttention.value", "hidden_states.transpose.transpose.size", "value_vectors.view().transpose.view().transpose.view().transpose", "modeling_longskim.LongSkimformerSelfAttention.transpose().reshape().contiguous", "modeling_longskim.LongSkimformerSelfAttention._get_global_attn_indices", "modeling_longskim.LongSkimformerSelfAttention._compute_attn_output_with_global_indices", "modeling_longskim.LongSkimformerSelfAttention._sliding_chunks_matmul_attn_probs_value", "modeling_longskim.LongSkimformerSelfAttention.size", "modeling_longskim.LongSkimformerSelfAttention._compute_global_attn_output_from_hidden", "nonzero_global_attn_output.view", "modeling_longskim.LongSkimformerSelfAttention.transpose", "layer_head_mask.size", "layer_head_mask.view", "value_vectors.view().transpose.view().transpose.view", "modeling_longskim.LongSkimformerSelfAttention.transpose().reshape", "len", "layer_head_mask.size", "modeling_longskim.LongSkimformerSelfAttention.transpose"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._get_global_attn_indices", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._compute_attn_output_with_global_indices", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._sliding_chunks_matmul_attn_probs_value", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._compute_global_attn_output_from_hidden"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attn_probs", ",", "\n", "layer_head_mask", "=", "None", ",", "\n", "is_index_masked", "=", "None", ",", "\n", "is_index_global_attn", "=", "None", ",", "\n", "is_global_attn", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        :class:`LongformerSelfAttention` expects `len(hidden_states)` to be multiple of `attention_window`. Padding to\n        `attention_window` happens in :meth:`LongformerModel.forward` to avoid redoing the padding on each layer.\n\n        The `attention_mask` is changed in :meth:`LongformerModel.forward` from 0, 1, 2 to:\n\n            * -10000: no attention\n            * 0: local attention\n            * +10000: global attention\n        \"\"\"", "\n", "hidden_states", "=", "hidden_states", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "if", "is_global_attn", ":", "\n", "            ", "attn_probs", ",", "global_attn_probs", "=", "attn_probs", "\n", "\n", "# project hidden states", "\n", "", "value_vectors", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "\n", "seq_len", ",", "batch_size", ",", "embed_dim", "=", "hidden_states", ".", "size", "(", ")", "\n", "assert", "(", "\n", "embed_dim", "==", "self", ".", "embed_dim", "\n", ")", ",", "f\"hidden_states should have embed_dim = {self.embed_dim}, but has {embed_dim}\"", "\n", "\n", "if", "layer_head_mask", "is", "not", "None", ":", "\n", "            ", "assert", "layer_head_mask", ".", "size", "(", ")", "==", "(", "\n", "self", ".", "num_heads", ",", "\n", ")", ",", "f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"", "\n", "attn_probs", "=", "layer_head_mask", ".", "view", "(", "1", ",", "1", ",", "-", "1", ",", "1", ")", "*", "attn_probs", "\n", "\n", "", "value_vectors", "=", "value_vectors", ".", "view", "(", "seq_len", ",", "batch_size", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# compute local attention output with global attention value and add", "\n", "if", "is_global_attn", ":", "\n", "            ", "(", "\n", "max_num_global_attn_indices", ",", "\n", "is_index_global_attn_nonzero", ",", "\n", "is_local_index_global_attn_nonzero", ",", "\n", "is_local_index_no_global_attn_nonzero", ",", "\n", ")", "=", "self", ".", "_get_global_attn_indices", "(", "is_index_global_attn", ")", "\n", "# compute sum of global and local attn", "\n", "attn_output", "=", "self", ".", "_compute_attn_output_with_global_indices", "(", "\n", "value_vectors", "=", "value_vectors", ",", "\n", "attn_probs", "=", "attn_probs", ",", "\n", "max_num_global_attn_indices", "=", "max_num_global_attn_indices", ",", "\n", "is_index_global_attn_nonzero", "=", "is_index_global_attn_nonzero", ",", "\n", "is_local_index_global_attn_nonzero", "=", "is_local_index_global_attn_nonzero", ",", "\n", ")", "\n", "", "else", ":", "\n", "# compute local attn only", "\n", "            ", "attn_output", "=", "self", ".", "_sliding_chunks_matmul_attn_probs_value", "(", "\n", "attn_probs", ",", "value_vectors", ",", "self", ".", "one_sided_attn_window_size", "\n", ")", "\n", "\n", "", "assert", "attn_output", ".", "size", "(", ")", "==", "(", "batch_size", ",", "seq_len", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ",", "\"Unexpected size\"", "\n", "attn_output", "=", "attn_output", ".", "transpose", "(", "0", ",", "1", ")", ".", "reshape", "(", "seq_len", ",", "batch_size", ",", "embed_dim", ")", ".", "contiguous", "(", ")", "\n", "\n", "# compute value for global attention and overwrite to attention output", "\n", "# TODO: remove the redundant computation", "\n", "if", "is_global_attn", ":", "\n", "            ", "global_attn_output", "=", "self", ".", "_compute_global_attn_output_from_hidden", "(", "\n", "hidden_states", "=", "hidden_states", ",", "\n", "global_attn_probs", "=", "global_attn_probs", ",", "\n", "max_num_global_attn_indices", "=", "max_num_global_attn_indices", ",", "\n", "layer_head_mask", "=", "layer_head_mask", ",", "\n", "is_local_index_global_attn_nonzero", "=", "is_local_index_global_attn_nonzero", ",", "\n", "is_index_global_attn_nonzero", "=", "is_index_global_attn_nonzero", ",", "\n", "is_local_index_no_global_attn_nonzero", "=", "is_local_index_no_global_attn_nonzero", ",", "\n", "is_index_masked", "=", "is_index_masked", ",", "\n", ")", "\n", "\n", "# get only non zero global attn output", "\n", "nonzero_global_attn_output", "=", "global_attn_output", "[", "\n", "is_local_index_global_attn_nonzero", "[", "0", "]", ",", ":", ",", "is_local_index_global_attn_nonzero", "[", "1", "]", "\n", "]", "\n", "\n", "# overwrite values with global attention", "\n", "attn_output", "[", "is_index_global_attn_nonzero", "[", ":", ":", "-", "1", "]", "]", "=", "nonzero_global_attn_output", ".", "view", "(", "\n", "len", "(", "is_local_index_global_attn_nonzero", "[", "0", "]", ")", ",", "-", "1", "\n", ")", "\n", "\n", "", "outputs", "=", "(", "attn_output", ".", "transpose", "(", "0", ",", "1", ")", ",", ")", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._pad_and_transpose_last_two_dims": [[814, 824], ["torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "hidden_states_padded.view.view.view", "hidden_states_padded.view.view.size", "hidden_states_padded.view.view.size", "hidden_states_padded.view.view.size"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_pad_and_transpose_last_two_dims", "(", "hidden_states_padded", ",", "padding", ")", ":", "\n", "        ", "\"\"\"pads rows and then flips rows and columns\"\"\"", "\n", "hidden_states_padded", "=", "F", ".", "pad", "(", "\n", "hidden_states_padded", ",", "padding", "\n", ")", "# padding value is not important because it will be overwritten", "\n", "hidden_states_padded", "=", "hidden_states_padded", ".", "view", "(", "\n", "*", "hidden_states_padded", ".", "size", "(", ")", "[", ":", "-", "2", "]", ",", "hidden_states_padded", ".", "size", "(", "-", "1", ")", ",", "hidden_states_padded", ".", "size", "(", "-", "2", ")", "\n", ")", "\n", "return", "hidden_states_padded", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._pad_and_diagonalize": [[825, 858], ["chunked_hidden_states.view.view.size", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "chunked_hidden_states.view.view.view", "chunked_hidden_states.view.view.view"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_pad_and_diagonalize", "(", "chunked_hidden_states", ")", ":", "\n", "        ", "\"\"\"\n        shift every row 1 step right, converting columns into diagonals.\n\n        Example::\n\n              chunked_hidden_states: [ 0.4983,  2.6918, -0.0071,  1.0492,\n                                       -1.8348,  0.7672,  0.2986,  0.0285,\n                                       -0.7584,  0.4206, -0.0405,  0.1599,\n                                       2.0514, -1.1600,  0.5372,  0.2629 ]\n              window_overlap = num_rows = 4\n             (pad & diagonalize) =>\n             [ 0.4983,  2.6918, -0.0071,  1.0492, 0.0000,  0.0000,  0.0000\n               0.0000,  -1.8348,  0.7672,  0.2986,  0.0285, 0.0000,  0.0000\n               0.0000,  0.0000, -0.7584,  0.4206, -0.0405,  0.1599, 0.0000\n               0.0000,  0.0000,  0.0000, 2.0514, -1.1600,  0.5372,  0.2629 ]\n        \"\"\"", "\n", "total_num_heads", ",", "num_chunks", ",", "window_overlap", ",", "hidden_dim", "=", "chunked_hidden_states", ".", "size", "(", ")", "\n", "chunked_hidden_states", "=", "F", ".", "pad", "(", "\n", "chunked_hidden_states", ",", "(", "0", ",", "window_overlap", "+", "1", ")", "\n", ")", "# total_num_heads x num_chunks x window_overlap x (hidden_dim+window_overlap+1). Padding value is not important because it'll be overwritten", "\n", "chunked_hidden_states", "=", "chunked_hidden_states", ".", "view", "(", "\n", "total_num_heads", ",", "num_chunks", ",", "-", "1", "\n", ")", "# total_num_heads x num_chunks x window_overlap*window_overlap+window_overlap", "\n", "chunked_hidden_states", "=", "chunked_hidden_states", "[", "\n", ":", ",", ":", ",", ":", "-", "window_overlap", "\n", "]", "# total_num_heads x num_chunks x window_overlap*window_overlap", "\n", "chunked_hidden_states", "=", "chunked_hidden_states", ".", "view", "(", "\n", "total_num_heads", ",", "num_chunks", ",", "window_overlap", ",", "window_overlap", "+", "hidden_dim", "\n", ")", "\n", "chunked_hidden_states", "=", "chunked_hidden_states", "[", ":", ",", ":", ",", ":", ",", ":", "-", "1", "]", "\n", "return", "chunked_hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._chunk": [[859, 878], ["hidden_states.view.view.view", "list", "list", "hidden_states.view.view.as_strided", "hidden_states.view.view.size", "hidden_states.view.view.size", "hidden_states.view.view.size", "hidden_states.view.view.stride", "hidden_states.view.view.size"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_chunk", "(", "hidden_states", ",", "window_overlap", ")", ":", "\n", "        ", "\"\"\"convert into overlapping chunks. Chunk size = 2w, overlap size = w\"\"\"", "\n", "\n", "# non-overlapping chunks of size = 2w", "\n", "hidden_states", "=", "hidden_states", ".", "view", "(", "\n", "hidden_states", ".", "size", "(", "0", ")", ",", "\n", "hidden_states", ".", "size", "(", "1", ")", "//", "(", "window_overlap", "*", "2", ")", ",", "\n", "window_overlap", "*", "2", ",", "\n", "hidden_states", ".", "size", "(", "2", ")", ",", "\n", ")", "\n", "\n", "# use `as_strided` to make the chunks overlap with an overlap size = window_overlap", "\n", "chunk_size", "=", "list", "(", "hidden_states", ".", "size", "(", ")", ")", "\n", "chunk_size", "[", "1", "]", "=", "chunk_size", "[", "1", "]", "*", "2", "-", "1", "\n", "\n", "chunk_stride", "=", "list", "(", "hidden_states", ".", "stride", "(", ")", ")", "\n", "chunk_stride", "[", "1", "]", "=", "chunk_stride", "[", "1", "]", "//", "2", "\n", "return", "hidden_states", ".", "as_strided", "(", "size", "=", "chunk_size", ",", "stride", "=", "chunk_stride", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._mask_invalid_locations": [[879, 890], ["input_tensor.new_ones().tril().flip", "beginning_mask.expand.expand.flip", "beginning_mask.expand.expand.expand", "beginning_input.masked_fill_", "ending_mask.expand.expand.expand", "ending_input.masked_fill_", "beginning_input.size", "ending_input.size", "input_tensor.new_ones().tril", "float", "float", "input_tensor.new_ones"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_mask_invalid_locations", "(", "input_tensor", ",", "affected_seq_len", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "beginning_mask_2d", "=", "input_tensor", ".", "new_ones", "(", "affected_seq_len", ",", "affected_seq_len", "+", "1", ")", ".", "tril", "(", ")", ".", "flip", "(", "dims", "=", "[", "0", "]", ")", "\n", "beginning_mask", "=", "beginning_mask_2d", "[", "None", ",", ":", ",", "None", ",", ":", "]", "\n", "ending_mask", "=", "beginning_mask", ".", "flip", "(", "dims", "=", "(", "1", ",", "3", ")", ")", "\n", "beginning_input", "=", "input_tensor", "[", ":", ",", ":", "affected_seq_len", ",", ":", ",", ":", "affected_seq_len", "+", "1", "]", "\n", "beginning_mask", "=", "beginning_mask", ".", "expand", "(", "beginning_input", ".", "size", "(", ")", ")", "\n", "beginning_input", ".", "masked_fill_", "(", "beginning_mask", "==", "1", ",", "-", "float", "(", "\"inf\"", ")", ")", "# `== 1` converts to bool or uint8", "\n", "ending_input", "=", "input_tensor", "[", ":", ",", "-", "affected_seq_len", ":", ",", ":", ",", "-", "(", "affected_seq_len", "+", "1", ")", ":", "]", "\n", "ending_mask", "=", "ending_mask", ".", "expand", "(", "ending_input", ".", "size", "(", ")", ")", "\n", "ending_input", ".", "masked_fill_", "(", "ending_mask", "==", "1", ",", "-", "float", "(", "\"inf\"", ")", ")", "# `== 1` converts to bool or uint8", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._sliding_chunks_query_key_matmul": [[891, 956], ["modeling_longskim.LongSkimformerSelfAttention.size", "modeling_longskim.LongSkimformerSelfAttention.transpose().reshape", "modeling_longskim.LongSkimformerSelfAttention.transpose().reshape", "modeling_longskim.LongSkimformerSelfAttention._chunk", "modeling_longskim.LongSkimformerSelfAttention._chunk", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "modeling_longskim.LongSkimformerSelfAttention._pad_and_transpose_last_two_dims", "modeling_longskim.LongSkimformerSelfAttention.new_empty", "diagonal_attention_scores.view().transpose.view().transpose.view().transpose", "modeling_longskim.LongSkimformerSelfAttention._mask_invalid_locations", "modeling_longskim.LongSkimformerSelfAttention.size", "modeling_longskim.LongSkimformerSelfAttention.size", "modeling_longskim.LongSkimformerSelfAttention.transpose", "modeling_longskim.LongSkimformerSelfAttention.transpose", "diagonal_attention_scores.view().transpose.view().transpose.view"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._chunk", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._chunk", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._pad_and_transpose_last_two_dims", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._mask_invalid_locations"], ["", "def", "_sliding_chunks_query_key_matmul", "(", "self", ",", "query", ":", "torch", ".", "Tensor", ",", "key", ":", "torch", ".", "Tensor", ",", "window_overlap", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Matrix multiplication of query and key tensors using with a sliding window attention pattern. This\n        implementation splits the input into overlapping chunks of size 2w (e.g. 512 for pretrained Longformer) with an\n        overlap of size window_overlap\n        \"\"\"", "\n", "batch_size", ",", "seq_len", ",", "num_heads", ",", "head_dim", "=", "query", ".", "size", "(", ")", "\n", "assert", "(", "\n", "seq_len", "%", "(", "window_overlap", "*", "2", ")", "==", "0", "\n", ")", ",", "f\"Sequence length should be multiple of {window_overlap * 2}. Given {seq_len}\"", "\n", "assert", "query", ".", "size", "(", ")", "==", "key", ".", "size", "(", ")", "\n", "\n", "chunks_count", "=", "seq_len", "//", "window_overlap", "-", "1", "\n", "\n", "# group batch_size and num_heads dimensions into one, then chunk seq_len into chunks of size window_overlap * 2", "\n", "query", "=", "query", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "batch_size", "*", "num_heads", ",", "seq_len", ",", "head_dim", ")", "\n", "key", "=", "key", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "batch_size", "*", "num_heads", ",", "seq_len", ",", "head_dim", ")", "\n", "\n", "query", "=", "self", ".", "_chunk", "(", "query", ",", "window_overlap", ")", "\n", "key", "=", "self", ".", "_chunk", "(", "key", ",", "window_overlap", ")", "\n", "\n", "# matrix multiplication", "\n", "# bcxd: batch_size * num_heads x chunks x 2window_overlap x head_dim", "\n", "# bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim", "\n", "# bcxy: batch_size * num_heads x chunks x 2window_overlap x window_overlap", "\n", "diagonal_chunked_attention_scores", "=", "torch", ".", "einsum", "(", "\"bcxd,bcyd->bcxy\"", ",", "(", "query", ",", "key", ")", ")", "# multiply", "\n", "\n", "# convert diagonals into columns", "\n", "diagonal_chunked_attention_scores", "=", "self", ".", "_pad_and_transpose_last_two_dims", "(", "\n", "diagonal_chunked_attention_scores", ",", "padding", "=", "(", "0", ",", "0", ",", "0", ",", "1", ")", "\n", ")", "\n", "\n", "# allocate space for the overall attention matrix where the chunks are combined. The last dimension", "\n", "# has (window_overlap * 2 + 1) columns. The first (window_overlap) columns are the window_overlap lower triangles (attention from a word to", "\n", "# window_overlap previous words). The following column is attention score from each word to itself, then", "\n", "# followed by window_overlap columns for the upper triangle.", "\n", "\n", "diagonal_attention_scores", "=", "diagonal_chunked_attention_scores", ".", "new_empty", "(", "\n", "(", "batch_size", "*", "num_heads", ",", "chunks_count", "+", "1", ",", "window_overlap", ",", "window_overlap", "*", "2", "+", "1", ")", "\n", ")", "\n", "\n", "# copy parts from diagonal_chunked_attention_scores into the combined matrix of attentions", "\n", "# - copying the main diagonal and the upper triangle", "\n", "diagonal_attention_scores", "[", ":", ",", ":", "-", "1", ",", ":", ",", "window_overlap", ":", "]", "=", "diagonal_chunked_attention_scores", "[", "\n", ":", ",", ":", ",", ":", "window_overlap", ",", ":", "window_overlap", "+", "1", "\n", "]", "\n", "diagonal_attention_scores", "[", ":", ",", "-", "1", ",", ":", ",", "window_overlap", ":", "]", "=", "diagonal_chunked_attention_scores", "[", "\n", ":", ",", "-", "1", ",", "window_overlap", ":", ",", ":", "window_overlap", "+", "1", "\n", "]", "\n", "# - copying the lower triangle", "\n", "diagonal_attention_scores", "[", ":", ",", "1", ":", ",", ":", ",", ":", "window_overlap", "]", "=", "diagonal_chunked_attention_scores", "[", "\n", ":", ",", ":", ",", "-", "(", "window_overlap", "+", "1", ")", ":", "-", "1", ",", "window_overlap", "+", "1", ":", "\n", "]", "\n", "\n", "diagonal_attention_scores", "[", ":", ",", "0", ",", "1", ":", "window_overlap", ",", "1", ":", "window_overlap", "]", "=", "diagonal_chunked_attention_scores", "[", "\n", ":", ",", "0", ",", ":", "window_overlap", "-", "1", ",", "1", "-", "window_overlap", ":", "\n", "]", "\n", "\n", "# separate batch_size and num_heads dimensions again", "\n", "diagonal_attention_scores", "=", "diagonal_attention_scores", ".", "view", "(", "\n", "batch_size", ",", "num_heads", ",", "seq_len", ",", "2", "*", "window_overlap", "+", "1", "\n", ")", ".", "transpose", "(", "2", ",", "1", ")", "\n", "\n", "self", ".", "_mask_invalid_locations", "(", "diagonal_attention_scores", ",", "window_overlap", ")", "\n", "return", "diagonal_attention_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._sliding_chunks_matmul_attn_probs_value": [[957, 997], ["value.transpose().reshape.transpose().reshape.size", "attn_probs.transpose().reshape", "value.transpose().reshape.transpose().reshape.transpose().reshape", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad.stride", "torch.nn.functional.pad.as_strided", "modeling_longskim.LongSkimformerSelfAttention._pad_and_diagonalize", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum.view().transpose", "torch.einsum.view().transpose", "torch.einsum.view().transpose", "attn_probs.size", "attn_probs.size", "value.transpose().reshape.transpose().reshape.size", "attn_probs.transpose", "value.transpose().reshape.transpose().reshape.transpose", "torch.einsum.view", "torch.einsum.view", "torch.einsum.view"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._pad_and_diagonalize"], ["", "def", "_sliding_chunks_matmul_attn_probs_value", "(", "\n", "self", ",", "attn_probs", ":", "torch", ".", "Tensor", ",", "value", ":", "torch", ".", "Tensor", ",", "window_overlap", ":", "int", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Same as _sliding_chunks_query_key_matmul but for attn_probs and value tensors. Returned tensor will be of the\n        same shape as `attn_probs`\n        \"\"\"", "\n", "batch_size", ",", "seq_len", ",", "num_heads", ",", "head_dim", "=", "value", ".", "size", "(", ")", "\n", "\n", "assert", "seq_len", "%", "(", "window_overlap", "*", "2", ")", "==", "0", "\n", "assert", "attn_probs", ".", "size", "(", ")", "[", ":", "3", "]", "==", "value", ".", "size", "(", ")", "[", ":", "3", "]", "\n", "assert", "attn_probs", ".", "size", "(", "3", ")", "==", "2", "*", "window_overlap", "+", "1", "\n", "chunks_count", "=", "seq_len", "//", "window_overlap", "-", "1", "\n", "# group batch_size and num_heads dimensions into one, then chunk seq_len into chunks of size 2 window overlap", "\n", "\n", "chunked_attn_probs", "=", "attn_probs", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "\n", "batch_size", "*", "num_heads", ",", "seq_len", "//", "window_overlap", ",", "window_overlap", ",", "2", "*", "window_overlap", "+", "1", "\n", ")", "\n", "\n", "# group batch_size and num_heads dimensions into one", "\n", "value", "=", "value", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "batch_size", "*", "num_heads", ",", "seq_len", ",", "head_dim", ")", "\n", "\n", "# pad seq_len with w at the beginning of the sequence and another window overlap at the end", "\n", "padded_value", "=", "F", ".", "pad", "(", "value", ",", "(", "0", ",", "0", ",", "window_overlap", ",", "window_overlap", ")", ",", "value", "=", "-", "1", ")", "\n", "\n", "# chunk padded_value into chunks of size 3 window overlap and an overlap of size window overlap", "\n", "chunked_value_size", "=", "(", "batch_size", "*", "num_heads", ",", "chunks_count", "+", "1", ",", "3", "*", "window_overlap", ",", "head_dim", ")", "\n", "chunked_value_stride", "=", "padded_value", ".", "stride", "(", ")", "\n", "chunked_value_stride", "=", "(", "\n", "chunked_value_stride", "[", "0", "]", ",", "\n", "window_overlap", "*", "chunked_value_stride", "[", "1", "]", ",", "\n", "chunked_value_stride", "[", "1", "]", ",", "\n", "chunked_value_stride", "[", "2", "]", ",", "\n", ")", "\n", "chunked_value", "=", "padded_value", ".", "as_strided", "(", "size", "=", "chunked_value_size", ",", "stride", "=", "chunked_value_stride", ")", "\n", "\n", "chunked_attn_probs", "=", "self", ".", "_pad_and_diagonalize", "(", "chunked_attn_probs", ")", "\n", "\n", "context", "=", "torch", ".", "einsum", "(", "\"bcwd,bcdh->bcwh\"", ",", "(", "chunked_attn_probs", ",", "chunked_value", ")", ")", "\n", "return", "context", ".", "view", "(", "batch_size", ",", "num_heads", ",", "seq_len", ",", "head_dim", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._get_global_attn_indices": [[998, 1025], ["is_index_global_attn.long().sum", "is_index_global_attn.long().sum.max", "is_index_global_attn.nonzero", "is_local_index_global_attn.nonzero", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "is_index_global_attn.long().sum.unsqueeze", "is_index_global_attn.long"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_global_attn_indices", "(", "is_index_global_attn", ")", ":", "\n", "        ", "\"\"\" compute global attn indices required throughout forward pass \"\"\"", "\n", "# helper variable", "\n", "num_global_attn_indices", "=", "is_index_global_attn", ".", "long", "(", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "\n", "# max number of global attn indices in batch", "\n", "max_num_global_attn_indices", "=", "num_global_attn_indices", ".", "max", "(", ")", "\n", "\n", "# indices of global attn", "\n", "is_index_global_attn_nonzero", "=", "is_index_global_attn", ".", "nonzero", "(", "as_tuple", "=", "True", ")", "\n", "\n", "# helper variable", "\n", "is_local_index_global_attn", "=", "torch", ".", "arange", "(", "\n", "max_num_global_attn_indices", ",", "device", "=", "is_index_global_attn", ".", "device", "\n", ")", "<", "num_global_attn_indices", ".", "unsqueeze", "(", "dim", "=", "-", "1", ")", "\n", "\n", "# location of the non-padding values within global attention indices", "\n", "is_local_index_global_attn_nonzero", "=", "is_local_index_global_attn", ".", "nonzero", "(", "as_tuple", "=", "True", ")", "\n", "\n", "# location of the padding values within global attention indices", "\n", "is_local_index_no_global_attn_nonzero", "=", "(", "is_local_index_global_attn", "==", "0", ")", ".", "nonzero", "(", "as_tuple", "=", "True", ")", "\n", "return", "(", "\n", "max_num_global_attn_indices", ",", "\n", "is_index_global_attn_nonzero", ",", "\n", "is_local_index_global_attn_nonzero", ",", "\n", "is_local_index_no_global_attn_nonzero", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._concat_with_global_key_attn_probs": [[1027, 1053], ["key_vectors.new_zeros", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "_concat_with_global_key_attn_probs", "(", "\n", "self", ",", "\n", "key_vectors", ",", "\n", "query_vectors", ",", "\n", "max_num_global_attn_indices", ",", "\n", "is_index_global_attn_nonzero", ",", "\n", "is_local_index_global_attn_nonzero", ",", "\n", "is_local_index_no_global_attn_nonzero", ",", "\n", ")", ":", "\n", "        ", "batch_size", "=", "key_vectors", ".", "shape", "[", "0", "]", "\n", "\n", "# create only global key vectors", "\n", "key_vectors_only_global", "=", "key_vectors", ".", "new_zeros", "(", "\n", "batch_size", ",", "max_num_global_attn_indices", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", "\n", ")", "\n", "\n", "key_vectors_only_global", "[", "is_local_index_global_attn_nonzero", "]", "=", "key_vectors", "[", "is_index_global_attn_nonzero", "]", "\n", "\n", "# (batch_size, seq_len, num_heads, max_num_global_attn_indices)", "\n", "attn_probs_from_global_key", "=", "torch", ".", "einsum", "(", "\"blhd,bshd->blhs\"", ",", "(", "query_vectors", ",", "key_vectors_only_global", ")", ")", "\n", "\n", "attn_probs_from_global_key", "[", "\n", "is_local_index_no_global_attn_nonzero", "[", "0", "]", ",", ":", ",", ":", ",", "is_local_index_no_global_attn_nonzero", "[", "1", "]", "\n", "]", "=", "-", "10000.0", "\n", "\n", "return", "attn_probs_from_global_key", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._compute_attn_output_with_global_indices": [[1054, 1089], ["attn_probs.narrow", "value_vectors.new_zeros", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "torch.matmul().transpose", "attn_probs.narrow().contiguous", "modeling_longskim.LongSkimformerSelfAttention._sliding_chunks_matmul_attn_probs_value", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "attn_probs.narrow", "attn_probs.narrow.transpose", "value_vectors.new_zeros.transpose", "attn_probs.size"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._sliding_chunks_matmul_attn_probs_value"], ["", "def", "_compute_attn_output_with_global_indices", "(", "\n", "self", ",", "\n", "value_vectors", ",", "\n", "attn_probs", ",", "\n", "max_num_global_attn_indices", ",", "\n", "is_index_global_attn_nonzero", ",", "\n", "is_local_index_global_attn_nonzero", ",", "\n", ")", ":", "\n", "        ", "batch_size", "=", "attn_probs", ".", "shape", "[", "0", "]", "\n", "\n", "# cut local attn probs to global only", "\n", "attn_probs_only_global", "=", "attn_probs", ".", "narrow", "(", "-", "1", ",", "0", ",", "max_num_global_attn_indices", ")", "\n", "# get value vectors for global only", "\n", "value_vectors_only_global", "=", "value_vectors", ".", "new_zeros", "(", "\n", "batch_size", ",", "max_num_global_attn_indices", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", "\n", ")", "\n", "value_vectors_only_global", "[", "is_local_index_global_attn_nonzero", "]", "=", "value_vectors", "[", "is_index_global_attn_nonzero", "]", "\n", "\n", "# use `matmul` because `einsum` crashes sometimes with fp16", "\n", "# attn = torch.einsum('blhs,bshd->blhd', (selected_attn_probs, selected_v))", "\n", "# compute attn output only global", "\n", "attn_output_only_global", "=", "torch", ".", "matmul", "(", "\n", "attn_probs_only_global", ".", "transpose", "(", "1", ",", "2", ")", ",", "value_vectors_only_global", ".", "transpose", "(", "1", ",", "2", ")", "\n", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# reshape attn probs", "\n", "attn_probs_without_global", "=", "attn_probs", ".", "narrow", "(", "\n", "-", "1", ",", "max_num_global_attn_indices", ",", "attn_probs", ".", "size", "(", "-", "1", ")", "-", "max_num_global_attn_indices", "\n", ")", ".", "contiguous", "(", ")", "\n", "\n", "# compute attn output with global", "\n", "attn_output_without_global", "=", "self", ".", "_sliding_chunks_matmul_attn_probs_value", "(", "\n", "attn_probs_without_global", ",", "value_vectors", ",", "self", ".", "one_sided_attn_window_size", "\n", ")", "\n", "return", "attn_output_only_global", "+", "attn_output_without_global", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfAttention._compute_global_attn_output_from_hidden": [[1090, 1132], ["modeling_longskim.LongSkimformerSelfAttention.value_global", "global_value_vectors.contiguous().view().transpose.contiguous().view().transpose.contiguous().view().transpose", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "global_attn_probs.view.view.view", "global_attn_output.view.view.view", "global_attn_probs.view.view.view", "list", "global_value_vectors.contiguous().view().transpose.contiguous().view().transpose.contiguous().view", "layer_head_mask.size", "layer_head_mask.view", "global_attn_probs.view.view.view", "global_attn_output.view.view.size", "global_attn_output.view.view.size", "layer_head_mask.size", "global_value_vectors.contiguous().view().transpose.contiguous().view().transpose.contiguous"], "methods", ["None"], ["", "def", "_compute_global_attn_output_from_hidden", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "global_attn_probs", ",", "\n", "max_num_global_attn_indices", ",", "\n", "layer_head_mask", ",", "\n", ")", ":", "\n", "        ", "seq_len", ",", "batch_size", "=", "hidden_states", ".", "shape", "[", ":", "2", "]", "\n", "\n", "# global value", "\n", "global_value_vectors", "=", "self", ".", "value_global", "(", "hidden_states", ")", "\n", "\n", "global_value_vectors", "=", "(", "\n", "global_value_vectors", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "batch_size", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", ")", "# batch_size * self.num_heads, seq_len, head_dim)", "\n", "\n", "# apply layer head masking", "\n", "if", "layer_head_mask", "is", "not", "None", ":", "\n", "            ", "assert", "layer_head_mask", ".", "size", "(", ")", "==", "(", "\n", "self", ".", "num_heads", ",", "\n", ")", ",", "f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"", "\n", "global_attn_probs", "=", "layer_head_mask", ".", "view", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "*", "global_attn_probs", ".", "view", "(", "\n", "batch_size", ",", "self", ".", "num_heads", ",", "max_num_global_attn_indices", ",", "seq_len", "\n", ")", "\n", "global_attn_probs", "=", "global_attn_probs", ".", "view", "(", "\n", "batch_size", "*", "self", ".", "num_heads", ",", "max_num_global_attn_indices", ",", "seq_len", "\n", ")", "\n", "\n", "# global attn output", "\n", "", "global_attn_output", "=", "torch", ".", "bmm", "(", "global_attn_probs", ",", "global_value_vectors", ")", "\n", "\n", "assert", "list", "(", "global_attn_output", ".", "size", "(", ")", ")", "==", "[", "\n", "batch_size", "*", "self", ".", "num_heads", ",", "\n", "max_num_global_attn_indices", ",", "\n", "self", ".", "head_dim", ",", "\n", "]", ",", "f\"global_attn_output tensor has the wrong size. Size should be {(batch_size * self.num_heads, max_num_global_attn_indices, self.head_dim)}, but is {global_attn_output.size()}.\"", "\n", "\n", "global_attn_probs", "=", "global_attn_probs", ".", "view", "(", "batch_size", ",", "self", ".", "num_heads", ",", "max_num_global_attn_indices", ",", "seq_len", ")", "\n", "global_attn_output", "=", "global_attn_output", ".", "view", "(", "\n", "batch_size", ",", "self", ".", "num_heads", ",", "max_num_global_attn_indices", ",", "self", ".", "head_dim", "\n", ")", "\n", "return", "global_attn_output", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfOutput.__init__": [[1135, 1141], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "all_head_size", "=", "config", ".", "num_attention_heads", "*", "config", ".", "attention_head_size", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "all_head_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerSelfOutput.forward": [[1142, 1147], ["modeling_longskim.LongSkimformerSelfOutput.dense", "modeling_longskim.LongSkimformerSelfOutput.dropout", "modeling_longskim.LongSkimformerSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerAttention.__init__": [[1150, 1155], ["torch.Module.__init__", "modeling_longskim.LongSkimformerSelfAttention", "modeling_longskim.LongSkimformerSelfOutput", "set"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "LongSkimformerSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "LongSkimformerSelfOutput", "(", "config", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerAttention.prune_heads": [[1156, 1171], ["transformers.modeling_utils.find_pruneable_heads_and_indices", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "modeling_longskim.LongSkimformerAttention.pruned_heads.union", "len", "len"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "heads", ",", "index", "=", "find_pruneable_heads_and_indices", "(", "\n", "heads", ",", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ",", "self", ".", "pruned_heads", "\n", ")", "\n", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "\n", "# Update hyper params and store pruned heads", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerAttention.forward": [[1172, 1196], ["modeling_longskim.LongSkimformerAttention.self", "modeling_longskim.LongSkimformerAttention.output", "len", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "range"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attn_probs", ",", "\n", "layer_head_mask", "=", "None", ",", "\n", "is_index_masked", "=", "None", ",", "\n", "is_index_global_attn", "=", "None", ",", "\n", "is_global_attn", "=", "None", ",", "\n", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "pruned_heads", ")", ">", "0", ":", "\n", "            ", "num_attention_heads", "=", "attn_probs", ".", "shape", "[", "1", "]", "\n", "indices", "=", "[", "idx", "for", "idx", "in", "range", "(", "num_attention_heads", ")", "if", "idx", "not", "in", "self", ".", "pruned_heads", "]", "\n", "attn_probs", "=", "torch", ".", "index_select", "(", "attn_probs", ",", "1", ",", "indices", ")", "\n", "\n", "", "self_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "\n", "attn_probs", ",", "\n", "layer_head_mask", "=", "layer_head_mask", ",", "\n", "is_index_masked", "=", "is_index_masked", ",", "\n", "is_index_global_attn", "=", "is_index_global_attn", ",", "\n", "is_global_attn", "=", "is_global_attn", ",", "\n", ")", "\n", "attn_output", "=", "self", ".", "output", "(", "self_outputs", "[", "0", "]", ",", "hidden_states", ")", "\n", "return", "attn_output", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerIntermediate.__init__": [[1200, 1207], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerIntermediate.forward": [[1208, 1212], ["modeling_longskim.LongSkimformerIntermediate.dense", "modeling_longskim.LongSkimformerIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerOutput.__init__": [[1216, 1221], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerOutput.forward": [[1222, 1227], ["modeling_longskim.LongSkimformerOutput.dense", "modeling_longskim.LongSkimformerOutput.dropout", "modeling_longskim.LongSkimformerOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerLayer.__init__": [[1230, 1237], ["torch.Module.__init__", "modeling_longskim.LongSkimformerAttention", "modeling_longskim.LongSkimformerIntermediate", "modeling_longskim.LongSkimformerOutput"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "LongSkimformerAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "LongSkimformerIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "LongSkimformerOutput", "(", "config", ")", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerLayer.forward": [[1238, 1260], ["modeling_longskim.LongSkimformerLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attn_probs", ",", "\n", "layer_head_mask", "=", "None", ",", "\n", "is_index_masked", "=", "None", ",", "\n", "is_index_global_attn", "=", "None", ",", "\n", "is_global_attn", "=", "None", ",", "\n", ")", ":", "\n", "        ", "attn_output", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attn_probs", ",", "\n", "layer_head_mask", "=", "layer_head_mask", ",", "\n", "is_index_masked", "=", "is_index_masked", ",", "\n", "is_index_global_attn", "=", "is_index_global_attn", ",", "\n", "is_global_attn", "=", "is_global_attn", ",", "\n", ")", "\n", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "ff_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attn_output", "\n", ")", "\n", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerLayer.ff_chunk": [[1261, 1265], ["modeling_longskim.LongSkimformerLayer.intermediate", "modeling_longskim.LongSkimformerLayer.output"], "methods", ["None"], ["", "def", "ff_chunk", "(", "self", ",", "attn_output", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attn_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attn_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerEncoder.__init__": [[1268, 1272], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "modeling_longskim.LongSkimformerLayer", "range"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "LongSkimformerLayer", "(", "config", ")", "for", "i", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerEncoder.forward": [[1273, 1333], ["enumerate", "modeling_longskim.LongSkimformerBaseModelOutput", "tuple", "len", "getattr", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "layer_module", "head_mask.size", "len", "modeling_longskim.LongSkimformerEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attn_probs", ",", "\n", "head_mask", "=", "None", ",", "\n", "is_index_masked", "=", "None", ",", "\n", "is_index_global_attn", "=", "None", ",", "\n", "is_global_attn", "=", "None", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "return_dict", "=", "True", ",", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "\n", "# check if head_mask has a correct number of layers specified if desired", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "assert", "head_mask", ".", "size", "(", ")", "[", "0", "]", "==", "(", "\n", "len", "(", "self", ".", "layer", ")", "\n", ")", ",", "f\"The head_mask should be specified for {len(self.layer)} layers, but it is for {head_mask.size()[0]}.\"", "\n", "", "for", "idx", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "and", "self", ".", "training", ":", "\n", "\n", "                ", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "is_global_attn", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attn_probs", ",", "\n", "head_mask", "[", "idx", "]", "if", "head_mask", "is", "not", "None", "else", "None", ",", "\n", "is_index_masked", ",", "\n", "is_index_global_attn", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attn_probs", ",", "\n", "layer_head_mask", "=", "head_mask", "[", "idx", "]", "if", "head_mask", "is", "not", "None", "else", "None", ",", "\n", "is_index_masked", "=", "is_index_masked", ",", "\n", "is_index_global_attn", "=", "is_index_global_attn", ",", "\n", "is_global_attn", "=", "is_global_attn", ",", "\n", ")", "\n", "", "hidden_states", "=", "layer_outputs", "\n", "\n", "# Add last layer", "\n", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "for", "v", "in", "[", "hidden_states", ",", "all_hidden_states", "]", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "LongSkimformerBaseModelOutput", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerPooler.__init__": [[1337, 1341], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerPooler.forward": [[1342, 1349], ["modeling_longskim.LongSkimformerPooler.dense", "modeling_longskim.LongSkimformerPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerLMHead.__init__": [[1355, 1365], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "layer_norm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "vocab_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "config", ".", "vocab_size", ")", ")", "\n", "\n", "# Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`", "\n", "self", ".", "decoder", ".", "bias", "=", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerLMHead.forward": [[1366, 1375], ["modeling_longskim.LongSkimformerLMHead.dense", "transformers.activations.gelu", "modeling_longskim.LongSkimformerLMHead.layer_norm", "modeling_longskim.LongSkimformerLMHead.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "self", ".", "dense", "(", "features", ")", "\n", "x", "=", "gelu", "(", "x", ")", "\n", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "# project back to size of vocabulary with bias", "\n", "x", "=", "self", ".", "decoder", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerPreTrainedModel._init_weights": [[1387, 1398], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerModel.__init__": [[1402, 1437], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_longskim.LongSkimformerTextEmbeddings", "modeling_longskim.LongSkimformer2DPositionEmbeddings", "modeling_longskim.LongSkimAttention", "modeling_longskim.LongSkimformerEncoder", "modeling_longskim.LongSkimformerModel.init_weights", "transformers.models.longformer.modeling_longformer.LongformerEncoder", "modeling_longskim.LongSkimformerPooler", "transformers.models.longformer.configuration_longformer.LongformerConfig"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "assert", "config", ".", "attention_window", "%", "2", "==", "0", ",", "\"`config.attention_window` has to be an even value\"", "\n", "assert", "config", ".", "attention_window", ">", "0", ",", "\"`config.attention_window` has to be positive\"", "\n", "\n", "self", ".", "text_embeddings", "=", "LongSkimformerTextEmbeddings", "(", "config", ")", "\n", "self", ".", "two_dim_pos_embeddings", "=", "LongSkimformer2DPositionEmbeddings", "(", "config", ")", "\n", "\n", "self", ".", "contextualize_2d_positions", "=", "config", ".", "contextualize_2d_positions", "\n", "if", "self", ".", "contextualize_2d_positions", ":", "\n", "            ", "self", ".", "layout_encoder", "=", "LongformerEncoder", "(", "\n", "LongformerConfig", "(", "\n", "hidden_size", "=", "config", ".", "hidden_layout_size", ",", "\n", "num_hidden_layers", "=", "config", ".", "num_hidden_layers_layout_encoder", ",", "\n", "num_attention_heads", "=", "config", ".", "num_attention_heads_layout_encoder", ",", "\n", "intermediate_size", "=", "config", ".", "intermediate_size", ",", "\n", "hidden_act", "=", "config", ".", "hidden_act", ",", "\n", "hidden_dropout_prob", "=", "config", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "config", ".", "attention_probs_dropout_prob", ",", "\n", "max_position_embeddings", "=", "config", ".", "max_2d_position_embeddings", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "layer_norm_eps", "=", "config", ".", "layer_norm_eps", ",", "\n", "gradient_checkpointing", "=", "config", ".", "gradient_checkpointing", ",", "\n", "attention_window", "=", "[", "config", ".", "attention_window", "]", "*", "config", ".", "num_hidden_layers_layout_encoder", ",", "\n", "sep_token_id", "=", "config", ".", "sep_token_id", ",", "\n", ")", "\n", ")", "\n", "\n", "", "self", ".", "skim_attention", "=", "LongSkimAttention", "(", "config", ")", "\n", "self", ".", "encoder", "=", "LongSkimformerEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "LongSkimformerPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerModel.get_input_embeddings": [[1438, 1440], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "text_embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerModel.set_input_embeddings": [[1441, 1443], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "text_embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerModel._prune_heads": [[1444, 1451], ["heads_to_prune.items", "modeling_longskim.LongSkimformerModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerModel._pad_to_window_size": [[1452, 1498], ["logger.info", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.nn.functional.pad", "torch.cat.new_full", "torch.cat.new_full", "torch.cat.new_full", "modeling_longskim.LongSkimformerModel.embeddings", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "", "def", "_pad_to_window_size", "(", "\n", "self", ",", "\n", "input_ids", ":", "torch", ".", "Tensor", ",", "\n", "bbox", ":", "torch", ".", "Tensor", ",", "\n", "attention_mask", ":", "torch", ".", "Tensor", ",", "\n", "token_type_ids", ":", "torch", ".", "Tensor", ",", "\n", "position_ids", ":", "torch", ".", "Tensor", ",", "\n", "inputs_embeds", ":", "torch", ".", "Tensor", ",", "\n", "pad_token_id", ":", "int", ",", "\n", "pad_token_bbox_value", ":", "int", ",", "\n", ")", ":", "\n", "        ", "\"\"\"A helper function to pad tokens and mask to work with implementation of Longformer self-attention.\"\"\"", "\n", "# padding", "\n", "attention_window", "=", "self", ".", "config", ".", "attention_window", "\n", "\n", "assert", "attention_window", "%", "2", "==", "0", ",", "f\"`attention_window` should be an even value. Given {attention_window}\"", "\n", "input_shape", "=", "input_ids", ".", "shape", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "shape", "\n", "batch_size", ",", "seq_len", "=", "input_shape", "[", ":", "2", "]", "\n", "\n", "padding_len", "=", "(", "attention_window", "-", "seq_len", "%", "attention_window", ")", "%", "attention_window", "\n", "if", "padding_len", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Input ids are automatically padded from {} to {} to be a multiple of `config.attention_window`: {}\"", ".", "format", "(", "\n", "seq_len", ",", "seq_len", "+", "padding_len", ",", "attention_window", "\n", ")", "\n", ")", "\n", "if", "input_ids", "is", "not", "None", ":", "\n", "                ", "input_ids", "=", "F", ".", "pad", "(", "input_ids", ",", "(", "0", ",", "padding_len", ")", ",", "value", "=", "pad_token_id", ")", "\n", "", "if", "bbox", "is", "not", "None", ":", "\n", "                ", "bbox", "=", "F", ".", "pad", "(", "bbox", ",", "(", "0", ",", "0", ",", "0", ",", "padding_len", ")", ",", "value", "=", "pad_token_bbox_value", ")", "\n", "", "if", "position_ids", "is", "not", "None", ":", "\n", "# pad with position_id = pad_token_id as in modeling_roberta.RobertaEmbeddings", "\n", "                ", "position_ids", "=", "F", ".", "pad", "(", "position_ids", ",", "(", "0", ",", "padding_len", ")", ",", "value", "=", "pad_token_bbox_value", ")", "\n", "", "if", "inputs_embeds", "is", "not", "None", ":", "\n", "                ", "input_ids_padding", "=", "inputs_embeds", ".", "new_full", "(", "\n", "(", "batch_size", ",", "padding_len", ")", ",", "\n", "self", ".", "config", ".", "pad_token_id", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", ")", "\n", "inputs_embeds_padding", "=", "self", ".", "embeddings", "(", "input_ids_padding", ")", "\n", "inputs_embeds", "=", "torch", ".", "cat", "(", "[", "inputs_embeds", ",", "inputs_embeds_padding", "]", ",", "dim", "=", "-", "2", ")", "\n", "\n", "", "attention_mask", "=", "F", ".", "pad", "(", "attention_mask", ",", "(", "0", ",", "padding_len", ")", ",", "value", "=", "False", ")", "# no attention on the padding tokens", "\n", "token_type_ids", "=", "F", ".", "pad", "(", "token_type_ids", ",", "(", "0", ",", "padding_len", ")", ",", "value", "=", "0", ")", "# pad with token_type_id = 0", "\n", "\n", "", "return", "padding_len", ",", "input_ids", ",", "bbox", ",", "attention_mask", ",", "token_type_ids", ",", "position_ids", ",", "inputs_embeds", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerModel._merge_to_attention_mask": [[1499, 1510], ["None"], "methods", ["None"], ["", "def", "_merge_to_attention_mask", "(", "self", ",", "attention_mask", ":", "torch", ".", "Tensor", ",", "global_attention_mask", ":", "torch", ".", "Tensor", ")", ":", "\n", "# longformer self attention expects attention mask to have 0 (no attn), 1 (local attn), 2 (global attn)", "\n", "# (global_attention_mask + 1) => 1 for local attention, 2 for global attention", "\n", "# => final attention_mask => 0 for no attention, 1 for local attention 2 for global attention", "\n", "        ", "if", "attention_mask", "is", "not", "None", ":", "\n", "            ", "attention_mask", "=", "attention_mask", "*", "(", "global_attention_mask", "+", "1", ")", "\n", "", "else", ":", "\n", "# simply use `global_attention_mask` as `attention_mask`", "\n", "# if no `attention_mask` is given", "\n", "            ", "attention_mask", "=", "global_attention_mask", "+", "1", "\n", "", "return", "attention_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerModel.forward": [[1511, 1662], ["modeling_longskim.LongSkimformerModel._pad_to_window_size", "modeling_longskim.LongSkimformerModel.text_embeddings", "modeling_longskim.LongSkimformerModel.two_dim_pos_embeddings", "is_index_global_attn.flatten().any().item", "modeling_longskim.LongSkimformerModel.skim_attention", "modeling_longskim.LongSkimformerModel.encoder", "modeling_longskim.LongSkimformerBaseModelOutputWithPooling", "ValueError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "modeling_longskim.LongSkimformerModel._merge_to_attention_mask", "modeling_longskim.LongSkimformerModel.get_extended_attention_mask", "modeling_longskim.LongSkimformerModel.pooler", "input_ids.size", "tuple", "modeling_longskim.LongSkimformerModel.layout_encoder", "is_index_global_attn.flatten().any", "ValueError", "isinstance", "inputs_embeds.size", "list", "is_index_global_attn.flatten"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerModel._pad_to_window_size", "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerModel._merge_to_attention_mask"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "bbox", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "global_attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n\n        Returns:\n\n        Examples::\n\n            >>> import torch\n            >>> from transformers import LongformerModel, LongformerTokenizer\n\n            >>> model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n            >>> tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n\n            >>> SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document\n            >>> input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1\n\n            >>> # Attention mask values -- 0: no attention, 1: local attention, 2: global attention\n            >>> attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention\n            >>> global_attention_mask = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to global attention to be deactivated for all tokens\n            >>> global_attention_mask[:, [1, 4, 21,]] = 1  # Set global attention to random tokens for the sake of this example\n            ...                                     # Usually, set global attention based on the task. For example,\n            ...                                     # classification: the <s> token\n            ...                                     # QA: question tokens\n            ...                                     # LM: potentially on the beginning of sentences and paragraphs\n            >>> outputs = model(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n            >>> sequence_output = outputs.last_hidden_state\n            >>> pooled_output = outputs.pooler_output\n        \"\"\"", "\n", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "input_shape", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "", "if", "bbox", "is", "None", ":", "\n", "            ", "bbox", "=", "torch", ".", "zeros", "(", "tuple", "(", "list", "(", "input_shape", ")", "+", "[", "4", "]", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# merge `global_attention_mask` and `attention_mask`", "\n", "", "if", "global_attention_mask", "is", "not", "None", ":", "\n", "            ", "attention_mask", "=", "self", ".", "_merge_to_attention_mask", "(", "attention_mask", ",", "global_attention_mask", ")", "\n", "\n", "", "padding_len", ",", "input_ids", ",", "bbox", ",", "attention_mask", ",", "token_type_ids", ",", "position_ids", ",", "inputs_embeds", "=", "self", ".", "_pad_to_window_size", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "bbox", "=", "bbox", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "pad_token_id", "=", "self", ".", "config", ".", "pad_token_id", ",", "\n", "pad_token_bbox_value", "=", "self", ".", "config", ".", "pad_token_bbox_value", ",", "\n", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "[", "\n", ":", ",", "0", ",", "0", ",", ":", "\n", "]", "\n", "\n", "text_embedding_output", "=", "self", ".", "text_embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "inputs_embeds", "=", "inputs_embeds", "\n", ")", "\n", "\n", "pos_embedding_output", "=", "self", ".", "two_dim_pos_embeddings", "(", "\n", "bbox", "=", "bbox", ",", "\n", ")", "\n", "if", "self", ".", "contextualize_2d_positions", ":", "\n", "            ", "pos_embedding_output", "=", "self", ".", "layout_encoder", "(", "\n", "hidden_states", "=", "pos_embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", ")", "[", "0", "]", "\n", "\n", "", "is_index_masked", "=", "extended_attention_mask", "<", "0", "\n", "is_index_global_attn", "=", "extended_attention_mask", ">", "0", "\n", "is_global_attn", "=", "is_index_global_attn", ".", "flatten", "(", ")", ".", "any", "(", ")", ".", "item", "(", ")", "\n", "\n", "# compute attention probs once ", "\n", "skim_attn_output", "=", "self", ".", "skim_attention", "(", "\n", "pos_embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "is_index_masked", "=", "is_index_masked", ",", "\n", "is_index_global_attn", "=", "is_index_global_attn", ",", "\n", "is_global_attn", "=", "is_global_attn", ",", "\n", ")", "\n", "# reuse them throughout each layer", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "text_embedding_output", ",", "\n", "skim_attn_output", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "is_index_masked", "=", "is_index_masked", ",", "\n", "is_index_global_attn", "=", "is_index_global_attn", ",", "\n", "is_global_attn", "=", "is_global_attn", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "# undo padding", "\n", "if", "padding_len", ">", "0", ":", "\n", "# unpad `sequence_output` because the calling function is expecting a length == input_ids.size(1)", "\n", "            ", "sequence_output", "=", "sequence_output", "[", ":", ",", ":", "-", "padding_len", "]", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "outputs", "=", "(", "sequence_output", ",", "pooled_output", ")", "\n", "if", "output_attentions", ":", "\n", "                ", "outputs", "=", "outputs", "+", "skim_attn_output", "if", "isinstance", "(", "skim_attn_output", ",", "tuple", ")", "else", "outputs", "+", "(", "skim_attn_output", ",", ")", "\n", "", "if", "output_hidden_states", ":", "\n", "                ", "outputs", "=", "outputs", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "", "return", "outputs", "\n", "\n", "", "if", "is_global_attn", ":", "\n", "            ", "attn_probs", ",", "global_attn_probs", "=", "skim_attn_output", "\n", "", "else", ":", "\n", "            ", "attn_probs", "=", "skim_attn_output", "\n", "global_attn_probs", "=", "None", "\n", "\n", "", "return", "LongSkimformerBaseModelOutputWithPooling", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "attn_probs", "if", "output_attentions", "else", "None", ",", "\n", "global_attentions", "=", "global_attn_probs", "if", "output_attentions", "else", "None", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerForMaskedLM.__init__": [[1668, 1675], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_longskim.LongSkimformerModel", "modeling_longskim.LongSkimformerLMHead", "modeling_longskim.LongSkimformerForMaskedLM.init_weights"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "longskimformer", "=", "LongSkimformerModel", "(", "config", ",", "add_pooling_layer", "=", "False", ")", "\n", "self", ".", "lm_head", "=", "LongSkimformerLMHead", "(", "config", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerForMaskedLM.get_output_embeddings": [[1676, 1678], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerForMaskedLM.set_output_embeddings": [[1679, 1681], ["None"], "methods", ["None"], ["", "def", "set_output_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "lm_head", ".", "decoder", "=", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerForMaskedLM.forward": [[1682, 1756], ["modeling_longskim.LongSkimformerForMaskedLM.longskimformer", "modeling_longskim.LongSkimformerForMaskedLM.lm_head", "modeling_longskim.LongSkimformerMaskedLMOutput", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_longskim.LongSkimformerForMaskedLM.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "bbox", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "global_attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n            Used to hide legacy arguments that have been deprecated.\n\n        Returns:\n\n        Examples::\n\n            >>> import torch\n            >>> from transformers import LongformerForMaskedLM, LongformerTokenizer\n\n            >>> model = LongformerForMaskedLM.from_pretrained('allenai/longformer-base-4096')\n            >>> tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n\n            >>> SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document\n            >>> input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1\n\n            >>> attention_mask = None  # default is local attention everywhere, which is a good choice for MaskedLM\n            ...                        # check ``LongformerModel.forward`` for more details how to set `attention_mask`\n            >>> outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n            >>> loss = outputs.loss\n            >>> prediction_logits = output.logits\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "longskimformer", "(", "\n", "input_ids", ",", "\n", "bbox", "=", "bbox", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "global_attention_mask", "=", "global_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "prediction_scores", "=", "self", ".", "lm_head", "(", "sequence_output", ")", "\n", "\n", "masked_lm_loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "prediction_scores", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "masked_lm_loss", ",", ")", "+", "output", ")", "if", "masked_lm_loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "LongSkimformerMaskedLMOutput", "(", "\n", "loss", "=", "masked_lm_loss", ",", "\n", "logits", "=", "prediction_scores", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerForTokenClassification.__init__": [[1763, 1772], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_longskim.LongSkimformerModel", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "modeling_longskim.LongSkimformerForTokenClassification.init_weights"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "longskimformer", "=", "LongSkimformerModel", "(", "config", ",", "add_pooling_layer", "=", "False", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.skim_attention.modeling_longskim.LongSkimformerForTokenClassification.forward": [[1773, 1837], ["modeling_longskim.LongSkimformerForTokenClassification.longskimformer", "modeling_longskim.LongSkimformerForTokenClassification.dropout", "modeling_longskim.LongSkimformerForTokenClassification.classifier", "modeling_longskim.LongSkimformerTokenClassifierOutput", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "modeling_longskim.LongSkimformerForTokenClassification.view", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "labels.view", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "modeling_longskim.LongSkimformerForTokenClassification.view", "labels.view", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "bbox", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "global_attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n            1]``.\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "outputs", "=", "self", ".", "longskimformer", "(", "\n", "input_ids", ",", "\n", "bbox", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "global_attention_mask", "=", "global_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "\n", "active_labels", "=", "torch", ".", "where", "(", "\n", "active_loss", ",", "labels", ".", "view", "(", "-", "1", ")", ",", "torch", ".", "tensor", "(", "loss_fct", ".", "ignore_index", ")", ".", "type_as", "(", "labels", ")", "\n", ")", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "LongSkimformerTokenClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.data.data_collator.DataCollatorForMaskedLM.__post_init__": [[99, 103], ["ValueError"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "tokenizer", ".", "mask_token", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"This tokenizer does not have a mask token which is necessary for masked language modeling. \"", "\n", "\"You should pass `mlm=False` to train on causal language modeling instead.\"", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.data.data_collator.DataCollatorForMaskedLM.__call__": [[106, 156], ["isinstance", "data_collator.DataCollatorForMaskedLM.pop", "data_collator.DataCollatorForMaskedLM.mask_tokens", "data_collator.DataCollatorForMaskedLM.tokenizer.pad", "data_collator._collate_batch", "torch.tensor", "len", "data_collator.DataCollatorForMaskedLM.keys", "data_collator.DataCollatorForMaskedLM.items", "torch.tensor", "len", "len"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.data.data_collator.DataCollatorForMaskedLM.mask_tokens", "home.repos.pwc.inspect_result.recitalai_skim-attention.data.data_collator._collate_batch"], ["", "", "def", "__call__", "(", "\n", "self", ",", "\n", "examples", ":", "List", "[", "Union", "[", "\n", "List", "[", "int", "]", ",", "\n", "torch", ".", "Tensor", ",", "\n", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", "\n", "]", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# Handle dict or lists with proper padding and conversion to tensor.", "\n", "        ", "if", "isinstance", "(", "examples", "[", "0", "]", ",", "(", "dict", ",", "BatchEncoding", ")", ")", ":", "\n", "            ", "batch", "=", "self", ".", "tokenizer", ".", "pad", "(", "\n", "examples", ",", "\n", "padding", "=", "self", ".", "padding", ",", "\n", "max_length", "=", "self", ".", "max_length", ",", "\n", "pad_to_multiple_of", "=", "self", ".", "pad_to_multiple_of", "\n", ")", "\n", "\n", "has_bbox_input", "=", "\"bbox\"", "in", "batch", ".", "keys", "(", ")", "and", "batch", "[", "\"bbox\"", "]", "is", "not", "None", "\n", "if", "has_bbox_input", ":", "\n", "                ", "sequence_length", "=", "torch", ".", "tensor", "(", "batch", "[", "\"input_ids\"", "]", ")", ".", "shape", "[", "1", "]", "\n", "padding_side", "=", "self", ".", "tokenizer", ".", "padding_side", "\n", "if", "padding_side", "==", "\"right\"", ":", "\n", "                    ", "batch", "[", "\"bbox\"", "]", "=", "[", "bbox", "+", "[", "[", "self", ".", "pad_token_bbox_value", "]", "*", "4", "]", "*", "(", "sequence_length", "-", "len", "(", "bbox", ")", ")", "for", "bbox", "in", "batch", "[", "\"bbox\"", "]", "]", "\n", "", "else", ":", "\n", "                    ", "batch", "[", "\"bbox\"", "]", "=", "[", "[", "[", "self", ".", "pad_token_bbox_value", "]", "*", "4", "]", "*", "(", "sequence_length", "-", "len", "(", "bbox", ")", ")", "+", "bbox", "for", "bbox", "in", "batch", "[", "\"bbox\"", "]", "]", "\n", "\n", "", "", "batch", "=", "{", "k", ":", "torch", ".", "tensor", "(", "v", ",", "dtype", "=", "torch", ".", "int64", ")", "for", "k", ",", "v", "in", "batch", ".", "items", "(", ")", "}", "\n", "\n", "", "else", ":", "\n", "            ", "collate_output", "=", "_collate_batch", "(", "\n", "examples", ",", "\n", "self", ".", "tokenizer", ",", "\n", "pad_to_multiple_of", "=", "self", ".", "pad_to_multiple_of", ",", "\n", "pad_token_bbox_value", "=", "self", ".", "pad_token_bbox_value", "\n", ")", "\n", "batch", "=", "{", "\"input_ids\"", ":", "collate_output", "[", "0", "]", "}", "\n", "if", "len", "(", "collate_output", ")", "==", "2", ":", "\n", "                ", "batch", "[", "\"bbox\"", "]", "=", "collate_output", "[", "1", "]", "\n", "\n", "", "", "if", "self", ".", "assign_same_bbox", ":", "\n", "            ", "batch", "[", "\"bbox\"", "]", "=", "None", "\n", "\n", "# If special token mask has been preprocessed, pop it from the dict.", "\n", "", "special_tokens_mask", "=", "batch", ".", "pop", "(", "\"special_tokens_mask\"", ",", "None", ")", "\n", "batch", "[", "\"input_ids\"", "]", ",", "batch", "[", "\"labels\"", "]", "=", "self", ".", "mask_tokens", "(", "\n", "batch", "[", "\"input_ids\"", "]", ",", "special_tokens_mask", "=", "special_tokens_mask", "\n", ")", "\n", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.data.data_collator.DataCollatorForMaskedLM.mask_tokens": [[157, 189], ["inputs.clone", "torch.full", "torch.full.masked_fill_", "torch.bernoulli().bool", "data_collator.DataCollatorForMaskedLM.tokenizer.convert_tokens_to_ids", "torch.randint", "torch.tensor", "special_tokens_mask.bool.bool.bool", "torch.bernoulli().bool", "len", "data_collator.DataCollatorForMaskedLM.tokenizer.get_special_tokens_mask", "torch.bernoulli", "torch.bernoulli().bool", "inputs.clone.tolist", "torch.bernoulli", "torch.full", "torch.bernoulli", "torch.full"], "methods", ["None"], ["", "def", "mask_tokens", "(", "\n", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "special_tokens_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n        \"\"\"", "\n", "labels", "=", "inputs", ".", "clone", "(", ")", "\n", "# We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)", "\n", "probability_matrix", "=", "torch", ".", "full", "(", "labels", ".", "shape", ",", "self", ".", "mlm_probability", ")", "\n", "if", "special_tokens_mask", "is", "None", ":", "\n", "            ", "special_tokens_mask", "=", "[", "\n", "self", ".", "tokenizer", ".", "get_special_tokens_mask", "(", "val", ",", "already_has_special_tokens", "=", "True", ")", "for", "val", "in", "labels", ".", "tolist", "(", ")", "\n", "]", "\n", "special_tokens_mask", "=", "torch", ".", "tensor", "(", "special_tokens_mask", ",", "dtype", "=", "torch", ".", "bool", ")", "\n", "", "else", ":", "\n", "            ", "special_tokens_mask", "=", "special_tokens_mask", ".", "bool", "(", ")", "\n", "\n", "", "probability_matrix", ".", "masked_fill_", "(", "special_tokens_mask", ",", "value", "=", "0.0", ")", "\n", "masked_indices", "=", "torch", ".", "bernoulli", "(", "probability_matrix", ")", ".", "bool", "(", ")", "\n", "labels", "[", "~", "masked_indices", "]", "=", "-", "100", "# We only compute loss on masked tokens", "\n", "\n", "# 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])", "\n", "indices_replaced", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.8", ")", ")", ".", "bool", "(", ")", "&", "masked_indices", "\n", "inputs", "[", "indices_replaced", "]", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "mask_token", ")", "\n", "\n", "# 10% of the time, we replace masked input tokens with random word", "\n", "indices_random", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "labels", ".", "shape", ",", "0.5", ")", ")", ".", "bool", "(", ")", "&", "masked_indices", "&", "~", "indices_replaced", "\n", "random_words", "=", "torch", ".", "randint", "(", "len", "(", "self", ".", "tokenizer", ")", ",", "labels", ".", "shape", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "inputs", "[", "indices_random", "]", "=", "random_words", "[", "indices_random", "]", "\n", "\n", "# The rest of the time (10% of the time) we keep the masked input tokens unchanged", "\n", "return", "inputs", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.data.data_collator.DataCollatorForTokenClassification.__call__": [[228, 283], ["data_collator.DataCollatorForTokenClassification.tokenizer.pad", "torch.tensor", "torch.tensor", "features[].keys", "features[].keys", "token_ids.count", "attention_mask_2d.append", "attention_mask_2d[].new_full", "range", "attention_mask_2d[].new_full", "range", "data_collator.DataCollatorForTokenClassification.items", "torch.ones", "len", "len", "len", "len", "len", "len"], "methods", ["None"], ["def", "__call__", "(", "\n", "self", ",", "\n", "features", ":", "List", "[", "Union", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", "]", "]", "\n", ")", ":", "\n", "        ", "label_name", "=", "\"label\"", "if", "\"label\"", "in", "features", "[", "0", "]", ".", "keys", "(", ")", "else", "\"labels\"", "\n", "labels", "=", "[", "feature", "[", "label_name", "]", "for", "feature", "in", "features", "]", "if", "label_name", "in", "features", "[", "0", "]", ".", "keys", "(", ")", "else", "None", "\n", "\n", "has_bbox_input", "=", "\"bbox\"", "in", "features", "[", "0", "]", "\n", "\n", "batch", "=", "self", ".", "tokenizer", ".", "pad", "(", "\n", "features", ",", "\n", "padding", "=", "self", ".", "padding", ",", "\n", "max_length", "=", "self", ".", "max_length", ",", "\n", "pad_to_multiple_of", "=", "self", ".", "pad_to_multiple_of", ",", "\n", "# Conversion to tensors will fail if we have labels as they are not of the same length yet.", "\n", "return_tensors", "=", "\"pt\"", "if", "labels", "is", "None", "else", "None", ",", "\n", ")", "\n", "\n", "if", "self", ".", "use_2d_attn_mask", ":", "\n", "            ", "attention_mask_2d", "=", "[", "]", "\n", "for", "token_ids", "in", "batch", "[", "\"input_ids\"", "]", ":", "\n", "                ", "num_padding", "=", "token_ids", ".", "count", "(", "self", ".", "tokenizer", ".", "pad_token_id", ")", "\n", "attention_mask_2d", ".", "append", "(", "\n", "torch", ".", "ones", "(", "(", "len", "(", "token_ids", ")", "-", "num_padding", ",", "len", "(", "token_ids", ")", "-", "num_padding", ")", ")", "\n", ")", "\n", "\n", "\n", "", "", "batch_size", ",", "sequence_length", "=", "torch", ".", "tensor", "(", "batch", "[", "\"input_ids\"", "]", ")", ".", "shape", "\n", "padding_side", "=", "self", ".", "tokenizer", ".", "padding_side", "\n", "if", "padding_side", "==", "\"right\"", ":", "\n", "            ", "if", "labels", ":", "\n", "                ", "batch", "[", "\"labels\"", "]", "=", "[", "label", "+", "[", "self", ".", "label_pad_token_id", "]", "*", "(", "sequence_length", "-", "len", "(", "label", ")", ")", "for", "label", "in", "labels", "]", "\n", "", "if", "has_bbox_input", ":", "\n", "                ", "batch", "[", "\"bbox\"", "]", "=", "[", "bbox", "+", "[", "[", "self", ".", "pad_token_bbox_value", "]", "*", "4", "]", "*", "(", "sequence_length", "-", "len", "(", "bbox", ")", ")", "for", "bbox", "in", "batch", "[", "\"bbox\"", "]", "]", "\n", "", "if", "self", ".", "use_2d_attn_mask", ":", "\n", "                ", "batch_2d_mask", "=", "attention_mask_2d", "[", "0", "]", ".", "new_full", "(", "[", "batch_size", ",", "self", ".", "max_length", ",", "self", ".", "max_length", "]", ",", "0", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "batch_2d_mask", "[", "i", ",", ":", "attention_mask_2d", "[", "i", "]", ".", "shape", "[", "0", "]", ",", ":", "attention_mask_2d", "[", "i", "]", ".", "shape", "[", "0", "]", "]", "=", "attention_mask_2d", "[", "i", "]", "\n", "", "", "", "else", ":", "\n", "            ", "if", "labels", ":", "\n", "                ", "batch", "[", "\"labels\"", "]", "=", "[", "[", "self", ".", "label_pad_token_id", "]", "*", "(", "sequence_length", "-", "len", "(", "label", ")", ")", "+", "label", "for", "label", "in", "labels", "]", "\n", "", "if", "has_bbox_input", ":", "\n", "                ", "batch", "[", "\"bbox\"", "]", "=", "[", "[", "[", "self", ".", "pad_token_bbox_value", "]", "*", "4", "]", "*", "(", "sequence_length", "-", "len", "(", "bbox", ")", ")", "+", "bbox", "for", "bbox", "in", "batch", "[", "\"bbox\"", "]", "]", "\n", "", "if", "self", ".", "use_2d_attn_mask", ":", "\n", "                ", "batch_2d_mask", "=", "attention_mask_2d", "[", "0", "]", ".", "new_full", "(", "[", "batch_size", ",", "self", ".", "max_length", ",", "self", ".", "max_length", "]", ",", "0", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "batch_2d_mask", "[", "i", ",", "(", "self", ".", "max_length", "-", "attention_mask_2d", "[", "i", "]", ".", "shape", "[", "0", "]", ")", ":", ",", "(", "self", ".", "max_length", "-", "attention_mask_2d", "[", "i", "]", ".", "shape", "[", "0", "]", ")", ":", "]", "=", "attention_mask_2d", "[", "i", "]", "\n", "\n", "\n", "", "", "", "batch", "=", "{", "k", ":", "torch", ".", "tensor", "(", "v", ",", "dtype", "=", "torch", ".", "int64", ")", "for", "k", ",", "v", "in", "batch", ".", "items", "(", ")", "}", "\n", "\n", "if", "self", ".", "use_2d_attn_mask", ":", "\n", "            ", "batch", "[", "\"attention_mask\"", "]", "=", "batch_2d_mask", "\n", "\n", "", "return", "batch", "", "", "", ""]], "home.repos.pwc.inspect_result.recitalai_skim-attention.data.data_collator._collate_batch": [[9, 65], ["isinstance", "examples[].size", "all", "max", "examples[].new_full", "enumerate", "isinstance", "ValueError", "bboxes[].new_full", "torch.tensor", "torch.stack", "x.size", "len", "torch.tensor", "torch.tensor", "x.size", "len", "torch.stack"], "function", ["None"], ["def", "_collate_batch", "(", "\n", "examples", ",", "\n", "tokenizer", ",", "\n", "pad_to_multiple_of", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "pad_token_bbox_value", ":", "int", "=", "0", ",", "\n", ")", ":", "\n", "    ", "\"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"", "\n", "bboxes", "=", "None", "\n", "\n", "# Tensorize if necessary.", "\n", "if", "isinstance", "(", "examples", "[", "0", "]", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "examples", "=", "[", "\n", "torch", ".", "tensor", "(", "e", ",", "dtype", "=", "torch", ".", "long", ")", "for", "e", "in", "examples", "\n", "]", "\n", "", "elif", "isinstance", "(", "\n", "examples", "[", "0", "]", ",", "\n", "Union", "[", "Tuple", "[", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", "]", "\n", ")", ":", "\n", "        ", "examples", "=", "[", "torch", ".", "tensor", "(", "e", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "for", "e", "in", "examples", "]", "\n", "bboxes", "=", "[", "torch", ".", "tensor", "(", "e", "[", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", "for", "e", "in", "examples", "]", "\n", "\n", "# Check if padding is necessary.", "\n", "", "length_of_first", "=", "examples", "[", "0", "]", ".", "size", "(", "0", ")", "\n", "are_tensors_same_length", "=", "all", "(", "x", ".", "size", "(", "0", ")", "==", "length_of_first", "for", "x", "in", "examples", ")", "\n", "if", "are_tensors_same_length", "and", "(", "pad_to_multiple_of", "is", "None", "or", "length_of_first", "%", "pad_to_multiple_of", "==", "0", ")", ":", "\n", "        ", "output", "=", "(", "torch", ".", "stack", "(", "examples", ",", "dim", "=", "0", ")", ",", ")", "\n", "if", "bboxes", ":", "\n", "            ", "output", "=", "output", "+", "(", "torch", ".", "stack", "(", "bboxes", ",", "dim", "=", "0", ")", ",", ")", "\n", "", "return", "output", "\n", "\n", "# If yes, check if we have a `pad_token`.", "\n", "", "if", "tokenizer", ".", "_pad_token", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"You are attempting to pad samples but the tokenizer you are using\"", "\n", "f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"", "\n", ")", "\n", "\n", "# Creating the full tensor and filling it with our data.", "\n", "", "max_length", "=", "max", "(", "x", ".", "size", "(", "0", ")", "for", "x", "in", "examples", ")", "\n", "if", "pad_to_multiple_of", "is", "not", "None", "and", "(", "max_length", "%", "pad_to_multiple_of", "!=", "0", ")", ":", "\n", "        ", "max_length", "=", "(", "(", "max_length", "//", "pad_to_multiple_of", ")", "+", "1", ")", "*", "pad_to_multiple_of", "\n", "", "result", "=", "examples", "[", "0", "]", ".", "new_full", "(", "[", "len", "(", "examples", ")", ",", "max_length", "]", ",", "tokenizer", ".", "pad_token_id", ")", "\n", "if", "bboxes", ":", "\n", "        ", "bbox_result", "=", "bboxes", "[", "0", "]", ".", "new_full", "(", "[", "len", "(", "examples", ")", ",", "max_length", ",", "4", "]", ",", "pad_token_bbox_value", ")", "\n", "", "for", "i", ",", "example", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "if", "tokenizer", ".", "padding_side", "==", "\"right\"", ":", "\n", "            ", "result", "[", "i", ",", ":", "example", ".", "shape", "[", "0", "]", "]", "=", "example", "\n", "if", "bboxes", ":", "\n", "                ", "bbox_result", "[", "i", ",", ":", "bboxes", "[", "i", "]", ".", "shape", "[", "0", "]", "]", "=", "bboxes", "[", "i", "]", "\n", "", "", "else", ":", "\n", "            ", "result", "[", "i", ",", "-", "example", ".", "shape", "[", "0", "]", ":", "]", "=", "example", "\n", "if", "bboxes", ":", "\n", "                ", "bbox_result", "[", "i", ",", "-", "bboxes", ".", "shape", "[", "0", "]", ":", "]", "=", "bboxes", "[", "i", "]", "\n", "", "", "", "output", "=", "(", "result", ",", ")", "if", "not", "bboxes", "else", "(", "result", ",", "bbox_result", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.mix.MixConfig.__init__": [[13, 19], ["datasets.BuilderConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"BuilderConfig for MIX.\n        Args:\n          **kwargs: keyword arguments forwarded to super.\n        \"\"\"", "\n", "super", "(", "MixConfig", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.mix.MixDataset._info": [[27, 37], ["datasets.DatasetInfo", "datasets.Features", "datasets.Value", "datasets.Sequence", "datasets.Sequence", "datasets.Value", "datasets.Sequence", "datasets.Value"], "methods", ["None"], ["def", "_info", "(", "self", ")", ":", "\n", "        ", "return", "datasets", ".", "DatasetInfo", "(", "\n", "features", "=", "datasets", ".", "Features", "(", "\n", "{", "\n", "\"id\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"words\"", ":", "datasets", ".", "Sequence", "(", "datasets", ".", "Value", "(", "\"string\"", ")", ")", ",", "\n", "\"bboxes\"", ":", "datasets", ".", "Sequence", "(", "datasets", ".", "Sequence", "(", "datasets", ".", "Value", "(", "\"int64\"", ")", ")", ")", ",", "\n", "}", "\n", ")", ",", "\n", "supervised_keys", "=", "None", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.mix.MixDataset._split_generators": [[39, 59], ["datasets.SplitGenerator", "datasets.SplitGenerator", "datasets.SplitGenerator"], "methods", ["None"], ["", "def", "_split_generators", "(", "self", ",", "dl_manager", ")", ":", "\n", "        ", "\"\"\"Returns SplitGenerators.\"\"\"", "\n", "data_dir", "=", "dl_manager", ".", "manual_dir", "\n", "return", "[", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TRAIN", ",", "gen_kwargs", "=", "{", "\n", "\"index_path\"", ":", "f\"{data_dir}/indexed_files/train.txt\"", ",", "\n", "\"data_path\"", ":", "f\"{data_dir}/data\"", ",", "\n", "}", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "VALIDATION", ",", "gen_kwargs", "=", "{", "\n", "\"index_path\"", ":", "f\"{data_dir}/indexed_files/dev.txt\"", ",", "\n", "\"data_path\"", ":", "f\"{data_dir}/data\"", ",", "\n", "}", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TEST", ",", "gen_kwargs", "=", "{", "\n", "\"index_path\"", ":", "f\"{data_dir}/indexed_files/test.txt\"", ",", "\n", "\"data_path\"", ":", "f\"{data_dir}/data\"", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.mix.MixDataset._generate_examples": [[63, 89], ["logger.info", "tqdm.tqdm.tqdm", "open", "enumerate", "os.path.join", "line.rstrip", "open", "line.split", "words.append", "bboxes.append", "int", "str"], "methods", ["None"], ["", "def", "_generate_examples", "(", "self", ",", "index_path", ",", "data_path", ")", ":", "\n", "        ", "logger", ".", "info", "(", "f\"\u23f3 Generating examples from {data_path} using {index_path}\"", ")", "\n", "\n", "with", "open", "(", "index_path", ")", "as", "f_index", ":", "\n", "            ", "split_fnames", "=", "[", "line", ".", "rstrip", "(", ")", "for", "line", "in", "f_index", "]", "\n", "\n", "", "for", "guid", ",", "fname", "in", "tqdm", "(", "enumerate", "(", "split_fnames", ")", ",", "desc", "=", "f\"Reading files in {index_path}\"", ")", ":", "\n", "            ", "filepath", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "fname", ")", "\n", "\n", "words", "=", "[", "]", "\n", "bboxes", "=", "[", "]", "\n", "\n", "with", "open", "(", "filepath", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "for", "line", "in", "f", ":", "\n", "                    ", "splits", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "word", "=", "splits", "[", "0", "]", "\n", "bbox", "=", "splits", "[", "1", ":", "5", "]", "\n", "bbox", "=", "[", "int", "(", "b", ")", "for", "b", "in", "bbox", "]", "\n", "\n", "if", "word", "==", "\"##LTFigure##\"", "or", "word", "==", "\"##LTLine##\"", "or", "word", "==", "\"\"", ":", "\n", "                        ", "continue", "\n", "\n", "", "words", ".", "append", "(", "word", ")", "\n", "bboxes", ".", "append", "(", "bbox", ")", "# bbox is already normalized", "\n", "\n", "", "", "yield", "guid", ",", "{", "\"id\"", ":", "str", "(", "guid", ")", ",", "\"words\"", ":", "words", ",", "\"bboxes\"", ":", "bboxes", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__": [[13, 19], ["datasets.BuilderConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLAConfig.__init__"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"BuilderConfig for DocBankLA.\n        Args:\n          **kwargs: keyword arguments forwarded to super.\n        \"\"\"", "\n", "super", "(", "DocBankLAConfig", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLADataset._info": [[27, 56], ["datasets.DatasetInfo", "datasets.Features", "datasets.Value", "datasets.Sequence", "datasets.Sequence", "datasets.Sequence", "datasets.Value", "datasets.Sequence", "datasets.features.ClassLabel", "datasets.Value"], "methods", ["None"], ["def", "_info", "(", "self", ")", ":", "\n", "        ", "return", "datasets", ".", "DatasetInfo", "(", "\n", "features", "=", "datasets", ".", "Features", "(", "\n", "{", "\n", "\"id\"", ":", "datasets", ".", "Value", "(", "\"string\"", ")", ",", "\n", "\"words\"", ":", "datasets", ".", "Sequence", "(", "datasets", ".", "Value", "(", "\"string\"", ")", ")", ",", "\n", "\"bboxes\"", ":", "datasets", ".", "Sequence", "(", "datasets", ".", "Sequence", "(", "datasets", ".", "Value", "(", "\"int64\"", ")", ")", ")", ",", "\n", "\"tags\"", ":", "datasets", ".", "Sequence", "(", "\n", "datasets", ".", "features", ".", "ClassLabel", "(", "\n", "names", "=", "[", "\n", "\"abstract\"", ",", "\n", "\"author\"", ",", "\n", "\"caption\"", ",", "\n", "\"date\"", ",", "\n", "\"equation\"", ",", "\n", "\"figure\"", ",", "\n", "\"footer\"", ",", "\n", "\"list\"", ",", "\n", "\"paragraph\"", ",", "\n", "\"reference\"", ",", "\n", "\"section\"", ",", "\n", "\"table\"", ",", "\n", "\"title\"", ",", "\n", "]", "\n", ")", "\n", ")", ",", "\n", "}", "\n", ")", ",", "\n", "supervised_keys", "=", "None", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLADataset._split_generators": [[59, 77], ["os.path.basename", "os.path.normpath", "datasets.SplitGenerator", "datasets.SplitGenerator", "datasets.SplitGenerator"], "methods", ["None"], ["", "def", "_split_generators", "(", "self", ",", "dl_manager", ")", ":", "\n", "        ", "\"\"\"Returns SplitGenerators.\"\"\"", "\n", "data_dir", "=", "dl_manager", ".", "manual_dir", "\n", "data_dir_basename", "=", "os", ".", "path", ".", "basename", "(", "os", ".", "path", ".", "normpath", "(", "data_dir", ")", ")", "\n", "return", "[", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TRAIN", ",", "gen_kwargs", "=", "{", "\n", "\"data_path\"", ":", "f\"{data_dir}/{data_dir_basename}_train\"", "\n", "}", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "VALIDATION", ",", "gen_kwargs", "=", "{", "\n", "\"data_path\"", ":", "f\"{data_dir}/{data_dir_basename}_dev\"", "\n", "}", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TEST", ",", "gen_kwargs", "=", "{", "\n", "\"data_path\"", ":", "f\"{data_dir}/{data_dir_basename}_test\"", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.datasets.docbank.DocBankLADataset._generate_examples": [[81, 111], ["logger.info", "sorted", "tqdm.tqdm.tqdm", "os.listdir", "enumerate", "os.path.join", "open", "len", "len", "len", "len", "line.split", "splits[].rstrip", "words.append", "bboxes.append", "labels.append", "len", "int", "str"], "methods", ["None"], ["", "def", "_generate_examples", "(", "self", ",", "data_path", ")", ":", "\n", "        ", "logger", ".", "info", "(", "f\"\u23f3 Generating examples from {data_path}\"", ")", "\n", "filenames", "=", "sorted", "(", "os", ".", "listdir", "(", "data_path", ")", ")", "\n", "\n", "for", "guid", ",", "fname", "in", "tqdm", "(", "enumerate", "(", "filenames", ")", ",", "desc", "=", "f\"Reading files in {data_path}\"", ")", ":", "\n", "            ", "filepath", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "fname", ")", "\n", "words", "=", "[", "]", "\n", "bboxes", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "with", "open", "(", "filepath", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "for", "line", "in", "f", ":", "\n", "                    ", "splits", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "assert", "len", "(", "splits", ")", "==", "10", "\n", "word", "=", "splits", "[", "0", "]", "\n", "bbox", "=", "splits", "[", "1", ":", "5", "]", "\n", "label", "=", "splits", "[", "-", "1", "]", ".", "rstrip", "(", ")", "\n", "bbox", "=", "[", "int", "(", "b", ")", "for", "b", "in", "bbox", "]", "\n", "\n", "if", "word", "==", "\"##LTFigure##\"", "or", "word", "==", "\"##LTLine##\"", ":", "\n", "                        ", "continue", "\n", "\n", "", "words", ".", "append", "(", "word", ")", "\n", "bboxes", ".", "append", "(", "bbox", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "\n", "", "", "assert", "len", "(", "words", ")", "==", "len", "(", "bboxes", ")", "\n", "assert", "len", "(", "bboxes", ")", "==", "len", "(", "labels", ")", "\n", "\n", "yield", "guid", ",", "{", "\"id\"", ":", "str", "(", "guid", ")", ",", "\"words\"", ":", "words", ",", "\"bboxes\"", ":", "bboxes", ",", "\"tags\"", ":", "labels", "}", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.recitalai_skim-attention.experiments.run_layout_analysis.main": [[186, 711], ["transformers.HfArgumentParser", "logging.basicConfig", "logger.setLevel", "logger.warning", "transformers.trainer_utils.is_main_process", "logger.info", "transformers.set_seed", "datasets.load_dataset", "isinstance", "len", "model_args.model_type.lower", "model_args.core_model_type.lower", "transformers.AutoTokenizer.from_pretrained", "AutoModelForTokenClassification.from_config.resize_token_embeddings", "skim.data.DataCollatorForTokenClassification", "transformers.Trainer", "sys.argv[].endswith", "transformers.HfArgumentParser.parse_json_file", "transformers.HfArgumentParser.parse_args_into_dataclasses", "os.path.isdir", "transformers.trainer_utils.get_last_checkpoint", "transformers.utils.logging.set_verbosity_info", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_explicit_format", "os.path.abspath", "set", "list", "get_label_list.sort", "run_layout_analysis.main.get_label_list"], "function", ["None"], ["", "def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ")", ")", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "2", "and", "sys", ".", "argv", "[", "1", "]", ".", "endswith", "(", "\".json\"", ")", ":", "\n", "# If we pass only one argument to the script and it's the path to a json file,", "\n", "# let's parse it to get our arguments.", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_json_file", "(", "json_file", "=", "os", ".", "path", ".", "abspath", "(", "sys", ".", "argv", "[", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "# Detecting last checkpoint.", "\n", "", "last_checkpoint", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "training_args", ".", "output_dir", ")", "and", "training_args", ".", "do_train", "and", "not", "training_args", ".", "overwrite_output_dir", ":", "\n", "        ", "last_checkpoint", "=", "get_last_checkpoint", "(", "training_args", ".", "output_dir", ")", "\n", "if", "last_checkpoint", "is", "None", "and", "len", "(", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", ")", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"", "\n", "\"Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"", "\n", "\"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"", "\n", ")", "\n", "\n", "# Setup logging", "\n", "", "", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "handlers", "=", "[", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "]", ",", "\n", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "INFO", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", "else", "logging", ".", "WARN", ")", "\n", "\n", "# Log on each process the small summary:", "\n", "logger", ".", "warning", "(", "\n", "f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"", "\n", "+", "f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"", "\n", ")", "\n", "# Set the verbosity to info of the Transformers logger (on main process only):", "\n", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", ":", "\n", "        ", "transformers", ".", "utils", ".", "logging", ".", "set_verbosity_info", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_default_handler", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_explicit_format", "(", ")", "\n", "", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "# Set seed before initializing model.", "\n", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "datasets", "=", "load_dataset", "(", "\n", "os", ".", "path", ".", "abspath", "(", "skim", ".", "data", ".", "datasets", ".", "docbank", ".", "__file__", ")", ",", "\n", "data_dir", "=", "data_args", ".", "data_dir", ",", "\n", "cache_dir", "=", "data_args", ".", "cached_data_dir", ",", "\n", ")", "\n", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "column_names", "=", "datasets", "[", "\"train\"", "]", ".", "column_names", "\n", "features", "=", "datasets", "[", "\"train\"", "]", ".", "features", "\n", "", "else", ":", "\n", "        ", "column_names", "=", "datasets", "[", "\"validation\"", "]", ".", "column_names", "\n", "features", "=", "datasets", "[", "\"validation\"", "]", ".", "features", "\n", "", "text_column_name", "=", "\"words\"", "if", "\"words\"", "in", "column_names", "else", "column_names", "[", "0", "]", "\n", "label_column_name", "=", "\"tags\"", "if", "\"tags\"", "in", "column_names", "else", "column_names", "[", "1", "]", "\n", "\n", "remove_columns", "=", "column_names", "\n", "\n", "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the", "\n", "# unique labels.", "\n", "def", "get_label_list", "(", "labels", ")", ":", "\n", "        ", "unique_labels", "=", "set", "(", ")", "\n", "for", "label", "in", "labels", ":", "\n", "            ", "unique_labels", "=", "unique_labels", "|", "set", "(", "label", ")", "\n", "", "label_list", "=", "list", "(", "unique_labels", ")", "\n", "label_list", ".", "sort", "(", ")", "\n", "return", "label_list", "\n", "\n", "", "if", "isinstance", "(", "features", "[", "label_column_name", "]", ".", "feature", ",", "ClassLabel", ")", ":", "\n", "        ", "label_list", "=", "features", "[", "label_column_name", "]", ".", "feature", ".", "names", "\n", "# No need to convert the labels since they are already ints.", "\n", "label_to_id", "=", "{", "i", ":", "i", "for", "i", "in", "range", "(", "len", "(", "label_list", ")", ")", "}", "\n", "", "else", ":", "\n", "        ", "label_list", "=", "get_label_list", "(", "datasets", "[", "\"train\"", "]", "[", "label_column_name", "]", ")", "\n", "label_to_id", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "label_list", ")", "}", "\n", "", "num_labels", "=", "len", "(", "label_list", ")", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "model_args", ".", "model_type", "=", "model_args", ".", "model_type", ".", "lower", "(", ")", "\n", "model_args", ".", "core_model_type", "=", "model_args", ".", "core_model_type", ".", "lower", "(", ")", "\n", "\n", "config_kwargs", "=", "{", "\n", "\"cache_dir\"", ":", "model_args", ".", "cache_dir", ",", "\n", "\"revision\"", ":", "model_args", ".", "model_revision", ",", "\n", "\"use_auth_token\"", ":", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "}", "\n", "\n", "# initialize config if not BertWithSkimEmbed or SkimmingMask", "\n", "if", "(", "\n", "model_args", ".", "model_type", "not", "in", "[", "\"bertwithskimembed\"", ",", "\"skimmingmask\"", "]", "\n", "or", "(", "\n", "model_args", ".", "model_type", "in", "[", "\"bertwithskimembed\"", ",", "\"skimmingmask\"", "]", "\n", "and", "model_args", ".", "model_name_or_path", "\n", ")", "\n", ")", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "model_args", ".", "config_name", "if", "model_args", ".", "config_name", "else", "model_args", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "**", "config_kwargs", ",", "\n", ")", "\n", "\n", "", "tokenizer_kwargs", "=", "{", "\n", "\"cache_dir\"", ":", "model_args", ".", "cache_dir", ",", "\n", "\"use_fast\"", ":", "model_args", ".", "use_fast_tokenizer", ",", "\n", "\"revision\"", ":", "model_args", ".", "model_revision", ",", "\n", "\"use_auth_token\"", ":", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "}", "\n", "if", "model_args", ".", "model_type", "in", "[", "\"longformer\"", ",", "\"longskimformer\"", "]", ":", "\n", "        ", "tokenizer_kwargs", "[", "\"add_prefix_space\"", "]", "=", "True", "\n", "\n", "", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "model_args", ".", "tokenizer_name", "if", "model_args", ".", "tokenizer_name", "else", "model_args", ".", "model_name_or_path", ",", "\n", "**", "tokenizer_kwargs", ",", "\n", ")", "\n", "\n", "if", "model_args", ".", "model_type", "not", "in", "[", "\"bertwithskimembed\"", ",", "\"skimmingmask\"", "]", ":", "\n", "        ", "logger", ".", "info", "(", "\n", "f\"Fine-tuning {model_args.model_type} with weights initialized from \"", "\n", "f\"{model_args.model_name_or_path}.\"", "\n", ")", "\n", "model_kwargs", "=", "{", "\n", "\"cache_dir\"", ":", "model_args", ".", "cache_dir", ",", "\n", "\"revision\"", ":", "model_args", ".", "model_revision", ",", "\n", "\"use_auth_token\"", ":", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "}", "\n", "model", "=", "AutoModelForTokenClassification", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "# config=config,", "\n", "**", "model_kwargs", ",", "\n", ")", "\n", "\n", "if", "model", ".", "config", ".", "num_labels", "!=", "config", ".", "num_labels", "or", "model", ".", "config", ".", "id2label", "!=", "config", ".", "id2label", ":", "\n", "            ", "model_architecture", "=", "MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING", "[", "CONFIG_MAPPING", "[", "model_args", ".", "model_type", "]", "]", "\n", "if", "model_architecture", "==", "model", ".", "config", ".", "architectures", "[", "0", "]", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "f\"`model.config.num_labels` ({model.config.num_labels}) != `config.num_labels` ({config.num_labels}) \"", "\n", "\"Reseting `model.classifier`\"", "\n", ")", "\n", "", "model", ".", "classifier", "=", "nn", ".", "Linear", "(", "model", ".", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "model", ".", "classifier", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "model", ".", "config", ".", "initializer_range", ")", "\n", "model", ".", "config", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "model", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "", "", "elif", "model_args", ".", "model_type", "==", "\"bertwithskimembed\"", ":", "\n", "        ", "assert", "model_args", ".", "skim_model_name_or_path", "and", "model_args", ".", "core_model_name_or_path", ",", "\"Must provided model checkpoints for \"", "\"`skim_model_name_or_path` and `core_model_name_or_path`\"", "\n", "logger", ".", "info", "(", "\n", "f\"Fine-tuning BertWithSkimEmbed with 2D position embeddings initialized \"", "f\"with weights from {model_args.skim_model_name_or_path} and \"", "f\"core model initialized with weights from {model_args.core_model_name_or_path}\"", "\n", ")", "\n", "\n", "skim_model", "=", "AutoModelForMaskedLM", ".", "from_pretrained", "(", "\n", "model_args", ".", "skim_model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "skim_model_name_or_path", ")", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "core_model", "=", "AutoModelForMaskedLM", ".", "from_pretrained", "(", "\n", "model_args", ".", "core_model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "core_model_name_or_path", ")", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "\n", "config", "=", "CONFIG_MAPPING", "[", "model_args", ".", "model_type", "]", "(", "\n", "vocab_size", "=", "core_model", ".", "config", ".", "vocab_size", ",", "\n", "hidden_size", "=", "core_model", ".", "config", ".", "hidden_size", ",", "\n", "hidden_layout_size", "=", "skim_model", ".", "config", ".", "hidden_layout_size", ",", "\n", "num_hidden_layers", "=", "core_model", ".", "config", ".", "num_hidden_layers", ",", "\n", "num_attention_heads", "=", "core_model", ".", "config", ".", "num_attention_heads", ",", "\n", "intermediate_size", "=", "core_model", ".", "config", ".", "intermediate_size", ",", "\n", "hidden_act", "=", "core_model", ".", "config", ".", "hidden_act", ",", "\n", "hidden_dropout_prob", "=", "core_model", ".", "config", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "core_model", ".", "config", ".", "attention_probs_dropout_prob", ",", "\n", "max_position_embeddings", "=", "core_model", ".", "config", ".", "max_position_embeddings", ",", "\n", "type_vocab_size", "=", "core_model", ".", "config", ".", "type_vocab_size", ",", "\n", "initializer_range", "=", "core_model", ".", "config", ".", "initializer_range", ",", "\n", "layer_norm_eps", "=", "core_model", ".", "config", ".", "layer_norm_eps", ",", "\n", "pad_token_id", "=", "core_model", ".", "config", ".", "pad_token_id", ",", "\n", "gradient_checkpointing", "=", "core_model", ".", "config", ".", "gradient_checkpointing", ",", "\n", "max_2d_position_embeddings", "=", "skim_model", ".", "config", ".", "max_2d_position_embeddings", ",", "\n", "contextualize_2d_positions", "=", "model_args", ".", "contextualize_2d_positions", ",", "\n", "num_hidden_layers_layout_encoder", "=", "skim_model", ".", "config", ".", "num_hidden_layers_layout_encoder", ",", "\n", "num_attention_heads_layout_encoder", "=", "skim_model", ".", "config", ".", "num_attention_heads_layout_encoder", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", ")", "\n", "model", "=", "AutoModelForTokenClassification", ".", "from_config", "(", "config", "=", "config", ")", "\n", "\n", "# Copy weights", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Copy layout embeddings from Skimformer", "\n", "            ", "model", ".", "bert_with_skim_embed", ".", "embeddings", ".", "x_position_embeddings", ".", "load_state_dict", "(", "\n", "skim_model", ".", "skimformer", ".", "two_dim_pos_embeddings", ".", "x_position_embeddings", ".", "state_dict", "(", ")", "\n", ")", "\n", "model", ".", "bert_with_skim_embed", ".", "embeddings", ".", "y_position_embeddings", ".", "load_state_dict", "(", "\n", "skim_model", ".", "skimformer", ".", "two_dim_pos_embeddings", ".", "y_position_embeddings", ".", "state_dict", "(", ")", "\n", ")", "\n", "model", ".", "bert_with_skim_embed", ".", "embeddings", ".", "h_position_embeddings", ".", "load_state_dict", "(", "\n", "skim_model", ".", "skimformer", ".", "two_dim_pos_embeddings", ".", "h_position_embeddings", ".", "state_dict", "(", ")", "\n", ")", "\n", "model", ".", "bert_with_skim_embed", ".", "embeddings", ".", "w_position_embeddings", ".", "load_state_dict", "(", "\n", "skim_model", ".", "skimformer", ".", "two_dim_pos_embeddings", ".", "w_position_embeddings", ".", "state_dict", "(", ")", "\n", ")", "\n", "\n", "# Copy text embeddings and encoder weights from core model               ", "\n", "model", ".", "bert_with_skim_embed", ".", "embeddings", ".", "word_embeddings", ".", "load_state_dict", "(", "\n", "core_model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "state_dict", "(", ")", "\n", ")", "\n", "model", ".", "bert_with_skim_embed", ".", "embeddings", ".", "position_embeddings", ".", "load_state_dict", "(", "\n", "core_model", ".", "bert", ".", "embeddings", ".", "position_embeddings", ".", "state_dict", "(", ")", "\n", ")", "\n", "model", ".", "bert_with_skim_embed", ".", "embeddings", ".", "token_type_embeddings", ".", "load_state_dict", "(", "\n", "core_model", ".", "bert", ".", "embeddings", ".", "token_type_embeddings", ".", "state_dict", "(", ")", "\n", ")", "\n", "model", ".", "bert_with_skim_embed", ".", "embeddings", ".", "LayerNorm", ".", "load_state_dict", "(", "\n", "core_model", ".", "bert", ".", "embeddings", ".", "LayerNorm", ".", "state_dict", "(", ")", "\n", ")", "\n", "\n", "model", ".", "bert_with_skim_embed", ".", "encoder", ".", "load_state_dict", "(", "core_model", ".", "bert", ".", "encoder", ".", "state_dict", "(", ")", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "assert", "(", "\n", "model_args", ".", "skim_model_name_or_path", "and", "model_args", ".", "core_model_name_or_path", "\n", ")", ",", "f\"Must provide `skim_model_name_or_path` and `core_model_name_or_path` to instantiate {model_args.model_type} model.\"", "\n", "\n", "logger", ".", "info", "(", "\n", "f\"Fine-tuning SkimmingMask with layout embeddings and Skim-Attention initialized \"", "f\"with weights from {model_args.skim_model_name_or_path}, \"", "f\"core model initialized with weights from {model_args.core_model_name_or_path}\"", "f\" and `top_k` = {model_args.top_k}\"", "\n", ")", "\n", "\n", "skim_model", "=", "AutoModelForMaskedLM", ".", "from_pretrained", "(", "\n", "model_args", ".", "skim_model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "skim_model_name_or_path", ")", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "core_model", "=", "AutoModelForMaskedLM", ".", "from_pretrained", "(", "\n", "model_args", ".", "core_model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "core_model_name_or_path", ")", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "assert", "model_args", ".", "top_k", ">", "0", ",", "\"`top_k` must be > 0\"", "\n", "\n", "config", "=", "CONFIG_MAPPING", "[", "model_args", ".", "model_type", "]", "(", "\n", "vocab_size", "=", "core_model", ".", "config", ".", "vocab_size", ",", "\n", "hidden_size", "=", "core_model", ".", "config", ".", "hidden_size", ",", "\n", "hidden_layout_size", "=", "skim_model", ".", "config", ".", "hidden_layout_size", ",", "\n", "num_hidden_layers", "=", "core_model", ".", "config", ".", "num_hidden_layers", ",", "\n", "num_attention_heads", "=", "core_model", ".", "config", ".", "num_attention_heads", ",", "\n", "num_layout_attention_heads", "=", "skim_model", ".", "config", ".", "num_attention_heads", ",", "\n", "intermediate_size", "=", "core_model", ".", "config", ".", "intermediate_size", ",", "\n", "hidden_act", "=", "core_model", ".", "config", ".", "hidden_act", ",", "\n", "hidden_dropout_prob", "=", "core_model", ".", "config", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "core_model", ".", "config", ".", "attention_probs_dropout_prob", ",", "\n", "max_position_embeddings", "=", "core_model", ".", "config", ".", "max_position_embeddings", ",", "\n", "type_vocab_size", "=", "core_model", ".", "config", ".", "type_vocab_size", ",", "\n", "initializer_range", "=", "core_model", ".", "config", ".", "initializer_range", ",", "\n", "layer_norm_eps", "=", "core_model", ".", "config", ".", "layer_norm_eps", ",", "\n", "skim_attention_head_size", "=", "skim_model", ".", "config", ".", "skim_attention_head_size", ",", "\n", "pad_token_id", "=", "core_model", ".", "config", ".", "pad_token_id", ",", "\n", "gradient_checkpointing", "=", "core_model", ".", "config", ".", "gradient_checkpointing", ",", "\n", "max_2d_position_embeddings", "=", "skim_model", ".", "config", ".", "max_2d_position_embeddings", ",", "\n", "contextualize_2d_positions", "=", "skim_model", ".", "config", ".", "contextualize_2d_positions", ",", "\n", "num_hidden_layers_layout_encoder", "=", "skim_model", ".", "config", ".", "num_hidden_layers_layout_encoder", ",", "\n", "num_attention_heads_layout_encoder", "=", "skim_model", ".", "config", ".", "num_attention_heads_layout_encoder", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "top_k", "=", "model_args", ".", "top_k", ",", "\n", "core_model_type", "=", "model_args", ".", "core_model_type", ",", "\n", ")", "\n", "\n", "model", "=", "AutoModelForTokenClassification", ".", "from_config", "(", "config", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# copy layout embeddings from Skimformer", "\n", "            ", "model", ".", "skimming_mask_model", ".", "two_dim_pos_embeddings", ".", "load_state_dict", "(", "\n", "skim_model", ".", "skimformer", ".", "two_dim_pos_embeddings", ".", "state_dict", "(", ")", "\n", ")", "\n", "\n", "# copy contextualizer", "\n", "if", "skim_model", ".", "config", ".", "contextualize_2d_positions", ":", "\n", "                ", "logger", ".", "info", "(", "\"Contextualizing 2d positions\"", ")", "\n", "model", ".", "skimming_mask_model", ".", "layout_encoder", ".", "load_state_dict", "(", "\n", "skim_model", ".", "skimformer", ".", "layout_encoder", ".", "state_dict", "(", ")", "\n", ")", "\n", "# copy Skim-Attention", "\n", "", "model", ".", "skimming_mask_model", ".", "skim_attention", ".", "query", ".", "load_state_dict", "(", "\n", "skim_model", ".", "skimformer", ".", "skim_attention", ".", "query", ".", "state_dict", "(", ")", "\n", ")", "\n", "model", ".", "skimming_mask_model", ".", "skim_attention", ".", "key", ".", "load_state_dict", "(", "\n", "skim_model", ".", "skimformer", ".", "skim_attention", ".", "key", ".", "state_dict", "(", ")", "\n", ")", "\n", "\n", "# Copy text embeddings and encoder weights from core model", "\n", "if", "model_args", ".", "core_model_type", "==", "\"bert\"", ":", "\n", "                ", "core_model_base", "=", "core_model", ".", "bert", "\n", "", "else", ":", "\n", "                ", "assert", "model_args", ".", "core_model_type", "==", "\"layoutlm\"", "\n", "core_model_base", "=", "core_model", ".", "layoutlm", "\n", "\n", "", "model", ".", "skimming_mask_model", ".", "embeddings", ".", "load_state_dict", "(", "\n", "core_model_base", ".", "embeddings", ".", "state_dict", "(", ")", "\n", ")", "\n", "model", ".", "skimming_mask_model", ".", "encoder", ".", "load_state_dict", "(", "core_model_base", ".", "encoder", ".", "state_dict", "(", ")", ")", "\n", "\n", "\n", "", "", "model", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "\n", "# Preprocessing the dataset", "\n", "# Padding strategy", "\n", "padding", "=", "\"max_length\"", "if", "data_args", ".", "pad_to_max_length", "else", "False", "\n", "\n", "# Tokenize all texts and align the labels with them.", "\n", "def", "tokenize_and_align_labels", "(", "examples", ")", ":", "\n", "        ", "tokenized_inputs", "=", "tokenizer", "(", "\n", "examples", "[", "text_column_name", "]", ",", "\n", "padding", "=", "padding", ",", "\n", "max_length", "=", "data_args", ".", "max_seq_length", ",", "\n", "truncation", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "True", ",", "\n", "# We use this argument because the texts in our dataset are lists of words (with a label for each word).", "\n", "is_split_into_words", "=", "True", ",", "\n", ")", "\n", "\n", "labels", "=", "[", "]", "\n", "bboxes", "=", "[", "]", "\n", "\n", "for", "batch_index", "in", "range", "(", "len", "(", "tokenized_inputs", "[", "\"input_ids\"", "]", ")", ")", ":", "\n", "            ", "word_ids", "=", "tokenized_inputs", ".", "word_ids", "(", "batch_index", "=", "batch_index", ")", "\n", "org_batch_index", "=", "tokenized_inputs", "[", "\"overflow_to_sample_mapping\"", "]", "[", "batch_index", "]", "\n", "\n", "label", "=", "examples", "[", "label_column_name", "]", "[", "org_batch_index", "]", "\n", "bbox", "=", "examples", "[", "\"bboxes\"", "]", "[", "org_batch_index", "]", "\n", "\n", "previous_word_idx", "=", "None", "\n", "label_ids", "=", "[", "]", "\n", "bbox_inputs", "=", "[", "]", "\n", "for", "word_idx", "in", "word_ids", ":", "\n", "# Special tokens have a word id that is None. We set the label to -100 so they are automatically", "\n", "# ignored in the loss function.", "\n", "                ", "if", "word_idx", "is", "None", ":", "\n", "                    ", "label_ids", ".", "append", "(", "-", "100", ")", "\n", "bbox_inputs", ".", "append", "(", "[", "0", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "# We set the label for the first token of each word.", "\n", "", "elif", "word_idx", "!=", "previous_word_idx", ":", "\n", "                    ", "label_ids", ".", "append", "(", "label_to_id", "[", "label", "[", "word_idx", "]", "]", ")", "\n", "bbox_inputs", ".", "append", "(", "bbox", "[", "word_idx", "]", ")", "\n", "# For the other tokens in a word, we set the label to either the current label or -100, depending on", "\n", "# the label_all_tokens flag.", "\n", "", "else", ":", "\n", "                    ", "label_ids", ".", "append", "(", "label_to_id", "[", "label", "[", "word_idx", "]", "]", "if", "data_args", ".", "label_all_tokens", "else", "-", "100", ")", "\n", "bbox_inputs", ".", "append", "(", "bbox", "[", "word_idx", "]", ")", "\n", "", "previous_word_idx", "=", "word_idx", "\n", "\n", "", "labels", ".", "append", "(", "label_ids", ")", "\n", "bboxes", ".", "append", "(", "bbox_inputs", ")", "\n", "\n", "", "tokenized_inputs", "[", "\"labels\"", "]", "=", "labels", "\n", "tokenized_inputs", "[", "\"bbox\"", "]", "=", "bboxes", "\n", "return", "tokenized_inputs", "\n", "\n", "", "if", "training_args", ".", "do_train", ":", "\n", "        ", "if", "\"train\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_train requires a train dataset\"", ")", "\n", "", "train_dataset", "=", "datasets", "[", "\"train\"", "]", "\n", "if", "data_args", ".", "max_train_samples", "is", "not", "None", ":", "\n", "            ", "train_dataset", "=", "train_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_train_samples", ")", ")", "\n", "", "train_dataset", "=", "train_dataset", ".", "map", "(", "\n", "tokenize_and_align_labels", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "remove_columns", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "if", "\"validation\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_eval requires a validation dataset\"", ")", "\n", "", "eval_dataset", "=", "datasets", "[", "\"validation\"", "]", "\n", "if", "data_args", ".", "max_val_samples", "is", "not", "None", ":", "\n", "            ", "eval_dataset", "=", "eval_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_val_samples", ")", ")", "\n", "", "eval_dataset", "=", "eval_dataset", ".", "map", "(", "\n", "tokenize_and_align_labels", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "remove_columns", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "if", "\"test\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_predict requires a test dataset\"", ")", "\n", "", "test_dataset", "=", "datasets", "[", "\"test\"", "]", "\n", "if", "data_args", ".", "max_test_samples", "is", "not", "None", ":", "\n", "            ", "test_dataset", "=", "test_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_test_samples", ")", ")", "\n", "", "test_dataset", "=", "test_dataset", ".", "map", "(", "\n", "tokenize_and_align_labels", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "remove_columns", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "", "data_collator", "=", "DataCollatorForTokenClassification", "(", "\n", "tokenizer", ",", "\n", "padding", "=", "padding", ",", "\n", "max_length", "=", "data_args", ".", "max_seq_length", ",", "\n", "pad_to_multiple_of", "=", "8", "if", "training_args", ".", "fp16", "else", "None", ",", "\n", "use_2d_attn_mask", "=", "(", "model_args", ".", "model_type", "==", "\"skimmingmask\"", ")", ",", "\n", ")", "\n", "\n", "def", "compute_metrics", "(", "p", ")", ":", "\n", "        ", "predictions", ",", "labels", "=", "p", "\n", "predictions", "=", "np", ".", "argmax", "(", "predictions", ",", "axis", "=", "2", ")", "\n", "\n", "# Map from label_id to label", "\n", "# Remove ignored index (special tokens)", "\n", "true_predictions", "=", "[", "\n", "[", "label_list", "[", "p", "]", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "predictions", ",", "labels", ")", "\n", "]", "\n", "true_labels", "=", "[", "\n", "[", "label_list", "[", "l", "]", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "predictions", ",", "labels", ")", "\n", "]", "\n", "\n", "# flatten lists", "\n", "flat_true_predictions", "=", "[", "pred", "for", "sublist", "in", "true_predictions", "for", "pred", "in", "sublist", "]", "\n", "flat_true_labels", "=", "[", "label", "for", "sublist", "in", "true_labels", "for", "label", "in", "sublist", "]", "\n", "\n", "labels_to_detect", "=", "np", ".", "unique", "(", "flat_true_labels", ")", "\n", "\n", "report", "=", "classification_report", "(", "\n", "flat_true_labels", ",", "\n", "flat_true_predictions", ",", "\n", "labels", "=", "labels_to_detect", ",", "\n", "output_dict", "=", "True", ",", "\n", "zero_division", "=", "0", ",", "\n", ")", "\n", "\n", "results", "=", "{", "}", "\n", "\n", "for", "key_label", ",", "value_label", "in", "sorted", "(", "report", ".", "items", "(", ")", ")", ":", "\n", "            ", "if", "type", "(", "value_label", ")", "!=", "dict", ":", "\n", "                ", "results", "[", "key_label", "]", "=", "value_label", "\n", "", "else", ":", "\n", "                ", "for", "key_metric", ",", "value_metric", "in", "value_label", ".", "items", "(", ")", ":", "\n", "                    ", "results", "[", "key_label", "+", "\"_\"", "+", "key_metric", "]", "=", "value_metric", "\n", "\n", "", "", "", "return", "results", "\n", "\n", "", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "eval_dataset", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", "compute_metrics", "=", "compute_metrics", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "checkpoint", "=", "last_checkpoint", "if", "last_checkpoint", "else", "None", "\n", "train_result", "=", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "checkpoint", ")", "\n", "metrics", "=", "train_result", ".", "metrics", "\n", "trainer", ".", "save_model", "(", ")", "# Saves the tokenizer too for easy upload", "\n", "\n", "max_train_samples", "=", "(", "\n", "data_args", ".", "max_train_samples", "if", "data_args", ".", "max_train_samples", "is", "not", "None", "else", "len", "(", "train_dataset", ")", "\n", ")", "\n", "metrics", "[", "\"train_samples\"", "]", "=", "min", "(", "max_train_samples", ",", "len", "(", "train_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_state", "(", ")", "\n", "\n", "# Evaluation", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "metrics", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "max_val_samples", "=", "data_args", ".", "max_val_samples", "if", "data_args", ".", "max_val_samples", "is", "not", "None", "else", "len", "(", "eval_dataset", ")", "\n", "metrics", "[", "\"eval_samples\"", "]", "=", "min", "(", "max_val_samples", ",", "len", "(", "eval_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "\n", "# Predict", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Predict ***\"", ")", "\n", "\n", "predictions", ",", "labels", ",", "metrics", "=", "trainer", ".", "predict", "(", "test_dataset", ")", "\n", "predictions", "=", "np", ".", "argmax", "(", "predictions", ",", "axis", "=", "2", ")", "\n", "\n", "# Remove ignored index (special tokens)", "\n", "true_predictions", "=", "[", "\n", "[", "label_list", "[", "p", "]", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "predictions", ",", "labels", ")", "\n", "]", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"test\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"test\"", ",", "metrics", ")", "\n", "\n", "# Save predictions", "\n", "output_test_predictions_file", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"test_predictions.txt\"", ")", "\n", "if", "trainer", ".", "is_world_process_zero", "(", ")", ":", "\n", "            ", "with", "open", "(", "output_test_predictions_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                ", "for", "prediction", "in", "true_predictions", ":", "\n", "                    ", "writer", ".", "write", "(", "\" \"", ".", "join", "(", "prediction", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.experiments.run_layout_analysis._mp_fn": [[713, 716], ["run_layout_analysis.main"], "function", ["home.repos.pwc.inspect_result.recitalai_skim-attention.experiments.run_pretraining.main"], ["", "", "", "", "", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "    ", "main", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.experiments.run_pretraining.main": [[181, 485], ["transformers.HfArgumentParser", "logging.basicConfig", "logger.setLevel", "logger.warning", "transformers.trainer_utils.is_main_process", "logger.info", "transformers.set_seed", "datasets.load_dataset", "model_args.model_type.lower", "AutoModelForMaskedLM.from_config.resize_token_embeddings", "skim.data.DataCollatorForMaskedLM", "transformers.Trainer", "sys.argv[].endswith", "transformers.HfArgumentParser.parse_json_file", "transformers.HfArgumentParser.parse_args_into_dataclasses", "os.path.isdir", "transformers.trainer_utils.get_last_checkpoint", "transformers.utils.logging.set_verbosity_info", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_explicit_format", "ValueError", "os.path.abspath", "transformers.AutoConfig.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModelForMaskedLM.from_pretrained", "logger.info", "transformers.AutoModelForMaskedLM.from_config", "len", "AutoTokenizer.from_pretrained.", "range", "train_dataset.select.map", "eval_dataset.select.map", "test_dataset.select.map", "transformers.Trainer.train", "transformers.Trainer.save_model", "min", "transformers.Trainer.log_metrics", "transformers.Trainer.save_metrics", "transformers.Trainer.save_state", "logger.info", "transformers.Trainer.evaluate", "min", "math.exp", "transformers.Trainer.log_metrics", "transformers.Trainer.save_metrics", "os.path.join", "transformers.Trainer.is_world_process_zero", "logger.info", "transformers.Trainer.evaluate", "min", "math.exp", "transformers.Trainer.log_metrics", "transformers.Trainer.save_metrics", "os.path.join", "transformers.Trainer.is_world_process_zero", "len", "ValueError", "transformers.trainer_utils.is_main_process", "os.path.exists", "os.listdir", "transformers.AutoConfig.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "ValueError", "len", "tokenizer.word_ids", "bboxes.append", "ValueError", "train_dataset.select.select", "ValueError", "eval_dataset.select.select", "ValueError", "test_dataset.select.select", "len", "len", "len", "len", "len", "len", "os.path.abspath", "len", "logger.info", "logging.StreamHandler", "config_class", "bool", "range", "range", "range", "open", "logger.info", "logger.info", "writer.write", "open", "logger.info", "logger.info", "writer.write", "os.listdir", "bool", "config_class", "config_class", "logger.warning", "bbox_inputs.append", "str", "str", "bbox_inputs.append", "bbox_inputs.append", "str", "str"], "function", ["None"], ["", "def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ")", ")", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "2", "and", "sys", ".", "argv", "[", "1", "]", ".", "endswith", "(", "\".json\"", ")", ":", "\n", "# If we pass only one argument to the script and it's the path to a json file,", "\n", "# let's parse it to get our arguments.", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_json_file", "(", "json_file", "=", "os", ".", "path", ".", "abspath", "(", "sys", ".", "argv", "[", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "# Detecting last checkpoint.", "\n", "", "last_checkpoint", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "training_args", ".", "output_dir", ")", "and", "training_args", ".", "do_train", "and", "not", "training_args", ".", "overwrite_output_dir", ":", "\n", "        ", "last_checkpoint", "=", "get_last_checkpoint", "(", "training_args", ".", "output_dir", ")", "\n", "if", "last_checkpoint", "is", "None", "and", "len", "(", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", ")", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"", "\n", "\"Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"", "\n", "\"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"", "\n", ")", "\n", "\n", "# Setup logging", "\n", "", "", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "handlers", "=", "[", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "]", ",", "\n", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "INFO", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", "else", "logging", ".", "WARN", ")", "\n", "\n", "# Log on each process the small summary:", "\n", "logger", ".", "warning", "(", "\n", "f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"", "\n", "+", "f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"", "\n", ")", "\n", "# Set the verbosity to info of the Transformers logger (on main process only):", "\n", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", ":", "\n", "        ", "transformers", ".", "utils", ".", "logging", ".", "set_verbosity_info", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_default_handler", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_explicit_format", "(", ")", "\n", "", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_args", ".", "data_dir", ")", "or", "not", "os", ".", "listdir", "(", "data_args", ".", "data_dir", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Supply a valid path to --data_dir.\"", ")", "\n", "\n", "# Set seed before initializing model.", "\n", "", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "datasets", "=", "load_dataset", "(", "\n", "os", ".", "path", ".", "abspath", "(", "skim", ".", "data", ".", "datasets", ".", "mix", ".", "__file__", ")", ",", "\n", "data_dir", "=", "data_args", ".", "data_dir", ",", "\n", "cache_dir", "=", "data_args", ".", "cached_data_dir", ",", "\n", ")", "\n", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "column_names", "=", "datasets", "[", "\"train\"", "]", ".", "column_names", "\n", "", "else", ":", "\n", "        ", "column_names", "=", "datasets", "[", "\"test\"", "]", ".", "column_names", "\n", "", "text_column_name", "=", "\"words\"", "if", "\"words\"", "in", "column_names", "else", "column_names", "[", "0", "]", "\n", "\n", "remove_columns", "=", "column_names", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "\n", "model_args", ".", "model_type", "=", "model_args", ".", "model_type", ".", "lower", "(", ")", "\n", "\n", "config_kwargs", "=", "{", "\n", "\"cache_dir\"", ":", "model_args", ".", "cache_dir", ",", "\n", "\"revision\"", ":", "model_args", ".", "model_revision", ",", "\n", "\"use_auth_token\"", ":", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "}", "\n", "if", "model_args", ".", "config_name", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "config_name", ",", "**", "config_kwargs", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "**", "config_kwargs", ")", "\n", "", "else", ":", "\n", "        ", "config_class", "=", "CONFIG_MAPPING", "[", "model_args", ".", "model_type", "]", "\n", "if", "model_args", ".", "model_type", "==", "\"skimformer\"", ":", "\n", "            ", "config", "=", "config_class", "(", "\n", "hidden_layout_size", "=", "model_args", ".", "hidden_layout_size", ",", "\n", "skim_attention_head_size", "=", "model_args", ".", "skim_attention_head_size", ",", "\n", "attention_head_size", "=", "model_args", ".", "attention_head_size", ",", "\n", "use_1d_positions", "=", "model_args", ".", "use_1d_positions", ",", "\n", "degrade_2d_positions", "=", "model_args", ".", "degrade_2d_positions", ",", "\n", "contextualize_2d_positions", "=", "model_args", ".", "contextualize_2d_positions", ",", "\n", ")", "\n", "", "elif", "model_args", ".", "model_type", "==", "\"longskimformer\"", ":", "\n", "            ", "config", "=", "config_class", "(", "\n", "hidden_layout_size", "=", "model_args", ".", "hidden_layout_size", ",", "\n", "skim_attention_head_size", "=", "model_args", ".", "skim_attention_head_size", ",", "\n", "attention_head_size", "=", "model_args", ".", "attention_head_size", ",", "\n", "use_1d_positions", "=", "model_args", ".", "use_1d_positions", ",", "\n", "degrade_2d_positions", "=", "model_args", ".", "degrade_2d_positions", ",", "\n", "contextualize_2d_positions", "=", "model_args", ".", "contextualize_2d_positions", ",", "\n", "attention_window", "=", "model_args", ".", "attention_window", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "config", "=", "config_class", "(", ")", "\n", "logger", ".", "warning", "(", "\"You are instantiating a new config instance from scratch.\"", ")", "\n", "\n", "", "", "tokenizer_kwargs", "=", "{", "\n", "\"cache_dir\"", ":", "model_args", ".", "cache_dir", ",", "\n", "\"use_fast\"", ":", "model_args", ".", "use_fast_tokenizer", ",", "\n", "\"revision\"", ":", "model_args", ".", "model_revision", ",", "\n", "\"use_auth_token\"", ":", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "}", "\n", "if", "model_args", ".", "model_type", "in", "[", "\"longformer\"", ",", "\"longskimformer\"", "]", ":", "\n", "        ", "tokenizer_kwargs", "[", "\"add_prefix_space\"", "]", "=", "True", "\n", "", "if", "model_args", ".", "tokenizer_name", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "tokenizer_name", ",", "**", "tokenizer_kwargs", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "**", "tokenizer_kwargs", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"", "\n", "\"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"", "\n", ")", "\n", "\n", "", "if", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "model", "=", "AutoModelForMaskedLM", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "f\"Training new {model_args.model_type} model from scratch\"", ")", "\n", "model", "=", "AutoModelForMaskedLM", ".", "from_config", "(", "config", ")", "\n", "\n", "", "model", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "\n", "# Preprocessing the dataset", "\n", "# Padding strategy", "\n", "padding", "=", "\"max_length\"", "if", "data_args", ".", "pad_to_max_length", "else", "False", "\n", "\n", "# Tokenize all texts and align the bboxes with them.", "\n", "def", "tokenize_and_align_bboxes", "(", "examples", ")", ":", "\n", "        ", "tokenized_inputs", "=", "tokenizer", "(", "\n", "examples", "[", "text_column_name", "]", ",", "\n", "padding", "=", "padding", ",", "\n", "max_length", "=", "data_args", ".", "max_seq_length", ",", "\n", "truncation", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "True", ",", "\n", "# We use this argument because the texts in our dataset are lists of words (with a label for each word).", "\n", "is_split_into_words", "=", "True", ",", "\n", ")", "\n", "\n", "bboxes", "=", "[", "]", "\n", "for", "batch_index", "in", "range", "(", "len", "(", "tokenized_inputs", "[", "\"input_ids\"", "]", ")", ")", ":", "\n", "            ", "word_ids", "=", "tokenized_inputs", ".", "word_ids", "(", "batch_index", "=", "batch_index", ")", "\n", "org_batch_index", "=", "tokenized_inputs", "[", "\"overflow_to_sample_mapping\"", "]", "[", "batch_index", "]", "\n", "\n", "bbox", "=", "examples", "[", "\"bboxes\"", "]", "[", "org_batch_index", "]", "\n", "\n", "previous_word_idx", "=", "None", "\n", "bbox_inputs", "=", "[", "]", "\n", "for", "word_idx", "in", "word_ids", ":", "\n", "                ", "if", "word_idx", "is", "None", ":", "\n", "                    ", "bbox_inputs", ".", "append", "(", "[", "0", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "", "elif", "word_idx", "!=", "previous_word_idx", ":", "\n", "                    ", "bbox_inputs", ".", "append", "(", "bbox", "[", "word_idx", "]", ")", "\n", "# For the other tokens in a word, we set the label to either the current label or -100, depending on", "\n", "# the label_all_tokens flag.", "\n", "", "else", ":", "\n", "                    ", "bbox_inputs", ".", "append", "(", "bbox", "[", "word_idx", "]", ")", "\n", "", "previous_word_idx", "=", "word_idx", "\n", "", "bboxes", ".", "append", "(", "bbox_inputs", ")", "\n", "", "tokenized_inputs", "[", "\"bbox\"", "]", "=", "bboxes", "\n", "return", "tokenized_inputs", "\n", "\n", "# Get datasets", "\n", "", "if", "training_args", ".", "do_train", ":", "\n", "        ", "if", "\"train\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_train requires a train dataset\"", ")", "\n", "", "train_dataset", "=", "datasets", "[", "\"train\"", "]", "\n", "if", "data_args", ".", "max_train_samples", "is", "not", "None", ":", "\n", "            ", "train_dataset", "=", "train_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_train_samples", ")", ")", "\n", "", "train_dataset", "=", "train_dataset", ".", "map", "(", "\n", "tokenize_and_align_bboxes", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "remove_columns", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "if", "\"validation\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_eval requires a validation dataset\"", ")", "\n", "", "eval_dataset", "=", "datasets", "[", "\"validation\"", "]", "\n", "if", "data_args", ".", "max_val_samples", "is", "not", "None", ":", "\n", "            ", "eval_dataset", "=", "eval_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_val_samples", ")", ")", "\n", "", "eval_dataset", "=", "eval_dataset", ".", "map", "(", "\n", "tokenize_and_align_bboxes", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "remove_columns", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "if", "\"test\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_predict requires a test dataset\"", ")", "\n", "", "test_dataset", "=", "datasets", "[", "\"test\"", "]", "\n", "if", "data_args", ".", "max_test_samples", "is", "not", "None", ":", "\n", "            ", "test_dataset", "=", "test_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_test_samples", ")", ")", "\n", "", "test_dataset", "=", "test_dataset", ".", "map", "(", "\n", "tokenize_and_align_bboxes", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "remove_columns", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "# Data collator", "\n", "", "data_collator", "=", "DataCollatorForMaskedLM", "(", "\n", "tokenizer", ",", "\n", "mlm_probability", "=", "data_args", ".", "mlm_probability", ",", "\n", "max_length", "=", "data_args", ".", "max_seq_length", ",", "\n", "pad_to_multiple_of", "=", "8", "if", "training_args", ".", "fp16", "else", "None", ",", "\n", "padding", "=", "padding", ",", "\n", "pad_token_bbox_value", "=", "0", ",", "\n", "assign_same_bbox", "=", "data_args", ".", "assign_same_bbox", ",", "\n", ")", "\n", "\n", "# Initialize our Trainer", "\n", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "eval_dataset", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "checkpoint", "=", "last_checkpoint", "if", "last_checkpoint", "else", "None", "\n", "train_result", "=", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "checkpoint", ")", "\n", "metrics", "=", "train_result", ".", "metrics", "\n", "trainer", ".", "save_model", "(", ")", "# Saves the tokenizer too for easy upload", "\n", "\n", "max_train_samples", "=", "(", "\n", "data_args", ".", "max_train_samples", "if", "data_args", ".", "max_train_samples", "is", "not", "None", "else", "len", "(", "train_dataset", ")", "\n", ")", "\n", "metrics", "[", "\"train_samples\"", "]", "=", "min", "(", "max_train_samples", ",", "len", "(", "train_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_state", "(", ")", "\n", "\n", "\n", "# Evaluation", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "metrics", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "max_val_samples", "=", "data_args", ".", "max_val_samples", "if", "data_args", ".", "max_val_samples", "is", "not", "None", "else", "len", "(", "eval_dataset", ")", "\n", "metrics", "[", "\"eval_samples\"", "]", "=", "min", "(", "max_val_samples", ",", "len", "(", "eval_dataset", ")", ")", "\n", "perplexity", "=", "math", ".", "exp", "(", "metrics", "[", "\"eval_loss\"", "]", ")", "\n", "metrics", "[", "\"perplexity\"", "]", "=", "perplexity", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "\n", "output_eval_ppl_file", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"eval_results_lm.txt\"", ")", "\n", "if", "trainer", ".", "is_world_process_zero", "(", ")", ":", "\n", "            ", "with", "open", "(", "output_eval_ppl_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "logger", ".", "info", "(", "\"  ppl = %s\"", ",", "str", "(", "metrics", "[", "\"perplexity\"", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"ppl = %s\\n\"", "%", "str", "(", "metrics", "[", "\"perplexity\"", "]", ")", ")", "\n", "\n", "# Predict", "\n", "", "", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Predict ***\"", ")", "\n", "\n", "metrics", "=", "trainer", ".", "evaluate", "(", "test_dataset", ")", "\n", "\n", "max_test_samples", "=", "data_args", ".", "max_test_samples", "if", "data_args", ".", "max_test_samples", "is", "not", "None", "else", "len", "(", "test_dataset", ")", "\n", "metrics", "[", "\"test_samples\"", "]", "=", "min", "(", "max_test_samples", ",", "len", "(", "test_dataset", ")", ")", "\n", "perplexity", "=", "math", ".", "exp", "(", "metrics", "[", "\"eval_loss\"", "]", ")", "\n", "metrics", "[", "\"perplexity\"", "]", "=", "perplexity", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"test\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"test\"", ",", "metrics", ")", "\n", "\n", "output_test_ppl_file", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"test_results_lm.txt\"", ")", "\n", "if", "trainer", ".", "is_world_process_zero", "(", ")", ":", "\n", "            ", "with", "open", "(", "output_test_ppl_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                ", "logger", ".", "info", "(", "\"***** Test results *****\"", ")", "\n", "logger", ".", "info", "(", "\"  ppl = %s\"", ",", "str", "(", "metrics", "[", "\"perplexity\"", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"ppl = %s\\n\"", "%", "str", "(", "metrics", "[", "\"perplexity\"", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.recitalai_skim-attention.experiments.run_pretraining._mp_fn": [[487, 490], ["run_pretraining.main"], "function", ["home.repos.pwc.inspect_result.recitalai_skim-attention.experiments.run_pretraining.main"], ["", "", "", "", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "    ", "main", "(", ")", "\n", "\n"]]}