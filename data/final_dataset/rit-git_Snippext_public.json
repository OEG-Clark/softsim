{"home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.read_asc_file": [[8, 27], ["open", "len", "line.strip().split", "gensim.utils.simple_preprocess", "res.append", "line.strip"], "function", ["None"], ["def", "read_asc_file", "(", "fn", ")", ":", "\n", "    ", "res", "=", "[", "]", "\n", "for", "line", "in", "open", "(", "fn", ")", ":", "\n", "        ", "if", "len", "(", "line", ")", "<", "3", ":", "\n", "            ", "continue", "\n", "", "else", ":", "\n", "            ", "LL", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "# token = twt.tokenize(LL[0])", "\n", "# term = twt.tokenize(LL[1])", "\n", "tokens", "=", "simple_preprocess", "(", "LL", "[", "0", "]", ")", "\n", "term", "=", "LL", "[", "1", "]", "\n", "polarity", "=", "LL", "[", "2", "]", "\n", "res", ".", "append", "(", "{", "\n", "'raw'", ":", "line", ",", "\n", "'token'", ":", "tokens", ",", "\n", "'term'", ":", "term", ",", "\n", "'polarity'", ":", "polarity", "\n", "}", ")", "\n", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.is_stopword": [[28, 30], ["token.isalpha"], "function", ["None"], ["", "def", "is_stopword", "(", "token", ")", ":", "\n", "    ", "return", "token", "in", "[", "'[SEP]'", ",", "'[CLS]'", "]", "or", "token", "in", "stopword_set", "or", "not", "token", ".", "isalpha", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.read_tagging_file": [[31, 45], ["open", "len", "tokens.append", "labels.append", "line.strip().split", "tokens[].append", "labels[].append", "line.strip"], "function", ["None"], ["", "def", "read_tagging_file", "(", "fn", ")", ":", "\n", "    ", "tokens", "=", "[", "[", "]", "]", "\n", "labels", "=", "[", "[", "]", "]", "\n", "for", "line", "in", "open", "(", "fn", ")", ":", "\n", "        ", "if", "len", "(", "line", ")", "<", "3", ":", "\n", "            ", "tokens", ".", "append", "(", "[", "]", ")", "\n", "labels", ".", "append", "(", "[", "]", ")", "\n", "", "else", ":", "\n", "            ", "LL", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "token", "=", "LL", "[", "0", "]", "\n", "label", "=", "LL", "[", "-", "1", "]", "\n", "tokens", "[", "-", "1", "]", ".", "append", "(", "token", ")", "\n", "labels", "[", "-", "1", "]", ".", "append", "(", "label", ")", "\n", "", "", "return", "tokens", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.build_idf_dict": [[46, 64], ["open", "sklearn.feature_extraction.text.TfidfVectorizer", "sklearn.feature_extraction.text.TfidfVectorizer.fit", "dict", "len", "corpus.append"], "function", ["None"], ["", "def", "build_idf_dict", "(", "fn", ")", ":", "\n", "    ", "corpus", "=", "[", "''", "]", "\n", "cnt", "=", "0", "\n", "for", "line", "in", "open", "(", "fn", ")", ":", "\n", "        ", "if", "len", "(", "line", ")", "<", "2", ":", "\n", "            ", "corpus", ".", "append", "(", "''", ")", "\n", "cnt", "+=", "1", "\n", "# if cnt % 10 == 0:", "\n", "#     print(cnt)", "\n", "", "else", ":", "\n", "            ", "corpus", "[", "-", "1", "]", "+=", "' '", "+", "line", "\n", "\n", "", "", "vectorizer", "=", "TfidfVectorizer", "(", ")", "\n", "vectorizer", ".", "fit", "(", "corpus", ")", "\n", "idf_dict", "=", "dict", "(", ")", "\n", "for", "w", "in", "vectorizer", ".", "vocabulary_", ":", "\n", "        ", "idf_dict", "[", "w", "]", "=", "vectorizer", ".", "idf_", "[", "vectorizer", ".", "vocabulary_", "[", "w", "]", "]", "\n", "", "return", "idf_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.sign": [[65, 67], ["None"], "function", ["None"], ["", "def", "sign", "(", "a", ")", ":", "\n", "    ", "return", "(", "a", ">", "0", ")", "-", "(", "a", "<", "0", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.handle_punct": [[25, 48], ["text.replace().replace.replace().replace", "len", "len", "text.replace().replace.replace", "next_chr.isalnum", "prev_chr.isalnum"], "function", ["None"], ["def", "handle_punct", "(", "text", ")", ":", "\n", "    ", "\"\"\"Basic handling of punctuations\n\n    Args:\n        text (str): the input text\n    Returns:\n        str: the string with the bad characters replaced and\n             new characters inserted\n    \"\"\"", "\n", "text", "=", "text", ".", "replace", "(", "\"''\"", ",", "\"'\"", ")", ".", "replace", "(", "\"\\\\n\"", ",", "' '", ")", "\n", "new_text", "=", "''", "\n", "i", "=", "0", "\n", "N", "=", "len", "(", "text", ")", "\n", "while", "i", "<", "len", "(", "text", ")", ":", "\n", "        ", "curr_chr", "=", "text", "[", "i", "]", "\n", "new_text", "+=", "curr_chr", "\n", "if", "i", ">", "0", "and", "i", "<", "N", "-", "1", ":", "\n", "            ", "next_chr", "=", "text", "[", "i", "+", "1", "]", "\n", "prev_chr", "=", "text", "[", "i", "-", "1", "]", "\n", "if", "next_chr", ".", "isalnum", "(", ")", "and", "prev_chr", ".", "isalnum", "(", ")", "and", "curr_chr", "in", "'!?.,();:'", ":", "\n", "                ", "new_text", "+=", "' '", "\n", "", "", "i", "+=", "1", "\n", "", "return", "new_text", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.sent_tokenizer": [[50, 67], ["text.split", "run_pipeline.handle_punct", "nlp", "len", "ori_sentences.append"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.handle_punct"], ["", "def", "sent_tokenizer", "(", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizer a paragraph of text into a list of sentences.\n\n    Args:\n        text (str): the input paragraph\n\n    Returns:\n        list of spacy Sentence: the tokenized sentences\n    \"\"\"", "\n", "text", "=", "handle_punct", "(", "text", ")", "[", ":", "1000000", "]", "\n", "ori_sentences", "=", "[", "]", "\n", "for", "line", "in", "text", ".", "split", "(", "'\\n'", ")", ":", "\n", "        ", "for", "sent", "in", "nlp", "(", "line", ",", "disable", "=", "[", "'tagger'", ",", "'ner'", "]", ")", ".", "sents", ":", "\n", "            ", "if", "len", "(", "sent", ")", ">=", "2", ":", "\n", "                ", "ori_sentences", ".", "append", "(", "sent", ")", "\n", "\n", "", "", "", "return", "ori_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.do_tagging": [[68, 128], ["run_pipeline.sent_tokenizer", "snippext.dataset.SnippextDataset", "torch.utils.data.DataLoader", "model.eval", "zip", "source.append", "token_pos_list.append", "torch.no_grad", "torch.no_grad", "enumerate", "results.append", "model", "Words.extend", "Is_heads.extend", "Tags.extend", "Y.extend", "Y_hat.extend", "zip", "y.numpy().tolist", "y_hat.cpu().numpy().tolist", "print", "y.numpy", "y_hat.cpu().numpy", "y_hat.cpu"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.sent_tokenizer"], ["", "def", "do_tagging", "(", "text", ",", "config", ",", "model", ")", ":", "\n", "    ", "\"\"\"Apply the tagging model.\n\n    Args:\n        text (str): the input paragraph\n        config (dict): the model configuration\n        model (MultiTaskNet): the model in pytorch\n\n    Returns:\n        list of list of str: the tokens in each sentences\n        list of list of int: each token's starting position in the original text\n        list of list of str: the tags assigned to each token\n    \"\"\"", "\n", "# load data and tokenization", "\n", "source", "=", "[", "]", "\n", "token_pos_list", "=", "[", "]", "\n", "# print('Tokenize sentences')", "\n", "for", "sent", "in", "sent_tokenizer", "(", "text", ")", ":", "\n", "        ", "tokens", "=", "[", "token", ".", "text", "for", "token", "in", "sent", "]", "\n", "token_pos", "=", "[", "token", ".", "idx", "for", "token", "in", "sent", "]", "\n", "source", ".", "append", "(", "tokens", ")", "\n", "token_pos_list", ".", "append", "(", "token_pos", ")", "\n", "\n", "", "dataset", "=", "SnippextDataset", "(", "source", ",", "config", "[", "'vocab'", "]", ",", "config", "[", "'name'", "]", ",", "\n", "lm", "=", "model", ".", "lm", ",", "\n", "max_len", "=", "64", ")", "\n", "iterator", "=", "data", ".", "DataLoader", "(", "dataset", "=", "dataset", ",", "\n", "batch_size", "=", "32", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "SnippextDataset", ".", "pad", ")", "\n", "\n", "# prediction", "\n", "model", ".", "eval", "(", ")", "\n", "Words", ",", "Is_heads", ",", "Tags", ",", "Y", ",", "Y_hat", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# print('Tagging')", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "iterator", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "words", ",", "x", ",", "is_heads", ",", "tags", ",", "mask", ",", "y", ",", "seqlens", ",", "taskname", "=", "batch", "\n", "taskname", "=", "taskname", "[", "0", "]", "\n", "_", ",", "_", ",", "y_hat", "=", "model", "(", "x", ",", "y", ",", "task", "=", "taskname", ")", "# y_hat: (N, T)", "\n", "\n", "Words", ".", "extend", "(", "words", ")", "\n", "Is_heads", ".", "extend", "(", "is_heads", ")", "\n", "Tags", ".", "extend", "(", "tags", ")", "\n", "Y", ".", "extend", "(", "y", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "\n", "Y_hat", ".", "extend", "(", "y_hat", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "\n", "", "except", ":", "\n", "                ", "print", "(", "'error @'", ",", "batch", ")", "\n", "\n", "# gets results and save", "\n", "", "", "", "results", "=", "[", "]", "\n", "for", "words", ",", "is_heads", ",", "tags", ",", "y_hat", "in", "zip", "(", "Words", ",", "Is_heads", ",", "Tags", ",", "Y_hat", ")", ":", "\n", "        ", "y_hat", "=", "[", "hat", "for", "head", ",", "hat", "in", "zip", "(", "is_heads", ",", "y_hat", ")", "if", "head", "==", "1", "]", "\n", "# remove the first and the last token", "\n", "preds", "=", "[", "dataset", ".", "idx2tag", "[", "hat", "]", "for", "hat", "in", "y_hat", "]", "[", "1", ":", "-", "1", "]", "\n", "results", ".", "append", "(", "preds", ")", "\n", "\n", "", "return", "source", ",", "token_pos_list", ",", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.do_pairing": [[129, 264], ["zip", "snippext.dataset.SnippextDataset", "torch.utils.data.DataLoader", "set", "enumerate", "enumerate", "candidate_pairs.sort", "torch.no_grad", "torch.no_grad", "enumerate", "results.append", "[].append", "range", "range", "sent_ids.append", "candidates.append", "positions.append", "model", "Y_hat.extend", "Y.extend", "samples[].split", "candidate_pairs.append", "token_ids.append", "token_ids.append", "samples.append", "samples.append", "y_hat.cpu().numpy().tolist", "y.cpu().numpy().tolist", "[].append", "aspects.append", "opinions.append", "abs", "set.add", "len", "y_hat.cpu().numpy", "y.cpu().numpy", "y_hat.cpu", "y.cpu"], "function", ["None"], ["", "def", "do_pairing", "(", "all_tokens", ",", "all_tags", ",", "config", ",", "model", ")", ":", "\n", "    ", "\"\"\"Apply the pairing model.\n\n    Args:\n        all_tokens (list of list of str): the tokenized text\n        all_tags (list of list of str): the tags assigned to each token\n        config (dict): the model configuration\n        model (MultiTaskNet): the model in pytorch\n\n    Returns:\n        list of dict: For each sentence, the list of extracted\n            opinions/experiences from the sentence. Each dictionary includes\n            an aspect term and an opinion term and the start/end\n            position of the aspect/opinion term.\n    \"\"\"", "\n", "samples", "=", "[", "]", "\n", "sent_ids", "=", "[", "]", "\n", "candidates", "=", "[", "]", "\n", "positions", "=", "[", "]", "\n", "all_spans", "=", "{", "}", "\n", "\n", "sid", "=", "0", "\n", "for", "tokens", ",", "tags", "in", "zip", "(", "all_tokens", ",", "all_tags", ")", ":", "\n", "        ", "aspects", "=", "[", "]", "\n", "opinions", "=", "[", "]", "\n", "# find aspects", "\n", "# find opinions", "\n", "for", "i", ",", "tag", "in", "enumerate", "(", "tags", ")", ":", "\n", "            ", "if", "tag", "[", "0", "]", "==", "'B'", ":", "\n", "                ", "start", "=", "i", "\n", "end", "=", "i", "\n", "while", "end", "+", "1", "<", "len", "(", "tags", ")", "and", "tags", "[", "end", "+", "1", "]", "[", "0", "]", "==", "'I'", ":", "\n", "                    ", "end", "+=", "1", "\n", "", "if", "tag", "==", "'B-AS'", ":", "\n", "                    ", "aspects", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "all_spans", "[", "(", "sid", ",", "start", ",", "end", ")", "]", "=", "{", "'aspect'", ":", "' '", ".", "join", "(", "tokens", "[", "start", ":", "end", "+", "1", "]", ")", ",", "\n", "'sid'", ":", "sid", ",", "\n", "'asp_start'", ":", "start", ",", "\n", "'asp_end'", ":", "end", "}", "\n", "", "else", ":", "\n", "                    ", "opinions", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "all_spans", "[", "(", "sid", ",", "start", ",", "end", ")", "]", "=", "{", "'opinion'", ":", "' '", ".", "join", "(", "tokens", "[", "start", ":", "end", "+", "1", "]", ")", ",", "\n", "'sid'", ":", "sid", ",", "\n", "'op_start'", ":", "start", ",", "\n", "'op_end'", ":", "end", "}", "\n", "\n", "", "", "", "candidate_pairs", "=", "[", "]", "\n", "for", "asp", "in", "aspects", ":", "\n", "            ", "for", "opi", "in", "opinions", ":", "\n", "                ", "candidate_pairs", ".", "append", "(", "(", "asp", ",", "opi", ")", ")", "\n", "", "", "candidate_pairs", ".", "sort", "(", "key", "=", "lambda", "ao", ":", "abs", "(", "ao", "[", "0", "]", "[", "0", "]", "-", "ao", "[", "1", "]", "[", "0", "]", ")", ")", "\n", "\n", "for", "asp", ",", "opi", "in", "candidate_pairs", ":", "\n", "            ", "asp_start", ",", "asp_end", "=", "asp", "\n", "op_start", ",", "op_end", "=", "opi", "\n", "token_ids", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "asp_start", ",", "asp_end", "+", "1", ")", ":", "\n", "                ", "token_ids", ".", "append", "(", "(", "sid", ",", "i", ")", ")", "\n", "", "for", "i", "in", "range", "(", "op_start", ",", "op_end", "+", "1", ")", ":", "\n", "                ", "token_ids", ".", "append", "(", "(", "sid", ",", "i", ")", ")", "\n", "\n", "", "if", "op_start", "<", "asp_start", ":", "\n", "                ", "samples", ".", "append", "(", "' '", ".", "join", "(", "tokens", ")", "+", "' [SEP] '", "+", "' '", ".", "join", "(", "tokens", "[", "op_start", ":", "op_end", "+", "1", "]", ")", "+", "' '", "+", "' '", ".", "join", "(", "tokens", "[", "asp_start", ":", "asp_end", "+", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "samples", ".", "append", "(", "' '", ".", "join", "(", "tokens", ")", "+", "' [SEP] '", "+", "' '", ".", "join", "(", "tokens", "[", "asp_start", ":", "asp_end", "+", "1", "]", ")", "+", "' '", "+", "' '", ".", "join", "(", "tokens", "[", "op_start", ":", "op_end", "+", "1", "]", ")", ")", "\n", "\n", "", "sent_ids", ".", "append", "(", "sid", ")", "\n", "candidates", ".", "append", "(", "{", "'opinion'", ":", "' '", ".", "join", "(", "tokens", "[", "op_start", ":", "op_end", "+", "1", "]", ")", ",", "\n", "'aspect'", ":", "' '", ".", "join", "(", "tokens", "[", "asp_start", ":", "asp_end", "+", "1", "]", ")", ",", "\n", "'sid'", ":", "sid", ",", "\n", "'asp_start'", ":", "asp_start", ",", "\n", "'asp_end'", ":", "asp_end", ",", "\n", "'op_start'", ":", "op_start", ",", "\n", "'op_end'", ":", "op_end", "}", ")", "\n", "positions", ".", "append", "(", "token_ids", ")", "\n", "", "sid", "+=", "1", "\n", "\n", "", "dataset", "=", "SnippextDataset", "(", "samples", ",", "config", "[", "'vocab'", "]", ",", "config", "[", "'name'", "]", ",", "\n", "lm", "=", "model", ".", "lm", ")", "\n", "iterator", "=", "data", ".", "DataLoader", "(", "dataset", "=", "dataset", ",", "\n", "batch_size", "=", "32", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "SnippextDataset", ".", "pad", ")", "\n", "\n", "# prediction", "\n", "Y_hat", "=", "[", "]", "\n", "Y", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "iterator", ")", ":", "\n", "            ", "words", ",", "x", ",", "is_heads", ",", "tags", ",", "mask", ",", "y", ",", "seqlens", ",", "taskname", "=", "batch", "\n", "taskname", "=", "taskname", "[", "0", "]", "\n", "_", ",", "y", ",", "y_hat", "=", "model", "(", "x", ",", "y", ",", "task", "=", "taskname", ")", "# y_hat: (N, T)", "\n", "Y_hat", ".", "extend", "(", "y_hat", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "\n", "Y", ".", "extend", "(", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "results", "=", "[", "]", "\n", "for", "tokens", "in", "all_tokens", ":", "\n", "        ", "results", ".", "append", "(", "{", "'sentence'", ":", "' '", ".", "join", "(", "tokens", ")", ",", "\n", "'extractions'", ":", "[", "]", "}", ")", "\n", "\n", "", "used", "=", "set", "(", "[", "]", ")", "\n", "for", "i", ",", "yhat", "in", "enumerate", "(", "Y_hat", ")", ":", "\n", "        ", "phrase", "=", "samples", "[", "i", "]", ".", "split", "(", "' [SEP] '", ")", "[", "1", "]", "\n", "# print(phrase, yhat)", "\n", "if", "yhat", "==", "1", ":", "\n", "# do some filtering", "\n", "            ", "assigned", "=", "False", "\n", "for", "tid", "in", "positions", "[", "i", "]", ":", "\n", "                ", "if", "tid", "in", "used", ":", "\n", "                    ", "assigned", "=", "True", "\n", "break", "\n", "\n", "", "", "if", "not", "assigned", ":", "\n", "                ", "results", "[", "sent_ids", "[", "i", "]", "]", "[", "'extractions'", "]", ".", "append", "(", "candidates", "[", "i", "]", ")", "\n", "for", "tid", "in", "positions", "[", "i", "]", ":", "\n", "                    ", "used", ".", "add", "(", "tid", ")", "\n", "# drop from all_spans", "\n", "", "sid", "=", "candidates", "[", "i", "]", "[", "'sid'", "]", "\n", "del", "all_spans", "[", "(", "sid", ",", "\n", "candidates", "[", "i", "]", "[", "'asp_start'", "]", ",", "\n", "candidates", "[", "i", "]", "[", "'asp_end'", "]", ")", "]", "\n", "del", "all_spans", "[", "(", "sid", ",", "\n", "candidates", "[", "i", "]", "[", "'op_start'", "]", ",", "\n", "candidates", "[", "i", "]", "[", "'op_end'", "]", ")", "]", "\n", "\n", "# add aspects/opinions that are not paired", "\n", "", "", "", "for", "sid", ",", "start", ",", "end", "in", "all_spans", ":", "\n", "        ", "results", "[", "sid", "]", "[", "'extractions'", "]", ".", "append", "(", "all_spans", "[", "(", "sid", ",", "start", ",", "end", ")", "]", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.classify": [[266, 325], ["enumerate", "snippext.dataset.SnippextDataset", "torch.utils.data.DataLoader", "range", "enumerate", "torch.no_grad", "torch.no_grad", "enumerate", "len", "phrases.append", "index.append", "model", "Y_hat.extend", "y_hat.cpu().numpy().tolist", "y_hat.cpu().numpy", "y_hat.cpu"], "function", ["None"], ["", "def", "classify", "(", "extractions", ",", "config", ",", "model", ",", "sents", "=", "None", ")", ":", "\n", "    ", "\"\"\"Apply the classification models (for Sentiment and Attribute Classification).\n\n    Args:\n        extractions (list of dict): the partial extraction results by the pairing model\n        config (dict): the model configuration\n        model (MultiTaskNet): the model in pytorch\n\n    Returns:\n        list of dict: the extraction results with attribute name and sentiment score\n            assigned to the field \"attribute\" and \"sentiment\".\n    \"\"\"", "\n", "phrases", "=", "[", "]", "\n", "index", "=", "[", "]", "\n", "# print('Prepare classification data')", "\n", "for", "sid", ",", "sent", "in", "enumerate", "(", "extractions", ")", ":", "\n", "        ", "for", "eid", ",", "ext", "in", "enumerate", "(", "sent", "[", "'extractions'", "]", ")", ":", "\n", "            ", "if", "'asc'", "in", "config", "[", "'name'", "]", ":", "\n", "                ", "if", "'aspect'", "in", "ext", ":", "\n", "                    ", "phrase", "=", "' '", ".", "join", "(", "sents", "[", "ext", "[", "'sid'", "]", "]", ")", "+", "'\\t'", "+", "ext", "[", "'aspect'", "]", "\n", "", "else", ":", "\n", "                    ", "phrase", "=", "' '", ".", "join", "(", "sents", "[", "ext", "[", "'sid'", "]", "]", ")", "+", "'\\t'", "+", "ext", "[", "'opinion'", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "'aspect'", "in", "ext", "and", "'opinion'", "in", "ext", ":", "\n", "                    ", "phrase", "=", "ext", "[", "'opinion'", "]", "+", "' '", "+", "ext", "[", "'aspect'", "]", "\n", "", "elif", "'aspect'", "in", "ext", ":", "\n", "                    ", "phrase", "=", "ext", "[", "'aspect'", "]", "\n", "", "else", ":", "\n", "                    ", "phrase", "=", "ext", "[", "'opinion'", "]", "\n", "", "", "phrases", ".", "append", "(", "phrase", ")", "\n", "index", ".", "append", "(", "(", "sid", ",", "eid", ")", ")", "\n", "\n", "", "", "dataset", "=", "SnippextDataset", "(", "phrases", ",", "config", "[", "'vocab'", "]", ",", "config", "[", "'name'", "]", ",", "\n", "lm", "=", "model", ".", "lm", ")", "\n", "iterator", "=", "data", ".", "DataLoader", "(", "dataset", "=", "dataset", ",", "\n", "batch_size", "=", "32", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "SnippextDataset", ".", "pad", ")", "\n", "\n", "# prediction", "\n", "Y_hat", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# print('Classification')", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "iterator", ")", ":", "\n", "            ", "words", ",", "x", ",", "is_heads", ",", "tags", ",", "mask", ",", "y", ",", "seqlens", ",", "taskname", "=", "batch", "\n", "taskname", "=", "taskname", "[", "0", "]", "\n", "_", ",", "_", ",", "y_hat", "=", "model", "(", "x", ",", "y", ",", "task", "=", "taskname", ")", "# y_hat: (N, T)", "\n", "Y_hat", ".", "extend", "(", "y_hat", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "for", "i", "in", "range", "(", "len", "(", "phrases", ")", ")", ":", "\n", "        ", "attr", "=", "dataset", ".", "idx2tag", "[", "Y_hat", "[", "i", "]", "]", "\n", "sid", ",", "eid", "=", "index", "[", "i", "]", "\n", "if", "'asc'", "in", "config", "[", "'name'", "]", ":", "\n", "            ", "extractions", "[", "sid", "]", "[", "'extractions'", "]", "[", "eid", "]", "[", "'sentiment'", "]", "=", "attr", "\n", "", "else", ":", "\n", "            ", "extractions", "[", "sid", "]", "[", "'extractions'", "]", "[", "eid", "]", "[", "'attribute'", "]", "=", "attr", "\n", "\n", "", "", "return", "extractions", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.extract": [[326, 355], ["time.time", "run_pipeline.do_tagging", "run_pipeline.do_pairing", "run_pipeline.classify", "run_pipeline.classify", "zip", "review[].append"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.do_tagging", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.do_pairing", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.classify", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.classify"], ["", "def", "extract", "(", "review", ",", "config_list", ",", "models", ")", ":", "\n", "    ", "\"\"\"Extract experiences and opinions from a paragraph of text\n\n    Args:\n        review (Dictionary): a review object with a text field to be extracted\n        config_list (list of dictionary): a list of task config dictionary\n        models (list of MultiTaskNet): the most of models\n\n    Returns:\n        Dictionary: the same review object with a new extraction field\n    \"\"\"", "\n", "text", "=", "review", "[", "'content'", "]", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "# tagging", "\n", "all_tokens", ",", "token_pos", ",", "all_tags", "=", "do_tagging", "(", "text", ",", "config_list", "[", "0", "]", ",", "models", "[", "0", "]", ")", "\n", "# pairing", "\n", "extractions", "=", "do_pairing", "(", "all_tokens", ",", "all_tags", ",", "config_list", "[", "1", "]", ",", "models", "[", "1", "]", ")", "\n", "# classification", "\n", "extractions", "=", "classify", "(", "extractions", ",", "config_list", "[", "2", "]", ",", "models", "[", "2", "]", ")", "\n", "# asc", "\n", "extractions", "=", "classify", "(", "extractions", ",", "config_list", "[", "3", "]", ",", "models", "[", "3", "]", ",", "sents", "=", "all_tokens", ")", "\n", "\n", "review", "[", "'extractions'", "]", "=", "[", "]", "\n", "review", "[", "'sentences'", "]", "=", "[", "]", "\n", "for", "sent", ",", "tokens", "in", "zip", "(", "extractions", ",", "all_tokens", ")", ":", "\n", "        ", "review", "[", "'extractions'", "]", "+=", "sent", "[", "'extractions'", "]", "\n", "review", "[", "'sentences'", "]", ".", "append", "(", "tokens", ")", "\n", "", "return", "review", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.load_model": [[357, 383], ["snippext.model.MultiTaskNet", "torch.load", "torch.load", "amp.initialize.load_state_dict", "amp.initialize.to", "amp.initialize"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.initialize"], ["", "def", "load_model", "(", "config", ",", "\n", "path", ",", "\n", "device", "=", "'gpu'", ",", "\n", "lm", "=", "'bert'", ",", "\n", "fp16", "=", "False", ")", ":", "\n", "    ", "\"\"\"Load a model for a specific task.\n\n    Args:\n        config (dictionary): the task dictionary\n        path (string): the path to the checkpoint\n        lm (str, optional): the language model (bert, distilbert, or albert)\n        fp16 (boolean): whether to use fp16 optimization\n\n    Returns:\n        MultiTaskNet: the model\n    \"\"\"", "\n", "model", "=", "MultiTaskNet", "(", "[", "config", "]", ",", "device", ",", "True", ",", "lm", "=", "lm", ")", "\n", "saved_state", "=", "torch", ".", "load", "(", "path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "model", ".", "load_state_dict", "(", "saved_state", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "fp16", "and", "'cuda'", "in", "device", ":", "\n", "        ", "from", "apex", "import", "amp", "\n", "model", "=", "amp", ".", "initialize", "(", "model", ",", "opt_level", "=", "'O2'", ")", "\n", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.predict": [[384, 405], ["jsonlines.open", "open", "csv.DictReader", "tqdm.tqdm", "enumerate", "run_pipeline.extract", "writer.write", "writer.write"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.extract"], ["", "def", "predict", "(", "input_fn", ",", "output_fn", ",", "config_list", ",", "models", ")", ":", "\n", "    ", "\"\"\"Run the extraction on an input csv file.\n\n    Args:\n        input_fn (str): the input file name (.csv)\n        output_fn (str): the output file name (.jsonl)\n        config_list (list of dict): the list of configuration\n        models (list of MultiTaskNet): the list of models\n\n    Returns:\n        None\n    \"\"\"", "\n", "with", "jsonlines", ".", "open", "(", "output_fn", ",", "mode", "=", "'w'", ")", "as", "writer", ":", "\n", "        ", "with", "open", "(", "input_fn", ")", "as", "fin", ":", "\n", "            ", "reader", "=", "csv", ".", "DictReader", "(", "fin", ")", "\n", "for", "idx", ",", "row", "in", "tqdm", "(", "enumerate", "(", "reader", ")", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "review", "=", "extract", "(", "row", ",", "config_list", ",", "models", ")", "\n", "writer", ".", "write", "(", "review", ")", "\n", "", "except", ":", "\n", "                    ", "writer", ".", "write", "(", "row", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.initialize": [[406, 442], ["dict", "json.load", "open", "run_pipeline.load_model", "torch.cuda.is_available", "torch.cuda.is_available", "os.path.join"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.load_model"], ["", "", "", "", "", "def", "initialize", "(", "checkpoint_path", ",", "\n", "use_gpu", "=", "False", ",", "\n", "lm", "=", "'bert'", ",", "\n", "fp16", "=", "False", ",", "\n", "tasks", "=", "[", "'hotel_tagging'", ",", "\n", "'pairing'", ",", "\n", "'sf_hotel_classification'", ",", "\n", "'restaurant_asc'", "]", ")", ":", "\n", "    ", "\"\"\"load the models from a path storing the checkpoints.\n\n    Args:\n        checkpoint_path (str): the path to the checkpoints\n        use_gpu (boolean, optional): whether to use gpu\n        lm (string, optional): the language model (default: bert)\n        fp16 (boolean): whether to use fp16\n        tasks (list of str, optional): the list of snippext tasks\n    Returns:\n        list of dictionary: the configuration list\n        list of MultiTaskNet: the list of models\n    \"\"\"", "\n", "# load models", "\n", "checkpoints", "=", "dict", "(", "[", "(", "task", ",", "os", ".", "path", ".", "join", "(", "checkpoint_path", ",", "'%s.pt'", "%", "task", ")", ")", "for", "task", "in", "tasks", "]", ")", "\n", "configs", "=", "json", ".", "load", "(", "open", "(", "'configs.json'", ")", ")", "\n", "configs", "=", "{", "conf", "[", "'name'", "]", ":", "conf", "for", "conf", "in", "configs", "}", "\n", "\n", "if", "use_gpu", ":", "\n", "        ", "device", "=", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", "\n", "", "else", ":", "\n", "        ", "device", "=", "'cpu'", "\n", "\n", "", "models", "=", "[", "load_model", "(", "configs", "[", "task", "]", ",", "checkpoints", "[", "task", "]", ",", "device", "=", "device", ",", "\n", "lm", "=", "lm", ",", "fp16", "=", "fp16", ")", "for", "task", "in", "tasks", "]", "\n", "config_list", "=", "[", "configs", "[", "task", "]", "for", "task", "in", "tasks", "]", "\n", "\n", "return", "config_list", ",", "models", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_index_builder.IndexBuilder.__init__": [[33, 54], ["json.load", "list", "list", "dict", "snippext.dataset.get_tokenizer", "augment_index_builder.IndexBuilder.init_token_index", "augment_index_builder.IndexBuilder.init_span_index", "augment_index_builder.IndexBuilder.index_token_replacement", "augment_utils.read_tagging_file", "augment_utils.read_asc_file", "list", "open", "dict", "dict", "augment_index_builder.IndexBuilder.calc_senti_score", "map"], "methods", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.get_tokenizer", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_index_builder.IndexBuilder.init_token_index", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_index_builder.IndexBuilder.init_span_index", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_index_builder.IndexBuilder.index_token_replacement", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.read_tagging_file", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.read_asc_file", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_index_builder.IndexBuilder.calc_senti_score"], ["def", "__init__", "(", "self", ",", "train_fn", ",", "idf_fn", ",", "w2v", ",", "task", ",", "bert_path", ",", "lm", "=", "'bert'", ")", ":", "\n", "        ", "if", "'tagging'", "in", "task", "or", "'qa'", "in", "task", ":", "\n", "            ", "self", ".", "tokens", ",", "self", ".", "labels", "=", "read_tagging_file", "(", "train_fn", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "sents", "=", "read_asc_file", "(", "train_fn", ")", "\n", "self", ".", "tokens", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "'token'", "]", ",", "self", ".", "sents", ")", ")", "\n", "\n", "", "idf_dict", "=", "json", ".", "load", "(", "open", "(", "idf_fn", ")", ")", "\n", "self", ".", "w2v", "=", "w2v", "\n", "self", ".", "task", "=", "task", "\n", "self", ".", "index", "=", "{", "'token'", ":", "dict", "(", ")", ",", "'span'", ":", "dict", "(", ")", "}", "\n", "self", ".", "all_spans", "=", "list", "(", ")", "\n", "self", ".", "span_freqs", "=", "list", "(", ")", "\n", "self", ".", "avg_senti", "=", "dict", "(", ")", "\n", "if", "self", ".", "task", "==", "'classification'", ":", "\n", "# sentiment sensitive", "\n", "            ", "self", ".", "calc_senti_score", "(", ")", "\n", "", "self", ".", "tokenizer", "=", "get_tokenizer", "(", "lm", "=", "lm", ")", "\n", "self", ".", "init_token_index", "(", "idf_dict", ")", "\n", "self", ".", "init_span_index", "(", "bert_path", "=", "bert_path", ")", "\n", "self", ".", "index_token_replacement", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_index_builder.IndexBuilder.init_token_index": [[55, 73], ["math.log", "dict", "w.lower", "augment_index_builder.IndexBuilder.tokenizer.tokenize", "len"], "methods", ["None"], ["", "def", "init_token_index", "(", "self", ",", "idf_dict", ")", ":", "\n", "        ", "oov_th", "=", "math", ".", "log", "(", "1e8", ")", "\n", "for", "token", "in", "self", ".", "tokens", ":", "\n", "            ", "for", "w", "in", "token", ":", "\n", "                ", "if", "w", "not", "in", "self", ".", "index", "[", "'token'", "]", ":", "\n", "                    ", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "=", "dict", "(", ")", "\n", "wl", "=", "w", ".", "lower", "(", ")", "\n", "\n", "if", "wl", "not", "in", "idf_dict", ":", "\n", "                        ", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "[", "'idf'", "]", "=", "oov_th", "\n", "", "else", ":", "\n", "                        ", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "[", "'idf'", "]", "=", "idf_dict", "[", "wl", "]", "\n", "", "tokenized_w", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "w", ")", "\n", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "[", "'bert_token'", "]", "=", "tokenized_w", "\n", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "[", "'bert_length'", "]", "=", "len", "(", "tokenized_w", ")", "\n", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "[", "'similar_words'", "]", "=", "None", "\n", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "[", "'similar_words_bert'", "]", "=", "None", "\n", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "[", "'similar_words_length'", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_index_builder.IndexBuilder.init_span_index": [[74, 294], ["transformers.BertModel.from_pretrained.eval", "dict", "dict", "len", "range", "range", "range", "transformers.BertModel.from_pretrained", "torch.load", "transformers.BertModel.from_pretrained", "as_ids.append", "op_ids.append", "len", "torch.LongTensor", "as_encoded_layers[].detach", "len", "torch.LongTensor", "op_encoded_layers[].detach", "len", "torch.argsort", "len", "torch.argsort", "augment_index_builder.IndexBuilder.tokenizer.tokenize", "enumerate", "len", "max", "len", "augment_index_builder.IndexBuilder.tokenizer.convert_tokens_to_ids", "augment_index_builder.IndexBuilder.tokenizer.convert_tokens_to_ids", "transformers.BertModel.from_pretrained.", "transformers.BertModel.from_pretrained.", "aspect_token_list.append", "aspect_raw_token_list.append", "enumerate", "len", "max", "enumerate", "len", "max", "torch.sum", "torch.tensor", "len", "[].append", "[].append", "torch.sum", "torch.tensor", "len", "[].append", "[].append", "as_labels.append", "as_labels.append", "aspects.append", "augment_index_builder.IndexBuilder.tokenizer.tokenize", "enumerate", "aspect_token_list.append", "aspect_raw_token_list.append", "augment_index_builder.IndexBuilder.tokenizer.tokenize", "enumerate", "opinion_token_list.append", "opinion_raw_token_list.append", "range", "range", "torch.sum", "torch.tensor", "torch.sum", "torch.tensor", "numpy.linalg.norm", "numpy.linalg.norm", "torch.sum", "torch.tensor", "score[].item", "numpy.linalg.norm", "numpy.linalg.norm", "torch.sum", "torch.tensor", "score[].item", "len", "aspects[].append", "opinions.append", "as_labels.append", "as_labels.append", "op_labels.append", "op_labels.append", "len", "len", "numpy.linalg.norm", "numpy.linalg.norm", "numpy.linalg.norm", "numpy.linalg.norm", "numpy.linalg.norm", "numpy.linalg.norm", "numpy.linalg.norm", "numpy.linalg.norm", "len", "opinions[].append"], "methods", ["None"], ["", "", "", "", "def", "init_span_index", "(", "self", ",", "sim_token", "=", "'cls'", ",", "sim_topk", "=", "100", ",", "bert_path", "=", "None", ")", ":", "\n", "        ", "if", "bert_path", "is", "None", ":", "\n", "            ", "bert_model", "=", "BertModel", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "output_hidden_states", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "model_state_dict", "=", "torch", ".", "load", "(", "bert_path", ")", "\n", "bert_model", "=", "BertModel", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "\n", "state_dict", "=", "model_state_dict", ",", "\n", "output_hidden_states", "=", "True", ")", "\n", "", "bert_model", ".", "eval", "(", ")", "\n", "\n", "aspect_dict", "=", "dict", "(", ")", "\n", "opinion_dict", "=", "dict", "(", ")", "\n", "aspect_token_list", "=", "[", "]", "\n", "opinion_token_list", "=", "[", "]", "\n", "aspect_raw_token_list", "=", "[", "]", "\n", "opinion_raw_token_list", "=", "[", "]", "\n", "\n", "n", "=", "len", "(", "self", ".", "tokens", ")", "\n", "max_len_as", "=", "0", "\n", "max_len_op", "=", "0", "\n", "for", "j", "in", "range", "(", "n", ")", ":", "\n", "            ", "if", "'classification'", "in", "self", ".", "task", ":", "\n", "# for ASC datasets, the aspect is given in the term field", "\n", "                ", "as_term", "=", "self", ".", "sents", "[", "j", "]", "[", "'term'", "]", "\n", "\n", "# as_labels", "\n", "as_labels", "=", "[", "]", "\n", "tokenized_as", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "as_term", ")", "\n", "for", "idx_t", ",", "t", "in", "enumerate", "(", "tokenized_as", ")", ":", "\n", "                    ", "if", "idx_t", "==", "0", ":", "\n", "                        ", "as_labels", ".", "append", "(", "1", "if", "idx_t", "==", "0", "else", "2", ")", "\n", "", "else", ":", "\n", "                        ", "as_labels", ".", "append", "(", "0", ")", "\n", "\n", "", "", "len_as", "=", "len", "(", "tokenized_as", ")", "\n", "max_len_as", "=", "max", "(", "len_as", ",", "max_len_as", ")", "\n", "as_str", "=", "' '", ".", "join", "(", "tokenized_as", ")", "\n", "if", "as_term", "not", "in", "aspect_dict", ":", "\n", "                    ", "aspect_dict", "[", "as_term", "]", "=", "{", "\n", "'document_freq'", ":", "1", ",", "\n", "'bert_token'", ":", "tokenized_as", ",", "\n", "'bert_length'", ":", "len_as", ",", "\n", "'bert_label'", ":", "as_labels", ",", "\n", "'similar_spans'", ":", "[", "]", ",", "\n", "'similar_spans_length'", ":", "[", "]", "\n", "}", "\n", "aspect_token_list", ".", "append", "(", "tokenized_as", ")", "\n", "aspect_raw_token_list", ".", "append", "(", "as_term", ")", "\n", "", "else", ":", "\n", "                    ", "aspect_dict", "[", "as_term", "]", "[", "'document_freq'", "]", "+=", "1", "\n", "\n", "", "", "else", ":", "\n", "# for AOE datasets, we have to enumerate tokens to find all aspects and opinions", "\n", "                ", "aspects", "=", "[", "]", "\n", "opinions", "=", "[", "]", "\n", "m", "=", "len", "(", "self", ".", "tokens", "[", "j", "]", ")", "\n", "k", "=", "0", "\n", "while", "k", "<", "m", ":", "\n", "                    ", "if", "'B-AS'", "in", "self", ".", "labels", "[", "j", "]", "[", "k", "]", ":", "\n", "                        ", "aspects", ".", "append", "(", "[", "self", ".", "tokens", "[", "j", "]", "[", "k", "]", "]", ")", "\n", "k", "+=", "1", "\n", "", "elif", "'I-AS'", "in", "self", ".", "labels", "[", "j", "]", "[", "k", "]", ":", "\n", "# ignore spans that are incorrectly labeled", "\n", "                        ", "if", "len", "(", "aspects", ")", ">", "0", ":", "\n", "                            ", "aspects", "[", "-", "1", "]", ".", "append", "(", "self", ".", "tokens", "[", "j", "]", "[", "k", "]", ")", "\n", "", "k", "+=", "1", "\n", "", "elif", "'B-OP'", "in", "self", ".", "labels", "[", "j", "]", "[", "k", "]", ":", "\n", "                        ", "opinions", ".", "append", "(", "[", "self", ".", "tokens", "[", "j", "]", "[", "k", "]", "]", ")", "\n", "k", "+=", "1", "\n", "", "elif", "'I-OP'", "in", "self", ".", "labels", "[", "j", "]", "[", "k", "]", ":", "\n", "# ignore spans that are incorrectly labeled", "\n", "                        ", "if", "len", "(", "opinions", ")", ">", "0", ":", "\n", "                            ", "opinions", "[", "-", "1", "]", ".", "append", "(", "self", ".", "tokens", "[", "j", "]", "[", "k", "]", ")", "\n", "", "k", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "k", "+=", "1", "\n", "\n", "", "", "for", "as_term", "in", "aspects", ":", "\n", "                    ", "tokenized_as", "=", "[", "]", "\n", "as_labels", "=", "[", "]", "\n", "for", "idx_as", ",", "w", "in", "enumerate", "(", "as_term", ")", ":", "\n", "                        ", "tokenized_w", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "w", ")", "\n", "for", "idx_t", ",", "t", "in", "enumerate", "(", "tokenized_w", ")", ":", "\n", "                            ", "if", "idx_t", "==", "0", ":", "\n", "                                ", "as_labels", ".", "append", "(", "1", "if", "idx_as", "==", "0", "else", "2", ")", "\n", "", "else", ":", "\n", "                                ", "as_labels", ".", "append", "(", "0", ")", "\n", "", "", "tokenized_as", "+=", "tokenized_w", "\n", "", "len_as", "=", "len", "(", "tokenized_as", ")", "\n", "max_len_as", "=", "max", "(", "len_as", ",", "max_len_as", ")", "\n", "as_str", "=", "' '", ".", "join", "(", "tokenized_as", ")", "\n", "as_raw", "=", "' '", ".", "join", "(", "as_term", ")", "\n", "if", "as_raw", "not", "in", "aspect_dict", ":", "\n", "                        ", "aspect_dict", "[", "as_raw", "]", "=", "{", "\n", "'document_freq'", ":", "1", ",", "\n", "'bert_token'", ":", "tokenized_as", ",", "\n", "'bert_length'", ":", "len_as", ",", "\n", "'bert_label'", ":", "as_labels", ",", "\n", "'similar_spans'", ":", "[", "]", ",", "\n", "'similar_spans_length'", ":", "[", "]", "\n", "}", "\n", "aspect_token_list", ".", "append", "(", "tokenized_as", ")", "\n", "aspect_raw_token_list", ".", "append", "(", "as_raw", ")", "\n", "", "else", ":", "\n", "                        ", "aspect_dict", "[", "as_raw", "]", "[", "'document_freq'", "]", "+=", "1", "\n", "\n", "", "", "for", "op_term", "in", "opinions", ":", "\n", "                    ", "tokenized_op", "=", "[", "]", "\n", "op_labels", "=", "[", "]", "\n", "for", "idx_op", ",", "w", "in", "enumerate", "(", "op_term", ")", ":", "\n", "                        ", "tokenized_w", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "w", ")", "\n", "for", "idx_t", ",", "t", "in", "enumerate", "(", "tokenized_w", ")", ":", "\n", "                            ", "if", "idx_t", "==", "0", ":", "\n", "                                ", "op_labels", ".", "append", "(", "3", "if", "idx_op", "==", "0", "else", "4", ")", "\n", "", "else", ":", "\n", "                                ", "op_labels", ".", "append", "(", "0", ")", "\n", "", "", "tokenized_op", "+=", "tokenized_w", "\n", "", "len_op", "=", "len", "(", "tokenized_op", ")", "\n", "max_len_op", "=", "max", "(", "len_op", ",", "max_len_op", ")", "\n", "op_str", "=", "' '", ".", "join", "(", "tokenized_op", ")", "\n", "op_raw", "=", "' '", ".", "join", "(", "op_term", ")", "\n", "if", "op_raw", "not", "in", "opinion_dict", ":", "\n", "                        ", "opinion_dict", "[", "op_raw", "]", "=", "{", "\n", "'document_freq'", ":", "1", ",", "\n", "'bert_token'", ":", "tokenized_op", ",", "\n", "'bert_length'", ":", "len_op", ",", "\n", "'bert_label'", ":", "op_labels", ",", "\n", "'similar_spans'", ":", "[", "]", ",", "\n", "'similar_spans_length'", ":", "[", "]", "\n", "}", "\n", "opinion_token_list", ".", "append", "(", "tokenized_op", ")", "\n", "opinion_raw_token_list", ".", "append", "(", "op_raw", ")", "\n", "", "else", ":", "\n", "                        ", "opinion_dict", "[", "op_raw", "]", "[", "'document_freq'", "]", "+=", "1", "\n", "\n", "", "", "", "", "as_ids", "=", "[", "]", "\n", "op_ids", "=", "[", "]", "\n", "# Pad to max length and convert to ids", "\n", "for", "as_term", "in", "aspect_token_list", ":", "\n", "            ", "tk_as", "=", "[", "'[CLS]'", "]", "+", "as_term", "+", "[", "'[SEP]'", "]", "+", "[", "'[PAD]'", "for", "k", "in", "range", "(", "max_len_as", "-", "len", "(", "as_term", ")", ")", "]", "\n", "as_ids", ".", "append", "(", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tk_as", ")", ")", "\n", "", "for", "op_term", "in", "opinion_token_list", ":", "\n", "            ", "tk_op", "=", "[", "'[CLS]'", "]", "+", "op_term", "+", "[", "'[SEP]'", "]", "+", "[", "'[PAD]'", "for", "k", "in", "range", "(", "max_len_op", "-", "len", "(", "op_term", ")", ")", "]", "\n", "op_ids", ".", "append", "(", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tk_op", ")", ")", "\n", "\n", "# migrated to transformers", "\n", "", "if", "len", "(", "aspect_token_list", ")", ">", "0", ":", "\n", "            ", "X_as", "=", "torch", ".", "LongTensor", "(", "as_ids", ")", "\n", "as_encoded_layers", "=", "bert_model", "(", "X_as", ")", "[", "2", "]", "\n", "X_as", "=", "as_encoded_layers", "[", "-", "2", "]", ".", "detach", "(", ")", "\n", "\n", "", "if", "len", "(", "opinion_token_list", ")", ">", "0", ":", "\n", "            ", "X_op", "=", "torch", ".", "LongTensor", "(", "op_ids", ")", "\n", "op_encoded_layers", "=", "bert_model", "(", "X_op", ")", "[", "2", "]", "\n", "X_op", "=", "op_encoded_layers", "[", "-", "2", "]", ".", "detach", "(", ")", "\n", "\n", "# Compute the dot-product between all pairs of spans", "\n", "", "for", "i", "in", "range", "(", "len", "(", "aspect_token_list", ")", ")", ":", "\n", "            ", "if", "sim_token", "==", "'all'", ":", "\n", "# using all tokens", "\n", "                ", "q", "=", "X_as", "[", "i", "]", "\n", "z", "=", "q", "*", "X_as", "\n", "score", "=", "torch", ".", "sum", "(", "z", ",", "dim", "=", "(", "1", ",", "2", ")", ")", "/", "torch", ".", "tensor", "(", "\n", "np", ".", "linalg", ".", "norm", "(", "q", ")", "*", "np", ".", "linalg", ".", "norm", "(", "X_as", ",", "axis", "=", "(", "1", ",", "2", ")", ")", ")", "\n", "", "elif", "sim_token", "==", "'cls'", ":", "\n", "# using the CLS token", "\n", "                ", "q", "=", "X_as", "[", "i", "]", "[", "0", "]", "\n", "z", "=", "q", "*", "X_as", "[", ":", ",", "0", ",", ":", "]", "\n", "score", "=", "torch", ".", "sum", "(", "z", ",", "dim", "=", "(", "1", ")", ")", "/", "torch", ".", "tensor", "(", "\n", "np", ".", "linalg", ".", "norm", "(", "q", ")", "*", "np", ".", "linalg", ".", "norm", "(", "X_as", "[", ":", ",", "0", ",", ":", "]", ",", "axis", "=", "(", "1", ")", ")", ")", "\n", "", "elif", "sim_token", "==", "'bas'", ":", "\n", "# using the first token of the span", "\n", "                ", "q", "=", "X_as", "[", "i", "]", "[", "1", "]", "\n", "z", "=", "q", "*", "X_as", "[", ":", ",", "1", ",", ":", "]", "\n", "score", "=", "torch", ".", "sum", "(", "z", ",", "dim", "=", "(", "1", ")", ")", "/", "torch", ".", "tensor", "(", "\n", "np", ".", "linalg", ".", "norm", "(", "q", ")", "*", "np", ".", "linalg", ".", "norm", "(", "X_as", "[", ":", ",", "1", ",", ":", "]", ",", "axis", "=", "(", "1", ")", ")", ")", "\n", "\n", "", "topk_idx", "=", "torch", ".", "argsort", "(", "score", ",", "dim", "=", "0", ",", "descending", "=", "True", ")", "\n", "for", "idx", "in", "topk_idx", ":", "\n", "                ", "if", "idx", "==", "i", ":", "\n", "                    ", "continue", "\n", "", "if", "len", "(", "aspect_dict", "[", "aspect_raw_token_list", "[", "i", "]", "]", "[", "'similar_spans'", "]", ")", "<", "sim_topk", ":", "\n", "                    ", "aspect_dict", "[", "aspect_raw_token_list", "[", "i", "]", "]", "[", "'similar_spans'", "]", ".", "append", "(", "\n", "[", "aspect_raw_token_list", "[", "idx", "]", ",", "score", "[", "idx", "]", ".", "item", "(", ")", "]", ")", "\n", "aspect_dict", "[", "aspect_raw_token_list", "[", "i", "]", "]", "[", "'similar_spans_length'", "]", ".", "append", "(", "\n", "aspect_dict", "[", "aspect_raw_token_list", "[", "idx", "]", "]", "[", "'bert_length'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "for", "i", "in", "range", "(", "len", "(", "opinion_token_list", ")", ")", ":", "\n", "            ", "if", "sim_token", "==", "'all'", ":", "\n", "# using all tokens", "\n", "                ", "q", "=", "X_op", "[", "i", "]", "\n", "z", "=", "q", "*", "X_op", "\n", "score", "=", "torch", ".", "sum", "(", "z", ",", "dim", "=", "(", "1", ",", "2", ")", ")", "/", "torch", ".", "tensor", "(", "\n", "np", ".", "linalg", ".", "norm", "(", "q", ")", "*", "np", ".", "linalg", ".", "norm", "(", "X_op", ",", "axis", "=", "(", "1", ",", "2", ")", ")", ")", "\n", "", "elif", "sim_token", "==", "'cls'", ":", "\n", "# using the CLS token", "\n", "                ", "q", "=", "X_op", "[", "i", "]", "[", "0", "]", "\n", "z", "=", "q", "*", "X_op", "[", ":", ",", "0", ",", ":", "]", "\n", "score", "=", "torch", ".", "sum", "(", "z", ",", "dim", "=", "(", "1", ")", ")", "/", "torch", ".", "tensor", "(", "\n", "np", ".", "linalg", ".", "norm", "(", "q", ")", "*", "np", ".", "linalg", ".", "norm", "(", "X_op", "[", ":", ",", "0", ",", ":", "]", ",", "axis", "=", "(", "1", ")", ")", ")", "\n", "", "elif", "sim_token", "==", "'bas'", ":", "\n", "# using the first token of the span", "\n", "                ", "q", "=", "X_op", "[", "i", "]", "[", "1", "]", "\n", "z", "=", "q", "*", "X_op", "[", ":", ",", "1", ",", ":", "]", "\n", "score", "=", "torch", ".", "sum", "(", "z", ",", "dim", "=", "(", "1", ")", ")", "/", "torch", ".", "tensor", "(", "\n", "np", ".", "linalg", ".", "norm", "(", "q", ")", "*", "np", ".", "linalg", ".", "norm", "(", "X_op", "[", ":", ",", "1", ",", ":", "]", ",", "axis", "=", "(", "1", ")", ")", ")", "\n", "", "topk_idx", "=", "torch", ".", "argsort", "(", "score", ",", "dim", "=", "0", ",", "descending", "=", "True", ")", "\n", "for", "idx", "in", "topk_idx", ":", "\n", "                ", "if", "idx", "==", "i", ":", "\n", "                    ", "continue", "\n", "", "if", "len", "(", "opinion_dict", "[", "opinion_raw_token_list", "[", "i", "]", "]", "[", "'similar_spans'", "]", ")", "<", "sim_topk", ":", "\n", "                    ", "opinion_dict", "[", "opinion_raw_token_list", "[", "i", "]", "]", "[", "'similar_spans'", "]", ".", "append", "(", "\n", "[", "opinion_raw_token_list", "[", "idx", "]", ",", "score", "[", "idx", "]", ".", "item", "(", ")", "]", ")", "\n", "opinion_dict", "[", "opinion_raw_token_list", "[", "i", "]", "]", "[", "'similar_spans_length'", "]", ".", "append", "(", "\n", "opinion_dict", "[", "opinion_raw_token_list", "[", "idx", "]", "]", "[", "'bert_length'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "", "", "", "self", ".", "index", "[", "'span'", "]", "=", "{", "'aspect'", ":", "aspect_dict", ",", "'opinion'", ":", "opinion_dict", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_index_builder.IndexBuilder.index_token_replacement": [[296, 320], ["augment_index_builder.IndexBuilder.find_word_replacement", "dict", "augment_utils.is_stopword", "len", "list", "s[].split", "augment_index_builder.IndexBuilder.tokenizer.tokenize", "len", "[].append", "[].append"], "methods", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_index_builder.IndexBuilder.find_word_replacement", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.is_stopword"], ["", "def", "index_token_replacement", "(", "self", ")", ":", "\n", "# pre-compute all token replacement candidates and store them in the index", "\n", "        ", "for", "token", "in", "self", ".", "tokens", ":", "\n", "            ", "for", "w", "in", "token", ":", "\n", "                ", "if", "is_stopword", "(", "w", ")", "or", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "[", "'similar_words'", "]", "is", "not", "None", ":", "\n", "                    ", "continue", "\n", "", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "[", "'similar_words'", "]", "=", "[", "]", "\n", "# self.index['token'][w]['similar_words_bert'] = []", "\n", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "[", "'similar_words_length'", "]", "=", "[", "]", "\n", "\n", "synonyms", "=", "self", ".", "find_word_replacement", "(", "word_str", "=", "w", ")", "\n", "similar_words_dict", "=", "dict", "(", ")", "\n", "if", "len", "(", "synonyms", ")", ">=", "1", ":", "\n", "                    ", "for", "s", "in", "list", "(", "synonyms", ")", ":", "\n", "                        ", "s_arr", "=", "s", "[", "0", "]", ".", "split", "(", "'_'", ")", "\n", "if", "s_arr", "[", "0", "]", "not", "in", "similar_words_dict", ":", "\n", "                            ", "similar_words_dict", "[", "s_arr", "[", "0", "]", "]", "=", "True", "\n", "", "else", ":", "\n", "                            ", "continue", "\n", "", "tokenized_s", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "s_arr", "[", "0", "]", ")", "\n", "l_s", "=", "len", "(", "tokenized_s", ")", "\n", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "[", "'similar_words'", "]", ".", "append", "(", "[", "s_arr", "[", "0", "]", ",", "s", "[", "1", "]", "]", ")", "\n", "# self.index['token'][w]['similar_words_bert'].append(tokenized_s)", "\n", "self", ".", "index", "[", "'token'", "]", "[", "w", "]", "[", "'similar_words_length'", "]", ".", "append", "(", "l_s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_index_builder.IndexBuilder.find_word_replacement": [[321, 398], ["word_str.lower.lower.lower", "nltk.corpus.wordnet.synsets", "list", "word_str.lower.lower.lower", "syn.lemmas", "len", "zip", "augment_index_builder.IndexBuilder.w2v.wv.most_similar", "nltk.corpus.wordnet.synsets", "dict", "syn.lemmas", "len", "list", "list", "lem.name", "dict.keys", "zip", "syn_list.append", "range", "augment_utils.sign", "augment_utils.sign", "arr.append", "lem.name", "word_str.lower.lower.lower", "word_str.lower.lower.lower", "augment_utils.sign", "augment_utils.sign", "syn_list.append", "lem.name", "len", "lem.name", "augment_index_builder.IndexBuilder.w2v.wv.most_similar", "lem.name", "range", "len", "lem.name", "augment_utils.sign", "augment_utils.sign", "arr.append"], "methods", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.sign", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.sign", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.sign", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.sign", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.sign", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_utils.sign"], ["", "", "", "", "", "def", "find_word_replacement", "(", "self", ",", "word_str", ",", "sim_topk", "=", "10", ",", "is_senti_sensitive", "=", "False", ")", ":", "\n", "# find sim_topk similar words to word_str", "\n", "        ", "if", "is_senti_sensitive", ":", "\n", "# if sentiment sensitive, compute the senti score of word_str", "\n", "            ", "senti_score", "=", "0", "\n", "word_str", "=", "word_str", ".", "lower", "(", ")", "\n", "if", "word_str", ".", "lower", "(", ")", "in", "self", ".", "avg_senti", ":", "\n", "                ", "senti_score", "=", "self", ".", "avg_senti", "[", "word_str", ".", "lower", "(", ")", "]", "[", "'pos_score'", "]", "-", "avg_senti", "[", "word_str", ".", "lower", "(", ")", "]", "[", "'neg_score'", "]", "\n", "\n", "", "", "if", "self", ".", "w2v", "is", "None", ":", "\n", "# if Word2Vec is not given, using wordnet", "\n", "            ", "syns", "=", "wordnet", ".", "synsets", "(", "word_str", ")", "\n", "syn_list", "=", "[", "]", "\n", "for", "syn", "in", "syns", ":", "\n", "                ", "for", "lem", "in", "syn", ".", "lemmas", "(", ")", ":", "\n", "                    ", "if", "lem", ".", "name", "(", ")", "!=", "word_str", ":", "\n", "                        ", "if", "is_senti_sensitive", ":", "\n", "                            ", "lem_senti_score", "=", "0", "\n", "if", "lem_str", "in", "self", ".", "avg_senti", ":", "\n", "                                ", "lem_senti_score", "=", "self", ".", "avg_senti", "[", "lem_str", "]", "[", "'pos_score'", "]", "-", "self", ".", "avg_senti", "[", "lem_str", "]", "[", "'neg_score'", "]", "\n", "", "''' maybe we can use a different way to determine whether two words\n                            are of the same sentiment '''", "\n", "if", "sign", "(", "lem_senti_score", ")", "==", "sign", "(", "senti_score", ")", ":", "\n", "                                ", "syn_list", ".", "append", "(", "lem_str", ")", "\n", "", "", "else", ":", "\n", "                            ", "syn_list", ".", "append", "(", "lem", ".", "name", "(", ")", ")", "\n", "", "", "", "", "if", "len", "(", "syn_list", ")", "==", "0", ":", "\n", "                ", "return", "[", "]", "\n", "", "return", "list", "(", "zip", "(", "syn_list", ",", "[", "1.0", "for", "i", "in", "range", "(", "len", "(", "syn_list", ")", ")", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "word_str", "in", "self", ".", "w2v", ".", "wv", ".", "vocab", ":", "\n", "# if word_str appears in Word2Vec vocabulary", "\n", "                ", "similar_list", "=", "self", ".", "w2v", ".", "wv", ".", "most_similar", "(", "positive", "=", "[", "word_str", "]", ",", "topn", "=", "sim_topk", ")", "\n", "if", "is_senti_sensitive", ":", "\n", "                    ", "arr", "=", "[", "]", "\n", "for", "ws", "in", "similar_list", ":", "\n", "                        ", "w_senti_score", "=", "0", "\n", "w", "=", "ws", "[", "0", "]", "\n", "if", "w", "in", "self", ".", "avg_senti", ":", "\n", "                            ", "w_senti_score", "=", "self", ".", "avg_senti", "[", "w", "]", "[", "'pos_score'", "]", "-", "self", ".", "avg_senti", "[", "w", "]", "[", "'neg_score'", "]", "\n", "", "if", "sign", "(", "w_senti_score", ")", "==", "sign", "(", "senti_score", ")", ":", "\n", "                            ", "arr", ".", "append", "(", "ws", ")", "\n", "", "", "return", "arr", "\n", "", "else", ":", "\n", "                    ", "return", "similar_list", "\n", "", "", "else", ":", "\n", "# if word_str does not appear in Word2Vec vocabulary, find a synonym of it using WordNet", "\n", "# if the synonym appears in Word2Vec vocabulary, use similar words of this synonym", "\n", "                ", "syns", "=", "wordnet", ".", "synsets", "(", "word_str", ")", "\n", "syns_dict", "=", "dict", "(", ")", "\n", "arr", "=", "[", "]", "\n", "for", "syn", "in", "syns", ":", "\n", "                    ", "flag", "=", "False", "\n", "for", "lem", "in", "syn", ".", "lemmas", "(", ")", ":", "\n", "                        ", "if", "lem", ".", "name", "(", ")", "!=", "word_str", ":", "\n", "                            ", "syns_dict", "[", "lem", ".", "name", "(", ")", "]", "=", "True", "\n", "if", "lem", ".", "name", "(", ")", "in", "self", ".", "w2v", ".", "wv", ".", "vocab", ":", "\n", "                                ", "similar_list", "=", "self", ".", "w2v", ".", "wv", ".", "most_similar", "(", "positive", "=", "[", "lem", ".", "name", "(", ")", "]", ",", "topn", "=", "sim_topk", ")", "\n", "if", "is_senti_sensitive", ":", "\n", "                                    ", "for", "ws", "in", "similar_list", ":", "\n", "                                        ", "w_senti_score", "=", "0", "\n", "w", "=", "ws", "[", "0", "]", "\n", "if", "w", "in", "self", ".", "avg_senti", ":", "\n", "                                            ", "w_senti_score", "=", "self", ".", "avg_senti", "[", "w", "]", "[", "'pos_score'", "]", "-", "self", ".", "avg_senti", "[", "w", "]", "[", "'neg_score'", "]", "\n", "", "if", "sign", "(", "w_senti_score", ")", "==", "sign", "(", "senti_score", ")", ":", "\n", "                                            ", "arr", ".", "append", "(", "ws", ")", "\n", "", "", "", "else", ":", "\n", "                                    ", "arr", "=", "similar_list", "\n", "", "flag", "=", "True", "\n", "break", "\n", "", "", "", "if", "flag", ":", "\n", "                        ", "break", "\n", "", "", "if", "len", "(", "arr", ")", "==", "0", ":", "\n", "                    ", "res", "=", "list", "(", "syns_dict", ".", "keys", "(", ")", ")", "\n", "return", "list", "(", "zip", "(", "res", ",", "[", "1.0", "for", "i", "in", "range", "(", "len", "(", "res", ")", ")", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "return", "arr", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_index_builder.IndexBuilder.calc_senti_score": [[399, 417], ["nltk.corpus.reader.sentiwordnet.SentiWordNetCorpusReader", "nltk.corpus.reader.sentiwordnet.SentiWordNetCorpusReader.all_senti_synsets", "senti_synset.pos_score", "senti_synset.neg_score", "senti_synset.synset.name().split", "senti_synset.synset.name"], "methods", ["None"], ["", "", "", "", "def", "calc_senti_score", "(", "self", ",", "swn_filename", "=", "'combined_data/SentiWordNet_3.0.0_20100705.txt'", ")", ":", "\n", "# aggregate sentiment score of tokens using SentiWordNet", "\n", "        ", "swn", "=", "SentiWordNetCorpusReader", "(", "'./'", ",", "[", "swn_filename", "]", ")", "\n", "for", "senti_synset", "in", "swn", ".", "all_senti_synsets", "(", ")", ":", "\n", "            ", "w", "=", "senti_synset", ".", "synset", ".", "name", "(", ")", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "if", "w", "not", "in", "self", ".", "avg_senti", ":", "\n", "                ", "self", ".", "avg_senti", "[", "w", "]", "=", "{", "\n", "'pos_score'", ":", "0", ",", "\n", "'neg_score'", ":", "0", ",", "\n", "'count'", ":", "0", "\n", "}", "\n", "", "self", ".", "avg_senti", "[", "w", "]", "[", "'pos_score'", "]", "+=", "senti_synset", ".", "pos_score", "(", ")", "\n", "self", ".", "avg_senti", "[", "w", "]", "[", "'neg_score'", "]", "+=", "senti_synset", ".", "neg_score", "(", ")", "\n", "self", ".", "avg_senti", "[", "w", "]", "[", "'count'", "]", "+=", "1", "\n", "\n", "", "for", "w", "in", "self", ".", "avg_senti", ":", "\n", "            ", "self", ".", "avg_senti", "[", "w", "]", "[", "'pos_score'", "]", "/=", "self", ".", "avg_senti", "[", "w", "]", "[", "'count'", "]", "\n", "self", ".", "avg_senti", "[", "w", "]", "[", "'neg_score'", "]", "/=", "self", ".", "avg_senti", "[", "w", "]", "[", "'count'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.augment_index_builder.IndexBuilder.dump_index": [[418, 422], ["open", "json.dump", "open.close"], "methods", ["None"], ["", "", "def", "dump_index", "(", "self", ",", "index_filename", "=", "'augment_index.json'", ")", ":", "\n", "        ", "outfile", "=", "open", "(", "index_filename", ",", "'w'", ")", "\n", "json", ".", "dump", "(", "self", ".", "index", ",", "outfile", ")", "\n", "outfile", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixda.mixda": [[29, 80], ["numpy.random.beta", "model", "y.view.view", "y.view.view", "logits.view.view", "logits.view.view", "x.size", "criterion", "criterion"], "function", ["None"], ["def", "mixda", "(", "model", ",", "batch", ",", "alpha_aug", "=", "0.4", ")", ":", "\n", "    ", "\"\"\"Perform one iteration of MixDA\n\n    Args:\n        model (MultiTaskNet): the model state\n        batch (tuple): the input batch\n        alpha_aug (float, Optional): the parameter for MixDA\n\n    Returns:\n        Tensor: the loss (of 0-d)\n    \"\"\"", "\n", "_", ",", "x", ",", "_", ",", "_", ",", "mask", ",", "y", ",", "_", ",", "taskname", "=", "batch", "\n", "taskname", "=", "taskname", "[", "0", "]", "\n", "# two batches", "\n", "batch_size", "=", "x", ".", "size", "(", ")", "[", "0", "]", "//", "2", "\n", "\n", "# augmented", "\n", "aug_x", "=", "x", "[", "batch_size", ":", "]", "\n", "aug_y", "=", "y", "[", "batch_size", ":", "]", "\n", "aug_lam", "=", "np", ".", "random", ".", "beta", "(", "alpha_aug", ",", "alpha_aug", ")", "\n", "\n", "# labeled", "\n", "x", "=", "x", "[", ":", "batch_size", "]", "\n", "\n", "# back prop", "\n", "logits", ",", "y", ",", "_", "=", "model", "(", "x", ",", "y", ",", "\n", "augment_batch", "=", "(", "aug_x", ",", "aug_lam", ")", ",", "\n", "task", "=", "taskname", ")", "\n", "if", "'sts-b'", "in", "taskname", ":", "\n", "        ", "logits", "=", "logits", ".", "view", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "        ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "\n", "", "aug_y", "=", "y", "[", "batch_size", ":", "]", "\n", "y", "=", "y", "[", ":", "batch_size", "]", "\n", "aug_y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "\n", "# cross entropy", "\n", "if", "'tagging'", "in", "taskname", ":", "\n", "        ", "criterion", "=", "tagging_criterion", "\n", "", "elif", "'sts-b'", "in", "taskname", ":", "\n", "        ", "criterion", "=", "regression_criterion", "\n", "", "else", ":", "\n", "        ", "criterion", "=", "classifier_criterion", "\n", "\n", "# mix the labels", "\n", "", "loss", "=", "criterion", "(", "logits", ",", "y", ")", "*", "aug_lam", "+", "criterion", "(", "logits", ",", "aug_y", ")", "*", "(", "1", "-", "aug_lam", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixda.create_mixda_batches": [[82, 116], ["len", "numpy.random.permutation", "enumerate", "l_batch.append", "l_batch_aug.append", "len", "l_batch.clear", "l_batch_aug.clear", "padder", "len", "padder", "len"], "function", ["None"], ["", "def", "create_mixda_batches", "(", "l_set", ",", "aug_set", ",", "batch_size", "=", "16", ")", ":", "\n", "    ", "\"\"\"Create batches for mixda\n\n    Each batch is the concatenation of (1) a labeled batch and (2) an augmented\n    labeled batch (having the same order of (1) )\n\n    Args:\n        l_set (SnippextDataset): the train set\n        aug_set (SnippextDataset): the augmented train set\n        batch_size (int, optional): batch size (of each component)\n\n    Returns:\n        list of list: the created batches\n    \"\"\"", "\n", "num_labeled", "=", "len", "(", "l_set", ")", "\n", "l_index", "=", "np", ".", "random", ".", "permutation", "(", "num_labeled", ")", "\n", "\n", "l_batch", "=", "[", "]", "\n", "l_batch_aug", "=", "[", "]", "\n", "padder", "=", "l_set", ".", "pad", "\n", "\n", "for", "i", ",", "idx", "in", "enumerate", "(", "l_index", ")", ":", "\n", "        ", "l_batch", ".", "append", "(", "l_set", "[", "idx", "]", ")", "\n", "l_batch_aug", ".", "append", "(", "aug_set", "[", "idx", "]", ")", "\n", "\n", "if", "len", "(", "l_batch", ")", "==", "batch_size", "or", "i", "==", "len", "(", "l_index", ")", "-", "1", ":", "\n", "            ", "batches", "=", "l_batch", "+", "l_batch_aug", "\n", "yield", "padder", "(", "batches", ")", "\n", "l_batch", ".", "clear", "(", ")", "\n", "l_batch_aug", ".", "clear", "(", ")", "\n", "\n", "", "", "if", "len", "(", "l_batch", ")", ">", "0", ":", "\n", "        ", "batches", "=", "l_batch", "+", "l_batch_aug", "\n", "yield", "padder", "(", "batches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixda.train": [[118, 180], ["mixda.create_mixda_batches", "model.train", "enumerate", "optimizer.zero_grad", "mixda.mixda", "optimizer.step", "mixda.backward", "scheduler.step", "print", "print", "print", "print", "print", "numpy.isscalar", "print", "print", "print", "print", "print", "print", "apex.amp.scale_loss", "scaled_loss.backward", "_y.cpu().numpy", "print", "print", "dataset.get_tokenizer().convert_ids_to_tokens", "x.cpu().numpy", "_y.cpu", "mixda.item", "dataset.get_tokenizer", "x.cpu().numpy", "x.cpu", "x.cpu"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixda.create_mixda_batches", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.baseline.train", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixda.mixda", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.get_tokenizer"], ["", "", "def", "train", "(", "model", ",", "l_set", ",", "aug_set", ",", "optimizer", ",", "\n", "scheduler", "=", "None", ",", "\n", "fp16", "=", "False", ",", "\n", "batch_size", "=", "32", ",", "\n", "alpha_aug", "=", "0.8", ")", ":", "\n", "    ", "\"\"\"Perform one epoch of MixDA\n\n    Args:\n        model (MultiTaskModel): the model state\n        train_dataset (SnippextDataset): the train set\n        augment_dataset (SnippextDataset): the augmented train set\n        optimizer (Optimizer): Adam\n        fp16 (boolean, Optional): whether to use fp16\n        batch_size (int, Optional): batch size\n        alpha_aug (float, Optional): the alpha for MixDA\n\n    Returns:\n        None\n    \"\"\"", "\n", "mixda_batches", "=", "create_mixda_batches", "(", "l_set", ",", "\n", "aug_set", ",", "\n", "batch_size", "=", "batch_size", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "mixda_batches", ")", ":", "\n", "# for monitoring", "\n", "        ", "words", ",", "x", ",", "is_heads", ",", "tags", ",", "mask", ",", "y", ",", "seqlens", ",", "taskname", "=", "batch", "\n", "taskname", "=", "taskname", "[", "0", "]", "\n", "_y", "=", "y", "\n", "\n", "# perform mixmatch", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", "=", "mixda", "(", "model", ",", "batch", ",", "alpha_aug", ")", "\n", "if", "fp16", ":", "\n", "            ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "loss", ".", "backward", "(", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "if", "scheduler", ":", "\n", "            ", "scheduler", ".", "step", "(", ")", "\n", "\n", "", "if", "i", "==", "0", ":", "\n", "            ", "print", "(", "\"=====sanity check======\"", ")", "\n", "print", "(", "\"words:\"", ",", "words", "[", "0", "]", ")", "\n", "print", "(", "\"x:\"", ",", "x", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "[", ":", "seqlens", "[", "0", "]", "]", ")", "\n", "print", "(", "\"tokens:\"", ",", "get_tokenizer", "(", ")", ".", "convert_ids_to_tokens", "(", "x", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", ")", "[", ":", "seqlens", "[", "0", "]", "]", ")", "\n", "print", "(", "\"is_heads:\"", ",", "is_heads", "[", "0", "]", ")", "\n", "y_sample", "=", "_y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "\n", "if", "np", ".", "isscalar", "(", "y_sample", ")", ":", "\n", "                ", "print", "(", "\"y:\"", ",", "y_sample", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"y:\"", ",", "y_sample", "[", ":", "seqlens", "[", "0", "]", "]", ")", "\n", "", "print", "(", "\"tags:\"", ",", "tags", "[", "0", "]", ")", "\n", "print", "(", "\"mask:\"", ",", "mask", "[", "0", "]", ")", "\n", "print", "(", "\"seqlen:\"", ",", "seqlens", "[", "0", "]", ")", "\n", "print", "(", "\"task_name:\"", ",", "taskname", ")", "\n", "print", "(", "\"=======================\"", ")", "\n", "\n", "", "if", "i", "%", "10", "==", "0", ":", "# monitoring", "\n", "            ", "print", "(", "f\"step: {i}, task: {taskname}, loss: {loss.item()}\"", ")", "\n", "del", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixda.initialize_and_train": [[183, 278], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "transformers.get_linear_schedule_with_warmup", "tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter.close", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "model.MultiTaskNet", "transformers.AdamW", "model.MultiTaskNet().cuda", "transformers.AdamW", "os.path.exists", "os.makedirs", "mixda.train", "print", "train_util.eval_on_task", "MultiTaskNet().cuda.parameters", "MultiTaskNet().cuda.parameters", "apex.amp.initialize", "len", "model.MultiTaskNet", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "MultiTaskNet().cuda.state_dict", "MultiTaskNet().cuda.state_dict"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.baseline.train", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_on_task", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.initialize"], ["", "", "", "def", "initialize_and_train", "(", "task_config", ",", "\n", "trainset", ",", "\n", "augmentset", ",", "\n", "validset", ",", "\n", "testset", ",", "\n", "hp", ",", "\n", "run_tag", ")", ":", "\n", "    ", "\"\"\"The train process.\n\n    Args:\n        task_config (dictionary): the configuration of the task\n        trainset (SnippextDataset): the training set\n        augmentset (SnippextDataset): the augmented training set\n        validset (SnippextDataset): the validation set\n        testset (SnippextDataset): the testset\n        hp (Namespace): the parsed hyperparameters\n        run_tag (string): the tag of the run (for logging purpose)\n\n    Returns:\n        None\n    \"\"\"", "\n", "padder", "=", "SnippextDataset", ".", "pad", "\n", "\n", "# iterators for dev/test set", "\n", "valid_iter", "=", "data", ".", "DataLoader", "(", "dataset", "=", "validset", ",", "\n", "batch_size", "=", "hp", ".", "batch_size", "*", "4", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "padder", ")", "\n", "test_iter", "=", "data", ".", "DataLoader", "(", "dataset", "=", "testset", ",", "\n", "batch_size", "=", "hp", ".", "batch_size", "*", "4", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "padder", ")", "\n", "\n", "\n", "# initialize model", "\n", "device", "=", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", "\n", "if", "device", "==", "'cpu'", ":", "\n", "        ", "model", "=", "MultiTaskNet", "(", "[", "task_config", "]", ",", "device", ",", "\n", "hp", ".", "finetuning", ",", "lm", "=", "hp", ".", "lm", ",", "bert_path", "=", "hp", ".", "bert_path", ")", "\n", "optimizer", "=", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "hp", ".", "lr", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "MultiTaskNet", "(", "[", "task_config", "]", ",", "device", ",", "\n", "hp", ".", "finetuning", ",", "lm", "=", "hp", ".", "lm", ",", "bert_path", "=", "hp", ".", "bert_path", ")", ".", "cuda", "(", ")", "\n", "optimizer", "=", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "hp", ".", "lr", ")", "\n", "if", "hp", ".", "fp16", ":", "\n", "            ", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "'O2'", ")", "\n", "\n", "# learning rate scheduler", "\n", "", "", "num_steps", "=", "(", "len", "(", "trainset", ")", "//", "hp", ".", "batch_size", ")", "*", "hp", ".", "n_epochs", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "\n", "num_warmup_steps", "=", "num_steps", "//", "10", ",", "\n", "num_training_steps", "=", "num_steps", ")", "\n", "# create logging", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "hp", ".", "logdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "hp", ".", "logdir", ")", "\n", "", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "hp", ".", "logdir", ")", "\n", "\n", "# start training", "\n", "best_dev_f1", "=", "best_test_f1", "=", "0.0", "\n", "epoch", "=", "1", "\n", "while", "epoch", "<=", "hp", ".", "n_epochs", ":", "\n", "        ", "train", "(", "model", ",", "\n", "trainset", ",", "\n", "augmentset", ",", "\n", "optimizer", ",", "\n", "scheduler", "=", "scheduler", ",", "\n", "fp16", "=", "hp", ".", "fp16", ",", "\n", "batch_size", "=", "hp", ".", "batch_size", ",", "\n", "alpha_aug", "=", "hp", ".", "alpha_aug", ")", "\n", "\n", "print", "(", "f\"=========eval at epoch={epoch}=========\"", ")", "\n", "dev_f1", ",", "test_f1", "=", "eval_on_task", "(", "epoch", ",", "\n", "model", ",", "\n", "task_config", "[", "'name'", "]", ",", "\n", "valid_iter", ",", "\n", "validset", ",", "\n", "test_iter", ",", "\n", "testset", ",", "\n", "writer", ",", "\n", "run_tag", ")", "\n", "\n", "# skip the epochs with zero f1", "\n", "if", "dev_f1", ">", "1e-6", ":", "\n", "            ", "epoch", "+=", "1", "\n", "if", "hp", ".", "save_model", ":", "\n", "                ", "if", "dev_f1", ">", "best_dev_f1", ":", "\n", "                    ", "best_dev_f1", "=", "dev_f1", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "run_tag", "+", "'_dev.pt'", ")", "\n", "", "if", "test_f1", ">", "best_test_f1", ":", "\n", "                    ", "best_test_f1", "=", "test_f1", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "run_tag", "+", "'_test.pt'", ")", "\n", "\n", "", "", "", "", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.__init__": [[16, 26], ["json.load", "open", "list", "[].keys", "numpy.array", "numpy.sum"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "index_fn", ",", "valid_spans", "=", "None", ")", ":", "\n", "        ", "self", ".", "index", "=", "json", ".", "load", "(", "open", "(", "index_fn", ")", ")", "\n", "self", ".", "all_spans", "=", "{", "}", "\n", "self", ".", "span_freqs", "=", "{", "}", "\n", "\n", "for", "span_type", "in", "self", ".", "index", "[", "'span'", "]", ":", "\n", "            ", "self", ".", "all_spans", "[", "span_type", "]", "=", "list", "(", "self", ".", "index", "[", "'span'", "]", "[", "span_type", "]", ".", "keys", "(", ")", ")", "\n", "span_freqs_tmp", "=", "[", "self", ".", "index", "[", "'span'", "]", "[", "span_type", "]", "[", "sp", "]", "[", "'document_freq'", "]", "for", "sp", "in", "self", ".", "all_spans", "[", "span_type", "]", "]", "\n", "self", ".", "span_freqs", "[", "span_type", "]", "=", "np", ".", "array", "(", "span_freqs_tmp", ")", "/", "np", ".", "sum", "(", "span_freqs_tmp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.augment": [[27, 138], ["op.split", "augment.Augmenter.sample_span_position", "label.startswith", "len", "augment.Augmenter.sample_position", "random.choice.split", "random.choice", "random.choice", "random.choice.split", "augment.Augmenter.sample_token", "numpy.random.choice", "augment.Augmenter.sample_token", "augment.Augmenter.sample_position", "list", "list", "token.replace"], "methods", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.sample_span_position", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.sample_position", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.sample_token", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.sample_token", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.sample_position"], ["", "", "def", "augment", "(", "self", ",", "tokens", ",", "labels", ",", "op", "=", "'token_del_tfidf'", ")", ":", "\n", "        ", "\"\"\" Performs data augmentation on a tagging example.\n\n        We support deletion (del), insertion (ins), replacement (repl),\n        and swapping(swap) at the token level. At the span level, we support\n        replacement with the options of replacing with random, frequent (freq),\n        or similar spans (sim).\n\n        The supported ops:\n          ['token_del_tfidf',\n           'token_del',\n           'token_repl_tfidf',\n           'token_repl',\n           'token_swap',\n           'token_ins',\n           'span_sim',\n           'span_freq',\n           'span']\n\n        Args:\n            tokens (list of strings): the input tokens\n            labels (list of strings): the labels of the tokens\n            op (str, optional): a string encoding of the operator to be applied\n\n        Returns:\n            list of strings: the augmented tokens\n            list of strings: the augmented labels\n        \"\"\"", "\n", "flags", "=", "op", ".", "split", "(", "'_'", ")", "\n", "if", "'span'", "in", "flags", ":", "\n", "            ", "start", ",", "end", "=", "self", ".", "sample_span_position", "(", "tokens", ",", "labels", ")", "\n", "if", "start", "<", "0", ":", "\n", "                ", "return", "tokens", ",", "labels", "\n", "", "span", "=", "' '", ".", "join", "(", "tokens", "[", "start", ":", "end", "+", "1", "]", ")", "# .lower()", "\n", "label", "=", "labels", "[", "start", "]", "\n", "if", "label", ".", "startswith", "(", "'B-'", ")", ":", "\n", "                ", "labelI", "=", "'I'", "+", "labels", "[", "start", "]", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "                ", "labelI", "=", "label", "\n", "\n", "", "if", "'AS'", "in", "label", ":", "\n", "                ", "span_type", "=", "'aspect'", "\n", "", "else", ":", "\n", "                ", "span_type", "=", "'opinion'", "\n", "\n", "", "if", "'sim'", "in", "op", ":", "\n", "                ", "candidates", "=", "self", ".", "index", "[", "'span'", "]", "[", "span_type", "]", "[", "span", "]", "[", "'similar_spans'", "]", "\n", "new_span", "=", "random", ".", "choice", "(", "candidates", ")", "[", "0", "]", "\n", "", "elif", "'freq'", "in", "op", ":", "\n", "                ", "candidates", "=", "self", ".", "all_spans", "[", "span_type", "]", "\n", "new_span", "=", "np", ".", "random", ".", "choice", "(", "candidates", ",", "1", ",", "\n", "p", "=", "self", ".", "span_freqs", "[", "span_type", "]", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "candidates", "=", "self", ".", "all_spans", "[", "span_type", "]", "\n", "new_span", "=", "random", ".", "choice", "(", "candidates", ")", "\n", "\n", "", "new_span_len", "=", "len", "(", "new_span", ".", "split", "(", "' '", ")", ")", "\n", "new_tokens", "=", "tokens", "[", ":", "start", "]", "+", "new_span", ".", "split", "(", "' '", ")", "+", "tokens", "[", "end", "+", "1", ":", "]", "\n", "new_labels", "=", "labels", "[", ":", "start", "]", "+", "[", "label", "]", "+", "[", "labelI", "]", "*", "(", "new_span_len", "-", "1", ")", "+", "labels", "[", "end", "+", "1", ":", "]", "\n", "return", "new_tokens", ",", "new_labels", "\n", "", "else", ":", "\n", "            ", "tfidf", "=", "'tfidf'", "in", "op", "\n", "pos1", "=", "self", ".", "sample_position", "(", "tokens", ",", "labels", ",", "tfidf", ")", "\n", "if", "pos1", "<", "0", ":", "\n", "                ", "return", "tokens", ",", "labels", "\n", "\n", "", "if", "'del'", "in", "op", ":", "\n", "# insert padding to keep the length consistent", "\n", "                ", "if", "tokens", "[", "pos1", "]", "in", "self", ".", "index", "[", "'token'", "]", ":", "\n", "                    ", "length", "=", "self", ".", "index", "[", "'token'", "]", "[", "tokens", "[", "pos1", "]", "]", "[", "'bert_length'", "]", "\n", "", "else", ":", "\n", "                    ", "length", "=", "1", "\n", "", "new_tokens", "=", "tokens", "[", ":", "pos1", "]", "+", "[", "'[PAD]'", "]", "*", "(", "length", ")", "+", "tokens", "[", "pos1", "+", "1", ":", "]", "\n", "new_labels", "=", "labels", "[", ":", "pos1", "]", "+", "[", "'<PAD>'", "]", "*", "(", "length", ")", "+", "labels", "[", "pos1", "+", "1", ":", "]", "\n", "", "elif", "'ins'", "in", "op", ":", "\n", "                ", "ins_token", "=", "self", ".", "sample_token", "(", "tokens", "[", "pos1", "]", ",", "same_length", "=", "False", ")", "\n", "new_tokens", "=", "tokens", "[", ":", "pos1", "]", "+", "[", "ins_token", "]", "+", "tokens", "[", "pos1", ":", "]", "\n", "new_labels", "=", "labels", "[", ":", "pos1", "]", "+", "[", "'O'", "]", "+", "labels", "[", "pos1", ":", "]", "\n", "", "elif", "'repl'", "in", "op", ":", "\n", "                ", "ins_token", "=", "self", ".", "sample_token", "(", "tokens", "[", "pos1", "]", ",", "same_length", "=", "False", ")", "\n", "if", "tokens", "[", "pos1", "]", "in", "self", ".", "index", "[", "'token'", "]", "and", "ins_token", "in", "self", ".", "index", "[", "'token'", "]", ":", "\n", "                    ", "len1", "=", "self", ".", "index", "[", "'token'", "]", "[", "tokens", "[", "pos1", "]", "]", "[", "'bert_length'", "]", "\n", "len2", "=", "self", ".", "index", "[", "'token'", "]", "[", "ins_token", "]", "[", "'bert_length'", "]", "\n", "if", "len1", "<", "len2", ":", "\n", "# truncate the new sequence", "\n", "                        ", "bert_tokens", "=", "self", ".", "index", "[", "'token'", "]", "[", "ins_token", "]", "[", "'bert_token'", "]", "[", ":", "len1", "]", "\n", "bert_tokens", "=", "[", "token", ".", "replace", "(", "'##'", ",", "''", ")", "for", "token", "in", "bert_tokens", "]", "\n", "ins_token", "=", "''", ".", "join", "(", "bert_tokens", ")", "\n", "new_tokens", "=", "tokens", "[", ":", "pos1", "]", "+", "[", "ins_token", "]", "+", "tokens", "[", "pos1", "+", "1", ":", "]", "\n", "new_labels", "=", "labels", "[", ":", "pos1", "]", "+", "[", "'O'", "]", "+", "labels", "[", "pos1", "+", "1", ":", "]", "\n", "", "else", ":", "\n", "# pad the new sequence", "\n", "                        ", "more", "=", "len1", "-", "len2", "\n", "new_tokens", "=", "tokens", "[", ":", "pos1", "]", "+", "[", "ins_token", "]", "+", "[", "'[PAD]'", "]", "*", "more", "+", "tokens", "[", "pos1", "+", "1", ":", "]", "\n", "new_labels", "=", "labels", "[", ":", "pos1", "]", "+", "[", "'O'", "]", "+", "[", "'<PAD>'", "]", "*", "more", "+", "labels", "[", "pos1", "+", "1", ":", "]", "\n", "", "", "else", ":", "\n", "# backup", "\n", "                    ", "new_tokens", "=", "tokens", "[", ":", "pos1", "]", "+", "[", "ins_token", "]", "+", "tokens", "[", "pos1", "+", "1", ":", "]", "\n", "new_labels", "=", "labels", "[", ":", "pos1", "]", "+", "[", "'O'", "]", "+", "labels", "[", "pos1", "+", "1", ":", "]", "\n", "", "", "elif", "'swap'", "in", "op", ":", "\n", "                ", "pos2", "=", "self", ".", "sample_position", "(", "tokens", ",", "labels", ",", "tfidf", ")", "\n", "new_tokens", "=", "list", "(", "tokens", ")", "\n", "new_labels", "=", "list", "(", "labels", ")", "\n", "new_tokens", "[", "pos1", "]", ",", "new_tokens", "[", "pos2", "]", "=", "tokens", "[", "pos2", "]", ",", "tokens", "[", "pos1", "]", "\n", "", "else", ":", "\n", "                ", "new_tokens", ",", "new_labels", "=", "tokens", ",", "labels", "\n", "\n", "", "return", "new_tokens", ",", "new_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.augment_sent": [[140, 207], ["text.split", "enumerate", "augment.Augmenter.augment", "ch.isalnum", "tokens.append", "len", "span.split", "range", "len", "len", "int", "tokens.append", "tokens.append", "range", "len", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.augment"], ["", "", "def", "augment_sent", "(", "self", ",", "text", ",", "op", "=", "'token_del_tfidf'", ")", ":", "\n", "        ", "\"\"\" Performs data augmentation on a classification example.\n\n        Similar to augment(tokens, labels) but works for sentences\n        or sentence-pairs.\n\n        Args:\n            text (str): the input sentence\n            op (str, optional): a string encoding of the operator to be applied\n\n        Returns:\n            str: the augmented sentence\n        \"\"\"", "\n", "# handling sentence pairs", "\n", "sents", "=", "text", ".", "split", "(", "' [SEP] '", ")", "\n", "text", "=", "sents", "[", "0", "]", "\n", "target_spans", "=", "sents", "[", "1", ":", "]", "\n", "\n", "# tokenize the sentence", "\n", "current", "=", "''", "\n", "tokens", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "for", "ch", "in", "text", ":", "\n", "            ", "if", "ch", ".", "isalnum", "(", ")", ":", "\n", "                ", "current", "+=", "ch", "\n", "", "else", ":", "\n", "                ", "if", "current", "!=", "''", ":", "\n", "                    ", "tokens", ".", "append", "(", "current", ")", "\n", "", "if", "ch", "not", "in", "' \\t\\r\\n'", ":", "\n", "                    ", "tokens", ".", "append", "(", "ch", ")", "\n", "", "current", "=", "''", "\n", "", "", "if", "current", "!=", "''", ":", "\n", "            ", "tokens", ".", "append", "(", "current", ")", "\n", "\n", "", "labels", "=", "[", "'O'", "]", "*", "len", "(", "tokens", ")", "\n", "for", "idx", ",", "span", "in", "enumerate", "(", "target_spans", ")", ":", "\n", "            ", "span_tokens", "=", "span", ".", "split", "(", "' '", ")", "\n", "for", "tid", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "\n", "                ", "if", "tid", "+", "len", "(", "span_tokens", ")", "<=", "len", "(", "tokens", ")", "and", "tokens", "[", "tid", ":", "tid", "+", "len", "(", "span_tokens", ")", "]", "==", "span_tokens", ":", "\n", "                    ", "for", "i", "in", "range", "(", "tid", ",", "tid", "+", "len", "(", "span_tokens", ")", ")", ":", "\n", "                        ", "labels", "[", "i", "]", "=", "'AS%d'", "%", "idx", "\n", "\n", "# print(tokens)", "\n", "# print(labels)", "\n", "# only augment the original sentence", "\n", "", "", "", "", "tokens", ",", "labels", "=", "self", ".", "augment", "(", "tokens", ",", "labels", ",", "op", "=", "op", ")", "\n", "\n", "# check consistency", "\n", "tid", "=", "0", "\n", "while", "tid", "<", "len", "(", "tokens", ")", ":", "\n", "            ", "if", "labels", "[", "tid", "]", "[", ":", "2", "]", "==", "'AS'", ":", "\n", "                ", "new_span", "=", "tokens", "[", "tid", "]", "\n", "idx", "=", "int", "(", "labels", "[", "tid", "]", "[", "2", ":", "]", ")", "\n", "while", "tid", "+", "1", "<", "len", "(", "tokens", ")", "and", "labels", "[", "tid", "+", "1", "]", "==", "labels", "[", "tid", "]", ":", "\n", "                    ", "tid", "+=", "1", "\n", "new_span", "+=", "' '", "+", "tokens", "[", "tid", "]", "\n", "", "if", "target_spans", "[", "idx", "]", "!=", "new_span", ":", "\n", "                    ", "target_spans", "[", "idx", "]", "=", "new_span", "\n", "", "", "tid", "+=", "1", "\n", "\n", "# error handling", "\n", "", "results", "=", "' '", ".", "join", "(", "tokens", ")", "\n", "for", "span", "in", "target_spans", ":", "\n", "            ", "results", "+=", "' [SEP] '", "+", "span", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.sample_position": [[209, 262], ["enumerate", "len", "enumerate", "random.choice", "candidates.append", "max", "weights.append", "numpy.array", "sum", "numpy.random.choice", "len"], "methods", ["None"], ["", "def", "sample_position", "(", "self", ",", "tokens", ",", "labels", ",", "tfidf", "=", "False", ")", ":", "\n", "        ", "\"\"\" Randomly sample a token's position from a training example\n\n        When tfidf is turned on, the weight of each token is proportional\n        to MaxTfIdf - Tfidf of each token. When it is off, the sampling is uniform.\n        Only tokens with 'O' labels and at least 1 position away from a non 'O'\n        labels will be sampled.\n\n        Args:\n            tokens (list of strings): the input tokens\n            labels (list of strings): the labels of the tokens\n            tfidf (bool, optional): whether the sampled position is by tfidf\n\n        Returns:\n            int: the sampled position (-1 if no such position)\n        \"\"\"", "\n", "index", "=", "self", ".", "index", "[", "'token'", "]", "\n", "candidates", "=", "[", "]", "\n", "for", "idx", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "if", "labels", "[", "idx", "]", "==", "'O'", "and", "token", "in", "index", "and", "index", "[", "token", "]", "[", "'similar_words'", "]", "!=", "None", "and", "len", "(", "index", "[", "token", "]", "[", "'similar_words'", "]", ")", ">", "0", ":", "\n", "                ", "candidates", ".", "append", "(", "idx", ")", "\n", "# if token.lower() in index and \\", "\n", "#     labels[idx] == 'O' and \\", "\n", "#     (idx + 1 >= len(tokens) or labels[idx + 1] == 'O') and \\", "\n", "#     (idx - 1 < 0 or labels[idx - 1] == 'O'):", "\n", "#     candidates.append(idx)", "\n", "\n", "", "", "if", "len", "(", "candidates", ")", "<=", "0", ":", "\n", "            ", "return", "-", "1", "\n", "", "if", "tfidf", ":", "\n", "            ", "weight", "=", "{", "}", "\n", "max_weight", "=", "0.0", "\n", "for", "idx", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# token = token.lower()", "\n", "                ", "if", "token", "not", "in", "index", ":", "\n", "                    ", "continue", "\n", "", "if", "token", "not", "in", "weight", ":", "\n", "                    ", "weight", "[", "token", "]", "=", "0.0", "\n", "", "weight", "[", "token", "]", "+=", "index", "[", "token", "]", "[", "'idf'", "]", "\n", "max_weight", "=", "max", "(", "max_weight", ",", "weight", "[", "token", "]", ")", "\n", "\n", "", "weights", "=", "[", "]", "\n", "for", "idx", "in", "candidates", ":", "\n", "                ", "weights", ".", "append", "(", "max_weight", "-", "weight", "[", "tokens", "[", "idx", "]", "]", "+", "1e-6", ")", "\n", "# weights.append(max_weight - weight[tokens[idx].lower()] + 1e-6)", "\n", "", "weights", "=", "np", ".", "array", "(", "weights", ")", "/", "sum", "(", "weights", ")", "\n", "\n", "return", "np", ".", "random", ".", "choice", "(", "candidates", ",", "1", ",", "p", "=", "weights", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "return", "random", ".", "choice", "(", "candidates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.sample_token": [[263, 298], ["zip", "len", "random.choice", "candidates.append", "len"], "methods", ["None"], ["", "", "def", "sample_token", "(", "self", ",", "token", ",", "same_length", "=", "True", ",", "max_candidates", "=", "10", ")", ":", "\n", "        ", "\"\"\" Randomly sample a token's similar token stored in the index\n\n        Args:\n            token (str): the input token\n            same_length (bool, optional): whether the return token should have the same\n                length in BERT\n            max_candidates (int, optional): the maximal number of candidates\n                to be sampled\n\n        Returns:\n            str: the sampled token (unchanged if the input is not in index)\n        \"\"\"", "\n", "# token = token.lower()", "\n", "index", "=", "self", ".", "index", "[", "'token'", "]", "\n", "if", "token", "in", "index", "and", "index", "[", "token", "]", "[", "'similar_words'", "]", "!=", "None", ":", "\n", "            ", "if", "same_length", ":", "\n", "                ", "bert_length", "=", "index", "[", "token", "]", "[", "'bert_length'", "]", "\n", "candidates", "=", "[", "]", "\n", "for", "ts", ",", "bl", "in", "zip", "(", "index", "[", "token", "]", "[", "'similar_words'", "]", ",", "\n", "index", "[", "token", "]", "[", "'similar_words_length'", "]", ")", ":", "\n", "                    ", "t", ",", "_", "=", "ts", "\n", "if", "bl", "==", "bert_length", ":", "\n", "                        ", "candidates", ".", "append", "(", "t", ")", "\n", "if", "len", "(", "candidates", ")", ">=", "max_candidates", ":", "\n", "                            ", "break", "\n", "", "", "", "", "else", ":", "\n", "                ", "candidates", "=", "[", "t", "for", "t", ",", "_", "in", "index", "[", "token", "]", "[", "'similar_words'", "]", "[", ":", "max_candidates", "]", "]", "\n", "", "if", "len", "(", "candidates", ")", "<=", "0", ":", "\n", "                ", "return", "token", "\n", "", "else", ":", "\n", "                ", "return", "random", ".", "choice", "(", "candidates", ")", "\n", "", "", "else", ":", "\n", "            ", "return", "token", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.sample_span_position": [[299, 333], ["len", "len", "random.choice", "candidates.append", "len"], "methods", ["None"], ["", "", "def", "sample_span_position", "(", "self", ",", "tokens", ",", "labels", ")", ":", "\n", "        ", "\"\"\" Uniformly sample a span from a training example and return its positions.\n\n        The output is a pair (start_op, end_op) of the span.\n\n        Args:\n            tokens (list of strings): the input tokens\n            labels (list of strings): the labels of the tokens\n\n        Returns:\n            int: the start position (-1 if no available span)\n            int: the ending position (-1 if no available span)\n        \"\"\"", "\n", "index", "=", "self", ".", "index", "[", "'span'", "]", "\n", "candidates", "=", "[", "]", "\n", "idx", "=", "0", "\n", "while", "idx", "<", "len", "(", "tokens", ")", ":", "\n", "            ", "if", "labels", "[", "idx", "]", "!=", "'O'", ":", "\n", "                ", "start", "=", "idx", "\n", "while", "idx", "+", "1", "<", "len", "(", "tokens", ")", "and", "labels", "[", "idx", "+", "1", "]", "[", "1", ":", "]", "==", "labels", "[", "idx", "]", "[", "1", ":", "]", ":", "\n", "                    ", "idx", "+=", "1", "\n", "", "end", "=", "idx", "\n", "\n", "span", "=", "' '", ".", "join", "(", "tokens", "[", "start", ":", "end", "+", "1", "]", ")", "# .lower()", "\n", "if", "span", "in", "index", "[", "'aspect'", "]", "or", "span", "in", "index", "[", "'opinion'", "]", ":", "\n", "                    ", "candidates", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "", "", "idx", "+=", "1", "\n", "\n", "", "if", "len", "(", "candidates", ")", ">", "0", ":", "\n", "            ", "return", "random", ".", "choice", "(", "candidates", ")", "\n", "", "else", ":", "\n", "            ", "return", "(", "-", "1", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixmatchnl.tagging_criterion": [[21, 34], ["torch.sum().mean", "torch.sum().mean", "torch.sum().mean", "torch.sum().mean", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "pred_labeled[].log"], "function", ["None"], ["def", "tagging_criterion", "(", "pred_labeled", ",", "y_labeled", ")", ":", "\n", "    ", "\"\"\"The loss function for tagging task (with float tensor input)\n\n    Args:\n        pred_labeled (Tensor): the predicted float tensor\n        y_labeled (Tensor): the groundtruth float tensor\n\n    Returns:\n        Tensor: the cross entropy loss with the 0'th dimention ignored\n    \"\"\"", "\n", "# cross-entropy, ignore the 0 class", "\n", "loss_x", "=", "torch", ".", "sum", "(", "-", "y_labeled", "[", ":", ",", "1", ":", "]", "*", "pred_labeled", "[", ":", ",", "1", ":", "]", ".", "log", "(", ")", ",", "-", "1", ")", ".", "mean", "(", ")", "\n", "return", "loss_x", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixmatchnl.classifier_criterion": [[35, 47], ["torch.sum().mean", "torch.sum().mean", "torch.sum().mean", "torch.sum().mean", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "pred_labeled.log"], "function", ["None"], ["", "def", "classifier_criterion", "(", "pred_labeled", ",", "y_labeled", ")", ":", "\n", "    ", "\"\"\"The loss function for classification task (with float tensor input)\n\n    Args:\n        pred_labeled (Tensor): the predicted float tensor\n        y_labeled (Tensor): the groundtruth float tensor\n\n    Returns:\n        Tensor: the cross entropy loss\n    \"\"\"", "\n", "loss_x", "=", "torch", ".", "sum", "(", "-", "y_labeled", "*", "pred_labeled", ".", "log", "(", ")", ",", "-", "1", ")", ".", "mean", "(", ")", "\n", "return", "loss_x", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixmatchnl.mixmatch": [[49, 176], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.eval", "model", "u_guess.repeat.pow", "model.train", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "numpy.random.beta", "max", "torch.one_hot().float", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model", "torch.softmax", "logits[].view", "logits[].view", "y[].view", "y[].view", "u_augs.append", "model", "torch.softmax", "u_guess.repeat.detach", "u_guesses.append", "u_aug_enc_list.append", "sum", "len", "u_guess.pow.sum", "len", "u_guess.repeat.repeat", "u_guess.repeat.repeat", "numpy.random.beta", "mixmatchnl.tagging_criterion", "torch.mse_loss", "mixmatchnl.classifier_criterion", "torch.mse_loss", "torch.cat.size", "numpy.random.beta", "u_guess.repeat.size", "torch.one_hot", "u_guess.repeat.size"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.baseline.train", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixmatchnl.tagging_criterion", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixmatchnl.classifier_criterion"], ["", "def", "mixmatch", "(", "model", ",", "batch", ",", "num_aug", "=", "2", ",", "alpha", "=", "0.4", ",", "alpha_aug", "=", "0.4", ",", "u_lambda", "=", "0.5", ")", ":", "\n", "    ", "\"\"\"Perform one iteration of MixMatchNL\n\n    Args:\n        model (MultiTaskNet): the model state\n        batch (tuple): the input batch\n        num_aug (int, Optional): the number of augmented examples in the batch\n        alpha (float, Optional): the parameter for MixUp\n        alpha_aug (float, Optional): the parameter for MixDA\n        u_lambda (float, Optional): the parameter controlling\n            the weight of unlabeled data\n\n    Returns:\n        Tensor: the loss (of 0-d)\n    \"\"\"", "\n", "_", ",", "x", ",", "_", ",", "_", ",", "mask", ",", "y", ",", "_", ",", "taskname", "=", "batch", "\n", "taskname", "=", "taskname", "[", "0", "]", "\n", "# two batches of labeled and two batches of unlabeled", "\n", "batch_size", "=", "x", ".", "size", "(", ")", "[", "0", "]", "//", "(", "num_aug", "+", "3", ")", "\n", "\n", "y", "=", "y", "[", ":", "batch_size", "]", "\n", "\n", "# the unlabeled half", "\n", "u0", "=", "x", "[", "batch_size", ":", "2", "*", "batch_size", "]", "\n", "\n", "# augmented", "\n", "aug_x", "=", "x", "[", "2", "*", "batch_size", ":", "3", "*", "batch_size", "]", "\n", "\n", "# augmented unlabeled", "\n", "u_augs", "=", "[", "]", "\n", "for", "uid", "in", "range", "(", "num_aug", ")", ":", "\n", "        ", "u_augs", ".", "append", "(", "x", "[", "(", "3", "+", "uid", ")", "*", "batch_size", ":", "(", "4", "+", "uid", ")", "*", "batch_size", "]", ")", "\n", "\n", "# labeled + original unlabeled", "\n", "", "x", "=", "torch", ".", "cat", "(", "(", "x", "[", ":", "batch_size", "]", ",", "x", "[", "3", "*", "batch_size", ":", "]", ")", ")", "\n", "\n", "# label guessing", "\n", "model", ".", "eval", "(", ")", "\n", "u_guesses", "=", "[", "]", "\n", "u_aug_enc_list", "=", "[", "]", "\n", "_", ",", "_", ",", "_", ",", "u_enc", "=", "model", "(", "u0", ",", "y", ",", "\n", "task", "=", "taskname", ",", "get_enc", "=", "True", ")", "\n", "\n", "for", "x_u", "in", "u_augs", ":", "\n", "        ", "if", "alpha_aug", "<=", "0", ":", "\n", "            ", "u_aug_lam", "=", "1.0", "\n", "", "else", ":", "\n", "            ", "u_aug_lam", "=", "np", ".", "random", ".", "beta", "(", "alpha_aug", ",", "alpha_aug", ")", "\n", "\n", "# it is fine to switch the order of x_u and u0 in this case", "\n", "", "u_logits", ",", "y", ",", "_", ",", "u_aug_enc", "=", "model", "(", "x_u", ",", "y", ",", "\n", "augment_batch", "=", "(", "u0", ",", "u_aug_lam", ")", ",", "\n", "aug_enc", "=", "u_enc", ",", "\n", "task", "=", "taskname", ",", "\n", "get_enc", "=", "True", ")", "\n", "# softmax", "\n", "u_guess", "=", "F", ".", "softmax", "(", "u_logits", ",", "dim", "=", "-", "1", ")", "\n", "u_guess", "=", "u_guess", ".", "detach", "(", ")", "\n", "u_guesses", ".", "append", "(", "u_guess", ")", "\n", "\n", "# save u_aug_enc", "\n", "u_aug_enc_list", ".", "append", "(", "u_aug_enc", ")", "\n", "\n", "# averaging", "\n", "", "u_guess", "=", "sum", "(", "u_guesses", ")", "/", "len", "(", "u_guesses", ")", "\n", "\n", "# temperature sharpening", "\n", "T", "=", "0.5", "\n", "u_power", "=", "u_guess", ".", "pow", "(", "1", "/", "T", ")", "\n", "u_guess", "=", "u_power", "/", "u_power", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "# make duplicate of u_guess", "\n", "if", "len", "(", "u_guess", ".", "size", "(", ")", ")", "==", "2", ":", "\n", "        ", "u_guess", "=", "u_guess", ".", "repeat", "(", "num_aug", ",", "1", ")", "\n", "", "else", ":", "\n", "        ", "u_guess", "=", "u_guess", ".", "repeat", "(", "num_aug", ",", "1", ",", "1", ")", "\n", "\n", "", "vocab", "=", "u_guess", ".", "shape", "[", "-", "1", "]", "\n", "# switch back to training mode", "\n", "model", ".", "train", "(", ")", "\n", "\n", "# shuffle", "\n", "index", "=", "torch", ".", "randperm", "(", "batch_size", "+", "u_guess", ".", "size", "(", ")", "[", "0", "]", ")", "\n", "lam", "=", "np", ".", "random", ".", "beta", "(", "alpha", ",", "alpha", ")", "\n", "lam", "=", "max", "(", "lam", ",", "1.0", "-", "lam", ")", "\n", "\n", "# convert y to one-hot", "\n", "y_onehot", "=", "F", ".", "one_hot", "(", "y", ",", "vocab", ")", ".", "float", "(", ")", "\n", "y_concat", "=", "torch", ".", "cat", "(", "(", "y_onehot", ",", "u_guess", ")", ")", "\n", "y_mixed", "=", "y_concat", "[", "index", ",", ":", "]", "\n", "\n", "# x_aug_enc", "\n", "_", ",", "_", ",", "_", ",", "x_enc", "=", "model", "(", "x", "[", ":", "batch_size", "]", ",", "y", ",", "\n", "task", "=", "taskname", ",", "\n", "get_enc", "=", "True", ")", "\n", "# concatenate the augmented encodings", "\n", "x_enc", "=", "torch", ".", "cat", "(", "[", "x_enc", "]", "+", "u_aug_enc_list", ")", "\n", "\n", "# forward", "\n", "if", "alpha_aug", "<=", "0", ":", "\n", "        ", "aug_lam", "=", "1.0", "\n", "", "else", ":", "\n", "        ", "aug_lam", "=", "np", ".", "random", ".", "beta", "(", "alpha_aug", ",", "alpha_aug", ")", "\n", "", "logits", ",", "y_concat", ",", "_", "=", "model", "(", "x", ",", "y_concat", ",", "\n", "augment_batch", "=", "(", "aug_x", ",", "aug_lam", ")", ",", "\n", "x_enc", "=", "x_enc", ",", "\n", "second_batch", "=", "(", "index", ",", "lam", ")", ",", "\n", "task", "=", "taskname", ")", "\n", "logits", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "l_pred", "=", "logits", "[", ":", "batch_size", "]", ".", "view", "(", "-", "1", ",", "vocab", ")", "\n", "u_pred", "=", "logits", "[", "batch_size", ":", "]", ".", "view", "(", "-", "1", ",", "vocab", ")", "\n", "\n", "# mixup y's", "\n", "y", "=", "lam", "*", "y_concat", "+", "(", "1.0", "-", "lam", ")", "*", "y_mixed", "\n", "l_y", "=", "y", "[", ":", "batch_size", "]", ".", "view", "(", "-", "1", ",", "vocab", ")", "\n", "u_y", "=", "y", "[", "batch_size", ":", "]", ".", "view", "(", "-", "1", ",", "vocab", ")", "\n", "\n", "# cross entropy on label data + mse on unlabeled data", "\n", "if", "'tagging'", "in", "taskname", ":", "\n", "        ", "loss_x", "=", "tagging_criterion", "(", "l_pred", ",", "l_y", ")", "\n", "loss_u", "=", "F", ".", "mse_loss", "(", "u_pred", "[", ":", ",", "1", ":", "]", ",", "u_y", "[", ":", ",", "1", ":", "]", ")", "\n", "", "else", ":", "\n", "        ", "loss_x", "=", "classifier_criterion", "(", "l_pred", ",", "l_y", ")", "\n", "loss_u", "=", "F", ".", "mse_loss", "(", "u_pred", ",", "u_y", ")", "\n", "\n", "", "loss", "=", "loss_x", "+", "loss_u", "*", "u_lambda", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixmatchnl.create_mixmatch_batches": [[182, 250], ["len", "numpy.random.permutation", "len", "enumerate", "random.shuffle", "len", "list", "random.shuffle", "numpy.array", "numpy.random.permutation", "l_batch.append", "l_batch_aug.append", "u_batch.append", "range", "range", "range", "u_batch_aug[].append", "mixed_batches.append", "l_batch.clear", "l_batch_aug.clear", "u_batch.clear", "len", "len", "padder", "ub.clear", "len"], "function", ["None"], ["def", "create_mixmatch_batches", "(", "l_set", ",", "aug_set", ",", "u_set", ",", "u_set_aug", ",", "\n", "num_aug", "=", "2", ",", "\n", "batch_size", "=", "16", ")", ":", "\n", "    ", "\"\"\"Create batches for mixmatchnl\n\n    Each batch is the concatenation of (1) a labeled batch, (2) an augmented\n    labeled batch (having the same order of (1) ), (3) an unlabeled batch,\n    and (4) multiple augmented unlabeled batches of the same order\n    of (3).\n\n    Args:\n        l_set (SnippextDataset): the train set\n        aug_set (SnippextDataset): the augmented train set\n        u_set (SnippextDataset): the unlabeled set\n        u_set_aug (SnippextDataset): the augmented unlabeled set\n        num_aug (int, optional): number of unlabeled augmentations to be created\n        batch_size (int, optional): batch size (of each component)\n\n    Returns:\n        list of list: the created batches\n    \"\"\"", "\n", "mixed_batches", "=", "[", "]", "\n", "num_labeled", "=", "len", "(", "l_set", ")", "\n", "l_index", "=", "np", ".", "random", ".", "permutation", "(", "num_labeled", ")", "\n", "# num_unlabeled = len(u_set)", "\n", "# u_index = np.random.permutation(num_unlabeled)", "\n", "\n", "global", "u_order", "\n", "if", "len", "(", "u_order", ")", "==", "0", ":", "\n", "        ", "u_order", "=", "list", "(", "range", "(", "len", "(", "u_set", ")", ")", ")", "\n", "random", ".", "shuffle", "(", "u_order", ")", "\n", "u_order", "=", "np", ".", "array", "(", "u_order", ")", "\n", "\n", "", "global", "epoch_idx", "\n", "u_index", "=", "np", ".", "random", ".", "permutation", "(", "num_labeled", ")", "+", "num_labeled", "*", "epoch_idx", "\n", "u_index", "%=", "len", "(", "u_set", ")", "\n", "u_index", "=", "u_order", "[", "u_index", "]", "\n", "epoch_idx", "+=", "1", "\n", "\n", "l_batch", "=", "[", "]", "\n", "l_batch_aug", "=", "[", "]", "\n", "u_batch", "=", "[", "]", "\n", "u_batch_aug", "=", "[", "[", "]", "for", "_", "in", "range", "(", "num_aug", ")", "]", "\n", "padder", "=", "l_set", ".", "pad", "\n", "\n", "for", "i", ",", "idx", "in", "enumerate", "(", "l_index", ")", ":", "\n", "        ", "u_idx", "=", "u_index", "[", "i", "]", "\n", "l_batch", ".", "append", "(", "l_set", "[", "idx", "]", ")", "\n", "l_batch_aug", ".", "append", "(", "aug_set", "[", "idx", "]", ")", "\n", "# add augmented examples of unlabeled", "\n", "u_batch", ".", "append", "(", "u_set", "[", "u_idx", "]", ")", "\n", "for", "uid", "in", "range", "(", "num_aug", ")", ":", "\n", "            ", "u_batch_aug", "[", "uid", "]", ".", "append", "(", "u_set_aug", "[", "u_idx", "]", ")", "\n", "\n", "", "if", "len", "(", "l_batch", ")", "==", "batch_size", "or", "i", "==", "len", "(", "l_index", ")", "-", "1", ":", "\n", "            ", "batches", "=", "l_batch", "+", "u_batch", "+", "l_batch_aug", "\n", "for", "ub", "in", "u_batch_aug", ":", "\n", "                ", "batches", "+=", "ub", "\n", "\n", "", "mixed_batches", ".", "append", "(", "padder", "(", "batches", ")", ")", "\n", "l_batch", ".", "clear", "(", ")", "\n", "l_batch_aug", ".", "clear", "(", ")", "\n", "u_batch", ".", "clear", "(", ")", "\n", "for", "ub", "in", "u_batch_aug", ":", "\n", "                ", "ub", ".", "clear", "(", ")", "\n", "", "", "", "random", ".", "shuffle", "(", "mixed_batches", ")", "\n", "\n", "return", "mixed_batches", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixmatchnl.train": [[252, 332], ["mixmatchnl.create_mixmatch_batches", "model.train", "enumerate", "optimizer.zero_grad", "mixmatchnl.mixmatch", "optimizer.step", "mixmatch.backward", "scheduler.step", "print", "print", "print", "print", "print", "numpy.isscalar", "print", "print", "print", "print", "print", "print", "print", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "apex.amp.scale_loss", "scaled_loss.backward", "_y.cpu().numpy", "print", "print", "max", "dataset.get_tokenizer().convert_ids_to_tokens", "x.cpu().numpy", "_y.cpu", "mixmatch.item", "dataset.get_tokenizer", "x.cpu().numpy", "x.cpu", "x.cpu"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixmatchnl.create_mixmatch_batches", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.baseline.train", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixmatchnl.mixmatch", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.get_tokenizer"], ["", "def", "train", "(", "model", ",", "l_set", ",", "aug_set", ",", "u_set", ",", "u_set_aug", ",", "optimizer", ",", "\n", "scheduler", "=", "None", ",", "\n", "batch_size", "=", "32", ",", "\n", "num_aug", "=", "2", ",", "\n", "alpha", "=", "0.4", ",", "\n", "alpha_aug", "=", "0.8", ",", "\n", "u_lambda", "=", "1.0", ",", "\n", "fp16", "=", "False", ")", ":", "\n", "    ", "\"\"\"Perform one epoch of MixMatchNL\n\n    Args:\n        model (MultiTaskModel): the model state\n        train_dataset (SnippextDataset): the train set\n        augment_dataset (SnippextDataset): the augmented train set\n        u_dataset (SnippextDataset): the unlabeled set\n        u_dataset_aug (SnippextDataset): the augmented unlabeled set\n        optimizer (Optimizer): Adam\n        scheduler (Scheduler, optional): the learning rate scheduler\n        fp16 (boolean): whether to use fp16\n        num_aug (int, Optional):\n        batch_size (int, Optional): batch size\n        alpha (float, Optional): the alpha for MixUp\n        alpha_aug (float, Optional): the alpha for MixDA\n        u_lambda (float, Optional): the weight of unlabeled data\n\n    Returns:\n        None\n    \"\"\"", "\n", "mixed_batches", "=", "create_mixmatch_batches", "(", "l_set", ",", "\n", "aug_set", ",", "\n", "u_set", ",", "\n", "u_set_aug", ",", "\n", "num_aug", "=", "num_aug", ",", "\n", "batch_size", "=", "batch_size", "//", "2", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "mixed_batches", ")", ":", "\n", "# for monitoring", "\n", "# print('memory:', torch.cuda.memory_allocated(), 'cached:', torch.cuda.memory_cached())", "\n", "        ", "words", ",", "x", ",", "is_heads", ",", "tags", ",", "mask", ",", "y", ",", "seqlens", ",", "taskname", "=", "batch", "\n", "taskname", "=", "taskname", "[", "0", "]", "\n", "_y", "=", "y", "\n", "\n", "# perform mixmatch", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "try", ":", "\n", "            ", "loss", "=", "mixmatch", "(", "model", ",", "batch", ",", "num_aug", ",", "alpha", ",", "alpha_aug", ",", "u_lambda", ")", "\n", "if", "fp16", ":", "\n", "                ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "if", "scheduler", ":", "\n", "                ", "scheduler", ".", "step", "(", ")", "\n", "\n", "", "if", "i", "==", "0", ":", "\n", "                ", "print", "(", "\"=====sanity check======\"", ")", "\n", "print", "(", "\"words:\"", ",", "words", "[", "0", "]", ")", "\n", "print", "(", "\"x:\"", ",", "x", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "[", ":", "seqlens", "[", "0", "]", "]", ")", "\n", "print", "(", "\"tokens:\"", ",", "get_tokenizer", "(", ")", ".", "convert_ids_to_tokens", "(", "x", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", ")", "[", ":", "seqlens", "[", "0", "]", "]", ")", "\n", "print", "(", "\"is_heads:\"", ",", "is_heads", "[", "0", "]", ")", "\n", "y_sample", "=", "_y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "\n", "if", "np", ".", "isscalar", "(", "y_sample", ")", ":", "\n", "                    ", "print", "(", "\"y:\"", ",", "y_sample", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"y:\"", ",", "y_sample", "[", ":", "seqlens", "[", "0", "]", "]", ")", "\n", "", "print", "(", "\"tags:\"", ",", "tags", "[", "0", "]", ")", "\n", "print", "(", "\"mask:\"", ",", "mask", "[", "0", "]", ")", "\n", "print", "(", "\"seqlen:\"", ",", "seqlens", "[", "0", "]", ")", "\n", "print", "(", "\"task_name:\"", ",", "taskname", ")", "\n", "print", "(", "\"=======================\"", ")", "\n", "\n", "", "if", "i", "%", "10", "==", "0", ":", "# monitoring", "\n", "                ", "print", "(", "f\"step: {i}, task: {taskname}, loss: {loss.item()}\"", ")", "\n", "del", "loss", "\n", "", "", "except", ":", "\n", "            ", "print", "(", "\"debug - seqlen:\"", ",", "max", "(", "seqlens", ")", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.mixmatchnl.initialize_and_train": [[334, 434], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "transformers.get_linear_schedule_with_warmup", "tensorboardX.SummaryWriter", "range", "tensorboardX.SummaryWriter.close", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "model.MultiTaskNet", "transformers.AdamW", "model.MultiTaskNet().cuda", "transformers.AdamW", "apex.amp.initialize", "os.path.exists", "os.makedirs", "mixmatchnl.train", "print", "train_util.eval_on_task", "MultiTaskNet().cuda.parameters", "MultiTaskNet().cuda.parameters", "model.MultiTaskNet", "len", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "MultiTaskNet().cuda.state_dict", "MultiTaskNet().cuda.state_dict"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.initialize", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.baseline.train", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_on_task"], ["", "", "", "def", "initialize_and_train", "(", "task_config", ",", "\n", "trainset", ",", "\n", "augmentset", ",", "\n", "validset", ",", "\n", "testset", ",", "\n", "uset", ",", "\n", "uset_aug", ",", "\n", "hp", ",", "\n", "run_tag", ")", ":", "\n", "    ", "\"\"\"The train process.\n\n    Args:\n        task_config (dictionary): the configuration of the task\n        trainset (SnippextDataset): the training set\n        augmentset (SnippextDataset): the augmented training set\n        validset (SnippextDataset): the validation set\n        testset (SnippextDataset): the testset\n        uset (SnippextDataset): the unlabeled dataset\n        uset_aug (SnippextDataset): the unlabeled dataset, augmented\n        hp (Namespace): the parsed hyperparameters\n        run_tag (string): the tag of the run (for logging purpose)\n\n    Returns:\n        None\n    \"\"\"", "\n", "padder", "=", "SnippextDataset", ".", "pad", "\n", "\n", "# iterators for dev/test set", "\n", "valid_iter", "=", "data", ".", "DataLoader", "(", "dataset", "=", "validset", ",", "\n", "batch_size", "=", "hp", ".", "batch_size", "*", "4", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "padder", ")", "\n", "test_iter", "=", "data", ".", "DataLoader", "(", "dataset", "=", "testset", ",", "\n", "batch_size", "=", "hp", ".", "batch_size", "*", "4", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "padder", ")", "\n", "\n", "\n", "# initialize model", "\n", "device", "=", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", "\n", "if", "device", "==", "'cpu'", ":", "\n", "        ", "model", "=", "MultiTaskNet", "(", "[", "task_config", "]", ",", "device", ",", "\n", "hp", ".", "finetuning", ",", "bert_path", "=", "hp", ".", "bert_path", ")", "\n", "optimizer", "=", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "hp", ".", "lr", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "MultiTaskNet", "(", "[", "task_config", "]", ",", "device", ",", "\n", "hp", ".", "finetuning", ",", "bert_path", "=", "hp", ".", "bert_path", ")", ".", "cuda", "(", ")", "\n", "optimizer", "=", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "hp", ".", "lr", ")", "\n", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "'O2'", ")", "\n", "\n", "# learning rate scheduler", "\n", "", "num_steps", "=", "(", "len", "(", "trainset", ")", "//", "hp", ".", "batch_size", "*", "2", ")", "*", "hp", ".", "n_epochs", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "\n", "num_warmup_steps", "=", "num_steps", "//", "10", ",", "\n", "num_training_steps", "=", "num_steps", ")", "\n", "\n", "# create logging", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "hp", ".", "logdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "hp", ".", "logdir", ")", "\n", "", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "hp", ".", "logdir", ")", "\n", "\n", "# start training", "\n", "best_dev_f1", "=", "best_test_f1", "=", "0.0", "\n", "for", "epoch", "in", "range", "(", "1", ",", "hp", ".", "n_epochs", "+", "1", ")", ":", "\n", "        ", "train", "(", "model", ",", "\n", "trainset", ",", "\n", "augmentset", ",", "\n", "uset", ",", "\n", "uset_aug", ",", "\n", "optimizer", ",", "\n", "scheduler", "=", "scheduler", ",", "\n", "batch_size", "=", "hp", ".", "batch_size", ",", "\n", "num_aug", "=", "hp", ".", "num_aug", ",", "\n", "alpha", "=", "hp", ".", "alpha", ",", "\n", "alpha_aug", "=", "hp", ".", "alpha_aug", ",", "\n", "u_lambda", "=", "hp", ".", "u_lambda", ",", "\n", "fp16", "=", "hp", ".", "fp16", ")", "\n", "\n", "print", "(", "f\"=========eval at epoch={epoch}=========\"", ")", "\n", "dev_f1", ",", "test_f1", "=", "eval_on_task", "(", "epoch", ",", "\n", "model", ",", "\n", "task_config", "[", "'name'", "]", ",", "\n", "valid_iter", ",", "\n", "validset", ",", "\n", "test_iter", ",", "\n", "testset", ",", "\n", "writer", ",", "\n", "run_tag", ")", "\n", "\n", "if", "hp", ".", "save_model", ":", "\n", "            ", "if", "dev_f1", ">", "best_dev_f1", ":", "\n", "                ", "best_dev_f1", "=", "dev_f1", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "run_tag", "+", "'_dev.pt'", ")", "\n", "", "if", "test_f1", ">", "best_test_f1", ":", "\n", "                ", "best_test_f1", "=", "test_f1", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "run_tag", "+", "'_test.pt'", ")", "\n", "\n", "", "", "", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_tagging": [[12, 79], ["model.eval", "conlleval.evaluate_conll_file", "os.remove", "print", "print", "print", "print", "print", "print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "open", "zip", "open", "sum", "torch.CrossEntropyLoss", "model", "logits.view.view", "y.view.view", "nn.CrossEntropyLoss.", "loss_list.append", "Words.extend", "Is_heads.extend", "Tags.extend", "Y.extend", "Y_hat.extend", "uuid.uuid4", "y.view.cpu().numpy().tolist", "y_hat.cpu().numpy().tolist", "len", "len", "len", "zip", "fout.write", "loss_fct.item", "zip", "words.split", "tags.split", "fout.write", "y.view.cpu().numpy", "y_hat.cpu().numpy", "words.split", "tags.split", "y.view.cpu", "y_hat.cpu"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.evaluate_conll_file"], ["def", "eval_tagging", "(", "model", ",", "iterator", ",", "idx2tag", ")", ":", "\n", "    ", "\"\"\"Evaluate a tagging model state on a dev/test set.\n\n    Args:\n        model (MultiTaskNet): the model state\n        iterator (DataLoader): a batch iterator of the dev/test set\n        idx2tag (dict): a mapping from tag indices to tag names\n\n    Returns:\n        float: precision\n        float: recall\n        float: f1\n        float: loss\n    \"\"\"", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "Words", ",", "Is_heads", ",", "Tags", ",", "Y", ",", "Y_hat", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "loss_list", "=", "[", "]", "\n", "total_size", "=", "0", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "iterator", ")", ":", "\n", "            ", "words", ",", "x", ",", "is_heads", ",", "tags", ",", "mask", ",", "y", ",", "seqlens", ",", "taskname", "=", "batch", "\n", "\n", "taskname", "=", "taskname", "[", "0", "]", "\n", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "0", ")", "\n", "batch_size", "=", "y", ".", "shape", "[", "0", "]", "\n", "\n", "logits", ",", "y", ",", "y_hat", "=", "model", "(", "x", ",", "y", ",", "task", "=", "taskname", ")", "# y_hat: (N, T)", "\n", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "y", ")", "\n", "loss_list", ".", "append", "(", "loss", ".", "item", "(", ")", "*", "batch_size", ")", "\n", "total_size", "+=", "batch_size", "\n", "\n", "Words", ".", "extend", "(", "words", ")", "\n", "Is_heads", ".", "extend", "(", "is_heads", ")", "\n", "Tags", ".", "extend", "(", "tags", ")", "\n", "Y", ".", "extend", "(", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "\n", "Y_hat", ".", "extend", "(", "y_hat", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "# gets results and save", "\n", "", "", "eval_fname", "=", "\"temp_da_\"", "+", "taskname", "+", "'_'", "+", "uuid", ".", "uuid4", "(", ")", ".", "hex", "\n", "with", "open", "(", "eval_fname", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "for", "words", ",", "is_heads", ",", "tags", ",", "y_hat", "in", "zip", "(", "Words", ",", "Is_heads", ",", "Tags", ",", "Y_hat", ")", ":", "\n", "            ", "y_hat", "=", "[", "hat", "for", "head", ",", "hat", "in", "zip", "(", "is_heads", ",", "y_hat", ")", "if", "head", "==", "1", "]", "\n", "preds", "=", "[", "idx2tag", "[", "hat", "]", "for", "hat", "in", "y_hat", "]", "\n", "if", "len", "(", "preds", ")", "==", "len", "(", "words", ".", "split", "(", ")", ")", "==", "len", "(", "tags", ".", "split", "(", ")", ")", ":", "\n", "                ", "for", "w", ",", "t", ",", "p", "in", "zip", "(", "words", ".", "split", "(", ")", "[", "1", ":", "-", "1", "]", ",", "tags", ".", "split", "(", ")", "[", "1", ":", "-", "1", "]", ",", "preds", "[", "1", ":", "-", "1", "]", ")", ":", "\n", "                    ", "if", "p", "==", "'<PAD>'", ":", "\n", "                        ", "p", "=", "'O'", "\n", "", "if", "t", "==", "'<PAD>'", ":", "\n", "                        ", "p", "=", "t", "=", "'O'", "\n", "", "fout", ".", "write", "(", "f\"{w} {t} {p}\\n\"", ")", "\n", "", "fout", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "## calc metric", "\n", "", "", "", "precision", ",", "recall", ",", "f1", "=", "evaluate_conll_file", "(", "open", "(", "eval_fname", ")", ")", "\n", "loss", "=", "sum", "(", "loss_list", ")", "/", "total_size", "\n", "os", ".", "remove", "(", "eval_fname", ")", "\n", "print", "(", "\"=============%s==================\"", "%", "taskname", ")", "\n", "print", "(", "\"precision=%.3f\"", "%", "precision", ")", "\n", "print", "(", "\"recall=%.3f\"", "%", "recall", ")", "\n", "print", "(", "\"f1=%.3f\"", "%", "f1", ")", "\n", "print", "(", "\"loss=%.3f\"", "%", "loss", ")", "\n", "print", "(", "\"=====================================\"", ")", "\n", "return", "precision", ",", "recall", ",", "f1", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_classifier": [[80, 188], ["model.eval", "print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "sum", "numpy.array().squeeze", "numpy.array", "transformers.data.glue_compute_metrics", "print", "model", "logits.view.view", "y1.view.view", "loss_list.append", "np.array.extend", "np.array().squeeze.extend", "Y_prob.extend", "[].lower", "numpy.array().squeeze", "numpy.array", "transformers.data.glue_compute_metrics", "print", "len", "taskname.lower", "y.numpy().tolist", "y_hat.cpu().numpy().tolist", "[].cpu().numpy().tolist", "numpy.array", "set", "sklearn.accuracy_score", "sklearn.precision_score", "sklearn.recall_score", "sklearn.f1_score", "any", "print", "print", "print", "print", "print", "sklearn.accuracy_score", "sklearn.f1_score", "print", "print", "print", "torch.MSELoss", "torch.CrossEntropyLoss", "loss.item", "numpy.array", "y.numpy", "y_hat.cpu().numpy", "[].cpu().numpy", "taskname.split", "numpy.arange", "sklearn.accuracy_score", "sklearn.precision_score", "sklearn.recall_score", "sklearn.f1_score", "sklearn.f1_score", "y_hat.cpu", "[].cpu", "sklearn.accuracy_score", "sklearn.precision_score", "sklearn.recall_score", "zip", "zip", "logits.view.softmax().max", "logits.view.softmax"], "function", ["None"], ["", "def", "eval_classifier", "(", "model", ",", "iterator", ",", "threshold", "=", "None", ",", "get_threshold", "=", "False", ")", ":", "\n", "    ", "\"\"\"Evaluate a classification model state on a dev/test set.\n\n    Args:\n        model (MultiTaskNet): the model state\n        iterator (DataLoader): a batch iterator of the dev/test set\n        threshold (float, optional): the cut-off threshold for binary cls\n        get_threshold (boolean, optional): return the selected threshold if True\n\n    Returns:\n        float: Precision (or accuracy if more than 2 classes)\n        float: Recall (or accuracy if more than 2 classes)\n        float: F1 (or macro F1 if more than 2 classes)\n        float: The Loss\n        float: The cut-off threshold\n    \"\"\"", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "Y", "=", "[", "]", "\n", "Y_hat", "=", "[", "]", "\n", "Y_prob", "=", "[", "]", "\n", "loss_list", "=", "[", "]", "\n", "total_size", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "iterator", ")", ":", "\n", "            ", "_", ",", "x", ",", "_", ",", "_", ",", "_", ",", "y", ",", "_", ",", "taskname", "=", "batch", "\n", "taskname", "=", "taskname", "[", "0", "]", "\n", "logits", ",", "y1", ",", "y_hat", "=", "model", "(", "x", ",", "y", ",", "task", "=", "taskname", ")", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "y1", "=", "y1", ".", "view", "(", "-", "1", ")", "\n", "if", "'sts-b'", "in", "taskname", ".", "lower", "(", ")", ":", "\n", "                ", "loss", "=", "nn", ".", "MSELoss", "(", ")", "(", "logits", ",", "y1", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "(", "logits", ",", "y1", ")", "\n", "\n", "", "loss_list", ".", "append", "(", "loss", ".", "item", "(", ")", "*", "y", ".", "shape", "[", "0", "]", ")", "\n", "total_size", "+=", "y", ".", "shape", "[", "0", "]", "\n", "\n", "Y", ".", "extend", "(", "y", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "\n", "Y_hat", ".", "extend", "(", "y_hat", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "\n", "Y_prob", ".", "extend", "(", "logits", ".", "softmax", "(", "dim", "=", "-", "1", ")", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "loss", "=", "sum", "(", "loss_list", ")", "/", "total_size", "\n", "\n", "print", "(", "\"=============%s==================\"", "%", "taskname", ")", "\n", "\n", "# for glue", "\n", "if", "taskname", "in", "glue_processors", ":", "\n", "        ", "Y_hat", "=", "np", ".", "array", "(", "Y_hat", ")", ".", "squeeze", "(", ")", "\n", "Y", "=", "np", ".", "array", "(", "Y", ")", "\n", "result", "=", "glue_compute_metrics", "(", "taskname", ",", "Y_hat", ",", "Y", ")", "\n", "result", "[", "'loss'", "]", "=", "loss", "\n", "print", "(", "result", ")", "\n", "return", "result", "\n", "", "elif", "taskname", "[", ":", "5", "]", "==", "'glue_'", ":", "\n", "        ", "task", "=", "taskname", ".", "split", "(", "'_'", ")", "[", "1", "]", ".", "lower", "(", ")", "\n", "Y_hat", "=", "np", ".", "array", "(", "Y_hat", ")", ".", "squeeze", "(", ")", "\n", "Y", "=", "np", ".", "array", "(", "Y", ")", "\n", "result", "=", "glue_compute_metrics", "(", "task", ",", "Y_hat", ",", "Y", ")", "\n", "result", "[", "'loss'", "]", "=", "loss", "\n", "print", "(", "result", ")", "\n", "return", "result", "\n", "", "else", ":", "\n", "        ", "num_classes", "=", "len", "(", "set", "(", "Y", ")", ")", "\n", "# Binary classification", "\n", "if", "num_classes", "<=", "2", ":", "\n", "            ", "accuracy", "=", "metrics", ".", "accuracy_score", "(", "Y", ",", "Y_hat", ")", "\n", "precision", "=", "metrics", ".", "precision_score", "(", "Y", ",", "Y_hat", ")", "\n", "recall", "=", "metrics", ".", "recall_score", "(", "Y", ",", "Y_hat", ")", "\n", "f1", "=", "metrics", ".", "f1_score", "(", "Y", ",", "Y_hat", ")", "\n", "if", "any", "(", "[", "prefix", "in", "taskname", "for", "prefix", "in", "[", "'cleaning_'", ",", "'Structured'", ",", "'Textual'", ",", "'Dirty'", "]", "]", ")", ":", "# handle imbalance:", "\n", "                ", "max_f1", "=", "f1", "\n", "if", "threshold", "is", "None", ":", "\n", "                    ", "for", "th", "in", "np", ".", "arange", "(", "0.9", ",", "1.0", ",", "0.005", ")", ":", "\n", "                        ", "Y_hat", "=", "[", "y", "if", "p", ">", "th", "else", "0", "for", "(", "y", ",", "p", ")", "in", "zip", "(", "Y_hat", ",", "Y_prob", ")", "]", "\n", "f1", "=", "metrics", ".", "f1_score", "(", "Y", ",", "Y_hat", ")", "\n", "if", "f1", ">", "max_f1", ":", "\n", "                            ", "max_f1", "=", "f1", "\n", "accuracy", "=", "metrics", ".", "accuracy_score", "(", "Y", ",", "Y_hat", ")", "\n", "precision", "=", "metrics", ".", "precision_score", "(", "Y", ",", "Y_hat", ")", "\n", "recall", "=", "metrics", ".", "recall_score", "(", "Y", ",", "Y_hat", ")", "\n", "threshold", "=", "th", "\n", "", "", "f1", "=", "max_f1", "\n", "", "else", ":", "\n", "                    ", "Y_hat", "=", "[", "y", "if", "p", ">", "threshold", "else", "0", "for", "(", "y", ",", "p", ")", "in", "zip", "(", "Y_hat", ",", "Y_prob", ")", "]", "\n", "accuracy", "=", "metrics", ".", "accuracy_score", "(", "Y", ",", "Y_hat", ")", "\n", "precision", "=", "metrics", ".", "precision_score", "(", "Y", ",", "Y_hat", ")", "\n", "recall", "=", "metrics", ".", "recall_score", "(", "Y", ",", "Y_hat", ")", "\n", "f1", "=", "metrics", ".", "f1_score", "(", "Y", ",", "Y_hat", ")", "\n", "\n", "", "", "print", "(", "\"accuracy=%.3f\"", "%", "accuracy", ")", "\n", "print", "(", "\"precision=%.3f\"", "%", "precision", ")", "\n", "print", "(", "\"recall=%.3f\"", "%", "recall", ")", "\n", "print", "(", "\"f1=%.3f\"", "%", "f1", ")", "\n", "print", "(", "\"======================================\"", ")", "\n", "if", "get_threshold", ":", "\n", "                ", "return", "accuracy", ",", "precision", ",", "recall", ",", "f1", ",", "loss", ",", "threshold", "\n", "", "else", ":", "\n", "                ", "return", "accuracy", ",", "precision", ",", "recall", ",", "f1", ",", "loss", "\n", "", "", "else", ":", "\n", "            ", "accuracy", "=", "metrics", ".", "accuracy_score", "(", "Y", ",", "Y_hat", ")", "\n", "f1", "=", "metrics", ".", "f1_score", "(", "Y", ",", "Y_hat", ",", "average", "=", "'macro'", ")", "\n", "precision", "=", "recall", "=", "accuracy", "# We might just not return anything", "\n", "print", "(", "\"accuracy=%.3f\"", "%", "accuracy", ")", "\n", "print", "(", "\"macro_f1=%.3f\"", "%", "f1", ")", "\n", "print", "(", "\"======================================\"", ")", "\n", "return", "accuracy", ",", "f1", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_on_task": [[190, 300], ["writer.add_scalars", "print", "train_util.eval_tagging", "print", "train_util.eval_tagging", "print", "train_util.eval_classifier", "print", "train_util.eval_classifier", "any", "print", "train_util.eval_classifier", "print", "train_util.eval_classifier", "print", "train_util.eval_classifier", "print", "train_util.eval_classifier", "print", "train_util.eval_classifier", "len"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_tagging", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_tagging", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_classifier", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_classifier", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_classifier", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_classifier", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_classifier", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_classifier", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_classifier"], ["", "", "", "def", "eval_on_task", "(", "epoch", ",", "\n", "model", ",", "\n", "task", ",", "\n", "valid_iter", ",", "\n", "valid_dataset", ",", "\n", "test_iter", ",", "\n", "test_dataset", ",", "\n", "writer", ",", "\n", "run_tag", ")", ":", "\n", "    ", "\"\"\"Run the eval function on the dev/test datasets and log the results.\n\n    Args:\n        epoch (int): the epoch number of the training process\n        model (MultiTaskNet): the model state\n        task (str): the task name to be evaluated\n        valid_iter (DataLoader): the dev set iterator\n        valid_dataset (Dataset): the dev dataset\n        test_iter (DataLoader): the test set iterator\n        test_dataset (Datset): the test dataset\n        writer (SummaryWriter): the logging writer for tensorboard\n        run_tag (str): the tag of the run\n\n    Returns:\n        float: dev F1\n        float: test F1\n    \"\"\"", "\n", "t_prec", "=", "t_recall", "=", "t_f1", "=", "t_loss", "=", "None", "\n", "if", "'tagging'", "in", "task", ":", "\n", "        ", "print", "(", "'Validation:'", ")", "\n", "prec", ",", "recall", ",", "f1", ",", "v_loss", "=", "eval_tagging", "(", "model", ",", "\n", "valid_iter", ",", "\n", "valid_dataset", ".", "idx2tag", ")", "\n", "if", "test_iter", "is", "not", "None", ":", "\n", "            ", "print", "(", "'Test:'", ")", "\n", "t_prec", ",", "t_recall", ",", "t_f1", ",", "t_loss", "=", "eval_tagging", "(", "model", ",", "\n", "test_iter", ",", "\n", "test_dataset", ".", "idx2tag", ")", "\n", "", "scalars", "=", "{", "'precision'", ":", "prec", ",", "\n", "'recall'", ":", "recall", ",", "\n", "'f1'", ":", "f1", ",", "\n", "'v_loss'", ":", "v_loss", ",", "\n", "'t_precision'", ":", "t_prec", ",", "\n", "'t_recall'", ":", "t_recall", ",", "\n", "'t_f1'", ":", "t_f1", ",", "\n", "'t_loss'", ":", "t_loss", "}", "\n", "", "elif", "task", "in", "glue_processors", ":", "\n", "        ", "print", "(", "'Validation:'", ")", "\n", "scalars", "=", "eval_classifier", "(", "model", ",", "valid_iter", ")", "\n", "f1", ",", "t_f1", "=", "0.0", ",", "0.0", "\n", "", "elif", "task", "[", ":", "5", "]", "==", "'glue_'", ":", "\n", "        ", "print", "(", "'Validation:'", ")", "\n", "scalars", "=", "eval_classifier", "(", "model", ",", "valid_iter", ")", "\n", "\n", "if", "test_iter", "is", "not", "None", ":", "\n", "            ", "print", "(", "'Test:'", ")", "\n", "t_output", "=", "eval_classifier", "(", "model", ",", "test_iter", ")", "\n", "for", "key", "in", "t_output", ":", "\n", "                ", "scalars", "[", "'t_'", "+", "key", "]", "=", "t_output", "[", "key", "]", "\n", "\n", "", "", "f1", ",", "t_f1", "=", "0.0", ",", "0.0", "\n", "", "elif", "any", "(", "[", "prefix", "in", "task", "for", "prefix", "in", "[", "'cleaning_'", ",", "'Structured'", ",", "'Textual'", ",", "'Dirty'", "]", "]", ")", ":", "# handle imbalance:", "\n", "        ", "print", "(", "'Validation:'", ")", "\n", "acc", ",", "prec", ",", "recall", ",", "f1", ",", "v_loss", ",", "th", "=", "eval_classifier", "(", "model", ",", "valid_iter", ",", "get_threshold", "=", "True", ")", "\n", "print", "(", "'Test:'", ")", "\n", "t_acc", ",", "t_prec", ",", "t_recall", ",", "t_f1", ",", "t_loss", "=", "eval_classifier", "(", "model", ",", "test_iter", ",", "threshold", "=", "th", ")", "\n", "scalars", "=", "{", "'acc'", ":", "acc", ",", "\n", "'precision'", ":", "prec", ",", "\n", "'recall'", ":", "recall", ",", "\n", "'f1'", ":", "f1", ",", "\n", "'v_loss'", ":", "v_loss", ",", "\n", "'t_acc'", ":", "t_acc", ",", "\n", "'t_precision'", ":", "t_prec", ",", "\n", "'t_recall'", ":", "t_recall", ",", "\n", "'t_f1'", ":", "t_f1", ",", "\n", "'t_loss'", ":", "t_loss", "}", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Validation:'", ")", "\n", "v_output", "=", "eval_classifier", "(", "model", ",", "valid_iter", ")", "\n", "\n", "if", "test_iter", "is", "not", "None", ":", "\n", "            ", "print", "(", "'Test:'", ")", "\n", "t_output", "=", "eval_classifier", "(", "model", ",", "test_iter", ")", "\n", "\n", "", "if", "len", "(", "v_output", ")", "==", "5", ":", "\n", "            ", "acc", ",", "prec", ",", "recall", ",", "f1", ",", "v_loss", "=", "v_output", "\n", "t_acc", ",", "t_prec", ",", "t_recall", ",", "t_f1", ",", "t_loss", "=", "t_output", "\n", "scalars", "=", "{", "'acc'", ":", "acc", ",", "\n", "'precision'", ":", "prec", ",", "\n", "'recall'", ":", "recall", ",", "\n", "'f1'", ":", "f1", ",", "\n", "'v_loss'", ":", "v_loss", ",", "\n", "'t_acc'", ":", "t_acc", ",", "\n", "'t_precision'", ":", "t_prec", ",", "\n", "'t_recall'", ":", "t_recall", ",", "\n", "'t_f1'", ":", "t_f1", ",", "\n", "'t_loss'", ":", "t_loss", "}", "\n", "", "else", ":", "\n", "            ", "acc", ",", "f1", ",", "v_loss", "=", "v_output", "\n", "t_acc", ",", "t_f1", ",", "t_loss", "=", "t_output", "\n", "scalars", "=", "{", "'acc'", ":", "acc", ",", "\n", "'f1'", ":", "f1", ",", "\n", "'v_loss'", ":", "v_loss", ",", "\n", "'t_acc'", ":", "t_acc", ",", "\n", "'t_f1'", ":", "t_f1", ",", "\n", "'t_loss'", ":", "t_loss", "}", "\n", "\n", "# logging", "\n", "", "", "writer", ".", "add_scalars", "(", "run_tag", ",", "scalars", ",", "epoch", ")", "\n", "return", "f1", ",", "t_f1", "\n", "", ""]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.split_tag": [[33, 43], ["chunk_tag.split"], "function", ["None"], ["def", "split_tag", "(", "chunk_tag", ")", ":", "\n", "    ", "\"\"\"\n    split chunk tag into IOBES prefix and chunk_type\n    e.g.\n    B-PER -> (B, PER)\n    O -> (O, None)\n    \"\"\"", "\n", "if", "chunk_tag", "==", "'O'", ":", "\n", "        ", "return", "(", "'O'", ",", "None", ")", "\n", "", "return", "chunk_tag", ".", "split", "(", "'-'", ",", "maxsplit", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.is_chunk_end": [[44, 66], ["conlleval.split_tag", "conlleval.split_tag"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.split_tag", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.split_tag"], ["", "def", "is_chunk_end", "(", "prev_tag", ",", "tag", ")", ":", "\n", "    ", "\"\"\"\n    check if the previous chunk ended between the previous and current word\n    e.g.\n    (B-PER, I-PER) -> False\n    (B-LOC, O)  -> True\n\n    Note: in case of contradicting tags, e.g. (B-PER, I-LOC)\n    this is considered as (B-PER, B-LOC)\n    \"\"\"", "\n", "prefix1", ",", "chunk_type1", "=", "split_tag", "(", "prev_tag", ")", "\n", "prefix2", ",", "chunk_type2", "=", "split_tag", "(", "tag", ")", "\n", "\n", "if", "prefix1", "==", "'O'", ":", "\n", "        ", "return", "False", "\n", "", "if", "prefix2", "==", "'O'", ":", "\n", "        ", "return", "prefix1", "!=", "'O'", "\n", "\n", "", "if", "chunk_type1", "!=", "chunk_type2", ":", "\n", "        ", "return", "True", "\n", "\n", "", "return", "prefix2", "in", "[", "'B'", ",", "'S'", "]", "or", "prefix1", "in", "[", "'E'", ",", "'S'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.is_chunk_start": [[67, 83], ["conlleval.split_tag", "conlleval.split_tag"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.split_tag", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.split_tag"], ["", "def", "is_chunk_start", "(", "prev_tag", ",", "tag", ")", ":", "\n", "    ", "\"\"\"\n    check if a new chunk started between the previous and current word\n    \"\"\"", "\n", "prefix1", ",", "chunk_type1", "=", "split_tag", "(", "prev_tag", ")", "\n", "prefix2", ",", "chunk_type2", "=", "split_tag", "(", "tag", ")", "\n", "\n", "if", "prefix2", "==", "'O'", ":", "\n", "        ", "return", "False", "\n", "", "if", "prefix1", "==", "'O'", ":", "\n", "        ", "return", "prefix2", "!=", "'O'", "\n", "\n", "", "if", "chunk_type1", "!=", "chunk_type2", ":", "\n", "        ", "return", "True", "\n", "\n", "", "return", "prefix2", "in", "[", "'B'", ",", "'S'", "]", "or", "prefix1", "in", "[", "'E'", ",", "'S'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.calc_metrics": [[85, 97], ["None"], "function", ["None"], ["", "def", "calc_metrics", "(", "tp", ",", "p", ",", "t", ",", "percent", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    compute overall precision, recall and FB1 (default values are 0.0)\n    if percent is True, return 100 * original decimal value\n    \"\"\"", "\n", "precision", "=", "tp", "/", "p", "if", "p", "else", "0", "\n", "recall", "=", "tp", "/", "t", "if", "t", "else", "0", "\n", "fb1", "=", "2", "*", "precision", "*", "recall", "/", "(", "precision", "+", "recall", ")", "if", "precision", "+", "recall", "else", "0", "\n", "if", "percent", ":", "\n", "        ", "return", "100", "*", "precision", ",", "100", "*", "recall", ",", "100", "*", "fb1", "\n", "", "else", ":", "\n", "        ", "return", "precision", ",", "recall", ",", "fb1", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.count_chunks": [[99, 159], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "zip", "conlleval.split_tag", "conlleval.split_tag", "conlleval.is_chunk_start", "conlleval.is_chunk_start", "conlleval.is_chunk_end", "conlleval.is_chunk_end"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.split_tag", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.split_tag", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.is_chunk_start", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.is_chunk_start", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.is_chunk_end", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.is_chunk_end"], ["", "", "def", "count_chunks", "(", "true_seqs", ",", "pred_seqs", ")", ":", "\n", "    ", "\"\"\"\n    true_seqs: a list of true tags\n    pred_seqs: a list of predicted tags\n\n    return:\n    correct_chunks: a dict (counter),\n                    key = chunk types,\n                    value = number of correctly identified chunks per type\n    true_chunks:    a dict, number of true chunks per type\n    pred_chunks:    a dict, number of identified chunks per type\n\n    correct_counts, true_counts, pred_counts: similar to above, but for tags\n    \"\"\"", "\n", "correct_chunks", "=", "defaultdict", "(", "int", ")", "\n", "true_chunks", "=", "defaultdict", "(", "int", ")", "\n", "pred_chunks", "=", "defaultdict", "(", "int", ")", "\n", "\n", "correct_counts", "=", "defaultdict", "(", "int", ")", "\n", "true_counts", "=", "defaultdict", "(", "int", ")", "\n", "pred_counts", "=", "defaultdict", "(", "int", ")", "\n", "\n", "prev_true_tag", ",", "prev_pred_tag", "=", "'O'", ",", "'O'", "\n", "correct_chunk", "=", "None", "\n", "\n", "for", "true_tag", ",", "pred_tag", "in", "zip", "(", "true_seqs", ",", "pred_seqs", ")", ":", "\n", "        ", "if", "true_tag", "==", "pred_tag", ":", "\n", "            ", "correct_counts", "[", "true_tag", "]", "+=", "1", "\n", "", "true_counts", "[", "true_tag", "]", "+=", "1", "\n", "pred_counts", "[", "pred_tag", "]", "+=", "1", "\n", "\n", "_", ",", "true_type", "=", "split_tag", "(", "true_tag", ")", "\n", "_", ",", "pred_type", "=", "split_tag", "(", "pred_tag", ")", "\n", "\n", "if", "correct_chunk", "is", "not", "None", ":", "\n", "            ", "true_end", "=", "is_chunk_end", "(", "prev_true_tag", ",", "true_tag", ")", "\n", "pred_end", "=", "is_chunk_end", "(", "prev_pred_tag", ",", "pred_tag", ")", "\n", "\n", "if", "pred_end", "and", "true_end", ":", "\n", "                ", "correct_chunks", "[", "correct_chunk", "]", "+=", "1", "\n", "correct_chunk", "=", "None", "\n", "", "elif", "pred_end", "!=", "true_end", "or", "true_type", "!=", "pred_type", ":", "\n", "                ", "correct_chunk", "=", "None", "\n", "\n", "", "", "true_start", "=", "is_chunk_start", "(", "prev_true_tag", ",", "true_tag", ")", "\n", "pred_start", "=", "is_chunk_start", "(", "prev_pred_tag", ",", "pred_tag", ")", "\n", "\n", "if", "true_start", "and", "pred_start", "and", "true_type", "==", "pred_type", ":", "\n", "            ", "correct_chunk", "=", "true_type", "\n", "", "if", "true_start", ":", "\n", "            ", "true_chunks", "[", "true_type", "]", "+=", "1", "\n", "", "if", "pred_start", ":", "\n", "            ", "pred_chunks", "[", "pred_type", "]", "+=", "1", "\n", "\n", "", "prev_true_tag", ",", "prev_pred_tag", "=", "true_tag", ",", "pred_tag", "\n", "", "if", "correct_chunk", "is", "not", "None", ":", "\n", "        ", "correct_chunks", "[", "correct_chunk", "]", "+=", "1", "\n", "\n", "", "return", "(", "correct_chunks", ",", "true_chunks", ",", "pred_chunks", ",", "\n", "correct_counts", ",", "true_counts", ",", "pred_counts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.get_result": [[160, 203], ["sum", "sum", "sum", "sum", "sum", "sum", "sum", "sorted", "conlleval.calc_metrics", "print", "print", "print", "print", "print", "correct_chunks.values", "true_chunks.values", "pred_chunks.values", "correct_counts.values", "true_counts.values", "list", "conlleval.calc_metrics", "print", "print", "print", "set", "correct_counts.items", "true_counts.items", "list", "list"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.calc_metrics", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.calc_metrics"], ["", "def", "get_result", "(", "correct_chunks", ",", "true_chunks", ",", "pred_chunks", ",", "\n", "correct_counts", ",", "true_counts", ",", "pred_counts", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    if verbose, print overall performance, as well as preformance per chunk type;\n    otherwise, simply return overall prec, rec, f1 scores\n    \"\"\"", "\n", "# sum counts", "\n", "sum_correct_chunks", "=", "sum", "(", "correct_chunks", ".", "values", "(", ")", ")", "\n", "sum_true_chunks", "=", "sum", "(", "true_chunks", ".", "values", "(", ")", ")", "\n", "sum_pred_chunks", "=", "sum", "(", "pred_chunks", ".", "values", "(", ")", ")", "\n", "\n", "sum_correct_counts", "=", "sum", "(", "correct_counts", ".", "values", "(", ")", ")", "\n", "sum_true_counts", "=", "sum", "(", "true_counts", ".", "values", "(", ")", ")", "\n", "\n", "nonO_correct_counts", "=", "sum", "(", "v", "for", "k", ",", "v", "in", "correct_counts", ".", "items", "(", ")", "if", "k", "!=", "'O'", ")", "\n", "nonO_true_counts", "=", "sum", "(", "v", "for", "k", ",", "v", "in", "true_counts", ".", "items", "(", ")", "if", "k", "!=", "'O'", ")", "\n", "\n", "chunk_types", "=", "sorted", "(", "list", "(", "set", "(", "list", "(", "true_chunks", ")", "+", "list", "(", "pred_chunks", ")", ")", ")", ")", "\n", "\n", "# compute overall precision, recall and FB1 (default values are 0.0)", "\n", "prec", ",", "rec", ",", "f1", "=", "calc_metrics", "(", "sum_correct_chunks", ",", "sum_pred_chunks", ",", "sum_true_chunks", ")", "\n", "res", "=", "(", "prec", ",", "rec", ",", "f1", ")", "\n", "if", "not", "verbose", ":", "\n", "        ", "return", "res", "\n", "\n", "# print overall performance, and performance per chunk type", "\n", "\n", "", "print", "(", "\"processed %i tokens with %i phrases; \"", "%", "(", "sum_true_counts", ",", "sum_true_chunks", ")", ",", "end", "=", "''", ")", "\n", "print", "(", "\"found: %i phrases; correct: %i.\\n\"", "%", "(", "sum_pred_chunks", ",", "sum_correct_chunks", ")", ",", "end", "=", "''", ")", "\n", "\n", "print", "(", "\"accuracy: %6.2f%%; (non-O)\"", "%", "(", "100", "*", "nonO_correct_counts", "/", "nonO_true_counts", ")", ")", "\n", "print", "(", "\"accuracy: %6.2f%%; \"", "%", "(", "100", "*", "sum_correct_counts", "/", "sum_true_counts", ")", ",", "end", "=", "''", ")", "\n", "print", "(", "\"precision: %6.2f%%; recall: %6.2f%%; FB1: %6.2f\"", "%", "(", "prec", ",", "rec", ",", "f1", ")", ")", "\n", "\n", "# for each chunk type, compute precision, recall and FB1 (default values are 0.0)", "\n", "for", "t", "in", "chunk_types", ":", "\n", "        ", "prec", ",", "rec", ",", "f1", "=", "calc_metrics", "(", "correct_chunks", "[", "t", "]", ",", "pred_chunks", "[", "t", "]", ",", "true_chunks", "[", "t", "]", ")", "\n", "print", "(", "\"%17s: \"", "%", "t", ",", "end", "=", "''", ")", "\n", "print", "(", "\"precision: %6.2f%%; recall: %6.2f%%; FB1: %6.2f\"", "%", "\n", "(", "prec", ",", "rec", ",", "f1", ")", ",", "end", "=", "''", ")", "\n", "print", "(", "\"  %d\"", "%", "pred_chunks", "[", "t", "]", ")", "\n", "\n", "", "return", "res", "\n", "# you can generate LaTeX output for tables like in", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.evaluate": [[207, 213], ["conlleval.count_chunks", "conlleval.get_result"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.count_chunks", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.get_result"], ["", "def", "evaluate", "(", "true_seqs", ",", "pred_seqs", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "(", "correct_chunks", ",", "true_chunks", ",", "pred_chunks", ",", "\n", "correct_counts", ",", "true_counts", ",", "pred_counts", ")", "=", "count_chunks", "(", "true_seqs", ",", "pred_seqs", ")", "\n", "result", "=", "get_result", "(", "correct_chunks", ",", "true_chunks", ",", "pred_chunks", ",", "\n", "correct_counts", ",", "true_counts", ",", "pred_counts", ",", "verbose", "=", "verbose", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.evaluate_conll_file": [[214, 230], ["conlleval.evaluate", "line.strip().split", "true_seqs.append", "pred_seqs.append", "line.strip", "len", "IOError", "true_seqs.append", "pred_seqs.append"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.conlleval.evaluate"], ["", "def", "evaluate_conll_file", "(", "fileIterator", ")", ":", "\n", "    ", "true_seqs", ",", "pred_seqs", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "line", "in", "fileIterator", ":", "\n", "        ", "cols", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "# each non-empty line must contain >= 3 columns", "\n", "if", "not", "cols", ":", "\n", "            ", "true_seqs", ".", "append", "(", "'O'", ")", "\n", "pred_seqs", ".", "append", "(", "'O'", ")", "\n", "", "elif", "len", "(", "cols", ")", "<", "3", ":", "\n", "            ", "raise", "IOError", "(", "\"conlleval: too few columns in line %s\\n\"", "%", "line", ")", "\n", "", "else", ":", "\n", "# extract tags from last 2 columns", "\n", "            ", "true_seqs", ".", "append", "(", "cols", "[", "-", "2", "]", ")", "\n", "pred_seqs", ".", "append", "(", "cols", "[", "-", "1", "]", ")", "\n", "", "", "return", "evaluate", "(", "true_seqs", ",", "pred_seqs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.__init__": [[45, 117], ["dataset.get_tokenizer", "type", "zip", "dataset.SnippextDataset.load_t5_examples", "dataset.SnippextDataset.read_tagging_file", "dataset.SnippextDataset.read_classification_file", "dataset.SnippextDataset.vocab.append", "dataset.SnippextDataset.vocab.insert", "enumerate", "enumerate", "augment.Augmenter", "sents.append", "tags_li.append", "sents.append", "tags_li.append", "enumerate"], "methods", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.get_tokenizer", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.load_t5_examples", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.read_tagging_file", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.read_classification_file"], ["    ", "def", "__init__", "(", "self", ",", "\n", "source", ",", "\n", "vocab", ",", "\n", "taskname", ",", "\n", "max_len", "=", "512", ",", "\n", "lm", "=", "'bert'", ",", "\n", "augment_index", "=", "None", ",", "\n", "augment_op", "=", "None", ",", "\n", "size", "=", "None", ")", ":", "\n", "        ", "\"\"\" TODO\n        Args:\n\n        \"\"\"", "\n", "# tokens and tags", "\n", "sents", ",", "tags_li", "=", "[", "]", ",", "[", "]", "# list of lists", "\n", "self", ".", "max_len", "=", "max_len", "\n", "get_tokenizer", "(", "lm", ")", "\n", "\n", "if", "type", "(", "source", ")", "is", "str", ":", "\n", "# read from file (for training/evaluation)", "\n", "            ", "if", "'_tagging'", "in", "taskname", "or", "'_qa'", "in", "taskname", ":", "\n", "                ", "sents", ",", "tags_li", "=", "self", ".", "read_tagging_file", "(", "source", ")", "\n", "", "else", ":", "\n", "                ", "sents", ",", "tags_li", "=", "self", ".", "read_classification_file", "(", "source", ")", "\n", "", "if", "size", "is", "not", "None", ":", "\n", "                ", "sents", ",", "tags_li", "=", "sents", "[", ":", "size", "]", ",", "tags_li", "[", ":", "size", "]", "\n", "", "", "else", ":", "\n", "# read from list of tokens (for prediction)", "\n", "            ", "if", "'_tagging'", "in", "taskname", "or", "'_qa'", "in", "taskname", ":", "\n", "                ", "for", "tokens", "in", "source", ":", "\n", "                    ", "sents", ".", "append", "(", "[", "\"[CLS]\"", "]", "+", "[", "token", "for", "token", "in", "tokens", "]", "+", "[", "\"[SEP]\"", "]", ")", "\n", "tags_li", ".", "append", "(", "[", "\"<PAD>\"", "]", "+", "[", "'O'", "for", "token", "in", "tokens", "]", "+", "[", "\"<PAD>\"", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "for", "sent", "in", "source", ":", "\n", "                    ", "sents", ".", "append", "(", "sent", ")", "\n", "tags_li", ".", "append", "(", "vocab", "[", "0", "]", ")", "\n", "\n", "# handling QA datasets. Mark the question tokens with <PAD> so that", "\n", "# the model does not predict those tokens.", "\n", "", "", "", "if", "'_qa'", "in", "taskname", ":", "\n", "            ", "for", "tokens", ",", "labels", "in", "zip", "(", "sents", ",", "tags_li", ")", ":", "\n", "                ", "if", "\"[SEP]\"", "in", "tokens", "[", ":", "-", "1", "]", ":", "\n", "                    ", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "                        ", "labels", "[", "i", "]", "=", "\"<PAD>\"", "\n", "if", "token", "==", "\"[SEP]\"", ":", "\n", "                            ", "break", "\n", "\n", "# assign class variables", "\n", "", "", "", "", "", "self", ".", "sents", ",", "self", ".", "tags_li", "=", "sents", ",", "tags_li", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n", "# add special tags for tagging", "\n", "if", "'_tagging'", "in", "taskname", ":", "\n", "            ", "if", "'O'", "not", "in", "self", ".", "vocab", ":", "\n", "                ", "self", ".", "vocab", ".", "append", "(", "'O'", ")", "\n", "", "if", "self", ".", "vocab", "[", "0", "]", "!=", "'<PAD>'", ":", "\n", "                ", "self", ".", "vocab", ".", "insert", "(", "0", ",", "'<PAD>'", ")", "\n", "\n", "# index for tags/labels", "\n", "", "", "self", ".", "tag2idx", "=", "{", "tag", ":", "idx", "for", "idx", ",", "tag", "in", "enumerate", "(", "self", ".", "vocab", ")", "}", "\n", "self", ".", "idx2tag", "=", "{", "idx", ":", "tag", "for", "idx", ",", "tag", "in", "enumerate", "(", "self", ".", "vocab", ")", "}", "\n", "self", ".", "taskname", "=", "taskname", "\n", "\n", "# augmentation index and op", "\n", "self", ".", "augment_op", "=", "augment_op", "\n", "if", "augment_op", "==", "'t5'", ":", "\n", "            ", "self", ".", "load_t5_examples", "(", "source", ")", "\n", "", "elif", "augment_index", "!=", "None", ":", "\n", "            ", "self", ".", "augmenter", "=", "Augmenter", "(", "augment_index", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "augmenter", "=", "None", "\n", "self", ".", "augment_op", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.load_t5_examples": [[119, 140], ["jsonlines.open", "jsonlines.open", "dataset.SnippextDataset.augmented_examples.append", "dataset.SnippextDataset.augmented_examples.append", "dataset.SnippextDataset.read_tagging_file", "exms.append", "exms.append", "entry.split"], "methods", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.read_tagging_file"], ["", "", "def", "load_t5_examples", "(", "self", ",", "source", ")", ":", "\n", "        ", "self", ".", "augmenter", "=", "None", "\n", "# read augmented examples", "\n", "self", ".", "augmented_examples", "=", "[", "]", "\n", "if", "'_tagging'", "in", "self", ".", "taskname", ":", "\n", "            ", "with", "jsonlines", ".", "open", "(", "source", "+", "'.augment.jsonl'", ",", "mode", "=", "'r'", ")", "as", "reader", ":", "\n", "                ", "for", "row", "in", "reader", ":", "\n", "                    ", "exms", "=", "[", "]", "\n", "for", "entry", "in", "row", "[", "'augment'", "]", ":", "\n", "                        ", "tokens", ",", "labels", "=", "self", ".", "read_tagging_file", "(", "entry", ",", "is_file", "=", "False", ")", "\n", "exms", ".", "append", "(", "(", "tokens", "[", "0", "]", ",", "labels", "[", "0", "]", ")", ")", "\n", "", "self", ".", "augmented_examples", ".", "append", "(", "exms", ")", "\n", "", "", "", "else", ":", "\n", "            ", "with", "jsonlines", ".", "open", "(", "source", "+", "'.augment.jsonl'", ",", "mode", "=", "'r'", ")", "as", "reader", ":", "\n", "                ", "for", "row", "in", "reader", ":", "\n", "                    ", "exms", "=", "[", "]", "\n", "label", "=", "row", "[", "'label'", "]", "\n", "for", "entry", "in", "row", "[", "'augment'", "]", ":", "\n", "                        ", "sent", "=", "' [SEP] '", ".", "join", "(", "entry", ".", "split", "(", "'\\t'", ")", ")", "\n", "exms", ".", "append", "(", "(", "sent", ",", "label", ")", ")", "\n", "", "self", ".", "augmented_examples", ".", "append", "(", "exms", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.read_tagging_file": [[142, 177], ["open().read().strip().split", "path.strip", "sents.append", "tags_li.append", "open().read().strip", "print", "line.split", "entry.splitlines", "line.split", "entry.splitlines", "open().read", "open"], "methods", ["None"], ["", "", "", "", "def", "read_tagging_file", "(", "self", ",", "path", ",", "is_file", "=", "True", ")", ":", "\n", "        ", "\"\"\"Read a train/eval tagging dataset from file\n\n        The input file should contain multiple entries separated by empty lines.\n        The format of each entry:\n\n        The O\n        room B-AS\n        is O\n        very B-OP\n        clean I-OP\n        . O\n\n        Args:\n            path (str): the path to the dataset file\n\n        Returns:\n            list of list of str: the tokens\n            list of list of str: the labels\n        \"\"\"", "\n", "sents", ",", "tags_li", "=", "[", "]", ",", "[", "]", "\n", "if", "is_file", ":", "\n", "            ", "entries", "=", "open", "(", "path", ",", "'r'", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\\n\"", ")", "\n", "", "else", ":", "\n", "            ", "entries", "=", "[", "path", ".", "strip", "(", ")", "]", "\n", "\n", "", "for", "entry", "in", "entries", ":", "\n", "            ", "try", ":", "\n", "                ", "words", "=", "[", "line", ".", "split", "(", ")", "[", "0", "]", "for", "line", "in", "entry", ".", "splitlines", "(", ")", "]", "\n", "tags", "=", "[", "line", ".", "split", "(", ")", "[", "-", "1", "]", "for", "line", "in", "entry", ".", "splitlines", "(", ")", "]", "\n", "sents", ".", "append", "(", "[", "\"[CLS]\"", "]", "+", "words", "[", ":", "self", ".", "max_len", "]", "+", "[", "\"[SEP]\"", "]", ")", "\n", "tags_li", ".", "append", "(", "[", "\"<PAD>\"", "]", "+", "tags", "[", ":", "self", ".", "max_len", "]", "+", "[", "\"<PAD>\"", "]", ")", "\n", "", "except", ":", "\n", "                ", "print", "(", "'error @'", ",", "entry", ")", "\n", "", "", "return", "sents", ",", "tags_li", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.read_classification_file": [[179, 210], ["open().readlines", "line.strip().split", "open", "line.strip", "len", "len", "len", "sents.append", "labels.append", "sents.append", "labels.append", "print", "line.strip"], "methods", ["None"], ["", "def", "read_classification_file", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"Read a train/eval classification dataset from file\n\n        The input file should contain multiple lines where each line is an example.\n        The format of each line:\n        The room is clean.\\troom\\tpositive\n\n        Args:\n            path (str): the path to the dataset file\n\n        Returns:\n            list of str: the input sequences\n            list of str: the labels\n        \"\"\"", "\n", "sents", ",", "labels", "=", "[", "]", ",", "[", "]", "\n", "lines", "=", "open", "(", "path", ")", ".", "readlines", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "items", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "# only consider sentence and sentence pairs", "\n", "if", "len", "(", "items", ")", "<", "2", "or", "len", "(", "items", ")", ">", "3", ":", "\n", "                ", "continue", "\n", "", "try", ":", "\n", "                ", "if", "len", "(", "items", ")", "==", "2", ":", "\n", "                    ", "sents", ".", "append", "(", "items", "[", "0", "]", ")", "\n", "labels", ".", "append", "(", "items", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                    ", "sents", ".", "append", "(", "items", "[", "0", "]", "+", "' [SEP] '", "+", "items", "[", "1", "]", ")", "\n", "labels", ".", "append", "(", "items", "[", "2", "]", ")", "\n", "", "", "except", ":", "\n", "                ", "print", "(", "'error @'", ",", "line", ".", "strip", "(", ")", ")", "\n", "", "", "return", "sents", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.__len__": [[212, 215], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the length of the dataset\"\"\"", "\n", "return", "len", "(", "self", ".", "sents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.get": [[216, 222], ["dataset.SnippextDataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.__getitem__"], ["", "def", "get", "(", "self", ",", "idx", ",", "op", "=", "[", "]", ")", ":", "\n", "        ", "ag", "=", "self", ".", "augmenter", "\n", "self", ".", "augmenter", "=", "None", "\n", "item", "=", "self", ".", "__getitem__", "(", "idx", ")", "\n", "self", ".", "augmenter", "=", "ag", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.__getitem__": [[223, 308], ["zip", "len", "enumerate", "tokenizer.encode", "len", "tokenizer.convert_tokens_to_ids", "tokenizer.encode.extend", "is_heads.extend", "y.extend", "len", "len", "len", "dataset.SnippextDataset.split", "len", "len", "len", "len", "len", "len", "random.choice", "dataset.SnippextDataset.augmenter.augment", "tokenizer.tokenize", "len", "len", "len", "len", "len", "len", "random.choice", "dataset.SnippextDataset.augmenter.augment_sent", "len", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.augment", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.augment.Augmenter.augment_sent"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Return the ith item of in the dataset.\n\n        Args:\n            idx (int): the element index\n        Returns (TODO):\n            words, x, is_heads, tags, mask, y, seqlen, self.taskname\n        \"\"\"", "\n", "words", ",", "tags", "=", "self", ".", "sents", "[", "idx", "]", ",", "self", ".", "tags_li", "[", "idx", "]", "\n", "\n", "if", "'_tagging'", "in", "self", ".", "taskname", ":", "\n", "# apply data augmentation if specified", "\n", "            ", "if", "self", ".", "augment_op", "==", "'t5'", ":", "\n", "                ", "if", "len", "(", "self", ".", "augmented_examples", "[", "idx", "]", ")", ">", "0", ":", "\n", "                    ", "words", ",", "tags", "=", "random", ".", "choice", "(", "self", ".", "augmented_examples", "[", "idx", "]", ")", "\n", "", "", "elif", "self", ".", "augmenter", "!=", "None", ":", "\n", "                ", "words", ",", "tags", "=", "self", ".", "augmenter", ".", "augment", "(", "words", ",", "tags", ",", "self", ".", "augment_op", ")", "\n", "\n", "# We give credits only to the first piece.", "\n", "", "x", ",", "y", "=", "[", "]", ",", "[", "]", "# list of ids", "\n", "is_heads", "=", "[", "]", "# list. 1: the token is the first piece of a word", "\n", "\n", "for", "w", ",", "t", "in", "zip", "(", "words", ",", "tags", ")", ":", "\n", "# avoid bad tokens", "\n", "                ", "w", "=", "w", "[", ":", "50", "]", "\n", "tokens", "=", "tokenizer", ".", "tokenize", "(", "w", ")", "if", "w", "not", "in", "(", "\"[CLS]\"", ",", "\"[SEP]\"", ")", "else", "[", "w", "]", "\n", "xx", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "if", "len", "(", "xx", ")", "==", "0", ":", "\n", "                    ", "continue", "\n", "\n", "", "is_head", "=", "[", "1", "]", "+", "[", "0", "]", "*", "(", "len", "(", "tokens", ")", "-", "1", ")", "\n", "\n", "t", "=", "[", "t", "]", "+", "[", "\"<PAD>\"", "]", "*", "(", "len", "(", "tokens", ")", "-", "1", ")", "# <PAD>: no decision", "\n", "yy", "=", "[", "self", ".", "tag2idx", "[", "each", "]", "for", "each", "in", "t", "]", "# (T,)", "\n", "\n", "x", ".", "extend", "(", "xx", ")", "\n", "is_heads", ".", "extend", "(", "is_head", ")", "\n", "y", ".", "extend", "(", "yy", ")", "\n", "# make sure that the length of x is not too large", "\n", "if", "len", "(", "x", ")", ">", "self", ".", "max_len", ":", "\n", "                    ", "break", "\n", "\n", "", "", "assert", "len", "(", "x", ")", "==", "len", "(", "y", ")", "==", "len", "(", "is_heads", ")", ",", "f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}, {' '.join(tokens)}\"", "\n", "\n", "# seqlen", "\n", "seqlen", "=", "len", "(", "y", ")", "\n", "\n", "mask", "=", "[", "1", "]", "*", "seqlen", "\n", "# masking for QA", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "tags", ")", ":", "\n", "                ", "if", "t", "!=", "'<PAD>'", ":", "\n", "                    ", "break", "\n", "", "mask", "[", "i", "]", "=", "0", "\n", "\n", "# to string", "\n", "", "words", "=", "\" \"", ".", "join", "(", "words", ")", "\n", "tags", "=", "\" \"", ".", "join", "(", "tags", ")", "\n", "", "else", ":", "# classification", "\n", "            ", "if", "self", ".", "augment_op", "==", "'t5'", ":", "\n", "              ", "if", "len", "(", "self", ".", "augmented_examples", "[", "idx", "]", ")", ">", "0", ":", "\n", "                  ", "words", ",", "tags", "=", "random", ".", "choice", "(", "self", ".", "augmented_examples", "[", "idx", "]", ")", "\n", "", "", "elif", "self", ".", "augmenter", "!=", "None", ":", "\n", "                ", "words", "=", "self", ".", "augmenter", ".", "augment_sent", "(", "words", ",", "self", ".", "augment_op", ")", "\n", "\n", "", "if", "' [SEP] '", "in", "words", ":", "\n", "                ", "sent_a", ",", "sent_b", "=", "words", ".", "split", "(", "' [SEP] '", ")", "\n", "", "else", ":", "\n", "                ", "sent_a", ",", "sent_b", "=", "words", ",", "None", "\n", "\n", "", "x", "=", "tokenizer", ".", "encode", "(", "sent_a", ",", "text_pair", "=", "sent_b", ",", "\n", "truncation", "=", "\"longest_first\"", ",", "\n", "max_length", "=", "self", ".", "max_len", ",", "\n", "add_special_tokens", "=", "True", ")", "\n", "\n", "y", "=", "self", ".", "tag2idx", "[", "tags", "]", "# label", "\n", "is_heads", "=", "[", "1", "]", "*", "len", "(", "x", ")", "\n", "mask", "=", "[", "1", "]", "*", "len", "(", "x", ")", "\n", "\n", "assert", "len", "(", "x", ")", "==", "len", "(", "mask", ")", "==", "len", "(", "is_heads", ")", ",", "f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}\"", "\n", "# seqlen", "\n", "seqlen", "=", "len", "(", "mask", ")", "\n", "\n", "", "return", "words", ",", "x", ",", "is_heads", ",", "tags", ",", "mask", ",", "y", ",", "seqlen", ",", "self", ".", "taskname", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.SnippextDataset.pad": [[309, 347], ["f", "numpy.array().max", "f", "f", "g", "f", "f", "g", "isinstance", "g", "f", "torch.Tensor", "torch.LongTensor", "f", "f", "numpy.array", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "pad", "(", "batch", ")", ":", "\n", "        ", "'''Pads to the longest sample\n\n        Args:\n            batch:\n\n        Returns (TODO):\n            return words, f(x), is_heads, tags, f(mask), f(y), seqlens, name\n        '''", "\n", "f", "=", "lambda", "x", ":", "[", "sample", "[", "x", "]", "for", "sample", "in", "batch", "]", "\n", "g", "=", "lambda", "x", ",", "seqlen", ",", "val", ":", "[", "sample", "[", "x", "]", "+", "[", "val", "]", "*", "(", "seqlen", "-", "len", "(", "sample", "[", "x", "]", ")", ")", "for", "sample", "in", "batch", "]", "# 0: <pad>", "\n", "\n", "# get maximal sequence length", "\n", "seqlens", "=", "f", "(", "6", ")", "\n", "maxlen", "=", "np", ".", "array", "(", "seqlens", ")", ".", "max", "(", ")", "\n", "\n", "# get task name", "\n", "name", "=", "f", "(", "7", ")", "\n", "\n", "words", "=", "f", "(", "0", ")", "\n", "x", "=", "g", "(", "1", ",", "maxlen", ",", "0", ")", "\n", "is_heads", "=", "f", "(", "2", ")", "\n", "tags", "=", "f", "(", "3", ")", "\n", "mask", "=", "g", "(", "4", ",", "maxlen", ",", "1", ")", "\n", "if", "'_tagging'", "in", "name", "[", "0", "]", ":", "\n", "            ", "y", "=", "g", "(", "5", ",", "maxlen", ",", "0", ")", "\n", "", "else", ":", "\n", "            ", "y", "=", "f", "(", "5", ")", "\n", "\n", "", "f", "=", "torch", ".", "LongTensor", "\n", "if", "isinstance", "(", "y", "[", "0", "]", ",", "float", ")", ":", "\n", "            ", "y", "=", "torch", ".", "Tensor", "(", "y", ")", "\n", "", "else", ":", "\n", "            ", "y", "=", "torch", ".", "LongTensor", "(", "y", ")", "\n", "", "return", "words", ",", "f", "(", "x", ")", ",", "is_heads", ",", "tags", ",", "f", "(", "mask", ")", ",", "y", ",", "seqlens", ",", "name", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.get_tokenizer": [[11, 42], ["BertTokenizer.from_pretrained", "DistilBertTokenizer.from_pretrained", "AlbertTokenizer.from_pretrained", "RobertaTokenizer.from_pretrained", "XLNetTokenizer.from_pretrained", "LongformerTokenizer.from_pretrained"], "function", ["None"], ["def", "get_tokenizer", "(", "lm", "=", "'bert'", ")", ":", "\n", "    ", "\"\"\"Return the tokenizer. Intiailize it if not initialized.\n\n    Args:\n        lm (string, optional): the name of the language model\n            (bert, albert, roberta, distilbert, etc.)\n\n    Returns:\n        Tokenizer: the tokenizer to be used\n    \"\"\"", "\n", "global", "tokenizer", "\n", "if", "tokenizer", "is", "None", ":", "\n", "        ", "if", "lm", "==", "'bert'", ":", "\n", "            ", "from", "transformers", "import", "BertTokenizer", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "", "elif", "lm", "==", "'distilbert'", ":", "\n", "            ", "from", "transformers", "import", "DistilBertTokenizer", "\n", "tokenizer", "=", "DistilBertTokenizer", ".", "from_pretrained", "(", "'distilbert-base-uncased'", ")", "\n", "", "elif", "lm", "==", "'albert'", ":", "\n", "            ", "from", "transformers", "import", "AlbertTokenizer", "\n", "tokenizer", "=", "AlbertTokenizer", ".", "from_pretrained", "(", "'albert-base-v2'", ")", "\n", "", "elif", "lm", "==", "'roberta'", ":", "\n", "            ", "from", "transformers", "import", "RobertaTokenizer", "\n", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "'roberta-base'", ")", "\n", "", "elif", "lm", "==", "'xlnet'", ":", "\n", "            ", "from", "transformers", "import", "XLNetTokenizer", "\n", "tokenizer", "=", "XLNetTokenizer", ".", "from_pretrained", "(", "'xlnet-base-cased'", ")", "\n", "", "elif", "lm", "==", "'longformer'", ":", "\n", "            ", "from", "transformers", "import", "LongformerTokenizer", "\n", "tokenizer", "=", "LongformerTokenizer", ".", "from_pretrained", "(", "'allenai/longformer-base-4096'", ")", "\n", "", "", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.model.MultiTaskNet.__init__": [[13, 85], ["torch.Module.__init__", "torch.ModuleDict", "torch.ModuleDict", "len", "torch.load", "torch.load", "torch.load", "torch.load", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "transformers.BertModel.from_pretrained", "transformers.BertModel.from_pretrained", "len", "len", "transformers.DistilBertModel.from_pretrained", "transformers.DistilBertModel.from_pretrained", "transformers.AlbertModel.from_pretrained", "transformers.AlbertModel.from_pretrained", "transformers.XLNetModel.from_pretrained", "transformers.XLNetModel.from_pretrained", "transformers.RobertaModel.from_pretrained", "transformers.RobertaModel.from_pretrained", "transformers.LongformerModel.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.model.MultiTaskNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "task_configs", "=", "[", "]", ",", "\n", "device", "=", "'cpu'", ",", "\n", "finetuning", "=", "True", ",", "\n", "lm", "=", "'bert'", ",", "\n", "bert_pt", "=", "None", ",", "\n", "bert_path", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "len", "(", "task_configs", ")", ">", "0", "\n", "\n", "# load the model or model checkpoint", "\n", "if", "bert_path", "==", "None", ":", "\n", "            ", "if", "lm", "==", "'bert'", ":", "\n", "                ", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "model_ckpts", "[", "lm", "]", ")", "\n", "", "elif", "lm", "==", "'distilbert'", ":", "\n", "                ", "self", ".", "bert", "=", "DistilBertModel", ".", "from_pretrained", "(", "model_ckpts", "[", "lm", "]", ")", "\n", "", "elif", "lm", "==", "'albert'", ":", "\n", "                ", "self", ".", "bert", "=", "AlbertModel", ".", "from_pretrained", "(", "model_ckpts", "[", "lm", "]", ")", "\n", "", "elif", "lm", "==", "'xlnet'", ":", "\n", "                ", "self", ".", "bert", "=", "XLNetModel", ".", "from_pretrained", "(", "model_ckpts", "[", "lm", "]", ")", "\n", "", "elif", "lm", "==", "'roberta'", ":", "\n", "                ", "self", ".", "bert", "=", "RobertaModel", ".", "from_pretrained", "(", "model_ckpts", "[", "lm", "]", ")", "\n", "", "elif", "lm", "==", "'longformer'", ":", "\n", "                ", "self", ".", "bert", "=", "LongformerModel", ".", "from_pretrained", "(", "model_ckpts", "[", "lm", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "output_model_file", "=", "bert_path", "\n", "model_state_dict", "=", "torch", ".", "load", "(", "output_model_file", ",", "\n", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "if", "lm", "==", "'bert'", ":", "\n", "                ", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "model_ckpts", "[", "lm", "]", ",", "\n", "state_dict", "=", "model_state_dict", ")", "\n", "", "elif", "lm", "==", "'distilbert'", ":", "\n", "                ", "self", ".", "bert", "=", "DistilBertModel", ".", "from_pretrained", "(", "model_ckpts", "[", "lm", "]", ",", "\n", "state_dict", "=", "model_state_dict", ")", "\n", "", "elif", "lm", "==", "'albert'", ":", "\n", "                ", "self", ".", "bert", "=", "AlbertModel", ".", "from_pretrained", "(", "model_ckpts", "[", "lm", "]", ",", "\n", "state_dict", "=", "model_state_dict", ")", "\n", "", "elif", "lm", "==", "'xlnet'", ":", "\n", "                ", "self", ".", "bert", "=", "XLNetModel", ".", "from_pretrained", "(", "model_ckpts", "[", "lm", "]", ",", "\n", "state_dict", "=", "model_state_dict", ")", "\n", "", "elif", "lm", "==", "'roberta'", ":", "\n", "                ", "self", ".", "bert", "=", "RobertaModel", ".", "from_pretrained", "(", "model_ckpts", "[", "lm", "]", ",", "\n", "state_dict", "=", "model_state_dict", ")", "\n", "\n", "", "", "self", ".", "device", "=", "device", "\n", "self", ".", "finetuning", "=", "finetuning", "\n", "self", ".", "task_configs", "=", "task_configs", "\n", "self", ".", "module_dict", "=", "nn", ".", "ModuleDict", "(", "{", "}", ")", "\n", "self", ".", "lm", "=", "lm", "\n", "\n", "# hard corded for now", "\n", "hidden_size", "=", "768", "\n", "hidden_dropout_prob", "=", "0.1", "\n", "\n", "for", "config", "in", "task_configs", ":", "\n", "            ", "name", "=", "config", "[", "'name'", "]", "\n", "task_type", "=", "config", "[", "'task_type'", "]", "\n", "vocab", "=", "config", "[", "'vocab'", "]", "\n", "\n", "if", "task_type", "==", "'tagging'", ":", "\n", "# for tagging", "\n", "                ", "vocab_size", "=", "len", "(", "vocab", ")", "# 'O' and '<PAD>'", "\n", "if", "'O'", "not", "in", "vocab", ":", "\n", "                    ", "vocab_size", "+=", "1", "\n", "", "if", "'<PAD>'", "not", "in", "vocab", ":", "\n", "                    ", "vocab_size", "+=", "1", "\n", "", "", "else", ":", "\n", "# for pairing and classification", "\n", "                ", "vocab_size", "=", "len", "(", "vocab", ")", "\n", "\n", "", "self", ".", "module_dict", "[", "'%s_dropout'", "%", "name", "]", "=", "nn", ".", "Dropout", "(", "hidden_dropout_prob", ")", "\n", "self", ".", "module_dict", "[", "'%s_fc'", "%", "name", "]", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "vocab_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.model.MultiTaskNet.forward": [[87, 189], ["x.to.to.to", "y.to.to.to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "aug_x.to.to.to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "fc", "fc.argmax", "fc", "model.MultiTaskNet.bert.train", "dropout", "model.MultiTaskNet.bert.eval", "dropout", "model.MultiTaskNet.bert.train", "model.MultiTaskNet.bert.eval", "fc.argmax", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.MultiTaskNet.bert", "dropout", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.MultiTaskNet.bert", "dropout", "model.MultiTaskNet.bert", "model.MultiTaskNet.bert", "model.MultiTaskNet.bert", "model.MultiTaskNet.bert"], "methods", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.baseline.train", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.baseline.train"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "\n", "augment_batch", "=", "None", ",", "\n", "aug_enc", "=", "None", ",", "\n", "second_batch", "=", "None", ",", "\n", "x_enc", "=", "None", ",", "\n", "task", "=", "'hotel_tagging'", ",", "\n", "get_enc", "=", "False", ")", ":", "\n", "        ", "\"\"\"Forward function of the BERT models for classification/tagging.\n\n        Args:\n            x (Tensor):\n            y (Tensor):\n            augment_batch (tuple of Tensor, optional):\n            aug_enc (Tensor, optional):\n            second_batch (Tensor, optional):\n            task (string, optional):\n            get_enc (boolean, optional):\n\n        Returns:\n            Tensor: logits\n            Tensor: y\n            Tensor: yhat\n            Tensor (optional): enc\"\"\"", "\n", "\n", "# move input to GPU", "\n", "x", "=", "x", ".", "to", "(", "self", ".", "device", ")", "\n", "y", "=", "y", ".", "to", "(", "self", ".", "device", ")", "\n", "if", "second_batch", "!=", "None", ":", "\n", "            ", "index", ",", "lam", "=", "second_batch", "\n", "lam", "=", "torch", ".", "tensor", "(", "lam", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "if", "augment_batch", "!=", "None", ":", "\n", "            ", "aug_x", ",", "aug_lam", "=", "augment_batch", "\n", "aug_x", "=", "aug_x", ".", "to", "(", "self", ".", "device", ")", "\n", "aug_lam", "=", "torch", ".", "tensor", "(", "aug_lam", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "dropout", "=", "self", ".", "module_dict", "[", "task", "+", "'_dropout'", "]", "\n", "fc", "=", "self", ".", "module_dict", "[", "task", "+", "'_fc'", "]", "\n", "\n", "if", "'tagging'", "in", "task", ":", "# TODO: this needs to be changed later", "\n", "            ", "if", "self", ".", "training", "and", "self", ".", "finetuning", ":", "\n", "                ", "self", ".", "bert", ".", "train", "(", ")", "\n", "if", "x_enc", "is", "None", ":", "\n", "                    ", "enc", "=", "self", ".", "bert", "(", "x", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "                    ", "enc", "=", "x_enc", "\n", "# Dropout", "\n", "", "enc", "=", "dropout", "(", "enc", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "bert", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "enc", "=", "self", ".", "bert", "(", "x", ")", "[", "0", "]", "\n", "\n", "", "", "if", "augment_batch", "!=", "None", ":", "\n", "                ", "if", "aug_enc", "is", "None", ":", "\n", "                    ", "aug_enc", "=", "self", ".", "bert", "(", "aug_x", ")", "[", "0", "]", "\n", "", "enc", "[", ":", "aug_x", ".", "shape", "[", "0", "]", "]", "*=", "aug_lam", "\n", "enc", "[", ":", "aug_x", ".", "shape", "[", "0", "]", "]", "+=", "aug_enc", "*", "(", "1", "-", "aug_lam", ")", "\n", "\n", "", "if", "second_batch", "!=", "None", ":", "\n", "                ", "enc", "=", "enc", "*", "lam", "+", "enc", "[", "index", "]", "*", "(", "1", "-", "lam", ")", "\n", "enc", "=", "dropout", "(", "enc", ")", "\n", "\n", "", "logits", "=", "fc", "(", "enc", ")", "\n", "y_hat", "=", "logits", ".", "argmax", "(", "-", "1", ")", "\n", "if", "get_enc", ":", "\n", "                ", "return", "logits", ",", "y", ",", "y_hat", ",", "enc", "\n", "", "else", ":", "\n", "                ", "return", "logits", ",", "y", ",", "y_hat", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "training", "and", "self", ".", "finetuning", ":", "\n", "                ", "self", ".", "bert", ".", "train", "(", ")", "\n", "if", "x_enc", "is", "None", ":", "\n", "                    ", "output", "=", "self", ".", "bert", "(", "x", ")", "\n", "pooled_output", "=", "output", "[", "0", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "pooled_output", "=", "dropout", "(", "pooled_output", ")", "\n", "", "else", ":", "\n", "                    ", "pooled_output", "=", "x_enc", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "bert", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "output", "=", "self", ".", "bert", "(", "x", ")", "\n", "pooled_output", "=", "output", "[", "0", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "pooled_output", "=", "dropout", "(", "pooled_output", ")", "\n", "\n", "", "", "if", "augment_batch", "!=", "None", ":", "\n", "                ", "if", "aug_enc", "is", "None", ":", "\n", "                    ", "aug_enc", "=", "self", ".", "bert", "(", "aug_x", ")", "[", "0", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "", "pooled_output", "[", ":", "aug_x", ".", "shape", "[", "0", "]", "]", "*=", "aug_lam", "\n", "pooled_output", "[", ":", "aug_x", ".", "shape", "[", "0", "]", "]", "+=", "aug_enc", "*", "(", "1", "-", "aug_lam", ")", "\n", "\n", "", "if", "second_batch", "!=", "None", ":", "\n", "                ", "pooled_output", "=", "pooled_output", "*", "lam", "+", "pooled_output", "[", "index", "]", "*", "(", "1", "-", "lam", ")", "\n", "\n", "", "logits", "=", "fc", "(", "pooled_output", ")", "\n", "if", "'sts-b'", "in", "task", ":", "\n", "                ", "y_hat", "=", "logits", "\n", "", "else", ":", "\n", "                ", "y_hat", "=", "logits", ".", "argmax", "(", "-", "1", ")", "\n", "", "if", "get_enc", ":", "\n", "                ", "return", "logits", ",", "y", ",", "y_hat", ",", "pooled_output", "\n", "", "else", ":", "\n", "                ", "return", "logits", ",", "y", ",", "y_hat", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.baseline.train": [[18, 95], ["torch.utils.data.DataLoader", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.MSELoss", "model.train", "enumerate", "optimizer.zero_grad", "model", "y.view.view", "criterion", "optimizer.step", "logits.view.view", "logits.view.view", "criterion.backward", "scheduler.step", "print", "print", "print", "print", "print", "numpy.isscalar", "print", "print", "print", "print", "print", "print", "apex.amp.scale_loss", "scaled_loss.backward", "_y.cpu().numpy", "print", "print", "dataset.get_tokenizer().convert_ids_to_tokens", "x.cpu().numpy", "_y.cpu", "criterion.item", "dataset.get_tokenizer", "x.cpu().numpy", "x.cpu", "x.cpu"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.baseline.train", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.dataset.get_tokenizer"], ["def", "train", "(", "model", ",", "train_set", ",", "optimizer", ",", "scheduler", "=", "None", ",", "batch_size", "=", "32", ",", "fp16", "=", "False", ")", ":", "\n", "    ", "\"\"\"Perfrom one epoch of the training process.\n\n    Args:\n        model (MultiTaskNet): the current model state\n        train_set (SnippextDataset): the training dataset\n        optimizer: the optimizer for training (e.g., Adam)\n        batch_size (int, optional): the batch size\n        fp16 (boolean): whether to use fp16\n\n    Returns:\n        None\n    \"\"\"", "\n", "iterator", "=", "data", ".", "DataLoader", "(", "dataset", "=", "train_set", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "1", ",", "\n", "collate_fn", "=", "SnippextDataset", ".", "pad", ")", "\n", "\n", "tagging_criterion", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "0", ")", "\n", "classifier_criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "regression_criterion", "=", "nn", ".", "MSELoss", "(", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "iterator", ")", ":", "\n", "# for monitoring", "\n", "        ", "words", ",", "x", ",", "is_heads", ",", "tags", ",", "mask", ",", "y", ",", "seqlens", ",", "taskname", "=", "batch", "\n", "taskname", "=", "taskname", "[", "0", "]", "\n", "_y", "=", "y", "\n", "\n", "if", "'tagging'", "in", "taskname", ":", "\n", "            ", "criterion", "=", "tagging_criterion", "\n", "", "elif", "'sts-b'", "in", "taskname", ":", "\n", "            ", "criterion", "=", "regression_criterion", "\n", "", "else", ":", "\n", "            ", "criterion", "=", "classifier_criterion", "\n", "\n", "# forward", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "logits", ",", "y", ",", "_", "=", "model", "(", "x", ",", "y", ",", "task", "=", "taskname", ")", "\n", "if", "'sts-b'", "in", "taskname", ":", "\n", "            ", "logits", "=", "logits", ".", "view", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "shape", "[", "-", "1", "]", ")", "\n", "", "y", "=", "y", ".", "view", "(", "-", "1", ")", "\n", "loss", "=", "criterion", "(", "logits", ",", "y", ")", "\n", "\n", "# back propagation", "\n", "if", "fp16", ":", "\n", "            ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "loss", ".", "backward", "(", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "if", "scheduler", ":", "\n", "            ", "scheduler", ".", "step", "(", ")", "\n", "\n", "", "if", "i", "==", "0", ":", "\n", "            ", "print", "(", "\"=====sanity check======\"", ")", "\n", "print", "(", "\"words:\"", ",", "words", "[", "0", "]", ")", "\n", "print", "(", "\"x:\"", ",", "x", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "[", ":", "seqlens", "[", "0", "]", "]", ")", "\n", "print", "(", "\"tokens:\"", ",", "get_tokenizer", "(", ")", ".", "convert_ids_to_tokens", "(", "x", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", ")", "[", ":", "seqlens", "[", "0", "]", "]", ")", "\n", "print", "(", "\"is_heads:\"", ",", "is_heads", "[", "0", "]", ")", "\n", "y_sample", "=", "_y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "\n", "if", "np", ".", "isscalar", "(", "y_sample", ")", ":", "\n", "                ", "print", "(", "\"y:\"", ",", "y_sample", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"y:\"", ",", "y_sample", "[", ":", "seqlens", "[", "0", "]", "]", ")", "\n", "", "print", "(", "\"tags:\"", ",", "tags", "[", "0", "]", ")", "\n", "print", "(", "\"mask:\"", ",", "mask", "[", "0", "]", ")", "\n", "print", "(", "\"seqlen:\"", ",", "seqlens", "[", "0", "]", ")", "\n", "print", "(", "\"task_name:\"", ",", "taskname", ")", "\n", "print", "(", "\"=======================\"", ")", "\n", "\n", "", "if", "i", "%", "10", "==", "0", ":", "# monitoring", "\n", "            ", "print", "(", "f\"step: {i}, task: {taskname}, loss: {loss.item()}\"", ")", "\n", "del", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.baseline.initialize_and_train": [[96, 187], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "model.MultiTaskNet", "transformers.get_linear_schedule_with_warmup", "tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter.close", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "transformers.AdamW", "model.cuda.cuda", "transformers.AdamW", "os.path.exists", "os.makedirs", "baseline.train", "print", "train_util.eval_on_task", "model.cuda.parameters", "model.cuda.parameters", "apex.amp.initialize", "len", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "model.cuda.state_dict", "model.cuda.state_dict"], "function", ["home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.baseline.train", "home.repos.pwc.inspect_result.rit-git_Snippext_public.snippext.train_util.eval_on_task", "home.repos.pwc.inspect_result.rit-git_Snippext_public.None.run_pipeline.initialize"], ["", "", "", "def", "initialize_and_train", "(", "task_config", ",", "\n", "trainset", ",", "\n", "validset", ",", "\n", "testset", ",", "\n", "hp", ",", "\n", "run_tag", ")", ":", "\n", "    ", "\"\"\"The train process.\n\n    Args:\n        task_config (dictionary): the configuration of the task\n        trainset (SnippextDataset): the training set\n        validset (SnippextDataset): the validation set\n        testset (SnippextDataset): the testset\n        hp (Namespace): the parsed hyperparameters\n        run_tag (string): the tag of the run (for logging purpose)\n\n    Returns:\n        None\n    \"\"\"", "\n", "# create iterators for validation and test", "\n", "padder", "=", "SnippextDataset", ".", "pad", "\n", "valid_iter", "=", "data", ".", "DataLoader", "(", "dataset", "=", "validset", ",", "\n", "batch_size", "=", "hp", ".", "batch_size", "*", "4", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "padder", ")", "\n", "test_iter", "=", "data", ".", "DataLoader", "(", "dataset", "=", "testset", ",", "\n", "batch_size", "=", "hp", ".", "batch_size", "*", "4", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "0", ",", "\n", "collate_fn", "=", "padder", ")", "\n", "\n", "# initialize model", "\n", "device", "=", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", "\n", "model", "=", "MultiTaskNet", "(", "[", "task_config", "]", ",", "\n", "device", ",", "\n", "hp", ".", "finetuning", ",", "\n", "lm", "=", "hp", ".", "lm", ",", "\n", "bert_path", "=", "hp", ".", "bert_path", ")", "\n", "if", "device", "==", "'cpu'", ":", "\n", "        ", "optimizer", "=", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "hp", ".", "lr", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "model", ".", "cuda", "(", ")", "\n", "optimizer", "=", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "hp", ".", "lr", ")", "\n", "if", "hp", ".", "fp16", ":", "\n", "            ", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "'O2'", ")", "\n", "\n", "# learning rate scheduler", "\n", "", "", "num_steps", "=", "(", "len", "(", "trainset", ")", "//", "hp", ".", "batch_size", ")", "*", "hp", ".", "n_epochs", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "\n", "num_warmup_steps", "=", "num_steps", "//", "10", ",", "\n", "num_training_steps", "=", "num_steps", ")", "\n", "\n", "# create logging directory", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "hp", ".", "logdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "hp", ".", "logdir", ")", "\n", "", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "hp", ".", "logdir", ")", "\n", "\n", "# start training", "\n", "best_dev_f1", "=", "best_test_f1", "=", "0.0", "\n", "epoch", "=", "1", "\n", "while", "epoch", "<=", "hp", ".", "n_epochs", ":", "\n", "        ", "train", "(", "model", ",", "\n", "trainset", ",", "\n", "optimizer", ",", "\n", "scheduler", "=", "scheduler", ",", "\n", "batch_size", "=", "hp", ".", "batch_size", ",", "\n", "fp16", "=", "hp", ".", "fp16", ")", "\n", "\n", "print", "(", "f\"=========eval at epoch={epoch}=========\"", ")", "\n", "dev_f1", ",", "test_f1", "=", "eval_on_task", "(", "epoch", ",", "\n", "model", ",", "\n", "task_config", "[", "'name'", "]", ",", "\n", "valid_iter", ",", "\n", "validset", ",", "\n", "test_iter", ",", "\n", "testset", ",", "\n", "writer", ",", "\n", "run_tag", ")", "\n", "\n", "if", "dev_f1", ">", "1e-6", ":", "\n", "            ", "epoch", "+=", "1", "\n", "if", "hp", ".", "save_model", ":", "\n", "                ", "if", "dev_f1", ">", "best_dev_f1", ":", "\n", "                    ", "best_dev_f1", "=", "dev_f1", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "run_tag", "+", "'_dev.pt'", ")", "\n", "", "if", "test_f1", ">", "best_test_f1", ":", "\n", "                    ", "best_test_f1", "=", "test_f1", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "run_tag", "+", "'_test.pt'", ")", "\n", "\n", "", "", "", "", "writer", ".", "close", "(", ")", "\n", "\n"]]}