{"home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.COCOEvalCap.__init__": [[187, 194], ["coco.keys"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "coco", ",", "cocoRes", ")", ":", "\n", "        ", "self", ".", "evalImgs", "=", "[", "]", "\n", "self", ".", "eval", "=", "{", "}", "\n", "self", ".", "imgToEval", "=", "{", "}", "\n", "self", ".", "coco", "=", "coco", "\n", "self", ".", "cocoRes", "=", "cocoRes", "\n", "self", ".", "params", "=", "{", "'image_id'", ":", "coco", ".", "keys", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.COCOEvalCap.evaluate": [[195, 243], ["tokenizer.ptbtokenizer.PTBTokenizer.ptbtokenizer.PTBTokenizer", "tokenizer.ptbtokenizer.PTBTokenizer.ptbtokenizer.PTBTokenizer.tokenize", "tokenizer.ptbtokenizer.PTBTokenizer.ptbtokenizer.PTBTokenizer.tokenize", "answerability_score.COCOEvalCap.setEvalImgs", "ngram_metric.startswith", "int.isdigit", "int", "scorer.compute_score", "type", "zip", "answerability_score.COCOEvalCap.setEval", "answerability_score.COCOEvalCap.setImgToEvalImgs", "bleu.bleu.Bleu", "rouge.rouge.Rouge", "len", "bleu.bleu.Bleu", "answerability_score.COCOEvalCap.setEval", "answerability_score.COCOEvalCap.setImgToEvalImgs", "range"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.tokenizer.ptbtokenizer.PTBTokenizer.tokenize", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.tokenizer.ptbtokenizer.PTBTokenizer.tokenize", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.COCOEvalCap.setEvalImgs", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.Rouge.compute_score", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.COCOEvalCap.setEval", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.COCOEvalCap.setImgToEvalImgs", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.COCOEvalCap.setEval", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.COCOEvalCap.setImgToEvalImgs"], ["", "def", "evaluate", "(", "self", ",", "ngram_metric", ")", ":", "\n", "        ", "imgIds", "=", "self", ".", "params", "[", "'image_id'", "]", "\n", "# imgIds = self.coco.getImgIds()", "\n", "gts", "=", "{", "}", "\n", "res", "=", "{", "}", "\n", "for", "imgId", "in", "imgIds", ":", "\n", "            ", "gts", "[", "imgId", "]", "=", "self", ".", "coco", "[", "imgId", "]", "#.imgToAnns[imgId]", "\n", "res", "[", "imgId", "]", "=", "self", ".", "cocoRes", "[", "imgId", "]", "#.imgToAnns[imgId]", "\n", "\n", "# =================================================", "\n", "# Set up scorers", "\n", "# =================================================", "\n", "", "tokenizer", "=", "PTBTokenizer", "(", ")", "\n", "gts", "=", "tokenizer", ".", "tokenize", "(", "gts", ")", "\n", "res", "=", "tokenizer", ".", "tokenize", "(", "res", ")", "\n", "\n", "# =================================================", "\n", "# Set up scorers", "\n", "# =================================================", "\n", "if", "ngram_metric", "==", "'ROUGE_L'", ":", "\n", "            ", "scorers", "=", "[", "\n", "(", "Bleu", "(", "1", ")", ",", "[", "\"Bleu_1\"", "]", ")", ",", "\n", "(", "Rouge", "(", ")", ",", "\"ROUGE_L\"", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "assert", "ngram_metric", ".", "startswith", "(", "'Bleu_'", ")", "\n", "i", "=", "ngram_metric", "[", "len", "(", "'Bleu_'", ")", ":", "]", "\n", "assert", "i", ".", "isdigit", "(", ")", "\n", "i", "=", "int", "(", "i", ")", "\n", "assert", "i", ">", "0", "\n", "scorers", "=", "[", "\n", "(", "Bleu", "(", "i", ")", ",", "[", "'Bleu_{}'", ".", "format", "(", "j", ")", "for", "j", "in", "range", "(", "1", ",", "i", "+", "1", ")", "]", ")", ",", "\n", "]", "\n", "\n", "# =================================================", "\n", "# Compute scores", "\n", "# =================================================", "\n", "", "for", "scorer", ",", "method", "in", "scorers", ":", "\n", "            ", "score", ",", "scores", "=", "scorer", ".", "compute_score", "(", "gts", ",", "res", ")", "\n", "if", "type", "(", "method", ")", "==", "list", ":", "\n", "                ", "for", "sc", ",", "scs", ",", "m", "in", "zip", "(", "score", ",", "scores", ",", "method", ")", ":", "\n", "                    ", "self", ".", "setEval", "(", "sc", ",", "m", ")", "\n", "self", ".", "setImgToEvalImgs", "(", "scs", ",", "imgIds", ",", "m", ")", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "setEval", "(", "score", ",", "method", ")", "\n", "self", ".", "setImgToEvalImgs", "(", "scores", ",", "imgIds", ",", "method", ")", "\n", "", "", "self", ".", "setEvalImgs", "(", ")", "\n", "return", "self", ".", "evalImgs", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.COCOEvalCap.setEval": [[244, 246], ["None"], "methods", ["None"], ["", "def", "setEval", "(", "self", ",", "score", ",", "method", ")", ":", "\n", "        ", "self", ".", "eval", "[", "method", "]", "=", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.COCOEvalCap.setImgToEvalImgs": [[247, 253], ["zip"], "methods", ["None"], ["", "def", "setImgToEvalImgs", "(", "self", ",", "scores", ",", "imgIds", ",", "method", ")", ":", "\n", "        ", "for", "imgId", ",", "score", "in", "zip", "(", "imgIds", ",", "scores", ")", ":", "\n", "            ", "if", "imgId", "not", "in", "self", ".", "imgToEval", ":", "\n", "                ", "self", ".", "imgToEval", "[", "imgId", "]", "=", "{", "}", "\n", "self", ".", "imgToEval", "[", "imgId", "]", "[", "\"image_id\"", "]", "=", "imgId", "\n", "", "self", ".", "imgToEval", "[", "imgId", "]", "[", "method", "]", "=", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.COCOEvalCap.setEvalImgs": [[254, 256], ["answerability_score.COCOEvalCap.imgToEval.items"], "methods", ["None"], ["", "", "def", "setEvalImgs", "(", "self", ")", ":", "\n", "        ", "self", ".", "evalImgs", "=", "[", "eval", "for", "imgId", ",", "eval", "in", "self", ".", "imgToEval", ".", "items", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.remove_stopwords_and_NER_line": [[41, 76], ["question.split.split", "question.split.split", "enumerate", "i[].isupper", "temp_words.append", "temp_words.append", "j.lower", "temp_words.append", "temp_words.append", "len", "w.lower", "w.lower"], "function", ["None"], ["def", "remove_stopwords_and_NER_line", "(", "question", ",", "relevant_words", "=", "None", ",", "question_words", "=", "None", ")", ":", "\n", "    ", "if", "relevant_words", "is", "None", ":", "\n", "\n", "        ", "question", "=", "question", ".", "split", "(", ")", "\n", "if", "question_words", "is", "None", ":", "\n", "           ", "question_words", "=", "question_words_global", "\n", "\n", "", "temp_words", "=", "[", "]", "\n", "for", "word", "in", "question_words", ":", "\n", "            ", "for", "i", ",", "w", "in", "enumerate", "(", "question", ")", ":", "\n", "                ", "if", "w", "==", "word", ":", "\n", "                    ", "temp_words", ".", "append", "(", "w", ")", "\n", "# If the question type is 'what' or 'which' the following word is generally associated with", "\n", "# with the answer type. Thus it is important that it is considered a part of the question.", "\n", "if", "i", "+", "1", "<", "len", "(", "question", ")", "and", "(", "w", ".", "lower", "(", ")", "==", "\"what\"", "or", "w", ".", "lower", "(", ")", "==", "\"which\"", ")", ":", "\n", "                        ", "temp_words", ".", "append", "(", "question", "[", "i", "+", "1", "]", ")", "\n", "\n", "", "", "", "", "question_split", "=", "[", "item", "for", "item", "in", "question", "if", "item", "not", "in", "temp_words", "]", "\n", "ner_words", "=", "question_split", "\n", "temp_words", "=", "[", "]", "\n", "\n", "for", "i", "in", "ner_words", ":", "\n", "            ", "if", "i", "[", "0", "]", ".", "isupper", "(", ")", "==", "False", ":", "\n", "                ", "if", "i", "not", "in", "stop_words", ":", "\n", "                    ", "temp_words", ".", "append", "(", "i", ")", "\n", "\n", "", "", "", "return", "\" \"", ".", "join", "(", "temp_words", ")", "\n", "", "else", ":", "\n", "        ", "question_words", "=", "question", ".", "split", "(", ")", "\n", "temp_words", "=", "[", "]", "\n", "for", "i", "in", "question_words", ":", "\n", "            ", "for", "j", "in", "relevant_words", ":", "\n", "                ", "if", "j", ".", "lower", "(", ")", "in", "i", ":", "\n", "                    ", "temp_words", ".", "append", "(", "i", ")", "\n", "", "", "", "return", "\" \"", ".", "join", "(", "temp_words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.NER_line": [[77, 89], ["question.split", "question_words[].lower", "i[].isupper", "temp_words.append"], "function", ["None"], ["", "", "def", "NER_line", "(", "question", ")", ":", "\n", "    ", "q_types", "=", "question_words_global", "\n", "question_words", "=", "question", ".", "split", "(", ")", "\n", "if", "question_words", "[", "0", "]", ".", "lower", "(", ")", "in", "q_types", ":", "\n", "        ", "question_words", "=", "question_words", "[", "1", ":", "]", "\n", "\n", "", "temp_words", "=", "[", "]", "\n", "for", "i", "in", "question_words", ":", "\n", "        ", "if", "i", "[", "0", "]", ".", "isupper", "(", ")", ":", "\n", "            ", "temp_words", ".", "append", "(", "i", ")", "\n", "\n", "", "", "return", "\" \"", ".", "join", "(", "temp_words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.get_stopwords": [[91, 99], ["question.split", "i.lower", "temp_words.append", "i.lower"], "function", ["None"], ["", "def", "get_stopwords", "(", "question", ")", ":", "\n", "    ", "question_words", "=", "question", ".", "split", "(", ")", "\n", "temp_words", "=", "[", "]", "\n", "for", "i", "in", "question_words", ":", "\n", "        ", "if", "i", ".", "lower", "(", ")", "in", "stop_words", ":", "\n", "            ", "temp_words", ".", "append", "(", "i", ".", "lower", "(", ")", ")", "\n", "\n", "", "", "return", "\" \"", ".", "join", "(", "temp_words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.questiontype": [[101, 123], ["question.split.strip", "question.split.split", "enumerate", "question.split.startswith", "temp_words.append", "temp_words.append", "len", "w.lower", "w.lower"], "function", ["None"], ["", "def", "questiontype", "(", "question", ",", "questiontypes", "=", "None", ")", ":", "\n", "\n", "    ", "if", "questiontypes", "is", "None", ":", "\n", "        ", "types", "=", "question_words_global", "\n", "question", "=", "question", ".", "strip", "(", ")", "\n", "temp_words", "=", "[", "]", "\n", "question", "=", "question", ".", "split", "(", ")", "\n", "\n", "for", "word", "in", "types", ":", "\n", "            ", "for", "i", ",", "w", "in", "enumerate", "(", "question", ")", ":", "\n", "                ", "if", "w", "==", "word", ":", "\n", "                    ", "temp_words", ".", "append", "(", "w", ")", "\n", "if", "i", "+", "1", "<", "len", "(", "question", ")", "and", "(", "w", ".", "lower", "(", ")", "==", "\"what\"", "or", "w", ".", "lower", "(", ")", "==", "\"which\"", ")", ":", "\n", "                        ", "temp_words", ".", "append", "(", "question", "[", "i", "+", "1", "]", ")", "\n", "\n", "", "", "", "", "return", "\" \"", ".", "join", "(", "temp_words", ")", "\n", "", "else", ":", "\n", "        ", "for", "i", "in", "questiontypes", ":", "\n", "            ", "if", "question", ".", "startswith", "(", "i", "+", "\" \"", ")", ":", "\n", "                ", "return", "i", "\n", "", "else", ":", "\n", "                ", "return", "\" \"", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score._get_json_format_qbleu": [[125, 170], ["enumerate", "os.path.exists", "os.makedirs", "answerability_score.remove_stopwords_and_NER_line", "answerability_score.NER_line", "answerability_score.questiontype", "answerability_score.get_stopwords", "pred_sents_impwords.append", "pred_sents_ner.append", "pred_sents_qt.append", "pred_sents_sw.append", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "data_pred_impwords.append", "data_pred_qt.append", "data_pred_ner.append", "data_pred.append", "data_pred_sw.append", "open", "json.dump", "open", "json.dump", "open", "json.dump", "open", "json.dump", "open", "json.dump", "os.path.dirname", "os.path.dirname", "dict", "dict", "dict", "dict", "dict"], "function", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.remove_stopwords_and_NER_line", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.NER_line", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.questiontype", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.get_stopwords"], ["", "", "", "", "def", "_get_json_format_qbleu", "(", "lines", ",", "output_path_prefix", ",", "relevant_words", "=", "None", ",", "questiontypes", "=", "None", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "dirname", "(", "output_path_prefix", ")", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "output_path_prefix", ")", ")", "\n", "", "name", "=", "output_path_prefix", "+", "'_components'", "\n", "pred_sents_impwords", "=", "[", "]", "\n", "pred_sents_ner", "=", "[", "]", "\n", "pred_sents_qt", "=", "[", "]", "\n", "pred_sents_sw", "=", "[", "]", "\n", "for", "line", "in", "lines", ":", "\n", "        ", "line_impwords", "=", "remove_stopwords_and_NER_line", "(", "line", ",", "relevant_words", ")", "\n", "line_ner", "=", "NER_line", "(", "line", ")", "\n", "line_qt", "=", "questiontype", "(", "line", ",", "questiontypes", ")", "\n", "line_sw", "=", "get_stopwords", "(", "line", ")", "\n", "pred_sents_impwords", ".", "append", "(", "line_impwords", ")", "\n", "pred_sents_ner", ".", "append", "(", "line_ner", ")", "\n", "pred_sents_qt", ".", "append", "(", "line_qt", ")", "\n", "pred_sents_sw", ".", "append", "(", "line_sw", ")", "\n", "\n", "", "ref_files", "=", "[", "os", ".", "path", ".", "join", "(", "name", "+", "\"_impwords\"", ")", ",", "os", ".", "path", ".", "join", "(", "name", "+", "\"_ner\"", ")", ",", "os", ".", "path", ".", "join", "(", "name", "+", "\"_qt\"", ")", ",", "os", ".", "path", ".", "join", "(", "name", "+", "\"_fluent\"", ")", ",", "os", ".", "path", ".", "join", "(", "name", "+", "\"_sw\"", ")", "]", "\n", "\n", "data_pred_impwords", "=", "[", "]", "\n", "data_pred_qt", "=", "[", "]", "\n", "data_pred_ner", "=", "[", "]", "\n", "data_pred", "=", "[", "]", "\n", "data_pred_sw", "=", "[", "]", "\n", "\n", "for", "index", ",", "s", "in", "enumerate", "(", "pred_sents_impwords", ")", ":", "\n", "        ", "data_pred_impwords", ".", "append", "(", "dict", "(", "image_id", "=", "index", ",", "caption", "=", "s", ")", ")", "\n", "data_pred_qt", ".", "append", "(", "dict", "(", "image_id", "=", "index", ",", "caption", "=", "pred_sents_qt", "[", "index", "]", ")", ")", "\n", "data_pred_ner", ".", "append", "(", "dict", "(", "image_id", "=", "index", ",", "caption", "=", "pred_sents_ner", "[", "index", "]", ")", ")", "\n", "data_pred", ".", "append", "(", "dict", "(", "image_id", "=", "index", ",", "caption", "=", "lines", "[", "index", "]", ")", ")", "\n", "data_pred_sw", ".", "append", "(", "dict", "(", "image_id", "=", "index", ",", "caption", "=", "pred_sents_sw", "[", "index", "]", ")", ")", "\n", "\n", "", "with", "open", "(", "ref_files", "[", "0", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data_pred_impwords", ",", "f", ",", "separators", "=", "(", "','", ",", "':'", ")", ")", "\n", "", "with", "open", "(", "ref_files", "[", "1", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data_pred_ner", ",", "f", ",", "separators", "=", "(", "','", ",", "':'", ")", ")", "\n", "", "with", "open", "(", "ref_files", "[", "2", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data_pred_qt", ",", "f", ",", "separators", "=", "(", "','", ",", "':'", ")", ")", "\n", "", "with", "open", "(", "ref_files", "[", "3", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data_pred", ",", "f", ",", "separators", "=", "(", "','", ",", "':'", ")", ")", "\n", "", "with", "open", "(", "ref_files", "[", "4", "]", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data_pred_sw", ",", "f", ",", "separators", "=", "(", "','", ",", "':'", ")", ")", "\n", "\n", "", "return", "ref_files", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.loadJsonToMap": [[172, 184], ["codecs.open", "json.load", "dict", "img_to_anns[].append", "length_of_sents.append", "len"], "function", ["None"], ["", "def", "loadJsonToMap", "(", "json_file", ")", ":", "\n", "    ", "with", "codecs", ".", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ",", "errors", "=", "\"ignore\"", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "", "img_to_anns", "=", "{", "}", "\n", "length_of_sents", "=", "[", "]", "\n", "for", "entry", "in", "data", ":", "\n", "        ", "if", "entry", "[", "'image_id'", "]", "not", "in", "img_to_anns", ":", "\n", "            ", "img_to_anns", "[", "entry", "[", "'image_id'", "]", "]", "=", "[", "]", "\n", "", "summary", "=", "dict", "(", "caption", "=", "entry", "[", "'caption'", "]", ",", "image_id", "=", "entry", "[", "'caption'", "]", ")", "\n", "img_to_anns", "[", "entry", "[", "'image_id'", "]", "]", ".", "append", "(", "summary", ")", "\n", "length_of_sents", ".", "append", "(", "len", "(", "entry", "[", "'caption'", "]", ")", ")", "\n", "", "return", "img_to_anns", ",", "length_of_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.compute_answerability_scores": [[258, 288], ["_logger.debug", "range", "numpy.mean", "numpy.mean", "_logger.info", "len", "len", "new_scores.append", "_logger.info", "numpy.savetxt", "numpy.savetxt", "os.path.exists", "os.makedirs", "os.path.join", "os.path.join"], "function", ["None"], ["", "", "def", "compute_answerability_scores", "(", "all_scores", ",", "ner_weight", ",", "qt_weight", ",", "re_weight", ",", "d", ",", "output_dir", ",", "ngram_metric", "=", "\"Bleu_4\"", ",", "\n", "save_to_files", "=", "False", ")", ":", "\n", "    ", "_logger", ".", "debug", "(", "\"Number of samples: %s\"", ",", "len", "(", "all_scores", ")", ")", "\n", "fluent_scores", "=", "[", "x", "[", "ngram_metric", "]", "for", "x", "in", "all_scores", "]", "\n", "imp_scores", "=", "[", "x", "[", "'imp'", "]", "for", "x", "in", "all_scores", "]", "\n", "qt_scores", "=", "[", "x", "[", "'qt'", "]", "for", "x", "in", "all_scores", "]", "\n", "sw_scores", "=", "[", "x", "[", "'sw'", "]", "for", "x", "in", "all_scores", "]", "\n", "ner_scores", "=", "[", "x", "[", "'ner'", "]", "for", "x", "in", "all_scores", "]", "\n", "\n", "new_scores", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "imp_scores", ")", ")", ":", "\n", "        ", "answerability", "=", "re_weight", "*", "imp_scores", "[", "i", "]", "+", "ner_weight", "*", "ner_scores", "[", "i", "]", "+", "qt_weight", "*", "qt_scores", "[", "i", "]", "+", "(", "1", "-", "re_weight", "-", "ner_weight", "-", "qt_weight", ")", "*", "sw_scores", "[", "i", "]", "\n", "\n", "temp", "=", "d", "*", "answerability", "+", "(", "1", "-", "d", ")", "*", "fluent_scores", "[", "i", "]", "\n", "new_scores", ".", "append", "(", "temp", ")", "\n", "_logger", ".", "info", "(", "\"New Score: %.3f\\nNER Score: %.3f\\nRE Score: %.3f\\nSW Score %.3f\\nQT Score: %.3f\"", ",", "\n", "temp", ",", "ner_scores", "[", "i", "]", ",", "imp_scores", "[", "i", "]", ",", "sw_scores", "[", "i", "]", ",", "qt_scores", "[", "i", "]", ")", "\n", "\n", "", "mean_answerability_score", "=", "np", ".", "mean", "(", "new_scores", ")", "\n", "mean_fluent_score", "=", "np", ".", "mean", "(", "fluent_scores", ")", "\n", "_logger", ".", "info", "(", "\"Mean Answerability Score Across Questions: %.3f\\nN-gram Score: %.3f\"", ",", "\n", "mean_answerability_score", ",", "mean_fluent_score", ")", "\n", "if", "save_to_files", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "output_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "", "np", ".", "savetxt", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'ngram_scores.txt'", ")", ",", "fluent_scores", ")", "\n", "np", ".", "savetxt", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'answerability_scores.txt'", ")", ",", "new_scores", ")", "\n", "", "return", "mean_answerability_score", ",", "mean_fluent_score", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.new_eval_metric": [[290, 308], ["range", "len", "new_eval_per_line.append", "numpy.mean"], "function", ["None"], ["", "def", "new_eval_metric", "(", "final_eval_perline_impwords", ",", "final_eval_perline_ner", ",", "final_eval_perline_qt", ",", "fluent_eval_perline", ",", "final_eval_perline_sw", ",", "new_scores", ")", ":", "\n", "\n", "    ", "new_eval_per_line", "=", "[", "]", "\n", "alpha", "=", "new_scores", "[", "'alpha'", "]", "\n", "beta", "=", "new_scores", "[", "'beta'", "]", "\n", "gamma", "=", "new_scores", "[", "'gamma'", "]", "\n", "theta", "=", "new_scores", "[", "'theta'", "]", "\n", "fluent_perline", "=", "1", "-", "alpha", "-", "beta", "-", "gamma", "-", "theta", "\n", "for", "i", "in", "range", "(", "len", "(", "final_eval_perline_impwords", ")", ")", ":", "\n", "        ", "new_eval_alpha", "=", "alpha", "*", "final_eval_perline_impwords", "[", "i", "]", "[", "'Bleu_1'", "]", "\n", "new_eval_beta", "=", "beta", "*", "final_eval_perline_ner", "[", "i", "]", "[", "'Bleu_1'", "]", "\n", "new_eval_gamma", "=", "gamma", "*", "final_eval_perline_qt", "[", "i", "]", "[", "'Bleu_1'", "]", "\n", "new_eval_theta", "=", "theta", "*", "final_eval_perline_sw", "[", "i", "]", "[", "'Bleu_1'", "]", "\n", "fluent_per_line", "=", "fluent_perline", "*", "fluent_eval_perline", "[", "i", "]", "[", "sys", ".", "arg", "[", "7", "]", "]", "\n", "\n", "new_eval_per_line", ".", "append", "(", "new_eval_alpha", "+", "new_eval_beta", "+", "new_eval_gamma", "+", "new_eval_theta", "+", "fluent_per_line", ")", "\n", "\n", "", "return", "new_eval_per_line", ",", "np", ".", "mean", "(", "new_eval_per_line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.get_answerability_scores": [[310, 383], ["answerability_score._get_json_format_qbleu", "_logger.debug", "answerability_score._get_json_format_qbleu", "_logger.debug", "zip", "zip", "answerability_score.compute_answerability_scores", "data_type.lower.lower", "tempfile.gettempdir", "os.path.join", "os.path.join", "answerability_score.loadJsonToMap", "os.remove", "answerability_score.loadJsonToMap", "os.remove", "answerability_score.COCOEvalCap", "answerability_score.COCOEvalCap", "cocoRes.keys", "cocoRes.keys", "answerability_score.COCOEvalCap.evaluate", "answerability_score.COCOEvalCap.evaluate", "zip", "final_eval_f.append", "final_eval.append", "numpy.loadtxt", "save_all.append", "temp_f.append", "os.path.join", "numpy.loadtxt", "temp_f.append", "os.path.join", "temp_f.append"], "function", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score._get_json_format_qbleu", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score._get_json_format_qbleu", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.compute_answerability_scores", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.loadJsonToMap", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.loadJsonToMap", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.COCOEvalCap.evaluate", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.COCOEvalCap.evaluate"], ["", "def", "get_answerability_scores", "(", "hypotheses", ",", "\n", "ner_weight", ",", "\n", "qt_weight", ",", "\n", "re_weight", ",", "\n", "references", ",", "\n", "output_dir", "=", "None", ",", "\n", "ngram_metric", "=", "'Blue_3'", ",", "\n", "nist_meteor_scores_dir", "=", "None", ",", "\n", "delta", "=", "0.7", ",", "\n", "data_type", "=", "'SQuAD'", ",", "\n", "save_to_files", "=", "False", ")", ":", "\n", "    ", "if", "data_type", "is", "not", "None", ":", "\n", "        ", "data_type", "=", "data_type", ".", "lower", "(", ")", "\n", "", "if", "data_type", "==", "'wikimovies'", ":", "\n", "        ", "relevant_words", "=", "[", "'act'", ",", "'write'", ",", "'direct'", ",", "'describ'", ",", "'appear'", ",", "'star'", ",", "'genre'", ",", "'language'", ",", "'about'", ",", "'appear'", ",", "\n", "'cast'", "]", "\n", "question_words", "=", "None", "\n", "", "else", ":", "\n", "        ", "relevant_words", "=", "None", "\n", "question_words", "=", "None", "\n", "\n", "", "if", "output_dir", "is", "None", ":", "\n", "        ", "output_dir", "=", "tempfile", ".", "gettempdir", "(", ")", "\n", "", "filenames_1", "=", "_get_json_format_qbleu", "(", "references", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'refs'", ")", ",", "\n", "relevant_words", ",", "question_words", ")", "\n", "_logger", ".", "debug", "(", "\"Reference files written.\"", ")", "\n", "filenames_2", "=", "_get_json_format_qbleu", "(", "hypotheses", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'hyps'", ")", ",", "\n", "relevant_words", ",", "question_words", ")", "\n", "_logger", ".", "debug", "(", "\"Predicted files written.\"", ")", "\n", "final_eval", "=", "[", "]", "\n", "final_eval_f", "=", "[", "]", "\n", "for", "file_1", ",", "file_2", "in", "zip", "(", "filenames_1", ",", "filenames_2", ")", ":", "\n", "        ", "coco", ",", "len_sents", "=", "loadJsonToMap", "(", "file_1", ")", "\n", "os", ".", "remove", "(", "file_1", ")", "\n", "cocoRes", ",", "len_sents2", "=", "loadJsonToMap", "(", "file_2", ")", "\n", "os", ".", "remove", "(", "file_2", ")", "\n", "cocoEval_precision", "=", "COCOEvalCap", "(", "coco", ",", "cocoRes", ")", "\n", "cocoEval_recall", "=", "COCOEvalCap", "(", "cocoRes", ",", "coco", ")", "\n", "cocoEval_precision", ".", "params", "[", "'image_id'", "]", "=", "cocoRes", ".", "keys", "(", ")", "\n", "cocoEval_recall", ".", "params", "[", "'image_id'", "]", "=", "cocoRes", ".", "keys", "(", ")", "\n", "eval_per_line_p", "=", "cocoEval_precision", ".", "evaluate", "(", "ngram_metric", ")", "\n", "eval_per_line_r", "=", "cocoEval_recall", ".", "evaluate", "(", "ngram_metric", ")", "\n", "\n", "f_score", "=", "zip", "(", "eval_per_line_p", ",", "eval_per_line_r", ",", "len_sents", ",", "len_sents2", ")", "\n", "temp_f", "=", "[", "]", "\n", "for", "p", ",", "r", ",", "l1", ",", "l2", "in", "f_score", ":", "\n", "            ", "if", "l1", "==", "0", "and", "l2", "==", "0", ":", "\n", "                ", "temp_f", ".", "append", "(", "1", ")", "\n", "continue", "\n", "", "elif", "(", "p", "[", "'Bleu_1'", "]", "+", "r", "[", "'Bleu_1'", "]", "==", "0", ")", ":", "\n", "                ", "temp_f", ".", "append", "(", "0", ")", "\n", "continue", "\n", "", "temp_f", ".", "append", "(", "2", "*", "(", "p", "[", "'Bleu_1'", "]", "*", "r", "[", "'Bleu_1'", "]", ")", "/", "(", "p", "[", "'Bleu_1'", "]", "+", "r", "[", "'Bleu_1'", "]", ")", ")", "\n", "\n", "", "final_eval_f", ".", "append", "(", "temp_f", ")", "\n", "final_eval", ".", "append", "(", "eval_per_line_p", ")", "\n", "\n", "", "if", "ngram_metric", "==", "'NIST'", ":", "\n", "        ", "assert", "nist_meteor_scores_dir", "is", "not", "None", "\n", "metric_scores", "=", "np", ".", "loadtxt", "(", "os", ".", "path", ".", "join", "(", "nist_meteor_scores_dir", ",", "\"nist_scores\"", ")", ")", "\n", "", "elif", "ngram_metric", "==", "'METEOR'", ":", "\n", "        ", "assert", "nist_meteor_scores_dir", "is", "not", "None", "\n", "metric_scores", "=", "np", ".", "loadtxt", "(", "os", ".", "path", ".", "join", "(", "nist_meteor_scores_dir", ",", "\"meteor_scores\"", ")", ")", "\n", "", "else", ":", "\n", "        ", "metric_scores", "=", "[", "fl", "[", "ngram_metric", "]", "for", "fl", "in", "final_eval", "[", "3", "]", "]", "\n", "", "save_all", "=", "[", "]", "\n", "all_scores", "=", "zip", "(", "final_eval_f", "[", "0", "]", ",", "final_eval_f", "[", "1", "]", ",", "final_eval_f", "[", "2", "]", ",", "final_eval_f", "[", "4", "]", ",", "\n", "metric_scores", ")", "\n", "for", "imp", ",", "ner", ",", "qt", ",", "sw", ",", "metric_score", "in", "all_scores", ":", "\n", "        ", "d", "=", "{", "'imp'", ":", "imp", ",", "'ner'", ":", "ner", ",", "'qt'", ":", "qt", ",", "'sw'", ":", "sw", ",", "ngram_metric", ":", "metric_score", "}", "\n", "save_all", ".", "append", "(", "d", ")", "\n", "", "return", "compute_answerability_scores", "(", "save_all", ",", "ner_weight", ",", "qt_weight", ",", "re_weight", ",", "delta", ",", "output_dir", ",", "ngram_metric", ",", "\n", "save_to_files", "=", "save_to_files", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.main": [[385, 424], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logging.basicConfig", "answerability_score.get_answerability_scores", "open", "f.readlines", "open", "f.readlines", "tempfile.gettempdir"], "function", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.get_answerability_scores"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Get the arguments'", ")", "\n", "parser", ".", "add_argument", "(", "'--data_type'", ",", "dest", "=", "'data_type'", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Whether the data_type is [squad, wikimovies,vqa]. The relevant words in case of wikimovies is different.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ref_file'", ",", "dest", "=", "'ref_file'", ",", "type", "=", "str", ",", "help", "=", "\"Path to the reference question files\"", ")", "\n", "parser", ".", "add_argument", "(", "'--hyp_file'", ",", "dest", "=", "'hyp_file'", ",", "type", "=", "str", ",", "help", "=", "\"Path to the predicted question files\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ner_weight'", ",", "dest", "=", "'ner_weight'", ",", "type", "=", "float", ",", "help", "=", "\"Weight to be given to NEs\"", ")", "\n", "parser", ".", "add_argument", "(", "'--qt_weight'", ",", "dest", "=", "'qt_weight'", ",", "type", "=", "float", ",", "help", "=", "\"Weight to be given to Question types\"", ")", "\n", "parser", ".", "add_argument", "(", "'--re_weight'", ",", "dest", "=", "'re_weight'", ",", "type", "=", "float", ",", "help", "=", "\"Weight to be given to Relevant words\"", ")", "\n", "parser", ".", "add_argument", "(", "'--delta'", ",", "dest", "=", "'delta'", ",", "type", "=", "float", ",", "\n", "default", "=", "0.7", ",", "\n", "help", "=", "\"Weight to be given to answerability scores\"", ")", "\n", "parser", ".", "add_argument", "(", "'--output_dir'", ",", "dest", "=", "'output_dir'", ",", "type", "=", "str", ",", "default", "=", "tempfile", ".", "gettempdir", "(", ")", ",", "\n", "help", "=", "\"Path to directory to store the scores per line, and auxiliary files\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ngram_metric'", ",", "dest", "=", "'ngram_metric'", ",", "type", "=", "str", ",", "\n", "help", "=", "\"N-gram metric that needs to be considered\"", ")", "\n", "parser", ".", "add_argument", "(", "'--nist_meteor_scores_dir'", ",", "dest", "=", "\"nist_meteor_scores_dir\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Nist and Meteor needs to computed through different tools, provide the path to the precomputed scores\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "logging", ".", "basicConfig", "(", "format", "=", "'[%(levelname)s] %(asctime)s - %(filename)s::%(funcName)s\\n%(message)s'", ",", "\n", "level", "=", "logging", ".", "INFO", ")", "\n", "\n", "with", "open", "(", "args", ".", "hyp_file", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "hypothesis_lines", "=", "f", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "args", ".", "ref_file", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "references_lines", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "get_answerability_scores", "(", "delta", "=", "args", ".", "delta", ",", "\n", "hypotheses", "=", "hypothesis_lines", ",", "\n", "ner_weight", "=", "args", ".", "ner_weight", ",", "\n", "ngram_metric", "=", "args", ".", "ngram_metric", ",", "\n", "nist_meteor_scores_dir", "=", "args", ".", "nist_meteor_scores_dir", ",", "\n", "output_dir", "=", "args", ".", "output_dir", ",", "\n", "qt_weight", "=", "args", ".", "qt_weight", ",", "\n", "re_weight", "=", "args", ".", "re_weight", ",", "\n", "references", "=", "references_lines", ",", "\n", "data_type", "=", "args", ".", "data_type", ",", "\n", "save_to_files", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.avg_std.get_answerability_score": [[27, 49], ["print", "range", "print", "numpy.savetxt", "numpy.savetxt", "len", "len", "new_scores.append", "print", "os.path.join", "os.path.join", "numpy.mean", "numpy.mean"], "function", ["None"], ["def", "get_answerability_score", "(", "all_scores", ",", "ner_weight", ",", "qt_weight", ",", "re_weight", ",", "d", ",", "ngram_metric", "=", "\"Bleu_4\"", ")", ":", "\n", "\t", "print", "(", "len", "(", "all_scores", ")", ")", "\n", "ref_scores", "=", "[", "x", "[", "'ref'", "]", "for", "x", "in", "all_scores", "]", "\n", "fluent_scores", "=", "[", "x", "[", "ngram_metric", "]", "for", "x", "in", "all_scores", "]", "\n", "imp_scores", "=", "[", "x", "[", "'imp'", "]", "for", "x", "in", "all_scores", "]", "\n", "qt_scores", "=", "[", "x", "[", "'qt'", "]", "for", "x", "in", "all_scores", "]", "\n", "sw_scores", "=", "[", "x", "[", "'sw'", "]", "for", "x", "in", "all_scores", "]", "\n", "ner_scores", "=", "[", "x", "[", "'ner'", "]", "for", "x", "in", "all_scores", "]", "\n", "\n", "new_scores", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "imp_scores", ")", ")", ":", "\n", "\t    ", "answerability", "=", "re_weight", "*", "imp_scores", "[", "i", "]", "+", "ner_weight", "*", "ner_scores", "[", "i", "]", "+", "qt_weight", "*", "qt_scores", "[", "i", "]", "+", "(", "1", "-", "re_weight", "-", "ner_weight", "-", "qt_weight", ")", "*", "sw_scores", "[", "i", "]", "\n", "\n", "temp", "=", "d", "*", "answerability", "+", "(", "1", "-", "d", ")", "*", "fluent_scores", "[", "i", "]", "\n", "new_scores", ".", "append", "(", "temp", ")", "\n", "print", "(", "\"New Score:{} Ner Score: {} RE Score {} SW Score {} QT Score {} \"", ".", "format", "(", "temp", ",", "ner_scores", "[", "i", "]", ",", "imp_scores", "[", "i", "]", ",", "sw_scores", "[", "i", "]", ",", "qt_scores", "[", "i", "]", ")", ")", "\n", "\n", "", "print", "(", "\"Mean Answerability Score Across Questions: {} N-gram Score: {}\"", ".", "format", "(", "np", ".", "mean", "(", "fluent_scores", ")", ",", "np", ".", "mean", "(", "new_scores", ")", ")", ")", "\n", "np", ".", "savetxt", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'ngram_scores.txt'", ")", ",", "fluent_scores", ")", "\n", "np", ".", "savetxt", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'answerability_scores.txt'", ")", ",", "new_scores", ")", "\n", ""]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu.Bleu.__init__": [[15, 20], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "n", "=", "4", ")", ":", "\n", "# default compute Blue score up to 4", "\n", "        ", "self", ".", "_n", "=", "n", "\n", "self", ".", "_hypo_for_image", "=", "{", "}", "\n", "self", ".", "ref_for_image", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu.Bleu.compute_score": [[21, 45], ["gts.keys", "bleu_scorer.BleuScorer.BleuScorer", "bleu_scorer.BleuScorer.BleuScorer.compute_score", "gts.keys", "res.keys", "type", "len", "type", "len"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.Rouge.compute_score"], ["", "def", "compute_score", "(", "self", ",", "gts", ",", "res", ")", ":", "\n", "\n", "        ", "assert", "(", "gts", ".", "keys", "(", ")", "==", "res", ".", "keys", "(", ")", ")", "\n", "imgIds", "=", "gts", ".", "keys", "(", ")", "\n", "\n", "bleu_scorer", "=", "BleuScorer", "(", "n", "=", "self", ".", "_n", ")", "\n", "for", "id", "in", "imgIds", ":", "\n", "            ", "hypo", "=", "res", "[", "id", "]", "\n", "ref", "=", "gts", "[", "id", "]", "\n", "\n", "# Sanity check.", "\n", "assert", "(", "type", "(", "hypo", ")", "is", "list", ")", "\n", "assert", "(", "len", "(", "hypo", ")", "==", "1", ")", "\n", "assert", "(", "type", "(", "ref", ")", "is", "list", ")", "\n", "assert", "(", "len", "(", "ref", ")", ">=", "1", ")", "\n", "\n", "bleu_scorer", "+=", "(", "hypo", "[", "0", "]", ",", "ref", ")", "\n", "\n", "#score, scores = bleu_scorer.compute_score(option='shortest')", "\n", "", "score", ",", "scores", "=", "bleu_scorer", ".", "compute_score", "(", "option", "=", "'closest'", ",", "verbose", "=", "0", ")", "\n", "#score, scores = bleu_scorer.compute_score(option='average', verbose=1)", "\n", "\n", "# return (bleu, bleu_info)", "\n", "return", "score", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu.Bleu.method": [[46, 48], ["None"], "methods", ["None"], ["", "def", "method", "(", "self", ")", ":", "\n", "        ", "return", "\"Bleu\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.copy": [[97, 104], ["bleu_scorer.BleuScorer", "copy.copy", "copy.copy"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.copy", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.copy"], ["def", "copy", "(", "self", ")", ":", "\n", "        ", "''' copy the refs.'''", "\n", "new", "=", "BleuScorer", "(", "n", "=", "self", ".", "n", ")", "\n", "new", ".", "ctest", "=", "copy", ".", "copy", "(", "self", ".", "ctest", ")", "\n", "new", ".", "crefs", "=", "copy", ".", "copy", "(", "self", ".", "crefs", ")", "\n", "new", ".", "_score", "=", "None", "\n", "return", "new", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.__init__": [[105, 113], ["bleu_scorer.BleuScorer.cook_append"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.cook_append"], ["", "def", "__init__", "(", "self", ",", "test", "=", "None", ",", "refs", "=", "None", ",", "n", "=", "4", ",", "special_reflen", "=", "None", ")", ":", "\n", "        ", "''' singular instance '''", "\n", "\n", "self", ".", "n", "=", "n", "\n", "self", ".", "crefs", "=", "[", "]", "\n", "self", ".", "ctest", "=", "[", "]", "\n", "self", ".", "cook_append", "(", "test", ",", "refs", ")", "\n", "self", ".", "special_reflen", "=", "special_reflen", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.cook_append": [[114, 126], ["bleu_scorer.BleuScorer.crefs.append", "bleu_scorer.cook_refs", "bleu_scorer.cook_test", "bleu_scorer.BleuScorer.ctest.append", "bleu_scorer.BleuScorer.ctest.append"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.cook_refs", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.cook_test"], ["", "def", "cook_append", "(", "self", ",", "test", ",", "refs", ")", ":", "\n", "        ", "'''called by constructor and __iadd__ to avoid creating new instances.'''", "\n", "\n", "if", "refs", "is", "not", "None", ":", "\n", "            ", "self", ".", "crefs", ".", "append", "(", "cook_refs", "(", "refs", ")", ")", "\n", "if", "test", "is", "not", "None", ":", "\n", "                ", "cooked_test", "=", "cook_test", "(", "test", ",", "self", ".", "crefs", "[", "-", "1", "]", ")", "\n", "self", ".", "ctest", ".", "append", "(", "cooked_test", ")", "## N.B.: -1", "\n", "", "else", ":", "\n", "                ", "self", ".", "ctest", ".", "append", "(", "None", ")", "# lens of crefs and ctest have to match", "\n", "\n", "", "", "self", ".", "_score", "=", "None", "## need to recompute", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.ratio": [[127, 130], ["bleu_scorer.BleuScorer.compute_score"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.Rouge.compute_score"], ["", "def", "ratio", "(", "self", ",", "option", "=", "None", ")", ":", "\n", "        ", "self", ".", "compute_score", "(", "option", "=", "option", ")", "\n", "return", "self", ".", "_ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.score_ratio": [[131, 134], ["bleu_scorer.BleuScorer.fscore", "bleu_scorer.BleuScorer.ratio"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.ratio"], ["", "def", "score_ratio", "(", "self", ",", "option", "=", "None", ")", ":", "\n", "        ", "'''return (bleu, len_ratio) pair'''", "\n", "return", "(", "self", ".", "fscore", "(", "option", "=", "option", ")", ",", "self", ".", "ratio", "(", "option", "=", "option", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.score_ratio_str": [[135, 137], ["bleu_scorer.BleuScorer.score_ratio"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.score_ratio"], ["", "def", "score_ratio_str", "(", "self", ",", "option", "=", "None", ")", ":", "\n", "        ", "return", "\"%.4f (%.2f)\"", "%", "self", ".", "score_ratio", "(", "option", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.reflen": [[138, 141], ["bleu_scorer.BleuScorer.compute_score"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.Rouge.compute_score"], ["", "def", "reflen", "(", "self", ",", "option", "=", "None", ")", ":", "\n", "        ", "self", ".", "compute_score", "(", "option", "=", "option", ")", "\n", "return", "self", ".", "_reflen", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.testlen": [[142, 145], ["bleu_scorer.BleuScorer.compute_score"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.Rouge.compute_score"], ["", "def", "testlen", "(", "self", ",", "option", "=", "None", ")", ":", "\n", "        ", "self", ".", "compute_score", "(", "option", "=", "option", ")", "\n", "return", "self", ".", "_testlen", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.retest": [[146, 156], ["zip", "type", "len", "len", "bleu_scorer.BleuScorer.ctest.append", "bleu_scorer.cook_test"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.cook_test"], ["", "def", "retest", "(", "self", ",", "new_test", ")", ":", "\n", "        ", "if", "type", "(", "new_test", ")", "is", "str", ":", "\n", "            ", "new_test", "=", "[", "new_test", "]", "\n", "", "assert", "len", "(", "new_test", ")", "==", "len", "(", "self", ".", "crefs", ")", ",", "new_test", "\n", "self", ".", "ctest", "=", "[", "]", "\n", "for", "t", ",", "rs", "in", "zip", "(", "new_test", ",", "self", ".", "crefs", ")", ":", "\n", "            ", "self", ".", "ctest", ".", "append", "(", "cook_test", "(", "t", ",", "rs", ")", ")", "\n", "", "self", ".", "_score", "=", "None", "\n", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.rescore": [[157, 161], ["bleu_scorer.BleuScorer.retest().compute_score", "bleu_scorer.BleuScorer.retest"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.Rouge.compute_score", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.retest"], ["", "def", "rescore", "(", "self", ",", "new_test", ")", ":", "\n", "        ", "''' replace test(s) with new test(s), and returns the new score.'''", "\n", "\n", "return", "self", ".", "retest", "(", "new_test", ")", ".", "compute_score", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.size": [[162, 165], ["len", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "size", "(", "self", ")", ":", "\n", "        ", "assert", "len", "(", "self", ".", "crefs", ")", "==", "len", "(", "self", ".", "ctest", ")", ",", "\"refs/test mismatch! %d<>%d\"", "%", "(", "len", "(", "self", ".", "crefs", ")", ",", "len", "(", "self", ".", "ctest", ")", ")", "\n", "return", "len", "(", "self", ".", "crefs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.__iadd__": [[166, 179], ["type", "bleu_scorer.BleuScorer.cook_append", "bleu_scorer.BleuScorer.compatible", "bleu_scorer.BleuScorer.ctest.extend", "bleu_scorer.BleuScorer.crefs.extend"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.cook_append", "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.compatible"], ["", "def", "__iadd__", "(", "self", ",", "other", ")", ":", "\n", "        ", "'''add an instance (e.g., from another sentence).'''", "\n", "\n", "if", "type", "(", "other", ")", "is", "tuple", ":", "\n", "## avoid creating new BleuScorer instances", "\n", "            ", "self", ".", "cook_append", "(", "other", "[", "0", "]", ",", "other", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "assert", "self", ".", "compatible", "(", "other", ")", ",", "\"incompatible BLEUs.\"", "\n", "self", ".", "ctest", ".", "extend", "(", "other", ".", "ctest", ")", "\n", "self", ".", "crefs", ".", "extend", "(", "other", ".", "crefs", ")", "\n", "self", ".", "_score", "=", "None", "## need to recompute", "\n", "\n", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.compatible": [[180, 182], ["isinstance"], "methods", ["None"], ["", "def", "compatible", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "isinstance", "(", "other", ",", "BleuScorer", ")", "and", "self", ".", "n", "==", "other", ".", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.single_reflen": [[183, 185], ["bleu_scorer.BleuScorer._single_reflen"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer._single_reflen"], ["", "def", "single_reflen", "(", "self", ",", "option", "=", "\"average\"", ")", ":", "\n", "        ", "return", "self", ".", "_single_reflen", "(", "self", ".", "crefs", "[", "0", "]", "[", "0", "]", ",", "option", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer._single_reflen": [[186, 198], ["min", "float", "len", "sum", "min", "abs"], "methods", ["None"], ["", "def", "_single_reflen", "(", "self", ",", "reflens", ",", "option", "=", "None", ",", "testlen", "=", "None", ")", ":", "\n", "\n", "        ", "if", "option", "==", "\"shortest\"", ":", "\n", "            ", "reflen", "=", "min", "(", "reflens", ")", "\n", "", "elif", "option", "==", "\"average\"", ":", "\n", "            ", "reflen", "=", "float", "(", "sum", "(", "reflens", ")", ")", "/", "len", "(", "reflens", ")", "\n", "", "elif", "option", "==", "\"closest\"", ":", "\n", "            ", "reflen", "=", "min", "(", "(", "abs", "(", "l", "-", "testlen", ")", ",", "l", ")", "for", "l", "in", "reflens", ")", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "\"unsupported reflen option %s\"", "%", "option", "\n", "\n", "", "return", "reflen", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.recompute_score": [[199, 202], ["bleu_scorer.BleuScorer.compute_score"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.Rouge.compute_score"], ["", "def", "recompute_score", "(", "self", ",", "option", "=", "None", ",", "verbose", "=", "0", ")", ":", "\n", "        ", "self", ".", "_score", "=", "None", "\n", "return", "self", ".", "compute_score", "(", "option", ",", "verbose", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.compute_score": [[203, 269], ["six.moves.xrange", "six.moves.xrange", "bleus.append", "six.moves.xrange", "print", "print", "six.moves.xrange", "bleu_scorer.BleuScorer._single_reflen", "six.moves.xrange", "bleu_list[].append", "six.moves.xrange", "print", "float", "math.exp", "len", "math.exp", "float", "float"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer._single_reflen"], ["", "def", "compute_score", "(", "self", ",", "option", "=", "None", ",", "verbose", "=", "0", ")", ":", "\n", "        ", "n", "=", "self", ".", "n", "\n", "small", "=", "1e-9", "\n", "tiny", "=", "1e-15", "## so that if guess is 0 still return 0", "\n", "bleu_list", "=", "[", "[", "]", "for", "_", "in", "range", "(", "n", ")", "]", "\n", "\n", "if", "self", ".", "_score", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_score", "\n", "\n", "", "if", "option", "is", "None", ":", "\n", "            ", "option", "=", "\"average\"", "if", "len", "(", "self", ".", "crefs", ")", "==", "1", "else", "\"closest\"", "\n", "\n", "", "self", ".", "_testlen", "=", "0", "\n", "self", ".", "_reflen", "=", "0", "\n", "totalcomps", "=", "{", "'testlen'", ":", "0", ",", "'reflen'", ":", "0", ",", "'guess'", ":", "[", "0", "]", "*", "n", ",", "'correct'", ":", "[", "0", "]", "*", "n", "}", "\n", "\n", "# for each sentence", "\n", "for", "comps", "in", "self", ".", "ctest", ":", "\n", "            ", "testlen", "=", "comps", "[", "'testlen'", "]", "\n", "self", ".", "_testlen", "+=", "testlen", "\n", "\n", "if", "self", ".", "special_reflen", "is", "None", ":", "## need computation", "\n", "                ", "reflen", "=", "self", ".", "_single_reflen", "(", "comps", "[", "'reflen'", "]", ",", "option", ",", "testlen", ")", "\n", "", "else", ":", "\n", "                ", "reflen", "=", "self", ".", "special_reflen", "\n", "\n", "", "self", ".", "_reflen", "+=", "reflen", "\n", "\n", "for", "key", "in", "[", "'guess'", ",", "'correct'", "]", ":", "\n", "                ", "for", "k", "in", "range", "(", "n", ")", ":", "\n", "                    ", "totalcomps", "[", "key", "]", "[", "k", "]", "+=", "comps", "[", "key", "]", "[", "k", "]", "\n", "\n", "# append per image bleu score", "\n", "", "", "bleu", "=", "1.", "\n", "for", "k", "in", "range", "(", "n", ")", ":", "\n", "                ", "bleu", "*=", "(", "float", "(", "comps", "[", "'correct'", "]", "[", "k", "]", ")", "+", "tiny", ")", "/", "(", "float", "(", "comps", "[", "'guess'", "]", "[", "k", "]", ")", "+", "small", ")", "\n", "bleu_list", "[", "k", "]", ".", "append", "(", "bleu", "**", "(", "1.", "/", "(", "k", "+", "1", ")", ")", ")", "\n", "", "ratio", "=", "(", "testlen", "+", "tiny", ")", "/", "(", "reflen", "+", "small", ")", "## N.B.: avoid zero division", "\n", "if", "ratio", "<", "1", ":", "\n", "                ", "for", "k", "in", "range", "(", "n", ")", ":", "\n", "                    ", "bleu_list", "[", "k", "]", "[", "-", "1", "]", "*=", "math", ".", "exp", "(", "1", "-", "1", "/", "ratio", ")", "\n", "\n", "", "", "if", "verbose", ">", "1", ":", "\n", "                ", "print", "(", "\"{}, {}\"", ".", "format", "(", "comps", ",", "reflen", ")", ")", "\n", "\n", "", "", "totalcomps", "[", "'reflen'", "]", "=", "self", ".", "_reflen", "\n", "totalcomps", "[", "'testlen'", "]", "=", "self", ".", "_testlen", "\n", "\n", "bleus", "=", "[", "]", "\n", "bleu", "=", "1.", "\n", "for", "k", "in", "range", "(", "n", ")", ":", "\n", "            ", "bleu", "*=", "float", "(", "totalcomps", "[", "'correct'", "]", "[", "k", "]", "+", "tiny", ")", "/", "(", "totalcomps", "[", "'guess'", "]", "[", "k", "]", "+", "small", ")", "\n", "bleus", ".", "append", "(", "bleu", "**", "(", "1.", "/", "(", "k", "+", "1", ")", ")", ")", "\n", "", "ratio", "=", "(", "self", ".", "_testlen", "+", "tiny", ")", "/", "(", "self", ".", "_reflen", "+", "small", ")", "## N.B.: avoid zero division", "\n", "if", "ratio", "<", "1", ":", "\n", "            ", "for", "k", "in", "range", "(", "n", ")", ":", "\n", "                ", "bleus", "[", "k", "]", "*=", "math", ".", "exp", "(", "1", "-", "1", "/", "ratio", ")", "\n", "\n", "", "", "if", "verbose", ">", "0", ":", "\n", "            ", "print", "(", "totalcomps", ")", "\n", "print", "(", "\"ratio: {}\"", ".", "format", "(", "ratio", ")", ")", "\n", "\n", "", "self", ".", "_score", "=", "bleus", "\n", "return", "self", ".", "_score", ",", "bleu_list", "\n", "", "", ""]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.precook": [[27, 38], ["s.split", "collections.defaultdict", "six.moves.xrange", "six.moves.xrange", "len", "tuple", "len"], "function", ["None"], ["def", "precook", "(", "s", ",", "n", "=", "4", ",", "out", "=", "False", ")", ":", "\n", "    ", "\"\"\"Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.\"\"\"", "\n", "words", "=", "s", ".", "split", "(", ")", "\n", "counts", "=", "defaultdict", "(", "int", ")", "\n", "for", "k", "in", "range", "(", "1", ",", "n", "+", "1", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "words", ")", "-", "k", "+", "1", ")", ":", "\n", "            ", "ngram", "=", "tuple", "(", "words", "[", "i", ":", "i", "+", "k", "]", ")", "\n", "counts", "[", "ngram", "]", "+=", "1", "\n", "", "", "return", "(", "len", "(", "words", ")", ",", "counts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.cook_refs": [[39, 63], ["bleu_scorer.precook", "min.append", "six.iteritems", "min", "max", "maxcounts.get", "float", "len", "sum"], "function", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.precook"], ["", "def", "cook_refs", "(", "refs", ",", "eff", "=", "None", ",", "n", "=", "4", ")", ":", "## lhuang: oracle will call with \"average\"", "\n", "    ", "'''Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.'''", "\n", "\n", "reflen", "=", "[", "]", "\n", "maxcounts", "=", "{", "}", "\n", "for", "ref", "in", "refs", ":", "\n", "        ", "rl", ",", "counts", "=", "precook", "(", "ref", ",", "n", ")", "\n", "reflen", ".", "append", "(", "rl", ")", "\n", "for", "(", "ngram", ",", "count", ")", "in", "six", ".", "iteritems", "(", "counts", ")", ":", "\n", "            ", "maxcounts", "[", "ngram", "]", "=", "max", "(", "maxcounts", ".", "get", "(", "ngram", ",", "0", ")", ",", "count", ")", "\n", "\n", "# Calculate effective reference sentence length.", "\n", "", "", "if", "eff", "==", "\"shortest\"", ":", "\n", "        ", "reflen", "=", "min", "(", "reflen", ")", "\n", "", "elif", "eff", "==", "\"average\"", ":", "\n", "        ", "reflen", "=", "float", "(", "sum", "(", "reflen", ")", ")", "/", "len", "(", "reflen", ")", "\n", "\n", "## lhuang: N.B.: leave reflen computaiton to the very end!!", "\n", "\n", "## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)", "\n", "\n", "", "return", "(", "reflen", ",", "maxcounts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.cook_test": [[64, 89], ["bleu_scorer.precook", "six.iteritems", "max", "min", "min", "six.moves.xrange", "refmaxcounts.get", "len", "abs"], "function", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.precook"], ["", "def", "cook_test", "(", "test", ",", "reflen_refmaxcounts", ",", "eff", "=", "None", ",", "n", "=", "4", ")", ":", "\n", "    ", "'''Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.'''", "\n", "\n", "reflen", ",", "refmaxcounts", "=", "reflen_refmaxcounts", "\n", "testlen", ",", "counts", "=", "precook", "(", "test", ",", "n", ",", "True", ")", "\n", "\n", "result", "=", "{", "}", "\n", "\n", "# Calculate effective reference sentence length.", "\n", "\n", "if", "eff", "==", "\"closest\"", ":", "\n", "        ", "result", "[", "\"reflen\"", "]", "=", "min", "(", "(", "abs", "(", "l", "-", "testlen", ")", ",", "l", ")", "for", "l", "in", "reflen", ")", "[", "1", "]", "\n", "", "else", ":", "## i.e., \"average\" or \"shortest\" or None", "\n", "        ", "result", "[", "\"reflen\"", "]", "=", "reflen", "\n", "\n", "", "result", "[", "\"testlen\"", "]", "=", "testlen", "\n", "\n", "result", "[", "\"guess\"", "]", "=", "[", "max", "(", "0", ",", "testlen", "-", "k", "+", "1", ")", "for", "k", "in", "range", "(", "1", ",", "n", "+", "1", ")", "]", "\n", "\n", "result", "[", "'correct'", "]", "=", "[", "0", "]", "*", "n", "\n", "for", "(", "ngram", ",", "count", ")", "in", "six", ".", "iteritems", "(", "counts", ")", ":", "\n", "        ", "result", "[", "\"correct\"", "]", "[", "len", "(", "ngram", ")", "-", "1", "]", "+=", "min", "(", "refmaxcounts", ".", "get", "(", "ngram", ",", "0", ")", ",", "count", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.Rouge.__init__": [[40, 43], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "# vrama91: updated the value below based on discussion with Hovey", "\n", "        ", "self", ".", "beta", "=", "1.2", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.Rouge.calc_score": [[44, 75], ["candidate[].split", "max", "max", "len", "len", "reference.split", "rouge.my_lcs", "prec.append", "rec.append", "float", "float", "float", "len", "len"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.my_lcs"], ["", "def", "calc_score", "(", "self", ",", "candidate", ",", "refs", ")", ":", "\n", "        ", "\"\"\"\n        Compute ROUGE-L score given one candidate and references for an image\n        :param candidate: str : candidate sentence to be evaluated\n        :param refs: list of str : COCO reference sentences for the particular image to be evaluated\n        :returns score: int (ROUGE-L score for the candidate evaluated against references)\n        \"\"\"", "\n", "assert", "(", "len", "(", "candidate", ")", "==", "1", ")", "\n", "assert", "(", "len", "(", "refs", ")", ">", "0", ")", "\n", "prec", "=", "[", "]", "\n", "rec", "=", "[", "]", "\n", "\n", "# split into tokens", "\n", "token_c", "=", "candidate", "[", "0", "]", ".", "split", "(", "\" \"", ")", "\n", "\n", "for", "reference", "in", "refs", ":", "\n", "# split into tokens", "\n", "            ", "token_r", "=", "reference", ".", "split", "(", "\" \"", ")", "\n", "# compute the longest common subsequence", "\n", "lcs", "=", "my_lcs", "(", "token_r", ",", "token_c", ")", "\n", "prec", ".", "append", "(", "lcs", "/", "float", "(", "len", "(", "token_c", ")", ")", ")", "\n", "rec", ".", "append", "(", "lcs", "/", "float", "(", "len", "(", "token_r", ")", ")", ")", "\n", "\n", "", "prec_max", "=", "max", "(", "prec", ")", "\n", "rec_max", "=", "max", "(", "rec", ")", "\n", "\n", "if", "(", "prec_max", "!=", "0", "and", "rec_max", "!=", "0", ")", ":", "\n", "            ", "score", "=", "(", "(", "1", "+", "self", ".", "beta", "**", "2", ")", "*", "prec_max", "*", "rec_max", ")", "/", "float", "(", "rec_max", "+", "self", ".", "beta", "**", "2", "*", "prec_max", ")", "\n", "", "else", ":", "\n", "            ", "score", "=", "0.0", "\n", "", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.Rouge.compute_score": [[76, 102], ["gts.keys", "numpy.mean", "gts.keys", "res.keys", "score.append", "numpy.array", "numpy.array", "rouge.Rouge.calc_score", "type", "len", "type", "len"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.Rouge.calc_score"], ["", "def", "compute_score", "(", "self", ",", "gts", ",", "res", ")", ":", "\n", "        ", "\"\"\"\n        Computes Rouge-L score given a set of reference and candidate sentences for the dataset\n        Invoked by evaluate_captions.py \n        :param hypo_for_image: dict : candidate / test sentences with \"image name\" key and \"tokenized sentences\" as values \n        :param ref_for_image: dict : reference MS-COCO sentences with \"image name\" key and \"tokenized sentences\" as values\n        :returns: average_score: float (mean ROUGE-L score computed by averaging scores for all the images)\n        \"\"\"", "\n", "assert", "(", "gts", ".", "keys", "(", ")", "==", "res", ".", "keys", "(", ")", ")", "\n", "imgIds", "=", "gts", ".", "keys", "(", ")", "\n", "\n", "score", "=", "[", "]", "\n", "for", "id", "in", "imgIds", ":", "\n", "            ", "hypo", "=", "res", "[", "id", "]", "\n", "ref", "=", "gts", "[", "id", "]", "\n", "\n", "score", ".", "append", "(", "self", ".", "calc_score", "(", "hypo", ",", "ref", ")", ")", "\n", "\n", "# Sanity check.", "\n", "assert", "(", "type", "(", "hypo", ")", "is", "list", ")", "\n", "assert", "(", "len", "(", "hypo", ")", "==", "1", ")", "\n", "assert", "(", "type", "(", "ref", ")", "is", "list", ")", "\n", "assert", "(", "len", "(", "ref", ")", ">", "0", ")", "\n", "\n", "", "average_score", "=", "np", ".", "mean", "(", "np", ".", "array", "(", "score", ")", ")", "\n", "return", "average_score", ",", "np", ".", "array", "(", "score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.Rouge.method": [[103, 105], ["None"], "methods", ["None"], ["", "def", "method", "(", "self", ")", ":", "\n", "        ", "return", "\"Rouge\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.rouge.rouge.my_lcs": [[12, 34], ["range", "len", "len", "range", "range", "len", "len", "range", "len", "max", "len", "len", "len"], "function", ["None"], ["def", "my_lcs", "(", "string", ",", "sub", ")", ":", "\n", "    ", "\"\"\"\n    Calculates longest common subsequence for a pair of tokenized strings\n    :param string : list of str : tokens from a string split using whitespace\n    :param sub : list of str : shorter string, also split using whitespace\n    :returns: length (list of int): length of the longest common subsequence between the two strings\n\n    Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\n    \"\"\"", "\n", "if", "(", "len", "(", "string", ")", "<", "len", "(", "sub", ")", ")", ":", "\n", "        ", "sub", ",", "string", "=", "string", ",", "sub", "\n", "\n", "", "lengths", "=", "[", "[", "0", "for", "i", "in", "range", "(", "0", ",", "len", "(", "sub", ")", "+", "1", ")", "]", "for", "j", "in", "range", "(", "0", ",", "len", "(", "string", ")", "+", "1", ")", "]", "\n", "\n", "for", "j", "in", "range", "(", "1", ",", "len", "(", "sub", ")", "+", "1", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "1", ",", "len", "(", "string", ")", "+", "1", ")", ":", "\n", "            ", "if", "(", "string", "[", "i", "-", "1", "]", "==", "sub", "[", "j", "-", "1", "]", ")", ":", "\n", "                ", "lengths", "[", "i", "]", "[", "j", "]", "=", "lengths", "[", "i", "-", "1", "]", "[", "j", "-", "1", "]", "+", "1", "\n", "", "else", ":", "\n", "                ", "lengths", "[", "i", "]", "[", "j", "]", "=", "max", "(", "lengths", "[", "i", "-", "1", "]", "[", "j", "]", ",", "lengths", "[", "i", "]", "[", "j", "-", "1", "]", ")", "\n", "\n", "", "", "", "return", "lengths", "[", "len", "(", "string", ")", "]", "[", "len", "(", "sub", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.tests.test_answerability_score.test_get_answerability_scores_Bleu_1": [[6, 16], ["answerability_score.get_answerability_scores", "numpy.testing.assert_almost_equal", "numpy.testing.assert_almost_equal"], "function", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.get_answerability_scores"], ["def", "test_get_answerability_scores_Bleu_1", "(", ")", ":", "\n", "    ", "mean_answerability_score", ",", "mean_fluent_score", "=", "get_answerability_scores", "(", "hypotheses", "=", "[", "\"Is this a great question?\"", "]", ",", "\n", "references", "=", "[", "\"Is this a good question?\"", "]", ",", "\n", "ngram_metric", "=", "'Bleu_1'", ",", "\n", "delta", "=", "0.7", ",", "\n", "ner_weight", "=", "0.6", ",", "\n", "qt_weight", "=", "0.2", ",", "\n", "re_weight", "=", "0.1", ")", "\n", "assert_almost_equal", "(", "mean_answerability_score", ",", "0.485", ",", "decimal", "=", "3", ")", "\n", "assert_almost_equal", "(", "mean_fluent_score", ",", "0.800", ",", "decimal", "=", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.tests.test_answerability_score.test_get_answerability_scores_Bleu_2": [[18, 28], ["answerability_score.get_answerability_scores", "numpy.testing.assert_almost_equal", "numpy.testing.assert_almost_equal"], "function", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.get_answerability_scores"], ["", "def", "test_get_answerability_scores_Bleu_2", "(", ")", ":", "\n", "    ", "mean_answerability_score", ",", "mean_fluent_score", "=", "get_answerability_scores", "(", "hypotheses", "=", "[", "\"Is this a great question?\"", "]", ",", "\n", "references", "=", "[", "\"Is this a good question?\"", "]", ",", "\n", "ngram_metric", "=", "'Bleu_2'", ",", "\n", "delta", "=", "0.7", ",", "\n", "ner_weight", "=", "0.6", ",", "\n", "qt_weight", "=", "0.2", ",", "\n", "re_weight", "=", "0.1", ")", "\n", "assert_almost_equal", "(", "mean_answerability_score", ",", "0.435", ",", "decimal", "=", "3", ")", "\n", "assert_almost_equal", "(", "mean_fluent_score", ",", "0.632", ",", "decimal", "=", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.tests.test_answerability_score.test_get_answerability_scores_Bleu_3": [[30, 40], ["answerability_score.get_answerability_scores", "numpy.testing.assert_almost_equal", "numpy.testing.assert_almost_equal"], "function", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.get_answerability_scores"], ["", "def", "test_get_answerability_scores_Bleu_3", "(", ")", ":", "\n", "    ", "mean_answerability_score", ",", "mean_fluent_score", "=", "get_answerability_scores", "(", "hypotheses", "=", "[", "\"Is this a great question?\"", "]", ",", "\n", "references", "=", "[", "\"Is this a good question?\"", "]", ",", "\n", "ngram_metric", "=", "'Bleu_3'", ",", "\n", "delta", "=", "0.7", ",", "\n", "ner_weight", "=", "0.6", ",", "\n", "qt_weight", "=", "0.2", ",", "\n", "re_weight", "=", "0.1", ")", "\n", "assert_almost_equal", "(", "mean_answerability_score", ",", "0.398", ",", "decimal", "=", "3", ")", "\n", "assert_almost_equal", "(", "mean_fluent_score", ",", "0.511", ",", "decimal", "=", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.tests.test_answerability_score.test_get_answerability_scores_Rouge_L": [[42, 52], ["answerability_score.get_answerability_scores", "numpy.testing.assert_almost_equal", "numpy.testing.assert_almost_equal"], "function", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.None.answerability_score.get_answerability_scores"], ["", "def", "test_get_answerability_scores_Rouge_L", "(", ")", ":", "\n", "    ", "mean_answerability_score", ",", "mean_fluent_score", "=", "get_answerability_scores", "(", "hypotheses", "=", "[", "\"Is this a great question?\"", "]", ",", "\n", "references", "=", "[", "\"Is this a good question?\"", "]", ",", "\n", "ngram_metric", "=", "'ROUGE_L'", ",", "\n", "delta", "=", "0.7", ",", "\n", "ner_weight", "=", "0.6", ",", "\n", "qt_weight", "=", "0.2", ",", "\n", "re_weight", "=", "0.1", ")", "\n", "assert_almost_equal", "(", "mean_answerability_score", ",", "0.485", ",", "decimal", "=", "3", ")", "\n", "assert_almost_equal", "(", "mean_fluent_score", ",", "0.8", ",", "decimal", "=", "3", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.tests.test_ptbtokenizer.test_tokenize": [[4, 12], ["tokenizer.ptbtokenizer.PTBTokenizer", "tokenizer.ptbtokenizer.PTBTokenizer.tokenize", "dict", "dict", "dict", "dict", "dict"], "function", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.tokenizer.ptbtokenizer.PTBTokenizer.tokenize"], ["def", "test_tokenize", "(", ")", ":", "\n", "    ", "t", "=", "PTBTokenizer", "(", ")", "\n", "tokens", "=", "t", ".", "tokenize", "(", "dict", "(", "id1", "=", "[", "dict", "(", "caption", "=", "\"Is this a good question?\"", ")", ",", "\n", "dict", "(", "caption", "=", "\"Is this a better question?\"", ")", "]", ",", "\n", "id2", "=", "[", "dict", "(", "caption", "=", "\"How's this question?\"", ")", "]", ")", ")", "\n", "assert", "tokens", "==", "dict", "(", "id1", "=", "[", "'is this a good question'", ",", "\n", "'is this a better question'", "]", ",", "\n", "id2", "=", "[", "'how \\'s this question'", "]", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.tokenizer.ptbtokenizer.PTBTokenizer.__init__": [[29, 34], ["os.path.dirname", "os.path.abspath"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_cmd", "=", "[", "'java'", ",", "'-cp'", ",", "STANFORD_CORENLP_3_4_1_JAR", ",", "\n", "'edu.stanford.nlp.process.PTBTokenizer'", ",", "\n", "'-preserveLines'", ",", "'-lowerCase'", "]", "\n", "self", ".", "_path_to_jar_dirname", "=", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.tokenizer.ptbtokenizer.PTBTokenizer.tokenize": [[35, 69], ["os.environ.copy", "subprocess.Popen", "subprocess.Popen.communicate", "token_lines.decode().split", "zip", "sentences.encode", "_logger.isEnabledFor", "_logger.debug", "final_tokenized_captions_for_image[].append", "captions_for_image.items", "range", "c[].replace", "err.decode().rstrip", "token_lines.decode", "len", "captions_for_image.values", "err.decode", "line.rstrip().split", "line.rstrip"], "methods", ["home.repos.pwc.inspect_result.PrekshaNema25_Answerability-Metric.bleu.bleu_scorer.BleuScorer.copy"], ["", "def", "tokenize", "(", "self", ",", "captions_for_image", ")", ":", "\n", "# ======================================================", "\n", "# prepare data for PTB Tokenizer", "\n", "# ======================================================", "\n", "        ", "final_tokenized_captions_for_image", "=", "{", "}", "\n", "image_id", "=", "[", "k", "for", "k", ",", "v", "in", "captions_for_image", ".", "items", "(", ")", "for", "_", "in", "range", "(", "len", "(", "v", ")", ")", "]", "\n", "\n", "sentences", "=", "'\\n'", ".", "join", "(", "[", "c", "[", "'caption'", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", "for", "v", "in", "captions_for_image", ".", "values", "(", ")", "for", "c", "in", "v", "]", ")", "\n", "\n", "# ======================================================", "\n", "# tokenize sentence", "\n", "# ======================================================", "\n", "env", "=", "os", ".", "environ", ".", "copy", "(", ")", "\n", "env", "[", "'LC_ALL'", "]", "=", "\"C\"", "\n", "p_tokenizer", "=", "subprocess", ".", "Popen", "(", "self", ".", "_cmd", ",", "cwd", "=", "self", ".", "_path_to_jar_dirname", ",", "\n", "env", "=", "env", ",", "\n", "stdin", "=", "subprocess", ".", "PIPE", ",", "\n", "stdout", "=", "subprocess", ".", "PIPE", ",", "stderr", "=", "subprocess", ".", "PIPE", ")", "\n", "token_lines", ",", "err", "=", "p_tokenizer", ".", "communicate", "(", "sentences", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "if", "err", "and", "_logger", ".", "isEnabledFor", "(", "logging", ".", "DEBUG", ")", ":", "\n", "            ", "_logger", ".", "debug", "(", "err", ".", "decode", "(", "'utf-8'", ")", ".", "rstrip", "(", ")", ")", "\n", "", "lines", "=", "token_lines", ".", "decode", "(", "'utf-8'", ")", ".", "split", "(", "'\\n'", ")", "\n", "\n", "# ======================================================", "\n", "# create dictionary for tokenized captions", "\n", "# ======================================================", "\n", "for", "k", ",", "line", "in", "zip", "(", "image_id", ",", "lines", ")", ":", "\n", "            ", "if", "not", "k", "in", "final_tokenized_captions_for_image", ":", "\n", "                ", "final_tokenized_captions_for_image", "[", "k", "]", "=", "[", "]", "\n", "", "tokenized_caption", "=", "' '", ".", "join", "(", "[", "w", "for", "w", "in", "line", ".", "rstrip", "(", ")", ".", "split", "(", "' '", ")", "if", "w", "not", "in", "PUNCTUATIONS", "]", ")", "\n", "final_tokenized_captions_for_image", "[", "k", "]", ".", "append", "(", "tokenized_caption", ")", "\n", "\n", "", "return", "final_tokenized_captions_for_image", "\n", "", "", ""]]}