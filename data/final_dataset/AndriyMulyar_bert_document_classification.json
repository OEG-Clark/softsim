{"home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.None.setup.PyTest.initialize_options": [[21, 24], ["setuptools.command.test.test.initialize_options"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.None.setup.PyTest.initialize_options"], ["def", "initialize_options", "(", "self", ")", ":", "\n", "        ", "TestCommand", ".", "initialize_options", "(", "self", ")", "\n", "self", ".", "pytest_args", "=", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.None.setup.PyTest.run_tests": [[25, 32], ["pytest.main", "sys.exit", "shlex.split"], "methods", ["None"], ["", "def", "run_tests", "(", "self", ")", ":", "\n", "        ", "import", "shlex", "\n", "# import here, cause outside the eggs aren't loaded", "\n", "import", "pytest", "\n", "\n", "errno", "=", "pytest", ".", "main", "(", "shlex", ".", "split", "(", "self", ".", "pytest_args", ")", ")", "\n", "sys", ".", "exit", "(", "errno", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.None.setup.readme": [[8, 11], ["open", "f.read"], "function", ["None"], ["def", "readme", "(", ")", ":", "\n", "    ", "with", "open", "(", "'README.md'", ")", "as", "f", ":", "\n", "        ", "return", "f", ".", "read", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.examples.training._initialize_arguments": [[26, 89], ["p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.parse_args", "os.path.join", "os.makedirs", "log.handlers.clear", "logging.Formatter", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.FileHandler.setFormatter", "log.addHandler", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "log.setLevel", "log.addHandler", "log.info", "time.strftime", "os.path.join", "p.format_values", "torch.cuda.is_available", "log.info", "p.parse_args.labels.split", "torch.cuda.device_count", "log.info", "log.info", "socket.gethostname", "torch.cuda.device_count", "torch.cuda.current_device"], "function", ["None"], ["def", "_initialize_arguments", "(", "p", ":", "configargparse", ".", "ArgParser", ")", ":", "\n", "    ", "p", ".", "add", "(", "'--model_storage_directory'", ",", "help", "=", "'The directory caching all model runs'", ")", "\n", "p", ".", "add", "(", "'--bert_model_path'", ",", "help", "=", "'Model path to BERT'", ")", "\n", "p", ".", "add", "(", "'--labels'", ",", "help", "=", "'Numbers of labels to predict over'", ",", "type", "=", "str", ")", "\n", "p", ".", "add", "(", "'--architecture'", ",", "help", "=", "'Training architecture'", ",", "type", "=", "str", ")", "\n", "p", ".", "add", "(", "'--freeze_bert'", ",", "help", "=", "'Whether to freeze bert'", ",", "type", "=", "bool", ")", "\n", "\n", "p", ".", "add", "(", "'--batch_size'", ",", "help", "=", "'Batch size for training multi-label document classifier'", ",", "type", "=", "int", ")", "\n", "p", ".", "add", "(", "'--bert_batch_size'", ",", "help", "=", "'Batch size for feeding 510 token subsets of documents through BERT'", ",", "type", "=", "int", ")", "\n", "p", ".", "add", "(", "'--epochs'", ",", "help", "=", "'Epochs to train'", ",", "type", "=", "int", ")", "\n", "#Optimizer arguments", "\n", "p", ".", "add", "(", "'--learning_rate'", ",", "help", "=", "'Optimizer step size'", ",", "type", "=", "float", ")", "\n", "p", ".", "add", "(", "'--weight_decay'", ",", "help", "=", "'Adam regularization'", ",", "type", "=", "float", ")", "\n", "\n", "p", ".", "add", "(", "'--evaluation_interval'", ",", "help", "=", "'Evaluate model on test set every evaluation_interval epochs'", ",", "type", "=", "int", ")", "\n", "p", ".", "add", "(", "'--checkpoint_interval'", ",", "help", "=", "'Save a model checkpoint to disk every checkpoint_interval epochs'", ",", "type", "=", "int", ")", "\n", "\n", "#Non-config arguments", "\n", "p", ".", "add", "(", "'--cuda'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Utilize GPU for training or prediction'", ")", "\n", "p", ".", "add", "(", "'--device'", ")", "\n", "p", ".", "add", "(", "'--timestamp'", ",", "help", "=", "'Run specific signature'", ")", "\n", "p", ".", "add", "(", "'--model_directory'", ",", "help", "=", "'The directory storing this model run, a sub-directory of model_storage_directory'", ")", "\n", "p", ".", "add", "(", "'--use_tensorboard'", ",", "help", "=", "'Use tensorboard logging'", ",", "type", "=", "bool", ")", "\n", "args", "=", "p", ".", "parse_args", "(", ")", "\n", "\n", "args", ".", "labels", "=", "[", "x", "for", "x", "in", "args", ".", "labels", ".", "split", "(", "', '", ")", "]", "\n", "\n", "\n", "\n", "\n", "\n", "#Set run specific envirorment configurations", "\n", "args", ".", "timestamp", "=", "time", ".", "strftime", "(", "\"run_%Y_%m_%d_%H_%M_%S\"", ")", "+", "\"_{machine}\"", ".", "format", "(", "machine", "=", "socket", ".", "gethostname", "(", ")", ")", "\n", "args", ".", "model_directory", "=", "os", ".", "path", ".", "join", "(", "args", ".", "model_storage_directory", ",", "args", ".", "timestamp", ")", "#directory", "\n", "os", ".", "makedirs", "(", "args", ".", "model_directory", ",", "exist_ok", "=", "True", ")", "\n", "\n", "#Handle logging configurations", "\n", "log", ".", "handlers", ".", "clear", "(", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "'%(message)s'", ")", "\n", "fh", "=", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_directory", ",", "\"log.txt\"", ")", ")", "\n", "fh", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "fh", ".", "setFormatter", "(", "formatter", ")", "\n", "log", ".", "addHandler", "(", "fh", ")", "\n", "ch", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "ch", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "ch", ".", "setFormatter", "(", "formatter", ")", "\n", "log", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "log", ".", "addHandler", "(", "ch", ")", "\n", "log", ".", "info", "(", "p", ".", "format_values", "(", ")", ")", "\n", "\n", "\n", "#Set global GPU state", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "cuda", ":", "\n", "        ", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "\n", "            ", "log", ".", "info", "(", "\"Using %i CUDA devices\"", "%", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "log", ".", "info", "(", "\"Using CUDA device:{0}\"", ".", "format", "(", "torch", ".", "cuda", ".", "current_device", "(", ")", ")", ")", "\n", "", "args", ".", "device", "=", "'cuda'", "\n", "", "else", ":", "\n", "        ", "log", ".", "info", "(", "\"Not using CUDA :(\"", ")", "\n", "args", ".", "dev", "=", "'cpu'", "\n", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.ml4health_2019_replication.data.load_n2c2_2006_train_dev_split": [[7, 25], ["list", "numpy.random.seed", "numpy.random.shuffle", "labels.keys", "data.load_n2c2_2006", "labels[].append", "tuple", "int", "int", "len", "len"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.ml4health_2019_replication.data.load_n2c2_2006"], ["def", "load_n2c2_2006_train_dev_split", "(", ")", ":", "\n", "    ", "train", "=", "list", "(", "load_n2c2_2006", "(", "partition", "=", "'train'", ")", ")", "\n", "numpy", ".", "random", ".", "seed", "(", "0", ")", "\n", "numpy", ".", "random", ".", "shuffle", "(", "train", ")", "\n", "\n", "labels", "=", "{", "}", "\n", "for", "id", ",", "doc", ",", "label", "in", "train", ":", "\n", "        ", "if", "label", "not", "in", "labels", ":", "\n", "            ", "labels", "[", "label", "]", "=", "[", "]", "\n", "", "labels", "[", "label", "]", ".", "append", "(", "tuple", "(", "(", "id", ",", "doc", ",", "label", ")", ")", ")", "\n", "\n", "", "dev", "=", "[", "]", "\n", "train", "=", "[", "]", "\n", "for", "label", "in", "labels", ".", "keys", "(", ")", ":", "\n", "        ", "dev", "+=", "labels", "[", "label", "]", "[", ":", "int", "(", "len", "(", "labels", "[", "label", "]", ")", "*", ".2", ")", "]", "\n", "train", "+=", "labels", "[", "label", "]", "[", "int", "(", "len", "(", "labels", "[", "label", "]", ")", "*", ".2", ")", ":", "]", "\n", "\n", "", "return", "train", ",", "dev", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.ml4health_2019_replication.data.load_n2c2_2006": [[26, 49], ["xml.fromstring", "ET.fromstring.findall", "zip", "open", "raw.read().strip", "ids.append", "notes.append", "labels.append", "raw.read", "document.findall", "document.findall"], "function", ["None"], ["", "def", "load_n2c2_2006", "(", "partition", "=", "'train'", ")", ":", "\n", "    ", "\"\"\"\n    Yields a generator of id, doc, label tuples.\n    :param partition:\n    :return:\n    \"\"\"", "\n", "assert", "partition", "in", "[", "'train'", ",", "'test'", "]", "\n", "\n", "with", "open", "(", "\"data/smokers_surrogate_%s_all_version2.xml\"", "%", "partition", ")", "as", "raw", ":", "\n", "        ", "file", "=", "raw", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "# file = resource_string('clinical_data', 'phenotyping/n2c2_2006/smokers_surrogate_%s_all_version2.xml' % partition).decode('utf-8').strip()", "\n", "", "root", "=", "ET", ".", "fromstring", "(", "file", ")", "\n", "ids", "=", "[", "]", "\n", "notes", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "documents", "=", "root", ".", "findall", "(", "\"./RECORD\"", ")", "\n", "for", "document", "in", "documents", ":", "\n", "        ", "ids", ".", "append", "(", "document", ".", "attrib", "[", "'ID'", "]", ")", "\n", "notes", ".", "append", "(", "document", ".", "findall", "(", "'./TEXT'", ")", "[", "0", "]", ".", "text", ")", "\n", "labels", ".", "append", "(", "document", ".", "findall", "(", "'./SMOKING'", ")", "[", "0", "]", ".", "attrib", "[", "'STATUS'", "]", ")", "\n", "\n", "", "for", "id", ",", "note", ",", "label", "in", "zip", "(", "ids", ",", "notes", ",", "labels", ")", ":", "\n", "        ", "yield", "(", "id", ",", "note", ",", "label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.ml4health_2019_replication.data.load_n2c2_2008_train_dev_split": [[51, 55], ["list", "data.load_n2c2_2008", "int", "int", "len", "len"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.ml4health_2019_replication.data.load_n2c2_2008"], ["", "", "def", "load_n2c2_2008_train_dev_split", "(", ")", ":", "\n", "    ", "train", "=", "list", "(", "load_n2c2_2008", "(", "partition", "=", "'train'", ")", ")", "\n", "\n", "return", "train", "[", ":", "int", "(", "len", "(", "train", ")", "*", ".8", ")", "]", ",", "train", "[", "int", "(", "len", "(", "train", ")", "*", ".8", ")", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.ml4health_2019_replication.data.load_n2c2_2008": [[57, 134], ["set", "tuple", "tuple", "list", "xml.fromstring", "ET.fromstring.findall", "xml.fromstring", "ET.fromstring.findall", "open", "open", "t1.read().strip", "t2.read().strip", "ET.fromstring.findall", "open", "open", "open", "open", "t1.read().strip", "t2.read().strip", "t3.read().strip", "t4.read().strip", "isinstance", "diseases_annotation.findall", "open", "t1.read().strip", "open", "t1.read().strip", "list.add", "disease.findall", "t1.read", "t2.read", "document.findall", "t1.read", "t2.read", "t3.read", "t4.read", "t1.read", "t1.read"], "function", ["None"], ["", "def", "load_n2c2_2008", "(", "partition", "=", "'train'", ")", ":", "\n", "    ", "assert", "partition", "in", "[", "'train'", ",", "'test'", "]", "\n", "documents", "=", "{", "}", "#id : text", "\n", "all_diseases", "=", "set", "(", ")", "\n", "notes", "=", "tuple", "(", ")", "\n", "if", "partition", "==", "'train'", ":", "\n", "        ", "with", "open", "(", "'data/obesity_patient_records_training.xml'", ")", "as", "t1", ",", "open", "(", "'data/obesity_patient_records_training2.xml'", ")", "as", "t2", ":", "\n", "            ", "notes1", "=", "t1", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "notes2", "=", "t2", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "", "notes", "=", "(", "notes1", ",", "notes2", ")", "\n", "", "elif", "partition", "==", "'test'", ":", "\n", "        ", "with", "open", "(", "'data/obesity_patient_records_test.xml'", ")", "as", "t1", ":", "\n", "            ", "notes1", "=", "t1", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "", "notes", "=", "(", "notes1", ",", ")", "\n", "\n", "", "for", "file", "in", "notes", ":", "\n", "        ", "root", "=", "ET", ".", "fromstring", "(", "file", ")", "\n", "root", "=", "root", ".", "findall", "(", "\"./docs\"", ")", "[", "0", "]", "\n", "for", "document", "in", "root", ".", "findall", "(", "\"./doc\"", ")", ":", "\n", "            ", "assert", "document", ".", "attrib", "[", "'id'", "]", "not", "in", "documents", "\n", "documents", "[", "document", ".", "attrib", "[", "'id'", "]", "]", "=", "{", "}", "\n", "documents", "[", "document", ".", "attrib", "[", "'id'", "]", "]", "[", "'text'", "]", "=", "document", ".", "findall", "(", "\"./text\"", ")", "[", "0", "]", ".", "text", "\n", "\n", "", "", "annotation_files", "=", "tuple", "(", ")", "\n", "if", "partition", "==", "'train'", ":", "\n", "        ", "with", "open", "(", "'data/obesity_standoff_annotations_training.xml'", ")", "as", "t1", ",", "open", "(", "'data/obesity_standoff_annotations_training_addendum.xml'", ")", "as", "t2", ",", "open", "(", "'data/obesity_standoff_annotations_training_addendum2.xml'", ")", "as", "t3", ",", "open", "(", "'data/obesity_standoff_annotations_training_addendum3.xml'", ")", "as", "t4", ":", "\n", "            ", "train1", "=", "t1", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "train2", "=", "t2", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "train3", "=", "t3", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "train4", "=", "t4", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "# train1 = resource_string('clinical_data', 'phenotyping/n2c2_2008/train/obesity_standoff_annotations_training.xml').decode('utf-8').strip()", "\n", "# train2 = resource_string('clinical_data', 'phenotyping/n2c2_2008/train/obesity_standoff_annotations_training_addendum.xml').decode('utf-8').strip()", "\n", "# train3 = resource_string('clinical_data', 'phenotyping/n2c2_2008/train/obesity_standoff_annotations_training_addendum2.xml').decode('utf-8').strip()", "\n", "# train4 = resource_string('clinical_data', 'phenotyping/n2c2_2008/train/obesity_standoff_annotations_training_addendum3.xml').decode('utf-8').strip()", "\n", "", "annotation_files", "=", "(", "train1", ",", "train2", ",", "train3", ",", "train4", ")", "\n", "", "elif", "partition", "==", "'test'", ":", "\n", "        ", "with", "open", "(", "'data/obesity_standoff_annotations_test.xml'", ")", "as", "t1", ":", "\n", "            ", "test1", "=", "t1", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "# test1 = resource_string('clinical_data','phenotyping/n2c2_2008/test/obesity_standoff_annotations_test.xml').decode('utf-8').strip()", "\n", "", "annotation_files", "=", "(", "test1", ",", ")", "\n", "\n", "", "for", "file", "in", "annotation_files", ":", "\n", "        ", "root", "=", "ET", ".", "fromstring", "(", "file", ")", "\n", "for", "diseases_annotation", "in", "root", ".", "findall", "(", "\"./diseases\"", ")", ":", "\n", "\n", "            ", "annotation_source", "=", "diseases_annotation", ".", "attrib", "[", "'source'", "]", "\n", "assert", "isinstance", "(", "annotation_source", ",", "str", ")", "\n", "for", "disease", "in", "diseases_annotation", ".", "findall", "(", "\"./disease\"", ")", ":", "\n", "                ", "disease_name", "=", "disease", ".", "attrib", "[", "'name'", "]", "\n", "all_diseases", ".", "add", "(", "disease_name", ")", "\n", "for", "annotation", "in", "disease", ".", "findall", "(", "\"./doc\"", ")", ":", "\n", "                    ", "doc_id", "=", "annotation", ".", "attrib", "[", "'id'", "]", "\n", "if", "not", "annotation_source", "in", "documents", "[", "doc_id", "]", ":", "\n", "                        ", "documents", "[", "doc_id", "]", "[", "annotation_source", "]", "=", "{", "}", "\n", "", "assert", "doc_id", "in", "documents", "\n", "judgment", "=", "annotation", ".", "attrib", "[", "'judgment'", "]", "\n", "documents", "[", "doc_id", "]", "[", "annotation_source", "]", "[", "disease_name", "]", "=", "judgment", "\n", "\n", "", "", "", "", "all_diseases", "=", "list", "(", "all_diseases", ")", "\n", "#print(all_diseases)", "\n", "\n", "for", "id", "in", "documents", ":", "#set unlabeled instances to None", "\n", "        ", "for", "annotation_type", "in", "(", "'textual'", ",", "'intuitive'", ")", ":", "\n", "            ", "for", "disease", "in", "all_diseases", ":", "\n", "                ", "if", "not", "annotation_type", "in", "documents", "[", "id", "]", ":", "\n", "                    ", "documents", "[", "id", "]", "[", "annotation_type", "]", "=", "{", "}", "\n", "", "if", "not", "disease", "in", "documents", "[", "id", "]", "[", "annotation_type", "]", ":", "\n", "#print(id, annotation_type, disease)", "\n", "                    ", "documents", "[", "id", "]", "[", "annotation_type", "]", "[", "disease", "]", "=", "None", "\n", "\n", "", "", "", "", "for", "id", "in", "documents", ":", "\n", "        ", "yield", "id", ",", "documents", "[", "id", "]", "[", "'text'", "]", ",", "documents", "[", "id", "]", "[", "'textual'", "]", ",", "documents", "[", "id", "]", "[", "'intuitive'", "]", "\n", "", "from", "pprint", "import", "pprint", "\n", "#pprint(documents[list(documents.keys())[1]])", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.ml4health_2019_replication.train_n2c2_2008._initialize_arguments": [[11, 73], ["p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.parse_args", "os.path.join", "os.makedirs", "log.handlers.clear", "logging.Formatter", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.FileHandler.setFormatter", "log.addHandler", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "log.setLevel", "log.addHandler", "log.info", "time.strftime", "os.path.join", "p.format_values", "torch.cuda.is_available", "log.info", "p.parse_args.labels.split", "torch.cuda.device_count", "log.info", "log.info", "socket.gethostname", "torch.cuda.device_count", "torch.cuda.current_device"], "function", ["None"], ["def", "_initialize_arguments", "(", "p", ":", "configargparse", ".", "ArgParser", ")", ":", "\n", "    ", "p", ".", "add", "(", "'--model_storage_directory'", ",", "help", "=", "'The directory caching all model runs'", ")", "\n", "p", ".", "add", "(", "'--bert_model_path'", ",", "help", "=", "'Model path to BERT'", ")", "\n", "p", ".", "add", "(", "'--labels'", ",", "help", "=", "'Numbers of labels to predict over'", ",", "type", "=", "str", ")", "\n", "p", ".", "add", "(", "'--architecture'", ",", "help", "=", "'Training architecture'", ",", "type", "=", "str", ")", "\n", "\n", "p", ".", "add", "(", "'--batch_size'", ",", "help", "=", "'Batch size for training multi-label document classifier'", ",", "type", "=", "int", ")", "\n", "p", ".", "add", "(", "'--bert_batch_size'", ",", "help", "=", "'Batch size for feeding 510 token subsets of documents through BERT'", ",", "type", "=", "int", ")", "\n", "p", ".", "add", "(", "'--epochs'", ",", "help", "=", "'Epochs to train'", ",", "type", "=", "int", ")", "\n", "#Optimizer arguments", "\n", "p", ".", "add", "(", "'--learning_rate'", ",", "help", "=", "'Optimizer step size'", ",", "type", "=", "float", ")", "\n", "p", ".", "add", "(", "'--weight_decay'", ",", "help", "=", "'Adam regularization'", ",", "type", "=", "float", ")", "\n", "\n", "p", ".", "add", "(", "'--evaluation_interval'", ",", "help", "=", "'Evaluate model on test set every evaluation_interval epochs'", ",", "type", "=", "int", ")", "\n", "p", ".", "add", "(", "'--checkpoint_interval'", ",", "help", "=", "'Save a model checkpoint to disk every checkpoint_interval epochs'", ",", "type", "=", "int", ")", "\n", "\n", "#Non-config arguments", "\n", "p", ".", "add", "(", "'--cuda'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Utilize GPU for training or prediction'", ")", "\n", "p", ".", "add", "(", "'--device'", ")", "\n", "p", ".", "add", "(", "'--timestamp'", ",", "help", "=", "'Run specific signature'", ")", "\n", "p", ".", "add", "(", "'--model_directory'", ",", "help", "=", "'The directory storing this model run, a sub-directory of model_storage_directory'", ")", "\n", "p", ".", "add", "(", "'--use_tensorboard'", ",", "help", "=", "'Use tensorboard logging'", ",", "type", "=", "bool", ")", "\n", "args", "=", "p", ".", "parse_args", "(", ")", "\n", "\n", "args", ".", "labels", "=", "[", "x", "for", "x", "in", "args", ".", "labels", ".", "split", "(", "', '", ")", "]", "\n", "\n", "\n", "\n", "\n", "\n", "#Set run specific envirorment configurations", "\n", "args", ".", "timestamp", "=", "time", ".", "strftime", "(", "\"run_%Y_%m_%d_%H_%M_%S\"", ")", "+", "\"_{machine}\"", ".", "format", "(", "machine", "=", "socket", ".", "gethostname", "(", ")", ")", "\n", "args", ".", "model_directory", "=", "os", ".", "path", ".", "join", "(", "args", ".", "model_storage_directory", ",", "args", ".", "timestamp", ")", "#directory", "\n", "os", ".", "makedirs", "(", "args", ".", "model_directory", ",", "exist_ok", "=", "True", ")", "\n", "\n", "#Handle logging configurations", "\n", "log", ".", "handlers", ".", "clear", "(", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "'%(message)s'", ")", "\n", "fh", "=", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_directory", ",", "\"log.txt\"", ")", ")", "\n", "fh", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "fh", ".", "setFormatter", "(", "formatter", ")", "\n", "log", ".", "addHandler", "(", "fh", ")", "\n", "ch", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "ch", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "ch", ".", "setFormatter", "(", "formatter", ")", "\n", "log", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "log", ".", "addHandler", "(", "ch", ")", "\n", "log", ".", "info", "(", "p", ".", "format_values", "(", ")", ")", "\n", "\n", "\n", "#Set global GPU state", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "cuda", ":", "\n", "        ", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "\n", "            ", "log", ".", "info", "(", "\"Using %i CUDA devices\"", "%", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "log", ".", "info", "(", "\"Using CUDA device:{0}\"", ".", "format", "(", "torch", ".", "cuda", ".", "current_device", "(", ")", ")", ")", "\n", "", "args", ".", "device", "=", "'cuda'", "\n", "", "else", ":", "\n", "        ", "log", ".", "info", "(", "\"Not using CUDA :(\"", ")", "\n", "args", ".", "dev", "=", "'cpu'", "\n", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.ml4health_2019_replication.train_n2c2_2006._initialize_arguments": [[11, 73], ["p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.add", "p.parse_args", "os.path.join", "os.makedirs", "log.handlers.clear", "logging.Formatter", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.FileHandler.setFormatter", "log.addHandler", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "log.setLevel", "log.addHandler", "log.info", "time.strftime", "os.path.join", "p.format_values", "torch.cuda.is_available", "log.info", "p.parse_args.labels.split", "torch.cuda.device_count", "log.info", "log.info", "socket.gethostname", "torch.cuda.device_count", "torch.cuda.current_device"], "function", ["None"], ["def", "_initialize_arguments", "(", "p", ":", "configargparse", ".", "ArgParser", ")", ":", "\n", "    ", "p", ".", "add", "(", "'--model_storage_directory'", ",", "help", "=", "'The directory caching all model runs'", ")", "\n", "p", ".", "add", "(", "'--bert_model_path'", ",", "help", "=", "'Model path to BERT'", ")", "\n", "p", ".", "add", "(", "'--labels'", ",", "help", "=", "'Numbers of labels to predict over'", ",", "type", "=", "str", ")", "\n", "p", ".", "add", "(", "'--architecture'", ",", "help", "=", "'Training architecture'", ",", "type", "=", "str", ")", "\n", "\n", "p", ".", "add", "(", "'--batch_size'", ",", "help", "=", "'Batch size for training multi-label document classifier'", ",", "type", "=", "int", ")", "\n", "p", ".", "add", "(", "'--bert_batch_size'", ",", "help", "=", "'Batch size for feeding 510 token subsets of documents through BERT'", ",", "type", "=", "int", ")", "\n", "p", ".", "add", "(", "'--epochs'", ",", "help", "=", "'Epochs to train'", ",", "type", "=", "int", ")", "\n", "#Optimizer arguments", "\n", "p", ".", "add", "(", "'--learning_rate'", ",", "help", "=", "'Optimizer step size'", ",", "type", "=", "float", ")", "\n", "p", ".", "add", "(", "'--weight_decay'", ",", "help", "=", "'Adam regularization'", ",", "type", "=", "float", ")", "\n", "\n", "p", ".", "add", "(", "'--evaluation_interval'", ",", "help", "=", "'Evaluate model on test set every evaluation_interval epochs'", ",", "type", "=", "int", ")", "\n", "p", ".", "add", "(", "'--checkpoint_interval'", ",", "help", "=", "'Save a model checkpoint to disk every checkpoint_interval epochs'", ",", "type", "=", "int", ")", "\n", "\n", "#Non-config arguments", "\n", "p", ".", "add", "(", "'--cuda'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Utilize GPU for training or prediction'", ")", "\n", "p", ".", "add", "(", "'--device'", ")", "\n", "p", ".", "add", "(", "'--timestamp'", ",", "help", "=", "'Run specific signature'", ")", "\n", "p", ".", "add", "(", "'--model_directory'", ",", "help", "=", "'The directory storing this model run, a sub-directory of model_storage_directory'", ")", "\n", "p", ".", "add", "(", "'--use_tensorboard'", ",", "help", "=", "'Use tensorboard logging'", ",", "type", "=", "bool", ")", "\n", "args", "=", "p", ".", "parse_args", "(", ")", "\n", "\n", "args", ".", "labels", "=", "[", "x", "for", "x", "in", "args", ".", "labels", ".", "split", "(", "', '", ")", "]", "\n", "\n", "\n", "\n", "\n", "\n", "#Set run specific envirorment configurations", "\n", "args", ".", "timestamp", "=", "time", ".", "strftime", "(", "\"run_%Y_%m_%d_%H_%M_%S\"", ")", "+", "\"_{machine}\"", ".", "format", "(", "machine", "=", "socket", ".", "gethostname", "(", ")", ")", "\n", "args", ".", "model_directory", "=", "os", ".", "path", ".", "join", "(", "args", ".", "model_storage_directory", ",", "args", ".", "timestamp", ")", "#directory", "\n", "os", ".", "makedirs", "(", "args", ".", "model_directory", ",", "exist_ok", "=", "True", ")", "\n", "\n", "#Handle logging configurations", "\n", "log", ".", "handlers", ".", "clear", "(", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "'%(message)s'", ")", "\n", "fh", "=", "logging", ".", "FileHandler", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_directory", ",", "\"log.txt\"", ")", ")", "\n", "fh", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "fh", ".", "setFormatter", "(", "formatter", ")", "\n", "log", ".", "addHandler", "(", "fh", ")", "\n", "ch", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "ch", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "ch", ".", "setFormatter", "(", "formatter", ")", "\n", "log", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "log", ".", "addHandler", "(", "ch", ")", "\n", "log", ".", "info", "(", "p", ".", "format_values", "(", ")", ")", "\n", "\n", "\n", "#Set global GPU state", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "cuda", ":", "\n", "        ", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "\n", "            ", "log", ".", "info", "(", "\"Using %i CUDA devices\"", "%", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "log", ".", "info", "(", "\"Using CUDA device:{0}\"", ".", "format", "(", "torch", ".", "cuda", ".", "current_device", "(", ")", ")", ")", "\n", "", "args", ".", "device", "=", "'cuda'", "\n", "", "else", ":", "\n", "        ", "log", ".", "info", "(", "\"Not using CUDA :(\"", ")", "\n", "args", ".", "dev", "=", "'cpu'", "\n", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert.BertForDocumentClassification.__init__": [[78, 140], ["logging.getLogger", "pytorch_transformers.tokenization_bert.BertTokenizer.from_pretrained", "os.path.exists", "pytorch_transformers.modeling_bert.BertConfig.from_json_file.__setattr__", "pytorch_transformers.modeling_bert.BertConfig.from_json_file.__setattr__", "document_bert_architectures[].from_pretrained", "document_bert.BertForDocumentClassification.bert_doc_classification.freeze_bert_encoder", "document_bert.BertForDocumentClassification.bert_doc_classification.unfreeze_bert_encoder_last_layers", "torch.optim.Adam", "vars", "os.path.exists", "pytorch_transformers.modeling_bert.BertConfig.from_pretrained", "len", "SummaryWriter", "document_bert.BertForDocumentClassification.bert_doc_classification.parameters", "os.path.join", "pytorch_transformers.modeling_bert.BertConfig.from_json_file", "os.path.exists", "os.path.join", "os.path.join", "os.path.join", "pytorch_transformers.modeling_bert.BertConfig.from_json_file", "ValueError", "os.path.join", "str", "document_bert.BertForDocumentClassification.args[].split"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertTransformer.freeze_bert_encoder", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertTransformer.unfreeze_bert_encoder_last_layers"], ["    ", "def", "__init__", "(", "self", ",", "args", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "device", "=", "'cuda'", ",", "\n", "bert_model_path", "=", "'bert-base-uncased'", ",", "\n", "architecture", "=", "\"DocumentBertLSTM\"", ",", "\n", "batch_size", "=", "10", ",", "\n", "bert_batch_size", "=", "7", ",", "\n", "learning_rate", "=", "5e-5", ",", "\n", "weight_decay", "=", "0", ",", "\n", "use_tensorboard", "=", "False", ")", ":", "\n", "        ", "if", "args", "is", "not", "None", ":", "\n", "            ", "self", ".", "args", "=", "vars", "(", "args", ")", "\n", "", "if", "not", "args", ":", "\n", "            ", "self", ".", "args", "=", "{", "}", "\n", "self", ".", "args", "[", "'bert_model_path'", "]", "=", "bert_model_path", "\n", "self", ".", "args", "[", "'device'", "]", "=", "device", "\n", "self", ".", "args", "[", "'learning_rate'", "]", "=", "learning_rate", "\n", "self", ".", "args", "[", "'weight_decay'", "]", "=", "weight_decay", "\n", "self", ".", "args", "[", "'batch_size'", "]", "=", "batch_size", "\n", "self", ".", "args", "[", "'labels'", "]", "=", "labels", "\n", "self", ".", "args", "[", "'bert_batch_size'", "]", "=", "bert_batch_size", "\n", "self", ".", "args", "[", "'architecture'", "]", "=", "architecture", "\n", "self", ".", "args", "[", "'use_tensorboard'", "]", "=", "use_tensorboard", "\n", "", "if", "'fold'", "not", "in", "self", ".", "args", ":", "\n", "            ", "self", ".", "args", "[", "'fold'", "]", "=", "0", "\n", "\n", "", "assert", "self", ".", "args", "[", "'labels'", "]", "is", "not", "None", ",", "\"Must specify all labels in prediction\"", "\n", "\n", "self", ".", "log", "=", "logging", ".", "getLogger", "(", ")", "\n", "self", ".", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "self", ".", "args", "[", "'bert_model_path'", "]", ")", "\n", "\n", "\n", "#account for some random tensorflow naming scheme", "\n", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "args", "[", "'bert_model_path'", "]", ")", ":", "\n", "            ", "if", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "self", ".", "args", "[", "'bert_model_path'", "]", ",", "CONFIG_NAME", ")", ")", ":", "\n", "                ", "config", "=", "BertConfig", ".", "from_json_file", "(", "os", ".", "path", ".", "join", "(", "self", ".", "args", "[", "'bert_model_path'", "]", ",", "CONFIG_NAME", ")", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "self", ".", "args", "[", "'bert_model_path'", "]", ",", "'bert_config.json'", ")", ")", ":", "\n", "                ", "config", "=", "BertConfig", ".", "from_json_file", "(", "os", ".", "path", ".", "join", "(", "self", ".", "args", "[", "'bert_model_path'", "]", ",", "'bert_config.json'", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Cannot find a configuration for the BERT based model you are attempting to load.\"", ")", "\n", "", "", "else", ":", "\n", "            ", "config", "=", "BertConfig", ".", "from_pretrained", "(", "self", ".", "args", "[", "'bert_model_path'", "]", ")", "\n", "", "config", ".", "__setattr__", "(", "'num_labels'", ",", "len", "(", "self", ".", "args", "[", "'labels'", "]", ")", ")", "\n", "config", ".", "__setattr__", "(", "'bert_batch_size'", ",", "self", ".", "args", "[", "'bert_batch_size'", "]", ")", "\n", "\n", "if", "'use_tensorboard'", "in", "self", ".", "args", "and", "self", ".", "args", "[", "'use_tensorboard'", "]", ":", "\n", "            ", "assert", "'model_directory'", "in", "self", ".", "args", "is", "not", "None", ",", "\"Must have a logging and checkpoint directory set.\"", "\n", "from", "torch", ".", "utils", ".", "tensorboard", "import", "SummaryWriter", "\n", "self", ".", "tensorboard_writer", "=", "SummaryWriter", "(", "os", ".", "path", ".", "join", "(", "self", ".", "args", "[", "'model_directory'", "]", ",", "\n", "\"..\"", ",", "\n", "\"runs\"", ",", "\n", "self", ".", "args", "[", "'model_directory'", "]", ".", "split", "(", "os", ".", "path", ".", "sep", ")", "[", "-", "1", "]", "+", "'_'", "+", "self", ".", "args", "[", "'architecture'", "]", "+", "'_'", "+", "str", "(", "self", ".", "args", "[", "'fold'", "]", ")", ")", ")", "\n", "\n", "\n", "", "self", ".", "bert_doc_classification", "=", "document_bert_architectures", "[", "self", ".", "args", "[", "'architecture'", "]", "]", ".", "from_pretrained", "(", "self", ".", "args", "[", "'bert_model_path'", "]", ",", "config", "=", "config", ")", "\n", "self", ".", "bert_doc_classification", ".", "freeze_bert_encoder", "(", ")", "\n", "self", ".", "bert_doc_classification", ".", "unfreeze_bert_encoder_last_layers", "(", ")", "\n", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "self", ".", "bert_doc_classification", ".", "parameters", "(", ")", ",", "\n", "weight_decay", "=", "self", ".", "args", "[", "'weight_decay'", "]", ",", "\n", "lr", "=", "self", ".", "args", "[", "'learning_rate'", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert.BertForDocumentClassification.fit": [[144, 208], ["document_bert.BertForDocumentClassification.bert_doc_classification.train", "document_bert.encode_documents", "torch.FloatTensor", "torch.nn.BCEWithLogitsLoss", "document_bert.BertForDocumentClassification.bert_doc_classification.to", "range", "torch.cuda.device_count", "torch.nn.DataParallel", "torch.randperm", "range", "int", "document_bert.BertForDocumentClassification.log.info", "document_representations[].to", "document_bert.BertForDocumentClassification.bert_doc_classification", "correct_output[].to", "document_bert.BertForDocumentClassification.loss_function", "float", "document_bert.BertForDocumentClassification.backward", "document_bert.BertForDocumentClassification.optimizer.step", "document_bert.BertForDocumentClassification.optimizer.zero_grad", "document_bert.BertForDocumentClassification.tensorboard_writer.add_scalar", "document_bert.BertForDocumentClassification.save_checkpoint", "document_bert.BertForDocumentClassification.predict", "document_bert.BertForDocumentClassification.item", "os.path.join", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert.encode_documents", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert.BertForDocumentClassification.save_checkpoint", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert.BertForDocumentClassification.predict"], ["", "def", "fit", "(", "self", ",", "train", ",", "dev", ")", ":", "\n", "        ", "\"\"\"\n        A list of\n        :param documents: a list of documents\n        :param labels: a list of label vectors\n        :return:\n        \"\"\"", "\n", "\n", "train_documents", ",", "train_labels", "=", "train", "\n", "dev_documents", ",", "dev_labels", "=", "dev", "\n", "\n", "self", ".", "bert_doc_classification", ".", "train", "(", ")", "\n", "\n", "document_representations", ",", "document_sequence_lengths", "=", "encode_documents", "(", "train_documents", ",", "self", ".", "bert_tokenizer", ")", "\n", "\n", "correct_output", "=", "torch", ".", "FloatTensor", "(", "train_labels", ")", "\n", "\n", "loss_weight", "=", "(", "(", "correct_output", ".", "shape", "[", "0", "]", "/", "torch", ".", "sum", "(", "correct_output", ",", "dim", "=", "0", ")", ")", "-", "1", ")", ".", "to", "(", "device", "=", "self", ".", "args", "[", "'device'", "]", ")", "\n", "self", ".", "loss_function", "=", "torch", ".", "nn", ".", "BCEWithLogitsLoss", "(", "pos_weight", "=", "loss_weight", ")", "\n", "\n", "assert", "document_representations", ".", "shape", "[", "0", "]", "==", "correct_output", ".", "shape", "[", "0", "]", "\n", "\n", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "\n", "            ", "self", ".", "bert_doc_classification", "=", "torch", ".", "nn", ".", "DataParallel", "(", "self", ".", "bert_doc_classification", ")", "\n", "", "self", ".", "bert_doc_classification", ".", "to", "(", "device", "=", "self", ".", "args", "[", "'device'", "]", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "1", ",", "self", ".", "args", "[", "'epochs'", "]", "+", "1", ")", ":", "\n", "# shuffle", "\n", "            ", "permutation", "=", "torch", ".", "randperm", "(", "document_representations", ".", "shape", "[", "0", "]", ")", "\n", "document_representations", "=", "document_representations", "[", "permutation", "]", "\n", "document_sequence_lengths", "=", "document_sequence_lengths", "[", "permutation", "]", "\n", "correct_output", "=", "correct_output", "[", "permutation", "]", "\n", "\n", "self", ".", "epoch", "=", "epoch", "\n", "epoch_loss", "=", "0.0", "\n", "for", "i", "in", "range", "(", "0", ",", "document_representations", ".", "shape", "[", "0", "]", ",", "self", ".", "args", "[", "'batch_size'", "]", ")", ":", "\n", "\n", "                ", "batch_document_tensors", "=", "document_representations", "[", "i", ":", "i", "+", "self", ".", "args", "[", "'batch_size'", "]", "]", ".", "to", "(", "device", "=", "self", ".", "args", "[", "'device'", "]", ")", "\n", "batch_document_sequence_lengths", "=", "document_sequence_lengths", "[", "i", ":", "i", "+", "self", ".", "args", "[", "'batch_size'", "]", "]", "\n", "#self.log.info(batch_document_tensors.shape)", "\n", "batch_predictions", "=", "self", ".", "bert_doc_classification", "(", "batch_document_tensors", ",", "\n", "batch_document_sequence_lengths", ",", "device", "=", "self", ".", "args", "[", "'device'", "]", ")", "\n", "\n", "batch_correct_output", "=", "correct_output", "[", "i", ":", "i", "+", "self", ".", "args", "[", "'batch_size'", "]", "]", ".", "to", "(", "device", "=", "self", ".", "args", "[", "'device'", "]", ")", "\n", "loss", "=", "self", ".", "loss_function", "(", "batch_predictions", ",", "batch_correct_output", ")", "\n", "epoch_loss", "+=", "float", "(", "loss", ".", "item", "(", ")", ")", "\n", "#self.log.info(batch_predictions)", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "", "epoch_loss", "/=", "int", "(", "document_representations", ".", "shape", "[", "0", "]", "/", "self", ".", "args", "[", "'batch_size'", "]", ")", "# divide by number of batches per epoch", "\n", "\n", "if", "'use_tensorboard'", "in", "self", ".", "args", "and", "self", ".", "args", "[", "'use_tensorboard'", "]", ":", "\n", "                ", "self", ".", "tensorboard_writer", ".", "add_scalar", "(", "'Loss/Train'", ",", "epoch_loss", ",", "self", ".", "epoch", ")", "\n", "\n", "", "self", ".", "log", ".", "info", "(", "'Epoch %i Completed: %f'", "%", "(", "epoch", ",", "epoch_loss", ")", ")", "\n", "\n", "if", "epoch", "%", "self", ".", "args", "[", "'checkpoint_interval'", "]", "==", "0", ":", "\n", "                ", "self", ".", "save_checkpoint", "(", "os", ".", "path", ".", "join", "(", "self", ".", "args", "[", "'model_directory'", "]", ",", "\"checkpoint_%s\"", "%", "epoch", ")", ")", "\n", "\n", "# evaluate on development data", "\n", "", "if", "epoch", "%", "self", ".", "args", "[", "'evaluation_interval'", "]", "==", "0", ":", "\n", "                ", "self", ".", "predict", "(", "(", "dev_documents", ",", "dev_labels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert.BertForDocumentClassification.predict": [[209, 287], ["isinstance", "document_bert.BertForDocumentClassification.bert_doc_classification.to", "document_bert.BertForDocumentClassification.bert_doc_classification.eval", "range", "torch.empty.transpose", "document_bert.BertForDocumentClassification.bert_doc_classification.train", "document_bert.encode_documents", "isinstance", "document_bert.BertForDocumentClassification.log.info", "document_bert.encode_documents", "torch.FloatTensor().transpose", "torch.no_grad", "torch.empty", "range", "range", "torch.empty.cpu", "range", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "len", "document_representations[].to", "document_bert.BertForDocumentClassification.bert_doc_classification", "correct_output[].cpu().view().numpy", "predictions[].cpu().view().numpy", "sklearn.metrics.f1_score", "sklearn.metrics.precision_score", "sklearn.metrics.recall_score", "precisions.append", "recalls.append", "fmeasures.append", "logging.info", "torch.FloatTensor().transpose.reshape().numpy", "torch.empty.reshape().numpy", "torch.FloatTensor().transpose.reshape().numpy", "torch.empty.reshape().numpy", "range", "document_bert.BertForDocumentClassification.tensorboard_writer.add_scalar", "document_bert.BertForDocumentClassification.tensorboard_writer.add_scalar", "open", "eval_results.write", "eval_results.write", "eval_results.write", "eval_results.write", "eval_results.write", "eval_results.write", "torch.FloatTensor", "len", "document_bert.BertForDocumentClassification.tensorboard_writer.add_scalar", "document_bert.BertForDocumentClassification.tensorboard_writer.add_scalar", "document_bert.BertForDocumentClassification.tensorboard_writer.add_scalar", "os.path.join", "correct_output[].cpu().view", "predictions[].cpu().view", "torch.FloatTensor().transpose.reshape", "torch.empty.reshape", "torch.FloatTensor().transpose.reshape", "torch.empty.reshape", "[].replace", "[].replace", "[].replace", "str", "str", "correct_output[].cpu", "predictions[].cpu", "str", "str", "str", "range", "range", "range", "range"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert.encode_documents", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert.encode_documents"], ["", "", "", "def", "predict", "(", "self", ",", "data", ",", "threshold", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        A tuple containing\n        :param data:\n        :return:\n        \"\"\"", "\n", "document_representations", "=", "None", "\n", "document_sequence_lengths", "=", "None", "\n", "correct_output", "=", "None", "\n", "if", "isinstance", "(", "data", ",", "list", ")", ":", "\n", "            ", "document_representations", ",", "document_sequence_lengths", "=", "encode_documents", "(", "data", ",", "self", ".", "bert_tokenizer", ")", "\n", "", "if", "isinstance", "(", "data", ",", "tuple", ")", "and", "len", "(", "data", ")", "==", "2", ":", "\n", "            ", "self", ".", "log", ".", "info", "(", "'Evaluating on Epoch %i'", "%", "(", "self", ".", "epoch", ")", ")", "\n", "document_representations", ",", "document_sequence_lengths", "=", "encode_documents", "(", "data", "[", "0", "]", ",", "self", ".", "bert_tokenizer", ")", "\n", "correct_output", "=", "torch", ".", "FloatTensor", "(", "data", "[", "1", "]", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "assert", "self", ".", "args", "[", "'labels'", "]", "is", "not", "None", "\n", "\n", "", "self", ".", "bert_doc_classification", ".", "to", "(", "device", "=", "self", ".", "args", "[", "'device'", "]", ")", "\n", "self", ".", "bert_doc_classification", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "predictions", "=", "torch", ".", "empty", "(", "(", "document_representations", ".", "shape", "[", "0", "]", ",", "len", "(", "self", ".", "args", "[", "'labels'", "]", ")", ")", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "document_representations", ".", "shape", "[", "0", "]", ",", "self", ".", "args", "[", "'batch_size'", "]", ")", ":", "\n", "                ", "batch_document_tensors", "=", "document_representations", "[", "i", ":", "i", "+", "self", ".", "args", "[", "'batch_size'", "]", "]", ".", "to", "(", "device", "=", "self", ".", "args", "[", "'device'", "]", ")", "\n", "batch_document_sequence_lengths", "=", "document_sequence_lengths", "[", "i", ":", "i", "+", "self", ".", "args", "[", "'batch_size'", "]", "]", "\n", "\n", "prediction", "=", "self", ".", "bert_doc_classification", "(", "batch_document_tensors", ",", "\n", "batch_document_sequence_lengths", ",", "device", "=", "self", ".", "args", "[", "'device'", "]", ")", "\n", "predictions", "[", "i", ":", "i", "+", "self", ".", "args", "[", "'batch_size'", "]", "]", "=", "prediction", "\n", "\n", "", "", "for", "r", "in", "range", "(", "0", ",", "predictions", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "for", "c", "in", "range", "(", "0", ",", "predictions", ".", "shape", "[", "1", "]", ")", ":", "\n", "                ", "if", "predictions", "[", "r", "]", "[", "c", "]", ">", "threshold", ":", "\n", "                    ", "predictions", "[", "r", "]", "[", "c", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "predictions", "[", "r", "]", "[", "c", "]", "=", "0", "\n", "", "", "", "predictions", "=", "predictions", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "\n", "if", "correct_output", "is", "None", ":", "\n", "            ", "return", "predictions", ".", "cpu", "(", ")", "\n", "", "else", ":", "\n", "            ", "assert", "correct_output", ".", "shape", "==", "predictions", ".", "shape", "\n", "precisions", "=", "[", "]", "\n", "recalls", "=", "[", "]", "\n", "fmeasures", "=", "[", "]", "\n", "\n", "for", "label_idx", "in", "range", "(", "predictions", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "correct", "=", "correct_output", "[", "label_idx", "]", ".", "cpu", "(", ")", ".", "view", "(", "-", "1", ")", ".", "numpy", "(", ")", "\n", "predicted", "=", "predictions", "[", "label_idx", "]", ".", "cpu", "(", ")", ".", "view", "(", "-", "1", ")", ".", "numpy", "(", ")", "\n", "present_f1_score", "=", "f1_score", "(", "correct", ",", "predicted", ",", "average", "=", "'binary'", ",", "pos_label", "=", "1", ")", "\n", "present_precision_score", "=", "precision_score", "(", "correct", ",", "predicted", ",", "average", "=", "'binary'", ",", "pos_label", "=", "1", ")", "\n", "present_recall_score", "=", "recall_score", "(", "correct", ",", "predicted", ",", "average", "=", "'binary'", ",", "pos_label", "=", "1", ")", "\n", "\n", "precisions", ".", "append", "(", "present_precision_score", ")", "\n", "recalls", ".", "append", "(", "present_recall_score", ")", "\n", "fmeasures", ".", "append", "(", "present_f1_score", ")", "\n", "logging", ".", "info", "(", "'F1\\t%s\\t%f'", "%", "(", "self", ".", "args", "[", "'labels'", "]", "[", "label_idx", "]", ",", "present_f1_score", ")", ")", "\n", "\n", "", "micro_f1", "=", "f1_score", "(", "correct_output", ".", "reshape", "(", "-", "1", ")", ".", "numpy", "(", ")", ",", "predictions", ".", "reshape", "(", "-", "1", ")", ".", "numpy", "(", ")", ",", "average", "=", "'micro'", ")", "\n", "macro_f1", "=", "f1_score", "(", "correct_output", ".", "reshape", "(", "-", "1", ")", ".", "numpy", "(", ")", ",", "predictions", ".", "reshape", "(", "-", "1", ")", ".", "numpy", "(", ")", ",", "average", "=", "'macro'", ")", "\n", "\n", "if", "'use_tensorboard'", "in", "self", ".", "args", "and", "self", ".", "args", "[", "'use_tensorboard'", "]", ":", "\n", "                ", "for", "label_idx", "in", "range", "(", "predictions", ".", "shape", "[", "0", "]", ")", ":", "\n", "                    ", "self", ".", "tensorboard_writer", ".", "add_scalar", "(", "'Precision/%s/Test'", "%", "self", ".", "args", "[", "'labels'", "]", "[", "label_idx", "]", ".", "replace", "(", "\" \"", ",", "\"_\"", ")", ",", "precisions", "[", "label_idx", "]", ",", "self", ".", "epoch", ")", "\n", "self", ".", "tensorboard_writer", ".", "add_scalar", "(", "'Recall/%s/Test'", "%", "self", ".", "args", "[", "'labels'", "]", "[", "label_idx", "]", ".", "replace", "(", "\" \"", ",", "\"_\"", ")", ",", "recalls", "[", "label_idx", "]", ",", "self", ".", "epoch", ")", "\n", "self", ".", "tensorboard_writer", ".", "add_scalar", "(", "'F1/%s/Test'", "%", "self", ".", "args", "[", "'labels'", "]", "[", "label_idx", "]", ".", "replace", "(", "\" \"", ",", "\"_\"", ")", ",", "fmeasures", "[", "label_idx", "]", ",", "self", ".", "epoch", ")", "\n", "", "self", ".", "tensorboard_writer", ".", "add_scalar", "(", "'Micro-F1/Test'", ",", "micro_f1", ",", "self", ".", "epoch", ")", "\n", "self", ".", "tensorboard_writer", ".", "add_scalar", "(", "'Macro-F1/Test'", ",", "macro_f1", ",", "self", ".", "epoch", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "args", "[", "'model_directory'", "]", ",", "\"eval_%s.csv\"", "%", "self", ".", "epoch", ")", ",", "'w'", ")", "as", "eval_results", ":", "\n", "                ", "eval_results", ".", "write", "(", "'Metric\\t'", "+", "'\\t'", ".", "join", "(", "[", "self", ".", "args", "[", "'labels'", "]", "[", "label_idx", "]", "for", "label_idx", "in", "range", "(", "predictions", ".", "shape", "[", "0", "]", ")", "]", ")", "+", "'\\n'", ")", "\n", "eval_results", ".", "write", "(", "'Precision\\t'", "+", "'\\t'", ".", "join", "(", "[", "str", "(", "precisions", "[", "label_idx", "]", ")", "for", "label_idx", "in", "range", "(", "predictions", ".", "shape", "[", "0", "]", ")", "]", ")", "+", "'\\n'", ")", "\n", "eval_results", ".", "write", "(", "'Recall\\t'", "+", "'\\t'", ".", "join", "(", "[", "str", "(", "recalls", "[", "label_idx", "]", ")", "for", "label_idx", "in", "range", "(", "predictions", ".", "shape", "[", "0", "]", ")", "]", ")", "+", "'\\n'", ")", "\n", "eval_results", ".", "write", "(", "'F1\\t'", "+", "'\\t'", ".", "join", "(", "[", "str", "(", "fmeasures", "[", "label_idx", "]", ")", "for", "label_idx", "in", "range", "(", "predictions", ".", "shape", "[", "0", "]", ")", "]", ")", "+", "'\\n'", ")", "\n", "eval_results", ".", "write", "(", "'Micro-F1\\t'", "+", "str", "(", "micro_f1", ")", "+", "'\\n'", ")", "\n", "eval_results", ".", "write", "(", "'Macro-F1\\t'", "+", "str", "(", "macro_f1", ")", "+", "'\\n'", ")", "\n", "\n", "", "", "self", ".", "bert_doc_classification", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert.BertForDocumentClassification.save_checkpoint": [[288, 308], ["document_bert.BertForDocumentClassification.log.info", "isinstance", "torch.save", "net.config.to_json_file", "document_bert.BertForDocumentClassification.bert_tokenizer.save_vocabulary", "os.path.exists", "os.mkdir", "ValueError", "net.state_dict", "os.path.join", "os.path.join"], "methods", ["None"], ["", "def", "save_checkpoint", "(", "self", ",", "checkpoint_path", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Saves an instance of the current model to the specified path.\n        :return:\n        \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "checkpoint_path", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "checkpoint_path", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Attempting to save checkpoint to an existing directory\"", ")", "\n", "", "self", ".", "log", ".", "info", "(", "\"Saving checkpoint: %s\"", "%", "checkpoint_path", ")", "\n", "\n", "#save finetune parameters", "\n", "net", "=", "self", ".", "bert_doc_classification", "\n", "if", "isinstance", "(", "self", ".", "bert_doc_classification", ",", "nn", ".", "DataParallel", ")", ":", "\n", "            ", "net", "=", "self", ".", "bert_doc_classification", ".", "module", "\n", "", "torch", ".", "save", "(", "net", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "checkpoint_path", ",", "WEIGHTS_NAME", ")", ")", "\n", "#save configurations", "\n", "net", ".", "config", ".", "to_json_file", "(", "os", ".", "path", ".", "join", "(", "checkpoint_path", ",", "CONFIG_NAME", ")", ")", "\n", "#save exact vocabulary utilized", "\n", "self", ".", "bert_tokenizer", ".", "save_vocabulary", "(", "checkpoint_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert.encode_documents": [[11, 62], ["math.ceil", "torch.zeros", "enumerate", "tokenizer.tokenize", "max", "enumerate", "document_seq_lengths.append", "torch.LongTensor", "range", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "tokenizer.convert_tokens_to_ids", "torch.cat", "len", "len", "tokens.append", "input_type_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_type_ids.append", "attention_masks.append", "len", "len", "len", "len", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "function", ["None"], ["def", "encode_documents", "(", "documents", ":", "list", ",", "tokenizer", ":", "BertTokenizer", ",", "max_input_length", "=", "512", ")", ":", "\n", "    ", "\"\"\"\n    Returns a len(documents) * max_sequences_per_document * 3 * 512 tensor where len(documents) is the batch\n    dimension and the others encode bert input.\n\n    This is the input to any of the document bert architectures.\n\n    :param documents: a list of text documents\n    :param tokenizer: the sentence piece bert tokenizer\n    :return:\n    \"\"\"", "\n", "tokenized_documents", "=", "[", "tokenizer", ".", "tokenize", "(", "document", ")", "for", "document", "in", "documents", "]", "\n", "max_sequences_per_document", "=", "math", ".", "ceil", "(", "max", "(", "len", "(", "x", ")", "/", "(", "max_input_length", "-", "2", ")", "for", "x", "in", "tokenized_documents", ")", ")", "\n", "assert", "max_sequences_per_document", "<=", "20", ",", "\"Your document is to large, arbitrary size when writing\"", "\n", "\n", "output", "=", "torch", ".", "zeros", "(", "size", "=", "(", "len", "(", "documents", ")", ",", "max_sequences_per_document", ",", "3", ",", "512", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "document_seq_lengths", "=", "[", "]", "#number of sequence generated per document", "\n", "#Need to use 510 to account for 2 padding tokens", "\n", "for", "doc_index", ",", "tokenized_document", "in", "enumerate", "(", "tokenized_documents", ")", ":", "\n", "        ", "max_seq_index", "=", "0", "\n", "for", "seq_index", ",", "i", "in", "enumerate", "(", "range", "(", "0", ",", "len", "(", "tokenized_document", ")", ",", "(", "max_input_length", "-", "2", ")", ")", ")", ":", "\n", "            ", "raw_tokens", "=", "tokenized_document", "[", "i", ":", "i", "+", "(", "max_input_length", "-", "2", ")", "]", "\n", "tokens", "=", "[", "]", "\n", "input_type_ids", "=", "[", "]", "\n", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "raw_tokens", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "attention_masks", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "while", "len", "(", "input_ids", ")", "<", "max_input_length", ":", "\n", "                ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "attention_masks", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "512", "and", "len", "(", "attention_masks", ")", "==", "512", "and", "len", "(", "input_type_ids", ")", "==", "512", "\n", "\n", "#we are ready to rumble", "\n", "output", "[", "doc_index", "]", "[", "seq_index", "]", "=", "torch", ".", "cat", "(", "(", "torch", ".", "LongTensor", "(", "input_ids", ")", ".", "unsqueeze", "(", "0", ")", ",", "\n", "torch", ".", "LongTensor", "(", "input_type_ids", ")", ".", "unsqueeze", "(", "0", ")", ",", "\n", "torch", ".", "LongTensor", "(", "attention_masks", ")", ".", "unsqueeze", "(", "0", ")", ")", ",", "\n", "dim", "=", "0", ")", "\n", "max_seq_index", "=", "seq_index", "\n", "", "document_seq_lengths", ".", "append", "(", "max_seq_index", "+", "1", ")", "\n", "", "return", "output", ",", "torch", ".", "LongTensor", "(", "document_seq_lengths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.LayerNorm.__init__": [[393, 407], ["torch.nn.Module.__init__", "isinstance", "tuple", "transformer.LayerNorm.reset_parameters", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "transformer.LayerNorm.register_parameter", "transformer.LayerNorm.register_parameter", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.LayerNorm.reset_parameters"], ["def", "__init__", "(", "self", ",", "normalized_shape", ",", "eps", "=", "1e-5", ",", "elementwise_affine", "=", "True", ")", ":", "\n", "        ", "super", "(", "LayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "isinstance", "(", "normalized_shape", ",", "numbers", ".", "Integral", ")", ":", "\n", "            ", "normalized_shape", "=", "(", "normalized_shape", ",", ")", "\n", "", "self", ".", "normalized_shape", "=", "tuple", "(", "normalized_shape", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "elementwise_affine", "=", "elementwise_affine", "\n", "if", "self", ".", "elementwise_affine", ":", "\n", "            ", "self", ".", "weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "*", "normalized_shape", ")", ")", "\n", "self", ".", "bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "*", "normalized_shape", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'weight'", ",", "None", ")", "\n", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.LayerNorm.reset_parameters": [[408, 412], ["torch.nn.init.ones_", "torch.nn.init.zeros_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "elementwise_affine", ":", "\n", "            ", "init", ".", "ones_", "(", "self", ".", "weight", ")", "\n", "init", ".", "zeros_", "(", "self", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.LayerNorm.forward": [[413, 416], ["torch.nn.functional.layer_norm"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "F", ".", "layer_norm", "(", "\n", "input", ",", "self", ".", "normalized_shape", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.LayerNorm.extra_repr": [[417, 420], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "'{normalized_shape}, eps={eps}, '", "'elementwise_affine={elementwise_affine}'", ".", "format", "(", "**", "self", ".", "__dict__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.MultiheadAttention.__init__": [[445, 479], ["torch.nn.Module.__init__", "torch.nn.parameter.Parameter", "torch.nn.Linear", "transformer.MultiheadAttention._reset_parameters", "torch.empty", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "transformer.MultiheadAttention.register_parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.empty", "torch.empty", "torch.empty"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.Transformer._reset_parameters"], ["def", "__init__", "(", "self", ",", "embed_dim", ",", "num_heads", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ",", "add_bias_kv", "=", "False", ",", "add_zero_attn", "=", "False", ",", "kdim", "=", "None", ",", "vdim", "=", "None", ")", ":", "\n", "        ", "super", "(", "MultiheadAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "kdim", "=", "kdim", "if", "kdim", "is", "not", "None", "else", "embed_dim", "\n", "self", ".", "vdim", "=", "vdim", "if", "vdim", "is", "not", "None", "else", "embed_dim", "\n", "self", ".", "_qkv_same_embed_dim", "=", "self", ".", "kdim", "==", "embed_dim", "and", "self", ".", "vdim", "==", "embed_dim", "\n", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "assert", "self", ".", "head_dim", "*", "num_heads", "==", "self", ".", "embed_dim", ",", "\"embed_dim must be divisible by num_heads\"", "\n", "\n", "self", ".", "in_proj_weight", "=", "Parameter", "(", "torch", ".", "empty", "(", "3", "*", "embed_dim", ",", "embed_dim", ")", ")", "\n", "\n", "if", "self", ".", "_qkv_same_embed_dim", "is", "False", ":", "\n", "            ", "self", ".", "q_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "embed_dim", ",", "embed_dim", ")", ")", "\n", "self", ".", "k_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "embed_dim", ",", "self", ".", "kdim", ")", ")", "\n", "self", ".", "v_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "embed_dim", ",", "self", ".", "vdim", ")", ")", "\n", "\n", "", "if", "bias", ":", "\n", "            ", "self", ".", "in_proj_bias", "=", "Parameter", "(", "torch", ".", "empty", "(", "3", "*", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'in_proj_bias'", ",", "None", ")", "\n", "", "self", ".", "out_proj", "=", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "\n", "if", "add_bias_kv", ":", "\n", "            ", "self", ".", "bias_k", "=", "Parameter", "(", "torch", ".", "empty", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "bias_v", "=", "Parameter", "(", "torch", ".", "empty", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias_k", "=", "self", ".", "bias_v", "=", "None", "\n", "\n", "", "self", ".", "add_zero_attn", "=", "add_zero_attn", "\n", "\n", "self", ".", "_reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.MultiheadAttention._reset_parameters": [[480, 495], ["torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_"], "methods", ["None"], ["", "def", "_reset_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_qkv_same_embed_dim", ":", "\n", "            ", "xavier_uniform_", "(", "self", ".", "in_proj_weight", ")", "\n", "", "else", ":", "\n", "            ", "xavier_uniform_", "(", "self", ".", "q_proj_weight", ")", "\n", "xavier_uniform_", "(", "self", ".", "k_proj_weight", ")", "\n", "xavier_uniform_", "(", "self", ".", "v_proj_weight", ")", "\n", "\n", "", "if", "self", ".", "in_proj_bias", "is", "not", "None", ":", "\n", "            ", "constant_", "(", "self", ".", "in_proj_bias", ",", "0.", ")", "\n", "constant_", "(", "self", ".", "out_proj", ".", "bias", ",", "0.", ")", "\n", "", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "xavier_normal_", "(", "self", ".", "bias_k", ")", "\n", "", "if", "self", ".", "bias_v", "is", "not", "None", ":", "\n", "            ", "xavier_normal_", "(", "self", ".", "bias_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.MultiheadAttention.forward": [[496, 549], ["hasattr", "transformer.multi_head_attention_forward", "transformer.multi_head_attention_forward", "hasattr", "warnings.warn"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.multi_head_attention_forward", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.multi_head_attention_forward"], ["", "", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "key_padding_mask", "=", "None", ",", "\n", "need_weights", "=", "True", ",", "attn_mask", "=", "None", ")", ":", "\n", "        ", "r\"\"\"\n    Args:\n        query, key, value: map a query and a set of key-value pairs to an output.\n            See \"Attention Is All You Need\" for more details.\n        key_padding_mask: if provided, specified padding elements in the key will\n            be ignored by the attention. This is an binary mask. When the value is True,\n            the corresponding value on the attention layer will be filled with -inf.\n        need_weights: output attn_output_weights.\n        attn_mask: mask that prevents attention to certain positions. This is an additive mask\n            (i.e. the values will be added to the attention layer).\n    Shape:\n        - Inputs:\n        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n          the embedding dimension.\n        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n          the embedding dimension.\n        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n          the embedding dimension.\n        - key_padding_mask: :math:`(N, S)`, ByteTensor, where N is the batch size, S is the source sequence length.\n        - attn_mask: :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n        - Outputs:\n        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n          E is the embedding dimension.\n        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n          L is the target sequence length, S is the source sequence length.\n        \"\"\"", "\n", "if", "hasattr", "(", "self", ",", "'_qkv_same_embed_dim'", ")", "and", "self", ".", "_qkv_same_embed_dim", "is", "False", ":", "\n", "            ", "return", "multi_head_attention_forward", "(", "\n", "query", ",", "key", ",", "value", ",", "self", ".", "embed_dim", ",", "self", ".", "num_heads", ",", "\n", "self", ".", "in_proj_weight", ",", "self", ".", "in_proj_bias", ",", "\n", "self", ".", "bias_k", ",", "self", ".", "bias_v", ",", "self", ".", "add_zero_attn", ",", "\n", "self", ".", "dropout", ",", "self", ".", "out_proj", ".", "weight", ",", "self", ".", "out_proj", ".", "bias", ",", "\n", "training", "=", "self", ".", "training", ",", "\n", "key_padding_mask", "=", "key_padding_mask", ",", "need_weights", "=", "need_weights", ",", "\n", "attn_mask", "=", "attn_mask", ",", "use_separate_proj_weight", "=", "True", ",", "\n", "q_proj_weight", "=", "self", ".", "q_proj_weight", ",", "k_proj_weight", "=", "self", ".", "k_proj_weight", ",", "\n", "v_proj_weight", "=", "self", ".", "v_proj_weight", ")", "\n", "", "else", ":", "\n", "            ", "if", "not", "hasattr", "(", "self", ",", "'_qkv_same_embed_dim'", ")", ":", "\n", "                ", "warnings", ".", "warn", "(", "'A new version of MultiheadAttention module has been implemented. \\\n                    Please re-train your model with the new module'", ",", "\n", "UserWarning", ")", "\n", "\n", "", "return", "multi_head_attention_forward", "(", "\n", "query", ",", "key", ",", "value", ",", "self", ".", "embed_dim", ",", "self", ".", "num_heads", ",", "\n", "self", ".", "in_proj_weight", ",", "self", ".", "in_proj_bias", ",", "\n", "self", ".", "bias_k", ",", "self", ".", "bias_v", ",", "self", ".", "add_zero_attn", ",", "\n", "self", ".", "dropout", ",", "self", ".", "out_proj", ".", "weight", ",", "self", ".", "out_proj", ".", "bias", ",", "\n", "training", "=", "self", ".", "training", ",", "\n", "key_padding_mask", "=", "key_padding_mask", ",", "need_weights", "=", "need_weights", ",", "\n", "attn_mask", "=", "attn_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.Transformer.__init__": [[570, 593], ["torch.nn.Module.__init__", "transformer.Transformer._reset_parameters", "transformer.TransformerEncoderLayer", "transformer.LayerNorm", "transformer.TransformerEncoder", "transformer.TransformerDecoderLayer", "transformer.LayerNorm", "transformer.TransformerDecoder"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.Transformer._reset_parameters"], ["def", "__init__", "(", "self", ",", "d_model", "=", "512", ",", "nhead", "=", "8", ",", "num_encoder_layers", "=", "6", ",", "\n", "num_decoder_layers", "=", "6", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ",", "\n", "custom_encoder", "=", "None", ",", "custom_decoder", "=", "None", ")", ":", "\n", "        ", "super", "(", "Transformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "custom_encoder", "is", "not", "None", ":", "\n", "            ", "self", ".", "encoder", "=", "custom_encoder", "\n", "", "else", ":", "\n", "            ", "encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", ",", "nhead", ",", "dim_feedforward", ",", "dropout", ")", "\n", "encoder_norm", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "encoder", "=", "TransformerEncoder", "(", "encoder_layer", ",", "num_encoder_layers", ",", "encoder_norm", ")", "\n", "\n", "", "if", "custom_decoder", "is", "not", "None", ":", "\n", "            ", "self", ".", "decoder", "=", "custom_decoder", "\n", "", "else", ":", "\n", "            ", "decoder_layer", "=", "TransformerDecoderLayer", "(", "d_model", ",", "nhead", ",", "dim_feedforward", ",", "dropout", ")", "\n", "decoder_norm", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "decoder", "=", "TransformerDecoder", "(", "decoder_layer", ",", "num_decoder_layers", ",", "decoder_norm", ")", "\n", "\n", "", "self", ".", "_reset_parameters", "(", ")", "\n", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "nhead", "=", "nhead", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.Transformer.forward": [[594, 645], ["transformer.Transformer.encoder", "transformer.Transformer.decoder", "src.size", "tgt.size", "RuntimeError", "RuntimeError", "src.size", "tgt.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ",", "tgt", ",", "src_mask", "=", "None", ",", "tgt_mask", "=", "None", ",", "\n", "memory_mask", "=", "None", ",", "src_key_padding_mask", "=", "None", ",", "\n", "tgt_key_padding_mask", "=", "None", ",", "memory_key_padding_mask", "=", "None", ")", ":", "\n", "        ", "r\"\"\"Take in and process masked source/target sequences.\n        Args:\n            src: the sequence to the encoder (required).\n            tgt: the sequence to the decoder (required).\n            src_mask: the additive mask for the src sequence (optional).\n            tgt_mask: the additive mask for the tgt sequence (optional).\n            memory_mask: the additive mask for the encoder output (optional).\n            src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n            tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n            memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).\n        Shape:\n            - src: :math:`(S, N, E)`.\n            - tgt: :math:`(T, N, E)`.\n            - src_mask: :math:`(S, S)`.\n            - tgt_mask: :math:`(T, T)`.\n            - memory_mask: :math:`(T, S)`.\n            - src_key_padding_mask: :math:`(N, S)`.\n            - tgt_key_padding_mask: :math:`(N, T)`.\n            - memory_key_padding_mask: :math:`(N, S)`.\n            Note: [src/tgt/memory]_mask should be filled with\n            float('-inf') for the masked positions and float(0.0) else. These masks\n            ensure that predictions for position i depend only on the unmasked positions\n            j and are applied identically for each sequence in a batch.\n            [src/tgt/memory]_key_padding_mask should be a ByteTensor where True values are positions\n            that should be masked with float('-inf') and False values will be unchanged.\n            This mask ensures that no information will be taken from position i if\n            it is masked, and has a separate mask for each sequence in a batch.\n            - output: :math:`(T, N, E)`.\n            Note: Due to the multi-head attention architecture in the transformer model,\n            the output sequence length of a transformer is same as the input sequence\n            (i.e. target) length of the decode.\n            where S is the source sequence length, T is the target sequence length, N is the\n            batch size, E is the feature number\n        Examples:\n            >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n        \"\"\"", "\n", "\n", "if", "src", ".", "size", "(", "1", ")", "!=", "tgt", ".", "size", "(", "1", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"the batch number of src and tgt must be equal\"", ")", "\n", "\n", "", "if", "src", ".", "size", "(", "2", ")", "!=", "self", ".", "d_model", "or", "tgt", ".", "size", "(", "2", ")", "!=", "self", ".", "d_model", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"the feature number of src and tgt must be equal to d_model\"", ")", "\n", "\n", "", "memory", "=", "self", ".", "encoder", "(", "src", ",", "mask", "=", "src_mask", ",", "src_key_padding_mask", "=", "src_key_padding_mask", ")", "\n", "output", "=", "self", ".", "decoder", "(", "tgt", ",", "memory", ",", "tgt_mask", "=", "tgt_mask", ",", "memory_mask", "=", "memory_mask", ",", "\n", "tgt_key_padding_mask", "=", "tgt_key_padding_mask", ",", "\n", "memory_key_padding_mask", "=", "memory_key_padding_mask", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.Transformer.generate_square_subsequent_mask": [[646, 653], ["mask.float().masked_fill().masked_fill.float().masked_fill().masked_fill.float().masked_fill().masked_fill", "float", "mask.float().masked_fill().masked_fill.float().masked_fill().masked_fill.float().masked_fill", "torch.triu", "float", "torch.ones", "mask.float().masked_fill().masked_fill.float().masked_fill().masked_fill.float"], "methods", ["None"], ["", "def", "generate_square_subsequent_mask", "(", "self", ",", "sz", ")", ":", "\n", "        ", "r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n            Unmasked positions are filled with float(0.0).\n        \"\"\"", "\n", "mask", "=", "(", "torch", ".", "triu", "(", "torch", ".", "ones", "(", "sz", ",", "sz", ")", ")", "==", "1", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "mask", "=", "mask", ".", "float", "(", ")", ".", "masked_fill", "(", "mask", "==", "0", ",", "float", "(", "'-inf'", ")", ")", ".", "masked_fill", "(", "mask", "==", "1", ",", "float", "(", "0.0", ")", ")", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.Transformer._reset_parameters": [[654, 660], ["transformer.Transformer.parameters", "p.dim", "torch.nn.init.xavier_uniform_"], "methods", ["None"], ["", "def", "_reset_parameters", "(", "self", ")", ":", "\n", "        ", "r\"\"\"Initiate parameters in the transformer model.\"\"\"", "\n", "\n", "for", "p", "in", "self", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "p", ".", "dim", "(", ")", ">", "1", ":", "\n", "                ", "xavier_uniform_", "(", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.TransformerEncoder.__init__": [[673, 678], ["torch.nn.Module.__init__", "transformer._get_clones"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer._get_clones"], ["def", "__init__", "(", "self", ",", "encoder_layer", ",", "num_layers", ",", "norm", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_get_clones", "(", "encoder_layer", ",", "num_layers", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "norm", "=", "norm", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.TransformerEncoder.forward": [[679, 698], ["range", "transformer.TransformerEncoder.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ",", "mask", "=", "None", ",", "src_key_padding_mask", "=", "None", ")", ":", "\n", "        ", "r\"\"\"Pass the input through the endocder layers in turn.\n        Args:\n            src: the sequnce to the encoder (required).\n            mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "output", "=", "src", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "output", "=", "self", ".", "layers", "[", "i", "]", "(", "output", ",", "src_mask", "=", "mask", ",", "\n", "src_key_padding_mask", "=", "src_key_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "norm", ":", "\n", "            ", "output", "=", "self", ".", "norm", "(", "output", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.TransformerDecoder.__init__": [[711, 716], ["torch.nn.Module.__init__", "transformer._get_clones"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer._get_clones"], ["def", "__init__", "(", "self", ",", "decoder_layer", ",", "num_layers", ",", "norm", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_get_clones", "(", "decoder_layer", ",", "num_layers", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "norm", "=", "norm", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.TransformerDecoder.forward": [[717, 743], ["range", "transformer.TransformerDecoder.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tgt", ",", "memory", ",", "tgt_mask", "=", "None", ",", "\n", "memory_mask", "=", "None", ",", "tgt_key_padding_mask", "=", "None", ",", "\n", "memory_key_padding_mask", "=", "None", ")", ":", "\n", "        ", "r\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\n        Args:\n            tgt: the sequence to the decoder (required).\n            memory: the sequnce from the last layer of the encoder (required).\n            tgt_mask: the mask for the tgt sequence (optional).\n            memory_mask: the mask for the memory sequence (optional).\n            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "output", "=", "tgt", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "output", "=", "self", ".", "layers", "[", "i", "]", "(", "output", ",", "memory", ",", "tgt_mask", "=", "tgt_mask", ",", "\n", "memory_mask", "=", "memory_mask", ",", "\n", "tgt_key_padding_mask", "=", "tgt_key_padding_mask", ",", "\n", "memory_key_padding_mask", "=", "memory_key_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "norm", ":", "\n", "            ", "output", "=", "self", ".", "norm", "(", "output", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.TransformerEncoderLayer.__init__": [[760, 772], ["torch.nn.Module.__init__", "transformer.MultiheadAttention", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Linear", "transformer.LayerNorm", "transformer.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "nhead", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "TransformerEncoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "# Implementation of Feedforward model", "\n", "self", ".", "linear1", "=", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "dropout", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "linear2", "=", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm1", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm2", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "dropout1", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.TransformerEncoderLayer.forward": [[773, 790], ["transformer.TransformerEncoderLayer.norm1", "transformer.TransformerEncoderLayer.linear2", "transformer.TransformerEncoderLayer.norm2", "transformer.TransformerEncoderLayer.self_attn", "transformer.TransformerEncoderLayer.dropout1", "transformer.TransformerEncoderLayer.dropout", "transformer.TransformerEncoderLayer.dropout2", "torch.nn.functional.relu", "transformer.TransformerEncoderLayer.linear1"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.dropout"], ["", "def", "forward", "(", "self", ",", "src", ",", "src_mask", "=", "None", ",", "src_key_padding_mask", "=", "None", ")", ":", "\n", "        ", "r\"\"\"Pass the input through the endocder layer.\n        Args:\n            src: the sequnce to the encoder layer (required).\n            src_mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "src2", "=", "self", ".", "self_attn", "(", "src", ",", "src", ",", "src", ",", "attn_mask", "=", "src_mask", ",", "\n", "key_padding_mask", "=", "src_key_padding_mask", ")", "[", "0", "]", "\n", "src", "=", "src", "+", "self", ".", "dropout1", "(", "src2", ")", "\n", "src", "=", "self", ".", "norm1", "(", "src", ")", "\n", "src2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "F", ".", "relu", "(", "self", ".", "linear1", "(", "src", ")", ")", ")", ")", "\n", "src", "=", "src", "+", "self", ".", "dropout2", "(", "src2", ")", "\n", "src", "=", "self", ".", "norm2", "(", "src", ")", "\n", "return", "src", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.TransformerDecoderLayer.__init__": [[808, 823], ["torch.nn.Module.__init__", "transformer.MultiheadAttention", "transformer.MultiheadAttention", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Linear", "transformer.LayerNorm", "transformer.LayerNorm", "transformer.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "nhead", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "TransformerDecoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "multihead_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "# Implementation of Feedforward model", "\n", "self", ".", "linear1", "=", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "dropout", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "linear2", "=", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm1", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm2", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm3", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "dropout1", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout3", "=", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.TransformerDecoderLayer.forward": [[824, 849], ["transformer.TransformerDecoderLayer.norm1", "transformer.TransformerDecoderLayer.norm2", "transformer.TransformerDecoderLayer.linear2", "transformer.TransformerDecoderLayer.norm3", "transformer.TransformerDecoderLayer.self_attn", "transformer.TransformerDecoderLayer.dropout1", "transformer.TransformerDecoderLayer.multihead_attn", "transformer.TransformerDecoderLayer.dropout2", "transformer.TransformerDecoderLayer.dropout", "transformer.TransformerDecoderLayer.dropout3", "torch.nn.functional.relu", "transformer.TransformerDecoderLayer.linear1"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.dropout"], ["", "def", "forward", "(", "self", ",", "tgt", ",", "memory", ",", "tgt_mask", "=", "None", ",", "memory_mask", "=", "None", ",", "\n", "tgt_key_padding_mask", "=", "None", ",", "memory_key_padding_mask", "=", "None", ")", ":", "\n", "        ", "r\"\"\"Pass the inputs (and mask) through the decoder layer.\n        Args:\n            tgt: the sequence to the decoder layer (required).\n            memory: the sequnce from the last layer of the encoder (required).\n            tgt_mask: the mask for the tgt sequence (optional).\n            memory_mask: the mask for the memory sequence (optional).\n            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "tgt2", "=", "self", ".", "self_attn", "(", "tgt", ",", "tgt", ",", "tgt", ",", "attn_mask", "=", "tgt_mask", ",", "\n", "key_padding_mask", "=", "tgt_key_padding_mask", ")", "[", "0", "]", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout1", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm1", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "multihead_attn", "(", "tgt", ",", "memory", ",", "memory", ",", "attn_mask", "=", "memory_mask", ",", "\n", "key_padding_mask", "=", "memory_key_padding_mask", ")", "[", "0", "]", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout2", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm2", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "F", ".", "relu", "(", "self", ".", "linear1", "(", "tgt", ")", ")", ")", ")", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout3", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm3", "(", "tgt", ")", "\n", "return", "tgt", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer._get_softmax_dim": [[17, 26], ["warnings.warn"], "function", ["None"], ["def", "_get_softmax_dim", "(", "name", ",", "ndim", ",", "stacklevel", ")", ":", "\n", "# type: (str, int, int) -> int", "\n", "    ", "warnings", ".", "warn", "(", "\"Implicit dimension choice for {} has been deprecated. \"", "\n", "\"Change the call to include dim=X as an argument.\"", ".", "format", "(", "name", ")", ",", "stacklevel", "=", "stacklevel", ")", "\n", "if", "ndim", "==", "0", "or", "ndim", "==", "1", "or", "ndim", "==", "3", ":", "\n", "        ", "ret", "=", "0", "\n", "", "else", ":", "\n", "        ", "ret", "=", "1", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.softmax": [[27, 53], ["transformer._get_softmax_dim", "input.softmax", "input.softmax", "input.dim"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer._get_softmax_dim", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.softmax", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.softmax"], ["", "def", "softmax", "(", "input", ",", "dim", "=", "None", ",", "_stacklevel", "=", "3", ",", "dtype", "=", "None", ")", ":", "\n", "# type: (Tensor, Optional[int], int, Optional[int]) -> Tensor", "\n", "    ", "r\"\"\"Applies a softmax function.\n    Softmax is defined as:\n    :math:`\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)}`\n    It is applied to all slices along dim, and will re-scale them so that the elements\n    lie in the range `[0, 1]` and sum to 1.\n    See :class:`~torch.nn.Softmax` for more details.\n    Arguments:\n        input (Tensor): input\n        dim (int): A dimension along which softmax will be computed.\n        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n          If specified, the input tensor is casted to :attr:`dtype` before the operation\n          is performed. This is useful for preventing data type overflows. Default: None.\n    .. note::\n        This function doesn't work directly with NLLLoss,\n        which expects the Log to be computed between the Softmax and itself.\n        Use log_softmax instead (it's faster and has better numerical properties).\n    \"\"\"", "\n", "if", "dim", "is", "None", ":", "\n", "        ", "dim", "=", "_get_softmax_dim", "(", "'softmax'", ",", "input", ".", "dim", "(", ")", ",", "_stacklevel", ")", "\n", "", "if", "dtype", "is", "None", ":", "\n", "        ", "ret", "=", "input", ".", "softmax", "(", "dim", ")", "\n", "", "else", ":", "\n", "        ", "ret", "=", "input", ".", "softmax", "(", "dim", ",", "dtype", "=", "dtype", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear": [[54, 74], ["torch.addmm", "input.matmul", "input.dim", "weight.t", "weight.t"], "function", ["None"], ["", "def", "linear", "(", "input", ",", "weight", ",", "bias", "=", "None", ")", ":", "\n", "# type: (Tensor, Tensor, Optional[Tensor]) -> Tensor", "\n", "    ", "r\"\"\"\n    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n    Shape:\n        - Input: :math:`(N, *, in\\_features)` where `*` means any number of\n          additional dimensions\n        - Weight: :math:`(out\\_features, in\\_features)`\n        - Bias: :math:`(out\\_features)`\n        - Output: :math:`(N, *, out\\_features)`\n    \"\"\"", "\n", "if", "input", ".", "dim", "(", ")", "==", "2", "and", "bias", "is", "not", "None", ":", "\n", "# fused op is marginally faster", "\n", "        ", "ret", "=", "torch", ".", "addmm", "(", "bias", ",", "input", ",", "weight", ".", "t", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "output", "=", "input", ".", "matmul", "(", "weight", ".", "t", "(", ")", ")", "\n", "if", "bias", "is", "not", "None", ":", "\n", "            ", "output", "+=", "bias", "\n", "", "ret", "=", "output", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.dropout": [[75, 93], ["ValueError", "torch.nn._VF.dropout_", "torch.nn._VF.dropout"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.dropout"], ["", "def", "dropout", "(", "input", ",", "p", "=", "0.5", ",", "training", "=", "True", ",", "inplace", "=", "False", ")", ":", "\n", "# type: (Tensor, float, bool, bool) -> Tensor", "\n", "    ", "r\"\"\"\n    During training, randomly zeroes some of the elements of the input\n    tensor with probability :attr:`p` using samples from a Bernoulli\n    distribution.\n    See :class:`~torch.nn.Dropout` for details.\n    Args:\n        p: probability of an element to be zeroed. Default: 0.5\n        training: apply dropout if is ``True``. Default: ``True``\n        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n    \"\"\"", "\n", "if", "p", "<", "0.", "or", "p", ">", "1.", ":", "\n", "        ", "raise", "ValueError", "(", "\"dropout probability has to be between 0 and 1, \"", "\n", "\"but got {}\"", ".", "format", "(", "p", ")", ")", "\n", "", "return", "(", "_VF", ".", "dropout_", "(", "input", ",", "p", ",", "training", ")", "\n", "if", "inplace", "\n", "else", "_VF", ".", "dropout", "(", "input", ",", "p", ",", "training", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.multi_head_attention_forward": [[94, 344], ["torch.equal", "query.size", "linear.contiguous().view().transpose", "linear.size", "torch.bmm", "transformer.softmax", "transformer.dropout", "torch.bmm", "linear.transpose().contiguous().view", "transformer.linear", "torch.equal", "torch.equal", "list", "key.size", "value.size", "float", "torch.jit._unwrap_optional", "torch.jit._unwrap_optional.size", "torch.jit._unwrap_optional", "torch.jit._unwrap_optional.size", "torch.jit._unwrap_optional", "torch.jit._unwrap_optional.size", "linear.contiguous().view().transpose", "linear.contiguous().view().transpose", "torch.cat", "torch.cat", "linear.transpose", "list", "torch.cat.unsqueeze", "attn_output_weights.view.view", "attn_output_weights.view.masked_fill", "attn_output_weights.view.view", "list", "attn_output_weights.view.view", "query.size", "linear().chunk", "transformer.linear", "transformer.linear", "transformer.linear", "transformer.linear", "transformer.linear", "transformer.linear", "torch.cat", "torch.cat", "linear.contiguous().view", "static_k.size", "static_k.size", "static_v.size", "static_v.size", "torch.cat.size", "torch.cat.size", "torch.cat", "torch.cat", "attn_output_weights.view.size", "torch.cat.unsqueeze().unsqueeze", "float", "linear.size", "linear.transpose().contiguous", "transformer.linear", "transformer.linear", "transformer.linear", "transformer.linear", "query.size", "key.size", "value.size", "torch.cat", "torch.cat", "linear.contiguous().view", "linear.contiguous().view", "torch.zeros", "torch.zeros", "attn_output_weights.view.sum", "transformer.linear", "linear().chunk", "bias_k.repeat", "bias_v.repeat", "linear.contiguous", "torch.zeros", "torch.zeros", "torch.cat.unsqueeze", "linear.transpose", "torch.zeros", "torch.zeros", "linear.contiguous", "linear.contiguous", "transformer.linear", "linear.size", "linear.size", "linear.size", "linear.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size"], "function", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.softmax", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.dropout", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.linear"], ["", "def", "multi_head_attention_forward", "(", "query", ",", "# type: Tensor", "\n", "key", ",", "# type: Tensor", "\n", "value", ",", "# type: Tensor", "\n", "embed_dim_to_check", ",", "# type: int", "\n", "num_heads", ",", "# type: int", "\n", "in_proj_weight", ",", "# type: Tensor", "\n", "in_proj_bias", ",", "# type: Tensor", "\n", "bias_k", ",", "# type: Optional[Tensor]", "\n", "bias_v", ",", "# type: Optional[Tensor]", "\n", "add_zero_attn", ",", "# type: bool", "\n", "dropout_p", ",", "# type: float", "\n", "out_proj_weight", ",", "# type: Tensor", "\n", "out_proj_bias", ",", "# type: Tensor", "\n", "training", "=", "True", ",", "# type: bool", "\n", "key_padding_mask", "=", "None", ",", "# type: Optional[Tensor]", "\n", "need_weights", "=", "True", ",", "# type: bool", "\n", "attn_mask", "=", "None", ",", "# type: Optional[Tensor]", "\n", "use_separate_proj_weight", "=", "False", ",", "# type: bool", "\n", "q_proj_weight", "=", "None", ",", "# type: Optional[Tensor]", "\n", "k_proj_weight", "=", "None", ",", "# type: Optional[Tensor]", "\n", "v_proj_weight", "=", "None", ",", "# type: Optional[Tensor]", "\n", "static_k", "=", "None", ",", "# type: Optional[Tensor]", "\n", "static_v", "=", "None", "# type: Optional[Tensor]", "\n", ")", ":", "\n", "# type: (...) -> Tuple[Tensor, Optional[Tensor]]", "\n", "    ", "r\"\"\"\n    Args:\n        query, key, value: map a query and a set of key-value pairs to an output.\n            See \"Attention Is All You Need\" for more details.\n        embed_dim_to_check: total dimension of the model.\n        num_heads: parallel attention heads.\n        in_proj_weight, in_proj_bias: input projection weight and bias.\n        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n        add_zero_attn: add a new batch of zeros to the key and\n                       value sequences at dim=1.\n        dropout_p: probability of an element to be zeroed.\n        out_proj_weight, out_proj_bias: the output projection weight and bias.\n        training: apply dropout if is ``True``.\n        key_padding_mask: if provided, specified padding elements in the key will\n            be ignored by the attention. This is an binary mask. When the value is True,\n            the corresponding value on the attention layer will be filled with -inf.\n        need_weights: output attn_output_weights.\n        attn_mask: mask that prevents attention to certain positions. This is an additive mask\n            (i.e. the values will be added to the attention layer).\n        use_separate_proj_weight: the function accept the proj. weights for query, key,\n            and value in differnt forms. If false, in_proj_weight will be used, which is\n            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n        static_k, static_v: static key and value used for attention operators.\n    Shape:\n        Inputs:\n        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n          the embedding dimension.\n        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n          the embedding dimension.\n        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n          the embedding dimension.\n        - key_padding_mask: :math:`(N, S)`, ByteTensor, where N is the batch size, S is the source sequence length.\n        - attn_mask: :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n        Outputs:\n        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n          E is the embedding dimension.\n        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n          L is the target sequence length, S is the source sequence length.\n    \"\"\"", "\n", "\n", "qkv_same", "=", "torch", ".", "equal", "(", "query", ",", "key", ")", "and", "torch", ".", "equal", "(", "key", ",", "value", ")", "\n", "kv_same", "=", "torch", ".", "equal", "(", "key", ",", "value", ")", "\n", "\n", "tgt_len", ",", "bsz", ",", "embed_dim", "=", "query", ".", "size", "(", ")", "\n", "assert", "embed_dim", "==", "embed_dim_to_check", "\n", "assert", "list", "(", "query", ".", "size", "(", ")", ")", "==", "[", "tgt_len", ",", "bsz", ",", "embed_dim", "]", "\n", "assert", "key", ".", "size", "(", ")", "==", "value", ".", "size", "(", ")", "\n", "\n", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "assert", "head_dim", "*", "num_heads", "==", "embed_dim", ",", "\"embed_dim must be divisible by num_heads\"", "\n", "scaling", "=", "float", "(", "head_dim", ")", "**", "-", "0.5", "\n", "\n", "if", "use_separate_proj_weight", "is", "not", "True", ":", "\n", "        ", "if", "qkv_same", ":", "\n", "# self-attention", "\n", "            ", "q", ",", "k", ",", "v", "=", "linear", "(", "query", ",", "in_proj_weight", ",", "in_proj_bias", ")", ".", "chunk", "(", "3", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "elif", "kv_same", ":", "\n", "# encoder-decoder attention", "\n", "# This is inline in_proj function with in_proj_weight and in_proj_bias", "\n", "            ", "_b", "=", "in_proj_bias", "\n", "_start", "=", "0", "\n", "_end", "=", "embed_dim", "\n", "_w", "=", "in_proj_weight", "[", "_start", ":", "_end", ",", ":", "]", "\n", "if", "_b", "is", "not", "None", ":", "\n", "                ", "_b", "=", "_b", "[", "_start", ":", "_end", "]", "\n", "", "q", "=", "linear", "(", "query", ",", "_w", ",", "_b", ")", "\n", "\n", "if", "key", "is", "None", ":", "\n", "                ", "assert", "value", "is", "None", "\n", "k", "=", "None", "\n", "v", "=", "None", "\n", "", "else", ":", "\n", "\n", "# This is inline in_proj function with in_proj_weight and in_proj_bias", "\n", "                ", "_b", "=", "in_proj_bias", "\n", "_start", "=", "embed_dim", "\n", "_end", "=", "None", "\n", "_w", "=", "in_proj_weight", "[", "_start", ":", ",", ":", "]", "\n", "if", "_b", "is", "not", "None", ":", "\n", "                    ", "_b", "=", "_b", "[", "_start", ":", "]", "\n", "", "k", ",", "v", "=", "linear", "(", "key", ",", "_w", ",", "_b", ")", ".", "chunk", "(", "2", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "", "else", ":", "\n", "# This is inline in_proj function with in_proj_weight and in_proj_bias", "\n", "            ", "_b", "=", "in_proj_bias", "\n", "_start", "=", "0", "\n", "_end", "=", "embed_dim", "\n", "_w", "=", "in_proj_weight", "[", "_start", ":", "_end", ",", ":", "]", "\n", "if", "_b", "is", "not", "None", ":", "\n", "                ", "_b", "=", "_b", "[", "_start", ":", "_end", "]", "\n", "", "q", "=", "linear", "(", "query", ",", "_w", ",", "_b", ")", "\n", "\n", "# This is inline in_proj function with in_proj_weight and in_proj_bias", "\n", "_b", "=", "in_proj_bias", "\n", "_start", "=", "embed_dim", "\n", "_end", "=", "embed_dim", "*", "2", "\n", "_w", "=", "in_proj_weight", "[", "_start", ":", "_end", ",", ":", "]", "\n", "if", "_b", "is", "not", "None", ":", "\n", "                ", "_b", "=", "_b", "[", "_start", ":", "_end", "]", "\n", "", "k", "=", "linear", "(", "key", ",", "_w", ",", "_b", ")", "\n", "\n", "# This is inline in_proj function with in_proj_weight and in_proj_bias", "\n", "_b", "=", "in_proj_bias", "\n", "_start", "=", "embed_dim", "*", "2", "\n", "_end", "=", "None", "\n", "_w", "=", "in_proj_weight", "[", "_start", ":", ",", ":", "]", "\n", "if", "_b", "is", "not", "None", ":", "\n", "                ", "_b", "=", "_b", "[", "_start", ":", "]", "\n", "", "v", "=", "linear", "(", "value", ",", "_w", ",", "_b", ")", "\n", "", "", "else", ":", "\n", "        ", "q_proj_weight_non_opt", "=", "torch", ".", "jit", ".", "_unwrap_optional", "(", "q_proj_weight", ")", "\n", "len1", ",", "len2", "=", "q_proj_weight_non_opt", ".", "size", "(", ")", "\n", "assert", "len1", "==", "embed_dim", "and", "len2", "==", "query", ".", "size", "(", "-", "1", ")", "\n", "\n", "k_proj_weight_non_opt", "=", "torch", ".", "jit", ".", "_unwrap_optional", "(", "k_proj_weight", ")", "\n", "len1", ",", "len2", "=", "k_proj_weight_non_opt", ".", "size", "(", ")", "\n", "assert", "len1", "==", "embed_dim", "and", "len2", "==", "key", ".", "size", "(", "-", "1", ")", "\n", "\n", "v_proj_weight_non_opt", "=", "torch", ".", "jit", ".", "_unwrap_optional", "(", "v_proj_weight", ")", "\n", "len1", ",", "len2", "=", "v_proj_weight_non_opt", ".", "size", "(", ")", "\n", "assert", "len1", "==", "embed_dim", "and", "len2", "==", "value", ".", "size", "(", "-", "1", ")", "\n", "\n", "if", "in_proj_bias", "is", "not", "None", ":", "\n", "            ", "q", "=", "linear", "(", "query", ",", "q_proj_weight_non_opt", ",", "in_proj_bias", "[", "0", ":", "embed_dim", "]", ")", "\n", "k", "=", "linear", "(", "key", ",", "k_proj_weight_non_opt", ",", "in_proj_bias", "[", "embed_dim", ":", "(", "embed_dim", "*", "2", ")", "]", ")", "\n", "v", "=", "linear", "(", "value", ",", "v_proj_weight_non_opt", ",", "in_proj_bias", "[", "(", "embed_dim", "*", "2", ")", ":", "]", ")", "\n", "", "else", ":", "\n", "            ", "q", "=", "linear", "(", "query", ",", "q_proj_weight_non_opt", ",", "in_proj_bias", ")", "\n", "k", "=", "linear", "(", "key", ",", "k_proj_weight_non_opt", ",", "in_proj_bias", ")", "\n", "v", "=", "linear", "(", "value", ",", "v_proj_weight_non_opt", ",", "in_proj_bias", ")", "\n", "", "", "q", "=", "q", "*", "scaling", "\n", "\n", "if", "bias_k", "is", "not", "None", "and", "bias_v", "is", "not", "None", ":", "\n", "        ", "if", "static_k", "is", "None", "and", "static_v", "is", "None", ":", "\n", "            ", "k", "=", "torch", ".", "cat", "(", "[", "k", ",", "bias_k", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v", ",", "bias_v", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "attn_mask", "=", "torch", ".", "cat", "(", "[", "attn_mask", ",", "\n", "torch", ".", "zeros", "(", "(", "attn_mask", ".", "size", "(", "0", ")", ",", "1", ")", ",", "\n", "dtype", "=", "attn_mask", ".", "dtype", ",", "\n", "device", "=", "attn_mask", ".", "device", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "                ", "key_padding_mask", "=", "torch", ".", "cat", "(", "\n", "[", "key_padding_mask", ",", "torch", ".", "zeros", "(", "(", "key_padding_mask", ".", "size", "(", "0", ")", ",", "1", ")", ",", "\n", "dtype", "=", "key_padding_mask", ".", "dtype", ",", "\n", "device", "=", "key_padding_mask", ".", "device", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "assert", "static_k", "is", "None", ",", "\"bias cannot be added to static key.\"", "\n", "assert", "static_v", "is", "None", ",", "\"bias cannot be added to static value.\"", "\n", "", "", "else", ":", "\n", "        ", "assert", "bias_k", "is", "None", "\n", "assert", "bias_v", "is", "None", "\n", "\n", "", "q", "=", "q", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", "*", "num_heads", ",", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "k", "is", "not", "None", ":", "\n", "        ", "k", "=", "k", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "num_heads", ",", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "if", "v", "is", "not", "None", ":", "\n", "        ", "v", "=", "v", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "num_heads", ",", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "", "if", "static_k", "is", "not", "None", ":", "\n", "        ", "assert", "static_k", ".", "size", "(", "0", ")", "==", "bsz", "*", "num_heads", "\n", "assert", "static_k", ".", "size", "(", "2", ")", "==", "head_dim", "\n", "k", "=", "static_k", "\n", "\n", "", "if", "static_v", "is", "not", "None", ":", "\n", "        ", "assert", "static_v", ".", "size", "(", "0", ")", "==", "bsz", "*", "num_heads", "\n", "assert", "static_v", ".", "size", "(", "2", ")", "==", "head_dim", "\n", "v", "=", "static_v", "\n", "\n", "", "src_len", "=", "k", ".", "size", "(", "1", ")", "\n", "\n", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "        ", "assert", "key_padding_mask", ".", "size", "(", "0", ")", "==", "bsz", "\n", "assert", "key_padding_mask", ".", "size", "(", "1", ")", "==", "src_len", "\n", "\n", "", "if", "add_zero_attn", ":", "\n", "        ", "src_len", "+=", "1", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k", ",", "torch", ".", "zeros", "(", "(", "k", ".", "size", "(", "0", ")", ",", "1", ")", "+", "k", ".", "size", "(", ")", "[", "2", ":", "]", ",", "dtype", "=", "k", ".", "dtype", ",", "device", "=", "k", ".", "device", ")", "]", ",", "dim", "=", "1", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v", ",", "torch", ".", "zeros", "(", "(", "v", ".", "size", "(", "0", ")", ",", "1", ")", "+", "v", ".", "size", "(", ")", "[", "2", ":", "]", ",", "dtype", "=", "v", ".", "dtype", ",", "device", "=", "v", ".", "device", ")", "]", ",", "dim", "=", "1", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "attn_mask", "=", "torch", ".", "cat", "(", "[", "attn_mask", ",", "torch", ".", "zeros", "(", "(", "attn_mask", ".", "size", "(", "0", ")", ",", "1", ")", ",", "\n", "dtype", "=", "attn_mask", ".", "dtype", ",", "\n", "device", "=", "attn_mask", ".", "device", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "            ", "key_padding_mask", "=", "torch", ".", "cat", "(", "\n", "[", "key_padding_mask", ",", "torch", ".", "zeros", "(", "(", "key_padding_mask", ".", "size", "(", "0", ")", ",", "1", ")", ",", "\n", "dtype", "=", "key_padding_mask", ".", "dtype", ",", "\n", "device", "=", "key_padding_mask", ".", "device", ")", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "", "attn_output_weights", "=", "torch", ".", "bmm", "(", "q", ",", "k", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "assert", "list", "(", "attn_output_weights", ".", "size", "(", ")", ")", "==", "[", "bsz", "*", "num_heads", ",", "tgt_len", ",", "src_len", "]", "\n", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "        ", "attn_mask", "=", "attn_mask", ".", "unsqueeze", "(", "0", ")", "\n", "attn_output_weights", "+=", "attn_mask", "\n", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "        ", "attn_output_weights", "=", "attn_output_weights", ".", "view", "(", "bsz", ",", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "attn_output_weights", "=", "attn_output_weights", ".", "masked_fill", "(", "\n", "key_padding_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ",", "\n", "float", "(", "'-inf'", ")", ",", "\n", ")", "\n", "attn_output_weights", "=", "attn_output_weights", ".", "view", "(", "bsz", "*", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "\n", "", "attn_output_weights", "=", "softmax", "(", "\n", "attn_output_weights", ",", "dim", "=", "-", "1", ")", "\n", "attn_output_weights", "=", "dropout", "(", "attn_output_weights", ",", "p", "=", "dropout_p", ",", "training", "=", "training", ")", "\n", "\n", "attn_output", "=", "torch", ".", "bmm", "(", "attn_output_weights", ",", "v", ")", "\n", "assert", "list", "(", "attn_output", ".", "size", "(", ")", ")", "==", "[", "bsz", "*", "num_heads", ",", "tgt_len", ",", "head_dim", "]", "\n", "attn_output", "=", "attn_output", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "attn_output", "=", "linear", "(", "attn_output", ",", "out_proj_weight", ",", "out_proj_bias", ")", "\n", "\n", "if", "need_weights", ":", "\n", "# average attention weights over heads", "\n", "        ", "attn_output_weights", "=", "attn_output_weights", ".", "view", "(", "bsz", ",", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "return", "attn_output", ",", "attn_output_weights", ".", "sum", "(", "dim", "=", "1", ")", "/", "num_heads", "\n", "", "else", ":", "\n", "        ", "return", "attn_output", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer._get_clones": [[851, 853], ["torch.nn.ModuleList", "copy.deepcopy", "range"], "function", ["None"], ["", "", "def", "_get_clones", "(", "module", ",", "N", ")", ":", "\n", "    ", "return", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "module", ")", "for", "i", "in", "range", "(", "N", ")", "]", ")", "", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertLSTM.__init__": [[12, 22], ["pytorch_transformers.modeling_bert.BertPreTrainedModel.__init__", "pytorch_transformers.modeling_bert.BertModel", "torch.nn.Dropout", "torch.nn.LSTM", "torch.nn.Sequential", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__"], ["def", "__init__", "(", "self", ",", "bert_model_config", ":", "BertConfig", ")", ":", "\n", "        ", "super", "(", "DocumentBertLSTM", ",", "self", ")", ".", "__init__", "(", "bert_model_config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "bert_model_config", ")", "\n", "self", ".", "bert_batch_size", "=", "self", ".", "bert", ".", "config", ".", "bert_batch_size", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "bert_model_config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "lstm", "=", "LSTM", "(", "bert_model_config", ".", "hidden_size", ",", "bert_model_config", ".", "hidden_size", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Dropout", "(", "p", "=", "bert_model_config", ".", "hidden_dropout_prob", ")", ",", "\n", "nn", ".", "Linear", "(", "bert_model_config", ".", "hidden_size", ",", "bert_model_config", ".", "num_labels", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertLSTM.forward": [[25, 49], ["torch.zeros", "range", "document_bert_architectures.DocumentBertLSTM.lstm", "document_bert_architectures.DocumentBertLSTM.classifier", "document_bert_architectures.DocumentBertLSTM.dropout", "torch.zeros.permute", "min", "document_bert_architectures.DocumentBertLSTM.bert"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.dropout"], ["", "def", "forward", "(", "self", ",", "document_batch", ":", "torch", ".", "Tensor", ",", "document_sequence_lengths", ":", "list", ",", "device", "=", "'cuda'", ")", ":", "\n", "\n", "#contains all BERT sequences", "\n", "#bert should output a (batch_size, num_sequences, bert_hidden_size)", "\n", "        ", "bert_output", "=", "torch", ".", "zeros", "(", "size", "=", "(", "document_batch", ".", "shape", "[", "0", "]", ",", "\n", "min", "(", "document_batch", ".", "shape", "[", "1", "]", ",", "self", ".", "bert_batch_size", ")", ",", "\n", "self", ".", "bert", ".", "config", ".", "hidden_size", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "\n", "#only pass through bert_batch_size numbers of inputs into bert.", "\n", "#this means that we are possibly cutting off the last part of documents.", "\n", "\n", "for", "doc_id", "in", "range", "(", "document_batch", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "bert_output", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", "]", "=", "self", ".", "dropout", "(", "self", ".", "bert", "(", "document_batch", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", ",", "0", "]", ",", "\n", "token_type_ids", "=", "document_batch", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", ",", "1", "]", ",", "\n", "attention_mask", "=", "document_batch", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", ",", "2", "]", ")", "[", "1", "]", ")", "\n", "\n", "", "output", ",", "(", "_", ",", "_", ")", "=", "self", ".", "lstm", "(", "bert_output", ".", "permute", "(", "1", ",", "0", ",", "2", ")", ")", "\n", "\n", "last_layer", "=", "output", "[", "-", "1", "]", "\n", "\n", "prediction", "=", "self", ".", "classifier", "(", "last_layer", ")", "\n", "\n", "assert", "prediction", ".", "shape", "[", "0", "]", "==", "document_batch", ".", "shape", "[", "0", "]", "\n", "return", "prediction", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertLSTM.freeze_bert_encoder": [[50, 53], ["document_bert_architectures.DocumentBertLSTM.bert.parameters"], "methods", ["None"], ["", "def", "freeze_bert_encoder", "(", "self", ")", ":", "\n", "        ", "for", "param", "in", "self", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertLSTM.unfreeze_bert_encoder": [[54, 57], ["document_bert_architectures.DocumentBertLSTM.bert.parameters"], "methods", ["None"], ["", "", "def", "unfreeze_bert_encoder", "(", "self", ")", ":", "\n", "        ", "for", "param", "in", "self", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertLSTM.unfreeze_bert_encoder_last_layers": [[58, 62], ["document_bert_architectures.DocumentBertLSTM.bert.named_parameters"], "methods", ["None"], ["", "", "def", "unfreeze_bert_encoder_last_layers", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "param", "in", "self", ".", "bert", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "\"encoder.layer.11\"", "in", "name", "or", "\"pooler\"", "in", "name", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", "def", "unfreeze_bert_encoder_pooler_layer", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertLSTM.unfreeze_bert_encoder_pooler_layer": [[62, 66], ["document_bert_architectures.DocumentBertLSTM.bert.named_parameters"], "methods", ["None"], ["", "", "", "def", "unfreeze_bert_encoder_pooler_layer", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "param", "in", "self", ".", "bert", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "\"pooler\"", "in", "name", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertLinear.__init__": [[73, 83], ["pytorch_transformers.modeling_bert.BertPreTrainedModel.__init__", "pytorch_transformers.modeling_bert.BertModel", "torch.nn.Dropout", "torch.nn.Sequential", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__"], ["def", "__init__", "(", "self", ",", "bert_model_config", ":", "BertConfig", ")", ":", "\n", "        ", "super", "(", "DocumentBertLinear", ",", "self", ")", ".", "__init__", "(", "bert_model_config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "bert_model_config", ")", "\n", "self", ".", "bert_batch_size", "=", "self", ".", "bert", ".", "config", ".", "bert_batch_size", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "bert_model_config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Dropout", "(", "p", "=", "bert_model_config", ".", "hidden_dropout_prob", ")", ",", "\n", "nn", ".", "Linear", "(", "bert_model_config", ".", "hidden_size", "*", "self", ".", "bert_batch_size", ",", "bert_model_config", ".", "num_labels", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertLinear.forward": [[86, 106], ["torch.zeros", "range", "document_bert_architectures.DocumentBertLinear.classifier", "document_bert_architectures.DocumentBertLinear.dropout", "torch.zeros.view", "min", "document_bert_architectures.DocumentBertLinear.bert"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.dropout"], ["", "def", "forward", "(", "self", ",", "document_batch", ":", "torch", ".", "Tensor", ",", "document_sequence_lengths", ":", "list", ",", "device", "=", "'cuda'", ")", ":", "\n", "\n", "#contains all BERT sequences", "\n", "#bert should output a (batch_size, num_sequences, bert_hidden_size)", "\n", "        ", "bert_output", "=", "torch", ".", "zeros", "(", "size", "=", "(", "document_batch", ".", "shape", "[", "0", "]", ",", "\n", "min", "(", "document_batch", ".", "shape", "[", "1", "]", ",", "self", ".", "bert_batch_size", ")", ",", "\n", "self", ".", "bert", ".", "config", ".", "hidden_size", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "'cuda'", ")", "\n", "\n", "#only pass through bert_batch_size numbers of inputs into bert.", "\n", "#this means that we are possibly cutting off the last part of documents.", "\n", "\n", "for", "doc_id", "in", "range", "(", "document_batch", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "bert_output", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", "]", "=", "self", ".", "dropout", "(", "self", ".", "bert", "(", "document_batch", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", ",", "0", "]", ",", "\n", "token_type_ids", "=", "document_batch", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", ",", "1", "]", ",", "\n", "attention_mask", "=", "document_batch", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", ",", "2", "]", ")", "[", "1", "]", ")", "\n", "\n", "\n", "", "prediction", "=", "self", ".", "classifier", "(", "bert_output", ".", "view", "(", "bert_output", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ")", "\n", "assert", "prediction", ".", "shape", "[", "0", "]", "==", "document_batch", ".", "shape", "[", "0", "]", "\n", "return", "prediction", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertLinear.freeze_bert_encoder": [[107, 110], ["document_bert_architectures.DocumentBertLinear.bert.parameters"], "methods", ["None"], ["", "def", "freeze_bert_encoder", "(", "self", ")", ":", "\n", "        ", "for", "param", "in", "self", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertLinear.unfreeze_bert_encoder": [[111, 114], ["document_bert_architectures.DocumentBertLinear.bert.parameters"], "methods", ["None"], ["", "", "def", "unfreeze_bert_encoder", "(", "self", ")", ":", "\n", "        ", "for", "param", "in", "self", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertLinear.unfreeze_bert_encoder_last_layers": [[115, 119], ["document_bert_architectures.DocumentBertLinear.bert.named_parameters"], "methods", ["None"], ["", "", "def", "unfreeze_bert_encoder_last_layers", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "param", "in", "self", ".", "bert", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "\"encoder.layer.11\"", "in", "name", "or", "\"pooler\"", "in", "name", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", "def", "unfreeze_bert_encoder_pooler_layer", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertLinear.unfreeze_bert_encoder_pooler_layer": [[119, 123], ["document_bert_architectures.DocumentBertLinear.bert.named_parameters"], "methods", ["None"], ["", "", "", "def", "unfreeze_bert_encoder_pooler_layer", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "param", "in", "self", ".", "bert", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "\"pooler\"", "in", "name", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertMaxPool.__init__": [[130, 144], ["pytorch_transformers.modeling_bert.BertPreTrainedModel.__init__", "pytorch_transformers.modeling_bert.BertModel", "torch.nn.Dropout", "torch.nn.Sequential", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__"], ["def", "__init__", "(", "self", ",", "bert_model_config", ":", "BertConfig", ")", ":", "\n", "        ", "super", "(", "DocumentBertMaxPool", ",", "self", ")", ".", "__init__", "(", "bert_model_config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "bert_model_config", ")", "\n", "self", ".", "bert_batch_size", "=", "self", ".", "bert", ".", "config", ".", "bert_batch_size", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "bert_model_config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# self.transformer_encoder = TransformerEncoderLayer(d_model=bert_model_config.hidden_size,", "\n", "#                                            nhead=6,", "\n", "#                                            dropout=bert_model_config.hidden_dropout_prob)", "\n", "#self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=6, norm=nn.LayerNorm(bert_model_config.hidden_size))", "\n", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Dropout", "(", "p", "=", "bert_model_config", ".", "hidden_dropout_prob", ")", ",", "\n", "nn", ".", "Linear", "(", "bert_model_config", ".", "hidden_size", ",", "bert_model_config", ".", "num_labels", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertMaxPool.forward": [[147, 167], ["torch.zeros", "range", "document_bert_architectures.DocumentBertMaxPool.classifier", "document_bert_architectures.DocumentBertMaxPool.dropout", "torch.zeros.max", "min", "document_bert_architectures.DocumentBertMaxPool.bert"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.dropout"], ["", "def", "forward", "(", "self", ",", "document_batch", ":", "torch", ".", "Tensor", ",", "document_sequence_lengths", ":", "list", ",", "device", "=", "'cuda'", ")", ":", "\n", "\n", "#contains all BERT sequences", "\n", "#bert should output a (batch_size, num_sequences, bert_hidden_size)", "\n", "        ", "bert_output", "=", "torch", ".", "zeros", "(", "size", "=", "(", "document_batch", ".", "shape", "[", "0", "]", ",", "\n", "min", "(", "document_batch", ".", "shape", "[", "1", "]", ",", "self", ".", "bert_batch_size", ")", ",", "\n", "self", ".", "bert", ".", "config", ".", "hidden_size", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "'cuda'", ")", "\n", "\n", "#only pass through bert_batch_size numbers of inputs into bert.", "\n", "#this means that we are possibly cutting off the last part of documents.", "\n", "\n", "for", "doc_id", "in", "range", "(", "document_batch", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "bert_output", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", "]", "=", "self", ".", "dropout", "(", "self", ".", "bert", "(", "document_batch", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", ",", "0", "]", ",", "\n", "token_type_ids", "=", "document_batch", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", ",", "1", "]", ",", "\n", "attention_mask", "=", "document_batch", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", ",", "2", "]", ")", "[", "1", "]", ")", "\n", "\n", "\n", "", "prediction", "=", "self", ".", "classifier", "(", "bert_output", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", ")", "\n", "assert", "prediction", ".", "shape", "[", "0", "]", "==", "document_batch", ".", "shape", "[", "0", "]", "\n", "return", "prediction", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertMaxPool.freeze_bert_encoder": [[168, 171], ["document_bert_architectures.DocumentBertMaxPool.bert.parameters"], "methods", ["None"], ["", "def", "freeze_bert_encoder", "(", "self", ")", ":", "\n", "        ", "for", "param", "in", "self", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertMaxPool.unfreeze_bert_encoder": [[172, 175], ["document_bert_architectures.DocumentBertMaxPool.bert.parameters"], "methods", ["None"], ["", "", "def", "unfreeze_bert_encoder", "(", "self", ")", ":", "\n", "        ", "for", "param", "in", "self", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertMaxPool.unfreeze_bert_encoder_last_layers": [[176, 180], ["document_bert_architectures.DocumentBertMaxPool.bert.named_parameters"], "methods", ["None"], ["", "", "def", "unfreeze_bert_encoder_last_layers", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "param", "in", "self", ".", "bert", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "\"encoder.layer.11\"", "in", "name", "or", "\"pooler\"", "in", "name", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", "def", "unfreeze_bert_encoder_pooler_layer", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertMaxPool.unfreeze_bert_encoder_pooler_layer": [[180, 184], ["document_bert_architectures.DocumentBertMaxPool.bert.named_parameters"], "methods", ["None"], ["", "", "", "def", "unfreeze_bert_encoder_pooler_layer", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "param", "in", "self", ".", "bert", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "\"pooler\"", "in", "name", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertTransformer.__init__": [[191, 205], ["pytorch_transformers.modeling_bert.BertPreTrainedModel.__init__", "pytorch_transformers.modeling_bert.BertModel", "torch.nn.Dropout", "transformer.TransformerEncoderLayer", "transformer.TransformerEncoder", "torch.nn.Sequential", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__"], ["def", "__init__", "(", "self", ",", "bert_model_config", ":", "BertConfig", ")", ":", "\n", "        ", "super", "(", "DocumentBertTransformer", ",", "self", ")", ".", "__init__", "(", "bert_model_config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "bert_model_config", ")", "\n", "self", ".", "bert_batch_size", "=", "self", ".", "bert", ".", "config", ".", "bert_batch_size", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "bert_model_config", ".", "hidden_dropout_prob", ")", "\n", "\n", "encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", "=", "bert_model_config", ".", "hidden_size", ",", "\n", "nhead", "=", "6", ",", "\n", "dropout", "=", "bert_model_config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "transformer_encoder", "=", "TransformerEncoder", "(", "encoder_layer", ",", "num_layers", "=", "6", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Dropout", "(", "p", "=", "bert_model_config", ".", "hidden_dropout_prob", ")", ",", "\n", "nn", ".", "Linear", "(", "bert_model_config", ".", "hidden_size", ",", "bert_model_config", ".", "num_labels", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertTransformer.freeze_bert_encoder": [[246, 249], ["document_bert_architectures.DocumentBertTransformer.bert.parameters"], "methods", ["None"], ["", "def", "freeze_bert_encoder", "(", "self", ")", ":", "\n", "        ", "for", "param", "in", "self", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertTransformer.unfreeze_bert_encoder": [[250, 253], ["document_bert_architectures.DocumentBertTransformer.bert.parameters"], "methods", ["None"], ["", "", "def", "unfreeze_bert_encoder", "(", "self", ")", ":", "\n", "        ", "for", "param", "in", "self", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertTransformer.unfreeze_bert_encoder_last_layers": [[254, 258], ["document_bert_architectures.DocumentBertTransformer.bert.named_parameters"], "methods", ["None"], ["", "", "def", "unfreeze_bert_encoder_last_layers", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "param", "in", "self", ".", "bert", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "\"encoder.layer.11\"", "in", "name", "or", "\"pooler\"", "in", "name", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", "def", "unfreeze_bert_encoder_pooler_layer", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertTransformer.unfreeze_bert_encoder_pooler_layer": [[258, 262], ["document_bert_architectures.DocumentBertTransformer.bert.named_parameters"], "methods", ["None"], ["", "", "", "def", "unfreeze_bert_encoder_pooler_layer", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "param", "in", "self", ".", "bert", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "\"pooler\"", "in", "name", ":", "\n", "                ", "param", ".", "requires_grad", "=", "True", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.document_bert_architectures.DocumentBertTransformer.forward": [[225, 245], ["torch.zeros", "range", "document_bert_architectures.DocumentBertTransformer.transformer_encoder", "document_bert_architectures.DocumentBertTransformer.classifier", "document_bert_architectures.DocumentBertTransformer.dropout", "torch.zeros.permute", "document_bert_architectures.DocumentBertTransformer.permute().max", "min", "document_bert_architectures.DocumentBertTransformer.bert", "document_bert_architectures.DocumentBertTransformer.permute"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.bert_document_classification.transformer.dropout"], ["", "", "", "def", "forward", "(", "self", ",", "document_batch", ":", "torch", ".", "Tensor", ",", "document_sequence_lengths", ":", "list", ")", ":", "\n", "\n", "#contains all BERT sequences", "\n", "#bert should output a (batch_size, num_sequences, bert_hidden_size)", "\n", "        ", "bert_output", "=", "torch", ".", "zeros", "(", "size", "=", "(", "document_batch", ".", "shape", "[", "0", "]", ",", "\n", "min", "(", "document_batch", ".", "shape", "[", "1", "]", ",", "self", ".", "bert_batch_size", ")", ",", "\n", "self", ".", "bert", ".", "config", ".", "hidden_size", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "'cuda'", ")", "\n", "\n", "#only pass through bert_batch_size numbers of inputs into bert.", "\n", "#this means that we are possibly cutting off the last part of documents.", "\n", "for", "doc_id", "in", "range", "(", "document_batch", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "bert_output", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", "]", "=", "self", ".", "dropout", "(", "self", ".", "bert", "(", "document_batch", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", ",", "0", "]", ",", "\n", "token_type_ids", "=", "document_batch", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", ",", "1", "]", ",", "\n", "attention_mask", "=", "document_batch", "[", "doc_id", "]", "[", ":", "self", ".", "bert_batch_size", ",", "2", "]", ")", "[", "1", "]", ")", "\n", "\n", "", "transformer_output", "=", "self", ".", "transformer_encoder", "(", "bert_output", ".", "permute", "(", "1", ",", "0", ",", "2", ")", ")", "\n", "\n", "prediction", "=", "self", ".", "classifier", "(", "transformer_output", ".", "permute", "(", "1", ",", "0", ",", "2", ")", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", ")", "\n", "assert", "prediction", ".", "shape", "[", "0", "]", "==", "document_batch", ".", "shape", "[", "0", "]", "\n", "return", "prediction", "\n", "\n"]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.obesity_document_bert.ObesityPhenotypingBert.__init__": [[4, 14], ["util.get_model_path", "document_bert.BertForDocumentClassification.__init__"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.util.get_model_path", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "device", "=", "'cuda'", ",", "batch_size", "=", "10", ",", "model_name", "=", "\"n2c2_2008_obesity_lstm\"", ")", ":", "\n", "        ", "model_path", "=", "get_model_path", "(", "model_name", ")", "\n", "self", ".", "labels", "=", "\"Gout, Venous Insufficiency, Gallstones, Hypertension, Obesity, Asthma, GERD, Hypercholesterolemia, Hypertriglyceridemia, CHF, OSA, OA, PVD, CAD, Depression, Diabetes\"", ".", "split", "(", "', '", ")", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "device", "=", "device", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "bert_batch_size", "=", "7", ",", "\n", "bert_model_path", "=", "model_path", ",", "\n", "architecture", "=", "'DocumentBertLSTM'", ",", "\n", "labels", "=", "self", ".", "labels", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__": [[4, 14], ["util.get_model_path", "document_bert.BertForDocumentClassification.__init__"], "methods", ["home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.util.get_model_path", "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.smoker_document_bert.SmokerPhenotypingBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "device", "=", "'cuda'", ",", "batch_size", "=", "10", ",", "model_name", "=", "\"n2c2_2006_smoker_lstm\"", ")", ":", "\n", "        ", "model_path", "=", "get_model_path", "(", "model_name", ")", "\n", "self", ".", "labels", "=", "\"PAST SMOKER, CURRENT SMOKER, NON-SMOKER, UNKNOWN\"", ".", "split", "(", "', '", ")", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "device", "=", "device", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "bert_batch_size", "=", "7", ",", "\n", "bert_model_path", "=", "model_path", ",", "\n", "architecture", "=", "'DocumentBertLSTM'", ",", "\n", "labels", "=", "self", ".", "labels", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.AndriyMulyar_bert_document_classification.models.util.get_model_path": [[18, 48], ["pytorch_transformers.file_utils.url_to_filename", "os.path.join", "os.path.exists", "os.makedirs", "tempfile.NamedTemporaryFile", "print", "temp_file.flush", "temp_file.seek", "tarfile.open.extractall", "pytorch_transformers.file_utils.http_get", "tarfile.open", "print", "os.rmdir"], "function", ["None"], ["def", "get_model_path", "(", "model_name", ":", "str", ")", ":", "\n", "\n", "    ", "if", "model_name", "in", "MODEL_URL", ":", "\n", "        ", "model_url", "=", "MODEL_URL", "[", "model_name", "]", "\n", "", "else", ":", "\n", "        ", "return", "model_name", "\n", "", "model_url_hash", "=", "url_to_filename", "(", "model_url", ")", "\n", "\n", "model_path", "=", "os", ".", "path", ".", "join", "(", "default_cache_path", ",", "model_url_hash", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "model_path", ")", ":", "\n", "        ", "return", "model_path", "\n", "", "else", ":", "\n", "        ", "os", ".", "makedirs", "(", "model_path", ")", "\n", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "print", "(", "\"Downloading model: %s from %s\"", "%", "(", "model_name", ",", "model_url", ")", ")", "\n", "try", ":", "\n", "                ", "http_get", "(", "model_url", ",", "temp_file", ")", "\n", "tar", "=", "tarfile", ".", "open", "(", "temp_file", ".", "name", ")", "\n", "", "except", "BaseException", "as", "exc", ":", "\n", "                ", "print", "(", "\"Failed to download model: %s\"", "%", "model_name", ")", "\n", "os", ".", "rmdir", "(", "model_path", ")", "\n", "raise", "exc", "\n", "\n", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "tar", ".", "extractall", "(", "model_path", ")", "\n", "\n", "return", "model_path", "\n", "\n"]]}