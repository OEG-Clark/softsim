{"home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.None.train.parse_option": [[21, 81], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "parse_option", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'training'", ")", "\n", "\n", "# dataset", "\n", "# for video_dataset", "\n", "parser", ".", "add_argument", "(", "'--list-path'", ",", "type", "=", "str", ",", "default", "=", "'data/ucf101/train_list_01.csv'", ",", "help", "=", "'path of list file'", ")", "\n", "parser", ".", "add_argument", "(", "'--root-path'", ",", "type", "=", "str", ",", "default", "=", "'/lr/export2/home/zhangyiheng8/dataset/UCF-101/UCF-101-lmdb240'", ",", "help", "=", "'path of rgb root folder'", ")", "\n", "parser", ".", "add_argument", "(", "'--root-path-flow'", ",", "type", "=", "str", ",", "default", "=", "'/dev/shm/UCF-101-flow-lmdb240'", ",", "help", "=", "'path of flow root folder'", ")", "\n", "parser", ".", "add_argument", "(", "'--root-path-mag'", ",", "type", "=", "str", ",", "default", "=", "'/dev/shm/UCF-101-flow-mag-lmdb7'", ",", "help", "=", "'path of flow mag root folder'", ")", "\n", "parser", ".", "add_argument", "(", "'--data-form'", ",", "type", "=", "str", ",", "default", "=", "'lmdb'", ",", "choices", "=", "[", "'jpg'", ",", "'lmdb'", ",", "'video'", "]", ",", "help", "=", "'data type of input'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "type", "=", "str", ",", "default", "=", "'ucf101'", ",", "choices", "=", "[", "'kinetics'", ",", "'ucf101'", "]", ",", "help", "=", "'dataset to training'", ")", "\n", "parser", ".", "add_argument", "(", "'--crop'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "'minimum crop'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "help", "=", "'batch_size'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-workers'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "help", "=", "'num of workers to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-length'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'num of clip length'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'num of sampling steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-segments'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "'num of segments'", ")", "\n", "parser", ".", "add_argument", "(", "'--input-size'", ",", "type", "=", "int", ",", "default", "=", "224", ",", "help", "=", "'size pf input rgb'", ")", "\n", "parser", ".", "add_argument", "(", "'--mag-size'", ",", "type", "=", "int", ",", "default", "=", "7", ",", "help", "=", "'size pf input mag'", ")", "\n", "\n", "# model and loss function", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.999", ",", "help", "=", "'exponential moving average weight'", ")", "\n", "parser", ".", "add_argument", "(", "'--nce-k'", ",", "type", "=", "int", ",", "default", "=", "131072", ",", "help", "=", "'num negative sampler'", ")", "\n", "parser", ".", "add_argument", "(", "'--nce-t'", ",", "type", "=", "float", ",", "default", "=", "0.10", ",", "help", "=", "'NCE temperature'", ")", "\n", "parser", ".", "add_argument", "(", "'--nce-t-intra'", ",", "type", "=", "float", ",", "default", "=", "0.10", ",", "help", "=", "'NCE temperature'", ")", "\n", "\n", "# optimization", "\n", "parser", ".", "add_argument", "(", "'--base-learning-rate'", ",", "'--base-lr'", ",", "type", "=", "float", ",", "default", "=", "0.2", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ",", "help", "=", "'warmup epoch'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-multiplier'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'warmup multiplier'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-decay-epochs'", ",", "type", "=", "int", ",", "default", "=", "[", "120", ",", "160", ",", "200", "]", ",", "nargs", "=", "'+'", ",", "\n", "help", "=", "'for step scheduler. where to decay lr, can be a list'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-decay-rate'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'for step scheduler. decay rate for learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-scheduler'", ",", "type", "=", "str", ",", "default", "=", "'cosine'", ",", "\n", "choices", "=", "[", "\"step\"", ",", "\"cosine\"", "]", ",", "help", "=", "\"learning rate scheduler\"", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "type", "=", "float", ",", "default", "=", "1e-4", ",", "help", "=", "'weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "help", "=", "'momentum for SGD'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "400", ",", "help", "=", "'number of training epochs'", ")", "\n", "parser", ".", "add_argument", "(", "'--time-dim'", ",", "type", "=", "str", ",", "default", "=", "'T'", ",", "\n", "choices", "=", "[", "\"T\"", ",", "\"C\"", "]", ",", "help", "=", "\"dimension for time\"", ")", "\n", "parser", ".", "add_argument", "(", "'--resume'", ",", "type", "=", "str2bool", ",", "default", "=", "'false'", ",", "help", "=", "'warmup epoch'", ")", "\n", "\n", "# io", "\n", "parser", ".", "add_argument", "(", "'--pretrained-model'", ",", "default", "=", "'/lr/export/home/lirui295/models/moco_v2_200ep_pretrain.pth.tar'", ",", "type", "=", "str", ",", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "'path to pretrained weights like moco imagenet (default: none)'", ")", "\n", "parser", ".", "add_argument", "(", "'--print-freq'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "'print frequency'", ")", "\n", "parser", ".", "add_argument", "(", "'--save-freq'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'save frequency'", ")", "\n", "parser", ".", "add_argument", "(", "'--output-dir'", ",", "type", "=", "str", ",", "default", "=", "'output'", ",", "help", "=", "'output director'", ")", "\n", "\n", "# misc", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "help", "=", "'local rank for DistributedDataParallel'", ",", "default", "=", "0", ")", "\n", "\n", "# model", "\n", "parser", ".", "add_argument", "(", "\"--inflate-weights\"", ",", "type", "=", "str2bool", ",", "default", "=", "'true'", ")", "\n", "parser", ".", "add_argument", "(", "\"--target-module\"", ",", "type", "=", "str", ",", "default", "=", "'module.backbone.layer4.2.conv3'", ",", "help", "=", "\"the target module name\"", ")", "\n", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.None.train.get_loader": [[83, 121], ["torchvision.transforms.RandomApply", "dataset.augmentations.clip_transforms.ClipRandomGrayscale", "torchvision.transforms.RandomApply", "dataset.augmentations.clip_transforms.ClipRandomHorizontalFlip", "dataset.augmentations.clip_transforms.ToClipTensor", "dataset.augmentations.clip_transforms.ClipNormalize", "dataset.augmentations.clip_transforms.Compose", "dataset.augmentations.clip_transforms.ClipRandomResizedCropMotion", "dataset.video_dataset.VideoRGBTrainDataset_Motion", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataset.augmentations.clip_transforms.Lambda", "torchvision.transforms.Lambda", "dataset.augmentations.clip_transforms.ClipColorJitter", "dataset.augmentations.clip_transforms.ClipGaussianBlur", "torch.stack", "torch.stack", "torch.stack", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "get_loader", "(", "args", ",", "mode", "=", "'train'", ")", ":", "\n", "\n", "    ", "transoform_list", "=", "[", "\n", "transforms", ".", "RandomApply", "(", "[", "clip_transforms", ".", "ClipColorJitter", "(", "0.4", ",", "0.4", ",", "0.4", ",", "0.1", ")", "]", ",", "p", "=", "0.8", ")", ",", "\n", "clip_transforms", ".", "ClipRandomGrayscale", "(", "p", "=", "0.2", ")", ",", "\n", "transforms", ".", "RandomApply", "(", "[", "clip_transforms", ".", "ClipGaussianBlur", "(", "[", ".1", ",", "2.", "]", ")", "]", ",", "p", "=", "0.5", ")", ",", "\n", "clip_transforms", ".", "ClipRandomHorizontalFlip", "(", ")", ",", "\n", "clip_transforms", ".", "ToClipTensor", "(", ")", ",", "\n", "clip_transforms", ".", "ClipNormalize", "(", "mean", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "std", "=", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", ",", "\n", "clip_transforms", ".", "Lambda", "(", "lambda", "clip", ":", "torch", ".", "stack", "(", "clip", ",", "dim", "=", "1", ")", ")", "if", "opt", ".", "time_dim", "==", "\"T\"", "else", "transforms", ".", "Lambda", "(", "\n", "lambda", "clip", ":", "torch", ".", "cat", "(", "clip", ",", "dim", "=", "0", ")", ")", "\n", "]", "\n", "\n", "if", "mode", "==", "'train'", ":", "\n", "        ", "train_transform", "=", "clip_transforms", ".", "Compose", "(", "transoform_list", ")", "\n", "motion_focus_spatial_crop", "=", "clip_transforms", ".", "ClipRandomResizedCropMotion", "(", "opt", ".", "input_size", ",", "scale", "=", "(", "args", ".", "crop", ",", "1.", ")", ",", "ratio", "=", "(", "0.75", ",", "1.3333333333333333", ")", ")", "\n", "\n", "train_dataset", "=", "VideoRGBTrainDataset_Motion", "(", "\n", "root_path_flow", "=", "args", ".", "root_path_flow", ",", "\n", "root_path_mag", "=", "args", ".", "root_path_mag", ",", "\n", "input_size", "=", "args", ".", "input_size", ",", "mag_size", "=", "args", ".", "mag_size", ",", "\n", "list_root", "=", "args", ".", "list_path", ",", "root_path", "=", "args", ".", "root_path", ",", "\n", "transform", "=", "train_transform", ",", "motion_focus_spatial_crop", "=", "motion_focus_spatial_crop", ",", "\n", "clip_length", "=", "args", ".", "clip_length", ",", "num_steps", "=", "args", ".", "num_steps", ",", "\n", "dataset", "=", "args", ".", "dataset", ",", "\n", "data_form", "=", "args", ".", "data_form", "\n", ")", "\n", "\n", "\n", "train_sampler", "=", "torch", ".", "utils", ".", "data", ".", "distributed", ".", "DistributedSampler", "(", "train_dataset", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "train_dataset", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "pin_memory", "=", "True", ",", "sampler", "=", "train_sampler", ",", "\n", "drop_last", "=", "True", ")", "\n", "\n", "return", "train_loader", "\n", "", "else", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.None.train.build_model": [[122, 134], ["model.encoder.Encoder().cuda", "model.encoder.Encoder().cuda", "model.mal.MAL().cuda", "utils.util.moment_update", "train.load_pretrained", "model.encoder.Encoder", "model.encoder.Encoder", "model.mal.MAL"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.moment_update", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.load_pretrained"], ["", "", "def", "build_model", "(", "args", ")", ":", "\n", "    ", "model", "=", "Encoder", "(", "num_channels", "=", "128", ",", "mlp_layers", "=", "2", ",", "order", "=", "True", ")", ".", "cuda", "(", ")", "\n", "model_ema", "=", "Encoder", "(", "num_channels", "=", "128", ",", "mlp_layers", "=", "2", ",", "order", "=", "True", ")", ".", "cuda", "(", ")", "\n", "mal", "=", "MAL", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "if", "args", ".", "pretrained_model", ":", "\n", "        ", "load_pretrained", "(", "args", ",", "model", ")", "\n", "\n", "# copy weights from `model' to `model_ema'", "\n", "", "moment_update", "(", "model", ",", "model_ema", ",", "0", ")", "\n", "\n", "return", "model", ",", "model_ema", ",", "mal", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.None.train.load_pretrained": [[135, 147], ["torch.load", "torch.load", "torch.load", "model.load_state_dict", "logger.info", "logger.info", "logger.info", "k.replace", "utils.util.convert_pretrained_weights", "ckpt[].items"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.load_state_dict", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.convert_pretrained_weights"], ["", "def", "load_pretrained", "(", "args", ",", "model", ")", ":", "\n", "    ", "ckpt", "=", "torch", ".", "load", "(", "args", ".", "pretrained_model", ",", "map_location", "=", "'cpu'", ")", "\n", "state_dict", "=", "{", "k", ".", "replace", "(", "\"module.encoder_q.\"", ",", "\"backbone.\"", ")", ":", "v", "for", "k", ",", "v", "in", "ckpt", "[", "'state_dict'", "]", ".", "items", "(", ")", "}", "\n", "\n", "# convert initial weights", "\n", "if", "args", ".", "inflate_weights", ":", "\n", "        ", "state_dict", "=", "convert_pretrained_weights", "(", "state_dict", ",", "args", ")", "\n", "\n", "", "[", "misskeys", ",", "unexpkeys", "]", "=", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "False", ")", "\n", "logger", ".", "info", "(", "misskeys", ")", "\n", "logger", ".", "info", "(", "unexpkeys", ")", "\n", "logger", ".", "info", "(", "\"==> loaded checkpoint '{}' (epoch {})\"", ".", "format", "(", "args", ".", "pretrained_model", ",", "ckpt", "[", "'epoch'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.None.train.save_checkpoint": [[149, 165], ["logger.info", "os.path.join", "os.makedirs", "torch.save", "torch.save", "torch.save", "model.state_dict", "model_ema.state_dict", "contrast.state_dict", "optimizer.state_dict", "scheduler.state_dict", "os.path.join", "torch.save", "torch.save", "torch.save", "os.path.join"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict"], ["", "def", "save_checkpoint", "(", "args", ",", "epoch", ",", "model", ",", "model_ema", ",", "contrast", ",", "scheduler", ",", "optimizer", ")", ":", "\n", "    ", "logger", ".", "info", "(", "'==> Saving...'", ")", "\n", "state", "=", "{", "\n", "'opt'", ":", "args", ",", "\n", "'model'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'model_ema'", ":", "model_ema", ".", "state_dict", "(", ")", ",", "\n", "'contrast'", ":", "contrast", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'scheduler'", ":", "scheduler", ".", "state_dict", "(", ")", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "}", "\n", "save_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'checkpoints'", ")", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "torch", ".", "save", "(", "state", ",", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'current.pth'", ")", ")", "\n", "if", "epoch", "%", "args", ".", "save_freq", "==", "0", ":", "\n", "        ", "torch", ".", "save", "(", "state", ",", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'ckpt_epoch_{}.pth'", ".", "format", "(", "epoch", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.None.train.main": [[167, 212], ["train.get_loader", "len", "logger.info", "train.build_model", "contrast.NCEContrast.MemoryMCL().cuda", "contrast.NCEContrast.NCESoftmaxLoss().cuda", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "utils.lr_scheduler.get_scheduler", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "range", "torch.nn.parallel.DistributedDataParallel.parameters", "len", "torch.get_rank", "tensorboardX.SummaryWriter", "get_loader.sampler.set_epoch", "time.time", "train.train_one_epoch", "logger.info", "logger.info", "contrast.NCEContrast.MemoryMCL", "contrast.NCEContrast.NCESoftmaxLoss", "tensorboardX.SummaryWriter.add_scalar", "tensorboardX.SummaryWriter.add_scalar", "tensorboardX.SummaryWriter.add_scalar", "torch.get_rank", "train.save_checkpoint", "os.path.join", "time.time"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.get_loader", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.build_model", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.get_scheduler", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.None.train.train_one_epoch", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.save_checkpoint"], ["", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "train_loader", "=", "get_loader", "(", "args", ")", "\n", "n_data", "=", "len", "(", "train_loader", ".", "dataset", ")", "\n", "logger", ".", "info", "(", "\"length of training dataset: {}\"", ".", "format", "(", "n_data", ")", ")", "\n", "\n", "model", ",", "model_ema", ",", "mal", "=", "build_model", "(", "args", ")", "\n", "\n", "contrast", "=", "MemoryMCL", "(", "128", ",", "args", ".", "nce_k", ",", "args", ".", "nce_t", ",", "args", ".", "nce_t_intra", ")", ".", "cuda", "(", ")", "\n", "criterion", "=", "NCESoftmaxLoss", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "args", ".", "base_learning_rate", ",", "\n", "momentum", "=", "args", ".", "momentum", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ")", "\n", "\n", "scheduler", "=", "get_scheduler", "(", "optimizer", ",", "len", "(", "train_loader", ")", ",", "args", ")", "\n", "\n", "start_epochs", "=", "1", "\n", "\n", "model", "=", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "broadcast_buffers", "=", "False", ")", "\n", "model_ema", "=", "DistributedDataParallel", "(", "model_ema", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "broadcast_buffers", "=", "False", ")", "\n", "\n", "# tensorboard", "\n", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "summary_writer", "=", "SummaryWriter", "(", "log_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'logs'", ")", ")", "\n", "", "else", ":", "\n", "        ", "summary_writer", "=", "None", "\n", "\n", "\n", "# routine", "\n", "", "for", "epoch", "in", "range", "(", "start_epochs", ",", "args", ".", "epochs", "+", "1", ")", ":", "\n", "        ", "train_loader", ".", "sampler", ".", "set_epoch", "(", "epoch", ")", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "loss", ",", "prob", "=", "train_one_epoch", "(", "epoch", ",", "train_loader", ",", "model", ",", "mal", ",", "model_ema", ",", "contrast", ",", "criterion", ",", "optimizer", ",", "scheduler", ",", "args", ",", "summary_writer", ")", "\n", "logger", ".", "info", "(", "'epoch {}, total time {:.2f}'", ".", "format", "(", "epoch", ",", "time", ".", "time", "(", ")", "-", "tic", ")", ")", "\n", "logger", ".", "info", "(", "'epoch {}, average loss is {:>0.3f}'", ".", "format", "(", "epoch", ",", "loss", ")", ")", "\n", "if", "summary_writer", "is", "not", "None", ":", "\n", "# tensorboard logger", "\n", "            ", "summary_writer", ".", "add_scalar", "(", "'Epoch/ins_loss'", ",", "loss", ",", "epoch", ")", "\n", "summary_writer", ".", "add_scalar", "(", "'Epoch/ins_prob'", ",", "prob", ",", "epoch", ")", "\n", "summary_writer", ".", "add_scalar", "(", "'Epoch/learning_rate'", ",", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ",", "epoch", ")", "\n", "\n", "", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "# save model", "\n", "            ", "save_checkpoint", "(", "args", ",", "epoch", ",", "model", ",", "model_ema", ",", "contrast", ",", "scheduler", ",", "optimizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.None.train.train_one_epoch": [[213, 323], ["model.train", "utils.util.set_bn_train", "utils.util.AverageMeter", "utils.util.AverageMeter", "utils.util.AverageMeter", "utils.util.AverageMeter", "time.time", "model.named_modules", "enumerate", "xq.cuda().requires_grad_.size", "xq.cuda().requires_grad_.cuda().requires_grad_", "x1.cuda.cuda", "x2.cuda.cuda", "x3.cuda.cuda", "order_target.cuda.cuda", "st_mag.cuda.cuda", "utils.util.forward_hook", "model", "utils.util.forward_hook.remove", "contrast", "contrast", "mal", "torch.cat", "torch.cat", "torch.cat", "torch.cross_entropy", "[].mean", "[].mean", "optimizer.zero_grad", "loss.backward", "optimizer.step", "scheduler.step", "utils.util.moment_update", "utils.util.AverageMeter.update", "utils.util.AverageMeter.update", "utils.util.AverageMeter.update", "utils.util.AverageMeter.update", "time.time", "torch.no_grad", "torch.no_grad", "torch.no_grad", "Shuffle.forward_shuffle", "model_ema", "Shuffle.backward_shuffle", "Shuffle.backward_shuffle", "Shuffle.forward_shuffle", "model_ema", "Shuffle.backward_shuffle", "Shuffle.backward_shuffle", "Shuffle.forward_shuffle", "model_ema", "Shuffle.backward_shuffle", "Shuffle.backward_shuffle", "criterion", "criterion", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "model", "float", "float", "loss.item", "[].mean.item", "[].mean.item", "logger.info", "xq.cuda().requires_grad_.cuda", "Shuffle.backward_shuffle", "Shuffle.backward_shuffle", "feat_order_x2.detach", "feat_order_x3.detach", "time.time", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.softmax", "torch.softmax", "len", "loss.item", "loss_nce.item", "mal.item", "torch.ones", "torch.ones", "torch.ones", "l_pos.size"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.train", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.set_bn_train", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.forward_hook", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.step", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.step", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.moment_update", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.forward_shuffle", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.backward_shuffle", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.backward_shuffle", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.forward_shuffle", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.backward_shuffle", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.backward_shuffle", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.forward_shuffle", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.backward_shuffle", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.backward_shuffle", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.backward_shuffle", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.backward_shuffle"], ["", "", "", "def", "train_one_epoch", "(", "epoch", ",", "train_loader", ",", "model", ",", "mal", ",", "model_ema", ",", "contrast", ",", "criterion", ",", "optimizer", ",", "scheduler", ",", "args", ",", "summary_writer", ")", ":", "\n", "\n", "    ", "model", ".", "train", "(", ")", "\n", "set_bn_train", "(", "model_ema", ")", "\n", "\n", "batch_time", "=", "AverageMeter", "(", ")", "\n", "loss_meter", "=", "AverageMeter", "(", ")", "\n", "prob_inter_meter", "=", "AverageMeter", "(", ")", "\n", "prob_intra_meter", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "for", "name", ",", "module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "        ", "if", "name", "==", "args", ".", "target_module", ":", "\n", "            ", "target_module", "=", "module", "\n", "\n", "", "", "for", "idx", ",", "(", "xq", ",", "st_mag", ",", "x1", ",", "x2", ",", "x3", ",", "order_target", ")", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "        ", "bsz", "=", "xq", ".", "size", "(", "0", ")", "\n", "# forward", "\n", "xq", "=", "xq", ".", "cuda", "(", "non_blocking", "=", "True", ")", ".", "requires_grad_", "(", ")", "# quary", "\n", "x1", "=", "x1", ".", "cuda", "(", "non_blocking", "=", "True", ")", "# same clip diff aug", "\n", "x2", "=", "x2", ".", "cuda", "(", "non_blocking", "=", "True", ")", "# diff clip 1", "\n", "x3", "=", "x3", ".", "cuda", "(", "non_blocking", "=", "True", ")", "# diff clip 2", "\n", "order_target", "=", "order_target", ".", "cuda", "(", "non_blocking", "=", "True", ")", "# order: binary labels", "\n", "st_mag", "=", "st_mag", ".", "cuda", "(", "non_blocking", "=", "True", ")", "# st-motion", "\n", "\n", "total_feat_out", "=", "[", "]", "\n", "hook", "=", "forward_hook", "(", "model", ",", "total_feat_out", ",", "target_module", ")", "\n", "\n", "# forward and get the 128d encoded representations of query", "\n", "feat_inter_q", ",", "feat_intra_q", ",", "feat_order_q", "=", "model", "(", "xq", ")", "\n", "\n", "xconv", "=", "total_feat_out", "[", "-", "1", "]", "\n", "hook", ".", "remove", "(", ")", "\n", "\n", "# forward and get the 128d encoded representations of keys", "\n", "Shuffle", "=", "DistributedShuffle", "\n", "\n", "# shuffled bn is applied", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# 1st key", "\n", "            ", "x1_shuffled", ",", "backward_inds", "=", "Shuffle", ".", "forward_shuffle", "(", "x1", ",", "epoch", ")", "\n", "feat_inter_x1", ",", "feat_intra_x1", ",", "_", "=", "model_ema", "(", "x1_shuffled", ")", "\n", "# get x_all for update memory", "\n", "feat_inter_x1_all", ",", "feat_inter_x1", "=", "Shuffle", ".", "backward_shuffle", "(", "feat_inter_x1", ",", "backward_inds", ",", "return_local", "=", "True", ")", "\n", "feat_intra_x1_all", ",", "feat_intra_x1", "=", "Shuffle", ".", "backward_shuffle", "(", "feat_intra_x1", ",", "backward_inds", ",", "return_local", "=", "True", ")", "\n", "# 2nd key", "\n", "x2_shuffled", ",", "backward_inds", "=", "Shuffle", ".", "forward_shuffle", "(", "x2", ",", "epoch", ")", "\n", "feat_inter_x2", ",", "feat_intra_x2", ",", "feat_order_x2", "=", "model_ema", "(", "x2_shuffled", ")", "\n", "feat_inter_x2_all", ",", "feat_inter_x2", "=", "Shuffle", ".", "backward_shuffle", "(", "feat_inter_x2", ",", "backward_inds", ",", "return_local", "=", "True", ")", "\n", "feat_intra_x2_all", ",", "feat_intra_x2", "=", "Shuffle", ".", "backward_shuffle", "(", "feat_intra_x2", ",", "backward_inds", ",", "return_local", "=", "True", ")", "\n", "if", "feat_order_x2", "is", "not", "None", ":", "\n", "                ", "_", ",", "feat_order_x2", "=", "Shuffle", ".", "backward_shuffle", "(", "feat_order_x2", ",", "backward_inds", ",", "return_local", "=", "True", ")", "\n", "# 3nd key", "\n", "", "x3_shuffled", ",", "backward_inds", "=", "Shuffle", ".", "forward_shuffle", "(", "x3", ",", "epoch", ")", "\n", "feat_inter_x3", ",", "feat_intra_x3", ",", "feat_order_x3", "=", "model_ema", "(", "x3_shuffled", ")", "\n", "feat_inter_x3_all", ",", "feat_inter_x3", "=", "Shuffle", ".", "backward_shuffle", "(", "feat_inter_x3", ",", "backward_inds", ",", "return_local", "=", "True", ")", "\n", "feat_intra_x3_all", ",", "feat_intra_x3", "=", "Shuffle", ".", "backward_shuffle", "(", "feat_intra_x3", ",", "backward_inds", ",", "return_local", "=", "True", ")", "\n", "if", "feat_order_x3", "is", "not", "None", ":", "\n", "                ", "_", ",", "feat_order_x3", "=", "Shuffle", ".", "backward_shuffle", "(", "feat_order_x3", ",", "backward_inds", ",", "return_local", "=", "True", ")", "\n", "\n", "# calc nce loss", "\n", "", "", "out_inter", ",", "l_pos", "=", "contrast", "(", "feat_inter_q", ",", "feat_inter_x1", ",", "feat_inter_x2", ",", "feat_inter_x3", ",", "feat_inter_x1_all", ",", "feat_inter_x2_all", ",", "feat_inter_x3_all", ",", "inter", "=", "True", ")", "\n", "out_intra", "=", "contrast", "(", "feat_intra_q", ",", "feat_intra_x1", ",", "feat_intra_x2", ",", "feat_intra_x3", ",", "inter", "=", "False", ")", "\n", "loss_nce", "=", "criterion", "(", "out_inter", ")", "+", "criterion", "(", "out_intra", ")", "\n", "\n", "# calc gradient with regard to target module", "\n", "gradient", "=", "torch", ".", "autograd", ".", "grad", "(", "outputs", "=", "l_pos", ",", "inputs", "=", "xconv", ",", "\n", "grad_outputs", "=", "torch", ".", "ones", "(", "l_pos", ".", "size", "(", ")", ")", ".", "cuda", "(", ")", ",", "retain_graph", "=", "True", ",", "\n", "create_graph", "=", "True", ")", "[", "0", "]", "\n", "\n", "# calc mal loss", "\n", "loss_mal", "=", "mal", "(", "gradient", ",", "xconv", ",", "st_mag", ")", "\n", "\n", "# calc order loss", "\n", "feat_order", "=", "torch", ".", "cat", "(", "(", "feat_order_q", ",", "feat_order_x2", ".", "detach", "(", ")", ",", "feat_order_x3", ".", "detach", "(", ")", ")", ",", "dim", "=", "-", "1", ")", "\n", "loss_order", "=", "F", ".", "cross_entropy", "(", "model", "(", "feat_order", ",", "mode", "=", "'classfier_order'", ")", ",", "order_target", ")", "\n", "loss_order_w", "=", "float", "(", "epoch", ")", "/", "float", "(", "args", ".", "epochs", ")", "\n", "if", "args", ".", "dataset", "==", "'kinetics'", ":", "loss_order_w", "=", "0", "\n", "loss", "=", "loss_nce", "+", "loss_order_w", "*", "loss_order", "+", "loss_mal", "if", "epoch", ">", "args", ".", "warmup_epoch", "else", "loss_nce", "+", "loss_order_w", "*", "loss_order", "+", "2", "*", "loss_mal", "\n", "\n", "prob_inter", "=", "F", ".", "softmax", "(", "out_inter", ",", "dim", "=", "1", ")", "[", ":", ",", "0", "]", ".", "mean", "(", ")", "\n", "prob_intra", "=", "F", ".", "softmax", "(", "out_intra", ",", "dim", "=", "1", ")", "[", ":", ",", "0", "]", ".", "mean", "(", ")", "\n", "\n", "# backward", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "\n", "moment_update", "(", "model", ",", "model_ema", ",", "args", ".", "alpha", ")", "\n", "\n", "# update meters", "\n", "loss_meter", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "bsz", ")", "\n", "prob_inter_meter", ".", "update", "(", "prob_inter", ".", "item", "(", ")", ",", "bsz", ")", "\n", "prob_intra_meter", ".", "update", "(", "prob_intra", ".", "item", "(", ")", ",", "bsz", ")", "\n", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "# print info", "\n", "if", "idx", "%", "args", ".", "print_freq", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'Train: [{:>3d}]/[{:>4d}/{:>4d}] BT={:>0.3f}/{:>0.3f} Loss={:>0.3f} {:>0.3f} {:>0.3f} / {:>0.3f} Pinter={:>0.3f}/{:>0.3f} Pintra={:>0.3f}/{:>0.3f}'", ".", "format", "(", "\n", "epoch", ",", "idx", ",", "len", "(", "train_loader", ")", ",", "\n", "batch_time", ".", "val", ",", "batch_time", ".", "avg", ",", "\n", "loss", ".", "item", "(", ")", ",", "loss_nce", ".", "item", "(", ")", ",", "loss_mal", ".", "item", "(", ")", ",", "loss_meter", ".", "avg", ",", "\n", "prob_inter_meter", ".", "val", ",", "prob_inter_meter", ".", "avg", ",", "\n", "prob_intra_meter", ".", "val", ",", "prob_intra_meter", ".", "avg", ",", "\n", ")", ")", "\n", "\n", "", "", "return", "loss_meter", ".", "avg", ",", "prob_inter_meter", ".", "avg", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.logger._ColorfulFormatter.__init__": [[9, 15], ["kwargs.pop", "len", "logging.Formatter.__init__", "kwargs.pop"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "_root_name", "=", "kwargs", ".", "pop", "(", "\"root_name\"", ")", "+", "\".\"", "\n", "self", ".", "_abbrev_name", "=", "kwargs", ".", "pop", "(", "\"abbrev_name\"", ",", "\"\"", ")", "\n", "if", "len", "(", "self", ".", "_abbrev_name", ")", ":", "\n", "            ", "self", ".", "_abbrev_name", "=", "self", ".", "_abbrev_name", "+", "\".\"", "\n", "", "super", "(", "_ColorfulFormatter", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.logger._ColorfulFormatter.formatMessage": [[16, 26], ["record.name.replace", "super().formatMessage", "termcolor.colored", "termcolor.colored"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.logger._ColorfulFormatter.formatMessage"], ["", "def", "formatMessage", "(", "self", ",", "record", ")", ":", "\n", "        ", "record", ".", "name", "=", "record", ".", "name", ".", "replace", "(", "self", ".", "_root_name", ",", "self", ".", "_abbrev_name", ")", "\n", "log", "=", "super", "(", "_ColorfulFormatter", ",", "self", ")", ".", "formatMessage", "(", "record", ")", "\n", "if", "record", ".", "levelno", "==", "logging", ".", "WARNING", ":", "\n", "            ", "prefix", "=", "colored", "(", "\"WARNING\"", ",", "\"red\"", ",", "attrs", "=", "[", "\"blink\"", "]", ")", "\n", "", "elif", "record", ".", "levelno", "==", "logging", ".", "ERROR", "or", "record", ".", "levelno", "==", "logging", ".", "CRITICAL", ":", "\n", "            ", "prefix", "=", "colored", "(", "\"ERROR\"", ",", "\"red\"", ",", "attrs", "=", "[", "\"blink\"", ",", "\"underline\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "log", "\n", "", "return", "prefix", "+", "\" \"", "+", "log", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.logger.setup_logger": [[29, 75], ["functools.lru_cache", "logging.getLogger", "logging.getLogger.setLevel", "logging.Formatter", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler", "os.makedirs", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler", "logger._ColorfulFormatter", "output.endswith", "output.endswith", "os.path.join", "os.path.dirname", "logger._cached_log_stream", "termcolor.colored", "str"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.logger._cached_log_stream"], ["", "", "@", "functools", ".", "lru_cache", "(", ")", "\n", "def", "setup_logger", "(", "\n", "output", "=", "None", ",", "distributed_rank", "=", "0", ",", "*", ",", "color", "=", "True", ",", "name", "=", "\"seco\"", ",", "abbrev_name", "=", "None", "\n", ")", ":", "\n", "    ", "logger", "=", "logging", ".", "getLogger", "(", "name", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "logger", ".", "propagate", "=", "False", "\n", "\n", "if", "abbrev_name", "is", "None", ":", "\n", "        ", "abbrev_name", "=", "name", "\n", "\n", "", "plain_formatter", "=", "logging", ".", "Formatter", "(", "\n", "\"[%(asctime)s] %(name)s %(levelname)s: %(message)s\"", ",", "datefmt", "=", "\"%m/%d %H:%M:%S\"", "\n", ")", "\n", "# stdout logging: master only", "\n", "if", "distributed_rank", "==", "0", ":", "\n", "        ", "ch", "=", "logging", ".", "StreamHandler", "(", "stream", "=", "sys", ".", "stdout", ")", "\n", "ch", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "if", "color", ":", "\n", "            ", "formatter", "=", "_ColorfulFormatter", "(", "\n", "colored", "(", "\"[%(asctime)s %(name)s]: \"", ",", "\"green\"", ")", "+", "\"%(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d %H:%M:%S\"", ",", "\n", "root_name", "=", "name", ",", "\n", "abbrev_name", "=", "str", "(", "abbrev_name", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "formatter", "=", "plain_formatter", "\n", "", "ch", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "ch", ")", "\n", "\n", "# file logging: all workers", "\n", "", "if", "output", "is", "not", "None", ":", "\n", "        ", "if", "output", ".", "endswith", "(", "\".txt\"", ")", "or", "output", ".", "endswith", "(", "\".log\"", ")", ":", "\n", "            ", "filename", "=", "output", "\n", "", "else", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "join", "(", "output", ",", "\"log.txt\"", ")", "\n", "", "if", "distributed_rank", ">", "0", ":", "\n", "            ", "filename", "=", "filename", "+", "\".rank{}\"", ".", "format", "(", "distributed_rank", ")", "\n", "", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "filename", ")", ",", "exist_ok", "=", "True", ")", "\n", "\n", "fh", "=", "logging", ".", "StreamHandler", "(", "_cached_log_stream", "(", "filename", ")", ")", "\n", "fh", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "fh", ".", "setFormatter", "(", "plain_formatter", ")", "\n", "logger", ".", "addHandler", "(", "fh", ")", "\n", "\n", "", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.logger._cached_log_stream": [[79, 82], ["functools.lru_cache", "open"], "function", ["None"], ["", "@", "functools", ".", "lru_cache", "(", "maxsize", "=", "None", ")", "\n", "def", "_cached_log_stream", "(", "filename", ")", ":", "\n", "    ", "return", "open", "(", "filename", ",", "\"a\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.__init__": [[8, 16], ["torch.optim.lr_scheduler._LRScheduler.__init__", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__"], ["    ", "def", "__init__", "(", "self", ",", "optimizer", ",", "multiplier", ",", "warmup_epoch", ",", "after_scheduler", ",", "last_epoch", "=", "-", "1", ")", ":", "\n", "        ", "self", ".", "multiplier", "=", "multiplier", "\n", "if", "self", ".", "multiplier", "<=", "1.", ":", "\n", "            ", "raise", "ValueError", "(", "'multiplier should be greater than 1.'", ")", "\n", "", "self", ".", "warmup_epoch", "=", "warmup_epoch", "\n", "self", ".", "after_scheduler", "=", "after_scheduler", "\n", "self", ".", "finished", "=", "False", "\n", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "last_epoch", "=", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.get_lr": [[17, 23], ["lr_scheduler.GradualWarmupScheduler.after_scheduler.get_lr"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.get_lr"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "last_epoch", ">", "self", ".", "warmup_epoch", ":", "\n", "            ", "return", "self", ".", "after_scheduler", ".", "get_lr", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "[", "base_lr", "/", "self", ".", "multiplier", "*", "(", "(", "self", ".", "multiplier", "-", "1.", ")", "*", "self", ".", "last_epoch", "/", "self", ".", "warmup_epoch", "+", "1.", ")", "\n", "for", "base_lr", "in", "self", ".", "base_lrs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.step": [[24, 33], ["lr_scheduler.GradualWarmupScheduler.after_scheduler.step", "super().step"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.step", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.step"], ["", "", "def", "step", "(", "self", ",", "epoch", "=", "None", ")", ":", "\n", "        ", "if", "epoch", "is", "None", ":", "\n", "            ", "epoch", "=", "self", ".", "last_epoch", "+", "1", "\n", "", "self", ".", "last_epoch", "=", "epoch", "\n", "if", "epoch", ">", "self", ".", "warmup_epoch", ":", "\n", "            ", "self", ".", "after_scheduler", ".", "step", "(", "epoch", "-", "self", ".", "warmup_epoch", ")", "\n", "", "else", ":", "\n", "            ", "super", "(", "GradualWarmupScheduler", ",", "self", ")", ".", "step", "(", "epoch", ")", "\n", "\n", "", "", "def", "state_dict", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict": [[34, 38], ["lr_scheduler.GradualWarmupScheduler.after_scheduler.state_dict", "lr_scheduler.GradualWarmupScheduler.__dict__.items"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict"], ["        ", "state", "=", "{", "key", ":", "value", "for", "key", ",", "value", "in", "self", ".", "__dict__", ".", "items", "(", ")", "if", "key", "!=", "'optimizer'", "and", "key", "!=", "'after_scheduler'", "}", "\n", "state", "[", "'after_scheduler'", "]", "=", "self", ".", "after_scheduler", ".", "state_dict", "(", ")", "\n", "return", "state", "\n", "\n", "", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.load_state_dict": [[39, 43], ["state_dict.pop", "lr_scheduler.GradualWarmupScheduler.__dict__.update", "lr_scheduler.GradualWarmupScheduler.after_scheduler.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.load_state_dict"], ["        ", "after_scheduler_state", "=", "state_dict", ".", "pop", "(", "'after_scheduler'", ")", "\n", "self", ".", "__dict__", ".", "update", "(", "state_dict", ")", "\n", "self", ".", "after_scheduler", ".", "load_state_dict", "(", "after_scheduler_state", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.get_scheduler": [[45, 65], ["lr_scheduler.GradualWarmupScheduler", "torch.optim.lr_scheduler.CosineAnnealingLR", "torch.optim.lr_scheduler.MultiStepLR", "NotImplementedError"], "function", ["None"], ["    ", "if", "\"cosine\"", "in", "args", ".", "lr_scheduler", ":", "\n", "        ", "scheduler", "=", "CosineAnnealingLR", "(", "\n", "optimizer", "=", "optimizer", ",", "\n", "eta_min", "=", "0.000001", ",", "\n", "T_max", "=", "(", "args", ".", "epochs", "-", "args", ".", "warmup_epoch", ")", "*", "n_iter_per_epoch", ")", "\n", "", "elif", "\"step\"", "in", "args", ".", "lr_scheduler", ":", "\n", "        ", "scheduler", "=", "MultiStepLR", "(", "\n", "optimizer", "=", "optimizer", ",", "\n", "gamma", "=", "args", ".", "lr_decay_rate", ",", "\n", "milestones", "=", "[", "(", "m", "-", "args", ".", "warmup_epoch", ")", "*", "n_iter_per_epoch", "for", "m", "in", "args", ".", "lr_decay_epochs", "]", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"scheduler {} not supported\"", ".", "format", "(", "args", ".", "lr_scheduler", ")", ")", "\n", "\n", "", "if", "args", ".", "warmup_epoch", ">", "0", ":", "\n", "        ", "scheduler", "=", "GradualWarmupScheduler", "(", "\n", "optimizer", ",", "\n", "multiplier", "=", "args", ".", "warmup_multiplier", ",", "\n", "after_scheduler", "=", "scheduler", ",", "\n", "warmup_epoch", "=", "args", ".", "warmup_epoch", "*", "n_iter_per_epoch", ")", "\n", "", "else", ":", "\n", "        ", "pass", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.__init__": [[11, 17], ["util.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.reset"], ["\n", "class", "AverageMeter", "(", "object", ")", ":", "\n", "    ", "\"\"\"Computes and stores the average and current value\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.reset": [[18, 23], ["None"], "methods", ["None"], ["self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "reset", "(", ")", "\n", "\n", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update": [[24, 29], ["None"], "methods", ["None"], ["self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n", "", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "val", "=", "val", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.DistributedShuffle.forward_shuffle": [[70, 81], ["util.dist_collect", "util.DistributedShuffle.get_shuffle_ids", "util.DistributedShuffle.get_local_id"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.dist_collect", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.DistributedShuffle.get_shuffle_ids", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.DistributedShuffle.get_local_id"], ["        ", "\"\"\" backward shuffle, return data which have been shuffled back\n        x is the shared data, should be local data\n        if return_local, only return the local batch data of x.\n            otherwise, return collected all data on all process.\n        \"\"\"", "\n", "x_all", "=", "dist_collect", "(", "x", ")", "\n", "if", "return_local", ":", "\n", "            ", "backward_inds_local", "=", "DistributedShuffle", ".", "get_local_id", "(", "backward_inds", ")", "\n", "return", "x_all", "[", "backward_inds", "]", ",", "x_all", "[", "backward_inds_local", "]", "\n", "", "else", ":", "\n", "            ", "return", "x_all", "[", "backward_inds", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.DistributedShuffle.backward_shuffle": [[82, 95], ["util.dist_collect", "util.DistributedShuffle.get_local_id"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.dist_collect", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.DistributedShuffle.get_local_id"], ["", "", "@", "staticmethod", "\n", "def", "get_local_id", "(", "ids", ")", ":", "\n", "        ", "return", "ids", ".", "chunk", "(", "dist", ".", "get_world_size", "(", ")", ")", "[", "dist", ".", "get_rank", "(", ")", "]", "\n", "\n", "", "@", "staticmethod", "\n", "def", "get_shuffle_ids", "(", "bsz", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"generate shuffle ids for ShuffleBN\"\"\"", "\n", "torch", ".", "manual_seed", "(", "epoch", ")", "# only update shuffle idx each epoch", "\n", "# global forward shuffle id  for all process", "\n", "forward_inds", "=", "torch", ".", "randperm", "(", "bsz", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "# global backward shuffle id", "\n", "backward_inds", "=", "torch", ".", "zeros", "(", "forward_inds", ".", "shape", "[", "0", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "value", "=", "torch", ".", "arange", "(", "bsz", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.DistributedShuffle.get_local_id": [[96, 99], ["ids.chunk", "torch.get_world_size", "torch.get_world_size", "torch.get_rank", "torch.get_rank"], "methods", ["None"], ["backward_inds", ".", "index_copy_", "(", "0", ",", "forward_inds", ",", "value", ")", "\n", "\n", "return", "forward_inds", ",", "backward_inds", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.DistributedShuffle.get_shuffle_ids": [[100, 113], ["torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.randperm().long().cuda", "torch.randperm().long().cuda", "torch.randperm().long().cuda", "torch.randperm().long().cuda", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.arange().long().cuda", "torch.zeros().long().cuda.index_copy_", "torch.zeros().long().cuda.index_copy_", "torch.randperm().long", "torch.randperm().long", "torch.randperm().long", "torch.randperm().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["\n", "", "", "def", "set_bn_train", "(", "model", ")", ":", "\n", "    ", "def", "set_bn_train_helper", "(", "m", ")", ":", "\n", "        ", "classname", "=", "m", ".", "__class__", ".", "__name__", "\n", "if", "classname", ".", "find", "(", "'BatchNorm'", ")", "!=", "-", "1", ":", "\n", "            ", "m", ".", "train", "(", ")", "\n", "\n", "", "", "model", ".", "eval", "(", ")", "\n", "model", ".", "apply", "(", "set_bn_train_helper", ")", "\n", "\n", "\n", "", "def", "moment_update", "(", "model", ",", "model_ema", ",", "m", ")", ":", "\n", "    ", "\"\"\" model_ema = m * model_ema + (1 - m) model \"\"\"", "\n", "for", "p1", ",", "p2", "in", "zip", "(", "model", ".", "parameters", "(", ")", ",", "model_ema", ".", "parameters", "(", ")", ")", ":", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.forward_shuffle": [[119, 125], ["util.DistributedShuffle.get_shuffle_ids"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.DistributedShuffle.get_shuffle_ids"], ["    ", "def", "forward_shuffle", "(", "x", ",", "epoch", ")", ":", "\n", "        ", "forward_inds", ",", "backward_inds", "=", "DistributedShuffle", ".", "get_shuffle_ids", "(", "x", ".", "shape", "[", "0", "]", ",", "epoch", ")", "\n", "\n", "#forward_inds_local = DistributedShuffle.get_local_id(forward_inds)", "\n", "\n", "return", "x", "[", "forward_inds", "]", ",", "backward_inds", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.SingeShuffle.backward_shuffle": [[127, 136], ["None"], "methods", ["None"], ["", "def", "backward_shuffle", "(", "x", ",", "backward_inds", ",", "return_local", "=", "True", ")", ":", "\n", "        ", "\"\"\" backward shuffle, return data which have been shuffled back\n        x is the shared data, should be local data\n        if return_local, only return the local batch data of x.\n            otherwise, return collected all data on all process.\n        \"\"\"", "\n", "x_all", "=", "x", "\n", "\n", "return", "x_all", "[", "backward_inds", "]", ",", "x_all", "[", "backward_inds", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.dist_collect": [[48, 60], ["x.contiguous.contiguous", "torch.all_gather", "torch.cat", "torch.cat", "torch.zeros_like", "torch.zeros_like", "range", "torch.get_world_size"], "function", ["None"], ["\n", "", "def", "reduce_tensor", "(", "tensor", ")", ":", "\n", "    ", "rt", "=", "tensor", ".", "clone", "(", ")", "\n", "dist", ".", "all_reduce", "(", "rt", ",", "op", "=", "dist", ".", "ReduceOp", ".", "SUM", ")", "\n", "rt", "/=", "dist", ".", "get_world_size", "(", ")", "\n", "return", "rt", "\n", "\n", "\n", "", "class", "DistributedShuffle", ":", "\n", "    ", "@", "staticmethod", "\n", "def", "forward_shuffle", "(", "x", ",", "epoch", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.reduce_tensor": [[62, 67], ["tensor.clone", "torch.all_reduce", "torch.get_world_size"], "function", ["None"], ["x_all", "=", "dist_collect", "(", "x", ")", "\n", "forward_inds", ",", "backward_inds", "=", "DistributedShuffle", ".", "get_shuffle_ids", "(", "x_all", ".", "shape", "[", "0", "]", ",", "epoch", ")", "\n", "\n", "forward_inds_local", "=", "DistributedShuffle", ".", "get_local_id", "(", "forward_inds", ")", "\n", "return", "x_all", "[", "forward_inds_local", "]", ",", "backward_inds", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.set_bn_train": [[115, 123], ["model.eval", "model.apply", "classname.find", "m.train"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.train"], ["\n", "\n", "", "", "class", "SingeShuffle", "(", "DistributedShuffle", ")", ":", "\n", "\n", "    ", "def", "forward_shuffle", "(", "x", ",", "epoch", ")", ":", "\n", "        ", "forward_inds", ",", "backward_inds", "=", "DistributedShuffle", ".", "get_shuffle_ids", "(", "x", ".", "shape", "[", "0", "]", ",", "epoch", ")", "\n", "\n", "#forward_inds_local = DistributedShuffle.get_local_id(forward_inds)", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.moment_update": [[111, 115], ["zip", "model.parameters", "model_ema.parameters", "p2.data.mul_().add_", "p2.data.mul_", "p1.detach"], "function", ["None"], ["", "def", "moment_update", "(", "model", ",", "model_ema", ",", "m", ")", ":", "\n", "    ", "\"\"\" model_ema = m * model_ema + (1 - m) model \"\"\"", "\n", "for", "p1", ",", "p2", "in", "zip", "(", "model", ".", "parameters", "(", ")", ",", "model_ema", ".", "parameters", "(", ")", ")", ":", "\n", "        ", "p2", ".", "data", ".", "mul_", "(", "m", ")", ".", "add_", "(", "1", "-", "m", ",", "p1", ".", "detach", "(", ")", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.str2bool": [[137, 144], ["v.lower", "v.lower", "argparse.ArgumentTypeError"], "function", ["None"], ["", "", "def", "str2bool", "(", "v", ")", ":", "\n", "    ", "if", "v", ".", "lower", "(", ")", "in", "(", "'yes'", ",", "'true'", ",", "'t'", ",", "'y'", ",", "'1'", ")", ":", "\n", "        ", "return", "True", "\n", "", "elif", "v", ".", "lower", "(", ")", "in", "(", "'no'", ",", "'false'", ",", "'f'", ",", "'n'", ",", "'0'", ")", ":", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "'Unsupported value encountered.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.forward_hook": [[145, 152], ["module.register_forward_hook", "total_feat_out.append"], "function", ["None"], ["", "", "def", "forward_hook", "(", "model", ",", "total_feat_out", ",", "module", ")", ":", "\n", "\n", "    ", "def", "hook_fn_forward", "(", "module", ",", "input", ",", "output", ")", ":", "\n", "        ", "total_feat_out", ".", "append", "(", "output", ")", "\n", "\n", "", "hook", "=", "module", ".", "register_forward_hook", "(", "hook_fn_forward", ")", "\n", "return", "hook", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.convert_pretrained_weights": [[153, 183], ["state_dict.items", "np.concatenate.detach().numpy", "numpy.reshape", "np.concatenate.detach", "numpy.zeros", "range", "k.split", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "k.split", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.concatenate", "numpy.zeros", "numpy.zeros", "len"], "function", ["None"], ["", "def", "convert_pretrained_weights", "(", "state_dict", ",", "args", ")", ":", "\n", "    ", "new_state_dict", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "        ", "v", "=", "v", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "if", "(", "'conv'", "in", "k", ")", "or", "(", "'downsample.0'", "in", "k", ")", ":", "\n", "            ", "shape", "=", "v", ".", "shape", "\n", "v", "=", "np", ".", "reshape", "(", "v", ",", "newshape", "=", "[", "shape", "[", "0", "]", ",", "shape", "[", "1", "]", ",", "1", ",", "shape", "[", "2", "]", ",", "shape", "[", "3", "]", "]", ")", "\n", "if", "(", "shape", "[", "2", "]", "==", "3", ")", "and", "(", "shape", "[", "3", "]", "==", "3", ")", ":", "# basic conv3x3 layer", "\n", "                ", "kernel", "=", "np", ".", "zeros", "(", "shape", "=", "(", "shape", "[", "0", "]", ",", "shape", "[", "0", "]", ",", "3", ",", "1", ",", "1", ")", ")", "\n", "for", "n", "in", "range", "(", "0", ",", "shape", "[", "0", "]", ")", ":", "\n", "                    ", "kernel", "[", "n", ",", "n", ",", "0", ",", "0", ",", "0", "]", "=", "0.0", "\n", "kernel", "[", "n", ",", "n", ",", "1", ",", "0", ",", "0", "]", "=", "1.0", "\n", "kernel", "[", "n", ",", "n", ",", "2", ",", "0", ",", "0", "]", "=", "0.0", "\n", "", "ss", "=", "k", ".", "split", "(", "'.'", ")", "\n", "new_state_dict", "[", "k", "[", ":", "-", "len", "(", "ss", "[", "-", "1", "]", ")", "-", "1", "]", "+", "'_t.'", "+", "ss", "[", "-", "1", "]", "]", "=", "torch", ".", "from_numpy", "(", "kernel", ")", "\n", "", "else", ":", "\n", "                ", "if", "(", "shape", "[", "2", "]", "==", "7", ")", "and", "(", "shape", "[", "3", "]", "==", "7", ")", ":", "# first conv7x7 layer", "\n", "                    ", "v", "=", "np", ".", "concatenate", "(", "(", "np", ".", "zeros", "(", "shape", "=", "(", "shape", "[", "0", "]", ",", "shape", "[", "1", "]", ",", "1", ",", "shape", "[", "2", "]", ",", "shape", "[", "3", "]", ")", ")", ",", "v", ",", "np", ".", "zeros", "(", "shape", "=", "(", "shape", "[", "0", "]", ",", "shape", "[", "1", "]", ",", "2", ",", "shape", "[", "2", "]", ",", "shape", "[", "3", "]", ")", ")", ")", ",", "axis", "=", "2", ")", "\n", "", "", "", "if", "args", ".", "dataset", "==", "'ucf101'", ":", "\n", "            ", "if", "not", "'fc'", "in", "k", ":", "# remove final fc layer", "\n", "                ", "new_state_dict", "[", "k", "]", "=", "torch", ".", "from_numpy", "(", "v", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "'fc'", "in", "k", ":", "\n", "                ", "strs", "=", "k", ".", "split", "(", "'.'", ")", "\n", "new_key", "=", "'nce_inter_head.head.'", "+", "'.'", ".", "join", "(", "strs", "[", "-", "2", ":", "]", ")", "\n", "new_state_dict", "[", "new_key", "]", "=", "torch", ".", "from_numpy", "(", "v", ")", "\n", "", "else", ":", "\n", "                ", "new_state_dict", "[", "k", "]", "=", "torch", ".", "from_numpy", "(", "v", ")", "\n", "\n", "", "", "", "return", "new_state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.calc_topk_accuracy": [[184, 202], ["max", "target.size", "output.topk", "pred.t.t", "pred.t.eq", "target.view().expand_as", "correct[].view().float().sum", "res.append", "correct[].view().float().sum.mul_", "target.view", "correct[].view().float", "correct[].view"], "function", ["None"], ["", "def", "calc_topk_accuracy", "(", "output", ",", "target", ",", "topk", "=", "(", "1", ",", ")", ")", ":", "\n", "    ", "\"\"\"\n    Modified from: https://gist.github.com/agermanidis/275b23ad7a10ee89adccf021536bb97e\n    Given predicted and ground truth labels,\n    calculate top-k accuracies.\n    \"\"\"", "\n", "maxk", "=", "max", "(", "topk", ")", "\n", "batch_size", "=", "target", ".", "size", "(", "0", ")", "\n", "\n", "_", ",", "pred", "=", "output", ".", "topk", "(", "maxk", ",", "1", ",", "True", ",", "True", ")", "\n", "pred", "=", "pred", ".", "t", "(", ")", "\n", "correct", "=", "pred", ".", "eq", "(", "target", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "pred", ")", ")", "\n", "\n", "res", "=", "[", "]", "\n", "for", "k", "in", "topk", ":", "\n", "        ", "correct_k", "=", "correct", "[", ":", "k", "]", ".", "view", "(", "-", "1", ")", ".", "float", "(", ")", ".", "sum", "(", "0", ")", "\n", "res", ".", "append", "(", "correct_k", ".", "mul_", "(", "1", "/", "batch_size", ")", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.seed_torch": [[204, 213], ["random.seed", "str", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "function", ["None"], ["", "def", "seed_torch", "(", "seed", ")", ":", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "os", ".", "environ", "[", "'PYTHONHASHSEED'", "]", "=", "str", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "# if you are using multi-GPU.", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ToClipTensor.__call__": [[44, 54], ["torchvision.to_tensor"], "methods", ["None"], ["", "", "return", "input", "\n", "\n", "\n", "", "", "class", "ToClipTensor", "(", "object", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ToClipTensor.__repr__": [[55, 57], ["None"], "methods", ["None"], ["\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ClipRandomResizedCrop.__call__": [[60, 70], ["clip_transforms.ClipRandomResizedCrop.get_params", "torchvision.resized_crop"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipRandomResizedCropMotion.get_params"], ["\n", "\n", "return", "(", "[", "F", ".", "to_tensor", "(", "img", ")", "for", "img", "in", "clip", "]", ",", "is_flip", ")", "\n", "\n", "", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'()'", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ClipColorJitter.__call__": [[73, 104], ["torch.randperm", "torch.tensor().uniform_().item", "torch.tensor().uniform_().item", "torch.tensor().uniform_().item", "torch.tensor().uniform_().item", "torchvision.adjust_brightness", "torchvision.adjust_contrast", "torchvision.adjust_saturation", "torchvision.adjust_hue", "torch.tensor().uniform_", "torch.tensor().uniform_", "torch.tensor().uniform_", "torch.tensor().uniform_", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "", "class", "ClipRandomResizedCrop", "(", "transforms", ".", "RandomResizedCrop", ")", ":", "\n", "    ", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of PIL Image or Tensor): Clip to be cropped and resized.\n\n        Returns:\n            List of PIL Image or Tensor: Randomly cropped and resized clip.\n        \"\"\"", "\n", "i", ",", "j", ",", "h", ",", "w", "=", "self", ".", "get_params", "(", "clip", "[", "0", "]", ",", "self", ".", "scale", ",", "self", ".", "ratio", ")", "\n", "return", "[", "F", ".", "resized_crop", "(", "img", ",", "i", ",", "j", ",", "h", ",", "w", ",", "self", ".", "size", ",", "self", ".", "interpolation", ")", "for", "img", "in", "clip", "]", "\n", "\n", "\n", "", "", "class", "ClipRandomResizedCropMotion", "(", "object", ")", ":", "\n", "    ", "\"\"\"Crop the given PIL Image to random size and aspect ratio base on motion.\n    \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "size", ",", "scale", "=", "(", "0.08", ",", "1.0", ")", ",", "ratio", "=", "(", "3.", "/", "4.", ",", "4.", "/", "3.", ")", ",", "iou", "=", "0.9", ",", "interpolation", "=", "Image", ".", "BILINEAR", ")", ":", "\n", "        ", "if", "isinstance", "(", "size", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "self", ".", "size", "=", "size", "\n", "", "else", ":", "\n", "            ", "self", ".", "size", "=", "(", "size", ",", "size", ")", "\n", "", "if", "(", "scale", "[", "0", "]", ">", "scale", "[", "1", "]", ")", "or", "(", "ratio", "[", "0", "]", ">", "ratio", "[", "1", "]", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"range should be of kind (min, max)\"", ")", "\n", "\n", "", "self", ".", "interpolation", "=", "interpolation", "\n", "self", ".", "scale", "=", "scale", "\n", "self", ".", "ratio", "=", "ratio", "\n", "self", ".", "iou", "=", "iou", "\n", "\n", "", "@", "staticmethod", "\n", "def", "get_params", "(", "img", ",", "scale", ",", "ratio", ",", "mags", ")", ":", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ClipRandomGrayscale.__call__": [[107, 119], ["random.random", "torchvision.to_grayscale"], "methods", ["None"], ["\n", "width", ",", "height", "=", "_get_image_size", "(", "img", ")", "\n", "area", "=", "height", "*", "width", "\n", "\n", "mags", "=", "list", "(", "map", "(", "lambda", "x", ":", "np", ".", "array", "(", "x", ")", ".", "astype", "(", "np", ".", "float32", ")", ",", "mags", ")", ")", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ClipRandomHorizontalFlip.__call__": [[122, 133], ["torch.rand", "torchvision.hflip"], "methods", ["None"], ["for", "_", "in", "range", "(", "20", ")", ":", "\n", "            ", "sc", "=", "random", ".", "uniform", "(", "*", "scale", ")", "\n", "target_area", "=", "sc", "*", "area", "\n", "log_ratio", "=", "(", "math", ".", "log", "(", "ratio", "[", "0", "]", ")", ",", "math", ".", "log", "(", "ratio", "[", "1", "]", ")", ")", "\n", "aspect_ratio", "=", "math", ".", "exp", "(", "random", ".", "uniform", "(", "*", "log_ratio", ")", ")", "\n", "\n", "w", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "*", "aspect_ratio", ")", ")", ")", "\n", "h", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "/", "aspect_ratio", ")", ")", ")", "\n", "\n", "if", "0", "<", "w", "<=", "width", "and", "0", "<", "h", "<=", "height", ":", "\n", "                ", "if", "mags", "==", "[", "]", "or", "crop_ratio", "==", "0", ":", "\n", "                    ", "i", "=", "random", ".", "randint", "(", "0", ",", "height", "-", "h", ")", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ClipNormalize.__init__": [[152, 156], ["None"], "methods", ["None"], ["\n", "if", "ratio_out", ">=", "crop_ratio", ":", "\n", "                            ", "return", "i", ",", "j", ",", "h", ",", "w", "\n", "", "else", ":", "\n", "                            ", "continue", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ClipNormalize.__call__": [[157, 166], ["torchvision.normalize"], "methods", ["None"], ["\n", "# max ratio_out", "\n", "", "", "ratio_outs", "=", "np", ".", "array", "(", "ratio_out_list", ")", "\n", "idx", "=", "np", ".", "argsort", "(", "ratio_outs", ")", "[", "-", "1", "]", "\n", "i", ",", "j", ",", "h", ",", "w", "=", "crop_list", "[", "idx", "]", "\n", "\n", "return", "i", ",", "j", ",", "h", ",", "w", "\n", "\n", "# crop from corners", "\n", "", "", "", "if", "crop_ratio", "!=", "0", ":", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ClipNormalize.__repr__": [[167, 169], ["None"], "methods", ["None"], ["            ", "return", "crop_from_corners", "(", "row_s", ",", "col_s", ",", "height", ",", "width", ")", "\n", "\n", "# Fallback to central crop", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ClipResize.__call__": [[184, 193], ["torchvision.resize"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "clip", ",", "mags", ",", "flows", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of PIL Image or Tensor): Input clip.\n            flows (List of PIL Image or Tensor): Input clip's optical flow.\n            mags (List of PIL Image or Tensor): Input clip's motion magnitude.\n            return_flow : if crop and resize flows.\n        Returns:\n            List of PIL Image or Tensor: croped and resized clip and flows.\n        \"\"\"", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ClipCenterCrop.__call__": [[204, 213], ["torchvision.center_crop"], "methods", ["None"], ["\n", "", "", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "interpolate_str", "=", "_pil_interpolation_to_str", "[", "self", ".", "interpolation", "]", "\n", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'(size={0}'", ".", "format", "(", "self", ".", "size", ")", "\n", "format_string", "+=", "', scale={0}'", ".", "format", "(", "tuple", "(", "round", "(", "s", ",", "4", ")", "for", "s", "in", "self", ".", "scale", ")", ")", "\n", "format_string", "+=", "', ratio={0}'", ".", "format", "(", "tuple", "(", "round", "(", "r", ",", "4", ")", "for", "r", "in", "self", ".", "ratio", ")", ")", "\n", "format_string", "+=", "', interpolation={0})'", ".", "format", "(", "interpolate_str", ")", "\n", "return", "format_string", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ClipFirstCrop.__call__": [[224, 243], ["isinstance", "ValueError", "img.crop", "int", "int", "len", "msg.format"], "methods", ["None"], ["fn_idx", "=", "torch", ".", "randperm", "(", "4", ")", "\n", "for", "fn_id", "in", "fn_idx", ":", "\n", "            ", "if", "fn_id", "==", "0", "and", "self", ".", "brightness", "is", "not", "None", ":", "\n", "                ", "brightness", "=", "self", ".", "brightness", "\n", "brightness_factor", "=", "torch", ".", "tensor", "(", "1.0", ")", ".", "uniform_", "(", "brightness", "[", "0", "]", ",", "brightness", "[", "1", "]", ")", ".", "item", "(", ")", "\n", "clip", "=", "[", "F", ".", "adjust_brightness", "(", "img", ",", "brightness_factor", ")", "for", "img", "in", "clip", "]", "\n", "\n", "", "if", "fn_id", "==", "1", "and", "self", ".", "contrast", "is", "not", "None", ":", "\n", "                ", "contrast", "=", "self", ".", "contrast", "\n", "contrast_factor", "=", "torch", ".", "tensor", "(", "1.0", ")", ".", "uniform_", "(", "contrast", "[", "0", "]", ",", "contrast", "[", "1", "]", ")", ".", "item", "(", ")", "\n", "clip", "=", "[", "F", ".", "adjust_contrast", "(", "img", ",", "contrast_factor", ")", "for", "img", "in", "clip", "]", "\n", "\n", "", "if", "fn_id", "==", "2", "and", "self", ".", "saturation", "is", "not", "None", ":", "\n", "                ", "saturation", "=", "self", ".", "saturation", "\n", "saturation_factor", "=", "torch", ".", "tensor", "(", "1.0", ")", ".", "uniform_", "(", "saturation", "[", "0", "]", ",", "saturation", "[", "1", "]", ")", ".", "item", "(", ")", "\n", "clip", "=", "[", "F", ".", "adjust_saturation", "(", "img", ",", "saturation_factor", ")", "for", "img", "in", "clip", "]", "\n", "\n", "", "if", "fn_id", "==", "3", "and", "self", ".", "hue", "is", "not", "None", ":", "\n", "                ", "hue", "=", "self", ".", "hue", "\n", "hue_factor", "=", "torch", ".", "tensor", "(", "1.0", ")", ".", "uniform_", "(", "hue", "[", "0", "]", ",", "hue", "[", "1", "]", ")", ".", "item", "(", ")", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_transforms.ClipThirdCrop.__call__": [[254, 273], ["isinstance", "ValueError", "img.crop", "int", "int", "len", "msg.format"], "methods", ["None"], ["\n", "num_output_channels", "=", "1", "if", "clip", "[", "0", "]", ".", "mode", "==", "'L'", "else", "3", "\n", "if", "random", ".", "random", "(", ")", "<", "self", ".", "p", ":", "\n", "            ", "return", "[", "F", ".", "to_grayscale", "(", "img", ",", "num_output_channels", "=", "num_output_channels", ")", "for", "img", "in", "clip", "]", "\n", "", "return", "clip", "\n", "\n", "\n", "", "", "class", "ClipRandomHorizontalFlip", "(", "transforms", ".", "RandomHorizontalFlip", ")", ":", "\n", "\n", "    ", "def", "__call__", "(", "self", ",", "clip", ",", "is_flip", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of PIL Image or Tensor): Clip to be flipped.\n\n        Returns:\n            List of PIL Image or Tensor: Randomly flipped clip.\n        \"\"\"", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.ClipRandAugment.__init__": [[194, 198], ["clip_augmentations.augment_list"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.augment_list"], ["    ", "def", "__init__", "(", "self", ",", "n", ",", "m", ")", ":", "\n", "        ", "self", ".", "n", "=", "n", "\n", "self", ".", "m", "=", "m", "# [0, 30]", "\n", "self", ".", "augment_list", "=", "augment_list", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.ClipRandAugment.__call__": [[199, 205], ["random.choices", "op", "float", "float"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "ops", "=", "random", ".", "choices", "(", "self", ".", "augment_list", ",", "k", "=", "self", ".", "n", ")", "\n", "for", "op", ",", "minval", ",", "maxval", "in", "ops", ":", "\n", "            ", "val", "=", "(", "float", "(", "self", ".", "m", ")", "/", "30", ")", "*", "float", "(", "maxval", "-", "minval", ")", "+", "minval", "\n", "clip", "=", "op", "(", "clip", ",", "val", ")", "\n", "", "return", "clip", "", "", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.ShearX": [[12, 17], ["random.random", "img.transform"], "function", ["None"], ["def", "ShearX", "(", "clip", ",", "v", ")", ":", "# [0, 0.3]", "\n", "    ", "assert", "0", "<=", "v", "<=", "0.3", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "        ", "v", "=", "-", "v", "\n", "", "return", "[", "img", ".", "transform", "(", "img", ".", "size", ",", "PIL", ".", "Image", ".", "AFFINE", ",", "(", "1", ",", "v", ",", "0", ",", "0", ",", "1", ",", "0", ")", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.ShearY": [[19, 24], ["random.random", "img.transform"], "function", ["None"], ["", "def", "ShearY", "(", "clip", ",", "v", ")", ":", "# [0, 0.3]", "\n", "    ", "assert", "0", "<=", "v", "<=", "0.3", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "        ", "v", "=", "-", "v", "\n", "", "return", "[", "img", ".", "transform", "(", "img", ".", "size", ",", "PIL", ".", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "0", ",", "v", ",", "1", ",", "0", ")", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.TranslateXabs": [[26, 31], ["random.random", "img.transform"], "function", ["None"], ["", "def", "TranslateXabs", "(", "clip", ",", "v", ")", ":", "# [0, 100]", "\n", "    ", "assert", "0", "<=", "v", "<=", "100", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "        ", "v", "=", "-", "v", "\n", "", "return", "[", "img", ".", "transform", "(", "img", ".", "size", ",", "PIL", ".", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "v", ",", "0", ",", "1", ",", "0", ")", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.TranslateX": [[33, 39], ["random.random", "img.transform"], "function", ["None"], ["", "def", "TranslateX", "(", "clip", ",", "v", ")", ":", "# [0, 0.4464]", "\n", "    ", "assert", "0", "<=", "v", "<=", "0.4464", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "        ", "v", "=", "-", "v", "\n", "", "v", "=", "v", "*", "clip", "[", "0", "]", ".", "size", "[", "0", "]", "\n", "return", "[", "img", ".", "transform", "(", "img", ".", "size", ",", "PIL", ".", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "v", ",", "0", ",", "1", ",", "0", ")", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.TranslateYabs": [[41, 46], ["random.random", "img.transform"], "function", ["None"], ["", "def", "TranslateYabs", "(", "clip", ",", "v", ")", ":", "# [0, 100]", "\n", "    ", "assert", "0", "<=", "v", "<=", "100", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "        ", "v", "=", "-", "v", "\n", "", "return", "[", "img", ".", "transform", "(", "img", ".", "size", ",", "PIL", ".", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "0", ",", "0", ",", "1", ",", "v", ")", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.TranslateY": [[48, 54], ["random.random", "img.transform"], "function", ["None"], ["", "def", "TranslateY", "(", "clip", ",", "v", ")", ":", "# [0, 0.4464]", "\n", "    ", "assert", "0", "<=", "v", "<=", "0.4464", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "        ", "v", "=", "-", "v", "\n", "", "v", "=", "v", "*", "clip", "[", "0", "]", ".", "size", "[", "1", "]", "\n", "return", "[", "img", ".", "transform", "(", "img", ".", "size", ",", "PIL", ".", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "0", ",", "0", ",", "1", ",", "v", ")", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Rotate": [[56, 61], ["random.random", "img.rotate"], "function", ["None"], ["", "def", "Rotate", "(", "clip", ",", "v", ")", ":", "# [0, 30]", "\n", "    ", "assert", "0", "<=", "v", "<=", "30", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "        ", "v", "=", "-", "v", "\n", "", "return", "[", "img", ".", "rotate", "(", "v", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.AutoContrast": [[63, 65], ["PIL.ImageOps.autocontrast", "PIL.ImageOps.autocontrast", "PIL.ImageOps.autocontrast", "PIL.ImageOps.autocontrast"], "function", ["None"], ["", "def", "AutoContrast", "(", "clip", ",", "_", ")", ":", "\n", "    ", "return", "[", "PIL", ".", "ImageOps", ".", "autocontrast", "(", "img", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Invert": [[67, 69], ["PIL.ImageOps.invert", "PIL.ImageOps.invert", "PIL.ImageOps.invert", "PIL.ImageOps.invert"], "function", ["None"], ["", "def", "Invert", "(", "clip", ",", "_", ")", ":", "\n", "    ", "return", "[", "PIL", ".", "ImageOps", ".", "invert", "(", "img", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Equalize": [[71, 73], ["PIL.ImageOps.equalize", "PIL.ImageOps.equalize", "PIL.ImageOps.equalize", "PIL.ImageOps.equalize"], "function", ["None"], ["", "def", "Equalize", "(", "clip", ",", "_", ")", ":", "\n", "    ", "return", "[", "PIL", ".", "ImageOps", ".", "equalize", "(", "img", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Solarize": [[75, 78], ["PIL.ImageOps.solarize", "PIL.ImageOps.solarize", "PIL.ImageOps.solarize", "PIL.ImageOps.solarize"], "function", ["None"], ["", "def", "Solarize", "(", "clip", ",", "v", ")", ":", "# [0, 256]", "\n", "    ", "assert", "0", "<=", "v", "<=", "256", "\n", "return", "[", "PIL", ".", "ImageOps", ".", "solarize", "(", "img", ",", "256", "-", "v", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.SolarizeAdd": [[80, 94], ["numpy.array", "np.array.astype", "numpy.clip", "img_add.astype.astype", "numpy.where", "clip_out.append", "PIL.Image.fromarray"], "function", ["None"], ["", "def", "SolarizeAdd", "(", "clip", ",", "v", ")", ":", "# [0, 110]", "\n", "    ", "assert", "0", "<=", "v", "<=", "110", "\n", "threshold", "=", "128", "\n", "\n", "clip_out", "=", "[", "]", "\n", "for", "img", "in", "clip", ":", "\n", "        ", "img_raw", "=", "np", ".", "array", "(", "img", ")", "\n", "img_add", "=", "img_raw", ".", "astype", "(", "np", ".", "int", ")", "\n", "img_add", "=", "img_add", "+", "v", "\n", "img_add", "=", "np", ".", "clip", "(", "img_add", ",", "0", ",", "255", ")", "\n", "img_add", "=", "img_add", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "img_np", "=", "np", ".", "where", "(", "img_raw", "<", "threshold", ",", "img_add", ",", "img_raw", ")", "\n", "clip_out", ".", "append", "(", "Image", ".", "fromarray", "(", "img_np", ")", ")", "\n", "", "return", "clip_out", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Posterize": [[96, 100], ["int", "PIL.ImageOps.posterize", "PIL.ImageOps.posterize", "PIL.ImageOps.posterize", "PIL.ImageOps.posterize"], "function", ["None"], ["", "def", "Posterize", "(", "clip", ",", "v", ")", ":", "# [0, 4]", "\n", "    ", "assert", "0", "<=", "v", "<=", "4", "\n", "v", "=", "int", "(", "v", ")", "\n", "return", "[", "PIL", ".", "ImageOps", ".", "posterize", "(", "img", ",", "4", "-", "v", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Contrast": [[102, 107], ["random.random", "PIL.ImageEnhance.Contrast().enhance", "PIL.ImageEnhance.Contrast().enhance", "PIL.ImageEnhance.Contrast().enhance", "PIL.ImageEnhance.Contrast().enhance", "PIL.ImageEnhance.Contrast", "PIL.ImageEnhance.Contrast", "PIL.ImageEnhance.Contrast", "PIL.ImageEnhance.Contrast"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Contrast", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Contrast", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Contrast", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Contrast"], ["", "def", "Contrast", "(", "clip", ",", "v", ")", ":", "# [0, 0.9]", "\n", "    ", "assert", "0", "<=", "v", "<=", "0.9", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "        ", "v", "=", "-", "v", "\n", "", "return", "[", "PIL", ".", "ImageEnhance", ".", "Contrast", "(", "img", ")", ".", "enhance", "(", "1", "-", "v", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Color": [[109, 114], ["random.random", "PIL.ImageEnhance.Color().enhance", "PIL.ImageEnhance.Color().enhance", "PIL.ImageEnhance.Color().enhance", "PIL.ImageEnhance.Color().enhance", "PIL.ImageEnhance.Color", "PIL.ImageEnhance.Color", "PIL.ImageEnhance.Color", "PIL.ImageEnhance.Color"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Color", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Color", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Color", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Color"], ["", "def", "Color", "(", "clip", ",", "v", ")", ":", "# [0, 0.9]", "\n", "    ", "assert", "0", "<=", "v", "<=", "0.9", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "        ", "v", "=", "-", "v", "\n", "", "return", "[", "PIL", ".", "ImageEnhance", ".", "Color", "(", "img", ")", ".", "enhance", "(", "1", "-", "v", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Brightness": [[116, 121], ["random.random", "PIL.ImageEnhance.Brightness().enhance", "PIL.ImageEnhance.Brightness().enhance", "PIL.ImageEnhance.Brightness().enhance", "PIL.ImageEnhance.Brightness().enhance", "PIL.ImageEnhance.Brightness", "PIL.ImageEnhance.Brightness", "PIL.ImageEnhance.Brightness", "PIL.ImageEnhance.Brightness"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Brightness", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Brightness", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Brightness", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Brightness"], ["", "def", "Brightness", "(", "clip", ",", "v", ")", ":", "# [0, 0.9]", "\n", "    ", "assert", "0", "<=", "v", "<=", "0.9", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "        ", "v", "=", "-", "v", "\n", "", "return", "[", "PIL", ".", "ImageEnhance", ".", "Brightness", "(", "img", ")", ".", "enhance", "(", "1", "-", "v", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Sharpness": [[123, 128], ["random.random", "PIL.ImageEnhance.Sharpness().enhance", "PIL.ImageEnhance.Sharpness().enhance", "PIL.ImageEnhance.Sharpness().enhance", "PIL.ImageEnhance.Sharpness().enhance", "PIL.ImageEnhance.Sharpness", "PIL.ImageEnhance.Sharpness", "PIL.ImageEnhance.Sharpness", "PIL.ImageEnhance.Sharpness"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Sharpness", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Sharpness", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Sharpness", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Sharpness"], ["", "def", "Sharpness", "(", "clip", ",", "v", ")", ":", "# [0, 0.9]", "\n", "    ", "assert", "0", "<=", "v", "<=", "0.9", "\n", "if", "random", ".", "random", "(", ")", ">", "0.5", ":", "\n", "        ", "v", "=", "-", "v", "\n", "", "return", "[", "PIL", ".", "ImageEnhance", ".", "Sharpness", "(", "img", ")", ".", "enhance", "(", "1", "-", "v", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.Cutout": [[130, 135], ["clip_augmentations.CutoutAbs"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.CutoutAbs"], ["", "def", "Cutout", "(", "clip", ",", "v", ")", ":", "# [0, 0.3571]", "\n", "    ", "assert", "0.0", "<=", "v", "<=", "0.3571", "\n", "\n", "v", "=", "v", "*", "clip", "[", "0", "]", ".", "size", "[", "0", "]", "\n", "return", "CutoutAbs", "(", "clip", ",", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.CutoutAbs": [[137, 160], ["numpy.random.uniform", "numpy.random.uniform", "int", "int", "min", "min", "max", "max", "img.copy.copy", "PIL.ImageDraw.Draw().rectangle", "PIL.ImageDraw.Draw().rectangle", "PIL.ImageDraw.Draw().rectangle", "PIL.ImageDraw.Draw().rectangle", "clip_out.append", "PIL.ImageDraw.Draw", "PIL.ImageDraw.Draw", "PIL.ImageDraw.Draw", "PIL.ImageDraw.Draw"], "function", ["None"], ["", "def", "CutoutAbs", "(", "clip", ",", "v", ")", ":", "# [0, 80]", "\n", "# assert 0 <= v <= 80", "\n", "    ", "if", "v", "<", "0", ":", "\n", "        ", "return", "clip", "\n", "\n", "", "w", ",", "h", "=", "clip", "[", "0", "]", ".", "size", "\n", "x0", "=", "np", ".", "random", ".", "uniform", "(", "w", ")", "\n", "y0", "=", "np", ".", "random", ".", "uniform", "(", "h", ")", "\n", "\n", "x0", "=", "int", "(", "max", "(", "0", ",", "x0", "-", "v", "/", "2.", ")", ")", "\n", "y0", "=", "int", "(", "max", "(", "0", ",", "y0", "-", "v", "/", "2.", ")", ")", "\n", "x1", "=", "min", "(", "w", ",", "x0", "+", "v", ")", "\n", "y1", "=", "min", "(", "h", ",", "y0", "+", "v", ")", "\n", "\n", "xy", "=", "(", "x0", ",", "y0", ",", "x1", ",", "y1", ")", "\n", "color", "=", "(", "125", ",", "123", ",", "114", ")", "\n", "\n", "clip_out", "=", "[", "]", "\n", "for", "img", "in", "clip", ":", "\n", "        ", "img", "=", "img", ".", "copy", "(", ")", "\n", "PIL", ".", "ImageDraw", ".", "Draw", "(", "img", ")", ".", "rectangle", "(", "xy", ",", "color", ")", "\n", "clip_out", ".", "append", "(", "img", ")", "\n", "", "return", "clip_out", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations.augment_list": [[162, 187], ["None"], "function", ["None"], ["", "def", "augment_list", "(", ")", ":", "# 16 oeprations and their ranges", "\n", "# https://github.com/tensorflow/tpu/blob/8462d083dd89489a79e3200bcc8d4063bf362186/models/official/efficientnet/autoaugment.py#L505", "\n", "    ", "l", "=", "[", "\n", "(", "AutoContrast", ",", "0", ",", "1", ")", ",", "\n", "(", "Equalize", ",", "0", ",", "1", ")", ",", "\n", "(", "Invert", ",", "0", ",", "1", ")", ",", "\n", "\n", "(", "Rotate", ",", "0", ",", "30", ")", ",", "\n", "(", "Posterize", ",", "0", ",", "4", ")", ",", "\n", "(", "Solarize", ",", "0", ",", "256", ")", ",", "\n", "(", "SolarizeAdd", ",", "0", ",", "110", ")", ",", "\n", "\n", "(", "Color", ",", "0", ",", "0.9", ")", ",", "\n", "(", "Contrast", ",", "0", ",", "0.9", ")", ",", "\n", "(", "Brightness", ",", "0", ",", "0.9", ")", ",", "\n", "(", "Sharpness", ",", "0", ",", "0.9", ")", ",", "\n", "\n", "(", "ShearX", ",", "0.", ",", "0.3", ")", ",", "\n", "(", "ShearY", ",", "0.", ",", "0.3", ")", ",", "\n", "(", "Cutout", ",", "0", ",", "0.3571", ")", ",", "\n", "(", "TranslateX", ",", "0.", ",", "0.4464", ")", ",", "\n", "(", "TranslateY", ",", "0.", ",", "0.4464", ")", ",", "\n", "]", "\n", "\n", "return", "l", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations._is_pil_image": [[189, 191], ["isinstance"], "function", ["None"], ["", "def", "_is_pil_image", "(", "img", ")", ":", "\n", "    ", "return", "isinstance", "(", "img", ",", "Image", ".", "Image", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.get_jit_model_analysis": [[69, 123], ["isinstance", "hasattr", "collections.Counter", "collections.Counter", "torch.jit.get_trace_graph", "torch.jit.get_trace_graph", "trace.graph().nodes", "torch.jit._get_trace_graph", "torch.jit._get_trace_graph", "trace.nodes", "node.kind", "ops_handles.get", "ops_handles.get.", "ops_handles.keys", "list", "list", "trace.graph", "node.inputs", "node.outputs"], "function", ["None"], ["def", "get_jit_model_analysis", "(", "\n", "model", ":", "nn", ".", "Module", ",", "\n", "inputs", ":", "typing", ".", "Tuple", "[", "object", ",", "...", "]", ",", "\n", "ops_handles", ":", "typing", ".", "Dict", "[", "str", ",", "typing", ".", "Callable", "]", ",", "\n", ")", "->", "typing", ".", "Tuple", "[", "typing", ".", "Counter", "[", "str", "]", ",", "typing", ".", "Counter", "[", "str", "]", "]", ":", "\n", "    ", "\"\"\"\n    Given a model, the inputs and the handles for each operation, return the\n    results for the model analysis.\n\n    Args:\n        model (nn.Module): The model for torch script to trace.\n        inputs (tuple): Inputs that are passed to `model` to trace. Inputs need\n            to be in a tuple.\n        ops_handles (typing.Dict[str, typing.Callable]): A dictionary of handles\n            for model analysis.\n\n    Returns:\n        typing.Tuple[typing.Counter[str], typing.Counter[str]]: A counter that\n            contains the results of per operation analysis of the model and a\n            Counter of ignored operations.\n    \"\"\"", "\n", "# Torch script does not support parallel torch models.", "\n", "if", "isinstance", "(", "\n", "model", ",", "(", "nn", ".", "parallel", ".", "distributed", ".", "DistributedDataParallel", ",", "nn", ".", "DataParallel", ")", "\n", ")", ":", "\n", "        ", "model", "=", "model", ".", "module", "\n", "\n", "# Compatibility with torch.jit.", "\n", "", "if", "hasattr", "(", "torch", ".", "jit", ",", "\"get_trace_graph\"", ")", ":", "\n", "        ", "trace", ",", "_", "=", "torch", ".", "jit", ".", "get_trace_graph", "(", "model", ",", "inputs", ")", "\n", "trace_nodes", "=", "trace", ".", "graph", "(", ")", ".", "nodes", "(", ")", "\n", "", "else", ":", "\n", "        ", "trace", ",", "_", "=", "torch", ".", "jit", ".", "_get_trace_graph", "(", "model", ",", "inputs", ")", "\n", "trace_nodes", "=", "trace", ".", "nodes", "(", ")", "\n", "\n", "", "skipped_ops", "=", "Counter", "(", ")", "\n", "total_count", "=", "Counter", "(", ")", "\n", "\n", "for", "node", "in", "trace_nodes", ":", "\n", "        ", "kind", "=", "node", ".", "kind", "(", ")", "\n", "if", "kind", "not", "in", "ops_handles", ".", "keys", "(", ")", ":", "\n", "# If the operation is not in _IGNORED_OPS, count skipped operations.", "\n", "            ", "if", "kind", "not", "in", "_IGNORED_OPS", ":", "\n", "                ", "skipped_ops", "[", "kind", "]", "+=", "1", "\n", "", "continue", "\n", "\n", "", "handle_count", "=", "ops_handles", ".", "get", "(", "kind", ",", "None", ")", "\n", "if", "handle_count", "is", "None", ":", "\n", "            ", "continue", "\n", "# pyre-ignore", "\n", "", "inputs", ",", "outputs", "=", "list", "(", "node", ".", "inputs", "(", ")", ")", ",", "list", "(", "node", ".", "outputs", "(", ")", ")", "\n", "op_count", "=", "handle_count", "(", "inputs", ",", "outputs", ")", "\n", "total_count", "+=", "op_count", "\n", "", "return", "total_count", ",", "skipped_ops", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.generic_activation_jit": [[125, 156], ["jit_handles.get_shape", "numpy.prod", "collections.Counter", "jit_handles.generic_activation_jit._generic_activation_jit"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.get_shape"], ["", "def", "generic_activation_jit", "(", "\n", "op_name", ":", "str", ",", "\n", ")", "->", "typing", ".", "Callable", "[", "[", "typing", ".", "List", "[", "object", "]", ",", "typing", ".", "List", "[", "object", "]", "]", ",", "typing", ".", "Counter", "[", "str", "]", "]", ":", "\n", "    ", "\"\"\"\n    This method return a handle that counts the number of activation from the\n    output shape for the specified operation.\n\n    Args:\n        op_name (str): The name of the operation.\n\n    Returns:\n        typing.Callable: An activation handle for the given operation.\n    \"\"\"", "\n", "\n", "def", "_generic_activation_jit", "(", "outputs", ":", "typing", ".", "List", "[", "object", "]", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        This is a generic jit handle that counts the number of activations for any\n        operation given the output shape.\n\n        Args:\n            outputs (list(torch._C.Value)): The output shape in the form of a list\n                of jit object.\n\n        Returns:\n            int: Total number of activations for each operation.\n        \"\"\"", "\n", "out_shape", "=", "get_shape", "(", "outputs", "[", "0", "]", ")", "\n", "ac_count", "=", "prod", "(", "out_shape", ")", "\n", "return", "ac_count", "\n", "\n", "", "return", "lambda", "inputs", ",", "outputs", ":", "Counter", "(", "{", "op_name", ":", "_generic_activation_jit", "(", "outputs", ")", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.get_shape": [[158, 172], ["val.isCompleteTensor", "val.type().sizes", "ValueError", "val.type"], "function", ["None"], ["", "def", "get_shape", "(", "val", ":", "object", ")", "->", "typing", ".", "List", "[", "int", "]", ":", "\n", "    ", "\"\"\"\n    Get the shapes from a jit value object.\n\n    Args:\n        val (torch._C.Value): jit value object.\n\n    Returns:\n        list(int): return a list of ints.\n    \"\"\"", "\n", "if", "val", ".", "isCompleteTensor", "(", ")", ":", "# pyre-ignore", "\n", "        ", "return", "val", ".", "type", "(", ")", ".", "sizes", "(", ")", "# pyre-ignore", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.addmm_flop_jit": [[174, 202], ["collections.Counter", "jit_handles.get_shape", "len", "len"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.get_shape"], ["", "", "def", "addmm_flop_jit", "(", "\n", "inputs", ":", "typing", ".", "List", "[", "object", "]", ",", "outputs", ":", "typing", ".", "List", "[", "object", "]", "\n", ")", "->", "typing", ".", "Counter", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    This method counts the flops for fully connected layers with torch script.\n\n    Args:\n        inputs (list(torch._C.Value)): The input shape in the form of a list of\n            jit object.\n        outputs (list(torch._C.Value)): The output shape in the form of a list\n            of jit object.\n\n    Returns:\n        Counter: A Counter dictionary that records the number of flops for each\n            operation.\n    \"\"\"", "\n", "# Count flop for nn.Linear", "\n", "# inputs is a list of length 3.", "\n", "input_shapes", "=", "[", "get_shape", "(", "v", ")", "for", "v", "in", "inputs", "[", "1", ":", "3", "]", "]", "\n", "# input_shapes[0]: [batch size, input feature dimension]", "\n", "# input_shapes[1]: [batch size, output feature dimension]", "\n", "assert", "len", "(", "input_shapes", "[", "0", "]", ")", "==", "2", ",", "input_shapes", "[", "0", "]", "\n", "assert", "len", "(", "input_shapes", "[", "1", "]", ")", "==", "2", ",", "input_shapes", "[", "1", "]", "\n", "batch_size", ",", "input_dim", "=", "input_shapes", "[", "0", "]", "\n", "output_dim", "=", "input_shapes", "[", "1", "]", "[", "1", "]", "\n", "flop", "=", "batch_size", "*", "input_dim", "*", "output_dim", "\n", "flop_counter", "=", "Counter", "(", "{", "\"addmm\"", ":", "flop", "}", ")", "\n", "return", "flop_counter", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.conv_flop_count": [[204, 225], ["int", "int", "collections.Counter", "numpy.prod", "numpy.prod"], "function", ["None"], ["", "def", "conv_flop_count", "(", "\n", "x_shape", ":", "typing", ".", "List", "[", "int", "]", ",", "w_shape", ":", "typing", ".", "List", "[", "int", "]", ",", "out_shape", ":", "typing", ".", "List", "[", "int", "]", "\n", ")", "->", "typing", ".", "Counter", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    This method counts the flops for convolution. Note only multiplication is\n    counted. Computation for addition and bias is ignored.\n\n    Args:\n        x_shape (list(int)): The input shape before convolution.\n        w_shape (list(int)): The filter shape.\n        out_shape (list(int)): The output shape after convolution.\n    Returns:\n        Counter: A Counter dictionary that records the number of flops for each\n            operation.\n    \"\"\"", "\n", "batch_size", ",", "Cin_dim", ",", "Cout_dim", "=", "x_shape", "[", "0", "]", ",", "w_shape", "[", "1", "]", ",", "out_shape", "[", "1", "]", "\n", "out_size", "=", "int", "(", "prod", "(", "out_shape", "[", "2", ":", "]", ")", ")", "\n", "kernel_size", "=", "int", "(", "prod", "(", "w_shape", "[", "2", ":", "]", ")", ")", "\n", "flop", "=", "batch_size", "*", "out_size", "*", "Cout_dim", "*", "Cin_dim", "*", "kernel_size", "\n", "flop_counter", "=", "Counter", "(", "{", "\"conv\"", ":", "flop", "}", ")", "\n", "return", "flop_counter", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.conv_flop_jit": [[227, 252], ["len", "jit_handles.conv_flop_count", "jit_handles.get_shape", "jit_handles.get_shape", "jit_handles.get_shape", "len", "len"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.conv_flop_count", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.get_shape", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.get_shape", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.get_shape"], ["", "def", "conv_flop_jit", "(", "\n", "inputs", ":", "typing", ".", "List", "[", "object", "]", ",", "outputs", ":", "typing", ".", "List", "[", "object", "]", "\n", ")", "->", "typing", ".", "Counter", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    This method counts the flops for convolution using torch script.\n\n    Args:\n        inputs (list(torch._C.Value)): The input shape in the form of a list of\n            jit object before convolution.\n        outputs (list(torch._C.Value)): The output shape in the form of a list\n            of jit object after convolution.\n\n    Returns:\n        Counter: A Counter dictionary that records the number of flops for each\n            operation.\n    \"\"\"", "\n", "# Inputs of Convolution should be a list of length 12 or 13. They represent:", "\n", "# 0) input tensor, 1) convolution filter, 2) bias, 3) stride, 4) padding,", "\n", "# 5) dilation, 6) transposed, 7) out_pad, 8) groups, 9) benchmark_cudnn,", "\n", "# 10) deterministic_cudnn and 11) user_enabled_cudnn.", "\n", "# starting with #40737 it will be 12) user_enabled_tf32", "\n", "assert", "len", "(", "inputs", ")", "==", "12", "or", "len", "(", "inputs", ")", "==", "13", ",", "len", "(", "inputs", ")", "\n", "x", ",", "w", "=", "inputs", "[", ":", "2", "]", "\n", "x_shape", ",", "w_shape", ",", "out_shape", "=", "(", "get_shape", "(", "x", ")", ",", "get_shape", "(", "w", ")", ",", "get_shape", "(", "outputs", "[", "0", "]", ")", ")", "\n", "return", "conv_flop_count", "(", "x_shape", ",", "w_shape", ",", "out_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.einsum_flop_jit": [[254, 302], ["len", "inputs[].toIValue", "equation.translate.replace", "collections.OrderedDict().keys", "equation.translate.translate", "inputs[].node().inputs", "len", "ord", "jit_handles.get_shape", "collections.Counter", "collections.OrderedDict", "enumerate", "inputs[].node", "collections.Counter", "NotImplementedError", "k.isalpha"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.get_shape"], ["", "def", "einsum_flop_jit", "(", "\n", "inputs", ":", "typing", ".", "List", "[", "object", "]", ",", "outputs", ":", "typing", ".", "List", "[", "object", "]", "\n", ")", "->", "typing", ".", "Counter", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    This method counts the flops for the einsum operation. We currently support\n    two einsum operations: \"nct,ncp->ntp\" and \"ntg,ncg->nct\".\n\n    Args:\n        inputs (list(torch._C.Value)): The input shape in the form of a list of\n            jit object before einsum.\n        outputs (list(torch._C.Value)): The output shape in the form of a list\n            of jit object after einsum.\n\n    Returns:\n        Counter: A Counter dictionary that records the number of flops for each\n            operation.\n    \"\"\"", "\n", "# Inputs of einsum should be a list of length 2.", "\n", "# Inputs[0] stores the equation used for einsum.", "\n", "# Inputs[1] stores the list of input shapes.", "\n", "assert", "len", "(", "inputs", ")", "==", "2", ",", "len", "(", "inputs", ")", "\n", "equation", "=", "inputs", "[", "0", "]", ".", "toIValue", "(", ")", "# pyre-ignore", "\n", "# Get rid of white space in the equation string.", "\n", "equation", "=", "equation", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "\n", "# Re-map equation so that same equation with different alphabet", "\n", "# representations will look the same.", "\n", "letter_order", "=", "OrderedDict", "(", "(", "k", ",", "0", ")", "for", "k", "in", "equation", "if", "k", ".", "isalpha", "(", ")", ")", ".", "keys", "(", ")", "\n", "mapping", "=", "{", "ord", "(", "x", ")", ":", "97", "+", "i", "for", "i", ",", "x", "in", "enumerate", "(", "letter_order", ")", "}", "\n", "equation", "=", "equation", ".", "translate", "(", "mapping", ")", "\n", "input_shapes_jit", "=", "inputs", "[", "1", "]", ".", "node", "(", ")", ".", "inputs", "(", ")", "# pyre-ignore", "\n", "input_shapes", "=", "[", "get_shape", "(", "v", ")", "for", "v", "in", "input_shapes_jit", "]", "\n", "\n", "if", "equation", "==", "\"abc,abd->acd\"", ":", "\n", "        ", "n", ",", "c", ",", "t", "=", "input_shapes", "[", "0", "]", "\n", "p", "=", "input_shapes", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "flop", "=", "n", "*", "c", "*", "t", "*", "p", "\n", "flop_counter", "=", "Counter", "(", "{", "\"einsum\"", ":", "flop", "}", ")", "\n", "return", "flop_counter", "\n", "\n", "", "elif", "equation", "==", "\"abc,adc->adb\"", ":", "\n", "        ", "n", ",", "t", ",", "g", "=", "input_shapes", "[", "0", "]", "\n", "c", "=", "input_shapes", "[", "-", "1", "]", "[", "1", "]", "\n", "flop", "=", "n", "*", "t", "*", "g", "*", "c", "\n", "flop_counter", "=", "Counter", "(", "{", "\"einsum\"", ":", "flop", "}", ")", "\n", "return", "flop_counter", "\n", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Unsupported einsum operation.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.matmul_flop_jit": [[304, 331], ["collections.Counter", "jit_handles.get_shape", "len", "len"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.get_shape"], ["", "", "def", "matmul_flop_jit", "(", "\n", "inputs", ":", "typing", ".", "List", "[", "object", "]", ",", "outputs", ":", "typing", ".", "List", "[", "object", "]", "\n", ")", "->", "typing", ".", "Counter", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    This method counts the flops for matmul.\n\n    Args:\n        inputs (list(torch._C.Value)): The input shape in the form of a list of\n            jit object before matmul.\n        outputs (list(torch._C.Value)): The output shape in the form of a list\n            of jit object after matmul.\n\n    Returns:\n        Counter: A Counter dictionary that records the number of flops for each\n            operation.\n    \"\"\"", "\n", "# Inputs should be a list of length 2.", "\n", "# Inputs contains the shapes of two matrices.", "\n", "input_shapes", "=", "[", "get_shape", "(", "v", ")", "for", "v", "in", "inputs", "]", "\n", "assert", "len", "(", "input_shapes", ")", "==", "2", ",", "input_shapes", "\n", "assert", "len", "(", "input_shapes", "[", "1", "]", ")", "==", "2", ",", "input_shapes", "\n", "assert", "input_shapes", "[", "0", "]", "[", "-", "1", "]", "==", "input_shapes", "[", "1", "]", "[", "0", "]", ",", "input_shapes", "\n", "batch_dim", "=", "input_shapes", "[", "0", "]", "[", "0", "]", "\n", "m1_dim", ",", "m2_dim", "=", "input_shapes", "[", "1", "]", "\n", "flop", "=", "m1_dim", "*", "m2_dim", "*", "batch_dim", "\n", "flop_counter", "=", "Counter", "(", "{", "\"matmul\"", ":", "flop", "}", ")", "\n", "return", "flop_counter", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.batchnorm_flop_jit": [[333, 355], ["jit_handles.get_shape", "collections.Counter", "len", "int", "numpy.prod"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.jit_handles.get_shape"], ["", "def", "batchnorm_flop_jit", "(", "\n", "inputs", ":", "typing", ".", "List", "[", "object", "]", ",", "outputs", ":", "typing", ".", "List", "[", "object", "]", "\n", ")", "->", "typing", ".", "Counter", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    This method counts the flops for batch norm.\n\n    Args:\n        inputs (list(torch._C.Value)): The input shape in the form of a list of\n            jit object before batch norm.\n        outputs (list(torch._C.Value)): The output shape in the form of a list\n            of jit object after batch norm.\n\n    Returns:\n        Counter: A Counter dictionary that records the number of flops for each\n            operation.\n    \"\"\"", "\n", "# Inputs[0] contains the shape of the input.", "\n", "input_shape", "=", "get_shape", "(", "inputs", "[", "0", "]", ")", "\n", "assert", "2", "<=", "len", "(", "input_shape", ")", "<=", "5", ",", "input_shape", "\n", "flop", "=", "int", "(", "prod", "(", "input_shape", ")", ")", "*", "4", "\n", "flop_counter", "=", "Counter", "(", "{", "\"batchnorm\"", ":", "flop", "}", ")", "\n", "return", "flop_counter", "\n", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.GaussianBlur.__init__": [[128, 130], ["None"], "methods", ["None"], ["        "]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.GaussianBlur.__call__": [[131, 135], ["random.uniform", "x.filter.filter.filter", "PIL.ImageFilter.GaussianBlur"], "methods", ["None"], ["\n", "x_all", "=", "x", "\n", "\n", "return", "x_all", "[", "backward_inds", "]", ",", "x_all", "[", "backward_inds", "]", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.ClipGaussianBlur.__init__": [[139, 141], ["None"], "methods", ["None"], ["        ", "return", "True", "\n", "", "elif", "v", ".", "lower", "(", ")", "in", "(", "'no'", ",", "'false'", ",", "'f'", ",", "'n'", ",", "'0'", ")", ":", "\n", "        ", "return", "False", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.ClipGaussianBlur.__call__": [[142, 145], ["random.uniform", "img.filter", "PIL.ImageFilter.GaussianBlur"], "methods", ["None"], ["", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "'Unsupported value encountered.'", ")", "\n", "\n", "", "", "def", "forward_hook", "(", "model", ",", "total_feat_out", ",", "module", ")", ":", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.accuracy": [[31, 46], ["torch.no_grad", "torch.no_grad", "max", "target.size", "output.topk", "pred.t.t", "pred.t.eq", "target.view().expand_as", "correct[].view().float().sum", "res.append", "correct[].view().float().sum.mul_", "target.view", "correct[].view().float", "correct[].view"], "function", ["None"], ["self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n", "\n", "", "", "def", "dist_collect", "(", "x", ")", ":", "\n", "    ", "\"\"\" collect all tensor from all GPUs\n    args:\n        x: shape (mini_batch, ...)\n    returns:\n        shape (mini_batch * num_gpu, ...)\n    \"\"\"", "\n", "x", "=", "x", ".", "contiguous", "(", ")", "\n", "out_list", "=", "[", "torch", ".", "zeros_like", "(", "x", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", "\n", "for", "_", "in", "range", "(", "dist", ".", "get_world_size", "(", ")", ")", "]", "\n", "dist", ".", "all_gather", "(", "out_list", ",", "x", ")", "\n", "return", "torch", ".", "cat", "(", "out_list", ",", "dim", "=", "0", ")", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset.__init__": [[14, 28], ["super().__init__", "video_dataset.VideoDataset._load_list"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset._load_list"], ["from", "PIL", "import", "Image", "\n", "from", "torchvision", "import", "transforms", "\n", "import", "json", "\n", "import", "csv", "\n", "import", "pickle", "\n", "\n", "\n", "class", "VideoDataset", "(", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "list_root", ",", "\n", "transform", ",", "\n", "root_path", ",", "\n", "root_path_flow", "=", "None", ",", "root_path_mag", "=", "None", ",", "\n", "clip_length", "=", "1", ",", "num_steps", "=", "1", ",", "num_segments", "=", "1", ",", "num_channels", "=", "3", ",", "\n", "dataset", "=", "'ucf101'", ",", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset._load_list_csv": [[55, 88], ["open", "csv.DictReader", "list", "enumerate", "os.path.join", "int", "math.ceil", "samples.append", "os.path.exists", "os.path.exists", "os.path.join", "samples.append", "samples.append", "int"], "methods", ["None"], ["", "", "def", "_load_list_csv", "(", "self", ")", ":", "\n", "        ", "\"\"\" load annotation from csv \"\"\"", "\n", "samples", "=", "[", "]", "\n", "# csv format", "\n", "cols_name", "=", "[", "'duration_flow'", ",", "'duration_rgb'", ",", "'label'", ",", "'video_class'", ",", "'video_path'", ",", "'vname'", "]", "\n", "with", "open", "(", "self", ".", "list_root", ")", "as", "f", ":", "\n", "            ", "f_csv", "=", "csv", ".", "DictReader", "(", "f", ",", "fieldnames", "=", "cols_name", ",", "delimiter", "=", "' '", ")", "\n", "rows", "=", "list", "(", "f_csv", ")", "\n", "for", "idx", ",", "sample", "in", "enumerate", "(", "rows", ")", ":", "\n", "                ", "if", "idx", "==", "0", ":", "continue", "\n", "\n", "# filter out missing data especially for kinetics dataset", "\n", "sample", "[", "'video_path'", "]", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root_path", ",", "sample", "[", "'video_path'", "]", ")", "\n", "if", "self", ".", "with_motion", ":", "\n", "                    ", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "self", ".", "root_path_mag", ",", "sample", "[", "'video_path'", "]", ")", ")", ":", "continue", "\n", "", "else", ":", "\n", "                    ", "if", "not", "os", ".", "path", ".", "exists", "(", "sample", "[", "'video_path'", "]", ")", ":", "continue", "\n", "\n", "", "duration", "=", "int", "(", "sample", "[", "\"duration_rgb\"", "]", ")", "\n", "if", "self", ".", "split", "==", "'train'", ":", "\n", "# filter out short video for training", "\n", "                    ", "if", "self", ".", "dataset", "==", "'ucf101'", ":", "\n", "                        ", "if", "duration", ">=", "2", "*", "self", ".", "length_ext", ":", "\n", "                            ", "samples", ".", "append", "(", "sample", ")", "\n", "", "", "elif", "duration", ">", "self", ".", "length_ext", ":", "\n", "                        ", "samples", ".", "append", "(", "sample", ")", "\n", "", "", "else", ":", "\n", "# for short video, we duplicate it when test", "\n", "                    ", "dup", "=", "math", ".", "ceil", "(", "self", ".", "length_ext", "/", "int", "(", "sample", "[", "'duration_rgb'", "]", ")", ")", "\n", "sample", "[", "'dup'", "]", "=", "dup", "\n", "samples", ".", "append", "(", "sample", ")", "\n", "\n", "", "", "", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset._parse_rgb_lmdb": [[34, 48], ["lmdb.open", "lmdb.open.close", "os.path.join", "lmdb.open.begin", "range", "math.ceil", "io.BytesIO", "PIL.Image.open().convert", "image_list.append", "lmdb_txn.get", "PIL.Image.open"], "methods", ["None"], ["        ", "super", "(", "VideoDataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "transform", "\n", "self", ".", "root_path", "=", "root_path", "\n", "self", ".", "root_path_flow", "=", "root_path_flow", "\n", "self", ".", "root_path_mag", "=", "root_path_mag", "\n", "self", ".", "list_root", "=", "list_root", "\n", "self", ".", "clip_length", "=", "clip_length", "\n", "self", ".", "num_steps", "=", "num_steps", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "num_channels", "=", "num_channels", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "data_form", "=", "data_form", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "with_motion", "=", "with_motion", "\n", "self", ".", "length_ext", "=", "self", ".", "clip_length", "*", "self", ".", "num_steps", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset._parse_rgb_jpg": [[108, 123], ["enumerate", "range", "frame_list_all.append", "math.ceil", "os.path.join", "PIL.Image.open().convert", "frame_list.append", "PIL.Image.open"], "methods", ["None"], ["", "def", "_parse_rgb_jpg", "(", "self", ",", "video_path", ",", "offsets", ",", "frame_num", ",", "clip_length", ",", "num_steps", ",", "dup_time", "=", "1", ")", ":", "\n", "        ", "\"\"\"Return the clip buffer sample from video jpgs.\"\"\"", "\n", "frame_list_all", "=", "[", "]", "\n", "for", "idx", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "            ", "frame_list", "=", "[", "]", "\n", "for", "frame_id", "in", "range", "(", "offset", ",", "offset", "+", "num_steps", "*", "clip_length", ",", "num_steps", ")", ":", "\n", "                ", "if", "frame_id", ">", "frame_num", ":", "\n", "                    ", "frame_id", "=", "frame_num", "\n", "", "frame_id", "=", "math", ".", "ceil", "(", "frame_id", "/", "dup_time", ")", "\n", "frame_path", "=", "os", ".", "path", ".", "join", "(", "video_path", ",", "'image_{:05d}.jpg'", ".", "format", "(", "frame_id", ")", ")", "\n", "image", "=", "Image", ".", "open", "(", "frame_path", ")", ".", "convert", "(", "'RGB'", ")", "\n", "frame_list", ".", "append", "(", "image", ")", "\n", "", "frame_list_all", ".", "append", "(", "frame_list", ")", "\n", "\n", "", "return", "frame_list_all", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset._parse_flow_two_channel_lmdb": [[124, 142], ["lmdb.open", "flow_list_all.append", "lmdb.open.begin", "range", "io.BytesIO", "PIL.Image.open().convert", "PIL.Image.open().convert.split", "flows_u.append", "flows_v.append", "lmdb_txn.get", "PIL.Image.open"], "methods", ["None"], ["", "def", "_parse_flow_two_channel_lmdb", "(", "self", ",", "video_path", ",", "offsets", ",", "frame_num", ",", "clip_length", ",", "num_steps", ")", ":", "\n", "        ", "\"\"\" Return the clip buffer sample with seprate channel from video flow lmdb  \"\"\"", "\n", "flow_list_all", "=", "[", "]", "\n", "for", "offset", "in", "offsets", ":", "\n", "            ", "flows_u", "=", "[", "]", "\n", "flows_v", "=", "[", "]", "\n", "lmdb_env", "=", "lmdb", ".", "open", "(", "video_path", ",", "readonly", "=", "True", ",", "lock", "=", "False", ")", "\n", "with", "lmdb_env", ".", "begin", "(", ")", "as", "lmdb_txn", ":", "\n", "                ", "for", "frame_id", "in", "range", "(", "offset", ",", "offset", "+", "num_steps", "*", "clip_length", ",", "num_steps", ")", ":", "\n", "                    ", "if", "frame_id", ">", "frame_num", ":", "\n", "                        ", "frame_id", "=", "frame_num", "\n", "", "bio", "=", "io", ".", "BytesIO", "(", "lmdb_txn", ".", "get", "(", "'image_{:05d}.jpg'", ".", "format", "(", "frame_id", ")", ".", "encode", "(", ")", ")", ")", "\n", "frame_flow", "=", "Image", ".", "open", "(", "bio", ")", ".", "convert", "(", "'RGB'", ")", "\n", "_", ",", "v", ",", "u", "=", "frame_flow", ".", "split", "(", ")", "\n", "flows_u", ".", "append", "(", "u", ")", "\n", "flows_v", ".", "append", "(", "v", ")", "\n", "", "", "flow_list_all", ".", "append", "(", "[", "flows_u", ",", "flows_v", "]", ")", "\n", "", "return", "flow_list_all", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset.__len__": [[49, 51], ["len"], "methods", ["None"], ["self", ".", "samples", "=", "self", ".", "_load_list_csv", "(", ")", "\n", "\n", "# for motion-focus temporal sampling", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset.__getitem__": [[52, 54], ["None"], "methods", ["None"], ["with", "open", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "self", ".", "root_path", ")", ",", "'video_clip_mag_{}.pickle'", ".", "format", "(", "self", ".", "dataset", ")", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "self", ".", "video_clip_mags", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTrainDataset.temporal_sampling": [[152, 157], ["range", "offsets.append", "random.randint"], "methods", ["None"], ["    ", "def", "temporal_sampling", "(", "self", ",", "duration", ",", "length_ext", ",", "num_segment", "=", "1", ")", ":", "\n", "        ", "offsets", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_segment", ")", ":", "\n", "            ", "offsets", ".", "append", "(", "random", ".", "randint", "(", "1", ",", "duration", "-", "length_ext", ")", ")", "\n", "", "return", "offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTrainDataset.temporal_sampling_triplet": [[158, 177], ["sorted.append", "sorted.extend", "sorted", "sorted.append", "sorted.extend", "sorted", "random.randint", "sorted", "video_dataset.VideoTrainDataset.temporal_sampling", "random.randint", "sorted", "video_dataset.VideoTrainDataset.temporal_sampling", "random.randint", "random.randint", "random.randint", "random.randint"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTrainDataset.temporal_sampling", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTrainDataset.temporal_sampling"], ["", "def", "temporal_sampling_triplet", "(", "self", ",", "order", ",", "duration", ",", "length_ext", ")", ":", "\n", "        ", "offsets", "=", "[", "]", "\n", "if", "order", ":", "\n", "            ", "if", "duration", "//", "length_ext", ">=", "2", ":", "\n", "                ", "offsets", ".", "append", "(", "random", ".", "randint", "(", "1", ",", "duration", "//", "2", "-", "length_ext", "+", "1", ")", ")", "\n", "offsets", ".", "extend", "(", "sorted", "(", "[", "random", ".", "randint", "(", "duration", "//", "2", ",", "duration", "-", "length_ext", ")", ",", "\n", "random", ".", "randint", "(", "duration", "//", "2", ",", "duration", "-", "length_ext", ")", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "offsets", "=", "sorted", "(", "self", ".", "temporal_sampling", "(", "duration", ",", "length_ext", ",", "3", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "duration", "//", "length_ext", ">=", "2", ":", "\n", "                ", "offsets", ".", "append", "(", "random", ".", "randint", "(", "duration", "//", "2", ",", "duration", "-", "length_ext", ")", ")", "\n", "offsets", ".", "extend", "(", "sorted", "(", "[", "random", ".", "randint", "(", "1", ",", "duration", "//", "2", "-", "length_ext", "+", "1", ")", ",", "\n", "random", ".", "randint", "(", "1", ",", "duration", "//", "2", "-", "length_ext", "+", "1", ")", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "offsets", "=", "sorted", "(", "self", ".", "temporal_sampling", "(", "duration", ",", "length_ext", ",", "3", ")", ",", "reverse", "=", "True", ")", "\n", "offsets", "[", "2", "]", ",", "offsets", "[", "1", "]", "=", "offsets", "[", "1", "]", ",", "offsets", "[", "2", "]", "\n", "\n", "", "", "return", "offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTrainDataset.motion_focus_temporal_sampling": [[178, 212], ["numpy.array", "list", "list", "list", "list", "video_path.split", "range", "range", "int", "sorted.append", "sorted.extend", "sorted", "sorted.append", "sorted.extend", "sorted", "len", "len", "random.choice", "sorted", "video_dataset.VideoTrainDataset.temporal_sampling", "random.choice", "sorted", "video_dataset.VideoTrainDataset.temporal_sampling", "numpy.argsort", "len", "random.choice", "random.choice", "random.choice", "random.choice"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTrainDataset.temporal_sampling", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTrainDataset.temporal_sampling"], ["", "def", "motion_focus_temporal_sampling", "(", "self", ",", "order", ",", "duration", ",", "length_ext", ",", "video_path", "=", "None", ",", "threshold", "=", "0.5", ")", ":", "\n", "        ", "video_name", "=", "'/'", ".", "join", "(", "video_path", ".", "split", "(", "'/'", ")", "[", "-", "2", ":", "]", ")", "\n", "clip_mags", "=", "np", ".", "array", "(", "self", ".", "video_clip_mags", "[", "video_name", "]", ")", "\n", "max_clip_idxs", "=", "(", "np", ".", "argsort", "(", "clip_mags", ")", "[", ":", ":", "-", "1", "]", "+", "1", ")", ".", "tolist", "(", ")", "\n", "\n", "if", "duration", "//", "length_ext", ">=", "2", ":", "\n", "            ", "idxs_1", "=", "list", "(", "range", "(", "1", ",", "duration", "//", "2", "-", "length_ext", "+", "1", ")", ")", "\n", "idxs_2", "=", "list", "(", "range", "(", "duration", "//", "2", ",", "duration", "-", "length_ext", ")", ")", "\n", "max_idxs_1", "=", "list", "(", "[", "i", "for", "i", "in", "max_clip_idxs", "if", "i", "in", "idxs_1", "]", ")", "\n", "max_idxs_2", "=", "list", "(", "[", "i", "for", "i", "in", "max_clip_idxs", "if", "i", "in", "idxs_2", "]", ")", "\n", "\n", "num_max", "=", "int", "(", "len", "(", "idxs_1", ")", "*", "threshold", ")", "+", "1", "\n", "max_idxs_1", "=", "max_idxs_1", "[", ":", "num_max", "]", "\n", "max_idxs_2", "=", "max_idxs_2", "[", ":", "num_max", "]", "\n", "if", "len", "(", "max_idxs_1", ")", "==", "0", "or", "len", "(", "max_idxs_2", ")", "==", "0", ":", "\n", "                ", "max_idxs_1", "=", "[", "1", "]", "\n", "max_idxs_2", "=", "[", "length_ext", "]", "\n", "\n", "", "", "offsets", "=", "[", "]", "\n", "if", "order", ":", "\n", "            ", "if", "duration", "//", "length_ext", ">=", "2", ":", "\n", "                ", "offsets", ".", "append", "(", "random", ".", "choice", "(", "max_idxs_1", ")", ")", "\n", "offsets", ".", "extend", "(", "sorted", "(", "[", "random", ".", "choice", "(", "max_idxs_2", ")", ",", "random", ".", "choice", "(", "max_idxs_2", ")", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "offsets", "=", "sorted", "(", "self", ".", "temporal_sampling", "(", "duration", ",", "length_ext", ",", "3", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "duration", "//", "length_ext", ">=", "2", ":", "\n", "                ", "offsets", ".", "append", "(", "random", ".", "choice", "(", "max_idxs_2", ")", ")", "\n", "offsets", ".", "extend", "(", "sorted", "(", "[", "random", ".", "choice", "(", "max_idxs_1", ")", ",", "random", ".", "choice", "(", "max_idxs_1", ")", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "offsets", "=", "sorted", "(", "self", ".", "temporal_sampling", "(", "duration", ",", "length_ext", ",", "3", ")", ",", "reverse", "=", "True", ")", "\n", "offsets", "[", "2", "]", ",", "offsets", "[", "1", "]", "=", "offsets", "[", "1", "]", ",", "offsets", "[", "2", "]", "\n", "\n", "", "", "return", "offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTrainDataset._parse_sample_str": [[57, 83], ["sample.split", "int", "int", "math.ceil", "range", "range", "offsets.append", "float", "float", "offsets.append", "len", "random.randint", "random.randint", "int", "int", "len"], "methods", ["None"], ["samples", "=", "[", "]", "\n", "# csv format", "\n", "cols_name", "=", "[", "'duration_flow'", ",", "'duration_rgb'", ",", "'label'", ",", "'video_class'", ",", "'video_path'", ",", "'vname'", "]", "\n", "with", "open", "(", "self", ".", "list_root", ")", "as", "f", ":", "\n", "            ", "f_csv", "=", "csv", ".", "DictReader", "(", "f", ",", "fieldnames", "=", "cols_name", ",", "delimiter", "=", "' '", ")", "\n", "rows", "=", "list", "(", "f_csv", ")", "\n", "for", "idx", ",", "sample", "in", "enumerate", "(", "rows", ")", ":", "\n", "                ", "if", "idx", "==", "0", ":", "continue", "\n", "\n", "# filter out missing data especially for kinetics dataset", "\n", "sample", "[", "'video_path'", "]", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root_path", ",", "sample", "[", "'video_path'", "]", ")", "\n", "if", "self", ".", "with_motion", ":", "\n", "                    ", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "self", ".", "root_path_mag", ",", "sample", "[", "'video_path'", "]", ")", ")", ":", "continue", "\n", "", "else", ":", "\n", "                    ", "if", "not", "os", ".", "path", ".", "exists", "(", "sample", "[", "'video_path'", "]", ")", ":", "continue", "\n", "\n", "", "duration", "=", "int", "(", "sample", "[", "\"duration_rgb\"", "]", ")", "\n", "if", "self", ".", "split", "==", "'train'", ":", "\n", "# filter out short video for training", "\n", "                    ", "if", "self", ".", "dataset", "==", "'ucf101'", ":", "\n", "                        ", "if", "duration", ">=", "2", "*", "self", ".", "length_ext", ":", "\n", "                            ", "samples", ".", "append", "(", "sample", ")", "\n", "", "", "elif", "duration", ">", "self", ".", "length_ext", ":", "\n", "                        ", "samples", ".", "append", "(", "sample", ")", "\n", "", "", "else", ":", "\n", "# for short video, we duplicate it when test", "\n", "                    ", "dup", "=", "math", ".", "ceil", "(", "self", ".", "length_ext", "/", "int", "(", "sample", "[", "'duration_rgb'", "]", ")", ")", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTestDataset.__init__": [[86, 91], ["video_dataset.VideoDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__"], ["\n", "", "", "", "return", "samples", "\n", "\n", "\n", "", "def", "_parse_rgb_lmdb", "(", "self", ",", "video_path", ",", "offsets", ",", "frame_num", ",", "clip_length", ",", "num_steps", ",", "dup_time", "=", "1", ")", ":", "\n", "        ", "\"\"\"Return the clip buffer sample from video lmdb.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTestDataset.__len__": [[92, 94], ["len"], "methods", ["None"], ["lmdb_env", "=", "lmdb", ".", "open", "(", "video_path", ",", "readonly", "=", "True", ",", "lock", "=", "False", ")", "\n", "with", "lmdb_env", ".", "begin", "(", ")", "as", "lmdb_txn", ":", "\n", "            ", "frame_list_all", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTestDataset._parse_sample_str": [[95, 125], ["sample.split", "int", "int", "math.ceil", "range", "offsets.append", "range", "float", "float", "offsets.append", "len", "int", "int", "int", "len", "float"], "methods", ["None"], ["for", "idx", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "                ", "frame_list", "=", "[", "]", "\n", "for", "frame_id", "in", "range", "(", "offset", ",", "offset", "+", "num_steps", "*", "clip_length", ",", "num_steps", ")", ":", "\n", "                    ", "if", "frame_id", ">", "frame_num", ":", "\n", "                        ", "frame_id", "=", "frame_num", "\n", "", "frame_id", "=", "math", ".", "ceil", "(", "frame_id", "/", "dup_time", ")", "\n", "bio", "=", "io", ".", "BytesIO", "(", "lmdb_txn", ".", "get", "(", "'image_{:05d}.jpg'", ".", "format", "(", "frame_id", ")", ".", "encode", "(", ")", ")", ")", "\n", "image", "=", "Image", ".", "open", "(", "bio", ")", ".", "convert", "(", "'RGB'", ")", "\n", "frame_list", ".", "append", "(", "image", ")", "\n", "", "frame_list_all", ".", "append", "(", "frame_list", ")", "\n", "", "", "lmdb_env", ".", "close", "(", ")", "\n", "return", "frame_list_all", "\n", "\n", "", "def", "_parse_rgb_jpg", "(", "self", ",", "video_path", ",", "offsets", ",", "frame_num", ",", "clip_length", ",", "num_steps", ",", "dup_time", "=", "1", ")", ":", "\n", "        ", "\"\"\"Return the clip buffer sample from video jpgs.\"\"\"", "\n", "frame_list_all", "=", "[", "]", "\n", "for", "idx", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "            ", "frame_list", "=", "[", "]", "\n", "for", "frame_id", "in", "range", "(", "offset", ",", "offset", "+", "num_steps", "*", "clip_length", ",", "num_steps", ")", ":", "\n", "                ", "if", "frame_id", ">", "frame_num", ":", "\n", "                    ", "frame_id", "=", "frame_num", "\n", "", "frame_id", "=", "math", ".", "ceil", "(", "frame_id", "/", "dup_time", ")", "\n", "frame_path", "=", "os", ".", "path", ".", "join", "(", "video_path", ",", "'image_{:05d}.jpg'", ".", "format", "(", "frame_id", ")", ")", "\n", "image", "=", "Image", ".", "open", "(", "frame_path", ")", ".", "convert", "(", "'RGB'", ")", "\n", "frame_list", ".", "append", "(", "image", ")", "\n", "", "frame_list_all", ".", "append", "(", "frame_list", ")", "\n", "\n", "", "return", "frame_list_all", "\n", "\n", "", "def", "_parse_flow_two_channel_lmdb", "(", "self", ",", "video_path", ",", "offsets", ",", "frame_num", ",", "clip_length", ",", "num_steps", ")", ":", "\n", "        ", "\"\"\" Return the clip buffer sample with seprate channel from video flow lmdb  \"\"\"", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoRGBTrainDataset.__getitem__": [[128, 136], ["video_dataset.VideoRGBTrainDataset._parse_sample_str", "video_dataset.VideoRGBTrainDataset._parse_rgb_lmdb", "video_dataset.VideoRGBTrainDataset.transform"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTestDataset._parse_sample_str", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset._parse_rgb_lmdb"], ["            ", "flows_u", "=", "[", "]", "\n", "flows_v", "=", "[", "]", "\n", "lmdb_env", "=", "lmdb", ".", "open", "(", "video_path", ",", "readonly", "=", "True", ",", "lock", "=", "False", ")", "\n", "with", "lmdb_env", ".", "begin", "(", ")", "as", "lmdb_txn", ":", "\n", "                ", "for", "frame_id", "in", "range", "(", "offset", ",", "offset", "+", "num_steps", "*", "clip_length", ",", "num_steps", ")", ":", "\n", "                    ", "if", "frame_id", ">", "frame_num", ":", "\n", "                        ", "frame_id", "=", "frame_num", "\n", "", "bio", "=", "io", ".", "BytesIO", "(", "lmdb_txn", ".", "get", "(", "'image_{:05d}.jpg'", ".", "format", "(", "frame_id", ")", ".", "encode", "(", ")", ")", ")", "\n", "frame_flow", "=", "Image", ".", "open", "(", "bio", ")", ".", "convert", "(", "'RGB'", ")", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoRGBTestDataset.__getitem__": [[139, 149], ["video_dataset.VideoRGBTestDataset._parse_sample_str", "video_dataset.VideoRGBTestDataset._parse_rgb_lmdb", "video_dataset.VideoRGBTestDataset.transform"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTestDataset._parse_sample_str", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset._parse_rgb_lmdb"], ["flows_v", ".", "append", "(", "v", ")", "\n", "", "", "flow_list_all", ".", "append", "(", "[", "flows_u", ",", "flows_v", "]", ")", "\n", "", "return", "flow_list_all", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "samples", ")", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoRGBTrainDataset_Motion.__init__": [[322, 332], ["video_dataset.VideoDataset.__init__", "dataset.augmentations.clip_transforms.ClipRandomHorizontalFlip", "dataset.augmentations.clip_transforms.Compose", "dataset.augmentations.clip_transforms.ToClipTensor", "dataset.augmentations.clip_transforms.Lambda", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__"], ["    ", "def", "__init__", "(", "self", ",", "motion_focus_spatial_crop", ",", "input_size", ",", "mag_size", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "VideoRGBTrainDataset_Motion", ",", "self", ")", ".", "__init__", "(", "with_motion", "=", "True", ",", "**", "kwargs", ")", "\n", "self", ".", "motion_focus_spatial_crop", "=", "motion_focus_spatial_crop", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "mag_size", "=", "mag_size", "\n", "\n", "self", ".", "flip_trans", "=", "clip_transforms", ".", "ClipRandomHorizontalFlip", "(", ")", "\n", "self", ".", "mag_trans", "=", "clip_transforms", ".", "Compose", "(", "[", "\n", "clip_transforms", ".", "ToClipTensor", "(", ")", ",", "\n", "clip_transforms", ".", "Lambda", "(", "lambda", "clip", ":", "torch", ".", "stack", "(", "clip", ",", "dim", "=", "1", ")", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoRGBTrainDataset_Motion.__getitem__": [[334, 372], ["video_dataset.VideoRGBTrainDataset_Motion._parse_sample_str", "os.path.join", "os.path.join", "enumerate", "enumerate", "video_dataset.VideoRGBTrainDataset_Motion.get_st_motion_mag", "random.randint", "video_dataset.VideoRGBTrainDataset_Motion._parse_rgb_lmdb", "mags.append", "video_dataset.VideoRGBTrainDataset_Motion.mag_trans", "video_dataset.VideoRGBTrainDataset_Motion.squeeze", "video_dataset.VideoRGBTrainDataset_Motion._parse_rgb_jpg", "flows.append", "video_dataset.VideoRGBTrainDataset_Motion.motion_focus_spatial_trans", "video_dataset.VideoRGBTrainDataset_Motion.motion_focus_spatial_trans", "video_dataset.VideoRGBTrainDataset_Motion.motion_focus_spatial_trans", "k_list.append", "video_path.split", "video_path.split", "video_dataset.VideoRGBTrainDataset_Motion._parse_rgb_lmdb", "video_dataset.VideoRGBTrainDataset_Motion._parse_flow_two_channel_lmdb"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoTestDataset._parse_sample_str", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoRGBTrainDataset_Motion.get_st_motion_mag", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset._parse_rgb_lmdb", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset._parse_rgb_jpg", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoRGBTrainDataset_Motion.motion_focus_spatial_trans", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoRGBTrainDataset_Motion.motion_focus_spatial_trans", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoRGBTrainDataset_Motion.motion_focus_spatial_trans", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset._parse_rgb_lmdb", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset._parse_flow_two_channel_lmdb"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "if", "random", ".", "randint", "(", "0", ",", "1", ")", "==", "0", ":", "\n", "            ", "order_cls", "=", "1", "\n", "", "else", ":", "\n", "            ", "order_cls", "=", "0", "\n", "\n", "", "video_path", ",", "offsets", ",", "label", ",", "org_duration", ",", "vname", "=", "self", ".", "_parse_sample_str", "(", "self", ".", "samples", "[", "item", "]", ",", "order_cls", ",", "item", ")", "\n", "if", "self", ".", "data_form", "==", "'lmdb'", ":", "\n", "            ", "frame_list_all", "=", "self", ".", "_parse_rgb_lmdb", "(", "video_path", ",", "offsets", ",", "org_duration", ",", "self", ".", "clip_length", ",", "self", ".", "num_steps", ")", "\n", "", "elif", "self", ".", "data_form", "==", "'rgb'", ":", "\n", "            ", "frame_list_all", "=", "self", ".", "_parse_rgb_jpg", "(", "video_path", ",", "offsets", ",", "org_duration", ",", "self", ".", "clip_length", ",", "self", ".", "num_steps", ")", "\n", "\n", "", "mags", "=", "[", "]", "\n", "flows", "=", "[", "]", "\n", "path_flow_mag", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root_path_mag", ",", "'/'", ".", "join", "(", "video_path", ".", "split", "(", "'/'", ")", "[", "-", "2", ":", "]", ")", ")", "\n", "path_flow", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root_path_flow", ",", "'/'", ".", "join", "(", "video_path", ".", "split", "(", "'/'", ")", "[", "-", "2", ":", "]", ")", ")", "\n", "\n", "for", "idx", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "            ", "mags", ".", "append", "(", "self", ".", "_parse_rgb_lmdb", "(", "path_flow_mag", ",", "[", "offset", "]", ",", "org_duration", ",", "self", ".", "clip_length", ",", "self", ".", "num_steps", ")", "[", "0", "]", ")", "\n", "if", "idx", "==", "0", ":", "\n", "                ", "flows", ".", "append", "(", "self", ".", "_parse_flow_two_channel_lmdb", "(", "path_flow", ",", "[", "offset", "]", ",", "org_duration", ",", "self", ".", "clip_length", ",", "self", ".", "num_steps", ")", "[", "0", "]", ")", "\n", "\n", "", "", "k_list", "=", "[", "]", "\n", "for", "idx", ",", "frame_list", "in", "enumerate", "(", "frame_list_all", ")", ":", "\n", "            ", "if", "idx", "==", "0", ":", "\n", "                ", "q_aug1", ",", "flows_q1_u", ",", "flows_q1_v", "=", "self", ".", "motion_focus_spatial_trans", "(", "frame_list", ",", "mags", "[", "idx", "]", ",", "flows", "[", "idx", "]", ")", "\n", "q_aug2", ",", "_", ",", "_", "=", "self", ".", "motion_focus_spatial_trans", "(", "frame_list", ",", "mags", "[", "idx", "]", ")", "\n", "\n", "", "else", ":", "\n", "                ", "aug", ",", "_", ",", "_", ",", "=", "self", ".", "motion_focus_spatial_trans", "(", "frame_list", ",", "mags", "[", "idx", "]", ")", "\n", "k_list", ".", "append", "(", "aug", ")", "\n", "\n", "# calc ST-motoin maps base on croppped flows", "\n", "", "", "st_motion_mags", "=", "self", ".", "get_st_motion_mag", "(", "flows_q1_u", ",", "flows_q1_v", ")", "\n", "st_motion_mags", "=", "self", ".", "mag_trans", "(", "st_motion_mags", ")", "[", "0", "]", "\n", "\n", "return", "q_aug1", ",", "st_motion_mags", ".", "squeeze", "(", "0", ")", ",", "q_aug2", ",", "k_list", "[", "0", "]", ",", "k_list", "[", "1", "]", ",", "order_cls", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoRGBTrainDataset_Motion.motion_focus_spatial_trans": [[374, 382], ["video_dataset.VideoRGBTrainDataset_Motion.motion_focus_spatial_crop", "video_dataset.VideoRGBTrainDataset_Motion.transform", "video_dataset.VideoRGBTrainDataset_Motion.flip_trans", "video_dataset.VideoRGBTrainDataset_Motion.flip_trans"], "methods", ["None"], ["", "def", "motion_focus_spatial_trans", "(", "self", ",", "image_list", ",", "mag", ",", "flows", "=", "None", ")", ":", "\n", "\n", "        ", "motion_aug", ",", "flows_u", ",", "flows_v", "=", "self", ".", "motion_focus_spatial_crop", "(", "image_list", ",", "mag", ",", "flows", "=", "flows", ")", "\n", "aug", ",", "is_flip", "=", "self", ".", "transform", "(", "motion_aug", ")", "\n", "if", "flows", "!=", "None", ":", "\n", "            ", "flows_u", ",", "_", "=", "self", ".", "flip_trans", "(", "flows_u", ",", "is_flip", ")", "\n", "flows_v", ",", "_", "=", "self", ".", "flip_trans", "(", "flows_v", ",", "is_flip", ")", "\n", "", "return", "aug", ",", "flows_u", ",", "flows_v", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoRGBTrainDataset_Motion.get_st_motion_mag": [[383, 403], ["numpy.stack().astype", "numpy.stack().astype", "range", "list", "enumerate", "map", "numpy.stack", "numpy.stack", "range", "range", "motion_sts", "st_mags[].append", "sum().astype", "numpy.array", "numpy.array", "sum"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.motion_sts"], ["", "def", "get_st_motion_mag", "(", "self", ",", "flows_u", ",", "flows_v", ")", ":", "\n", "\n", "        ", "flows_u", "=", "np", ".", "stack", "(", "[", "np", ".", "array", "(", "flow", ")", "for", "flow", "in", "flows_u", "]", ",", "0", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "flows_v", "=", "np", ".", "stack", "(", "[", "np", ".", "array", "(", "flow", ")", "for", "flow", "in", "flows_v", "]", ",", "0", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "flows", "=", "[", "flows_u", ",", "flows_v", "]", "\n", "\n", "clip_per_frames", "=", "flows_v", ".", "shape", "[", "0", "]", "//", "4", "\n", "st_mags", "=", "[", "[", "]", "for", "i", "in", "range", "(", "4", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "            ", "flows_", "=", "flows", "[", "i", "]", "\n", "for", "idx", ",", "j", "in", "enumerate", "(", "range", "(", "0", ",", "clip_per_frames", "*", "4", ",", "clip_per_frames", ")", ")", ":", "\n", "                ", "flow_clip", "=", "flows_", "[", "j", ":", "j", "+", "clip_per_frames", "]", "\n", "clip_mag", "=", "motion_sts", "(", "flow_clip", ",", "self", ".", "mag_size", ",", "self", ".", "input_size", ")", "\n", "st_mags", "[", "idx", "]", ".", "append", "(", "clip_mag", ")", "\n", "\n", "", "", "st_mags", "=", "list", "(", "map", "(", "lambda", "x", ":", "sum", "(", "x", ")", ".", "astype", "(", "np", ".", "float32", ")", ",", "st_mags", ")", ")", "\n", "\n", "return", "st_mags", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.dataset.video_dataset.VideoDataset._load_list": [[29, 33], ["open", "f.readlines"], "methods", ["None"], ["data_form", "=", "'lmdb'", ",", "\n", "split", "=", "'train'", ",", "\n", "with_motion", "=", "False", "\n", ")", ":", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.Compose.__call__": [[38, 45], ["isinstance", "t", "t"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "input", ")", ":", "\n", "        ", "for", "t", "in", "self", ".", "transforms", ":", "\n", "            ", "if", "isinstance", "(", "input", ",", "tuple", ")", ":", "\n", "                ", "input", "=", "t", "(", "*", "input", ")", "\n", "", "else", ":", "\n", "                ", "input", "=", "t", "(", "input", ")", "\n", "", "", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ToClipTensor.__call__": [[58, 68], ["torchvision.to_tensor"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "clip", ",", "is_flip", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of PIL Image or numpy.ndarray): Clip to be converted to tensor.\n\n        Returns:\n            Tensor: Converted clip.\n        \"\"\"", "\n", "\n", "return", "(", "[", "F", ".", "to_tensor", "(", "img", ")", "for", "img", "in", "clip", "]", ",", "is_flip", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ToClipTensor.__repr__": [[69, 71], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'()'", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipRandomResizedCrop.__call__": [[74, 84], ["clip_transforms.ClipRandomResizedCrop.get_params", "torchvision.resized_crop"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipRandomResizedCropMotion.get_params"], ["    ", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of PIL Image or Tensor): Clip to be cropped and resized.\n\n        Returns:\n            List of PIL Image or Tensor: Randomly cropped and resized clip.\n        \"\"\"", "\n", "i", ",", "j", ",", "h", ",", "w", "=", "self", ".", "get_params", "(", "clip", "[", "0", "]", ",", "self", ".", "scale", ",", "self", ".", "ratio", ")", "\n", "return", "[", "F", ".", "resized_crop", "(", "img", ",", "i", ",", "j", ",", "h", ",", "w", ",", "self", ".", "size", ",", "self", ".", "interpolation", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipRandomResizedCropMotion.__init__": [[90, 102], ["isinstance", "warnings.warn"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "size", ",", "scale", "=", "(", "0.08", ",", "1.0", ")", ",", "ratio", "=", "(", "3.", "/", "4.", ",", "4.", "/", "3.", ")", ",", "iou", "=", "0.9", ",", "interpolation", "=", "Image", ".", "BILINEAR", ")", ":", "\n", "        ", "if", "isinstance", "(", "size", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "self", ".", "size", "=", "size", "\n", "", "else", ":", "\n", "            ", "self", ".", "size", "=", "(", "size", ",", "size", ")", "\n", "", "if", "(", "scale", "[", "0", "]", ">", "scale", "[", "1", "]", ")", "or", "(", "ratio", "[", "0", "]", ">", "ratio", "[", "1", "]", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"range should be of kind (min, max)\"", ")", "\n", "\n", "", "self", ".", "interpolation", "=", "interpolation", "\n", "self", ".", "scale", "=", "scale", "\n", "self", ".", "ratio", "=", "ratio", "\n", "self", ".", "iou", "=", "iou", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipRandomResizedCropMotion.get_params": [[103, 183], ["dataset.augmentations.augmentation_util._get_image_size", "list", "dataset.augmentations.augmentation_util.get_cor", "range", "map", "random.uniform", "math.exp", "int", "int", "dataset.augmentations.augmentation_util.crop_from_corners", "float", "float", "min", "int", "math.log", "math.log", "random.uniform", "round", "round", "round", "max", "int", "numpy.array().astype", "math.sqrt", "math.sqrt", "random.randint", "random.randint", "range", "numpy.array", "round", "random.randint", "random.randint", "dataset.augmentations.augmentation_util.calc_over_lab", "dataset.augmentations.augmentation_util.calc_over_lab", "crop_list.append", "ratio_out_list.append", "numpy.argsort", "min", "numpy.array", "max"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.augmentation_util._get_image_size", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.augmentation_util.get_cor", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.augmentation_util.crop_from_corners", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.augmentation_util.calc_over_lab", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.augmentation_util.calc_over_lab"], ["", "@", "staticmethod", "\n", "def", "get_params", "(", "img", ",", "scale", ",", "ratio", ",", "mags", ")", ":", "\n", "        ", "\"\"\"Get parameters for ``crop`` for a random sized crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        \"\"\"", "\n", "width", ",", "height", "=", "_get_image_size", "(", "img", ")", "\n", "area", "=", "height", "*", "width", "\n", "\n", "mags", "=", "list", "(", "map", "(", "lambda", "x", ":", "np", ".", "array", "(", "x", ")", ".", "astype", "(", "np", ".", "float32", ")", ",", "mags", ")", ")", "\n", "row_s", ",", "col_s", ",", "crop_ratio", "=", "get_cor", "(", "mags", ",", "height", ",", "width", ")", "\n", "\n", "for", "_", "in", "range", "(", "20", ")", ":", "\n", "            ", "sc", "=", "random", ".", "uniform", "(", "*", "scale", ")", "\n", "target_area", "=", "sc", "*", "area", "\n", "log_ratio", "=", "(", "math", ".", "log", "(", "ratio", "[", "0", "]", ")", ",", "math", ".", "log", "(", "ratio", "[", "1", "]", ")", ")", "\n", "aspect_ratio", "=", "math", ".", "exp", "(", "random", ".", "uniform", "(", "*", "log_ratio", ")", ")", "\n", "\n", "w", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "*", "aspect_ratio", ")", ")", ")", "\n", "h", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "/", "aspect_ratio", ")", ")", ")", "\n", "\n", "if", "0", "<", "w", "<=", "width", "and", "0", "<", "h", "<=", "height", ":", "\n", "                ", "if", "mags", "==", "[", "]", "or", "crop_ratio", "==", "0", ":", "\n", "                    ", "i", "=", "random", ".", "randint", "(", "0", ",", "height", "-", "h", ")", "\n", "j", "=", "random", ".", "randint", "(", "0", ",", "width", "-", "w", ")", "\n", "return", "i", ",", "j", ",", "h", ",", "w", "\n", "", "else", ":", "\n", "                    ", "crop_list", "=", "[", "]", "\n", "ratio_out_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "40", ")", ":", "\n", "                        ", "i", "=", "random", ".", "randint", "(", "0", ",", "height", "-", "h", ")", "\n", "j", "=", "random", ".", "randint", "(", "0", ",", "width", "-", "w", ")", "\n", "\n", "row_", "=", "(", "i", ",", "i", "+", "h", ")", "\n", "col_", "=", "(", "j", ",", "j", "+", "w", ")", "\n", "\n", "ratio_row", "=", "calc_over_lab", "(", "row_s", ",", "row_", ")", "\n", "ratio_col", "=", "calc_over_lab", "(", "col_s", ",", "col_", ")", "\n", "ratio_out", "=", "ratio_col", "*", "ratio_row", "\n", "\n", "crop_list", ".", "append", "(", "(", "i", ",", "j", ",", "h", ",", "w", ")", ")", "\n", "ratio_out_list", ".", "append", "(", "ratio_out", ")", "\n", "\n", "if", "ratio_out", ">=", "crop_ratio", ":", "\n", "                            ", "return", "i", ",", "j", ",", "h", ",", "w", "\n", "", "else", ":", "\n", "                            ", "continue", "\n", "\n", "# max ratio_out", "\n", "", "", "ratio_outs", "=", "np", ".", "array", "(", "ratio_out_list", ")", "\n", "idx", "=", "np", ".", "argsort", "(", "ratio_outs", ")", "[", "-", "1", "]", "\n", "i", ",", "j", ",", "h", ",", "w", "=", "crop_list", "[", "idx", "]", "\n", "\n", "return", "i", ",", "j", ",", "h", ",", "w", "\n", "\n", "# crop from corners", "\n", "", "", "", "if", "crop_ratio", "!=", "0", ":", "\n", "            ", "return", "crop_from_corners", "(", "row_s", ",", "col_s", ",", "height", ",", "width", ")", "\n", "\n", "# Fallback to central crop", "\n", "", "in_ratio", "=", "float", "(", "width", ")", "/", "float", "(", "height", ")", "\n", "if", "(", "in_ratio", "<", "min", "(", "ratio", ")", ")", ":", "\n", "            ", "w", "=", "width", "\n", "h", "=", "int", "(", "round", "(", "w", "/", "min", "(", "ratio", ")", ")", ")", "\n", "", "elif", "(", "in_ratio", ">", "max", "(", "ratio", ")", ")", ":", "\n", "            ", "h", "=", "height", "\n", "w", "=", "int", "(", "round", "(", "h", "*", "max", "(", "ratio", ")", ")", ")", "\n", "", "else", ":", "# whole image", "\n", "            ", "w", "=", "width", "\n", "h", "=", "height", "\n", "", "i", "=", "(", "height", "-", "h", ")", "//", "2", "\n", "j", "=", "(", "width", "-", "w", ")", "//", "2", "\n", "return", "i", ",", "j", ",", "h", ",", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipRandomResizedCropMotion.__call__": [[184, 203], ["clip_transforms.ClipRandomResizedCropMotion.get_params", "torchvision.resized_crop", "torchvision.resized_crop", "torchvision.resized_crop", "torchvision.resized_crop"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipRandomResizedCropMotion.get_params"], ["", "def", "__call__", "(", "self", ",", "clip", ",", "mags", ",", "flows", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of PIL Image or Tensor): Input clip.\n            flows (List of PIL Image or Tensor): Input clip's optical flow.\n            mags (List of PIL Image or Tensor): Input clip's motion magnitude.\n            return_flow : if crop and resize flows.\n        Returns:\n            List of PIL Image or Tensor: croped and resized clip and flows.\n        \"\"\"", "\n", "i", ",", "j", ",", "h", ",", "w", "=", "self", ".", "get_params", "(", "clip", "[", "0", "]", ",", "self", ".", "scale", ",", "self", ".", "ratio", ",", "mags", ")", "\n", "if", "flows", "==", "None", ":", "\n", "            ", "return", "[", "F", ".", "resized_crop", "(", "img", ",", "i", ",", "j", ",", "h", ",", "w", ",", "self", ".", "size", ",", "self", ".", "interpolation", ")", "for", "img", "in", "clip", "]", ",", "None", ",", "None", "\n", "", "else", ":", "\n", "            ", "return", "[", "F", ".", "resized_crop", "(", "img", ",", "i", ",", "j", ",", "h", ",", "w", ",", "self", ".", "size", ",", "self", ".", "interpolation", ")", "for", "img", "in", "clip", "]", ",", "[", "F", ".", "resized_crop", "(", "flow", ",", "i", ",", "j", ",", "h", ",", "w", ",", "self", ".", "size", ",", "self", ".", "interpolation", ")", "for", "flow", "in", "flows", "[", "0", "]", "]", ",", "[", "F", ".", "resized_crop", "(", "flow", ",", "i", ",", "j", ",", "h", ",", "w", ",", "self", ".", "size", ",", "self", ".", "interpolation", ")", "for", "flow", "in", "flows", "[", "1", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipRandomResizedCropMotion.__repr__": [[205, 212], ["tuple", "tuple", "round", "round"], "methods", ["None"], ["", "", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "interpolate_str", "=", "_pil_interpolation_to_str", "[", "self", ".", "interpolation", "]", "\n", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'(size={0}'", ".", "format", "(", "self", ".", "size", ")", "\n", "format_string", "+=", "', scale={0}'", ".", "format", "(", "tuple", "(", "round", "(", "s", ",", "4", ")", "for", "s", "in", "self", ".", "scale", ")", ")", "\n", "format_string", "+=", "', ratio={0}'", ".", "format", "(", "tuple", "(", "round", "(", "r", ",", "4", ")", "for", "r", "in", "self", ".", "ratio", ")", ")", "\n", "format_string", "+=", "', interpolation={0})'", ".", "format", "(", "interpolate_str", ")", "\n", "return", "format_string", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipColorJitter.__call__": [[216, 247], ["torch.randperm", "torch.tensor().uniform_().item", "torch.tensor().uniform_().item", "torch.tensor().uniform_().item", "torch.tensor().uniform_().item", "torchvision.adjust_brightness", "torchvision.adjust_contrast", "torchvision.adjust_saturation", "torchvision.adjust_hue", "torch.tensor().uniform_", "torch.tensor().uniform_", "torch.tensor().uniform_", "torch.tensor().uniform_", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["     ", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of PIL Image or Tensor): Input clip.\n\n        Returns:\n            List of PIL Image or Tensor: Color jittered clip.\n        \"\"\"", "\n", "fn_idx", "=", "torch", ".", "randperm", "(", "4", ")", "\n", "for", "fn_id", "in", "fn_idx", ":", "\n", "            ", "if", "fn_id", "==", "0", "and", "self", ".", "brightness", "is", "not", "None", ":", "\n", "                ", "brightness", "=", "self", ".", "brightness", "\n", "brightness_factor", "=", "torch", ".", "tensor", "(", "1.0", ")", ".", "uniform_", "(", "brightness", "[", "0", "]", ",", "brightness", "[", "1", "]", ")", ".", "item", "(", ")", "\n", "clip", "=", "[", "F", ".", "adjust_brightness", "(", "img", ",", "brightness_factor", ")", "for", "img", "in", "clip", "]", "\n", "\n", "", "if", "fn_id", "==", "1", "and", "self", ".", "contrast", "is", "not", "None", ":", "\n", "                ", "contrast", "=", "self", ".", "contrast", "\n", "contrast_factor", "=", "torch", ".", "tensor", "(", "1.0", ")", ".", "uniform_", "(", "contrast", "[", "0", "]", ",", "contrast", "[", "1", "]", ")", ".", "item", "(", ")", "\n", "clip", "=", "[", "F", ".", "adjust_contrast", "(", "img", ",", "contrast_factor", ")", "for", "img", "in", "clip", "]", "\n", "\n", "", "if", "fn_id", "==", "2", "and", "self", ".", "saturation", "is", "not", "None", ":", "\n", "                ", "saturation", "=", "self", ".", "saturation", "\n", "saturation_factor", "=", "torch", ".", "tensor", "(", "1.0", ")", ".", "uniform_", "(", "saturation", "[", "0", "]", ",", "saturation", "[", "1", "]", ")", ".", "item", "(", ")", "\n", "clip", "=", "[", "F", ".", "adjust_saturation", "(", "img", ",", "saturation_factor", ")", "for", "img", "in", "clip", "]", "\n", "\n", "", "if", "fn_id", "==", "3", "and", "self", ".", "hue", "is", "not", "None", ":", "\n", "                ", "hue", "=", "self", ".", "hue", "\n", "hue_factor", "=", "torch", ".", "tensor", "(", "1.0", ")", ".", "uniform_", "(", "hue", "[", "0", "]", ",", "hue", "[", "1", "]", ")", ".", "item", "(", ")", "\n", "clip", "=", "[", "F", ".", "adjust_hue", "(", "img", ",", "hue_factor", ")", "for", "img", "in", "clip", "]", "\n", "\n", "", "", "return", "clip", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipRandomGrayscale.__call__": [[250, 262], ["random.random", "torchvision.to_grayscale"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of PIL Image or Tensor): Clip to be converted to grayscale.\n\n        Returns:\n            List of PIL Image or Tensor: Randomly grayscaled clip.\n        \"\"\"", "\n", "num_output_channels", "=", "1", "if", "clip", "[", "0", "]", ".", "mode", "==", "'L'", "else", "3", "\n", "if", "random", ".", "random", "(", ")", "<", "self", ".", "p", ":", "\n", "            ", "return", "[", "F", ".", "to_grayscale", "(", "img", ",", "num_output_channels", "=", "num_output_channels", ")", "for", "img", "in", "clip", "]", "\n", "", "return", "clip", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipRandomHorizontalFlip.__call__": [[266, 282], ["torch.rand", "torchvision.hflip"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "clip", ",", "is_flip", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of PIL Image or Tensor): Clip to be flipped.\n\n        Returns:\n            List of PIL Image or Tensor: Randomly flipped clip.\n        \"\"\"", "\n", "\n", "if", "torch", ".", "rand", "(", "1", ")", "<", "self", ".", "p", "and", "is_flip", ":", "\n", "            ", "clip", "=", "[", "F", ".", "hflip", "(", "img", ")", "for", "img", "in", "clip", "]", "\n", "is_flip", "=", "True", "\n", "", "else", ":", "\n", "            ", "is_flip", "=", "False", "\n", "\n", "", "return", "(", "clip", ",", "is_flip", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipNormalize.__init__": [[301, 305], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mean", ",", "std", ",", "inplace", "=", "False", ")", ":", "\n", "        ", "self", ".", "mean", "=", "mean", "\n", "self", ".", "std", "=", "std", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipNormalize.__call__": [[306, 316], ["torchvision.normalize"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "clip", ",", "is_flip", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of Tensor): List of tensor image of size (C, H, W) to be normalized.\n\n        Returns:\n            Tensor: Normalized Tensor list.\n        \"\"\"", "\n", "\n", "return", "(", "[", "F", ".", "normalize", "(", "img", ",", "self", ".", "mean", ",", "self", ".", "std", ",", "self", ".", "inplace", ")", "for", "img", "in", "clip", "]", ",", "is_flip", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipNormalize.__repr__": [[317, 319], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(mean={0}, std={1})'", ".", "format", "(", "self", ".", "mean", ",", "self", ".", "std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipResize.__call__": [[334, 343], ["torchvision.resize"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of Tensor):: Clip to be scaled.\n\n        Returns:\n            List of PIL Image: Rescaled clip.\n        \"\"\"", "\n", "return", "[", "F", ".", "resize", "(", "img", ",", "self", ".", "size", ",", "self", ".", "interpolation", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipCenterCrop.__call__": [[354, 363], ["torchvision.center_crop"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of Tensor): Clip to be cropped.\n\n        Returns:\n            List of PIL Image: Cropped clip.\n        \"\"\"", "\n", "return", "[", "F", ".", "center_crop", "(", "img", ",", "self", ".", "size", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipFirstCrop.__call__": [[374, 393], ["isinstance", "ValueError", "img.crop", "int", "int", "len", "msg.format"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of Tensor): Clip to be cropped.\n\n        Returns:\n            List of PIL Image: Cropped clip.\n        \"\"\"", "\n", "if", "isinstance", "(", "self", ".", "size", ",", "numbers", ".", "Number", ")", ":", "\n", "            ", "self", ".", "size", "=", "(", "int", "(", "self", ".", "size", ")", ",", "int", "(", "self", ".", "size", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "len", "(", "self", ".", "size", ")", "==", "2", ",", "\"Please provide only two dimensions (h, w) for size.\"", "\n", "", "image_width", ",", "image_height", "=", "clip", "[", "0", "]", ".", "size", "\n", "crop_height", ",", "crop_width", "=", "self", ".", "size", "\n", "if", "crop_width", ">", "image_width", "or", "crop_height", ">", "image_height", ":", "\n", "            ", "msg", "=", "\"Requested crop size {} is bigger than input size {}\"", "\n", "raise", "ValueError", "(", "msg", ".", "format", "(", "self", ".", "size", ",", "(", "image_height", ",", "image_width", ")", ")", ")", "\n", "\n", "", "return", "[", "img", ".", "crop", "(", "(", "0", ",", "0", ",", "crop_width", ",", "crop_height", ")", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipThirdCrop.__call__": [[404, 423], ["isinstance", "ValueError", "img.crop", "int", "int", "len", "msg.format"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            clip (List of Tensor): Clip to be cropped.\n\n        Returns:\n            List of PIL Image: Cropped clip.\n        \"\"\"", "\n", "if", "isinstance", "(", "self", ".", "size", ",", "numbers", ".", "Number", ")", ":", "\n", "            ", "self", ".", "size", "=", "(", "int", "(", "self", ".", "size", ")", ",", "int", "(", "self", ".", "size", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "len", "(", "self", ".", "size", ")", "==", "2", ",", "\"Please provide only two dimensions (h, w) for size.\"", "\n", "", "image_width", ",", "image_height", "=", "clip", "[", "0", "]", ".", "size", "\n", "crop_height", ",", "crop_width", "=", "self", ".", "size", "\n", "if", "crop_width", ">", "image_width", "or", "crop_height", ">", "image_height", ":", "\n", "            ", "msg", "=", "\"Requested crop size {} is bigger than input size {}\"", "\n", "raise", "ValueError", "(", "msg", ".", "format", "(", "self", ".", "size", ",", "(", "image_height", ",", "image_width", ")", ")", ")", "\n", "\n", "", "return", "[", "img", ".", "crop", "(", "(", "image_width", "-", "crop_width", ",", "image_height", "-", "crop_height", ",", "image_width", ",", "image_height", ")", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipGaussianBlur.__init__": [[426, 428], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "sigma", "=", "[", ".1", ",", "2.", "]", ")", ":", "\n", "        ", "self", ".", "sigma", "=", "sigma", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.ClipGaussianBlur.__call__": [[429, 432], ["random.uniform", "img.filter", "PIL.ImageFilter.GaussianBlur"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "sigma", "=", "random", ".", "uniform", "(", "self", ".", "sigma", "[", "0", "]", ",", "self", ".", "sigma", "[", "1", "]", ")", "\n", "return", "[", "img", ".", "filter", "(", "ImageFilter", ".", "GaussianBlur", "(", "radius", "=", "sigma", ")", ")", "for", "img", "in", "clip", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.Lambda.__init__": [[440, 443], ["callable", "repr", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "lambd", ")", ":", "\n", "        ", "assert", "callable", "(", "lambd", ")", ",", "repr", "(", "type", "(", "lambd", ")", ".", "__name__", ")", "+", "\" object is not callable\"", "\n", "self", ".", "lambd", "=", "lambd", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.Lambda.__call__": [[444, 447], ["clip_transforms.Lambda.lambd"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "clip", ",", "is_flip", "=", "False", ")", ":", "\n", "\n", "        ", "return", "(", "self", ".", "lambd", "(", "clip", ")", ",", "is_flip", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.clip_transforms.Lambda.__repr__": [[448, 450], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'()'", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.augmentation_util.pil_from_raw_rgb": [[10, 12], ["PIL.Image.open().convert", "PIL.Image.open", "io.BytesIO"], "function", ["None"], ["def", "pil_from_raw_rgb", "(", "raw", ")", ":", "\n", "    ", "return", "Image", ".", "open", "(", "io", ".", "BytesIO", "(", "raw", ")", ")", ".", "convert", "(", "'RGB'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.augmentation_util.pil_from_raw_rgba": [[13, 15], ["PIL.Image.open().convert", "PIL.Image.open", "io.BytesIO"], "function", ["None"], ["", "def", "pil_from_raw_rgba", "(", "raw", ")", ":", "\n", "    ", "return", "Image", ".", "open", "(", "io", ".", "BytesIO", "(", "raw", ")", ")", ".", "convert", "(", "'L'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.augmentation_util._get_image_size": [[16, 23], ["torchvision._is_pil_image", "isinstance", "TypeError", "img.dim", "type"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.clip_augmentations._is_pil_image"], ["", "def", "_get_image_size", "(", "img", ")", ":", "\n", "    ", "if", "F", ".", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "return", "img", ".", "size", "\n", "", "elif", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", "and", "img", ".", "dim", "(", ")", ">", "2", ":", "\n", "        ", "return", "img", ".", "shape", "[", "-", "2", ":", "]", "[", ":", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "        ", "raise", "TypeError", "(", "\"Unexpected type {}\"", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.augmentation_util.calc_over_lab": [[25, 32], ["max", "min"], "function", ["None"], ["", "", "def", "calc_over_lab", "(", "z1", ",", "z2", ")", ":", "\n", "    ", "z_min", "=", "max", "(", "z1", "[", "0", "]", ",", "z2", "[", "0", "]", ")", "\n", "z_max", "=", "min", "(", "z1", "[", "1", "]", ",", "z2", "[", "1", "]", ")", "\n", "if", "z_min", ">=", "z_max", ":", "\n", "        ", "return", "0", "\n", "", "else", ":", "\n", "        ", "return", "(", "z_max", "-", "z_min", ")", "/", "(", "z1", "[", "1", "]", "-", "z1", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.augmentation_util.get_cor": [[33, 61], ["isinstance", "clip_mag.reshape", "list", "list", "len", "row.max", "col.max", "map", "map", "sum", "len", "numpy.argsort", "math.floor", "row.min", "col.min"], "function", ["None"], ["", "", "def", "get_cor", "(", "clip_mag", ",", "height", ",", "width", ",", "base_ratio", "=", "0.8", ",", "t", "=", "0.1", ")", ":", "\n", "    ", "if", "isinstance", "(", "clip_mag", ",", "list", ")", ":", "\n", "        ", "clip_mag", "=", "sum", "(", "clip_mag", ")", "/", "len", "(", "clip_mag", ")", "\n", "", "if", "len", "(", "clip_mag", ".", "shape", ")", "==", "3", ":", "\n", "        ", "clip_mag", "=", "clip_mag", "[", ":", ",", ":", ",", "0", "]", "\n", "\n", "", "h", ",", "w", "=", "clip_mag", ".", "shape", "\n", "flat_mag", "=", "clip_mag", ".", "reshape", "(", "-", "1", ")", "\n", "idx", "=", "np", ".", "argsort", "(", "flat_mag", ")", "[", ":", ":", "-", "1", "]", "[", ":", "math", ".", "floor", "(", "h", "*", "w", "*", "t", ")", "]", "\n", "row", "=", "(", "idx", "//", "w", ")", "+", "1", "\n", "col", "=", "(", "idx", "%", "w", ")", "+", "1", "\n", "\n", "row_s", "=", "[", "row", ".", "min", "(", ")", "-", "1", ",", "row", ".", "max", "(", ")", "]", "\n", "col_s", "=", "[", "col", ".", "min", "(", ")", "-", "1", ",", "col", ".", "max", "(", ")", "]", "\n", "\n", "# c represent the area of the region with high motion", "\n", "c", "=", "(", "row_s", "[", "1", "]", "-", "row_s", "[", "0", "]", ")", "*", "(", "col_s", "[", "1", "]", "-", "col_s", "[", "0", "]", ")", "\n", "\n", "# for different c, we would make minor change on base_ratio", "\n", "if", "c", "<=", "4", ":", "crop_ratio", "=", "base_ratio", "+", "0.1", "\n", "elif", "c", "<=", "8", ":", "crop_ratio", "=", "base_ratio", "\n", "elif", "c", "<=", "12", ":", "crop_ratio", "=", "base_ratio", "-", "0.1", "\n", "else", ":", "crop_ratio", "=", "0", "\n", "\n", "row_s", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "*", "height", "/", "7", ",", "row_s", ")", ")", "\n", "col_s", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "*", "width", "/", "7", ",", "col_s", ")", ")", "\n", "\n", "return", "row_s", ",", "col_s", ",", "crop_ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.augmentations.augmentation_util.crop_from_corners": [[63, 77], ["random.randint", "random.randint", "random.randint", "random.randint", "random.randint", "int", "int", "int", "int"], "function", ["None"], ["", "def", "crop_from_corners", "(", "row_s", ",", "col_s", ",", "height", ",", "width", ")", ":", "\n", "\n", "    ", "if", "random", ".", "randint", "(", "0", ",", "1", ")", "==", "0", ":", "\n", "        ", "i_h", "=", "random", ".", "randint", "(", "0", ",", "int", "(", "row_s", "[", "0", "]", ")", "+", "1", ")", "\n", "i_w", "=", "random", ".", "randint", "(", "0", ",", "int", "(", "col_s", "[", "0", "]", ")", "+", "1", ")", "\n", "w", "=", "col_s", "[", "1", "]", "-", "i_w", "\n", "h", "=", "row_s", "[", "1", "]", "-", "i_h", "\n", "return", "i_h", ",", "i_w", ",", "h", ",", "w", "\n", "", "else", ":", "\n", "        ", "i_h", "=", "random", ".", "randint", "(", "int", "(", "row_s", "[", "1", "]", "-", "1", ")", ",", "height", ")", "\n", "i_w", "=", "random", ".", "randint", "(", "int", "(", "col_s", "[", "1", "]", "-", "1", ")", ",", "width", ")", "\n", "w", "=", "i_w", "-", "col_s", "[", "0", "]", "\n", "h", "=", "i_h", "-", "row_s", "[", "0", "]", "\n", "return", "row_s", "[", "0", "]", ",", "col_s", "[", "0", "]", ",", "h", ",", "w", "", "", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.process_data.process_video.get_video_frames_cv": [[6, 41], ["cv2.VideoCapture", "int", "cv2.VideoCapture.get", "cv2.VideoCapture.get", "min", "cv2.VideoCapture.read", "cv2.VideoCapture.release", "cv2.VideoCapture.get", "frames.append", "cv2.VideoCapture.read", "int", "int", "int", "int", "cv2.resize", "cv2.resize"], "function", ["None"], ["def", "get_video_frames_cv", "(", "v_path", ",", "dataset", "=", "'ucf101'", ")", ":", "\n", "\n", "    ", "target", "=", "256", "# for kinetics", "\n", "vidcap", "=", "cv2", ".", "VideoCapture", "(", "v_path", ")", "\n", "nb_frames", "=", "int", "(", "vidcap", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_COUNT", ")", ")", "\n", "\n", "if", "nb_frames", "==", "0", ":", "return", "None", "\n", "w", "=", "vidcap", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_WIDTH", ")", "# float", "\n", "h", "=", "vidcap", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_HEIGHT", ")", "# float", "\n", "\n", "\n", "short_size", "=", "min", "(", "w", ",", "h", ")", "\n", "success", ",", "image", "=", "vidcap", ".", "read", "(", ")", "\n", "count", "=", "1", "\n", "\n", "if", "w", ">=", "h", ":", "\n", "        ", "size", "=", "(", "int", "(", "target", "*", "w", "/", "h", ")", ",", "int", "(", "target", ")", ")", "\n", "", "else", ":", "\n", "        ", "size", "=", "(", "int", "(", "target", ")", ",", "int", "(", "target", "*", "h", "/", "w", ")", ")", "\n", "\n", "", "frames", "=", "[", "]", "\n", "while", "success", ":", "\n", "        ", "if", "dataset", "==", "'kinetics'", ":", "\n", "            ", "if", "short_size", "<=", "256", ":", "\n", "                ", "image", "=", "cv2", ".", "resize", "(", "image", ",", "size", ",", "cv2", ".", "INTER_CUBIC", ")", "\n", "", "else", ":", "\n", "                ", "image", "=", "cv2", ".", "resize", "(", "image", ",", "size", ",", "cv2", ".", "INTER_AREA", ")", "\n", "\n", "", "", "frames", ".", "append", "(", "image", ")", "\n", "\n", "success", ",", "image", "=", "vidcap", ".", "read", "(", ")", "\n", "count", "+=", "1", "\n", "\n", "", "vidcap", ".", "release", "(", ")", "\n", "return", "frames", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.process_data.process_video.compute_TVL1": [[42, 54], ["cv2.optflow.DualTVL1OpticalFlow_create", "cv2.optflow.DualTVL1OpticalFlow_create.calc", "numpy.clip", "numpy.round().astype", "numpy.round"], "function", ["None"], ["", "def", "compute_TVL1", "(", "prev", ",", "curr", ",", "bound", "=", "20", ")", ":", "\n", "    ", "\"\"\"Compute the TV-L1 optical flow.\"\"\"", "\n", "\n", "TVL1", "=", "cv2", ".", "optflow", ".", "DualTVL1OpticalFlow_create", "(", ")", "\n", "\n", "flow", "=", "TVL1", ".", "calc", "(", "prev", ",", "curr", ",", "None", ")", "\n", "flow", "=", "np", ".", "clip", "(", "flow", ",", "-", "bound", ",", "bound", ")", "\n", "\n", "flow", "=", "(", "flow", "+", "bound", ")", "*", "(", "255.0", "/", "(", "2", "*", "bound", ")", ")", "\n", "flow", "=", "np", ".", "round", "(", "flow", ")", ".", "astype", "(", "'uint8'", ")", "\n", "\n", "return", "flow", "", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.process_data.create_lmdb.create_lmdb_video_dataset_rgb": [[16, 49], ["glob.glob", "print", "os.path.join", "os.path.join", "os.makedirs", "cv2.imencode", "lmdb.open", "len", "range", "joblib.Parallel", "dataset.process_data.process_video.get_video_frames_cv", "lmdb.open.begin", "cv2.imencode", "env.begin.put", "env.begin.commit", "open", "f.write", "video_path.split", "int", "key.encode", "os.path.join", "str", "joblib.delayed", "tqdm.tqdm", "len", "int", "len"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.process_data.process_video.get_video_frames_cv"], ["def", "create_lmdb_video_dataset_rgb", "(", "dataset", ",", "root_path", ",", "dst_path", ",", "workers", "=", "-", "1", ",", "quality", "=", "100", ",", "video_type", "=", "'mp4'", ")", ":", "\n", "\n", "    ", "if", "dataset", "==", "'kinetics'", ":", "video_type", "=", "'mp4'", "\n", "else", ":", "video_type", "=", "'avi'", "\n", "\n", "videos", "=", "glob", "(", "os", ".", "path", ".", "join", "(", "root_path", ",", "'*/*.{}'", ".", "format", "(", "video_type", ")", ")", ")", "\n", "print", "(", "'begin'", ")", "\n", "\n", "def", "make_video", "(", "video_path", ",", "dst_path", ")", ":", "\n", "        ", "vid_names", "=", "'/'", ".", "join", "(", "video_path", ".", "split", "(", "'/'", ")", "[", "-", "2", ":", "]", ")", "\n", "dst_file", "=", "os", ".", "path", ".", "join", "(", "dst_path", ",", "vid_names", "[", ":", "-", "4", "]", ")", "\n", "os", ".", "makedirs", "(", "dst_file", ",", "exist_ok", "=", "True", ")", "\n", "try", ":", "\n", "            ", "frames", "=", "get_video_frames_cv", "(", "video_path", ",", "dataset", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "return", "\n", "", "else", ":", "\n", "            ", "if", "frames", "==", "None", ":", "return", "\n", "\n", "", "_", ",", "frame_byte", "=", "cv2", ".", "imencode", "(", "'.jpg'", ",", "frames", "[", "0", "]", ",", "[", "int", "(", "cv2", ".", "IMWRITE_JPEG_QUALITY", ")", ",", "quality", "]", ")", "\n", "env", "=", "lmdb", ".", "open", "(", "dst_file", ",", "frame_byte", ".", "nbytes", "*", "len", "(", "frames", ")", "*", "50", ")", "\n", "frames_num", "=", "len", "(", "frames", ")", "\n", "for", "i", "in", "range", "(", "frames_num", ")", ":", "\n", "            ", "txn", "=", "env", ".", "begin", "(", "write", "=", "True", ")", "\n", "key", "=", "'image_{:05d}.jpg'", ".", "format", "(", "i", "+", "1", ")", "\n", "frame", "=", "frames", "[", "i", "]", "\n", "_", ",", "frame_byte", "=", "cv2", ".", "imencode", "(", "'.jpg'", ",", "frame", ",", "[", "int", "(", "cv2", ".", "IMWRITE_JPEG_QUALITY", ")", ",", "quality", "]", ")", "\n", "txn", ".", "put", "(", "key", ".", "encode", "(", ")", ",", "frame_byte", ")", "\n", "txn", ".", "commit", "(", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "dst_file", ",", "'split.txt'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "frames_num", ")", ")", "\n", "\n", "", "", "Parallel", "(", "n_jobs", "=", "workers", ")", "(", "delayed", "(", "make_video", ")", "(", "vp", ",", "dst_path", ")", "for", "vp", "in", "tqdm", "(", "videos", ",", "total", "=", "len", "(", "videos", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.process_data.create_lmdb.create_lmdb_video_dataset_optical_flow": [[51, 102], ["glob.glob", "print", "os.path.join", "os.path.join", "os.makedirs", "lmdb.open", "lmdb.open.begin", "env.begin.cursor", "lmdb.open.close", "enumerate", "cv2.imencode", "lmdb.open", "len", "range", "joblib.Parallel", "cv2.imdecode", "frames.append", "numpy.ones().astype", "cv2.cvtColor", "dataset.process_data.process_video.compute_TVL1", "empty_img.copy", "flows.append", "lmdb.open.begin", "cv2.imencode", "env.begin.put", "env.begin.commit", "open", "f.write", "video_path.split", "numpy.frombuffer", "cv2.cvtColor", "int", "key.encode", "os.path.join", "str", "joblib.delayed", "tqdm.tqdm", "numpy.ones", "len", "int", "len", "int", "int"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.process_data.process_video.compute_TVL1"], ["", "def", "create_lmdb_video_dataset_optical_flow", "(", "dataset", ",", "root_path", ",", "dst_path", ",", "workers", "=", "-", "1", ",", "quality", "=", "100", ")", ":", "\n", "\n", "    ", "videos", "=", "glob", "(", "os", ".", "path", ".", "join", "(", "root_path", ",", "'*/*'", ")", ")", "\n", "print", "(", "'begin'", ")", "\n", "\n", "def", "make_video_optical_flow", "(", "video_path", ",", "dst_path", ")", ":", "\n", "        ", "vid_names", "=", "'/'", ".", "join", "(", "video_path", ".", "split", "(", "'/'", ")", "[", "-", "2", ":", "]", ")", "\n", "dst_file", "=", "os", ".", "path", ".", "join", "(", "dst_path", ",", "vid_names", ")", "\n", "os", ".", "makedirs", "(", "dst_file", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# load rgb frames from lmdb. You can change the code to load it in another way", "\n", "frames", "=", "[", "]", "\n", "env", "=", "lmdb", ".", "open", "(", "video_path", ",", "readonly", "=", "True", ")", "\n", "txn", "=", "env", ".", "begin", "(", "write", "=", "False", ")", "\n", "for", "k", ",", "v", "in", "txn", ".", "cursor", "(", ")", ":", "\n", "            ", "frame_decode", "=", "cv2", ".", "imdecode", "(", "np", ".", "frombuffer", "(", "v", ",", "np", ".", "uint8", ")", ",", "cv2", ".", "IMREAD_COLOR", ")", "\n", "frames", ".", "append", "(", "frame_decode", ")", "\n", "", "env", ".", "close", "(", ")", "\n", "\n", "height", ",", "width", ",", "_", "=", "frames", "[", "0", "]", ".", "shape", "\n", "empty_img", "=", "128", "*", "np", ".", "ones", "(", "(", "int", "(", "height", ")", ",", "int", "(", "width", ")", ",", "3", ")", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "# extract flows", "\n", "flows", "=", "[", "]", "\n", "for", "idx", ",", "frame", "in", "enumerate", "(", "frames", ")", ":", "\n", "            ", "if", "idx", "==", "0", ":", "\n", "                ", "pre_frame", "=", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_BGR2GRAY", ")", "\n", "continue", "\n", "", "frame_gray", "=", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_BGR2GRAY", ")", "\n", "flow", "=", "compute_TVL1", "(", "pre_frame", ",", "frame_gray", ")", "\n", "# create flow frame with 3 channel", "\n", "flow_img", "=", "empty_img", ".", "copy", "(", ")", "\n", "flow_img", "[", ":", ",", ":", ",", "0", ":", "2", "]", "=", "flow", "\n", "flows", ".", "append", "(", "flow_img", ")", "\n", "pre_frame", "=", "frame_gray", "\n", "\n", "# save flows", "\n", "", "_", ",", "frame_byte", "=", "cv2", ".", "imencode", "(", "'.jpg'", ",", "flows", "[", "0", "]", ",", "[", "int", "(", "cv2", ".", "IMWRITE_JPEG_QUALITY", ")", ",", "quality", "]", ")", "\n", "env", "=", "lmdb", ".", "open", "(", "dst_file", ",", "frame_byte", ".", "nbytes", "*", "len", "(", "flows", ")", "*", "50", ")", "\n", "frames_num", "=", "len", "(", "flows", ")", "\n", "\n", "for", "i", "in", "range", "(", "frames_num", ")", ":", "\n", "            ", "txn", "=", "env", ".", "begin", "(", "write", "=", "True", ")", "\n", "key", "=", "'image_{:05d}.jpg'", ".", "format", "(", "i", "+", "1", ")", "\n", "flow_img", "=", "flows", "[", "i", "]", "\n", "_", ",", "frame_byte", "=", "cv2", ".", "imencode", "(", "'.jpg'", ",", "flow_img", ",", "[", "int", "(", "cv2", ".", "IMWRITE_JPEG_QUALITY", ")", ",", "quality", "]", ")", "\n", "txn", ".", "put", "(", "key", ".", "encode", "(", ")", ",", "frame_byte", ")", "\n", "txn", ".", "commit", "(", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "dst_file", ",", "'split.txt'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "frames_num", ")", ")", "\n", "\n", "", "", "Parallel", "(", "n_jobs", "=", "workers", ")", "(", "delayed", "(", "make_video_optical_flow", ")", "(", "vp", ",", "dst_path", ")", "for", "vp", "in", "tqdm", "(", "videos", ",", "total", "=", "len", "(", "videos", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.process_data.create_lmdb.create_lmdb_video_dataset_flow_mag": [[104, 172], ["glob.glob", "print", "os.path.join", "os.path.join", "os.makedirs", "lmdb.open", "lmdb.open.begin", "env.begin.cursor", "lmdb.open.close", "len", "lmdb.open", "range", "joblib.Parallel", "os.path.exists", "cv2.imdecode", "flows.append", "lmdb.open.begin", "list", "list", "sts.motion_sts.compute_motion_boudary", "sts.motion_sts.compute_motion_boudary", "cv2.cartToPolar", "cv2.cartToPolar", "sts.motion_sts.zero_boundary", "cv2.imencode", "env_flow_mag.begin.put", "env_flow_mag.begin.commit", "open", "f.write", "video_path.split", "os.path.join", "numpy.frombuffer", "int", "sts.motion_sts.motion_mag_downsample().astype", "key.encode", "os.path.join", "str", "joblib.delayed", "tqdm.tqdm", "cv2.resize().astype", "cv2.resize().astype", "int", "sts.motion_sts.motion_mag_downsample", "len", "cv2.resize", "cv2.resize", "sts.motion_sts.motion_mag_downsample"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.compute_motion_boudary", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.compute_motion_boudary", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.zero_boundary", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.motion_mag_downsample", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.motion_mag_downsample"], ["", "def", "create_lmdb_video_dataset_flow_mag", "(", "dataset", ",", "root_path", ",", "dst_path", ",", "workers", "=", "-", "1", ",", "ws", "=", "8", ",", "quality", "=", "100", ")", ":", "\n", "    ", "videos", "=", "glob", "(", "os", ".", "path", ".", "join", "(", "root_path", ",", "'*/*'", ")", ")", "\n", "print", "(", "'begin'", ")", "\n", "\n", "def", "make_video_flow_mag", "(", "video_path", ",", "dst_path", ")", ":", "\n", "        ", "vid_names", "=", "'/'", ".", "join", "(", "video_path", ".", "split", "(", "'/'", ")", "[", "-", "2", ":", "]", ")", "\n", "dst_file", "=", "os", ".", "path", ".", "join", "(", "dst_path", ",", "vid_names", ")", "\n", "os", ".", "makedirs", "(", "dst_file", ",", "exist_ok", "=", "True", ")", "\n", "\n", "if", "dataset", "==", "'kinetics'", ":", "ws", "=", "4", "\n", "else", ":", "ws", "=", "8", "\n", "\n", "# load flows from lmdb. You can change the code to load it in another way", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "video_path", ",", "'data.mdb'", ")", ")", ":", "return", "\n", "flows", "=", "[", "]", "\n", "env", "=", "lmdb", ".", "open", "(", "video_path", ",", "readonly", "=", "True", ")", "\n", "txn", "=", "env", ".", "begin", "(", "write", "=", "False", ")", "\n", "for", "k", ",", "v", "in", "txn", ".", "cursor", "(", ")", ":", "\n", "            ", "flow_decode", "=", "cv2", ".", "imdecode", "(", "np", ".", "frombuffer", "(", "v", ",", "np", ".", "uint8", ")", ",", "cv2", ".", "IMREAD_COLOR", ")", "\n", "flows", ".", "append", "(", "flow_decode", ")", "\n", "", "env", ".", "close", "(", ")", "\n", "duration", "=", "len", "(", "flows", ")", "\n", "\n", "# compute frame mag offline with a sliding window", "\n", "env_flow_mag", "=", "lmdb", ".", "open", "(", "dst_file", ",", "readonly", "=", "False", ",", "map_size", "=", "int", "(", "2e12", ")", ")", "\n", "for", "idx", "in", "range", "(", "1", ",", "duration", "+", "1", ")", ":", "\n", "            ", "txn_flow_mag", "=", "env_flow_mag", ".", "begin", "(", "write", "=", "True", ")", "\n", "if", "ws", "==", "1", ":", "\n", "                ", "flow_clip", "=", "[", "flows", "[", "idx", "-", "1", "]", "]", "\n", "", "else", ":", "\n", "                ", "if", "idx", "-", "ws", "//", "2", ">=", "0", "and", "idx", "+", "ws", "//", "2", "<=", "duration", ":", "\n", "                    ", "flow_clip", "=", "flows", "[", "idx", "-", "ws", "//", "2", ":", "idx", "+", "ws", "//", "2", "]", "\n", "", "elif", "idx", "-", "ws", "//", "2", ">=", "0", "and", "idx", "+", "ws", "//", "2", ">", "duration", ":", "\n", "                    ", "flow_clip", "=", "flows", "[", "-", "ws", ":", "]", "\n", "", "elif", "idx", "+", "ws", "//", "2", "<=", "duration", "and", "idx", "-", "ws", "//", "2", "<", "0", ":", "\n", "                    ", "flow_clip", "=", "flows", "[", ":", "ws", "]", "\n", "", "else", ":", "\n", "                    ", "flow_clip", "=", "flows", "[", ":", "]", "\n", "\n", "", "", "flows_u", "=", "list", "(", "[", "cv2", ".", "resize", "(", "flow", "[", ":", ",", ":", ",", "0", "]", ",", "(", "224", ",", "224", ")", ",", "interpolation", "=", "cv2", ".", "INTER_CUBIC", ")", ".", "astype", "(", "np", ".", "float32", ")", "for", "flow", "in", "flow_clip", "]", ")", "\n", "flows_v", "=", "list", "(", "[", "cv2", ".", "resize", "(", "flow", "[", ":", ",", ":", ",", "1", "]", ",", "(", "224", ",", "224", ")", ",", "interpolation", "=", "cv2", ".", "INTER_CUBIC", ")", ".", "astype", "(", "np", ".", "float32", ")", "for", "flow", "in", "flow_clip", "]", ")", "\n", "\n", "_", ",", "_", ",", "mb_x_u", ",", "mb_y_u", "=", "compute_motion_boudary", "(", "flows_u", ")", "\n", "_", ",", "_", ",", "mb_x_v", ",", "mb_y_v", "=", "compute_motion_boudary", "(", "flows_v", ")", "\n", "\n", "frame_mag_u", ",", "_", "=", "cv2", ".", "cartToPolar", "(", "mb_x_u", ",", "mb_y_u", ",", "angleInDegrees", "=", "True", ")", "\n", "frame_mag_v", ",", "_", "=", "cv2", ".", "cartToPolar", "(", "mb_x_v", ",", "mb_y_v", ",", "angleInDegrees", "=", "True", ")", "\n", "frame_mag", "=", "(", "frame_mag_u", "+", "frame_mag_v", ")", "/", "2", "\n", "\n", "# zero boundary", "\n", "frame_mag", "=", "zero_boundary", "(", "frame_mag", ")", "\n", "\n", "# downsample to match the fearture size of backbone output", "\n", "if", "ws", "==", "1", ":", "\n", "                ", "frame_mag_down", "=", "(", "motion_mag_downsample", "(", "frame_mag", ",", "7", ",", "224", ")", "*", "5", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "", "else", ":", "\n", "                ", "frame_mag_down", "=", "motion_mag_downsample", "(", "frame_mag", ",", "7", ",", "224", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n", "# save frame mag", "\n", "", "key", "=", "'image_{:05d}.jpg'", ".", "format", "(", "idx", ")", "\n", "_", ",", "frame_byte", "=", "cv2", ".", "imencode", "(", "'.jpg'", ",", "frame_mag_down", ",", "[", "int", "(", "cv2", ".", "IMWRITE_JPEG_QUALITY", ")", ",", "quality", "]", ")", "\n", "txn_flow_mag", ".", "put", "(", "key", ".", "encode", "(", ")", ",", "frame_byte", ")", "\n", "txn_flow_mag", ".", "commit", "(", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "dst_file", ",", "'split.txt'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "duration", ")", ")", "\n", "\n", "", "", "Parallel", "(", "n_jobs", "=", "workers", ")", "(", "delayed", "(", "make_video_flow_mag", ")", "(", "vp", ",", "dst_path", ")", "for", "vp", "in", "tqdm", "(", "videos", ",", "total", "=", "len", "(", "videos", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.process_data.create_lmdb.create_lmdb_video_dataset_clip_mag": [[173, 224], ["glob.glob", "print", "os.path.join", "lmdb.open", "lmdb.open.begin", "env.begin.cursor", "lmdb.open.close", "len", "range", "joblib.Parallel", "open", "pickle.dump", "os.path.exists", "cv2.imdecode", "flows.append", "list", "list", "sts.motion_sts.compute_motion_boudary", "sts.motion_sts.compute_motion_boudary", "cv2.cartToPolar", "cv2.cartToPolar", "sts.motion_sts.zero_boundary().mean", "clip_mag_list.append", "os.path.join", "video_path.split", "os.path.join", "numpy.frombuffer", "joblib.delayed", "tqdm.tqdm", "flow[].astype", "flow[].astype", "sts.motion_sts.zero_boundary", "len"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.compute_motion_boudary", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.compute_motion_boudary", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.zero_boundary"], ["", "def", "create_lmdb_video_dataset_clip_mag", "(", "dataset", ",", "root_path", ",", "dst_path", ",", "clip_length", "=", "16", ",", "steps", "=", "2", ",", "workers", "=", "-", "1", ")", ":", "\n", "    ", "videos", "=", "glob", "(", "os", ".", "path", ".", "join", "(", "root_path", ",", "'*/*'", ")", ")", "\n", "\n", "print", "(", "'begin'", ")", "\n", "\n", "def", "make_video_clip_mag", "(", "video_path", ")", ":", "\n", "        ", "vid_names", "=", "'/'", ".", "join", "(", "video_path", ".", "split", "(", "'/'", ")", "[", "-", "2", ":", "]", ")", "\n", "\n", "# load flows from lmdb. You can change the code to load it in another way", "\n", "flows", "=", "[", "]", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "video_path", ",", "'data.mdb'", ")", ")", ":", "return", "\n", "\n", "env", "=", "lmdb", ".", "open", "(", "video_path", ",", "readonly", "=", "True", ",", "lock", "=", "False", ")", "\n", "txn", "=", "env", ".", "begin", "(", "write", "=", "False", ")", "\n", "for", "k", ",", "v", "in", "txn", ".", "cursor", "(", ")", ":", "\n", "            ", "flow_decode", "=", "cv2", ".", "imdecode", "(", "np", ".", "frombuffer", "(", "v", ",", "np", ".", "uint8", ")", ",", "cv2", ".", "IMREAD_COLOR", ")", "\n", "flows", ".", "append", "(", "flow_decode", ")", "\n", "", "env", ".", "close", "(", ")", "\n", "duration", "=", "len", "(", "flows", ")", "\n", "\n", "clip_mag_list", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "1", ",", "duration", "+", "1", ")", ":", "\n", "            ", "if", "idx", "+", "clip_length", "*", "steps", "<=", "duration", "+", "1", ":", "\n", "                ", "flow_clip", "=", "flows", "[", "idx", "-", "1", ":", "idx", "+", "clip_length", "*", "steps", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "break", "\n", "\n", "", "flows_u", "=", "list", "(", "[", "flow", "[", ":", ",", ":", ",", "0", "]", ".", "astype", "(", "np", ".", "float32", ")", "for", "flow", "in", "flow_clip", "]", ")", "\n", "flows_v", "=", "list", "(", "[", "flow", "[", ":", ",", ":", ",", "1", "]", ".", "astype", "(", "np", ".", "float32", ")", "for", "flow", "in", "flow_clip", "]", ")", "\n", "\n", "_", ",", "_", ",", "mb_x_u", ",", "mb_y_u", "=", "compute_motion_boudary", "(", "flows_u", ")", "\n", "_", ",", "_", ",", "mb_x_v", ",", "mb_y_v", "=", "compute_motion_boudary", "(", "flows_v", ")", "\n", "\n", "clip_mag_u", ",", "_", "=", "cv2", ".", "cartToPolar", "(", "mb_x_u", ",", "mb_y_u", ",", "angleInDegrees", "=", "True", ")", "\n", "clip_mag_v", ",", "_", "=", "cv2", ".", "cartToPolar", "(", "mb_x_v", ",", "mb_y_v", ",", "angleInDegrees", "=", "True", ")", "\n", "clip_mag", "=", "(", "clip_mag_u", "+", "clip_mag_v", ")", "/", "2", "\n", "\n", "# zero boundary", "\n", "clip_mag", "=", "zero_boundary", "(", "clip_mag", ")", ".", "mean", "(", ")", "\n", "clip_mag_list", ".", "append", "(", "clip_mag", ")", "\n", "\n", "", "return", "vid_names", ",", "clip_mag_list", "\n", "\n", "", "video_clip_mag_dic", "=", "{", "}", "\n", "data", "=", "Parallel", "(", "n_jobs", "=", "workers", ")", "(", "delayed", "(", "make_video_clip_mag", ")", "(", "vp", ")", "for", "vp", "in", "tqdm", "(", "videos", ",", "total", "=", "len", "(", "videos", ")", ")", ")", "\n", "\n", "for", "vid_names", ",", "l", "in", "data", ":", "\n", "        ", "video_clip_mag_dic", "[", "vid_names", "]", "=", "l", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "dst_path", ",", "'video_clip_mag_{}.pickle'", ".", "format", "(", "dataset", ")", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "video_clip_mag_dic", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.process_data.create_lmdb.parse_option": [[225, 241], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "", "def", "parse_option", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'training'", ")", "\n", "\n", "# dataset", "\n", "parser", ".", "add_argument", "(", "'--root-path'", ",", "type", "=", "str", ",", "default", "=", "'/lr/export2/home/zhangyiheng8/dataset/UCF-101/UCF-101-flow-lmdb240'", ",", "help", "=", "'path of original data'", ")", "\n", "parser", ".", "add_argument", "(", "'--dst-path'", ",", "type", "=", "str", ",", "default", "=", "'/dev/shm/UCF-101-flow-lmdb240'", ",", "help", "=", "'path to store generated data'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "type", "=", "str", ",", "default", "=", "'ucf101'", ",", "choices", "=", "[", "'kinetics'", ",", "'ucf101'", "]", ",", "help", "=", "'dataset to training'", ")", "\n", "parser", ".", "add_argument", "(", "'--data-type'", ",", "type", "=", "str", ",", "default", "=", "'mag'", ",", "choices", "=", "[", "'rgb'", ",", "'flow'", ",", "'mag'", ",", "'clip-mag'", "]", ",", "help", "=", "'which data'", ")", "\n", "parser", ".", "add_argument", "(", "'--video-type'", ",", "type", "=", "str", ",", "default", "=", "'mp4'", ",", "choices", "=", "[", "'mp4'", ",", "'avi'", "]", ",", "help", "=", "'which data'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-workers'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'num of workers to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-length'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'num of clip length'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'num of sampling steps'", ")", "\n", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.encoder.Encoder.__init__": [[8, 23], ["torch.Module.__init__", "model.backbone.p3da_resnet50", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "model.head.MLP_Head", "model.head.MLP_Head", "model.head.MLP_Head", "model.head.Classfier_Head", "model.head.Classfier_Head"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_resnet50"], ["    ", "def", "__init__", "(", "self", ",", "num_channels", "=", "128", ",", "mlp_layers", "=", "2", ",", "order", "=", "False", ",", "classfier", "=", "False", ",", "num_classes", "=", "400", ")", ":", "\n", "        ", "super", "(", "Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "backbone", "=", "backbone", "(", ")", "\n", "self", ".", "feature_size", "=", "self", ".", "backbone", ".", "feature_size", "\n", "self", ".", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "nce_inter_head", "=", "MLP_Head", "(", "mlp_layers", ",", "self", ".", "feature_size", ",", "num_channels", ")", "\n", "self", ".", "nce_intra_head", "=", "MLP_Head", "(", "mlp_layers", ",", "self", ".", "feature_size", ",", "num_channels", ")", "\n", "\n", "if", "order", ":", "\n", "            ", "self", ".", "order_head", "=", "MLP_Head", "(", "mlp_layers", ",", "self", ".", "feature_size", ",", "num_channels", ")", "\n", "self", ".", "order_classfier", "=", "Classfier_Head", "(", "num_channels", "*", "3", ",", "2", ")", "\n", "\n", "", "if", "classfier", ":", "\n", "            ", "self", ".", "classfier", "=", "Classfier_Head", "(", "self", ".", "feature_size", ",", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.encoder.Encoder.forward": [[25, 44], ["encoder.Encoder.backbone", "encoder.Encoder.forward_order_classfier", "out_list.extend", "encoder.Encoder.forward_nce_order", "len", "encoder.Encoder.forward_classfier"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.encoder.Encoder.forward_order_classfier", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.encoder.Encoder.forward_nce_order", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.encoder.Encoder.forward_classfier"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "mode", "=", "'all'", ")", ":", "\n", "\n", "        ", "\"\"\" mode = {'all', 'classfier_order', 'eval', 'classfier'} \"\"\"", "\n", "\n", "# without backbone forward", "\n", "if", "mode", "==", "'classfier_order'", ":", "\n", "            ", "return", "self", ".", "forward_order_classfier", "(", "x", ")", "\n", "\n", "# with backbone forward", "\n", "", "out_list", "=", "[", "]", "\n", "backbone_out", "=", "self", ".", "backbone", "(", "x", ",", "layer", "=", "5", ")", "\n", "if", "mode", "==", "'all'", ":", "\n", "            ", "out_list", ".", "extend", "(", "self", ".", "forward_nce_order", "(", "backbone_out", ")", ")", "\n", "", "elif", "mode", "==", "'eval'", ":", "\n", "            ", "return", "backbone_out", "\n", "", "elif", "mode", "==", "'classfier'", ":", "\n", "            ", "return", "self", ".", "forward_classfier", "(", "backbone_out", ")", "\n", "\n", "", "return", "out_list", "if", "len", "(", "out_list", ")", ">", "1", "else", "out_list", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.encoder.Encoder.forward_nce_order": [[45, 50], ["encoder.Encoder.avg_pool().view", "torch.functional.normalize", "torch.functional.normalize", "torch.functional.normalize", "torch.functional.normalize", "torch.functional.normalize", "torch.functional.normalize", "encoder.Encoder.avg_pool", "encoder.Encoder.nce_inter_head", "encoder.Encoder.nce_intra_head", "encoder.Encoder.order_head"], "methods", ["None"], ["", "def", "forward_nce_order", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "avg_pool", "(", "x", ")", ".", "view", "(", "-", "1", ",", "self", ".", "feature_size", ")", "\n", "return", "nn", ".", "functional", ".", "normalize", "(", "self", ".", "nce_inter_head", "(", "x", ")", ",", "p", "=", "2", ",", "dim", "=", "1", ")", ",", "nn", ".", "functional", ".", "normalize", "(", "self", ".", "nce_intra_head", "(", "x", ")", ",", "p", "=", "2", ",", "dim", "=", "1", ")", ",", "nn", ".", "functional", ".", "normalize", "(", "self", ".", "order_head", "(", "x", ")", ",", "p", "=", "2", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.encoder.Encoder.forward_order_classfier": [[51, 53], ["encoder.Encoder.order_classfier"], "methods", ["None"], ["", "def", "forward_order_classfier", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "order_classfier", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.encoder.Encoder.forward_classfier": [[54, 57], ["encoder.Encoder.avg_pool().view", "encoder.Encoder.classfier", "encoder.Encoder.avg_pool"], "methods", ["None"], ["", "def", "forward_classfier", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "avg_pool", "(", "x", ")", ".", "view", "(", "-", "1", ",", "self", ".", "feature_size", ")", "\n", "return", "self", ".", "classfier", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.head.MLP_Head.__init__": [[5, 16], ["torch.Module.__init__", "range", "torch.Sequential", "torch.Sequential", "linear_list.append", "linear_list.append", "linear_list.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__"], ["    ", "def", "__init__", "(", "self", ",", "layers", ",", "input_channel", ",", "output_channel", ")", ":", "\n", "        ", "super", "(", "MLP_Head", ",", "self", ")", ".", "__init__", "(", ")", "\n", "linear_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "layers", ")", ":", "\n", "            ", "if", "i", "==", "layers", "-", "1", ":", "\n", "                ", "linear_list", ".", "append", "(", "nn", ".", "Linear", "(", "input_channel", ",", "output_channel", ")", ")", "\n", "", "else", ":", "\n", "                ", "linear_list", ".", "append", "(", "nn", ".", "Linear", "(", "input_channel", ",", "input_channel", ")", ")", "\n", "linear_list", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "\n", "", "", "self", ".", "head", "=", "nn", ".", "Sequential", "(", "*", "linear_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.head.MLP_Head.forward": [[18, 20], ["head.MLP_Head.head"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "self", ".", "head", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.head.Classfier_Head.__init__": [[22, 27], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_channel", ",", "num_classes", ",", "dropout_ratio", "=", "0.0", ",", "add_norm", "=", "True", ")", ":", "\n", "        ", "super", "(", "Classfier_Head", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "input_channel", ",", "num_classes", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout_ratio", ")", "\n", "self", ".", "add_norm", "=", "add_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.head.Classfier_Head.forward": [[28, 32], ["head.Classfier_Head.dropout", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "head.Classfier_Head.fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "if", "self", ".", "add_norm", ":", "\n", "            ", "input", "=", "torch", ".", "nn", ".", "functional", ".", "normalize", "(", "input", ",", "dim", "=", "-", "1", ")", "\n", "", "return", "self", ".", "dropout", "(", "self", ".", "fc", "(", "input", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.mal.MAL.__init__": [[6, 11], ["torch.Module.__init__", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.Softmax", "torch.Softmax", "torch.Softmax"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "MAL", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "avgpool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "4", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.mal.MAL.alignment": [[12, 31], ["gradient.size", "mal.MAL.avgpool", "mal.MAL.relu", "mal.MAL.forward_spatial_temporal", "mal.MAL.forward_temporal", "mal.MAL.forward_spatial", "weight_predict.sum"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.mal.MAL.forward_spatial_temporal", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.mal.MAL.forward_temporal", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.mal.MAL.forward_spatial"], ["", "def", "alignment", "(", "self", ",", "gradient", ",", "xconv", ",", "target", ")", ":", "\n", "\n", "        ", "bsz", "=", "gradient", ".", "size", "(", "0", ")", "\n", "weight", "=", "self", ".", "avgpool", "(", "gradient", ")", "\n", "\n", "weight_predict", "=", "weight", "*", "gradient", "\n", "\n", "predict", "=", "self", ".", "relu", "(", "weight_predict", ".", "sum", "(", "1", ")", ")", "\n", "\n", "# for spatial-temporal alignment", "\n", "sim_spa_temp", "=", "self", ".", "forward_spatial_temporal", "(", "predict", ",", "target", ",", "bsz", ")", "\n", "\n", "# for temporal alignment", "\n", "sim_tempo", "=", "self", ".", "forward_temporal", "(", "predict", ",", "target", ",", "bsz", ")", "\n", "\n", "# for spa alignment", "\n", "sim_spa", "=", "self", ".", "forward_spatial", "(", "predict", ",", "target", ",", "bsz", ")", "\n", "\n", "return", "sim_spa_temp", ",", "sim_spa", ",", "sim_tempo", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.mal.MAL.forward_spatial_temporal": [[32, 41], ["target.reshape().sum", "mal.MAL.softmax", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "predict.reshape", "target.reshape", "target.reshape"], "methods", ["None"], ["", "def", "forward_spatial_temporal", "(", "self", ",", "predict", ",", "target", ",", "bsz", ")", ":", "\n", "\n", "        ", "target_score", "=", "target", ".", "reshape", "(", "bsz", ",", "4", ",", "-", "1", ")", ".", "sum", "(", "-", "1", ")", "\n", "target_att", "=", "self", ".", "softmax", "(", "target_score", ")", "\n", "pre_norm", "=", "F", ".", "normalize", "(", "predict", ".", "reshape", "(", "bsz", ",", "4", ",", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "target_norm", "=", "F", ".", "normalize", "(", "target", ".", "reshape", "(", "bsz", ",", "4", ",", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "sim", "=", "(", "pre_norm", "*", "target_norm", ")", ".", "sum", "(", "-", "1", ")", "\n", "sim_att", "=", "(", "sim", "*", "target_att", ")", ".", "sum", "(", "-", "1", ")", "\n", "return", "sim_att", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.mal.MAL.forward_temporal": [[42, 49], ["predict.reshape().sum", "target.reshape().sum", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "predict.reshape", "target.reshape"], "methods", ["None"], ["", "def", "forward_temporal", "(", "self", ",", "predict", ",", "target", ",", "bsz", ")", ":", "\n", "        ", "pre_score", "=", "predict", ".", "reshape", "(", "bsz", ",", "4", ",", "-", "1", ")", ".", "sum", "(", "-", "1", ")", "\n", "motion_score", "=", "target", ".", "reshape", "(", "bsz", ",", "4", ",", "-", "1", ")", ".", "sum", "(", "-", "1", ")", "\n", "pre_norm", "=", "F", ".", "normalize", "(", "pre_score", ",", "dim", "=", "-", "1", ")", "\n", "target_norm", "=", "F", ".", "normalize", "(", "motion_score", ",", "dim", "=", "-", "1", ")", "\n", "sim", "=", "(", "pre_norm", "*", "target_norm", ")", ".", "sum", "(", "-", "1", ")", "\n", "return", "sim", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.mal.MAL.forward_spatial": [[50, 55], ["torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "predict.reshape().mean", "target.reshape().mean", "predict.reshape", "target.reshape"], "methods", ["None"], ["", "def", "forward_spatial", "(", "self", ",", "predict", ",", "target", ",", "bsz", ")", ":", "\n", "        ", "pre_norm", "=", "F", ".", "normalize", "(", "predict", ".", "reshape", "(", "bsz", ",", "4", ",", "-", "1", ")", ".", "mean", "(", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "target_norm", "=", "F", ".", "normalize", "(", "target", ".", "reshape", "(", "bsz", ",", "4", ",", "-", "1", ")", ".", "mean", "(", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "sim", "=", "(", "pre_norm", "*", "target_norm", ")", ".", "sum", "(", "-", "1", ")", "\n", "return", "sim", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.mal.MAL.forward": [[56, 68], ["mal.MAL.alignment"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.mal.MAL.alignment"], ["", "def", "forward", "(", "self", ",", "gradient", ",", "xconv", ",", "target", ")", ":", "\n", "\n", "        ", "sim_spa_temp", ",", "sim_spa", ",", "sim_tempo", "=", "self", ".", "alignment", "(", "gradient", ",", "xconv", ",", "target", ")", "\n", "\n", "# cosine similarity equal to l2-normalized mse", "\n", "loss_spa_temp", "=", "(", "1", "-", "sim_spa_temp", ")", ".", "mean", "(", ")", "\n", "loss_tempo", "=", "(", "1", "-", "sim_tempo", ")", ".", "mean", "(", ")", "\n", "loss_spa", "=", "(", "1", "-", "sim_spa", ")", ".", "mean", "(", ")", "\n", "\n", "loss_mal", "=", "loss_spa_temp", "+", "loss_spa", "+", "loss_tempo", "\n", "\n", "return", "loss_mal", "", "", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.BasicBlock.__init__": [[31, 50], ["torch.Module.__init__", "backbone.conv1x3x3", "backbone.conv3x1x1", "norm_layer", "torch.ReLU", "torch.ReLU", "backbone.conv1x3x3", "backbone.conv3x1x1", "norm_layer", "ValueError", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.conv1x3x3", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.conv3x1x1", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.conv1x3x3", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.conv3x1x1"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "groups", "=", "1", ",", "\n", "base_width", "=", "64", ",", "dilation", "=", "1", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "BasicBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "nn", ".", "BatchNorm3d", "\n", "", "if", "groups", "!=", "1", "or", "base_width", "!=", "64", ":", "\n", "            ", "raise", "ValueError", "(", "'BasicBlock only supports groups=1 and base_width=64'", ")", "\n", "", "if", "dilation", ">", "1", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Dilation > 1 not supported in BasicBlock\"", ")", "\n", "# Both self.conv1 and self.downsample layers downsample the input when stride != 1", "\n", "", "self", ".", "conv1", "=", "conv1x3x3", "(", "inplanes", ",", "planes", ",", "stride", ")", "\n", "self", ".", "conv1_t", "=", "conv3x1x1", "(", "planes", ",", "planes", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "planes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv2", "=", "conv1x3x3", "(", "planes", ",", "planes", ")", "\n", "self", ".", "conv2_t", "=", "conv3x1x1", "(", "planes", ",", "planes", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "planes", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.BasicBlock.forward": [[51, 70], ["backbone.BasicBlock.conv1", "backbone.BasicBlock.conv1_t", "backbone.BasicBlock.bn1", "backbone.BasicBlock.relu", "backbone.BasicBlock.conv2", "backbone.BasicBlock.conv2_t", "backbone.BasicBlock.bn2", "backbone.BasicBlock.relu", "backbone.BasicBlock.downsample"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "conv1_t", "(", "out", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "conv2_t", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "identity", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.Bottleneck.__init__": [[76, 93], ["torch.Module.__init__", "backbone.conv1x1x1", "norm_layer", "backbone.conv1x3x3", "backbone.conv3x1x1", "norm_layer", "backbone.conv1x1x1", "norm_layer", "torch.ReLU", "torch.ReLU", "int"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.conv1x1x1", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.conv1x3x3", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.conv3x1x1", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.conv1x1x1"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "groups", "=", "1", ",", "\n", "base_width", "=", "64", ",", "dilation", "=", "1", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "Bottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "nn", ".", "BatchNorm3d", "\n", "", "width", "=", "int", "(", "planes", "*", "(", "base_width", "/", "64.", ")", ")", "*", "groups", "\n", "# Both self.conv2 and self.downsample layers downsample the input when stride != 1", "\n", "self", ".", "conv1", "=", "conv1x1x1", "(", "inplanes", ",", "width", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "width", ")", "\n", "self", ".", "conv2", "=", "conv1x3x3", "(", "width", ",", "width", ",", "stride", ",", "groups", ",", "dilation", ")", "\n", "self", ".", "conv2_t", "=", "conv3x1x1", "(", "width", ",", "width", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "width", ")", "\n", "self", ".", "conv3", "=", "conv1x1x1", "(", "width", ",", "planes", "*", "self", ".", "expansion", ")", "\n", "self", ".", "bn3", "=", "norm_layer", "(", "planes", "*", "self", ".", "expansion", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.Bottleneck.forward": [[94, 116], ["backbone.Bottleneck.conv1", "backbone.Bottleneck.bn1", "backbone.Bottleneck.relu", "backbone.Bottleneck.conv2", "backbone.Bottleneck.conv2_t", "backbone.Bottleneck.bn2", "backbone.Bottleneck.relu", "backbone.Bottleneck.conv3", "backbone.Bottleneck.bn3", "backbone.Bottleneck.relu", "backbone.Bottleneck.downsample"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "conv2_t", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "bn3", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "identity", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.P3DA_ResNet.__init__": [[120, 190], ["torch.Module.__init__", "torch.MaxPool3d", "torch.MaxPool3d", "backbone.P3DA_ResNet._make_layer", "backbone.P3DA_ResNet._make_layer", "backbone.P3DA_ResNet._make_layer", "backbone.P3DA_ResNet._make_layer", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "backbone.P3DA_ResNet.modules", "len", "ValueError", "torch.Conv3d", "torch.Conv3d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.Conv3d", "torch.Conv3d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.Conv3d", "torch.Conv3d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.Conv3d", "torch.Conv3d", "norm_layer", "torch.ReLU", "torch.ReLU", "isinstance", "backbone.P3DA_ResNet.modules", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "isinstance", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.P3DA_ResNet._make_layer", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.P3DA_ResNet._make_layer", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.P3DA_ResNet._make_layer", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.P3DA_ResNet._make_layer"], ["    ", "def", "__init__", "(", "self", ",", "block", ",", "layers", ",", "num_classes", "=", "400", ",", "dropout_ratio", "=", "0.5", ",", "zero_init_residual", "=", "False", ",", "\n", "groups", "=", "1", ",", "width_per_group", "=", "64", ",", "replace_stride_with_dilation", "=", "None", ",", "\n", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "P3DA_ResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "nn", ".", "BatchNorm3d", "\n", "", "self", ".", "_norm_layer", "=", "norm_layer", "\n", "self", ".", "feature_size", "=", "512", "*", "block", ".", "expansion", "\n", "self", ".", "inplanes", "=", "64", "\n", "self", ".", "dilation", "=", "1", "\n", "if", "replace_stride_with_dilation", "is", "None", ":", "\n", "# each element in the tuple indicates if we should replace", "\n", "# the 2x2 stride with a dilated convolution instead", "\n", "            ", "replace_stride_with_dilation", "=", "[", "False", ",", "False", ",", "False", "]", "\n", "", "if", "len", "(", "replace_stride_with_dilation", ")", "!=", "3", ":", "\n", "            ", "raise", "ValueError", "(", "\"replace_stride_with_dilation should be None \"", "\n", "\"or a 3-element tuple, got {}\"", ".", "format", "(", "replace_stride_with_dilation", ")", ")", "\n", "", "self", ".", "groups", "=", "groups", "\n", "self", ".", "base_width", "=", "width_per_group", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv3d", "(", "3", ",", "self", ".", "inplanes", ",", "kernel_size", "=", "[", "4", ",", "7", ",", "7", "]", ",", "stride", "=", "[", "4", ",", "2", ",", "2", "]", ",", "padding", "=", "[", "0", ",", "3", ",", "3", "]", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "self", ".", "inplanes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "maxpool", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "[", "1", ",", "3", ",", "3", "]", ",", "stride", "=", "[", "1", ",", "2", ",", "2", "]", ",", "padding", "=", "[", "0", ",", "1", ",", "1", "]", ")", "\n", "self", ".", "layer1", "=", "self", ".", "_make_layer", "(", "block", ",", "64", ",", "layers", "[", "0", "]", ")", "\n", "self", ".", "layer2", "=", "self", ".", "_make_layer", "(", "block", ",", "128", ",", "layers", "[", "1", "]", ",", "stride", "=", "2", ",", "\n", "dilate", "=", "replace_stride_with_dilation", "[", "0", "]", ")", "\n", "self", ".", "layer3", "=", "self", ".", "_make_layer", "(", "block", ",", "256", ",", "layers", "[", "2", "]", ",", "stride", "=", "2", ",", "\n", "dilate", "=", "replace_stride_with_dilation", "[", "1", "]", ")", "\n", "self", ".", "layer4", "=", "self", ".", "_make_layer", "(", "block", ",", "512", ",", "layers", "[", "3", "]", ",", "stride", "=", "2", ",", "\n", "dilate", "=", "replace_stride_with_dilation", "[", "2", "]", ")", "\n", "self", ".", "avgpool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "\n", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "(", "nn", ".", "BatchNorm3d", ",", "nn", ".", "GroupNorm", ")", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n", "# Zero-initialize the last BN in each residual branch,", "\n", "# so that the residual branch starts with zeros, and each residual block behaves like an identity.", "\n", "# This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677", "\n", "", "", "if", "zero_init_residual", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "Bottleneck", ")", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bn3", ".", "weight", ",", "0", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "BasicBlock", ")", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bn2", ".", "weight", ",", "0", ")", "\n", "\n", "", "", "", "", "def", "_make_layer", "(", "self", ",", "block", ",", "planes", ",", "blocks", ",", "stride", "=", "1", ",", "dilate", "=", "False", ")", ":", "\n", "        ", "norm_layer", "=", "self", ".", "_norm_layer", "\n", "downsample", "=", "None", "\n", "previous_dilation", "=", "self", ".", "dilation", "\n", "if", "dilate", ":", "\n", "            ", "self", ".", "dilation", "*=", "stride", "\n", "stride", "=", "1", "\n", "", "if", "stride", "!=", "1", "or", "self", ".", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "nn", ".", "Sequential", "(", "\n", "conv1x1x1", "(", "self", ".", "inplanes", ",", "planes", "*", "block", ".", "expansion", ",", "stride", ")", ",", "\n", "norm_layer", "(", "planes", "*", "block", ".", "expansion", ")", ",", "\n", ")", "\n", "\n", "", "layers", "=", "[", "]", "\n", "layers", ".", "append", "(", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "stride", ",", "downsample", ",", "self", ".", "groups", ",", "\n", "self", ".", "base_width", ",", "previous_dilation", ",", "norm_layer", ")", ")", "\n", "self", ".", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "_", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "groups", "=", "self", ".", "groups", ",", "\n", "base_width", "=", "self", ".", "base_width", ",", "dilation", "=", "self", ".", "dilation", ",", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.P3DA_ResNet._make_layer": [[191, 214], ["layers.append", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "block", "layers.append", "backbone.conv1x1x1", "norm_layer", "block"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.conv1x1x1"], ["norm_layer", "=", "norm_layer", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ",", "layer", "=", "6", ")", ":", "\n", "# See note [TorchScript super()]", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "maxpool", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "layer1", "(", "x", ")", "\n", "x", "=", "self", ".", "layer2", "(", "x", ")", "\n", "x", "=", "self", ".", "layer3", "(", "x", ")", "\n", "x", "=", "self", ".", "layer4", "(", "x", ")", "\n", "if", "layer", "==", "5", ":", "\n", "            ", "return", "x", "\n", "", "x", "=", "self", ".", "avgpool", "(", "x", ")", "\n", "x_g", "=", "torch", ".", "flatten", "(", "x", ",", "1", ")", "\n", "\n", "return", "x_g", "\n", "\n", "", "", "def", "_p3da_resnet", "(", "arch", ",", "block", ",", "layers", ",", "pretrained", ",", "progress", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "P3DA_ResNet", "(", "block", ",", "layers", ",", "**", "kwargs", ")", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.P3DA_ResNet.forward": [[250, 252], ["backbone.P3DA_ResNet._forward_impl"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.P3DA_ResNet._forward_impl"], ["return", "_p3da_resnet", "(", "'resnet50'", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "pretrained", ",", "progress", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.conv1x3x3": [[10, 14], ["torch.Conv3d"], "function", ["None"], ["def", "conv1x3x3", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ",", "groups", "=", "1", ",", "dilation", "=", "1", ")", ":", "\n", "    ", "\"\"\"1x3x3 convolution with padding\"\"\"", "\n", "return", "nn", ".", "Conv3d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "[", "1", ",", "3", ",", "3", "]", ",", "stride", "=", "[", "1", ",", "stride", ",", "stride", "]", ",", "\n", "padding", "=", "[", "0", ",", "dilation", ",", "dilation", "]", ",", "groups", "=", "groups", ",", "bias", "=", "False", ",", "dilation", "=", "[", "1", ",", "dilation", ",", "dilation", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.conv3x1x1": [[16, 20], ["torch.Conv3d"], "function", ["None"], ["", "def", "conv3x1x1", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ",", "groups", "=", "1", ",", "dilation", "=", "1", ")", ":", "\n", "    ", "\"\"\"3x1x1 convolution with padding\"\"\"", "\n", "return", "nn", ".", "Conv3d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "[", "3", ",", "1", ",", "1", "]", ",", "stride", "=", "[", "stride", ",", "1", ",", "1", "]", ",", "\n", "padding", "=", "[", "dilation", ",", "0", ",", "0", "]", ",", "groups", "=", "groups", ",", "bias", "=", "False", ",", "dilation", "=", "[", "dilation", ",", "1", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.conv1x1x1": [[22, 25], ["torch.Conv3d"], "function", ["None"], ["", "def", "conv1x1x1", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ")", ":", "\n", "    ", "\"\"\"1x1x1 convolution\"\"\"", "\n", "return", "nn", ".", "Conv3d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "1", ",", "stride", "=", "[", "1", ",", "stride", ",", "stride", "]", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone._p3da_resnet": [[254, 257], ["backbone.P3DA_ResNet"], "function", ["None"], ["", "def", "p3da_resnet101", "(", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_resnet18": [[259, 268], ["backbone._p3da_resnet"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone._p3da_resnet"], ["\n", "return", "_p3da_resnet", "(", "'resnet101'", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "pretrained", ",", "progress", ",", "\n", "**", "kwargs", ")", "\n", "\n", "\n", "", "def", "p3da_resnet152", "(", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_resnet34": [[270, 279], ["backbone._p3da_resnet"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone._p3da_resnet"], ["\n", "return", "_p3da_resnet", "(", "'resnet152'", ",", "Bottleneck", ",", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "pretrained", ",", "progress", ",", "\n", "**", "kwargs", ")", "\n", "\n", "\n", "", "def", "p3da_resnext50_32x4d", "(", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_resnet50": [[281, 290], ["backbone._p3da_resnet"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone._p3da_resnet"], ["\n", "kwargs", "[", "'groups'", "]", "=", "32", "\n", "kwargs", "[", "'width_per_group'", "]", "=", "4", "\n", "return", "_p3da_resnet", "(", "'resnext50_32x4d'", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "pretrained", ",", "progress", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_resnet101": [[292, 301], ["backbone._p3da_resnet"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone._p3da_resnet"], ["", "def", "p3da_resnext101_32x8d", "(", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"ResNeXt-101 32x8d model from\n    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n", "kwargs", "[", "'groups'", "]", "=", "32", "\n", "kwargs", "[", "'width_per_group'", "]", "=", "8", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_resnet152": [[314, 323], ["backbone._p3da_resnet"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone._p3da_resnet"], ["\n", "kwargs", "[", "'width_per_group'", "]", "=", "64", "*", "2", "\n", "return", "_p3da_resnet", "(", "'wide_resnet50_2'", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "pretrained", ",", "progress", ",", "**", "kwargs", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_resnext50_32x4d": [[325, 336], ["backbone._p3da_resnet"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone._p3da_resnet"], ["    ", "r\"\"\"Wide ResNet-101-2 model from\n    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_resnext101_32x8d": [[338, 349], ["backbone._p3da_resnet"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone._p3da_resnet"], ["return", "_p3da_resnet", "(", "'wide_resnet101_2'", ",", "Bottleneck", ",", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "\n", "pretrained", ",", "progress", ",", "**", "kwargs", ")", "\n", "\n", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "model", "=", "p3da_resnet50", "(", ")", "\n", "print", "(", "model", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_wide_resnet50_2": [[351, 366], ["backbone._p3da_resnet"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone._p3da_resnet"], []], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_wide_resnet101_2": [[368, 383], ["backbone._p3da_resnet"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone._p3da_resnet"], []], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.P3DA_ResNet._forward_impl": [[215, 249], ["backbone.P3DA_ResNet.maxpool", "backbone.P3DA_ResNet.layer1", "backbone.P3DA_ResNet.layer2", "backbone.P3DA_ResNet.layer3", "backbone.P3DA_ResNet.layer4", "backbone.P3DA_ResNet.pool", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "backbone.P3DA_ResNet.drop", "backbone.P3DA_ResNet.fc", "backbone.P3DA_ResNet.conv1", "backbone.P3DA_ResNet.bn1", "backbone.P3DA_ResNet.relu", "backbone.P3DA_ResNet.conv1", "backbone.P3DA_ResNet.bn1", "backbone.P3DA_ResNet.relu1", "backbone.P3DA_ResNet.conv2", "backbone.P3DA_ResNet.bn2", "backbone.P3DA_ResNet.relu2", "backbone.P3DA_ResNet.conv3", "backbone.P3DA_ResNet.bn3", "backbone.P3DA_ResNet.relu3"], "methods", ["None"], ["return", "model", "\n", "\n", "\n", "", "def", "p3da_resnet18", "(", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"ResNet-18 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n", "return", "_p3da_resnet", "(", "'resnet18'", ",", "BasicBlock", ",", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "pretrained", ",", "progress", ",", "\n", "**", "kwargs", ")", "\n", "\n", "\n", "", "def", "p3da_resnet34", "(", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"ResNet-34 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n", "return", "_p3da_resnet", "(", "'resnet34'", ",", "BasicBlock", ",", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "pretrained", ",", "progress", ",", "\n", "**", "kwargs", ")", "\n", "\n", "\n", "", "def", "p3da_resnet50", "(", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "r\"\"\"ResNet-50 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_resnet103": [[303, 312], ["backbone._p3da_resnet"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone._p3da_resnet"], ["pretrained", ",", "progress", ",", "**", "kwargs", ")", "\n", "\n", "\n", "", "def", "p3da_wide_resnet50_2", "(", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.compute_motion_boudary": [[5, 28], ["numpy.array", "numpy.array", "numpy.array", "numpy.array", "scipy.ndimage.convolve", "scipy.ndimage.convolve", "np.array.append", "np.array.append"], "function", ["None"], ["def", "compute_motion_boudary", "(", "flow_clip", ")", ":", "\n", "\n", "    ", "mx", "=", "np", ".", "array", "(", "[", "[", "-", "1", ",", "0", ",", "1", "]", ",", "[", "-", "1", ",", "0", ",", "1", "]", ",", "[", "-", "1", ",", "0", ",", "1", "]", "]", ")", "\n", "my", "=", "np", ".", "array", "(", "[", "[", "-", "1", ",", "-", "1", ",", "-", "1", "]", ",", "[", "0", ",", "0", ",", "0", "]", ",", "[", "1", ",", "1", ",", "1", "]", "]", ")", "\n", "dx_all", "=", "[", "]", "\n", "dy_all", "=", "[", "]", "\n", "mb_x", "=", "0", "\n", "mb_y", "=", "0", "\n", "\n", "for", "flow_img", "in", "flow_clip", ":", "\n", "        ", "d_x", "=", "ndimage", ".", "convolve", "(", "flow_img", ",", "mx", ")", "\n", "d_y", "=", "ndimage", ".", "convolve", "(", "flow_img", ",", "my", ")", "\n", "\n", "dx_all", ".", "append", "(", "d_x", ")", "\n", "dy_all", ".", "append", "(", "d_y", ")", "\n", "\n", "mb_x", "+=", "d_x", "\n", "mb_y", "+=", "d_y", "\n", "\n", "", "dx_all", "=", "np", ".", "array", "(", "dx_all", ")", "\n", "dy_all", "=", "np", ".", "array", "(", "dy_all", ")", "\n", "\n", "return", "dx_all", ",", "dy_all", ",", "mb_x", ",", "mb_y", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.zero_boundary": [[29, 37], ["None"], "function", ["None"], ["", "def", "zero_boundary", "(", "frame_mag", ")", ":", "\n", "\n", "    ", "frame_mag", "[", ":", "8", ",", ":", "]", "=", "0", "\n", "frame_mag", "[", ":", ",", ":", "8", "]", "=", "0", "\n", "frame_mag", "[", "-", "8", ":", ",", ":", "]", "=", "0", "\n", "frame_mag", "[", ":", ",", "-", "8", ":", "]", "=", "0", "\n", "\n", "return", "frame_mag", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.motion_mag_downsample": [[38, 53], ["numpy.zeros", "range", "range", "numpy.mean"], "function", ["None"], ["", "def", "motion_mag_downsample", "(", "mag", ",", "size", ",", "input_size", ")", ":", "\n", "    ", "block_size", "=", "input_size", "//", "size", "\n", "mask", "=", "np", ".", "zeros", "(", "(", "size", ",", "size", ")", ")", "\n", "for", "i", "in", "range", "(", "size", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "size", ")", ":", "\n", "            ", "x_start", "=", "i", "*", "block_size", "\n", "x_end", "=", "x_start", "+", "block_size", "\n", "y_start", "=", "j", "*", "block_size", "\n", "y_end", "=", "y_start", "+", "block_size", "\n", "\n", "tmp_block", "=", "mag", "[", "x_start", ":", "x_end", ",", "y_start", ":", "y_end", "]", "\n", "\n", "block_mean", "=", "np", ".", "mean", "(", "tmp_block", ")", "\n", "mask", "[", "i", ",", "j", "]", "=", "block_mean", "\n", "", "", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.motion_sts": [[54, 61], ["motion_sts.compute_motion_boudary", "cv2.cartToPolar", "motion_sts.motion_mag_downsample"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.compute_motion_boudary", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.sts.motion_sts.motion_mag_downsample"], ["", "def", "motion_sts", "(", "flow_clip", ",", "size", ",", "input_size", ")", ":", "\n", "\n", "    ", "dx_all", ",", "dy_all", ",", "dx_sum", ",", "dy_sum", "=", "compute_motion_boudary", "(", "flow_clip", ")", "\n", "mag", ",", "ang", "=", "cv2", ".", "cartToPolar", "(", "dx_sum", ",", "dy_sum", ",", "angleInDegrees", "=", "True", ")", "\n", "mag_down", "=", "motion_mag_downsample", "(", "mag", ",", "size", ",", "input_size", ")", "\n", "\n", "return", "mag_down", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_svm_feature_perf.main": [[10, 82], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "range", "numpy.load", "numpy.load", "range", "numpy.concatenate().squeeze", "numpy.concatenate().squeeze", "print", "print", "range", "numpy.concatenate", "numpy.concatenate", "print", "print", "print", "evaluate.problem", "print", "evaluate.parameter", "print", "evaluate.train", "print", "evaluate.save_model", "print", "evaluate.predict", "evaluate.evaluations", "print", "print", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.join", "os.path.join", "np.concatenate().squeeze.append", "np.concatenate().squeeze.append", "range", "np.concatenate.append", "np.concatenate.append", "range", "os.path.join", "open", "f.write", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "numpy.load", "numpy.load", "numpy.concatenate", "numpy.concatenate", "numpy.load", "numpy.load", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.train", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinearutil.save_model", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinearutil.predict", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.commonutil.evaluations"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'svm_perf'", ")", "\n", "parser", ".", "add_argument", "(", "'--output-dir'", ",", "type", "=", "str", ",", "default", "=", "'output/eval_output_linear'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_replica'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--cost'", ",", "type", "=", "float", ",", "default", "=", "1.0", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# check feature files", "\n", "for", "i", "in", "range", "(", "args", ".", "num_replica", ")", ":", "\n", "        ", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'feature_train_{}.npy'", ".", "format", "(", "i", ")", ")", ")", "\n", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'feature_train_cls_{}.npy'", ".", "format", "(", "i", ")", ")", ")", "\n", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'feature_test_{}.npy'", ".", "format", "(", "i", ")", ")", ")", "\n", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'feature_test_cls_{}.npy'", ".", "format", "(", "i", ")", ")", ")", "\n", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'vid_num.npy'", ")", ")", "\n", "\n", "# load feature index", "\n", "", "vid_num_train", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'vid_num_train.npy'", ")", ")", "\n", "train_padding_num", "=", "vid_num_train", "[", "0", "]", "%", "args", ".", "num_replica", "\n", "vid_num_test", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'vid_num_test.npy'", ")", ")", "\n", "test_padding_num", "=", "vid_num_test", "[", "0", "]", "%", "args", ".", "num_replica", "\n", "\n", "# load feature and GT: training set", "\n", "feat_train", "=", "[", "]", "\n", "feat_train_cls", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "args", ".", "num_replica", ")", ":", "\n", "        ", "feat_train", ".", "append", "(", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'feature_train_{}.npy'", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "feat_train_cls", ".", "append", "(", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'feature_train_cls_{}.npy'", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "", "if", "train_padding_num", ">", "0", ":", "\n", "        ", "for", "i", "in", "range", "(", "train_padding_num", ",", "args", ".", "num_replica", ")", ":", "\n", "            ", "feat_train", "[", "i", "]", "=", "feat_train", "[", "i", "]", "[", ":", "-", "1", ",", ":", "]", "\n", "feat_train_cls", "[", "i", "]", "=", "feat_train_cls", "[", "i", "]", "[", ":", "-", "1", "]", "\n", "", "", "feat_train", "=", "np", ".", "concatenate", "(", "feat_train", ",", "axis", "=", "0", ")", ".", "squeeze", "(", ")", "\n", "feat_train_cls", "=", "np", ".", "concatenate", "(", "feat_train_cls", ",", "axis", "=", "0", ")", ".", "squeeze", "(", ")", "\n", "print", "(", "'feat_train: {}'", ".", "format", "(", "feat_train", ".", "shape", ")", ")", "\n", "print", "(", "'feat_train_cls: {}'", ".", "format", "(", "feat_train_cls", ".", "shape", ")", ")", "\n", "\n", "# load feature and GT: test set", "\n", "feat_test", "=", "[", "]", "\n", "feat_test_cls", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "args", ".", "num_replica", ")", ":", "\n", "        ", "feat_test", ".", "append", "(", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'feature_test_{}.npy'", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "feat_test_cls", ".", "append", "(", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'feature_test_cls_{}.npy'", ".", "format", "(", "i", ")", ")", ")", ")", "\n", "\n", "", "if", "test_padding_num", ">", "0", ":", "\n", "        ", "for", "i", "in", "range", "(", "test_padding_num", ",", "args", ".", "num_replica", ")", ":", "\n", "            ", "feat_test", "[", "i", "]", "=", "feat_test", "[", "i", "]", "[", ":", "-", "1", ",", ":", "]", "\n", "feat_test_cls", "[", "i", "]", "=", "feat_test_cls", "[", "i", "]", "[", ":", "-", "1", "]", "\n", "", "", "feat_test", "=", "np", ".", "concatenate", "(", "feat_test", ",", "axis", "=", "0", ")", "\n", "feat_test_cls", "=", "np", ".", "concatenate", "(", "feat_test_cls", ",", "axis", "=", "0", ")", "\n", "print", "(", "'feat_test: {}'", ".", "format", "(", "feat_test", ".", "shape", ")", ")", "\n", "print", "(", "'feat_test_cls: {}'", ".", "format", "(", "feat_test_cls", ".", "shape", ")", ")", "\n", "\n", "# solving SVM", "\n", "print", "(", "'form svm problem'", ")", "\n", "svm_problem", "=", "liblinearsvm", ".", "problem", "(", "feat_train_cls", ",", "feat_train", ")", "\n", "print", "(", "'L2-regularized L2-loss support vector classification (primal), cost={}'", ".", "format", "(", "args", ".", "cost", ")", ")", "\n", "svm_parameter", "=", "liblinearsvm", ".", "parameter", "(", "'-s 2 -n 32 -c {}'", ".", "format", "(", "args", ".", "cost", ")", ")", "\n", "svm_filename", "=", "'multicore_linearsvm_primal_c{}.svmmodel'", ".", "format", "(", "args", ".", "cost", ")", "\n", "\n", "print", "(", "'train svm'", ")", "\n", "svm_model", "=", "liblinearsvm", ".", "train", "(", "svm_problem", ",", "svm_parameter", ")", "\n", "print", "(", "'save svm'", ")", "\n", "liblinearsvm", ".", "save_model", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "svm_filename", ")", ",", "svm_model", ")", "\n", "print", "(", "'eval svm'", ")", "\n", "pd_label", ",", "pd_acc", ",", "pd_test", "=", "liblinearsvm", ".", "predict", "(", "feat_test_cls", ",", "feat_test", ",", "svm_model", ")", "\n", "eval_acc", ",", "eval_mse", ",", "eval_scc", "=", "liblinearsvm", ".", "evaluations", "(", "feat_test_cls", ",", "pd_label", ")", "\n", "print", "(", "'{}/{}'", ".", "format", "(", "pd_acc", ",", "eval_acc", ")", ")", "\n", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "svm_filename", "+", "'.txt'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "'{}/{}'", ".", "format", "(", "pd_acc", ",", "eval_acc", ")", ")", "\n", "", "print", "(", "'Done'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_fc.parse_option": [[26, 72], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "parse_option", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'fc test'", ")", "\n", "parser", ".", "add_argument", "(", "'--root-path'", ",", "type", "=", "str", ",", "default", "=", "'/lr/export/home/lirui295/dataset/Kinetics/Kinetics_frame256'", ",", "help", "=", "'root director of dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--list-path'", ",", "type", "=", "str", ",", "default", "=", "'data/kinetics/train_list_no_dup.csv'", ",", "help", "=", "'path of list file'", ")", "\n", "parser", ".", "add_argument", "(", "'--list-path-test'", ",", "type", "=", "str", ",", "default", "=", "'data/kinetics/val_list_no_dup.csv'", ",", "help", "=", "'path of list file'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained-model'", ",", "type", "=", "str", ",", "default", "=", "'output/checkpoints/current.pth'", ",", "help", "=", "\"pretrained model path\"", ")", "\n", "parser", ".", "add_argument", "(", "'--output-dir'", ",", "type", "=", "str", ",", "default", "=", "'output/eval_output_linear'", ",", "help", "=", "'output director'", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "help", "=", "'local rank for DistributedDataParallel'", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--time-dim'", ",", "type", "=", "str", ",", "default", "=", "'T'", ",", "\n", "choices", "=", "[", "\"T\"", ",", "\"C\"", "]", ",", "help", "=", "\"dimension for time\"", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-length'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'num of clip length'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "16", ",", "help", "=", "'num of sampling steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'num of batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size-test'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'num of batch size for evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--trans'", ",", "type", "=", "str", ",", "default", "=", "'0-5'", ",", "help", "=", "'select augmentations'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'num of training epoch'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-epoch'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'num of warmup epoch'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-multiplier'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'warmup multiplier'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "'weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-scheduler'", ",", "type", "=", "str", ",", "default", "=", "'cosine'", ",", "\n", "choices", "=", "[", "\"step\"", ",", "\"cosine\"", "]", ",", "help", "=", "\"learning rate scheduler\"", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-decay-epochs'", ",", "type", "=", "int", ",", "default", "=", "[", "60", ",", "80", "]", ",", "nargs", "=", "'+'", ",", "\n", "help", "=", "'for step scheduler. where to decay lr, can be a list'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-decay-rate'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'for step scheduler. decay rate for learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "help", "=", "'momentum for SGD'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'num of sampling steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-segments'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'num of segments'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-segments-test'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "help", "=", "'num of segments for evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-workers'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "help", "=", "'num of workers to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--print-freq'", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "'--save-freq'", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--resume'", ",", "type", "=", "str2bool", ",", "default", "=", "'false'", ")", "\n", "parser", ".", "add_argument", "(", "'--inflate-weights'", ",", "type", "=", "str2bool", ",", "default", "=", "'false'", ",", "help", "=", "'initialize from 2d backbone'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "type", "=", "str", ",", "default", "=", "'kinetics'", ",", "choices", "=", "[", "'kinetics'", ",", "'ucf101'", "]", ",", "\n", "help", "=", "'dataset to training'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "default", "=", "400", ",", "help", "=", "'num of classes'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "2560", ",", "help", "=", "'num of random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu-id'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--trainval'", ",", "type", "=", "str", ",", "default", "=", "'train'", ",", "help", "=", "'train or test'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_fc.load_pretrained": [[73, 85], ["torch.load", "torch.load", "torch.load", "torch.load", "model.load_state_dict", "logger.info", "logger.info", "logger.info", "k.replace", "k.replace", "ckpt[].items", "torch.load.items"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.load_state_dict"], ["", "def", "load_pretrained", "(", "args", ",", "model", ")", ":", "\n", "    ", "ckpt", "=", "torch", ".", "load", "(", "args", ".", "pretrained_model", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "if", "'model'", "in", "cpkt", ":", "\n", "        ", "state_dict", "=", "{", "k", ".", "replace", "(", "\"module.\"", ",", "\"\"", ")", ":", "v", "for", "k", ",", "v", "in", "ckpt", "[", "'model'", "]", ".", "items", "(", ")", "}", "\n", "", "else", ":", "\n", "        ", "state_dict", "=", "{", "k", ".", "replace", "(", "\"module.\"", ",", "\"\"", ")", ":", "v", "for", "k", ",", "v", "in", "ckpt", ".", "items", "(", ")", "}", "\n", "\n", "", "[", "misskeys", ",", "unexpkeys", "]", "=", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "False", ")", "\n", "logger", ".", "info", "(", "misskeys", ")", "\n", "logger", ".", "info", "(", "unexpkeys", ")", "\n", "logger", ".", "info", "(", "\"==> loaded checkpoint '{}' (epoch {})\"", ".", "format", "(", "args", ".", "pretrained_model", ",", "ckpt", "[", "'epoch'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_fc.get_loader": [[87, 147], ["args.trans.split", "T.extend", "dataset.augmentations.clip_transforms.Compose", "dataset.video_dataset.VideoRGBTrainDataset", "dataset.video_dataset.VideoRGBTestDataset", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataset.augmentations.clip_transforms.ClipRandomResizedCrop", "dataset.augmentations.clip_transforms.ClipResize", "torchvision.RandomApply", "dataset.augmentations.clip_transforms.ClipRandomGrayscale", "torchvision.RandomApply", "dataset.augmentations.clip_transforms.ClipRandomHorizontalFlip", "dataset.augmentations.clip_transforms.ToClipTensor", "dataset.augmentations.clip_transforms.ClipNormalize", "T.append", "dataset.augmentations.clip_transforms.Lambda", "torchvision.Lambda", "dataset.augmentations.clip_transforms.ClipColorJitter", "dataset.augmentations.clip_transforms.ClipGaussianBlur", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "int"], "function", ["None"], ["", "def", "get_loader", "(", "args", ",", "trainval", ")", ":", "\n", "\n", "    ", "select_transform", "=", "[", "\n", "clip_transforms", ".", "ClipRandomResizedCrop", "(", "224", ",", "scale", "=", "(", "0.2", ",", "1.", ")", ",", "ratio", "=", "(", "0.75", ",", "1.3333333333333333", ")", ")", ",", "\n", "clip_transforms", ".", "ClipResize", "(", "(", "224", ",", "224", ")", ")", ",", "\n", "transforms", ".", "RandomApply", "(", "[", "clip_transforms", ".", "ClipColorJitter", "(", "0.4", ",", "0.4", ",", "0.4", ",", "0.1", ")", "]", ",", "p", "=", "0.8", ")", ",", "\n", "clip_transforms", ".", "ClipRandomGrayscale", "(", "p", "=", "0.2", ")", ",", "\n", "transforms", ".", "RandomApply", "(", "[", "clip_transforms", ".", "ClipGaussianBlur", "(", "[", ".1", ",", "2.", "]", ")", "]", ",", "p", "=", "0.5", ")", ",", "\n", "clip_transforms", ".", "ClipRandomHorizontalFlip", "(", ")", ",", "\n", "]", "\n", "basic_transoform", "=", "[", "\n", "clip_transforms", ".", "ToClipTensor", "(", ")", ",", "\n", "clip_transforms", ".", "ClipNormalize", "(", "mean", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "std", "=", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", ",", "\n", "clip_transforms", ".", "Lambda", "(", "lambda", "clip", ":", "torch", ".", "stack", "(", "clip", ",", "dim", "=", "1", ")", ")", "if", "opt", ".", "time_dim", "==", "\"T\"", "else", "transforms", ".", "Lambda", "(", "\n", "lambda", "clip", ":", "torch", ".", "cat", "(", "clip", ",", "dim", "=", "0", ")", ")", "\n", "]", "\n", "\n", "T", "=", "[", "]", "\n", "for", "id", "in", "args", ".", "trans", ".", "split", "(", "'-'", ")", ":", "\n", "        ", "T", ".", "append", "(", "select_transform", "[", "int", "(", "id", ")", "]", ")", "\n", "", "T", ".", "extend", "(", "basic_transoform", ")", "\n", "data_transforms", "=", "clip_transforms", ".", "Compose", "(", "T", ")", "\n", "\n", "dataset", "=", "VideoRGBTrainDataset", "(", "\n", "list_root", "=", "args", ".", "list_path", ",", "\n", "root_path", "=", "args", ".", "root_path", ",", "\n", "transform", "=", "data_transforms", ",", "\n", "clip_length", "=", "args", ".", "clip_length", ",", "\n", "num_steps", "=", "args", ".", "num_steps", ",", "\n", "num_segments", "=", "args", ".", "num_segments", ",", "\n", "dataset", "=", "args", ".", "dataset", ",", "\n", "split", "=", "trainval", ",", "\n", "data_form", "=", "'lmdb'", "\n", ")", "\n", "\n", "dataset_test", "=", "VideoRGBTestDataset", "(", "\n", "list_root", "=", "args", ".", "list_path_test", ",", "\n", "root_path", "=", "args", ".", "root_path", ",", "\n", "transform", "=", "data_transforms", ",", "\n", "clip_length", "=", "args", ".", "clip_length", ",", "\n", "num_steps", "=", "args", ".", "num_steps", ",", "\n", "num_segments", "=", "args", ".", "num_segments_test", ",", "\n", "dataset", "=", "args", ".", "dataset", ",", "\n", "split", "=", "'test'", ",", "\n", "data_form", "=", "'lmdb'", "\n", ")", "\n", "\n", "\n", "sampler", "=", "torch", ".", "utils", ".", "data", ".", "distributed", ".", "DistributedSampler", "(", "dataset", ")", "\n", "loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "False", ",", "sampler", "=", "sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "pin_memory", "=", "False", ",", "\n", "drop_last", "=", "False", ")", "\n", "\n", "loader_test", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset_test", ",", "batch_size", "=", "args", ".", "batch_size_test", ",", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "pin_memory", "=", "False", ",", "\n", "drop_last", "=", "False", ")", "\n", "\n", "return", "loader", ",", "loader_test", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_fc.save_checkpoint": [[150, 164], ["logger.info", "os.path.join", "os.path.join", "os.makedirs", "os.makedirs", "torch.save", "torch.save", "torch.save", "torch.save", "model.state_dict", "optimizer.state_dict", "scheduler.state_dict", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict"], ["", "def", "save_checkpoint", "(", "args", ",", "epoch", ",", "model", ",", "scheduler", ",", "optimizer", ")", ":", "\n", "    ", "logger", ".", "info", "(", "'==> Saving...'", ")", "\n", "state", "=", "{", "\n", "'opt'", ":", "args", ",", "\n", "'model'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'scheduler'", ":", "scheduler", ".", "state_dict", "(", ")", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "}", "\n", "save_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'checkpoints'", ")", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "torch", ".", "save", "(", "state", ",", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'current.pth'", ")", ")", "\n", "if", "epoch", "%", "args", ".", "save_freq", "==", "0", ":", "\n", "        ", "torch", ".", "save", "(", "state", ",", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'ckpt_epoch_{}.pth'", ".", "format", "(", "epoch", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_fc.main": [[165, 228], ["eval_fc.get_loader", "model.encoder.Encoder", "torch.nn.parallel.DistributedDataParallel.named_parameters", "torch.nn.parallel.DistributedDataParallel.classfier.fc.weight.data.normal_", "torch.nn.parallel.DistributedDataParallel.classfier.fc.bias.data.zero_", "eval_fc.load_pretrained", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "seed_torch", "torch.nn.parallel.DistributedDataParallel.cuda", "torch.nn.parallel.DistributedDataParallel.cuda", "torch.nn.parallel.DistributedDataParallel", "torch.CrossEntropyLoss().cuda", "list", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "utils.lr_scheduler.get_scheduler", "range", "torch.get_rank", "eval_fc.validate", "logger.info", "filter", "len", "len", "torch.load", "torch.load", "torch.load", "torch.load", "utils.lr_scheduler.get_scheduler.load_state_dict", "torch.optim.SGD.load_state_dict", "time.time", "eval_fc.train", "logger.info", "logger.info", "torch.CrossEntropyLoss", "torch.nn.parallel.DistributedDataParallel.parameters", "torch.get_rank", "eval_fc.save_checkpoint", "time.time"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.get_loader", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.load_pretrained", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.seed_torch", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.get_scheduler", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_fc.validate", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.load_state_dict", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.load_state_dict", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.train", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.save_checkpoint"], ["", "", "def", "main", "(", "args", ",", "logger", ")", ":", "\n", "\n", "    ", "data_loader", ",", "data_loader_test", "=", "get_loader", "(", "opt", ",", "opt", ".", "trainval", ")", "\n", "\n", "model", "=", "Encoder", "(", "classfier", "=", "True", ",", "num_classes", "=", "args", ".", "num_classes", ")", "\n", "\n", "# freeze all layers but the last fc", "\n", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "name", "not", "in", "[", "'classfier.fc.weight'", ",", "'classfier.fc.bias'", "]", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "# init the fc layer", "\n", "", "", "model", ".", "classfier", ".", "fc", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "0.01", ")", "\n", "model", ".", "classfier", ".", "fc", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "load_pretrained", "(", "opt", ",", "model", ")", "\n", "\n", "if", "args", ".", "trainval", "==", "'test'", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "gpu_id", ")", "\n", "seed_torch", "(", "args", ".", "seed", ")", "\n", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "\n", "            ", "top1", ",", "top5", "=", "validate", "(", "args", ",", "model", ",", "data_loader_test", ")", "\n", "logger", ".", "info", "(", "'epoch {}, top1 acc:{} top5 acc: {}'", ".", "format", "(", "100", ",", "top1", ",", "top5", ")", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "model", "=", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "broadcast_buffers", "=", "False", ")", "\n", "\n", "# for loss", "\n", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "# optimize only the linear classifier", "\n", "parameters", "=", "list", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", ")", "\n", "assert", "len", "(", "parameters", ")", "==", "2", "# fc.weight, fc.bias", "\n", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "parameters", ",", "args", ".", "lr", ",", "\n", "momentum", "=", "args", ".", "momentum", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ")", "\n", "\n", "# for sch", "\n", "scheduler", "=", "get_scheduler", "(", "optimizer", ",", "len", "(", "data_loader", ")", ",", "args", ")", "\n", "\n", "if", "args", ".", "resume", ":", "\n", "            ", "ckpt", "=", "torch", ".", "load", "(", "args", ".", "pretrained_model", ",", "map_location", "=", "'cpu'", ")", "\n", "scheduler", ".", "load_state_dict", "(", "ckpt", "[", "'scheduler'", "]", ")", "\n", "optimizer", ".", "load_state_dict", "(", "ckpt", "[", "'optimizer'", "]", ")", "\n", "start_epochs", "=", "ckpt", "[", "'epoch'", "]", "+", "1", "\n", "", "else", ":", "\n", "            ", "start_epochs", "=", "1", "\n", "\n", "", "for", "epoch", "in", "range", "(", "start_epochs", ",", "args", ".", "epochs", "+", "1", ")", ":", "\n", "            ", "start", "=", "time", ".", "time", "(", ")", "\n", "loss", "=", "train", "(", "model", ",", "data_loader", ",", "criterion", ",", "optimizer", ",", "epoch", ",", "scheduler", ",", "logger", ",", "args", ")", "\n", "\n", "logger", ".", "info", "(", "'epoch {}, total time {:.2f}'", ".", "format", "(", "epoch", ",", "time", ".", "time", "(", ")", "-", "start", ")", ")", "\n", "logger", ".", "info", "(", "'epoch {}, average loss is {:>0.3f}'", ".", "format", "(", "epoch", ",", "loss", ")", ")", "\n", "\n", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "# save model", "\n", "                ", "save_checkpoint", "(", "args", ",", "epoch", ",", "model", ",", "scheduler", ",", "optimizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_fc.train": [[231, 262], ["model.eval", "AverageMeter", "enumerate", "data.cuda.size", "data.cuda.cuda", "label.cuda.cuda", "model", "criterion", "optimizer.zero_grad", "criterion.backward", "optimizer.step", "scheduler.step", "AverageMeter.update", "logger.info", "criterion.item", "len", "criterion.item"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.step", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.step", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update"], ["", "", "", "", "def", "train", "(", "model", ",", "data_loader", ",", "criterion", ",", "optimizer", ",", "epoch", ",", "scheduler", ",", "logger", ",", "args", ")", ":", "\n", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "loss_meter", "=", "AverageMeter", "(", ")", "\n", "\n", "for", "idx", ",", "(", "data", ",", "label", ",", "vname", ")", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "        ", "bsz", "=", "data", ".", "size", "(", "0", ")", "\n", "data", "=", "data", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "label", "=", "label", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "# compute output", "\n", "output", "=", "model", "(", "data", ",", "mode", "=", "'classfier'", ")", "\n", "loss", "=", "criterion", "(", "output", ",", "label", ")", "\n", "\n", "# compute gradient and do SGD step", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "\n", "\n", "if", "idx", "%", "args", ".", "print_freq", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'Train: [{:>3d}]/[{:>4d}/{:>4d}] Loss={:>0.3f} / {:>0.3f}'", ".", "format", "(", "\n", "epoch", ",", "idx", ",", "len", "(", "data_loader", ")", ",", "\n", "loss", ".", "item", "(", ")", ",", "loss_meter", ".", "avg", ")", "\n", ")", "\n", "\n", "# update meters", "\n", "", "loss_meter", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "bsz", ")", "\n", "\n", "", "return", "loss_meter", ".", "avg", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_fc.validate": [[264, 288], ["AverageMeter", "AverageMeter", "model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "tqdm.tqdm", "data.cuda.size", "data.cuda.view", "data.cuda.cuda", "model", "prediction.view().mean().cpu.view().mean().cpu", "calc_topk_accuracy", "AverageMeter.update", "AverageMeter.update", "len", "prediction.view().mean().cpu.view().mean", "prediction.view().mean().cpu.view"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.calc_topk_accuracy", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update"], ["", "def", "validate", "(", "args", ",", "model", ",", "data_loader", ")", ":", "\n", "\n", "    ", "top1_meter", "=", "AverageMeter", "(", ")", "\n", "top5_meter", "=", "AverageMeter", "(", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "idx", ",", "(", "data", ",", "label", ",", "vname", ")", "in", "enumerate", "(", "tqdm", "(", "data_loader", ",", "total", "=", "len", "(", "data_loader", ")", ")", ")", ":", "\n", "\n", "            ", "data_size", "=", "data", ".", "size", "(", ")", "\n", "data", "=", "data", ".", "view", "(", "(", "-", "1", ",", "3", ",", "data_size", "[", "-", "3", "]", ",", "data_size", "[", "-", "2", "]", ",", "data_size", "[", "-", "1", "]", ")", ")", "\n", "data", "=", "data", ".", "cuda", "(", ")", "\n", "\n", "\n", "prediction", "=", "model", "(", "data", ",", "mode", "=", "'classfier'", ")", "\n", "prediction", "=", "prediction", ".", "view", "(", "args", ".", "batch_size_test", ",", "args", ".", "num_segments_test", ",", "args", ".", "num_classes", ")", ".", "mean", "(", "1", ")", ".", "cpu", "(", ")", "\n", "\n", "\n", "top1", ",", "top5", "=", "calc_topk_accuracy", "(", "prediction", ",", "label", ",", "(", "1", ",", "5", ")", ")", "\n", "top1_meter", ".", "update", "(", "top1", ",", "args", ".", "batch_size_test", ")", "\n", "top5_meter", ".", "update", "(", "top5", ",", "args", ".", "batch_size_test", ")", "\n", "\n", "", "", "return", "top1_meter", ".", "avg", ",", "top5_meter", ".", "avg", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_svm_feature_extract.parse_option": [[20, 37], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "parse_option", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'svm eval'", ")", "\n", "parser", ".", "add_argument", "(", "'--root-path'", ",", "type", "=", "str", ",", "default", "=", "'/lr/exportssd/home/lirui295/dataset/Kinetics/Kinetics_frame256'", ",", "help", "=", "'root director of dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--mode'", ",", "type", "=", "str", ",", "default", "=", "'train'", ")", "\n", "parser", ".", "add_argument", "(", "'--list-path'", ",", "type", "=", "str", ",", "default", "=", "'../data/K400/val_list.csv'", ",", "help", "=", "'path of list file'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained-model'", ",", "type", "=", "str", ",", "default", "=", "'output/checkpoints/current.pth'", ",", "help", "=", "\"pretrained model path\"", ")", "\n", "parser", ".", "add_argument", "(", "'--output-dir'", ",", "type", "=", "str", ",", "default", "=", "'output/eval_output_linear'", ",", "help", "=", "'output director'", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "help", "=", "'local rank for DistributedDataParallel'", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--time-dim'", ",", "type", "=", "str", ",", "default", "=", "'T'", ",", "choices", "=", "[", "\"T\"", ",", "\"C\"", "]", ",", "help", "=", "\"dimension for time\"", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-length'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'num of clip length'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'num of sampling steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-segments'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "help", "=", "'num of segments'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-workers'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "help", "=", "'num of workers to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--inflate-weights'", ",", "type", "=", "str", ",", "default", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "type", "=", "str", ",", "default", "=", "'kinetics'", ",", "choices", "=", "[", "'kinetics'", ",", "'ucf101'", "]", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_svm_feature_extract.load_pretrained": [[38, 51], ["torch.load", "torch.load", "torch.load", "torch.load", "model.load_state_dict", "logger.info", "logger.info", "logger.info", "k.replace", "k.replace", "ckpt[].items", "torch.load.items"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.load_state_dict"], ["", "def", "load_pretrained", "(", "args", ",", "model", ")", ":", "\n", "    ", "ckpt", "=", "torch", ".", "load", "(", "args", ".", "pretrained_model", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "if", "'model'", "in", "ckpt", ":", "\n", "        ", "state_dict", "=", "{", "k", ".", "replace", "(", "\"module.backbone.\"", ",", "\"\"", ")", ":", "v", "for", "k", ",", "v", "in", "ckpt", "[", "'model'", "]", ".", "items", "(", ")", "}", "\n", "", "else", ":", "\n", "        ", "state_dict", "=", "{", "k", ".", "replace", "(", "\"module.backbone.\"", ",", "\"\"", ")", ":", "v", "for", "k", ",", "v", "in", "ckpt", ".", "items", "(", ")", "}", "\n", "\n", "", "[", "misskeys", ",", "unexpkeys", "]", "=", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "False", ")", "\n", "\n", "logger", ".", "info", "(", "misskeys", ")", "\n", "logger", ".", "info", "(", "unexpkeys", ")", "\n", "logger", ".", "info", "(", "\"==> loaded checkpoint '{}' (epoch {})\"", ".", "format", "(", "args", ".", "pretrained_model", ",", "ckpt", "[", "'epoch'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_svm_feature_extract.get_loader": [[53, 81], ["dataset.augmentations.clip_transforms.ClipCenterCrop", "dataset.augmentations.clip_transforms.Compose", "dataset.video_dataset.VideoRGBTestDataset", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "len", "dataset.augmentations.clip_transforms.ToClipTensor", "dataset.augmentations.clip_transforms.ClipNormalize", "dataset.augmentations.clip_transforms.Lambda", "torchvision.Lambda", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "get_loader", "(", "args", ",", "mode", ")", ":", "\n", "\n", "    ", "crop", "=", "clip_transforms", ".", "ClipCenterCrop", "(", "(", "224", ",", "224", ")", ")", "\n", "test_transform", "=", "clip_transforms", ".", "Compose", "(", "[", "\n", "crop", ",", "\n", "clip_transforms", ".", "ToClipTensor", "(", ")", ",", "\n", "clip_transforms", ".", "ClipNormalize", "(", "mean", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "std", "=", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", ",", "\n", "clip_transforms", ".", "Lambda", "(", "lambda", "clip", ":", "torch", ".", "stack", "(", "clip", ",", "dim", "=", "1", ")", ")", "if", "opt", ".", "time_dim", "==", "\"T\"", "else", "transforms", ".", "Lambda", "(", "\n", "lambda", "clip", ":", "torch", ".", "cat", "(", "clip", ",", "dim", "=", "0", ")", ")", "\n", "]", ")", "\n", "\n", "\n", "dataset", "=", "VideoRGBTestDataset", "(", "\n", "num_clips", "=", "1", ",", "\n", "list_root", "=", "args", ".", "list_path", ",", "root_path", "=", "args", ".", "root_path", ",", "\n", "transform", "=", "test_transform", ",", "\n", "clip_length", "=", "args", ".", "clip_length", ",", "num_steps", "=", "args", ".", "num_steps", ",", "\n", "num_segments", "=", "args", ".", "num_segments", ",", "dataset", "=", "args", ".", "dataset", ",", "\n", "split", "=", "mode", "\n", ")", "\n", "\n", "sampler", "=", "torch", ".", "utils", ".", "data", ".", "distributed", ".", "DistributedSampler", "(", "dataset", ")", "\n", "loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ",", "sampler", "=", "sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "pin_memory", "=", "True", ",", "\n", "drop_last", "=", "False", ")", "\n", "\n", "return", "loader", ",", "len", "(", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.linear_probe.eval_svm_feature_extract.main": [[83, 122], ["eval_svm_feature_extract.get_loader", "model.backbone.p3da_resnet50.cuda", "eval_svm_feature_extract.load_pretrained", "backbone().cuda.eval", "torch.AdaptiveAvgPool3d", "logger.info", "numpy.stack", "numpy.array", "numpy.save", "numpy.save", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "tqdm.tqdm", "os.path.join", "os.path.join", "open", "json.dump", "torch.get_rank", "numpy.save", "model.backbone.p3da_resnet50", "enumerate", "cls.view().item", "data.cuda.size", "data.cuda.view", "data.cuda.cuda", "nn.AdaptiveAvgPool3d.squeeze", "torch.mean().view", "torch.mean().view", "torch.mean().view", "torch.mean().view", "np.stack.append", "feat_cls.append", "paths.append", "os.path.join", "os.path.join", "numpy.array", "len", "torch.mean().view.data.cpu().numpy", "cls.item", "cls.view", "nn.AdaptiveAvgPool3d.", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "backbone().cuda.", "torch.mean().view.data.cpu"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.get_loader", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.load_pretrained", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_resnet50"], ["", "def", "main", "(", "opt", ")", ":", "\n", "    ", "data_loader", ",", "total_num", "=", "get_loader", "(", "opt", ",", "opt", ".", "mode", ")", "\n", "\n", "if", "opt", ".", "mode", "==", "'trainsvm'", ":", "\n", "        ", "opt", ".", "mode", "=", "'train'", "\n", "\n", "", "model", "=", "backbone", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "load_pretrained", "(", "opt", ",", "model", ")", "\n", "model", ".", "eval", "(", ")", "\n", "global_pooling", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "\n", "logger", ".", "info", "(", "'model init done'", ")", "\n", "all_feat", "=", "[", "]", "\n", "feat_cls", "=", "[", "]", "\n", "paths", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "idx", ",", "(", "data", ",", "cls", ",", "vpath", ")", "in", "tqdm", "(", "enumerate", "(", "data_loader", ")", ",", "total", "=", "len", "(", "data_loader", ")", ")", ":", "\n", "            ", "is_normal", "=", "cls", ".", "view", "(", "-", "1", ")", ".", "item", "(", ")", "\n", "if", "is_normal", "==", "-", "1", ":", "continue", "\n", "data_size", "=", "data", ".", "size", "(", ")", "\n", "\n", "data", "=", "data", ".", "view", "(", "(", "-", "1", ",", "3", ",", "data_size", "[", "-", "3", "]", ",", "data_size", "[", "-", "2", "]", ",", "data_size", "[", "-", "1", "]", ")", ")", "\n", "data", "=", "data", ".", "cuda", "(", ")", "\n", "feat", "=", "global_pooling", "(", "model", "(", "data", ",", "layer", "=", "5", ")", ")", ".", "squeeze", "(", ")", "\n", "feat_avg", "=", "torch", ".", "mean", "(", "feat", ",", "dim", "=", "0", ")", ".", "view", "(", "-", "1", ")", "\n", "all_feat", ".", "append", "(", "feat_avg", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "feat_cls", ".", "append", "(", "cls", ".", "item", "(", ")", ")", "\n", "paths", ".", "append", "(", "vpath", "[", "0", "]", ")", "\n", "\n", "", "", "all_feat", "=", "np", ".", "stack", "(", "all_feat", ",", "axis", "=", "0", ")", "\n", "all_feat_cls", "=", "np", ".", "array", "(", "feat_cls", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "output_dir", ",", "'feature_{}_{}.npy'", ".", "format", "(", "opt", ".", "mode", ",", "opt", ".", "local_rank", ")", ")", ",", "all_feat", ")", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "output_dir", ",", "'feature_{}_cls_{}.npy'", ".", "format", "(", "opt", ".", "mode", ",", "opt", ".", "local_rank", ")", ")", ",", "all_feat_cls", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "output_dir", ",", "'paths_{}_{}.json'", ".", "format", "(", "opt", ".", "mode", ",", "opt", ".", "local_rank", ")", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "paths", ",", "f", ")", "\n", "\n", "", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "output_dir", ",", "'vid_num_{}.npy'", ".", "format", "(", "opt", ".", "mode", ")", ")", ",", "np", ".", "array", "(", "[", "total_num", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.commonutil.svm_read_problem": [[17, 67], ["enumerate", "array.array", "array.array", "array.array", "array.array", "open", "line.split.split", "scipy.frombuffer.append", "scipy.frombuffer", "scipy.frombuffer", "scipy.frombuffer", "scipy.frombuffer", "sparse.csr_matrix", "len", "float", "features.split", "scipy.frombuffer.append", "features.split", "e.split", "float", "e.split", "float", "scipy.frombuffer.append", "sparse.csr_matrix.append", "int", "int"], "function", ["None"], ["def", "svm_read_problem", "(", "data_file_name", ",", "return_scipy", "=", "False", ")", ":", "\n", "\t", "\"\"\"\n\tsvm_read_problem(data_file_name, return_scipy=False) -> [y, x], y: list, x: list of dictionary\n\tsvm_read_problem(data_file_name, return_scipy=True)  -> [y, x], y: ndarray, x: csr_matrix\n\n\tRead LIBSVM-format data from data_file_name and return labels y\n\tand data instances x.\n\t\"\"\"", "\n", "if", "scipy", "!=", "None", "and", "return_scipy", ":", "\n", "\t\t", "prob_y", "=", "array", "(", "'d'", ")", "\n", "prob_x", "=", "array", "(", "'d'", ")", "\n", "row_ptr", "=", "array", "(", "'l'", ",", "[", "0", "]", ")", "\n", "col_idx", "=", "array", "(", "'l'", ")", "\n", "", "else", ":", "\n", "\t\t", "prob_y", "=", "[", "]", "\n", "prob_x", "=", "[", "]", "\n", "row_ptr", "=", "[", "0", "]", "\n", "col_idx", "=", "[", "]", "\n", "", "indx_start", "=", "1", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "open", "(", "data_file_name", ")", ")", ":", "\n", "\t\t", "line", "=", "line", ".", "split", "(", "None", ",", "1", ")", "\n", "# In case an instance with all zero features", "\n", "if", "len", "(", "line", ")", "==", "1", ":", "line", "+=", "[", "''", "]", "\n", "label", ",", "features", "=", "line", "\n", "prob_y", ".", "append", "(", "float", "(", "label", ")", ")", "\n", "if", "scipy", "!=", "None", "and", "return_scipy", ":", "\n", "\t\t\t", "nz", "=", "0", "\n", "for", "e", "in", "features", ".", "split", "(", ")", ":", "\n", "\t\t\t\t", "ind", ",", "val", "=", "e", ".", "split", "(", "\":\"", ")", "\n", "if", "ind", "==", "'0'", ":", "\n", "\t\t\t\t\t", "indx_start", "=", "0", "\n", "", "val", "=", "float", "(", "val", ")", "\n", "if", "val", "!=", "0", ":", "\n", "\t\t\t\t\t", "col_idx", ".", "append", "(", "int", "(", "ind", ")", "-", "indx_start", ")", "\n", "prob_x", ".", "append", "(", "val", ")", "\n", "nz", "+=", "1", "\n", "", "", "row_ptr", ".", "append", "(", "row_ptr", "[", "-", "1", "]", "+", "nz", ")", "\n", "", "else", ":", "\n", "\t\t\t", "xi", "=", "{", "}", "\n", "for", "e", "in", "features", ".", "split", "(", ")", ":", "\n", "\t\t\t\t", "ind", ",", "val", "=", "e", ".", "split", "(", "\":\"", ")", "\n", "xi", "[", "int", "(", "ind", ")", "]", "=", "float", "(", "val", ")", "\n", "", "prob_x", "+=", "[", "xi", "]", "\n", "", "", "if", "scipy", "!=", "None", "and", "return_scipy", ":", "\n", "\t\t", "prob_y", "=", "scipy", ".", "frombuffer", "(", "prob_y", ",", "dtype", "=", "'d'", ")", "\n", "prob_x", "=", "scipy", ".", "frombuffer", "(", "prob_x", ",", "dtype", "=", "'d'", ")", "\n", "col_idx", "=", "scipy", ".", "frombuffer", "(", "col_idx", ",", "dtype", "=", "'l'", ")", "\n", "row_ptr", "=", "scipy", ".", "frombuffer", "(", "row_ptr", ",", "dtype", "=", "'l'", ")", "\n", "prob_x", "=", "sparse", ".", "csr_matrix", "(", "(", "prob_x", ",", "col_idx", ",", "row_ptr", ")", ")", "\n", "", "return", "(", "prob_y", ",", "prob_x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.commonutil.evaluations_scipy": [[68, 94], ["len", "pv.sum", "ty.sum", "TypeError", "len", "len", "ValueError", "scipy.errstate", "float", "float", "float", "isinstance", "isinstance", "float"], "function", ["None"], ["", "def", "evaluations_scipy", "(", "ty", ",", "pv", ")", ":", "\n", "\t", "\"\"\"\n\tevaluations_scipy(ty, pv) -> (ACC, MSE, SCC)\n\tty, pv: ndarray\n\n\tCalculate accuracy, mean squared error and squared correlation coefficient\n\tusing the true values (ty) and predicted values (pv).\n\t\"\"\"", "\n", "if", "not", "(", "scipy", "!=", "None", "and", "isinstance", "(", "ty", ",", "scipy", ".", "ndarray", ")", "and", "isinstance", "(", "pv", ",", "scipy", ".", "ndarray", ")", ")", ":", "\n", "\t\t", "raise", "TypeError", "(", "\"type of ty and pv must be ndarray\"", ")", "\n", "", "if", "len", "(", "ty", ")", "!=", "len", "(", "pv", ")", ":", "\n", "\t\t", "raise", "ValueError", "(", "\"len(ty) must be equal to len(pv)\"", ")", "\n", "", "ACC", "=", "100.0", "*", "(", "ty", "==", "pv", ")", ".", "mean", "(", ")", "\n", "MSE", "=", "(", "(", "ty", "-", "pv", ")", "**", "2", ")", ".", "mean", "(", ")", "\n", "l", "=", "len", "(", "ty", ")", "\n", "sumv", "=", "pv", ".", "sum", "(", ")", "\n", "sumy", "=", "ty", ".", "sum", "(", ")", "\n", "sumvy", "=", "(", "pv", "*", "ty", ")", ".", "sum", "(", ")", "\n", "sumvv", "=", "(", "pv", "*", "pv", ")", ".", "sum", "(", ")", "\n", "sumyy", "=", "(", "ty", "*", "ty", ")", ".", "sum", "(", ")", "\n", "with", "scipy", ".", "errstate", "(", "all", "=", "'raise'", ")", ":", "\n", "\t\t", "try", ":", "\n", "\t\t\t", "SCC", "=", "(", "(", "l", "*", "sumvy", "-", "sumv", "*", "sumy", ")", "*", "(", "l", "*", "sumvy", "-", "sumv", "*", "sumy", ")", ")", "/", "(", "(", "l", "*", "sumvv", "-", "sumv", "*", "sumv", ")", "*", "(", "l", "*", "sumyy", "-", "sumy", "*", "sumy", ")", ")", "\n", "", "except", ":", "\n", "\t\t\t", "SCC", "=", "float", "(", "'nan'", ")", "\n", "", "", "return", "(", "float", "(", "ACC", ")", ",", "float", "(", "MSE", ")", ",", "float", "(", "SCC", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.commonutil.evaluations": [[95, 127], ["zip", "len", "commonutil.evaluations_scipy", "len", "len", "ValueError", "float", "float", "float", "scipy.asarray", "scipy.asarray", "float"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.commonutil.evaluations_scipy"], ["", "def", "evaluations", "(", "ty", ",", "pv", ",", "useScipy", "=", "True", ")", ":", "\n", "\t", "\"\"\"\n\tevaluations(ty, pv, useScipy) -> (ACC, MSE, SCC)\n\tty, pv: list, tuple or ndarray\n\tuseScipy: convert ty, pv to ndarray, and use scipy functions for the evaluation\n\n\tCalculate accuracy, mean squared error and squared correlation coefficient\n\tusing the true values (ty) and predicted values (pv).\n\t\"\"\"", "\n", "if", "scipy", "!=", "None", "and", "useScipy", ":", "\n", "\t\t", "return", "evaluations_scipy", "(", "scipy", ".", "asarray", "(", "ty", ")", ",", "scipy", ".", "asarray", "(", "pv", ")", ")", "\n", "", "if", "len", "(", "ty", ")", "!=", "len", "(", "pv", ")", ":", "\n", "\t\t", "raise", "ValueError", "(", "\"len(ty) must be equal to len(pv)\"", ")", "\n", "", "total_correct", "=", "total_error", "=", "0", "\n", "sumv", "=", "sumy", "=", "sumvv", "=", "sumyy", "=", "sumvy", "=", "0", "\n", "for", "v", ",", "y", "in", "zip", "(", "pv", ",", "ty", ")", ":", "\n", "\t\t", "if", "y", "==", "v", ":", "\n", "\t\t\t", "total_correct", "+=", "1", "\n", "", "total_error", "+=", "(", "v", "-", "y", ")", "*", "(", "v", "-", "y", ")", "\n", "sumv", "+=", "v", "\n", "sumy", "+=", "y", "\n", "sumvv", "+=", "v", "*", "v", "\n", "sumyy", "+=", "y", "*", "y", "\n", "sumvy", "+=", "v", "*", "y", "\n", "", "l", "=", "len", "(", "ty", ")", "\n", "ACC", "=", "100.0", "*", "total_correct", "/", "l", "\n", "MSE", "=", "total_error", "/", "l", "\n", "try", ":", "\n", "\t\t", "SCC", "=", "(", "(", "l", "*", "sumvy", "-", "sumv", "*", "sumy", ")", "*", "(", "l", "*", "sumvy", "-", "sumv", "*", "sumy", ")", ")", "/", "(", "(", "l", "*", "sumvv", "-", "sumv", "*", "sumv", ")", "*", "(", "l", "*", "sumyy", "-", "sumy", "*", "sumy", ")", ")", "\n", "", "except", ":", "\n", "\t\t", "SCC", "=", "float", "(", "'nan'", ")", "\n", "", "return", "(", "float", "(", "ACC", ")", ",", "float", "(", "MSE", ")", ",", "float", "(", "SCC", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.commonutil.csr_find_scale_param": [[128, 151], ["isinstance", "x.min().toarray().flatten", "x.max().toarray().flatten", "print", "x.min().toarray", "x.max().toarray", "sum", "x.getnnz", "x.min", "x.max"], "function", ["None"], ["", "def", "csr_find_scale_param", "(", "x", ",", "lower", "=", "-", "1", ",", "upper", "=", "1", ")", ":", "\n", "\t", "assert", "isinstance", "(", "x", ",", "sparse", ".", "csr_matrix", ")", "\n", "assert", "lower", "<", "upper", "\n", "l", ",", "n", "=", "x", ".", "shape", "\n", "feat_min", "=", "x", ".", "min", "(", "axis", "=", "0", ")", ".", "toarray", "(", ")", ".", "flatten", "(", ")", "\n", "feat_max", "=", "x", ".", "max", "(", "axis", "=", "0", ")", ".", "toarray", "(", ")", ".", "flatten", "(", ")", "\n", "coef", "=", "(", "feat_max", "-", "feat_min", ")", "/", "(", "upper", "-", "lower", ")", "\n", "coef", "[", "coef", "!=", "0", "]", "=", "1.0", "/", "coef", "[", "coef", "!=", "0", "]", "\n", "\n", "# (x - ones(l,1) * feat_min') * diag(coef) + lower", "\n", "# = x * diag(coef) - ones(l, 1) * (feat_min' * diag(coef)) + lower", "\n", "# = x * diag(coef) + ones(l, 1) * (-feat_min' * diag(coef) + lower)", "\n", "# = x * diag(coef) + ones(l, 1) * offset'", "\n", "offset", "=", "-", "feat_min", "*", "coef", "+", "lower", "\n", "offset", "[", "coef", "==", "0", "]", "=", "0", "\n", "\n", "if", "sum", "(", "offset", "!=", "0", ")", "*", "l", ">", "3", "*", "x", ".", "getnnz", "(", ")", ":", "\n", "\t\t", "print", "(", "\n", "\"WARNING: The #nonzeros of the scaled data is at least 2 times larger than the original one.\\n\"", "\n", "\"If feature values are non-negative and sparse, set lower=0 rather than the default lower=-1.\"", ",", "\n", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "", "return", "{", "'coef'", ":", "coef", ",", "'offset'", ":", "offset", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.commonutil.csr_scale": [[152, 179], ["isinstance", "sparse.csr_matrix", "sparse.vstack", "len", "len", "print", "resize", "resize", "resize.reshape", "x.dot", "scaled_x.getnnz", "x.getnnz", "print", "len", "sparse.diags", "x.getnnz", "scaled_x.getnnz"], "function", ["None"], ["", "def", "csr_scale", "(", "x", ",", "scale_param", ")", ":", "\n", "\t", "assert", "isinstance", "(", "x", ",", "sparse", ".", "csr_matrix", ")", "\n", "\n", "offset", "=", "scale_param", "[", "'offset'", "]", "\n", "coef", "=", "scale_param", "[", "'coef'", "]", "\n", "assert", "len", "(", "coef", ")", "==", "len", "(", "offset", ")", "\n", "\n", "l", ",", "n", "=", "x", ".", "shape", "\n", "\n", "if", "not", "n", "==", "len", "(", "coef", ")", ":", "\n", "\t\t", "print", "(", "\"WARNING: The dimension of scaling parameters and feature number do not match.\"", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "coef", "=", "resize", "(", "coef", ",", "n", ")", "\n", "offset", "=", "resize", "(", "offset", ",", "n", ")", "\n", "\n", "# scaled_x = x * diag(coef) + ones(l, 1) * offset'", "\n", "", "offset", "=", "sparse", ".", "csr_matrix", "(", "offset", ".", "reshape", "(", "1", ",", "n", ")", ")", "\n", "offset", "=", "sparse", ".", "vstack", "(", "[", "offset", "]", "*", "l", ",", "format", "=", "'csr'", ",", "dtype", "=", "x", ".", "dtype", ")", "\n", "scaled_x", "=", "x", ".", "dot", "(", "sparse", ".", "diags", "(", "coef", ",", "0", ",", "shape", "=", "(", "n", ",", "n", ")", ")", ")", "+", "offset", "\n", "\n", "if", "scaled_x", ".", "getnnz", "(", ")", ">", "x", ".", "getnnz", "(", ")", ":", "\n", "\t\t", "print", "(", "\n", "\"WARNING: original #nonzeros %d\\n\"", "%", "x", ".", "getnnz", "(", ")", "+", "\n", "\"       > new      #nonzeros %d\\n\"", "%", "scaled_x", ".", "getnnz", "(", ")", "+", "\n", "\"If feature values are non-negative and sparse, get scale_param by setting lower=0 rather than the default lower=-1.\"", ",", "\n", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "", "return", "scaled_x", "\n", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinearutil.load_model": [[22, 34], ["liblinear.load_model", "liblinear.toPyModel", "_cstr", "print"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinearutil.load_model", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.toPyModel"], ["def", "load_model", "(", "model_file_name", ")", ":", "\n", "\t", "\"\"\"\n\tload_model(model_file_name) -> model\n\n\tLoad a LIBLINEAR model from model_file_name and return.\n\t\"\"\"", "\n", "model", "=", "liblinear", ".", "load_model", "(", "_cstr", "(", "model_file_name", ")", ")", "\n", "if", "not", "model", ":", "\n", "\t\t", "print", "(", "\"can't open model file %s\"", "%", "model_file_name", ")", "\n", "return", "None", "\n", "", "model", "=", "toPyModel", "(", "model", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinearutil.save_model": [[35, 42], ["liblinear.save_model", "_cstr"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinearutil.save_model"], ["", "def", "save_model", "(", "model_file_name", ",", "model", ")", ":", "\n", "\t", "\"\"\"\n\tsave_model(model_file_name, model) -> None\n\n\tSave a LIBLINEAR model to the file model_file_name.\n\t\"\"\"", "\n", "liblinear", ".", "save_model", "(", "_cstr", "(", "model_file_name", ")", ",", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinearutil.train": [[43, 160], ["liblinear.problem.set_bias", "liblinear.set_print_string_function", "liblinear.check_parameter", "isinstance", "liblinear.problem", "liblinear.parameter", "isinstance", "TypeError", "ValueError", "ctypes.c_double", "ctypes.c_double", "ctypes.c_double", "liblinear.find_parameters", "isinstance", "isinstance", "isinstance", "print", "liblinear.cross_validation", "commonutil.evaluations", "liblinear.train", "liblinear.toPyModel", "isinstance", "liblinear.parameter", "print", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.problem.set_bias", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.commonutil.evaluations", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.train", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.toPyModel"], ["", "def", "train", "(", "arg1", ",", "arg2", "=", "None", ",", "arg3", "=", "None", ")", ":", "\n", "\t", "\"\"\"\n\ttrain(y, x [, options]) -> model | ACC\n\n\ty: a list/tuple/ndarray of l true labels (type must be int/double).\n\n\tx: 1. a list/tuple of l training instances. Feature vector of\n\t      each training instance is a list/tuple or dictionary.\n\n\t   2. an l * n numpy ndarray or scipy spmatrix (n: number of features).\n\n\ttrain(prob [, options]) -> model | ACC\n\ttrain(prob, param) -> model | ACC\n\n\tTrain a model from data (y, x) or a problem prob using\n\t'options' or a parameter param.\n\n\tIf '-v' is specified in 'options' (i.e., cross validation)\n\teither accuracy (ACC) or mean-squared error (MSE) is returned.\n\n\toptions:\n\t\t-s type : set type of solver (default 1)\n\t\t  for multi-class classification\n\t\t\t 0 -- L2-regularized logistic regression (primal)\n\t\t\t 1 -- L2-regularized L2-loss support vector classification (dual)\n\t\t\t 2 -- L2-regularized L2-loss support vector classification (primal)\n\t\t\t 3 -- L2-regularized L1-loss support vector classification (dual)\n\t\t\t 4 -- support vector classification by Crammer and Singer\n\t\t\t 5 -- L1-regularized L2-loss support vector classification\n\t\t\t 6 -- L1-regularized logistic regression\n\t\t\t 7 -- L2-regularized logistic regression (dual)\n\t\t  for regression\n\t\t\t11 -- L2-regularized L2-loss support vector regression (primal)\n\t\t\t12 -- L2-regularized L2-loss support vector regression (dual)\n\t\t\t13 -- L2-regularized L1-loss support vector regression (dual)\n\t\t-c cost : set the parameter C (default 1)\n\t\t-p epsilon : set the epsilon in loss function of SVR (default 0.1)\n\t\t-e epsilon : set tolerance of termination criterion\n\t\t\t-s 0 and 2\n\t\t\t\t|f'(w)|_2 <= eps*min(pos,neg)/l*|f'(w0)|_2,\n\t\t\t\twhere f is the primal function, (default 0.01)\n\t\t\t-s 11\n\t\t\t\t|f'(w)|_2 <= eps*|f'(w0)|_2 (default 0.0001)\n\t\t\t-s 1, 3, 4, and 7\n\t\t\t\tDual maximal violation <= eps; similar to liblinear (default 0.)\n\t\t\t-s 5 and 6\n\t\t\t\t|f'(w)|_inf <= eps*min(pos,neg)/l*|f'(w0)|_inf,\n\t\t\t\twhere f is the primal function (default 0.01)\n\t\t\t-s 12 and 13\n\t\t\t\t|f'(alpha)|_1 <= eps |f'(alpha0)|,\n\t\t\t\twhere f is the dual function (default 0.1)\n\t\t-B bias : if bias >= 0, instance x becomes [x; bias]; if < 0, no bias term added (default -1)\n\t\t-wi weight: weights adjust the parameter C of different classes (see README for details)\n\t\t-v n: n-fold cross validation mode\n\t\t-C : find parameters (C for -s 0, 2 and C, p for -s 11)\n\t\t-n nr_thread : parallel version with [nr_thread] threads (default 1; only for -s 0, 1, 2, 3, 11)\n\t\t-q : quiet mode (no outputs)\n\t\"\"\"", "\n", "prob", ",", "param", "=", "None", ",", "None", "\n", "if", "isinstance", "(", "arg1", ",", "(", "list", ",", "tuple", ")", ")", "or", "(", "scipy", "and", "isinstance", "(", "arg1", ",", "scipy", ".", "ndarray", ")", ")", ":", "\n", "\t\t", "assert", "isinstance", "(", "arg2", ",", "(", "list", ",", "tuple", ")", ")", "or", "(", "scipy", "and", "isinstance", "(", "arg2", ",", "(", "scipy", ".", "ndarray", ",", "sparse", ".", "spmatrix", ")", ")", ")", "\n", "y", ",", "x", ",", "options", "=", "arg1", ",", "arg2", ",", "arg3", "\n", "prob", "=", "problem", "(", "y", ",", "x", ")", "\n", "param", "=", "parameter", "(", "options", ")", "\n", "", "elif", "isinstance", "(", "arg1", ",", "problem", ")", ":", "\n", "\t\t", "prob", "=", "arg1", "\n", "if", "isinstance", "(", "arg2", ",", "parameter", ")", ":", "\n", "\t\t\t", "param", "=", "arg2", "\n", "", "else", ":", "\n", "\t\t\t", "param", "=", "parameter", "(", "arg2", ")", "\n", "", "", "if", "prob", "==", "None", "or", "param", "==", "None", ":", "\n", "\t\t", "raise", "TypeError", "(", "\"Wrong types for the arguments\"", ")", "\n", "\n", "", "prob", ".", "set_bias", "(", "param", ".", "bias", ")", "\n", "liblinear", ".", "set_print_string_function", "(", "param", ".", "print_func", ")", "\n", "err_msg", "=", "liblinear", ".", "check_parameter", "(", "prob", ",", "param", ")", "\n", "if", "err_msg", ":", "\n", "\t\t", "raise", "ValueError", "(", "'Error: %s'", "%", "err_msg", ")", "\n", "\n", "", "if", "param", ".", "flag_find_parameters", ":", "\n", "\t\t", "nr_fold", "=", "param", ".", "nr_fold", "\n", "best_C", "=", "c_double", "(", ")", "\n", "best_p", "=", "c_double", "(", ")", "\n", "best_score", "=", "c_double", "(", ")", "\n", "if", "param", ".", "flag_C_specified", ":", "\n", "\t\t\t", "start_C", "=", "param", ".", "C", "\n", "", "else", ":", "\n", "\t\t\t", "start_C", "=", "-", "1.0", "\n", "", "if", "param", ".", "flag_p_specified", ":", "\n", "\t\t\t", "start_p", "=", "param", ".", "p", "\n", "", "else", ":", "\n", "\t\t\t", "start_p", "=", "-", "1.0", "\n", "", "liblinear", ".", "find_parameters", "(", "prob", ",", "param", ",", "nr_fold", ",", "start_C", ",", "start_p", ",", "best_C", ",", "best_p", ",", "best_score", ")", "\n", "if", "param", ".", "solver_type", "in", "[", "L2R_LR", ",", "L2R_L2LOSS_SVC", "]", ":", "\n", "\t\t\t", "print", "(", "\"Best C = %g  CV accuracy = %g%%\\n\"", "%", "(", "best_C", ".", "value", ",", "100.0", "*", "best_score", ".", "value", ")", ")", "\n", "", "elif", "param", ".", "solver_type", "in", "[", "L2R_L2LOSS_SVR", "]", ":", "\n", "\t\t\t", "print", "(", "\"Best C = %g Best p = %g  CV MSE = %g\\n\"", "%", "(", "best_C", ".", "value", ",", "best_p", ".", "value", ",", "best_score", ".", "value", ")", ")", "\n", "", "return", "best_C", ".", "value", ",", "best_p", ".", "value", ",", "best_score", ".", "value", "\n", "\n", "\n", "", "elif", "param", ".", "flag_cross_validation", ":", "\n", "\t\t", "l", ",", "nr_fold", "=", "prob", ".", "l", ",", "param", ".", "nr_fold", "\n", "target", "=", "(", "c_double", "*", "l", ")", "(", ")", "\n", "liblinear", ".", "cross_validation", "(", "prob", ",", "param", ",", "nr_fold", ",", "target", ")", "\n", "ACC", ",", "MSE", ",", "SCC", "=", "evaluations", "(", "prob", ".", "y", "[", ":", "l", "]", ",", "target", "[", ":", "l", "]", ")", "\n", "if", "param", ".", "solver_type", "in", "[", "L2R_L2LOSS_SVR", ",", "L2R_L2LOSS_SVR_DUAL", ",", "L2R_L1LOSS_SVR_DUAL", "]", ":", "\n", "\t\t\t", "print", "(", "\"Cross Validation Mean squared error = %g\"", "%", "MSE", ")", "\n", "print", "(", "\"Cross Validation Squared correlation coefficient = %g\"", "%", "SCC", ")", "\n", "return", "MSE", "\n", "", "else", ":", "\n", "\t\t\t", "print", "(", "\"Cross Validation Accuracy = %g%%\"", "%", "ACC", ")", "\n", "return", "ACC", "\n", "", "", "else", ":", "\n", "\t\t", "m", "=", "liblinear", ".", "train", "(", "prob", ",", "param", ")", "\n", "m", "=", "toPyModel", "(", "m", ")", "\n", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinearutil.predict": [[161, 280], ["options.split", "m.get_nr_class", "m.get_nr_feature", "m.is_probability_model", "commonutil.evaluations", "m.is_regression_model", "print", "isinstance", "liblinear.scipy.ascontiguousarray", "TypeError", "len", "liblinear.feature_node", "liblinear.feature_node", "isinstance", "len", "range", "range", "len", "liblinearutil.predict.info"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_nr_class", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_nr_feature", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.is_probability_model", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.commonutil.evaluations", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.is_regression_model"], ["", "", "def", "predict", "(", "y", ",", "x", ",", "m", ",", "options", "=", "\"\"", ")", ":", "\n", "\t", "\"\"\"\n\tpredict(y, x, m [, options]) -> (p_labels, p_acc, p_vals)\n\n\ty: a list/tuple/ndarray of l true labels (type must be int/double).\n\t   It is used for calculating the accuracy. Use [] if true labels are\n\t   unavailable.\n\n\tx: 1. a list/tuple of l training instances. Feature vector of\n\t      each training instance is a list/tuple or dictionary.\n\n\t   2. an l * n numpy ndarray or scipy spmatrix (n: number of features).\n\n\tPredict data (y, x) with the SVM model m.\n\toptions:\n\t    -b probability_estimates: whether to output probability estimates, 0 or 1 (default 0); currently for logistic regression only\n\t    -q quiet mode (no outputs)\n\n\tThe return tuple contains\n\tp_labels: a list of predicted labels\n\tp_acc: a tuple including  accuracy (for classification), mean-squared\n\t       error, and squared correlation coefficient (for regression).\n\tp_vals: a list of decision values or probability estimates (if '-b 1'\n\t        is specified). If k is the number of classes, for decision values,\n\t        each element includes results of predicting k binary-class\n\t        SVMs. if k = 2 and solver is not MCSVM_CS, only one decision value\n\t        is returned. For probabilities, each element contains k values\n\t        indicating the probability that the testing instance is in each class.\n\t        Note that the order of classes here is the same as 'model.label'\n\t        field in the model structure.\n\t\"\"\"", "\n", "\n", "def", "info", "(", "s", ")", ":", "\n", "\t\t", "print", "(", "s", ")", "\n", "\n", "", "if", "scipy", "and", "isinstance", "(", "x", ",", "scipy", ".", "ndarray", ")", ":", "\n", "\t\t", "x", "=", "scipy", ".", "ascontiguousarray", "(", "x", ")", "# enforce row-major", "\n", "", "elif", "sparse", "and", "isinstance", "(", "x", ",", "sparse", ".", "spmatrix", ")", ":", "\n", "\t\t", "x", "=", "x", ".", "tocsr", "(", ")", "\n", "", "elif", "not", "isinstance", "(", "x", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "\t\t", "raise", "TypeError", "(", "\"type of x: {0} is not supported!\"", ".", "format", "(", "type", "(", "x", ")", ")", ")", "\n", "\n", "", "if", "(", "not", "isinstance", "(", "y", ",", "(", "list", ",", "tuple", ")", ")", ")", "and", "(", "not", "(", "scipy", "and", "isinstance", "(", "y", ",", "scipy", ".", "ndarray", ")", ")", ")", ":", "\n", "\t\t", "raise", "TypeError", "(", "\"type of y: {0} is not supported!\"", ".", "format", "(", "type", "(", "y", ")", ")", ")", "\n", "\n", "", "predict_probability", "=", "0", "\n", "argv", "=", "options", ".", "split", "(", ")", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "argv", ")", ":", "\n", "\t\t", "if", "argv", "[", "i", "]", "==", "'-b'", ":", "\n", "\t\t\t", "i", "+=", "1", "\n", "predict_probability", "=", "int", "(", "argv", "[", "i", "]", ")", "\n", "", "elif", "argv", "[", "i", "]", "==", "'-q'", ":", "\n", "\t\t\t", "info", "=", "print_null", "\n", "", "else", ":", "\n", "\t\t\t", "raise", "ValueError", "(", "\"Wrong options\"", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "solver_type", "=", "m", ".", "param", ".", "solver_type", "\n", "nr_class", "=", "m", ".", "get_nr_class", "(", ")", "\n", "nr_feature", "=", "m", ".", "get_nr_feature", "(", ")", "\n", "is_prob_model", "=", "m", ".", "is_probability_model", "(", ")", "\n", "bias", "=", "m", ".", "bias", "\n", "if", "bias", ">=", "0", ":", "\n", "\t\t", "biasterm", "=", "feature_node", "(", "nr_feature", "+", "1", ",", "bias", ")", "\n", "", "else", ":", "\n", "\t\t", "biasterm", "=", "feature_node", "(", "-", "1", ",", "bias", ")", "\n", "", "pred_labels", "=", "[", "]", "\n", "pred_values", "=", "[", "]", "\n", "\n", "if", "scipy", "and", "isinstance", "(", "x", ",", "sparse", ".", "spmatrix", ")", ":", "\n", "\t\t", "nr_instance", "=", "x", ".", "shape", "[", "0", "]", "\n", "", "else", ":", "\n", "\t\t", "nr_instance", "=", "len", "(", "x", ")", "\n", "\n", "", "if", "predict_probability", ":", "\n", "\t\t", "if", "not", "is_prob_model", ":", "\n", "\t\t\t", "raise", "TypeError", "(", "'probability output is only supported for logistic regression'", ")", "\n", "", "prob_estimates", "=", "(", "c_double", "*", "nr_class", ")", "(", ")", "\n", "for", "i", "in", "range", "(", "nr_instance", ")", ":", "\n", "\t\t\t", "if", "scipy", "and", "isinstance", "(", "x", ",", "sparse", ".", "spmatrix", ")", ":", "\n", "\t\t\t\t", "indslice", "=", "slice", "(", "x", ".", "indptr", "[", "i", "]", ",", "x", ".", "indptr", "[", "i", "+", "1", "]", ")", "\n", "xi", ",", "idx", "=", "gen_feature_nodearray", "(", "(", "x", ".", "indices", "[", "indslice", "]", ",", "x", ".", "data", "[", "indslice", "]", ")", ",", "feature_max", "=", "nr_feature", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "xi", ",", "idx", "=", "gen_feature_nodearray", "(", "x", "[", "i", "]", ",", "feature_max", "=", "nr_feature", ")", "\n", "", "xi", "[", "-", "2", "]", "=", "biasterm", "\n", "label", "=", "liblinear", ".", "predict_probability", "(", "m", ",", "xi", ",", "prob_estimates", ")", "\n", "values", "=", "prob_estimates", "[", ":", "nr_class", "]", "\n", "pred_labels", "+=", "[", "label", "]", "\n", "pred_values", "+=", "[", "values", "]", "\n", "", "", "else", ":", "\n", "\t\t", "if", "nr_class", "<=", "2", ":", "\n", "\t\t\t", "nr_classifier", "=", "1", "\n", "", "else", ":", "\n", "\t\t\t", "nr_classifier", "=", "nr_class", "\n", "", "dec_values", "=", "(", "c_double", "*", "nr_classifier", ")", "(", ")", "\n", "for", "i", "in", "range", "(", "nr_instance", ")", ":", "\n", "\t\t\t", "if", "scipy", "and", "isinstance", "(", "x", ",", "sparse", ".", "spmatrix", ")", ":", "\n", "\t\t\t\t", "indslice", "=", "slice", "(", "x", ".", "indptr", "[", "i", "]", ",", "x", ".", "indptr", "[", "i", "+", "1", "]", ")", "\n", "xi", ",", "idx", "=", "gen_feature_nodearray", "(", "(", "x", ".", "indices", "[", "indslice", "]", ",", "x", ".", "data", "[", "indslice", "]", ")", ",", "feature_max", "=", "nr_feature", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "xi", ",", "idx", "=", "gen_feature_nodearray", "(", "x", "[", "i", "]", ",", "feature_max", "=", "nr_feature", ")", "\n", "", "xi", "[", "-", "2", "]", "=", "biasterm", "\n", "label", "=", "liblinear", ".", "predict_values", "(", "m", ",", "xi", ",", "dec_values", ")", "\n", "values", "=", "dec_values", "[", ":", "nr_classifier", "]", "\n", "pred_labels", "+=", "[", "label", "]", "\n", "pred_values", "+=", "[", "values", "]", "\n", "\n", "", "", "if", "len", "(", "y", ")", "==", "0", ":", "\n", "\t\t", "y", "=", "[", "0", "]", "*", "nr_instance", "\n", "", "ACC", ",", "MSE", ",", "SCC", "=", "evaluations", "(", "y", ",", "pred_labels", ")", "\n", "\n", "if", "m", ".", "is_regression_model", "(", ")", ":", "\n", "\t\t", "info", "(", "\"Mean squared error = %g (regression)\"", "%", "MSE", ")", "\n", "info", "(", "\"Squared correlation coefficient = %g (regression)\"", "%", "SCC", ")", "\n", "", "else", ":", "\n", "\t\t", "info", "(", "\"Accuracy = %g%% (%d/%d) (classification)\"", "%", "(", "ACC", ",", "int", "(", "round", "(", "nr_instance", "*", "ACC", "/", "100", ")", ")", ",", "nr_instance", ")", ")", "\n", "\n", "", "return", "pred_labels", ",", "(", "ACC", ",", "MSE", ",", "SCC", ")", ",", "pred_values", "\n", "", ""]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.feature_node.__str__": [[68, 70], ["None"], "methods", ["None"], ["def", "__str__", "(", "self", ")", ":", "\n", "\t\t", "return", "'%d:%g'", "%", "(", "self", ".", "index", ",", "self", ".", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.problem.__init__": [[159, 207], ["isinstance", "len", "liblinear.problem.set_bias", "TypeError", "isinstance", "liblinear.csr_to_problem", "enumerate", "isinstance", "enumerate", "isinstance", "addressof", "cast", "scipy.ctypeslib.as_array", "enumerate", "isinstance", "len", "len", "ValueError", "isinstance", "isinstance", "isinstance", "TypeError", "liblinear.gen_feature_nodearray", "max", "scipy.ctypeslib.as_array", "POINTER", "POINTER", "isinstance", "type", "len", "ValueError", "scipy.ascontiguousarray", "x.tocsr.tocsr.tocsr", "liblinear.problem.x_space.ctypes.data_as", "sizeof", "type", "POINTER"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.problem.set_bias", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.csr_to_problem", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.gen_feature_nodearray"], ["def", "__init__", "(", "self", ",", "y", ",", "x", ",", "bias", "=", "-", "1", ")", ":", "\n", "\t\t", "if", "(", "not", "isinstance", "(", "y", ",", "(", "list", ",", "tuple", ")", ")", ")", "and", "(", "not", "(", "scipy", "and", "isinstance", "(", "y", ",", "scipy", ".", "ndarray", ")", ")", ")", ":", "\n", "\t\t\t", "raise", "TypeError", "(", "\"type of y: {0} is not supported!\"", ".", "format", "(", "type", "(", "y", ")", ")", ")", "\n", "\n", "", "if", "isinstance", "(", "x", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "\t\t\t", "if", "len", "(", "y", ")", "!=", "len", "(", "x", ")", ":", "\n", "\t\t\t\t", "raise", "ValueError", "(", "\"len(y) != len(x)\"", ")", "\n", "", "", "elif", "scipy", "!=", "None", "and", "isinstance", "(", "x", ",", "(", "scipy", ".", "ndarray", ",", "sparse", ".", "spmatrix", ")", ")", ":", "\n", "\t\t\t", "if", "len", "(", "y", ")", "!=", "x", ".", "shape", "[", "0", "]", ":", "\n", "\t\t\t\t", "raise", "ValueError", "(", "\"len(y) != len(x)\"", ")", "\n", "", "if", "isinstance", "(", "x", ",", "scipy", ".", "ndarray", ")", ":", "\n", "\t\t\t\t", "x", "=", "scipy", ".", "ascontiguousarray", "(", "x", ")", "# enforce row-major", "\n", "", "if", "isinstance", "(", "x", ",", "sparse", ".", "spmatrix", ")", ":", "\n", "\t\t\t\t", "x", "=", "x", ".", "tocsr", "(", ")", "\n", "pass", "\n", "", "", "else", ":", "\n", "\t\t\t", "raise", "TypeError", "(", "\"type of x: {0} is not supported!\"", ".", "format", "(", "type", "(", "x", ")", ")", ")", "\n", "", "self", ".", "l", "=", "l", "=", "len", "(", "y", ")", "\n", "self", ".", "bias", "=", "-", "1", "\n", "\n", "max_idx", "=", "0", "\n", "x_space", "=", "self", ".", "x_space", "=", "[", "]", "\n", "if", "scipy", "!=", "None", "and", "isinstance", "(", "x", ",", "sparse", ".", "csr_matrix", ")", ":", "\n", "\t\t\t", "csr_to_problem", "(", "x", ",", "self", ")", "\n", "max_idx", "=", "x", ".", "shape", "[", "1", "]", "\n", "", "else", ":", "\n", "\t\t\t", "for", "i", ",", "xi", "in", "enumerate", "(", "x", ")", ":", "\n", "\t\t\t\t", "tmp_xi", ",", "tmp_idx", "=", "gen_feature_nodearray", "(", "xi", ")", "\n", "x_space", "+=", "[", "tmp_xi", "]", "\n", "max_idx", "=", "max", "(", "max_idx", ",", "tmp_idx", ")", "\n", "", "", "self", ".", "n", "=", "max_idx", "\n", "\n", "self", ".", "y", "=", "(", "c_double", "*", "l", ")", "(", ")", "\n", "if", "scipy", "!=", "None", "and", "isinstance", "(", "y", ",", "scipy", ".", "ndarray", ")", ":", "\n", "\t\t\t", "scipy", ".", "ctypeslib", ".", "as_array", "(", "self", ".", "y", ",", "(", "self", ".", "l", ",", ")", ")", "[", ":", "]", "=", "y", "\n", "", "else", ":", "\n", "\t\t\t", "for", "i", ",", "yi", "in", "enumerate", "(", "y", ")", ":", "self", ".", "y", "[", "i", "]", "=", "yi", "\n", "\n", "", "self", ".", "x", "=", "(", "POINTER", "(", "feature_node", ")", "*", "l", ")", "(", ")", "\n", "if", "scipy", "!=", "None", "and", "isinstance", "(", "x", ",", "sparse", ".", "csr_matrix", ")", ":", "\n", "\t\t\t", "base", "=", "addressof", "(", "self", ".", "x_space", ".", "ctypes", ".", "data_as", "(", "POINTER", "(", "feature_node", ")", ")", "[", "0", "]", ")", "\n", "x_ptr", "=", "cast", "(", "self", ".", "x", ",", "POINTER", "(", "c_uint64", ")", ")", "\n", "x_ptr", "=", "scipy", ".", "ctypeslib", ".", "as_array", "(", "x_ptr", ",", "(", "self", ".", "l", ",", ")", ")", "\n", "x_ptr", "[", ":", "]", "=", "self", ".", "rowptr", "[", ":", "-", "1", "]", "*", "sizeof", "(", "feature_node", ")", "+", "base", "\n", "", "else", ":", "\n", "\t\t\t", "for", "i", ",", "xi", "in", "enumerate", "(", "self", ".", "x_space", ")", ":", "self", ".", "x", "[", "i", "]", "=", "xi", "\n", "\n", "", "self", ".", "set_bias", "(", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.problem.set_bias": [[208, 226], ["isinstance", "liblinear.feature_node", "liblinear.feature_node"], "methods", ["None"], ["", "def", "set_bias", "(", "self", ",", "bias", ")", ":", "\n", "\t\t", "if", "self", ".", "bias", "==", "bias", ":", "\n", "\t\t\t", "return", "\n", "", "if", "bias", ">=", "0", "and", "self", ".", "bias", "<", "0", ":", "\n", "\t\t\t", "self", ".", "n", "+=", "1", "\n", "node", "=", "feature_node", "(", "self", ".", "n", ",", "bias", ")", "\n", "", "if", "bias", "<", "0", "and", "self", ".", "bias", ">=", "0", ":", "\n", "\t\t\t", "self", ".", "n", "-=", "1", "\n", "node", "=", "feature_node", "(", "-", "1", ",", "bias", ")", "\n", "\n", "", "if", "isinstance", "(", "self", ".", "x_space", ",", "list", ")", ":", "\n", "\t\t\t", "for", "xi", "in", "self", ".", "x_space", ":", "\n", "\t\t\t\t", "xi", "[", "-", "2", "]", "=", "node", "\n", "", "", "else", ":", "\n", "\t\t\t", "self", ".", "x_space", "[", "\"index\"", "]", "[", "self", ".", "rowptr", "[", "1", ":", "]", "-", "2", "]", "=", "node", ".", "index", "\n", "self", ".", "x_space", "[", "\"value\"", "]", "[", "self", ".", "rowptr", "[", "1", ":", "]", "-", "2", "]", "=", "node", ".", "value", "\n", "\n", "", "self", ".", "bias", "=", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.parameter.__init__": [[233, 237], ["liblinear.parameter.parse_options"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.parameter.parse_options"], ["def", "__init__", "(", "self", ",", "options", "=", "None", ")", ":", "\n", "\t\t", "if", "options", "==", "None", ":", "\n", "\t\t\t", "options", "=", "''", "\n", "", "self", ".", "parse_options", "(", "options", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.parameter.__str__": [[238, 247], ["map", "zip", "s.strip.strip.strip", "list", "liblinear.parameter.__dict__.keys", "getattr"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "\t\t", "s", "=", "''", "\n", "attrs", "=", "parameter", ".", "_names", "+", "list", "(", "self", ".", "__dict__", ".", "keys", "(", ")", ")", "\n", "values", "=", "map", "(", "lambda", "attr", ":", "getattr", "(", "self", ",", "attr", ")", ",", "attrs", ")", "\n", "for", "attr", ",", "val", "in", "zip", "(", "attrs", ",", "values", ")", ":", "\n", "\t\t\t", "s", "+=", "(", "' %s: %s\\n'", "%", "(", "attr", ",", "val", ")", ")", "\n", "", "s", "=", "s", ".", "strip", "(", ")", "\n", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.parameter.set_to_default_values": [[248, 267], ["float", "cast"], "methods", ["None"], ["", "def", "set_to_default_values", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "solver_type", "=", "L2R_L2LOSS_SVC_DUAL", "\n", "self", ".", "eps", "=", "float", "(", "'inf'", ")", "\n", "self", ".", "C", "=", "1", "\n", "self", ".", "p", "=", "0.1", "\n", "self", ".", "nr_thread", "=", "1", "\n", "self", ".", "nr_weight", "=", "0", "\n", "self", ".", "weight_label", "=", "None", "\n", "self", ".", "weight", "=", "None", "\n", "self", ".", "init_sol", "=", "None", "\n", "self", ".", "bias", "=", "-", "1", "\n", "self", ".", "flag_cross_validation", "=", "False", "\n", "self", ".", "flag_C_specified", "=", "False", "\n", "self", ".", "flag_p_specified", "=", "False", "\n", "self", ".", "flag_solver_specified", "=", "False", "\n", "self", ".", "flag_find_parameters", "=", "False", "\n", "self", ".", "flag_omp", "=", "False", "\n", "self", ".", "nr_fold", "=", "0", "\n", "self", ".", "print_func", "=", "cast", "(", "None", ",", "PRINT_STRING_FUN", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.parameter.parse_options": [[268, 359], ["isinstance", "liblinear.parameter.set_to_default_values", "cast", "liblinear.set_print_string_function", "range", "isinstance", "len", "float", "options.split", "TypeError", "int", "float", "ValueError", "ValueError", "float", "float", "float", "int", "ValueError", "int", "argv[].startswith", "int", "float", "PRINT_STRING_FUN", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.parameter.set_to_default_values"], ["", "def", "parse_options", "(", "self", ",", "options", ")", ":", "\n", "\t\t", "if", "isinstance", "(", "options", ",", "list", ")", ":", "\n", "\t\t\t", "argv", "=", "options", "\n", "", "elif", "isinstance", "(", "options", ",", "str", ")", ":", "\n", "\t\t\t", "argv", "=", "options", ".", "split", "(", ")", "\n", "", "else", ":", "\n", "\t\t\t", "raise", "TypeError", "(", "\"arg 1 should be a list or a str.\"", ")", "\n", "", "self", ".", "set_to_default_values", "(", ")", "\n", "self", ".", "print_func", "=", "cast", "(", "None", ",", "PRINT_STRING_FUN", ")", "\n", "weight_label", "=", "[", "]", "\n", "weight", "=", "[", "]", "\n", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "argv", ")", ":", "\n", "\t\t\t", "if", "argv", "[", "i", "]", "==", "\"-s\"", ":", "\n", "\t\t\t\t", "i", "=", "i", "+", "1", "\n", "self", ".", "solver_type", "=", "int", "(", "argv", "[", "i", "]", ")", "\n", "self", ".", "flag_solver_specified", "=", "True", "\n", "", "elif", "argv", "[", "i", "]", "==", "\"-c\"", ":", "\n", "\t\t\t\t", "i", "=", "i", "+", "1", "\n", "self", ".", "C", "=", "float", "(", "argv", "[", "i", "]", ")", "\n", "self", ".", "flag_C_specified", "=", "True", "\n", "", "elif", "argv", "[", "i", "]", "==", "\"-p\"", ":", "\n", "\t\t\t\t", "i", "=", "i", "+", "1", "\n", "self", ".", "p", "=", "float", "(", "argv", "[", "i", "]", ")", "\n", "self", ".", "flag_p_specified", "=", "True", "\n", "", "elif", "argv", "[", "i", "]", "==", "\"-e\"", ":", "\n", "\t\t\t\t", "i", "=", "i", "+", "1", "\n", "self", ".", "eps", "=", "float", "(", "argv", "[", "i", "]", ")", "\n", "", "elif", "argv", "[", "i", "]", "==", "\"-B\"", ":", "\n", "\t\t\t\t", "i", "=", "i", "+", "1", "\n", "self", ".", "bias", "=", "float", "(", "argv", "[", "i", "]", ")", "\n", "", "elif", "argv", "[", "i", "]", "==", "\"-v\"", ":", "\n", "\t\t\t\t", "i", "=", "i", "+", "1", "\n", "self", ".", "flag_cross_validation", "=", "1", "\n", "self", ".", "nr_fold", "=", "int", "(", "argv", "[", "i", "]", ")", "\n", "if", "self", ".", "nr_fold", "<", "2", ":", "\n", "\t\t\t\t\t", "raise", "ValueError", "(", "\"n-fold cross validation: n must >= 2\"", ")", "\n", "", "", "elif", "argv", "[", "i", "]", "==", "\"-n\"", ":", "\n", "\t\t\t\t", "i", "=", "i", "+", "1", "\n", "self", ".", "flag_omp", "=", "True", "\n", "self", ".", "nr_thread", "=", "int", "(", "argv", "[", "i", "]", ")", "\n", "", "elif", "argv", "[", "i", "]", ".", "startswith", "(", "\"-w\"", ")", ":", "\n", "\t\t\t\t", "i", "=", "i", "+", "1", "\n", "self", ".", "nr_weight", "+=", "1", "\n", "weight_label", "+=", "[", "int", "(", "argv", "[", "i", "-", "1", "]", "[", "2", ":", "]", ")", "]", "\n", "weight", "+=", "[", "float", "(", "argv", "[", "i", "]", ")", "]", "\n", "", "elif", "argv", "[", "i", "]", "==", "\"-q\"", ":", "\n", "\t\t\t\t", "self", ".", "print_func", "=", "PRINT_STRING_FUN", "(", "print_null", ")", "\n", "", "elif", "argv", "[", "i", "]", "==", "\"-C\"", ":", "\n", "\t\t\t\t", "self", ".", "flag_find_parameters", "=", "True", "\n", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "(", "\"Wrong options\"", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "liblinear", ".", "set_print_string_function", "(", "self", ".", "print_func", ")", "\n", "self", ".", "weight_label", "=", "(", "c_int", "*", "self", ".", "nr_weight", ")", "(", ")", "\n", "self", ".", "weight", "=", "(", "c_double", "*", "self", ".", "nr_weight", ")", "(", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "nr_weight", ")", ":", "\n", "\t\t\t", "self", ".", "weight", "[", "i", "]", "=", "weight", "[", "i", "]", "\n", "self", ".", "weight_label", "[", "i", "]", "=", "weight_label", "[", "i", "]", "\n", "\n", "# default solver for parameter selection is L2R_L2LOSS_SVC", "\n", "", "if", "self", ".", "flag_find_parameters", ":", "\n", "\t\t\t", "if", "not", "self", ".", "flag_cross_validation", ":", "\n", "\t\t\t\t", "self", ".", "nr_fold", "=", "5", "\n", "", "if", "not", "self", ".", "flag_solver_specified", ":", "\n", "\t\t\t\t", "self", ".", "solver_type", "=", "L2R_L2LOSS_SVC", "\n", "self", ".", "flag_solver_specified", "=", "True", "\n", "", "elif", "self", ".", "solver_type", "not", "in", "[", "L2R_LR", ",", "L2R_L2LOSS_SVC", ",", "L2R_L2LOSS_SVR", "]", ":", "\n", "\t\t\t\t", "raise", "ValueError", "(", "\"Warm-start parameter search only available for -s 0, -s 2 and -s 11\"", ")", "\n", "\n", "", "", "if", "self", ".", "flag_omp", ":", "\n", "\t\t\t", "if", "not", "self", ".", "flag_solver_specified", ":", "\n", "\t\t\t\t", "self", ".", "solver_type", "=", "L2R_L2LOSS_SVC", "\n", "self", ".", "flag_solver_specified", "=", "True", "\n", "", "elif", "self", ".", "solver_type", "not", "in", "[", "L2R_LR", ",", "L2R_L2LOSS_SVC", ",", "L2R_L2LOSS_SVR", ",", "L2R_L2LOSS_SVC_DUAL", ",", "L2R_L1LOSS_SVC_DUAL", ",", "L1R_LR", ",", "L1R_L2LOSS_SVC", "]", ":", "\n", "\t\t\t\t", "raise", "ValueError", "(", "\"Parallel LIBLINEAR is only available for -s 0, 1, 2, 3, 5, 6, 11 now\"", ")", "\n", "\n", "", "", "if", "self", ".", "eps", "==", "float", "(", "'inf'", ")", ":", "\n", "\t\t\t", "if", "self", ".", "solver_type", "in", "[", "L2R_LR", ",", "L2R_L2LOSS_SVC", "]", ":", "\n", "\t\t\t\t", "self", ".", "eps", "=", "0.01", "\n", "", "elif", "self", ".", "solver_type", "in", "[", "L2R_L2LOSS_SVR", "]", ":", "\n", "\t\t\t\t", "self", ".", "eps", "=", "0.0001", "\n", "", "elif", "self", ".", "solver_type", "in", "[", "L2R_L2LOSS_SVC_DUAL", ",", "L2R_L1LOSS_SVC_DUAL", ",", "MCSVM_CS", ",", "L2R_LR_DUAL", "]", ":", "\n", "\t\t\t\t", "self", ".", "eps", "=", "0.1", "\n", "", "elif", "self", ".", "solver_type", "in", "[", "L1R_L2LOSS_SVC", ",", "L1R_LR", "]", ":", "\n", "\t\t\t\t", "self", ".", "eps", "=", "0.01", "\n", "", "elif", "self", ".", "solver_type", "in", "[", "L2R_L2LOSS_SVR_DUAL", ",", "L2R_L1LOSS_SVR_DUAL", "]", ":", "\n", "\t\t\t\t", "self", ".", "eps", "=", "0.1", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.__init__": [[365, 367], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "__createfrom__", "=", "'python'", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.__del__": [[368, 372], ["hasattr", "liblinear.free_and_destroy_model", "pointer"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "# free memory created by C to avoid memory leak", "\n", "\t\t", "if", "hasattr", "(", "self", ",", "'__createfrom__'", ")", "and", "self", ".", "__createfrom__", "==", "'C'", ":", "\n", "\t\t\t", "liblinear", ".", "free_and_destroy_model", "(", "pointer", "(", "self", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_nr_feature": [[373, 375], ["liblinear.get_nr_feature"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_nr_feature"], ["", "", "def", "get_nr_feature", "(", "self", ")", ":", "\n", "\t\t", "return", "liblinear", ".", "get_nr_feature", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_nr_class": [[376, 378], ["liblinear.get_nr_class"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_nr_class"], ["", "def", "get_nr_class", "(", "self", ")", ":", "\n", "\t\t", "return", "liblinear", ".", "get_nr_class", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_labels": [[379, 384], ["liblinear.model.get_nr_class", "liblinear.get_labels"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_nr_class", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_labels"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "\t\t", "nr_class", "=", "self", ".", "get_nr_class", "(", ")", "\n", "labels", "=", "(", "c_int", "*", "nr_class", ")", "(", ")", "\n", "liblinear", ".", "get_labels", "(", "self", ",", "labels", ")", "\n", "return", "labels", "[", ":", "nr_class", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_decfun_coef": [[385, 387], ["liblinear.get_decfun_coef"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_decfun_coef"], ["", "def", "get_decfun_coef", "(", "self", ",", "feat_idx", ",", "label_idx", "=", "0", ")", ":", "\n", "\t\t", "return", "liblinear", ".", "get_decfun_coef", "(", "self", ",", "feat_idx", ",", "label_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_decfun_bias": [[388, 390], ["liblinear.get_decfun_bias"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_decfun_bias"], ["", "def", "get_decfun_bias", "(", "self", ",", "label_idx", "=", "0", ")", ":", "\n", "\t\t", "return", "liblinear", ".", "get_decfun_bias", "(", "self", ",", "label_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_decfun": [[391, 395], ["liblinear.get_decfun_bias", "liblinear.get_decfun_coef", "range"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_decfun_bias", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.get_decfun_coef"], ["", "def", "get_decfun", "(", "self", ",", "label_idx", "=", "0", ")", ":", "\n", "\t\t", "w", "=", "[", "liblinear", ".", "get_decfun_coef", "(", "self", ",", "feat_idx", ",", "label_idx", ")", "for", "feat_idx", "in", "range", "(", "1", ",", "self", ".", "nr_feature", "+", "1", ")", "]", "\n", "b", "=", "liblinear", ".", "get_decfun_bias", "(", "self", ",", "label_idx", ")", "\n", "return", "(", "w", ",", "b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.is_probability_model": [[396, 398], ["liblinear.check_probability_model"], "methods", ["None"], ["", "def", "is_probability_model", "(", "self", ")", ":", "\n", "\t\t", "return", "(", "liblinear", ".", "check_probability_model", "(", "self", ")", "==", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.model.is_regression_model": [[399, 401], ["liblinear.check_regression_model"], "methods", ["None"], ["", "def", "is_regression_model", "(", "self", ")", ":", "\n", "\t\t", "return", "(", "liblinear", ".", "check_regression_model", "(", "self", ")", "==", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.print_null": [[53, 55], ["None"], "function", ["None"], ["def", "print_null", "(", "s", ")", ":", "\n", "\t", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.genFields": [[56, 58], ["list", "zip"], "function", ["None"], ["", "def", "genFields", "(", "names", ",", "types", ")", ":", "\n", "\t", "return", "list", "(", "zip", "(", "names", ",", "types", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.fillprototype": [[59, 62], ["None"], "function", ["None"], ["", "def", "fillprototype", "(", "f", ",", "restype", ",", "argtypes", ")", ":", "\n", "\t", "f", ".", "restype", "=", "restype", "\n", "f", ".", "argtypes", "=", "argtypes", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.gen_feature_nodearray": [[71, 118], ["isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "enumerate", "enumerate", "len", "len", "isinstance", "isinstance", "len", "isinstance", "filter", "sorted", "TypeError", "len", "scipy.where", "xi.nonzero", "xi.keys", "isinstance", "filter", "scipy.where", "range", "len"], "function", ["None"], ["", "", "def", "gen_feature_nodearray", "(", "xi", ",", "feature_max", "=", "None", ")", ":", "\n", "\t", "if", "feature_max", ":", "\n", "\t\t", "assert", "(", "isinstance", "(", "feature_max", ",", "int", ")", ")", "\n", "\n", "", "xi_shift", "=", "0", "# ensure correct indices of xi", "\n", "if", "scipy", "and", "isinstance", "(", "xi", ",", "tuple", ")", "and", "len", "(", "xi", ")", "==", "2", "and", "isinstance", "(", "xi", "[", "0", "]", ",", "scipy", ".", "ndarray", ")", "and", "isinstance", "(", "xi", "[", "1", "]", ",", "scipy", ".", "ndarray", ")", ":", "# for a sparse vector", "\n", "\t\t", "index_range", "=", "xi", "[", "0", "]", "+", "1", "# index starts from 1", "\n", "if", "feature_max", ":", "\n", "\t\t\t", "index_range", "=", "index_range", "[", "scipy", ".", "where", "(", "index_range", "<=", "feature_max", ")", "]", "\n", "", "", "elif", "scipy", "and", "isinstance", "(", "xi", ",", "scipy", ".", "ndarray", ")", ":", "\n", "\t\t", "xi_shift", "=", "1", "\n", "index_range", "=", "xi", ".", "nonzero", "(", ")", "[", "0", "]", "+", "1", "# index starts from 1", "\n", "if", "feature_max", ":", "\n", "\t\t\t", "index_range", "=", "index_range", "[", "scipy", ".", "where", "(", "index_range", "<=", "feature_max", ")", "]", "\n", "", "", "elif", "isinstance", "(", "xi", ",", "(", "dict", ",", "list", ",", "tuple", ")", ")", ":", "\n", "\t\t", "if", "isinstance", "(", "xi", ",", "dict", ")", ":", "\n", "\t\t\t", "index_range", "=", "xi", ".", "keys", "(", ")", "\n", "", "elif", "isinstance", "(", "xi", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "\t\t\t", "xi_shift", "=", "1", "\n", "index_range", "=", "range", "(", "1", ",", "len", "(", "xi", ")", "+", "1", ")", "\n", "", "index_range", "=", "filter", "(", "lambda", "j", ":", "xi", "[", "j", "-", "xi_shift", "]", "!=", "0", ",", "index_range", ")", "\n", "\n", "if", "feature_max", ":", "\n", "\t\t\t", "index_range", "=", "filter", "(", "lambda", "j", ":", "j", "<=", "feature_max", ",", "index_range", ")", "\n", "", "index_range", "=", "sorted", "(", "index_range", ")", "\n", "", "else", ":", "\n", "\t\t", "raise", "TypeError", "(", "'xi should be a dictionary, list, tuple, 1-d numpy array, or tuple of (index, data)'", ")", "\n", "\n", "", "ret", "=", "(", "feature_node", "*", "(", "len", "(", "index_range", ")", "+", "2", ")", ")", "(", ")", "\n", "ret", "[", "-", "1", "]", ".", "index", "=", "-", "1", "# for bias term", "\n", "ret", "[", "-", "2", "]", ".", "index", "=", "-", "1", "\n", "\n", "if", "scipy", "and", "isinstance", "(", "xi", ",", "tuple", ")", "and", "len", "(", "xi", ")", "==", "2", "and", "isinstance", "(", "xi", "[", "0", "]", ",", "scipy", ".", "ndarray", ")", "and", "isinstance", "(", "xi", "[", "1", "]", ",", "scipy", ".", "ndarray", ")", ":", "# for a sparse vector", "\n", "\t\t", "for", "idx", ",", "j", "in", "enumerate", "(", "index_range", ")", ":", "\n", "\t\t\t", "ret", "[", "idx", "]", ".", "index", "=", "j", "\n", "ret", "[", "idx", "]", ".", "value", "=", "(", "xi", "[", "1", "]", ")", "[", "idx", "]", "\n", "", "", "else", ":", "\n", "\t\t", "for", "idx", ",", "j", "in", "enumerate", "(", "index_range", ")", ":", "\n", "\t\t\t", "ret", "[", "idx", "]", ".", "index", "=", "j", "\n", "ret", "[", "idx", "]", ".", "value", "=", "xi", "[", "j", "-", "xi_shift", "]", "\n", "\n", "", "", "max_idx", "=", "0", "\n", "if", "len", "(", "index_range", ")", ">", "0", ":", "\n", "\t\t", "max_idx", "=", "index_range", "[", "-", "1", "]", "\n", "", "return", "ret", ",", "max_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.csr_to_problem_jit": [[126, 134], ["range", "range"], "function", ["None"], ["", "@", "jit", "\n", "def", "csr_to_problem_jit", "(", "l", ",", "x_val", ",", "x_ind", ",", "x_rowptr", ",", "prob_val", ",", "prob_ind", ",", "prob_rowptr", ")", ":", "\n", "\t", "for", "i", "in", "range", "(", "l", ")", ":", "\n", "\t\t", "b1", ",", "e1", "=", "x_rowptr", "[", "i", "]", ",", "x_rowptr", "[", "i", "+", "1", "]", "\n", "b2", ",", "e2", "=", "prob_rowptr", "[", "i", "]", ",", "prob_rowptr", "[", "i", "+", "1", "]", "-", "2", "\n", "for", "j", "in", "range", "(", "b1", ",", "e1", ")", ":", "\n", "\t\t\t", "prob_ind", "[", "j", "-", "b1", "+", "b2", "]", "=", "x_ind", "[", "j", "]", "+", "1", "\n", "prob_val", "[", "j", "-", "b1", "+", "b2", "]", "=", "x_val", "[", "j", "]", "\n", "", "", "", "def", "csr_to_problem_nojit", "(", "l", ",", "x_val", ",", "x_ind", ",", "x_rowptr", ",", "prob_val", ",", "prob_ind", ",", "prob_rowptr", ")", ":", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.csr_to_problem_nojit": [[134, 140], ["range", "slice", "slice"], "function", ["None"], ["", "", "", "def", "csr_to_problem_nojit", "(", "l", ",", "x_val", ",", "x_ind", ",", "x_rowptr", ",", "prob_val", ",", "prob_ind", ",", "prob_rowptr", ")", ":", "\n", "\t", "for", "i", "in", "range", "(", "l", ")", ":", "\n", "\t\t", "x_slice", "=", "slice", "(", "x_rowptr", "[", "i", "]", ",", "x_rowptr", "[", "i", "+", "1", "]", ")", "\n", "prob_slice", "=", "slice", "(", "prob_rowptr", "[", "i", "]", ",", "prob_rowptr", "[", "i", "+", "1", "]", "-", "2", ")", "\n", "prob_ind", "[", "prob_slice", "]", "=", "x_ind", "[", "x_slice", "]", "+", "1", "\n", "prob_val", "[", "prob_slice", "]", "=", "x_val", "[", "x_slice", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.csr_to_problem": [[141, 153], ["scipy.empty", "x.indptr.copy", "scipy.arange", "liblinear.csr_to_problem_jit", "liblinear.csr_to_problem_nojit"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.csr_to_problem_jit", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.csr_to_problem_nojit"], ["", "", "def", "csr_to_problem", "(", "x", ",", "prob", ")", ":", "\n", "# Extra space for termination node and (possibly) bias term", "\n", "\t", "x_space", "=", "prob", ".", "x_space", "=", "scipy", ".", "empty", "(", "(", "x", ".", "nnz", "+", "x", ".", "shape", "[", "0", "]", "*", "2", ")", ",", "dtype", "=", "feature_node", ")", "\n", "prob", ".", "rowptr", "=", "x", ".", "indptr", ".", "copy", "(", ")", "\n", "prob", ".", "rowptr", "[", "1", ":", "]", "+=", "2", "*", "scipy", ".", "arange", "(", "1", ",", "x", ".", "shape", "[", "0", "]", "+", "1", ")", "\n", "prob_ind", "=", "x_space", "[", "\"index\"", "]", "\n", "prob_val", "=", "x_space", "[", "\"value\"", "]", "\n", "prob_ind", "[", ":", "]", "=", "-", "1", "\n", "if", "jit_enabled", ":", "\n", "\t\t", "csr_to_problem_jit", "(", "x", ".", "shape", "[", "0", "]", ",", "x", ".", "data", ",", "x", ".", "indices", ",", "x", ".", "indptr", ",", "prob_val", ",", "prob_ind", ",", "prob", ".", "rowptr", ")", "\n", "", "else", ":", "\n", "\t\t", "csr_to_problem_nojit", "(", "x", ".", "shape", "[", "0", "]", ",", "x", ".", "data", ",", "x", ".", "indices", ",", "x", ".", "indptr", ",", "prob_val", ",", "prob_ind", ",", "prob", ".", "rowptr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.liblinearsvm.liblinear.toPyModel": [[402, 413], ["bool", "ValueError"], "function", ["None"], ["", "", "def", "toPyModel", "(", "model_ptr", ")", ":", "\n", "\t", "\"\"\"\n\ttoPyModel(model_ptr) -> model\n\n\tConvert a ctypes POINTER(model) to a Python model\n\t\"\"\"", "\n", "if", "bool", "(", "model_ptr", ")", "==", "False", ":", "\n", "\t\t", "raise", "ValueError", "(", "\"Null pointer\"", ")", "\n", "", "m", "=", "model_ptr", ".", "contents", "\n", "m", ".", "__createfrom__", "=", "'C'", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.merge_score.parse_option": [[5, 17], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "parse_option", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'training'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--num-gpu'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "help", "=", "'num of gpu'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-crop'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "'num of crop'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "required", "=", "True", ",", "help", "=", "'num of predict classes'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-clips'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "help", "=", "'num of sampled clips'", ")", "\n", "parser", ".", "add_argument", "(", "'--output-dir'", ",", "type", "=", "str", ",", "default", "=", "'../../output/eval_output_finetune'", ",", "help", "=", "'output director'", ")", "\n", "parser", ".", "add_argument", "(", "'--list-file'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "'list of dataset'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.extract_score_3d.parse_option": [[25, 60], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "parse_option", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'training'", ")", "\n", "\n", "# dataset", "\n", "parser", ".", "add_argument", "(", "'--list-file'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "'list of dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--root-path'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "'root path of dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--format'", ",", "type", "=", "str", ",", "default", "=", "'LMDB'", ",", "\n", "choices", "=", "[", "\"RAW\"", ",", "\"LMDB\"", ",", "\"FRAME\"", "]", ",", "help", "=", "\"video format\"", ")", "\n", "# other parameters", "\n", "parser", ".", "add_argument", "(", "'--time-dim'", ",", "type", "=", "str", ",", "default", "=", "'T'", ",", "\n", "choices", "=", "[", "\"T\"", ",", "\"C\"", "]", ",", "help", "=", "\"dimension for time\"", ")", "\n", "parser", ".", "add_argument", "(", "'--crop-size'", ",", "type", "=", "int", ",", "default", "=", "256", ",", "help", "=", "'crop_size'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "required", "=", "True", ",", "help", "=", "'num of predict classes'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'batch_size'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-workers'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "help", "=", "'num of workers to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-length'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'num of clip length'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'num of sampling steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-segments'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'num of segments'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-clips'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "help", "=", "'num of sampled clips'", ")", "\n", "\n", "# network", "\n", "parser", ".", "add_argument", "(", "'--pooling-name'", ",", "type", "=", "str", ",", "default", "=", "'PoolingAverage'", ",", "help", "=", "'name of pooling architecture'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout-ratio'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'dropout ratio'", ")", "\n", "\n", "# io", "\n", "parser", ".", "add_argument", "(", "'--pretrained-model'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "'path to pretrained weights like imagenet (default: none)'", ")", "\n", "parser", ".", "add_argument", "(", "'--output-dir'", ",", "type", "=", "str", ",", "default", "=", "'../../output/eval_output_finetune'", ",", "help", "=", "'output director'", ")", "\n", "parser", ".", "add_argument", "(", "'--crop-idx'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'the place index [0,1,2]'", ")", "\n", "\n", "# misc", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "help", "=", "'local rank for DistributedDataParallel'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.extract_score_3d.get_loader": [[62, 90], ["torchvision.transforms.Compose", "dataset.video_dataset.VideoRGBTestDataset", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "utils.clip_transforms.ClipResize", "crop", "utils.clip_transforms.ToClipTensor", "utils.clip_transforms.ClipNormalize", "torchvision.transforms.Lambda", "torchvision.transforms.Lambda", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "get_loader", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "crop_idx", "==", "0", ":", "\n", "        ", "crop", "=", "clip_transforms", ".", "ClipCenterCrop", "\n", "", "elif", "args", ".", "crop_idx", "==", "1", ":", "\n", "        ", "crop", "=", "clip_transforms", ".", "ClipFirstCrop", "\n", "", "elif", "args", ".", "crop_idx", "==", "2", ":", "\n", "        ", "crop", "=", "clip_transforms", ".", "ClipThirdCrop", "\n", "\n", "", "test_transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "clip_transforms", ".", "ClipResize", "(", "size", "=", "args", ".", "crop_size", ")", ",", "\n", "crop", "(", "size", "=", "args", ".", "crop_size", ")", ",", "\n", "clip_transforms", ".", "ToClipTensor", "(", ")", ",", "\n", "clip_transforms", ".", "ClipNormalize", "(", "mean", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "std", "=", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", ",", "\n", "transforms", ".", "Lambda", "(", "lambda", "clip", ":", "torch", ".", "stack", "(", "clip", ",", "dim", "=", "1", ")", ")", "if", "args", ".", "time_dim", "==", "\"T\"", "else", "transforms", ".", "Lambda", "(", "\n", "lambda", "clip", ":", "torch", ".", "cat", "(", "clip", ",", "dim", "=", "0", ")", ")", "\n", "]", ")", "\n", "\n", "test_dataset", "=", "VideoRGBTestDataset", "(", "args", ".", "list_file", ",", "num_clips", "=", "args", ".", "num_clips", ",", "\n", "transform", "=", "test_transform", ",", "root_path", "=", "args", ".", "root_path", ",", "\n", "clip_length", "=", "args", ".", "clip_length", ",", "num_steps", "=", "args", ".", "num_steps", ",", "\n", "num_segments", "=", "args", ".", "num_segments", ",", "\n", "format", "=", "args", ".", "format", ")", "\n", "test_sampler", "=", "torch", ".", "utils", ".", "data", ".", "distributed", ".", "DistributedSampler", "(", "test_dataset", ",", "shuffle", "=", "False", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "test_dataset", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "test_sampler", ",", "drop_last", "=", "False", ")", "\n", "return", "test_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.extract_score_3d.build_model": [[92, 97], ["model.backbone.p3da_resnet50.cuda", "extract_score_3d.load_pretrained", "model.backbone.p3da_resnet50"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.load_pretrained", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_resnet50"], ["", "def", "build_model", "(", "args", ")", ":", "\n", "    ", "model", "=", "backbone", "(", "num_classes", "=", "args", ".", "num_classes", ",", "dropout_ratio", "=", "args", ".", "dropout_ratio", ")", ".", "cuda", "(", ")", "\n", "if", "args", ".", "pretrained_model", ":", "\n", "        ", "load_pretrained", "(", "args", ",", "model", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.extract_score_3d.load_pretrained": [[99, 110], ["torch.load", "torch.load", "torch.load", "torch.load", "model.load_state_dict", "logger.info", "logger.info", "logger.info", "k.replace", "ckpt[].items"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.load_state_dict"], ["", "def", "load_pretrained", "(", "args", ",", "model", ")", ":", "\n", "    ", "ckpt", "=", "torch", ".", "load", "(", "args", ".", "pretrained_model", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "'model'", "in", "ckpt", ":", "\n", "        ", "state_dict", "=", "{", "k", ".", "replace", "(", "\"module.\"", ",", "\"\"", ")", ":", "v", "for", "k", ",", "v", "in", "ckpt", "[", "'model'", "]", ".", "items", "(", ")", "}", "\n", "", "else", ":", "\n", "        ", "state_dict", "=", "ckpt", "\n", "\n", "", "[", "misskeys", ",", "unexpkeys", "]", "=", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "False", ")", "\n", "logger", ".", "info", "(", "'Missing keys: {}'", ".", "format", "(", "misskeys", ")", ")", "\n", "logger", ".", "info", "(", "'Unexpect keys: {}'", ".", "format", "(", "unexpkeys", ")", ")", "\n", "logger", ".", "info", "(", "\"==> loaded checkpoint '{}'\"", ".", "format", "(", "args", ".", "pretrained_model", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.extract_score_3d.main": [[112, 140], ["extract_score_3d.get_loader", "len", "logger.info", "extract_score_3d.build_model", "torch.nn.parallel.DistributedDataParallel.eval", "torch.nn.parallel.DistributedDataParallel", "numpy.zeros", "numpy.save", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "os.path.join", "x.size", "torch.nn.parallel.DistributedDataParallel.", "isinstance", "len", "logger.info", "model.data.cpu().numpy", "len", "len", "score[].data.cpu().numpy", "score[].data.cpu().numpy", "model.data.cpu", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "score[].data.cpu", "score[].data.cpu"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.get_loader", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.build_model"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "test_loader", "=", "get_loader", "(", "args", ")", "\n", "n_data", "=", "len", "(", "test_loader", ".", "dataset", ")", "\n", "logger", ".", "info", "(", "\"length of testing dataset: {}\"", ".", "format", "(", "n_data", ")", ")", "\n", "\n", "model", "=", "build_model", "(", "args", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "model", "=", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "broadcast_buffers", "=", "False", ")", "\n", "\n", "# routine", "\n", "all_scores", "=", "np", ".", "zeros", "(", "[", "len", "(", "test_loader", ")", "*", "args", ".", "batch_size", ",", "args", ".", "num_classes", "]", ",", "dtype", "=", "np", ".", "float", ")", "\n", "top_idx", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "idx", ",", "(", "x", ",", "cls", ")", "in", "enumerate", "(", "test_loader", ")", ":", "\n", "            ", "if", "(", "idx", "%", "100", "==", "0", ")", "or", "(", "idx", "==", "len", "(", "test_loader", ")", "-", "1", ")", ":", "\n", "                ", "logger", ".", "info", "(", "'{}/{}'", ".", "format", "(", "idx", ",", "len", "(", "test_loader", ")", ")", ")", "\n", "", "bsz", "=", "x", ".", "size", "(", "0", ")", "\n", "score", "=", "model", "(", "x", ")", "\n", "if", "isinstance", "(", "score", ",", "list", ")", ":", "\n", "                ", "score_numpy", "=", "(", "score", "[", "0", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "+", "score", "[", "1", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "/", "2", "\n", "", "else", ":", "\n", "                ", "score_numpy", "=", "score", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "all_scores", "[", "top_idx", ":", "top_idx", "+", "bsz", ",", ":", "]", "=", "score_numpy", "\n", "top_idx", "+=", "bsz", "\n", "", "", "all_scores", "=", "all_scores", "[", ":", "top_idx", ",", ":", "]", "\n", "np", ".", "save", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'all_scores_{}.npy'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "*", "args", ".", "crop_idx", "+", "args", ".", "local_rank", ")", ")", ",", "all_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.str2bool": [[26, 33], ["v.lower", "v.lower", "argparse.ArgumentTypeError"], "function", ["None"], ["def", "str2bool", "(", "v", ")", ":", "\n", "    ", "if", "v", ".", "lower", "(", ")", "in", "(", "'yes'", ",", "'true'", ",", "'t'", ",", "'y'", ",", "'1'", ")", ":", "\n", "        ", "return", "True", "\n", "", "elif", "v", ".", "lower", "(", ")", "in", "(", "'no'", ",", "'false'", ",", "'f'", ",", "'n'", ",", "'0'", ")", ":", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "'Unsupported value encountered.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.parse_option": [[35, 94], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "", "def", "parse_option", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'training'", ")", "\n", "\n", "# dataset", "\n", "parser", ".", "add_argument", "(", "'--dataset-class'", ",", "type", "=", "str", ",", "default", "=", "'video_dataset'", ",", "help", "=", "\"class of dataset\"", ")", "\n", "# for video_dataset", "\n", "parser", ".", "add_argument", "(", "'--list-file'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'path of list file'", ")", "\n", "parser", ".", "add_argument", "(", "'--root-path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'path of root folder'", ")", "\n", "parser", ".", "add_argument", "(", "'--format'", ",", "type", "=", "str", ",", "default", "=", "'LMDB'", ",", "\n", "choices", "=", "[", "\"RAW\"", ",", "\"LMDB\"", ",", "\"FRAME\"", "]", ",", "help", "=", "\"video format\"", ")", "\n", "# other parameters", "\n", "parser", ".", "add_argument", "(", "'--time-dim'", ",", "type", "=", "str", ",", "default", "=", "'T'", ",", "\n", "choices", "=", "[", "\"T\"", ",", "\"C\"", "]", ",", "help", "=", "\"dimension for time\"", ")", "\n", "parser", ".", "add_argument", "(", "'--crop-size'", ",", "type", "=", "int", ",", "default", "=", "224", ",", "help", "=", "'crop_size'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "required", "=", "True", ",", "help", "=", "'num of predict classes'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'batch_size'", ")", "\n", "parser", ".", "add_argument", "(", "'--iter-size'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "help", "=", "'iter_size'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-workers'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "help", "=", "'num of workers to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-segments'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'num of segments'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-length'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'num of clip length'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'num of sampling steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--ra-n'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'num of sampling steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--ra-m'", ",", "type", "=", "int", ",", "default", "=", "23", ",", "help", "=", "'num of sampling steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--horizontal-flip'", ",", "type", "=", "str2bool", ",", "default", "=", "'true'", ",", "help", "=", "'if horizontal flip the data'", ")", "\n", "\n", "# network", "\n", "parser", ".", "add_argument", "(", "'--pooling-name'", ",", "type", "=", "str", ",", "default", "=", "'PoolingAverage'", ",", "help", "=", "'name of pooling architecture'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout-ratio'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "help", "=", "'dropout ratio'", ")", "\n", "parser", ".", "add_argument", "(", "'--frozen-bn'", ",", "type", "=", "str2bool", ",", "default", "=", "'false'", ",", "help", "=", "'if frozen batch_norm layers'", ")", "\n", "\n", "# optimization", "\n", "parser", ".", "add_argument", "(", "'--base-learning-rate'", ",", "'--base-lr'", ",", "type", "=", "float", ",", "default", "=", "0.01", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-epoch'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "'warmup epoch'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-multiplier'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'warmup multiplier'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-decay-epochs'", ",", "type", "=", "int", ",", "default", "=", "[", "50", ",", "100", "]", ",", "nargs", "=", "'+'", ",", "\n", "help", "=", "'for step scheduler. where to decay lr, can be a list'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-decay-rate'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'for step scheduler. decay rate for learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-scheduler'", ",", "type", "=", "str", ",", "default", "=", "'cosine'", ",", "\n", "choices", "=", "[", "\"step\"", ",", "\"cosine\"", "]", ",", "help", "=", "\"learning rate scheduler\"", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "type", "=", "float", ",", "default", "=", "1e-4", ",", "help", "=", "'weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "help", "=", "'momentum for SGD'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "150", ",", "help", "=", "'number of training epochs'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-gradient'", ",", "type", "=", "float", ",", "default", "=", "40", ",", "help", "=", "'norm to clip gradient'", ")", "\n", "parser", ".", "add_argument", "(", "'--loss-weight'", ",", "type", "=", "float", ",", "default", "=", "1", ",", "help", "=", "'loss weight'", ")", "\n", "parser", ".", "add_argument", "(", "'--label-smooth'", ",", "type", "=", "str2bool", ",", "default", "=", "'false'", ",", "help", "=", "'if apply label smooth'", ")", "\n", "\n", "# io", "\n", "parser", ".", "add_argument", "(", "'--pretrained-model'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "'path to pretrained weights like imagenet (default: none)'", ")", "\n", "parser", ".", "add_argument", "(", "'--print-freq'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "'print frequency'", ")", "\n", "parser", ".", "add_argument", "(", "'--save-freq'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "'save frequency'", ")", "\n", "parser", ".", "add_argument", "(", "'--output-dir'", ",", "type", "=", "str", ",", "default", "=", "'../../output/eval_output_finetune'", ",", "help", "=", "'output director'", ")", "\n", "\n", "# misc", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "help", "=", "'local rank for DistributedDataParallel'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.get_loader": [[96, 119], ["torchvision.transforms.Compose", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataset.video_dataset.VideoRGBTrainDataset", "utils.clip_transforms.ClipRandomResizedCrop", "utils.clip_augmentations.ClipRandAugment", "utils.clip_transforms.ClipRandomHorizontalFlip", "utils.clip_transforms.ToClipTensor", "utils.clip_transforms.ClipNormalize", "torchvision.transforms.Lambda", "torchvision.transforms.Lambda", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "get_loader", "(", "args", ")", ":", "\n", "    ", "train_transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "clip_transforms", ".", "ClipRandomResizedCrop", "(", "args", ".", "crop_size", ",", "scale", "=", "(", "0.2", ",", "1.", ")", ",", "ratio", "=", "(", "0.75", ",", "1.3333333333333333", ")", ")", ",", "\n", "ClipRandAugment", "(", "n", "=", "args", ".", "ra_n", ",", "m", "=", "args", ".", "ra_m", ")", ",", "# N = [1, 2, 3], M = [5, 7, 9, 11, 13, 15]", "\n", "clip_transforms", ".", "ClipRandomHorizontalFlip", "(", "p", "=", "0.5", "if", "args", ".", "horizontal_flip", "else", "0.0", ")", ",", "\n", "clip_transforms", ".", "ToClipTensor", "(", ")", ",", "\n", "clip_transforms", ".", "ClipNormalize", "(", "mean", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "std", "=", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", ",", "\n", "transforms", ".", "Lambda", "(", "lambda", "clip", ":", "torch", ".", "stack", "(", "clip", ",", "dim", "=", "1", ")", ")", "if", "args", ".", "time_dim", "==", "\"T\"", "else", "transforms", ".", "Lambda", "(", "\n", "lambda", "clip", ":", "torch", ".", "cat", "(", "clip", ",", "dim", "=", "0", ")", ")", "\n", "]", ")", "\n", "if", "args", ".", "dataset_class", "==", "'video_dataset'", ":", "\n", "        ", "assert", "(", "args", ".", "list_file", "!=", "''", "and", "args", ".", "root_path", "!=", "''", ")", "\n", "train_dataset", "=", "VideoRGBTrainDataset", "(", "list_file", "=", "args", ".", "list_file", ",", "root_path", "=", "args", ".", "root_path", ",", "\n", "transform", "=", "train_transform", ",", "clip_length", "=", "args", ".", "clip_length", ",", "\n", "num_steps", "=", "args", ".", "num_steps", ",", "num_segments", "=", "args", ".", "num_segments", ",", "\n", "format", "=", "args", ".", "format", ")", "\n", "\n", "", "train_sampler", "=", "torch", ".", "utils", ".", "data", ".", "distributed", ".", "DistributedSampler", "(", "train_dataset", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "train_dataset", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "train_sampler", ",", "drop_last", "=", "True", ")", "\n", "return", "train_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.build_model": [[121, 126], ["model.backbone.p3da_resnet50.cuda", "train_3d.load_pretrained", "model.backbone.p3da_resnet50"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.load_pretrained", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.model.backbone.p3da_resnet50"], ["", "def", "build_model", "(", "args", ")", ":", "\n", "    ", "model", "=", "backbone", "(", "num_classes", "=", "args", ".", "num_classes", ",", "dropout_ratio", "=", "args", ".", "dropout_ratio", ")", ".", "cuda", "(", ")", "\n", "if", "args", ".", "pretrained_model", ":", "\n", "        ", "load_pretrained", "(", "args", ",", "model", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.load_pretrained": [[128, 139], ["torch.load", "torch.load", "torch.load", "torch.load", "model.load_state_dict", "logger.info", "logger.info", "logger.info", "k.replace", "k.replace", "ckpt[].items", "torch.load.items"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.load_state_dict"], ["", "def", "load_pretrained", "(", "args", ",", "model", ")", ":", "\n", "    ", "ckpt", "=", "torch", ".", "load", "(", "args", ".", "pretrained_model", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "'model'", "in", "ckpt", ":", "\n", "        ", "state_dict", "=", "{", "k", ".", "replace", "(", "\"module.backbone.\"", ",", "\"\"", ")", ":", "v", "for", "k", ",", "v", "in", "ckpt", "[", "'model'", "]", ".", "items", "(", ")", "}", "\n", "", "else", ":", "\n", "        ", "state_dict", "=", "{", "k", ".", "replace", "(", "\"module.backbone.\"", ",", "\"\"", ")", ":", "v", "for", "k", ",", "v", "in", "ckpt", ".", "items", "(", ")", "}", "\n", "\n", "", "[", "misskeys", ",", "unexpkeys", "]", "=", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "False", ")", "\n", "logger", ".", "info", "(", "'Missing keys: {}'", ".", "format", "(", "misskeys", ")", ")", "\n", "logger", ".", "info", "(", "'Unexpect keys: {}'", ".", "format", "(", "unexpkeys", ")", ")", "\n", "logger", ".", "info", "(", "\"==> loaded checkpoint '{}'\"", ".", "format", "(", "args", ".", "pretrained_model", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.save_checkpoint": [[141, 153], ["logger.info", "torch.save", "torch.save", "torch.save", "torch.save", "model.state_dict", "optimizer.state_dict", "scheduler.state_dict", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "os.path.join"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.state_dict"], ["", "def", "save_checkpoint", "(", "args", ",", "epoch", ",", "model", ",", "optimizer", ",", "scheduler", ")", ":", "\n", "    ", "logger", ".", "info", "(", "'==> Saving...'", ")", "\n", "state", "=", "{", "\n", "'opt'", ":", "args", ",", "\n", "'model'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'scheduler'", ":", "scheduler", ".", "state_dict", "(", ")", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "}", "\n", "torch", ".", "save", "(", "state", ",", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'current.pth'", ")", ")", "\n", "if", "epoch", "%", "args", ".", "save_freq", "==", "0", ":", "\n", "        ", "torch", ".", "save", "(", "state", ",", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'ckpt_epoch_{}.pth'", ".", "format", "(", "epoch", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.main": [[155, 196], ["train_3d.get_loader", "len", "logger.info", "train_3d.build_model", "torch.nn.CrossEntropyLoss().cuda", "torch.nn.CrossEntropyLoss().cuda", "torch.nn.CrossEntropyLoss().cuda", "torch.nn.CrossEntropyLoss().cuda", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "utils.lr_scheduler.get_scheduler", "torch.nn.parallel.DistributedDataParallel", "range", "torch.get_rank", "logger.info", "filter", "len", "torch.get_rank", "tensorboardX.SummaryWriter", "get_loader.sampler.set_epoch", "time.time", "train_3d.train", "logger.info", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.parallel.DistributedDataParallel.parameters", "tensorboardX.SummaryWriter.add_scalar", "tensorboardX.SummaryWriter.add_scalar", "torch.get_rank", "train_3d.save_checkpoint", "time.time"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.get_loader", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.build_model", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.get_scheduler", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.train", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.save_checkpoint"], ["", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "train_loader", "=", "get_loader", "(", "args", ")", "\n", "n_data", "=", "len", "(", "train_loader", ".", "dataset", ")", "\n", "logger", ".", "info", "(", "\"length of training dataset: {}\"", ".", "format", "(", "n_data", ")", ")", "\n", "\n", "model", "=", "build_model", "(", "args", ")", "\n", "# print network architecture", "\n", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "logger", ".", "info", "(", "model", ")", "\n", "\n", "", "criterion", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", ",", "\n", "lr", "=", "args", ".", "base_learning_rate", ",", "\n", "momentum", "=", "args", ".", "momentum", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", "nesterov", "=", "True", ")", "\n", "scheduler", "=", "get_scheduler", "(", "optimizer", ",", "len", "(", "train_loader", ")", ",", "args", ")", "\n", "\n", "model", "=", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "broadcast_buffers", "=", "True", ",", "\n", "find_unused_parameters", "=", "True", ")", "\n", "\n", "# tensorboard", "\n", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "summary_writer", "=", "SummaryWriter", "(", "log_dir", "=", "args", ".", "output_dir", ")", "\n", "", "else", ":", "\n", "        ", "summary_writer", "=", "None", "\n", "\n", "# routine", "\n", "", "for", "epoch", "in", "range", "(", "1", ",", "args", ".", "epochs", "+", "1", ")", ":", "\n", "        ", "train_loader", ".", "sampler", ".", "set_epoch", "(", "epoch", ")", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "loss", "=", "train", "(", "epoch", ",", "train_loader", ",", "model", ",", "criterion", ",", "optimizer", ",", "scheduler", ",", "args", ")", "\n", "logger", ".", "info", "(", "'epoch {}, total time {:.2f}'", ".", "format", "(", "epoch", ",", "time", ".", "time", "(", ")", "-", "tic", ")", ")", "\n", "if", "summary_writer", "is", "not", "None", ":", "\n", "# tensorboard logger", "\n", "            ", "summary_writer", ".", "add_scalar", "(", "'ins_loss'", ",", "loss", ",", "epoch", ")", "\n", "summary_writer", ".", "add_scalar", "(", "'learning_rate'", ",", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ",", "epoch", ")", "\n", "", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "# save model", "\n", "            ", "save_checkpoint", "(", "args", ",", "epoch", ",", "model", ",", "scheduler", ",", "optimizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.frozen_bn": [[198, 209], ["model.named_modules", "isinstance", "m.eval", "print"], "function", ["None"], ["", "", "", "def", "frozen_bn", "(", "model", ")", ":", "\n", "    ", "first_bn", "=", "True", "\n", "for", "name", ",", "m", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "(", "torch", ".", "nn", ".", "BatchNorm2d", ",", "torch", ".", "nn", ".", "BatchNorm3d", ")", ")", ":", "\n", "            ", "if", "first_bn", ":", "\n", "                ", "first_bn", "=", "False", "\n", "print", "(", "'Skip frozen first bn layer: '", "+", "name", ")", "\n", "continue", "\n", "", "m", ".", "eval", "(", ")", "\n", "m", ".", "weight", ".", "requires_grad", "=", "False", "\n", "m", ".", "bias", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.train": [[211, 276], ["model.train", "utils.util.AverageMeter", "utils.util.AverageMeter", "utils.util.AverageMeter", "time.time", "optimizer.zero_grad", "torch.cuda.amp.GradScaler", "enumerate", "train_3d.frozen_bn", "x.cuda.size", "x.cuda.cuda", "label.cuda.cuda", "model", "isinstance", "torch.cuda.amp.GradScaler.scale().backward", "scheduler.step", "utils.util.AverageMeter.update", "utils.util.AverageMeter.update", "utils.util.AverageMeter.update", "time.time", "criterion", "torch.cuda.amp.GradScaler.unscale_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.cuda.amp.GradScaler.step", "torch.cuda.amp.GradScaler.update", "optimizer.zero_grad", "criterion.item", "logger.info", "criterion", "criterion", "torch.cuda.amp.GradScaler.scale", "filter", "time.time", "model.parameters", "len", "criterion.item", "next", "iter"], "function", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.train", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.downstream_finetune.train_3d.frozen_bn", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.step", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.lr_scheduler.GradualWarmupScheduler.step", "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.utils.util.AverageMeter.update"], ["", "", "", "def", "train", "(", "epoch", ",", "train_loader", ",", "model", ",", "criterion", ",", "optimizer", ",", "scheduler", ",", "args", ")", ":", "\n", "    ", "model", ".", "train", "(", ")", "\n", "if", "args", ".", "frozen_bn", ":", "\n", "        ", "frozen_bn", "(", "model", ")", "\n", "\n", "", "batch_time", "=", "AverageMeter", "(", ")", "\n", "loss_meter", "=", "AverageMeter", "(", ")", "\n", "norm_meter", "=", "AverageMeter", "(", ")", "\n", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "scaler", "=", "GradScaler", "(", ")", "\n", "bnorm", "=", "0", "\n", "\n", "for", "idx", ",", "train_data", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "        ", "x", "=", "train_data", "[", "0", "]", "\n", "label", "=", "train_data", "[", "1", "]", "\n", "\n", "bsz", "=", "x", ".", "size", "(", "0", ")", "\n", "\n", "# forward", "\n", "x", "=", "x", ".", "cuda", "(", "non_blocking", "=", "True", ")", "# clip", "\n", "label", "=", "label", ".", "cuda", "(", "non_blocking", "=", "True", ")", "# label", "\n", "\n", "# with torch.cuda.amp.autocast():", "\n", "# forward and get the predict score", "\n", "score", "=", "model", "(", "x", ")", "\n", "# get crossentropy loss", "\n", "if", "isinstance", "(", "score", ",", "list", ")", ":", "\n", "            ", "loss", "=", "criterion", "(", "score", "[", "0", "]", ",", "label", ")", "+", "criterion", "(", "score", "[", "1", "]", ",", "label", ")", "\n", "", "else", ":", "\n", "            ", "loss", "=", "criterion", "(", "score", ",", "label", ")", "\n", "\n", "# backward", "\n", "", "scaler", ".", "scale", "(", "loss", "/", "args", ".", "iter_size", "*", "args", ".", "loss_weight", ")", ".", "backward", "(", ")", "\n", "\n", "if", "(", "idx", "+", "1", ")", "%", "args", ".", "iter_size", "==", "0", ":", "\n", "            ", "scaler", ".", "unscale_", "(", "optimizer", ")", "\n", "bnorm", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", ",", "\n", "args", ".", "clip_gradient", ")", "\n", "scaler", ".", "step", "(", "optimizer", ")", "\n", "scaler", ".", "update", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "", "scheduler", ".", "step", "(", ")", "\n", "\n", "# update meters", "\n", "loss_meter", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "bsz", ")", "\n", "norm_meter", ".", "update", "(", "bnorm", ",", "bsz", ")", "\n", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "# print info", "\n", "if", "idx", "%", "args", ".", "print_freq", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "'Train: [{:>3d}]/[{:>4d}/{:>4d}] BT={:>0.3f}/{:>0.3f} LR={:>0.3f} Loss={:>0.3f}/{:>0.3f} GradNorm={:>0.3f}/{:>0.3f}'", ".", "format", "(", "\n", "epoch", ",", "idx", ",", "len", "(", "train_loader", ")", ",", "\n", "batch_time", ".", "val", ",", "batch_time", ".", "avg", ",", "\n", "next", "(", "iter", "(", "optimizer", ".", "param_groups", ")", ")", "[", "'lr'", "]", ",", "\n", "loss", ".", "item", "(", ")", ",", "loss_meter", ".", "avg", ",", "\n", "bnorm", ",", "norm_meter", ".", "avg", "\n", ")", ")", "\n", "\n", "", "", "return", "loss_meter", ".", "avg", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.MemoryMCL.__init__": [[8, 20], ["torch.nn.Module.__init__", "NCEContrast.MemoryMCL.register_buffer", "torch.rand().mul_().add_", "NCEContrast.MemoryMCL.register_buffer", "torch.tensor", "math.sqrt", "torch.rand().mul_", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__"], ["def", "__init__", "(", "self", ",", "feature_dim", ",", "queue_size", ",", "temperature", "=", "0.07", ",", "temperature_intra", "=", "1.0", ",", "multi_clip", "=", "True", ")", ":", "\n", "        ", "super", "(", "MemoryMCL", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "queue_size", "=", "queue_size", "\n", "self", ".", "temperature", "=", "temperature", "\n", "self", ".", "temperature_intra", "=", "temperature_intra", "\n", "self", ".", "index", "=", "0", "\n", "self", ".", "multi_clip", "=", "multi_clip", "\n", "# noinspection PyCallingNonCallable", "\n", "self", ".", "register_buffer", "(", "'params'", ",", "torch", ".", "tensor", "(", "[", "-", "1", "]", ")", ")", "\n", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "feature_dim", "/", "3", ")", "\n", "memory", "=", "torch", ".", "rand", "(", "self", ".", "queue_size", ",", "feature_dim", ",", "requires_grad", "=", "False", ")", ".", "mul_", "(", "2", "*", "stdv", ")", ".", "add_", "(", "-", "stdv", ")", "\n", "self", ".", "register_buffer", "(", "'memory'", ",", "memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.MemoryMCL.forward": [[21, 46], ["torch.mm", "torch.div().contiguous", "torch.div().contiguous", "NCEContrast.MemoryMCL.memory.clone().detach().t", "torch.cat", "torch.cat", "torch.no_grad", "torch.fmod", "NCEContrast.MemoryMCL.memory.index_copy_", "k_sf.detach", "torch.div", "torch.cat", "torch.div", "NCEContrast.MemoryMCL.memory.clone().detach", "torch.cat", "torch.mm.repeat", "torch.arange().cuda", "k_df1.detach", "k_df2.detach", "torch.cat", "k_df1.detach", "k_df2.detach", "NCEContrast.MemoryMCL.memory.clone", "torch.arange", "l_pos_sf.repeat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "q", ",", "k_sf", ",", "k_df1", "=", "None", ",", "k_df2", "=", "None", ",", "k_all_sf", "=", "None", ",", "k_all_df1", "=", "None", ",", "k_all_df2", "=", "None", ",", "inter", "=", "True", ")", ":", "\n", "        ", "l_pos_sf", "=", "(", "q", "*", "k_sf", ".", "detach", "(", ")", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "# shape: (batchSize, 1)", "\n", "if", "inter", ":", "\n", "            ", "l_neg", "=", "torch", ".", "mm", "(", "q", ",", "self", ".", "memory", ".", "clone", "(", ")", ".", "detach", "(", ")", ".", "t", "(", ")", ")", "\n", "if", "self", ".", "multi_clip", ":", "\n", "                ", "l_pos_df1", "=", "(", "q", "*", "k_df1", ".", "detach", "(", ")", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "# shape: (batchSize, 1)", "\n", "l_pos_df2", "=", "(", "q", "*", "k_df2", ".", "detach", "(", ")", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "# shape: (batchSize, 1)", "\n", "out", "=", "torch", ".", "cat", "(", "(", "torch", ".", "cat", "(", "(", "l_pos_sf", ",", "l_pos_df1", ",", "l_pos_df2", ")", ",", "dim", "=", "0", ")", ",", "l_neg", ".", "repeat", "(", "3", ",", "1", ")", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "out", "=", "torch", ".", "cat", "(", "(", "l_pos_sf", ",", "l_neg", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "out", "=", "torch", ".", "div", "(", "out", ",", "self", ".", "temperature", ")", ".", "contiguous", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "# update memory", "\n", "                ", "k_all", "=", "torch", ".", "cat", "(", "(", "k_all_sf", ",", "k_all_df1", ",", "k_all_df2", ")", ",", "dim", "=", "0", ")", "if", "self", ".", "multi_clip", "else", "k_all_sf", "\n", "all_size", "=", "k_all", ".", "shape", "[", "0", "]", "\n", "out_ids", "=", "torch", ".", "fmod", "(", "torch", ".", "arange", "(", "all_size", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "+", "self", ".", "index", ",", "self", ".", "queue_size", ")", "\n", "self", ".", "memory", ".", "index_copy_", "(", "0", ",", "out_ids", ",", "k_all", ")", "\n", "self", ".", "index", "=", "(", "self", ".", "index", "+", "all_size", ")", "%", "self", ".", "queue_size", "\n", "return", "out", ",", "l_pos_sf", "\n", "", "", "else", ":", "\n", "# out intra-frame similarity", "\n", "            ", "l_pos_df1", "=", "(", "q", "*", "k_df1", ".", "detach", "(", ")", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "# shape: (batchSize, 1)", "\n", "l_pos_df2", "=", "(", "q", "*", "k_df2", ".", "detach", "(", ")", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "# shape: (batchSize, 1)", "\n", "out", "=", "torch", ".", "div", "(", "torch", ".", "cat", "(", "(", "l_pos_sf", ".", "repeat", "(", "2", ",", "1", ")", ",", "torch", ".", "cat", "(", "(", "l_pos_df1", ",", "l_pos_df2", ")", ",", "dim", "=", "0", ")", ")", ",", "dim", "=", "-", "1", ")", ",", "self", ".", "temperature_intra", ")", ".", "contiguous", "(", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__": [[50, 53], ["torch.nn.Module.__init__", "torch.nn.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "NCESoftmaxLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yihengzhang-cv_mcl-motion-focused-contrastive-learning.contrast.NCEContrast.NCESoftmaxLoss.forward": [[54, 57], ["torch.zeros().long().to", "NCEContrast.NCESoftmaxLoss.criterion", "torch.zeros().long", "torch.zeros"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "label", "=", "torch", ".", "zeros", "(", "[", "x", ".", "shape", "[", "0", "]", "]", ")", ".", "long", "(", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "return", "self", ".", "criterion", "(", "x", ",", "label", ")", "\n", "", "", ""]]}