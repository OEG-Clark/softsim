{"home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.main.get_output_folder": [[25, 74], ["os.listdir", "os.path.join", "os.makedirs", "os.makedirs", "os.makedirs", "os.path.exists", "os.makedirs", "print", "os.path.exists", "os.makedirs", "print", "print", "future.builtins.input", "os.system", "os.path.isdir", "int", "print", "os.path.join", "int.split"], "function", ["None"], ["def", "get_output_folder", "(", "args", ",", "parent_dir", ",", "env_name", ",", "task_name", ")", ":", "\n", "    ", "\"\"\"Return save folder.\n\n    Assumes folders in the parent_dir have suffix -run{run\n    number}. Finds the highest run number and sets the output folder\n    to that number + 1. This is just convenient so that if you run the\n    same script multiple times tensorboard can plot all of the results\n    on the same plots with different names.\n\n    Parameters\n    ----------\n    parent_dir: str\n      Path of the directory containing all experiment runs.\n\n    Returns\n    -------\n    parent_dir/run_dir\n      Path to this run's save directory.\n    \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "parent_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "parent_dir", ")", "\n", "print", "(", "'===== Folder did not exist; creating... %s'", "%", "parent_dir", ")", "\n", "\n", "", "experiment_id", "=", "0", "\n", "for", "folder_name", "in", "os", ".", "listdir", "(", "parent_dir", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "parent_dir", ",", "folder_name", ")", ")", ":", "\n", "            ", "continue", "\n", "", "try", ":", "\n", "            ", "folder_name", "=", "int", "(", "folder_name", ".", "split", "(", "'-run'", ")", "[", "-", "1", "]", "[", "0", "]", ")", "\n", "print", "(", "folder_name", ")", "\n", "if", "folder_name", ">", "experiment_id", ":", "\n", "                ", "experiment_id", "=", "folder_name", "\n", "", "", "except", ":", "\n", "            ", "pass", "\n", "", "", "experiment_id", "+=", "1", "\n", "\n", "parent_dir", "=", "os", ".", "path", ".", "join", "(", "parent_dir", ",", "env_name", ")", "\n", "parent_dir", "=", "parent_dir", "+", "'-run{}'", ".", "format", "(", "experiment_id", ")", "+", "'-'", "+", "task_name", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "parent_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "parent_dir", ")", "\n", "print", "(", "'===== Folder did not exist; creating... %s'", "%", "parent_dir", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'===== Folder exists; delete? %s'", "%", "parent_dir", ")", "\n", "response", "=", "input", "(", "\"Press Enter to continue...\"", ")", "\n", "os", ".", "system", "(", "'rm -rf %s/'", "%", "(", "parent_dir", ")", ")", "\n", "", "os", ".", "makedirs", "(", "parent_dir", "+", "'/videos/'", ")", "\n", "os", ".", "makedirs", "(", "parent_dir", "+", "'/images/'", ")", "\n", "os", ".", "makedirs", "(", "parent_dir", "+", "'/losses/'", ")", "\n", "return", "parent_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.main.main": [[75, 136], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "main.get_output_folder", "gym.make", "print", "print", "print", "print", "deeprl_prj.dqn_keras.DQNAgent", "print", "deeprl_prj.dqn_keras.DQNAgent.fit", "print", "deeprl_prj.dqn_keras.DQNAgent.evaluate"], "function", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.main.get_output_folder", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.fit", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.evaluate"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Run DQN on Atari Breakout'", ")", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "default", "=", "'QuadCopter-v4'", ",", "help", "=", "'Atari env name'", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--output'", ",", "default", "=", "'./log/'", ",", "help", "=", "'Directory to save data to'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "help", "=", "'Random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "default", "=", "0.99", ",", "type", "=", "float", ",", "help", "=", "'Discount factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "default", "=", "32", ",", "type", "=", "int", ",", "help", "=", "'Minibatch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "default", "=", "0.0001", ",", "type", "=", "float", ",", "help", "=", "'Learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--initial_epsilon'", ",", "default", "=", "1.0", ",", "type", "=", "float", ",", "help", "=", "'Initial exploration probability in epsilon-greedy'", ")", "\n", "parser", ".", "add_argument", "(", "'--final_epsilon'", ",", "default", "=", "0.05", ",", "type", "=", "float", ",", "help", "=", "'Final exploration probability in epsilon-greedy'", ")", "\n", "parser", ".", "add_argument", "(", "'--exploration_steps'", ",", "default", "=", "24000", ",", "type", "=", "int", ",", "help", "=", "'Number of steps over which the initial value of epsilon is linearly annealed to its final value'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_samples'", ",", "default", "=", "40000", ",", "type", "=", "int", ",", "help", "=", "'Number of training samples from the environment in training'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_frames'", ",", "default", "=", "4", ",", "type", "=", "int", ",", "help", "=", "'Number of frames to feed to Q-Network'", ")", "\n", "parser", ".", "add_argument", "(", "'--frame_width'", ",", "default", "=", "84", ",", "type", "=", "int", ",", "help", "=", "'Resized frame width'", ")", "\n", "parser", ".", "add_argument", "(", "'--frame_height'", ",", "default", "=", "84", ",", "type", "=", "int", ",", "help", "=", "'Resized frame height'", ")", "\n", "parser", ".", "add_argument", "(", "'--replay_memory_size'", ",", "default", "=", "50000", ",", "type", "=", "int", ",", "help", "=", "'Number of replay memory the agent uses for training'", ")", "\n", "parser", ".", "add_argument", "(", "'--target_update_freq'", ",", "default", "=", "200", ",", "type", "=", "int", ",", "help", "=", "'The frequency with which the target network is updated'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_freq'", ",", "default", "=", "4", ",", "type", "=", "int", ",", "help", "=", "'The frequency of actions wrt Q-network update'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_freq'", ",", "default", "=", "500", ",", "type", "=", "int", ",", "help", "=", "'The frequency with which the network is saved'", ")", "\n", "parser", ".", "add_argument", "(", "'--eval_freq'", ",", "default", "=", "200", ",", "type", "=", "int", ",", "help", "=", "'The frequency with which the policy is evlauted'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_burn_in'", ",", "default", "=", "10000", ",", "type", "=", "int", ",", "help", "=", "'Number of steps to populate the replay memory before training starts'", ")", "\n", "parser", ".", "add_argument", "(", "'--load_network'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "'Load trained mode'", ")", "\n", "parser", ".", "add_argument", "(", "'--load_network_path'", ",", "default", "=", "''", ",", "help", "=", "'the path to the trained mode file'", ")", "\n", "parser", ".", "add_argument", "(", "'--net_mode'", ",", "default", "=", "'dqn'", ",", "help", "=", "'choose the mode of net, can be linear, dqn, duel'", ")", "\n", "parser", ".", "add_argument", "(", "'--max_episode_length'", ",", "default", "=", "1000", ",", "type", "=", "int", ",", "help", "=", "'max length of each episode'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_episodes_at_test'", ",", "default", "=", "20", ",", "type", "=", "int", ",", "help", "=", "'Number of episodes the agent plays at test'", ")", "\n", "parser", ".", "add_argument", "(", "'--ddqn'", ",", "default", "=", "False", ",", "dest", "=", "'ddqn'", ",", "action", "=", "'store_true'", ",", "help", "=", "'enable ddqn'", ")", "\n", "parser", ".", "add_argument", "(", "'--train'", ",", "default", "=", "True", ",", "dest", "=", "'train'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Train mode'", ")", "\n", "parser", ".", "add_argument", "(", "'--test'", ",", "dest", "=", "'train'", ",", "action", "=", "'store_false'", ",", "help", "=", "'Test mode'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_experience'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "'do not use experience replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_target'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "'do not use target fixing'", ")", "\n", "parser", ".", "add_argument", "(", "'--monitor'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "help", "=", "'record video'", ")", "\n", "parser", ".", "add_argument", "(", "'--task_name'", ",", "default", "=", "''", ",", "help", "=", "'task name'", ")", "\n", "parser", ".", "add_argument", "(", "'--recurrent'", ",", "default", "=", "False", ",", "dest", "=", "'recurrent'", ",", "action", "=", "'store_true'", ",", "help", "=", "'enable recurrent DQN'", ")", "\n", "parser", ".", "add_argument", "(", "'--a_t'", ",", "default", "=", "False", ",", "dest", "=", "'a_t'", ",", "action", "=", "'store_true'", ",", "help", "=", "'enable temporal/spatial attention'", ")", "\n", "parser", ".", "add_argument", "(", "'--global_a_t'", ",", "default", "=", "False", ",", "dest", "=", "'global_a_t'", ",", "action", "=", "'store_true'", ",", "help", "=", "'enable global temporal attention'", ")", "\n", "parser", ".", "add_argument", "(", "'--selector'", ",", "default", "=", "False", ",", "dest", "=", "'selector'", ",", "action", "=", "'store_true'", ",", "help", "=", "'enable selector for spatial attention'", ")", "\n", "parser", ".", "add_argument", "(", "'--bidir'", ",", "default", "=", "False", ",", "dest", "=", "'bidir'", ",", "action", "=", "'store_true'", ",", "help", "=", "'enable two layer bidirectional lstm'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "output", "=", "get_output_folder", "(", "args", ",", "args", ".", "output", ",", "args", ".", "env", ",", "args", ".", "task_name", ")", "\n", "\n", "env", "=", "gym", ".", "make", "(", "args", ".", "env", ")", "\n", "print", "(", "\"==== Output saved to: \"", ",", "args", ".", "output", ")", "\n", "print", "(", "\"==== Args used:\"", ")", "\n", "print", "(", "args", ")", "\n", "\n", "# here is where you should start up a session,", "\n", "# create your DQN agent, create your model, etc.", "\n", "# then you can run your fit method.", "\n", "\n", "num_actions", "=", "env", ".", "action_space", ".", "n", "\n", "print", "(", "\">>>> Game \"", ",", "args", ".", "env", ",", "\" #actions: \"", ",", "num_actions", ")", "\n", "\n", "dqn", "=", "DQNAgent", "(", "args", ",", "num_actions", ")", "\n", "if", "args", ".", "train", ":", "\n", "        ", "print", "(", "\">> Training mode.\"", ")", "\n", "dqn", ".", "fit", "(", "env", ",", "args", ".", "num_samples", ",", "args", ".", "max_episode_length", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\">> Evaluation mode.\"", ")", "\n", "dqn", ".", "evaluate", "(", "env", ",", "args", ".", "num_episodes_at_test", ",", "0", ",", "args", ".", "max_episode_length", ",", "args", ".", "monitor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.processState": [[12, 14], ["numpy.reshape"], "function", ["None"], ["def", "processState", "(", "state1", ")", ":", "\n", "    ", "return", "np", ".", "reshape", "(", "state1", ",", "[", "21168", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.updateTargetGraph": [[16, 24], ["len", "enumerate", "print", "op_holder.append", "tfVars[].assign", "var.value"], "function", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append"], ["", "def", "updateTargetGraph", "(", "tfVars", ",", "tau", ")", ":", "\n", "    ", "total_vars", "=", "len", "(", "tfVars", ")", "\n", "op_holder", "=", "[", "]", "\n", "for", "idx", ",", "var", "in", "enumerate", "(", "tfVars", "[", "0", ":", "total_vars", "//", "2", "]", ")", ":", "\n", "        ", "print", "(", "\"copy from %s ===> %s\"", "%", "(", "var", ".", "op", ".", "name", ",", "tfVars", "[", "idx", "+", "total_vars", "//", "2", "]", ".", "op", ".", "name", ")", ")", "\n", "# op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))", "\n", "op_holder", ".", "append", "(", "tfVars", "[", "idx", "+", "total_vars", "//", "2", "]", ".", "assign", "(", "var", ".", "value", "(", ")", ")", ")", "\n", "", "return", "op_holder", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.updateTarget": [[25, 35], ["len", "[].eval", "[].eval", "sess.run", "tensorflow.trainable_variables", "[].eval.all", "[].eval.all", "print", "print", "tensorflow.trainable_variables", "tensorflow.trainable_variables"], "function", ["None"], ["", "def", "updateTarget", "(", "op_holder", ",", "sess", ")", ":", "\n", "    ", "for", "op", "in", "op_holder", ":", "\n", "        ", "sess", ".", "run", "(", "op", ")", "\n", "", "total_vars", "=", "len", "(", "tf", ".", "trainable_variables", "(", ")", ")", "\n", "a", "=", "tf", ".", "trainable_variables", "(", ")", "[", "0", "]", ".", "eval", "(", "session", "=", "sess", ")", "\n", "b", "=", "tf", ".", "trainable_variables", "(", ")", "[", "total_vars", "//", "2", "]", ".", "eval", "(", "session", "=", "sess", ")", "\n", "if", "a", ".", "all", "(", ")", "==", "b", ".", "all", "(", ")", ":", "\n", "        ", "print", "(", "\"Target Set Success\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Target Set Failed\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.saveToCenter": [[37, 69], ["open", "enumerate", "numpy.vstack", "numpy.resize", "numpy.max", "numpy.multiply", "helper.make_gif", "zip", "np.resize.append", "numpy.vstack", "numpy.resize", "helper.make_gif", "csv.writer", "csv.writer.writerow", "myfile.close", "open", "csv.writer", "csv.writer.writerow", "sess.run", "csv.writer.writerows", "numpy.zeros", "numpy.zeros", "numpy.vstack", "sess.run", "np.multiply.append", "numpy.ones", "numpy.reshape", "numpy.ones", "numpy.zeros", "numpy.zeros", "zip", "numpy.min", "numpy.max", "numpy.min", "len", "len", "numpy.mean", "numpy.mean", "len", "len", "len", "str", "len", "str", "len", "str", "len", "str", "str", "str", "numpy.vstack", "numpy.reshape"], "function", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.make_gif", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.make_gif", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append"], ["", "", "def", "saveToCenter", "(", "i", ",", "rList", ",", "jList", ",", "bufferArray", ",", "summaryLength", ",", "h_size", ",", "sess", ",", "mainQN", ",", "time_per_step", ")", ":", "\n", "    ", "with", "open", "(", "'./Center/log.csv'", ",", "'a'", ")", "as", "myfile", ":", "\n", "        ", "state_display", "=", "(", "np", ".", "zeros", "(", "[", "1", ",", "h_size", "]", ")", ",", "np", ".", "zeros", "(", "[", "1", ",", "h_size", "]", ")", ")", "\n", "imagesS", "=", "[", "]", "\n", "for", "idx", ",", "z", "in", "enumerate", "(", "np", ".", "vstack", "(", "bufferArray", "[", ":", ",", "0", "]", ")", ")", ":", "\n", "            ", "img", ",", "state_display", "=", "sess", ".", "run", "(", "[", "mainQN", ".", "salience", ",", "mainQN", ".", "rnn_state", "]", ",", "feed_dict", "=", "{", "mainQN", ".", "scalarInput", ":", "np", ".", "reshape", "(", "bufferArray", "[", "idx", ",", "0", "]", ",", "[", "1", ",", "21168", "]", ")", "/", "255.0", ",", "mainQN", ".", "trainLength", ":", "1", ",", "mainQN", ".", "state_in", ":", "state_display", ",", "mainQN", ".", "batch_size", ":", "1", "}", ")", "\n", "imagesS", ".", "append", "(", "img", ")", "\n", "", "imagesS", "=", "(", "imagesS", "-", "np", ".", "min", "(", "imagesS", ")", ")", "/", "(", "np", ".", "max", "(", "imagesS", ")", "-", "np", ".", "min", "(", "imagesS", ")", ")", "\n", "imagesS", "=", "np", ".", "vstack", "(", "imagesS", ")", "\n", "imagesS", "=", "np", ".", "resize", "(", "imagesS", ",", "[", "len", "(", "imagesS", ")", ",", "84", ",", "84", ",", "3", "]", ")", "\n", "luminance", "=", "np", ".", "max", "(", "imagesS", ",", "3", ")", "\n", "imagesS", "=", "np", ".", "multiply", "(", "np", ".", "ones", "(", "[", "len", "(", "imagesS", ")", ",", "84", ",", "84", ",", "3", "]", ")", ",", "np", ".", "reshape", "(", "luminance", ",", "[", "len", "(", "imagesS", ")", ",", "84", ",", "84", ",", "1", "]", ")", ")", "\n", "make_gif", "(", "np", ".", "ones", "(", "[", "len", "(", "imagesS", ")", ",", "84", ",", "84", ",", "3", "]", ")", ",", "'./Center/frames/sal'", "+", "str", "(", "i", ")", "+", "'.gif'", ",", "duration", "=", "len", "(", "imagesS", ")", "*", "time_per_step", ",", "true_image", "=", "False", ",", "salience", "=", "True", ",", "salIMGS", "=", "luminance", ")", "\n", "\n", "images", "=", "zip", "(", "bufferArray", "[", ":", ",", "0", "]", ")", "\n", "images", ".", "append", "(", "bufferArray", "[", "-", "1", ",", "3", "]", ")", "\n", "images", "=", "np", ".", "vstack", "(", "images", ")", "\n", "images", "=", "np", ".", "resize", "(", "images", ",", "[", "len", "(", "images", ")", ",", "84", ",", "84", ",", "3", "]", ")", "\n", "make_gif", "(", "images", ",", "'./Center/frames/image'", "+", "str", "(", "i", ")", "+", "'.gif'", ",", "duration", "=", "len", "(", "images", ")", "*", "time_per_step", ",", "true_image", "=", "True", ",", "salience", "=", "False", ")", "\n", "\n", "wr", "=", "csv", ".", "writer", "(", "myfile", ",", "quoting", "=", "csv", ".", "QUOTE_ALL", ")", "\n", "wr", ".", "writerow", "(", "[", "i", ",", "np", ".", "mean", "(", "jList", "[", "-", "100", ":", "]", ")", ",", "np", ".", "mean", "(", "rList", "[", "-", "summaryLength", ":", "]", ")", ",", "'./frames/image'", "+", "str", "(", "i", ")", "+", "'.gif'", ",", "'./frames/log'", "+", "str", "(", "i", ")", "+", "'.csv'", ",", "'./frames/sal'", "+", "str", "(", "i", ")", "+", "'.gif'", "]", ")", "\n", "myfile", ".", "close", "(", ")", "\n", "", "with", "open", "(", "'./Center/frames/log'", "+", "str", "(", "i", ")", "+", "'.csv'", ",", "'w'", ")", "as", "myfile", ":", "\n", "        ", "state_train", "=", "(", "np", ".", "zeros", "(", "[", "1", ",", "h_size", "]", ")", ",", "np", ".", "zeros", "(", "[", "1", ",", "h_size", "]", ")", ")", "\n", "wr", "=", "csv", ".", "writer", "(", "myfile", ",", "quoting", "=", "csv", ".", "QUOTE_ALL", ")", "\n", "wr", ".", "writerow", "(", "[", "\"ACTION\"", ",", "\"REWARD\"", ",", "\"A0\"", ",", "\"A1\"", ",", "'A2'", ",", "'A3'", ",", "'V'", "]", ")", "\n", "a", ",", "v", "=", "sess", ".", "run", "(", "[", "mainQN", ".", "Advantage", ",", "mainQN", ".", "Value", "]", ",", "feed_dict", "=", "{", "mainQN", ".", "scalarInput", ":", "np", ".", "vstack", "(", "bufferArray", "[", ":", ",", "0", "]", ")", "/", "255.0", ",", "mainQN", ".", "trainLength", ":", "len", "(", "bufferArray", ")", ",", "mainQN", ".", "state_in", ":", "state_train", ",", "mainQN", ".", "batch_size", ":", "1", "}", ")", "\n", "wr", ".", "writerows", "(", "zip", "(", "bufferArray", "[", ":", ",", "1", "]", ",", "bufferArray", "[", ":", ",", "2", "]", ",", "a", "[", ":", ",", "0", "]", ",", "a", "[", ":", ",", "1", "]", ",", "a", "[", ":", ",", "2", "]", ",", "a", "[", ":", ",", "3", "]", ",", "v", "[", ":", ",", "0", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.make_gif": [[71, 102], ["mpy.VideoClip", "mpy.VideoClip", "mpy.VideoClip.set_mask", "mpy.VideoClip.set_opacity", "mask.set_opacity.set_opacity", "mask.set_opacity.write_gif", "mpy.VideoClip.write_gif", "x.astype", "int", "int", "len", "len", "len", "len"], "function", ["None"], ["", "", "def", "make_gif", "(", "images", ",", "fname", ",", "duration", "=", "2", ",", "true_image", "=", "False", ",", "salience", "=", "False", ",", "salIMGS", "=", "None", ")", ":", "\n", "  ", "import", "moviepy", ".", "editor", "as", "mpy", "\n", "\n", "def", "make_frame", "(", "t", ")", ":", "\n", "    ", "try", ":", "\n", "      ", "x", "=", "images", "[", "int", "(", "len", "(", "images", ")", "/", "duration", "*", "t", ")", "]", "\n", "", "except", ":", "\n", "      ", "x", "=", "images", "[", "-", "1", "]", "\n", "\n", "", "if", "true_image", ":", "\n", "      ", "return", "x", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "", "else", ":", "\n", "      ", "return", "(", "(", "x", "+", "1", ")", "/", "2", "*", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n", "", "", "def", "make_mask", "(", "t", ")", ":", "\n", "    ", "try", ":", "\n", "      ", "x", "=", "salIMGS", "[", "int", "(", "len", "(", "salIMGS", ")", "/", "duration", "*", "t", ")", "]", "\n", "", "except", ":", "\n", "      ", "x", "=", "salIMGS", "[", "-", "1", "]", "\n", "", "return", "x", "\n", "\n", "", "clip", "=", "mpy", ".", "VideoClip", "(", "make_frame", ",", "duration", "=", "duration", ")", "\n", "if", "salience", "==", "True", ":", "\n", "    ", "mask", "=", "mpy", ".", "VideoClip", "(", "make_mask", ",", "ismask", "=", "True", ",", "duration", "=", "duration", ")", "\n", "clipB", "=", "clip", ".", "set_mask", "(", "mask", ")", "\n", "clipB", "=", "clip", ".", "set_opacity", "(", "0", ")", "\n", "mask", "=", "mask", ".", "set_opacity", "(", "0.1", ")", "\n", "mask", ".", "write_gif", "(", "fname", ",", "fps", "=", "len", "(", "images", ")", "/", "duration", ",", "verbose", "=", "False", ")", "\n", "#clipB.write_gif(fname, fps = len(images) / duration,verbose=False)", "\n", "", "else", ":", "\n", "    ", "clip", ".", "write_gif", "(", "fname", ",", "fps", "=", "len", "(", "images", ")", "/", "duration", ",", "verbose", "=", "False", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.Sample.__init__": [[32, 38], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "is_terminal", ")", ":", "\n", "        ", "self", ".", "state", "=", "state", "\n", "self", ".", "action", "=", "action", "\n", "self", ".", "reward", "=", "reward", "\n", "self", ".", "next_state", "=", "next_state", "\n", "self", ".", "is_terminal", "=", "is_terminal", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.Preprocessor.process_state_for_network": [[59, 82], ["None"], "methods", ["None"], ["def", "process_state_for_network", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Preprocess the given state before giving it to the network.\n\n        Should be called just before the action is selected.\n\n        This is a different method from the process_state_for_memory\n        because the replay memory may require a different storage\n        format to reduce memory usage. For example, storing images as\n        uint8 in memory is a lot more efficient thant float32, but the\n        networks work better with floating point images.\n\n        Parameters\n        ----------\n        state: np.ndarray\n          Generally a numpy array. A single state from an environment.\n\n        Returns\n        -------\n        processed_state: np.ndarray\n          Generally a numpy array. The state after processing. Can be\n          modified in anyway.\n        \"\"\"", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.Preprocessor.process_state_for_memory": [[83, 106], ["None"], "methods", ["None"], ["", "def", "process_state_for_memory", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Preprocess the given state before giving it to the replay memory.\n\n        Should be called just before appending this to the replay memory.\n\n        This is a different method from the process_state_for_network\n        because the replay memory may require a different storage\n        format to reduce memory usage. For example, storing images as\n        uint8 in memory and the network expecting images in floating\n        point.\n\n        Parameters\n        ----------\n        state: np.ndarray\n          A single state from an environmnet. Generally a numpy array.\n\n        Returns\n        -------\n        processed_state: np.ndarray\n          Generally a numpy array. The state after processing. Can be\n          modified in any manner.\n        \"\"\"", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.Preprocessor.process_batch": [[107, 126], ["None"], "methods", ["None"], ["", "def", "process_batch", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Process batch of samples.\n\n        If your replay memory storage format is different than your\n        network input, you may want to apply this function to your\n        sampled batch before running it through your update function.\n\n        Parameters\n        ----------\n        samples: list(tensorflow_rl.core.Sample)\n          List of samples to process\n\n        Returns\n        -------\n        processed_samples: list(tensorflow_rl.core.Sample)\n          Samples after processing. Can be modified in anyways, but\n          the list length will generally stay the same.\n        \"\"\"", "\n", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.Preprocessor.process_reward": [[127, 145], ["None"], "methods", ["None"], ["", "def", "process_reward", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"Process the reward.\n\n        Useful for things like reward clipping. The Atari environments\n        from DQN paper do this. Instead of taking real score, they\n        take the sign of the delta of the score.\n\n        Parameters\n        ----------\n        reward: float\n          Reward to process\n\n        Returns\n        -------\n        processed_reward: float\n          The processed reward\n        \"\"\"", "\n", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.Preprocessor.reset": [[146, 153], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Reset any internal state.\n\n        Will be called at the start of every new episode. Makes it\n        possible to do history snapshots.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.__init__": [[173, 191], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "\"\"\"Setup memory.\n\n        You should specify the maximum size o the memory. Once the\n        memory fills up oldest values should be removed. You can try\n        the collections.deque class as the underlying storage, but\n        your sample method will be very slow.\n\n        We recommend using a list as a ring buffer. Just track the\n        index where the next sample should be inserted in the list.\n        \"\"\"", "\n", "self", ".", "memory_size", "=", "args", ".", "replay_memory_size", "\n", "self", ".", "history_length", "=", "args", ".", "num_frames", "\n", "self", ".", "actions", "=", "np", ".", "zeros", "(", "self", ".", "memory_size", ",", "dtype", "=", "np", ".", "int8", ")", "\n", "self", ".", "rewards", "=", "np", ".", "zeros", "(", "self", ".", "memory_size", ",", "dtype", "=", "np", ".", "int8", ")", "\n", "self", ".", "screens", "=", "np", ".", "zeros", "(", "(", "self", ".", "memory_size", ",", "args", ".", "frame_height", ",", "args", ".", "frame_width", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "self", ".", "terminals", "=", "np", ".", "zeros", "(", "self", ".", "memory_size", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "self", ".", "current", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append": [[192, 201], ["None"], "methods", ["None"], ["", "def", "append", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "is_terminal", ")", ":", "\n", "        ", "self", ".", "actions", "[", "self", ".", "current", "%", "self", ".", "memory_size", "]", "=", "action", "\n", "self", ".", "rewards", "[", "self", ".", "current", "%", "self", ".", "memory_size", "]", "=", "reward", "\n", "self", ".", "screens", "[", "self", ".", "current", "%", "self", ".", "memory_size", "]", "=", "state", "\n", "self", ".", "terminals", "[", "self", ".", "current", "%", "self", ".", "memory_size", "]", "=", "is_terminal", "\n", "# img = Image.fromarray(state, mode = 'L')", "\n", "# path = \"./tmp/%05d-%s.png\" % (self.current, is_terminal)", "\n", "# img.save(path)", "\n", "self", ".", "current", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.get_state": [[202, 206], ["numpy.transpose"], "methods", ["None"], ["", "def", "get_state", "(", "self", ",", "index", ")", ":", "\n", "        ", "state", "=", "self", ".", "screens", "[", "index", "-", "self", ".", "history_length", "+", "1", ":", "index", "+", "1", ",", ":", ",", ":", "]", "\n", "# history dimention last", "\n", "return", "np", ".", "transpose", "(", "state", ",", "(", "1", ",", "2", ",", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.sample": [[207, 227], ["min", "len", "numpy.random.randint", "core.ReplayMemory.terminals[].any", "indexes.append", "core.Sample", "samples.append", "core.ReplayMemory.get_state", "core.ReplayMemory.get_state"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.get_state", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.get_state"], ["", "def", "sample", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "samples", "=", "[", "]", "\n", "indexes", "=", "[", "]", "\n", "# ensure enough frames to sample", "\n", "assert", "self", ".", "current", ">", "self", ".", "history_length", "\n", "# -1 because still need next frame", "\n", "end", "=", "min", "(", "self", ".", "current", ",", "self", ".", "memory_size", ")", "-", "1", "\n", "\n", "while", "len", "(", "indexes", ")", "<", "batch_size", ":", "\n", "            ", "index", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "history_length", "-", "1", ",", "end", ")", "\n", "# sampled state shouldn't contain episode end", "\n", "if", "self", ".", "terminals", "[", "index", "-", "self", ".", "history_length", "+", "1", ":", "index", "+", "1", "]", ".", "any", "(", ")", ":", "\n", "                ", "continue", "\n", "", "indexes", ".", "append", "(", "index", ")", "\n", "\n", "", "for", "idx", "in", "indexes", ":", "\n", "            ", "new_sample", "=", "Sample", "(", "self", ".", "get_state", "(", "idx", ")", ",", "self", ".", "actions", "[", "idx", "]", ",", "\n", "self", ".", "rewards", "[", "idx", "]", ",", "self", ".", "get_state", "(", "idx", "+", "1", ")", ",", "self", ".", "terminals", "[", "idx", "]", ")", "\n", "samples", ".", "append", "(", "new_sample", ")", "\n", "", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.clear": [[228, 230], ["None"], "methods", ["None"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "self", ".", "current", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.policy.Policy.select_action": [[20, 30], ["NotImplementedError"], "methods", ["None"], ["def", "select_action", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Used by agents to select actions.\n\n        Returns\n        -------\n        Any:\n          An object representing the chosen action. Type depends on\n          the hierarchy of policy instances.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "'This method should be overriden.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.policy.UniformRandomPolicy.__init__": [[45, 48], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "num_actions", ")", ":", "\n", "        ", "assert", "num_actions", ">=", "1", "\n", "self", ".", "num_actions", "=", "num_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.policy.UniformRandomPolicy.select_action": [[49, 60], ["numpy.random.randint"], "methods", ["None"], ["", "def", "select_action", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Return a random action index.\n\n        This policy cannot contain others (as they would just be ignored).\n\n        Returns\n        -------\n        int:\n          Action index in range [0, num_actions)\n        \"\"\"", "\n", "return", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "num_actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.policy.UniformRandomPolicy.get_config": [[61, 63], ["None"], "methods", ["None"], ["", "def", "get_config", "(", "self", ")", ":", "# noqa: D102", "\n", "        ", "return", "{", "'num_actions'", ":", "self", ".", "num_actions", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.policy.GreedyPolicy.select_action": [[70, 72], ["numpy.argmax"], "methods", ["None"], ["def", "select_action", "(", "self", ",", "q_values", ",", "**", "kwargs", ")", ":", "# noqa: D102", "\n", "        ", "return", "np", ".", "argmax", "(", "q_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.policy.GreedyEpsilonPolicy.__init__": [[85, 87], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "epsilon", ")", ":", "\n", "        ", "self", ".", "epsilon", "=", "epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.policy.GreedyEpsilonPolicy.select_action": [[88, 107], ["numpy.random.rand", "policy.UniformRandomPolicy.select_action", "policy.GreedyPolicy.select_action", "policy.UniformRandomPolicy", "policy.GreedyPolicy"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action"], ["", "def", "select_action", "(", "self", ",", "q_values", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Run Greedy-Epsilon for the given Q-values.\n\n        Parameters\n        ----------\n        q_values: array-like\n          Array-like structure of floats representing the Q-values for\n          each action.\n\n        Returns\n        -------\n        int:\n          The action index chosen.\n        \"\"\"", "\n", "num_actions", "=", "q_values", ".", "shape", "[", "1", "]", "\n", "if", "np", ".", "random", ".", "rand", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "UniformRandomPolicy", "(", "num_actions", ")", ".", "select_action", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "GreedyPolicy", "(", ")", ".", "select_action", "(", "q_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.policy.LinearDecayGreedyEpsilonPolicy.__init__": [[125, 130], ["float"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "start_value", ",", "end_value", ",", "num_steps", ")", ":", "# noqa: D102", "\n", "        ", "self", ".", "start_value", "=", "start_value", "\n", "self", ".", "decay_rate", "=", "float", "(", "end_value", "-", "start_value", ")", "/", "num_steps", "\n", "self", ".", "end_value", "=", "end_value", "\n", "self", ".", "step", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.policy.LinearDecayGreedyEpsilonPolicy.select_action": [[131, 152], ["max", "policy.GreedyEpsilonPolicy.select_action", "policy.GreedyEpsilonPolicy"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action"], ["", "def", "select_action", "(", "self", ",", "q_values", ",", "is_training", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Decay parameter and select action.\n\n        Parameters\n        ----------\n        q_values: np.array\n          The Q-values for each action.\n        is_training: bool, optional\n          If true then parameter will be decayed. Defaults to true.\n\n        Returns\n        -------\n        Any:\n          Selected action.\n        \"\"\"", "\n", "epsilon", "=", "self", ".", "start_value", "\n", "if", "is_training", ":", "\n", "            ", "epsilon", "+=", "self", ".", "decay_rate", "*", "self", ".", "step", "\n", "self", ".", "step", "+=", "1", "\n", "", "epsilon", "=", "max", "(", "epsilon", ",", "self", ".", "end_value", ")", "\n", "return", "GreedyEpsilonPolicy", "(", "epsilon", ")", ".", "select_action", "(", "q_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.policy.LinearDecayGreedyEpsilonPolicy.reset": [[153, 156], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Start the decay over at the start value.\"\"\"", "\n", "self", ".", "step", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.Qnetwork.__init__": [[19, 117], ["tensorflow.placeholder", "tensorflow.transpose", "tensorflow.reshape", "tensorflow.squeeze", "tensorflow.summary.merge", "tensorflow.contrib.layers.convolution2d", "tensorflow.contrib.layers.convolution2d", "tensorflow.contrib.layers.convolution2d", "tensorflow.placeholder", "tensorflow.argmax", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.one_hot", "tensorflow.reduce_sum", "tensorflow.square", "tensorflow.reduce_mean", "tensorflow.train.AdamOptimizer", "dqn_tf_spatialAt.Qnetwork.trainer.minimize", "tensorflow.gather", "tensorflow.contrib.layers.fully_connected", "tensorflow.reshape", "rnn_cell.zero_state", "tensorflow.nn.dynamic_rnn", "tensorflow.slice", "tensorflow.squeeze", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.constant_initializer", "tensorflow.reshape", "tensorflow.split", "tensorflow.contrib.rnn.BasicLSTMCell", "dqn_tf_spatialAt.Qnetwork._get_initial_lstm", "range", "tensorflow.contrib.layers.fully_connected", "tensorflow.contrib.layers.fully_connected", "tensorflow.contrib.layers.fully_connected", "tensorflow.contrib.layers.fully_connected", "tensorflow.contrib.layers.fully_connected", "tensorflow.multiply", "tensorflow.reshape", "tensorflow.summary.image", "tensorflow.contrib.layers.flatten", "tensorflow.squeeze", "dqn_tf_spatialAt.Qnetwork._batch_norm", "dqn_tf_spatialAt.Qnetwork._project_features", "dqn_tf_spatialAt.Qnetwork._attention_layer", "dqn_tf_spatialAt.Qnetwork.alpha_list.append", "tensorflow.subtract", "tensorflow.squeeze", "dqn_tf_spatialAt.Qnetwork._selector", "tensorflow.variable_scope", "tensorflow.contrib.rnn.BasicLSTMCell.", "tensorflow.reduce_mean", "tensorflow.concat"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.Qnetwork._get_initial_lstm", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.Qnetwork._batch_norm", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.Qnetwork._project_features", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.Qnetwork._attention_layer", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.Qnetwork._selector"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "h_size", ",", "num_frames", ",", "num_actions", ",", "rnn_cell", ",", "myScope", ")", ":", "\n", "#The network recieves a frame from the game, flattened into an array.", "\n", "#It then resizes it and processes it through four convolutional layers.", "\n", "        ", "self", ".", "imageIn", "=", "tf", ".", "placeholder", "(", "shape", "=", "[", "None", ",", "84", ",", "84", ",", "num_frames", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "image_permute", "=", "tf", ".", "transpose", "(", "self", ".", "imageIn", ",", "perm", "=", "[", "0", ",", "3", ",", "1", ",", "2", "]", ")", "\n", "self", ".", "image_reshape", "=", "tf", ".", "reshape", "(", "self", ".", "image_permute", ",", "[", "-", "1", ",", "84", ",", "84", ",", "1", "]", ")", "\n", "self", ".", "image_reshape_recoverd", "=", "tf", ".", "squeeze", "(", "tf", ".", "gather", "(", "tf", ".", "reshape", "(", "self", ".", "image_reshape", ",", "[", "-", "1", ",", "num_frames", ",", "84", ",", "84", ",", "1", "]", ")", ",", "[", "0", "]", ")", ",", "[", "0", "]", ")", "\n", "self", ".", "summary_merged", "=", "tf", ".", "summary", ".", "merge", "(", "[", "tf", ".", "summary", ".", "image", "(", "'image_reshape_recoverd'", ",", "self", ".", "image_reshape_recoverd", ",", "max_outputs", "=", "num_frames", ")", "]", ")", "\n", "# self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,1])", "\n", "self", ".", "conv1", "=", "tf", ".", "contrib", ".", "layers", ".", "convolution2d", "(", "inputs", "=", "self", ".", "image_reshape", ",", "num_outputs", "=", "32", ",", "kernel_size", "=", "[", "8", ",", "8", "]", ",", "stride", "=", "[", "4", ",", "4", "]", ",", "padding", "=", "'VALID'", ",", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ",", "biases_initializer", "=", "None", ",", "scope", "=", "myScope", "+", "'_conv1'", ")", "\n", "self", ".", "conv2", "=", "tf", ".", "contrib", ".", "layers", ".", "convolution2d", "(", "inputs", "=", "self", ".", "conv1", ",", "num_outputs", "=", "64", ",", "kernel_size", "=", "[", "4", ",", "4", "]", ",", "stride", "=", "[", "2", ",", "2", "]", ",", "padding", "=", "'VALID'", ",", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ",", "biases_initializer", "=", "None", ",", "scope", "=", "myScope", "+", "'_conv2'", ")", "\n", "self", ".", "conv3", "=", "tf", ".", "contrib", ".", "layers", ".", "convolution2d", "(", "inputs", "=", "self", ".", "conv2", ",", "num_outputs", "=", "64", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "stride", "=", "[", "1", ",", "1", "]", ",", "padding", "=", "'VALID'", ",", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ",", "biases_initializer", "=", "None", ",", "scope", "=", "myScope", "+", "'_conv3'", ")", "# (None, 10, 7, 7, 64)", "\n", "self", ".", "batch_size", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "if", "not", "(", "args", ".", "a_t", ")", ":", "\n", "            ", "self", ".", "conv4", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "tf", ".", "contrib", ".", "layers", ".", "flatten", "(", "self", ".", "conv3", ")", ",", "h_size", ",", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "\n", "#We take the output from the final convolutional layer and send it to a recurrent layer.", "\n", "#The input must be reshaped into [batch x trace x units] for rnn processing, ", "\n", "#and then returned to [batch x units] when sent through the upper levles.", "\n", "self", ".", "convFlat", "=", "tf", ".", "reshape", "(", "self", ".", "conv4", ",", "[", "self", ".", "batch_size", ",", "num_frames", ",", "h_size", "]", ")", "\n", "self", ".", "state_in", "=", "rnn_cell", ".", "zero_state", "(", "self", ".", "batch_size", ",", "tf", ".", "float32", ")", "\n", "self", ".", "rnn_outputs", ",", "self", ".", "rnn_state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "inputs", "=", "self", ".", "convFlat", ",", "cell", "=", "rnn_cell", ",", "dtype", "=", "tf", ".", "float32", ",", "initial_state", "=", "self", ".", "state_in", ",", "scope", "=", "myScope", "+", "'_rnn'", ")", "\n", "# print(\"======\", self.rnn_outputs.get_shape().as_list())", "\n", "\n", "self", ".", "rnn_last_output", "=", "tf", ".", "slice", "(", "self", ".", "rnn_outputs", ",", "[", "0", ",", "num_frames", "-", "1", ",", "0", "]", ",", "[", "-", "1", ",", "1", ",", "-", "1", "]", ")", "\n", "self", ".", "rnn", "=", "tf", ".", "squeeze", "(", "self", ".", "rnn_last_output", ",", "[", "1", "]", ")", "\n", "# print(\"==========\", self.rnn.get_shape().as_list())", "\n", "", "else", ":", "\n", "            ", "self", ".", "L", "=", "7", "*", "7", "\n", "self", ".", "D", "=", "64", "\n", "self", ".", "T", "=", "num_frames", "\n", "self", ".", "H", "=", "512", "\n", "self", ".", "selector", "=", "args", ".", "selector", "\n", "self", ".", "weight_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", "\n", "self", ".", "const_initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", "\n", "\n", "self", ".", "features", "=", "tf", ".", "reshape", "(", "self", ".", "conv3", ",", "[", "self", ".", "batch_size", ",", "num_frames", ",", "self", ".", "L", ",", "self", ".", "D", "]", ")", "\n", "self", ".", "features_list", "=", "tf", ".", "split", "(", "self", ".", "features", ",", "num_frames", ",", "axis", "=", "1", ")", "\n", "# print(len(self.features_list), self.features_list[0].get_shape().as_list()) # 10 [None, 1, 49, 64]", "\n", "self", ".", "alpha_list", "=", "[", "]", "\n", "lstm_cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "H", ")", "\n", "c", ",", "h", "=", "self", ".", "_get_initial_lstm", "(", "features", "=", "tf", ".", "squeeze", "(", "self", ".", "features_list", "[", "0", "]", ",", "[", "1", "]", ")", ",", "myScope", "=", "myScope", ")", "\n", "\n", "for", "t", "in", "range", "(", "self", ".", "T", ")", ":", "\n", "                ", "features", "=", "tf", ".", "squeeze", "(", "self", ".", "features_list", "[", "t", "]", ",", "[", "1", "]", ")", "\n", "features", "=", "self", ".", "_batch_norm", "(", "features", ",", "mode", "=", "'train'", ",", "name", "=", "myScope", "+", "'conv_features'", ",", "reuse", "=", "(", "t", "!=", "0", ")", ")", "\n", "features_proj", "=", "self", ".", "_project_features", "(", "features", "=", "features", ",", "myScope", "=", "myScope", ",", "reuse", "=", "(", "t", "!=", "0", ")", ")", "\n", "context", ",", "alpha", "=", "self", ".", "_attention_layer", "(", "features", ",", "features_proj", ",", "h", ",", "myScope", "=", "myScope", ",", "reuse", "=", "(", "t", "!=", "0", ")", ")", "\n", "self", ".", "alpha_list", ".", "append", "(", "alpha", ")", "\n", "\n", "if", "self", ".", "selector", ":", "\n", "                    ", "context", ",", "beta", "=", "self", ".", "_selector", "(", "context", ",", "h", ",", "myScope", "=", "myScope", ",", "reuse", "=", "(", "t", "!=", "0", ")", ")", "\n", "\n", "# print(\"========== context \", context.get_shape().as_list())", "\n", "# print(\"========== h \", h.get_shape().as_list())", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "myScope", "+", "'_lstmCell'", ",", "reuse", "=", "(", "t", "!=", "0", ")", ")", ":", "\n", "                    ", "_", ",", "(", "c", ",", "h", ")", "=", "lstm_cell", "(", "inputs", "=", "tf", ".", "concat", "(", "[", "context", ",", "h", "]", ",", "1", ")", ",", "state", "=", "[", "c", ",", "h", "]", ")", "\n", "# print(\"========== h \", h.get_shape().as_list())", "\n", "\n", "", "", "self", ".", "rnn", "=", "h", "\n", "\n", "\n", "", "if", "args", ".", "net_mode", "==", "\"duel\"", ":", "\n", "#The output from the recurrent player is then split into separate Value and Advantage streams", "\n", "            ", "self", ".", "ad_hidden", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "self", ".", "rnn", ",", "h_size", ",", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ",", "scope", "=", "myScope", "+", "'_fc_advantage_hidden'", ")", "\n", "self", ".", "Advantage", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "self", ".", "ad_hidden", ",", "num_actions", ",", "activation_fn", "=", "None", ",", "scope", "=", "myScope", "+", "'_fc_advantage'", ")", "\n", "self", ".", "value_hidden", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "self", ".", "rnn", ",", "h_size", ",", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ",", "scope", "=", "myScope", "+", "'_fc_value_hidden'", ")", "\n", "self", ".", "Value", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "self", ".", "value_hidden", ",", "1", ",", "activation_fn", "=", "None", ",", "scope", "=", "myScope", "+", "'_fc_value'", ")", "\n", "\n", "#Then combine them together to get our final Q-values.", "\n", "self", ".", "Qout", "=", "self", ".", "Value", "+", "tf", ".", "subtract", "(", "self", ".", "Advantage", ",", "tf", ".", "reduce_mean", "(", "self", ".", "Advantage", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "Qout", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "self", ".", "rnn", ",", "num_actions", ",", "activation_fn", "=", "None", ")", "\n", "", "self", ".", "predict", "=", "tf", ".", "argmax", "(", "self", ".", "Qout", ",", "1", ")", "\n", "\n", "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.", "\n", "self", ".", "targetQ", "=", "tf", ".", "placeholder", "(", "shape", "=", "[", "None", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "actions", "=", "tf", ".", "placeholder", "(", "shape", "=", "[", "None", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "self", ".", "actions_onehot", "=", "tf", ".", "one_hot", "(", "self", ".", "actions", ",", "num_actions", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "self", ".", "Q", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "self", ".", "Qout", ",", "self", ".", "actions_onehot", ")", ",", "axis", "=", "1", ")", "\n", "self", ".", "td_error", "=", "tf", ".", "square", "(", "self", ".", "targetQ", "-", "self", ".", "Q", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "td_error", ")", "\n", "\n", "self", ".", "trainer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "0.0001", ")", "\n", "self", ".", "updateModel", "=", "self", ".", "trainer", ".", "minimize", "(", "self", ".", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.Qnetwork._batch_norm": [[118, 127], ["tensorflow.contrib.layers.batch_norm"], "methods", ["None"], ["", "def", "_batch_norm", "(", "self", ",", "x", ",", "mode", "=", "'train'", ",", "name", "=", "None", ",", "reuse", "=", "False", ")", ":", "\n", "        ", "return", "tf", ".", "contrib", ".", "layers", ".", "batch_norm", "(", "inputs", "=", "x", ",", "\n", "decay", "=", "0.95", ",", "\n", "center", "=", "True", ",", "\n", "scale", "=", "True", ",", "\n", "is_training", "=", "(", "mode", "==", "'train'", ")", ",", "\n", "updates_collections", "=", "None", ",", "\n", "reuse", "=", "reuse", ",", "\n", "scope", "=", "(", "name", "+", "'batch_norm'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.Qnetwork._get_initial_lstm": [[128, 140], ["tensorflow.variable_scope", "tensorflow.reduce_mean", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.tanh", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.tanh", "tensorflow.matmul", "tensorflow.matmul"], "methods", ["None"], ["", "def", "_get_initial_lstm", "(", "self", ",", "features", ",", "myScope", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "myScope", "+", "'_initial_lstm'", ")", ":", "\n", "            ", "features_mean", "=", "tf", ".", "reduce_mean", "(", "features", ",", "1", ")", "\n", "\n", "w_h", "=", "tf", ".", "get_variable", "(", "'w_h'", ",", "[", "self", ".", "D", ",", "self", ".", "H", "]", ",", "initializer", "=", "self", ".", "weight_initializer", ")", "\n", "b_h", "=", "tf", ".", "get_variable", "(", "'b_h'", ",", "[", "self", ".", "H", "]", ",", "initializer", "=", "self", ".", "const_initializer", ")", "\n", "h", "=", "tf", ".", "nn", ".", "tanh", "(", "tf", ".", "matmul", "(", "features_mean", ",", "w_h", ")", "+", "b_h", ")", "\n", "\n", "w_c", "=", "tf", ".", "get_variable", "(", "'w_c'", ",", "[", "self", ".", "D", ",", "self", ".", "H", "]", ",", "initializer", "=", "self", ".", "weight_initializer", ")", "\n", "b_c", "=", "tf", ".", "get_variable", "(", "'b_c'", ",", "[", "self", ".", "H", "]", ",", "initializer", "=", "self", ".", "const_initializer", ")", "\n", "c", "=", "tf", ".", "nn", ".", "tanh", "(", "tf", ".", "matmul", "(", "features_mean", ",", "w_c", ")", "+", "b_c", ")", "\n", "return", "c", ",", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.Qnetwork._project_features": [[141, 148], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape"], "methods", ["None"], ["", "", "def", "_project_features", "(", "self", ",", "features", ",", "myScope", ",", "reuse", "=", "False", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "myScope", "+", "'_project_features'", ",", "reuse", "=", "reuse", ")", ":", "\n", "            ", "w", "=", "tf", ".", "get_variable", "(", "'w'", ",", "[", "self", ".", "D", ",", "self", ".", "D", "]", ",", "initializer", "=", "self", ".", "weight_initializer", ")", "\n", "features_flat", "=", "tf", ".", "reshape", "(", "features", ",", "[", "-", "1", ",", "self", ".", "D", "]", ")", "\n", "features_proj", "=", "tf", ".", "matmul", "(", "features_flat", ",", "w", ")", "\n", "features_proj", "=", "tf", ".", "reshape", "(", "features_proj", ",", "[", "-", "1", ",", "self", ".", "L", ",", "self", ".", "D", "]", ")", "\n", "return", "features_proj", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.Qnetwork._attention_layer": [[149, 160], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.relu", "tensorflow.reshape", "tensorflow.nn.softmax", "tensorflow.reduce_sum", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.matmul"], "methods", ["None"], ["", "", "def", "_attention_layer", "(", "self", ",", "features", ",", "features_proj", ",", "h", ",", "myScope", ",", "reuse", "=", "False", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "myScope", "+", "'_attention_layer'", ",", "reuse", "=", "reuse", ")", ":", "\n", "            ", "w", "=", "tf", ".", "get_variable", "(", "'w'", ",", "[", "self", ".", "H", ",", "self", ".", "D", "]", ",", "initializer", "=", "self", ".", "weight_initializer", ")", "\n", "b", "=", "tf", ".", "get_variable", "(", "'b'", ",", "[", "self", ".", "D", "]", ",", "initializer", "=", "self", ".", "const_initializer", ")", "\n", "w_att", "=", "tf", ".", "get_variable", "(", "'w_att'", ",", "[", "self", ".", "D", ",", "1", "]", ",", "initializer", "=", "self", ".", "weight_initializer", ")", "\n", "\n", "h_att", "=", "tf", ".", "nn", ".", "relu", "(", "features_proj", "+", "tf", ".", "expand_dims", "(", "tf", ".", "matmul", "(", "h", ",", "w", ")", ",", "1", ")", "+", "b", ")", "# (N, L, D)", "\n", "out_att", "=", "tf", ".", "reshape", "(", "tf", ".", "matmul", "(", "tf", ".", "reshape", "(", "h_att", ",", "[", "-", "1", ",", "self", ".", "D", "]", ")", ",", "w_att", ")", ",", "[", "-", "1", ",", "self", ".", "L", "]", ")", "# (N, L)", "\n", "alpha", "=", "tf", ".", "nn", ".", "softmax", "(", "out_att", ")", "\n", "context", "=", "tf", ".", "reduce_sum", "(", "features", "*", "tf", ".", "expand_dims", "(", "alpha", ",", "2", ")", ",", "1", ",", "name", "=", "'context'", ")", "#(N, D)", "\n", "return", "context", ",", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.Qnetwork._selector": [[161, 168], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.sigmoid", "tensorflow.multiply", "tensorflow.matmul"], "methods", ["None"], ["", "", "def", "_selector", "(", "self", ",", "context", ",", "h", ",", "myScope", ",", "reuse", "=", "False", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "myScope", "+", "'_selector'", ",", "reuse", "=", "reuse", ")", ":", "\n", "            ", "w", "=", "tf", ".", "get_variable", "(", "'w'", ",", "[", "self", ".", "H", ",", "1", "]", ",", "initializer", "=", "self", ".", "weight_initializer", ")", "\n", "b", "=", "tf", ".", "get_variable", "(", "'b'", ",", "[", "1", "]", ",", "initializer", "=", "self", ".", "const_initializer", ")", "\n", "beta", "=", "tf", ".", "nn", ".", "sigmoid", "(", "tf", ".", "matmul", "(", "h", ",", "w", ")", "+", "b", ",", "'beta'", ")", "# (N, 1)", "\n", "context", "=", "tf", ".", "multiply", "(", "beta", ",", "context", ",", "name", "=", "'selected_context'", ")", "\n", "return", "context", ",", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.DQNAgent.__init__": [[226, 280], ["HistoryPreprocessor", "AtariPreprocessor", "ReplayMemory", "LinearDecayGreedyEpsilonPolicy", "tensorflow.reset_default_graph", "tensorflow.contrib.rnn.BasicLSTMCell", "tensorflow.contrib.rnn.BasicLSTMCell", "dqn_tf_spatialAt.Qnetwork", "dqn_tf_spatialAt.Qnetwork", "print", "print", "tensorflow.global_variables_initializer", "tensorflow.train.Saver", "tensorflow.trainable_variables", "print", "updateTargetGraph", "tensorflow.ConfigProto", "tensorflow.Session", "dqn_tf_spatialAt.DQNAgent.sess.run", "updateTarget", "tensorflow.summary.FileWriter", "len"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.updateTargetGraph", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.updateTarget"], ["def", "__init__", "(", "self", ",", "args", ",", "num_actions", ")", ":", "\n", "        ", "self", ".", "num_actions", "=", "num_actions", "\n", "input_shape", "=", "(", "args", ".", "frame_height", ",", "args", ".", "frame_width", ",", "args", ".", "num_frames", ")", "\n", "self", ".", "history_processor", "=", "HistoryPreprocessor", "(", "args", ".", "num_frames", "-", "1", ")", "\n", "self", ".", "atari_processor", "=", "AtariPreprocessor", "(", ")", "\n", "self", ".", "memory", "=", "ReplayMemory", "(", "args", ")", "\n", "self", ".", "policy", "=", "LinearDecayGreedyEpsilonPolicy", "(", "args", ".", "initial_epsilon", ",", "args", ".", "final_epsilon", ",", "args", ".", "exploration_steps", ")", "\n", "self", ".", "gamma", "=", "args", ".", "gamma", "\n", "self", ".", "target_update_freq", "=", "args", ".", "target_update_freq", "\n", "self", ".", "num_burn_in", "=", "args", ".", "num_burn_in", "\n", "self", ".", "train_freq", "=", "args", ".", "train_freq", "\n", "self", ".", "batch_size", "=", "args", ".", "batch_size", "\n", "self", ".", "learning_rate", "=", "args", ".", "learning_rate", "\n", "self", ".", "frame_width", "=", "args", ".", "frame_width", "\n", "self", ".", "frame_height", "=", "args", ".", "frame_height", "\n", "self", ".", "num_frames", "=", "args", ".", "num_frames", "\n", "self", ".", "output_path", "=", "args", ".", "output", "\n", "self", ".", "output_path_videos", "=", "args", ".", "output", "+", "'/videos/'", "\n", "self", ".", "output_path_images", "=", "args", ".", "output", "+", "'/images/'", "\n", "self", ".", "save_freq", "=", "args", ".", "save_freq", "\n", "self", ".", "load_network", "=", "args", ".", "load_network", "\n", "self", ".", "load_network_path", "=", "args", ".", "load_network_path", "\n", "self", ".", "enable_ddqn", "=", "args", ".", "ddqn", "\n", "self", ".", "net_mode", "=", "args", ".", "net_mode", "\n", "\n", "self", ".", "h_size", "=", "512", "\n", "self", ".", "tau", "=", "0.001", "\n", "tf", ".", "reset_default_graph", "(", ")", "\n", "#We define the cells for the primary and target q-networks", "\n", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "h_size", ",", "state_is_tuple", "=", "True", ")", "\n", "cellT", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "h_size", ",", "state_is_tuple", "=", "True", ")", "\n", "self", ".", "q_network", "=", "Qnetwork", "(", "args", "=", "args", ",", "h_size", "=", "self", ".", "h_size", ",", "num_frames", "=", "self", ".", "num_frames", ",", "num_actions", "=", "self", ".", "num_actions", ",", "rnn_cell", "=", "cell", ",", "myScope", "=", "\"QNet\"", ")", "\n", "self", ".", "target_network", "=", "Qnetwork", "(", "args", "=", "args", ",", "h_size", "=", "self", ".", "h_size", ",", "num_frames", "=", "self", ".", "num_frames", ",", "num_actions", "=", "self", ".", "num_actions", ",", "rnn_cell", "=", "cellT", ",", "myScope", "=", "\"TargetNet\"", ")", "\n", "\n", "print", "(", "\">>>> Net mode: %s, Using double dqn: %s\"", "%", "(", "self", ".", "net_mode", ",", "self", ".", "enable_ddqn", ")", ")", "\n", "self", ".", "eval_freq", "=", "args", ".", "eval_freq", "\n", "self", ".", "no_experience", "=", "args", ".", "no_experience", "\n", "self", ".", "no_target", "=", "args", ".", "no_target", "\n", "print", "(", "\">>>> Target fixing: %s, Experience replay: %s\"", "%", "(", "not", "self", ".", "no_target", ",", "not", "self", ".", "no_experience", ")", ")", "\n", "\n", "# initialize target network", "\n", "init", "=", "tf", ".", "global_variables_initializer", "(", ")", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "2", ")", "\n", "trainables", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "print", "(", "trainables", ",", "len", "(", "trainables", ")", ")", "\n", "self", ".", "targetOps", "=", "updateTargetGraph", "(", "trainables", ",", "self", ".", "tau", ")", "\n", "\n", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "config", ".", "allow_soft_placement", "=", "True", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", "config", "=", "config", ")", "\n", "self", ".", "sess", ".", "run", "(", "init", ")", "\n", "updateTarget", "(", "self", ".", "targetOps", ",", "self", ".", "sess", ")", "\n", "self", ".", "writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "self", ".", "output_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.DQNAgent.calc_q_values": [[281, 300], ["dqn_tf_spatialAt.DQNAgent.sess.run"], "methods", ["None"], ["", "def", "calc_q_values", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Given a state (or batch of states) calculate the Q-values.\n\n        Basically run your network on these states.\n\n        Return\n        ------\n        Q-values for the state(s)\n        \"\"\"", "\n", "state", "=", "state", "[", "None", ",", ":", ",", ":", ",", ":", "]", "\n", "# return self.q_network.predict_on_batch(state)", "\n", "# print state.shape", "\n", "# Qout = self.sess.run(self.q_network.rnn_outputs,\\", "\n", "#             feed_dict={self.q_network.imageIn: state, self.q_network.batch_size:1})", "\n", "# print Qout.shape", "\n", "Qout", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "q_network", ".", "Qout", ",", "feed_dict", "=", "{", "self", ".", "q_network", ".", "imageIn", ":", "state", ",", "self", ".", "q_network", ".", "batch_size", ":", "1", "}", ")", "\n", "# print Qout.shape", "\n", "return", "Qout", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.DQNAgent.select_action": [[301, 332], ["dqn_tf_spatialAt.DQNAgent.calc_q_values", "GreedyPolicy().select_action", "UniformRandomPolicy().select_action", "dqn_tf_spatialAt.DQNAgent.policy.select_action", "GreedyPolicy", "UniformRandomPolicy"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.calc_q_values", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action"], ["", "def", "select_action", "(", "self", ",", "state", ",", "is_training", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Select the action based on the current state.\n\n        You will probably want to vary your behavior here based on\n        which stage of training your in. For example, if you're still\n        collecting random samples you might want to use a\n        UniformRandomPolicy.\n\n        If you're testing, you might want to use a GreedyEpsilonPolicy\n        with a low epsilon.\n\n        If you're training, you might want to use the\n        LinearDecayGreedyEpsilonPolicy.\n\n        This would also be a good place to call\n        process_state_for_network in your preprocessor.\n\n        Returns\n        --------\n        selected action\n        \"\"\"", "\n", "q_values", "=", "self", ".", "calc_q_values", "(", "state", ")", "\n", "if", "is_training", ":", "\n", "            ", "if", "kwargs", "[", "'policy_type'", "]", "==", "'UniformRandomPolicy'", ":", "\n", "                ", "return", "UniformRandomPolicy", "(", "self", ".", "num_actions", ")", ".", "select_action", "(", ")", "\n", "", "else", ":", "\n", "# linear decay greedy epsilon policy", "\n", "                ", "return", "self", ".", "policy", ".", "select_action", "(", "q_values", ",", "is_training", ")", "\n", "", "", "else", ":", "\n", "# return GreedyEpsilonPolicy(0.05).select_action(q_values)", "\n", "            ", "return", "GreedyPolicy", "(", ")", ".", "select_action", "(", "q_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.DQNAgent.update_policy": [[333, 402], ["dqn_tf_spatialAt.DQNAgent.sess.run", "numpy.stack", "numpy.stack", "numpy.asarray", "numpy.asarray", "numpy.zeros", "dqn_tf_spatialAt.DQNAgent.memory.sample", "dqn_tf_spatialAt.DQNAgent.atari_processor.process_batch", "numpy.stack", "numpy.asarray", "numpy.stack", "numpy.asarray", "numpy.asarray", "dqn_tf_spatialAt.DQNAgent.q_network.predict_on_batch", "dqn_tf_spatialAt.DQNAgent.sess.run", "dqn_tf_spatialAt.DQNAgent.sess.run", "numpy.argmax", "numpy.max", "numpy.mean", "int", "int", "range"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.sample", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_batch"], ["", "", "def", "update_policy", "(", "self", ",", "current_sample", ")", ":", "\n", "        ", "\"\"\"Update your policy.\n\n        Behavior may differ based on what stage of training your\n        in. If you're in training mode then you should check if you\n        should update your network parameters based on the current\n        step and the value you set for train_freq.\n\n        Inside, you'll want to sample a minibatch, calculate the\n        target values, update your network, and then update your\n        target values.\n\n        You might want to return the loss and other metrics as an\n        output. They can help you monitor how training is going.\n        \"\"\"", "\n", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "if", "self", ".", "no_experience", ":", "\n", "            ", "states", "=", "np", ".", "stack", "(", "[", "current_sample", ".", "state", "]", ")", "\n", "next_states", "=", "np", ".", "stack", "(", "[", "current_sample", ".", "next_state", "]", ")", "\n", "rewards", "=", "np", ".", "asarray", "(", "[", "current_sample", ".", "reward", "]", ")", "\n", "mask", "=", "np", ".", "asarray", "(", "[", "1", "-", "int", "(", "current_sample", ".", "is_terminal", ")", "]", ")", "\n", "\n", "action_mask", "=", "np", ".", "zeros", "(", "(", "1", ",", "self", ".", "num_actions", ")", ")", "\n", "action_mask", "[", "0", ",", "current_sample", ".", "action", "]", "=", "1.0", "\n", "", "else", ":", "\n", "            ", "samples", "=", "self", ".", "memory", ".", "sample", "(", "batch_size", ")", "\n", "samples", "=", "self", ".", "atari_processor", ".", "process_batch", "(", "samples", ")", "\n", "\n", "states", "=", "np", ".", "stack", "(", "[", "x", ".", "state", "for", "x", "in", "samples", "]", ")", "\n", "actions", "=", "np", ".", "asarray", "(", "[", "x", ".", "action", "for", "x", "in", "samples", "]", ")", "\n", "# action_mask = np.zeros((batch_size, self.num_actions))", "\n", "# action_mask[range(batch_size), actions] = 1.0", "\n", "\n", "next_states", "=", "np", ".", "stack", "(", "[", "x", ".", "next_state", "for", "x", "in", "samples", "]", ")", "\n", "mask", "=", "np", ".", "asarray", "(", "[", "1", "-", "int", "(", "x", ".", "is_terminal", ")", "for", "x", "in", "samples", "]", ")", "\n", "rewards", "=", "np", ".", "asarray", "(", "[", "x", ".", "reward", "for", "x", "in", "samples", "]", ")", "\n", "\n", "", "if", "self", ".", "no_target", ":", "\n", "            ", "next_qa_value", "=", "self", ".", "q_network", ".", "predict_on_batch", "(", "next_states", ")", "\n", "", "else", ":", "\n", "# next_qa_value = self.target_network.predict_on_batch(next_states)", "\n", "            ", "next_qa_value", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "target_network", ".", "Qout", ",", "feed_dict", "=", "{", "self", ".", "target_network", ".", "imageIn", ":", "next_states", ",", "self", ".", "target_network", ".", "batch_size", ":", "batch_size", "}", ")", "\n", "\n", "", "if", "self", ".", "enable_ddqn", ":", "\n", "# qa_value = self.q_network.predict_on_batch(next_states)", "\n", "            ", "qa_value", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "q_network", ".", "Qout", ",", "feed_dict", "=", "{", "self", ".", "q_network", ".", "imageIn", ":", "next_states", ",", "self", ".", "q_network", ".", "batch_size", ":", "batch_size", "}", ")", "\n", "max_actions", "=", "np", ".", "argmax", "(", "qa_value", ",", "axis", "=", "1", ")", "\n", "next_qa_value", "=", "next_qa_value", "[", "range", "(", "batch_size", ")", ",", "max_actions", "]", "\n", "", "else", ":", "\n", "            ", "next_qa_value", "=", "np", ".", "max", "(", "next_qa_value", ",", "axis", "=", "1", ")", "\n", "# print rewards.shape, mask.shape, next_qa_value.shape, batch_size", "\n", "", "target", "=", "rewards", "+", "self", ".", "gamma", "*", "mask", "*", "next_qa_value", "\n", "\n", "loss", ",", "_", ",", "rnn", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "q_network", ".", "loss", ",", "self", ".", "q_network", ".", "updateModel", ",", "self", ".", "q_network", ".", "rnn", "]", ",", "feed_dict", "=", "{", "self", ".", "q_network", ".", "imageIn", ":", "states", ",", "self", ".", "q_network", ".", "batch_size", ":", "batch_size", ",", "self", ".", "q_network", ".", "actions", ":", "actions", ",", "self", ".", "q_network", ".", "targetQ", ":", "target", "}", ")", "\n", "# print rnn[:5]", "\n", "# if np.random.random() < 0.001:", "\n", "#     merged = self.sess.run(self.q_network.summary_merged, \\", "\n", "#                 feed_dict={self.q_network.imageIn: states, self.q_network.batch_size:batch_size, \\", "\n", "#                 self.q_network.actions: actions, self.q_network.targetQ: target})", "\n", "#     self.writer.add_summary(merged)", "\n", "#     self.writer.flush()", "\n", "#     print '----- writer flushed.'", "\n", "# return self.final_model.train_on_batch([states, action_mask], target), np.mean(target)", "\n", "return", "loss", ",", "np", ".", "mean", "(", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.DQNAgent.fit": [[403, 514], ["print", "dqn_tf_spatialAt.DQNAgent.save_model", "env.reset", "range", "dqn_tf_spatialAt.DQNAgent.save_model", "print", "dqn_tf_spatialAt.DQNAgent.history_processor.process_state_for_network", "dqn_tf_spatialAt.DQNAgent.select_action", "dqn_tf_spatialAt.DQNAgent.atari_processor.process_state_for_memory", "env.step", "dqn_tf_spatialAt.DQNAgent.atari_processor.process_state_for_network", "numpy.dstack", "dqn_tf_spatialAt.DQNAgent.atari_processor.process_reward", "dqn_tf_spatialAt.DQNAgent.memory.append", "Sample", "dqn_tf_spatialAt.DQNAgent.atari_processor.process_state_for_network", "dqn_tf_spatialAt.DQNAgent.atari_processor.process_state_for_memory", "dqn_tf_spatialAt.DQNAgent.memory.append", "env.reset", "dqn_tf_spatialAt.DQNAgent.atari_processor.reset", "dqn_tf_spatialAt.DQNAgent.history_processor.reset", "print", "sys.stdout.flush", "dqn_tf_spatialAt.save_scalar", "dqn_tf_spatialAt.save_scalar", "dqn_tf_spatialAt.save_scalar", "dqn_tf_spatialAt.save_scalar", "dqn_tf_spatialAt.save_scalar", "dqn_tf_spatialAt.save_scalar", "dqn_tf_spatialAt.save_scalar", "dqn_tf_spatialAt.DQNAgent.update_policy", "updateTarget", "print", "dqn_tf_spatialAt.DQNAgent.save_model"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.save_model", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.save_model", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_memory", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_reward", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_memory", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.update_policy", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.updateTarget", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.save_model"], ["", "def", "fit", "(", "self", ",", "env", ",", "num_iterations", ",", "max_episode_length", "=", "None", ")", ":", "\n", "        ", "\"\"\"Fit your model to the provided environment.\n\n        Its a good idea to print out things like loss, average reward,\n        Q-values, etc to see if your agent is actually improving.\n\n        You should probably also periodically save your network\n        weights and any other useful info.\n\n        This is where you should sample actions from your network,\n        collect experience samples and add them to your replay memory,\n        and update your network parameters.\n\n        Parameters\n        ----------\n        env: gym.Env\n          This is your Atari environment. You should wrap the\n          environment using the wrap_atari_env function in the\n          utils.py\n        num_iterations: int\n          How many samples/updates to perform.\n        max_episode_length: int\n          How long a single episode should last before the agent\n          resets. Can help exploration.\n        \"\"\"", "\n", "is_training", "=", "True", "\n", "print", "(", "\"Training starts.\"", ")", "\n", "self", ".", "save_model", "(", "0", ")", "\n", "eval_count", "=", "0", "\n", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "burn_in", "=", "True", "\n", "idx_episode", "=", "1", "\n", "episode_loss", "=", ".0", "\n", "episode_frames", "=", "0", "\n", "episode_reward", "=", ".0", "\n", "episode_raw_reward", "=", ".0", "\n", "episode_target_value", "=", ".0", "\n", "for", "t", "in", "range", "(", "self", ".", "num_burn_in", "+", "num_iterations", ")", ":", "\n", "            ", "print", "(", "\"iteration --> %s, episode --> %s\"", "%", "(", "t", ",", "idx_episode", ")", ")", "\n", "action_state", "=", "self", ".", "history_processor", ".", "process_state_for_network", "(", "\n", "self", ".", "atari_processor", ".", "process_state_for_network", "(", "state", ")", ")", "\n", "policy_type", "=", "\"UniformRandomPolicy\"", "if", "burn_in", "else", "\"LinearDecayGreedyEpsilonPolicy\"", "\n", "action", "=", "self", ".", "select_action", "(", "action_state", ",", "is_training", ",", "policy_type", "=", "policy_type", ")", "\n", "processed_state", "=", "self", ".", "atari_processor", ".", "process_state_for_memory", "(", "state", ")", "\n", "\n", "state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "processed_next_state", "=", "self", ".", "atari_processor", ".", "process_state_for_network", "(", "state", ")", "\n", "action_next_state", "=", "np", ".", "dstack", "(", "(", "action_state", ",", "processed_next_state", ")", ")", "\n", "action_next_state", "=", "action_next_state", "[", ":", ",", ":", ",", "1", ":", "]", "\n", "\n", "processed_reward", "=", "self", ".", "atari_processor", ".", "process_reward", "(", "reward", ")", "\n", "\n", "self", ".", "memory", ".", "append", "(", "processed_state", ",", "action", ",", "processed_reward", ",", "done", ")", "\n", "current_sample", "=", "Sample", "(", "action_state", ",", "action", ",", "processed_reward", ",", "action_next_state", ",", "done", ")", "\n", "\n", "if", "not", "burn_in", ":", "\n", "                ", "episode_frames", "+=", "1", "\n", "episode_reward", "+=", "processed_reward", "\n", "episode_raw_reward", "+=", "reward", "\n", "if", "episode_frames", ">", "max_episode_length", ":", "\n", "                    ", "done", "=", "True", "\n", "\n", "", "", "if", "done", ":", "\n", "# adding last frame only to save last state", "\n", "                ", "last_frame", "=", "self", ".", "atari_processor", ".", "process_state_for_memory", "(", "state", ")", "\n", "# action, reward, done doesn't matter here", "\n", "self", ".", "memory", ".", "append", "(", "last_frame", ",", "action", ",", "0", ",", "done", ")", "\n", "if", "not", "burn_in", ":", "\n", "                    ", "avg_target_value", "=", "episode_target_value", "/", "episode_frames", "\n", "print", "(", "\">>> Training: time %d, episode %d, length %d, reward %.0f, raw_reward %.0f, loss %.4f, target value %.4f, policy step %d, memory cap %d\"", "%", "\n", "(", "t", ",", "idx_episode", ",", "episode_frames", ",", "episode_reward", ",", "episode_raw_reward", ",", "episode_loss", ",", "\n", "avg_target_value", ",", "self", ".", "policy", ".", "step", ",", "self", ".", "memory", ".", "current", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train/episode_frames'", ",", "episode_frames", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train/episode_reward'", ",", "episode_reward", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train/episode_raw_reward'", ",", "episode_raw_reward", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train/episode_loss'", ",", "episode_loss", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train_avg/avg_reward'", ",", "episode_reward", "/", "episode_frames", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train_avg/avg_target_value'", ",", "avg_target_value", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train_avg/avg_loss'", ",", "episode_loss", "/", "episode_frames", ",", "self", ".", "writer", ")", "\n", "episode_frames", "=", "0", "\n", "episode_reward", "=", ".0", "\n", "episode_raw_reward", "=", ".0", "\n", "episode_loss", "=", ".0", "\n", "episode_target_value", "=", ".0", "\n", "idx_episode", "+=", "1", "\n", "", "burn_in", "=", "(", "t", "<", "self", ".", "num_burn_in", ")", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "self", ".", "atari_processor", ".", "reset", "(", ")", "\n", "self", ".", "history_processor", ".", "reset", "(", ")", "\n", "\n", "", "if", "not", "burn_in", ":", "\n", "                ", "if", "t", "%", "self", ".", "train_freq", "==", "0", ":", "\n", "                    ", "loss", ",", "target_value", "=", "self", ".", "update_policy", "(", "current_sample", ")", "\n", "episode_loss", "+=", "loss", "\n", "episode_target_value", "+=", "target_value", "\n", "# update freq is based on train_freq", "\n", "", "if", "t", "%", "(", "self", ".", "train_freq", "*", "self", ".", "target_update_freq", ")", "==", "0", ":", "\n", "# self.target_network.set_weights(self.q_network.get_weights())", "\n", "                    ", "updateTarget", "(", "self", ".", "targetOps", ",", "self", ".", "sess", ")", "\n", "print", "(", "\"----- Synced.\"", ")", "\n", "", "if", "t", "%", "self", ".", "save_freq", "==", "0", ":", "\n", "                    ", "self", ".", "save_model", "(", "idx_episode", ")", "\n", "# if t % (self.eval_freq * self.train_freq) == 0:", "\n", "#     episode_reward_mean, episode_reward_std, eval_count = self.evaluate(env, 20, eval_count, max_episode_length, True)", "\n", "#     save_scalar(t, 'eval/eval_episode_reward_mean', episode_reward_mean, self.writer)", "\n", "#     save_scalar(t, 'eval/eval_episode_reward_std', episode_reward_std, self.writer)", "\n", "\n", "", "", "", "self", ".", "save_model", "(", "idx_episode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.DQNAgent.save_model": [[516, 521], ["dqn_tf_spatialAt.DQNAgent.saver.save", "print", "str"], "methods", ["None"], ["", "def", "save_model", "(", "self", ",", "idx_episode", ")", ":", "\n", "        ", "safe_path", "=", "self", ".", "output_path", "+", "\"/qnet\"", "+", "str", "(", "idx_episode", ")", "+", "\".cptk\"", "\n", "self", ".", "saver", ".", "save", "(", "self", ".", "sess", ",", "safe_path", ")", "\n", "# self.q_network.save_weights(safe_path)", "\n", "print", "(", "\"Network at\"", ",", "idx_episode", ",", "\"saved to:\"", ",", "safe_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.DQNAgent.evaluate": [[522, 604], ["print", "plt.figure", "gym.wrappers.Monitor.reset", "numpy.zeros", "numpy.mean", "numpy.std", "print", "sys.stdout.flush", "dqn_tf_spatialAt.DQNAgent.q_network.load_weights", "print", "gym.wrappers.Monitor", "dqn_tf_spatialAt.DQNAgent.history_processor.process_state_for_network", "dqn_tf_spatialAt.DQNAgent.select_action", "dqn_tf_spatialAt.DQNAgent.history_processor.process_state_for_network_ori", "gym.wrappers.Monitor.step", "dqn_tf_spatialAt.DQNAgent.atari_processor.process_state_for_network", "dqn_tf_spatialAt.DQNAgent.atari_processor.process_state_for_network_ori", "numpy.random.random", "dqn_tf_spatialAt.DQNAgent.sess.run", "range", "plt.savefig", "print", "dqn_tf_spatialAt.save_scalar", "dqn_tf_spatialAt.save_scalar", "sys.stdout.flush", "gym.wrappers.Monitor.reset", "dqn_tf_spatialAt.DQNAgent.atari_processor.reset", "dqn_tf_spatialAt.DQNAgent.history_processor.reset", "len", "plt.subplot", "plt.imshow", "alpha_list[].reshape", "skimage.transform.pyramid_expand", "plt.imshow", "plt.axis", "scipy.misc.imresize", "len"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network_ori", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network_ori", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset"], ["", "def", "evaluate", "(", "self", ",", "env", ",", "num_episodes", ",", "eval_count", ",", "max_episode_length", "=", "None", ",", "monitor", "=", "True", ")", ":", "\n", "        ", "\"\"\"Test your agent with a provided environment.\n        \n        You shouldn't update your network parameters here. Also if you\n        have any layers that vary in behavior between train/test time\n        (such as dropout or batch norm), you should set them to test.\n\n        Basically run your policy on the environment and collect stats\n        like cumulative reward, average episode length, etc.\n\n        You can also call the render function here if you want to\n        visually inspect your policy.\n        \"\"\"", "\n", "print", "(", "\"Evaluation starts.\"", ")", "\n", "plt", ".", "figure", "(", "1", ",", "figsize", "=", "(", "40", ",", "20", ")", ")", "\n", "\n", "is_training", "=", "False", "\n", "if", "self", ".", "load_network", ":", "\n", "            ", "self", ".", "q_network", ".", "load_weights", "(", "self", ".", "load_network_path", ")", "\n", "print", "(", "\"Load network from:\"", ",", "self", ".", "load_network_path", ")", "\n", "", "if", "monitor", ":", "\n", "            ", "env", "=", "wrappers", ".", "Monitor", "(", "env", ",", "self", ".", "output_path_videos", ",", "video_callable", "=", "lambda", "x", ":", "True", ",", "resume", "=", "True", ")", "\n", "", "state", "=", "env", ".", "reset", "(", ")", "\n", "\n", "idx_episode", "=", "1", "\n", "episode_frames", "=", "0", "\n", "episode_reward", "=", "np", ".", "zeros", "(", "num_episodes", ")", "\n", "t", "=", "0", "\n", "\n", "while", "idx_episode", "<=", "num_episodes", ":", "\n", "            ", "t", "+=", "1", "\n", "action_state", "=", "self", ".", "history_processor", ".", "process_state_for_network", "(", "\n", "self", ".", "atari_processor", ".", "process_state_for_network", "(", "state", ")", ")", "\n", "action", "=", "self", ".", "select_action", "(", "action_state", ",", "is_training", ",", "policy_type", "=", "'GreedyEpsilonPolicy'", ")", "\n", "\n", "action_state_ori", "=", "self", ".", "history_processor", ".", "process_state_for_network_ori", "(", "\n", "self", ".", "atari_processor", ".", "process_state_for_network_ori", "(", "state", ")", ")", "\n", "# print \"state.shape\", state.shape", "\n", "# print \"action_state_ori.shape\", action_state_ori.shape", "\n", "\n", "if", "np", ".", "random", ".", "random", "(", ")", "<", "1e-3", ":", "\n", "                ", "alpha_list", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "q_network", ".", "alpha_list", ",", "feed_dict", "=", "{", "self", ".", "q_network", ".", "imageIn", ":", "action_state", "[", "None", ",", ":", ",", ":", ",", ":", "]", ",", "self", ".", "q_network", ".", "batch_size", ":", "1", "}", ")", "\n", "# print alpha_list, len(alpha_list), alpha_list[0].shape #10 (1, 49)", "\n", "for", "alpha_idx", "in", "range", "(", "len", "(", "alpha_list", ")", ")", ":", "\n", "                    ", "plt", ".", "subplot", "(", "2", ",", "len", "(", "alpha_list", ")", "//", "2", ",", "alpha_idx", "+", "1", ")", "\n", "img", "=", "action_state_ori", "[", ":", ",", ":", ",", ":", ",", "alpha_idx", "]", "#(210, 160, 3)", "\n", "plt", ".", "imshow", "(", "img", ")", "\n", "alp_curr", "=", "alpha_list", "[", "alpha_idx", "]", ".", "reshape", "(", "7", ",", "7", ")", "\n", "alp_img", "=", "skimage", ".", "transform", ".", "pyramid_expand", "(", "alp_curr", ",", "upscale", "=", "22", ",", "sigma", "=", "20", ")", "\n", "plt", ".", "imshow", "(", "scipy", ".", "misc", ".", "imresize", "(", "alp_img", ",", "(", "img", ".", "shape", "[", "0", "]", ",", "img", ".", "shape", "[", "1", "]", ")", ")", ",", "alpha", "=", "0.7", ",", "cmap", "=", "'gray'", ")", "\n", "plt", ".", "axis", "(", "'off'", ")", "\n", "# plt.show()", "\n", "# plt.canvas.draw()", "\n", "", "plt", ".", "savefig", "(", "'%sattention_ep%d-frame%d.png'", "%", "(", "self", ".", "output_path_images", ",", "eval_count", ",", "episode_frames", ")", ")", "\n", "\n", "", "state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "episode_frames", "+=", "1", "\n", "episode_reward", "[", "idx_episode", "-", "1", "]", "+=", "reward", "\n", "if", "episode_frames", ">", "max_episode_length", ":", "\n", "                ", "done", "=", "True", "\n", "", "if", "done", ":", "\n", "                ", "print", "(", "\"Eval: time %d, episode %d, length %d, reward %.0f. @eval_count %s\"", "%", "\n", "(", "t", ",", "idx_episode", ",", "episode_frames", ",", "episode_reward", "[", "idx_episode", "-", "1", "]", ",", "eval_count", ")", ")", "\n", "eval_count", "+=", "1", "\n", "save_scalar", "(", "eval_count", ",", "'eval/eval_episode_raw_reward'", ",", "episode_reward", "[", "idx_episode", "-", "1", "]", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "eval_count", ",", "'eval/eval_episode_raw_length'", ",", "episode_frames", ",", "self", ".", "writer", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "episode_frames", "=", "0", "\n", "idx_episode", "+=", "1", "\n", "self", ".", "atari_processor", ".", "reset", "(", ")", "\n", "self", ".", "history_processor", ".", "reset", "(", ")", "\n", "\n", "\n", "", "", "reward_mean", "=", "np", ".", "mean", "(", "episode_reward", ")", "\n", "reward_std", "=", "np", ".", "std", "(", "episode_reward", ")", "\n", "print", "(", "\"Evaluation summury: num_episodes [%d], reward_mean [%.3f], reward_std [%.3f]\"", "%", "\n", "(", "num_episodes", ",", "reward_mean", ",", "reward_std", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "return", "reward_mean", ",", "reward_std", ",", "eval_count", "\n", "", "", ""]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_spatialAt.save_scalar": [[169, 187], ["tensorflow.Summary", "tf.Summary.value.add", "float", "writer.add_summary"], "function", ["None"], ["", "", "", "def", "save_scalar", "(", "step", ",", "name", ",", "value", ",", "writer", ")", ":", "\n", "    ", "\"\"\"Save a scalar value to tensorboard.\n      Parameters\n      ----------\n      step: int\n        Training step (sets the position on x-axis of tensorboard graph.\n      name: str\n        Name of variable. Will be the name of the graph in tensorboard.\n      value: float\n        The value of the variable at this step.\n      writer: tf.FileWriter\n        The tensorboard FileWriter instance.\n      \"\"\"", "\n", "summary", "=", "tf", ".", "Summary", "(", ")", "\n", "summary_value", "=", "summary", ".", "value", ".", "add", "(", ")", "\n", "summary_value", ".", "simple_value", "=", "float", "(", "value", ")", "\n", "summary_value", ".", "tag", "=", "name", "\n", "writer", ".", "add_summary", "(", "summary", ",", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.DQNAgent.__init__": [[157, 196], ["HistoryPreprocessor", "AtariPreprocessor", "ReplayMemory", "LinearDecayGreedyEpsilonPolicy", "dqn_keras.create_model", "dqn_keras.create_model", "print", "print", "dqn_keras.DQNAgent.target_network.set_weights", "dqn_keras.DQNAgent.compile", "tensorflow.summary.FileWriter", "print", "dqn_keras.DQNAgent.q_network.get_weights"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.create_model", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.create_model", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.DQNAgent.compile"], ["def", "__init__", "(", "self", ",", "args", ",", "num_actions", ")", ":", "\n", "        ", "self", ".", "num_actions", "=", "num_actions", "\n", "input_shape", "=", "(", "args", ".", "frame_height", ",", "args", ".", "frame_width", ",", "args", ".", "num_frames", ")", "\n", "self", ".", "history_processor", "=", "HistoryPreprocessor", "(", "args", ".", "num_frames", "-", "1", ")", "\n", "self", ".", "atari_processor", "=", "AtariPreprocessor", "(", ")", "\n", "self", ".", "memory", "=", "ReplayMemory", "(", "args", ")", "\n", "self", ".", "policy", "=", "LinearDecayGreedyEpsilonPolicy", "(", "args", ".", "initial_epsilon", ",", "args", ".", "final_epsilon", ",", "args", ".", "exploration_steps", ")", "\n", "self", ".", "gamma", "=", "args", ".", "gamma", "\n", "self", ".", "target_update_freq", "=", "args", ".", "target_update_freq", "\n", "self", ".", "num_burn_in", "=", "args", ".", "num_burn_in", "\n", "self", ".", "train_freq", "=", "args", ".", "train_freq", "\n", "self", ".", "batch_size", "=", "args", ".", "batch_size", "\n", "self", ".", "learning_rate", "=", "args", ".", "learning_rate", "\n", "self", ".", "frame_width", "=", "args", ".", "frame_width", "\n", "self", ".", "frame_height", "=", "args", ".", "frame_height", "\n", "self", ".", "num_frames", "=", "args", ".", "num_frames", "\n", "self", ".", "output_path", "=", "args", ".", "output", "\n", "self", ".", "output_path_videos", "=", "args", ".", "output", "+", "'/videos/'", "\n", "self", ".", "save_freq", "=", "args", ".", "save_freq", "\n", "self", ".", "load_network", "=", "args", ".", "load_network", "\n", "self", ".", "load_network_path", "=", "args", ".", "load_network_path", "\n", "self", ".", "enable_ddqn", "=", "args", ".", "ddqn", "\n", "self", ".", "net_mode", "=", "args", ".", "net_mode", "\n", "self", ".", "q_network", "=", "create_model", "(", "input_shape", ",", "num_actions", ",", "self", ".", "net_mode", ",", "args", ",", "\"QNet\"", ")", "\n", "self", ".", "target_network", "=", "create_model", "(", "input_shape", ",", "num_actions", ",", "self", ".", "net_mode", ",", "args", ",", "\"TargetNet\"", ")", "\n", "print", "(", "\">>>> Net mode: %s, Using double dqn: %s\"", "%", "(", "self", ".", "net_mode", ",", "self", ".", "enable_ddqn", ")", ")", "\n", "self", ".", "eval_freq", "=", "args", ".", "eval_freq", "\n", "self", ".", "no_experience", "=", "args", ".", "no_experience", "\n", "self", ".", "no_target", "=", "args", ".", "no_target", "\n", "print", "(", "\">>>> Target fixing: %s, Experience replay: %s\"", "%", "(", "not", "self", ".", "no_target", ",", "not", "self", ".", "no_experience", ")", ")", "\n", "\n", "# initialize target network", "\n", "self", ".", "target_network", ".", "set_weights", "(", "self", ".", "q_network", ".", "get_weights", "(", ")", ")", "\n", "self", ".", "final_model", "=", "None", "\n", "self", ".", "compile", "(", ")", "\n", "\n", "self", ".", "writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "self", ".", "output_path", ")", "\n", "\n", "print", "(", "\"*******__init__\"", ",", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.DQNAgent.compile": [[197, 221], ["keras.models.Model", "dqn_keras.DQNAgent.final_model.compile", "keras.optimizers.Adam", "tensorflow.variable_scope", "keras.layers.Input", "keras.layers.Input", "dqn_keras.DQNAgent.q_network", "keras.layers.merge", "keras.layers.Lambda", "tensorflow.reduce_sum"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.DQNAgent.compile"], ["", "def", "compile", "(", "self", ",", "optimizer", "=", "None", ",", "loss_func", "=", "None", ")", ":", "\n", "        ", "\"\"\"Setup all of the TF graph variables/ops.\n\n        This is inspired by the compile method on the\n        keras.models.Model class.\n\n        This is the place to create the target network, setup \n        loss function and any placeholders.\n        \"\"\"", "\n", "if", "loss_func", "is", "None", ":", "\n", "            ", "loss_func", "=", "mean_huber_loss", "\n", "# loss_func = 'mse'", "\n", "", "if", "optimizer", "is", "None", ":", "\n", "            ", "optimizer", "=", "Adam", "(", "lr", "=", "self", ".", "learning_rate", ")", "\n", "# optimizer = RMSprop(lr=0.00025)", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"Loss\"", ")", ":", "\n", "            ", "state", "=", "Input", "(", "shape", "=", "(", "self", ".", "frame_height", ",", "self", ".", "frame_width", ",", "self", ".", "num_frames", ")", ",", "name", "=", "\"states\"", ")", "\n", "action_mask", "=", "Input", "(", "shape", "=", "(", "self", ".", "num_actions", ",", ")", ",", "name", "=", "\"actions\"", ")", "\n", "qa_value", "=", "self", ".", "q_network", "(", "state", ")", "\n", "qa_value", "=", "merge", "(", "[", "qa_value", ",", "action_mask", "]", ",", "mode", "=", "'mul'", ",", "name", "=", "\"multiply\"", ")", "\n", "qa_value", "=", "Lambda", "(", "lambda", "x", ":", "tf", ".", "reduce_sum", "(", "x", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", ",", "name", "=", "\"sum\"", ")", "(", "qa_value", ")", "\n", "\n", "", "self", ".", "final_model", "=", "Model", "(", "inputs", "=", "[", "state", ",", "action_mask", "]", ",", "outputs", "=", "qa_value", ")", "\n", "self", ".", "final_model", ".", "compile", "(", "loss", "=", "loss_func", ",", "optimizer", "=", "optimizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.DQNAgent.calc_q_values": [[222, 233], ["dqn_keras.DQNAgent.q_network.predict_on_batch"], "methods", ["None"], ["", "def", "calc_q_values", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Given a state (or batch of states) calculate the Q-values.\n\n        Basically run your network on these states.\n\n        Return\n        ------\n        Q-values for the state(s)\n        \"\"\"", "\n", "state", "=", "state", "[", "None", ",", ":", ",", ":", ",", ":", "]", "\n", "return", "self", ".", "q_network", ".", "predict_on_batch", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.DQNAgent.select_action": [[234, 251], ["dqn_keras.DQNAgent.calc_q_values", "GreedyPolicy().select_action", "UniformRandomPolicy().select_action", "dqn_keras.DQNAgent.policy.select_action", "GreedyPolicy", "UniformRandomPolicy"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.calc_q_values", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action"], ["", "def", "select_action", "(", "self", ",", "state", ",", "is_training", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Select the action based on the current state.\n\n        Returns\n        --------\n        selected action\n        \"\"\"", "\n", "q_values", "=", "self", ".", "calc_q_values", "(", "state", ")", "\n", "if", "is_training", ":", "\n", "            ", "if", "kwargs", "[", "'policy_type'", "]", "==", "'UniformRandomPolicy'", ":", "\n", "                ", "return", "UniformRandomPolicy", "(", "self", ".", "num_actions", ")", ".", "select_action", "(", ")", "\n", "", "else", ":", "\n", "# linear decay greedy epsilon policy", "\n", "                ", "return", "self", ".", "policy", ".", "select_action", "(", "q_values", ",", "is_training", ")", "\n", "", "", "else", ":", "\n", "# return GreedyEpsilonPolicy(0.05).select_action(q_values)", "\n", "            ", "return", "GreedyPolicy", "(", ")", ".", "select_action", "(", "q_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.DQNAgent.update_policy": [[252, 304], ["numpy.stack", "numpy.stack", "numpy.asarray", "numpy.asarray", "numpy.zeros", "dqn_keras.DQNAgent.memory.sample", "dqn_keras.DQNAgent.atari_processor.process_batch", "numpy.stack", "numpy.asarray", "numpy.zeros", "numpy.stack", "numpy.asarray", "numpy.asarray", "dqn_keras.DQNAgent.q_network.predict_on_batch", "dqn_keras.DQNAgent.target_network.predict_on_batch", "dqn_keras.DQNAgent.q_network.predict_on_batch", "numpy.argmax", "numpy.max", "dqn_keras.DQNAgent.final_model.train_on_batch", "numpy.mean", "int", "range", "int", "range"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.sample", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_batch"], ["", "", "def", "update_policy", "(", "self", ",", "current_sample", ")", ":", "\n", "        ", "\"\"\"Update your policy.\n\n        Behavior may differ based on what stage of training your\n        in. If you're in training mode then you should check if you\n        should update your network parameters based on the current\n        step and the value you set for train_freq.\n\n        Inside, you'll want to sample a minibatch, calculate the\n        target values, update your network, and then update your\n        target values.\n\n        You might want to return the loss and other metrics as an\n        output. They can help you monitor how training is going.\n        \"\"\"", "\n", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "if", "self", ".", "no_experience", ":", "\n", "            ", "states", "=", "np", ".", "stack", "(", "[", "current_sample", ".", "state", "]", ")", "\n", "next_states", "=", "np", ".", "stack", "(", "[", "current_sample", ".", "next_state", "]", ")", "\n", "rewards", "=", "np", ".", "asarray", "(", "[", "current_sample", ".", "reward", "]", ")", "\n", "mask", "=", "np", ".", "asarray", "(", "[", "1", "-", "int", "(", "current_sample", ".", "is_terminal", ")", "]", ")", "\n", "\n", "action_mask", "=", "np", ".", "zeros", "(", "(", "1", ",", "self", ".", "num_actions", ")", ")", "\n", "action_mask", "[", "0", ",", "current_sample", ".", "action", "]", "=", "1.0", "\n", "", "else", ":", "\n", "            ", "samples", "=", "self", ".", "memory", ".", "sample", "(", "batch_size", ")", "\n", "samples", "=", "self", ".", "atari_processor", ".", "process_batch", "(", "samples", ")", "\n", "\n", "states", "=", "np", ".", "stack", "(", "[", "x", ".", "state", "for", "x", "in", "samples", "]", ")", "\n", "actions", "=", "np", ".", "asarray", "(", "[", "x", ".", "action", "for", "x", "in", "samples", "]", ")", "\n", "action_mask", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "self", ".", "num_actions", ")", ")", "\n", "action_mask", "[", "range", "(", "batch_size", ")", ",", "actions", "]", "=", "1.0", "\n", "\n", "next_states", "=", "np", ".", "stack", "(", "[", "x", ".", "next_state", "for", "x", "in", "samples", "]", ")", "\n", "mask", "=", "np", ".", "asarray", "(", "[", "1", "-", "int", "(", "x", ".", "is_terminal", ")", "for", "x", "in", "samples", "]", ")", "\n", "rewards", "=", "np", ".", "asarray", "(", "[", "x", ".", "reward", "for", "x", "in", "samples", "]", ")", "\n", "\n", "", "if", "self", ".", "no_target", ":", "\n", "            ", "next_qa_value", "=", "self", ".", "q_network", ".", "predict_on_batch", "(", "next_states", ")", "\n", "", "else", ":", "\n", "            ", "next_qa_value", "=", "self", ".", "target_network", ".", "predict_on_batch", "(", "next_states", ")", "\n", "\n", "", "if", "self", ".", "enable_ddqn", ":", "\n", "            ", "qa_value", "=", "self", ".", "q_network", ".", "predict_on_batch", "(", "next_states", ")", "\n", "max_actions", "=", "np", ".", "argmax", "(", "qa_value", ",", "axis", "=", "1", ")", "\n", "next_qa_value", "=", "next_qa_value", "[", "range", "(", "batch_size", ")", ",", "max_actions", "]", "\n", "", "else", ":", "\n", "            ", "next_qa_value", "=", "np", ".", "max", "(", "next_qa_value", ",", "axis", "=", "1", ")", "\n", "", "target", "=", "rewards", "+", "self", ".", "gamma", "*", "mask", "*", "next_qa_value", "\n", "\n", "return", "self", ".", "final_model", ".", "train_on_batch", "(", "[", "states", ",", "action_mask", "]", ",", "target", ")", ",", "np", ".", "mean", "(", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.DQNAgent.fit": [[305, 453], ["print", "dqn_keras.DQNAgent.save_model", "env.reset", "list", "list", "range", "dqn_keras.DQNAgent.save_model", "print", "dqn_keras.DQNAgent.history_processor.process_state_for_network", "dqn_keras.DQNAgent.select_action", "dqn_keras.DQNAgent.atari_processor.process_state_for_memory", "env.render", "env.step", "dqn_keras.DQNAgent.atari_processor.process_state_for_network", "numpy.dstack", "dqn_keras.DQNAgent.atari_processor.process_reward", "dqn_keras.DQNAgent.memory.append", "Sample", "dqn_keras.DQNAgent.atari_processor.process_state_for_network", "list.append", "dqn_keras.DQNAgent.atari_processor.process_state_for_memory", "dqn_keras.DQNAgent.memory.append", "env.reset", "dqn_keras.DQNAgent.atari_processor.reset", "dqn_keras.DQNAgent.history_processor.reset", "print", "sys.stdout.flush", "dqn_keras.save_scalar", "dqn_keras.save_scalar", "dqn_keras.save_scalar", "dqn_keras.save_scalar", "dqn_keras.save_scalar", "dqn_keras.save_scalar", "dqn_keras.save_scalar", "list.append", "dqn_keras.DQNAgent.update_policy", "dqn_keras.DQNAgent.target_network.set_weights", "dqn_keras.DQNAgent.save_model", "numpy.asarray", "print", "numpy.savetxt", "numpy.asarray", "print", "numpy.savetxt", "dqn_keras.DQNAgent.q_network.get_weights", "str", "str"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.save_model", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.save_model", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_memory", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_reward", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_memory", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.update_policy", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.save_model"], ["", "def", "fit", "(", "self", ",", "env", ",", "num_iterations", ",", "max_episode_length", "=", "None", ")", ":", "\n", "        ", "\"\"\"Fit your model to the provided environment.\n\n        This is where you sample actions from your network,\n        collect experience samples and add them to your replay memory,\n        and update your network parameters.\n\n        Parameters\n        ----------\n        env: gym.Env\n          This is the Atari environment. \n        num_iterations: int\n          How many samples/updates to perform.\n        max_episode_length: int\n          How long a single episode should last before the agent\n          resets. Can help exploration.\n        \"\"\"", "\n", "is_training", "=", "True", "\n", "print", "(", "\"Training starts.\"", ")", "\n", "self", ".", "save_model", "(", "0", ")", "\n", "eval_count", "=", "0", "\n", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "\n", "burn_in", "=", "True", "\n", "idx_episode", "=", "1", "\n", "episode_loss", "=", ".0", "\n", "episode_frames", "=", "0", "\n", "episode_reward", "=", ".0", "\n", "episode_raw_reward", "=", ".0", "\n", "episode_target_value", "=", ".0", "\n", "\n", "# Logs", "\n", "losses_list", "=", "list", "(", ")", "\n", "step_loss_list", "=", "list", "(", ")", "\n", "step_reward", "=", "0.0", "\n", "step_reward_raw", "=", "0.0", "\n", "\n", "for", "t", "in", "range", "(", "self", ".", "num_burn_in", "+", "num_iterations", ")", ":", "\n", "            ", "print", "(", "\"iteration --> %s, episode --> %s\"", "%", "(", "t", ",", "idx_episode", ")", ")", "\n", "action_state", "=", "self", ".", "history_processor", ".", "process_state_for_network", "(", "\n", "self", ".", "atari_processor", ".", "process_state_for_network", "(", "state", ")", ")", "\n", "policy_type", "=", "\"UniformRandomPolicy\"", "if", "burn_in", "else", "\"LinearDecayGreedyEpsilonPolicy\"", "\n", "action", "=", "self", ".", "select_action", "(", "action_state", ",", "is_training", ",", "policy_type", "=", "policy_type", ")", "\n", "processed_state", "=", "self", ".", "atari_processor", ".", "process_state_for_memory", "(", "state", ")", "\n", "\n", "# print(\"******* fit_action\", action_state.shape)", "\n", "# print(\"******* fit_proecess\", processed_state.shape)", "\n", "\n", "env", ".", "render", "(", ")", "\n", "state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "processed_next_state", "=", "self", ".", "atari_processor", ".", "process_state_for_network", "(", "state", ")", "\n", "action_next_state", "=", "np", ".", "dstack", "(", "(", "action_state", ",", "processed_next_state", ")", ")", "\n", "action_next_state", "=", "action_next_state", "[", ":", ",", ":", ",", "1", ":", "]", "\n", "\n", "processed_reward", "=", "self", ".", "atari_processor", ".", "process_reward", "(", "reward", ")", "\n", "\n", "self", ".", "memory", ".", "append", "(", "processed_state", ",", "action", ",", "processed_reward", ",", "done", ")", "\n", "current_sample", "=", "Sample", "(", "action_state", ",", "action", ",", "processed_reward", ",", "action_next_state", ",", "done", ")", "\n", "\n", "if", "not", "burn_in", ":", "\n", "                ", "episode_frames", "+=", "1", "\n", "episode_reward", "+=", "processed_reward", "\n", "episode_raw_reward", "+=", "reward", "\n", "if", "episode_frames", ">", "max_episode_length", ":", "\n", "                    ", "done", "=", "True", "\n", "\n", "", "", "if", "not", "burn_in", ":", "\n", "                ", "step_reward", "+=", "processed_reward", "\n", "step_reward_raw", "+=", "reward", "\n", "step_losses", "=", "[", "t", "-", "last_burn", "-", "1", ",", "step_reward", ",", "step_reward_raw", ",", "step_reward", "/", "(", "t", "-", "last_burn", "-", "1", ")", ",", "step_reward_raw", "/", "(", "t", "-", "last_burn", "-", "1", ")", "]", "\n", "step_loss_list", ".", "append", "(", "step_losses", ")", "\n", "\n", "\n", "", "if", "done", ":", "\n", "# adding last frame only to save last state", "\n", "                ", "last_frame", "=", "self", ".", "atari_processor", ".", "process_state_for_memory", "(", "state", ")", "\n", "# action, reward, done doesn't matter here", "\n", "self", ".", "memory", ".", "append", "(", "last_frame", ",", "action", ",", "0", ",", "done", ")", "\n", "if", "not", "burn_in", ":", "\n", "                    ", "avg_target_value", "=", "episode_target_value", "/", "episode_frames", "\n", "print", "(", "\">>> Training: time %d, episode %d, length %d, reward %.0f, raw_reward %.0f, loss %.4f, target value %.4f, policy step %d, memory cap %d\"", "%", "\n", "(", "t", ",", "idx_episode", ",", "episode_frames", ",", "episode_reward", ",", "episode_raw_reward", ",", "episode_loss", ",", "\n", "avg_target_value", ",", "self", ".", "policy", ".", "step", ",", "self", ".", "memory", ".", "current", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train/episode_frames'", ",", "episode_frames", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train/episode_reward'", ",", "episode_reward", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train/episode_raw_reward'", ",", "episode_raw_reward", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train/episode_loss'", ",", "episode_loss", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train_avg/avg_reward'", ",", "episode_reward", "/", "episode_frames", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train_avg/avg_target_value'", ",", "avg_target_value", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train_avg/avg_loss'", ",", "episode_loss", "/", "episode_frames", ",", "self", ".", "writer", ")", "\n", "\n", "# log losses", "\n", "losses", "=", "[", "idx_episode", ",", "episode_frames", ",", "episode_reward", ",", "episode_raw_reward", ",", "episode_loss", ",", "episode_reward", "/", "episode_frames", ",", "avg_target_value", ",", "episode_loss", "/", "episode_frames", "]", "\n", "losses_list", ".", "append", "(", "losses", ")", "\n", "\n", "# reset values", "\n", "episode_frames", "=", "0", "\n", "episode_reward", "=", ".0", "\n", "episode_raw_reward", "=", ".0", "\n", "episode_loss", "=", ".0", "\n", "episode_target_value", "=", ".0", "\n", "idx_episode", "+=", "1", "\n", "", "burn_in", "=", "(", "t", "<", "self", ".", "num_burn_in", ")", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "self", ".", "atari_processor", ".", "reset", "(", ")", "\n", "self", ".", "history_processor", ".", "reset", "(", ")", "\n", "\n", "", "if", "burn_in", ":", "\n", "                ", "last_burn", "=", "t", "\n", "\n", "", "if", "not", "burn_in", ":", "\n", "                ", "if", "t", "%", "self", ".", "train_freq", "==", "0", ":", "\n", "                    ", "loss", ",", "target_value", "=", "self", ".", "update_policy", "(", "current_sample", ")", "\n", "episode_loss", "+=", "loss", "\n", "episode_target_value", "+=", "target_value", "\n", "# update freq is based on train_freq", "\n", "", "if", "t", "%", "(", "self", ".", "train_freq", "*", "self", ".", "target_update_freq", ")", "==", "0", ":", "\n", "# target updates can have the option to be hard or soft", "\n", "# related functions are defined in deeprl_prj.utils", "\n", "# here we use hard target update as default", "\n", "                    ", "self", ".", "target_network", ".", "set_weights", "(", "self", ".", "q_network", ".", "get_weights", "(", ")", ")", "\n", "", "if", "t", "%", "self", ".", "save_freq", "==", "0", ":", "\n", "                    ", "self", ".", "save_model", "(", "idx_episode", ")", "\n", "\n", "loss_array", "=", "np", ".", "asarray", "(", "losses_list", ")", "\n", "print", "(", "loss_array", ".", "shape", ")", "# 10 element vector", "\n", "\n", "# loss_path = os.path.join('./losses/loss_episode%s.csv' % (idx_episode))", "\n", "loss_path", "=", "self", ".", "output_path", "+", "\"/losses/loss_episodes\"", "+", "str", "(", "idx_episode", ")", "+", "\".csv\"", "\n", "np", ".", "savetxt", "(", "loss_path", ",", "loss_array", ",", "fmt", "=", "'%.5f'", ",", "delimiter", "=", "','", ")", "\n", "\n", "step_loss_array", "=", "np", ".", "asarray", "(", "step_loss_list", ")", "\n", "print", "(", "step_loss_array", ".", "shape", ")", "# 10 element vector", "\n", "\n", "step_loss_path", "=", "self", ".", "output_path", "+", "\"/losses/loss_steps\"", "+", "str", "(", "t", "-", "last_burn", "-", "1", ")", "+", "\".csv\"", "\n", "np", ".", "savetxt", "(", "step_loss_path", ",", "step_loss_array", ",", "fmt", "=", "'%.5f'", ",", "delimiter", "=", "','", ")", "\n", "\n", "\n", "# No evaluation while training", "\n", "# if t % (self.eval_freq * self.train_freq) == 0:", "\n", "#     episode_reward_mean, episode_reward_std, eval_count = self.evaluate(env, 1, eval_count, max_episode_length, True)", "\n", "#     save_scalar(t, 'eval/eval_episode_reward_mean', episode_reward_mean, self.writer)", "\n", "#     save_scalar(t, 'eval/eval_episode_reward_std', episode_reward_std, self.writer)", "\n", "\n", "", "", "", "self", ".", "save_model", "(", "idx_episode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.DQNAgent.save_model": [[455, 459], ["dqn_keras.DQNAgent.q_network.save_weights", "print", "str"], "methods", ["None"], ["", "def", "save_model", "(", "self", ",", "idx_episode", ")", ":", "\n", "        ", "safe_path", "=", "self", ".", "output_path", "+", "\"/qnet\"", "+", "str", "(", "idx_episode", ")", "+", "\".h5\"", "\n", "self", ".", "q_network", ".", "save_weights", "(", "safe_path", ")", "\n", "print", "(", "\"Network at\"", ",", "idx_episode", ",", "\"saved to:\"", ",", "safe_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.DQNAgent.evaluate": [[460, 514], ["print", "env.reset", "numpy.zeros", "numpy.mean", "numpy.std", "print", "sys.stdout.flush", "dqn_keras.DQNAgent.q_network.load_weights", "print", "dqn_keras.DQNAgent.history_processor.process_state_for_network", "dqn_keras.DQNAgent.select_action", "env.step", "dqn_keras.DQNAgent.atari_processor.process_state_for_network", "print", "dqn_keras.save_scalar", "dqn_keras.save_scalar", "sys.stdout.flush", "env.reset", "dqn_keras.DQNAgent.atari_processor.reset", "dqn_keras.DQNAgent.history_processor.reset"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset"], ["", "def", "evaluate", "(", "self", ",", "env", ",", "num_episodes", ",", "eval_count", ",", "max_episode_length", "=", "None", ",", "monitor", "=", "False", ")", ":", "\n", "        ", "\"\"\"Test your agent with a provided environment.\n\n        Basically run your policy on the environment and collect stats\n        like cumulative reward, average episode length, etc.\n\n        You can also call the render function here if you want to\n        visually inspect your policy.\n        \"\"\"", "\n", "print", "(", "\"Evaluation starts.\"", ")", "\n", "\n", "is_training", "=", "False", "\n", "if", "self", ".", "load_network", ":", "\n", "            ", "self", ".", "q_network", ".", "load_weights", "(", "self", ".", "load_network_path", ")", "\n", "print", "(", "\"Load network from:\"", ",", "self", ".", "load_network_path", ")", "\n", "# if monitor:", "\n", "#     env = wrappers.Monitor(env, self.output_path_videos, video_callable=lambda x:True, resume=True)", "\n", "", "state", "=", "env", ".", "reset", "(", ")", "\n", "\n", "idx_episode", "=", "1", "\n", "episode_frames", "=", "0", "\n", "episode_reward", "=", "np", ".", "zeros", "(", "num_episodes", ")", "\n", "t", "=", "0", "\n", "\n", "while", "idx_episode", "<=", "num_episodes", ":", "\n", "            ", "t", "+=", "1", "\n", "action_state", "=", "self", ".", "history_processor", ".", "process_state_for_network", "(", "\n", "self", ".", "atari_processor", ".", "process_state_for_network", "(", "state", ")", ")", "\n", "action", "=", "self", ".", "select_action", "(", "action_state", ",", "is_training", ",", "policy_type", "=", "'GreedyEpsilonPolicy'", ")", "\n", "state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "episode_frames", "+=", "1", "\n", "episode_reward", "[", "idx_episode", "-", "1", "]", "+=", "reward", "\n", "if", "episode_frames", ">", "max_episode_length", ":", "\n", "                ", "done", "=", "True", "\n", "", "if", "done", ":", "\n", "                ", "print", "(", "\"Eval: time %d, episode %d, length %d, reward %.0f\"", "%", "\n", "(", "t", ",", "idx_episode", ",", "episode_frames", ",", "episode_reward", "[", "idx_episode", "-", "1", "]", ")", ")", "\n", "eval_count", "+=", "1", "\n", "save_scalar", "(", "eval_count", ",", "'eval/eval_episode_raw_reward'", ",", "episode_reward", "[", "idx_episode", "-", "1", "]", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "eval_count", ",", "'eval/eval_episode_raw_length'", ",", "episode_frames", ",", "self", ".", "writer", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "episode_frames", "=", "0", "\n", "idx_episode", "+=", "1", "\n", "self", ".", "atari_processor", ".", "reset", "(", ")", "\n", "self", ".", "history_processor", ".", "reset", "(", ")", "\n", "\n", "", "", "reward_mean", "=", "np", ".", "mean", "(", "episode_reward", ")", "\n", "reward_std", "=", "np", ".", "std", "(", "episode_reward", ")", "\n", "print", "(", "\"Evaluation summury: num_episodes [%d], reward_mean [%.3f], reward_std [%.3f]\"", "%", "\n", "(", "num_episodes", ",", "reward_mean", ",", "reward_std", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "return", "reward_mean", ",", "reward_std", ",", "eval_count", "\n", "", "", ""]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.create_model": [[28, 106], ["keras.models.Model", "print", "tensorflow.variable_scope", "keras.layers.Input", "keras.models.Model.summary", "keras.layers.Flatten", "keras.layers.Dense", "print", "keras.layers.Convolution2D", "keras.layers.Convolution2D", "keras.layers.Convolution2D", "keras.layers.Flatten", "keras.layers.Reshape", "keras.layers.Permute", "keras.layers.TimeDistributed", "keras.layers.TimeDistributed", "keras.layers.TimeDistributed", "keras.layers.TimeDistributed", "keras.layers.TimeDistributed", "keras.layers.merge", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Convolution2D", "keras.layers.Convolution2D", "keras.layers.Convolution2D", "keras.layers.Flatten", "keras.layers.Dense", "keras.layers.LSTM", "keras.layers.TimeDistributed", "keras.layers.Flatten", "keras.layers.Activation", "keras.layers.RepeatVector", "keras.layers.Permute", "keras.layers.Lambda", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Lambda", "keras.layers.Lambda", "keras.layers.wrappers.Bidirectional", "keras.layers.wrappers.Bidirectional", "keras.layers.LSTM", "keras.layers.Dense", "keras.layers.LSTM", "keras.layers.LSTM", "keras.backend.sum", "tensorflow.reduce_mean"], "function", ["None"], ["def", "create_model", "(", "input_shape", ",", "num_actions", ",", "mode", ",", "args", ",", "model_name", "=", "'q_network'", ")", ":", "# noqa: D103", "\n", "    ", "\"\"\"Create the Q-network model.\n\n    Use Keras to construct a keras.models.Model instance.\n\n    Parameters\n    ----------\n    window: int\n      Each input to the network is a sequence of frames. This value\n      defines how many frames are in the sequence.\n    input_shape: tuple(int, int, int), rows, cols, channels\n      The expected input image size.\n    num_actions: int\n      Number of possible actions. Defined by the gym environment.\n    model_name: str\n      Useful when debugging. Makes the model show up nicer in tensorboard.\n\n    Returns\n    -------\n    keras.models.Model\n      The Q-model.\n    \"\"\"", "\n", "assert", "(", "mode", "in", "(", "\"linear\"", ",", "\"duel\"", ",", "\"dqn\"", ")", ")", "\n", "with", "tf", ".", "variable_scope", "(", "model_name", ")", ":", "\n", "        ", "input_data", "=", "Input", "(", "shape", "=", "input_shape", ",", "name", "=", "\"input\"", ")", "\n", "if", "mode", "==", "\"linear\"", ":", "# We will never enter this loop", "\n", "            ", "flatten_hidden", "=", "Flatten", "(", "name", "=", "\"flatten\"", ")", "(", "input_data", ")", "#(H, W, D, Batch)", "\n", "output", "=", "Dense", "(", "num_actions", ",", "name", "=", "\"output\"", ")", "(", "flatten_hidden", ")", "\n", "# Directly come here for DQN", "\n", "", "else", ":", "\n", "            ", "if", "not", "(", "args", ".", "recurrent", ")", ":", "# Only when \"not\" using DRQN", "\n", "                ", "h1", "=", "Convolution2D", "(", "32", ",", "(", "8", ",", "8", ")", ",", "strides", "=", "4", ",", "activation", "=", "\"relu\"", ",", "name", "=", "\"conv1\"", ")", "(", "input_data", ")", "\n", "h2", "=", "Convolution2D", "(", "64", ",", "(", "4", ",", "4", ")", ",", "strides", "=", "2", ",", "activation", "=", "\"relu\"", ",", "name", "=", "\"conv2\"", ")", "(", "h1", ")", "\n", "h3", "=", "Convolution2D", "(", "64", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "1", ",", "activation", "=", "\"relu\"", ",", "name", "=", "\"conv3\"", ")", "(", "h2", ")", "\n", "context", "=", "Flatten", "(", "name", "=", "\"flatten\"", ")", "(", "h3", ")", "\n", "# ENTER HERE FOR DRQN", "\n", "", "else", ":", "\n", "                ", "print", "(", "'>>>> Defining Recurrent Modules...'", ")", "\n", "input_data_expanded", "=", "Reshape", "(", "(", "input_shape", "[", "0", "]", ",", "input_shape", "[", "1", "]", ",", "input_shape", "[", "2", "]", ",", "1", ")", ",", "input_shape", "=", "input_shape", ")", "(", "input_data", ")", "\n", "input_data_TimeDistributed", "=", "Permute", "(", "(", "3", ",", "1", ",", "2", ",", "4", ")", ",", "input_shape", "=", "input_shape", ")", "(", "input_data_expanded", ")", "# (D, H, W, Batch)", "\n", "h1", "=", "TimeDistributed", "(", "Convolution2D", "(", "32", ",", "(", "8", ",", "8", ")", ",", "strides", "=", "4", ",", "activation", "=", "\"relu\"", ",", "name", "=", "\"conv1\"", ")", ",", "input_shape", "=", "(", "args", ".", "num_frames", ",", "input_shape", "[", "0", "]", ",", "input_shape", "[", "1", "]", ",", "1", ")", ")", "(", "input_data_TimeDistributed", ")", "\n", "h2", "=", "TimeDistributed", "(", "Convolution2D", "(", "64", ",", "(", "4", ",", "4", ")", ",", "strides", "=", "2", ",", "activation", "=", "\"relu\"", ",", "name", "=", "\"conv2\"", ")", ")", "(", "h1", ")", "\n", "h3", "=", "TimeDistributed", "(", "Convolution2D", "(", "64", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "1", ",", "activation", "=", "\"relu\"", ",", "name", "=", "\"conv3\"", ")", ")", "(", "h2", ")", "\n", "flatten_hidden", "=", "TimeDistributed", "(", "Flatten", "(", ")", ")", "(", "h3", ")", "\n", "hidden_input", "=", "TimeDistributed", "(", "Dense", "(", "512", ",", "activation", "=", "'relu'", ",", "name", "=", "'flat_to_512'", ")", ")", "(", "flatten_hidden", ")", "\n", "if", "not", "(", "args", ".", "a_t", ")", ":", "\n", "                    ", "context", "=", "LSTM", "(", "512", ",", "return_sequences", "=", "False", ",", "stateful", "=", "False", ",", "input_shape", "=", "(", "args", ".", "num_frames", ",", "512", ")", ")", "(", "hidden_input", ")", "\n", "", "else", ":", "\n", "                    ", "if", "args", ".", "bidir", ":", "\n", "                        ", "hidden_input", "=", "Bidirectional", "(", "LSTM", "(", "512", ",", "return_sequences", "=", "True", ",", "stateful", "=", "False", ",", "input_shape", "=", "(", "args", ".", "num_frames", ",", "512", ")", ")", ",", "merge_mode", "=", "'sum'", ")", "(", "hidden_input", ")", "\n", "all_outs", "=", "Bidirectional", "(", "LSTM", "(", "512", ",", "return_sequences", "=", "True", ",", "stateful", "=", "False", ",", "input_shape", "=", "(", "args", ".", "num_frames", ",", "512", ")", ")", ",", "merge_mode", "=", "'sum'", ")", "(", "hidden_input", ")", "\n", "", "else", ":", "\n", "                        ", "all_outs", "=", "LSTM", "(", "512", ",", "return_sequences", "=", "True", ",", "stateful", "=", "False", ",", "input_shape", "=", "(", "args", ".", "num_frames", ",", "512", ")", ")", "(", "hidden_input", ")", "\n", "# attention", "\n", "", "attention", "=", "TimeDistributed", "(", "Dense", "(", "1", ",", "activation", "=", "'tanh'", ")", ")", "(", "all_outs", ")", "\n", "# print(attention.shape)", "\n", "attention", "=", "Flatten", "(", ")", "(", "attention", ")", "\n", "attention", "=", "Activation", "(", "'softmax'", ")", "(", "attention", ")", "\n", "attention", "=", "RepeatVector", "(", "512", ")", "(", "attention", ")", "\n", "attention", "=", "Permute", "(", "[", "2", ",", "1", "]", ")", "(", "attention", ")", "\n", "sent_representation", "=", "merge", "(", "[", "all_outs", ",", "attention", "]", ",", "mode", "=", "'mul'", ")", "\n", "context", "=", "Lambda", "(", "lambda", "xin", ":", "K", ".", "sum", "(", "xin", ",", "axis", "=", "-", "2", ")", ",", "output_shape", "=", "(", "512", ",", ")", ")", "(", "sent_representation", ")", "\n", "# print(context.shape)", "\n", "\n", "", "", "if", "mode", "==", "\"dqn\"", ":", "\n", "                ", "h4", "=", "Dense", "(", "512", ",", "activation", "=", "'relu'", ",", "name", "=", "\"fc\"", ")", "(", "context", ")", "\n", "output", "=", "Dense", "(", "num_actions", ",", "name", "=", "\"output\"", ")", "(", "h4", ")", "\n", "", "elif", "mode", "==", "\"duel\"", ":", "\n", "                ", "value_hidden", "=", "Dense", "(", "512", ",", "activation", "=", "'relu'", ",", "name", "=", "'value_fc'", ")", "(", "context", ")", "\n", "value", "=", "Dense", "(", "1", ",", "name", "=", "\"value\"", ")", "(", "value_hidden", ")", "\n", "action_hidden", "=", "Dense", "(", "512", ",", "activation", "=", "'relu'", ",", "name", "=", "'action_fc'", ")", "(", "context", ")", "\n", "action", "=", "Dense", "(", "num_actions", ",", "name", "=", "\"action\"", ")", "(", "action_hidden", ")", "\n", "action_mean", "=", "Lambda", "(", "lambda", "x", ":", "tf", ".", "reduce_mean", "(", "x", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", ",", "name", "=", "'action_mean'", ")", "(", "action", ")", "\n", "output", "=", "Lambda", "(", "lambda", "x", ":", "x", "[", "0", "]", "+", "x", "[", "1", "]", "-", "x", "[", "2", "]", ",", "name", "=", "'output'", ")", "(", "[", "action", ",", "value", ",", "action_mean", "]", ")", "\n", "", "", "", "model", "=", "Model", "(", "inputs", "=", "input_data", ",", "outputs", "=", "output", ")", "\n", "print", "(", "model", ".", "summary", "(", ")", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_keras.save_scalar": [[107, 125], ["tensorflow.Summary", "tf.Summary.value.add", "float", "writer.add_summary"], "function", ["None"], ["", "def", "save_scalar", "(", "step", ",", "name", ",", "value", ",", "writer", ")", ":", "\n", "    ", "\"\"\"Save a scalar value to tensorboard.\n      Parameters\n      ----------\n      step: int\n        Training step (sets the position on x-axis of tensorboard graph.\n      name: str\n        Name of variable. Will be the name of the graph in tensorboard.\n      value: float\n        The value of the variable at this step.\n      writer: tf.FileWriter\n        The tensorboard FileWriter instance.\n      \"\"\"", "\n", "summary", "=", "tf", ".", "Summary", "(", ")", "\n", "summary_value", "=", "summary", ".", "value", ".", "add", "(", ")", "\n", "summary_value", ".", "simple_value", "=", "float", "(", "value", ")", "\n", "summary_value", ".", "tag", "=", "name", "\n", "writer", ".", "add_summary", "(", "summary", ",", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.HistoryPreprocessor.__init__": [[26, 30], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "history_length", "=", "1", ")", ":", "\n", "        ", "self", ".", "history_length", "=", "history_length", "\n", "self", ".", "past_states", "=", "None", "\n", "self", ".", "past_states_ori", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.HistoryPreprocessor.process_state_for_network": [[31, 39], ["numpy.dstack", "numpy.zeros"], "methods", ["None"], ["", "def", "process_state_for_network", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"You only want history when you're deciding the current action to take.\"\"\"", "\n", "row", ",", "col", "=", "state", ".", "shape", "\n", "if", "self", ".", "past_states", "is", "None", ":", "\n", "            ", "self", ".", "past_states", "=", "np", ".", "zeros", "(", "(", "row", ",", "col", ",", "self", ".", "history_length", ")", ")", "\n", "", "history", "=", "np", ".", "dstack", "(", "(", "self", ".", "past_states", ",", "state", ")", ")", "\n", "self", ".", "past_states", "=", "history", "[", ":", ",", ":", ",", "1", ":", "]", "\n", "return", "history", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.HistoryPreprocessor.process_state_for_network_ori": [[40, 49], ["numpy.concatenate", "numpy.zeros", "numpy.expand_dims"], "methods", ["None"], ["", "def", "process_state_for_network_ori", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"You only want history when you're deciding the current action to take.\"\"\"", "\n", "row", ",", "col", "=", "state", ".", "shape", "\n", "channel", "=", "1", "\n", "if", "self", ".", "past_states_ori", "is", "None", ":", "\n", "            ", "self", ".", "past_states_ori", "=", "np", ".", "zeros", "(", "(", "row", ",", "col", ",", "channel", ",", "self", ".", "history_length", ")", ")", "\n", "", "history", "=", "np", ".", "concatenate", "(", "(", "self", ".", "past_states_ori", ",", "np", ".", "expand_dims", "(", "state", ",", "-", "1", ")", ")", ",", "axis", "=", "3", ")", "\n", "self", ".", "past_states_ori", "=", "history", "[", ":", ",", ":", ",", ":", ",", "1", ":", "]", "\n", "return", "history", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.HistoryPreprocessor.reset": [[50, 57], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Reset the history sequence.\n\n        Useful when you start a new episode.\n        \"\"\"", "\n", "self", ".", "past_states", "=", "None", "\n", "self", ".", "past_states_ori", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.HistoryPreprocessor.get_config": [[58, 60], ["None"], "methods", ["None"], ["", "def", "get_config", "(", "self", ")", ":", "\n", "        ", "return", "{", "'history_length'", ":", "self", ".", "history_length", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_memory": [[97, 110], ["PIL.Image.fromarray().convert().resize", "numpy.array", "PIL.Image.fromarray().convert", "PIL.Image.fromarray"], "methods", ["None"], ["def", "process_state_for_memory", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Scale, convert to greyscale and store as uint8.\n\n        We don't want to save floating point numbers in the replay\n        memory. We get the same resolution as uint8, but use a quarter\n        to an eigth of the bytes (depending on float32 or float64)\n\n        We recommend using the Python Image Library (PIL) to do the\n        image conversions.\n        \"\"\"", "\n", "img", "=", "Image", ".", "fromarray", "(", "state", ")", ".", "convert", "(", "'L'", ")", ".", "resize", "(", "(", "84", ",", "84", ")", ",", "Image", ".", "BILINEAR", ")", "\n", "state", "=", "np", ".", "array", "(", "img", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network": [[111, 118], ["numpy.float32", "preprocessors.AtariPreprocessor.process_state_for_memory"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_memory"], ["", "def", "process_state_for_network", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Scale, convert to greyscale and store as float32.\n\n        Basically same as process state for memory, but this time\n        outputs float32 images.\n        \"\"\"", "\n", "return", "np", ".", "float32", "(", "self", ".", "process_state_for_memory", "(", "state", ")", "/", "255.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network_ori": [[119, 128], ["PIL.Image.fromarray", "numpy.float32", "numpy.array"], "methods", ["None"], ["", "def", "process_state_for_network_ori", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Scale, convert to greyscale and store as float32.\n\n        Basically same as process state for memory, but this time\n        outputs float32 images.\n        \"\"\"", "\n", "img", "=", "Image", ".", "fromarray", "(", "state", ")", "\n", "state", "=", "np", ".", "float32", "(", "np", ".", "array", "(", "img", ")", "/", "255.0", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_batch": [[129, 141], ["len", "range", "numpy.float32", "numpy.float32"], "methods", ["None"], ["", "def", "process_batch", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"The batches from replay memory will be uint8, convert to float32.\n\n        Same as process_state_for_network but works on a batch of\n        samples from the replay memory. Meaning you need to convert\n        both state and next state values.\n        \"\"\"", "\n", "batch_size", "=", "len", "(", "samples", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "samples", "[", "i", "]", ".", "state", "=", "np", ".", "float32", "(", "samples", "[", "i", "]", ".", "state", "/", "255.0", ")", "\n", "samples", "[", "i", "]", ".", "next_state", "=", "np", ".", "float32", "(", "samples", "[", "i", "]", ".", "next_state", "/", "255.0", ")", "\n", "", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_reward": [[142, 146], ["None"], "methods", ["None"], ["", "def", "process_reward", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"Clip reward between -1 and 1.\"\"\"", "\n", "# return np.clip(reward, -1, 1) ", "\n", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset": [[147, 149], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "last_state", "=", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.Qnetwork.__init__": [[19, 110], ["tensorflow.placeholder", "tensorflow.transpose", "tensorflow.reshape", "tensorflow.squeeze", "tensorflow.summary.merge", "tensorflow.contrib.layers.convolution2d", "tensorflow.contrib.layers.convolution2d", "tensorflow.contrib.layers.convolution2d", "tensorflow.contrib.layers.fully_connected", "tensorflow.placeholder", "tensorflow.reshape", "rnn_cell_1.zero_state", "tensorflow.argmax", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.one_hot", "tensorflow.reduce_sum", "tensorflow.square", "tensorflow.reduce_mean", "tensorflow.train.AdamOptimizer", "dqn_tf_temporalAt.Qnetwork.trainer.minimize", "tensorflow.gather", "tensorflow.contrib.layers.flatten", "rnn_cell_2.zero_state", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.concat", "tensorflow.nn.dynamic_rnn", "tensorflow.slice", "tensorflow.squeeze", "tensorflow.contrib.layers.fully_connected", "tensorflow.contrib.layers.fully_connected", "tensorflow.contrib.layers.fully_connected", "tensorflow.contrib.layers.fully_connected", "tensorflow.contrib.layers.fully_connected", "tensorflow.multiply", "tensorflow.reshape", "tensorflow.summary.image", "tensorflow.slice", "tensorflow.reshape", "tensorflow.tanh", "tensorflow.nn.softmax", "tensorflow.reduce_sum", "tensorflow.concat", "tensorflow.tanh", "tensorflow.nn.softmax", "tensorflow.reduce_sum", "tensorflow.subtract", "tensorflow.slice", "tensorflow.matmul", "tensorflow.multiply", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.map_fn", "tensorflow.multiply", "tensorflow.reduce_mean", "tensorflow.squeeze", "tensorflow.slice", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.matmul"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "h_size", ",", "num_frames", ",", "num_actions", ",", "rnn_cell_1", ",", "myScope", ",", "rnn_cell_2", "=", "None", ")", ":", "\n", "#The network recieves a frame from the game, flattened into an array.", "\n", "#It then resizes it and processes it through four convolutional layers.", "\n", "        ", "self", ".", "imageIn", "=", "tf", ".", "placeholder", "(", "shape", "=", "[", "None", ",", "84", ",", "84", ",", "num_frames", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "image_permute", "=", "tf", ".", "transpose", "(", "self", ".", "imageIn", ",", "perm", "=", "[", "0", ",", "3", ",", "1", ",", "2", "]", ")", "\n", "self", ".", "image_reshape", "=", "tf", ".", "reshape", "(", "self", ".", "image_permute", ",", "[", "-", "1", ",", "84", ",", "84", ",", "1", "]", ")", "\n", "self", ".", "image_reshape_recoverd", "=", "tf", ".", "squeeze", "(", "tf", ".", "gather", "(", "tf", ".", "reshape", "(", "self", ".", "image_reshape", ",", "[", "-", "1", ",", "num_frames", ",", "84", ",", "84", ",", "1", "]", ")", ",", "[", "0", "]", ")", ",", "[", "0", "]", ")", "\n", "self", ".", "summary_merged", "=", "tf", ".", "summary", ".", "merge", "(", "[", "tf", ".", "summary", ".", "image", "(", "'image_reshape_recoverd'", ",", "self", ".", "image_reshape_recoverd", ",", "max_outputs", "=", "num_frames", ")", "]", ")", "\n", "# self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,1])", "\n", "self", ".", "conv1", "=", "tf", ".", "contrib", ".", "layers", ".", "convolution2d", "(", "inputs", "=", "self", ".", "image_reshape", ",", "num_outputs", "=", "32", ",", "kernel_size", "=", "[", "8", ",", "8", "]", ",", "stride", "=", "[", "4", ",", "4", "]", ",", "padding", "=", "'VALID'", ",", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ",", "biases_initializer", "=", "None", ",", "scope", "=", "myScope", "+", "'_conv1'", ")", "\n", "self", ".", "conv2", "=", "tf", ".", "contrib", ".", "layers", ".", "convolution2d", "(", "inputs", "=", "self", ".", "conv1", ",", "num_outputs", "=", "64", ",", "kernel_size", "=", "[", "4", ",", "4", "]", ",", "stride", "=", "[", "2", ",", "2", "]", ",", "padding", "=", "'VALID'", ",", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ",", "biases_initializer", "=", "None", ",", "scope", "=", "myScope", "+", "'_conv2'", ")", "\n", "self", ".", "conv3", "=", "tf", ".", "contrib", ".", "layers", ".", "convolution2d", "(", "inputs", "=", "self", ".", "conv2", ",", "num_outputs", "=", "64", ",", "kernel_size", "=", "[", "3", ",", "3", "]", ",", "stride", "=", "[", "1", ",", "1", "]", ",", "padding", "=", "'VALID'", ",", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ",", "biases_initializer", "=", "None", ",", "scope", "=", "myScope", "+", "'_conv3'", ")", "\n", "self", ".", "conv4", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "tf", ".", "contrib", ".", "layers", ".", "flatten", "(", "self", ".", "conv3", ")", ",", "h_size", ",", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "\n", "#We take the output from the final convolutional layer and send it to a recurrent layer.", "\n", "#The input must be reshaped into [batch x trace x units] for rnn processing, ", "\n", "#and then returned to [batch x units] when sent through the upper levels.", "\n", "self", ".", "batch_size", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "int32", ")", "\n", "self", ".", "convFlat", "=", "tf", ".", "reshape", "(", "self", ".", "conv4", ",", "[", "self", ".", "batch_size", ",", "num_frames", ",", "h_size", "]", ")", "\n", "self", ".", "state_in_1", "=", "rnn_cell_1", ".", "zero_state", "(", "self", ".", "batch_size", ",", "tf", ".", "float32", ")", "\n", "\n", "if", "args", ".", "bidir", ":", "\n", "            ", "self", ".", "state_in_2", "=", "rnn_cell_2", ".", "zero_state", "(", "self", ".", "batch_size", ",", "tf", ".", "float32", ")", "\n", "self", ".", "rnn_outputs_tuple", ",", "self", ".", "rnn_state", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "cell_fw", "=", "rnn_cell_1", ",", "cell_bw", "=", "rnn_cell_2", ",", "inputs", "=", "self", ".", "convFlat", ",", "dtype", "=", "tf", ".", "float32", ",", "initial_state_fw", "=", "self", ".", "state_in_1", ",", "initial_state_bw", "=", "self", ".", "state_in_2", ",", "scope", "=", "myScope", "+", "'_rnn'", ")", "\n", "# print \"====== len(self.rnn_outputs_tuple), self.rnn_outputs_tuple[0] \", len(self.rnn_outputs_tuple), self.rnn_outputs_tuple[0].get_shape().as_list(), self.rnn_outputs_tuple[1].get_shape().as_list() # [None, 10, 512]", "\n", "# As we have Bi-LSTM, we have two output, which are not connected. So merge them", "\n", "self", ".", "rnn_outputs", "=", "tf", ".", "concat", "(", "[", "self", ".", "rnn_outputs_tuple", "[", "0", "]", ",", "self", ".", "rnn_outputs_tuple", "[", "1", "]", "]", ",", "axis", "=", "2", ")", "\n", "# self.rnn_outputs = tf.contrib.layers.fully_connected(tf.contrib.layers.flatten(self.rnn_outputs_double), h_size, activation_fn=None)", "\n", "self", ".", "rnn_output_dim", "=", "h_size", "*", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "rnn_outputs", ",", "self", ".", "rnn_state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "inputs", "=", "self", ".", "convFlat", ",", "cell", "=", "rnn_cell_1", ",", "dtype", "=", "tf", ".", "float32", ",", "initial_state", "=", "self", ".", "state_in_1", ",", "scope", "=", "myScope", "+", "'_rnn'", ")", "\n", "# print \"====== self.rnn_outputs \", self.rnn_outputs.get_shape().as_list() # [None, 10, 512]", "\n", "self", ".", "rnn_output_dim", "=", "h_size", "\n", "\n", "# attention machanism", "\n", "", "if", "not", "(", "args", ".", "a_t", ")", ":", "\n", "            ", "self", ".", "rnn_last_output", "=", "tf", ".", "slice", "(", "self", ".", "rnn_outputs", ",", "[", "0", ",", "num_frames", "-", "1", ",", "0", "]", ",", "[", "-", "1", ",", "1", ",", "-", "1", "]", ")", "\n", "self", ".", "rnn", "=", "tf", ".", "squeeze", "(", "self", ".", "rnn_last_output", ",", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "if", "args", ".", "global_a_t", ":", "\n", "                ", "self", ".", "rnn_outputs_before", "=", "tf", ".", "slice", "(", "self", ".", "rnn_outputs", ",", "[", "0", ",", "0", ",", "0", "]", ",", "[", "-", "1", ",", "num_frames", "-", "1", ",", "-", "1", "]", ")", "\n", "self", ".", "attention_v", "=", "tf", ".", "reshape", "(", "tf", ".", "slice", "(", "self", ".", "rnn_outputs", ",", "[", "0", ",", "num_frames", "-", "1", ",", "0", "]", ",", "[", "-", "1", ",", "1", ",", "-", "1", "]", ")", ",", "[", "-", "1", ",", "self", ".", "rnn_output_dim", ",", "1", "]", ")", "\n", "self", ".", "attention_va", "=", "tf", ".", "tanh", "(", "tf", ".", "matmul", "(", "self", ".", "rnn_outputs_before", ",", "self", ".", "attention_v", ")", ")", "\n", "self", ".", "attention_a", "=", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "attention_va", ",", "dim", "=", "1", ")", "\n", "self", ".", "rnn", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "self", ".", "rnn_outputs_before", ",", "self", ".", "attention_a", ")", ",", "axis", "=", "1", ")", "\n", "self", ".", "rnn", "=", "tf", ".", "concat", "(", "[", "self", ".", "rnn", ",", "tf", ".", "squeeze", "(", "tf", ".", "slice", "(", "self", ".", "rnn_outputs", ",", "[", "0", ",", "num_frames", "-", "1", ",", "0", "]", ",", "[", "-", "1", ",", "1", ",", "-", "1", "]", ")", ",", "[", "1", "]", ")", "]", ",", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "myScope", "+", "'_attention'", ")", ":", "\n", "                    ", "self", ".", "attention_v", "=", "tf", ".", "get_variable", "(", "name", "=", "'atten_v'", ",", "shape", "=", "[", "self", ".", "rnn_output_dim", ",", "1", "]", ",", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "", "self", ".", "attention_va", "=", "tf", ".", "tanh", "(", "tf", ".", "map_fn", "(", "lambda", "x", ":", "tf", ".", "matmul", "(", "x", ",", "self", ".", "attention_v", ")", ",", "self", ".", "rnn_outputs", ")", ")", "\n", "self", ".", "attention_a", "=", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "attention_va", ",", "dim", "=", "1", ")", "\n", "self", ".", "rnn", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "self", ".", "rnn_outputs", ",", "self", ".", "attention_a", ")", ",", "axis", "=", "1", ")", "\n", "# print \"========== self.rnn \", self.rnn.get_shape().as_list() #[None, 1024]", "\n", "\n", "", "", "if", "args", ".", "net_mode", "==", "\"duel\"", ":", "\n", "#The output from the recurrent player is then split into separate Value and Advantage streams", "\n", "            ", "self", ".", "ad_hidden", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "self", ".", "rnn", ",", "h_size", ",", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ",", "scope", "=", "myScope", "+", "'_fc_advantage_hidden'", ")", "\n", "self", ".", "Advantage", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "self", ".", "ad_hidden", ",", "num_actions", ",", "activation_fn", "=", "None", ",", "scope", "=", "myScope", "+", "'_fc_advantage'", ")", "\n", "self", ".", "value_hidden", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "self", ".", "rnn", ",", "h_size", ",", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ",", "scope", "=", "myScope", "+", "'_fc_value_hidden'", ")", "\n", "self", ".", "Value", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "self", ".", "value_hidden", ",", "1", ",", "activation_fn", "=", "None", ",", "scope", "=", "myScope", "+", "'_fc_value'", ")", "\n", "#Then combine them together to get our final Q-values.", "\n", "self", ".", "Qout", "=", "self", ".", "Value", "+", "tf", ".", "subtract", "(", "self", ".", "Advantage", ",", "tf", ".", "reduce_mean", "(", "self", ".", "Advantage", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "Qout", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "self", ".", "rnn", ",", "num_actions", ",", "activation_fn", "=", "None", ")", "\n", "\n", "", "self", ".", "predict", "=", "tf", ".", "argmax", "(", "self", ".", "Qout", ",", "1", ")", "\n", "\n", "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.", "\n", "self", ".", "targetQ", "=", "tf", ".", "placeholder", "(", "shape", "=", "[", "None", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "actions", "=", "tf", ".", "placeholder", "(", "shape", "=", "[", "None", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "self", ".", "actions_onehot", "=", "tf", ".", "one_hot", "(", "self", ".", "actions", ",", "num_actions", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "self", ".", "Q", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "self", ".", "Qout", ",", "self", ".", "actions_onehot", ")", ",", "axis", "=", "1", ")", "\n", "self", ".", "td_error", "=", "tf", ".", "square", "(", "self", ".", "targetQ", "-", "self", ".", "Q", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "td_error", ")", "\n", "\n", "self", ".", "trainer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "0.0001", ")", "\n", "self", ".", "updateModel", "=", "self", ".", "trainer", ".", "minimize", "(", "self", ".", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.__init__": [[168, 231], ["HistoryPreprocessor", "AtariPreprocessor", "ReplayMemory", "LinearDecayGreedyEpsilonPolicy", "tensorflow.reset_default_graph", "tensorflow.contrib.rnn.BasicLSTMCell", "tensorflow.contrib.rnn.BasicLSTMCell", "print", "print", "tensorflow.global_variables_initializer", "tensorflow.train.Saver", "tensorflow.trainable_variables", "print", "updateTargetGraph", "tensorflow.ConfigProto", "tensorflow.Session", "dqn_tf_temporalAt.DQNAgent.sess.run", "updateTarget", "tensorflow.summary.FileWriter", "tensorflow.contrib.rnn.BasicLSTMCell", "tensorflow.contrib.rnn.BasicLSTMCell", "dqn_tf_temporalAt.Qnetwork", "dqn_tf_temporalAt.Qnetwork", "dqn_tf_temporalAt.Qnetwork", "dqn_tf_temporalAt.Qnetwork", "len"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.updateTargetGraph", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.updateTarget"], ["def", "__init__", "(", "self", ",", "args", ",", "num_actions", ")", ":", "\n", "        ", "self", ".", "num_actions", "=", "num_actions", "\n", "input_shape", "=", "(", "args", ".", "frame_height", ",", "args", ".", "frame_width", ",", "args", ".", "num_frames", ")", "\n", "self", ".", "history_processor", "=", "HistoryPreprocessor", "(", "args", ".", "num_frames", "-", "1", ")", "\n", "self", ".", "atari_processor", "=", "AtariPreprocessor", "(", ")", "\n", "self", ".", "memory", "=", "ReplayMemory", "(", "args", ")", "\n", "self", ".", "policy", "=", "LinearDecayGreedyEpsilonPolicy", "(", "args", ".", "initial_epsilon", ",", "args", ".", "final_epsilon", ",", "args", ".", "exploration_steps", ")", "\n", "self", ".", "gamma", "=", "args", ".", "gamma", "\n", "self", ".", "target_update_freq", "=", "args", ".", "target_update_freq", "\n", "self", ".", "num_burn_in", "=", "args", ".", "num_burn_in", "\n", "self", ".", "train_freq", "=", "args", ".", "train_freq", "\n", "self", ".", "batch_size", "=", "args", ".", "batch_size", "\n", "self", ".", "learning_rate", "=", "args", ".", "learning_rate", "\n", "self", ".", "frame_width", "=", "args", ".", "frame_width", "\n", "self", ".", "frame_height", "=", "args", ".", "frame_height", "\n", "self", ".", "num_frames", "=", "args", ".", "num_frames", "\n", "self", ".", "output_path", "=", "args", ".", "output", "\n", "self", ".", "output_path_videos", "=", "args", ".", "output", "+", "'/videos/'", "\n", "self", ".", "output_path_images", "=", "args", ".", "output", "+", "'/images/'", "\n", "self", ".", "save_freq", "=", "args", ".", "save_freq", "\n", "self", ".", "load_network", "=", "args", ".", "load_network", "\n", "self", ".", "load_network_path", "=", "args", ".", "load_network_path", "\n", "self", ".", "enable_ddqn", "=", "args", ".", "ddqn", "\n", "self", ".", "net_mode", "=", "args", ".", "net_mode", "\n", "self", ".", "args", "=", "args", "\n", "\n", "self", ".", "h_size", "=", "512", "\n", "self", ".", "tau", "=", "0.001", "\n", "# self.q_network = create_model(input_shape, num_actions, self.net_mode, args, \"QNet\")", "\n", "# self.target_network = create_model(input_shape, num_actions, self.net_mode, args, \"TargetNet\")", "\n", "tf", ".", "reset_default_graph", "(", ")", "\n", "#We define the cells for the primary and target q-networks", "\n", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "h_size", ",", "state_is_tuple", "=", "True", ")", "\n", "cellT", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "h_size", ",", "state_is_tuple", "=", "True", ")", "\n", "if", "args", ".", "bidir", ":", "\n", "            ", "cell_2", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "h_size", ",", "state_is_tuple", "=", "True", ")", "\n", "cellT_2", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "h_size", ",", "state_is_tuple", "=", "True", ")", "\n", "self", ".", "q_network", "=", "Qnetwork", "(", "args", ",", "h_size", "=", "self", ".", "h_size", ",", "num_frames", "=", "self", ".", "num_frames", ",", "num_actions", "=", "self", ".", "num_actions", ",", "rnn_cell_1", "=", "cell", ",", "rnn_cell_2", "=", "cell_2", ",", "myScope", "=", "\"QNet\"", ")", "\n", "self", ".", "target_network", "=", "Qnetwork", "(", "args", ",", "h_size", "=", "self", ".", "h_size", ",", "num_frames", "=", "self", ".", "num_frames", ",", "num_actions", "=", "self", ".", "num_actions", ",", "rnn_cell_1", "=", "cellT", ",", "rnn_cell_2", "=", "cellT_2", ",", "myScope", "=", "\"TargetNet\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "q_network", "=", "Qnetwork", "(", "args", ",", "h_size", "=", "self", ".", "h_size", ",", "num_frames", "=", "self", ".", "num_frames", ",", "num_actions", "=", "self", ".", "num_actions", ",", "rnn_cell_1", "=", "cell", ",", "myScope", "=", "\"QNet\"", ")", "\n", "self", ".", "target_network", "=", "Qnetwork", "(", "args", ",", "h_size", "=", "self", ".", "h_size", ",", "num_frames", "=", "self", ".", "num_frames", ",", "num_actions", "=", "self", ".", "num_actions", ",", "rnn_cell_1", "=", "cellT", ",", "myScope", "=", "\"TargetNet\"", ")", "\n", "\n", "", "print", "(", "\">>>> Net mode: %s, Using double dqn: %s\"", "%", "(", "self", ".", "net_mode", ",", "self", ".", "enable_ddqn", ")", ")", "\n", "self", ".", "eval_freq", "=", "args", ".", "eval_freq", "\n", "self", ".", "no_experience", "=", "args", ".", "no_experience", "\n", "self", ".", "no_target", "=", "args", ".", "no_target", "\n", "print", "(", "\">>>> Target fixing: %s, Experience replay: %s\"", "%", "(", "not", "self", ".", "no_target", ",", "not", "self", ".", "no_experience", ")", ")", "\n", "\n", "# initialize target network", "\n", "init", "=", "tf", ".", "global_variables_initializer", "(", ")", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "2", ")", "\n", "trainables", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "print", "(", "trainables", ",", "len", "(", "trainables", ")", ")", "\n", "self", ".", "targetOps", "=", "updateTargetGraph", "(", "trainables", ",", "self", ".", "tau", ")", "\n", "\n", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "config", ".", "allow_soft_placement", "=", "True", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", "config", "=", "config", ")", "\n", "self", ".", "sess", ".", "run", "(", "init", ")", "\n", "updateTarget", "(", "self", ".", "targetOps", ",", "self", ".", "sess", ")", "\n", "self", ".", "writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "self", ".", "output_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.calc_q_values": [[232, 251], ["dqn_tf_temporalAt.DQNAgent.sess.run"], "methods", ["None"], ["", "def", "calc_q_values", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Given a state (or batch of states) calculate the Q-values.\n\n        Basically run your network on these states.\n\n        Return\n        ------\n        Q-values for the state(s)\n        \"\"\"", "\n", "state", "=", "state", "[", "None", ",", ":", ",", ":", ",", ":", "]", "\n", "# return self.q_network.predict_on_batch(state)", "\n", "# print state.shape", "\n", "# Qout = self.sess.run(self.q_network.rnn_outputs,\\", "\n", "#             feed_dict={self.q_network.imageIn: state, self.q_network.batch_size:1})", "\n", "# print Qout.shape", "\n", "Qout", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "q_network", ".", "Qout", ",", "feed_dict", "=", "{", "self", ".", "q_network", ".", "imageIn", ":", "state", ",", "self", ".", "q_network", ".", "batch_size", ":", "1", "}", ")", "\n", "# print Qout.shape", "\n", "return", "Qout", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action": [[252, 283], ["dqn_tf_temporalAt.DQNAgent.calc_q_values", "GreedyPolicy().select_action", "UniformRandomPolicy().select_action", "dqn_tf_temporalAt.DQNAgent.policy.select_action", "GreedyPolicy", "UniformRandomPolicy"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.calc_q_values", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action"], ["", "def", "select_action", "(", "self", ",", "state", ",", "is_training", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Select the action based on the current state.\n\n        You will probably want to vary your behavior here based on\n        which stage of training your in. For example, if you're still\n        collecting random samples you might want to use a\n        UniformRandomPolicy.\n\n        If you're testing, you might want to use a GreedyEpsilonPolicy\n        with a low epsilon.\n\n        If you're training, you might want to use the\n        LinearDecayGreedyEpsilonPolicy.\n\n        This would also be a good place to call\n        process_state_for_network in your preprocessor.\n\n        Returns\n        --------\n        selected action\n        \"\"\"", "\n", "q_values", "=", "self", ".", "calc_q_values", "(", "state", ")", "\n", "if", "is_training", ":", "\n", "            ", "if", "kwargs", "[", "'policy_type'", "]", "==", "'UniformRandomPolicy'", ":", "\n", "                ", "return", "UniformRandomPolicy", "(", "self", ".", "num_actions", ")", ".", "select_action", "(", ")", "\n", "", "else", ":", "\n", "# linear decay greedy epsilon policy", "\n", "                ", "return", "self", ".", "policy", ".", "select_action", "(", "q_values", ",", "is_training", ")", "\n", "", "", "else", ":", "\n", "# return GreedyEpsilonPolicy(0.05).select_action(q_values)", "\n", "            ", "return", "GreedyPolicy", "(", ")", ".", "select_action", "(", "q_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.update_policy": [[284, 351], ["numpy.stack", "numpy.stack", "numpy.asarray", "numpy.asarray", "numpy.zeros", "dqn_tf_temporalAt.DQNAgent.memory.sample", "dqn_tf_temporalAt.DQNAgent.atari_processor.process_batch", "numpy.stack", "numpy.asarray", "numpy.stack", "numpy.asarray", "numpy.asarray", "dqn_tf_temporalAt.DQNAgent.q_network.predict_on_batch", "dqn_tf_temporalAt.DQNAgent.sess.run", "dqn_tf_temporalAt.DQNAgent.sess.run", "numpy.argmax", "numpy.max", "dqn_tf_temporalAt.DQNAgent.sess.run", "dqn_tf_temporalAt.DQNAgent.sess.run", "numpy.mean", "numpy.random.random", "int", "int", "range"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.sample", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_batch"], ["", "", "def", "update_policy", "(", "self", ",", "current_sample", ")", ":", "\n", "        ", "\"\"\"Update your policy.\n\n        Behavior may differ based on what stage of training your\n        in. If you're in training mode then you should check if you\n        should update your network parameters based on the current\n        step and the value you set for train_freq.\n\n        Inside, you'll want to sample a minibatch, calculate the\n        target values, update your network, and then update your\n        target values.\n\n        You might want to return the loss and other metrics as an\n        output. They can help you monitor how training is going.\n        \"\"\"", "\n", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "if", "self", ".", "no_experience", ":", "\n", "            ", "states", "=", "np", ".", "stack", "(", "[", "current_sample", ".", "state", "]", ")", "\n", "next_states", "=", "np", ".", "stack", "(", "[", "current_sample", ".", "next_state", "]", ")", "\n", "rewards", "=", "np", ".", "asarray", "(", "[", "current_sample", ".", "reward", "]", ")", "\n", "mask", "=", "np", ".", "asarray", "(", "[", "1", "-", "int", "(", "current_sample", ".", "is_terminal", ")", "]", ")", "\n", "\n", "action_mask", "=", "np", ".", "zeros", "(", "(", "1", ",", "self", ".", "num_actions", ")", ")", "\n", "action_mask", "[", "0", ",", "current_sample", ".", "action", "]", "=", "1.0", "\n", "", "else", ":", "\n", "            ", "samples", "=", "self", ".", "memory", ".", "sample", "(", "batch_size", ")", "\n", "samples", "=", "self", ".", "atari_processor", ".", "process_batch", "(", "samples", ")", "\n", "\n", "states", "=", "np", ".", "stack", "(", "[", "x", ".", "state", "for", "x", "in", "samples", "]", ")", "\n", "actions", "=", "np", ".", "asarray", "(", "[", "x", ".", "action", "for", "x", "in", "samples", "]", ")", "\n", "# action_mask = np.zeros((batch_size, self.num_actions))", "\n", "# action_mask[range(batch_size), actions] = 1.0", "\n", "\n", "next_states", "=", "np", ".", "stack", "(", "[", "x", ".", "next_state", "for", "x", "in", "samples", "]", ")", "\n", "mask", "=", "np", ".", "asarray", "(", "[", "1", "-", "int", "(", "x", ".", "is_terminal", ")", "for", "x", "in", "samples", "]", ")", "\n", "rewards", "=", "np", ".", "asarray", "(", "[", "x", ".", "reward", "for", "x", "in", "samples", "]", ")", "\n", "\n", "", "if", "self", ".", "no_target", ":", "\n", "            ", "next_qa_value", "=", "self", ".", "q_network", ".", "predict_on_batch", "(", "next_states", ")", "\n", "", "else", ":", "\n", "# next_qa_value = self.target_network.predict_on_batch(next_states)", "\n", "            ", "next_qa_value", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "target_network", ".", "Qout", ",", "feed_dict", "=", "{", "self", ".", "target_network", ".", "imageIn", ":", "next_states", ",", "self", ".", "target_network", ".", "batch_size", ":", "batch_size", "}", ")", "\n", "\n", "", "if", "self", ".", "enable_ddqn", ":", "\n", "# qa_value = self.q_network.predict_on_batch(next_states)", "\n", "            ", "qa_value", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "q_network", ".", "Qout", ",", "feed_dict", "=", "{", "self", ".", "q_network", ".", "imageIn", ":", "next_states", ",", "self", ".", "q_network", ".", "batch_size", ":", "batch_size", "}", ")", "\n", "max_actions", "=", "np", ".", "argmax", "(", "qa_value", ",", "axis", "=", "1", ")", "\n", "next_qa_value", "=", "next_qa_value", "[", "range", "(", "batch_size", ")", ",", "max_actions", "]", "\n", "", "else", ":", "\n", "            ", "next_qa_value", "=", "np", ".", "max", "(", "next_qa_value", ",", "axis", "=", "1", ")", "\n", "# print rewards.shape, mask.shape, next_qa_value.shape, batch_size", "\n", "", "target", "=", "rewards", "+", "self", ".", "gamma", "*", "mask", "*", "next_qa_value", "\n", "\n", "if", "self", ".", "args", ".", "a_t", "and", "np", ".", "random", ".", "random", "(", ")", "<", "1e-3", ":", "\n", "            ", "loss", ",", "_", ",", "rnn", ",", "attention_v", ",", "attention_a", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "q_network", ".", "loss", ",", "self", ".", "q_network", ".", "updateModel", ",", "self", ".", "q_network", ".", "rnn", ",", "self", ".", "q_network", ".", "attention_v", ",", "self", ".", "q_network", ".", "attention_a", "]", ",", "feed_dict", "=", "{", "self", ".", "q_network", ".", "imageIn", ":", "states", ",", "self", ".", "q_network", ".", "batch_size", ":", "batch_size", ",", "self", ".", "q_network", ".", "actions", ":", "actions", ",", "self", ".", "q_network", ".", "targetQ", ":", "target", "}", ")", "\n", "# print(attention_a[0])", "\n", "", "else", ":", "\n", "            ", "loss", ",", "_", ",", "rnn", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "q_network", ".", "loss", ",", "self", ".", "q_network", ".", "updateModel", ",", "self", ".", "q_network", ".", "rnn", "]", ",", "feed_dict", "=", "{", "self", ".", "q_network", ".", "imageIn", ":", "states", ",", "self", ".", "q_network", ".", "batch_size", ":", "batch_size", ",", "self", ".", "q_network", ".", "actions", ":", "actions", ",", "self", ".", "q_network", ".", "targetQ", ":", "target", "}", ")", "\n", "\n", "", "return", "loss", ",", "np", ".", "mean", "(", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.fit": [[352, 462], ["print", "dqn_tf_temporalAt.DQNAgent.save_model", "env.reset", "range", "dqn_tf_temporalAt.DQNAgent.save_model", "dqn_tf_temporalAt.DQNAgent.history_processor.process_state_for_network", "dqn_tf_temporalAt.DQNAgent.select_action", "dqn_tf_temporalAt.DQNAgent.atari_processor.process_state_for_memory", "env.step", "dqn_tf_temporalAt.DQNAgent.atari_processor.process_state_for_network", "numpy.dstack", "dqn_tf_temporalAt.DQNAgent.atari_processor.process_reward", "dqn_tf_temporalAt.DQNAgent.memory.append", "Sample", "dqn_tf_temporalAt.DQNAgent.atari_processor.process_state_for_network", "dqn_tf_temporalAt.DQNAgent.atari_processor.process_state_for_memory", "dqn_tf_temporalAt.DQNAgent.memory.append", "env.reset", "dqn_tf_temporalAt.DQNAgent.atari_processor.reset", "dqn_tf_temporalAt.DQNAgent.history_processor.reset", "print", "sys.stdout.flush", "dqn_tf_temporalAt.save_scalar", "dqn_tf_temporalAt.save_scalar", "dqn_tf_temporalAt.save_scalar", "dqn_tf_temporalAt.save_scalar", "dqn_tf_temporalAt.save_scalar", "dqn_tf_temporalAt.save_scalar", "dqn_tf_temporalAt.save_scalar", "dqn_tf_temporalAt.DQNAgent.update_policy", "updateTarget", "print", "dqn_tf_temporalAt.DQNAgent.save_model", "dqn_tf_temporalAt.DQNAgent.evaluate", "dqn_tf_temporalAt.save_scalar", "dqn_tf_temporalAt.save_scalar"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.save_model", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.save_model", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_memory", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_reward", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_memory", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.core.ReplayMemory.append", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.update_policy", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.None.helper.updateTarget", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.save_model", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.evaluate", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar"], ["", "def", "fit", "(", "self", ",", "env", ",", "num_iterations", ",", "max_episode_length", "=", "None", ")", ":", "\n", "        ", "\"\"\"Fit your model to the provided environment.\n\n        Its a good idea to print out things like loss, average reward,\n        Q-values, etc to see if your agent is actually improving.\n\n        You should probably also periodically save your network\n        weights and any other useful info.\n\n        This is where you should sample actions from your network,\n        collect experience samples and add them to your replay memory,\n        and update your network parameters.\n\n        Parameters\n        ----------\n        env: gym.Env\n          This is your Atari environment. You should wrap the\n          environment using the wrap_atari_env function in the\n          utils.py\n        num_iterations: int\n          How many samples/updates to perform.\n        max_episode_length: int\n          How long a single episode should last before the agent\n          resets. Can help exploration.\n        \"\"\"", "\n", "is_training", "=", "True", "\n", "print", "(", "\"Training starts.\"", ")", "\n", "self", ".", "save_model", "(", "0", ")", "\n", "eval_count", "=", "0", "\n", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "burn_in", "=", "True", "\n", "idx_episode", "=", "1", "\n", "episode_loss", "=", ".0", "\n", "episode_frames", "=", "0", "\n", "episode_reward", "=", ".0", "\n", "episode_raw_reward", "=", ".0", "\n", "episode_target_value", "=", ".0", "\n", "for", "t", "in", "range", "(", "self", ".", "num_burn_in", "+", "num_iterations", ")", ":", "\n", "            ", "action_state", "=", "self", ".", "history_processor", ".", "process_state_for_network", "(", "\n", "self", ".", "atari_processor", ".", "process_state_for_network", "(", "state", ")", ")", "\n", "policy_type", "=", "\"UniformRandomPolicy\"", "if", "burn_in", "else", "\"LinearDecayGreedyEpsilonPolicy\"", "\n", "action", "=", "self", ".", "select_action", "(", "action_state", ",", "is_training", ",", "policy_type", "=", "policy_type", ")", "\n", "processed_state", "=", "self", ".", "atari_processor", ".", "process_state_for_memory", "(", "state", ")", "\n", "\n", "state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "processed_next_state", "=", "self", ".", "atari_processor", ".", "process_state_for_network", "(", "state", ")", "\n", "action_next_state", "=", "np", ".", "dstack", "(", "(", "action_state", ",", "processed_next_state", ")", ")", "\n", "action_next_state", "=", "action_next_state", "[", ":", ",", ":", ",", "1", ":", "]", "\n", "\n", "processed_reward", "=", "self", ".", "atari_processor", ".", "process_reward", "(", "reward", ")", "\n", "\n", "self", ".", "memory", ".", "append", "(", "processed_state", ",", "action", ",", "processed_reward", ",", "done", ")", "\n", "current_sample", "=", "Sample", "(", "action_state", ",", "action", ",", "processed_reward", ",", "action_next_state", ",", "done", ")", "\n", "\n", "if", "not", "burn_in", ":", "\n", "                ", "episode_frames", "+=", "1", "\n", "episode_reward", "+=", "processed_reward", "\n", "episode_raw_reward", "+=", "reward", "\n", "if", "episode_frames", ">", "max_episode_length", ":", "\n", "                    ", "done", "=", "True", "\n", "\n", "", "", "if", "done", ":", "\n", "# adding last frame only to save last state", "\n", "                ", "last_frame", "=", "self", ".", "atari_processor", ".", "process_state_for_memory", "(", "state", ")", "\n", "# action, reward, done doesn't matter here", "\n", "self", ".", "memory", ".", "append", "(", "last_frame", ",", "action", ",", "0", ",", "done", ")", "\n", "if", "not", "burn_in", ":", "\n", "                    ", "avg_target_value", "=", "episode_target_value", "/", "episode_frames", "\n", "print", "(", "\">>> Training: time %d, episode %d, length %d, reward %.0f, raw_reward %.0f, loss %.4f, target value %.4f, policy step %d, memory cap %d\"", "%", "\n", "(", "t", ",", "idx_episode", ",", "episode_frames", ",", "episode_reward", ",", "episode_raw_reward", ",", "episode_loss", ",", "\n", "avg_target_value", ",", "self", ".", "policy", ".", "step", ",", "self", ".", "memory", ".", "current", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train/episode_frames'", ",", "episode_frames", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train/episode_reward'", ",", "episode_reward", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train/episode_raw_reward'", ",", "episode_raw_reward", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train/episode_loss'", ",", "episode_loss", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train_avg/avg_reward'", ",", "episode_reward", "/", "episode_frames", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train_avg/avg_target_value'", ",", "avg_target_value", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "idx_episode", ",", "'train_avg/avg_loss'", ",", "episode_loss", "/", "episode_frames", ",", "self", ".", "writer", ")", "\n", "episode_frames", "=", "0", "\n", "episode_reward", "=", ".0", "\n", "episode_raw_reward", "=", ".0", "\n", "episode_loss", "=", ".0", "\n", "episode_target_value", "=", ".0", "\n", "idx_episode", "+=", "1", "\n", "", "burn_in", "=", "(", "t", "<", "self", ".", "num_burn_in", ")", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "self", ".", "atari_processor", ".", "reset", "(", ")", "\n", "self", ".", "history_processor", ".", "reset", "(", ")", "\n", "\n", "", "if", "not", "burn_in", ":", "\n", "                ", "if", "t", "%", "self", ".", "train_freq", "==", "0", ":", "\n", "                    ", "loss", ",", "target_value", "=", "self", ".", "update_policy", "(", "current_sample", ")", "\n", "episode_loss", "+=", "loss", "\n", "episode_target_value", "+=", "target_value", "\n", "# update freq is based on train_freq", "\n", "", "if", "t", "%", "(", "self", ".", "train_freq", "*", "self", ".", "target_update_freq", ")", "==", "0", ":", "\n", "# self.target_network.set_weights(self.q_network.get_weights())", "\n", "                    ", "updateTarget", "(", "self", ".", "targetOps", ",", "self", ".", "sess", ")", "\n", "print", "(", "\"----- Synced.\"", ")", "\n", "", "if", "t", "%", "self", ".", "save_freq", "==", "0", ":", "\n", "                    ", "self", ".", "save_model", "(", "idx_episode", ")", "\n", "", "if", "t", "%", "(", "self", ".", "eval_freq", "*", "self", ".", "train_freq", ")", "==", "0", ":", "\n", "                    ", "episode_reward_mean", ",", "episode_reward_std", ",", "eval_count", "=", "self", ".", "evaluate", "(", "env", ",", "20", ",", "eval_count", ",", "max_episode_length", ",", "True", ")", "\n", "save_scalar", "(", "t", ",", "'eval/eval_episode_reward_mean'", ",", "episode_reward_mean", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "t", ",", "'eval/eval_episode_reward_std'", ",", "episode_reward_std", ",", "self", ".", "writer", ")", "\n", "\n", "", "", "", "self", ".", "save_model", "(", "idx_episode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.save_model": [[464, 469], ["dqn_tf_temporalAt.DQNAgent.saver.save", "print", "str"], "methods", ["None"], ["", "def", "save_model", "(", "self", ",", "idx_episode", ")", ":", "\n", "        ", "safe_path", "=", "self", ".", "output_path", "+", "\"/qnet\"", "+", "str", "(", "idx_episode", ")", "+", "\".cptk\"", "\n", "self", ".", "saver", ".", "save", "(", "self", ".", "sess", ",", "safe_path", ")", "\n", "# self.q_network.save_weights(safe_path)", "\n", "print", "(", "\"+++++++++ Network at\"", ",", "idx_episode", ",", "\"saved to:\"", ",", "safe_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.restore_model": [[470, 473], ["dqn_tf_temporalAt.DQNAgent.saver.restore", "print"], "methods", ["None"], ["", "def", "restore_model", "(", "self", ",", "restore_path", ")", ":", "\n", "        ", "self", ".", "saver", ".", "restore", "(", "self", ".", "sess", ",", "restore_path", ")", "\n", "print", "(", "\"+++++++++ Network restored from: %s\"", ",", "restore_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.evaluate": [[474, 559], ["print", "plt.figure", "gym.wrappers.Monitor.reset", "numpy.zeros", "numpy.mean", "numpy.std", "print", "sys.stdout.flush", "dqn_tf_temporalAt.DQNAgent.restore_model", "gym.wrappers.Monitor", "dqn_tf_temporalAt.DQNAgent.history_processor.process_state_for_network", "dqn_tf_temporalAt.DQNAgent.select_action", "dqn_tf_temporalAt.DQNAgent.history_processor.process_state_for_network_ori", "numpy.random.random", "gym.wrappers.Monitor.step", "dqn_tf_temporalAt.DQNAgent.atari_processor.process_state_for_network", "dqn_tf_temporalAt.DQNAgent.atari_processor.process_state_for_network_ori", "dqn_tf_temporalAt.DQNAgent.sess.run", "numpy.reshape", "range", "plt.subplot", "plt.imshow", "plt.axis", "plt.savefig", "print", "print", "dqn_tf_temporalAt.save_scalar", "dqn_tf_temporalAt.save_scalar", "sys.stdout.flush", "gym.wrappers.Monitor.reset", "dqn_tf_temporalAt.DQNAgent.atari_processor.reset", "dqn_tf_temporalAt.DQNAgent.history_processor.reset", "plt.subplot", "plt.imshow"], "methods", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.restore_model", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.DQNAgent.select_action", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network_ori", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.process_state_for_network_ori", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset", "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.preprocessors.AtariPreprocessor.reset"], ["", "def", "evaluate", "(", "self", ",", "env", ",", "num_episodes", ",", "eval_count", ",", "max_episode_length", "=", "None", ",", "monitor", "=", "True", ")", ":", "\n", "        ", "\"\"\"Test your agent with a provided environment.\n        \n        You shouldn't update your network parameters here. Also if you\n        have any layers that vary in behavior between train/test time\n        (such as dropout or batch norm), you should set them to test.\n\n        Basically run your policy on the environment and collect stats\n        like cumulative reward, average episode length, etc.\n\n        You can also call the render function here if you want to\n        visually inspect your policy.\n        \"\"\"", "\n", "print", "(", "\"Evaluation starts.\"", ")", "\n", "plt", ".", "figure", "(", "1", ",", "figsize", "=", "(", "45", ",", "20", ")", ")", "\n", "\n", "is_training", "=", "False", "\n", "if", "self", ".", "load_network", ":", "\n", "# self.q_network.load_weights(self.load_network_path)", "\n", "# print(\"Load network from:\", self.load_network_path)", "\n", "            ", "self", ".", "restore_model", "(", "self", ".", "load_network_path", ")", "\n", "", "if", "monitor", ":", "\n", "            ", "env", "=", "wrappers", ".", "Monitor", "(", "env", ",", "self", ".", "output_path_videos", ",", "video_callable", "=", "lambda", "x", ":", "True", ",", "resume", "=", "True", ")", "\n", "", "state", "=", "env", ".", "reset", "(", ")", "\n", "\n", "idx_episode", "=", "1", "\n", "episode_frames", "=", "0", "\n", "episode_reward", "=", "np", ".", "zeros", "(", "num_episodes", ")", "\n", "t", "=", "0", "\n", "\n", "while", "idx_episode", "<=", "num_episodes", ":", "\n", "            ", "t", "+=", "1", "\n", "action_state", "=", "self", ".", "history_processor", ".", "process_state_for_network", "(", "\n", "self", ".", "atari_processor", ".", "process_state_for_network", "(", "state", ")", ")", "\n", "action", "=", "self", ".", "select_action", "(", "action_state", ",", "is_training", ",", "policy_type", "=", "'GreedyEpsilonPolicy'", ")", "\n", "\n", "action_state_ori", "=", "self", ".", "history_processor", ".", "process_state_for_network_ori", "(", "\n", "self", ".", "atari_processor", ".", "process_state_for_network_ori", "(", "state", ")", ")", "\n", "\n", "dice", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "\n", "state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "if", "dice", "<", "0.1", ":", "\n", "                ", "attention_a", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "q_network", ".", "attention_a", ",", "feed_dict", "=", "{", "self", ".", "q_network", ".", "imageIn", ":", "action_state", "[", "None", ",", ":", ",", ":", ",", ":", "]", ",", "self", ".", "q_network", ".", "batch_size", ":", "1", "}", ")", "\n", "# print attention_a.shape #(1, 10, 1)", "\n", "attention_a", "=", "np", ".", "reshape", "(", "attention_a", ",", "(", "-", "1", ")", ")", "\n", "for", "alpha_idx", "in", "range", "(", "action_state_ori", ".", "shape", "[", "3", "]", ")", ":", "\n", "                    ", "plt", ".", "subplot", "(", "2", ",", "action_state_ori", ".", "shape", "[", "3", "]", "//", "2", "+", "1", ",", "alpha_idx", "+", "1", ")", "\n", "img", "=", "action_state_ori", "[", ":", ",", ":", ",", ":", ",", "alpha_idx", "]", "#(210, 160, 3)", "\n", "plt", ".", "imshow", "(", "img", ")", "\n", "# plt.text(0, 1, 'Weight: %.4f'%(att ention_a[alpha_idx]) , color='black', weight='bold', backgroundcolor='white', fontsize=30)", "\n", "", "plt", ".", "subplot", "(", "2", ",", "action_state_ori", ".", "shape", "[", "3", "]", "//", "2", "+", "1", ",", "action_state_ori", ".", "shape", "[", "3", "]", "+", "2", ")", "\n", "plt", ".", "imshow", "(", "state", ")", "\n", "# plt.text(0, 1, 'Next state after taking the action %s'%(action), color='black', weight='bold', backgroundcolor='white', fontsize=20)", "\n", "plt", ".", "axis", "(", "'off'", ")", "\n", "plt", ".", "savefig", "(", "'%sattention_ep%d-frame%d.png'", "%", "(", "self", ".", "output_path_images", ",", "eval_count", ",", "episode_frames", ")", ")", "\n", "print", "(", "'---- Image saved at: %sattention_ep%d-frame%d.png'", "%", "(", "self", ".", "output_path_images", ",", "eval_count", ",", "episode_frames", ")", ")", "\n", "\n", "", "episode_frames", "+=", "1", "\n", "episode_reward", "[", "idx_episode", "-", "1", "]", "+=", "reward", "\n", "if", "episode_frames", ">", "max_episode_length", ":", "\n", "                ", "done", "=", "True", "\n", "", "if", "done", ":", "\n", "                ", "print", "(", "\"Eval: time %d, episode %d, length %d, reward %.0f. @eval_count %s\"", "%", "\n", "(", "t", ",", "idx_episode", ",", "episode_frames", ",", "episode_reward", "[", "idx_episode", "-", "1", "]", ",", "eval_count", ")", ")", "\n", "eval_count", "+=", "1", "\n", "save_scalar", "(", "eval_count", ",", "'eval/eval_episode_raw_reward'", ",", "episode_reward", "[", "idx_episode", "-", "1", "]", ",", "self", ".", "writer", ")", "\n", "save_scalar", "(", "eval_count", ",", "'eval/eval_episode_raw_length'", ",", "episode_frames", ",", "self", ".", "writer", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "episode_frames", "=", "0", "\n", "idx_episode", "+=", "1", "\n", "self", ".", "atari_processor", ".", "reset", "(", ")", "\n", "self", ".", "history_processor", ".", "reset", "(", ")", "\n", "\n", "\n", "", "", "reward_mean", "=", "np", ".", "mean", "(", "episode_reward", ")", "\n", "reward_std", "=", "np", ".", "std", "(", "episode_reward", ")", "\n", "print", "(", "\"Evaluation summury: num_episodes [%d], reward_mean [%.3f], reward_std [%.3f]\"", "%", "\n", "(", "num_episodes", ",", "reward_mean", ",", "reward_std", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "return", "reward_mean", ",", "reward_std", ",", "eval_count", "\n", "", "", ""]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.dqn_tf_temporalAt.save_scalar": [[111, 129], ["tensorflow.Summary", "tf.Summary.value.add", "float", "writer.add_summary"], "function", ["None"], ["", "", "def", "save_scalar", "(", "step", ",", "name", ",", "value", ",", "writer", ")", ":", "\n", "    ", "\"\"\"Save a scalar value to tensorboard.\n      Parameters\n      ----------\n      step: int\n        Training step (sets the position on x-axis of tensorboard graph.\n      name: str\n        Name of variable. Will be the name of the graph in tensorboard.\n      value: float\n        The value of the variable at this step.\n      writer: tf.FileWriter\n        The tensorboard FileWriter instance.\n      \"\"\"", "\n", "summary", "=", "tf", ".", "Summary", "(", ")", "\n", "summary_value", "=", "summary", ".", "value", ".", "add", "(", ")", "\n", "summary_value", ".", "simple_value", "=", "float", "(", "value", ")", "\n", "summary_value", ".", "tag", "=", "name", "\n", "writer", ".", "add_summary", "(", "summary", ",", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.objectives.huber_loss": [[6, 31], ["tensorflow.variable_scope", "tensorflow.abs", "tensorflow.where", "tensorflow.square"], "function", ["None"], ["def", "huber_loss", "(", "y_true", ",", "y_pred", ",", "max_grad", "=", "1.", ")", ":", "\n", "    ", "\"\"\"Calculate the huber loss.\n\n    See https://en.wikipedia.org/wiki/Huber_loss\n\n    Parameters\n    ----------\n    y_true: np.array, tf.Tensor\n      Target value.\n    y_pred: np.array, tf.Tensor\n      Predicted value.\n    max_grad: float, optional\n      Positive floating point value. Represents the maximum possible\n      gradient magnitude.\n\n    Returns\n    -------\n    tf.Tensor\n      The huber loss.\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"HuberLoss\"", ")", ":", "\n", "        ", "delta", "=", "max_grad", "\n", "diff", "=", "tf", ".", "abs", "(", "y_true", "-", "y_pred", ",", "name", "=", "\"diff\"", ")", "\n", "mask", "=", "diff", "<", "delta", "\n", "return", "tf", ".", "where", "(", "mask", ",", "0.5", "*", "tf", ".", "square", "(", "diff", ")", ",", "delta", "*", "(", "diff", "-", "0.5", "*", "delta", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.objectives.mean_huber_loss": [[32, 54], ["tensorflow.reduce_mean", "objectives.huber_loss"], "function", ["home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.objectives.huber_loss"], ["", "", "def", "mean_huber_loss", "(", "y_true", ",", "y_pred", ",", "max_grad", "=", "1.", ")", ":", "\n", "    ", "\"\"\"Return mean huber loss.\n\n    Same as huber_loss, but takes the mean over all values in the\n    output tensor.\n\n    Parameters\n    ----------\n    y_true: np.array, tf.Tensor\n      Target value.\n    y_pred: np.array, tf.Tensor\n      Predicted value.\n    max_grad: float, optional\n      Positive floating point value. Represents the maximum possible\n      gradient magnitude.\n\n    Returns\n    -------\n    tf.Tensor\n      The mean huber loss.\n    \"\"\"", "\n", "return", "tf", ".", "reduce_mean", "(", "huber_loss", "(", "y_true", ",", "y_pred", ",", "max_grad", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.utils.get_uninitialized_variables": [[6, 37], ["tensorflow.get_default_session", "semver.match", "tensorflow.global_variables", "list", "len", "tf.get_default_session.run", "tf.get_default_session.run", "tensorflow.pack", "tensorflow.stack", "zip", "tensorflow.is_variable_initialized", "tensorflow.is_variable_initialized"], "function", ["None"], ["def", "get_uninitialized_variables", "(", "variables", "=", "None", ")", ":", "\n", "    ", "\"\"\"Return a list of uninitialized tf variables.\n\n    Parameters\n    ----------\n    variables: tf.Variable, list(tf.Variable), optional\n      Filter variable list to only those that are uninitialized. If no\n      variables are specified the list of all variables in the graph\n      will be used.\n\n    Returns\n    -------\n    list(tf.Variable)\n      List of uninitialized tf variables.\n    \"\"\"", "\n", "sess", "=", "tf", ".", "get_default_session", "(", ")", "\n", "if", "variables", "is", "None", ":", "\n", "        ", "variables", "=", "tf", ".", "global_variables", "(", ")", "\n", "", "else", ":", "\n", "        ", "variables", "=", "list", "(", "variables", ")", "\n", "\n", "", "if", "len", "(", "variables", ")", "==", "0", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "", "if", "semver", ".", "match", "(", "tf", ".", "__version__", ",", "'<1.0.0'", ")", ":", "\n", "        ", "init_flag", "=", "sess", ".", "run", "(", "\n", "tf", ".", "pack", "(", "[", "tf", ".", "is_variable_initialized", "(", "v", ")", "for", "v", "in", "variables", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "init_flag", "=", "sess", ".", "run", "(", "\n", "tf", ".", "stack", "(", "[", "tf", ".", "is_variable_initialized", "(", "v", ")", "for", "v", "in", "variables", "]", ")", ")", "\n", "", "return", "[", "v", "for", "v", ",", "f", "in", "zip", "(", "variables", ",", "init_flag", ")", "if", "not", "f", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.utils.get_soft_target_model_updates": [[38, 70], ["target.get_weights", "source.get_weights", "range", "len"], "function", ["None"], ["", "def", "get_soft_target_model_updates", "(", "target", ",", "source", ",", "tau", ")", ":", "\n", "    ", "r\"\"\"Return list of target model update ops.\n\n    These are soft target updates. Meaning that the target values are\n    slowly adjusted, rather than directly copied over from the source\n    model.\n\n    The update is of the form:\n\n    $W' \\gets (1- \\tau) W' + \\tau W$ where $W'$ is the target weight\n    and $W$ is the source weight.\n\n    Parameters\n    ----------\n    target: keras.models.Model\n      The target model. Should have same architecture as source model.\n    source: keras.models.Model\n      The source model. Should have same architecture as target model.\n    tau: float\n      The weight of the source weights to the target weights used\n      during update.\n\n    Returns\n    -------\n    list(tf.Tensor)\n      List of tensor update ops.\n    \"\"\"", "\n", "target_weights", "=", "target", ".", "get_weights", "(", ")", "\n", "source_weights", "=", "source", ".", "get_weights", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "target_weights", ")", ")", ":", "\n", "        ", "target_weights", "[", "i", "]", "=", "(", "1", "-", "tau", ")", "*", "target_weights", "[", "i", "]", "+", "tau", "*", "source_weights", "[", "i", "]", "\n", "", "return", "target_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.utils.get_hard_target_model_updates": [[71, 90], ["source.get_weights"], "function", ["None"], ["", "def", "get_hard_target_model_updates", "(", "target", ",", "source", ")", ":", "\n", "    ", "\"\"\"Return list of target model update ops.\n\n    These are hard target updates. The source weights are copied\n    directly to the target network.\n\n    Parameters\n    ----------\n    target: keras.models.Model\n      The target model. Should have same architecture as source model.\n    source: keras.models.Model\n      The source model. Should have same architecture as target model.\n\n    Returns\n    -------\n    list(tf.Tensor)\n      List of tensor update ops.\n    \"\"\"", "\n", "return", "source", ".", "get_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.abhiksingla_UAV_obstacle_avoidance_controller.deeprl_prj.utils.compare_model": [[91, 100], ["target.get_weights", "source.get_weights", "print", "range", "len", "len", "print"], "function", ["None"], ["", "def", "compare_model", "(", "target", ",", "source", ")", ":", "\n", "    ", "target_weights", "=", "target", ".", "get_weights", "(", ")", "\n", "source_weights", "=", "source", ".", "get_weights", "(", ")", "\n", "print", "(", "len", "(", "target_weights", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "target_weights", ")", ")", ":", "\n", "        ", "print", "(", "target_weights", "[", "i", "]", ".", "shape", ",", "source_weights", "[", "i", "]", ".", "shape", ")", "\n", "if", "(", "target_weights", "[", "i", "]", "!=", "source_weights", "[", "i", "]", ")", ".", "any", "(", ")", ":", "\n", "            ", "return", "False", "\n", "", "", "return", "True", "\n", "", ""]]}