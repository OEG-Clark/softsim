{"home.repos.pwc.inspect_result.baidu_DuReader.server.main_server.ensemble_example": [[22, 41], ["dict", "dict.items", "sorted", "len", "dict", "sorted.append", "dict.setdefault", "answer_dict.setdefault.append", "numpy.sum"], "function", ["None"], ["def", "ensemble_example", "(", "answers", ",", "n_models", "=", "None", ")", ":", "\n", "    ", "if", "n_models", "is", "None", ":", "\n", "        ", "n_models", "=", "len", "(", "answers", ")", "\n", "", "answer_dict", "=", "dict", "(", ")", "\n", "for", "nbest_predictions", "in", "answers", ":", "\n", "        ", "for", "prediction", "in", "nbest_predictions", ":", "\n", "            ", "score_list", "=", "answer_dict", ".", "setdefault", "(", "prediction", "[", "'text'", "]", ",", "[", "]", ")", "\n", "score_list", ".", "append", "(", "prediction", "[", "'probability'", "]", ")", "\n", "\n", "", "", "ensemble_nbest_predictions", "=", "[", "]", "\n", "for", "answer", ",", "scores", "in", "answer_dict", ".", "items", "(", ")", ":", "\n", "        ", "prediction", "=", "dict", "(", ")", "\n", "prediction", "[", "'text'", "]", "=", "answer", "\n", "prediction", "[", "'probability'", "]", "=", "np", ".", "sum", "(", "scores", ")", "/", "n_models", "\n", "ensemble_nbest_predictions", ".", "append", "(", "prediction", ")", "\n", "\n", "", "ensemble_nbest_predictions", "=", "sorted", "(", "ensemble_nbest_predictions", ",", "key", "=", "lambda", "item", ":", "item", "[", "'probability'", "]", ",", "reverse", "=", "True", ")", "\n", "return", "ensemble_nbest_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.server.main_server.mrqa_main": [[43, 71], ["app.route", "flask.Response", "requests.post", "flask.request.get_json", "len", "multiprocessing.dummy.Pool", "multiprocessing.dummy.Pool.close", "multiprocessing.dummy.Pool.join", "list", "json.dumps", "multiprocessing.dummy.Pool.apply_async", "results.append", "nbests[].keys", "main_server.ensemble_example", "logger.exception", "pool.apply_async.get", "requests.post.json"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.server.main_server.ensemble_example"], ["", "@", "app", ".", "route", "(", "'/'", ",", "methods", "=", "[", "'POST'", "]", ")", "\n", "def", "mrqa_main", "(", ")", ":", "\n", "    ", "\"\"\"Description\"\"\"", "\n", "# parse input data", "\n", "pred", "=", "{", "}", "\n", "def", "_call_model", "(", "url", ",", "input_json", ")", ":", "\n", "        ", "nbest", "=", "requests", ".", "post", "(", "url", ",", "json", "=", "input_json", ")", "\n", "return", "nbest", "\n", "", "try", ":", "\n", "        ", "input_json", "=", "request", ".", "get_json", "(", "silent", "=", "True", ")", "\n", "n_models", "=", "len", "(", "urls", ")", "\n", "pool", "=", "ThreadPool", "(", "n_models", ")", "\n", "results", "=", "[", "]", "\n", "for", "url", "in", "urls", ":", "\n", "            ", "result", "=", "pool", ".", "apply_async", "(", "_call_model", ",", "(", "url", ",", "input_json", ")", ")", "\n", "results", ".", "append", "(", "result", ".", "get", "(", ")", ")", "\n", "", "pool", ".", "close", "(", ")", "\n", "pool", ".", "join", "(", ")", "\n", "nbests", "=", "[", "nbest", ".", "json", "(", ")", "[", "'results'", "]", "for", "nbest", "in", "results", "]", "\n", "qids", "=", "list", "(", "nbests", "[", "0", "]", ".", "keys", "(", ")", ")", "\n", "for", "qid", "in", "qids", ":", "\n", "            ", "ensemble_nbest", "=", "ensemble_example", "(", "[", "nbest", "[", "qid", "]", "for", "nbest", "in", "nbests", "]", ",", "n_models", "=", "n_models", ")", "\n", "pred", "[", "qid", "]", "=", "ensemble_nbest", "[", "0", "]", "[", "'text'", "]", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "        ", "pred", "[", "'error'", "]", "=", "'empty'", "\n", "logger", ".", "exception", "(", "e", ")", "\n", "\n", "", "return", "Response", "(", "json", ".", "dumps", "(", "pred", ")", ",", "mimetype", "=", "'application/json'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.bert_server.start_service.mrqa_service": [[35, 39], ["app.route", "server"], "function", ["None"], ["@", "app", ".", "route", "(", "'/'", ",", "methods", "=", "[", "'POST'", "]", ")", "\n", "def", "mrqa_service", "(", ")", ":", "\n", "    ", "\"\"\"Description\"\"\"", "\n", "return", "server", "(", "model", ",", "process_mode", "=", "mode", ",", "max_batch_size", "=", "max_batch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.bert_server.mrc_service.MRQAService.__init__": [[53, 61], ["logging.getLogger"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "name", ",", "logger", "=", "None", ",", "log_data", "=", "False", ")", ":", "\n", "        ", "\"\"\" \"\"\"", "\n", "self", ".", "name", "=", "name", "\n", "if", "logger", "is", "None", ":", "\n", "            ", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "'flask'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "logger", "=", "logger", "\n", "", "self", ".", "log_data", "=", "log_data", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.bert_server.mrc_service.MRQAService.__call__": [[62, 173], ["flask.request.get_json", "mrc_service.MRQAService._response_constructor", "time.time", "mrc_service._request_check", "mrc_service._abort", "mrc_service._split_input_json", "zip", "model.postprocessor", "time.time", "time.time", "mrc_service._timmer", "mrc_service.MRQAService.logger.error", "mrc_service.MRQAService.logger.exception", "mrc_service._abort", "time.time", "model.preprocessor", "len", "len", "processed.append", "time.time", "mrc_service._timmer", "mrc_service.MRQAService.logger.error", "mrc_service.MRQAService.logger.exception", "mrc_service._abort", "time.time", "mrc_service.MRQAService.examples.extend", "mrc_service.MRQAService.features.extend", "time.time", "mrc_service._timmer", "mrc_service.MRQAService.logger.error", "mrc_service.MRQAService.logger.exception", "mrc_service._abort", "time.time", "time.time", "mrc_service._timmer", "mrc_service.MRQAService.logger.error", "mrc_service.MRQAService.logger.exception", "mrc_service._abort", "logging.info", "mrc_service.MRQAService.logger.info", "mrc_service.MRQAService.mrc_results.extend", "len", "print", "NotImplementedError", "json.dumps", "json.dumps", "model.call_mrc", "mrc_service.MRQAService.mrc_results.extend", "model.call_mrc"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service.BasicMRCService._response_constructor", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._request_check", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._split_input_json", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.postprocessor", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.preprocessor", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.call_mrc", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.call_mrc"], ["", "def", "__call__", "(", "self", ",", "model", ",", "process_mode", "=", "'serial'", ",", "max_batch_size", "=", "5", ",", "timmer", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            mode: serial, parallel\n        \"\"\"", "\n", "if", "timmer", ":", "\n", "            ", "start", "=", "time", ".", "time", "(", ")", "\n", "", "\"\"\"Call mrc model wrapper and handle expectations\"\"\"", "\n", "self", ".", "input_json", "=", "request", ".", "get_json", "(", "silent", "=", "True", ")", "\n", "try", ":", "\n", "            ", "if", "timmer", ":", "\n", "                ", "start_request_check", "=", "time", ".", "time", "(", ")", "\n", "", "request_status", "=", "_request_check", "(", "self", ".", "input_json", ")", "\n", "if", "timmer", ":", "\n", "                ", "current_time", "=", "time", ".", "time", "(", ")", "\n", "_timmer", "(", "start", ",", "start_request_check", ",", "current_time", ",", "'request check'", ")", "\n", "", "if", "self", ".", "log_data", ":", "\n", "                ", "if", "self", ".", "logger", "is", "None", ":", "\n", "                    ", "logging", ".", "info", "(", "\n", "'Client input - {}'", ".", "format", "(", "json", ".", "dumps", "(", "self", ".", "input_json", ",", "ensure_ascii", "=", "False", ")", ")", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "logger", ".", "info", "(", "\n", "'Client input - {}'", ".", "format", "(", "json", ".", "dumps", "(", "self", ".", "input_json", ",", "ensure_ascii", "=", "False", ")", ")", "\n", ")", "\n", "", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "error", "(", "'server request checker error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'server request checker error - {}'", ".", "format", "(", "e", ")", ")", "\n", "", "if", "request_status", "!=", "'OK'", ":", "\n", "            ", "return", "_abort", "(", "400", ",", "request_status", ")", "\n", "\n", "# call preprocessor", "\n", "", "try", ":", "\n", "            ", "if", "timmer", ":", "\n", "                ", "start_preprocess", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "jsons", "=", "_split_input_json", "(", "self", ".", "input_json", ")", "\n", "processed", "=", "[", "]", "\n", "ex_start_idx", "=", "0", "\n", "feat_start_idx", "=", "1000000000", "\n", "for", "i", "in", "jsons", ":", "\n", "                ", "e", ",", "f", ",", "b", "=", "model", ".", "preprocessor", "(", "i", ",", "batch_size", "=", "max_batch_size", "if", "process_mode", "==", "'parallel'", "else", "1", ",", "examples_start_id", "=", "ex_start_idx", ",", "features_start_id", "=", "feat_start_idx", ")", "\n", "ex_start_idx", "+=", "len", "(", "e", ")", "\n", "feat_start_idx", "+=", "len", "(", "f", ")", "\n", "processed", ".", "append", "(", "[", "e", ",", "f", ",", "b", "]", ")", "\n", "\n", "", "if", "timmer", ":", "\n", "                ", "current_time", "=", "time", ".", "time", "(", ")", "\n", "_timmer", "(", "start", ",", "start_preprocess", ",", "current_time", ",", "'preprocess'", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "error", "(", "'preprocessor error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'preprocessor error - {}'", ".", "format", "(", "e", ")", ")", "\n", "\n", "", "def", "transpose", "(", "mat", ")", ":", "\n", "            ", "return", "zip", "(", "*", "mat", ")", "\n", "\n", "# call mrc", "\n", "", "try", ":", "\n", "            ", "if", "timmer", ":", "\n", "                ", "start_call_mrc", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "self", ".", "mrc_results", "=", "[", "]", "\n", "self", ".", "examples", "=", "[", "]", "\n", "self", ".", "features", "=", "[", "]", "\n", "for", "e", ",", "f", ",", "batches", "in", "processed", ":", "\n", "                ", "if", "verbose", ":", "\n", "                    ", "if", "len", "(", "f", ")", ">", "max_batch_size", ":", "\n", "                        ", "print", "(", "\"get a too long example....\"", ")", "\n", "", "", "if", "process_mode", "==", "'serial'", ":", "\n", "                    ", "self", ".", "mrc_results", ".", "extend", "(", "[", "model", ".", "call_mrc", "(", "b", ",", "squeeze_dim0", "=", "True", ")", "for", "b", "in", "batches", "[", ":", "max_batch_size", "]", "]", ")", "\n", "", "elif", "process_mode", "==", "'parallel'", ":", "\n", "# only keep first max_batch_size features", "\n", "# batches = batches[0]", "\n", "\n", "                    ", "for", "b", "in", "batches", ":", "\n", "                        ", "self", ".", "mrc_results", ".", "extend", "(", "model", ".", "call_mrc", "(", "b", ",", "return_list", "=", "True", ")", ")", "\n", "", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "(", ")", "\n", "", "self", ".", "examples", ".", "extend", "(", "e", ")", "\n", "# self.features.extend(f[:max_batch_size])", "\n", "self", ".", "features", ".", "extend", "(", "f", ")", "\n", "\n", "", "if", "timmer", ":", "\n", "                ", "current_time", "=", "time", ".", "time", "(", ")", "\n", "_timmer", "(", "start", ",", "start_call_mrc", ",", "current_time", ",", "'call mrc'", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "error", "(", "'call_mrc error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'call_mrc error - {}'", ".", "format", "(", "e", ")", ")", "\n", "\n", "# call post processor", "\n", "", "try", ":", "\n", "            ", "if", "timmer", ":", "\n", "                ", "start_post_precess", "=", "time", ".", "time", "(", ")", "\n", "", "self", ".", "results", "=", "model", ".", "postprocessor", "(", "self", ".", "examples", ",", "self", ".", "features", ",", "self", ".", "mrc_results", ")", "\n", "\n", "# only nbest results is POSTed back", "\n", "self", ".", "results", "=", "self", ".", "results", "[", "1", "]", "\n", "# self.results = self.results[0]", "\n", "\n", "if", "timmer", ":", "\n", "                ", "current_time", "=", "time", ".", "time", "(", ")", "\n", "_timmer", "(", "start", ",", "start_post_precess", ",", "current_time", ",", "'post process'", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "error", "(", "'postprocessor error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'postprocessor error - {}'", ".", "format", "(", "e", ")", ")", "\n", "\n", "", "return", "self", ".", "_response_constructor", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.bert_server.mrc_service.MRQAService._response_constructor": [[174, 190], ["flask.Response", "mrc_service.MRQAService.logger.info", "json.dumps", "mrc_service.MRQAService.logger.error", "mrc_service.MRQAService.logger.exception", "mrc_service._abort", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort"], ["", "def", "_response_constructor", "(", "self", ")", ":", "\n", "        ", "\"\"\"construct http response object\"\"\"", "\n", "try", ":", "\n", "            ", "response", "=", "{", "\n", "# 'requestID': self.input_json['requestID'],", "\n", "'results'", ":", "self", ".", "results", "\n", "}", "\n", "if", "self", ".", "log_data", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "\n", "'Response - {}'", ".", "format", "(", "json", ".", "dumps", "(", "response", ",", "ensure_ascii", "=", "False", ")", ")", "\n", ")", "\n", "", "return", "Response", "(", "json", ".", "dumps", "(", "response", ")", ",", "mimetype", "=", "'application/json'", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "error", "(", "'response constructor error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'response constructor error - {}'", ".", "format", "(", "e", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.bert_server.mrc_service._request_check": [[15, 27], ["isinstance"], "function", ["None"], ["def", "_request_check", "(", "input_json", ")", ":", "\n", "    ", "\"\"\"Check if the request json is valid\"\"\"", "\n", "if", "input_json", "is", "None", "or", "not", "isinstance", "(", "input_json", ",", "dict", ")", ":", "\n", "        ", "return", "'Can not parse the input json data - {}'", ".", "format", "(", "input_json", ")", "\n", "", "try", ":", "\n", "        ", "c", "=", "input_json", "[", "'context'", "]", "\n", "qa", "=", "input_json", "[", "'qas'", "]", "[", "0", "]", "\n", "qid", "=", "qa", "[", "'qid'", "]", "\n", "q", "=", "qa", "[", "'question'", "]", "\n", "", "except", "KeyError", "as", "e", ":", "\n", "        ", "return", "'Invalid request, key \"{}\" not found'", ".", "format", "(", "e", ")", "\n", "", "return", "'OK'", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.bert_server.mrc_service._abort": [[28, 31], ["flask.Response", "json.dumps"], "function", ["None"], ["", "def", "_abort", "(", "status_code", ",", "message", ")", ":", "\n", "    ", "\"\"\"Create custom error message and status code\"\"\"", "\n", "return", "Response", "(", "json", ".", "dumps", "(", "message", ")", ",", "status", "=", "status_code", ",", "mimetype", "=", "'application/json'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.bert_server.mrc_service._timmer": [[32, 37], ["print"], "function", ["None"], ["", "def", "_timmer", "(", "init_start", ",", "start", ",", "current", ",", "process_name", ")", ":", "\n", "    ", "cumulated_elapsed_time", "=", "(", "current", "-", "init_start", ")", "*", "1000", "\n", "current_elapsed_time", "=", "(", "current", "-", "start", ")", "*", "1000", "\n", "print", "(", "'{}\\t-\\t{:.2f}\\t{:.2f}'", ".", "format", "(", "process_name", ",", "cumulated_elapsed_time", ",", "\n", "current_elapsed_time", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.bert_server.mrc_service._split_input_json": [[38, 50], ["len", "len", "range", "len", "copy.deepcopy", "rets.append"], "function", ["None"], ["", "def", "_split_input_json", "(", "input_json", ")", ":", "\n", "    ", "if", "len", "(", "input_json", "[", "'context_tokens'", "]", ")", ">", "810", ":", "\n", "        ", "input_json", "[", "'context'", "]", "=", "input_json", "[", "'context'", "]", "[", ":", "5000", "]", "\n", "", "if", "len", "(", "input_json", "[", "'qas'", "]", ")", "==", "1", ":", "\n", "        ", "return", "[", "input_json", "]", "\n", "", "else", ":", "\n", "        ", "rets", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "input_json", "[", "'qas'", "]", ")", ")", ":", "\n", "            ", "temp", "=", "deepcopy", "(", "input_json", ")", "\n", "temp", "[", "'qas'", "]", "=", "[", "input_json", "[", "'qas'", "]", "[", "i", "]", "]", "\n", "rets", ".", "append", "(", "temp", ")", "\n", "", "return", "rets", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.bert_server.model_wrapper.BertModelWrapper.__init__": [[33, 53], ["paddle.Executor", "task_reader.mrqa.DataProcessor", "paddle.io.load_inference_model", "paddle.CUDAPlace", "paddle.core.get_cuda_device_count", "paddle.CPUPlace", "int", "os.environ.get", "os.path.join", "multiprocessing.cpu_count"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "model_dir", ")", ":", "\n", "        ", "\"\"\" \"\"\"", "\n", "if", "use_cuda", ":", "\n", "            ", "place", "=", "fluid", ".", "CUDAPlace", "(", "0", ")", "\n", "dev_count", "=", "fluid", ".", "core", ".", "get_cuda_device_count", "(", ")", "\n", "", "else", ":", "\n", "            ", "place", "=", "fluid", ".", "CPUPlace", "(", ")", "\n", "dev_count", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'CPU_NUM'", ",", "multiprocessing", ".", "cpu_count", "(", ")", ")", ")", "\n", "", "self", ".", "exe", "=", "fluid", ".", "Executor", "(", "place", ")", "\n", "\n", "self", ".", "bert_preprocessor", "=", "DataProcessor", "(", "\n", "vocab_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "'vocab.txt'", ")", ",", "\n", "do_lower_case", "=", "do_lower_case", ",", "\n", "max_seq_length", "=", "max_seq_len", ",", "\n", "in_tokens", "=", "in_tokens", ",", "\n", "doc_stride", "=", "doc_stride", ",", "\n", "max_query_length", "=", "max_query_length", ")", "\n", "\n", "self", ".", "inference_program", ",", "self", ".", "feed_target_names", ",", "self", ".", "fetch_targets", "=", "fluid", ".", "io", ".", "load_inference_model", "(", "dirname", "=", "model_dir", ",", "executor", "=", "self", ".", "exe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.bert_server.model_wrapper.BertModelWrapper.preprocessor": [[54, 61], ["model_wrapper.BertModelWrapper.bert_preprocessor.data_generator"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator"], ["", "def", "preprocessor", "(", "self", ",", "samples", ",", "batch_size", ",", "examples_start_id", ",", "features_start_id", ")", ":", "\n", "        ", "\"\"\"Preprocess the input samples, including word seg, padding, token to ids\"\"\"", "\n", "# Tokenization and paragraph padding", "\n", "examples", ",", "features", ",", "batch", "=", "self", ".", "bert_preprocessor", ".", "data_generator", "(", "\n", "samples", ",", "batch_size", ",", "max_len", "=", "max_seq_len", ",", "examples_start_id", "=", "examples_start_id", ",", "features_start_id", "=", "features_start_id", ")", "\n", "self", ".", "samples", "=", "samples", "\n", "return", "examples", ",", "features", ",", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.bert_server.model_wrapper.BertModelWrapper.call_mrc": [[62, 97], ["model_wrapper.BertModelWrapper.exe.run", "ValueError", "len", "zip"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["", "def", "call_mrc", "(", "self", ",", "batch", ",", "squeeze_dim0", "=", "False", ",", "return_list", "=", "False", ")", ":", "\n", "        ", "\"\"\"MRC\"\"\"", "\n", "if", "squeeze_dim0", "and", "return_list", ":", "\n", "            ", "raise", "ValueError", "(", "\"squeeze_dim0 only work for dict-type return value.\"", ")", "\n", "", "src_ids", "=", "batch", "[", "0", "]", "\n", "pos_ids", "=", "batch", "[", "1", "]", "\n", "sent_ids", "=", "batch", "[", "2", "]", "\n", "input_mask", "=", "batch", "[", "3", "]", "\n", "unique_id", "=", "batch", "[", "4", "]", "\n", "feed_dict", "=", "{", "\n", "self", ".", "feed_target_names", "[", "0", "]", ":", "src_ids", ",", "\n", "self", ".", "feed_target_names", "[", "1", "]", ":", "pos_ids", ",", "\n", "self", ".", "feed_target_names", "[", "2", "]", ":", "sent_ids", ",", "\n", "self", ".", "feed_target_names", "[", "3", "]", ":", "input_mask", ",", "\n", "self", ".", "feed_target_names", "[", "4", "]", ":", "unique_id", "\n", "}", "\n", "\n", "np_unique_ids", ",", "np_start_logits", ",", "np_end_logits", ",", "np_num_seqs", "=", "self", ".", "exe", ".", "run", "(", "self", ".", "inference_program", ",", "feed", "=", "feed_dict", ",", "fetch_list", "=", "self", ".", "fetch_targets", ")", "\n", "\n", "if", "len", "(", "np_unique_ids", ")", "==", "1", "and", "squeeze_dim0", ":", "\n", "            ", "np_unique_ids", "=", "np_unique_ids", "[", "0", "]", "\n", "np_start_logits", "=", "np_start_logits", "[", "0", "]", "\n", "np_end_logits", "=", "np_end_logits", "[", "0", "]", "\n", "\n", "", "if", "return_list", ":", "\n", "            ", "mrc_results", "=", "[", "{", "'unique_ids'", ":", "id", ",", "'start_logits'", ":", "st", ",", "'end_logits'", ":", "end", "}", "\n", "for", "id", ",", "st", ",", "end", "in", "zip", "(", "np_unique_ids", ",", "np_start_logits", ",", "np_end_logits", ")", "]", "\n", "", "else", ":", "\n", "            ", "mrc_results", "=", "{", "\n", "'unique_ids'", ":", "np_unique_ids", ",", "\n", "'start_logits'", ":", "np_start_logits", ",", "\n", "'end_logits'", ":", "np_end_logits", ",", "\n", "}", "\n", "", "return", "mrc_results", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.bert_server.model_wrapper.BertModelWrapper.postprocessor": [[98, 132], ["collections.namedtuple", "isinstance", "task_reader.mrqa.get_answers", "isinstance", "range", "results.append", "int", "results.append", "float", "float", "collections.namedtuple.", "float", "float", "collections.namedtuple."], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.get_answers"], ["", "def", "postprocessor", "(", "self", ",", "examples", ",", "features", ",", "mrc_results", ")", ":", "\n", "        ", "\"\"\"Extract answer\n         batch: [examples, features] from preprocessor\n         mrc_results: model results from call_mrc. if mrc_results is list, each element of which is a size=1 batch.\n        \"\"\"", "\n", "RawResult", "=", "collections", ".", "namedtuple", "(", "\"RawResult\"", ",", "\n", "[", "\"unique_id\"", ",", "\"start_logits\"", ",", "\"end_logits\"", "]", ")", "\n", "results", "=", "[", "]", "\n", "if", "isinstance", "(", "mrc_results", ",", "list", ")", ":", "\n", "            ", "for", "res", "in", "mrc_results", ":", "\n", "                ", "unique_id", "=", "res", "[", "'unique_ids'", "]", "[", "0", "]", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "res", "[", "'start_logits'", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "res", "[", "'end_logits'", "]", ".", "flat", "]", "\n", "results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "assert", "isinstance", "(", "mrc_results", ",", "dict", ")", "\n", "for", "idx", "in", "range", "(", "mrc_results", "[", "'unique_ids'", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "unique_id", "=", "int", "(", "mrc_results", "[", "'unique_ids'", "]", "[", "idx", "]", ")", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "mrc_results", "[", "'start_logits'", "]", "[", "idx", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "mrc_results", "[", "'end_logits'", "]", "[", "idx", "]", ".", "flat", "]", "\n", "results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "\n", "", "", "answers", "=", "get_answers", "(", "\n", "examples", ",", "features", ",", "results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "do_lower_case", ",", "verbose", ")", "\n", "return", "answers", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.batching.mask": [[21, 72], ["max", "numpy.random.rand", "numpy.random.randint", "enumerate", "numpy.array().astype().reshape", "numpy.array().astype().reshape", "enumerate", "len", "len", "int", "numpy.array().astype", "numpy.array().astype", "numpy.random.randint", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "numpy.array", "numpy.array", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "len"], "function", ["None"], ["def", "mask", "(", "batch_tokens", ",", "total_token_num", ",", "vocab_size", ",", "CLS", "=", "1", ",", "SEP", "=", "2", ",", "MASK", "=", "3", ")", ":", "\n", "    ", "\"\"\"\n    Add mask for batch_tokens, return out, mask_label, mask_pos;\n    Note: mask_pos responding the batch_tokens after padded;\n    \"\"\"", "\n", "max_len", "=", "max", "(", "[", "len", "(", "sent", ")", "for", "sent", "in", "batch_tokens", "]", ")", "\n", "mask_label", "=", "[", "]", "\n", "mask_pos", "=", "[", "]", "\n", "prob_mask", "=", "np", ".", "random", ".", "rand", "(", "total_token_num", ")", "\n", "# Note: the first token is [CLS], so [low=1]", "\n", "replace_ids", "=", "np", ".", "random", ".", "randint", "(", "1", ",", "high", "=", "vocab_size", ",", "size", "=", "total_token_num", ")", "\n", "pre_sent_len", "=", "0", "\n", "prob_index", "=", "0", "\n", "for", "sent_index", ",", "sent", "in", "enumerate", "(", "batch_tokens", ")", ":", "\n", "        ", "mask_flag", "=", "False", "\n", "prob_index", "+=", "pre_sent_len", "\n", "for", "token_index", ",", "token", "in", "enumerate", "(", "sent", ")", ":", "\n", "            ", "prob", "=", "prob_mask", "[", "prob_index", "+", "token_index", "]", "\n", "if", "prob", ">", "0.15", ":", "\n", "                ", "continue", "\n", "", "elif", "0.03", "<", "prob", "<=", "0.15", ":", "\n", "# mask", "\n", "                ", "if", "token", "!=", "SEP", "and", "token", "!=", "CLS", ":", "\n", "                    ", "mask_label", ".", "append", "(", "sent", "[", "token_index", "]", ")", "\n", "sent", "[", "token_index", "]", "=", "MASK", "\n", "mask_flag", "=", "True", "\n", "mask_pos", ".", "append", "(", "sent_index", "*", "max_len", "+", "token_index", ")", "\n", "", "", "elif", "0.015", "<", "prob", "<=", "0.03", ":", "\n", "# random replace", "\n", "                ", "if", "token", "!=", "SEP", "and", "token", "!=", "CLS", ":", "\n", "                    ", "mask_label", ".", "append", "(", "sent", "[", "token_index", "]", ")", "\n", "sent", "[", "token_index", "]", "=", "replace_ids", "[", "prob_index", "+", "token_index", "]", "\n", "mask_flag", "=", "True", "\n", "mask_pos", ".", "append", "(", "sent_index", "*", "max_len", "+", "token_index", ")", "\n", "", "", "else", ":", "\n", "# keep the original token", "\n", "                ", "if", "token", "!=", "SEP", "and", "token", "!=", "CLS", ":", "\n", "                    ", "mask_label", ".", "append", "(", "sent", "[", "token_index", "]", ")", "\n", "mask_pos", ".", "append", "(", "sent_index", "*", "max_len", "+", "token_index", ")", "\n", "", "", "", "pre_sent_len", "=", "len", "(", "sent", ")", "\n", "# ensure at least mask one word in a sentence", "\n", "while", "not", "mask_flag", ":", "\n", "            ", "token_index", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "1", ",", "high", "=", "len", "(", "sent", ")", "-", "1", ",", "size", "=", "1", ")", ")", "\n", "if", "sent", "[", "token_index", "]", "!=", "SEP", "and", "sent", "[", "token_index", "]", "!=", "CLS", ":", "\n", "                ", "mask_label", ".", "append", "(", "sent", "[", "token_index", "]", ")", "\n", "sent", "[", "token_index", "]", "=", "MASK", "\n", "mask_flag", "=", "True", "\n", "mask_pos", ".", "append", "(", "sent_index", "*", "max_len", "+", "token_index", ")", "\n", "", "", "", "mask_label", "=", "np", ".", "array", "(", "mask_label", ")", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "1", "]", ")", "\n", "mask_pos", "=", "np", ".", "array", "(", "mask_pos", ")", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "1", "]", ")", "\n", "return", "batch_tokens", ",", "mask_label", ",", "mask_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.batching.prepare_batch_data": [[74, 135], ["range", "batching.pad_batch_data", "batching.pad_batch_data", "batching.pad_batch_data", "len", "numpy.array().astype().reshape", "labels_list.append", "batching.mask", "len", "numpy.array().astype", "numpy.array"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.pad_batch_data", "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.pad_batch_data", "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.pad_batch_data", "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.mask"], ["", "def", "prepare_batch_data", "(", "insts", ",", "\n", "total_token_num", ",", "\n", "max_len", "=", "None", ",", "\n", "voc_size", "=", "0", ",", "\n", "pad_id", "=", "None", ",", "\n", "cls_id", "=", "None", ",", "\n", "sep_id", "=", "None", ",", "\n", "mask_id", "=", "None", ",", "\n", "return_input_mask", "=", "True", ",", "\n", "return_max_len", "=", "True", ",", "\n", "return_num_token", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    1. generate Tensor of data\n    2. generate Tensor of position\n    3. generate self attention mask, [shape: batch_size *  max_len * max_len]\n    \"\"\"", "\n", "batch_src_ids", "=", "[", "inst", "[", "0", "]", "for", "inst", "in", "insts", "]", "\n", "batch_sent_ids", "=", "[", "inst", "[", "1", "]", "for", "inst", "in", "insts", "]", "\n", "batch_pos_ids", "=", "[", "inst", "[", "2", "]", "for", "inst", "in", "insts", "]", "\n", "labels_list", "=", "[", "]", "\n", "# compatible with mrqa, whose example includes start/end positions, ", "\n", "# or unique id", "\n", "for", "i", "in", "range", "(", "3", ",", "len", "(", "insts", "[", "0", "]", ")", ",", "1", ")", ":", "\n", "        ", "labels", "=", "[", "inst", "[", "i", "]", "for", "inst", "in", "insts", "]", "\n", "labels", "=", "np", ".", "array", "(", "labels", ")", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "1", "]", ")", "\n", "labels_list", ".", "append", "(", "labels", ")", "\n", "# First step: do mask without padding", "\n", "", "if", "mask_id", ">=", "0", ":", "\n", "        ", "out", ",", "mask_label", ",", "mask_pos", "=", "mask", "(", "\n", "batch_src_ids", ",", "\n", "total_token_num", ",", "\n", "vocab_size", "=", "voc_size", ",", "\n", "CLS", "=", "cls_id", ",", "\n", "SEP", "=", "sep_id", ",", "\n", "MASK", "=", "mask_id", ")", "\n", "", "else", ":", "\n", "        ", "out", "=", "batch_src_ids", "\n", "# Second step: padding", "\n", "", "src_id", ",", "self_input_mask", "=", "pad_batch_data", "(", "\n", "out", ",", "\n", "max_len", "=", "max_len", ",", "\n", "pad_idx", "=", "pad_id", ",", "return_input_mask", "=", "True", ")", "\n", "pos_id", "=", "pad_batch_data", "(", "\n", "batch_pos_ids", ",", "\n", "max_len", "=", "max_len", ",", "\n", "pad_idx", "=", "pad_id", ",", "\n", "return_pos", "=", "False", ",", "\n", "return_input_mask", "=", "False", ")", "\n", "sent_id", "=", "pad_batch_data", "(", "\n", "batch_sent_ids", ",", "\n", "max_len", "=", "max_len", ",", "\n", "pad_idx", "=", "pad_id", ",", "\n", "return_pos", "=", "False", ",", "\n", "return_input_mask", "=", "False", ")", "\n", "if", "mask_id", ">=", "0", ":", "\n", "        ", "return_list", "=", "[", "\n", "src_id", ",", "pos_id", ",", "sent_id", ",", "self_input_mask", ",", "mask_label", ",", "mask_pos", "\n", "]", "+", "labels_list", "\n", "", "else", ":", "\n", "        ", "return_list", "=", "[", "src_id", ",", "pos_id", ",", "sent_id", ",", "self_input_mask", "]", "+", "labels_list", "\n", "", "return", "return_list", "if", "len", "(", "return_list", ")", ">", "1", "else", "return_list", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.batching.pad_batch_data": [[137, 178], ["numpy.array", "max", "np.array.astype().reshape", "numpy.array", "numpy.array", "numpy.expand_dims", "np.array.astype().reshape", "np.expand_dims.astype", "len", "len", "len", "list", "list", "np.array.astype", "list", "np.array.astype", "range", "len", "len", "len", "len", "len"], "function", ["None"], ["", "def", "pad_batch_data", "(", "insts", ",", "\n", "max_len", "=", "None", ",", "\n", "pad_idx", "=", "0", ",", "\n", "return_pos", "=", "False", ",", "\n", "return_input_mask", "=", "False", ",", "\n", "return_max_len", "=", "False", ",", "\n", "return_num_token", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Pad the instances to the max sequence length in batch, and generate the\n    corresponding position data and input mask.\n    \"\"\"", "\n", "return_list", "=", "[", "]", "\n", "if", "max_len", "is", "None", ":", "\n", "        ", "max_len", "=", "max", "(", "len", "(", "inst", ")", "for", "inst", "in", "insts", ")", "\n", "# Any token included in dict can be used to pad, since the paddings' loss", "\n", "# will be masked out by weights and make no effect on parameter gradients.", "\n", "", "inst_data", "=", "np", ".", "array", "(", "[", "\n", "list", "(", "inst", ")", "+", "list", "(", "[", "pad_idx", "]", "*", "(", "max_len", "-", "len", "(", "inst", ")", ")", ")", "for", "inst", "in", "insts", "\n", "]", ")", "\n", "return_list", "+=", "[", "inst_data", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "max_len", ",", "1", "]", ")", "]", "\n", "# position data", "\n", "if", "return_pos", ":", "\n", "        ", "inst_pos", "=", "np", ".", "array", "(", "[", "\n", "list", "(", "range", "(", "0", ",", "len", "(", "inst", ")", ")", ")", "+", "[", "pad_idx", "]", "*", "(", "max_len", "-", "len", "(", "inst", ")", ")", "\n", "for", "inst", "in", "insts", "\n", "]", ")", "\n", "return_list", "+=", "[", "inst_pos", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "max_len", ",", "1", "]", ")", "]", "\n", "", "if", "return_input_mask", ":", "\n", "# This is used to avoid attention on paddings.", "\n", "        ", "input_mask_data", "=", "np", ".", "array", "(", "[", "[", "1", "]", "*", "len", "(", "inst", ")", "+", "[", "0", "]", "*", "\n", "(", "max_len", "-", "len", "(", "inst", ")", ")", "for", "inst", "in", "insts", "]", ")", "\n", "input_mask_data", "=", "np", ".", "expand_dims", "(", "input_mask_data", ",", "axis", "=", "-", "1", ")", "\n", "return_list", "+=", "[", "input_mask_data", ".", "astype", "(", "\"float32\"", ")", "]", "\n", "", "if", "return_max_len", ":", "\n", "        ", "return_list", "+=", "[", "max_len", "]", "\n", "", "if", "return_num_token", ":", "\n", "        ", "num_token", "=", "0", "\n", "for", "inst", "in", "insts", ":", "\n", "            ", "num_token", "+=", "len", "(", "inst", ")", "\n", "", "return_list", "+=", "[", "num_token", "]", "\n", "", "return", "return_list", "if", "len", "(", "return_list", ")", ">", "1", "else", "return_list", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.FullTokenizer.__init__": [[112, 117], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "tokenization.FullTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.FullTokenizer.tokenize": [[118, 125], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.FullTokenizer.convert_tokens_to_ids": [[126, 128], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.FullTokenizer.convert_ids_to_tokens": [[129, 131], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.CharTokenizer.__init__": [[136, 140], ["tokenization.load_vocab", "tokenization.WordpieceTokenizer", "tokenization.CharTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.CharTokenizer.tokenize": [[141, 148], ["text.lower().split", "tokenization.CharTokenizer.wordpiece_tokenizer.tokenize", "text.lower", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "text", ".", "lower", "(", ")", ".", "split", "(", "\" \"", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.CharTokenizer.convert_tokens_to_ids": [[149, 151], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.CharTokenizer.convert_ids_to_tokens": [[152, 154], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.BasicTokenizer.__init__": [[159, 167], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n            do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "_never_lowercase", "=", "[", "'[UNK]'", ",", "'[SEP]'", ",", "'[PAD]'", ",", "'[CLS]'", ",", "'[MASK]'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.BasicTokenizer.tokenize": [[168, 194], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "split_tokens.extend", "split_tokens.extend", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._tokenize_chinese_chars", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "_never_lowercase", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "if", "token", "in", "self", ".", "_never_lowercase", ":", "\n", "                ", "split_tokens", ".", "extend", "(", "[", "token", "]", ")", "\n", "", "else", ":", "\n", "                ", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.BasicTokenizer._run_strip_accents": [[195, 205], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.normalize"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.BasicTokenizer._run_split_on_punc": [[206, 225], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.BasicTokenizer._tokenize_chinese_chars": [[226, 238], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.BasicTokenizer._is_chinese_char": [[239, 260], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#     https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.BasicTokenizer._clean_text": [[261, 273], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_whitespace", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.WordpieceTokenizer.__init__": [[278, 282], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.WordpieceTokenizer.tokenize": [[283, 335], ["tokenization.convert_to_unicode", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n            input = \"unaffable\"\n            output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n            text: A single token or whitespace separated tokens. This should have\n                already been passed through `BasicTokenizer.\n\n        Returns:\n            A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.convert_to_unicode": [[26, 44], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode", "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode"], ["def", "convert_to_unicode", "(", "text", ")", ":", "\n", "    ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.printable_text": [[46, 67], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "    ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.load_vocab": [[69, 82], ["collections.OrderedDict", "open", "enumerate", "convert_to_unicode().split", "token.strip.strip", "int", "len", "tokenization.convert_to_unicode", "len", "line.strip"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "fin", "=", "open", "(", "vocab_file", ")", "\n", "for", "num", ",", "line", "in", "enumerate", "(", "fin", ")", ":", "\n", "        ", "items", "=", "convert_to_unicode", "(", "line", ".", "strip", "(", ")", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "len", "(", "items", ")", ">", "2", ":", "\n", "            ", "break", "\n", "", "token", "=", "items", "[", "0", "]", "\n", "index", "=", "items", "[", "1", "]", "if", "len", "(", "items", ")", "==", "2", "else", "num", "\n", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "int", "(", "index", ")", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.convert_by_vocab": [[84, 90], ["output.append"], "function", ["None"], ["", "def", "convert_by_vocab", "(", "vocab", ",", "items", ")", ":", "\n", "    ", "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "        ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.convert_tokens_to_ids": [[92, 94], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.convert_ids_to_tokens": [[96, 98], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "inv_vocab", ",", "ids", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization.whitespace_tokenize": [[100, 107], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization._is_whitespace": [[337, 347], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization._is_control": [[349, 359], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.tokenization._is_punctuation": [[361, 375], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.MRQAExample.__init__": [[34, 49], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "qas_id", ",", "\n", "question_text", ",", "\n", "doc_tokens", ",", "\n", "orig_answer_text", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "False", ")", ":", "\n", "        ", "self", ".", "qas_id", "=", "qas_id", "\n", "self", ".", "question_text", "=", "question_text", "\n", "self", ".", "doc_tokens", "=", "doc_tokens", "\n", "self", ".", "orig_answer_text", "=", "orig_answer_text", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.MRQAExample.__str__": [[50, 52], ["mrqa.MRQAExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.SQuADExample.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.MRQAExample.__repr__": [[53, 66], ["task_reader.tokenization.printable_text", "task_reader.tokenization.printable_text"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"\"", "\n", "s", "+=", "\"qas_id: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "self", ".", "qas_id", ")", ")", "\n", "s", "+=", "\", question_text: %s\"", "%", "(", "\n", "tokenization", ".", "printable_text", "(", "self", ".", "question_text", ")", ")", "\n", "s", "+=", "\", doc_tokens: [%s]\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "doc_tokens", ")", ")", "\n", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", start_position: %d\"", "%", "(", "self", ".", "start_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", end_position: %d\"", "%", "(", "self", ".", "end_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", is_impossible: %r\"", "%", "(", "self", ".", "is_impossible", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.InputFeatures.__init__": [[71, 96], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "unique_id", ",", "\n", "example_index", ",", "\n", "doc_span_index", ",", "\n", "tokens", ",", "\n", "token_to_orig_map", ",", "\n", "token_is_max_context", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "segment_ids", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "None", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "example_index", "=", "example_index", "\n", "self", ".", "doc_span_index", "=", "doc_span_index", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "token_to_orig_map", "=", "token_to_orig_map", "\n", "self", ".", "token_is_max_context", "=", "token_is_max_context", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.DataProcessor.__init__": [[526, 548], ["task_reader.tokenization.FullTokenizer", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab_path", ",", "do_lower_case", ",", "max_seq_length", ",", "in_tokens", ",", "\n", "doc_stride", ",", "max_query_length", ")", ":", "\n", "        ", "self", ".", "_tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "vocab_path", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "_max_seq_length", "=", "max_seq_length", "\n", "self", ".", "_doc_stride", "=", "doc_stride", "\n", "self", ".", "_max_query_length", "=", "max_query_length", "\n", "self", ".", "_in_tokens", "=", "in_tokens", "\n", "\n", "self", ".", "vocab", "=", "self", ".", "_tokenizer", ".", "vocab", "\n", "self", ".", "vocab_size", "=", "len", "(", "self", ".", "vocab", ")", "\n", "self", ".", "pad_id", "=", "self", ".", "vocab", "[", "\"[PAD]\"", "]", "\n", "self", ".", "cls_id", "=", "self", ".", "vocab", "[", "\"[CLS]\"", "]", "\n", "self", ".", "sep_id", "=", "self", ".", "vocab", "[", "\"[SEP]\"", "]", "\n", "self", ".", "mask_id", "=", "self", ".", "vocab", "[", "\"[MASK]\"", "]", "\n", "\n", "self", ".", "current_train_example", "=", "-", "1", "\n", "self", ".", "num_train_examples", "=", "-", "1", "\n", "self", ".", "current_train_epoch", "=", "-", "1", "\n", "\n", "self", ".", "train_examples", "=", "None", "\n", "self", ".", "num_examples", "=", "{", "'train'", ":", "-", "1", ",", "'predict'", ":", "-", "1", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.DataProcessor.get_train_progress": [[549, 552], ["None"], "methods", ["None"], ["", "def", "get_train_progress", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets progress for training phase.\"\"\"", "\n", "return", "self", ".", "current_train_example", ",", "self", ".", "current_train_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.DataProcessor.get_examples": [[553, 562], ["mrqa.read_mrqa_examples"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.read_mrqa_examples"], ["", "def", "get_examples", "(", "self", ",", "\n", "data_path", ",", "\n", "is_training", ",", "\n", "with_negative", "=", "False", ")", ":", "\n", "        ", "examples", "=", "read_mrqa_examples", "(", "\n", "input_file", "=", "data_path", ",", "\n", "is_training", "=", "is_training", ",", "\n", "with_negative", "=", "with_negative", ")", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.DataProcessor.get_num_examples": [[563, 568], ["ValueError"], "methods", ["None"], ["", "def", "get_num_examples", "(", "self", ",", "phase", ")", ":", "\n", "        ", "if", "phase", "not", "in", "[", "'train'", ",", "'predict'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unknown phase, which should be in ['train', 'predict'].\"", ")", "\n", "", "return", "self", ".", "num_examples", "[", "phase", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.DataProcessor.estimate_runtime_examples": [[569, 575], ["mrqa.DataProcessor.estimate_runtime_examples"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.estimate_runtime_examples"], ["", "def", "estimate_runtime_examples", "(", "self", ",", "data_path", ",", "sample_rate", "=", "0.01", ",", "remove_impossible_questions", "=", "True", ",", "filter_invalid_spans", "=", "True", ")", ":", "\n", "        ", "\"\"\"Noted that this API Only support for Training phase.\"\"\"", "\n", "return", "estimate_runtime_examples", "(", "data_path", ",", "sample_rate", ",", "self", ".", "_tokenizer", ",", "self", ".", "_max_seq_length", ",", "self", ".", "_doc_stride", ",", "self", ".", "_max_query_length", ",", "remove_impossible_questions", "=", "True", ",", "filter_invalid_spans", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.DataProcessor.get_features": [[576, 587], ["mrqa.convert_examples_to_features"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.convert_examples_to_features"], ["", "def", "get_features", "(", "self", ",", "examples", ",", "is_training", ",", "examples_start_id", ",", "features_start_id", ")", ":", "\n", "        ", "features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "examples", ",", "\n", "tokenizer", "=", "self", ".", "_tokenizer", ",", "\n", "max_seq_length", "=", "self", ".", "_max_seq_length", ",", "\n", "doc_stride", "=", "self", ".", "_doc_stride", ",", "\n", "max_query_length", "=", "self", ".", "_max_query_length", ",", "\n", "examples_start_id", "=", "examples_start_id", ",", "\n", "features_start_id", "=", "features_start_id", ",", "\n", "is_training", "=", "is_training", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.DataProcessor.data_generator": [[588, 651], ["mrqa.read_mrqa_examples", "mrqa.DataProcessor.get_features", "mrqa.DataProcessor.data_generator.batch_reader"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.read_mrqa_examples", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_features", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.batch_reader"], ["", "def", "data_generator", "(", "self", ",", "\n", "raw_samples", ",", "\n", "batch_size", ",", "\n", "max_len", "=", "None", ",", "\n", "phase", "=", "'predict'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "with_negative", "=", "False", ",", "\n", "epoch", "=", "1", ",", "\n", "examples_start_id", "=", "0", ",", "\n", "features_start_id", "=", "1000000000", ")", ":", "\n", "        ", "examples", "=", "read_mrqa_examples", "(", "raw_samples", ")", "\n", "\n", "def", "batch_reader", "(", "features", ",", "batch_size", ",", "in_tokens", ")", ":", "\n", "            ", "batch", ",", "total_token_num", ",", "max_len", "=", "[", "]", ",", "0", ",", "0", "\n", "for", "(", "index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "                ", "if", "phase", "==", "'train'", ":", "\n", "                    ", "self", ".", "current_train_example", "=", "index", "+", "1", "\n", "", "seq_len", "=", "len", "(", "feature", ".", "input_ids", ")", "\n", "labels", "=", "[", "feature", ".", "unique_id", "\n", "]", "if", "feature", ".", "start_position", "is", "None", "else", "[", "\n", "feature", ".", "start_position", ",", "feature", ".", "end_position", "\n", "]", "\n", "example", "=", "[", "\n", "feature", ".", "input_ids", ",", "feature", ".", "segment_ids", ",", "range", "(", "seq_len", ")", "\n", "]", "+", "labels", "\n", "max_len", "=", "max", "(", "max_len", ",", "seq_len", ")", "\n", "\n", "#max_len = max(max_len, len(token_ids))", "\n", "if", "in_tokens", ":", "\n", "                    ", "to_append", "=", "(", "len", "(", "batch", ")", "+", "1", ")", "*", "max_len", "<=", "batch_size", "\n", "", "else", ":", "\n", "                    ", "to_append", "=", "len", "(", "batch", ")", "<", "batch_size", "\n", "\n", "", "if", "to_append", ":", "\n", "                    ", "batch", ".", "append", "(", "example", ")", "\n", "total_token_num", "+=", "seq_len", "\n", "", "else", ":", "\n", "                    ", "yield", "batch", ",", "total_token_num", "\n", "batch", ",", "total_token_num", ",", "max_len", "=", "[", "example", "\n", "]", ",", "seq_len", ",", "seq_len", "\n", "", "", "if", "len", "(", "batch", ")", ">", "0", ":", "\n", "                ", "yield", "batch", ",", "total_token_num", "\n", "\n", "", "", "features", "=", "self", ".", "get_features", "(", "examples", ",", "is_training", "=", "False", ",", "examples_start_id", "=", "examples_start_id", ",", "features_start_id", "=", "features_start_id", ")", "\n", "\n", "all_dev_batches", "=", "[", "]", "\n", "for", "batch_data", ",", "total_token_num", "in", "batch_reader", "(", "\n", "features", ",", "batch_size", ",", "self", ".", "_in_tokens", ")", ":", "\n", "            ", "batch_data", "=", "prepare_batch_data", "(", "\n", "batch_data", ",", "\n", "total_token_num", ",", "\n", "max_len", "=", "max_len", ",", "\n", "voc_size", "=", "-", "1", ",", "\n", "pad_id", "=", "self", ".", "pad_id", ",", "\n", "cls_id", "=", "self", ".", "cls_id", ",", "\n", "sep_id", "=", "self", ".", "sep_id", ",", "\n", "mask_id", "=", "-", "1", ",", "\n", "return_input_mask", "=", "True", ",", "\n", "return_max_len", "=", "False", ",", "\n", "return_num_token", "=", "False", ")", "\n", "all_dev_batches", ".", "append", "(", "batch_data", ")", "\n", "", "return", "examples", ",", "features", ",", "all_dev_batches", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.read_mrqa_examples": [[98, 139], ["re.sub", "mrqa.read_mrqa_examples.is_whitespace"], "function", ["None"], ["", "", "def", "read_mrqa_examples", "(", "sample", ",", "is_training", "=", "False", ",", "with_negative", "=", "False", ")", ":", "\n", "    ", "\"\"\"Read a MRQA json file into a list of MRQAExample.\"\"\"", "\n", "\n", "def", "is_whitespace", "(", "c", ")", ":", "\n", "        ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "examples", "=", "[", "]", "\n", "# sample = json.loads(raw_sample)", "\n", "paragraph_text", "=", "sample", "[", "\"context\"", "]", "\n", "paragraph_text", "=", "re", ".", "sub", "(", "r'\\[TLE\\]|\\[DOC\\]|\\[PAR\\]'", ",", "'[SEP]'", ",", "paragraph_text", ")", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "        ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "            ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "            ", "if", "prev_is_whitespace", ":", "\n", "                ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "", "for", "qa", "in", "sample", "[", "\"qas\"", "]", ":", "\n", "        ", "qas_id", "=", "qa", "[", "\"qid\"", "]", "\n", "question_text", "=", "qa", "[", "\"question\"", "]", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "is_impossible", "=", "False", "\n", "\n", "example", "=", "MRQAExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.convert_examples_to_features": [[141, 295], ["tokenizer.tokenize", "enumerate", "collections.namedtuple", "enumerate", "len", "orig_to_tok_index.append", "tokenizer.tokenize", "mrqa._improve_answer_span", "len", "doc_spans.append", "min", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "range", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "mrqa.InputFeatures", "features.append", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "len", "len", "collections.namedtuple.", "len", "tokens.append", "segment_ids.append", "mrqa._check_is_max_context", "tokens.append", "segment_ids.append", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._improve_answer_span", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._check_is_max_context"], ["", "def", "convert_examples_to_features", "(", "\n", "examples", ",", "\n", "tokenizer", ",", "\n", "max_seq_length", ",", "\n", "doc_stride", ",", "\n", "max_query_length", ",", "\n", "is_training", ",", "\n", "examples_start_id", "=", "0", ",", "\n", "features_start_id", "=", "1000000000", "\n", "#output_fn", "\n", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "unique_id", "=", "features_start_id", "\n", "example_index", "=", "examples_start_id", "\n", "\n", "features", "=", "[", "]", "\n", "for", "example", "in", "examples", ":", "\n", "        ", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "\n", "if", "len", "(", "query_tokens", ")", ">", "max_query_length", ":", "\n", "            ", "query_tokens", "=", "query_tokens", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "tok_to_orig_index", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "all_doc_tokens", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "doc_tokens", ")", ":", "\n", "            ", "orig_to_tok_index", ".", "append", "(", "len", "(", "all_doc_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "for", "sub_token", "in", "sub_tokens", ":", "\n", "                ", "tok_to_orig_index", ".", "append", "(", "i", ")", "\n", "all_doc_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "tok_start_position", "=", "None", "\n", "tok_end_position", "=", "None", "\n", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "            ", "tok_start_position", "=", "-", "1", "\n", "tok_end_position", "=", "-", "1", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "            ", "tok_start_position", "=", "orig_to_tok_index", "[", "example", ".", "start_position", "]", "\n", "if", "example", ".", "end_position", "<", "len", "(", "example", ".", "doc_tokens", ")", "-", "1", ":", "\n", "                ", "tok_end_position", "=", "orig_to_tok_index", "[", "example", ".", "end_position", "+", "\n", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "                ", "tok_end_position", "=", "len", "(", "all_doc_tokens", ")", "-", "1", "\n", "", "(", "tok_start_position", ",", "tok_end_position", ")", "=", "_improve_answer_span", "(", "\n", "all_doc_tokens", ",", "tok_start_position", ",", "tok_end_position", ",", "tokenizer", ",", "\n", "example", ".", "orig_answer_text", ")", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "", "max_tokens_for_doc", "=", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "# We can have documents that are longer than the maximum sequence length.", "\n", "# To deal with this we do a sliding window approach, where we take chunks", "\n", "# of the up to our max length with a stride of `doc_stride`.", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "            ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "                ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "                ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "            ", "tokens", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "token_is_max_context", "=", "{", "}", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "query_tokens", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "                ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "tok_to_orig_index", "[", "\n", "split_token_index", "]", "\n", "\n", "is_max_context", "=", "_check_is_max_context", "(", "\n", "doc_spans", ",", "doc_span_index", ",", "split_token_index", ")", "\n", "token_is_max_context", "[", "len", "(", "tokens", ")", "]", "=", "is_max_context", "\n", "tokens", ".", "append", "(", "all_doc_tokens", "[", "split_token_index", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "#while len(input_ids) < max_seq_length:", "\n", "#  input_ids.append(0)", "\n", "#  input_mask.append(0)", "\n", "#  segment_ids.append(0)", "\n", "\n", "#assert len(input_ids) == max_seq_length", "\n", "#assert len(input_mask) == max_seq_length", "\n", "#assert len(segment_ids) == max_seq_length", "\n", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "                ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "out_of_span", "=", "False", "\n", "if", "not", "(", "tok_start_position", ">=", "doc_start", "and", "\n", "tok_end_position", "<=", "doc_end", ")", ":", "\n", "                    ", "out_of_span", "=", "True", "\n", "", "if", "out_of_span", ":", "\n", "                    ", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "continue", "\n", "", "else", ":", "\n", "                    ", "doc_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "start_position", "=", "tok_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "tok_end_position", "-", "doc_start", "+", "doc_offset", "\n", "\n", "", "", "\"\"\"\n            if is_training and example.is_impossible:\n                start_position = 0\n                end_position = 0\n            \"\"\"", "\n", "\n", "feature", "=", "InputFeatures", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "example_index", "=", "example_index", ",", "\n", "doc_span_index", "=", "doc_span_index", ",", "\n", "tokens", "=", "tokens", ",", "\n", "token_to_orig_map", "=", "token_to_orig_map", ",", "\n", "token_is_max_context", "=", "token_is_max_context", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "example", ".", "is_impossible", ")", "\n", "\n", "unique_id", "+=", "1", "\n", "features", ".", "append", "(", "feature", ")", "\n", "", "example_index", "+=", "1", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.estimate_runtime_examples": [[297, 448], ["print", "print", "int", "open", "len", "float", "tokenizer.tokenize", "enumerate", "mrqa._improve_answer_span", "collections.namedtuple", "enumerate", "json.load", "len", "orig_to_tok_index.append", "tokenizer.tokenize", "len", "doc_spans.append", "min", "ord", "mrqa.MRQAExample", "sampled_examples.append", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "len", "len", "len", "len", "collections.namedtuple.", "len", "len", "len", "random.random", "mrqa.read_mrqa_examples.is_whitespace"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._improve_answer_span", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "estimate_runtime_examples", "(", "data_path", ",", "sample_rate", ",", "tokenizer", ",", "max_seq_length", ",", "doc_stride", ",", "max_query_length", ",", "remove_impossible_questions", "=", "True", ",", "filter_invalid_spans", "=", "True", ")", ":", "\n", "    ", "\"\"\"Count runtime examples which may differ from number of raw samples due to sliding window operation and etc.. This is useful to get correct warmup steps for training.\"\"\"", "\n", "\n", "assert", "sample_rate", ">", "0.0", "and", "sample_rate", "<=", "1.0", ",", "\"sample_rate must be set between 0.0~1.0\"", "\n", "\n", "print", "(", "\"loading data with json parser...\"", ")", "\n", "with", "open", "(", "data_path", ",", "\"r\"", ")", "as", "reader", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "\n", "", "num_raw_examples", "=", "0", "\n", "for", "entry", "in", "data", ":", "\n", "        ", "for", "paragraph", "in", "entry", "[", "\"paragraphs\"", "]", ":", "\n", "            ", "paragraph_text", "=", "paragraph", "[", "\"context\"", "]", "\n", "for", "qa", "in", "paragraph", "[", "\"qas\"", "]", ":", "\n", "                ", "num_raw_examples", "+=", "1", "\n", "", "", "", "print", "(", "\"num raw examples:{}\"", ".", "format", "(", "num_raw_examples", ")", ")", "\n", "\n", "def", "is_whitespace", "(", "c", ")", ":", "\n", "        ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "sampled_examples", "=", "[", "]", "\n", "for", "entry", "in", "data", ":", "\n", "        ", "for", "paragraph", "in", "entry", "[", "\"paragraphs\"", "]", ":", "\n", "            ", "doc_tokens", "=", "None", "\n", "for", "qa", "in", "paragraph", "[", "\"qas\"", "]", ":", "\n", "                ", "if", "random", ".", "random", "(", ")", ">", "sample_rate", "and", "sample_rate", "<", "1.0", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "doc_tokens", "is", "None", ":", "\n", "                    ", "paragraph_text", "=", "paragraph", "[", "\"context\"", "]", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "                        ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "                            ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "                            ", "if", "prev_is_whitespace", ":", "\n", "                                ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                                ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "", "", "assert", "len", "(", "qa", "[", "\"answers\"", "]", ")", "==", "1", ",", "\"For training, each question should have exactly 1 answer.\"", "\n", "\n", "qas_id", "=", "qa", "[", "\"id\"", "]", "\n", "question_text", "=", "qa", "[", "\"question\"", "]", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "is_impossible", "=", "False", "\n", "\n", "if", "(", "'is_impossible'", "in", "qa", ")", "and", "(", "qa", "[", "\"is_impossible\"", "]", ")", ":", "\n", "                    ", "if", "remove_impossible_questions", "or", "filter_invalid_spans", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "                        ", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "orig_answer_text", "=", "\"\"", "\n", "is_impossible", "=", "True", "\n", "", "", "else", ":", "\n", "                    ", "answer", "=", "qa", "[", "\"answers\"", "]", "[", "0", "]", "\n", "orig_answer_text", "=", "answer", "[", "\"text\"", "]", "\n", "answer_offset", "=", "answer", "[", "\"answer_start\"", "]", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "\n", "answer_length", "-", "1", "]", "\n", "\n", "# remove corrupt samples", "\n", "actual_text", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "start_position", ":", "(", "\n", "end_position", "+", "1", ")", "]", ")", "\n", "cleaned_answer_text", "=", "\" \"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "if", "actual_text", ".", "find", "(", "cleaned_answer_text", ")", "==", "-", "1", ":", "\n", "                        ", "print", "(", "\"Could not find answer: '%s' vs. '%s'\"", ",", "\n", "actual_text", ",", "cleaned_answer_text", ")", "\n", "continue", "\n", "\n", "", "", "example", "=", "MRQAExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "\n", "orig_answer_text", "=", "orig_answer_text", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "is_impossible", ")", "\n", "sampled_examples", ".", "append", "(", "example", ")", "\n", "\n", "\n", "", "", "", "runtime_sample_rate", "=", "len", "(", "sampled_examples", ")", "/", "float", "(", "num_raw_examples", ")", "\n", "# print(\"DEBUG-> runtime sampled examples: {}, sample rate: {}.\".format(len(sampled_examples), runtime_sample_rate))", "\n", "\n", "runtime_samp_cnt", "=", "0", "\n", "\n", "for", "example", "in", "sampled_examples", ":", "\n", "        ", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "\n", "if", "len", "(", "query_tokens", ")", ">", "max_query_length", ":", "\n", "            ", "query_tokens", "=", "query_tokens", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "tok_to_orig_index", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "all_doc_tokens", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "doc_tokens", ")", ":", "\n", "            ", "orig_to_tok_index", ".", "append", "(", "len", "(", "all_doc_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "for", "sub_token", "in", "sub_tokens", ":", "\n", "                ", "tok_to_orig_index", ".", "append", "(", "i", ")", "\n", "all_doc_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "tok_start_position", "=", "None", "\n", "tok_end_position", "=", "None", "\n", "\n", "tok_start_position", "=", "orig_to_tok_index", "[", "example", ".", "start_position", "]", "\n", "if", "example", ".", "end_position", "<", "len", "(", "example", ".", "doc_tokens", ")", "-", "1", ":", "\n", "            ", "tok_end_position", "=", "orig_to_tok_index", "[", "example", ".", "end_position", "+", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "            ", "tok_end_position", "=", "len", "(", "all_doc_tokens", ")", "-", "1", "\n", "", "(", "tok_start_position", ",", "tok_end_position", ")", "=", "_improve_answer_span", "(", "\n", "all_doc_tokens", ",", "tok_start_position", ",", "tok_end_position", ",", "tokenizer", ",", "\n", "example", ".", "orig_answer_text", ")", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "max_tokens_for_doc", "=", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "            ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "                ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "                ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "            ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "filter_invalid_spans", "and", "not", "(", "tok_start_position", ">=", "doc_start", "and", "tok_end_position", "<=", "doc_end", ")", ":", "\n", "                ", "continue", "\n", "", "runtime_samp_cnt", "+=", "1", "\n", "", "", "return", "int", "(", "runtime_samp_cnt", "/", "runtime_sample_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa._improve_answer_span": [[450, 485], ["range", "tokenizer.tokenize", "range"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "_improve_answer_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_answer_text", ")", ":", "\n", "    ", "\"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"", "\n", "\n", "# The MRQA annotations are character based. We first project them to", "\n", "# whitespace-tokenized words. But then after WordPiece tokenization, we can", "\n", "# often find a \"better match\". For example:", "\n", "#", "\n", "#   Question: What year was John Smith born?", "\n", "#   Context: The leader was John Smith (1895-1943).", "\n", "#   Answer: 1895", "\n", "#", "\n", "# The original whitespace-tokenized answer will be \"(1895-1943).\". However", "\n", "# after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match", "\n", "# the exact answer, 1895.", "\n", "#", "\n", "# However, this is not always possible. Consider the following:", "\n", "#", "\n", "#   Question: What country is the top exporter of electornics?", "\n", "#   Context: The Japanese electronics industry is the lagest in the world.", "\n", "#   Answer: Japan", "\n", "#", "\n", "# In this case, the annotator chose \"Japan\" as a character sub-span of", "\n", "# the word \"Japanese\". Since our WordPiece tokenizer does not split", "\n", "# \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare", "\n", "# in MRQA, but does happen.", "\n", "tok_answer_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_answer_text", ")", ")", "\n", "\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "        ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "text_span", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_answer_text", ":", "\n", "                ", "return", "(", "new_start", ",", "new_end", ")", "\n", "\n", "", "", "", "return", "(", "input_start", ",", "input_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa._check_is_max_context": [[487, 523], ["enumerate", "min"], "function", ["None"], ["", "def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "    ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "        ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "            ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "            ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "\n", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "            ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.get_answers": [[653, 803], ["collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "example_index_to_features[].append", "enumerate", "sorted", "collections.namedtuple", "mrqa._compute_softmax", "enumerate", "mrqa._get_best_indexes", "mrqa._get_best_indexes", "nbest.append", "nbest.append", "len", "total_scores.append", "print", "collections.OrderedDict", "nbest_json.append", "len", "len", "tok_text.strip.replace", "tok_text.strip.replace", "tok_text.strip.strip", "mrqa.get_final_text", "collections.namedtuple.", "collections.namedtuple.", "sorted.append", "tok_text.strip.split", "len", "len", "feature.token_is_max_context.get", "collections.namedtuple."], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._compute_softmax", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.get_final_text"], ["", "", "def", "get_answers", "(", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "do_lower_case", ",", "\n", "verbose", "=", "False", ")", ":", "\n", "    ", "\"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"", "\n", "\n", "example_index_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "        ", "example_index_to_features", "[", "feature", ".", "example_index", "]", ".", "append", "(", "feature", ")", "\n", "\n", "", "unique_id_to_result", "=", "{", "}", "\n", "for", "result", "in", "all_results", ":", "\n", "        ", "unique_id_to_result", "[", "result", ".", "unique_id", "]", "=", "result", "\n", "\n", "", "_PrelimPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"PrelimPrediction\"", ",", "[", "\n", "\"feature_index\"", ",", "\"start_index\"", ",", "\"end_index\"", ",", "\"start_logit\"", ",", "\n", "\"end_logit\"", "\n", "]", ")", "\n", "\n", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_nbest_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "scores_diff_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "all_examples", ")", ":", "\n", "        ", "features", "=", "example_index_to_features", "[", "example_index", "]", "\n", "\n", "prelim_predictions", "=", "[", "]", "\n", "# keep track of the minimum score of null start+end of position 0", "\n", "score_null", "=", "1000000", "# large and positive", "\n", "min_null_feature_index", "=", "0", "# the paragraph slice with min mull score", "\n", "null_start_logit", "=", "0", "# the start logit at the slice with min null score", "\n", "null_end_logit", "=", "0", "# the end logit at the slice with min null score", "\n", "for", "(", "feature_index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "            ", "result", "=", "unique_id_to_result", "[", "feature", ".", "unique_id", "]", "\n", "start_indexes", "=", "_get_best_indexes", "(", "result", ".", "start_logits", ",", "n_best_size", ")", "\n", "end_indexes", "=", "_get_best_indexes", "(", "result", ".", "end_logits", ",", "n_best_size", ")", "\n", "# if we could have irrelevant answers, get the min score of irrelevant", "\n", "for", "start_index", "in", "start_indexes", ":", "\n", "                ", "for", "end_index", "in", "end_indexes", ":", "\n", "# We could hypothetically create invalid predictions, e.g., predict", "\n", "# that the start of the span is in the question. We throw out all", "\n", "# invalid predictions.", "\n", "                    ", "if", "start_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "start_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                        ", "continue", "\n", "", "if", "not", "feature", ".", "token_is_max_context", ".", "get", "(", "start_index", ",", "False", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "                        ", "continue", "\n", "", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "if", "length", ">", "max_answer_length", ":", "\n", "                        ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "feature_index", ",", "\n", "start_index", "=", "start_index", ",", "\n", "end_index", "=", "end_index", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "start_index", "]", ",", "\n", "end_logit", "=", "result", ".", "end_logits", "[", "end_index", "]", ")", ")", "\n", "\n", "", "", "", "prelim_predictions", "=", "sorted", "(", "\n", "prelim_predictions", ",", "\n", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_logit", "+", "x", ".", "end_logit", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "_NbestPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"NbestPrediction\"", ",", "[", "\"text\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "seen_predictions", "=", "{", "}", "\n", "nbest", "=", "[", "]", "\n", "for", "pred", "in", "prelim_predictions", ":", "\n", "            ", "if", "len", "(", "nbest", ")", ">=", "n_best_size", ":", "\n", "                ", "break", "\n", "", "feature", "=", "features", "[", "pred", ".", "feature_index", "]", "\n", "if", "pred", ".", "start_index", ">", "0", ":", "# this is a non-null prediction", "\n", "                ", "tok_tokens", "=", "feature", ".", "tokens", "[", "pred", ".", "start_index", ":", "(", "pred", ".", "end_index", "+", "1", "\n", ")", "]", "\n", "orig_doc_start", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "start_index", "]", "\n", "orig_doc_end", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "end_index", "]", "\n", "orig_tokens", "=", "example", ".", "doc_tokens", "[", "orig_doc_start", ":", "(", "orig_doc_end", "+", "\n", "1", ")", "]", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_tokens", ")", "\n", "\n", "# De-tokenize WordPieces that have been split off.", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\"##\"", ",", "\"\"", ")", "\n", "\n", "# Clean whitespace", "\n", "tok_text", "=", "tok_text", ".", "strip", "(", ")", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_text", ".", "split", "(", ")", ")", "\n", "orig_text", "=", "\" \"", ".", "join", "(", "orig_tokens", ")", "\n", "\n", "final_text", "=", "get_final_text", "(", "tok_text", ",", "orig_text", ",", "do_lower_case", ",", "\n", "verbose", ")", "\n", "if", "final_text", "in", "seen_predictions", ":", "\n", "                    ", "continue", "\n", "\n", "", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "", "else", ":", "\n", "                ", "final_text", "=", "\"\"", "\n", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "\n", "", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "final_text", ",", "\n", "start_logit", "=", "pred", ".", "start_logit", ",", "\n", "end_logit", "=", "pred", ".", "end_logit", ")", ")", "\n", "\n", "# In very rare edge cases we could have no valid predictions. So we", "\n", "# just create a nonce prediction in this case to avoid failure.", "\n", "", "if", "not", "nbest", ":", "\n", "            ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "\"empty\"", ",", "start_logit", "=", "0.0", ",", "end_logit", "=", "0.0", ")", ")", "\n", "\n", "", "assert", "len", "(", "nbest", ")", ">=", "1", "\n", "\n", "total_scores", "=", "[", "]", "\n", "best_non_null_entry", "=", "None", "\n", "for", "entry", "in", "nbest", ":", "\n", "            ", "total_scores", ".", "append", "(", "entry", ".", "start_logit", "+", "entry", ".", "end_logit", ")", "\n", "if", "not", "best_non_null_entry", ":", "\n", "                ", "if", "entry", ".", "text", ":", "\n", "                    ", "best_non_null_entry", "=", "entry", "\n", "# debug", "\n", "", "", "", "if", "best_non_null_entry", "is", "None", ":", "\n", "            ", "print", "(", "\"Emmm..., sth wrong\"", ")", "\n", "\n", "", "probs", "=", "_compute_softmax", "(", "total_scores", ")", "\n", "\n", "nbest_json", "=", "[", "]", "\n", "for", "(", "i", ",", "entry", ")", "in", "enumerate", "(", "nbest", ")", ":", "\n", "            ", "output", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "output", "[", "\"text\"", "]", "=", "entry", ".", "text", "\n", "output", "[", "\"probability\"", "]", "=", "probs", "[", "i", "]", "\n", "output", "[", "\"start_logit\"", "]", "=", "entry", ".", "start_logit", "\n", "output", "[", "\"end_logit\"", "]", "=", "entry", ".", "end_logit", "\n", "nbest_json", ".", "append", "(", "output", ")", "\n", "\n", "", "assert", "len", "(", "nbest_json", ")", ">=", "1", "\n", "\n", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "[", "0", "]", "[", "\"text\"", "]", "\n", "all_nbest_json", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "\n", "\n", "", "return", "all_predictions", ",", "all_nbest_json", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa.get_final_text": [[805, 898], ["task_reader.tokenization.BasicTokenizer", "tok_text.find", "mrqa.get_final_text._strip_spaces"], "function", ["None"], ["", "def", "get_final_text", "(", "pred_text", ",", "orig_text", ",", "do_lower_case", ",", "verbose", ")", ":", "\n", "    ", "\"\"\"Project the tokenized prediction back to the original text.\"\"\"", "\n", "\n", "# When we created the data, we kept track of the alignment between original", "\n", "# (whitespace tokenized) tokens and our WordPiece tokenized tokens. So", "\n", "# now `orig_text` contains the span of our original text corresponding to the", "\n", "# span that we predicted.", "\n", "#", "\n", "# However, `orig_text` may contain extra characters that we don't want in", "\n", "# our prediction.", "\n", "#", "\n", "# For example, let's say:", "\n", "#   pred_text = steve smith", "\n", "#   orig_text = Steve Smith's", "\n", "#", "\n", "# We don't want to return `orig_text` because it contains the extra \"'s\".", "\n", "#", "\n", "# We don't want to return `pred_text` because it's already been normalized", "\n", "# (the MRQA eval script also does punctuation stripping/lower casing but", "\n", "# our tokenizer does additional normalization like stripping accent", "\n", "# characters).", "\n", "#", "\n", "# What we really want to return is \"Steve Smith\".", "\n", "#", "\n", "# Therefore, we have to apply a semi-complicated alignment heruistic between", "\n", "# `pred_text` and `orig_text` to get a character-to-charcter alignment. This", "\n", "# can fail in certain cases in which case we just return `orig_text`.", "\n", "\n", "def", "_strip_spaces", "(", "text", ")", ":", "\n", "        ", "ns_chars", "=", "[", "]", "\n", "ns_to_s_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "i", ",", "c", ")", "in", "enumerate", "(", "text", ")", ":", "\n", "            ", "if", "c", "==", "\" \"", ":", "\n", "                ", "continue", "\n", "", "ns_to_s_map", "[", "len", "(", "ns_chars", ")", "]", "=", "i", "\n", "ns_chars", ".", "append", "(", "c", ")", "\n", "", "ns_text", "=", "\"\"", ".", "join", "(", "ns_chars", ")", "\n", "return", "(", "ns_text", ",", "ns_to_s_map", ")", "\n", "\n", "# We first tokenize `orig_text`, strip whitespace from the result", "\n", "# and `pred_text`, and check if they are the same length. If they are", "\n", "# NOT the same length, the heuristic has failed. If they are the same", "\n", "# length, we assume the characters are one-to-one aligned.", "\n", "", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_text", ")", ")", "\n", "\n", "start_position", "=", "tok_text", ".", "find", "(", "pred_text", ")", "\n", "if", "start_position", "==", "-", "1", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "\"Unable to find text: '%s' in '%s'\"", "%", "(", "pred_text", ",", "orig_text", ")", ")", "\n", "", "return", "orig_text", "\n", "", "end_position", "=", "start_position", "+", "len", "(", "pred_text", ")", "-", "1", "\n", "\n", "(", "orig_ns_text", ",", "orig_ns_to_s_map", ")", "=", "_strip_spaces", "(", "orig_text", ")", "\n", "(", "tok_ns_text", ",", "tok_ns_to_s_map", ")", "=", "_strip_spaces", "(", "tok_text", ")", "\n", "\n", "if", "len", "(", "orig_ns_text", ")", "!=", "len", "(", "tok_ns_text", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "\"Length not equal after stripping spaces: '%s' vs '%s'\"", ",", "\n", "orig_ns_text", ",", "tok_ns_text", ")", "\n", "", "return", "orig_text", "\n", "\n", "# We then project the characters in `pred_text` back to `orig_text` using", "\n", "# the character-to-character alignment.", "\n", "", "tok_s_to_ns_map", "=", "{", "}", "\n", "for", "(", "i", ",", "tok_index", ")", "in", "six", ".", "iteritems", "(", "tok_ns_to_s_map", ")", ":", "\n", "        ", "tok_s_to_ns_map", "[", "tok_index", "]", "=", "i", "\n", "\n", "", "orig_start_position", "=", "None", "\n", "if", "start_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_start_position", "=", "tok_s_to_ns_map", "[", "start_position", "]", "\n", "if", "ns_start_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_start_position", "=", "orig_ns_to_s_map", "[", "ns_start_position", "]", "\n", "\n", "", "", "if", "orig_start_position", "is", "None", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "\"Couldn't map start position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "orig_end_position", "=", "None", "\n", "if", "end_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_end_position", "=", "tok_s_to_ns_map", "[", "end_position", "]", "\n", "if", "ns_end_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_end_position", "=", "orig_ns_to_s_map", "[", "ns_end_position", "]", "\n", "\n", "", "", "if", "orig_end_position", "is", "None", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "\"Couldn't map end position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "output_text", "=", "orig_text", "[", "orig_start_position", ":", "(", "orig_end_position", "+", "1", ")", "]", "\n", "return", "output_text", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa._get_best_indexes": [[900, 911], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "_get_best_indexes", "(", "logits", ",", "n_best_size", ")", ":", "\n", "    ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "\n", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "        ", "if", "i", ">=", "n_best_size", ":", "\n", "            ", "break", "\n", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa._compute_softmax": [[913, 934], ["math.exp", "exp_scores.append", "probs.append"], "function", ["None"], ["", "def", "_compute_softmax", "(", "scores", ")", ":", "\n", "    ", "\"\"\"Compute softmax probability over raw logits.\"\"\"", "\n", "if", "not", "scores", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "", "max_score", "=", "None", "\n", "for", "score", "in", "scores", ":", "\n", "        ", "if", "max_score", "is", "None", "or", "score", ">", "max_score", ":", "\n", "            ", "max_score", "=", "score", "\n", "\n", "", "", "exp_scores", "=", "[", "]", "\n", "total_sum", "=", "0.0", "\n", "for", "score", "in", "scores", ":", "\n", "        ", "x", "=", "math", ".", "exp", "(", "score", "-", "max_score", ")", "\n", "exp_scores", ".", "append", "(", "x", ")", "\n", "total_sum", "+=", "x", "\n", "\n", "", "probs", "=", "[", "]", "\n", "for", "score", "in", "exp_scores", ":", "\n", "        ", "probs", ".", "append", "(", "score", "/", "total_sum", ")", "\n", "", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.MRQAExample.__init__": [[34, 49], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "qas_id", ",", "\n", "question_text", ",", "\n", "doc_tokens", ",", "\n", "orig_answer_text", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "False", ")", ":", "\n", "        ", "self", ".", "qas_id", "=", "qas_id", "\n", "self", ".", "question_text", "=", "question_text", "\n", "self", ".", "doc_tokens", "=", "doc_tokens", "\n", "self", ".", "orig_answer_text", "=", "orig_answer_text", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.MRQAExample.__str__": [[50, 52], ["mrqa_infer.MRQAExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.SQuADExample.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.MRQAExample.__repr__": [[53, 66], ["task_reader.tokenization.printable_text", "task_reader.tokenization.printable_text"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"\"", "\n", "s", "+=", "\"qas_id: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "self", ".", "qas_id", ")", ")", "\n", "s", "+=", "\", question_text: %s\"", "%", "(", "\n", "tokenization", ".", "printable_text", "(", "self", ".", "question_text", ")", ")", "\n", "s", "+=", "\", doc_tokens: [%s]\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "doc_tokens", ")", ")", "\n", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", start_position: %d\"", "%", "(", "self", ".", "start_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", end_position: %d\"", "%", "(", "self", ".", "end_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", is_impossible: %r\"", "%", "(", "self", ".", "is_impossible", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.InputFeatures.__init__": [[71, 96], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "unique_id", ",", "\n", "example_index", ",", "\n", "doc_span_index", ",", "\n", "tokens", ",", "\n", "token_to_orig_map", ",", "\n", "token_is_max_context", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "segment_ids", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "None", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "example_index", "=", "example_index", "\n", "self", ".", "doc_span_index", "=", "doc_span_index", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "token_to_orig_map", "=", "token_to_orig_map", "\n", "self", ".", "token_is_max_context", "=", "token_is_max_context", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.DataProcessor.__init__": [[526, 548], ["task_reader.tokenization.FullTokenizer", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab_path", ",", "do_lower_case", ",", "max_seq_length", ",", "in_tokens", ",", "\n", "doc_stride", ",", "max_query_length", ")", ":", "\n", "        ", "self", ".", "_tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "vocab_path", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "_max_seq_length", "=", "max_seq_length", "\n", "self", ".", "_doc_stride", "=", "doc_stride", "\n", "self", ".", "_max_query_length", "=", "max_query_length", "\n", "self", ".", "_in_tokens", "=", "in_tokens", "\n", "\n", "self", ".", "vocab", "=", "self", ".", "_tokenizer", ".", "vocab", "\n", "self", ".", "vocab_size", "=", "len", "(", "self", ".", "vocab", ")", "\n", "self", ".", "pad_id", "=", "self", ".", "vocab", "[", "\"[PAD]\"", "]", "\n", "self", ".", "cls_id", "=", "self", ".", "vocab", "[", "\"[CLS]\"", "]", "\n", "self", ".", "sep_id", "=", "self", ".", "vocab", "[", "\"[SEP]\"", "]", "\n", "self", ".", "mask_id", "=", "self", ".", "vocab", "[", "\"[MASK]\"", "]", "\n", "\n", "self", ".", "current_train_example", "=", "-", "1", "\n", "self", ".", "num_train_examples", "=", "-", "1", "\n", "self", ".", "current_train_epoch", "=", "-", "1", "\n", "\n", "self", ".", "train_examples", "=", "None", "\n", "self", ".", "num_examples", "=", "{", "'train'", ":", "-", "1", ",", "'predict'", ":", "-", "1", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.DataProcessor.get_train_progress": [[549, 552], ["None"], "methods", ["None"], ["", "def", "get_train_progress", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets progress for training phase.\"\"\"", "\n", "return", "self", ".", "current_train_example", ",", "self", ".", "current_train_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.DataProcessor.get_examples": [[553, 562], ["mrqa_infer.read_mrqa_examples"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.read_mrqa_examples"], ["", "def", "get_examples", "(", "self", ",", "\n", "data_path", ",", "\n", "is_training", ",", "\n", "with_negative", "=", "False", ")", ":", "\n", "        ", "examples", "=", "read_mrqa_examples", "(", "\n", "input_file", "=", "data_path", ",", "\n", "is_training", "=", "is_training", ",", "\n", "with_negative", "=", "with_negative", ")", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.DataProcessor.get_num_examples": [[563, 568], ["ValueError"], "methods", ["None"], ["", "def", "get_num_examples", "(", "self", ",", "phase", ")", ":", "\n", "        ", "if", "phase", "not", "in", "[", "'train'", ",", "'predict'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unknown phase, which should be in ['train', 'predict'].\"", ")", "\n", "", "return", "self", ".", "num_examples", "[", "phase", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.DataProcessor.estimate_runtime_examples": [[569, 575], ["mrqa_infer.DataProcessor.estimate_runtime_examples"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.estimate_runtime_examples"], ["", "def", "estimate_runtime_examples", "(", "self", ",", "data_path", ",", "sample_rate", "=", "0.01", ",", "remove_impossible_questions", "=", "True", ",", "filter_invalid_spans", "=", "True", ")", ":", "\n", "        ", "\"\"\"Noted that this API Only support for Training phase.\"\"\"", "\n", "return", "estimate_runtime_examples", "(", "data_path", ",", "sample_rate", ",", "self", ".", "_tokenizer", ",", "self", ".", "_max_seq_length", ",", "self", ".", "_doc_stride", ",", "self", ".", "_max_query_length", ",", "remove_impossible_questions", "=", "True", ",", "filter_invalid_spans", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.DataProcessor.get_features": [[576, 587], ["mrqa_infer.convert_examples_to_features"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.convert_examples_to_features"], ["", "def", "get_features", "(", "self", ",", "examples", ",", "is_training", ",", "examples_start_id", ",", "features_start_id", ")", ":", "\n", "        ", "features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "examples", ",", "\n", "tokenizer", "=", "self", ".", "_tokenizer", ",", "\n", "max_seq_length", "=", "self", ".", "_max_seq_length", ",", "\n", "doc_stride", "=", "self", ".", "_doc_stride", ",", "\n", "max_query_length", "=", "self", ".", "_max_query_length", ",", "\n", "examples_start_id", "=", "examples_start_id", ",", "\n", "features_start_id", "=", "features_start_id", ",", "\n", "is_training", "=", "is_training", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.DataProcessor.data_generator": [[588, 651], ["mrqa_infer.read_mrqa_examples", "mrqa_infer.DataProcessor.get_features", "mrqa_infer.DataProcessor.data_generator.batch_reader"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.read_mrqa_examples", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_features", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.batch_reader"], ["", "def", "data_generator", "(", "self", ",", "\n", "raw_samples", ",", "\n", "batch_size", ",", "\n", "max_len", "=", "None", ",", "\n", "phase", "=", "'predict'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "with_negative", "=", "False", ",", "\n", "epoch", "=", "1", ",", "\n", "examples_start_id", "=", "0", ",", "\n", "features_start_id", "=", "1000000000", ")", ":", "\n", "        ", "examples", "=", "read_mrqa_examples", "(", "raw_samples", ")", "\n", "\n", "def", "batch_reader", "(", "features", ",", "batch_size", ",", "in_tokens", ")", ":", "\n", "            ", "batch", ",", "total_token_num", ",", "max_len", "=", "[", "]", ",", "0", ",", "0", "\n", "for", "(", "index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "                ", "if", "phase", "==", "'train'", ":", "\n", "                    ", "self", ".", "current_train_example", "=", "index", "+", "1", "\n", "", "seq_len", "=", "len", "(", "feature", ".", "input_ids", ")", "\n", "labels", "=", "[", "feature", ".", "unique_id", "\n", "]", "if", "feature", ".", "start_position", "is", "None", "else", "[", "\n", "feature", ".", "start_position", ",", "feature", ".", "end_position", "\n", "]", "\n", "example", "=", "[", "\n", "feature", ".", "input_ids", ",", "feature", ".", "segment_ids", ",", "range", "(", "seq_len", ")", "\n", "]", "+", "labels", "\n", "max_len", "=", "max", "(", "max_len", ",", "seq_len", ")", "\n", "\n", "#max_len = max(max_len, len(token_ids))", "\n", "if", "in_tokens", ":", "\n", "                    ", "to_append", "=", "(", "len", "(", "batch", ")", "+", "1", ")", "*", "max_len", "<=", "batch_size", "\n", "", "else", ":", "\n", "                    ", "to_append", "=", "len", "(", "batch", ")", "<", "batch_size", "\n", "\n", "", "if", "to_append", ":", "\n", "                    ", "batch", ".", "append", "(", "example", ")", "\n", "total_token_num", "+=", "seq_len", "\n", "", "else", ":", "\n", "                    ", "yield", "batch", ",", "total_token_num", "\n", "batch", ",", "total_token_num", ",", "max_len", "=", "[", "example", "\n", "]", ",", "seq_len", ",", "seq_len", "\n", "", "", "if", "len", "(", "batch", ")", ">", "0", ":", "\n", "                ", "yield", "batch", ",", "total_token_num", "\n", "\n", "", "", "features", "=", "self", ".", "get_features", "(", "examples", ",", "is_training", "=", "False", ",", "examples_start_id", "=", "examples_start_id", ",", "features_start_id", "=", "features_start_id", ")", "\n", "\n", "all_dev_batches", "=", "[", "]", "\n", "for", "batch_data", ",", "total_token_num", "in", "batch_reader", "(", "\n", "features", ",", "batch_size", ",", "self", ".", "_in_tokens", ")", ":", "\n", "            ", "batch_data", "=", "prepare_batch_data", "(", "\n", "batch_data", ",", "\n", "total_token_num", ",", "\n", "max_len", "=", "max_len", ",", "\n", "voc_size", "=", "-", "1", ",", "\n", "pad_id", "=", "self", ".", "pad_id", ",", "\n", "cls_id", "=", "self", ".", "cls_id", ",", "\n", "sep_id", "=", "self", ".", "sep_id", ",", "\n", "mask_id", "=", "-", "1", ",", "\n", "return_input_mask", "=", "True", ",", "\n", "return_max_len", "=", "False", ",", "\n", "return_num_token", "=", "False", ")", "\n", "all_dev_batches", ".", "append", "(", "batch_data", ")", "\n", "", "return", "examples", ",", "features", ",", "all_dev_batches", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.read_mrqa_examples": [[98, 139], ["re.sub", "mrqa_infer.read_mrqa_examples.is_whitespace"], "function", ["None"], ["", "", "def", "read_mrqa_examples", "(", "sample", ",", "is_training", "=", "False", ",", "with_negative", "=", "False", ")", ":", "\n", "    ", "\"\"\"Read a MRQA json file into a list of MRQAExample.\"\"\"", "\n", "\n", "def", "is_whitespace", "(", "c", ")", ":", "\n", "        ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "examples", "=", "[", "]", "\n", "# sample = json.loads(raw_sample)", "\n", "paragraph_text", "=", "sample", "[", "\"context\"", "]", "\n", "paragraph_text", "=", "re", ".", "sub", "(", "r'\\[TLE\\]|\\[DOC\\]|\\[PAR\\]'", ",", "'[SEP]'", ",", "paragraph_text", ")", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "        ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "            ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "            ", "if", "prev_is_whitespace", ":", "\n", "                ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "", "for", "qa", "in", "sample", "[", "\"qas\"", "]", ":", "\n", "        ", "qas_id", "=", "qa", "[", "\"qid\"", "]", "\n", "question_text", "=", "qa", "[", "\"question\"", "]", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "is_impossible", "=", "False", "\n", "\n", "example", "=", "MRQAExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.convert_examples_to_features": [[141, 295], ["tokenizer.tokenize", "enumerate", "collections.namedtuple", "enumerate", "len", "orig_to_tok_index.append", "tokenizer.tokenize", "mrqa_infer._improve_answer_span", "len", "doc_spans.append", "min", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "range", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "mrqa_infer.InputFeatures", "features.append", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "len", "len", "collections.namedtuple.", "len", "tokens.append", "segment_ids.append", "mrqa_infer._check_is_max_context", "tokens.append", "segment_ids.append", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._improve_answer_span", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._check_is_max_context"], ["", "def", "convert_examples_to_features", "(", "\n", "examples", ",", "\n", "tokenizer", ",", "\n", "max_seq_length", ",", "\n", "doc_stride", ",", "\n", "max_query_length", ",", "\n", "is_training", ",", "\n", "examples_start_id", "=", "0", ",", "\n", "features_start_id", "=", "1000000000", "\n", "#output_fn", "\n", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "unique_id", "=", "features_start_id", "\n", "example_index", "=", "examples_start_id", "\n", "\n", "features", "=", "[", "]", "\n", "for", "example", "in", "examples", ":", "\n", "        ", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "\n", "if", "len", "(", "query_tokens", ")", ">", "max_query_length", ":", "\n", "            ", "query_tokens", "=", "query_tokens", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "tok_to_orig_index", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "all_doc_tokens", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "doc_tokens", ")", ":", "\n", "            ", "orig_to_tok_index", ".", "append", "(", "len", "(", "all_doc_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "for", "sub_token", "in", "sub_tokens", ":", "\n", "                ", "tok_to_orig_index", ".", "append", "(", "i", ")", "\n", "all_doc_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "tok_start_position", "=", "None", "\n", "tok_end_position", "=", "None", "\n", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "            ", "tok_start_position", "=", "-", "1", "\n", "tok_end_position", "=", "-", "1", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "            ", "tok_start_position", "=", "orig_to_tok_index", "[", "example", ".", "start_position", "]", "\n", "if", "example", ".", "end_position", "<", "len", "(", "example", ".", "doc_tokens", ")", "-", "1", ":", "\n", "                ", "tok_end_position", "=", "orig_to_tok_index", "[", "example", ".", "end_position", "+", "\n", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "                ", "tok_end_position", "=", "len", "(", "all_doc_tokens", ")", "-", "1", "\n", "", "(", "tok_start_position", ",", "tok_end_position", ")", "=", "_improve_answer_span", "(", "\n", "all_doc_tokens", ",", "tok_start_position", ",", "tok_end_position", ",", "tokenizer", ",", "\n", "example", ".", "orig_answer_text", ")", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "", "max_tokens_for_doc", "=", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "# We can have documents that are longer than the maximum sequence length.", "\n", "# To deal with this we do a sliding window approach, where we take chunks", "\n", "# of the up to our max length with a stride of `doc_stride`.", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "            ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "                ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "                ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "            ", "tokens", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "token_is_max_context", "=", "{", "}", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "query_tokens", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "                ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "tok_to_orig_index", "[", "\n", "split_token_index", "]", "\n", "\n", "is_max_context", "=", "_check_is_max_context", "(", "\n", "doc_spans", ",", "doc_span_index", ",", "split_token_index", ")", "\n", "token_is_max_context", "[", "len", "(", "tokens", ")", "]", "=", "is_max_context", "\n", "tokens", ".", "append", "(", "all_doc_tokens", "[", "split_token_index", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "#while len(input_ids) < max_seq_length:", "\n", "#  input_ids.append(0)", "\n", "#  input_mask.append(0)", "\n", "#  segment_ids.append(0)", "\n", "\n", "#assert len(input_ids) == max_seq_length", "\n", "#assert len(input_mask) == max_seq_length", "\n", "#assert len(segment_ids) == max_seq_length", "\n", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "                ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "out_of_span", "=", "False", "\n", "if", "not", "(", "tok_start_position", ">=", "doc_start", "and", "\n", "tok_end_position", "<=", "doc_end", ")", ":", "\n", "                    ", "out_of_span", "=", "True", "\n", "", "if", "out_of_span", ":", "\n", "                    ", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "continue", "\n", "", "else", ":", "\n", "                    ", "doc_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "start_position", "=", "tok_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "tok_end_position", "-", "doc_start", "+", "doc_offset", "\n", "\n", "", "", "\"\"\"\n            if is_training and example.is_impossible:\n                start_position = 0\n                end_position = 0\n            \"\"\"", "\n", "\n", "feature", "=", "InputFeatures", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "example_index", "=", "example_index", ",", "\n", "doc_span_index", "=", "doc_span_index", ",", "\n", "tokens", "=", "tokens", ",", "\n", "token_to_orig_map", "=", "token_to_orig_map", ",", "\n", "token_is_max_context", "=", "token_is_max_context", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "example", ".", "is_impossible", ")", "\n", "\n", "unique_id", "+=", "1", "\n", "features", ".", "append", "(", "feature", ")", "\n", "", "example_index", "+=", "1", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.estimate_runtime_examples": [[297, 448], ["print", "print", "int", "open", "len", "float", "tokenizer.tokenize", "enumerate", "mrqa_infer._improve_answer_span", "collections.namedtuple", "enumerate", "json.load", "len", "orig_to_tok_index.append", "tokenizer.tokenize", "len", "doc_spans.append", "min", "ord", "mrqa_infer.MRQAExample", "sampled_examples.append", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "len", "len", "len", "len", "collections.namedtuple.", "len", "len", "len", "random.random", "mrqa_infer.read_mrqa_examples.is_whitespace"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._improve_answer_span", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "estimate_runtime_examples", "(", "data_path", ",", "sample_rate", ",", "tokenizer", ",", "max_seq_length", ",", "doc_stride", ",", "max_query_length", ",", "remove_impossible_questions", "=", "True", ",", "filter_invalid_spans", "=", "True", ")", ":", "\n", "    ", "\"\"\"Count runtime examples which may differ from number of raw samples due to sliding window operation and etc.. This is useful to get correct warmup steps for training.\"\"\"", "\n", "\n", "assert", "sample_rate", ">", "0.0", "and", "sample_rate", "<=", "1.0", ",", "\"sample_rate must be set between 0.0~1.0\"", "\n", "\n", "print", "(", "\"loading data with json parser...\"", ")", "\n", "with", "open", "(", "data_path", ",", "\"r\"", ")", "as", "reader", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "\n", "", "num_raw_examples", "=", "0", "\n", "for", "entry", "in", "data", ":", "\n", "        ", "for", "paragraph", "in", "entry", "[", "\"paragraphs\"", "]", ":", "\n", "            ", "paragraph_text", "=", "paragraph", "[", "\"context\"", "]", "\n", "for", "qa", "in", "paragraph", "[", "\"qas\"", "]", ":", "\n", "                ", "num_raw_examples", "+=", "1", "\n", "", "", "", "print", "(", "\"num raw examples:{}\"", ".", "format", "(", "num_raw_examples", ")", ")", "\n", "\n", "def", "is_whitespace", "(", "c", ")", ":", "\n", "        ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "sampled_examples", "=", "[", "]", "\n", "for", "entry", "in", "data", ":", "\n", "        ", "for", "paragraph", "in", "entry", "[", "\"paragraphs\"", "]", ":", "\n", "            ", "doc_tokens", "=", "None", "\n", "for", "qa", "in", "paragraph", "[", "\"qas\"", "]", ":", "\n", "                ", "if", "random", ".", "random", "(", ")", ">", "sample_rate", "and", "sample_rate", "<", "1.0", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "doc_tokens", "is", "None", ":", "\n", "                    ", "paragraph_text", "=", "paragraph", "[", "\"context\"", "]", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "                        ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "                            ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "                            ", "if", "prev_is_whitespace", ":", "\n", "                                ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                                ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "", "", "assert", "len", "(", "qa", "[", "\"answers\"", "]", ")", "==", "1", ",", "\"For training, each question should have exactly 1 answer.\"", "\n", "\n", "qas_id", "=", "qa", "[", "\"id\"", "]", "\n", "question_text", "=", "qa", "[", "\"question\"", "]", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "is_impossible", "=", "False", "\n", "\n", "if", "(", "'is_impossible'", "in", "qa", ")", "and", "(", "qa", "[", "\"is_impossible\"", "]", ")", ":", "\n", "                    ", "if", "remove_impossible_questions", "or", "filter_invalid_spans", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "                        ", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "orig_answer_text", "=", "\"\"", "\n", "is_impossible", "=", "True", "\n", "", "", "else", ":", "\n", "                    ", "answer", "=", "qa", "[", "\"answers\"", "]", "[", "0", "]", "\n", "orig_answer_text", "=", "answer", "[", "\"text\"", "]", "\n", "answer_offset", "=", "answer", "[", "\"answer_start\"", "]", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "\n", "answer_length", "-", "1", "]", "\n", "\n", "# remove corrupt samples", "\n", "actual_text", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "start_position", ":", "(", "\n", "end_position", "+", "1", ")", "]", ")", "\n", "cleaned_answer_text", "=", "\" \"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "if", "actual_text", ".", "find", "(", "cleaned_answer_text", ")", "==", "-", "1", ":", "\n", "                        ", "print", "(", "\"Could not find answer: '%s' vs. '%s'\"", ",", "\n", "actual_text", ",", "cleaned_answer_text", ")", "\n", "continue", "\n", "\n", "", "", "example", "=", "MRQAExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "\n", "orig_answer_text", "=", "orig_answer_text", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "is_impossible", ")", "\n", "sampled_examples", ".", "append", "(", "example", ")", "\n", "\n", "\n", "", "", "", "runtime_sample_rate", "=", "len", "(", "sampled_examples", ")", "/", "float", "(", "num_raw_examples", ")", "\n", "# print(\"DEBUG-> runtime sampled examples: {}, sample rate: {}.\".format(len(sampled_examples), runtime_sample_rate))", "\n", "\n", "runtime_samp_cnt", "=", "0", "\n", "\n", "for", "example", "in", "sampled_examples", ":", "\n", "        ", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "\n", "if", "len", "(", "query_tokens", ")", ">", "max_query_length", ":", "\n", "            ", "query_tokens", "=", "query_tokens", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "tok_to_orig_index", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "all_doc_tokens", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "doc_tokens", ")", ":", "\n", "            ", "orig_to_tok_index", ".", "append", "(", "len", "(", "all_doc_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "for", "sub_token", "in", "sub_tokens", ":", "\n", "                ", "tok_to_orig_index", ".", "append", "(", "i", ")", "\n", "all_doc_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "tok_start_position", "=", "None", "\n", "tok_end_position", "=", "None", "\n", "\n", "tok_start_position", "=", "orig_to_tok_index", "[", "example", ".", "start_position", "]", "\n", "if", "example", ".", "end_position", "<", "len", "(", "example", ".", "doc_tokens", ")", "-", "1", ":", "\n", "            ", "tok_end_position", "=", "orig_to_tok_index", "[", "example", ".", "end_position", "+", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "            ", "tok_end_position", "=", "len", "(", "all_doc_tokens", ")", "-", "1", "\n", "", "(", "tok_start_position", ",", "tok_end_position", ")", "=", "_improve_answer_span", "(", "\n", "all_doc_tokens", ",", "tok_start_position", ",", "tok_end_position", ",", "tokenizer", ",", "\n", "example", ".", "orig_answer_text", ")", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "max_tokens_for_doc", "=", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "            ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "                ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "                ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "            ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "filter_invalid_spans", "and", "not", "(", "tok_start_position", ">=", "doc_start", "and", "tok_end_position", "<=", "doc_end", ")", ":", "\n", "                ", "continue", "\n", "", "runtime_samp_cnt", "+=", "1", "\n", "", "", "return", "int", "(", "runtime_samp_cnt", "/", "runtime_sample_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer._improve_answer_span": [[450, 485], ["range", "tokenizer.tokenize", "range"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "_improve_answer_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_answer_text", ")", ":", "\n", "    ", "\"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"", "\n", "\n", "# The MRQA annotations are character based. We first project them to", "\n", "# whitespace-tokenized words. But then after WordPiece tokenization, we can", "\n", "# often find a \"better match\". For example:", "\n", "#", "\n", "#   Question: What year was John Smith born?", "\n", "#   Context: The leader was John Smith (1895-1943).", "\n", "#   Answer: 1895", "\n", "#", "\n", "# The original whitespace-tokenized answer will be \"(1895-1943).\". However", "\n", "# after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match", "\n", "# the exact answer, 1895.", "\n", "#", "\n", "# However, this is not always possible. Consider the following:", "\n", "#", "\n", "#   Question: What country is the top exporter of electornics?", "\n", "#   Context: The Japanese electronics industry is the lagest in the world.", "\n", "#   Answer: Japan", "\n", "#", "\n", "# In this case, the annotator chose \"Japan\" as a character sub-span of", "\n", "# the word \"Japanese\". Since our WordPiece tokenizer does not split", "\n", "# \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare", "\n", "# in MRQA, but does happen.", "\n", "tok_answer_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_answer_text", ")", ")", "\n", "\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "        ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "text_span", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_answer_text", ":", "\n", "                ", "return", "(", "new_start", ",", "new_end", ")", "\n", "\n", "", "", "", "return", "(", "input_start", ",", "input_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer._check_is_max_context": [[487, 523], ["enumerate", "min"], "function", ["None"], ["", "def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "    ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "        ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "            ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "            ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "\n", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "            ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.get_answers": [[653, 803], ["collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "example_index_to_features[].append", "enumerate", "sorted", "collections.namedtuple", "mrqa_infer._compute_softmax", "enumerate", "mrqa_infer._get_best_indexes", "mrqa_infer._get_best_indexes", "nbest.append", "nbest.append", "len", "total_scores.append", "print", "collections.OrderedDict", "nbest_json.append", "len", "len", "tok_text.strip.replace", "tok_text.strip.replace", "tok_text.strip.strip", "mrqa_infer.get_final_text", "collections.namedtuple.", "collections.namedtuple.", "sorted.append", "tok_text.strip.split", "len", "len", "feature.token_is_max_context.get", "collections.namedtuple."], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._compute_softmax", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.get_final_text"], ["", "", "def", "get_answers", "(", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "do_lower_case", ",", "\n", "verbose", "=", "False", ")", ":", "\n", "    ", "\"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"", "\n", "\n", "example_index_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "        ", "example_index_to_features", "[", "feature", ".", "example_index", "]", ".", "append", "(", "feature", ")", "\n", "\n", "", "unique_id_to_result", "=", "{", "}", "\n", "for", "result", "in", "all_results", ":", "\n", "        ", "unique_id_to_result", "[", "result", ".", "unique_id", "]", "=", "result", "\n", "\n", "", "_PrelimPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"PrelimPrediction\"", ",", "[", "\n", "\"feature_index\"", ",", "\"start_index\"", ",", "\"end_index\"", ",", "\"start_logit\"", ",", "\n", "\"end_logit\"", "\n", "]", ")", "\n", "\n", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_nbest_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "scores_diff_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "all_examples", ")", ":", "\n", "        ", "features", "=", "example_index_to_features", "[", "example_index", "]", "\n", "\n", "prelim_predictions", "=", "[", "]", "\n", "# keep track of the minimum score of null start+end of position 0", "\n", "score_null", "=", "1000000", "# large and positive", "\n", "min_null_feature_index", "=", "0", "# the paragraph slice with min mull score", "\n", "null_start_logit", "=", "0", "# the start logit at the slice with min null score", "\n", "null_end_logit", "=", "0", "# the end logit at the slice with min null score", "\n", "for", "(", "feature_index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "            ", "result", "=", "unique_id_to_result", "[", "feature", ".", "unique_id", "]", "\n", "start_indexes", "=", "_get_best_indexes", "(", "result", ".", "start_logits", ",", "n_best_size", ")", "\n", "end_indexes", "=", "_get_best_indexes", "(", "result", ".", "end_logits", ",", "n_best_size", ")", "\n", "# if we could have irrelevant answers, get the min score of irrelevant", "\n", "for", "start_index", "in", "start_indexes", ":", "\n", "                ", "for", "end_index", "in", "end_indexes", ":", "\n", "# We could hypothetically create invalid predictions, e.g., predict", "\n", "# that the start of the span is in the question. We throw out all", "\n", "# invalid predictions.", "\n", "                    ", "if", "start_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "start_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                        ", "continue", "\n", "", "if", "not", "feature", ".", "token_is_max_context", ".", "get", "(", "start_index", ",", "False", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "                        ", "continue", "\n", "", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "if", "length", ">", "max_answer_length", ":", "\n", "                        ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "feature_index", ",", "\n", "start_index", "=", "start_index", ",", "\n", "end_index", "=", "end_index", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "start_index", "]", ",", "\n", "end_logit", "=", "result", ".", "end_logits", "[", "end_index", "]", ")", ")", "\n", "\n", "", "", "", "prelim_predictions", "=", "sorted", "(", "\n", "prelim_predictions", ",", "\n", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_logit", "+", "x", ".", "end_logit", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "_NbestPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"NbestPrediction\"", ",", "[", "\"text\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "seen_predictions", "=", "{", "}", "\n", "nbest", "=", "[", "]", "\n", "for", "pred", "in", "prelim_predictions", ":", "\n", "            ", "if", "len", "(", "nbest", ")", ">=", "n_best_size", ":", "\n", "                ", "break", "\n", "", "feature", "=", "features", "[", "pred", ".", "feature_index", "]", "\n", "if", "pred", ".", "start_index", ">", "0", ":", "# this is a non-null prediction", "\n", "                ", "tok_tokens", "=", "feature", ".", "tokens", "[", "pred", ".", "start_index", ":", "(", "pred", ".", "end_index", "+", "1", "\n", ")", "]", "\n", "orig_doc_start", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "start_index", "]", "\n", "orig_doc_end", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "end_index", "]", "\n", "orig_tokens", "=", "example", ".", "doc_tokens", "[", "orig_doc_start", ":", "(", "orig_doc_end", "+", "\n", "1", ")", "]", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_tokens", ")", "\n", "\n", "# De-tokenize WordPieces that have been split off.", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\"##\"", ",", "\"\"", ")", "\n", "\n", "# Clean whitespace", "\n", "tok_text", "=", "tok_text", ".", "strip", "(", ")", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_text", ".", "split", "(", ")", ")", "\n", "orig_text", "=", "\" \"", ".", "join", "(", "orig_tokens", ")", "\n", "\n", "final_text", "=", "get_final_text", "(", "tok_text", ",", "orig_text", ",", "do_lower_case", ",", "\n", "verbose", ")", "\n", "if", "final_text", "in", "seen_predictions", ":", "\n", "                    ", "continue", "\n", "\n", "", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "", "else", ":", "\n", "                ", "final_text", "=", "\"\"", "\n", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "\n", "", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "final_text", ",", "\n", "start_logit", "=", "pred", ".", "start_logit", ",", "\n", "end_logit", "=", "pred", ".", "end_logit", ")", ")", "\n", "\n", "# In very rare edge cases we could have no valid predictions. So we", "\n", "# just create a nonce prediction in this case to avoid failure.", "\n", "", "if", "not", "nbest", ":", "\n", "            ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "\"empty\"", ",", "start_logit", "=", "0.0", ",", "end_logit", "=", "0.0", ")", ")", "\n", "\n", "", "assert", "len", "(", "nbest", ")", ">=", "1", "\n", "\n", "total_scores", "=", "[", "]", "\n", "best_non_null_entry", "=", "None", "\n", "for", "entry", "in", "nbest", ":", "\n", "            ", "total_scores", ".", "append", "(", "entry", ".", "start_logit", "+", "entry", ".", "end_logit", ")", "\n", "if", "not", "best_non_null_entry", ":", "\n", "                ", "if", "entry", ".", "text", ":", "\n", "                    ", "best_non_null_entry", "=", "entry", "\n", "# debug", "\n", "", "", "", "if", "best_non_null_entry", "is", "None", ":", "\n", "            ", "print", "(", "\"Emmm..., sth wrong\"", ")", "\n", "\n", "", "probs", "=", "_compute_softmax", "(", "total_scores", ")", "\n", "\n", "nbest_json", "=", "[", "]", "\n", "for", "(", "i", ",", "entry", ")", "in", "enumerate", "(", "nbest", ")", ":", "\n", "            ", "output", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "output", "[", "\"text\"", "]", "=", "entry", ".", "text", "\n", "output", "[", "\"probability\"", "]", "=", "probs", "[", "i", "]", "\n", "output", "[", "\"start_logit\"", "]", "=", "entry", ".", "start_logit", "\n", "output", "[", "\"end_logit\"", "]", "=", "entry", ".", "end_logit", "\n", "nbest_json", ".", "append", "(", "output", ")", "\n", "\n", "", "assert", "len", "(", "nbest_json", ")", ">=", "1", "\n", "\n", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "[", "0", "]", "[", "\"text\"", "]", "\n", "all_nbest_json", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "\n", "\n", "", "return", "all_predictions", ",", "all_nbest_json", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer.get_final_text": [[805, 898], ["task_reader.tokenization.BasicTokenizer", "tok_text.find", "mrqa_infer.get_final_text._strip_spaces"], "function", ["None"], ["", "def", "get_final_text", "(", "pred_text", ",", "orig_text", ",", "do_lower_case", ",", "verbose", ")", ":", "\n", "    ", "\"\"\"Project the tokenized prediction back to the original text.\"\"\"", "\n", "\n", "# When we created the data, we kept track of the alignment between original", "\n", "# (whitespace tokenized) tokens and our WordPiece tokenized tokens. So", "\n", "# now `orig_text` contains the span of our original text corresponding to the", "\n", "# span that we predicted.", "\n", "#", "\n", "# However, `orig_text` may contain extra characters that we don't want in", "\n", "# our prediction.", "\n", "#", "\n", "# For example, let's say:", "\n", "#   pred_text = steve smith", "\n", "#   orig_text = Steve Smith's", "\n", "#", "\n", "# We don't want to return `orig_text` because it contains the extra \"'s\".", "\n", "#", "\n", "# We don't want to return `pred_text` because it's already been normalized", "\n", "# (the MRQA eval script also does punctuation stripping/lower casing but", "\n", "# our tokenizer does additional normalization like stripping accent", "\n", "# characters).", "\n", "#", "\n", "# What we really want to return is \"Steve Smith\".", "\n", "#", "\n", "# Therefore, we have to apply a semi-complicated alignment heruistic between", "\n", "# `pred_text` and `orig_text` to get a character-to-charcter alignment. This", "\n", "# can fail in certain cases in which case we just return `orig_text`.", "\n", "\n", "def", "_strip_spaces", "(", "text", ")", ":", "\n", "        ", "ns_chars", "=", "[", "]", "\n", "ns_to_s_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "i", ",", "c", ")", "in", "enumerate", "(", "text", ")", ":", "\n", "            ", "if", "c", "==", "\" \"", ":", "\n", "                ", "continue", "\n", "", "ns_to_s_map", "[", "len", "(", "ns_chars", ")", "]", "=", "i", "\n", "ns_chars", ".", "append", "(", "c", ")", "\n", "", "ns_text", "=", "\"\"", ".", "join", "(", "ns_chars", ")", "\n", "return", "(", "ns_text", ",", "ns_to_s_map", ")", "\n", "\n", "# We first tokenize `orig_text`, strip whitespace from the result", "\n", "# and `pred_text`, and check if they are the same length. If they are", "\n", "# NOT the same length, the heuristic has failed. If they are the same", "\n", "# length, we assume the characters are one-to-one aligned.", "\n", "", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_text", ")", ")", "\n", "\n", "start_position", "=", "tok_text", ".", "find", "(", "pred_text", ")", "\n", "if", "start_position", "==", "-", "1", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "\"Unable to find text: '%s' in '%s'\"", "%", "(", "pred_text", ",", "orig_text", ")", ")", "\n", "", "return", "orig_text", "\n", "", "end_position", "=", "start_position", "+", "len", "(", "pred_text", ")", "-", "1", "\n", "\n", "(", "orig_ns_text", ",", "orig_ns_to_s_map", ")", "=", "_strip_spaces", "(", "orig_text", ")", "\n", "(", "tok_ns_text", ",", "tok_ns_to_s_map", ")", "=", "_strip_spaces", "(", "tok_text", ")", "\n", "\n", "if", "len", "(", "orig_ns_text", ")", "!=", "len", "(", "tok_ns_text", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "\"Length not equal after stripping spaces: '%s' vs '%s'\"", ",", "\n", "orig_ns_text", ",", "tok_ns_text", ")", "\n", "", "return", "orig_text", "\n", "\n", "# We then project the characters in `pred_text` back to `orig_text` using", "\n", "# the character-to-character alignment.", "\n", "", "tok_s_to_ns_map", "=", "{", "}", "\n", "for", "(", "i", ",", "tok_index", ")", "in", "six", ".", "iteritems", "(", "tok_ns_to_s_map", ")", ":", "\n", "        ", "tok_s_to_ns_map", "[", "tok_index", "]", "=", "i", "\n", "\n", "", "orig_start_position", "=", "None", "\n", "if", "start_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_start_position", "=", "tok_s_to_ns_map", "[", "start_position", "]", "\n", "if", "ns_start_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_start_position", "=", "orig_ns_to_s_map", "[", "ns_start_position", "]", "\n", "\n", "", "", "if", "orig_start_position", "is", "None", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "\"Couldn't map start position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "orig_end_position", "=", "None", "\n", "if", "end_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_end_position", "=", "tok_s_to_ns_map", "[", "end_position", "]", "\n", "if", "ns_end_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_end_position", "=", "orig_ns_to_s_map", "[", "ns_end_position", "]", "\n", "\n", "", "", "if", "orig_end_position", "is", "None", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "\"Couldn't map end position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "output_text", "=", "orig_text", "[", "orig_start_position", ":", "(", "orig_end_position", "+", "1", ")", "]", "\n", "return", "output_text", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer._get_best_indexes": [[900, 911], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "_get_best_indexes", "(", "logits", ",", "n_best_size", ")", ":", "\n", "    ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "\n", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "        ", "if", "i", ">=", "n_best_size", ":", "\n", "            ", "break", "\n", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.task_reader.mrqa_infer._compute_softmax": [[913, 934], ["math.exp", "exp_scores.append", "probs.append"], "function", ["None"], ["", "def", "_compute_softmax", "(", "scores", ")", ":", "\n", "    ", "\"\"\"Compute softmax probability over raw logits.\"\"\"", "\n", "if", "not", "scores", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "", "max_score", "=", "None", "\n", "for", "score", "in", "scores", ":", "\n", "        ", "if", "max_score", "is", "None", "or", "score", ">", "max_score", ":", "\n", "            ", "max_score", "=", "score", "\n", "\n", "", "", "exp_scores", "=", "[", "]", "\n", "total_sum", "=", "0.0", "\n", "for", "score", "in", "scores", ":", "\n", "        ", "x", "=", "math", ".", "exp", "(", "score", "-", "max_score", ")", "\n", "exp_scores", ".", "append", "(", "x", ")", "\n", "total_sum", "+=", "x", "\n", "\n", "", "probs", "=", "[", "]", "\n", "for", "score", "in", "exp_scores", ":", "\n", "        ", "probs", ".", "append", "(", "score", "/", "total_sum", ")", "\n", "", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.placeholder.Placeholder.__init__": [[22, 36], ["placeholder.Placeholder.append_placeholder", "len", "len"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.toolkit.placeholder.Placeholder.append_placeholder"], ["", "def", "__init__", "(", "self", ",", "input_shapes", ")", ":", "\n", "\n", "        ", "self", ".", "shapes", "=", "[", "]", "\n", "self", ".", "dtypes", "=", "[", "]", "\n", "self", ".", "lod_levels", "=", "[", "]", "\n", "self", ".", "names", "=", "[", "]", "\n", "\n", "for", "new_holder", "in", "input_shapes", ":", "\n", "            ", "shape", "=", "new_holder", "[", "0", "]", "\n", "dtype", "=", "new_holder", "[", "1", "]", "\n", "lod_level", "=", "new_holder", "[", "2", "]", "if", "len", "(", "new_holder", ")", ">=", "3", "else", "0", "\n", "name", "=", "new_holder", "[", "3", "]", "if", "len", "(", "new_holder", ")", ">=", "4", "else", "\"\"", "\n", "\n", "self", ".", "append_placeholder", "(", "shape", ",", "dtype", ",", "lod_level", "=", "lod_level", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.placeholder.Placeholder.append_placeholder": [[37, 42], ["placeholder.Placeholder.shapes.append", "placeholder.Placeholder.dtypes.append", "placeholder.Placeholder.lod_levels.append", "placeholder.Placeholder.names.append"], "methods", ["None"], ["", "", "def", "append_placeholder", "(", "self", ",", "shape", ",", "dtype", ",", "lod_level", "=", "0", ",", "name", "=", "\"\"", ")", ":", "\n", "        ", "self", ".", "shapes", ".", "append", "(", "shape", ")", "\n", "self", ".", "dtypes", ".", "append", "(", "dtype", ")", "\n", "self", ".", "lod_levels", ".", "append", "(", "lod_level", ")", "\n", "self", ".", "names", ".", "append", "(", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.placeholder.Placeholder.build": [[44, 54], ["paddle.layers.py_reader", "paddle.layers.read_file"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.read_file"], ["", "def", "build", "(", "self", ",", "capacity", ",", "reader_name", ",", "use_double_buffer", "=", "False", ")", ":", "\n", "        ", "pyreader", "=", "fluid", ".", "layers", ".", "py_reader", "(", "\n", "capacity", "=", "capacity", ",", "\n", "shapes", "=", "self", ".", "shapes", ",", "\n", "dtypes", "=", "self", ".", "dtypes", ",", "\n", "lod_levels", "=", "self", ".", "lod_levels", ",", "\n", "name", "=", "reader_name", ",", "\n", "use_double_buffer", "=", "use_double_buffer", ")", "\n", "\n", "return", "[", "pyreader", ",", "fluid", ".", "layers", ".", "read_file", "(", "pyreader", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.placeholder.Placeholder.__add__": [[56, 66], ["placeholder.Placeholder.append_placeholder", "isinstance", "isinstance", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.toolkit.placeholder.Placeholder.append_placeholder"], ["", "def", "__add__", "(", "self", ",", "new_holder", ")", ":", "\n", "        ", "assert", "isinstance", "(", "new_holder", ",", "tuple", ")", "or", "isinstance", "(", "new_holder", ",", "list", ")", "\n", "assert", "len", "(", "new_holder", ")", ">=", "2", "\n", "\n", "shape", "=", "new_holder", "[", "0", "]", "\n", "dtype", "=", "new_holder", "[", "1", "]", "\n", "lod_level", "=", "new_holder", "[", "2", "]", "if", "len", "(", "new_holder", ")", ">=", "3", "else", "0", "\n", "name", "=", "new_holder", "[", "3", "]", "if", "len", "(", "new_holder", ")", ">=", "4", "else", "\"\"", "\n", "\n", "self", ".", "append_placeholder", "(", "shape", ",", "dtype", ",", "lod_level", "=", "lod_level", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.init.cast_fp32_to_fp16": [[26, 38], ["print", "main_program.global_block().all_parameters", "main_program.global_block", "param.name.endswith", "paddle.global_scope().find_var().get_tensor", "numpy.array", "paddle.global_scope().find_var", "param.name.find", "fluid.global_scope().find_var().get_tensor.set", "fluid.global_scope().find_var.get_tensor().set", "paddle.global_scope().find_var", "numpy.float16().view", "paddle.global_scope", "fluid.global_scope().find_var.get_tensor", "paddle.global_scope", "numpy.float16"], "function", ["None"], ["def", "cast_fp32_to_fp16", "(", "exe", ",", "main_program", ")", ":", "\n", "    ", "print", "(", "\"Cast parameters to float16 data format.\"", ")", "\n", "for", "param", "in", "main_program", ".", "global_block", "(", ")", ".", "all_parameters", "(", ")", ":", "\n", "        ", "if", "not", "param", ".", "name", ".", "endswith", "(", "\".master\"", ")", ":", "\n", "            ", "param_t", "=", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "param", ".", "name", ")", ".", "get_tensor", "(", ")", "\n", "data", "=", "np", ".", "array", "(", "param_t", ")", "\n", "if", "param", ".", "name", ".", "find", "(", "\"layer_norm\"", ")", "==", "-", "1", ":", "\n", "                ", "param_t", ".", "set", "(", "np", ".", "float16", "(", "data", ")", ".", "view", "(", "np", ".", "uint16", ")", ",", "exe", ".", "place", ")", "\n", "", "master_param_var", "=", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "param", ".", "name", "+", "\n", "\".master\"", ")", "\n", "if", "master_param_var", "is", "not", "None", ":", "\n", "                ", "master_param_var", ".", "get_tensor", "(", ")", ".", "set", "(", "data", ",", "exe", ".", "place", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.init.init_checkpoint": [[40, 60], ["os.path.exists", "paddle.io.load_vars", "print", "os.path.exists", "init.cast_fp32_to_fp16", "paddle.io.is_persistable", "os.path.join"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.cast_fp32_to_fp16"], ["", "", "", "", "def", "init_checkpoint", "(", "exe", ",", "init_checkpoint_path", ",", "main_program", ",", "use_fp16", "=", "False", ",", "skip_list", "=", "[", "]", ")", ":", "\n", "    ", "assert", "os", ".", "path", ".", "exists", "(", "\n", "init_checkpoint_path", ")", ",", "\"[%s] cann't be found.\"", "%", "init_checkpoint_path", "\n", "\n", "def", "existed_persitables", "(", "var", ")", ":", "\n", "        ", "if", "not", "fluid", ".", "io", ".", "is_persistable", "(", "var", ")", ":", "\n", "            ", "return", "False", "\n", "", "if", "var", ".", "name", "in", "skip_list", ":", "\n", "            ", "return", "False", "\n", "", "return", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "init_checkpoint_path", ",", "var", ".", "name", ")", ")", "\n", "\n", "", "fluid", ".", "io", ".", "load_vars", "(", "\n", "exe", ",", "\n", "init_checkpoint_path", ",", "\n", "main_program", "=", "main_program", ",", "\n", "predicate", "=", "existed_persitables", ")", "\n", "print", "(", "\"Load model from {}\"", ".", "format", "(", "init_checkpoint_path", ")", ")", "\n", "\n", "if", "use_fp16", ":", "\n", "        ", "cast_fp32_to_fp16", "(", "exe", ",", "main_program", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.init.init_pretraining_params": [[62, 84], ["os.path.exists", "paddle.io.load_vars", "print", "os.path.exists", "init.cast_fp32_to_fp16", "isinstance", "os.path.join"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.cast_fp32_to_fp16"], ["", "", "def", "init_pretraining_params", "(", "exe", ",", "\n", "pretraining_params_path", ",", "\n", "main_program", ",", "\n", "use_fp16", "=", "False", ")", ":", "\n", "    ", "assert", "os", ".", "path", ".", "exists", "(", "pretraining_params_path", "\n", ")", ",", "\"[%s] cann't be found.\"", "%", "pretraining_params_path", "\n", "\n", "def", "existed_params", "(", "var", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "var", ",", "fluid", ".", "framework", ".", "Parameter", ")", ":", "\n", "            ", "return", "False", "\n", "", "return", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "pretraining_params_path", ",", "var", ".", "name", ")", ")", "\n", "\n", "", "fluid", ".", "io", ".", "load_vars", "(", "\n", "exe", ",", "\n", "pretraining_params_path", ",", "\n", "main_program", "=", "main_program", ",", "\n", "predicate", "=", "existed_params", ")", "\n", "print", "(", "\"Load pretraining parameters from {}.\"", ".", "format", "(", "\n", "pretraining_params_path", ")", ")", "\n", "\n", "if", "use_fp16", ":", "\n", "        ", "cast_fp32_to_fp16", "(", "exe", ",", "main_program", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.configure.JsonConfig.__init__": [[16, 18], ["configure.JsonConfig._parse"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig._parse"], ["    ", "def", "__init__", "(", "self", ",", "config_path", ")", ":", "\n", "        ", "self", ".", "_config_dict", "=", "self", ".", "_parse", "(", "config_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.configure.JsonConfig._parse": [[19, 28], ["open", "json.load", "IOError"], "methods", ["None"], ["", "def", "_parse", "(", "self", ",", "config_path", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "with", "open", "(", "config_path", ")", "as", "json_file", ":", "\n", "                ", "config_dict", "=", "json", ".", "load", "(", "json_file", ")", "\n", "", "", "except", ":", "\n", "            ", "raise", "IOError", "(", "\"Error in parsing bert model config file '%s'\"", "%", "\n", "config_path", ")", "\n", "", "else", ":", "\n", "            ", "return", "config_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.configure.JsonConfig.__getitem__": [[29, 31], ["None"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "self", ".", "_config_dict", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.configure.JsonConfig.print_config": [[32, 36], ["sorted", "print", "six.iteritems", "print"], "methods", ["None"], ["", "def", "print_config", "(", "self", ")", ":", "\n", "        ", "for", "arg", ",", "value", "in", "sorted", "(", "six", ".", "iteritems", "(", "self", ".", "_config_dict", ")", ")", ":", "\n", "            ", "print", "(", "'%s: %s'", "%", "(", "arg", ",", "value", ")", ")", "\n", "", "print", "(", "'------------------------------------------------'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.configure.ArgumentGroup.__init__": [[39, 41], ["parser.add_argument_group"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "parser", ",", "title", ",", "des", ")", ":", "\n", "        ", "self", ".", "_group", "=", "parser", ".", "add_argument_group", "(", "title", "=", "title", ",", "description", "=", "des", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.configure.ArgumentGroup.add_arg": [[42, 50], ["configure.ArgumentGroup._group.add_argument"], "methods", ["None"], ["", "def", "add_arg", "(", "self", ",", "name", ",", "type", ",", "default", ",", "help", ",", "**", "kwargs", ")", ":", "\n", "        ", "type", "=", "str2bool", "if", "type", "==", "bool", "else", "type", "\n", "self", ".", "_group", ".", "add_argument", "(", "\n", "\"--\"", "+", "name", ",", "\n", "default", "=", "default", ",", "\n", "type", "=", "type", ",", "\n", "help", "=", "help", "+", "' Default: %(default)s.'", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.configure.ArgConfig.__init__": [[53, 86], ["argparse.ArgumentParser", "configure.ArgumentGroup", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup.add_arg", "configure.ArgumentGroup"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg", "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "train_g", "=", "ArgumentGroup", "(", "parser", ",", "\"training\"", ",", "\"training options.\"", ")", "\n", "train_g", ".", "add_arg", "(", "\"epoch\"", ",", "int", ",", "3", ",", "\"Number of epoches for fine-tuning.\"", ")", "\n", "train_g", ".", "add_arg", "(", "\"learning_rate\"", ",", "float", ",", "5e-5", ",", "\"Learning rate used to train with warmup.\"", ")", "\n", "train_g", ".", "add_arg", "(", "\"lr_scheduler\"", ",", "str", ",", "\"linear_warmup_decay\"", ",", "\n", "\"scheduler of learning rate.\"", ",", "choices", "=", "[", "'linear_warmup_decay'", ",", "'noam_decay'", "]", ")", "\n", "train_g", ".", "add_arg", "(", "\"weight_decay\"", ",", "float", ",", "0.01", ",", "\"Weight decay rate for L2 regularizer.\"", ")", "\n", "train_g", ".", "add_arg", "(", "\"warmup_proportion\"", ",", "float", ",", "0.1", ",", "\n", "\"Proportion of training steps to perform linear learning rate warmup for.\"", ")", "\n", "train_g", ".", "add_arg", "(", "\"save_steps\"", ",", "int", ",", "1000", ",", "\"The steps interval to save checkpoints.\"", ")", "\n", "train_g", ".", "add_arg", "(", "\"use_fp16\"", ",", "bool", ",", "False", ",", "\"Whether to use fp16 mixed precision training.\"", ")", "\n", "train_g", ".", "add_arg", "(", "\"loss_scaling\"", ",", "float", ",", "1.0", ",", "\n", "\"Loss scaling factor for mixed precision training, only valid when use_fp16 is enabled.\"", ")", "\n", "train_g", ".", "add_arg", "(", "\"pred_dir\"", ",", "str", ",", "None", ",", "\"Path to save the prediction results\"", ")", "\n", "\n", "log_g", "=", "ArgumentGroup", "(", "parser", ",", "\"logging\"", ",", "\"logging related.\"", ")", "\n", "log_g", ".", "add_arg", "(", "\"skip_steps\"", ",", "int", ",", "10", ",", "\"The steps interval to print loss.\"", ")", "\n", "log_g", ".", "add_arg", "(", "\"verbose\"", ",", "bool", ",", "False", ",", "\"Whether to output verbose log.\"", ")", "\n", "\n", "run_type_g", "=", "ArgumentGroup", "(", "parser", ",", "\"run_type\"", ",", "\"running type options.\"", ")", "\n", "run_type_g", ".", "add_arg", "(", "\"use_cuda\"", ",", "bool", ",", "True", ",", "\"If set, use GPU for training.\"", ")", "\n", "run_type_g", ".", "add_arg", "(", "\"use_fast_executor\"", ",", "bool", ",", "False", ",", "\"If set, use fast parallel executor (in experiment).\"", ")", "\n", "run_type_g", ".", "add_arg", "(", "\"num_iteration_per_drop_scope\"", ",", "int", ",", "1", ",", "\"Ihe iteration intervals to clean up temporary variables.\"", ")", "\n", "run_type_g", ".", "add_arg", "(", "\"do_train\"", ",", "bool", ",", "True", ",", "\"Whether to perform training.\"", ")", "\n", "run_type_g", ".", "add_arg", "(", "\"do_predict\"", ",", "bool", ",", "True", ",", "\"Whether to perform prediction.\"", ")", "\n", "\n", "custom_g", "=", "ArgumentGroup", "(", "parser", ",", "\"customize\"", ",", "\"customized options.\"", ")", "\n", "\n", "self", ".", "custom_g", "=", "custom_g", "\n", "\n", "self", ".", "parser", "=", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.configure.ArgConfig.add_arg": [[87, 89], ["configure.ArgConfig.custom_g.add_arg"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg"], ["", "def", "add_arg", "(", "self", ",", "name", ",", "dtype", ",", "default", ",", "descrip", ")", ":", "\n", "        ", "self", ".", "custom_g", ".", "add_arg", "(", "name", ",", "dtype", ",", "default", ",", "descrip", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.configure.ArgConfig.build_conf": [[90, 92], ["configure.ArgConfig.parser.parse_args"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args"], ["", "def", "build_conf", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.configure.str2bool": [[94, 98], ["v.lower"], "function", ["None"], ["", "", "def", "str2bool", "(", "v", ")", ":", "\n", "# because argparse does not support to parse \"true, False\" as python", "\n", "# boolean directly", "\n", "    ", "return", "v", ".", "lower", "(", ")", "in", "(", "\"true\"", ",", "\"t\"", ",", "\"1\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.toolkit.configure.print_arguments": [[100, 111], ["print", "sorted", "print", "log.info", "sorted", "log.info", "six.iteritems", "print", "six.iteritems", "log.info", "vars", "vars"], "function", ["None"], ["", "def", "print_arguments", "(", "args", ",", "log", "=", "None", ")", ":", "\n", "    ", "if", "not", "log", ":", "\n", "        ", "print", "(", "'-----------  Configuration Arguments -----------'", ")", "\n", "for", "arg", ",", "value", "in", "sorted", "(", "six", ".", "iteritems", "(", "vars", "(", "args", ")", ")", ")", ":", "\n", "            ", "print", "(", "'%s: %s'", "%", "(", "arg", ",", "value", ")", ")", "\n", "", "print", "(", "'------------------------------------------------'", ")", "\n", "", "else", ":", "\n", "        ", "log", ".", "info", "(", "'-----------  Configuration Arguments -----------'", ")", "\n", "for", "arg", ",", "value", "in", "sorted", "(", "six", ".", "iteritems", "(", "vars", "(", "args", ")", ")", ")", ":", "\n", "            ", "log", ".", "info", "(", "'%s: %s'", "%", "(", "arg", ",", "value", ")", ")", "\n", "", "log", ".", "info", "(", "'------------------------------------------------'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.module.transformer_encoder.layer_norm": [[27, 53], ["paddle.fluid.layer_helper.LayerHelper", "paddle.reduce_mean", "paddle.elementwise_sub", "paddle.reduce_mean", "paddle.rsqrt", "paddle.elementwise_mul", "paddle.fluid.layer_helper.LayerHelper.create_parameter", "paddle.fluid.layer_helper.LayerHelper.create_parameter", "paddle.elementwise_mul", "paddle.elementwise_add", "paddle.square", "reduce", "locals", "paddle.initializer.Constant", "paddle.initializer.Constant"], "function", ["None"], ["def", "layer_norm", "(", "x", ",", "begin_norm_axis", "=", "1", ",", "epsilon", "=", "1e-6", ",", "param_attr", "=", "None", ",", "bias_attr", "=", "None", ")", ":", "\n", "    ", "helper", "=", "LayerHelper", "(", "'layer_norm'", ",", "**", "locals", "(", ")", ")", "\n", "mean", "=", "layers", ".", "reduce_mean", "(", "x", ",", "dim", "=", "begin_norm_axis", ",", "keep_dim", "=", "True", ")", "\n", "shift_x", "=", "layers", ".", "elementwise_sub", "(", "x", "=", "x", ",", "y", "=", "mean", ",", "axis", "=", "0", ")", "\n", "variance", "=", "layers", ".", "reduce_mean", "(", "layers", ".", "square", "(", "shift_x", ")", ",", "dim", "=", "begin_norm_axis", ",", "keep_dim", "=", "True", ")", "\n", "r_stdev", "=", "layers", ".", "rsqrt", "(", "variance", "+", "epsilon", ")", "\n", "norm_x", "=", "layers", ".", "elementwise_mul", "(", "x", "=", "shift_x", ",", "y", "=", "r_stdev", ",", "axis", "=", "0", ")", "\n", "\n", "param_shape", "=", "[", "reduce", "(", "lambda", "x", ",", "y", ":", "x", "*", "y", ",", "norm_x", ".", "shape", "[", "begin_norm_axis", ":", "]", ")", "]", "\n", "param_dtype", "=", "norm_x", ".", "dtype", "\n", "scale", "=", "helper", ".", "create_parameter", "(", "\n", "attr", "=", "param_attr", ",", "\n", "shape", "=", "param_shape", ",", "\n", "dtype", "=", "param_dtype", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "1.", ")", ")", "\n", "bias", "=", "helper", ".", "create_parameter", "(", "\n", "attr", "=", "bias_attr", ",", "\n", "shape", "=", "param_shape", ",", "\n", "dtype", "=", "param_dtype", ",", "\n", "is_bias", "=", "True", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", "\n", "\n", "out", "=", "layers", ".", "elementwise_mul", "(", "x", "=", "norm_x", ",", "y", "=", "scale", ",", "axis", "=", "-", "1", ")", "\n", "out", "=", "layers", ".", "elementwise_add", "(", "x", "=", "out", ",", "y", "=", "bias", ",", "axis", "=", "-", "1", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.module.transformer_encoder.multi_head_attention": [[54, 188], ["transformer_encoder.multi_head_attention.__compute_qkv"], "function", ["None"], ["", "def", "multi_head_attention", "(", "queries", ",", "\n", "keys", ",", "\n", "values", ",", "\n", "attn_bias", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "n_head", "=", "1", ",", "\n", "dropout_rate", "=", "0.", ",", "\n", "cache", "=", "None", ",", "\n", "param_initializer", "=", "None", ",", "\n", "name", "=", "'multi_head_att'", ")", ":", "\n", "    ", "\"\"\"\n    Multi-Head Attention. Note that attn_bias is added to the logit before\n    computing softmax activiation to mask certain selected positions so that\n    they will not considered in attention weights.\n    \"\"\"", "\n", "keys", "=", "queries", "if", "keys", "is", "None", "else", "keys", "\n", "values", "=", "keys", "if", "values", "is", "None", "else", "values", "\n", "\n", "if", "not", "(", "len", "(", "queries", ".", "shape", ")", "==", "len", "(", "keys", ".", "shape", ")", "==", "len", "(", "values", ".", "shape", ")", "==", "3", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Inputs: quries, keys and values should all be 3-D tensors.\"", ")", "\n", "\n", "", "def", "__compute_qkv", "(", "queries", ",", "keys", ",", "values", ",", "n_head", ",", "d_key", ",", "d_value", ")", ":", "\n", "        ", "\"\"\"\n        Add linear projection to queries, keys, and values.\n        \"\"\"", "\n", "q", "=", "layers", ".", "fc", "(", "input", "=", "queries", ",", "\n", "size", "=", "d_key", "*", "n_head", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_query_fc.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_query_fc.b_0'", ")", "\n", "k", "=", "layers", ".", "fc", "(", "input", "=", "keys", ",", "\n", "size", "=", "d_key", "*", "n_head", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_key_fc.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_key_fc.b_0'", ")", "\n", "v", "=", "layers", ".", "fc", "(", "input", "=", "values", ",", "\n", "size", "=", "d_value", "*", "n_head", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_value_fc.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_value_fc.b_0'", ")", "\n", "return", "q", ",", "k", ",", "v", "\n", "\n", "", "def", "__split_heads", "(", "x", ",", "n_head", ")", ":", "\n", "        ", "\"\"\"\n        Reshape the last dimension of inpunt tensor x so that it becomes two\n        dimensions and then transpose. Specifically, input a tensor with shape\n        [bs, max_sequence_length, n_head * hidden_dim] then output a tensor\n        with shape [bs, n_head, max_sequence_length, hidden_dim].\n        \"\"\"", "\n", "hidden_size", "=", "x", ".", "shape", "[", "-", "1", "]", "\n", "# The value 0 in shape attr means copying the corresponding dimension", "\n", "# size of the input as the output dimension size.", "\n", "reshaped", "=", "layers", ".", "reshape", "(", "\n", "x", "=", "x", ",", "shape", "=", "[", "0", ",", "0", ",", "n_head", ",", "hidden_size", "//", "n_head", "]", ",", "inplace", "=", "False", ")", "\n", "\n", "# permuate the dimensions into:", "\n", "# [batch_size, n_head, max_sequence_len, hidden_size_per_head]", "\n", "return", "layers", ".", "transpose", "(", "x", "=", "reshaped", ",", "perm", "=", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "", "def", "__combine_heads", "(", "x", ")", ":", "\n", "        ", "\"\"\"\n        Transpose and then reshape the last two dimensions of inpunt tensor x\n        so that it becomes one dimension, which is reverse to __split_heads.\n        \"\"\"", "\n", "if", "len", "(", "x", ".", "shape", ")", "==", "3", ":", "return", "x", "\n", "if", "len", "(", "x", ".", "shape", ")", "!=", "4", ":", "\n", "            ", "raise", "ValueError", "(", "\"Input(x) should be a 4-D Tensor.\"", ")", "\n", "\n", "", "trans_x", "=", "layers", ".", "transpose", "(", "x", ",", "perm", "=", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "# The value 0 in shape attr means copying the corresponding dimension", "\n", "# size of the input as the output dimension size.", "\n", "return", "layers", ".", "reshape", "(", "\n", "x", "=", "trans_x", ",", "\n", "shape", "=", "[", "0", ",", "0", ",", "trans_x", ".", "shape", "[", "2", "]", "*", "trans_x", ".", "shape", "[", "3", "]", "]", ",", "\n", "inplace", "=", "False", ")", "\n", "\n", "", "def", "scaled_dot_product_attention", "(", "q", ",", "k", ",", "v", ",", "attn_bias", ",", "d_key", ",", "dropout_rate", ")", ":", "\n", "        ", "\"\"\"\n        Scaled Dot-Product Attention\n        \"\"\"", "\n", "scaled_q", "=", "layers", ".", "scale", "(", "x", "=", "q", ",", "scale", "=", "d_key", "**", "-", "0.5", ")", "\n", "product", "=", "layers", ".", "matmul", "(", "x", "=", "scaled_q", ",", "y", "=", "k", ",", "transpose_y", "=", "True", ")", "\n", "if", "attn_bias", ":", "\n", "            ", "product", "+=", "attn_bias", "\n", "", "weights", "=", "layers", ".", "softmax", "(", "product", ")", "\n", "if", "dropout_rate", ":", "\n", "            ", "weights", "=", "layers", ".", "dropout", "(", "\n", "weights", ",", "\n", "dropout_prob", "=", "dropout_rate", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "\n", "is_test", "=", "False", ")", "\n", "", "out", "=", "layers", ".", "matmul", "(", "weights", ",", "v", ")", "\n", "return", "out", "\n", "\n", "", "q", ",", "k", ",", "v", "=", "__compute_qkv", "(", "queries", ",", "keys", ",", "values", ",", "n_head", ",", "d_key", ",", "d_value", ")", "\n", "\n", "if", "cache", "is", "not", "None", ":", "# use cache and concat time steps", "\n", "# Since the inplace reshape in __split_heads changes the shape of k and", "\n", "# v, which is the cache input for next time step, reshape the cache", "\n", "# input from the previous time step first.", "\n", "        ", "k", "=", "cache", "[", "\"k\"", "]", "=", "layers", ".", "concat", "(", "\n", "[", "layers", ".", "reshape", "(", "\n", "cache", "[", "\"k\"", "]", ",", "shape", "=", "[", "0", ",", "0", ",", "d_model", "]", ")", ",", "k", "]", ",", "axis", "=", "1", ")", "\n", "v", "=", "cache", "[", "\"v\"", "]", "=", "layers", ".", "concat", "(", "\n", "[", "layers", ".", "reshape", "(", "\n", "cache", "[", "\"v\"", "]", ",", "shape", "=", "[", "0", ",", "0", ",", "d_model", "]", ")", ",", "v", "]", ",", "axis", "=", "1", ")", "\n", "\n", "", "q", "=", "__split_heads", "(", "q", ",", "n_head", ")", "\n", "k", "=", "__split_heads", "(", "k", ",", "n_head", ")", "\n", "v", "=", "__split_heads", "(", "v", ",", "n_head", ")", "\n", "\n", "ctx_multiheads", "=", "scaled_dot_product_attention", "(", "q", ",", "k", ",", "v", ",", "attn_bias", ",", "d_key", ",", "\n", "dropout_rate", ")", "\n", "\n", "out", "=", "__combine_heads", "(", "ctx_multiheads", ")", "\n", "\n", "# Project back to the model size.", "\n", "proj_out", "=", "layers", ".", "fc", "(", "input", "=", "out", ",", "\n", "size", "=", "d_model", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_output_fc.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_output_fc.b_0'", ")", "\n", "return", "proj_out", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.module.transformer_encoder.positionwise_feed_forward": [[190, 225], ["paddle.fc", "paddle.fc", "paddle.dropout", "paddle.ParamAttr", "paddle.ParamAttr"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["", "def", "positionwise_feed_forward", "(", "x", ",", "\n", "d_inner_hid", ",", "\n", "d_hid", ",", "\n", "dropout_rate", ",", "\n", "hidden_act", ",", "\n", "param_initializer", "=", "None", ",", "\n", "name", "=", "'ffn'", ")", ":", "\n", "    ", "\"\"\"\n    Position-wise Feed-Forward Networks.\n    This module consists of two linear transformations with a ReLU activation\n    in between, which is applied to each position separately and identically.\n    \"\"\"", "\n", "hidden", "=", "layers", ".", "fc", "(", "input", "=", "x", ",", "\n", "size", "=", "d_inner_hid", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "act", "=", "hidden_act", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_fc_0.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_fc_0.b_0'", ")", "\n", "if", "dropout_rate", ":", "\n", "        ", "hidden", "=", "layers", ".", "dropout", "(", "\n", "hidden", ",", "\n", "dropout_prob", "=", "dropout_rate", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "\n", "is_test", "=", "False", ")", "\n", "\n", "", "out", "=", "layers", ".", "fc", "(", "input", "=", "hidden", ",", "\n", "size", "=", "d_hid", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_fc_1.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_fc_1.b_0'", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.module.transformer_encoder.pre_post_process_layer": [[227, 261], ["transformer_encoder.layer_norm", "paddle.cast", "paddle.cast", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.dropout", "len", "paddle.initializer.Constant", "paddle.initializer.Constant"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.layer_norm", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["", "def", "pre_post_process_layer", "(", "prev_out", ",", "out", ",", "process_cmd", ",", "dropout_rate", "=", "0.", ",", "\n", "name", "=", "''", ")", ":", "\n", "    ", "\"\"\"\n    Add residual connection, layer normalization and droput to the out tensor\n    optionally according to the value of process_cmd.\n    This will be used before or after multi-head attention and position-wise\n    feed-forward networks.\n    \"\"\"", "\n", "for", "cmd", "in", "process_cmd", ":", "\n", "        ", "if", "cmd", "==", "\"a\"", ":", "# add residual connection", "\n", "            ", "out", "=", "out", "+", "prev_out", "if", "prev_out", "else", "out", "\n", "", "elif", "cmd", "==", "\"n\"", ":", "# add layer normalization", "\n", "            ", "out_dtype", "=", "out", ".", "dtype", "\n", "if", "out_dtype", "==", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP16", ":", "\n", "                ", "out", "=", "layers", ".", "cast", "(", "x", "=", "out", ",", "dtype", "=", "\"float32\"", ")", "\n", "", "out", "=", "layer_norm", "(", "\n", "out", ",", "\n", "begin_norm_axis", "=", "len", "(", "out", ".", "shape", ")", "-", "1", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_layer_norm_scale'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "1.", ")", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_layer_norm_bias'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", ")", "\n", "if", "out_dtype", "==", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP16", ":", "\n", "                ", "out", "=", "layers", ".", "cast", "(", "x", "=", "out", ",", "dtype", "=", "\"float16\"", ")", "\n", "", "", "elif", "cmd", "==", "\"d\"", ":", "# add dropout", "\n", "            ", "if", "dropout_rate", ":", "\n", "                ", "out", "=", "layers", ".", "dropout", "(", "\n", "out", ",", "\n", "dropout_prob", "=", "dropout_rate", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "\n", "is_test", "=", "False", ")", "\n", "", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.module.transformer_encoder.encoder_layer": [[266, 327], ["transformer_encoder.multi_head_attention", "post_process_layer", "transformer_encoder.positionwise_feed_forward", "post_process_layer", "pre_process_layer", "pre_process_layer"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.multi_head_attention", "home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.positionwise_feed_forward"], ["def", "encoder_layer", "(", "enc_input", ",", "\n", "attn_bias", ",", "\n", "n_head", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "d_inner_hid", ",", "\n", "prepostprocess_dropout", ",", "\n", "attention_dropout", ",", "\n", "relu_dropout", ",", "\n", "hidden_act", ",", "\n", "preprocess_cmd", "=", "\"n\"", ",", "\n", "postprocess_cmd", "=", "\"da\"", ",", "\n", "param_initializer", "=", "None", ",", "\n", "name", "=", "''", ")", ":", "\n", "    ", "\"\"\"The encoder layers that can be stacked to form a deep encoder.\n    This module consits of a multi-head (self) attention followed by\n    position-wise feed-forward networks and both the two components companied\n    with the post_process_layer to add residual connection, layer normalization\n    and droput.\n    \"\"\"", "\n", "attn_output", "=", "multi_head_attention", "(", "\n", "pre_process_layer", "(", "\n", "enc_input", ",", "\n", "preprocess_cmd", ",", "\n", "prepostprocess_dropout", ",", "\n", "name", "=", "name", "+", "'_pre_att'", ")", ",", "\n", "None", ",", "\n", "None", ",", "\n", "attn_bias", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "n_head", ",", "\n", "attention_dropout", ",", "\n", "param_initializer", "=", "param_initializer", ",", "\n", "name", "=", "name", "+", "'_multi_head_att'", ")", "\n", "attn_output", "=", "post_process_layer", "(", "\n", "enc_input", ",", "\n", "attn_output", ",", "\n", "postprocess_cmd", ",", "\n", "prepostprocess_dropout", ",", "\n", "name", "=", "name", "+", "'_post_att'", ")", "\n", "ffd_output", "=", "positionwise_feed_forward", "(", "\n", "pre_process_layer", "(", "\n", "attn_output", ",", "\n", "preprocess_cmd", ",", "\n", "prepostprocess_dropout", ",", "\n", "name", "=", "name", "+", "'_pre_ffn'", ")", ",", "\n", "d_inner_hid", ",", "\n", "d_model", ",", "\n", "relu_dropout", ",", "\n", "hidden_act", ",", "\n", "param_initializer", "=", "param_initializer", ",", "\n", "name", "=", "name", "+", "'_ffn'", ")", "\n", "return", "post_process_layer", "(", "\n", "attn_output", ",", "\n", "ffd_output", ",", "\n", "postprocess_cmd", ",", "\n", "prepostprocess_dropout", ",", "\n", "name", "=", "name", "+", "'_post_ffn'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.module.transformer_encoder.encoder": [[329, 380], ["range", "pre_process_layer", "enc_outputs.append", "transformer_encoder.encoder_layer", "enc_outputs.append", "str"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.encoder_layer"], ["", "def", "encoder", "(", "enc_input", ",", "\n", "attn_bias", ",", "\n", "n_layer", ",", "\n", "n_head", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "d_inner_hid", ",", "\n", "prepostprocess_dropout", ",", "\n", "attention_dropout", ",", "\n", "relu_dropout", ",", "\n", "hidden_act", ",", "\n", "preprocess_cmd", "=", "\"n\"", ",", "\n", "postprocess_cmd", "=", "\"da\"", ",", "\n", "param_initializer", "=", "None", ",", "\n", "name", "=", "''", ",", "\n", "return_all", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    The encoder is composed of a stack of identical layers returned by calling\n    encoder_layer.\n    \"\"\"", "\n", "enc_outputs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_layer", ")", ":", "\n", "        ", "enc_output", "=", "encoder_layer", "(", "\n", "enc_input", ",", "\n", "attn_bias", ",", "\n", "n_head", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "d_inner_hid", ",", "\n", "prepostprocess_dropout", ",", "\n", "attention_dropout", ",", "\n", "relu_dropout", ",", "\n", "hidden_act", ",", "\n", "preprocess_cmd", ",", "\n", "postprocess_cmd", ",", "\n", "param_initializer", "=", "param_initializer", ",", "\n", "name", "=", "name", "+", "'_layer_'", "+", "str", "(", "i", ")", ")", "\n", "enc_input", "=", "enc_output", "\n", "if", "i", "<", "n_layer", "-", "1", ":", "\n", "            ", "enc_outputs", ".", "append", "(", "enc_output", ")", "\n", "\n", "", "", "enc_output", "=", "pre_process_layer", "(", "\n", "enc_output", ",", "preprocess_cmd", ",", "prepostprocess_dropout", ",", "name", "=", "\"post_encoder\"", ")", "\n", "enc_outputs", ".", "append", "(", "enc_output", ")", "\n", "\n", "if", "not", "return_all", ":", "\n", "        ", "return", "enc_output", "\n", "", "else", ":", "\n", "        ", "return", "enc_output", ",", "enc_outputs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.optimization.linear_warmup_decay": [[25, 51], ["paddle.default_main_program()._lr_schedule_guard", "paddle.layers.tensor.create_global_var", "paddle.layers.learning_rate_scheduler._decay_step_counter", "paddle.layers.control_flow.Switch", "paddle.default_main_program", "switch.case", "paddle.layers.tensor.assign", "switch.default", "paddle.layers.learning_rate_scheduler.polynomial_decay", "paddle.layers.tensor.assign"], "function", ["None"], ["def", "linear_warmup_decay", "(", "learning_rate", ",", "warmup_steps", ",", "num_train_steps", ")", ":", "\n", "    ", "\"\"\" Applies linear warmup of learning rate from 0 and decay to 0.\"\"\"", "\n", "with", "fluid", ".", "default_main_program", "(", ")", ".", "_lr_schedule_guard", "(", ")", ":", "\n", "        ", "lr", "=", "fluid", ".", "layers", ".", "tensor", ".", "create_global_var", "(", "\n", "shape", "=", "[", "1", "]", ",", "\n", "value", "=", "0.0", ",", "\n", "dtype", "=", "'float32'", ",", "\n", "persistable", "=", "True", ",", "\n", "name", "=", "\"scheduled_learning_rate\"", ")", "\n", "\n", "global_step", "=", "fluid", ".", "layers", ".", "learning_rate_scheduler", ".", "_decay_step_counter", "(", ")", "\n", "\n", "with", "fluid", ".", "layers", ".", "control_flow", ".", "Switch", "(", ")", "as", "switch", ":", "\n", "            ", "with", "switch", ".", "case", "(", "global_step", "<", "warmup_steps", ")", ":", "\n", "                ", "warmup_lr", "=", "learning_rate", "*", "(", "global_step", "/", "warmup_steps", ")", "\n", "fluid", ".", "layers", ".", "tensor", ".", "assign", "(", "warmup_lr", ",", "lr", ")", "\n", "", "with", "switch", ".", "default", "(", ")", ":", "\n", "                ", "decayed_lr", "=", "fluid", ".", "layers", ".", "learning_rate_scheduler", ".", "polynomial_decay", "(", "\n", "learning_rate", "=", "learning_rate", ",", "\n", "decay_steps", "=", "num_train_steps", ",", "\n", "end_learning_rate", "=", "0.0", ",", "\n", "power", "=", "1.0", ",", "\n", "cycle", "=", "False", ")", "\n", "fluid", ".", "layers", ".", "tensor", ".", "assign", "(", "decayed_lr", ",", "lr", ")", "\n", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.optimization.optimization": [[53, 140], ["paddle.clip.set_gradient_clip", "dict", "paddle.optimizer.Adam", "paddle.optimizer.Adam", "fluid.optimizer.Adam.backward", "pdnlp.extension.fp16.create_master_params_grads", "fluid.optimizer.Adam.apply_gradients", "pdnlp.extension.fp16.master_param_to_train_param", "train_program.global_block().all_parameters", "fluid.optimizer.Adam.minimize", "paddle.layers.learning_rate_scheduler.noam_decay", "paddle.clip.GradientClipByGlobalNorm", "name.find", "name.endswith", "optimization.linear_warmup_decay", "ValueError", "optimization.optimization.exclude_from_weight_decay"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.create_master_params_grads", "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.master_param_to_train_param", "home.repos.pwc.inspect_result.baidu_DuReader.src.optimization.linear_warmup_decay"], ["", "", "def", "optimization", "(", "loss", ",", "\n", "warmup_steps", ",", "\n", "num_train_steps", ",", "\n", "learning_rate", ",", "\n", "train_program", ",", "\n", "startup_prog", ",", "\n", "weight_decay", ",", "\n", "scheduler", "=", "'linear_warmup_decay'", ",", "\n", "use_fp16", "=", "False", ",", "\n", "loss_scaling", "=", "1.0", ")", ":", "\n", "    ", "if", "warmup_steps", ">", "0", ":", "\n", "        ", "if", "scheduler", "==", "'noam_decay'", ":", "\n", "            ", "scheduled_lr", "=", "fluid", ".", "layers", ".", "learning_rate_scheduler", ".", "noam_decay", "(", "1", "/", "(", "warmup_steps", "*", "(", "learning_rate", "**", "2", ")", ")", ",", "\n", "warmup_steps", ")", "\n", "", "elif", "scheduler", "==", "'linear_warmup_decay'", ":", "\n", "            ", "scheduled_lr", "=", "linear_warmup_decay", "(", "learning_rate", ",", "warmup_steps", ",", "\n", "num_train_steps", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unkown learning rate scheduler, should be \"", "\n", "\"'noam_decay' or 'linear_warmup_decay'\"", ")", "\n", "", "optimizer", "=", "fluid", ".", "optimizer", ".", "Adam", "(", "learning_rate", "=", "scheduled_lr", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "fluid", ".", "optimizer", ".", "Adam", "(", "learning_rate", "=", "learning_rate", ")", "\n", "scheduled_lr", "=", "learning_rate", "\n", "\n", "", "clip_norm_thres", "=", "1.0", "\n", "# When using mixed precision training, scale the gradient clip threshold", "\n", "# by loss_scaling", "\n", "if", "use_fp16", "and", "loss_scaling", ">", "1.0", ":", "\n", "        ", "clip_norm_thres", "*=", "loss_scaling", "\n", "", "fluid", ".", "clip", ".", "set_gradient_clip", "(", "\n", "clip", "=", "fluid", ".", "clip", ".", "GradientClipByGlobalNorm", "(", "clip_norm", "=", "clip_norm_thres", ")", ")", "\n", "\n", "def", "exclude_from_weight_decay", "(", "name", ")", ":", "\n", "        ", "if", "name", ".", "find", "(", "\"layer_norm\"", ")", ">", "-", "1", ":", "\n", "            ", "return", "True", "\n", "", "bias_suffix", "=", "[", "\"_bias\"", ",", "\"_b\"", ",", "\".b_0\"", "]", "\n", "for", "suffix", "in", "bias_suffix", ":", "\n", "            ", "if", "name", ".", "endswith", "(", "suffix", ")", ":", "\n", "                ", "return", "True", "\n", "", "", "return", "False", "\n", "\n", "", "param_list", "=", "dict", "(", ")", "\n", "\n", "if", "use_fp16", ":", "\n", "        ", "param_grads", "=", "optimizer", ".", "backward", "(", "loss", ")", "\n", "master_param_grads", "=", "create_master_params_grads", "(", "\n", "param_grads", ",", "train_program", ",", "startup_prog", ",", "loss_scaling", ")", "\n", "\n", "for", "param", ",", "_", "in", "master_param_grads", ":", "\n", "            ", "param_list", "[", "param", ".", "name", "]", "=", "param", "*", "1.0", "\n", "param_list", "[", "param", ".", "name", "]", ".", "stop_gradient", "=", "True", "\n", "\n", "", "optimizer", ".", "apply_gradients", "(", "master_param_grads", ")", "\n", "\n", "if", "weight_decay", ">", "0", ":", "\n", "            ", "for", "param", ",", "grad", "in", "master_param_grads", ":", "\n", "                ", "if", "exclude_from_weight_decay", "(", "param", ".", "name", ".", "rstrip", "(", "\".master\"", ")", ")", ":", "\n", "                    ", "continue", "\n", "", "with", "param", ".", "block", ".", "program", ".", "_optimized_guard", "(", "\n", "[", "param", ",", "grad", "]", ")", ",", "fluid", ".", "framework", ".", "name_scope", "(", "\"weight_decay\"", ")", ":", "\n", "                    ", "updated_param", "=", "param", "-", "param_list", "[", "\n", "param", ".", "name", "]", "*", "weight_decay", "*", "scheduled_lr", "\n", "fluid", ".", "layers", ".", "assign", "(", "output", "=", "param", ",", "input", "=", "updated_param", ")", "\n", "\n", "", "", "", "master_param_to_train_param", "(", "master_param_grads", ",", "param_grads", ",", "\n", "train_program", ")", "\n", "\n", "", "else", ":", "\n", "        ", "for", "param", "in", "train_program", ".", "global_block", "(", ")", ".", "all_parameters", "(", ")", ":", "\n", "            ", "param_list", "[", "param", ".", "name", "]", "=", "param", "*", "1.0", "\n", "param_list", "[", "param", ".", "name", "]", ".", "stop_gradient", "=", "True", "\n", "\n", "", "_", ",", "param_grads", "=", "optimizer", ".", "minimize", "(", "loss", ")", "\n", "\n", "if", "weight_decay", ">", "0", ":", "\n", "            ", "for", "param", ",", "grad", "in", "param_grads", ":", "\n", "                ", "if", "exclude_from_weight_decay", "(", "param", ".", "name", ")", ":", "\n", "                    ", "continue", "\n", "", "with", "param", ".", "block", ".", "program", ".", "_optimized_guard", "(", "\n", "[", "param", ",", "grad", "]", ")", ",", "fluid", ".", "framework", ".", "name_scope", "(", "\"weight_decay\"", ")", ":", "\n", "                    ", "updated_param", "=", "param", "-", "param_list", "[", "\n", "param", ".", "name", "]", "*", "weight_decay", "*", "scheduled_lr", "\n", "fluid", ".", "layers", ".", "assign", "(", "output", "=", "param", ",", "input", "=", "updated_param", ")", "\n", "\n", "", "", "", "", "return", "scheduled_lr", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.multitask.Task.__init__": [[14, 29], ["copy.deepcopy.deepcopy"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "conf", ",", "\n", "name", "=", "\"\"", ",", "\n", "is_training", "=", "False", ",", "\n", "_DataProcesser", "=", "None", ",", "\n", "shared_name", "=", "\"\"", ")", ":", "\n", "\n", "        ", "self", ".", "conf", "=", "copy", "(", "conf", ")", "\n", "\n", "self", ".", "name", "=", "name", "\n", "self", ".", "shared_name", "=", "shared_name", "\n", "\n", "self", ".", "is_training", "=", "is_training", "\n", "self", ".", "DataProcesser", "=", "_DataProcesser", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.multitask.Task._create_reader": [[30, 32], ["NotImplementedError"], "methods", ["None"], ["", "def", "_create_reader", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Task:_create_reader not implemented\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.multitask.Task._create_model": [[33, 35], ["NotImplementedError"], "methods", ["None"], ["", "def", "_create_model", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Task:_create_model not implemented\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.multitask.Task.prepare": [[36, 38], ["NotImplementedError"], "methods", ["None"], ["", "def", "prepare", "(", "self", ",", "args", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Task:prepare not implemented\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.multitask.Task.train_step": [[39, 41], ["NotImplementedError"], "methods", ["None"], ["", "def", "train_step", "(", "self", ",", "args", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Task:train_step not implemented\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.multitask.Task.predict": [[42, 44], ["NotImplementedError"], "methods", ["None"], ["", "def", "predict", "(", "self", ",", "args", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Task:_predict not implemented\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.multitask.JointTask.__init__": [[48, 60], ["paddle.Program", "paddle.Program"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "tasks", "=", "[", "]", "\n", "\n", "#self.startup_exe = None", "\n", "#self.train_exe = None", "\n", "\n", "self", ".", "exe", "=", "None", "\n", "\n", "self", ".", "share_vars_from", "=", "None", "\n", "\n", "self", ".", "startup_prog", "=", "fluid", ".", "Program", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.multitask.JointTask.__add__": [[61, 68], ["isinstance", "multitask.JointTask.tasks.append"], "methods", ["None"], ["", "def", "__add__", "(", "self", ",", "task", ")", ":", "\n", "\n", "        ", "assert", "isinstance", "(", "task", ",", "Task", ")", "\n", "\n", "self", ".", "tasks", ".", "append", "(", "task", ")", "\n", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.multitask.JointTask.prepare": [[69, 89], ["paddle.Executor", "paddle.Executor", "enumerate", "paddle.CUDAPlace", "paddle.CUDAPlace", "paddle.core.get_cuda_device_count", "paddle.core.get_cuda_device_count", "paddle.CPUPlace", "paddle.CPUPlace", "int", "os.environ.get", "print", "task.prepare", "print", "task.prepare", "multiprocessing.cpu_count"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.prepare", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.prepare"], ["", "def", "prepare", "(", "self", ",", "args", ")", ":", "\n", "\n", "        ", "if", "args", ".", "use_cuda", ":", "\n", "            ", "place", "=", "fluid", ".", "CUDAPlace", "(", "0", ")", "\n", "dev_count", "=", "fluid", ".", "core", ".", "get_cuda_device_count", "(", ")", "\n", "", "else", ":", "\n", "            ", "place", "=", "fluid", ".", "CPUPlace", "(", ")", "\n", "dev_count", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'CPU_NUM'", ",", "multiprocessing", ".", "cpu_count", "(", ")", ")", ")", "\n", "\n", "#self.startup_exe = fluid.Executor(place)", "\n", "", "self", ".", "exe", "=", "fluid", ".", "Executor", "(", "place", ")", "\n", "\n", "for", "idx", ",", "task", "in", "enumerate", "(", "self", ".", "tasks", ")", ":", "\n", "            ", "if", "idx", "==", "0", ":", "\n", "                ", "print", "(", "\"for idx : %d\"", "%", "idx", ")", "\n", "task", ".", "prepare", "(", "args", ",", "exe", "=", "self", ".", "exe", ")", "\n", "self", ".", "share_vars_from", "=", "task", ".", "compiled_train_prog", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"for idx : %d\"", "%", "idx", ")", "\n", "task", ".", "prepare", "(", "args", ",", "exe", "=", "self", ".", "exe", ",", "share_vars_from", "=", "self", ".", "share_vars_from", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.multitask.JointTask.train": [[90, 102], ["xrange", "multitask.JointTask.tasks[].train_step", "random.shuffle", "len", "xrange", "multitask.JointTask.tasks[].train_step", "joint_steps.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.algorithm.multitask.Task.train_step", "home.repos.pwc.inspect_result.baidu_DuReader.algorithm.multitask.Task.train_step"], ["", "", "", "def", "train", "(", "self", ",", "args", ")", ":", "\n", "\n", "        ", "joint_steps", "=", "[", "]", "\n", "for", "i", "in", "xrange", "(", "0", ",", "len", "(", "self", ".", "tasks", ")", ")", ":", "\n", "            ", "for", "_", "in", "xrange", "(", "0", ",", "self", ".", "tasks", "[", "i", "]", ".", "max_train_steps", ")", ":", "\n", "                ", "joint_steps", ".", "append", "(", "i", ")", "\n", "\n", "", "", "self", ".", "tasks", "[", "0", "]", ".", "train_step", "(", "args", ",", "exe", "=", "self", ".", "exe", ")", "\n", "\n", "random", ".", "shuffle", "(", "joint_steps", ")", "\n", "for", "next_task_id", "in", "joint_steps", ":", "\n", "            ", "self", ".", "tasks", "[", "next_task_id", "]", ".", "train_step", "(", "args", ",", "exe", "=", "self", ".", "exe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.extension.fp16.cast_fp16_to_fp32": [[20, 28], ["prog.global_block().append_op", "prog.global_block"], "function", ["None"], ["def", "cast_fp16_to_fp32", "(", "i", ",", "o", ",", "prog", ")", ":", "\n", "    ", "prog", ".", "global_block", "(", ")", ".", "append_op", "(", "\n", "type", "=", "\"cast\"", ",", "\n", "inputs", "=", "{", "\"X\"", ":", "i", "}", ",", "\n", "outputs", "=", "{", "\"Out\"", ":", "o", "}", ",", "\n", "attrs", "=", "{", "\n", "\"in_dtype\"", ":", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP16", ",", "\n", "\"out_dtype\"", ":", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP32", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.extension.fp16.cast_fp32_to_fp16": [[31, 39], ["prog.global_block().append_op", "prog.global_block"], "function", ["None"], ["", "def", "cast_fp32_to_fp16", "(", "i", ",", "o", ",", "prog", ")", ":", "\n", "    ", "prog", ".", "global_block", "(", ")", ".", "append_op", "(", "\n", "type", "=", "\"cast\"", ",", "\n", "inputs", "=", "{", "\"X\"", ":", "i", "}", ",", "\n", "outputs", "=", "{", "\"Out\"", ":", "o", "}", ",", "\n", "attrs", "=", "{", "\n", "\"in_dtype\"", ":", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP32", ",", "\n", "\"out_dtype\"", ":", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP16", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.extension.fp16.copy_to_master_param": [[42, 60], ["block.vars.get", "paddle.framework.Parameter", "ValueError"], "function", ["None"], ["", "def", "copy_to_master_param", "(", "p", ",", "block", ")", ":", "\n", "    ", "v", "=", "block", ".", "vars", ".", "get", "(", "p", ".", "name", ",", "None", ")", "\n", "if", "v", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"no param name %s found!\"", "%", "p", ".", "name", ")", "\n", "", "new_p", "=", "fluid", ".", "framework", ".", "Parameter", "(", "\n", "block", "=", "block", ",", "\n", "shape", "=", "v", ".", "shape", ",", "\n", "dtype", "=", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP32", ",", "\n", "type", "=", "v", ".", "type", ",", "\n", "lod_level", "=", "v", ".", "lod_level", ",", "\n", "stop_gradient", "=", "p", ".", "stop_gradient", ",", "\n", "trainable", "=", "p", ".", "trainable", ",", "\n", "optimize_attr", "=", "p", ".", "optimize_attr", ",", "\n", "regularizer", "=", "p", ".", "regularizer", ",", "\n", "gradient_clip_attr", "=", "p", ".", "gradient_clip_attr", ",", "\n", "error_clip", "=", "p", ".", "error_clip", ",", "\n", "name", "=", "v", ".", "name", "+", "\".master\"", ")", "\n", "return", "new_p", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.extension.fp16.create_master_params_grads": [[62, 89], ["fp16.copy_to_master_param", "startup_prog.global_block()._clone_variable", "startup_prog.global_block().var", "fp16.cast_fp16_to_fp32", "paddle.layers.cast", "master_params_grads.append", "main_prog.global_block", "g.name.find", "master_params_grads.append", "startup_prog.global_block", "startup_prog.global_block", "float", "float"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.copy_to_master_param", "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.cast_fp16_to_fp32"], ["", "def", "create_master_params_grads", "(", "params_grads", ",", "main_prog", ",", "startup_prog", ",", "\n", "loss_scaling", ")", ":", "\n", "    ", "master_params_grads", "=", "[", "]", "\n", "tmp_role", "=", "main_prog", ".", "_current_role", "\n", "OpRole", "=", "fluid", ".", "core", ".", "op_proto_and_checker_maker", ".", "OpRole", "\n", "main_prog", ".", "_current_role", "=", "OpRole", ".", "Backward", "\n", "for", "p", ",", "g", "in", "params_grads", ":", "\n", "# create master parameters", "\n", "        ", "master_param", "=", "copy_to_master_param", "(", "p", ",", "main_prog", ".", "global_block", "(", ")", ")", "\n", "startup_master_param", "=", "startup_prog", ".", "global_block", "(", ")", ".", "_clone_variable", "(", "\n", "master_param", ")", "\n", "startup_p", "=", "startup_prog", ".", "global_block", "(", ")", ".", "var", "(", "p", ".", "name", ")", "\n", "cast_fp16_to_fp32", "(", "startup_p", ",", "startup_master_param", ",", "startup_prog", ")", "\n", "# cast fp16 gradients to fp32 before apply gradients", "\n", "if", "g", ".", "name", ".", "find", "(", "\"layer_norm\"", ")", ">", "-", "1", ":", "\n", "            ", "if", "loss_scaling", ">", "1", ":", "\n", "                ", "scaled_g", "=", "g", "/", "float", "(", "loss_scaling", ")", "\n", "", "else", ":", "\n", "                ", "scaled_g", "=", "g", "\n", "", "master_params_grads", ".", "append", "(", "[", "p", ",", "scaled_g", "]", ")", "\n", "continue", "\n", "", "master_grad", "=", "fluid", ".", "layers", ".", "cast", "(", "g", ",", "\"float32\"", ")", "\n", "if", "loss_scaling", ">", "1", ":", "\n", "            ", "master_grad", "=", "master_grad", "/", "float", "(", "loss_scaling", ")", "\n", "", "master_params_grads", ".", "append", "(", "[", "master_param", ",", "master_grad", "]", ")", "\n", "", "main_prog", ".", "_current_role", "=", "tmp_role", "\n", "return", "master_params_grads", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.extension.fp16.master_param_to_train_param": [[91, 98], ["enumerate", "train_p.name.find", "main_prog._optimized_guard", "fp16.cast_fp32_to_fp16"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.cast_fp32_to_fp16"], ["", "def", "master_param_to_train_param", "(", "master_params_grads", ",", "params_grads", ",", "main_prog", ")", ":", "\n", "    ", "for", "idx", ",", "m_p_g", "in", "enumerate", "(", "master_params_grads", ")", ":", "\n", "        ", "train_p", ",", "_", "=", "params_grads", "[", "idx", "]", "\n", "if", "train_p", ".", "name", ".", "find", "(", "\"layer_norm\"", ")", ">", "-", "1", ":", "\n", "            ", "continue", "\n", "", "with", "main_prog", ".", "_optimized_guard", "(", "[", "m_p_g", "[", "0", "]", ",", "m_p_g", "[", "1", "]", "]", ")", ":", "\n", "            ", "cast_fp32_to_fp16", "(", "m_p_g", "[", "0", "]", ",", "train_p", ",", "main_prog", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.nets.transformer_encoder.multi_head_attention": [[27, 161], ["transformer_encoder.multi_head_attention.__compute_qkv"], "function", ["None"], ["def", "layer_norm", "(", "x", ",", "begin_norm_axis", "=", "1", ",", "epsilon", "=", "1e-6", ",", "param_attr", "=", "None", ",", "bias_attr", "=", "None", ")", ":", "\n", "    ", "helper", "=", "LayerHelper", "(", "'layer_norm'", ",", "**", "locals", "(", ")", ")", "\n", "mean", "=", "layers", ".", "reduce_mean", "(", "x", ",", "dim", "=", "begin_norm_axis", ",", "keep_dim", "=", "True", ")", "\n", "shift_x", "=", "layers", ".", "elementwise_sub", "(", "x", "=", "x", ",", "y", "=", "mean", ",", "axis", "=", "0", ")", "\n", "variance", "=", "layers", ".", "reduce_mean", "(", "layers", ".", "square", "(", "shift_x", ")", ",", "dim", "=", "begin_norm_axis", ",", "keep_dim", "=", "True", ")", "\n", "r_stdev", "=", "layers", ".", "rsqrt", "(", "variance", "+", "epsilon", ")", "\n", "norm_x", "=", "layers", ".", "elementwise_mul", "(", "x", "=", "shift_x", ",", "y", "=", "r_stdev", ",", "axis", "=", "0", ")", "\n", "\n", "param_shape", "=", "[", "reduce", "(", "lambda", "x", ",", "y", ":", "x", "*", "y", ",", "norm_x", ".", "shape", "[", "begin_norm_axis", ":", "]", ")", "]", "\n", "param_dtype", "=", "norm_x", ".", "dtype", "\n", "scale", "=", "helper", ".", "create_parameter", "(", "\n", "attr", "=", "param_attr", ",", "\n", "shape", "=", "param_shape", ",", "\n", "dtype", "=", "param_dtype", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "1.", ")", ")", "\n", "bias", "=", "helper", ".", "create_parameter", "(", "\n", "attr", "=", "bias_attr", ",", "\n", "shape", "=", "param_shape", ",", "\n", "dtype", "=", "param_dtype", ",", "\n", "is_bias", "=", "True", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", "\n", "\n", "out", "=", "layers", ".", "elementwise_mul", "(", "x", "=", "norm_x", ",", "y", "=", "scale", ",", "axis", "=", "-", "1", ")", "\n", "out", "=", "layers", ".", "elementwise_add", "(", "x", "=", "out", ",", "y", "=", "bias", ",", "axis", "=", "-", "1", ")", "\n", "\n", "return", "out", "\n", "\n", "", "def", "multi_head_attention", "(", "queries", ",", "\n", "keys", ",", "\n", "values", ",", "\n", "attn_bias", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "n_head", "=", "1", ",", "\n", "dropout_rate", "=", "0.", ",", "\n", "cache", "=", "None", ",", "\n", "param_initializer", "=", "None", ",", "\n", "name", "=", "'multi_head_att'", ")", ":", "\n", "    ", "\"\"\"\n    Multi-Head Attention. Note that attn_bias is added to the logit before\n    computing softmax activiation to mask certain selected positions so that\n    they will not considered in attention weights.\n    \"\"\"", "\n", "keys", "=", "queries", "if", "keys", "is", "None", "else", "keys", "\n", "values", "=", "keys", "if", "values", "is", "None", "else", "values", "\n", "\n", "if", "not", "(", "len", "(", "queries", ".", "shape", ")", "==", "len", "(", "keys", ".", "shape", ")", "==", "len", "(", "values", ".", "shape", ")", "==", "3", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Inputs: quries, keys and values should all be 3-D tensors.\"", ")", "\n", "\n", "", "def", "__compute_qkv", "(", "queries", ",", "keys", ",", "values", ",", "n_head", ",", "d_key", ",", "d_value", ")", ":", "\n", "        ", "\"\"\"\n        Add linear projection to queries, keys, and values.\n        \"\"\"", "\n", "q", "=", "layers", ".", "fc", "(", "input", "=", "queries", ",", "\n", "size", "=", "d_key", "*", "n_head", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_query_fc.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_query_fc.b_0'", ")", "\n", "k", "=", "layers", ".", "fc", "(", "input", "=", "keys", ",", "\n", "size", "=", "d_key", "*", "n_head", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_key_fc.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_key_fc.b_0'", ")", "\n", "v", "=", "layers", ".", "fc", "(", "input", "=", "values", ",", "\n", "size", "=", "d_value", "*", "n_head", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_value_fc.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_value_fc.b_0'", ")", "\n", "return", "q", ",", "k", ",", "v", "\n", "\n", "", "def", "__split_heads", "(", "x", ",", "n_head", ")", ":", "\n", "        ", "\"\"\"\n        Reshape the last dimension of inpunt tensor x so that it becomes two\n        dimensions and then transpose. Specifically, input a tensor with shape\n        [bs, max_sequence_length, n_head * hidden_dim] then output a tensor\n        with shape [bs, n_head, max_sequence_length, hidden_dim].\n        \"\"\"", "\n", "hidden_size", "=", "x", ".", "shape", "[", "-", "1", "]", "\n", "# The value 0 in shape attr means copying the corresponding dimension", "\n", "# size of the input as the output dimension size.", "\n", "reshaped", "=", "layers", ".", "reshape", "(", "\n", "x", "=", "x", ",", "shape", "=", "[", "0", ",", "0", ",", "n_head", ",", "hidden_size", "//", "n_head", "]", ",", "inplace", "=", "False", ")", "\n", "\n", "# permuate the dimensions into:", "\n", "# [batch_size, n_head, max_sequence_len, hidden_size_per_head]", "\n", "return", "layers", ".", "transpose", "(", "x", "=", "reshaped", ",", "perm", "=", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "", "def", "__combine_heads", "(", "x", ")", ":", "\n", "        ", "\"\"\"\n        Transpose and then reshape the last two dimensions of inpunt tensor x\n        so that it becomes one dimension, which is reverse to __split_heads.\n        \"\"\"", "\n", "if", "len", "(", "x", ".", "shape", ")", "==", "3", ":", "return", "x", "\n", "if", "len", "(", "x", ".", "shape", ")", "!=", "4", ":", "\n", "            ", "raise", "ValueError", "(", "\"Input(x) should be a 4-D Tensor.\"", ")", "\n", "\n", "", "trans_x", "=", "layers", ".", "transpose", "(", "x", ",", "perm", "=", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "# The value 0 in shape attr means copying the corresponding dimension", "\n", "# size of the input as the output dimension size.", "\n", "return", "layers", ".", "reshape", "(", "\n", "x", "=", "trans_x", ",", "\n", "shape", "=", "[", "0", ",", "0", ",", "trans_x", ".", "shape", "[", "2", "]", "*", "trans_x", ".", "shape", "[", "3", "]", "]", ",", "\n", "inplace", "=", "False", ")", "\n", "\n", "", "def", "scaled_dot_product_attention", "(", "q", ",", "k", ",", "v", ",", "attn_bias", ",", "d_key", ",", "dropout_rate", ")", ":", "\n", "        ", "\"\"\"\n        Scaled Dot-Product Attention\n        \"\"\"", "\n", "scaled_q", "=", "layers", ".", "scale", "(", "x", "=", "q", ",", "scale", "=", "d_key", "**", "-", "0.5", ")", "\n", "product", "=", "layers", ".", "matmul", "(", "x", "=", "scaled_q", ",", "y", "=", "k", ",", "transpose_y", "=", "True", ")", "\n", "if", "attn_bias", ":", "\n", "            ", "product", "+=", "attn_bias", "\n", "", "weights", "=", "layers", ".", "softmax", "(", "product", ")", "\n", "if", "dropout_rate", ":", "\n", "            ", "weights", "=", "layers", ".", "dropout", "(", "\n", "weights", ",", "\n", "dropout_prob", "=", "dropout_rate", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "\n", "is_test", "=", "False", ")", "\n", "", "out", "=", "layers", ".", "matmul", "(", "weights", ",", "v", ")", "\n", "return", "out", "\n", "\n", "", "q", ",", "k", ",", "v", "=", "__compute_qkv", "(", "queries", ",", "keys", ",", "values", ",", "n_head", ",", "d_key", ",", "d_value", ")", "\n", "\n", "if", "cache", "is", "not", "None", ":", "# use cache and concat time steps", "\n", "# Since the inplace reshape in __split_heads changes the shape of k and", "\n", "# v, which is the cache input for next time step, reshape the cache", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.nets.transformer_encoder.positionwise_feed_forward": [[163, 197], ["paddle.fc", "paddle.fc", "paddle.dropout", "paddle.ParamAttr", "paddle.ParamAttr"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["        ", "k", "=", "cache", "[", "\"k\"", "]", "=", "layers", ".", "concat", "(", "\n", "[", "layers", ".", "reshape", "(", "\n", "cache", "[", "\"k\"", "]", ",", "shape", "=", "[", "0", ",", "0", ",", "d_model", "]", ")", ",", "k", "]", ",", "axis", "=", "1", ")", "\n", "v", "=", "cache", "[", "\"v\"", "]", "=", "layers", ".", "concat", "(", "\n", "[", "layers", ".", "reshape", "(", "\n", "cache", "[", "\"v\"", "]", ",", "shape", "=", "[", "0", ",", "0", ",", "d_model", "]", ")", ",", "v", "]", ",", "axis", "=", "1", ")", "\n", "\n", "", "q", "=", "__split_heads", "(", "q", ",", "n_head", ")", "\n", "k", "=", "__split_heads", "(", "k", ",", "n_head", ")", "\n", "v", "=", "__split_heads", "(", "v", ",", "n_head", ")", "\n", "\n", "ctx_multiheads", "=", "scaled_dot_product_attention", "(", "q", ",", "k", ",", "v", ",", "attn_bias", ",", "d_key", ",", "\n", "dropout_rate", ")", "\n", "\n", "out", "=", "__combine_heads", "(", "ctx_multiheads", ")", "\n", "\n", "# Project back to the model size.", "\n", "proj_out", "=", "layers", ".", "fc", "(", "input", "=", "out", ",", "\n", "size", "=", "d_model", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_output_fc.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_output_fc.b_0'", ")", "\n", "return", "proj_out", "\n", "\n", "\n", "", "def", "positionwise_feed_forward", "(", "x", ",", "\n", "d_inner_hid", ",", "\n", "d_hid", ",", "\n", "dropout_rate", ",", "\n", "hidden_act", ",", "\n", "param_initializer", "=", "None", ",", "\n", "name", "=", "'ffn'", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.baidu_DuReader.nets.transformer_encoder.pre_post_process_layer": [[199, 233], ["paddle.layer_norm", "paddle.cast", "paddle.cast", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.dropout", "len", "paddle.initializer.Constant", "paddle.initializer.Constant"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.layer_norm", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["\n", "hidden", "=", "layers", ".", "fc", "(", "input", "=", "x", ",", "\n", "size", "=", "d_inner_hid", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "act", "=", "hidden_act", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_fc_0.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_fc_0.b_0'", ")", "\n", "if", "dropout_rate", ":", "\n", "        ", "hidden", "=", "layers", ".", "dropout", "(", "\n", "hidden", ",", "\n", "dropout_prob", "=", "dropout_rate", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "\n", "is_test", "=", "False", ")", "\n", "\n", "", "out", "=", "layers", ".", "fc", "(", "input", "=", "hidden", ",", "\n", "size", "=", "d_hid", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_fc_1.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_fc_1.b_0'", ")", "\n", "return", "out", "\n", "\n", "\n", "", "def", "pre_post_process_layer", "(", "prev_out", ",", "out", ",", "process_cmd", ",", "dropout_rate", "=", "0.", ",", "\n", "name", "=", "''", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.baidu_DuReader.nets.transformer_encoder.encoder_layer": [[239, 301], ["transformer_encoder.multi_head_attention", "post_process_layer", "transformer_encoder.positionwise_feed_forward", "post_process_layer", "pre_process_layer", "pre_process_layer"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.multi_head_attention", "home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.positionwise_feed_forward"], ["            ", "out_dtype", "=", "out", ".", "dtype", "\n", "if", "out_dtype", "==", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP16", ":", "\n", "                ", "out", "=", "layers", ".", "cast", "(", "x", "=", "out", ",", "dtype", "=", "\"float32\"", ")", "\n", "", "out", "=", "layer_norm", "(", "\n", "out", ",", "\n", "begin_norm_axis", "=", "len", "(", "out", ".", "shape", ")", "-", "1", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_layer_norm_scale'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "1.", ")", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_layer_norm_bias'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", ")", "\n", "if", "out_dtype", "==", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP16", ":", "\n", "                ", "out", "=", "layers", ".", "cast", "(", "x", "=", "out", ",", "dtype", "=", "\"float16\"", ")", "\n", "", "", "elif", "cmd", "==", "\"d\"", ":", "# add dropout", "\n", "            ", "if", "dropout_rate", ":", "\n", "                ", "out", "=", "layers", ".", "dropout", "(", "\n", "out", ",", "\n", "dropout_prob", "=", "dropout_rate", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "\n", "is_test", "=", "False", ")", "\n", "", "", "", "return", "out", "\n", "\n", "\n", "", "pre_process_layer", "=", "partial", "(", "pre_post_process_layer", ",", "None", ")", "\n", "post_process_layer", "=", "pre_post_process_layer", "\n", "\n", "def", "encoder_layer", "(", "enc_input", ",", "\n", "attn_bias", ",", "\n", "n_head", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "d_inner_hid", ",", "\n", "prepostprocess_dropout", ",", "\n", "attention_dropout", ",", "\n", "relu_dropout", ",", "\n", "hidden_act", ",", "\n", "preprocess_cmd", "=", "\"n\"", ",", "\n", "postprocess_cmd", "=", "\"da\"", ",", "\n", "param_initializer", "=", "None", ",", "\n", "name", "=", "''", ")", ":", "\n", "    ", "\"\"\"The encoder layers that can be stacked to form a deep encoder.\n    This module consits of a multi-head (self) attention followed by\n    position-wise feed-forward networks and both the two components companied\n    with the post_process_layer to add residual connection, layer normalization\n    and droput.\n    \"\"\"", "\n", "attn_output", "=", "multi_head_attention", "(", "\n", "pre_process_layer", "(", "\n", "enc_input", ",", "\n", "preprocess_cmd", ",", "\n", "prepostprocess_dropout", ",", "\n", "name", "=", "name", "+", "'_pre_att'", ")", ",", "\n", "None", ",", "\n", "None", ",", "\n", "attn_bias", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "n_head", ",", "\n", "attention_dropout", ",", "\n", "param_initializer", "=", "param_initializer", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.nets.transformer_encoder.encoder": [[303, 354], ["range", "pre_process_layer", "enc_outputs.append", "transformer_encoder.encoder_layer", "enc_outputs.append", "str"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.encoder_layer"], ["attn_output", "=", "post_process_layer", "(", "\n", "enc_input", ",", "\n", "attn_output", ",", "\n", "postprocess_cmd", ",", "\n", "prepostprocess_dropout", ",", "\n", "name", "=", "name", "+", "'_post_att'", ")", "\n", "ffd_output", "=", "positionwise_feed_forward", "(", "\n", "pre_process_layer", "(", "\n", "attn_output", ",", "\n", "preprocess_cmd", ",", "\n", "prepostprocess_dropout", ",", "\n", "name", "=", "name", "+", "'_pre_ffn'", ")", ",", "\n", "d_inner_hid", ",", "\n", "d_model", ",", "\n", "relu_dropout", ",", "\n", "hidden_act", ",", "\n", "param_initializer", "=", "param_initializer", ",", "\n", "name", "=", "name", "+", "'_ffn'", ")", "\n", "return", "post_process_layer", "(", "\n", "attn_output", ",", "\n", "ffd_output", ",", "\n", "postprocess_cmd", ",", "\n", "prepostprocess_dropout", ",", "\n", "name", "=", "name", "+", "'_post_ffn'", ")", "\n", "\n", "\n", "", "def", "encoder", "(", "enc_input", ",", "\n", "attn_bias", ",", "\n", "n_layer", ",", "\n", "n_head", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "d_inner_hid", ",", "\n", "prepostprocess_dropout", ",", "\n", "attention_dropout", ",", "\n", "relu_dropout", ",", "\n", "hidden_act", ",", "\n", "preprocess_cmd", "=", "\"n\"", ",", "\n", "postprocess_cmd", "=", "\"da\"", ",", "\n", "param_initializer", "=", "None", ",", "\n", "name", "=", "''", ",", "\n", "return_all", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    The encoder is composed of a stack of identical layers returned by calling\n    encoder_layer.\n    \"\"\"", "\n", "enc_outputs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_layer", ")", ":", "\n", "        ", "enc_output", "=", "encoder_layer", "(", "\n", "enc_input", ",", "\n", "attn_bias", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.nets.bert.BertModel.__init__": [[30, 64], ["paddle.initializer.TruncatedNormal", "bert.BertModel._build_model"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel._build_model"], ["    ", "def", "__init__", "(", "self", ",", "\n", "src_ids", ",", "\n", "position_ids", ",", "\n", "sentence_ids", ",", "\n", "input_mask", ",", "\n", "config", ",", "\n", "weight_sharing", "=", "True", ",", "\n", "use_fp16", "=", "False", ",", "\n", "model_name", "=", "''", ")", ":", "\n", "\n", "        ", "self", ".", "_emb_size", "=", "config", "[", "\"hidden_size\"", "]", "\n", "self", ".", "_n_layer", "=", "config", "[", "\"num_hidden_layers\"", "]", "\n", "self", ".", "_n_head", "=", "config", "[", "\"num_attention_heads\"", "]", "\n", "self", ".", "_voc_size", "=", "config", "[", "\"vocab_size\"", "]", "\n", "self", ".", "_max_position_seq_len", "=", "config", "[", "\"max_position_embeddings\"", "]", "\n", "self", ".", "_sent_types", "=", "config", "[", "\"type_vocab_size\"", "]", "\n", "self", ".", "_hidden_act", "=", "config", "[", "\"hidden_act\"", "]", "\n", "self", ".", "_prepostprocess_dropout", "=", "config", "[", "\"hidden_dropout_prob\"", "]", "\n", "self", ".", "_attention_dropout", "=", "config", "[", "\"attention_probs_dropout_prob\"", "]", "\n", "self", ".", "_weight_sharing", "=", "weight_sharing", "\n", "\n", "self", ".", "model_name", "=", "model_name", "\n", "\n", "self", ".", "_word_emb_name", "=", "self", ".", "model_name", "+", "\"word_embedding\"", "\n", "self", ".", "_pos_emb_name", "=", "self", ".", "model_name", "+", "\"pos_embedding\"", "\n", "self", ".", "_sent_emb_name", "=", "self", ".", "model_name", "+", "\"sent_embedding\"", "\n", "self", ".", "_dtype", "=", "\"float16\"", "if", "use_fp16", "else", "\"float32\"", "\n", "\n", "# Initialize all weigths by truncated normal initializer, and all biases ", "\n", "# will be initialized by constant zero by default.", "\n", "self", ".", "_param_initializer", "=", "fluid", ".", "initializer", ".", "TruncatedNormal", "(", "\n", "scale", "=", "config", "[", "\"initializer_range\"", "]", ")", "\n", "\n", "self", ".", "_build_model", "(", "src_ids", ",", "position_ids", ",", "sentence_ids", ",", "input_mask", ",", "\n", "config", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.nets.bert.BertModel._build_model": [[65, 132], ["paddle.layers.embedding", "paddle.layers.embedding", "paddle.layers.embedding", "pdnlp.module.transformer_encoder.pre_process_layer", "paddle.layers.matmul", "paddle.layers.scale", "paddle.layers.stack", "pdnlp.module.transformer_encoder.encoder", "paddle.layers.cast", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.ParamAttr"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.encoder"], ["\n", "", "def", "_build_model", "(", "self", ",", "src_ids", ",", "position_ids", ",", "sentence_ids", ",", "input_mask", ",", "\n", "config", ")", ":", "\n", "# padding id in vocabulary must be set to 0", "\n", "        ", "emb_out", "=", "fluid", ".", "layers", ".", "embedding", "(", "\n", "input", "=", "src_ids", ",", "\n", "size", "=", "[", "self", ".", "_voc_size", ",", "self", ".", "_emb_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "_word_emb_name", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "is_sparse", "=", "False", ")", "\n", "\n", "self", ".", "emb_out", "=", "emb_out", "\n", "\n", "position_emb_out", "=", "fluid", ".", "layers", ".", "embedding", "(", "\n", "input", "=", "position_ids", ",", "\n", "size", "=", "[", "self", ".", "_max_position_seq_len", ",", "self", ".", "_emb_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "_pos_emb_name", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ")", "\n", "\n", "self", ".", "position_emb_out", "=", "position_emb_out", "\n", "\n", "sent_emb_out", "=", "fluid", ".", "layers", ".", "embedding", "(", "\n", "sentence_ids", ",", "\n", "size", "=", "[", "self", ".", "_sent_types", ",", "self", ".", "_emb_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "_sent_emb_name", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ")", "\n", "\n", "self", ".", "sent_emb_out", "=", "sent_emb_out", "\n", "\n", "emb_out", "=", "emb_out", "+", "position_emb_out", "\n", "emb_out", "=", "emb_out", "+", "sent_emb_out", "\n", "\n", "emb_out", "=", "pre_process_layer", "(", "\n", "emb_out", ",", "'nd'", ",", "self", ".", "_prepostprocess_dropout", ",", "name", "=", "'pre_encoder'", ")", "\n", "\n", "if", "self", ".", "_dtype", "==", "\"float16\"", ":", "\n", "            ", "input_mask", "=", "fluid", ".", "layers", ".", "cast", "(", "x", "=", "input_mask", ",", "dtype", "=", "self", ".", "_dtype", ")", "\n", "\n", "", "self_attn_mask", "=", "fluid", ".", "layers", ".", "matmul", "(", "\n", "x", "=", "input_mask", ",", "y", "=", "input_mask", ",", "transpose_y", "=", "True", ")", "\n", "\n", "self_attn_mask", "=", "fluid", ".", "layers", ".", "scale", "(", "\n", "x", "=", "self_attn_mask", ",", "\n", "scale", "=", "config", "[", "\"self_att_scale\"", "]", ",", "\n", "bias", "=", "-", "1.0", ",", "\n", "bias_after_scale", "=", "False", ")", "\n", "\n", "n_head_self_attn_mask", "=", "fluid", ".", "layers", ".", "stack", "(", "\n", "x", "=", "[", "self_attn_mask", "]", "*", "self", ".", "_n_head", ",", "axis", "=", "1", ")", "\n", "\n", "n_head_self_attn_mask", ".", "stop_gradient", "=", "True", "\n", "\n", "self", ".", "_enc_out", "=", "encoder", "(", "\n", "enc_input", "=", "emb_out", ",", "\n", "attn_bias", "=", "n_head_self_attn_mask", ",", "\n", "n_layer", "=", "self", ".", "_n_layer", ",", "\n", "n_head", "=", "self", ".", "_n_head", ",", "\n", "d_key", "=", "self", ".", "_emb_size", "//", "self", ".", "_n_head", ",", "\n", "d_value", "=", "self", ".", "_emb_size", "//", "self", ".", "_n_head", ",", "\n", "d_model", "=", "self", ".", "_emb_size", ",", "\n", "d_inner_hid", "=", "self", ".", "_emb_size", "*", "4", ",", "\n", "prepostprocess_dropout", "=", "self", ".", "_prepostprocess_dropout", ",", "\n", "attention_dropout", "=", "self", ".", "_attention_dropout", ",", "\n", "relu_dropout", "=", "0", ",", "\n", "hidden_act", "=", "self", ".", "_hidden_act", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.nets.bert.BertModel.get_sequence_output": [[133, 135], ["None"], "methods", ["None"], ["preprocess_cmd", "=", "\"\"", ",", "\n", "postprocess_cmd", "=", "\"dan\"", ",", "\n", "param_initializer", "=", "self", ".", "_param_initializer", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.nets.bert.BertModel.get_pooled_output": [[136, 150], ["paddle.layers.slice", "paddle.layers.fc", "paddle.ParamAttr"], "methods", ["None"], ["name", "=", "self", ".", "model_name", "+", "'encoder'", ")", "\n", "\n", "", "def", "get_sequence_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_enc_out", "\n", "\n", "", "def", "get_pooled_output", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the first feature of each sequence for classification\"\"\"", "\n", "\n", "next_sent_feat", "=", "fluid", ".", "layers", ".", "slice", "(", "\n", "input", "=", "self", ".", "_enc_out", ",", "axes", "=", "[", "1", "]", ",", "starts", "=", "[", "0", "]", ",", "ends", "=", "[", "1", "]", ")", "\n", "next_sent_feat", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "next_sent_feat", ",", "\n", "size", "=", "self", ".", "_emb_size", ",", "\n", "act", "=", "\"tanh\"", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.nets.bert.BertModel.get_pretraining_output": [[151, 221], ["paddle.layers.cast", "bert.BertModel.get_pooled_output", "paddle.layers.reshape", "paddle.layers.gather", "paddle.layers.fc", "pdnlp.module.transformer_encoder.pre_process_layer", "paddle.ParamAttr", "paddle.layers.softmax_with_cross_entropy", "paddle.layers.mean", "paddle.layers.fc", "paddle.layers.softmax_with_cross_entropy", "paddle.layers.accuracy", "paddle.layers.mean", "paddle.layers.matmul", "paddle.layers.create_parameter", "paddle.layers.fc", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.initializer.Constant", "paddle.ParamAttr", "paddle.default_main_program().global_block().var", "paddle.ParamAttr", "paddle.default_main_program().global_block", "paddle.default_main_program"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.get_pooled_output"], ["name", "=", "self", ".", "model_name", "+", "\"pooled_fc.w_0\"", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "\"pooled_fc.b_0\"", ")", "\n", "return", "next_sent_feat", "\n", "\n", "", "def", "get_pretraining_output", "(", "self", ",", "mask_label", ",", "mask_pos", ",", "labels", ")", ":", "\n", "        ", "\"\"\"Get the loss & accuracy for pretraining\"\"\"", "\n", "\n", "mask_pos", "=", "fluid", ".", "layers", ".", "cast", "(", "x", "=", "mask_pos", ",", "dtype", "=", "'int32'", ")", "\n", "\n", "# extract the first token feature in each sentence", "\n", "next_sent_feat", "=", "self", ".", "get_pooled_output", "(", ")", "\n", "reshaped_emb_out", "=", "fluid", ".", "layers", ".", "reshape", "(", "\n", "x", "=", "self", ".", "_enc_out", ",", "shape", "=", "[", "-", "1", ",", "self", ".", "_emb_size", "]", ")", "\n", "# extract masked tokens' feature", "\n", "mask_feat", "=", "fluid", ".", "layers", ".", "gather", "(", "input", "=", "reshaped_emb_out", ",", "index", "=", "mask_pos", ")", "\n", "\n", "# transform: fc", "\n", "mask_trans_feat", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "mask_feat", ",", "\n", "size", "=", "self", ".", "_emb_size", ",", "\n", "act", "=", "self", ".", "_hidden_act", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "model_name", "+", "'mask_lm_trans_fc.w_0'", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "model_name", "+", "'mask_lm_trans_fc.b_0'", ")", ")", "\n", "# transform: layer norm ", "\n", "mask_trans_feat", "=", "pre_process_layer", "(", "\n", "mask_trans_feat", ",", "'n'", ",", "name", "=", "self", ".", "model_name", "+", "'mask_lm_trans'", ")", "\n", "\n", "mask_lm_out_bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "model_name", "+", "\"mask_lm_out_fc.b_0\"", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "value", "=", "0.0", ")", ")", "\n", "if", "self", ".", "_weight_sharing", ":", "\n", "            ", "fc_out", "=", "fluid", ".", "layers", ".", "matmul", "(", "\n", "x", "=", "mask_trans_feat", ",", "\n", "y", "=", "fluid", ".", "default_main_program", "(", ")", ".", "global_block", "(", ")", ".", "var", "(", "\n", "self", ".", "_word_emb_name", ")", ",", "\n", "transpose_y", "=", "True", ")", "\n", "fc_out", "+=", "fluid", ".", "layers", ".", "create_parameter", "(", "\n", "shape", "=", "[", "self", ".", "_voc_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "attr", "=", "mask_lm_out_bias_attr", ",", "\n", "is_bias", "=", "True", ")", "\n", "\n", "", "else", ":", "\n", "            ", "fc_out", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "mask_trans_feat", ",", "\n", "size", "=", "self", ".", "_voc_size", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "model_name", "+", "\"mask_lm_out_fc.w_0\"", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "mask_lm_out_bias_attr", ")", "\n", "\n", "", "mask_lm_loss", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "fc_out", ",", "label", "=", "mask_label", ")", "\n", "mean_mask_lm_loss", "=", "fluid", ".", "layers", ".", "mean", "(", "mask_lm_loss", ")", "\n", "\n", "next_sent_fc_out", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "next_sent_feat", ",", "\n", "size", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "model_name", "+", "\"next_sent_fc.w_0\"", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "self", ".", "model_name", "+", "\"next_sent_fc.b_0\"", ")", "\n", "\n", "next_sent_loss", ",", "next_sent_softmax", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "next_sent_fc_out", ",", "label", "=", "labels", ",", "return_softmax", "=", "True", ")", "\n", "\n", "next_sent_acc", "=", "fluid", ".", "layers", ".", "accuracy", "(", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.einsum4x4": [[6, 27], ["re.split", "list", "list", "list", "list", "paddle.layers.transpose", "paddle.layers.transpose", "paddle.layers.matmul", "paddle.layers.transpose", "idx_x.index", "idx_y.index", "new_idx_z.index", "set", "set", "set", "set", "set", "set", "set", "set", "set"], "function", ["None"], ["def", "einsum4x4", "(", "equation", ",", "x", ",", "y", ")", ":", "\n", "    ", "idx_x", ",", "idx_y", ",", "idx_z", "=", "re", ".", "split", "(", "\",|->\"", ",", "equation", ")", "\n", "repeated_idx", "=", "list", "(", "set", "(", "idx_x", "+", "idx_y", ")", "-", "set", "(", "idx_z", ")", ")", "\n", "\n", "unique_idx_x", "=", "list", "(", "set", "(", "idx_x", ")", "-", "set", "(", "idx_y", ")", ")", "\n", "unique_idx_y", "=", "list", "(", "set", "(", "idx_y", ")", "-", "set", "(", "idx_x", ")", ")", "\n", "common_idx", "=", "list", "(", "set", "(", "idx_x", ")", "&", "set", "(", "idx_y", ")", "-", "set", "(", "repeated_idx", ")", ")", "\n", "\n", "new_idx_x", "=", "common_idx", "+", "unique_idx_x", "+", "repeated_idx", "\n", "new_idx_y", "=", "common_idx", "+", "unique_idx_y", "+", "repeated_idx", "\n", "new_idx_z", "=", "common_idx", "+", "unique_idx_x", "+", "unique_idx_y", "\n", "\n", "perm_x", "=", "[", "idx_x", ".", "index", "(", "i", ")", "for", "i", "in", "new_idx_x", "]", "\n", "perm_y", "=", "[", "idx_y", ".", "index", "(", "i", ")", "for", "i", "in", "new_idx_y", "]", "\n", "perm_z", "=", "[", "new_idx_z", ".", "index", "(", "i", ")", "for", "i", "in", "idx_z", "]", "\n", "\n", "x", "=", "fluid", ".", "layers", ".", "transpose", "(", "x", ",", "perm", "=", "perm_x", ")", "\n", "y", "=", "fluid", ".", "layers", ".", "transpose", "(", "y", ",", "perm", "=", "perm_y", ")", "\n", "z", "=", "fluid", ".", "layers", ".", "matmul", "(", "x", "=", "x", ",", "y", "=", "y", ",", "transpose_y", "=", "True", ")", "\n", "z", "=", "fluid", ".", "layers", ".", "transpose", "(", "z", ",", "perm", "=", "perm_z", ")", "\n", "return", "z", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.positional_embedding": [[29, 40], ["paddle.layers.reshape", "paddle.layers.reshape", "paddle.layers.matmul", "paddle.layers.concat", "paddle.layers.unsqueeze", "paddle.layers.expand", "paddle.layers.sin", "paddle.layers.cos"], "function", ["None"], ["", "def", "positional_embedding", "(", "pos_seq", ",", "inv_freq", ",", "bsz", "=", "None", ")", ":", "\n", "    ", "pos_seq", "=", "fluid", ".", "layers", ".", "reshape", "(", "pos_seq", ",", "[", "-", "1", ",", "1", "]", ")", "\n", "inv_freq", "=", "fluid", ".", "layers", ".", "reshape", "(", "inv_freq", ",", "[", "1", ",", "-", "1", "]", ")", "\n", "sinusoid_inp", "=", "fluid", ".", "layers", ".", "matmul", "(", "pos_seq", ",", "inv_freq", ")", "\n", "pos_emb", "=", "fluid", ".", "layers", ".", "concat", "(", "input", "=", "[", "fluid", ".", "layers", ".", "sin", "(", "sinusoid_inp", ")", ",", "\n", "fluid", ".", "layers", ".", "cos", "(", "sinusoid_inp", ")", "]", ",", "axis", "=", "-", "1", ")", "\n", "pos_emb", "=", "fluid", ".", "layers", ".", "unsqueeze", "(", "pos_emb", ",", "[", "1", "]", ")", "\n", "if", "bsz", "is", "not", "None", ":", "\n", "        ", "pos_emb", "=", "fluid", ".", "layers", ".", "expand", "(", "pos_emb", ",", "[", "1", ",", "bsz", ",", "1", "]", ")", "\n", "\n", "", "return", "pos_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.positionwise_ffn": [[42, 69], ["paddle.layers.fc", "paddle.layers.dropout", "paddle.layers.fc", "paddle.layers.dropout", "paddle.layers.layer_norm", "ValueError", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.ParamAttr", "len", "paddle.initializer.Constant", "paddle.initializer.Constant"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout", "home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.layer_norm"], ["", "def", "positionwise_ffn", "(", "inp", ",", "d_model", ",", "d_inner", ",", "dropout_prob", ",", "param_initializer", "=", "None", ",", "\n", "act_type", "=", "'relu'", ",", "name", "=", "'ff'", ")", ":", "\n", "    ", "\"\"\"Position-wise Feed-forward Network.\"\"\"", "\n", "if", "act_type", "not", "in", "[", "'relu'", ",", "'gelu'", "]", ":", "\n", "        ", "raise", "ValueError", "(", "'Unsupported activation type {}'", ".", "format", "(", "act_type", ")", ")", "\n", "\n", "", "output", "=", "fluid", ".", "layers", ".", "fc", "(", "input", "=", "inp", ",", "size", "=", "d_inner", ",", "act", "=", "act_type", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_layer_1_weight'", ",", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_layer_1_bias'", ")", "\n", "output", "=", "fluid", ".", "layers", ".", "dropout", "(", "output", ",", "dropout_prob", "=", "dropout_prob", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "is_test", "=", "False", ")", "\n", "output", "=", "fluid", ".", "layers", ".", "fc", "(", "output", ",", "size", "=", "d_model", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_layer_2_weight'", ",", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_layer_2_bias'", ")", "\n", "output", "=", "fluid", ".", "layers", ".", "dropout", "(", "output", ",", "dropout_prob", "=", "dropout_prob", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "is_test", "=", "False", ")", "\n", "output", "=", "fluid", ".", "layers", ".", "layer_norm", "(", "output", "+", "inp", ",", "begin_norm_axis", "=", "len", "(", "output", ".", "shape", ")", "-", "1", ",", "\n", "epsilon", "=", "1e-12", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_layer_norm_scale'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "1.", ")", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "+", "'_layer_norm_bias'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.head_projection": [[71, 82], ["paddle.layers.create_parameter", "paddle.layers.mul", "paddle.ParamAttr"], "function", ["None"], ["", "def", "head_projection", "(", "h", ",", "d_model", ",", "n_head", ",", "d_head", ",", "param_initializer", ",", "name", "=", "''", ")", ":", "\n", "    ", "\"\"\"Project hidden states to a specific head with a 4D-shape.\"\"\"", "\n", "proj_weight", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "\n", "shape", "=", "[", "d_model", ",", "n_head", ",", "d_head", "]", ",", "\n", "dtype", "=", "h", ".", "dtype", ",", "\n", "attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_weight'", ",", "initializer", "=", "param_initializer", ")", ",", "\n", "is_bias", "=", "False", ")", "\n", "\n", "# ibh,hnd->ibnd ", "\n", "head", "=", "fluid", ".", "layers", ".", "mul", "(", "x", "=", "h", ",", "y", "=", "proj_weight", ",", "x_num_col_dims", "=", "2", ",", "y_num_col_dims", "=", "1", ")", "\n", "return", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.post_attention": [[84, 116], ["paddle.layers.create_parameter", "paddle.layers.transpose", "paddle.layers.mul", "paddle.layers.dropout", "paddle.layers.layer_norm", "paddle.layers.layer_norm", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.ParamAttr", "len", "len", "paddle.initializer.Constant", "paddle.initializer.Constant", "paddle.initializer.Constant", "paddle.initializer.Constant"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout", "home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.layer_norm", "home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.layer_norm"], ["", "def", "post_attention", "(", "h", ",", "attn_vec", ",", "d_model", ",", "n_head", ",", "d_head", ",", "dropout", ",", "\n", "param_initializer", ",", "residual", "=", "True", ",", "name", "=", "''", ")", ":", "\n", "    ", "\"\"\"Post-attention processing.\"\"\"", "\n", "# post-attention projection (back to `d_model`)", "\n", "proj_o", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "\n", "shape", "=", "[", "d_model", ",", "n_head", ",", "d_head", "]", ",", "\n", "dtype", "=", "h", ".", "dtype", ",", "\n", "attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_o_weight'", ",", "initializer", "=", "param_initializer", ")", ",", "\n", "is_bias", "=", "False", ")", "\n", "# ibnd,hnd->ibh", "\n", "proj_o", "=", "fluid", ".", "layers", ".", "transpose", "(", "proj_o", ",", "perm", "=", "[", "1", ",", "2", ",", "0", "]", ")", "\n", "attn_out", "=", "fluid", ".", "layers", ".", "mul", "(", "x", "=", "attn_vec", ",", "y", "=", "proj_o", ",", "x_num_col_dims", "=", "2", ",", "y_num_col_dims", "=", "2", ")", "\n", "\n", "attn_out", "=", "fluid", ".", "layers", ".", "dropout", "(", "attn_out", ",", "dropout_prob", "=", "dropout", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "is_test", "=", "False", ")", "\n", "\n", "if", "residual", ":", "\n", "        ", "output", "=", "fluid", ".", "layers", ".", "layer_norm", "(", "attn_out", "+", "h", ",", "begin_norm_axis", "=", "len", "(", "attn_out", ".", "shape", ")", "-", "1", ",", "\n", "epsilon", "=", "1e-12", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_layer_norm_scale'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "1.", ")", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "+", "'_layer_norm_bias'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "output", "=", "fluid", ".", "layers", ".", "layer_norm", "(", "attn_out", ",", "begin_norm_axis", "=", "len", "(", "attn_out", ".", "shape", ")", "-", "1", ",", "\n", "epsilon", "=", "1e-12", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_layer_norm_scale'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "1.", ")", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "+", "'_layer_norm_bias'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.abs_attn_core": [[118, 136], ["modeling.einsum4x4", "paddle.layers.softmax", "paddle.layers.dropout", "modeling.einsum4x4"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.einsum4x4", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.einsum4x4"], ["", "def", "abs_attn_core", "(", "q_head", ",", "k_head", ",", "v_head", ",", "attn_mask", ",", "dropatt", ",", "scale", ")", ":", "\n", "    ", "\"\"\"Core absolute positional attention operations.\"\"\"", "\n", "\n", "attn_score", "=", "einsum4x4", "(", "'ibnd,jbnd->ijbn'", ",", "q_head", ",", "k_head", ")", "\n", "\n", "attn_score", "*=", "scale", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "        ", "attn_score", "=", "attn_score", "-", "1e30", "*", "attn_mask", "\n", "\n", "# attention probability", "\n", "", "attn_prob", "=", "fluid", ".", "layers", ".", "softmax", "(", "attn_score", ",", "axis", "=", "1", ")", "\n", "attn_prob", "=", "fluid", ".", "layers", ".", "dropout", "(", "attn_prob", ",", "dropout_prob", "=", "dropatt", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "is_test", "=", "False", ")", "\n", "\n", "# attention output", "\n", "attn_vec", "=", "einsum4x4", "(", "'ijbn,jbnd->ibnd'", ",", "attn_prob", ",", "v_head", ")", "\n", "\n", "return", "attn_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.rel_attn_core": [[138, 185], ["modeling.einsum4x4", "modeling.einsum4x4", "modeling.rel_shift", "paddle.layers.transpose", "paddle.layers.softmax", "paddle.layers.transpose", "paddle.layers.dropout", "modeling.einsum4x4", "paddle.layers.elementwise_add", "paddle.layers.elementwise_add", "paddle.layers.stack", "modeling.einsum4x4", "modeling.einsum4x4", "paddle.layers.elementwise_add"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.einsum4x4", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.einsum4x4", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.rel_shift", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.einsum4x4", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.einsum4x4", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.einsum4x4"], ["", "def", "rel_attn_core", "(", "q_head", ",", "k_head_h", ",", "v_head_h", ",", "k_head_r", ",", "seg_embed", ",", "seg_mat", ",", "\n", "r_w_bias", ",", "r_r_bias", ",", "r_s_bias", ",", "attn_mask", ",", "dropatt", ",", "\n", "scale", ")", ":", "\n", "    ", "\"\"\"Core relative positional attention operations.\"\"\"", "\n", "## content based attention score", "\n", "ac", "=", "einsum4x4", "(", "'ibnd,jbnd->ijbn'", ",", "fluid", ".", "layers", ".", "elementwise_add", "(", "q_head", ",", "r_w_bias", ",", "2", ")", ",", "k_head_h", ")", "\n", "\n", "# position based attention score", "\n", "bd", "=", "einsum4x4", "(", "'ibnd,jbnd->ijbn'", ",", "fluid", ".", "layers", ".", "elementwise_add", "(", "q_head", ",", "r_r_bias", ",", "2", ")", ",", "k_head_r", ")", "\n", "\n", "#klen = fluid.layers.slice(fluid.layers.shape(ac), axes=[0], starts=[1], ends=[2])", "\n", "\n", "bd", "=", "rel_shift", "(", "bd", ",", "klen", "=", "ac", ".", "shape", "[", "1", "]", ")", "\n", "\n", "# segment based attention score", "\n", "if", "seg_mat", "is", "None", ":", "\n", "        ", "ef", "=", "0", "\n", "", "else", ":", "\n", "        ", "ef", "=", "0", "\n", "\"\"\"\n        bsz = fluid.layers.slice(fluid.layers.shape(q_head), axes=[0], starts=[1], ends=[2])\n        bsz.stop_gradient = True\n        \"\"\"", "\n", "#seg_embed = fluid.layers.unsqueeze(input=seg_embed, axes=[0])", "\n", "seg_embed", "=", "fluid", ".", "layers", ".", "stack", "(", "[", "seg_embed", "]", "*", "q_head", ".", "shape", "[", "0", "]", ",", "axis", "=", "0", ")", "\n", "\n", "ef", "=", "einsum4x4", "(", "'ibnd,isnd->ibns'", ",", "fluid", ".", "layers", ".", "elementwise_add", "(", "q_head", ",", "r_s_bias", ",", "2", ")", ",", "seg_embed", ")", "\n", "ef", "=", "einsum4x4", "(", "'ijbs,ibns->ijbn'", ",", "seg_mat", ",", "ef", ")", "\n", "# merge attention scores and perform masking", "\n", "\n", "", "attn_score", "=", "(", "ac", "+", "bd", "+", "ef", ")", "*", "scale", "\n", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "# attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask", "\n", "        ", "attn_score", "=", "attn_score", "-", "1e30", "*", "attn_mask", "\n", "\n", "# attention probability", "\n", "#attn_prob = fluid.layers.softmax(attn_score, axis=1)", "\n", "", "attn_score", "=", "fluid", ".", "layers", ".", "transpose", "(", "attn_score", ",", "[", "0", ",", "2", ",", "3", ",", "1", "]", ")", "\n", "attn_prob", "=", "fluid", ".", "layers", ".", "softmax", "(", "attn_score", ")", "\n", "attn_prob", "=", "fluid", ".", "layers", ".", "transpose", "(", "attn_prob", ",", "[", "0", ",", "3", ",", "1", ",", "2", "]", ")", "\n", "attn_prob", "=", "fluid", ".", "layers", ".", "dropout", "(", "attn_prob", ",", "dropatt", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ")", "\n", "\n", "# attention output", "\n", "attn_vec", "=", "einsum4x4", "(", "'ijbn,jbnd->ibnd'", ",", "attn_prob", ",", "v_head_h", ")", "\n", "return", "attn_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.rel_shift": [[187, 196], ["paddle.layers.reshape", "paddle.layers.slice", "paddle.layers.reshape", "paddle.layers.slice"], "function", ["None"], ["", "def", "rel_shift", "(", "x", ",", "klen", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\"perform relative shift to form the relative attention score.\"\"\"", "\n", "x_size", "=", "x", ".", "shape", "\n", "x", "=", "fluid", ".", "layers", ".", "reshape", "(", "x", ",", "[", "x_size", "[", "1", "]", ",", "x_size", "[", "0", "]", ",", "x_size", "[", "2", "]", ",", "x_size", "[", "3", "]", "]", ")", "\n", "x", "=", "fluid", ".", "layers", ".", "slice", "(", "x", ",", "axes", "=", "[", "0", "]", ",", "starts", "=", "[", "1", "]", ",", "ends", "=", "[", "x_size", "[", "1", "]", "]", ")", "\n", "x", "=", "fluid", ".", "layers", ".", "reshape", "(", "x", ",", "[", "x_size", "[", "0", "]", ",", "x_size", "[", "1", "]", "-", "1", ",", "x_size", "[", "2", "]", ",", "x_size", "[", "3", "]", "]", ")", "\n", "x", "=", "fluid", ".", "layers", ".", "slice", "(", "x", ",", "axes", "=", "[", "1", "]", ",", "starts", "=", "[", "0", "]", ",", "ends", "=", "[", "klen", "]", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling._cache_mem": [[198, 213], ["tf.concat"], "function", ["None"], ["", "def", "_cache_mem", "(", "curr_out", ",", "prev_mem", ",", "mem_len", ",", "reuse_len", "=", "None", ")", ":", "\n", "    ", "\"\"\"cache hidden states into memory.\"\"\"", "\n", "if", "mem_len", "is", "None", "or", "mem_len", "==", "0", ":", "\n", "        ", "return", "None", "\n", "", "else", ":", "\n", "        ", "if", "reuse_len", "is", "not", "None", "and", "reuse_len", ">", "0", ":", "\n", "            ", "curr_out", "=", "curr_out", "[", ":", "reuse_len", "]", "\n", "\n", "", "if", "prev_mem", "is", "None", ":", "\n", "            ", "new_mem", "=", "curr_out", "[", "-", "mem_len", ":", "]", "\n", "", "else", ":", "\n", "            ", "new_mem", "=", "tf", ".", "concat", "(", "[", "prev_mem", ",", "curr_out", "]", ",", "0", ")", "[", "-", "mem_len", ":", "]", "\n", "\n", "", "", "new_mem", ".", "stop_gradient", "=", "True", "\n", "return", "new_mem", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.relative_positional_encoding": [[215, 261], ["paddle.layers.range", "tf.cast", "paddle.layers.range", "paddle.layers.range", "paddle.layers.concat", "paddle.layers.range", "modeling.positional_embedding", "paddle.layers.reshape", "ValueError", "paddle.layers.cast", "paddle.layers.cast", "paddle.layers.clip", "paddle.layers.clip", "modeling.positional_embedding", "modeling.positional_embedding", "modeling.positional_embedding", "modeling.positional_embedding", "paddle.layers.cast", "paddle.layers.clip"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.positional_embedding", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.positional_embedding", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.positional_embedding", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.positional_embedding", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.positional_embedding"], ["", "def", "relative_positional_encoding", "(", "qlen", ",", "klen", ",", "d_model", ",", "clamp_len", ",", "attn_type", ",", "\n", "bi_data", ",", "bsz", "=", "None", ",", "dtype", "=", "None", ")", ":", "\n", "    ", "\"\"\"create relative positional encoding.\"\"\"", "\n", "freq_seq", "=", "fluid", ".", "layers", ".", "range", "(", "0", ",", "d_model", ",", "2.0", ",", "'float32'", ")", "\n", "if", "dtype", "is", "not", "None", "and", "dtype", "!=", "'float32'", ":", "\n", "        ", "freq_seq", "=", "tf", ".", "cast", "(", "freq_seq", ",", "dtype", "=", "dtype", ")", "\n", "", "inv_freq", "=", "1", "/", "(", "10000", "**", "(", "freq_seq", "/", "d_model", ")", ")", "\n", "\n", "if", "attn_type", "==", "'bi'", ":", "\n", "        ", "beg", ",", "end", "=", "klen", ",", "-", "qlen", "\n", "", "elif", "attn_type", "==", "'uni'", ":", "\n", "        ", "beg", ",", "end", "=", "klen", ",", "-", "1", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown `attn_type` {}.'", ".", "format", "(", "attn_type", ")", ")", "\n", "\n", "", "if", "bi_data", ":", "\n", "        ", "fwd_pos_seq", "=", "fluid", ".", "layers", ".", "range", "(", "beg", ",", "end", ",", "-", "1.0", ",", "'float32'", ")", "\n", "bwd_pos_seq", "=", "fluid", ".", "layers", ".", "range", "(", "-", "beg", ",", "-", "end", ",", "1.0", ",", "'float32'", ")", "\n", "\n", "if", "dtype", "is", "not", "None", "and", "dtype", "!=", "'float32'", ":", "\n", "            ", "fwd_pos_seq", "=", "fluid", ".", "layers", ".", "cast", "(", "fwd_pos_seq", ",", "dtype", "=", "'float32'", ")", "\n", "bwd_pos_seq", "=", "fluid", ".", "layers", ".", "cast", "(", "bwd_pos_seq", ",", "dtype", "=", "'float32'", ")", "\n", "\n", "", "if", "clamp_len", ">", "0", ":", "\n", "            ", "fwd_pos_seq", "=", "fluid", ".", "layers", ".", "clip", "(", "fwd_pos_seq", ",", "-", "clamp_len", ",", "clamp_len", ")", "\n", "bwd_pos_seq", "=", "fluid", ".", "layers", ".", "clip", "(", "bwd_pos_seq", ",", "-", "clamp_len", ",", "clamp_len", ")", "\n", "\n", "", "if", "bsz", "is", "not", "None", ":", "\n", "# With bi_data, the batch size should be divisible by 2.", "\n", "            ", "assert", "bsz", "%", "2", "==", "0", "\n", "fwd_pos_emb", "=", "positional_embedding", "(", "fwd_pos_seq", ",", "inv_freq", ",", "bsz", "//", "2", ")", "\n", "bwd_pos_emb", "=", "positional_embedding", "(", "bwd_pos_seq", ",", "inv_freq", ",", "bsz", "//", "2", ")", "\n", "", "else", ":", "\n", "            ", "fwd_pos_emb", "=", "positional_embedding", "(", "fwd_pos_seq", ",", "inv_freq", ")", "\n", "bwd_pos_emb", "=", "positional_embedding", "(", "bwd_pos_seq", ",", "inv_freq", ")", "\n", "\n", "", "pos_emb", "=", "fluid", ".", "layers", ".", "concat", "(", "[", "fwd_pos_emb", ",", "bwd_pos_emb", "]", ",", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "fwd_pos_seq", "=", "fluid", ".", "layers", ".", "range", "(", "beg", ",", "end", ",", "-", "1.0", ",", "'float32'", ")", "\n", "if", "dtype", "is", "not", "None", "and", "dtype", "!=", "'float32'", ":", "\n", "            ", "fwd_pos_seq", "=", "fluid", ".", "layers", ".", "cast", "(", "fwd_pos_seq", ",", "dtype", "=", "dtype", ")", "\n", "", "if", "clamp_len", ">", "0", ":", "\n", "            ", "fwd_pos_seq", "=", "fluid", ".", "layers", ".", "clip", "(", "fwd_pos_seq", ",", "-", "clamp_len", ",", "clamp_len", ")", "\n", "", "pos_emb", "=", "positional_embedding", "(", "fwd_pos_seq", ",", "inv_freq", ",", "bsz", ")", "\n", "fluid", ".", "layers", ".", "reshape", "(", "pos_emb", ",", "[", "2", "*", "qlen", ",", "-", "1", ",", "d_model", "]", ",", "inplace", "=", "True", ")", "\n", "", "return", "pos_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.rel_multihead_attn": [[263, 295], ["modeling.head_projection", "modeling.head_projection", "modeling.head_projection", "modeling.head_projection", "modeling.rel_attn_core", "modeling.post_attention", "paddle.layers.concat", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.head_projection", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.head_projection", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.head_projection", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.head_projection", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.rel_attn_core", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.post_attention"], ["", "def", "rel_multihead_attn", "(", "h", ",", "r", ",", "r_w_bias", ",", "r_r_bias", ",", "seg_mat", ",", "r_s_bias", ",", "seg_embed", ",", "\n", "attn_mask", ",", "mems", ",", "d_model", ",", "n_head", ",", "d_head", ",", "dropout", ",", "\n", "dropatt", ",", "initializer", ",", "name", "=", "''", ")", ":", "\n", "    ", "\"\"\"Multi-head attention with relative positional encoding.\"\"\"", "\n", "\n", "scale", "=", "1", "/", "(", "d_head", "**", "0.5", ")", "\n", "if", "mems", "is", "not", "None", "and", "len", "(", "mems", ".", "shape", ")", ">", "1", ":", "\n", "        ", "cat", "=", "fluid", ".", "layers", ".", "concat", "(", "[", "mems", ",", "h", "]", ",", "0", ")", "\n", "", "else", ":", "\n", "        ", "cat", "=", "h", "\n", "\n", "# content heads", "\n", "", "q_head_h", "=", "head_projection", "(", "\n", "h", ",", "d_model", ",", "n_head", ",", "d_head", ",", "initializer", ",", "name", "+", "'_rel_attn_q'", ")", "\n", "k_head_h", "=", "head_projection", "(", "\n", "cat", ",", "d_model", ",", "n_head", ",", "d_head", ",", "initializer", ",", "name", "+", "'_rel_attn_k'", ")", "\n", "v_head_h", "=", "head_projection", "(", "\n", "cat", ",", "d_model", ",", "n_head", ",", "d_head", ",", "initializer", ",", "name", "+", "'_rel_attn_v'", ")", "\n", "\n", "# positional heads", "\n", "k_head_r", "=", "head_projection", "(", "\n", "r", ",", "d_model", ",", "n_head", ",", "d_head", ",", "initializer", ",", "name", "+", "'_rel_attn_r'", ")", "\n", "\n", "# core attention ops", "\n", "attn_vec", "=", "rel_attn_core", "(", "\n", "q_head_h", ",", "k_head_h", ",", "v_head_h", ",", "k_head_r", ",", "seg_embed", ",", "seg_mat", ",", "r_w_bias", ",", "\n", "r_r_bias", ",", "r_s_bias", ",", "attn_mask", ",", "dropatt", ",", "scale", ")", "\n", "\n", "# post processing", "\n", "output", "=", "post_attention", "(", "h", ",", "attn_vec", ",", "d_model", ",", "n_head", ",", "d_head", ",", "dropout", ",", "initializer", ",", "name", "=", "name", "+", "'_rel_attn'", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.transformer_xl": [[297, 556], ["print", "print", "paddle.layers.slice", "paddle.layers.create_parameter", "paddle.layers.embedding", "paddle.layers.dropout", "modeling.relative_positional_encoding", "paddle.layers.dropout", "range", "paddle.layers.shape", "paddle.layers.create_global_var", "paddle.layers.zeros", "paddle.layers.expand", "paddle.layers.concat", "print", "paddle.layers.cast", "paddle.layers.diag", "paddle.layers.concat", "print", "paddle.layers.expand", "paddle.layers.unsqueeze", "paddle.layers.expand", "paddle.layers.cast", "paddle.layers.create_parameter", "paddle.layers.create_parameter", "paddle.layers.create_parameter", "paddle.layers.create_parameter", "paddle.layers.create_parameter", "paddle.layers.fill_constant_batch_size_like", "paddle.layers.concat", "paddle.layers.stack", "paddle.layers.stack", "paddle.layers.transpose", "paddle.layers.cast", "paddle.layers.transpose", "paddle.layers.unsqueeze", "paddle.layers.one_hot", "modeling.positionwise_ffn", "paddle.layers.dropout", "paddle.layers.dropout", "ValueError", "paddle.layers.unsqueeze", "paddle.layers.unsqueeze", "print", "paddle.layers.unsqueeze", "paddle.layers.unsqueeze", "numpy.array().astype", "paddle.layers.slice", "paddle.layers.squeeze", "paddle.layers.slice", "paddle.layers.squeeze", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.layers.create_parameter", "paddle.layers.create_parameter", "paddle.layers.slice", "paddle.layers.squeeze", "paddle.layers.logical_not", "modeling.rel_multihead_attn", "paddle.layers.zeros", "paddle.ParamAttr", "range", "range", "paddle.ParamAttr", "range", "range", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.layers.slice", "paddle.layers.squeeze", "paddle.ParamAttr", "range", "range", "paddle.layers.equal", "numpy.array", "paddle.ParamAttr", "range", "range", "paddle.ParamAttr"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.relative_positional_encoding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.positionwise_ffn", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.rel_multihead_attn"], ["", "def", "transformer_xl", "(", "inp_k", ",", "n_token", ",", "n_layer", ",", "d_model", ",", "n_head", ",", "\n", "d_head", ",", "d_inner", ",", "dropout", ",", "dropatt", ",", "attn_type", ",", "\n", "bi_data", ",", "initializer", ",", "mem_len", "=", "None", ",", "\n", "inp_q", "=", "None", ",", "mems", "=", "None", ",", "\n", "same_length", "=", "False", ",", "clamp_len", "=", "-", "1", ",", "untie_r", "=", "False", ",", "\n", "input_mask", "=", "None", ",", "\n", "perm_mask", "=", "None", ",", "seg_id", "=", "None", ",", "reuse_len", "=", "None", ",", "\n", "ff_activation", "=", "'relu'", ",", "target_mapping", "=", "None", ",", "\n", "use_fp16", "=", "False", ",", "name", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    Defines a Transformer-XL computation graph with additional\n    support for XLNet.\n    Args:\n    inp_k: int32 Tensor in shape [len, bsz], the input token IDs.\n    seg_id: int32 Tensor in shape [len, bsz], the input segment IDs.\n    input_mask: float32 Tensor in shape [len, bsz], the input mask.\n        0 for real tokens and 1 for padding.\n    mems: a list of float32 Tensors in shape [mem_len, bsz, d_model], memory\n        from previous batches. The length of the list equals n_layer.\n        If None, no memory is used.\n    perm_mask: float32 Tensor in shape [len, len, bsz].\n        If perm_mask[i, j, k] = 0, i attend to j in batch k;\n        if perm_mask[i, j, k] = 1, i does not attend to j in batch k.\n        If None, each position attends to all the others.\n    target_mapping: float32 Tensor in shape [num_predict, len, bsz].\n        If target_mapping[i, j, k] = 1, the i-th predict in batch k is\n        on the j-th token.\n        Only used during pretraining for partial prediction.\n        Set to None during finetuning.\n    inp_q: float32 Tensor in shape [len, bsz].\n        1 for tokens with losses and 0 for tokens without losses.\n        Only used during pretraining for two-stream attention.\n        Set to None during finetuning.\n    n_layer: int, the number of layers.\n    d_model: int, the hidden size.\n    n_head: int, the number of attention heads.\n    d_head: int, the dimension size of each attention head.\n    d_inner: int, the hidden size in feed-forward layers.\n    ff_activation: str, \"relu\" or \"gelu\".\n    untie_r: bool, whether to untie the biases in attention.\n    n_token: int, the vocab size.\n    is_training: bool, whether in training mode.\n    use_tpu: bool, whether TPUs are used.\n    use_fp16: bool, use bfloat16 instead of float32.\n    dropout: float, dropout rate.\n    dropatt: float, dropout rate on attention probabilities.\n    init: str, the initialization scheme, either \"normal\" or \"uniform\".\n    init_range: float, initialize the parameters with a uniform distribution\n        in [-init_range, init_range]. Only effective when init=\"uniform\".\n    init_std: float, initialize the parameters with a normal distribution\n        with mean 0 and stddev init_std. Only effective when init=\"normal\".\n    mem_len: int, the number of tokens to cache.\n    reuse_len: int, the number of tokens in the currect batch to be cached\n        and reused in the future.\n    bi_data: bool, whether to use bidirectional input pipeline.\n        Usually set to True during pretraining and False during finetuning.\n    clamp_len: int, clamp all relative distances larger than clamp_len.\n        -1 means no clamping.\n    same_length: bool, whether to use the same attention length for each token.\n    summary_type: str, \"last\", \"first\", \"mean\", or \"attn\". The method\n        to pool the input to get a vector representation.\n    initializer: A tf initializer.\n    scope: scope name for the computation graph.\n    \"\"\"", "\n", "print", "(", "'memory input {}'", ".", "format", "(", "mems", ")", ")", "\n", "data_type", "=", "\"float16\"", "if", "use_fp16", "else", "\"float32\"", "\n", "print", "(", "'Use float type {}'", ".", "format", "(", "data_type", ")", ")", "\n", "\n", "qlen", "=", "inp_k", ".", "shape", "[", "0", "]", "\n", "mlen", "=", "mems", "[", "0", "]", ".", "shape", "[", "0", "]", "if", "mems", "is", "not", "None", "else", "0", "\n", "klen", "=", "mlen", "+", "qlen", "\n", "bsz", "=", "fluid", ".", "layers", ".", "slice", "(", "fluid", ".", "layers", ".", "shape", "(", "inp_k", ")", ",", "axes", "=", "[", "0", "]", ",", "starts", "=", "[", "1", "]", ",", "ends", "=", "[", "2", "]", ")", "\n", "\n", "##### Attention mask", "\n", "# causal attention mask", "\n", "if", "attn_type", "==", "'uni'", ":", "\n", "        ", "attn_mask", "=", "fluid", ".", "layers", ".", "create_global_var", "(", "\n", "name", "=", "'attn_mask'", ",", "\n", "shape", "=", "[", "qlen", ",", "klen", ",", "1", ",", "1", "]", ",", "\n", "value", "=", "0.0", ",", "\n", "dtype", "=", "data_type", ",", "persistable", "=", "True", ")", "\n", "", "elif", "attn_type", "==", "'bi'", ":", "\n", "        ", "attn_mask", "=", "None", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unsupported attention type: {}'", ".", "format", "(", "attn_type", ")", ")", "\n", "\n", "# data mask: input mask & perm mask", "\n", "", "if", "input_mask", "is", "not", "None", "and", "perm_mask", "is", "not", "None", ":", "\n", "        ", "data_mask", "=", "fluid", ".", "layers", ".", "unsqueeze", "(", "input_mask", ",", "[", "0", "]", ")", "+", "perm_mask", "\n", "", "elif", "input_mask", "is", "not", "None", "and", "perm_mask", "is", "None", ":", "\n", "        ", "data_mask", "=", "fluid", ".", "layers", ".", "unsqueeze", "(", "input_mask", ",", "[", "0", "]", ")", "\n", "print", "(", "\"input mask shape\"", ",", "input_mask", ".", "shape", ")", "\n", "", "elif", "input_mask", "is", "None", "and", "perm_mask", "is", "not", "None", ":", "\n", "        ", "data_mask", "=", "perm_mask", "\n", "", "else", ":", "\n", "        ", "data_mask", "=", "None", "\n", "\n", "", "if", "data_mask", "is", "not", "None", ":", "\n", "# all mems can be attended to", "\n", "        ", "mems_mask", "=", "fluid", ".", "layers", ".", "zeros", "(", "shape", "=", "[", "data_mask", ".", "shape", "[", "0", "]", ",", "mlen", ",", "1", "]", ",", "dtype", "=", "'float32'", ")", "\n", "mems_mask", "=", "fluid", ".", "layers", ".", "expand", "(", "mems_mask", ",", "[", "1", ",", "1", ",", "bsz", "]", ")", "\n", "data_mask", "=", "fluid", ".", "layers", ".", "concat", "(", "[", "mems_mask", ",", "data_mask", "]", ",", "1", ")", "\n", "if", "attn_mask", "is", "None", ":", "\n", "            ", "attn_mask", "=", "fluid", ".", "layers", ".", "unsqueeze", "(", "data_mask", ",", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "attn_mask", "+=", "fluid", ".", "layers", ".", "unsqueeze", "(", "data_mask", ",", "[", "-", "1", "]", ")", "\n", "", "print", "(", "\"mems_mask, data_mask, attn mask shape\"", ",", "mems_mask", ".", "shape", ",", "data_mask", ".", "shape", ",", "attn_mask", ".", "shape", ")", "\n", "", "if", "attn_mask", "is", "not", "None", ":", "\n", "        ", "attn_mask", "=", "fluid", ".", "layers", ".", "cast", "(", "attn_mask", ">", "0", ",", "dtype", "=", "data_type", ")", "\n", "\n", "", "if", "attn_mask", "is", "not", "None", ":", "\n", "        ", "non_tgt_mask", "=", "fluid", ".", "layers", ".", "diag", "(", "np", ".", "array", "(", "[", "-", "1", "]", "*", "qlen", ")", ".", "astype", "(", "data_type", ")", ")", "\n", "non_tgt_mask", "=", "fluid", ".", "layers", ".", "concat", "(", "[", "fluid", ".", "layers", ".", "zeros", "(", "[", "qlen", ",", "mlen", "]", ",", "dtype", "=", "data_type", ")", ",", "\n", "non_tgt_mask", "]", ",", "axis", "=", "-", "1", ")", "\n", "print", "(", "\"attn_mask, non_tgt_mask shape\"", ",", "attn_mask", ".", "shape", ",", "non_tgt_mask", ".", "shape", ")", "\n", "attn_mask", "=", "fluid", ".", "layers", ".", "expand", "(", "attn_mask", ",", "[", "qlen", ",", "1", ",", "1", ",", "1", "]", ")", "\n", "non_tgt_mask", "=", "fluid", ".", "layers", ".", "unsqueeze", "(", "non_tgt_mask", ",", "axes", "=", "[", "2", ",", "3", "]", ")", "\n", "non_tgt_mask", "=", "fluid", ".", "layers", ".", "expand", "(", "non_tgt_mask", ",", "[", "1", ",", "1", ",", "bsz", ",", "1", "]", ")", "\n", "non_tgt_mask", "=", "fluid", ".", "layers", ".", "cast", "(", "(", "attn_mask", "+", "non_tgt_mask", ")", ">", "0", ",", "\n", "dtype", "=", "data_type", ")", "\n", "non_tgt_mask", ".", "stop_gradient", "=", "True", "\n", "", "else", ":", "\n", "        ", "non_tgt_mask", "=", "None", "\n", "\n", "", "if", "untie_r", ":", "\n", "        ", "r_w_bias", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "shape", "=", "[", "n_layer", ",", "n_head", ",", "d_head", "]", ",", "dtype", "=", "data_type", ",", "\n", "attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_r_w_bias'", ",", "initializer", "=", "initializer", ")", ",", "\n", "is_bias", "=", "True", ")", "\n", "r_w_bias", "=", "[", "fluid", ".", "layers", ".", "slice", "(", "r_w_bias", ",", "axes", "=", "[", "0", "]", ",", "starts", "=", "[", "i", "]", ",", "ends", "=", "[", "i", "+", "1", "]", ")", "for", "i", "in", "range", "(", "n_layer", ")", "]", "\n", "r_w_bias", "=", "[", "fluid", ".", "layers", ".", "squeeze", "(", "r_w_bias", "[", "i", "]", ",", "axes", "=", "[", "0", "]", ")", "for", "i", "in", "range", "(", "n_layer", ")", "]", "\n", "r_r_bias", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "shape", "=", "[", "n_layer", ",", "n_head", ",", "d_head", "]", ",", "dtype", "=", "data_type", ",", "\n", "attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_r_r_bias'", ",", "initializer", "=", "initializer", ")", ",", "\n", "is_bias", "=", "True", ")", "\n", "r_r_bias", "=", "[", "fluid", ".", "layers", ".", "slice", "(", "r_r_bias", ",", "axes", "=", "[", "0", "]", ",", "starts", "=", "[", "i", "]", ",", "ends", "=", "[", "i", "+", "1", "]", ")", "for", "i", "in", "range", "(", "n_layer", ")", "]", "\n", "r_r_bias", "=", "[", "fluid", ".", "layers", ".", "squeeze", "(", "r_r_bias", "[", "i", "]", ",", "axes", "=", "[", "0", "]", ")", "for", "i", "in", "range", "(", "n_layer", ")", "]", "\n", "", "else", ":", "\n", "        ", "r_w_bias", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "shape", "=", "[", "n_head", ",", "d_head", "]", ",", "dtype", "=", "data_type", ",", "\n", "attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_r_w_bias'", ",", "initializer", "=", "initializer", ")", ",", "\n", "is_bias", "=", "True", ")", "\n", "r_r_bias", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "shape", "=", "[", "n_head", ",", "d_head", "]", ",", "dtype", "=", "data_type", ",", "\n", "attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_r_r_bias'", ",", "initializer", "=", "initializer", ")", ",", "\n", "is_bias", "=", "True", ")", "\n", "\n", "", "lookup_table", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "shape", "=", "[", "n_token", ",", "d_model", "]", ",", "dtype", "=", "data_type", ",", "\n", "attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_word_embedding'", ",", "\n", "initializer", "=", "initializer", ")", ",", "\n", "is_bias", "=", "True", ")", "\n", "word_emb_k", "=", "fluid", ".", "layers", ".", "embedding", "(", "\n", "input", "=", "inp_k", ",", "\n", "size", "=", "[", "n_token", ",", "d_model", "]", ",", "\n", "dtype", "=", "data_type", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_word_embedding'", ",", "initializer", "=", "initializer", ")", ")", "\n", "\n", "if", "inp_q", "is", "not", "None", ":", "\n", "        ", "pass", "\n", "\n", "", "output_h", "=", "fluid", ".", "layers", ".", "dropout", "(", "word_emb_k", ",", "dropout_prob", "=", "dropout", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ")", "\n", "\n", "if", "inp_q", "is", "not", "None", ":", "\n", "        ", "pass", "\n", "\n", "", "if", "seg_id", "is", "not", "None", ":", "\n", "        ", "if", "untie_r", ":", "\n", "            ", "r_s_bias", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "shape", "=", "[", "n_layer", ",", "n_head", ",", "d_head", "]", ",", "dtype", "=", "data_type", ",", "\n", "attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_r_s_bias'", ",", "initializer", "=", "initializer", ")", ",", "\n", "is_bias", "=", "True", ")", "\n", "r_s_bias", "=", "[", "fluid", ".", "layers", ".", "slice", "(", "r_s_bias", ",", "axes", "=", "[", "0", "]", ",", "starts", "=", "[", "i", "]", ",", "ends", "=", "[", "i", "+", "1", "]", ")", "for", "i", "in", "range", "(", "n_layer", ")", "]", "\n", "r_s_bias", "=", "[", "fluid", ".", "layers", ".", "squeeze", "(", "r_s_bias", "[", "i", "]", ",", "axes", "=", "[", "0", "]", ")", "for", "i", "in", "range", "(", "n_layer", ")", "]", "\n", "", "else", ":", "\n", "            ", "r_s_bias", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "shape", "=", "[", "n_head", ",", "d_head", "]", ",", "dtype", "=", "data_type", ",", "\n", "attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_r_s_bias'", ",", "initializer", "=", "initializer", ")", ",", "\n", "is_bias", "=", "True", ")", "\n", "\n", "", "seg_embed", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "shape", "=", "[", "n_layer", ",", "2", ",", "n_head", ",", "d_head", "]", ",", "\n", "dtype", "=", "data_type", ",", "attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_seg_embed'", ",", "\n", "initializer", "=", "initializer", ")", ")", "\n", "seg_embed", "=", "[", "fluid", ".", "layers", ".", "slice", "(", "seg_embed", ",", "axes", "=", "[", "0", "]", ",", "starts", "=", "[", "i", "]", ",", "ends", "=", "[", "i", "+", "1", "]", ")", "for", "i", "in", "range", "(", "n_layer", ")", "]", "\n", "seg_embed", "=", "[", "fluid", ".", "layers", ".", "squeeze", "(", "seg_embed", "[", "i", "]", ",", "axes", "=", "[", "0", "]", ")", "for", "i", "in", "range", "(", "n_layer", ")", "]", "\n", "\n", "# COnver `seg_id` to one-hot seg_mat", "\n", "# seg_id: [bsz, qlen, 1]", "\n", "mem_pad", "=", "fluid", ".", "layers", ".", "fill_constant_batch_size_like", "(", "input", "=", "seg_id", ",", "shape", "=", "[", "-", "1", ",", "mlen", "]", ",", "value", "=", "0", ",", "dtype", "=", "'int64'", ")", "\n", "# cat_ids: [bsz, klen, 1]", "\n", "cat_ids", "=", "fluid", ".", "layers", ".", "concat", "(", "input", "=", "[", "mem_pad", ",", "seg_id", "]", ",", "axis", "=", "1", ")", "\n", "seg_id", "=", "fluid", ".", "layers", ".", "stack", "(", "[", "seg_id", "]", "*", "klen", ",", "axis", "=", "2", ")", "\n", "cat_ids", "=", "fluid", ".", "layers", ".", "stack", "(", "[", "cat_ids", "]", "*", "qlen", ",", "axis", "=", "2", ")", "\n", "cat_ids", "=", "fluid", ".", "layers", ".", "transpose", "(", "cat_ids", ",", "perm", "=", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "\n", "# seg_mat: [bsz, qlen, klen]", "\n", "seg_mat", "=", "fluid", ".", "layers", ".", "cast", "(", "\n", "fluid", ".", "layers", ".", "logical_not", "(", "fluid", ".", "layers", ".", "equal", "(", "seg_id", ",", "cat_ids", ")", ")", ",", "\n", "dtype", "=", "'int64'", ")", "\n", "\n", "seg_mat", "=", "fluid", ".", "layers", ".", "transpose", "(", "seg_mat", ",", "perm", "=", "[", "1", ",", "2", ",", "0", "]", ")", "\n", "seg_mat", "=", "fluid", ".", "layers", ".", "unsqueeze", "(", "seg_mat", ",", "[", "-", "1", "]", ")", "\n", "seg_mat", "=", "fluid", ".", "layers", ".", "one_hot", "(", "seg_mat", ",", "2", ")", "\n", "seg_mat", ".", "stop_gradient", "=", "True", "\n", "", "else", ":", "\n", "        ", "seg_mat", "=", "None", "\n", "\n", "", "pos_emb", "=", "relative_positional_encoding", "(", "\n", "qlen", ",", "klen", ",", "d_model", ",", "clamp_len", ",", "attn_type", ",", "bi_data", ",", "\n", "bsz", "=", "bsz", ",", "dtype", "=", "data_type", ")", "\n", "pos_emb", "=", "fluid", ".", "layers", ".", "dropout", "(", "pos_emb", ",", "dropout", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ")", "\n", "pos_emb", ".", "stop_gradient", "=", "True", "\n", "##### Attention layers", "\n", "if", "mems", "is", "None", ":", "\n", "        ", "mems", "=", "[", "None", "]", "*", "n_layer", "\n", "", "for", "i", "in", "range", "(", "n_layer", ")", ":", "\n", "# cache new mems", "\n", "#new_mems.append(_cache_mem(output_h, mems[i], mem_len, reuse_len)) ", "\n", "\n", "# segment bias", "\n", "        ", "if", "seg_id", "is", "None", ":", "\n", "            ", "r_s_bias_i", "=", "None", "\n", "seg_embed_i", "=", "None", "\n", "", "else", ":", "\n", "            ", "r_s_bias_i", "=", "r_s_bias", "if", "not", "untie_r", "else", "r_s_bias", "[", "i", "]", "\n", "seg_embed_i", "=", "seg_embed", "[", "i", "]", "\n", "\n", "", "if", "inp_q", "is", "not", "None", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "output_h", "=", "rel_multihead_attn", "(", "\n", "h", "=", "output_h", ",", "\n", "r", "=", "pos_emb", ",", "\n", "r_w_bias", "=", "r_w_bias", "if", "not", "untie_r", "else", "r_w_bias", "[", "i", "]", ",", "\n", "r_r_bias", "=", "r_r_bias", "if", "not", "untie_r", "else", "r_r_bias", "[", "i", "]", ",", "\n", "seg_mat", "=", "seg_mat", ",", "\n", "r_s_bias", "=", "r_s_bias_i", ",", "\n", "seg_embed", "=", "seg_embed_i", ",", "\n", "attn_mask", "=", "non_tgt_mask", ",", "\n", "mems", "=", "mems", "[", "i", "]", ",", "\n", "d_model", "=", "d_model", ",", "\n", "n_head", "=", "n_head", ",", "\n", "d_head", "=", "d_head", ",", "\n", "dropout", "=", "dropout", ",", "\n", "dropatt", "=", "dropatt", ",", "\n", "initializer", "=", "initializer", ",", "\n", "name", "=", "name", "+", "'_layer_{}'", ".", "format", "(", "i", ")", ")", "\n", "\n", "", "if", "inp_q", "is", "not", "None", ":", "\n", "            ", "pass", "\n", "\n", "", "output_h", "=", "positionwise_ffn", "(", "inp", "=", "output_h", ",", "d_model", "=", "d_model", ",", "\n", "d_inner", "=", "d_inner", ",", "dropout_prob", "=", "dropout", ",", "\n", "param_initializer", "=", "initializer", ",", "\n", "act_type", "=", "ff_activation", ",", "name", "=", "name", "+", "'_layer_{}_ff'", ".", "format", "(", "i", ")", ")", "\n", "\n", "", "if", "inp_q", "is", "not", "None", ":", "\n", "        ", "output", "=", "fluid", ".", "layers", ".", "dropout", "(", "output_g", ",", "dropout", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ")", "\n", "", "else", ":", "\n", "        ", "output", "=", "fluid", ".", "layers", ".", "dropout", "(", "output_h", ",", "dropout", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ")", "\n", "", "new_mems", "=", "None", "\n", "return", "output", ",", "new_mems", ",", "lookup_table", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.lm_loss": [[558, 583], ["paddle.layers.create_parameter", "paddle.layers.softmax_cross_entropy_with_logits", "paddle.layers.create_parameter", "paddle.layers.matmul", "paddle.ParamAttr", "paddle.ParamAttr"], "function", ["None"], ["", "def", "lm_loss", "(", "hidden", ",", "target", ",", "n_token", ",", "d_model", ",", "initializer", ",", "lookup_table", "=", "None", ",", "\n", "tie_weight", "=", "False", ",", "bi_data", "=", "True", ")", ":", "\n", "\n", "    ", "if", "tie_weight", ":", "\n", "        ", "assert", "lookup_table", "is", "not", "None", ",", "'lookup_table cannot be None for tie_weight'", "\n", "softmax_w", "=", "lookup_table", "\n", "", "else", ":", "\n", "        ", "softmax_w", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "\n", "shape", "=", "[", "n_token", ",", "d_model", "]", ",", "\n", "dtype", "=", "hidden", ".", "dtype", ",", "\n", "attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "'model_loss_weight'", ",", "initializer", "=", "initializer", ")", ",", "\n", "is_bias", "=", "False", ")", "\n", "\n", "", "softmax_b", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "\n", "shape", "=", "[", "n_token", "]", ",", "\n", "dtype", "=", "hidden", ".", "dtype", ",", "\n", "attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "'model_lm_loss_bias'", ",", "initializer", "=", "initializer", ")", ",", "\n", "is_bias", "=", "False", ")", "\n", "\n", "logits", "=", "fluid", ".", "layers", ".", "matmul", "(", "x", "=", "hidden", ",", "y", "=", "softmax_w", ",", "transpose_y", "=", "True", ")", "+", "softmax_b", "\n", "\n", "loss", "=", "fluid", ".", "layers", ".", "softmax_cross_entropy_with_logits", "(", "input", "=", "logits", ",", "label", "=", "target", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.summarize_sequence": [[584, 635], ["tf.variable_scope", "tf.layers.dropout", "tf.layers.dense", "tf.reduce_mean", "tf.get_variable", "tf.tile", "multihead_attn", "ValueError", "tf.shape"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["", "def", "summarize_sequence", "(", "summary_type", ",", "hidden", ",", "d_model", ",", "n_head", ",", "d_head", ",", "dropout", ",", "\n", "dropatt", ",", "input_mask", ",", "is_training", ",", "initializer", ",", "\n", "scope", "=", "None", ",", "reuse", "=", "None", ",", "use_proj", "=", "True", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Different classification tasks may not may not share the same parameters\n    to summarize the sequence features.\n    If shared, one can keep the `scope` to the default value `None`.\n    Otherwise, one should specify a different `scope` for each task.\n    \"\"\"", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "scope", ",", "'sequnece_summary'", ",", "reuse", "=", "reuse", ")", ":", "\n", "        ", "if", "summary_type", "==", "'last'", ":", "\n", "            ", "summary", "=", "hidden", "[", "-", "1", "]", "\n", "", "elif", "summary_type", "==", "'first'", ":", "\n", "            ", "summary", "=", "hidden", "[", "0", "]", "\n", "", "elif", "summary_type", "==", "'mean'", ":", "\n", "            ", "summary", "=", "tf", ".", "reduce_mean", "(", "hidden", ",", "axis", "=", "0", ")", "\n", "", "elif", "summary_type", "==", "'attn'", ":", "\n", "            ", "bsz", "=", "tf", ".", "shape", "(", "hidden", ")", "[", "1", "]", "\n", "\n", "summary_bias", "=", "tf", ".", "get_variable", "(", "'summary_bias'", ",", "[", "d_model", "]", ",", "\n", "dtype", "=", "hidden", ".", "dtype", ",", "\n", "initializer", "=", "initializer", ")", "\n", "summary_bias", "=", "tf", ".", "tile", "(", "summary_bias", "[", "None", ",", "None", "]", ",", "[", "1", ",", "bsz", ",", "1", "]", ")", "\n", "\n", "if", "input_mask", "is", "not", "None", ":", "\n", "                ", "input_mask", "=", "input_mask", "[", "None", ",", ":", ",", ":", ",", "None", "]", "\n", "\n", "", "summary", "=", "multihead_attn", "(", "summary_bias", ",", "hidden", ",", "hidden", ",", "input_mask", ",", "\n", "d_model", ",", "n_head", ",", "d_head", ",", "dropout", ",", "dropatt", ",", "\n", "is_training", ",", "initializer", ",", "residual", "=", "False", ")", "\n", "summary", "=", "summary", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unsupported summary type {}'", ".", "format", "(", "summary_type", ")", ")", "\n", "\n", "# use another projection as in BERT", "\n", "", "if", "use_proj", ":", "\n", "            ", "summary", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "summary", ",", "\n", "d_model", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "\n", "initializer", "=", "initializer", ",", "\n", "name", "=", "'summary'", ")", "\n", "\n", "# dropout", "\n", "", "summary", "=", "tf", ".", "layers", ".", "dropout", "(", "\n", "summary", ",", "dropout", ",", "training", "=", "is_training", ",", "\n", "name", "=", "'dropout'", ")", "\n", "\n", "", "return", "summary", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.classification_loss": [[637, 658], ["paddle.layers.fc", "paddle.layers.one_hot", "paddle.layers.reduce_sum", "paddle.ParamAttr", "paddle.layers.log_softmax"], "function", ["None"], ["", "def", "classification_loss", "(", "hidden", ",", "labels", ",", "n_class", ",", "initializer", ",", "name", ",", "reuse", "=", "None", ",", "\n", "return_logits", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n      Different classification tasks should use different scope names to ensure\n      different dense layers (parameters) are used to produce the logits.\n      An exception will be in transfer learning, where one hopes to transfer\n      the classification weights.\n    \"\"\"", "\n", "\n", "logits", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "hidden", ",", "\n", "size", "=", "n_class", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_logits'", ",", "initializer", "=", "initializer", ")", ")", "\n", "\n", "one_hot_target", "=", "fluid", ".", "layers", ".", "one_hot", "(", "labels", ",", "depth", "=", "n_class", ",", "dtype", "=", "hidden", ".", "dtype", ")", "\n", "loss", "=", "-", "fluid", ".", "layers", ".", "reduce_sum", "(", "fluid", ".", "layers", ".", "log_softmax", "(", "logits", ")", "*", "one_hot_target", ",", "-", "1", ")", "\n", "\n", "if", "return_logits", ":", "\n", "        ", "return", "loss", ",", "logits", "\n", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.regression_loss": [[660, 675], ["paddle.layers.fc", "tf.squeeze", "tf.square", "paddle.ParamAttr"], "function", ["None"], ["", "def", "regression_loss", "(", "hidden", ",", "labels", ",", "initializer", ",", "name", "=", "'transformer'", ",", "\n", "return_logits", "=", "False", ")", ":", "\n", "\n", "    ", "logits", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "hidden", ",", "\n", "size", "=", "1", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "name", "+", "'_logits'", ",", "initializer", "=", "initializer", ")", ")", "\n", "\n", "logits", "=", "tf", ".", "squeeze", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "loss", "=", "tf", ".", "square", "(", "logits", "-", "labels", ")", "\n", "\n", "if", "return_logits", ":", "\n", "        ", "return", "loss", ",", "logits", "\n", "\n", "", "return", "loss", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.server_utils.BasicMRCService.__init__": [[53, 61], ["logging.getLogger"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "name", ",", "logger", "=", "None", ",", "log_data", "=", "False", ")", ":", "\n", "        ", "\"\"\" \"\"\"", "\n", "self", ".", "name", "=", "name", "\n", "if", "logger", "is", "None", ":", "\n", "            ", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "'flask'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "logger", "=", "logger", "\n", "", "self", ".", "log_data", "=", "log_data", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.server_utils.BasicMRCService.__call__": [[62, 177], ["flask.request.get_json", "server_utils.BasicMRCService._response_constructor", "time.time", "server_utils._request_check", "server_utils._split_input_json", "server_utils._abort", "print", "print", "time.time", "time.time", "server_utils._timmer", "server_utils.BasicMRCService.logger.error", "server_utils.BasicMRCService.logger.exception", "server_utils._abort", "model.preprocessor", "zip", "len", "len", "print", "print", "print", "print", "model.postprocessor", "server_utils.BasicMRCService.results.update", "logging.info", "server_utils.BasicMRCService.logger.info", "time.time", "time.time", "server_utils._timmer", "server_utils.BasicMRCService.logger.error", "server_utils.BasicMRCService.logger.exception", "server_utils._abort", "time.time", "len", "len", "time.time", "server_utils._timmer", "server_utils.BasicMRCService.logger.error", "server_utils.BasicMRCService.logger.exception", "server_utils._abort", "time.time", "time.time", "server_utils._timmer", "server_utils.BasicMRCService.logger.error", "server_utils.BasicMRCService.logger.exception", "server_utils._abort", "len", "print", "model.call_mrc", "NotImplementedError", "json.dumps", "json.dumps", "mrc_results.extend", "model.call_mrc"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service.BasicMRCService._response_constructor", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._request_check", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._split_input_json", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.preprocessor", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.postprocessor", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.call_mrc", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.call_mrc"], ["", "def", "__call__", "(", "self", ",", "model", ",", "process_mode", "=", "'serial'", ",", "max_batch_size", "=", "5", ",", "timmer", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            mode: serial, parallel\n        \"\"\"", "\n", "if", "timmer", ":", "\n", "            ", "start", "=", "time", ".", "time", "(", ")", "\n", "", "\"\"\"Call mrc model wrapper and handle expectations\"\"\"", "\n", "self", ".", "input_json", "=", "request", ".", "get_json", "(", "silent", "=", "True", ")", "\n", "try", ":", "\n", "            ", "if", "timmer", ":", "\n", "                ", "start_request_check", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "request_status", "=", "_request_check", "(", "self", ".", "input_json", ")", "\n", "jsons", "=", "_split_input_json", "(", "self", ".", "input_json", ")", "\n", "\n", "if", "timmer", ":", "\n", "                ", "current_time", "=", "time", ".", "time", "(", ")", "\n", "_timmer", "(", "start", ",", "start_request_check", ",", "current_time", ",", "'request check'", ")", "\n", "", "if", "self", ".", "log_data", ":", "\n", "                ", "if", "self", ".", "logger", "is", "None", ":", "\n", "                    ", "logging", ".", "info", "(", "\n", "'Client input - {}'", ".", "format", "(", "json", ".", "dumps", "(", "self", ".", "input_json", ",", "ensure_ascii", "=", "False", ")", ")", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "logger", ".", "info", "(", "\n", "'Client input - {}'", ".", "format", "(", "json", ".", "dumps", "(", "self", ".", "input_json", ",", "ensure_ascii", "=", "False", ")", ")", "\n", ")", "\n", "", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "error", "(", "'server request checker error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'server request checker error - {}'", ".", "format", "(", "e", ")", ")", "\n", "", "if", "request_status", "!=", "'OK'", ":", "\n", "            ", "return", "_abort", "(", "400", ",", "request_status", ")", "\n", "\n", "", "self", ".", "results", "=", "{", "}", "\n", "for", "single_sample", "in", "jsons", ":", "\n", "# call preprocessor", "\n", "            ", "try", ":", "\n", "                ", "if", "timmer", ":", "\n", "                    ", "start_preprocess", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "example", ",", "features", ",", "batches", "=", "model", ".", "preprocessor", "(", "single_sample", ",", "batch_size", "=", "max_batch_size", "if", "process_mode", "==", "'parallel'", "else", "1", ")", "\n", "\n", "if", "timmer", ":", "\n", "                    ", "current_time", "=", "time", ".", "time", "(", ")", "\n", "_timmer", "(", "start", ",", "start_preprocess", ",", "current_time", ",", "'preprocess'", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "                ", "self", ".", "logger", ".", "error", "(", "'preprocessor error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'preprocessor error - {}'", ".", "format", "(", "e", ")", ")", "\n", "\n", "", "def", "transpose", "(", "mat", ")", ":", "\n", "                ", "return", "zip", "(", "*", "mat", ")", "\n", "\n", "", "print", "(", "len", "(", "features", ")", ")", "\n", "print", "(", "len", "(", "batches", ")", ")", "\n", "\n", "# call mrc", "\n", "try", ":", "\n", "                ", "if", "timmer", ":", "\n", "                    ", "start_call_mrc", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "mrc_results", "=", "[", "]", "\n", "# new_features = []", "\n", "\n", "if", "verbose", ":", "\n", "                    ", "if", "len", "(", "features", ")", ">", "max_batch_size", ":", "\n", "                        ", "print", "(", "\"get a too long example....\"", ")", "\n", "", "", "if", "process_mode", "==", "'serial'", ":", "\n", "                    ", "mrc_results", "=", "[", "model", ".", "call_mrc", "(", "b", ",", "squeeze_dim0", "=", "True", ")", "for", "b", "in", "batches", "[", ":", "max_batch_size", "]", "]", "\n", "", "elif", "process_mode", "==", "'parallel'", ":", "\n", "# only keep first max_batch_size features", "\n", "# batches = batches[0]", "\n", "\n", "                    ", "for", "b", "in", "batches", ":", "\n", "                        ", "mrc_results", ".", "extend", "(", "model", ".", "call_mrc", "(", "b", ",", "return_list", "=", "True", ")", ")", "\n", "", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "(", ")", "\n", "# new_features = features[:max_batch_size]", "\n", "\n", "", "print", "(", "'num examples:'", ")", "\n", "print", "(", "len", "(", "example", ")", ")", "\n", "print", "(", "'num features:'", ")", "\n", "print", "(", "len", "(", "features", ")", ")", "\n", "if", "timmer", ":", "\n", "                    ", "current_time", "=", "time", ".", "time", "(", ")", "\n", "_timmer", "(", "start", ",", "start_call_mrc", ",", "current_time", ",", "'call mrc'", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "                ", "self", ".", "logger", ".", "error", "(", "'call_mrc error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'call_mrc error - {}'", ".", "format", "(", "e", ")", ")", "\n", "\n", "# call post processor", "\n", "", "try", ":", "\n", "                ", "if", "timmer", ":", "\n", "                    ", "start_post_precess", "=", "time", ".", "time", "(", ")", "\n", "", "results", "=", "model", ".", "postprocessor", "(", "example", ",", "features", ",", "mrc_results", ")", "\n", "\n", "# only nbest results is POSTed back", "\n", "self", ".", "results", ".", "update", "(", "results", "[", "1", "]", ")", "\n", "\n", "# self.results = results[1]", "\n", "# # self.results = results[0]", "\n", "\n", "if", "timmer", ":", "\n", "                    ", "current_time", "=", "time", ".", "time", "(", ")", "\n", "_timmer", "(", "start", ",", "start_post_precess", ",", "current_time", ",", "'post process'", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "                ", "self", ".", "logger", ".", "error", "(", "'postprocessor error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'postprocessor error - {}'", ".", "format", "(", "e", ")", ")", "\n", "\n", "\n", "", "", "return", "self", ".", "_response_constructor", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.server_utils.BasicMRCService._response_constructor": [[178, 194], ["flask.Response", "server_utils.BasicMRCService.logger.info", "json.dumps", "server_utils.BasicMRCService.logger.error", "server_utils.BasicMRCService.logger.exception", "server_utils._abort", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort"], ["", "def", "_response_constructor", "(", "self", ")", ":", "\n", "        ", "\"\"\"construct http response object\"\"\"", "\n", "try", ":", "\n", "            ", "response", "=", "{", "\n", "# 'requestID': self.input_json['requestID'],", "\n", "'results'", ":", "self", ".", "results", "\n", "}", "\n", "if", "self", ".", "log_data", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "\n", "'Response - {}'", ".", "format", "(", "json", ".", "dumps", "(", "response", ",", "ensure_ascii", "=", "False", ")", ")", "\n", ")", "\n", "", "return", "Response", "(", "json", ".", "dumps", "(", "response", ")", ",", "mimetype", "=", "'application/json'", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "error", "(", "'response constructor error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'response constructor error - {}'", ".", "format", "(", "e", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.server_utils._request_check": [[15, 27], ["isinstance"], "function", ["None"], ["def", "_request_check", "(", "input_json", ")", ":", "\n", "    ", "\"\"\"Check if the request json is valid\"\"\"", "\n", "if", "input_json", "is", "None", "or", "not", "isinstance", "(", "input_json", ",", "dict", ")", ":", "\n", "        ", "return", "'Can not parse the input json data - {}'", ".", "format", "(", "input_json", ")", "\n", "", "try", ":", "\n", "        ", "c", "=", "input_json", "[", "'context'", "]", "\n", "qa", "=", "input_json", "[", "'qas'", "]", "[", "0", "]", "\n", "qid", "=", "qa", "[", "'qid'", "]", "\n", "q", "=", "qa", "[", "'question'", "]", "\n", "", "except", "KeyError", "as", "e", ":", "\n", "        ", "return", "'Invalid request, key \"{}\" not found'", ".", "format", "(", "e", ")", "\n", "", "return", "'OK'", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.server_utils._abort": [[28, 31], ["flask.Response", "json.dumps"], "function", ["None"], ["", "def", "_abort", "(", "status_code", ",", "message", ")", ":", "\n", "    ", "\"\"\"Create custom error message and status code\"\"\"", "\n", "return", "Response", "(", "json", ".", "dumps", "(", "message", ")", ",", "status", "=", "status_code", ",", "mimetype", "=", "'application/json'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.server_utils._timmer": [[32, 37], ["print"], "function", ["None"], ["", "def", "_timmer", "(", "init_start", ",", "start", ",", "current", ",", "process_name", ")", ":", "\n", "    ", "cumulated_elapsed_time", "=", "(", "current", "-", "init_start", ")", "*", "1000", "\n", "current_elapsed_time", "=", "(", "current", "-", "start", ")", "*", "1000", "\n", "print", "(", "'{}\\t-\\t{:.2f}\\t{:.2f}'", ".", "format", "(", "process_name", ",", "cumulated_elapsed_time", ",", "\n", "current_elapsed_time", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.server_utils._split_input_json": [[38, 50], ["len", "len", "range", "len", "copy.deepcopy", "rets.append"], "function", ["None"], ["", "def", "_split_input_json", "(", "input_json", ")", ":", "\n", "    ", "if", "len", "(", "input_json", "[", "'context_tokens'", "]", ")", ">", "810", ":", "\n", "        ", "input_json", "[", "'context'", "]", "=", "input_json", "[", "'context'", "]", "[", ":", "5000", "]", "\n", "", "if", "len", "(", "input_json", "[", "'qas'", "]", ")", "==", "1", ":", "\n", "        ", "return", "[", "input_json", "]", "\n", "", "else", ":", "\n", "        ", "rets", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "input_json", "[", "'qas'", "]", ")", ")", ":", "\n", "            ", "temp", "=", "deepcopy", "(", "input_json", ")", "\n", "temp", "[", "'qas'", "]", "=", "[", "input_json", "[", "'qas'", "]", "[", "i", "]", "]", "\n", "rets", ".", "append", "(", "temp", ")", "\n", "", "return", "rets", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.SquadExample.__init__": [[46, 59], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "qas_id", ",", "\n", "question_text", ",", "\n", "paragraph_text", ",", "\n", "orig_answer_text", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "is_impossible", "=", "False", ")", ":", "\n", "    ", "self", ".", "qas_id", "=", "qas_id", "\n", "self", ".", "question_text", "=", "question_text", "\n", "self", ".", "paragraph_text", "=", "paragraph_text", "\n", "self", ".", "orig_answer_text", "=", "orig_answer_text", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.SquadExample.__str__": [[60, 62], ["squad_reader.SquadExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.SQuADExample.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.SquadExample.__repr__": [[63, 74], ["prepro_utils.printable_text", "prepro_utils.printable_text"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "    ", "s", "=", "\"\"", "\n", "s", "+=", "\"qas_id: %s\"", "%", "(", "printable_text", "(", "self", ".", "qas_id", ")", ")", "\n", "s", "+=", "\", question_text: %s\"", "%", "(", "\n", "printable_text", "(", "self", ".", "question_text", ")", ")", "\n", "s", "+=", "\", paragraph_text: [%s]\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "paragraph_text", ")", ")", "\n", "if", "self", ".", "start_position", ":", "\n", "      ", "s", "+=", "\", start_position: %d\"", "%", "(", "self", ".", "start_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "      ", "s", "+=", "\", is_impossible: %r\"", "%", "(", "self", ".", "is_impossible", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.InputFeatures.__init__": [[78, 109], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "unique_id", ",", "\n", "example_index", ",", "\n", "doc_span_index", ",", "\n", "tok_start_to_orig_index", ",", "\n", "tok_end_to_orig_index", ",", "\n", "token_is_max_context", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "p_mask", ",", "\n", "segment_ids", ",", "\n", "paragraph_len", ",", "\n", "cls_index", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "None", ")", ":", "\n", "    ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "example_index", "=", "example_index", "\n", "self", ".", "doc_span_index", "=", "doc_span_index", "\n", "self", ".", "tok_start_to_orig_index", "=", "tok_start_to_orig_index", "\n", "self", ".", "tok_end_to_orig_index", "=", "tok_end_to_orig_index", "\n", "self", ".", "token_is_max_context", "=", "token_is_max_context", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "p_mask", "=", "p_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "paragraph_len", "=", "paragraph_len", "\n", "self", ".", "cls_index", "=", "cls_index", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.DataProcessor.__init__": [[528, 544], ["sentencepiece.SentencePieceProcessor", "squad_reader.DataProcessor._sp_model.Load"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "spiece_model_file", ",", "uncased", ",", "max_seq_length", ",", "\n", "doc_stride", ",", "max_query_length", ")", ":", "\n", "        ", "self", ".", "_sp_model", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "self", ".", "_sp_model", ".", "Load", "(", "spiece_model_file", ")", "\n", "self", ".", "_uncased", "=", "uncased", "\n", "self", ".", "_max_seq_length", "=", "max_seq_length", "\n", "self", ".", "_doc_stride", "=", "doc_stride", "\n", "self", ".", "_max_query_length", "=", "max_query_length", "\n", "\n", "self", ".", "current_train_example", "=", "-", "1", "\n", "self", ".", "num_train_examples", "=", "-", "1", "\n", "self", ".", "current_train_epoch", "=", "-", "1", "\n", "\n", "self", ".", "train_examples", "=", "None", "\n", "self", ".", "predict_examples", "=", "None", "\n", "self", ".", "num_examples", "=", "{", "'train'", ":", "-", "1", ",", "'predict'", ":", "-", "1", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.DataProcessor.get_train_progress": [[545, 548], ["None"], "methods", ["None"], ["", "def", "get_train_progress", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets progress for training phase.\"\"\"", "\n", "return", "self", ".", "current_train_example", ",", "self", ".", "current_train_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.DataProcessor.get_examples": [[549, 556], ["squad_reader.read_squad_examples"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.read_squad_examples"], ["", "def", "get_examples", "(", "self", ",", "\n", "sample", ",", "\n", "is_training", ")", ":", "\n", "        ", "examples", "=", "read_squad_examples", "(", "\n", "sample", ",", "\n", "is_training", "=", "is_training", ")", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.DataProcessor.get_num_examples": [[557, 562], ["ValueError"], "methods", ["None"], ["", "def", "get_num_examples", "(", "self", ",", "phase", ")", ":", "\n", "        ", "if", "phase", "not", "in", "[", "'train'", ",", "'predict'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unknown phase, which should be in ['train', 'predict'].\"", ")", "\n", "", "return", "self", ".", "num_examples", "[", "phase", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.DataProcessor.get_features": [[563, 573], ["squad_reader.convert_examples_to_features"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.convert_examples_to_features"], ["", "def", "get_features", "(", "self", ",", "examples", ",", "is_training", ")", ":", "\n", "        ", "features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "examples", ",", "\n", "sp_model", "=", "self", ".", "_sp_model", ",", "\n", "max_seq_length", "=", "self", ".", "_max_seq_length", ",", "\n", "doc_stride", "=", "self", ".", "_doc_stride", ",", "\n", "max_query_length", "=", "self", ".", "_max_query_length", ",", "\n", "is_training", "=", "is_training", ",", "\n", "uncased", "=", "self", ".", "_uncased", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.DataProcessor.data_generator": [[574, 642], ["squad_reader.DataProcessor.get_examples", "len", "squad_reader.DataProcessor.get_features", "squad_reader.DataProcessor.data_generator.batch_reader"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_examples", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_features", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.batch_reader"], ["", "def", "data_generator", "(", "self", ",", "\n", "sample", ",", "\n", "batch_size", ",", "\n", "phase", "=", "'predict'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "epoch", "=", "1", ")", ":", "\n", "        ", "self", ".", "predict_examples", "=", "self", ".", "get_examples", "(", "\n", "sample", ",", "\n", "is_training", "=", "False", ")", "\n", "examples", "=", "self", ".", "predict_examples", "\n", "self", ".", "num_examples", "[", "'predict'", "]", "=", "len", "(", "self", ".", "predict_examples", ")", "\n", "\n", "def", "batch_reader", "(", "features", ",", "batch_size", ")", ":", "\n", "            ", "batch", "=", "[", "]", "\n", "feats", "=", "[", "]", "\n", "for", "(", "index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "                ", "if", "phase", "==", "'train'", ":", "\n", "                    ", "self", ".", "current_train_example", "=", "index", "+", "1", "\n", "", "labels", "=", "[", "feature", ".", "unique_id", "]", "if", "feature", ".", "start_position", "is", "None", "else", "[", "\n", "feature", ".", "start_position", ",", "feature", ".", "end_position", ",", "feature", ".", "is_impossible", "\n", "]", "\n", "example", "=", "[", "\n", "feature", ".", "input_ids", ",", "feature", ".", "segment_ids", ",", "feature", ".", "input_mask", ",", "\n", "feature", ".", "cls_index", ",", "feature", ".", "p_mask", "\n", "]", "+", "labels", "\n", "\n", "to_append", "=", "len", "(", "batch", ")", "<", "batch_size", "\n", "if", "to_append", ":", "\n", "                    ", "batch", ".", "append", "(", "example", ")", "\n", "feats", ".", "append", "(", "feature", ")", "\n", "", "else", ":", "\n", "                    ", "yield", "batch", ",", "feats", "\n", "batch", "=", "[", "example", "]", "\n", "feats", "=", "[", "feature", "]", "\n", "\n", "", "", "if", "len", "(", "batch", ")", ">", "0", ":", "\n", "                ", "yield", "batch", ",", "feats", "\n", "\n", "", "", "def", "prepare_batch_data", "(", "insts", ")", ":", "\n", "            ", "\"\"\"Generate numpy tensors\"\"\"", "\n", "input_ids", "=", "np", ".", "expand_dims", "(", "np", ".", "array", "(", "[", "inst", "[", "0", "]", "for", "inst", "in", "insts", "]", ")", ".", "astype", "(", "'int64'", ")", ",", "axis", "=", "-", "1", ")", "\n", "segment_ids", "=", "np", ".", "array", "(", "[", "inst", "[", "1", "]", "for", "inst", "in", "insts", "]", ")", ".", "astype", "(", "'int64'", ")", "\n", "input_mask", "=", "np", ".", "array", "(", "[", "inst", "[", "2", "]", "for", "inst", "in", "insts", "]", ")", ".", "astype", "(", "'float32'", ")", "\n", "cls_index", "=", "np", ".", "expand_dims", "(", "np", ".", "array", "(", "[", "inst", "[", "3", "]", "for", "inst", "in", "insts", "]", ")", ".", "astype", "(", "'int64'", ")", ",", "axis", "=", "-", "1", ")", "\n", "p_mask", "=", "np", ".", "array", "(", "[", "inst", "[", "4", "]", "for", "inst", "in", "insts", "]", ")", ".", "astype", "(", "'float32'", ")", "\n", "\n", "ret_list", "=", "[", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "cls_index", ",", "p_mask", "]", "\n", "if", "phase", "==", "'train'", ":", "\n", "                ", "start_positions", "=", "np", ".", "expand_dims", "(", "np", ".", "array", "(", "[", "inst", "[", "5", "]", "for", "inst", "in", "insts", "]", ")", ".", "astype", "(", "'int64'", ")", ",", "axis", "=", "-", "1", ")", "\n", "end_positions", "=", "np", ".", "expand_dims", "(", "np", ".", "array", "(", "[", "inst", "[", "6", "]", "for", "inst", "in", "insts", "]", ")", ".", "astype", "(", "'int64'", ")", ",", "axis", "=", "-", "1", ")", "\n", "is_impossible", "=", "np", ".", "expand_dims", "(", "np", ".", "array", "(", "[", "inst", "[", "7", "]", "for", "inst", "in", "insts", "]", ")", ".", "astype", "(", "'float32'", ")", ",", "axis", "=", "-", "1", ")", "\n", "ret_list", "+=", "[", "start_positions", ",", "end_positions", ",", "is_impossible", "]", "\n", "", "else", ":", "\n", "                ", "unique_ids", "=", "np", ".", "expand_dims", "(", "np", ".", "array", "(", "[", "inst", "[", "5", "]", "for", "inst", "in", "insts", "]", ")", ".", "astype", "(", "'int64'", ")", ",", "axis", "=", "-", "1", ")", "\n", "ret_list", "+=", "[", "unique_ids", "]", "\n", "\n", "", "return", "ret_list", "\n", "\n", "", "feature_gen", "=", "self", ".", "get_features", "(", "examples", ",", "is_training", "=", "False", ")", "\n", "\n", "all_dev_batches", "=", "[", "]", "\n", "features", "=", "[", "]", "\n", "for", "batch_insts", ",", "feats", "in", "batch_reader", "(", "feature_gen", ",", "batch_size", ")", ":", "\n", "            ", "batch_data", "=", "prepare_batch_data", "(", "batch_insts", ")", "\n", "all_dev_batches", ".", "append", "(", "batch_data", ")", "\n", "features", ".", "extend", "(", "feats", ")", "\n", "", "return", "examples", ",", "features", ",", "all_dev_batches", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.read_squad_examples": [[111, 132], ["re.sub", "squad_reader.SquadExample", "examples.append"], "function", ["None"], ["", "", "def", "read_squad_examples", "(", "sample", ",", "is_training", ")", ":", "\n", "  ", "\"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"", "\n", "\n", "examples", "=", "[", "]", "\n", "paragraph_text", "=", "sample", "[", "\"context\"", "]", "\n", "paragraph_text", "=", "re", ".", "sub", "(", "r'\\[TLE\\]|\\[DOC\\]|\\[PAR\\]'", ",", "'[SEP]'", ",", "paragraph_text", ")", "\n", "\n", "for", "qa", "in", "sample", "[", "\"qas\"", "]", ":", "\n", "    ", "qas_id", "=", "qa", "[", "\"qid\"", "]", "\n", "question_text", "=", "qa", "[", "\"question\"", "]", "\n", "start_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "is_impossible", "=", "False", "\n", "\n", "example", "=", "SquadExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "paragraph_text", "=", "paragraph_text", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader._convert_index": [[133, 168], ["len"], "function", ["None"], ["", "def", "_convert_index", "(", "index", ",", "pos", ",", "M", "=", "None", ",", "is_start", "=", "True", ")", ":", "\n", "  ", "if", "index", "[", "pos", "]", "is", "not", "None", ":", "\n", "    ", "return", "index", "[", "pos", "]", "\n", "", "N", "=", "len", "(", "index", ")", "\n", "rear", "=", "pos", "\n", "while", "rear", "<", "N", "-", "1", "and", "index", "[", "rear", "]", "is", "None", ":", "\n", "    ", "rear", "+=", "1", "\n", "", "front", "=", "pos", "\n", "while", "front", ">", "0", "and", "index", "[", "front", "]", "is", "None", ":", "\n", "    ", "front", "-=", "1", "\n", "", "assert", "index", "[", "front", "]", "is", "not", "None", "or", "index", "[", "rear", "]", "is", "not", "None", "\n", "if", "index", "[", "front", "]", "is", "None", ":", "\n", "    ", "if", "index", "[", "rear", "]", ">=", "1", ":", "\n", "      ", "if", "is_start", ":", "\n", "        ", "return", "0", "\n", "", "else", ":", "\n", "        ", "return", "index", "[", "rear", "]", "-", "1", "\n", "", "", "return", "index", "[", "rear", "]", "\n", "", "if", "index", "[", "rear", "]", "is", "None", ":", "\n", "    ", "if", "M", "is", "not", "None", "and", "index", "[", "front", "]", "<", "M", "-", "1", ":", "\n", "      ", "if", "is_start", ":", "\n", "        ", "return", "index", "[", "front", "]", "+", "1", "\n", "", "else", ":", "\n", "        ", "return", "M", "-", "1", "\n", "", "", "return", "index", "[", "front", "]", "\n", "", "if", "is_start", ":", "\n", "    ", "if", "index", "[", "rear", "]", ">", "index", "[", "front", "]", "+", "1", ":", "\n", "      ", "return", "index", "[", "front", "]", "+", "1", "\n", "", "else", ":", "\n", "      ", "return", "index", "[", "rear", "]", "\n", "", "", "else", ":", "\n", "    ", "if", "index", "[", "rear", "]", ">", "index", "[", "front", "]", "+", "1", ":", "\n", "      ", "return", "index", "[", "rear", "]", "-", "1", "\n", "", "else", ":", "\n", "      ", "return", "index", "[", "front", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.convert_examples_to_features": [[170, 490], ["numpy.zeros", "enumerate", "print", "prepro_utils.encode_ids", "prepro_utils.encode_pieces", "enumerate", "range", "range", "list", "collections.namedtuple", "enumerate", "print", "prepro_utils.preprocess_text", "len", "prepro_utils.preprocess_text", "chartok_to_tok_index.extend", "tok_start_to_chartok_index.append", "len", "tok_end_to_chartok_index.append", "len", "len", "max", "max", "numpy.zeros", "gc.collect", "np.zeros.fill", "g.clear", "range", "abs", "squad_reader.convert_examples_to_features._lcs_match"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.prepro_utils.encode_ids", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.prepro_utils.encode_pieces", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.prepro_utils.preprocess_text", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.prepro_utils.preprocess_text"], ["", "", "", "def", "convert_examples_to_features", "(", "examples", ",", "sp_model", ",", "max_seq_length", ",", "\n", "doc_stride", ",", "max_query_length", ",", "is_training", ",", "\n", "uncased", ")", ":", "\n", "  ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "cnt_pos", ",", "cnt_neg", "=", "0", ",", "0", "\n", "unique_id", "=", "1000000000", "\n", "max_N", ",", "max_M", "=", "1024", ",", "1024", "\n", "f", "=", "np", ".", "zeros", "(", "(", "max_N", ",", "max_M", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "\n", "    ", "if", "example_index", "%", "100", "==", "0", ":", "\n", "      ", "print", "(", "'Converting {}/{} pos {} neg {}'", ".", "format", "(", "\n", "example_index", ",", "len", "(", "examples", ")", ",", "cnt_pos", ",", "cnt_neg", ")", ")", "\n", "\n", "", "query_tokens", "=", "encode_ids", "(", "\n", "sp_model", ",", "\n", "preprocess_text", "(", "example", ".", "question_text", ",", "lower", "=", "uncased", ")", ")", "\n", "\n", "if", "len", "(", "query_tokens", ")", ">", "max_query_length", ":", "\n", "      ", "query_tokens", "=", "query_tokens", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "paragraph_text", "=", "example", ".", "paragraph_text", "\n", "para_tokens", "=", "encode_pieces", "(", "\n", "sp_model", ",", "\n", "preprocess_text", "(", "example", ".", "paragraph_text", ",", "lower", "=", "uncased", ")", ")", "\n", "\n", "chartok_to_tok_index", "=", "[", "]", "\n", "tok_start_to_chartok_index", "=", "[", "]", "\n", "tok_end_to_chartok_index", "=", "[", "]", "\n", "char_cnt", "=", "0", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "para_tokens", ")", ":", "\n", "      ", "chartok_to_tok_index", ".", "extend", "(", "[", "i", "]", "*", "len", "(", "token", ")", ")", "\n", "tok_start_to_chartok_index", ".", "append", "(", "char_cnt", ")", "\n", "char_cnt", "+=", "len", "(", "token", ")", "\n", "tok_end_to_chartok_index", ".", "append", "(", "char_cnt", "-", "1", ")", "\n", "\n", "", "tok_cat_text", "=", "''", ".", "join", "(", "para_tokens", ")", ".", "replace", "(", "SPIECE_UNDERLINE", ",", "' '", ")", "\n", "N", ",", "M", "=", "len", "(", "paragraph_text", ")", ",", "len", "(", "tok_cat_text", ")", "\n", "\n", "if", "N", ">", "max_N", "or", "M", ">", "max_M", ":", "\n", "      ", "max_N", "=", "max", "(", "N", ",", "max_N", ")", "\n", "max_M", "=", "max", "(", "M", ",", "max_M", ")", "\n", "f", "=", "np", ".", "zeros", "(", "(", "max_N", ",", "max_M", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "gc", ".", "collect", "(", ")", "\n", "\n", "", "g", "=", "{", "}", "\n", "\n", "def", "_lcs_match", "(", "max_dist", ")", ":", "\n", "      ", "f", ".", "fill", "(", "0", ")", "\n", "g", ".", "clear", "(", ")", "\n", "\n", "### longest common sub sequence", "\n", "# f[i, j] = max(f[i - 1, j], f[i, j - 1], f[i - 1, j - 1] + match(i, j))", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "\n", "# note(zhiliny):", "\n", "# unlike standard LCS, this is specifically optimized for the setting", "\n", "# because the mismatch between sentence pieces and original text will", "\n", "# be small", "\n", "        ", "for", "j", "in", "range", "(", "i", "-", "max_dist", ",", "i", "+", "max_dist", ")", ":", "\n", "          ", "if", "j", ">=", "M", "or", "j", "<", "0", ":", "continue", "\n", "\n", "if", "i", ">", "0", ":", "\n", "            ", "g", "[", "(", "i", ",", "j", ")", "]", "=", "0", "\n", "f", "[", "i", ",", "j", "]", "=", "f", "[", "i", "-", "1", ",", "j", "]", "\n", "\n", "", "if", "j", ">", "0", "and", "f", "[", "i", ",", "j", "-", "1", "]", ">", "f", "[", "i", ",", "j", "]", ":", "\n", "            ", "g", "[", "(", "i", ",", "j", ")", "]", "=", "1", "\n", "f", "[", "i", ",", "j", "]", "=", "f", "[", "i", ",", "j", "-", "1", "]", "\n", "\n", "", "f_prev", "=", "f", "[", "i", "-", "1", ",", "j", "-", "1", "]", "if", "i", ">", "0", "and", "j", ">", "0", "else", "0", "\n", "if", "(", "preprocess_text", "(", "paragraph_text", "[", "i", "]", ",", "lower", "=", "uncased", ",", "\n", "remove_space", "=", "False", ")", "\n", "==", "tok_cat_text", "[", "j", "]", "\n", "and", "f_prev", "+", "1", ">", "f", "[", "i", ",", "j", "]", ")", ":", "\n", "            ", "g", "[", "(", "i", ",", "j", ")", "]", "=", "2", "\n", "f", "[", "i", ",", "j", "]", "=", "f_prev", "+", "1", "\n", "\n", "", "", "", "", "max_dist", "=", "abs", "(", "N", "-", "M", ")", "+", "5", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "      ", "_lcs_match", "(", "max_dist", ")", "\n", "if", "f", "[", "N", "-", "1", ",", "M", "-", "1", "]", ">", "0.8", "*", "N", ":", "break", "\n", "max_dist", "*=", "2", "\n", "\n", "", "orig_to_chartok_index", "=", "[", "None", "]", "*", "N", "\n", "chartok_to_orig_index", "=", "[", "None", "]", "*", "M", "\n", "i", ",", "j", "=", "N", "-", "1", ",", "M", "-", "1", "\n", "while", "i", ">=", "0", "and", "j", ">=", "0", ":", "\n", "      ", "if", "(", "i", ",", "j", ")", "not", "in", "g", ":", "break", "\n", "if", "g", "[", "(", "i", ",", "j", ")", "]", "==", "2", ":", "\n", "        ", "orig_to_chartok_index", "[", "i", "]", "=", "j", "\n", "chartok_to_orig_index", "[", "j", "]", "=", "i", "\n", "i", ",", "j", "=", "i", "-", "1", ",", "j", "-", "1", "\n", "", "elif", "g", "[", "(", "i", ",", "j", ")", "]", "==", "1", ":", "\n", "        ", "j", "=", "j", "-", "1", "\n", "", "else", ":", "\n", "        ", "i", "=", "i", "-", "1", "\n", "\n", "", "", "if", "all", "(", "v", "is", "None", "for", "v", "in", "orig_to_chartok_index", ")", "or", "f", "[", "N", "-", "1", ",", "M", "-", "1", "]", "<", "0.8", "*", "N", ":", "\n", "      ", "print", "(", "'MISMATCH DETECTED!'", ")", "\n", "continue", "\n", "\n", "", "tok_start_to_orig_index", "=", "[", "]", "\n", "tok_end_to_orig_index", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "para_tokens", ")", ")", ":", "\n", "      ", "start_chartok_pos", "=", "tok_start_to_chartok_index", "[", "i", "]", "\n", "end_chartok_pos", "=", "tok_end_to_chartok_index", "[", "i", "]", "\n", "start_orig_pos", "=", "_convert_index", "(", "chartok_to_orig_index", ",", "start_chartok_pos", ",", "\n", "N", ",", "is_start", "=", "True", ")", "\n", "end_orig_pos", "=", "_convert_index", "(", "chartok_to_orig_index", ",", "end_chartok_pos", ",", "\n", "N", ",", "is_start", "=", "False", ")", "\n", "\n", "tok_start_to_orig_index", ".", "append", "(", "start_orig_pos", ")", "\n", "tok_end_to_orig_index", ".", "append", "(", "end_orig_pos", ")", "\n", "\n", "", "if", "not", "is_training", ":", "\n", "      ", "tok_start_position", "=", "tok_end_position", "=", "None", "\n", "\n", "", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "      ", "tok_start_position", "=", "-", "1", "\n", "tok_end_position", "=", "-", "1", "\n", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "      ", "start_position", "=", "example", ".", "start_position", "\n", "end_position", "=", "start_position", "+", "len", "(", "example", ".", "orig_answer_text", ")", "-", "1", "\n", "\n", "start_chartok_pos", "=", "_convert_index", "(", "orig_to_chartok_index", ",", "start_position", ",", "\n", "is_start", "=", "True", ")", "\n", "tok_start_position", "=", "chartok_to_tok_index", "[", "start_chartok_pos", "]", "\n", "\n", "end_chartok_pos", "=", "_convert_index", "(", "orig_to_chartok_index", ",", "end_position", ",", "\n", "is_start", "=", "False", ")", "\n", "tok_end_position", "=", "chartok_to_tok_index", "[", "end_chartok_pos", "]", "\n", "assert", "tok_start_position", "<=", "tok_end_position", "\n", "\n", "", "def", "_piece_to_id", "(", "x", ")", ":", "\n", "      ", "if", "six", ".", "PY2", "and", "isinstance", "(", "x", ",", "unicode", ")", ":", "\n", "        ", "x", "=", "x", ".", "encode", "(", "'utf-8'", ")", "\n", "", "return", "sp_model", ".", "PieceToId", "(", "x", ")", "\n", "\n", "", "all_doc_tokens", "=", "list", "(", "map", "(", "_piece_to_id", ",", "para_tokens", ")", ")", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "max_tokens_for_doc", "=", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "# We can have documents that are longer than the maximum sequence length.", "\n", "# To deal with this we do a sliding window approach, where we take chunks", "\n", "# of the up to our max length with a stride of `doc_stride`.", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "      ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "        ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "        ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "      ", "tokens", "=", "[", "]", "\n", "token_is_max_context", "=", "{", "}", "\n", "segment_ids", "=", "[", "]", "\n", "p_mask", "=", "[", "]", "\n", "\n", "cur_tok_start_to_orig_index", "=", "[", "]", "\n", "cur_tok_end_to_orig_index", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "        ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "\n", "cur_tok_start_to_orig_index", ".", "append", "(", "\n", "tok_start_to_orig_index", "[", "split_token_index", "]", ")", "\n", "cur_tok_end_to_orig_index", ".", "append", "(", "\n", "tok_end_to_orig_index", "[", "split_token_index", "]", ")", "\n", "\n", "is_max_context", "=", "_check_is_max_context", "(", "doc_spans", ",", "doc_span_index", ",", "\n", "split_token_index", ")", "\n", "token_is_max_context", "[", "len", "(", "tokens", ")", "]", "=", "is_max_context", "\n", "tokens", ".", "append", "(", "all_doc_tokens", "[", "split_token_index", "]", ")", "\n", "segment_ids", ".", "append", "(", "SEG_ID_P", ")", "\n", "p_mask", ".", "append", "(", "0", ")", "\n", "\n", "", "paragraph_len", "=", "len", "(", "tokens", ")", "\n", "\n", "tokens", ".", "append", "(", "SEP_ID", ")", "\n", "segment_ids", ".", "append", "(", "SEG_ID_P", ")", "\n", "p_mask", ".", "append", "(", "1", ")", "\n", "\n", "# note(zhiliny): we put P before Q", "\n", "# because during pretraining, B is always shorter than A", "\n", "for", "token", "in", "query_tokens", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "SEG_ID_Q", ")", "\n", "p_mask", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "SEP_ID", ")", "\n", "segment_ids", ".", "append", "(", "SEG_ID_Q", ")", "\n", "p_mask", ".", "append", "(", "1", ")", "\n", "\n", "cls_index", "=", "len", "(", "segment_ids", ")", "\n", "tokens", ".", "append", "(", "CLS_ID", ")", "\n", "segment_ids", ".", "append", "(", "SEG_ID_CLS", ")", "\n", "p_mask", ".", "append", "(", "0", ")", "\n", "\n", "input_ids", "=", "tokens", "\n", "\n", "# The mask has 0 for real tokens and 1 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "        ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "1", ")", "\n", "segment_ids", ".", "append", "(", "SEG_ID_PAD", ")", "\n", "p_mask", ".", "append", "(", "1", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "p_mask", ")", "==", "max_seq_length", "\n", "\n", "span_is_impossible", "=", "example", ".", "is_impossible", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "if", "is_training", "and", "not", "span_is_impossible", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "        ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "out_of_span", "=", "False", "\n", "if", "not", "(", "tok_start_position", ">=", "doc_start", "and", "\n", "tok_end_position", "<=", "doc_end", ")", ":", "\n", "          ", "out_of_span", "=", "True", "\n", "", "if", "out_of_span", ":", "\n", "# continue", "\n", "          ", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "span_is_impossible", "=", "True", "\n", "", "else", ":", "\n", "# note(zhiliny): we put P before Q, so doc_offset should be zero.", "\n", "# doc_offset = len(query_tokens) + 2", "\n", "          ", "doc_offset", "=", "0", "\n", "start_position", "=", "tok_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "tok_end_position", "-", "doc_start", "+", "doc_offset", "\n", "\n", "", "", "if", "is_training", "and", "span_is_impossible", ":", "\n", "        ", "start_position", "=", "cls_index", "\n", "end_position", "=", "cls_index", "\n", "\n", "", "if", "example_index", "<", "0", ":", "\n", "        ", "print", "(", "\"*** Example ***\"", ")", "\n", "print", "(", "\"unique_id: %s\"", "%", "(", "unique_id", ")", ")", "\n", "print", "(", "\"example_index: %s\"", "%", "(", "example_index", ")", ")", "\n", "print", "(", "\"doc_span_index: %s\"", "%", "(", "doc_span_index", ")", ")", "\n", "print", "(", "\"tok_start_to_orig_index: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "cur_tok_start_to_orig_index", "]", ")", ")", "\n", "print", "(", "\"tok_end_to_orig_index: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "cur_tok_end_to_orig_index", "]", ")", ")", "\n", "print", "(", "\"token_is_max_context: %s\"", "%", "\" \"", ".", "join", "(", "[", "\n", "\"%d:%s\"", "%", "(", "x", ",", "y", ")", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_is_max_context", ")", "\n", "]", ")", ")", "\n", "print", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "print", "(", "\n", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "print", "(", "\n", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "\n", "if", "is_training", "and", "span_is_impossible", ":", "\n", "          ", "print", "(", "\"impossible example span\"", ")", "\n", "\n", "", "if", "is_training", "and", "not", "span_is_impossible", ":", "\n", "          ", "pieces", "=", "[", "sp_model", ".", "IdToPiece", "(", "token", ")", "for", "token", "in", "\n", "tokens", "[", "start_position", ":", "(", "end_position", "+", "1", ")", "]", "]", "\n", "answer_text", "=", "sp_model", ".", "DecodePieces", "(", "pieces", ")", "\n", "print", "(", "\"start_position: %d\"", "%", "(", "start_position", ")", ")", "\n", "print", "(", "\"end_position: %d\"", "%", "(", "end_position", ")", ")", "\n", "print", "(", "\n", "\"answer: %s\"", "%", "(", "printable_text", "(", "answer_text", ")", ")", ")", "\n", "\n", "# note(zhiliny): With multi processing,", "\n", "# the example_index is actually the index within the current process", "\n", "# therefore we use example_index=None to avoid being used in the future.", "\n", "# The current code does not use example_index of training data.", "\n", "", "", "if", "is_training", ":", "\n", "        ", "feat_example_index", "=", "None", "\n", "", "else", ":", "\n", "        ", "feat_example_index", "=", "example_index", "\n", "\n", "", "feature", "=", "InputFeatures", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "example_index", "=", "feat_example_index", ",", "\n", "doc_span_index", "=", "doc_span_index", ",", "\n", "tok_start_to_orig_index", "=", "cur_tok_start_to_orig_index", ",", "\n", "tok_end_to_orig_index", "=", "cur_tok_end_to_orig_index", ",", "\n", "token_is_max_context", "=", "token_is_max_context", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "p_mask", "=", "p_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "paragraph_len", "=", "paragraph_len", ",", "\n", "cls_index", "=", "cls_index", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "span_is_impossible", ")", "\n", "\n", "unique_id", "+=", "1", "\n", "if", "span_is_impossible", ":", "\n", "        ", "cnt_neg", "+=", "1", "\n", "", "else", ":", "\n", "        ", "cnt_pos", "+=", "1", "\n", "\n", "", "yield", "feature", "\n", "\n", "", "", "print", "(", "\"Total number of instances: {} = pos {} neg {}\"", ".", "format", "(", "\n", "cnt_pos", "+", "cnt_neg", ",", "cnt_pos", ",", "cnt_neg", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader._check_is_max_context": [[491, 526], ["enumerate", "min"], "function", ["None"], ["", "def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "  ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "    ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "      ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "      ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "      ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.get_answers": [[652, 785], ["collections.defaultdict", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "example_index_to_features[].append", "enumerate", "sorted", "squad_reader._compute_softmax", "enumerate", "print", "min", "range", "paragraph_text[].strip", "nbest.append", "nbest.append", "total_scores.append", "collections.OrderedDict", "nbest_json.append", "len", "range", "len", "_NbestPrediction", "_NbestPrediction", "sorted.append", "feature.token_is_max_context.get", "_PrelimPrediction"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._compute_softmax"], ["def", "get_answers", "(", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "start_n_top", ",", "end_n_top", ")", ":", "\n", "  ", "\"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"", "\n", "\n", "example_index_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "    ", "example_index_to_features", "[", "feature", ".", "example_index", "]", ".", "append", "(", "feature", ")", "\n", "\n", "", "unique_id_to_result", "=", "{", "}", "\n", "for", "result", "in", "all_results", ":", "\n", "    ", "unique_id_to_result", "[", "result", ".", "unique_id", "]", "=", "result", "\n", "\n", "", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_nbest_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "scores_diff_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "all_examples", ")", ":", "\n", "    ", "features", "=", "example_index_to_features", "[", "example_index", "]", "\n", "\n", "prelim_predictions", "=", "[", "]", "\n", "# keep track of the minimum score of null start+end of position 0", "\n", "score_null", "=", "1000000", "# large and positive", "\n", "\n", "for", "(", "feature_index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "      ", "result", "=", "unique_id_to_result", "[", "feature", ".", "unique_id", "]", "\n", "print", "(", "\"cls_logits\"", ",", "feature", ".", "unique_id", ",", "result", ".", "cls_logits", ")", "\n", "cur_null_score", "=", "result", ".", "cls_logits", "\n", "\n", "# if we could have irrelevant answers, get the min score of irrelevant", "\n", "score_null", "=", "min", "(", "score_null", ",", "cur_null_score", ")", "\n", "\n", "for", "i", "in", "range", "(", "start_n_top", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "end_n_top", ")", ":", "\n", "          ", "start_log_prob", "=", "result", ".", "start_top_log_probs", "[", "i", "]", "\n", "start_index", "=", "result", ".", "start_top_index", "[", "i", "]", "\n", "\n", "j_index", "=", "i", "*", "end_n_top", "+", "j", "\n", "\n", "end_log_prob", "=", "result", ".", "end_top_log_probs", "[", "j_index", "]", "\n", "end_index", "=", "result", ".", "end_top_index", "[", "j_index", "]", "\n", "\n", "# We could hypothetically create invalid predictions, e.g., predict", "\n", "# that the start of the span is in the question. We throw out all", "\n", "# invalid predictions.", "\n", "if", "start_index", ">=", "feature", ".", "paragraph_len", "-", "1", ":", "\n", "            ", "continue", "\n", "", "if", "end_index", ">=", "feature", ".", "paragraph_len", "-", "1", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "not", "feature", ".", "token_is_max_context", ".", "get", "(", "start_index", ",", "False", ")", ":", "\n", "            ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "            ", "continue", "\n", "", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "if", "length", ">", "max_answer_length", ":", "\n", "            ", "continue", "\n", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "feature_index", ",", "\n", "start_index", "=", "start_index", ",", "\n", "end_index", "=", "end_index", ",", "\n", "start_log_prob", "=", "start_log_prob", ",", "\n", "end_log_prob", "=", "end_log_prob", ")", ")", "\n", "\n", "", "", "", "prelim_predictions", "=", "sorted", "(", "\n", "prelim_predictions", ",", "\n", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_log_prob", "+", "x", ".", "end_log_prob", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "seen_predictions", "=", "{", "}", "\n", "nbest", "=", "[", "]", "\n", "for", "pred", "in", "prelim_predictions", ":", "\n", "      ", "if", "len", "(", "nbest", ")", ">=", "n_best_size", ":", "\n", "        ", "break", "\n", "", "feature", "=", "features", "[", "pred", ".", "feature_index", "]", "\n", "\n", "tok_start_to_orig_index", "=", "feature", ".", "tok_start_to_orig_index", "\n", "tok_end_to_orig_index", "=", "feature", ".", "tok_end_to_orig_index", "\n", "start_orig_pos", "=", "tok_start_to_orig_index", "[", "pred", ".", "start_index", "]", "\n", "end_orig_pos", "=", "tok_end_to_orig_index", "[", "pred", ".", "end_index", "]", "\n", "\n", "paragraph_text", "=", "example", ".", "paragraph_text", "\n", "final_text", "=", "paragraph_text", "[", "start_orig_pos", ":", "end_orig_pos", "+", "1", "]", ".", "strip", "(", ")", "\n", "\n", "if", "final_text", "in", "seen_predictions", ":", "\n", "        ", "continue", "\n", "\n", "", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "\n", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "final_text", ",", "\n", "start_log_prob", "=", "pred", ".", "start_log_prob", ",", "\n", "end_log_prob", "=", "pred", ".", "end_log_prob", ")", ")", "\n", "\n", "# In very rare edge cases we could have no valid predictions. So we", "\n", "# just create a nonce prediction in this case to avoid failure.", "\n", "", "if", "not", "nbest", ":", "\n", "      ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "text", "=", "\"\"", ",", "start_log_prob", "=", "-", "1e6", ",", "\n", "end_log_prob", "=", "-", "1e6", ")", ")", "\n", "\n", "", "total_scores", "=", "[", "]", "\n", "best_non_null_entry", "=", "None", "\n", "for", "entry", "in", "nbest", ":", "\n", "      ", "total_scores", ".", "append", "(", "entry", ".", "start_log_prob", "+", "entry", ".", "end_log_prob", ")", "\n", "if", "not", "best_non_null_entry", ":", "\n", "        ", "best_non_null_entry", "=", "entry", "\n", "\n", "", "", "probs", "=", "_compute_softmax", "(", "total_scores", ")", "\n", "\n", "nbest_json", "=", "[", "]", "\n", "for", "(", "i", ",", "entry", ")", "in", "enumerate", "(", "nbest", ")", ":", "\n", "      ", "output", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "output", "[", "\"text\"", "]", "=", "entry", ".", "text", "\n", "output", "[", "\"probability\"", "]", "=", "probs", "[", "i", "]", "\n", "output", "[", "\"start_log_prob\"", "]", "=", "entry", ".", "start_log_prob", "\n", "output", "[", "\"end_log_prob\"", "]", "=", "entry", ".", "end_log_prob", "\n", "nbest_json", ".", "append", "(", "output", ")", "\n", "\n", "", "assert", "len", "(", "nbest_json", ")", ">=", "1", "\n", "assert", "best_non_null_entry", "is", "not", "None", "\n", "\n", "score_diff", "=", "score_null", "\n", "scores_diff_json", "[", "example", ".", "qas_id", "]", "=", "score_diff", "\n", "# note(zhiliny): always predict best_non_null_entry", "\n", "# and the evaluation script will search for the best threshold", "\n", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "best_non_null_entry", ".", "text", "\n", "\n", "all_nbest_json", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "\n", "\n", "", "return", "all_predictions", ",", "all_nbest_json", "\n", "# with open(output_prediction_file, \"w\") as writer:", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader._get_best_indexes": [[805, 815], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "_get_best_indexes", "(", "logits", ",", "n_best_size", ")", ":", "\n", "  ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "    ", "if", "i", ">=", "n_best_size", ":", "\n", "      ", "break", "\n", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader._compute_softmax": [[817, 838], ["math.exp", "exp_scores.append", "probs.append"], "function", ["None"], ["", "def", "_compute_softmax", "(", "scores", ")", ":", "\n", "  ", "\"\"\"Compute softmax probability over raw logits.\"\"\"", "\n", "if", "not", "scores", ":", "\n", "    ", "return", "[", "]", "\n", "\n", "", "max_score", "=", "None", "\n", "for", "score", "in", "scores", ":", "\n", "    ", "if", "max_score", "is", "None", "or", "score", ">", "max_score", ":", "\n", "      ", "max_score", "=", "score", "\n", "\n", "", "", "exp_scores", "=", "[", "]", "\n", "total_sum", "=", "0.0", "\n", "for", "score", "in", "scores", ":", "\n", "    ", "x", "=", "math", ".", "exp", "(", "score", "-", "max_score", ")", "\n", "exp_scores", ".", "append", "(", "x", ")", "\n", "total_sum", "+=", "x", "\n", "\n", "", "probs", "=", "[", "]", "\n", "for", "score", "in", "exp_scores", ":", "\n", "    ", "probs", ".", "append", "(", "score", "/", "total_sum", ")", "\n", "", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.serve.mrqa_service": [[31, 36], ["app.route", "server"], "function", ["None"], ["@", "app", ".", "route", "(", "'/'", ",", "methods", "=", "[", "'POST'", "]", ")", "\n", "def", "mrqa_service", "(", ")", ":", "\n", "    ", "\"\"\"Description\"\"\"", "\n", "model", "=", "bert_model", "\n", "return", "server", "(", "model", ",", "process_mode", "=", "mode", ",", "max_batch_size", "=", "5", ")", "\n", "# return server(model)", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.wrapper.BertModelWrapper.__init__": [[42, 64], ["model.xlnet.XLNetConfig", "model.xlnet.XLNetConfig.print_config", "paddle.Executor", "squad_reader.DataProcessor", "paddle.io.load_inference_model", "paddle.CUDAPlace", "paddle.core.get_cuda_device_count", "paddle.CPUPlace", "int", "os.environ.get", "multiprocessing.cpu_count"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig.print_config"], ["def", "__init__", "(", "self", ",", "model_dir", ")", ":", "\n", "        ", "\"\"\" \"\"\"", "\n", "xlnet_config", "=", "XLNetConfig", "(", "bert_config_path", ")", "\n", "xlnet_config", ".", "print_config", "(", ")", "\n", "\n", "if", "use_cuda", ":", "\n", "            ", "place", "=", "fluid", ".", "CUDAPlace", "(", "0", ")", "\n", "dev_count", "=", "fluid", ".", "core", ".", "get_cuda_device_count", "(", ")", "\n", "", "else", ":", "\n", "            ", "place", "=", "fluid", ".", "CPUPlace", "(", ")", "\n", "dev_count", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'CPU_NUM'", ",", "multiprocessing", ".", "cpu_count", "(", ")", ")", ")", "\n", "", "self", ".", "exe", "=", "fluid", ".", "Executor", "(", "place", ")", "\n", "\n", "self", ".", "processor", "=", "DataProcessor", "(", "\n", "spiece_model_file", "=", "spiece_model_file", ",", "\n", "uncased", "=", "do_lower_case", ",", "\n", "max_seq_length", "=", "max_seq_len", ",", "\n", "doc_stride", "=", "doc_stride", ",", "\n", "max_query_length", "=", "max_query_length", ")", "\n", "\n", "self", ".", "inference_program", ",", "self", ".", "feed_target_names", ",", "self", ".", "fetch_targets", "=", "fluid", ".", "io", ".", "load_inference_model", "(", "dirname", "=", "model_dir", ",", "executor", "=", "self", ".", "exe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.wrapper.BertModelWrapper.preprocessor": [[70, 77], ["wrapper.BertModelWrapper.processor.data_generator"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator"], ["", "def", "preprocessor", "(", "self", ",", "samples", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"Preprocess the input samples, including word seg, padding, token to ids\"\"\"", "\n", "# Tokenization and paragraph padding", "\n", "examples", ",", "features", ",", "batch", "=", "self", ".", "processor", ".", "data_generator", "(", "\n", "samples", ",", "batch_size", ")", "\n", "self", ".", "samples", "=", "samples", "\n", "return", "examples", ",", "features", ",", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.wrapper.BertModelWrapper.call_mrc": [[78, 114], ["wrapper.BertModelWrapper.exe.run", "ValueError", "NotImplementedError", "len", "zip"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["", "def", "call_mrc", "(", "self", ",", "batch", ",", "squeeze_dim0", "=", "False", ",", "return_list", "=", "False", ")", ":", "\n", "        ", "\"\"\"MRC\"\"\"", "\n", "if", "squeeze_dim0", "and", "return_list", ":", "\n", "            ", "raise", "ValueError", "(", "\"squeeze_dim0 only work for dict-type return value.\"", ")", "\n", "", "src_ids", "=", "batch", "[", "0", "]", "\n", "pos_ids", "=", "batch", "[", "1", "]", "\n", "sent_ids", "=", "batch", "[", "2", "]", "\n", "input_mask", "=", "batch", "[", "3", "]", "\n", "unique_id", "=", "batch", "[", "4", "]", "\n", "emmmm", "=", "batch", "[", "5", "]", "\n", "feed_dict", "=", "{", "\n", "self", ".", "feed_target_names", "[", "0", "]", ":", "src_ids", ",", "\n", "self", ".", "feed_target_names", "[", "1", "]", ":", "pos_ids", ",", "\n", "self", ".", "feed_target_names", "[", "2", "]", ":", "sent_ids", ",", "\n", "self", ".", "feed_target_names", "[", "3", "]", ":", "input_mask", ",", "\n", "self", ".", "feed_target_names", "[", "4", "]", ":", "unique_id", ",", "\n", "self", ".", "feed_target_names", "[", "5", "]", ":", "emmmm", "\n", "}", "\n", "\n", "np_unique_ids", ",", "np_start_logits", ",", "np_start_top_index", ",", "np_end_logits", ",", "np_end_top_index", ",", "np_cls_logits", "=", "self", ".", "exe", ".", "run", "(", "self", ".", "inference_program", ",", "feed", "=", "feed_dict", ",", "fetch_list", "=", "self", ".", "fetch_targets", ",", "use_program_cache", "=", "True", ")", "\n", "\n", "# np_unique_ids, np_start_logits, np_end_logits, np_num_seqs = \\", "\n", "#     self.exe.run(feed=feed_dict, fetch_list=self.fetch_targets)", "\n", "\n", "if", "len", "(", "np_unique_ids", ")", "==", "1", "and", "squeeze_dim0", ":", "\n", "            ", "np_unique_ids", "=", "np_unique_ids", "[", "0", "]", "\n", "np_start_logits", "=", "np_start_logits", "[", "0", "]", "\n", "np_end_logits", "=", "np_end_logits", "[", "0", "]", "\n", "\n", "", "if", "return_list", ":", "\n", "            ", "mrc_results", "=", "[", "{", "'unique_ids'", ":", "id", ",", "'start_logits'", ":", "st", ",", "'start_idx'", ":", "st_idx", ",", "'end_logits'", ":", "end", ",", "'end_idx'", ":", "end_idx", ",", "'cls'", ":", "cls", "}", "\n", "for", "id", ",", "st", ",", "st_idx", ",", "end", ",", "end_idx", ",", "cls", "in", "zip", "(", "np_unique_ids", ",", "np_start_logits", ",", "np_start_top_index", ",", "np_end_logits", ",", "np_end_top_index", ",", "np_cls_logits", ")", "]", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "", "return", "mrc_results", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.wrapper.BertModelWrapper.postprocessor": [[115, 158], ["collections.namedtuple", "isinstance", "squad_reader.get_answers", "isinstance", "NotImplementedError", "range", "float", "results.append", "int", "results.append", "float", "int", "float", "int", "collections.namedtuple.", "float", "float", "collections.namedtuple."], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.get_answers"], ["", "def", "postprocessor", "(", "self", ",", "examples", ",", "features", ",", "mrc_results", ")", ":", "\n", "        ", "\"\"\"Extract answer\n         batch: [examples, features] from preprocessor\n         mrc_results: model results from call_mrc. if mrc_results is list, each element of which is a size=1 batch.\n        \"\"\"", "\n", "RawResult", "=", "collections", ".", "namedtuple", "(", "\"RawResult\"", ",", "\n", "[", "\"unique_id\"", ",", "\"start_top_log_probs\"", ",", "\"start_top_index\"", ",", "\n", "\"end_top_log_probs\"", ",", "\"end_top_index\"", ",", "\"cls_logits\"", "]", ")", "\n", "results", "=", "[", "]", "\n", "if", "isinstance", "(", "mrc_results", ",", "list", ")", ":", "\n", "            ", "for", "res", "in", "mrc_results", ":", "\n", "                ", "unique_id", "=", "res", "[", "'unique_ids'", "]", "[", "0", "]", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "res", "[", "'start_logits'", "]", ".", "flat", "]", "\n", "start_idx", "=", "[", "int", "(", "x", ")", "for", "x", "in", "res", "[", "'start_idx'", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "res", "[", "'end_logits'", "]", ".", "flat", "]", "\n", "end_idx", "=", "[", "int", "(", "x", ")", "for", "x", "in", "res", "[", "'end_idx'", "]", ".", "flat", "]", "\n", "cls_logits", "=", "float", "(", "res", "[", "'cls'", "]", ".", "flat", "[", "0", "]", ")", "\n", "\n", "results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_top_log_probs", "=", "start_logits", ",", "\n", "start_top_index", "=", "start_idx", ",", "\n", "end_top_log_probs", "=", "end_logits", ",", "\n", "end_top_index", "=", "end_idx", ",", "\n", "cls_logits", "=", "cls_logits", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "assert", "isinstance", "(", "mrc_results", ",", "dict", ")", "\n", "raise", "NotImplementedError", "(", ")", "\n", "for", "idx", "in", "range", "(", "mrc_results", "[", "'unique_ids'", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "unique_id", "=", "int", "(", "mrc_results", "[", "'unique_ids'", "]", "[", "idx", "]", ")", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "mrc_results", "[", "'start_logits'", "]", "[", "idx", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "mrc_results", "[", "'end_logits'", "]", "[", "idx", "]", ".", "flat", "]", "\n", "results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "\n", "", "", "answers", "=", "get_answers", "(", "\n", "examples", ",", "features", ",", "results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "start_n_top", ",", "end_n_top", ")", "\n", "return", "answers", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.prepro_utils.printable_text": [[14, 35], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode"], ["def", "printable_text", "(", "text", ")", ":", "\n", "    ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.prepro_utils.print_": [[37, 47], ["print", "isinstance", "new_args.append", "new_args.append", "prepro_utils.printable_text", "prepro_utils.printable_text"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["", "", "def", "print_", "(", "*", "args", ")", ":", "\n", "    ", "new_args", "=", "[", "]", "\n", "for", "arg", "in", "args", ":", "\n", "        ", "if", "isinstance", "(", "arg", ",", "list", ")", ":", "\n", "            ", "s", "=", "[", "printable_text", "(", "i", ")", "for", "i", "in", "arg", "]", "\n", "s", "=", "' '", ".", "join", "(", "s", ")", "\n", "new_args", ".", "append", "(", "s", ")", "\n", "", "else", ":", "\n", "            ", "new_args", ".", "append", "(", "printable_text", "(", "arg", ")", ")", "\n", "", "", "print", "(", "*", "new_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.prepro_utils.preprocess_text": [[49, 66], ["outputs.lower.replace().replace", "isinstance", "outputs.lower.decode", "unicodedata.normalize", "outputs.lower.lower", "inputs.strip().split", "outputs.lower.replace", "inputs.strip", "unicodedata.combining"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.normalize"], ["", "def", "preprocess_text", "(", "inputs", ",", "lower", "=", "False", ",", "remove_space", "=", "True", ",", "keep_accents", "=", "False", ")", ":", "\n", "    ", "if", "remove_space", ":", "\n", "        ", "outputs", "=", "' '", ".", "join", "(", "inputs", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "outputs", "=", "inputs", "\n", "", "outputs", "=", "outputs", ".", "replace", "(", "\"``\"", ",", "'\"'", ")", ".", "replace", "(", "\"''\"", ",", "'\"'", ")", "\n", "\n", "if", "six", ".", "PY2", "and", "isinstance", "(", "outputs", ",", "str", ")", ":", "\n", "        ", "outputs", "=", "outputs", ".", "decode", "(", "'utf-8'", ")", "\n", "\n", "", "if", "not", "keep_accents", ":", "\n", "        ", "outputs", "=", "unicodedata", ".", "normalize", "(", "'NFKD'", ",", "outputs", ")", "\n", "outputs", "=", "''", ".", "join", "(", "[", "c", "for", "c", "in", "outputs", "if", "not", "unicodedata", ".", "combining", "(", "c", ")", "]", ")", "\n", "", "if", "lower", ":", "\n", "        ", "outputs", "=", "outputs", ".", "lower", "(", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.prepro_utils.encode_pieces": [[68, 104], ["isinstance", "text.encode.encode", "sp_model.EncodeAsPieces", "sp_model.SampleEncodeAsPieces", "piece[].isdigit", "sp_model.EncodeAsPieces", "sp_model.EncodeAsPieces.append", "new_pieces.extend", "new_pieces.append", "isinstance", "ret_pieces.append", "len", "piece[].replace", "piece.decode.decode", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode"], ["", "def", "encode_pieces", "(", "sp_model", ",", "text", ",", "return_unicode", "=", "True", ",", "sample", "=", "False", ")", ":", "\n", "# return_unicode is used only for py2", "\n", "\n", "# note(zhiliny): in some systems, sentencepiece only accepts str for py2", "\n", "    ", "if", "six", ".", "PY2", "and", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "        ", "text", "=", "text", ".", "encode", "(", "'utf-8'", ")", "\n", "\n", "", "if", "not", "sample", ":", "\n", "        ", "pieces", "=", "sp_model", ".", "EncodeAsPieces", "(", "text", ")", "\n", "", "else", ":", "\n", "        ", "pieces", "=", "sp_model", ".", "SampleEncodeAsPieces", "(", "text", ",", "64", ",", "0.1", ")", "\n", "", "new_pieces", "=", "[", "]", "\n", "for", "piece", "in", "pieces", ":", "\n", "        ", "if", "len", "(", "piece", ")", ">", "1", "and", "piece", "[", "-", "1", "]", "==", "','", "and", "piece", "[", "-", "2", "]", ".", "isdigit", "(", ")", ":", "\n", "            ", "cur_pieces", "=", "sp_model", ".", "EncodeAsPieces", "(", "\n", "piece", "[", ":", "-", "1", "]", ".", "replace", "(", "SPIECE_UNDERLINE", ",", "''", ")", ")", "\n", "if", "piece", "[", "0", "]", "!=", "SPIECE_UNDERLINE", "and", "cur_pieces", "[", "0", "]", "[", "0", "]", "==", "SPIECE_UNDERLINE", ":", "\n", "                ", "if", "len", "(", "cur_pieces", "[", "0", "]", ")", "==", "1", ":", "\n", "                    ", "cur_pieces", "=", "cur_pieces", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "                    ", "cur_pieces", "[", "0", "]", "=", "cur_pieces", "[", "0", "]", "[", "1", ":", "]", "\n", "", "", "cur_pieces", ".", "append", "(", "piece", "[", "-", "1", "]", ")", "\n", "new_pieces", ".", "extend", "(", "cur_pieces", ")", "\n", "", "else", ":", "\n", "            ", "new_pieces", ".", "append", "(", "piece", ")", "\n", "\n", "# note(zhiliny): convert back to unicode for py2", "\n", "", "", "if", "six", ".", "PY2", "and", "return_unicode", ":", "\n", "        ", "ret_pieces", "=", "[", "]", "\n", "for", "piece", "in", "new_pieces", ":", "\n", "            ", "if", "isinstance", "(", "piece", ",", "str", ")", ":", "\n", "                ", "piece", "=", "piece", ".", "decode", "(", "'utf-8'", ")", "\n", "", "ret_pieces", ".", "append", "(", "piece", ")", "\n", "", "new_pieces", "=", "ret_pieces", "\n", "\n", "", "return", "new_pieces", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.prepro_utils.encode_ids": [[106, 110], ["prepro_utils.encode_pieces", "sp_model.PieceToId"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.prepro_utils.encode_pieces"], ["", "def", "encode_ids", "(", "sp_model", ",", "text", ",", "sample", "=", "False", ")", ":", "\n", "    ", "pieces", "=", "encode_pieces", "(", "sp_model", ",", "text", ",", "return_unicode", "=", "False", ",", "sample", "=", "sample", ")", "\n", "ids", "=", "[", "sp_model", ".", "PieceToId", "(", "piece", ")", "for", "piece", "in", "pieces", "]", "\n", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.parse_args": [[18, 35], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "len", "argparse.ArgumentParser.print_help", "sys.exit"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "  ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'Official evaluation script for SQuAD version 2.0.'", ")", "\n", "parser", ".", "add_argument", "(", "'data_file'", ",", "metavar", "=", "'data.json'", ",", "help", "=", "'Input data JSON file.'", ")", "\n", "parser", ".", "add_argument", "(", "'pred_file'", ",", "metavar", "=", "'pred.json'", ",", "help", "=", "'Model predictions.'", ")", "\n", "parser", ".", "add_argument", "(", "'--out-file'", ",", "'-o'", ",", "metavar", "=", "'eval.json'", ",", "\n", "help", "=", "'Write accuracy metrics to file (default is stdout).'", ")", "\n", "parser", ".", "add_argument", "(", "'--na-prob-file'", ",", "'-n'", ",", "metavar", "=", "'na_prob.json'", ",", "\n", "help", "=", "'Model estimates of probability of no answer.'", ")", "\n", "parser", ".", "add_argument", "(", "'--na-prob-thresh'", ",", "'-t'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Predict \"\" if no-answer probability exceeds this (default = 1.0).'", ")", "\n", "parser", ".", "add_argument", "(", "'--out-image-dir'", ",", "'-p'", ",", "metavar", "=", "'out_images'", ",", "default", "=", "None", ",", "\n", "help", "=", "'Save precision-recall curves to directory.'", ")", "\n", "parser", ".", "add_argument", "(", "'--verbose'", ",", "'-v'", ",", "action", "=", "'store_true'", ")", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "1", ":", "\n", "    ", "parser", ".", "print_help", "(", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.make_qid_to_has_ans": [[36, 43], ["bool"], "function", ["None"], ["", "def", "make_qid_to_has_ans", "(", "dataset", ")", ":", "\n", "  ", "qid_to_has_ans", "=", "{", "}", "\n", "for", "article", "in", "dataset", ":", "\n", "    ", "for", "p", "in", "article", "[", "'paragraphs'", "]", ":", "\n", "      ", "for", "qa", "in", "p", "[", "'qas'", "]", ":", "\n", "        ", "qid_to_has_ans", "[", "qa", "[", "'id'", "]", "]", "=", "bool", "(", "qa", "[", "'answers'", "]", ")", "\n", "", "", "", "return", "qid_to_has_ans", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.normalize_answer": [[44, 57], ["squad_utils.normalize_answer.white_space_fix"], "function", ["None"], ["", "def", "normalize_answer", "(", "s", ")", ":", "\n", "  ", "\"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"", "\n", "def", "remove_articles", "(", "text", ")", ":", "\n", "    ", "regex", "=", "re", ".", "compile", "(", "r'\\b(a|an|the)\\b'", ",", "re", ".", "UNICODE", ")", "\n", "return", "re", ".", "sub", "(", "regex", ",", "' '", ",", "text", ")", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "    ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "    ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "    ", "return", "text", ".", "lower", "(", ")", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.get_tokens": [[58, 61], ["normalize_answer().split", "squad_utils.normalize_answer"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.normalize_answer"], ["", "def", "get_tokens", "(", "s", ")", ":", "\n", "  ", "if", "not", "s", ":", "return", "[", "]", "\n", "return", "normalize_answer", "(", "s", ")", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.compute_exact": [[62, 64], ["int", "squad_utils.normalize_answer", "squad_utils.normalize_answer"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.normalize_answer", "home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.normalize_answer"], ["", "def", "compute_exact", "(", "a_gold", ",", "a_pred", ")", ":", "\n", "  ", "return", "int", "(", "normalize_answer", "(", "a_gold", ")", "==", "normalize_answer", "(", "a_pred", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.compute_f1": [[65, 79], ["squad_utils.get_tokens", "squad_utils.get_tokens", "sum", "collections.Counter", "collections.Counter", "common.values", "int", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.get_tokens", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.get_tokens"], ["", "def", "compute_f1", "(", "a_gold", ",", "a_pred", ")", ":", "\n", "  ", "gold_toks", "=", "get_tokens", "(", "a_gold", ")", "\n", "pred_toks", "=", "get_tokens", "(", "a_pred", ")", "\n", "common", "=", "collections", ".", "Counter", "(", "gold_toks", ")", "&", "collections", ".", "Counter", "(", "pred_toks", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "if", "len", "(", "gold_toks", ")", "==", "0", "or", "len", "(", "pred_toks", ")", "==", "0", ":", "\n", "# If either is no-answer, then F1 is 1 if they agree, 0 otherwise", "\n", "    ", "return", "int", "(", "gold_toks", "==", "pred_toks", ")", "\n", "", "if", "num_same", "==", "0", ":", "\n", "    ", "return", "0", "\n", "", "precision", "=", "1.0", "*", "num_same", "/", "len", "(", "pred_toks", ")", "\n", "recall", "=", "1.0", "*", "num_same", "/", "len", "(", "gold_toks", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "return", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.get_raw_scores": [[80, 100], ["max", "max", "print", "squad_utils.normalize_answer", "squad_utils.compute_exact", "squad_utils.compute_f1"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.normalize_answer", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.compute_exact", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.compute_f1"], ["", "def", "get_raw_scores", "(", "dataset", ",", "preds", ")", ":", "\n", "  ", "exact_scores", "=", "{", "}", "\n", "f1_scores", "=", "{", "}", "\n", "for", "article", "in", "dataset", ":", "\n", "    ", "for", "p", "in", "article", "[", "'paragraphs'", "]", ":", "\n", "      ", "for", "qa", "in", "p", "[", "'qas'", "]", ":", "\n", "        ", "qid", "=", "qa", "[", "'id'", "]", "\n", "gold_answers", "=", "[", "a", "[", "'text'", "]", "for", "a", "in", "qa", "[", "'answers'", "]", "\n", "if", "normalize_answer", "(", "a", "[", "'text'", "]", ")", "]", "\n", "if", "not", "gold_answers", ":", "\n", "# For unanswerable questions, only correct answer is empty string", "\n", "          ", "gold_answers", "=", "[", "''", "]", "\n", "", "if", "qid", "not", "in", "preds", ":", "\n", "          ", "print", "(", "'Missing prediction for %s'", "%", "qid", ")", "\n", "continue", "\n", "", "a_pred", "=", "preds", "[", "qid", "]", "\n", "# Take max over all gold answers", "\n", "exact_scores", "[", "qid", "]", "=", "max", "(", "compute_exact", "(", "a", ",", "a_pred", ")", "for", "a", "in", "gold_answers", ")", "\n", "f1_scores", "[", "qid", "]", "=", "max", "(", "compute_f1", "(", "a", ",", "a_pred", ")", "for", "a", "in", "gold_answers", ")", "\n", "", "", "", "return", "exact_scores", ",", "f1_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.apply_no_ans_threshold": [[101, 110], ["scores.items", "float"], "function", ["None"], ["", "def", "apply_no_ans_threshold", "(", "scores", ",", "na_probs", ",", "qid_to_has_ans", ",", "na_prob_thresh", ")", ":", "\n", "  ", "new_scores", "=", "{", "}", "\n", "for", "qid", ",", "s", "in", "scores", ".", "items", "(", ")", ":", "\n", "    ", "pred_na", "=", "na_probs", "[", "qid", "]", ">", "na_prob_thresh", "\n", "if", "pred_na", ":", "\n", "      ", "new_scores", "[", "qid", "]", "=", "float", "(", "not", "qid_to_has_ans", "[", "qid", "]", ")", "\n", "", "else", ":", "\n", "      ", "new_scores", "[", "qid", "]", "=", "s", "\n", "", "", "return", "new_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.make_eval_dict": [[111, 125], ["len", "collections.OrderedDict", "len", "collections.OrderedDict", "sum", "sum", "sum", "sum", "exact_scores.values", "f1_scores.values"], "function", ["None"], ["", "def", "make_eval_dict", "(", "exact_scores", ",", "f1_scores", ",", "qid_list", "=", "None", ")", ":", "\n", "  ", "if", "not", "qid_list", ":", "\n", "    ", "total", "=", "len", "(", "exact_scores", ")", "\n", "return", "collections", ".", "OrderedDict", "(", "[", "\n", "(", "'exact'", ",", "100.0", "*", "sum", "(", "exact_scores", ".", "values", "(", ")", ")", "/", "total", ")", ",", "\n", "(", "'f1'", ",", "100.0", "*", "sum", "(", "f1_scores", ".", "values", "(", ")", ")", "/", "total", ")", ",", "\n", "(", "'total'", ",", "total", ")", ",", "\n", "]", ")", "\n", "", "else", ":", "\n", "    ", "total", "=", "len", "(", "qid_list", ")", "\n", "return", "collections", ".", "OrderedDict", "(", "[", "\n", "(", "'exact'", ",", "100.0", "*", "sum", "(", "exact_scores", "[", "k", "]", "for", "k", "in", "qid_list", ")", "/", "total", ")", ",", "\n", "(", "'f1'", ",", "100.0", "*", "sum", "(", "f1_scores", "[", "k", "]", "for", "k", "in", "qid_list", ")", "/", "total", ")", ",", "\n", "(", "'total'", ",", "total", ")", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.merge_eval": [[127, 130], ["None"], "function", ["None"], ["", "", "def", "merge_eval", "(", "main_eval", ",", "new_eval", ",", "prefix", ")", ":", "\n", "  ", "for", "k", "in", "new_eval", ":", "\n", "    ", "main_eval", "[", "'%s_%s'", "%", "(", "prefix", ",", "k", ")", "]", "=", "new_eval", "[", "k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.plot_pr_curve": [[131, 141], ["plt.step", "plt.fill_between", "plt.xlabel", "plt.ylabel", "plt.xlim", "plt.ylim", "plt.title", "plt.savefig", "plt.clf"], "function", ["None"], ["", "", "def", "plot_pr_curve", "(", "precisions", ",", "recalls", ",", "out_image", ",", "title", ")", ":", "\n", "  ", "plt", ".", "step", "(", "recalls", ",", "precisions", ",", "color", "=", "'b'", ",", "alpha", "=", "0.2", ",", "where", "=", "'post'", ")", "\n", "plt", ".", "fill_between", "(", "recalls", ",", "precisions", ",", "step", "=", "'post'", ",", "alpha", "=", "0.2", ",", "color", "=", "'b'", ")", "\n", "plt", ".", "xlabel", "(", "'Recall'", ")", "\n", "plt", ".", "ylabel", "(", "'Precision'", ")", "\n", "plt", ".", "xlim", "(", "[", "0.0", ",", "1.05", "]", ")", "\n", "plt", ".", "ylim", "(", "[", "0.0", ",", "1.05", "]", ")", "\n", "plt", ".", "title", "(", "title", ")", "\n", "plt", ".", "savefig", "(", "out_image", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.make_precision_recall_eval": [[142, 164], ["sorted", "enumerate", "squad_utils.plot_pr_curve", "float", "float", "precisions.append", "recalls.append", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.plot_pr_curve"], ["", "def", "make_precision_recall_eval", "(", "scores", ",", "na_probs", ",", "num_true_pos", ",", "qid_to_has_ans", ",", "\n", "out_image", "=", "None", ",", "title", "=", "None", ")", ":", "\n", "  ", "qid_list", "=", "sorted", "(", "na_probs", ",", "key", "=", "lambda", "k", ":", "na_probs", "[", "k", "]", ")", "\n", "true_pos", "=", "0.0", "\n", "cur_p", "=", "1.0", "\n", "cur_r", "=", "0.0", "\n", "precisions", "=", "[", "1.0", "]", "\n", "recalls", "=", "[", "0.0", "]", "\n", "avg_prec", "=", "0.0", "\n", "for", "i", ",", "qid", "in", "enumerate", "(", "qid_list", ")", ":", "\n", "    ", "if", "qid_to_has_ans", "[", "qid", "]", ":", "\n", "      ", "true_pos", "+=", "scores", "[", "qid", "]", "\n", "", "cur_p", "=", "true_pos", "/", "float", "(", "i", "+", "1", ")", "\n", "cur_r", "=", "true_pos", "/", "float", "(", "num_true_pos", ")", "\n", "if", "i", "==", "len", "(", "qid_list", ")", "-", "1", "or", "na_probs", "[", "qid", "]", "!=", "na_probs", "[", "qid_list", "[", "i", "+", "1", "]", "]", ":", "\n", "# i.e., if we can put a threshold after this point", "\n", "      ", "avg_prec", "+=", "cur_p", "*", "(", "cur_r", "-", "recalls", "[", "-", "1", "]", ")", "\n", "precisions", ".", "append", "(", "cur_p", ")", "\n", "recalls", ".", "append", "(", "cur_r", ")", "\n", "", "", "if", "out_image", ":", "\n", "    ", "plot_pr_curve", "(", "precisions", ",", "recalls", ",", "out_image", ",", "title", ")", "\n", "", "return", "{", "'ap'", ":", "100.0", "*", "avg_prec", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.run_precision_recall_analysis": [[165, 188], ["sum", "squad_utils.make_precision_recall_eval", "squad_utils.make_precision_recall_eval", "squad_utils.make_precision_recall_eval", "squad_utils.merge_eval", "squad_utils.merge_eval", "squad_utils.merge_eval", "os.makedirs", "float", "os.path.exists", "os.path.join", "os.path.join", "qid_to_has_ans.items", "os.path.join", "qid_to_has_ans.values"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.make_precision_recall_eval", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.make_precision_recall_eval", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.make_precision_recall_eval", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.merge_eval", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.merge_eval", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.merge_eval"], ["", "def", "run_precision_recall_analysis", "(", "main_eval", ",", "exact_raw", ",", "f1_raw", ",", "na_probs", ",", "\n", "qid_to_has_ans", ",", "out_image_dir", ")", ":", "\n", "  ", "if", "out_image_dir", "and", "not", "os", ".", "path", ".", "exists", "(", "out_image_dir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "out_image_dir", ")", "\n", "", "num_true_pos", "=", "sum", "(", "1", "for", "v", "in", "qid_to_has_ans", ".", "values", "(", ")", "if", "v", ")", "\n", "if", "num_true_pos", "==", "0", ":", "\n", "    ", "return", "\n", "", "pr_exact", "=", "make_precision_recall_eval", "(", "\n", "exact_raw", ",", "na_probs", ",", "num_true_pos", ",", "qid_to_has_ans", ",", "\n", "out_image", "=", "os", ".", "path", ".", "join", "(", "out_image_dir", ",", "'pr_exact.png'", ")", ",", "\n", "title", "=", "'Precision-Recall curve for Exact Match score'", ")", "\n", "pr_f1", "=", "make_precision_recall_eval", "(", "\n", "f1_raw", ",", "na_probs", ",", "num_true_pos", ",", "qid_to_has_ans", ",", "\n", "out_image", "=", "os", ".", "path", ".", "join", "(", "out_image_dir", ",", "'pr_f1.png'", ")", ",", "\n", "title", "=", "'Precision-Recall curve for F1 score'", ")", "\n", "oracle_scores", "=", "{", "k", ":", "float", "(", "v", ")", "for", "k", ",", "v", "in", "qid_to_has_ans", ".", "items", "(", ")", "}", "\n", "pr_oracle", "=", "make_precision_recall_eval", "(", "\n", "oracle_scores", ",", "na_probs", ",", "num_true_pos", ",", "qid_to_has_ans", ",", "\n", "out_image", "=", "os", ".", "path", ".", "join", "(", "out_image_dir", ",", "'pr_oracle.png'", ")", ",", "\n", "title", "=", "'Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)'", ")", "\n", "merge_eval", "(", "main_eval", ",", "pr_exact", ",", "'pr_exact'", ")", "\n", "merge_eval", "(", "main_eval", ",", "pr_f1", ",", "'pr_f1'", ")", "\n", "merge_eval", "(", "main_eval", ",", "pr_oracle", ",", "'pr_oracle'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.histogram_na_prob": [[189, 200], ["plt.hist", "plt.xlabel", "plt.ylabel", "plt.title", "plt.savefig", "plt.clf", "numpy.ones_like", "float", "os.path.join", "len"], "function", ["None"], ["", "def", "histogram_na_prob", "(", "na_probs", ",", "qid_list", ",", "image_dir", ",", "name", ")", ":", "\n", "  ", "if", "not", "qid_list", ":", "\n", "    ", "return", "\n", "", "x", "=", "[", "na_probs", "[", "k", "]", "for", "k", "in", "qid_list", "]", "\n", "weights", "=", "np", ".", "ones_like", "(", "x", ")", "/", "float", "(", "len", "(", "x", ")", ")", "\n", "plt", ".", "hist", "(", "x", ",", "weights", "=", "weights", ",", "bins", "=", "20", ",", "range", "=", "(", "0.0", ",", "1.0", ")", ")", "\n", "plt", ".", "xlabel", "(", "'Model probability of no-answer'", ")", "\n", "plt", ".", "ylabel", "(", "'Proportion of dataset'", ")", "\n", "plt", ".", "title", "(", "'Histogram of no-answer probability: %s'", "%", "name", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "image_dir", ",", "'na_prob_hist_%s.png'", "%", "name", ")", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.find_best_thresh": [[201, 221], ["sum", "sorted", "enumerate", "len"], "function", ["None"], ["", "def", "find_best_thresh", "(", "preds", ",", "scores", ",", "na_probs", ",", "qid_to_has_ans", ")", ":", "\n", "  ", "num_no_ans", "=", "sum", "(", "1", "for", "k", "in", "qid_to_has_ans", "if", "not", "qid_to_has_ans", "[", "k", "]", ")", "\n", "cur_score", "=", "num_no_ans", "\n", "best_score", "=", "cur_score", "\n", "best_thresh", "=", "0.0", "\n", "qid_list", "=", "sorted", "(", "na_probs", ",", "key", "=", "lambda", "k", ":", "na_probs", "[", "k", "]", ")", "\n", "for", "i", ",", "qid", "in", "enumerate", "(", "qid_list", ")", ":", "\n", "    ", "if", "qid", "not", "in", "scores", ":", "continue", "\n", "if", "qid_to_has_ans", "[", "qid", "]", ":", "\n", "      ", "diff", "=", "scores", "[", "qid", "]", "\n", "", "else", ":", "\n", "      ", "if", "preds", "[", "qid", "]", ":", "\n", "        ", "diff", "=", "-", "1", "\n", "", "else", ":", "\n", "        ", "diff", "=", "0", "\n", "", "", "cur_score", "+=", "diff", "\n", "if", "cur_score", ">", "best_score", ":", "\n", "      ", "best_score", "=", "cur_score", "\n", "best_thresh", "=", "na_probs", "[", "qid", "]", "\n", "", "", "return", "100.0", "*", "best_score", "/", "len", "(", "scores", ")", ",", "best_thresh", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.find_best_thresh_v2": [[222, 251], ["sum", "sorted", "enumerate", "len"], "function", ["None"], ["", "def", "find_best_thresh_v2", "(", "preds", ",", "scores", ",", "na_probs", ",", "qid_to_has_ans", ")", ":", "\n", "  ", "num_no_ans", "=", "sum", "(", "1", "for", "k", "in", "qid_to_has_ans", "if", "not", "qid_to_has_ans", "[", "k", "]", ")", "\n", "cur_score", "=", "num_no_ans", "\n", "best_score", "=", "cur_score", "\n", "best_thresh", "=", "0.0", "\n", "qid_list", "=", "sorted", "(", "na_probs", ",", "key", "=", "lambda", "k", ":", "na_probs", "[", "k", "]", ")", "\n", "for", "i", ",", "qid", "in", "enumerate", "(", "qid_list", ")", ":", "\n", "    ", "if", "qid", "not", "in", "scores", ":", "continue", "\n", "if", "qid_to_has_ans", "[", "qid", "]", ":", "\n", "      ", "diff", "=", "scores", "[", "qid", "]", "\n", "", "else", ":", "\n", "      ", "if", "preds", "[", "qid", "]", ":", "\n", "        ", "diff", "=", "-", "1", "\n", "", "else", ":", "\n", "        ", "diff", "=", "0", "\n", "", "", "cur_score", "+=", "diff", "\n", "if", "cur_score", ">", "best_score", ":", "\n", "      ", "best_score", "=", "cur_score", "\n", "best_thresh", "=", "na_probs", "[", "qid", "]", "\n", "\n", "", "", "has_ans_score", ",", "has_ans_cnt", "=", "0", ",", "0", "\n", "for", "qid", "in", "qid_list", ":", "\n", "    ", "if", "not", "qid_to_has_ans", "[", "qid", "]", ":", "continue", "\n", "has_ans_cnt", "+=", "1", "\n", "\n", "if", "qid", "not", "in", "scores", ":", "continue", "\n", "has_ans_score", "+=", "scores", "[", "qid", "]", "\n", "\n", "", "return", "100.0", "*", "best_score", "/", "len", "(", "scores", ")", ",", "best_thresh", ",", "1.0", "*", "has_ans_score", "/", "has_ans_cnt", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.find_all_best_thresh": [[252, 259], ["squad_utils.find_best_thresh", "squad_utils.find_best_thresh"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.find_best_thresh", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.find_best_thresh"], ["", "def", "find_all_best_thresh", "(", "main_eval", ",", "preds", ",", "exact_raw", ",", "f1_raw", ",", "na_probs", ",", "qid_to_has_ans", ")", ":", "\n", "  ", "best_exact", ",", "exact_thresh", "=", "find_best_thresh", "(", "preds", ",", "exact_raw", ",", "na_probs", ",", "qid_to_has_ans", ")", "\n", "best_f1", ",", "f1_thresh", "=", "find_best_thresh", "(", "preds", ",", "f1_raw", ",", "na_probs", ",", "qid_to_has_ans", ")", "\n", "main_eval", "[", "'best_exact'", "]", "=", "best_exact", "\n", "main_eval", "[", "'best_exact_thresh'", "]", "=", "exact_thresh", "\n", "main_eval", "[", "'best_f1'", "]", "=", "best_f1", "\n", "main_eval", "[", "'best_f1_thresh'", "]", "=", "f1_thresh", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.find_all_best_thresh_v2": [[260, 269], ["squad_utils.find_best_thresh_v2", "squad_utils.find_best_thresh_v2"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.find_best_thresh_v2", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.find_best_thresh_v2"], ["", "def", "find_all_best_thresh_v2", "(", "main_eval", ",", "preds", ",", "exact_raw", ",", "f1_raw", ",", "na_probs", ",", "qid_to_has_ans", ")", ":", "\n", "  ", "best_exact", ",", "exact_thresh", ",", "has_ans_exact", "=", "find_best_thresh_v2", "(", "preds", ",", "exact_raw", ",", "na_probs", ",", "qid_to_has_ans", ")", "\n", "best_f1", ",", "f1_thresh", ",", "has_ans_f1", "=", "find_best_thresh_v2", "(", "preds", ",", "f1_raw", ",", "na_probs", ",", "qid_to_has_ans", ")", "\n", "main_eval", "[", "'best_exact'", "]", "=", "best_exact", "\n", "main_eval", "[", "'best_exact_thresh'", "]", "=", "exact_thresh", "\n", "main_eval", "[", "'best_f1'", "]", "=", "best_f1", "\n", "main_eval", "[", "'best_f1_thresh'", "]", "=", "f1_thresh", "\n", "main_eval", "[", "'has_ans_exact'", "]", "=", "has_ans_exact", "\n", "main_eval", "[", "'has_ans_f1'", "]", "=", "has_ans_f1", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.main": [[270, 319], ["squad_utils.make_qid_to_has_ans", "squad_utils.get_raw_scores", "squad_utils.apply_no_ans_threshold", "squad_utils.apply_no_ans_threshold", "squad_utils.make_eval_dict", "open", "json.load", "open", "json.load", "squad_utils.make_eval_dict", "squad_utils.merge_eval", "squad_utils.make_eval_dict", "squad_utils.merge_eval", "squad_utils.find_all_best_thresh", "squad_utils.run_precision_recall_analysis", "squad_utils.histogram_na_prob", "squad_utils.histogram_na_prob", "print", "open", "json.load", "make_qid_to_has_ans.items", "make_qid_to_has_ans.items", "open", "json.dump", "json.dumps", "new_orig_data.append"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.make_qid_to_has_ans", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.get_raw_scores", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.apply_no_ans_threshold", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.apply_no_ans_threshold", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.make_eval_dict", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.make_eval_dict", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.merge_eval", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.make_eval_dict", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.merge_eval", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.find_all_best_thresh", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.run_precision_recall_analysis", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.histogram_na_prob", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_utils.histogram_na_prob"], ["", "def", "main", "(", ")", ":", "\n", "  ", "with", "open", "(", "OPTS", ".", "data_file", ")", "as", "f", ":", "\n", "    ", "dataset_json", "=", "json", ".", "load", "(", "f", ")", "\n", "dataset", "=", "dataset_json", "[", "'data'", "]", "\n", "", "with", "open", "(", "OPTS", ".", "pred_file", ")", "as", "f", ":", "\n", "    ", "preds", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "new_orig_data", "=", "[", "]", "\n", "for", "article", "in", "dataset", ":", "\n", "    ", "for", "p", "in", "article", "[", "'paragraphs'", "]", ":", "\n", "      ", "for", "qa", "in", "p", "[", "'qas'", "]", ":", "\n", "        ", "if", "qa", "[", "'id'", "]", "in", "preds", ":", "\n", "          ", "new_para", "=", "{", "'qas'", ":", "[", "qa", "]", "}", "\n", "new_article", "=", "{", "'paragraphs'", ":", "[", "new_para", "]", "}", "\n", "new_orig_data", ".", "append", "(", "new_article", ")", "\n", "", "", "", "", "dataset", "=", "new_orig_data", "\n", "\n", "if", "OPTS", ".", "na_prob_file", ":", "\n", "    ", "with", "open", "(", "OPTS", ".", "na_prob_file", ")", "as", "f", ":", "\n", "      ", "na_probs", "=", "json", ".", "load", "(", "f", ")", "\n", "", "", "else", ":", "\n", "    ", "na_probs", "=", "{", "k", ":", "0.0", "for", "k", "in", "preds", "}", "\n", "", "qid_to_has_ans", "=", "make_qid_to_has_ans", "(", "dataset", ")", "# maps qid to True/False", "\n", "has_ans_qids", "=", "[", "k", "for", "k", ",", "v", "in", "qid_to_has_ans", ".", "items", "(", ")", "if", "v", "]", "\n", "no_ans_qids", "=", "[", "k", "for", "k", ",", "v", "in", "qid_to_has_ans", ".", "items", "(", ")", "if", "not", "v", "]", "\n", "exact_raw", ",", "f1_raw", "=", "get_raw_scores", "(", "dataset", ",", "preds", ")", "\n", "exact_thresh", "=", "apply_no_ans_threshold", "(", "exact_raw", ",", "na_probs", ",", "qid_to_has_ans", ",", "\n", "OPTS", ".", "na_prob_thresh", ")", "\n", "f1_thresh", "=", "apply_no_ans_threshold", "(", "f1_raw", ",", "na_probs", ",", "qid_to_has_ans", ",", "\n", "OPTS", ".", "na_prob_thresh", ")", "\n", "out_eval", "=", "make_eval_dict", "(", "exact_thresh", ",", "f1_thresh", ")", "\n", "if", "has_ans_qids", ":", "\n", "    ", "has_ans_eval", "=", "make_eval_dict", "(", "exact_thresh", ",", "f1_thresh", ",", "qid_list", "=", "has_ans_qids", ")", "\n", "merge_eval", "(", "out_eval", ",", "has_ans_eval", ",", "'HasAns'", ")", "\n", "", "if", "no_ans_qids", ":", "\n", "    ", "no_ans_eval", "=", "make_eval_dict", "(", "exact_thresh", ",", "f1_thresh", ",", "qid_list", "=", "no_ans_qids", ")", "\n", "merge_eval", "(", "out_eval", ",", "no_ans_eval", ",", "'NoAns'", ")", "\n", "", "if", "OPTS", ".", "na_prob_file", ":", "\n", "    ", "find_all_best_thresh", "(", "out_eval", ",", "preds", ",", "exact_raw", ",", "f1_raw", ",", "na_probs", ",", "qid_to_has_ans", ")", "\n", "", "if", "OPTS", ".", "na_prob_file", "and", "OPTS", ".", "out_image_dir", ":", "\n", "    ", "run_precision_recall_analysis", "(", "out_eval", ",", "exact_raw", ",", "f1_raw", ",", "na_probs", ",", "\n", "qid_to_has_ans", ",", "OPTS", ".", "out_image_dir", ")", "\n", "histogram_na_prob", "(", "na_probs", ",", "has_ans_qids", ",", "OPTS", ".", "out_image_dir", ",", "'hasAns'", ")", "\n", "histogram_na_prob", "(", "na_probs", ",", "no_ans_qids", ",", "OPTS", ".", "out_image_dir", ",", "'noAns'", ")", "\n", "", "if", "OPTS", ".", "out_file", ":", "\n", "    ", "with", "open", "(", "OPTS", ".", "out_file", ",", "'w'", ")", "as", "f", ":", "\n", "      ", "json", ".", "dump", "(", "out_eval", ",", "f", ")", "\n", "", "", "else", ":", "\n", "    ", "print", "(", "json", ".", "dumps", "(", "out_eval", ",", "indent", "=", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.multi_head_attention": [[27, 161], ["transformer_encoder.multi_head_attention.__compute_qkv"], "function", ["None"], ["def", "layer_norm", "(", "x", ",", "begin_norm_axis", "=", "1", ",", "epsilon", "=", "1e-6", ",", "param_attr", "=", "None", ",", "bias_attr", "=", "None", ")", ":", "\n", "    ", "helper", "=", "LayerHelper", "(", "'layer_norm'", ",", "**", "locals", "(", ")", ")", "\n", "mean", "=", "layers", ".", "reduce_mean", "(", "x", ",", "dim", "=", "begin_norm_axis", ",", "keep_dim", "=", "True", ")", "\n", "shift_x", "=", "layers", ".", "elementwise_sub", "(", "x", "=", "x", ",", "y", "=", "mean", ",", "axis", "=", "0", ")", "\n", "variance", "=", "layers", ".", "reduce_mean", "(", "layers", ".", "square", "(", "shift_x", ")", ",", "dim", "=", "begin_norm_axis", ",", "keep_dim", "=", "True", ")", "\n", "r_stdev", "=", "layers", ".", "rsqrt", "(", "variance", "+", "epsilon", ")", "\n", "norm_x", "=", "layers", ".", "elementwise_mul", "(", "x", "=", "shift_x", ",", "y", "=", "r_stdev", ",", "axis", "=", "0", ")", "\n", "\n", "param_shape", "=", "[", "reduce", "(", "lambda", "x", ",", "y", ":", "x", "*", "y", ",", "norm_x", ".", "shape", "[", "begin_norm_axis", ":", "]", ")", "]", "\n", "param_dtype", "=", "norm_x", ".", "dtype", "\n", "scale", "=", "helper", ".", "create_parameter", "(", "\n", "attr", "=", "param_attr", ",", "\n", "shape", "=", "param_shape", ",", "\n", "dtype", "=", "param_dtype", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "1.", ")", ")", "\n", "bias", "=", "helper", ".", "create_parameter", "(", "\n", "attr", "=", "bias_attr", ",", "\n", "shape", "=", "param_shape", ",", "\n", "dtype", "=", "param_dtype", ",", "\n", "is_bias", "=", "True", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", "\n", "\n", "out", "=", "layers", ".", "elementwise_mul", "(", "x", "=", "norm_x", ",", "y", "=", "scale", ",", "axis", "=", "-", "1", ")", "\n", "out", "=", "layers", ".", "elementwise_add", "(", "x", "=", "out", ",", "y", "=", "bias", ",", "axis", "=", "-", "1", ")", "\n", "\n", "return", "out", "\n", "\n", "", "def", "multi_head_attention", "(", "queries", ",", "\n", "keys", ",", "\n", "values", ",", "\n", "attn_bias", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "n_head", "=", "1", ",", "\n", "dropout_rate", "=", "0.", ",", "\n", "cache", "=", "None", ",", "\n", "param_initializer", "=", "None", ",", "\n", "name", "=", "'multi_head_att'", ")", ":", "\n", "    ", "\"\"\"\n    Multi-Head Attention. Note that attn_bias is added to the logit before\n    computing softmax activiation to mask certain selected positions so that\n    they will not considered in attention weights.\n    \"\"\"", "\n", "keys", "=", "queries", "if", "keys", "is", "None", "else", "keys", "\n", "values", "=", "keys", "if", "values", "is", "None", "else", "values", "\n", "\n", "if", "not", "(", "len", "(", "queries", ".", "shape", ")", "==", "len", "(", "keys", ".", "shape", ")", "==", "len", "(", "values", ".", "shape", ")", "==", "3", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Inputs: quries, keys and values should all be 3-D tensors.\"", ")", "\n", "\n", "", "def", "__compute_qkv", "(", "queries", ",", "keys", ",", "values", ",", "n_head", ",", "d_key", ",", "d_value", ")", ":", "\n", "        ", "\"\"\"\n        Add linear projection to queries, keys, and values.\n        \"\"\"", "\n", "q", "=", "layers", ".", "fc", "(", "input", "=", "queries", ",", "\n", "size", "=", "d_key", "*", "n_head", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_query_fc.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_query_fc.b_0'", ")", "\n", "k", "=", "layers", ".", "fc", "(", "input", "=", "keys", ",", "\n", "size", "=", "d_key", "*", "n_head", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_key_fc.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_key_fc.b_0'", ")", "\n", "v", "=", "layers", ".", "fc", "(", "input", "=", "values", ",", "\n", "size", "=", "d_value", "*", "n_head", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_value_fc.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_value_fc.b_0'", ")", "\n", "return", "q", ",", "k", ",", "v", "\n", "\n", "", "def", "__split_heads", "(", "x", ",", "n_head", ")", ":", "\n", "        ", "\"\"\"\n        Reshape the last dimension of inpunt tensor x so that it becomes two\n        dimensions and then transpose. Specifically, input a tensor with shape\n        [bs, max_sequence_length, n_head * hidden_dim] then output a tensor\n        with shape [bs, n_head, max_sequence_length, hidden_dim].\n        \"\"\"", "\n", "hidden_size", "=", "x", ".", "shape", "[", "-", "1", "]", "\n", "# The value 0 in shape attr means copying the corresponding dimension", "\n", "# size of the input as the output dimension size.", "\n", "reshaped", "=", "layers", ".", "reshape", "(", "\n", "x", "=", "x", ",", "shape", "=", "[", "0", ",", "0", ",", "n_head", ",", "hidden_size", "//", "n_head", "]", ",", "inplace", "=", "False", ")", "\n", "\n", "# permuate the dimensions into:", "\n", "# [batch_size, n_head, max_sequence_len, hidden_size_per_head]", "\n", "return", "layers", ".", "transpose", "(", "x", "=", "reshaped", ",", "perm", "=", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "", "def", "__combine_heads", "(", "x", ")", ":", "\n", "        ", "\"\"\"\n        Transpose and then reshape the last two dimensions of inpunt tensor x\n        so that it becomes one dimension, which is reverse to __split_heads.\n        \"\"\"", "\n", "if", "len", "(", "x", ".", "shape", ")", "==", "3", ":", "return", "x", "\n", "if", "len", "(", "x", ".", "shape", ")", "!=", "4", ":", "\n", "            ", "raise", "ValueError", "(", "\"Input(x) should be a 4-D Tensor.\"", ")", "\n", "\n", "", "trans_x", "=", "layers", ".", "transpose", "(", "x", ",", "perm", "=", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "# The value 0 in shape attr means copying the corresponding dimension", "\n", "# size of the input as the output dimension size.", "\n", "return", "layers", ".", "reshape", "(", "\n", "x", "=", "trans_x", ",", "\n", "shape", "=", "[", "0", ",", "0", ",", "trans_x", ".", "shape", "[", "2", "]", "*", "trans_x", ".", "shape", "[", "3", "]", "]", ",", "\n", "inplace", "=", "False", ")", "\n", "\n", "", "def", "scaled_dot_product_attention", "(", "q", ",", "k", ",", "v", ",", "attn_bias", ",", "d_key", ",", "dropout_rate", ")", ":", "\n", "        ", "\"\"\"\n        Scaled Dot-Product Attention\n        \"\"\"", "\n", "scaled_q", "=", "layers", ".", "scale", "(", "x", "=", "q", ",", "scale", "=", "d_key", "**", "-", "0.5", ")", "\n", "product", "=", "layers", ".", "matmul", "(", "x", "=", "scaled_q", ",", "y", "=", "k", ",", "transpose_y", "=", "True", ")", "\n", "if", "attn_bias", ":", "\n", "            ", "product", "+=", "attn_bias", "\n", "", "weights", "=", "layers", ".", "softmax", "(", "product", ")", "\n", "if", "dropout_rate", ":", "\n", "            ", "weights", "=", "layers", ".", "dropout", "(", "\n", "weights", ",", "\n", "dropout_prob", "=", "dropout_rate", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "\n", "is_test", "=", "False", ")", "\n", "", "out", "=", "layers", ".", "matmul", "(", "weights", ",", "v", ")", "\n", "return", "out", "\n", "\n", "", "q", ",", "k", ",", "v", "=", "__compute_qkv", "(", "queries", ",", "keys", ",", "values", ",", "n_head", ",", "d_key", ",", "d_value", ")", "\n", "\n", "if", "cache", "is", "not", "None", ":", "# use cache and concat time steps", "\n", "# Since the inplace reshape in __split_heads changes the shape of k and", "\n", "# v, which is the cache input for next time step, reshape the cache", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.positionwise_feed_forward": [[163, 196], ["paddle.fc", "paddle.fc", "paddle.dropout", "paddle.ParamAttr", "paddle.ParamAttr"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["        ", "k", "=", "cache", "[", "\"k\"", "]", "=", "layers", ".", "concat", "(", "\n", "[", "layers", ".", "reshape", "(", "\n", "cache", "[", "\"k\"", "]", ",", "shape", "=", "[", "0", ",", "0", ",", "d_model", "]", ")", ",", "k", "]", ",", "axis", "=", "1", ")", "\n", "v", "=", "cache", "[", "\"v\"", "]", "=", "layers", ".", "concat", "(", "\n", "[", "layers", ".", "reshape", "(", "\n", "cache", "[", "\"v\"", "]", ",", "shape", "=", "[", "0", ",", "0", ",", "d_model", "]", ")", ",", "v", "]", ",", "axis", "=", "1", ")", "\n", "\n", "", "q", "=", "__split_heads", "(", "q", ",", "n_head", ")", "\n", "k", "=", "__split_heads", "(", "k", ",", "n_head", ")", "\n", "v", "=", "__split_heads", "(", "v", ",", "n_head", ")", "\n", "\n", "ctx_multiheads", "=", "scaled_dot_product_attention", "(", "q", ",", "k", ",", "v", ",", "attn_bias", ",", "d_key", ",", "\n", "dropout_rate", ")", "\n", "\n", "out", "=", "__combine_heads", "(", "ctx_multiheads", ")", "\n", "\n", "# Project back to the model size.", "\n", "proj_out", "=", "layers", ".", "fc", "(", "input", "=", "out", ",", "\n", "size", "=", "d_model", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_output_fc.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_output_fc.b_0'", ")", "\n", "return", "proj_out", "\n", "\n", "\n", "", "def", "positionwise_feed_forward", "(", "x", ",", "\n", "d_inner_hid", ",", "\n", "d_hid", ",", "\n", "dropout_rate", ",", "\n", "hidden_act", ",", "\n", "param_initializer", "=", "None", ",", "\n", "name", "=", "'ffn'", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.pre_post_process_layer": [[198, 232], ["paddle.layer_norm", "paddle.cast", "paddle.cast", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.dropout", "len", "paddle.initializer.Constant", "paddle.initializer.Constant"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.layer_norm", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["\n", "hidden", "=", "layers", ".", "fc", "(", "input", "=", "x", ",", "\n", "size", "=", "d_inner_hid", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "act", "=", "hidden_act", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_fc_0.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_fc_0.b_0'", ")", "\n", "if", "dropout_rate", ":", "\n", "        ", "hidden", "=", "layers", ".", "dropout", "(", "\n", "hidden", ",", "\n", "dropout_prob", "=", "dropout_rate", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "\n", "is_test", "=", "False", ")", "\n", "\n", "", "out", "=", "layers", ".", "fc", "(", "input", "=", "hidden", ",", "\n", "size", "=", "d_hid", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_fc_1.w_0'", ",", "\n", "initializer", "=", "param_initializer", ")", ",", "\n", "bias_attr", "=", "name", "+", "'_fc_1.b_0'", ")", "\n", "return", "out", "\n", "\n", "\n", "", "def", "pre_post_process_layer", "(", "prev_out", ",", "out", ",", "process_cmd", ",", "dropout_rate", "=", "0.", ",", "\n", "name", "=", "''", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.encoder_layer": [[238, 299], ["transformer_encoder.multi_head_attention", "post_process_layer", "transformer_encoder.positionwise_feed_forward", "post_process_layer", "pre_process_layer", "pre_process_layer"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.multi_head_attention", "home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.positionwise_feed_forward"], ["", "elif", "cmd", "==", "\"n\"", ":", "# add layer normalization", "\n", "            ", "out_dtype", "=", "out", ".", "dtype", "\n", "if", "out_dtype", "==", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP16", ":", "\n", "                ", "out", "=", "layers", ".", "cast", "(", "x", "=", "out", ",", "dtype", "=", "\"float32\"", ")", "\n", "", "out", "=", "layer_norm", "(", "\n", "out", ",", "\n", "begin_norm_axis", "=", "len", "(", "out", ".", "shape", ")", "-", "1", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_layer_norm_scale'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "1.", ")", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "name", "+", "'_layer_norm_bias'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", ")", "\n", "if", "out_dtype", "==", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP16", ":", "\n", "                ", "out", "=", "layers", ".", "cast", "(", "x", "=", "out", ",", "dtype", "=", "\"float16\"", ")", "\n", "", "", "elif", "cmd", "==", "\"d\"", ":", "# add dropout", "\n", "            ", "if", "dropout_rate", ":", "\n", "                ", "out", "=", "layers", ".", "dropout", "(", "\n", "out", ",", "\n", "dropout_prob", "=", "dropout_rate", ",", "\n", "dropout_implementation", "=", "\"upscale_in_train\"", ",", "\n", "is_test", "=", "False", ")", "\n", "", "", "", "return", "out", "\n", "\n", "\n", "", "pre_process_layer", "=", "partial", "(", "pre_post_process_layer", ",", "None", ")", "\n", "post_process_layer", "=", "pre_post_process_layer", "\n", "\n", "def", "encoder_layer", "(", "enc_input", ",", "\n", "attn_bias", ",", "\n", "n_head", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "d_inner_hid", ",", "\n", "prepostprocess_dropout", ",", "\n", "attention_dropout", ",", "\n", "relu_dropout", ",", "\n", "hidden_act", ",", "\n", "preprocess_cmd", "=", "\"n\"", ",", "\n", "postprocess_cmd", "=", "\"da\"", ",", "\n", "param_initializer", "=", "None", ",", "\n", "name", "=", "''", ")", ":", "\n", "    ", "\"\"\"The encoder layers that can be stacked to form a deep encoder.\n    This module consits of a multi-head (self) attention followed by\n    position-wise feed-forward networks and both the two components companied\n    with the post_process_layer to add residual connection, layer normalization\n    and droput.\n    \"\"\"", "\n", "attn_output", "=", "multi_head_attention", "(", "\n", "pre_process_layer", "(", "\n", "enc_input", ",", "\n", "preprocess_cmd", ",", "\n", "prepostprocess_dropout", ",", "\n", "name", "=", "name", "+", "'_pre_att'", ")", ",", "\n", "None", ",", "\n", "None", ",", "\n", "attn_bias", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "n_head", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.encoder": [[301, 343], ["range", "pre_process_layer", "transformer_encoder.encoder_layer", "str"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.encoder_layer"], ["param_initializer", "=", "param_initializer", ",", "\n", "name", "=", "name", "+", "'_multi_head_att'", ")", "\n", "attn_output", "=", "post_process_layer", "(", "\n", "enc_input", ",", "\n", "attn_output", ",", "\n", "postprocess_cmd", ",", "\n", "prepostprocess_dropout", ",", "\n", "name", "=", "name", "+", "'_post_att'", ")", "\n", "ffd_output", "=", "positionwise_feed_forward", "(", "\n", "pre_process_layer", "(", "\n", "attn_output", ",", "\n", "preprocess_cmd", ",", "\n", "prepostprocess_dropout", ",", "\n", "name", "=", "name", "+", "'_pre_ffn'", ")", ",", "\n", "d_inner_hid", ",", "\n", "d_model", ",", "\n", "relu_dropout", ",", "\n", "hidden_act", ",", "\n", "param_initializer", "=", "param_initializer", ",", "\n", "name", "=", "name", "+", "'_ffn'", ")", "\n", "return", "post_process_layer", "(", "\n", "attn_output", ",", "\n", "ffd_output", ",", "\n", "postprocess_cmd", ",", "\n", "prepostprocess_dropout", ",", "\n", "name", "=", "name", "+", "'_post_ffn'", ")", "\n", "\n", "\n", "", "def", "encoder", "(", "enc_input", ",", "\n", "attn_bias", ",", "\n", "n_layer", ",", "\n", "n_head", ",", "\n", "d_key", ",", "\n", "d_value", ",", "\n", "d_model", ",", "\n", "d_inner_hid", ",", "\n", "prepostprocess_dropout", ",", "\n", "attention_dropout", ",", "\n", "relu_dropout", ",", "\n", "hidden_act", ",", "\n", "preprocess_cmd", "=", "\"n\"", ",", "\n", "postprocess_cmd", "=", "\"da\"", ",", "\n", "param_initializer", "=", "None", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet.XLNetConfig.__init__": [[56, 58], ["xlnet.XLNetConfig._parse"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig._parse"], ["    ", "def", "__init__", "(", "self", ",", "config_path", ")", ":", "\n", "        ", "self", ".", "_config_dict", "=", "self", ".", "_parse", "(", "config_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet.XLNetConfig._parse": [[59, 68], ["open", "json.load", "IOError"], "methods", ["None"], ["", "def", "_parse", "(", "self", ",", "config_path", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "with", "open", "(", "config_path", ")", "as", "json_file", ":", "\n", "                ", "config_dict", "=", "json", ".", "load", "(", "json_file", ")", "\n", "", "", "except", "Exception", ":", "\n", "            ", "raise", "IOError", "(", "\"Error in parsing xlnet model config file '%s'\"", "%", "\n", "config_path", ")", "\n", "", "else", ":", "\n", "            ", "return", "config_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet.XLNetConfig.__getitem__": [[69, 71], ["None"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "self", ".", "_config_dict", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet.XLNetConfig.has_key": [[72, 74], ["xlnet.XLNetConfig._config_dict.has_key"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet.XLNetConfig.has_key"], ["", "def", "has_key", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "self", ".", "_config_dict", ".", "has_key", "(", "key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet.XLNetConfig.print_config": [[75, 79], ["sorted", "print", "six.iteritems", "print"], "methods", ["None"], ["", "def", "print_config", "(", "self", ")", ":", "\n", "        ", "for", "arg", ",", "value", "in", "sorted", "(", "six", ".", "iteritems", "(", "self", ".", "_config_dict", ")", ")", ":", "\n", "            ", "print", "(", "'%s: %s'", "%", "(", "arg", ",", "value", ")", ")", "\n", "", "print", "(", "'------------------------------------------------'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet.XLNetModel.__init__": [[82, 144], ["xlnet._get_initiliaizer", "dict", "dict", "dict.update", "modeling.transformer_xl"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet._get_initiliaizer", "home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.modeling.transformer_xl"], ["    ", "def", "__init__", "(", "self", ",", "\n", "xlnet_config", ",", "\n", "input_ids", ",", "\n", "seg_ids", ",", "\n", "input_mask", ",", "\n", "args", ",", "\n", "mems", "=", "None", ",", "\n", "perm_mask", "=", "None", ",", "\n", "target_mapping", "=", "None", ",", "\n", "inp_q", "=", "None", ")", ":", "\n", "        ", "self", ".", "_tie_weight", "=", "True", "\n", "\n", "self", ".", "_d_head", "=", "xlnet_config", "[", "'d_head'", "]", "\n", "self", ".", "_d_inner", "=", "xlnet_config", "[", "'d_inner'", "]", "\n", "self", ".", "_d_model", "=", "xlnet_config", "[", "'d_model'", "]", "\n", "self", ".", "_ff_activation", "=", "xlnet_config", "[", "'ff_activation'", "]", "\n", "self", ".", "_n_head", "=", "xlnet_config", "[", "'n_head'", "]", "\n", "self", ".", "_n_layer", "=", "xlnet_config", "[", "'n_layer'", "]", "\n", "self", ".", "_n_token", "=", "xlnet_config", "[", "'n_token'", "]", "\n", "self", ".", "_untie_r", "=", "xlnet_config", "[", "'untie_r'", "]", "\n", "\n", "self", ".", "_mem_len", "=", "None", "if", "'mem_len'", "not", "in", "args", "else", "args", ".", "mem_len", "\n", "self", ".", "_reuse_len", "=", "None", "if", "'reuse_len'", "not", "in", "args", "else", "args", ".", "reuse_len", "\n", "self", ".", "_bi_data", "=", "False", "if", "'bi_data'", "not", "in", "args", "else", "args", ".", "bi_data", "\n", "self", ".", "_clamp_len", "=", "args", ".", "clamp_len", "\n", "self", ".", "_same_length", "=", "False", "if", "'same_length'", "not", "in", "args", "else", "args", ".", "same_length", "\n", "# Initialize all weigths by the specified initializer, and all biases ", "\n", "# will be initialized by constant zero by default.", "\n", "self", ".", "_param_initializer", "=", "_get_initiliaizer", "(", "args", ")", "\n", "\n", "tfm_args", "=", "dict", "(", "\n", "n_token", "=", "self", ".", "_n_token", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ",", "\n", "attn_type", "=", "\"bi\"", ",", "\n", "n_layer", "=", "self", ".", "_n_layer", ",", "\n", "d_model", "=", "self", ".", "_d_model", ",", "\n", "n_head", "=", "self", ".", "_n_head", ",", "\n", "d_head", "=", "self", ".", "_d_head", ",", "\n", "d_inner", "=", "self", ".", "_d_inner", ",", "\n", "ff_activation", "=", "self", ".", "_ff_activation", ",", "\n", "untie_r", "=", "self", ".", "_untie_r", ",", "\n", "\n", "use_bfloat16", "=", "args", ".", "use_fp16", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "dropatt", "=", "args", ".", "dropatt", ",", "\n", "\n", "mem_len", "=", "self", ".", "_mem_len", ",", "\n", "reuse_len", "=", "self", ".", "_reuse_len", ",", "\n", "bi_data", "=", "self", ".", "_bi_data", ",", "\n", "clamp_len", "=", "args", ".", "clamp_len", ",", "\n", "same_length", "=", "self", ".", "_same_length", ",", "\n", "name", "=", "'model_transformer'", ")", "\n", "input_args", "=", "dict", "(", "\n", "inp_k", "=", "input_ids", ",", "\n", "seg_id", "=", "seg_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "mems", "=", "mems", ",", "\n", "perm_mask", "=", "perm_mask", ",", "\n", "target_mapping", "=", "target_mapping", ",", "\n", "inp_q", "=", "inp_q", ")", "\n", "tfm_args", ".", "update", "(", "input_args", ")", "\n", "self", ".", "output", ",", "self", ".", "new_mems", ",", "self", ".", "lookup_table", "=", "modeling", ".", "transformer_xl", "(", "**", "tfm_args", ")", "\n", "#self._build_model(input_ids, sentence_ids, input_mask)", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet.XLNetModel.get_initializer": [[146, 148], ["None"], "methods", ["None"], ["", "def", "get_initializer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_param_initializer", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet.XLNetModel.get_sequence_output": [[151, 153], ["None"], "methods", ["None"], ["", "def", "get_sequence_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet.XLNetModel.get_pooled_output": [[154, 167], ["paddle.layers.slice", "paddle.layers.fc", "paddle.ParamAttr"], "methods", ["None"], ["", "def", "get_pooled_output", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the first feature of each sequence for classification\"\"\"", "\n", "\n", "next_sent_feat", "=", "fluid", ".", "layers", ".", "slice", "(", "\n", "input", "=", "self", ".", "_enc_out", ",", "axes", "=", "[", "1", "]", ",", "starts", "=", "[", "0", "]", ",", "ends", "=", "[", "1", "]", ")", "\n", "next_sent_feat", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "next_sent_feat", ",", "\n", "size", "=", "self", ".", "_emb_size", ",", "\n", "act", "=", "\"tanh\"", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"pooled_fc.w_0\"", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "\"pooled_fc.b_0\"", ")", "\n", "return", "next_sent_feat", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet.XLNetModel.get_pretraining_output": [[168, 238], ["paddle.layers.cast", "xlnet.XLNetModel.get_pooled_output", "paddle.layers.reshape", "paddle.layers.gather", "paddle.layers.fc", "model.transformer_encoder.pre_process_layer", "paddle.ParamAttr", "paddle.layers.softmax_with_cross_entropy", "paddle.layers.mean", "paddle.layers.fc", "paddle.layers.softmax_with_cross_entropy", "paddle.layers.accuracy", "paddle.layers.mean", "paddle.default_main_program().global_block().var", "paddle.layers.matmul", "paddle.layers.create_parameter", "paddle.layers.fc", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.initializer.Constant", "paddle.layers.cast", "paddle.ParamAttr", "paddle.default_main_program().global_block", "paddle.ParamAttr", "paddle.default_main_program"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.get_pooled_output"], ["", "def", "get_pretraining_output", "(", "self", ",", "mask_label", ",", "mask_pos", ",", "labels", ")", ":", "\n", "        ", "\"\"\"Get the loss & accuracy for pretraining\"\"\"", "\n", "\n", "mask_pos", "=", "fluid", ".", "layers", ".", "cast", "(", "x", "=", "mask_pos", ",", "dtype", "=", "'int32'", ")", "\n", "\n", "# extract the first token feature in each sentence", "\n", "next_sent_feat", "=", "self", ".", "get_pooled_output", "(", ")", "\n", "reshaped_emb_out", "=", "fluid", ".", "layers", ".", "reshape", "(", "\n", "x", "=", "self", ".", "_enc_out", ",", "shape", "=", "[", "-", "1", ",", "self", ".", "_emb_size", "]", ")", "\n", "# extract masked tokens' feature", "\n", "mask_feat", "=", "fluid", ".", "layers", ".", "gather", "(", "input", "=", "reshaped_emb_out", ",", "index", "=", "mask_pos", ")", "\n", "\n", "# transform: fc", "\n", "mask_trans_feat", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "mask_feat", ",", "\n", "size", "=", "self", ".", "_emb_size", ",", "\n", "act", "=", "self", ".", "_hidden_act", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "'mask_lm_trans_fc.w_0'", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "'mask_lm_trans_fc.b_0'", ")", ")", "\n", "# transform: layer norm ", "\n", "mask_trans_feat", "=", "pre_process_layer", "(", "\n", "mask_trans_feat", ",", "'n'", ",", "name", "=", "'mask_lm_trans'", ")", "\n", "\n", "mask_lm_out_bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"mask_lm_out_fc.b_0\"", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "value", "=", "0.0", ")", ")", "\n", "if", "self", ".", "_weight_sharing", ":", "\n", "            ", "word_emb", "=", "fluid", ".", "default_main_program", "(", ")", ".", "global_block", "(", ")", ".", "var", "(", "\n", "self", ".", "_word_emb_name", ")", "\n", "if", "self", ".", "_emb_dtype", "!=", "self", ".", "_dtype", ":", "\n", "                ", "word_emb", "=", "fluid", ".", "layers", ".", "cast", "(", "word_emb", ",", "self", ".", "_dtype", ")", "\n", "", "fc_out", "=", "fluid", ".", "layers", ".", "matmul", "(", "\n", "x", "=", "mask_trans_feat", ",", "y", "=", "word_emb", ",", "transpose_y", "=", "True", ")", "\n", "fc_out", "+=", "fluid", ".", "layers", ".", "create_parameter", "(", "\n", "shape", "=", "[", "self", ".", "_voc_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "attr", "=", "mask_lm_out_bias_attr", ",", "\n", "is_bias", "=", "True", ")", "\n", "\n", "", "else", ":", "\n", "            ", "fc_out", "=", "fluid", ".", "layers", ".", "fc", "(", "input", "=", "mask_trans_feat", ",", "\n", "size", "=", "self", ".", "_voc_size", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"mask_lm_out_fc.w_0\"", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "mask_lm_out_bias_attr", ")", "\n", "\n", "", "mask_lm_loss", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "fc_out", ",", "label", "=", "mask_label", ")", "\n", "mean_mask_lm_loss", "=", "fluid", ".", "layers", ".", "mean", "(", "mask_lm_loss", ")", "\n", "\n", "next_sent_fc_out", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "next_sent_feat", ",", "\n", "size", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"next_sent_fc.w_0\"", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "\"next_sent_fc.b_0\"", ")", "\n", "\n", "next_sent_loss", ",", "next_sent_softmax", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "next_sent_fc_out", ",", "label", "=", "labels", ",", "return_softmax", "=", "True", ")", "\n", "\n", "next_sent_acc", "=", "fluid", ".", "layers", ".", "accuracy", "(", "\n", "input", "=", "next_sent_softmax", ",", "label", "=", "labels", ")", "\n", "\n", "mean_next_sent_loss", "=", "fluid", ".", "layers", ".", "mean", "(", "next_sent_loss", ")", "\n", "\n", "loss", "=", "mean_next_sent_loss", "+", "mean_mask_lm_loss", "\n", "return", "next_sent_acc", ",", "mean_mask_lm_loss", ",", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet._get_initiliaizer": [[27, 36], ["paddle.initializer.Uniform", "paddle.initializer.Normal", "ValueError"], "function", ["None"], ["def", "_get_initiliaizer", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "init", "==", "\"uniform\"", ":", "\n", "        ", "param_initializer", "=", "fluid", ".", "initializer", ".", "Uniform", "(", "\n", "low", "=", "-", "args", ".", "init_range", ",", "high", "=", "args", ".", "init_range", ")", "\n", "", "elif", "args", ".", "init", "==", "\"normal\"", ":", "\n", "        ", "param_initializer", "=", "fluid", ".", "initializer", ".", "Normal", "(", "scale", "=", "args", ".", "init_std", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Initializer {} not supported\"", ".", "format", "(", "args", ".", "init", ")", ")", "\n", "", "return", "param_initializer", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.xlnet.init_attn_mask": [[37, 54], ["numpy.ones", "numpy.triu", "numpy.diag", "numpy.zeros", "numpy.concatenate", "paddle.global_scope().find_var().get_tensor", "fluid.global_scope().find_var().get_tensor.set", "numpy.diag", "numpy.tril", "numpy.concatenate", "paddle.global_scope().find_var", "paddle.global_scope"], "function", ["None"], ["", "def", "init_attn_mask", "(", "args", ",", "place", ")", ":", "\n", "    ", "\"\"\"create causal attention mask.\"\"\"", "\n", "qlen", "=", "args", ".", "max_seq_length", "\n", "mlen", "=", "0", "if", "'mem_len'", "not", "in", "args", "else", "args", ".", "mem_len", "\n", "same_length", "=", "False", "if", "'same_length'", "not", "in", "args", "else", "args", ".", "same_length", "\n", "dtype", "=", "'float16'", "if", "args", ".", "use_fp16", "else", "'float32'", "\n", "attn_mask", "=", "np", ".", "ones", "(", "[", "qlen", ",", "qlen", "]", ",", "dtype", "=", "dtype", ")", "\n", "mask_u", "=", "np", ".", "triu", "(", "attn_mask", ")", "\n", "mask_dia", "=", "np", ".", "diag", "(", "np", ".", "diag", "(", "attn_mask", ")", ")", "\n", "attn_mask_pad", "=", "np", ".", "zeros", "(", "[", "qlen", ",", "mlen", "]", ",", "dtype", "=", "dtype", ")", "\n", "attn_mask", "=", "np", ".", "concatenate", "(", "[", "attn_mask_pad", ",", "mask_u", "-", "mask_dia", "]", ",", "1", ")", "\n", "if", "same_length", ":", "\n", "        ", "mask_l", "=", "np", ".", "tril", "(", "attn_mask", ")", "\n", "attn_mask", "=", "np", ".", "concatenate", "(", "[", "ret", "[", ":", ",", ":", "qlen", "]", "+", "mask_l", "-", "mask_dia", ",", "ret", "[", ":", ",", "qlen", ":", "]", "]", ",", "1", ")", "\n", "", "attn_mask", "=", "attn_mask", "[", ":", ",", ":", ",", "None", ",", "None", "]", "\n", "attn_mask_t", "=", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "\"attn_mask\"", ")", ".", "get_tensor", "(", ")", "\n", "attn_mask_t", ".", "set", "(", "attn_mask", ",", "place", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.transformer_encoder.layer_norm": [[27, 56], ["paddle.fluid.layer_helper.LayerHelper", "paddle.reduce_mean", "paddle.elementwise_sub", "paddle.reduce_mean", "paddle.rsqrt", "paddle.elementwise_mul", "paddle.fluid.layer_helper.LayerHelper.create_parameter", "paddle.fluid.layer_helper.LayerHelper.create_parameter", "paddle.elementwise_mul", "paddle.elementwise_add", "paddle.square", "functools.reduce", "locals", "paddle.initializer.Constant", "paddle.initializer.Constant"], "function", ["None"], ["def", "layer_norm", "(", "x", ",", "begin_norm_axis", "=", "1", ",", "epsilon", "=", "1e-6", ",", "param_attr", "=", "None", ",", "bias_attr", "=", "None", ")", ":", "\n", "    ", "helper", "=", "LayerHelper", "(", "'layer_norm'", ",", "**", "locals", "(", ")", ")", "\n", "mean", "=", "layers", ".", "reduce_mean", "(", "x", ",", "dim", "=", "begin_norm_axis", ",", "keep_dim", "=", "True", ")", "\n", "shift_x", "=", "layers", ".", "elementwise_sub", "(", "x", "=", "x", ",", "y", "=", "mean", ",", "axis", "=", "0", ")", "\n", "variance", "=", "layers", ".", "reduce_mean", "(", "layers", ".", "square", "(", "shift_x", ")", ",", "dim", "=", "begin_norm_axis", ",", "keep_dim", "=", "True", ")", "\n", "r_stdev", "=", "layers", ".", "rsqrt", "(", "variance", "+", "epsilon", ")", "\n", "norm_x", "=", "layers", ".", "elementwise_mul", "(", "x", "=", "shift_x", ",", "y", "=", "r_stdev", ",", "axis", "=", "0", ")", "\n", "\n", "param_shape", "=", "[", "reduce", "(", "lambda", "x", ",", "y", ":", "x", "*", "y", ",", "norm_x", ".", "shape", "[", "begin_norm_axis", ":", "]", ")", "]", "\n", "param_dtype", "=", "norm_x", ".", "dtype", "\n", "scale", "=", "helper", ".", "create_parameter", "(", "\n", "attr", "=", "param_attr", ",", "\n", "shape", "=", "param_shape", ",", "\n", "dtype", "=", "param_dtype", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "1.", ")", ")", "\n", "bias", "=", "helper", ".", "create_parameter", "(", "\n", "attr", "=", "bias_attr", ",", "\n", "shape", "=", "param_shape", ",", "\n", "dtype", "=", "param_dtype", ",", "\n", "is_bias", "=", "True", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", "\n", "\n", "out", "=", "layers", ".", "elementwise_mul", "(", "x", "=", "norm_x", ",", "y", "=", "scale", ",", "axis", "=", "-", "1", ")", "\n", "out", "=", "layers", ".", "elementwise_add", "(", "x", "=", "out", ",", "y", "=", "bias", ",", "axis", "=", "-", "1", ")", "\n", "\n", "return", "out", "\n", "\n", "", "def", "multi_head_attention", "(", "queries", ",", "\n", "keys", ",", "\n", "values", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.bert.BertConfig.__init__": [[34, 36], ["bert.BertConfig._parse"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig._parse"], ["input_mask", ",", "\n", "config", ",", "\n", "weight_sharing", "=", "True", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.bert.BertConfig._parse": [[37, 46], ["open", "json.load", "IOError"], "methods", ["None"], ["use_fp16", "=", "False", ",", "\n", "model_name", "=", "''", ")", ":", "\n", "\n", "        ", "self", ".", "_emb_size", "=", "config", "[", "\"hidden_size\"", "]", "\n", "self", ".", "_n_layer", "=", "config", "[", "\"num_hidden_layers\"", "]", "\n", "self", ".", "_n_head", "=", "config", "[", "\"num_attention_heads\"", "]", "\n", "self", ".", "_voc_size", "=", "config", "[", "\"vocab_size\"", "]", "\n", "self", ".", "_max_position_seq_len", "=", "config", "[", "\"max_position_embeddings\"", "]", "\n", "self", ".", "_sent_types", "=", "config", "[", "\"type_vocab_size\"", "]", "\n", "self", ".", "_hidden_act", "=", "config", "[", "\"hidden_act\"", "]", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.bert.BertConfig.__getitem__": [[47, 49], ["None"], "methods", ["None"], ["self", ".", "_prepostprocess_dropout", "=", "config", "[", "\"hidden_dropout_prob\"", "]", "\n", "self", ".", "_attention_dropout", "=", "config", "[", "\"attention_probs_dropout_prob\"", "]", "\n", "self", ".", "_weight_sharing", "=", "weight_sharing", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.bert.BertConfig.print_config": [[50, 54], ["sorted", "logger.info", "six.iteritems", "logger.info"], "methods", ["None"], ["\n", "self", ".", "model_name", "=", "model_name", "\n", "\n", "self", ".", "_word_emb_name", "=", "self", ".", "model_name", "+", "\"word_embedding\"", "\n", "self", ".", "_pos_emb_name", "=", "self", ".", "model_name", "+", "\"pos_embedding\"", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.bert.BertModel.__init__": [[57, 88], ["paddle.initializer.TruncatedNormal", "bert.BertModel._build_model"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel._build_model"], ["\n", "# Initialize all weigths by truncated normal initializer, and all biases ", "\n", "# will be initialized by constant zero by default.", "\n", "self", ".", "_param_initializer", "=", "fluid", ".", "initializer", ".", "TruncatedNormal", "(", "\n", "scale", "=", "config", "[", "\"initializer_range\"", "]", ")", "\n", "\n", "self", ".", "_build_model", "(", "src_ids", ",", "position_ids", ",", "sentence_ids", ",", "input_mask", ",", "\n", "config", ")", "\n", "\n", "", "def", "_build_model", "(", "self", ",", "src_ids", ",", "position_ids", ",", "sentence_ids", ",", "input_mask", ",", "\n", "config", ")", ":", "\n", "# padding id in vocabulary must be set to 0", "\n", "        ", "emb_out", "=", "fluid", ".", "layers", ".", "embedding", "(", "\n", "input", "=", "src_ids", ",", "\n", "size", "=", "[", "self", ".", "_voc_size", ",", "self", ".", "_emb_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "_word_emb_name", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "is_sparse", "=", "False", ")", "\n", "\n", "self", ".", "emb_out", "=", "emb_out", "\n", "\n", "position_emb_out", "=", "fluid", ".", "layers", ".", "embedding", "(", "\n", "input", "=", "position_ids", ",", "\n", "size", "=", "[", "self", ".", "_max_position_seq_len", ",", "self", ".", "_emb_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "_pos_emb_name", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ")", "\n", "\n", "self", ".", "position_emb_out", "=", "position_emb_out", "\n", "\n", "sent_emb_out", "=", "fluid", ".", "layers", ".", "embedding", "(", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.bert.BertModel._build_model": [[89, 147], ["paddle.layers.embedding", "paddle.layers.embedding", "paddle.layers.embedding", "model.transformer_encoder.pre_process_layer", "paddle.layers.expand", "paddle.layers.scale", "paddle.layers.stack", "model.transformer_encoder.encoder", "paddle.layers.cast", "paddle.layers.transpose", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.ParamAttr"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.encoder"], ["sentence_ids", ",", "\n", "size", "=", "[", "self", ".", "_sent_types", ",", "self", ".", "_emb_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "_sent_emb_name", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ")", "\n", "\n", "self", ".", "sent_emb_out", "=", "sent_emb_out", "\n", "\n", "emb_out", "=", "emb_out", "+", "position_emb_out", "\n", "emb_out", "=", "emb_out", "+", "sent_emb_out", "\n", "\n", "emb_out", "=", "pre_process_layer", "(", "\n", "emb_out", ",", "'nd'", ",", "self", ".", "_prepostprocess_dropout", ",", "name", "=", "'pre_encoder'", ")", "\n", "\n", "if", "self", ".", "_dtype", "==", "\"float16\"", ":", "\n", "            ", "input_mask", "=", "fluid", ".", "layers", ".", "cast", "(", "x", "=", "input_mask", ",", "dtype", "=", "self", ".", "_dtype", ")", "\n", "\n", "", "self_attn_mask", "=", "fluid", ".", "layers", ".", "matmul", "(", "\n", "x", "=", "input_mask", ",", "y", "=", "input_mask", ",", "transpose_y", "=", "True", ")", "\n", "\n", "self_attn_mask", "=", "fluid", ".", "layers", ".", "scale", "(", "\n", "x", "=", "self_attn_mask", ",", "\n", "scale", "=", "config", "[", "\"self_att_scale\"", "]", ",", "\n", "bias", "=", "-", "1.0", ",", "\n", "bias_after_scale", "=", "False", ")", "\n", "\n", "n_head_self_attn_mask", "=", "fluid", ".", "layers", ".", "stack", "(", "\n", "x", "=", "[", "self_attn_mask", "]", "*", "self", ".", "_n_head", ",", "axis", "=", "1", ")", "\n", "\n", "n_head_self_attn_mask", ".", "stop_gradient", "=", "True", "\n", "\n", "self", ".", "_enc_out", "=", "encoder", "(", "\n", "enc_input", "=", "emb_out", ",", "\n", "attn_bias", "=", "n_head_self_attn_mask", ",", "\n", "n_layer", "=", "self", ".", "_n_layer", ",", "\n", "n_head", "=", "self", ".", "_n_head", ",", "\n", "d_key", "=", "self", ".", "_emb_size", "//", "self", ".", "_n_head", ",", "\n", "d_value", "=", "self", ".", "_emb_size", "//", "self", ".", "_n_head", ",", "\n", "d_model", "=", "self", ".", "_emb_size", ",", "\n", "d_inner_hid", "=", "self", ".", "_emb_size", "*", "4", ",", "\n", "prepostprocess_dropout", "=", "self", ".", "_prepostprocess_dropout", ",", "\n", "attention_dropout", "=", "self", ".", "_attention_dropout", ",", "\n", "relu_dropout", "=", "0", ",", "\n", "hidden_act", "=", "self", ".", "_hidden_act", ",", "\n", "preprocess_cmd", "=", "\"\"", ",", "\n", "postprocess_cmd", "=", "\"dan\"", ",", "\n", "param_initializer", "=", "self", ".", "_param_initializer", ",", "\n", "name", "=", "self", ".", "model_name", "+", "'encoder'", ")", "\n", "\n", "", "def", "get_sequence_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_enc_out", "\n", "\n", "", "def", "get_pooled_output", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the first feature of each sequence for classification\"\"\"", "\n", "\n", "next_sent_feat", "=", "fluid", ".", "layers", ".", "slice", "(", "\n", "input", "=", "self", ".", "_enc_out", ",", "axes", "=", "[", "1", "]", ",", "starts", "=", "[", "0", "]", ",", "ends", "=", "[", "1", "]", ")", "\n", "next_sent_feat", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "next_sent_feat", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.bert.BertModel.get_sequence_output": [[148, 150], ["None"], "methods", ["None"], ["size", "=", "self", ".", "_emb_size", ",", "\n", "act", "=", "\"tanh\"", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.bert.BertModel.get_pooled_output": [[151, 164], ["paddle.layers.slice", "paddle.layers.fc", "paddle.ParamAttr"], "methods", ["None"], ["name", "=", "self", ".", "model_name", "+", "\"pooled_fc.w_0\"", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "\"pooled_fc.b_0\"", ")", "\n", "return", "next_sent_feat", "\n", "\n", "", "def", "get_pretraining_output", "(", "self", ",", "mask_label", ",", "mask_pos", ",", "labels", ")", ":", "\n", "        ", "\"\"\"Get the loss & accuracy for pretraining\"\"\"", "\n", "\n", "mask_pos", "=", "fluid", ".", "layers", ".", "cast", "(", "x", "=", "mask_pos", ",", "dtype", "=", "'int32'", ")", "\n", "\n", "# extract the first token feature in each sentence", "\n", "next_sent_feat", "=", "self", ".", "get_pooled_output", "(", ")", "\n", "reshaped_emb_out", "=", "fluid", ".", "layers", ".", "reshape", "(", "\n", "x", "=", "self", ".", "_enc_out", ",", "shape", "=", "[", "-", "1", ",", "self", ".", "_emb_size", "]", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.bert.BertModel.get_pretraining_output": [[165, 234], ["paddle.layers.cast", "bert.BertModel.get_pooled_output", "paddle.layers.reshape", "paddle.layers.gather", "paddle.layers.fc", "model.transformer_encoder.pre_process_layer", "paddle.ParamAttr", "paddle.layers.softmax_with_cross_entropy", "paddle.layers.mean", "paddle.layers.fc", "paddle.layers.softmax_with_cross_entropy", "paddle.layers.accuracy", "paddle.layers.mean", "paddle.layers.matmul", "paddle.layers.create_parameter", "paddle.layers.fc", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.initializer.Constant", "paddle.ParamAttr", "paddle.default_main_program().global_block().var", "paddle.ParamAttr", "paddle.default_main_program().global_block", "paddle.default_main_program"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.get_pooled_output"], ["# extract masked tokens' feature", "\n", "mask_feat", "=", "fluid", ".", "layers", ".", "gather", "(", "input", "=", "reshaped_emb_out", ",", "index", "=", "mask_pos", ")", "\n", "\n", "# transform: fc", "\n", "mask_trans_feat", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "mask_feat", ",", "\n", "size", "=", "self", ".", "_emb_size", ",", "\n", "act", "=", "self", ".", "_hidden_act", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "model_name", "+", "'mask_lm_trans_fc.w_0'", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "model_name", "+", "'mask_lm_trans_fc.b_0'", ")", ")", "\n", "# transform: layer norm ", "\n", "mask_trans_feat", "=", "pre_process_layer", "(", "\n", "mask_trans_feat", ",", "'n'", ",", "name", "=", "self", ".", "model_name", "+", "'mask_lm_trans'", ")", "\n", "\n", "mask_lm_out_bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "model_name", "+", "\"mask_lm_out_fc.b_0\"", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "value", "=", "0.0", ")", ")", "\n", "if", "self", ".", "_weight_sharing", ":", "\n", "            ", "fc_out", "=", "fluid", ".", "layers", ".", "matmul", "(", "\n", "x", "=", "mask_trans_feat", ",", "\n", "y", "=", "fluid", ".", "default_main_program", "(", ")", ".", "global_block", "(", ")", ".", "var", "(", "\n", "self", ".", "_word_emb_name", ")", ",", "\n", "transpose_y", "=", "True", ")", "\n", "fc_out", "+=", "fluid", ".", "layers", ".", "create_parameter", "(", "\n", "shape", "=", "[", "self", ".", "_voc_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "attr", "=", "mask_lm_out_bias_attr", ",", "\n", "is_bias", "=", "True", ")", "\n", "\n", "", "else", ":", "\n", "            ", "fc_out", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "mask_trans_feat", ",", "\n", "size", "=", "self", ".", "_voc_size", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "model_name", "+", "\"mask_lm_out_fc.w_0\"", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "mask_lm_out_bias_attr", ")", "\n", "\n", "", "mask_lm_loss", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "fc_out", ",", "label", "=", "mask_label", ")", "\n", "mean_mask_lm_loss", "=", "fluid", ".", "layers", ".", "mean", "(", "mask_lm_loss", ")", "\n", "\n", "next_sent_fc_out", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "next_sent_feat", ",", "\n", "size", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "model_name", "+", "\"next_sent_fc.w_0\"", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "self", ".", "model_name", "+", "\"next_sent_fc.b_0\"", ")", "\n", "\n", "next_sent_loss", ",", "next_sent_softmax", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "next_sent_fc_out", ",", "label", "=", "labels", ",", "return_softmax", "=", "True", ")", "\n", "\n", "next_sent_acc", "=", "fluid", ".", "layers", ".", "accuracy", "(", "\n", "input", "=", "next_sent_softmax", ",", "label", "=", "labels", ")", "\n", "\n", "mean_next_sent_loss", "=", "fluid", ".", "layers", ".", "mean", "(", "next_sent_loss", ")", "\n", "\n", "loss", "=", "mean_next_sent_loss", "+", "mean_mask_lm_loss", "\n", "return", "next_sent_acc", ",", "mean_mask_lm_loss", ",", "loss", "\n", "\n", "\n", "", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "print", "(", "\"hello wolrd!\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.MemoryLayer.__init__": [[61, 69], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "bert_config", ",", "concept_size", ",", "mem_emb_size", ",", "mem_method", "=", "'cat'", ",", "prefix", "=", "None", ")", ":", "\n", "        ", "self", ".", "initializer_range", "=", "bert_config", "[", "'initializer_range'", "]", "\n", "self", ".", "bert_size", "=", "bert_config", "[", "'hidden_size'", "]", "\n", "self", ".", "concept_size", "=", "concept_size", "\n", "self", ".", "mem_emb_size", "=", "mem_emb_size", "\n", "assert", "mem_method", "in", "[", "'add'", ",", "'cat'", ",", "'raw'", "]", "\n", "self", ".", "mem_method", "=", "mem_method", "\n", "self", ".", "prefix", "=", "prefix", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.MemoryLayer.forward": [[70, 159], ["layers.print_tensor", "layers.print_tensor", "layers.print_tensor", "paddle.layers.fc", "logger.info", "paddle.layers.unsqueeze", "layers.MemoryLayer.add_sentinel", "layers.MemoryLayer.get_concept_oridinal", "paddle.layers.less_than", "paddle.layers.cast", "layers.print_tensor", "paddle.layers.scale", "layers.print_tensor", "paddle.layers.elementwise_add", "logger.info", "paddle.layers.softmax", "paddle.layers.unsqueeze", "logger.info", "logger.info", "paddle.layers.matmul", "paddle.layers.squeeze", "logger.info", "paddle.layers.expand", "paddle.layers.less_than", "paddle.layers.elementwise_mul", "layers.print_tensor", "paddle.layers.fc", "paddle.layers.sums", "paddle.ParamAttr", "layers.dynamic_expand", "paddle.layers.cast", "logger.info", "logger.info", "paddle.layers.concat", "paddle.layers.zeros", "logger.info", "logger.info", "ValueError", "paddle.initializer.NormalInitializer"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.MemoryLayer.add_sentinel", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.MemoryLayer.get_concept_oridinal", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.dynamic_expand"], ["", "def", "forward", "(", "self", ",", "bert_output", ",", "memory_embs", ",", "mem_length", ",", "ignore_no_memory_token", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        :param bert_output: [batch_size, seq_size, bert_size]\n        :param memory_embs: [batch_size, seq_size, concept_size, mem_emb_size]\n        :param mem_length: [batch_size, sent_size, 1]\n        :return: \n        \"\"\"", "\n", "\n", "bert_size", "=", "self", ".", "bert_size", "\n", "concept_size", "=", "self", ".", "concept_size", "\n", "mem_emb_size", "=", "self", ".", "mem_emb_size", "\n", "\n", "print_tensor", "(", "bert_output", ",", "\"bert_output\"", ")", "\n", "print_tensor", "(", "memory_embs", ",", "\"memory_embs\"", ")", "\n", "print_tensor", "(", "mem_length", ",", "\"mem_length\"", ")", "\n", "\n", "\n", "projected_bert", "=", "fluid", ".", "layers", ".", "fc", "(", "bert_output", ",", "size", "=", "mem_emb_size", ",", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "'{}_memory_layer_projection.w_0'", ".", "format", "(", "self", ".", "prefix", ")", "if", "self", ".", "prefix", "else", "'memory_layer_projection.w_0'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "NormalInitializer", "(", "\n", "loc", "=", "0.0", ",", "scale", "=", "self", ".", "initializer_range", ")", ")", ",", "\n", "bias_attr", "=", "False", ")", "# [batch_size *seq_size, mem_emb_size]", "\n", "logger", ".", "info", "(", "\"projected_bert: {}\"", ".", "format", "(", "projected_bert", ".", "shape", ")", ")", "\n", "\n", "expanded_bert", "=", "fluid", ".", "layers", ".", "unsqueeze", "(", "projected_bert", ",", "axes", "=", "[", "2", "]", ")", "# [batch_size, seq_size, 1, mem_emb_size]", "\n", "\n", "\n", "extended_memory", ",", "memory_score", "=", "self", ".", "add_sentinel", "(", "expanded_bert", ",", "memory_embs", ",", "mem_emb_size", ")", "\n", "# extended_memory: [batch_size, seq_size, 1+concept_size, mem_emb_size]", "\n", "# memory_score: [batch_size, seq_size, 1+concept_size]", "\n", "\n", "\n", "concept_ordinal", "=", "self", ".", "get_concept_oridinal", "(", "concept_size", ",", "memory_score", ")", "# [bs,sq,1+cs]", "\n", "\n", "memory_reverse_mask", "=", "fluid", ".", "layers", ".", "less_than", "(", "\n", "fluid", ".", "layers", ".", "expand", "(", "mem_length", ",", "expand_times", "=", "[", "1", ",", "1", ",", "1", "+", "concept_size", "]", ")", "\n", ",", "concept_ordinal", ")", "\n", "# [batch_size, seq_size, 1+concept_size]", "\n", "memory_reverse_mask", "=", "fluid", ".", "layers", ".", "cast", "(", "memory_reverse_mask", ",", "dtype", "=", "\"float32\"", ")", "\n", "print_tensor", "(", "memory_reverse_mask", ",", "\"memory_reverse_mask\"", ")", "\n", "\n", "memory_reverse_masked_infinity", "=", "fluid", ".", "layers", ".", "scale", "(", "memory_reverse_mask", ",", "scale", "=", "-", "1e6", ")", "\n", "# [batch_size, seq_size, 1+concept_size]", "\n", "print_tensor", "(", "memory_reverse_masked_infinity", ",", "\"memory_reverse_masked_infinity\"", ")", "\n", "\n", "memory_score", "=", "fluid", ".", "layers", ".", "elementwise_add", "(", "memory_score", ",", "memory_reverse_masked_infinity", ")", "\n", "# [batch_size, seq_size, 1+concept_size]", "\n", "logger", ".", "info", "(", "\"memory_score:{}\"", ".", "format", "(", "memory_score", ".", "shape", ")", ")", "\n", "\n", "memory_att", "=", "fluid", ".", "layers", ".", "softmax", "(", "memory_score", ")", "# [batch_size, seq_size, 1+concept_size]", "\n", "memory_att", "=", "fluid", ".", "layers", ".", "unsqueeze", "(", "memory_att", ",", "axes", "=", "[", "2", "]", ")", "# [batch_size, seq_size, 1, 1+concept_size]", "\n", "logger", ".", "info", "(", "\"memory_att: {}\"", ".", "format", "(", "memory_att", ".", "shape", ")", ")", "\n", "logger", ".", "info", "(", "\"extended_memory: {}\"", ".", "format", "(", "extended_memory", ".", "shape", ")", ")", "\n", "summ", "=", "fluid", ".", "layers", ".", "matmul", "(", "memory_att", ",", "extended_memory", ")", "# [batch_size, seq_size,1, mem_emb_size]", "\n", "summ", "=", "fluid", ".", "layers", ".", "squeeze", "(", "summ", ",", "axes", "=", "[", "2", "]", ")", "# [batch_size, seq_size,mem_emb_size]", "\n", "\n", "if", "ignore_no_memory_token", ":", "\n", "            ", "condition", "=", "fluid", ".", "layers", ".", "less_than", "(", "\n", "dynamic_expand", "(", "mem_length", ",", "fluid", ".", "layers", ".", "zeros", "(", "[", "1", "]", ",", "\"float32\"", ")", ")", ",", "\n", "mem_length", ")", "# [bs, sq]", "\n", "# summ_true = fluid.layers.elementwise_mul(", "\n", "#     summ,", "\n", "#     fluid.layers.cast(condition, \"float32\"))   # [bs, sq, ms]", "\n", "# summ_false = fluid.layers.elementwise_mul(", "\n", "#     summ,", "\n", "#     fluid.layers.scale(fluid.layers.cast(condition, \"float32\"), -1))  # [bs, sq, ms]", "\n", "# summ = fluid.layers.elementwise_add(summ_true, summ_false)  # [bs, sq, ms]", "\n", "summ", "=", "fluid", ".", "layers", ".", "elementwise_mul", "(", "\n", "summ", ",", "\n", "fluid", ".", "layers", ".", "cast", "(", "condition", ",", "\"float32\"", ")", ")", "# [bs, sq, ms]", "\n", "\n", "print_tensor", "(", "summ", ",", "\"summ\"", ")", "\n", "\n", "", "if", "self", ".", "mem_method", "==", "\"add\"", ":", "\n", "            ", "summ_transform", "=", "fluid", ".", "layers", ".", "fc", "(", "summ", ",", "size", "=", "bert_size", ",", "num_flatten_dims", "=", "2", ")", "# [batch_size, seq_size, bert_size]", "\n", "output", "=", "fluid", ".", "layers", ".", "sums", "(", "input", "=", "[", "summ_transform", ",", "bert_output", "]", ")", "# [batch_size, seq_size, bert_size]", "\n", "", "elif", "self", ".", "mem_method", "==", "\"cat\"", ":", "\n", "            ", "logger", ".", "info", "(", "\"bert_output: {}\"", ".", "format", "(", "bert_output", ".", "shape", ")", ")", "\n", "logger", ".", "info", "(", "\"summ: {}\"", ".", "format", "(", "summ", ".", "shape", ")", ")", "\n", "output", "=", "fluid", ".", "layers", ".", "concat", "(", "input", "=", "[", "bert_output", ",", "summ", "]", ",", "axis", "=", "2", ")", "# [batch_size, seq_size, bert_size + mem_emb_size]", "\n", "", "elif", "self", ".", "mem_method", "==", "\"raw\"", ":", "\n", "            ", "logger", ".", "info", "(", "\"bert_output: {}\"", ".", "format", "(", "bert_output", ".", "shape", ")", ")", "\n", "logger", ".", "info", "(", "\"summ: {}\"", ".", "format", "(", "summ", ".", "shape", ")", ")", "\n", "output", "=", "summ", "# [batch_size, seq_size, mem_emb_size]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"mem_method not supported\"", ")", "\n", "", "logger", ".", "info", "(", "\"output: {}\"", ".", "format", "(", "output", ".", "shape", ")", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.MemoryLayer.get_concept_oridinal": [[160, 177], ["paddle.layers.create_tensor", "paddle.layers.assign", "layers.print_tensor", "layers.print_tensor", "layers.dynamic_expand", "logger.info", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.dynamic_expand"], ["", "def", "get_concept_oridinal", "(", "self", ",", "concept_size", ",", "memory_score", ")", ":", "\n", "        ", "\"\"\"\n\n        :param concept_size:\n        :param memory_score: [batch_size, seq_size, 1+concept_size]\n        :return:\n        \"\"\"", "\n", "concept_ordinal", "=", "fluid", ".", "layers", ".", "create_tensor", "(", "dtype", "=", "\"float32\"", ")", "\n", "fluid", ".", "layers", ".", "assign", "(", "np", ".", "arange", "(", "start", "=", "0", ",", "stop", "=", "(", "1", "+", "concept_size", ")", ",", "step", "=", "1", ",", "dtype", "=", "np", ".", "float32", ")", ",", "\n", "concept_ordinal", ")", "# [1+cs]", "\n", "print_tensor", "(", "concept_ordinal", ",", "\"concept_ordinal\"", ")", "\n", "print_tensor", "(", "memory_score", ",", "\"memory_score\"", ")", "\n", "\n", "concept_ordinal", "=", "dynamic_expand", "(", "memory_score", ",", "concept_ordinal", ")", "# [bs,sq,1+cs]", "\n", "\n", "logger", ".", "info", "(", "\"concept_ordinal: {}\"", ".", "format", "(", "concept_ordinal", ".", "shape", ")", ")", "\n", "return", "concept_ordinal", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.MemoryLayer.add_sentinel": [[178, 213], ["paddle.layers.create_parameter", "layers.print_tensor", "paddle.layers.slice", "layers.print_tensor", "layers.dynamic_expand", "layers.print_tensor", "layers.print_tensor", "paddle.layers.concat", "paddle.layers.transpose", "logger.info", "paddle.layers.matmul", "paddle.layers.squeeze", "paddle.layers.transpose", "paddle.initializer.ConstantInitializer"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.dynamic_expand", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor"], ["", "def", "add_sentinel", "(", "self", ",", "expanded_bert", ",", "memory_embs", ",", "mem_emb_size", ")", ":", "\n", "        ", "\"\"\"\n\n        :param expanded_bert: [batch_size, seq_size, 1, mem_emb_size]\n        :param memory_embs: [batch_size, seq_size, concept_size, mem_emb_size]\n        :param mem_emb_size:\n        :return:\n        \"\"\"", "\n", "sentinel", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "\n", "name", "=", "'{}_memory_layer_sentinel'", ".", "format", "(", "self", ".", "prefix", ")", "if", "self", ".", "prefix", "else", "'memory_layer_sentinel'", ",", "\n", "dtype", "=", "\"float32\"", ",", "\n", "shape", "=", "[", "mem_emb_size", "]", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "ConstantInitializer", "(", "0", ")", ")", "# [mem_emb_size]", "\n", "print_tensor", "(", "sentinel", ",", "\"sentinel\"", ")", "\n", "\n", "memory_embs_squeeze", "=", "fluid", ".", "layers", ".", "slice", "(", "memory_embs", ",", "axes", "=", "[", "2", "]", ",", "starts", "=", "[", "0", "]", ",", "\n", "ends", "=", "[", "1", "]", ")", "# [bs,sq,1,ms]", "\n", "print_tensor", "(", "memory_embs_squeeze", ",", "\"memory_embs_squeeze\"", ")", "\n", "\n", "sentinel", "=", "dynamic_expand", "(", "memory_embs_squeeze", ",", "sentinel", ")", "# [bs,sq,1,ms]", "\n", "print_tensor", "(", "sentinel", ",", "\"sentinel\"", ")", "\n", "print_tensor", "(", "memory_embs", ",", "\"memory_embs\"", ")", "\n", "\n", "extended_memory", "=", "fluid", ".", "layers", ".", "concat", "(", "[", "sentinel", ",", "memory_embs", "]", ",", "\n", "axis", "=", "2", ")", "# [batch_size, seq_size, 1+concept_size, mem_emb_size]", "\n", "extended_memory", "=", "fluid", ".", "layers", ".", "transpose", "(", "extended_memory", ",", "perm", "=", "[", "0", ",", "1", ",", "3", ",", "2", "]", ")", "\n", "# [batch_size, seq_size, mem_emb_size, 1+concept_size]", "\n", "logger", ".", "info", "(", "\"extended_memory: {}\"", ".", "format", "(", "extended_memory", ".", "shape", ")", ")", "\n", "memory_score", "=", "fluid", ".", "layers", ".", "matmul", "(", "expanded_bert", ",", "\n", "extended_memory", ")", "# [batch_size, seq_size, 1, 1+concept_size]", "\n", "memory_score", "=", "fluid", ".", "layers", ".", "squeeze", "(", "memory_score", ",", "axes", "=", "[", "2", "]", ")", "\n", "# [batch_size, seq_size, 1+concept_size]", "\n", "extended_memory", "=", "fluid", ".", "layers", ".", "transpose", "(", "extended_memory", ",", "perm", "=", "[", "0", ",", "1", ",", "3", ",", "2", "]", ")", "\n", "# [batch_size, seq_size, 1+concept_size, mem_emb_size]", "\n", "return", "extended_memory", ",", "memory_score", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.TriLinearTwoTimeSelfAttentionLayer.__init__": [[216, 225], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "dropout_rate", "=", "0.0", ",", "\n", "cat_mul", "=", "False", ",", "cat_sub", "=", "False", ",", "cat_twotime", "=", "False", ",", "cat_twotime_mul", "=", "False", ",", "cat_twotime_sub", "=", "False", ")", ":", "\n", "        ", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "dropout_rate", "=", "dropout_rate", "\n", "self", ".", "cat_mul", "=", "cat_mul", "\n", "self", ".", "cat_sub", "=", "cat_sub", "\n", "self", ".", "cat_twotime", "=", "cat_twotime", "\n", "self", ".", "cat_twotime_mul", "=", "cat_twotime_mul", "\n", "self", ".", "cat_twotime_sub", "=", "cat_twotime_sub", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.TriLinearTwoTimeSelfAttentionLayer.forward": [[226, 327], ["paddle.layers.create_parameter", "paddle.layers.create_parameter", "paddle.layers.slice", "layers.print_tensor", "paddle.layers.transpose", "layers.print_tensor", "layers.print_tensor", "layers.dynamic_expand", "paddle.layers.transpose", "layers.print_tensor", "layers.print_tensor", "paddle.layers.matmul", "layers.print_tensor", "paddle.layers.create_parameter", "layers.dynamic_expand", "paddle.layers.transpose", "paddle.layers.matmul", "layers.print_tensor", "paddle.layers.create_parameter", "layers.dynamic_expand", "paddle.layers.elementwise_mul", "layers.print_tensor", "paddle.layers.matmul", "layers.print_tensor", "paddle.layers.squeeze", "layers.dynamic_expand", "paddle.layers.transpose", "paddle.layers.squeeze", "layers.dynamic_expand", "paddle.layers.transpose", "layers.dynamic_expand", "paddle.layers.sums", "layers.print_tensor", "paddle.layers.cast", "paddle.layers.elementwise_sub", "paddle.layers.scale", "paddle.layers.fill_constant", "logger.info", "logger.info", "paddle.layers.elementwise_mul", "paddle.layers.squeeze", "layers.dynamic_expand", "paddle.layers.transpose", "layers.print_tensor", "paddle.layers.elementwise_add", "layers.print_tensor", "paddle.layers.softmax", "paddle.layers.matmul", "any", "paddle.layers.concat", "layers.print_tensor", "paddle.layers.transpose", "paddle.layers.transpose", "paddle.layers.fill_constant", "paddle.layers.transpose", "paddle.layers.matmul", "paddle.layers.matmul", "out_tensors.append", "out_tensors.append", "out_tensors.append", "out_tensors.append", "out_tensors.append", "len", "len", "paddle.initializer.ConstantInitializer", "paddle.initializer.XavierInitializer", "paddle.initializer.XavierInitializer", "paddle.initializer.XavierInitializer", "paddle.layers.elementwise_mul", "paddle.layers.elementwise_sub", "paddle.layers.elementwise_mul", "paddle.layers.elementwise_sub"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.dynamic_expand", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.dynamic_expand", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.dynamic_expand", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.dynamic_expand", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.dynamic_expand", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.dynamic_expand", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.dynamic_expand", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor", "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor"], ["", "def", "forward", "(", "self", ",", "hidden_emb", ",", "sequence_mask", ")", ":", "\n", "        ", "\"\"\"\n        :param hidden_emb: [batch_size, seq_size, hidden_size]\n        :param sequence_mask: [batch_size, seq_size, 1]\n        :return:\n        \"\"\"", "\n", "assert", "len", "(", "hidden_emb", ".", "shape", ")", "==", "3", "and", "len", "(", "sequence_mask", ".", "shape", ")", "==", "3", "and", "sequence_mask", ".", "shape", "[", "-", "1", "]", "==", "1", "\n", "assert", "hidden_emb", ".", "shape", "[", ":", "2", "]", "==", "sequence_mask", ".", "shape", "[", ":", "2", "]", "\n", "\n", "hidden_size", "=", "self", ".", "hidden_size", "\n", "\n", "bias", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "name", "=", "'self_matching_layer_bias'", ",", "shape", "=", "[", "1", "]", ",", "dtype", "=", "\"float32\"", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "ConstantInitializer", "(", "0", ")", ")", "\n", "\n", "weight_1", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "name", "=", "'self_matching_layer_weight1'", ",", "shape", "=", "[", "hidden_size", "]", ",", "dtype", "=", "\"float32\"", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "XavierInitializer", "(", "uniform", "=", "True", ",", "fan_in", "=", "1", ",", "fan_out", "=", "hidden_size", ")", ")", "# [HS]", "\n", "bs_1_hs", "=", "fluid", ".", "layers", ".", "slice", "(", "hidden_emb", ",", "axes", "=", "[", "1", "]", ",", "starts", "=", "[", "0", "]", ",", "ends", "=", "[", "1", "]", ")", "# [bs, 1, hs]", "\n", "print_tensor", "(", "bs_1_hs", ",", "\"bs_1_hs\"", ")", "\n", "bs_hs_1", "=", "fluid", ".", "layers", ".", "transpose", "(", "bs_1_hs", ",", "perm", "=", "[", "0", ",", "2", ",", "1", "]", ")", "# [bs, hs, 1]", "\n", "print_tensor", "(", "bs_hs_1", ",", "\"bs_hs_1\"", ")", "\n", "print_tensor", "(", "weight_1", ",", "\"weight_1\"", ")", "\n", "weight_1", "=", "dynamic_expand", "(", "bs_1_hs", ",", "weight_1", ")", "# [BS, 1, HS] (a)jk", "\n", "weight_1", "=", "fluid", ".", "layers", ".", "transpose", "(", "weight_1", ",", "perm", "=", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "print_tensor", "(", "hidden_emb", ",", "\"hidden_emb\"", ")", "\n", "print_tensor", "(", "weight_1", ",", "\"weight_1\"", ")", "\n", "r1", "=", "fluid", ".", "layers", ".", "matmul", "(", "hidden_emb", ",", "weight_1", ")", "# [BS, SQ, 1]  aik", "\n", "print_tensor", "(", "r1", ",", "\"r1\"", ")", "\n", "\n", "weight_2", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "name", "=", "'self_matching_layer_weight2'", ",", "shape", "=", "[", "hidden_size", "]", ",", "dtype", "=", "\"float32\"", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "XavierInitializer", "(", "uniform", "=", "True", ",", "fan_in", "=", "1", ",", "fan_out", "=", "hidden_size", ")", ")", "# [HS]", "\n", "weight_2", "=", "dynamic_expand", "(", "bs_1_hs", ",", "weight_2", ")", "# # [BS, 1, HS] (a)jk", "\n", "hidden_emb_transpose", "=", "fluid", ".", "layers", ".", "transpose", "(", "hidden_emb", ",", "perm", "=", "[", "0", ",", "2", ",", "1", "]", ")", "# [BS, HS, SQ] aji", "\n", "r2", "=", "fluid", ".", "layers", ".", "matmul", "(", "weight_2", ",", "hidden_emb_transpose", ")", "# [BS, 1, SQ]  aki", "\n", "print_tensor", "(", "r2", ",", "\"r2\"", ")", "\n", "\n", "weight_mul", "=", "fluid", ".", "layers", ".", "create_parameter", "(", "name", "=", "'self_matching_layer_weightmul'", ",", "shape", "=", "[", "hidden_size", "]", ",", "dtype", "=", "\"float32\"", ",", "\n", "default_initializer", "=", "fluid", ".", "initializer", ".", "XavierInitializer", "(", "uniform", "=", "True", ")", ")", "# [HS]", "\n", "\n", "\n", "weight_mul", "=", "dynamic_expand", "(", "hidden_emb", ",", "weight_mul", ")", "\n", "rmul_1", "=", "fluid", ".", "layers", ".", "elementwise_mul", "(", "hidden_emb", ",", "weight_mul", ")", "# for \"hidden * self.weight_mul\". [bs, sq(i), hs(j)]", "\n", "print_tensor", "(", "rmul_1", ",", "\"rmul_1\"", ")", "\n", "rmul_2", "=", "fluid", ".", "layers", ".", "matmul", "(", "rmul_1", ",", "hidden_emb_transpose", ")", "# [bs, sq(i), hs(j)] mul [bs, hs(j), sq(k)] = [bs, sq(i), sq(k)]", "\n", "print_tensor", "(", "rmul_2", ",", "\"rmul_2\"", ")", "\n", "\n", "r1", "=", "fluid", ".", "layers", ".", "squeeze", "(", "r1", ",", "axes", "=", "[", "2", "]", ")", "# [BS, SQ]  aik", "\n", "r1", "=", "dynamic_expand", "(", "\n", "fluid", ".", "layers", ".", "transpose", "(", "rmul_2", ",", "[", "1", ",", "0", ",", "2", "]", ")", ",", "# [sq, bs, sq]", "\n", "r1", ")", "# [ SQ(from 1), bs, SQ]", "\n", "r1", "=", "fluid", ".", "layers", ".", "transpose", "(", "r1", ",", "[", "1", ",", "2", ",", "0", "]", ")", "# [bs, sq, sq(from 1)]", "\n", "\n", "r2", "=", "fluid", ".", "layers", ".", "squeeze", "(", "r2", ",", "axes", "=", "[", "1", "]", ")", "# [BS, SQ]  aik", "\n", "r2", "=", "dynamic_expand", "(", "\n", "fluid", ".", "layers", ".", "transpose", "(", "rmul_2", ",", "[", "1", ",", "0", ",", "2", "]", ")", ",", "# [sq, bs, sq]", "\n", "r2", ")", "# [ SQ(from 1), bs, SQ]", "\n", "r2", "=", "fluid", ".", "layers", ".", "transpose", "(", "r2", ",", "[", "1", ",", "0", ",", "2", "]", ")", "# [bs,sq(from 1),sq]", "\n", "\n", "bias", "=", "dynamic_expand", "(", "rmul_2", ",", "bias", ")", "# [BS, SQ, SQ]", "\n", "sim_score", "=", "fluid", ".", "layers", ".", "sums", "(", "input", "=", "[", "r1", ",", "r2", ",", "rmul_2", ",", "bias", "]", ")", "\n", "# [bs,sq,1]+[bs,1,sq]+[bs,sq,sq]+[bs,sq,sq]=[BS,SQ,SQ]", "\n", "print_tensor", "(", "sim_score", ",", "\"sim_score\"", ")", "\n", "\n", "sequence_mask", "=", "fluid", ".", "layers", ".", "cast", "(", "sequence_mask", ",", "dtype", "=", "\"float32\"", ")", "# [BS,SQ,1]", "\n", "softmax_mask", "=", "fluid", ".", "layers", ".", "elementwise_sub", "(", "\n", "sequence_mask", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "[", "1", "]", ",", "\"float32\"", ",", "1", ")", ")", "# [BS,SQ,1]", "\n", "softmax_mask", "=", "fluid", ".", "layers", ".", "scale", "(", "softmax_mask", ",", "-", "1", ")", "\n", "very_negative_number", "=", "fluid", ".", "layers", ".", "fill_constant", "(", "[", "1", "]", ",", "value", "=", "-", "1e6", ",", "dtype", "=", "\"float32\"", ")", "\n", "logger", ".", "info", "(", "\"softmax_mask: {}\"", ".", "format", "(", "softmax_mask", ".", "shape", ")", ")", "\n", "logger", ".", "info", "(", "\"very_negative_number: {}\"", ".", "format", "(", "very_negative_number", ".", "shape", ")", ")", "\n", "\n", "softmax_mask", "=", "fluid", ".", "layers", ".", "elementwise_mul", "(", "softmax_mask", ",", "very_negative_number", ")", "# [BS,SQ,1]", "\n", "\n", "softmax_mask", "=", "fluid", ".", "layers", ".", "squeeze", "(", "softmax_mask", ",", "axes", "=", "[", "2", "]", ")", "# [BS,SQ]", "\n", "softmax_mask", "=", "dynamic_expand", "(", "fluid", ".", "layers", ".", "transpose", "(", "sim_score", ",", "perm", "=", "[", "2", ",", "0", ",", "1", "]", ")", ",", "softmax_mask", ")", "# [sq(1),bs,sq]", "\n", "softmax_mask", "=", "fluid", ".", "layers", ".", "transpose", "(", "softmax_mask", ",", "perm", "=", "[", "1", ",", "0", ",", "2", "]", ")", "# [BS,sq(1),SQ]", "\n", "print_tensor", "(", "softmax_mask", ",", "\"softmax_mask\"", ")", "\n", "sim_score", "=", "fluid", ".", "layers", ".", "elementwise_add", "(", "sim_score", ",", "softmax_mask", ")", "# [bs,sq,sq]+[bs,sq(1),sq]=[BS,SQ,SQ]", "\n", "print_tensor", "(", "sim_score", ",", "\"sim_score\"", ")", "\n", "\n", "attn_prob", "=", "fluid", ".", "layers", ".", "softmax", "(", "sim_score", ")", "# [BS,SQ,SQ]", "\n", "weighted_sum", "=", "fluid", ".", "layers", ".", "matmul", "(", "attn_prob", ",", "hidden_emb", ")", "# [bs,sq,sq]*[bs,sq,hs]=[BS,SQ,HS]", "\n", "if", "any", "(", "[", "self", ".", "cat_twotime", ",", "self", ".", "cat_twotime_mul", ",", "self", ".", "cat_twotime_sub", "]", ")", ":", "\n", "            ", "twotime_att_prob", "=", "fluid", ".", "layers", ".", "matmul", "(", "attn_prob", ",", "attn_prob", ")", "# [bs,sq,sq]*[bs,sq,sq]=[BS,SQ,SQ]", "\n", "twotime_weited_sum", "=", "fluid", ".", "layers", ".", "matmul", "(", "twotime_att_prob", ",", "hidden_emb", ")", "# [BS,SQ,HS]", "\n", "\n", "", "out_tensors", "=", "[", "hidden_emb", ",", "weighted_sum", "]", "\n", "if", "self", ".", "cat_mul", ":", "\n", "            ", "out_tensors", ".", "append", "(", "fluid", ".", "layers", ".", "elementwise_mul", "(", "hidden_emb", ",", "weighted_sum", ")", ")", "\n", "", "if", "self", ".", "cat_sub", ":", "\n", "            ", "out_tensors", ".", "append", "(", "fluid", ".", "layers", ".", "elementwise_sub", "(", "hidden_emb", ",", "weighted_sum", ")", ")", "\n", "", "if", "self", ".", "cat_twotime", ":", "\n", "            ", "out_tensors", ".", "append", "(", "twotime_weited_sum", ")", "\n", "", "if", "self", ".", "cat_twotime_mul", ":", "\n", "            ", "out_tensors", ".", "append", "(", "fluid", ".", "layers", ".", "elementwise_mul", "(", "hidden_emb", ",", "twotime_weited_sum", ")", ")", "\n", "", "if", "self", ".", "cat_twotime_sub", ":", "\n", "            ", "out_tensors", ".", "append", "(", "fluid", ".", "layers", ".", "elementwise_sub", "(", "hidden_emb", ",", "twotime_weited_sum", ")", ")", "\n", "", "output", "=", "fluid", ".", "layers", ".", "concat", "(", "out_tensors", ",", "axis", "=", "2", ")", "# [BS,SQ, HS+HS+....]", "\n", "print_tensor", "(", "output", ",", "\"output\"", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.dynamic_expand": [[36, 52], ["paddle.layers.scale", "paddle.layers.elementwise_add", "len", "len", "type", "type", "len", "len"], "function", ["None"], ["def", "dynamic_expand", "(", "dynamic_tensor", ",", "smaller_tensor", ")", ":", "\n", "    ", "\"\"\"\n    :param dynamic_tensor:\n    :param smaller_tensor:\n    :return:\n    \"\"\"", "\n", "assert", "len", "(", "dynamic_tensor", ".", "shape", ")", ">", "len", "(", "smaller_tensor", ".", "shape", ")", "\n", "if", "type", "(", "smaller_tensor", ".", "shape", ")", "==", "list", ":", "\n", "        ", "for", "dim_idx", ",", "dim", "in", "smaller_tensor", ".", "shape", ":", "\n", "            ", "dynamic_tensor_dim_idx", "=", "len", "(", "dynamic_tensor", ")", "-", "len", "(", "smaller_tensor", ")", "+", "dim_idx", "\n", "assert", "dynamic_tensor", ".", "shape", "[", "dynamic_tensor_dim_idx", "]", "%", "dim", "==", "0", "\n", "", "", "elif", "type", "(", "smaller_tensor", ".", "shape", ")", "==", "int", ":", "\n", "        ", "assert", "dynamic_tensor", ".", "shape", "[", "-", "1", "]", "%", "smaller_tensor", ".", "shape", "==", "0", "\n", "", "memory_embs_zero", "=", "fluid", ".", "layers", ".", "scale", "(", "dynamic_tensor", ",", "scale", "=", "0.0", ")", "\n", "smaller_tensor", "=", "fluid", ".", "layers", ".", "elementwise_add", "(", "memory_embs_zero", ",", "smaller_tensor", ")", "\n", "return", "smaller_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.layers.print_tensor": [[54, 58], ["logger.info", "paddle.layers.Print"], "function", ["None"], ["", "def", "print_tensor", "(", "tensor", ",", "message", ",", "print_runtime", "=", "False", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\"{}: {}\"", ".", "format", "(", "message", ",", "tensor", ".", "shape", ")", ")", "\n", "if", "print_runtime", ":", "\n", "        ", "fluid", ".", "layers", ".", "Print", "(", "tensor", ",", "summarize", "=", "10", ",", "message", "=", "message", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig.__init__": [[28, 30], ["ernie.ErnieConfig._parse"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig._parse"], ["    ", "def", "__init__", "(", "self", ",", "config_path", ")", ":", "\n", "        ", "self", ".", "_config_dict", "=", "self", ".", "_parse", "(", "config_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig._parse": [[31, 40], ["open", "json.load", "IOError"], "methods", ["None"], ["", "def", "_parse", "(", "self", ",", "config_path", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "with", "open", "(", "config_path", ")", "as", "json_file", ":", "\n", "                ", "config_dict", "=", "json", ".", "load", "(", "json_file", ")", "\n", "", "", "except", "Exception", ":", "\n", "            ", "raise", "IOError", "(", "\"Error in parsing ernie model config file '%s'\"", "%", "\n", "config_path", ")", "\n", "", "else", ":", "\n", "            ", "return", "config_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig.__getitem__": [[41, 43], ["None"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "self", ".", "_config_dict", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig.print_config": [[44, 48], ["sorted", "print", "six.iteritems", "print"], "methods", ["None"], ["", "def", "print_config", "(", "self", ")", ":", "\n", "        ", "for", "arg", ",", "value", "in", "sorted", "(", "six", ".", "iteritems", "(", "self", ".", "_config_dict", ")", ")", ":", "\n", "            ", "print", "(", "'%s: %s'", "%", "(", "arg", ",", "value", ")", ")", "\n", "", "print", "(", "'------------------------------------------------'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.__init__": [[51, 82], ["paddle.initializer.TruncatedNormal", "ernie.ErnieModel._build_model"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel._build_model"], ["    ", "def", "__init__", "(", "self", ",", "\n", "src_ids", ",", "\n", "position_ids", ",", "\n", "sentence_ids", ",", "\n", "input_mask", ",", "\n", "config", ",", "\n", "weight_sharing", "=", "True", ",", "\n", "use_fp16", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "_emb_size", "=", "config", "[", "'hidden_size'", "]", "\n", "self", ".", "_n_layer", "=", "config", "[", "'num_hidden_layers'", "]", "\n", "self", ".", "_n_head", "=", "config", "[", "'num_attention_heads'", "]", "\n", "self", ".", "_voc_size", "=", "config", "[", "'vocab_size'", "]", "\n", "self", ".", "_max_position_seq_len", "=", "config", "[", "'max_position_embeddings'", "]", "\n", "self", ".", "_sent_types", "=", "config", "[", "'type_vocab_size'", "]", "\n", "self", ".", "_hidden_act", "=", "config", "[", "'hidden_act'", "]", "\n", "self", ".", "_prepostprocess_dropout", "=", "config", "[", "'hidden_dropout_prob'", "]", "\n", "self", ".", "_attention_dropout", "=", "config", "[", "'attention_probs_dropout_prob'", "]", "\n", "self", ".", "_weight_sharing", "=", "weight_sharing", "\n", "\n", "self", ".", "_word_emb_name", "=", "\"word_embedding\"", "\n", "self", ".", "_pos_emb_name", "=", "\"pos_embedding\"", "\n", "self", ".", "_sent_emb_name", "=", "\"sent_embedding\"", "\n", "self", ".", "_dtype", "=", "\"float16\"", "if", "use_fp16", "else", "\"float32\"", "\n", "\n", "# Initialize all weigths by truncated normal initializer, and all biases ", "\n", "# will be initialized by constant zero by default.", "\n", "self", ".", "_param_initializer", "=", "fluid", ".", "initializer", ".", "TruncatedNormal", "(", "\n", "scale", "=", "config", "[", "'initializer_range'", "]", ")", "\n", "\n", "self", ".", "_build_model", "(", "src_ids", ",", "position_ids", ",", "sentence_ids", ",", "input_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel._build_model": [[83, 140], ["paddle.embedding", "paddle.embedding", "paddle.embedding", "model.transformer_encoder.pre_process_layer", "paddle.layers.matmul", "paddle.layers.scale", "paddle.layers.stack", "model.transformer_encoder.encoder", "paddle.layers.cast", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.ParamAttr"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.encoder"], ["", "def", "_build_model", "(", "self", ",", "src_ids", ",", "position_ids", ",", "sentence_ids", ",", "input_mask", ")", ":", "\n", "# padding id in vocabulary must be set to 0", "\n", "        ", "emb_out", "=", "fluid", ".", "embedding", "(", "\n", "input", "=", "src_ids", ",", "\n", "size", "=", "[", "self", ".", "_voc_size", ",", "self", ".", "_emb_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "_word_emb_name", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "is_sparse", "=", "False", ")", "\n", "position_emb_out", "=", "fluid", ".", "embedding", "(", "\n", "input", "=", "position_ids", ",", "\n", "size", "=", "[", "self", ".", "_max_position_seq_len", ",", "self", ".", "_emb_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "_pos_emb_name", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ")", "\n", "\n", "sent_emb_out", "=", "fluid", ".", "embedding", "(", "\n", "sentence_ids", ",", "\n", "size", "=", "[", "self", ".", "_sent_types", ",", "self", ".", "_emb_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "self", ".", "_sent_emb_name", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ")", "\n", "\n", "emb_out", "=", "emb_out", "+", "position_emb_out", "\n", "emb_out", "=", "emb_out", "+", "sent_emb_out", "\n", "\n", "emb_out", "=", "pre_process_layer", "(", "\n", "emb_out", ",", "'nd'", ",", "self", ".", "_prepostprocess_dropout", ",", "name", "=", "'pre_encoder'", ")", "\n", "\n", "if", "self", ".", "_dtype", "==", "\"float16\"", ":", "\n", "            ", "input_mask", "=", "fluid", ".", "layers", ".", "cast", "(", "x", "=", "input_mask", ",", "dtype", "=", "self", ".", "_dtype", ")", "\n", "\n", "", "self_attn_mask", "=", "fluid", ".", "layers", ".", "matmul", "(", "\n", "x", "=", "input_mask", ",", "y", "=", "input_mask", ",", "transpose_y", "=", "True", ")", "\n", "self_attn_mask", "=", "fluid", ".", "layers", ".", "scale", "(", "\n", "x", "=", "self_attn_mask", ",", "scale", "=", "10000.0", ",", "bias", "=", "-", "1.0", ",", "bias_after_scale", "=", "False", ")", "\n", "n_head_self_attn_mask", "=", "fluid", ".", "layers", ".", "stack", "(", "\n", "x", "=", "[", "self_attn_mask", "]", "*", "self", ".", "_n_head", ",", "axis", "=", "1", ")", "\n", "n_head_self_attn_mask", ".", "stop_gradient", "=", "True", "\n", "\n", "self", ".", "_enc_out", "=", "encoder", "(", "\n", "enc_input", "=", "emb_out", ",", "\n", "attn_bias", "=", "n_head_self_attn_mask", ",", "\n", "n_layer", "=", "self", ".", "_n_layer", ",", "\n", "n_head", "=", "self", ".", "_n_head", ",", "\n", "d_key", "=", "self", ".", "_emb_size", "//", "self", ".", "_n_head", ",", "\n", "d_value", "=", "self", ".", "_emb_size", "//", "self", ".", "_n_head", ",", "\n", "d_model", "=", "self", ".", "_emb_size", ",", "\n", "d_inner_hid", "=", "self", ".", "_emb_size", "*", "4", ",", "\n", "prepostprocess_dropout", "=", "self", ".", "_prepostprocess_dropout", ",", "\n", "attention_dropout", "=", "self", ".", "_attention_dropout", ",", "\n", "relu_dropout", "=", "0", ",", "\n", "hidden_act", "=", "self", ".", "_hidden_act", ",", "\n", "preprocess_cmd", "=", "\"\"", ",", "\n", "postprocess_cmd", "=", "\"dan\"", ",", "\n", "param_initializer", "=", "self", ".", "_param_initializer", ",", "\n", "name", "=", "'encoder'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.get_sequence_output": [[141, 143], ["None"], "methods", ["None"], ["", "def", "get_sequence_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_enc_out", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.get_pooled_output": [[144, 158], ["paddle.layers.slice", "paddle.layers.fc", "paddle.ParamAttr"], "methods", ["None"], ["", "def", "get_pooled_output", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the first feature of each sequence for classification\"\"\"", "\n", "\n", "next_sent_feat", "=", "fluid", ".", "layers", ".", "slice", "(", "\n", "input", "=", "self", ".", "_enc_out", ",", "axes", "=", "[", "1", "]", ",", "starts", "=", "[", "0", "]", ",", "ends", "=", "[", "1", "]", ")", "\n", "next_sent_feat", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "next_sent_feat", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "size", "=", "self", ".", "_emb_size", ",", "\n", "act", "=", "\"tanh\"", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"pooled_fc.w_0\"", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "\"pooled_fc.b_0\"", ")", "\n", "return", "next_sent_feat", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.get_pretraining_output": [[159, 231], ["paddle.layers.cast", "ernie.ErnieModel.get_pooled_output", "paddle.layers.reshape", "paddle.layers.gather", "paddle.layers.fc", "model.transformer_encoder.pre_process_layer", "paddle.ParamAttr", "paddle.layers.softmax_with_cross_entropy", "paddle.layers.mean", "paddle.layers.fc", "paddle.layers.reshape", "paddle.layers.softmax_with_cross_entropy", "paddle.layers.accuracy", "paddle.layers.mean", "paddle.layers.matmul", "paddle.layers.create_parameter", "paddle.layers.fc", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.initializer.Constant", "paddle.ParamAttr", "paddle.default_main_program().global_block().var", "paddle.ParamAttr", "paddle.default_main_program().global_block", "paddle.default_main_program"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.get_pooled_output"], ["", "def", "get_pretraining_output", "(", "self", ",", "mask_label", ",", "mask_pos", ",", "labels", ")", ":", "\n", "        ", "\"\"\"Get the loss & accuracy for pretraining\"\"\"", "\n", "\n", "mask_pos", "=", "fluid", ".", "layers", ".", "cast", "(", "x", "=", "mask_pos", ",", "dtype", "=", "'int32'", ")", "\n", "\n", "# extract the first token feature in each sentence", "\n", "next_sent_feat", "=", "self", ".", "get_pooled_output", "(", ")", "\n", "reshaped_emb_out", "=", "fluid", ".", "layers", ".", "reshape", "(", "\n", "x", "=", "self", ".", "_enc_out", ",", "shape", "=", "[", "-", "1", ",", "self", ".", "_emb_size", "]", ")", "\n", "# extract masked tokens' feature", "\n", "mask_feat", "=", "fluid", ".", "layers", ".", "gather", "(", "input", "=", "reshaped_emb_out", ",", "index", "=", "mask_pos", ")", "\n", "\n", "# transform: fc", "\n", "mask_trans_feat", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "mask_feat", ",", "\n", "size", "=", "self", ".", "_emb_size", ",", "\n", "act", "=", "self", ".", "_hidden_act", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "'mask_lm_trans_fc.w_0'", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "'mask_lm_trans_fc.b_0'", ")", ")", "\n", "# transform: layer norm ", "\n", "mask_trans_feat", "=", "pre_process_layer", "(", "\n", "mask_trans_feat", ",", "'n'", ",", "name", "=", "'mask_lm_trans'", ")", "\n", "\n", "mask_lm_out_bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"mask_lm_out_fc.b_0\"", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "value", "=", "0.0", ")", ")", "\n", "if", "self", ".", "_weight_sharing", ":", "\n", "            ", "fc_out", "=", "fluid", ".", "layers", ".", "matmul", "(", "\n", "x", "=", "mask_trans_feat", ",", "\n", "y", "=", "fluid", ".", "default_main_program", "(", ")", ".", "global_block", "(", ")", ".", "var", "(", "\n", "self", ".", "_word_emb_name", ")", ",", "\n", "transpose_y", "=", "True", ")", "\n", "fc_out", "+=", "fluid", ".", "layers", ".", "create_parameter", "(", "\n", "shape", "=", "[", "self", ".", "_voc_size", "]", ",", "\n", "dtype", "=", "self", ".", "_dtype", ",", "\n", "attr", "=", "mask_lm_out_bias_attr", ",", "\n", "is_bias", "=", "True", ")", "\n", "\n", "", "else", ":", "\n", "            ", "fc_out", "=", "fluid", ".", "layers", ".", "fc", "(", "input", "=", "mask_trans_feat", ",", "\n", "size", "=", "self", ".", "_voc_size", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"mask_lm_out_fc.w_0\"", ",", "\n", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "mask_lm_out_bias_attr", ")", "\n", "\n", "", "mask_lm_loss", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "fc_out", ",", "label", "=", "mask_label", ")", "\n", "mean_mask_lm_loss", "=", "fluid", ".", "layers", ".", "mean", "(", "mask_lm_loss", ")", "\n", "\n", "next_sent_fc_out", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "next_sent_feat", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "size", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"next_sent_fc.w_0\"", ",", "initializer", "=", "self", ".", "_param_initializer", ")", ",", "\n", "bias_attr", "=", "\"next_sent_fc.b_0\"", ")", "\n", "\n", "next_sent_fc_out", "=", "fluid", ".", "layers", ".", "reshape", "(", "\n", "next_sent_fc_out", ",", "[", "-", "1", ",", "2", "]", ",", "inplace", "=", "True", ")", "\n", "next_sent_loss", ",", "next_sent_softmax", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "next_sent_fc_out", ",", "label", "=", "labels", ",", "return_softmax", "=", "True", ")", "\n", "\n", "next_sent_acc", "=", "fluid", ".", "layers", ".", "accuracy", "(", "\n", "input", "=", "next_sent_softmax", ",", "label", "=", "labels", ")", "\n", "\n", "mean_next_sent_loss", "=", "fluid", ".", "layers", ".", "mean", "(", "next_sent_loss", ")", "\n", "\n", "loss", "=", "mean_next_sent_loss", "+", "mean_mask_lm_loss", "\n", "return", "next_sent_acc", ",", "mean_mask_lm_loss", ",", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.start_service.mrqa_service": [[31, 36], ["app.route", "server"], "function", ["None"], ["app", ".", "logger", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "model", "=", "model_wrapper", ".", "BertModelWrapper", "(", "model_dir", "=", "model_dir", ")", "\n", "server", "=", "mrc_service", ".", "MRQAService", "(", "'MRQA service'", ",", "app", ".", "logger", ")", "\n", "\n", "@", "app", ".", "route", "(", "'/'", ",", "methods", "=", "[", "'POST'", "]", ")", "\n", "def", "mrqa_service", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service.BasicMRCService.__init__": [[53, 61], ["logging.getLogger"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "name", ",", "logger", "=", "None", ",", "log_data", "=", "False", ")", ":", "\n", "        ", "\"\"\" \"\"\"", "\n", "self", ".", "name", "=", "name", "\n", "if", "logger", "is", "None", ":", "\n", "            ", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "'flask'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "logger", "=", "logger", "\n", "", "self", ".", "log_data", "=", "log_data", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service.BasicMRCService.__call__": [[62, 173], ["flask.request.get_json", "mrc_service.BasicMRCService._response_constructor", "time.time", "mrc_service._request_check", "mrc_service._abort", "mrc_service._split_input_json", "zip", "model.postprocessor", "time.time", "time.time", "mrc_service._timmer", "mrc_service.BasicMRCService.logger.error", "mrc_service.BasicMRCService.logger.exception", "mrc_service._abort", "time.time", "model.preprocessor", "len", "len", "processed.append", "time.time", "mrc_service._timmer", "mrc_service.BasicMRCService.logger.error", "mrc_service.BasicMRCService.logger.exception", "mrc_service._abort", "time.time", "mrc_service.BasicMRCService.examples.extend", "mrc_service.BasicMRCService.features.extend", "time.time", "mrc_service._timmer", "mrc_service.BasicMRCService.logger.error", "mrc_service.BasicMRCService.logger.exception", "mrc_service._abort", "time.time", "time.time", "mrc_service._timmer", "mrc_service.BasicMRCService.logger.error", "mrc_service.BasicMRCService.logger.exception", "mrc_service._abort", "logging.info", "mrc_service.BasicMRCService.logger.info", "mrc_service.BasicMRCService.mrc_results.extend", "len", "print", "NotImplementedError", "json.dumps", "json.dumps", "model.call_mrc", "mrc_service.BasicMRCService.mrc_results.extend", "model.call_mrc"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service.BasicMRCService._response_constructor", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._request_check", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._split_input_json", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.postprocessor", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.preprocessor", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.call_mrc", "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.call_mrc"], ["", "def", "__call__", "(", "self", ",", "model", ",", "process_mode", "=", "'serial'", ",", "max_batch_size", "=", "5", ",", "timmer", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            mode: serial, parallel\n        \"\"\"", "\n", "if", "timmer", ":", "\n", "            ", "start", "=", "time", ".", "time", "(", ")", "\n", "", "\"\"\"Call mrc model wrapper and handle expectations\"\"\"", "\n", "self", ".", "input_json", "=", "request", ".", "get_json", "(", "silent", "=", "True", ")", "\n", "try", ":", "\n", "            ", "if", "timmer", ":", "\n", "                ", "start_request_check", "=", "time", ".", "time", "(", ")", "\n", "", "request_status", "=", "_request_check", "(", "self", ".", "input_json", ")", "\n", "if", "timmer", ":", "\n", "                ", "current_time", "=", "time", ".", "time", "(", ")", "\n", "_timmer", "(", "start", ",", "start_request_check", ",", "current_time", ",", "'request check'", ")", "\n", "", "if", "self", ".", "log_data", ":", "\n", "                ", "if", "self", ".", "logger", "is", "None", ":", "\n", "                    ", "logging", ".", "info", "(", "\n", "'Client input - {}'", ".", "format", "(", "json", ".", "dumps", "(", "self", ".", "input_json", ",", "ensure_ascii", "=", "False", ")", ")", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "logger", ".", "info", "(", "\n", "'Client input - {}'", ".", "format", "(", "json", ".", "dumps", "(", "self", ".", "input_json", ",", "ensure_ascii", "=", "False", ")", ")", "\n", ")", "\n", "", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "error", "(", "'server request checker error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'server request checker error - {}'", ".", "format", "(", "e", ")", ")", "\n", "", "if", "request_status", "!=", "'OK'", ":", "\n", "            ", "return", "_abort", "(", "400", ",", "request_status", ")", "\n", "\n", "# call preprocessor", "\n", "", "try", ":", "\n", "            ", "if", "timmer", ":", "\n", "                ", "start_preprocess", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "jsons", "=", "_split_input_json", "(", "self", ".", "input_json", ")", "\n", "processed", "=", "[", "]", "\n", "ex_start_idx", "=", "0", "\n", "feat_start_idx", "=", "1000000000", "\n", "for", "i", "in", "jsons", ":", "\n", "                ", "e", ",", "f", ",", "b", "=", "model", ".", "preprocessor", "(", "i", ",", "batch_size", "=", "max_batch_size", "if", "process_mode", "==", "'parallel'", "else", "1", ",", "examples_start_id", "=", "ex_start_idx", ",", "features_start_id", "=", "feat_start_idx", ")", "\n", "ex_start_idx", "+=", "len", "(", "e", ")", "\n", "feat_start_idx", "+=", "len", "(", "f", ")", "\n", "processed", ".", "append", "(", "[", "e", ",", "f", ",", "b", "]", ")", "\n", "\n", "", "if", "timmer", ":", "\n", "                ", "current_time", "=", "time", ".", "time", "(", ")", "\n", "_timmer", "(", "start", ",", "start_preprocess", ",", "current_time", ",", "'preprocess'", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "error", "(", "'preprocessor error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'preprocessor error - {}'", ".", "format", "(", "e", ")", ")", "\n", "\n", "", "def", "transpose", "(", "mat", ")", ":", "\n", "            ", "return", "zip", "(", "*", "mat", ")", "\n", "\n", "# call mrc", "\n", "", "try", ":", "\n", "            ", "if", "timmer", ":", "\n", "                ", "start_call_mrc", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "self", ".", "mrc_results", "=", "[", "]", "\n", "self", ".", "examples", "=", "[", "]", "\n", "self", ".", "features", "=", "[", "]", "\n", "for", "e", ",", "f", ",", "batches", "in", "processed", ":", "\n", "                ", "if", "verbose", ":", "\n", "                    ", "if", "len", "(", "f", ")", ">", "max_batch_size", ":", "\n", "                        ", "print", "(", "\"get a too long example....\"", ")", "\n", "", "", "if", "process_mode", "==", "'serial'", ":", "\n", "                    ", "self", ".", "mrc_results", ".", "extend", "(", "[", "model", ".", "call_mrc", "(", "b", ",", "squeeze_dim0", "=", "True", ")", "for", "b", "in", "batches", "[", ":", "max_batch_size", "]", "]", ")", "\n", "", "elif", "process_mode", "==", "'parallel'", ":", "\n", "# only keep first max_batch_size features", "\n", "# batches = batches[0]", "\n", "\n", "                    ", "for", "b", "in", "batches", ":", "\n", "                        ", "self", ".", "mrc_results", ".", "extend", "(", "model", ".", "call_mrc", "(", "b", ",", "return_list", "=", "True", ")", ")", "\n", "", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "(", ")", "\n", "", "self", ".", "examples", ".", "extend", "(", "e", ")", "\n", "# self.features.extend(f[:max_batch_size])", "\n", "self", ".", "features", ".", "extend", "(", "f", ")", "\n", "\n", "", "if", "timmer", ":", "\n", "                ", "current_time", "=", "time", ".", "time", "(", ")", "\n", "_timmer", "(", "start", ",", "start_call_mrc", ",", "current_time", ",", "'call mrc'", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "error", "(", "'call_mrc error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'call_mrc error - {}'", ".", "format", "(", "e", ")", ")", "\n", "\n", "# call post processor", "\n", "", "try", ":", "\n", "            ", "if", "timmer", ":", "\n", "                ", "start_post_precess", "=", "time", ".", "time", "(", ")", "\n", "", "self", ".", "results", "=", "model", ".", "postprocessor", "(", "self", ".", "examples", ",", "self", ".", "features", ",", "self", ".", "mrc_results", ")", "\n", "\n", "# only nbest results is POSTed back", "\n", "self", ".", "results", "=", "self", ".", "results", "[", "1", "]", "\n", "# self.results = self.results[0]", "\n", "\n", "if", "timmer", ":", "\n", "                ", "current_time", "=", "time", ".", "time", "(", ")", "\n", "_timmer", "(", "start", ",", "start_post_precess", ",", "current_time", ",", "'post process'", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "error", "(", "'postprocessor error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'postprocessor error - {}'", ".", "format", "(", "e", ")", ")", "\n", "\n", "", "return", "self", ".", "_response_constructor", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service.BasicMRCService._response_constructor": [[174, 190], ["flask.Response", "mrc_service.BasicMRCService.logger.info", "json.dumps", "mrc_service.BasicMRCService.logger.error", "mrc_service.BasicMRCService.logger.exception", "mrc_service._abort", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort"], ["", "def", "_response_constructor", "(", "self", ")", ":", "\n", "        ", "\"\"\"construct http response object\"\"\"", "\n", "try", ":", "\n", "            ", "response", "=", "{", "\n", "# 'requestID': self.input_json['requestID'],", "\n", "'results'", ":", "self", ".", "results", "\n", "}", "\n", "if", "self", ".", "log_data", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "\n", "'Response - {}'", ".", "format", "(", "json", ".", "dumps", "(", "response", ",", "ensure_ascii", "=", "False", ")", ")", "\n", ")", "\n", "", "return", "Response", "(", "json", ".", "dumps", "(", "response", ")", ",", "mimetype", "=", "'application/json'", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "logger", ".", "error", "(", "'response constructor error'", ")", "\n", "self", ".", "logger", ".", "exception", "(", "e", ")", "\n", "return", "_abort", "(", "500", ",", "'response constructor error - {}'", ".", "format", "(", "e", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._request_check": [[15, 27], ["isinstance"], "function", ["None"], ["def", "_request_check", "(", "input_json", ")", ":", "\n", "    ", "\"\"\"Check if the request json is valid\"\"\"", "\n", "if", "input_json", "is", "None", "or", "not", "isinstance", "(", "input_json", ",", "dict", ")", ":", "\n", "        ", "return", "'Can not parse the input json data - {}'", ".", "format", "(", "input_json", ")", "\n", "", "try", ":", "\n", "        ", "c", "=", "input_json", "[", "'context'", "]", "\n", "qa", "=", "input_json", "[", "'qas'", "]", "[", "0", "]", "\n", "qid", "=", "qa", "[", "'qid'", "]", "\n", "q", "=", "qa", "[", "'question'", "]", "\n", "", "except", "KeyError", "as", "e", ":", "\n", "        ", "return", "'Invalid request, key \"{}\" not found'", ".", "format", "(", "e", ")", "\n", "", "return", "'OK'", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._abort": [[28, 31], ["flask.Response", "json.dumps"], "function", ["None"], ["", "def", "_abort", "(", "status_code", ",", "message", ")", ":", "\n", "    ", "\"\"\"Create custom error message and status code\"\"\"", "\n", "return", "Response", "(", "json", ".", "dumps", "(", "message", ")", ",", "status", "=", "status_code", ",", "mimetype", "=", "'application/json'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._timmer": [[32, 37], ["print"], "function", ["None"], ["", "def", "_timmer", "(", "init_start", ",", "start", ",", "current", ",", "process_name", ")", ":", "\n", "    ", "cumulated_elapsed_time", "=", "(", "current", "-", "init_start", ")", "*", "1000", "\n", "current_elapsed_time", "=", "(", "current", "-", "start", ")", "*", "1000", "\n", "print", "(", "'{}\\t-\\t{:.2f}\\t{:.2f}'", ".", "format", "(", "process_name", ",", "cumulated_elapsed_time", ",", "\n", "current_elapsed_time", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.mrc_service._split_input_json": [[38, 50], ["len", "len", "range", "len", "copy.deepcopy", "rets.append"], "function", ["None"], ["", "def", "_split_input_json", "(", "input_json", ")", ":", "\n", "    ", "if", "len", "(", "input_json", "[", "'context_tokens'", "]", ")", ">", "810", ":", "\n", "        ", "input_json", "[", "'context'", "]", "=", "input_json", "[", "'context'", "]", "[", ":", "5000", "]", "\n", "", "if", "len", "(", "input_json", "[", "'qas'", "]", ")", "==", "1", ":", "\n", "        ", "return", "[", "input_json", "]", "\n", "", "else", ":", "\n", "        ", "rets", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "input_json", "[", "'qas'", "]", ")", ")", ":", "\n", "            ", "temp", "=", "deepcopy", "(", "input_json", ")", "\n", "temp", "[", "'qas'", "]", "=", "[", "input_json", "[", "'qas'", "]", "[", "i", "]", "]", "\n", "rets", ".", "append", "(", "temp", ")", "\n", "", "return", "rets", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.__init__": [[37, 57], ["paddle.Executor", "task_reader.mrqa_infer.DataProcessor", "paddle.io.load_inference_model", "paddle.CUDAPlace", "paddle.core.get_cuda_device_count", "paddle.CPUPlace", "int", "os.environ.get", "os.path.join", "multiprocessing.cpu_count"], "methods", ["None"], ["dev_count", "=", "fluid", ".", "core", ".", "get_cuda_device_count", "(", ")", "\n", "", "else", ":", "\n", "            ", "place", "=", "fluid", ".", "CPUPlace", "(", ")", "\n", "dev_count", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'CPU_NUM'", ",", "multiprocessing", ".", "cpu_count", "(", ")", ")", ")", "\n", "", "self", ".", "exe", "=", "fluid", ".", "Executor", "(", "place", ")", "\n", "\n", "self", ".", "bert_preprocessor", "=", "DataProcessor", "(", "\n", "vocab_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "'vocab.txt'", ")", ",", "\n", "do_lower_case", "=", "do_lower_case", ",", "\n", "max_seq_length", "=", "max_seq_len", ",", "\n", "in_tokens", "=", "in_tokens", ",", "\n", "doc_stride", "=", "doc_stride", ",", "\n", "max_query_length", "=", "max_query_length", ")", "\n", "\n", "self", ".", "inference_program", ",", "self", ".", "feed_target_names", ",", "self", ".", "fetch_targets", "=", "fluid", ".", "io", ".", "load_inference_model", "(", "dirname", "=", "model_dir", ",", "executor", "=", "self", ".", "exe", ")", "\n", "\n", "", "def", "preprocessor", "(", "self", ",", "samples", ",", "batch_size", ",", "examples_start_id", ",", "features_start_id", ")", ":", "\n", "        ", "\"\"\"Preprocess the input samples, including word seg, padding, token to ids\"\"\"", "\n", "# Tokenization and paragraph padding", "\n", "examples", ",", "features", ",", "batch", "=", "self", ".", "bert_preprocessor", ".", "data_generator", "(", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.preprocessor": [[58, 65], ["model_wrapper.ERNIEModelWrapper.bert_preprocessor.data_generator"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator"], ["samples", ",", "batch_size", ",", "max_len", "=", "max_seq_len", ",", "examples_start_id", "=", "examples_start_id", ",", "features_start_id", "=", "features_start_id", ")", "\n", "self", ".", "samples", "=", "samples", "\n", "return", "examples", ",", "features", ",", "batch", "\n", "\n", "", "def", "call_mrc", "(", "self", ",", "batch", ",", "squeeze_dim0", "=", "False", ",", "return_list", "=", "False", ")", ":", "\n", "        ", "\"\"\"MRC\"\"\"", "\n", "if", "squeeze_dim0", "and", "return_list", ":", "\n", "            ", "raise", "ValueError", "(", "\"squeeze_dim0 only work for dict-type return value.\"", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.call_mrc": [[66, 101], ["model_wrapper.ERNIEModelWrapper.exe.run", "ValueError", "len", "zip"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["", "src_ids", "=", "batch", "[", "0", "]", "\n", "pos_ids", "=", "batch", "[", "1", "]", "\n", "sent_ids", "=", "batch", "[", "2", "]", "\n", "input_mask", "=", "batch", "[", "3", "]", "\n", "unique_id", "=", "batch", "[", "4", "]", "\n", "feed_dict", "=", "{", "\n", "self", ".", "feed_target_names", "[", "0", "]", ":", "src_ids", ",", "\n", "self", ".", "feed_target_names", "[", "1", "]", ":", "pos_ids", ",", "\n", "self", ".", "feed_target_names", "[", "2", "]", ":", "sent_ids", ",", "\n", "self", ".", "feed_target_names", "[", "3", "]", ":", "input_mask", ",", "\n", "self", ".", "feed_target_names", "[", "4", "]", ":", "unique_id", "\n", "}", "\n", "\n", "np_unique_ids", ",", "np_start_logits", ",", "np_end_logits", ",", "np_num_seqs", "=", "self", ".", "exe", ".", "run", "(", "self", ".", "inference_program", ",", "feed", "=", "feed_dict", ",", "fetch_list", "=", "self", ".", "fetch_targets", ")", "\n", "\n", "if", "len", "(", "np_unique_ids", ")", "==", "1", "and", "squeeze_dim0", ":", "\n", "            ", "np_unique_ids", "=", "np_unique_ids", "[", "0", "]", "\n", "np_start_logits", "=", "np_start_logits", "[", "0", "]", "\n", "np_end_logits", "=", "np_end_logits", "[", "0", "]", "\n", "\n", "", "if", "return_list", ":", "\n", "            ", "mrc_results", "=", "[", "{", "'unique_ids'", ":", "id", ",", "'start_logits'", ":", "st", ",", "'end_logits'", ":", "end", "}", "\n", "for", "id", ",", "st", ",", "end", "in", "zip", "(", "np_unique_ids", ",", "np_start_logits", ",", "np_end_logits", ")", "]", "\n", "", "else", ":", "\n", "            ", "mrc_results", "=", "{", "\n", "'unique_ids'", ":", "np_unique_ids", ",", "\n", "'start_logits'", ":", "np_start_logits", ",", "\n", "'end_logits'", ":", "np_end_logits", ",", "\n", "}", "\n", "", "return", "mrc_results", "\n", "\n", "", "def", "postprocessor", "(", "self", ",", "examples", ",", "features", ",", "mrc_results", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.ernie_server.model_wrapper.ERNIEModelWrapper.postprocessor": [[102, 136], ["collections.namedtuple", "isinstance", "task_reader.mrqa_infer.get_answers", "isinstance", "range", "results.append", "int", "results.append", "float", "float", "collections.namedtuple.", "float", "float", "collections.namedtuple."], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.xlnet_server.squad_reader.get_answers"], ["\n", "RawResult", "=", "collections", ".", "namedtuple", "(", "\"RawResult\"", ",", "\n", "[", "\"unique_id\"", ",", "\"start_logits\"", ",", "\"end_logits\"", "]", ")", "\n", "results", "=", "[", "]", "\n", "if", "isinstance", "(", "mrc_results", ",", "list", ")", ":", "\n", "            ", "for", "res", "in", "mrc_results", ":", "\n", "                ", "unique_id", "=", "res", "[", "'unique_ids'", "]", "[", "0", "]", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "res", "[", "'start_logits'", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "res", "[", "'end_logits'", "]", ".", "flat", "]", "\n", "results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "assert", "isinstance", "(", "mrc_results", ",", "dict", ")", "\n", "for", "idx", "in", "range", "(", "mrc_results", "[", "'unique_ids'", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "unique_id", "=", "int", "(", "mrc_results", "[", "'unique_ids'", "]", "[", "idx", "]", ")", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "mrc_results", "[", "'start_logits'", "]", "[", "idx", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "mrc_results", "[", "'end_logits'", "]", "[", "idx", "]", ".", "flat", "]", "\n", "results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "\n", "", "", "answers", "=", "get_answers", "(", "\n", "examples", ",", "features", ",", "results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "do_lower_case", ",", "verbose", ")", "\n", "return", "answers", "\n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.convert_mrqa2squad.reader": [[27, 39], ["open", "enumerate", "json.loads", "line.strip"], "function", ["None"], ["def", "reader", "(", "filename", ")", ":", "\n", "    ", "\"\"\"\n    This function read a MRQA data file.\n    :param filename: name of a MRQA data file.\n    :return: original samples of a MRQA data file.\n    \"\"\"", "\n", "with", "open", "(", "filename", ")", "as", "fin", ":", "\n", "        ", "for", "lidx", ",", "line", "in", "enumerate", "(", "fin", ")", ":", "\n", "            ", "if", "lidx", "==", "0", ":", "\n", "                ", "continue", "\n", "", "sample", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "yield", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.convert_mrqa2squad.to_squad_para_train": [[41, 95], ["dict", "re.sub", "re.sub.find", "qas.append", "len", "len", "answer[].lower", "[].lower", "print", "print"], "function", ["None"], ["", "", "", "def", "to_squad_para_train", "(", "sample", ")", ":", "\n", "    ", "\"\"\"\n    This function convert training data from MRQA format to SQuAD format.\n    :param sample: one sample in MRQA format.\n    :return: paragraphs in SQuAD format.\n    \"\"\"", "\n", "squad_para", "=", "dict", "(", ")", "\n", "context", "=", "sample", "[", "'context'", "]", "\n", "context", "=", "re", ".", "sub", "(", "r'\\[TLE\\]|\\[DOC\\]|\\[PAR\\]'", ",", "'[SEP]'", ",", "context", ")", "\n", "# replace special tokens to [SEP] to avoid UNK in BERT", "\n", "squad_para", "[", "'context'", "]", "=", "context", "\n", "qas", "=", "[", "]", "\n", "for", "qa", "in", "sample", "[", "'qas'", "]", ":", "\n", "        ", "text", "=", "qa", "[", "'detected_answers'", "]", "[", "0", "]", "[", "'text'", "]", "\n", "new_start", "=", "context", ".", "find", "(", "text", ")", "\n", "# Try to find an exact match (without normalization) of the reference answer.", "\n", "# Some articles like {a|an|the} my get lost in the original spans.", "\n", "# E.g. the reference answer is \"The London Eye\",", "\n", "# while the original span may only contain \"London Eye\" due to normalization.", "\n", "new_end", "=", "new_start", "+", "len", "(", "text", ")", "-", "1", "\n", "org_start", "=", "qa", "[", "'detected_answers'", "]", "[", "0", "]", "[", "'char_spans'", "]", "[", "0", "]", "[", "0", "]", "\n", "org_end", "=", "qa", "[", "'detected_answers'", "]", "[", "0", "]", "[", "'char_spans'", "]", "[", "0", "]", "[", "1", "]", "\n", "if", "new_start", "==", "-", "1", "or", "len", "(", "text", ")", "<", "8", ":", "\n", "# If no exact match (without normalization) can be found or reference answer is too short", "\n", "# (e.g. only contain a character \"c\", which will cause problems using find),", "\n", "# use the original span in MRQA dataset.", "\n", "            ", "answer", "=", "{", "\n", "'text'", ":", "squad_para", "[", "'context'", "]", "[", "org_start", ":", "org_end", "+", "1", "]", ",", "\n", "'answer_start'", ":", "org_start", "\n", "}", "\n", "answer_start", "=", "org_start", "\n", "answer_end", "=", "org_end", "\n", "", "else", ":", "\n", "            ", "answer", "=", "{", "\n", "'text'", ":", "text", ",", "\n", "'answer_start'", ":", "new_start", "\n", "}", "\n", "answer_start", "=", "new_start", "\n", "answer_end", "=", "new_end", "\n", "# A sanity check", "\n", "", "try", ":", "\n", "            ", "assert", "answer", "[", "'text'", "]", ".", "lower", "(", ")", "==", "squad_para", "[", "'context'", "]", "[", "answer_start", ":", "answer_end", "+", "1", "]", ".", "lower", "(", ")", "\n", "", "except", "AssertionError", ":", "\n", "            ", "print", "(", "answer", "[", "'text'", "]", ")", "\n", "print", "(", "squad_para", "[", "'context'", "]", "[", "answer_start", ":", "answer_end", "+", "1", "]", ")", "\n", "continue", "\n", "", "squad_qa", "=", "{", "\n", "'question'", ":", "qa", "[", "'question'", "]", ",", "\n", "'id'", ":", "qa", "[", "'qid'", "]", ",", "\n", "'answers'", ":", "[", "answer", "]", "\n", "}", "\n", "qas", ".", "append", "(", "squad_qa", ")", "\n", "", "squad_para", "[", "'qas'", "]", "=", "qas", "\n", "return", "squad_para", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.convert_mrqa2squad.to_squad_para_dev": [[97, 126], ["dict", "re.sub", "qas.append", "answers.append"], "function", ["None"], ["", "def", "to_squad_para_dev", "(", "sample", ")", ":", "\n", "    ", "\"\"\"\n    This function convert development data from MRQA format to SQuAD format.\n    :param sample: one sample in MRQA format.\n    :return: paragraphs in SQuAD format.\n    \"\"\"", "\n", "\n", "squad_para", "=", "dict", "(", ")", "\n", "context", "=", "sample", "[", "'context'", "]", "\n", "context", "=", "re", ".", "sub", "(", "r'\\[TLE\\]|\\[DOC\\]|\\[PAR\\]'", ",", "'[SEP]'", ",", "context", ")", "\n", "squad_para", "[", "'context'", "]", "=", "context", "\n", "qas", "=", "[", "]", "\n", "for", "qa", "in", "sample", "[", "'qas'", "]", ":", "\n", "        ", "org_answers", "=", "qa", "[", "'answers'", "]", "\n", "answers", "=", "[", "]", "\n", "for", "org_answer", "in", "org_answers", ":", "\n", "            ", "answer", "=", "{", "\n", "'text'", ":", "org_answer", ",", "\n", "'answer_start'", ":", "-", "1", "\n", "}", "\n", "answers", ".", "append", "(", "answer", ")", "\n", "", "squad_qa", "=", "{", "\n", "'question'", ":", "qa", "[", "'question'", "]", ",", "\n", "'id'", ":", "qa", "[", "'qid'", "]", ",", "\n", "'answers'", ":", "answers", "\n", "}", "\n", "qas", ".", "append", "(", "squad_qa", ")", "\n", "", "squad_para", "[", "'qas'", "]", "=", "qas", "\n", "return", "squad_para", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.convert_mrqa2squad.doc_wrapper": [[128, 140], ["None"], "function", ["None"], ["", "def", "doc_wrapper", "(", "squad_para", ",", "title", "=", "\"\"", ")", ":", "\n", "    ", "\"\"\"\n    This function wrap paragraphs into a document.\n    :param squad_para: paragraphs in SQuAD format.\n    :param title: the title of paragraphs.\n    :return: wrap of title and paragraphs\n    \"\"\"", "\n", "squad_doc", "=", "{", "\n", "'title'", ":", "title", ",", "\n", "'paragraphs'", ":", "[", "squad_para", "]", "\n", "}", "\n", "return", "squad_doc", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.normalize_answer": [[11, 27], ["evaluate-v1.normalize_answer.white_space_fix"], "function", ["None"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.f1_score": [[29, 40], ["normalize_answer().split", "normalize_answer().split", "sum", "collections.Counter", "collections.Counter", "common.values", "len", "len", "evaluate-v1.normalize_answer", "evaluate-v1.normalize_answer"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.normalize_answer", "home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.normalize_answer"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.exact_match_score": [[42, 44], ["evaluate-v1.normalize_answer", "evaluate-v1.normalize_answer"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.normalize_answer", "home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.normalize_answer"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.metric_max_over_ground_truths": [[46, 52], ["max", "scores_for_ground_truths.append", "evaluate-v1.exact_match_score", "evaluate-v1.f1_score"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.exact_match_score", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.f1_score"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.evaluate-v1.evaluate": [[54, 76], ["list", "evaluate-v1.metric_max_over_ground_truths", "evaluate-v1.metric_max_over_ground_truths", "print", "map"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.metric_max_over_ground_truths", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.metric_max_over_ground_truths"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.macro_avg.extract_score": [[6, 11], ["json.loads", "float", "float"], "function", ["None"], ["def", "extract_score", "(", "line", ")", ":", "\n", "    ", "score_json", "=", "json", ".", "loads", "(", "line", ")", "\n", "f1", "=", "score_json", "[", "'f1'", "]", "\n", "em", "=", "score_json", "[", "'exact_match'", "]", "\n", "return", "float", "(", "f1", ")", ",", "float", "(", "em", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.convert_model_params.parse_args": [[29, 44], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "__doc__", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--init_tf_checkpoint\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Initial TF checkpoint (a pre-trained BERT model).\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fluid_params_dir\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The directory to store converted Fluid parameters.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.convert_model_params.parse": [[46, 143], ["collections.OrderedDict", "collections.OrderedDict", "tensorflow.train.list_variables", "print", "var_name.startswith", "key.startswith", "var_name.startswith", "key.endswith", "key.startswith", "print", "key.endswith", "int", "key.endswith", "key.startswith", "key.endswith", "str", "key.endswith", "key.endswith", "print", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "key.find", "key.endswith", "print", "print", "key.find", "key.endswith", "key.endswith", "key.endswith", "print", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "print"], "function", ["None"], ["", "def", "parse", "(", "init_checkpoint", ")", ":", "\n", "    ", "tf_fluid_param_name_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "tf_param_name_shape_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "init_checkpoint", ")", "\n", "for", "(", "var_name", ",", "var_shape", ")", "in", "init_vars", ":", "\n", "        ", "print", "(", "\"%s\\t%s\"", "%", "(", "var_name", ",", "var_shape", ")", ")", "\n", "fluid_param_name", "=", "''", "\n", "if", "var_name", ".", "startswith", "(", "'bert/'", ")", ":", "\n", "            ", "key", "=", "var_name", "[", "5", ":", "]", "\n", "if", "(", "key", ".", "startswith", "(", "'embeddings/'", ")", ")", ":", "\n", "                ", "if", "(", "key", ".", "endswith", "(", "'LayerNorm/gamma'", ")", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'pre_encoder_layer_norm_scale'", "\n", "", "elif", "(", "key", ".", "endswith", "(", "'LayerNorm/beta'", ")", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'pre_encoder_layer_norm_bias'", "\n", "", "elif", "(", "key", ".", "endswith", "(", "'position_embeddings'", ")", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'pos_embedding'", "\n", "", "elif", "(", "key", ".", "endswith", "(", "'word_embeddings'", ")", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'word_embedding'", "\n", "", "elif", "(", "key", ".", "endswith", "(", "'token_type_embeddings'", ")", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'sent_embedding'", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"ignored param: %s\"", "%", "var_name", ")", "\n", "", "", "elif", "(", "key", ".", "startswith", "(", "'encoder/'", ")", ")", ":", "\n", "                ", "key", "=", "key", "[", "8", ":", "]", "\n", "layer_num", "=", "int", "(", "key", "[", "key", ".", "find", "(", "'_'", ")", "+", "1", ":", "key", ".", "find", "(", "'/'", ")", "]", ")", "\n", "suffix", "=", "\"encoder_layer_\"", "+", "str", "(", "layer_num", ")", "\n", "if", "key", ".", "endswith", "(", "'attention/output/LayerNorm/beta'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_post_att_layer_norm_bias'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/output/LayerNorm/gamma'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_post_att_layer_norm_scale'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/output/dense/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_output_fc.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/output/dense/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_output_fc.w_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/self/key/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_key_fc.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/self/key/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_key_fc.w_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/self/query/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_query_fc.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/self/query/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_query_fc.w_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/self/value/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_value_fc.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/self/value/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_value_fc.w_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'intermediate/dense/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_ffn_fc_0.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'intermediate/dense/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_ffn_fc_0.w_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'output/LayerNorm/beta'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_post_ffn_layer_norm_bias'", "\n", "", "elif", "key", ".", "endswith", "(", "'output/LayerNorm/gamma'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_post_ffn_layer_norm_scale'", "\n", "", "elif", "key", ".", "endswith", "(", "'output/dense/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_ffn_fc_1.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'output/dense/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_ffn_fc_1.w_0'", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"ignored param: %s\"", "%", "var_name", ")", "\n", "", "", "elif", "(", "key", ".", "startswith", "(", "'pooler/'", ")", ")", ":", "\n", "                ", "if", "key", ".", "endswith", "(", "'dense/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'pooled_fc.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'dense/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'pooled_fc.w_0'", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"ignored param: %s\"", "%", "var_name", ")", "\n", "", "", "else", ":", "\n", "                ", "print", "(", "\"ignored param: %s\"", "%", "var_name", ")", "\n", "\n", "", "", "elif", "var_name", ".", "startswith", "(", "'output/'", ")", ":", "\n", "            ", "if", "var_name", "==", "'output/passage_regression/weights'", ":", "\n", "                ", "fluid_param_name", "=", "'passage_regression_weights'", "\n", "", "elif", "var_name", "==", "'output/span/start/weights'", ":", "\n", "                ", "fluid_param_name", "=", "'span_start_weights'", "\n", "", "elif", "var_name", "==", "\"output/span/end/conditional/dense/kernel\"", ":", "\n", "                ", "fluid_param_name", "=", "'conditional_fc_weights'", "\n", "", "elif", "var_name", "==", "\"output/span/end/conditional/dense/bias\"", ":", "\n", "                ", "fluid_param_name", "=", "'conditional_fc_bias'", "\n", "", "elif", "var_name", "==", "\"output/span/end/conditional/LayerNorm/beta\"", ":", "\n", "                ", "fluid_param_name", "=", "'conditional_layernorm_beta'", "\n", "", "elif", "var_name", "==", "\"output/span/end/conditional/LayerNorm/gamma\"", ":", "\n", "                ", "fluid_param_name", "=", "'conditional_layernorm_gamma'", "\n", "", "elif", "var_name", "==", "\"output/span/end/weights\"", ":", "\n", "                ", "fluid_param_name", "=", "'span_end_weights'", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"ignored param: %s\"", "%", "var_name", ")", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"ignored param: %s\"", "%", "var_name", ")", "\n", "\n", "", "if", "fluid_param_name", "!=", "''", ":", "\n", "            ", "tf_fluid_param_name_map", "[", "var_name", "]", "=", "fluid_param_name", "\n", "tf_param_name_shape_map", "[", "var_name", "]", "=", "var_shape", "\n", "fluid_param_name", "=", "''", "\n", "\n", "", "", "return", "tf_fluid_param_name_map", ",", "tf_param_name_shape_map", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.convert_model_params.convert": [[145, 173], ["convert_model_params.parse", "paddle.Program", "fluid.Program.global_block", "paddle.core.CPUPlace", "paddle.Executor", "fluid.Executor.run", "print", "print", "print", "tensorflow.python.pywrap_tensorflow.NewCheckpointReader", "paddle.io.save_params", "program.global_block.create_parameter", "pywrap_tensorflow.NewCheckpointReader.get_tensor", "paddle.global_scope().find_var().get_tensor().set", "print", "paddle.initializer.Constant", "paddle.global_scope().find_var().get_tensor", "paddle.global_scope().find_var", "paddle.global_scope"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src.convert_params.parse", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["", "def", "convert", "(", "args", ")", ":", "\n", "    ", "tf_fluid_param_name_map", ",", "tf_param_name_shape_map", "=", "parse", "(", "\n", "args", ".", "init_tf_checkpoint", ")", "\n", "program", "=", "fluid", ".", "Program", "(", ")", "\n", "global_block", "=", "program", ".", "global_block", "(", ")", "\n", "for", "param", "in", "tf_fluid_param_name_map", ":", "\n", "        ", "global_block", ".", "create_parameter", "(", "\n", "name", "=", "tf_fluid_param_name_map", "[", "param", "]", ",", "\n", "shape", "=", "tf_param_name_shape_map", "[", "param", "]", ",", "\n", "dtype", "=", "'float32'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "value", "=", "0.0", ")", ")", "\n", "\n", "", "place", "=", "fluid", ".", "core", ".", "CPUPlace", "(", ")", "\n", "exe", "=", "fluid", ".", "Executor", "(", "place", ")", "\n", "exe", ".", "run", "(", "program", ")", "\n", "\n", "print", "(", "'---------------------- Converted Parameters -----------------------'", ")", "\n", "print", "(", "'###### [TF param name] --> [Fluid param name]  [param shape] ######'", ")", "\n", "print", "(", "'-------------------------------------------------------------------'", ")", "\n", "\n", "reader", "=", "pywrap_tensorflow", ".", "NewCheckpointReader", "(", "args", ".", "init_tf_checkpoint", ")", "\n", "for", "param", "in", "tf_fluid_param_name_map", ":", "\n", "        ", "value", "=", "reader", ".", "get_tensor", "(", "param", ")", "\n", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "tf_fluid_param_name_map", "[", "\n", "param", "]", ")", ".", "get_tensor", "(", ")", ".", "set", "(", "value", ",", "place", ")", "\n", "print", "(", "param", ",", "' --> '", ",", "tf_fluid_param_name_map", "[", "param", "]", ",", "'  '", ",", "value", ".", "shape", ")", "\n", "\n", "", "fluid", ".", "io", ".", "save_params", "(", "exe", ",", "args", ".", "fluid_params_dir", ",", "main_program", "=", "program", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.args.ArgumentGroup.__init__": [[31, 33], ["parser.add_argument_group"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "parser", ",", "title", ",", "des", ")", ":", "\n", "        ", "self", ".", "_group", "=", "parser", ".", "add_argument_group", "(", "title", "=", "title", ",", "description", "=", "des", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.args.ArgumentGroup.add_arg": [[34, 42], ["args.ArgumentGroup._group.add_argument"], "methods", ["None"], ["", "def", "add_arg", "(", "self", ",", "name", ",", "type", ",", "default", ",", "help", ",", "**", "kwargs", ")", ":", "\n", "        ", "type", "=", "str2bool", "if", "type", "==", "bool", "else", "type", "\n", "self", ".", "_group", ".", "add_argument", "(", "\n", "\"--\"", "+", "name", ",", "\n", "default", "=", "default", ",", "\n", "type", "=", "type", ",", "\n", "help", "=", "help", "+", "' Default: %(default)s.'", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.args.str2bool": [[24, 28], ["v.lower"], "function", ["None"], ["def", "str2bool", "(", "v", ")", ":", "\n", "# because argparse does not support to parse \"true, False\" as python", "\n", "# boolean directly", "\n", "    ", "return", "v", ".", "lower", "(", ")", "in", "(", "\"true\"", ",", "\"t\"", ",", "\"1\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.scripts.args.print_arguments": [[44, 49], ["print", "sorted", "print", "six.iteritems", "print", "vars"], "function", ["None"], ["", "", "def", "print_arguments", "(", "args", ")", ":", "\n", "    ", "print", "(", "'-----------  Configuration Arguments -----------'", ")", "\n", "for", "arg", ",", "value", "in", "sorted", "(", "six", ".", "iteritems", "(", "vars", "(", "args", ")", ")", ")", ":", "\n", "        ", "print", "(", "'%s: %s'", "%", "(", "arg", ",", "value", ")", ")", "\n", "", "print", "(", "'------------------------------------------------'", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Checklist.evaluate._tokenize_chinese_chars": [[19, 64], ["ord", "output.append", "evaluate._tokenize_chinese_chars._is_chinese_char"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._is_chinese_char"], ["def", "_tokenize_chinese_chars", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    :param text: input text, unicode string\n    :return:\n        tokenized text, list\n    \"\"\"", "\n", "\n", "def", "_is_chinese_char", "(", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#     https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n", "", "output", "=", "[", "]", "\n", "buff", "=", "\"\"", "\n", "for", "char", "in", "text", ":", "\n", "        ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "_is_chinese_char", "(", "cp", ")", "or", "char", "==", "\"=\"", ":", "\n", "            ", "if", "buff", "!=", "\"\"", ":", "\n", "                ", "output", ".", "append", "(", "buff", ")", "\n", "buff", "=", "\"\"", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "else", ":", "\n", "            ", "buff", "+=", "char", "\n", "\n", "", "", "if", "buff", "!=", "\"\"", ":", "\n", "        ", "output", ".", "append", "(", "buff", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Checklist.evaluate._normalize": [[66, 83], ["in_str.lower.lower", "out_segs.append"], "function", ["None"], ["", "def", "_normalize", "(", "in_str", ")", ":", "\n", "    ", "\"\"\"\n    normalize the input unicode string\n    \"\"\"", "\n", "in_str", "=", "in_str", ".", "lower", "(", ")", "\n", "sp_char", "=", "[", "\n", "u':'", ",", "u'_'", ",", "u'`'", ",", "u'\uff0c'", ",", "u'\u3002'", ",", "u'\uff1a'", ",", "u'\uff1f'", ",", "u'\uff01'", ",", "u'('", ",", "u')'", ",", "\n", "u'\u201c'", ",", "u'\u201d'", ",", "u'\uff1b'", ",", "u'\u2019'", ",", "u'\u300a'", ",", "u'\u300b'", ",", "u'\u2026\u2026'", ",", "u'\u00b7'", ",", "u'\u3001'", ",", "u','", ",", "\n", "u'\u300c'", ",", "u'\u300d'", ",", "u'\uff08'", ",", "u'\uff09'", ",", "u'\uff0d'", ",", "u'\uff5e'", ",", "u'\u300e'", ",", "u'\u300f'", ",", "'|'", "\n", "]", "\n", "out_segs", "=", "[", "]", "\n", "for", "char", "in", "in_str", ":", "\n", "        ", "if", "char", "in", "sp_char", ":", "\n", "            ", "continue", "\n", "", "else", ":", "\n", "            ", "out_segs", ".", "append", "(", "char", ")", "\n", "", "", "return", "''", ".", "join", "(", "out_segs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Checklist.evaluate.find_lcs": [[85, 98], ["range", "len", "range", "range", "len", "range", "len", "len"], "function", ["None"], ["", "def", "find_lcs", "(", "s1", ",", "s2", ")", ":", "\n", "    ", "\"\"\"find the longest common subsequence between s1 ans s2\"\"\"", "\n", "m", "=", "[", "[", "0", "for", "i", "in", "range", "(", "len", "(", "s2", ")", "+", "1", ")", "]", "for", "j", "in", "range", "(", "len", "(", "s1", ")", "+", "1", ")", "]", "\n", "max_len", "=", "0", "\n", "p", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "s1", ")", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "len", "(", "s2", ")", ")", ":", "\n", "            ", "if", "s1", "[", "i", "]", "==", "s2", "[", "j", "]", ":", "\n", "                ", "m", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "m", "[", "i", "]", "[", "j", "]", "+", "1", "\n", "if", "m", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", ">", "max_len", ":", "\n", "                    ", "max_len", "=", "m", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "\n", "p", "=", "i", "+", "1", "\n", "", "", "", "", "return", "s1", "[", "p", "-", "max_len", ":", "p", "]", ",", "max_len", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Checklist.evaluate.evaluate": [[100, 156], ["ref_ans.items", "evaluate.calc_f1_score", "evaluate.calc_em_score", "print", "print", "print", "print", "print", "print", "print", "prediction.lower", "str", "print", "print", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate.calc_f1_score", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate.calc_em_score"], ["", "def", "evaluate", "(", "ref_ans", ",", "pred_ans", ",", "verbose", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    ref_ans: reference answers, dict\n    pred_ans: predicted answer, dict\n    return:\n        f1_score: averaged F1 score\n        em_score: averaged EM score\n        total_count: number of samples in the reference dataset\n        skip_count: number of samples skipped in the calculation due to unknown errors\n    \"\"\"", "\n", "f1", "=", "0", "\n", "em", "=", "0", "\n", "total_count", "=", "0", "\n", "skip_count", "=", "0", "\n", "for", "query_id", ",", "sample", "in", "ref_ans", ".", "items", "(", ")", ":", "\n", "        ", "total_count", "+=", "1", "\n", "para", "=", "sample", "[", "'para'", "]", "\n", "query_text", "=", "sample", "[", "'question'", "]", "\n", "title", "=", "sample", "[", "'title'", "]", "\n", "answers", "=", "sample", "[", "'answers'", "]", "\n", "is_impossible", "=", "sample", "[", "'is_impossible'", "]", "\n", "try", ":", "\n", "            ", "prediction", "=", "pred_ans", "[", "str", "(", "query_id", ")", "]", "\n", "", "except", ":", "\n", "            ", "skip_count", "+=", "1", "\n", "if", "verbose", ":", "\n", "                ", "print", "(", "\"para: {}\"", ".", "format", "(", "para", ")", ")", "\n", "print", "(", "\"query: {}\"", ".", "format", "(", "query_text", ")", ")", "\n", "print", "(", "\"ref: {}\"", ".", "format", "(", "'#'", ".", "join", "(", "answers", ")", ")", ")", "\n", "print", "(", "\"Skipped\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "", "continue", "\n", "", "if", "is_impossible", ":", "\n", "            ", "if", "prediction", ".", "lower", "(", ")", "==", "'no answer'", ":", "\n", "                ", "_f1", "=", "1.0", "\n", "_em", "=", "1.0", "\n", "", "else", ":", "\n", "                ", "_f1", "=", "0.0", "\n", "_em", "=", "0.0", "\n", "", "", "else", ":", "\n", "            ", "_f1", "=", "calc_f1_score", "(", "answers", ",", "prediction", ")", "\n", "_em", "=", "calc_em_score", "(", "answers", ",", "prediction", ")", "\n", "", "f1", "+=", "_f1", "\n", "em", "+=", "_em", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "\"para: {}\"", ".", "format", "(", "para", ")", ")", "\n", "print", "(", "\"query: {}\"", ".", "format", "(", "query_text", ")", ")", "\n", "print", "(", "\"title: {}\"", ".", "format", "(", "title", ")", ")", "\n", "print", "(", "\"ref: {}\"", ".", "format", "(", "'#'", ".", "join", "(", "answers", ")", ")", ")", "\n", "print", "(", "\"cand: {}\"", ".", "format", "(", "prediction", ")", ")", "\n", "print", "(", "\"score: {}\"", ".", "format", "(", "_f1", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "", "", "f1_score", "=", "100.0", "*", "f1", "/", "total_count", "\n", "em_score", "=", "100.0", "*", "em", "/", "total_count", "\n", "return", "f1_score", ",", "em_score", ",", "total_count", ",", "skip_count", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Checklist.evaluate.calc_f1_score": [[158, 175], ["max", "evaluate._tokenize_chinese_chars", "evaluate._tokenize_chinese_chars", "evaluate.find_lcs", "f1_scores.append", "evaluate._normalize", "evaluate._normalize", "print", "print", "f1_scores.append", "len", "len", "json.dumps", "json.dumps"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._tokenize_chinese_chars", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._tokenize_chinese_chars", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate.find_lcs", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._normalize", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._normalize"], ["", "def", "calc_f1_score", "(", "answers", ",", "prediction", ")", ":", "\n", "    ", "f1_scores", "=", "[", "]", "\n", "for", "ans", "in", "answers", ":", "\n", "        ", "ans_segs", "=", "_tokenize_chinese_chars", "(", "_normalize", "(", "ans", ")", ")", "\n", "prediction_segs", "=", "_tokenize_chinese_chars", "(", "_normalize", "(", "prediction", ")", ")", "\n", "if", "args", ".", "debug", ":", "\n", "            ", "print", "(", "json", ".", "dumps", "(", "ans_segs", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "print", "(", "json", ".", "dumps", "(", "prediction_segs", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "", "lcs", ",", "lcs_len", "=", "find_lcs", "(", "ans_segs", ",", "prediction_segs", ")", "\n", "if", "lcs_len", "==", "0", ":", "\n", "            ", "f1_scores", ".", "append", "(", "0", ")", "\n", "continue", "\n", "", "prec", "=", "1.0", "*", "lcs_len", "/", "len", "(", "prediction_segs", ")", "\n", "rec", "=", "1.0", "*", "lcs_len", "/", "len", "(", "ans_segs", ")", "\n", "f1", "=", "(", "2", "*", "prec", "*", "rec", ")", "/", "(", "prec", "+", "rec", ")", "\n", "f1_scores", ".", "append", "(", "f1", ")", "\n", "", "return", "max", "(", "f1_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Checklist.evaluate.calc_em_score": [[177, 186], ["evaluate._normalize", "evaluate._normalize"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._normalize", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._normalize"], ["", "def", "calc_em_score", "(", "answers", ",", "prediction", ")", ":", "\n", "    ", "em", "=", "0", "\n", "for", "ans", "in", "answers", ":", "\n", "        ", "ans_", "=", "_normalize", "(", "ans", ")", "\n", "prediction_", "=", "_normalize", "(", "prediction", ")", "\n", "if", "ans_", "==", "prediction_", ":", "\n", "            ", "em", "=", "1", "\n", "break", "\n", "", "", "return", "em", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Checklist.evaluate.read_mrc_dataset": [[188, 218], ["collections.OrderedDict", "io.open", "json.load", "paragraph[].strip", "qa[].strip", "qa[].startswith"], "function", ["None"], ["", "def", "read_mrc_dataset", "(", "filename", ",", "tag", "=", "None", ")", ":", "\n", "    ", "dataset", "=", "OrderedDict", "(", ")", "\n", "with", "io", ".", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "as", "fin", ":", "\n", "        ", "mrc_dataset", "=", "json", ".", "load", "(", "fin", ")", "\n", "", "for", "document", "in", "mrc_dataset", "[", "'data'", "]", ":", "\n", "        ", "for", "paragraph", "in", "document", "[", "'paragraphs'", "]", ":", "\n", "            ", "para", "=", "paragraph", "[", "'context'", "]", ".", "strip", "(", ")", "\n", "title", "=", "''", "\n", "if", "'title'", "in", "paragraph", ":", "\n", "                ", "title", "=", "paragraph", "[", "'title'", "]", "\n", "", "for", "qa", "in", "(", "paragraph", "[", "'qas'", "]", ")", ":", "\n", "                ", "query_id", "=", "qa", "[", "'id'", "]", "\n", "query_text", "=", "qa", "[", "'question'", "]", ".", "strip", "(", ")", "\n", "answers", "=", "[", "a", "[", "'text'", "]", "for", "a", "in", "qa", "[", "'answers'", "]", "]", "\n", "if", "tag", "is", "not", "None", ":", "\n", "                    ", "if", "not", "qa", "[", "'type'", "]", ".", "startswith", "(", "tag", ")", ":", "\n", "                        ", "continue", "\n", "", "", "is_impossible", "=", "False", "\n", "if", "'is_impossible'", "in", "qa", ":", "\n", "                    ", "is_impossible", "=", "qa", "[", "'is_impossible'", "]", "\n", "", "if", "is_impossible", ":", "\n", "                    ", "answers", "=", "[", "'no answer'", "]", "\n", "", "dataset", "[", "query_id", "]", "=", "{", "\n", "'answers'", ":", "answers", ",", "\n", "'question'", ":", "query_text", ",", "\n", "'para'", ":", "para", ",", "\n", "'is_impossible'", ":", "is_impossible", ",", "\n", "'title'", ":", "title", "\n", "}", "\n", "", "", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Checklist.evaluate.read_model_prediction": [[220, 224], ["io.open", "json.load"], "function", ["None"], ["", "def", "read_model_prediction", "(", "filename", ")", ":", "\n", "    ", "with", "io", ".", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "as", "fin", ":", "\n", "        ", "model_prediction", "=", "json", ".", "load", "(", "fin", ")", "\n", "", "return", "model_prediction", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.models.ErnieForQuestionAnswering.__init__": [[6, 12], ["paddlenlp.transformers.ErniePretrainedModel.__init__", "paddle.nn.Linear", "paddle.nn.Linear", "models.ErnieForQuestionAnswering.apply"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.__init__"], ["    ", "def", "__init__", "(", "self", ",", "ernie", ")", ":", "\n", "        ", "super", "(", "ErnieForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ernie", "=", "ernie", "# allow ernie to be config", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "self", ".", "ernie", ".", "config", "[", "\"hidden_size\"", "]", ",", "2", ")", "\n", "self", ".", "classifier_cls", "=", "nn", ".", "Linear", "(", "self", ".", "ernie", ".", "config", "[", "\"hidden_size\"", "]", ",", "2", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.models.ErnieForQuestionAnswering.forward": [[13, 30], ["models.ErnieForQuestionAnswering.ernie", "models.ErnieForQuestionAnswering.classifier", "paddle.transpose", "paddle.unstack", "models.ErnieForQuestionAnswering.classifier_cls"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "ernie", "(", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "attention_mask", "=", "attention_mask", ")", "\n", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "logits", "=", "paddle", ".", "transpose", "(", "logits", ",", "perm", "=", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "start_logits", ",", "end_logits", "=", "paddle", ".", "unstack", "(", "x", "=", "logits", ",", "axis", "=", "0", ")", "\n", "cls_logits", "=", "self", ".", "classifier_cls", "(", "pooled_output", ")", "\n", "\n", "return", "start_logits", ",", "end_logits", ",", "cls_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.models.BertForQuestionAnswering.__init__": [[32, 38], ["paddlenlp.transformers.BertPretrainedModel.__init__", "paddle.nn.Linear", "paddle.nn.Linear", "models.BertForQuestionAnswering.apply"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bert", ")", ":", "\n", "        ", "super", "(", "BertForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert", "=", "bert", "# allow bert to be config", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "self", ".", "bert", ".", "config", "[", "\"hidden_size\"", "]", ",", "2", ")", "\n", "self", ".", "classifier_cls", "=", "nn", ".", "Linear", "(", "self", ".", "bert", ".", "config", "[", "\"hidden_size\"", "]", ",", "2", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.models.BertForQuestionAnswering.forward": [[39, 56], ["models.BertForQuestionAnswering.bert", "models.BertForQuestionAnswering.classifier", "paddle.transpose", "paddle.unstack", "models.BertForQuestionAnswering.classifier_cls"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "attention_mask", "=", "attention_mask", ")", "\n", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "logits", "=", "paddle", ".", "transpose", "(", "logits", ",", "perm", "=", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "start_logits", ",", "end_logits", "=", "paddle", ".", "unstack", "(", "x", "=", "logits", ",", "axis", "=", "0", ")", "\n", "cls_logits", "=", "self", ".", "classifier_cls", "(", "pooled_output", ")", "\n", "\n", "return", "start_logits", ",", "end_logits", ",", "cls_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.models.RobertaForQuestionAnswering.__init__": [[58, 64], ["paddlenlp.transformers.RobertaPretrainedModel.__init__", "paddle.nn.Linear", "paddle.nn.Linear", "models.RobertaForQuestionAnswering.apply"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.__init__"], ["    ", "def", "__init__", "(", "self", ",", "roberta", ")", ":", "\n", "        ", "super", "(", "RobertaForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "roberta", "=", "roberta", "# allow roberta to be config", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "self", ".", "roberta", ".", "config", "[", "\"hidden_size\"", "]", ",", "2", ")", "\n", "self", ".", "classifier_cls", "=", "nn", ".", "Linear", "(", "self", ".", "roberta", ".", "config", "[", "\"hidden_size\"", "]", ",", "2", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.models.RobertaForQuestionAnswering.forward": [[65, 82], ["models.RobertaForQuestionAnswering.roberta", "models.RobertaForQuestionAnswering.classifier", "paddle.transpose", "paddle.unstack", "models.RobertaForQuestionAnswering.classifier_cls"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "roberta", "(", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "attention_mask", "=", "attention_mask", ")", "\n", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "logits", "=", "paddle", ".", "transpose", "(", "logits", ",", "perm", "=", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "start_logits", ",", "end_logits", "=", "paddle", ".", "unstack", "(", "x", "=", "logits", ",", "axis", "=", "0", ")", "\n", "cls_logits", "=", "self", ".", "classifier_cls", "(", "pooled_output", ")", "\n", "\n", "return", "start_logits", ",", "end_logits", ",", "cls_logits", "", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run.CrossEntropyLossForChecklist.__init__": [[47, 49], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "CrossEntropyLossForChecklist", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run.CrossEntropyLossForChecklist.forward": [[50, 68], ["paddle.unsqueeze", "paddle.unsqueeze", "paddle.unsqueeze", "paddle.nn.functional.softmax_with_cross_entropy", "paddle.mean", "paddle.nn.functional.softmax_with_cross_entropy", "paddle.mean", "paddle.nn.functional.softmax_with_cross_entropy", "paddle.mean"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "y", ",", "label", ")", ":", "\n", "        ", "start_logits", ",", "end_logits", ",", "cls_logits", "=", "y", "\n", "start_position", ",", "end_position", ",", "answerable_label", "=", "label", "\n", "start_position", "=", "paddle", ".", "unsqueeze", "(", "start_position", ",", "axis", "=", "-", "1", ")", "\n", "end_position", "=", "paddle", ".", "unsqueeze", "(", "end_position", ",", "axis", "=", "-", "1", ")", "\n", "answerable_label", "=", "paddle", ".", "unsqueeze", "(", "answerable_label", ",", "axis", "=", "-", "1", ")", "\n", "start_loss", "=", "paddle", ".", "nn", ".", "functional", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "start_logits", ",", "label", "=", "start_position", ",", "soft_label", "=", "False", ")", "\n", "start_loss", "=", "paddle", ".", "mean", "(", "start_loss", ")", "\n", "end_loss", "=", "paddle", ".", "nn", ".", "functional", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "end_logits", ",", "label", "=", "end_position", ",", "soft_label", "=", "False", ")", "\n", "end_loss", "=", "paddle", ".", "mean", "(", "end_loss", ")", "\n", "cls_loss", "=", "paddle", ".", "nn", ".", "functional", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "cls_logits", ",", "label", "=", "answerable_label", ",", "soft_label", "=", "False", ")", "\n", "cls_loss", "=", "paddle", ".", "mean", "(", "cls_loss", ")", "\n", "mrc_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "loss", "=", "(", "mrc_loss", "+", "cls_loss", ")", "/", "2", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run.set_seed": [[40, 44], ["random.seed", "numpy.random.seed", "paddle.seed"], "function", ["None"], ["def", "set_seed", "(", "args", ")", ":", "\n", "    ", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "paddle", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run.evaluate": [[70, 116], ["model.eval", "time.time", "squad.compute_prediction_checklist", "model.train", "model", "range", "os.path.exists", "os.makedirs", "open", "writer.write", "open", "writer.write", "all_start_logits.append", "all_end_logits.append", "all_cls_logits.append", "os.path.join", "os.path.join", "open", "len", "print", "print", "time.time", "json.dumps", "json.dumps", "os.path.join", "f_cls.write", "start_logits_tensor.numpy", "end_logits_tensor.numpy", "cls_logits_tensor.numpy", "len", "len", "time.time"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src.squad.compute_prediction_checklist", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.train"], ["", "", "def", "evaluate", "(", "model", ",", "data_loader", ",", "args", ",", "prefix", "=", "\"\"", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "all_start_logits", "=", "[", "]", "\n", "all_end_logits", "=", "[", "]", "\n", "all_cls_logits", "=", "[", "]", "\n", "tic_eval", "=", "time", ".", "time", "(", ")", "\n", "\n", "for", "batch", "in", "data_loader", ":", "\n", "        ", "input_ids", ",", "segment_ids", "=", "batch", "\n", "start_logits_tensor", ",", "end_logits_tensor", ",", "cls_logits_tensor", "=", "model", "(", "input_ids", ",", "segment_ids", ")", "\n", "\n", "for", "idx", "in", "range", "(", "start_logits_tensor", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "if", "len", "(", "all_start_logits", ")", "%", "1000", "==", "0", "and", "len", "(", "all_start_logits", ")", ":", "\n", "                ", "print", "(", "\"Processing example: %d\"", "%", "len", "(", "all_start_logits", ")", ")", "\n", "print", "(", "'time per 1000:'", ",", "time", ".", "time", "(", ")", "-", "tic_eval", ")", "\n", "tic_eval", "=", "time", ".", "time", "(", ")", "\n", "\n", "\n", "", "all_start_logits", ".", "append", "(", "start_logits_tensor", ".", "numpy", "(", ")", "[", "idx", "]", ")", "\n", "all_end_logits", ".", "append", "(", "end_logits_tensor", ".", "numpy", "(", ")", "[", "idx", "]", ")", "\n", "all_cls_logits", ".", "append", "(", "cls_logits_tensor", ".", "numpy", "(", ")", "[", "idx", "]", ")", "\n", "\n", "", "", "all_predictions", ",", "all_nbest_json", ",", "all_cls_predictions", "=", "compute_prediction_checklist", "(", "\n", "data_loader", ".", "dataset", ".", "data", ",", "data_loader", ".", "dataset", ".", "new_data", ",", "\n", "(", "all_start_logits", ",", "all_end_logits", ",", "all_cls_logits", ")", ",", "True", ",", "args", ".", "n_best_size", ",", "\n", "args", ".", "max_answer_length", ",", "args", ".", "cls_threshold", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "prefix", "+", "'_predictions.json'", ")", ",", "\"w\"", ",", "encoding", "=", "'utf-8'", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "\n", "json", ".", "dumps", "(", "\n", "all_predictions", ",", "ensure_ascii", "=", "False", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "prefix", "+", "'_nbest_predictions.json'", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_nbest_json", ",", "indent", "=", "4", ",", "ensure_ascii", "=", "False", ")", "+", "u\"\\n\"", ")", "\n", "\n", "", "if", "all_cls_predictions", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "prefix", "+", "\"_cls_preditions.json\"", ")", ",", "\"w\"", ")", "as", "f_cls", ":", "\n", "            ", "for", "cls_predictions", "in", "all_cls_predictions", ":", "\n", "                ", "qas_id", ",", "pred_cls_label", ",", "no_answer_prob", ",", "answerable_prob", "=", "cls_predictions", "\n", "f_cls", ".", "write", "(", "'{}\\t{}\\t{}\\t{}\\n'", ".", "format", "(", "qas_id", ",", "pred_cls_label", ",", "no_answer_prob", ",", "answerable_prob", ")", ")", "\n", "\n", "", "", "", "model", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run.run": [[118, 344], ["paddle.set_device", "args.model_type.lower", "tokenizer_class.from_pretrained", "run.set_seed", "model_class.from_pretrained", "paddle.distributed.get_world_size", "paddle.distributed.init_parallel_env", "paddle.distributed.get_rank", "os.path.exists", "paddle.distributed.get_world_size", "paddle.DataParallel", "tokenizer_class.from_pretrained.", "enumerate", "squad.DuReaderChecklist().read", "DuReaderChecklist().read.map", "paddle.io.DistributedBatchSampler", "paddle.io.DataLoader", "paddlenlp.transformers.LinearDecayWithWarmup", "paddle.optimizer.AdamW", "run.CrossEntropyLossForChecklist", "time.time", "range", "tokenizer_class.from_pretrained.", "enumerate", "print", "input_ids.index", "fn", "paddle.distributed.get_rank", "paddle.fluid.core.get_cuda_device_count", "print", "print", "print", "enumerate", "input_files.extend", "len", "print", "os.path.basename", "re.sub", "squad.DuReaderChecklist().read", "DuReaderChecklist().read.map", "paddle.io.BatchSampler", "paddle.io.DataLoader", "range", "range", "len", "squad.DuReaderChecklist", "paddlenlp.data.Dict", "len", "paddle.DataParallel.parameters", "paddle.DataParallel.", "CrossEntropyLossForChecklist.", "criterion.backward", "paddle.optimizer.AdamW.step", "paddlenlp.transformers.LinearDecayWithWarmup.step", "paddle.optimizer.AdamW.clear_gradients", "range", "range", "glob.glob", "fn", "paddle.distributed.get_rank", "run.evaluate", "len", "len", "len", "len", "len", "print", "time.time", "len", "len", "enumerate", "squad.DuReaderChecklist", "paddlenlp.data.Dict", "paddlenlp.data.Pad", "paddlenlp.data.Pad", "paddlenlp.data.Stack", "paddlenlp.data.Stack", "paddlenlp.data.Stack", "paddle.distributed.get_rank", "os.path.join", "model_to_save.save_pretrained", "tokenizer_class.from_pretrained.save_pretrained", "print", "len", "os.path.exists", "os.makedirs", "isinstance", "paddlenlp.data.Pad", "paddlenlp.data.Pad", "paddle.DataParallel.named_parameters", "any", "time.time"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.set_seed", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.evaluate"], ["", "def", "run", "(", "args", ")", ":", "\n", "    ", "paddle", ".", "set_device", "(", "args", ".", "device", ")", "\n", "if", "paddle", ".", "distributed", ".", "get_world_size", "(", ")", ">", "1", ":", "\n", "        ", "paddle", ".", "distributed", ".", "init_parallel_env", "(", ")", "\n", "\n", "", "args", ".", "model_type", "=", "args", ".", "model_type", ".", "lower", "(", ")", "\n", "model_class", ",", "tokenizer_class", "=", "MODEL_CLASSES", "[", "args", ".", "model_type", "]", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ")", "\n", "set_seed", "(", "args", ")", "\n", "\n", "if", "paddle", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "model_name_or_path", ")", ":", "\n", "            ", "print", "(", "\"init checkpoint from %s\"", "%", "args", ".", "model_name_or_path", ")", "\n", "", "", "model", "=", "model_class", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ")", "\n", "\n", "if", "paddle", ".", "distributed", ".", "get_world_size", "(", ")", ">", "1", ":", "\n", "        ", "model", "=", "paddle", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "def", "prepare_train_features", "(", "examples", ")", ":", "\n", "# Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results", "\n", "# in one example possible giving several features when a context is long, each of those features having a", "\n", "# context that overlaps a bit the context of the previous feature.", "\n", "        ", "contexts", "=", "[", "examples", "[", "i", "]", "[", "'context'", "]", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", "]", "\n", "questions", "=", "[", "examples", "[", "i", "]", "[", "'question'", "]", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", "]", "\n", "\n", "tokenized_examples", "=", "tokenizer", "(", "\n", "questions", ",", "\n", "contexts", ",", "\n", "stride", "=", "args", ".", "doc_stride", ",", "\n", "max_seq_len", "=", "args", ".", "max_seq_length", ")", "\n", "\n", "for", "i", ",", "tokenized_example", "in", "enumerate", "(", "tokenized_examples", ")", ":", "\n", "# We will label impossible answers with the index of the CLS token.", "\n", "            ", "input_ids", "=", "tokenized_example", "[", "\"input_ids\"", "]", "\n", "cls_index", "=", "input_ids", ".", "index", "(", "tokenizer", ".", "cls_token_id", ")", "\n", "\n", "# The offset mappings will give us a map from token to character position in the original context. This will", "\n", "# help us compute the start_positions and end_positions.", "\n", "offsets", "=", "tokenized_example", "[", "'offset_mapping'", "]", "\n", "\n", "# Grab the sequence corresponding to that example (to know what is the context and what is the question).", "\n", "sequence_ids", "=", "tokenized_example", "[", "'token_type_ids'", "]", "\n", "\n", "# One example can give several spans, this is the index of the example containing this span of text.", "\n", "sample_index", "=", "tokenized_example", "[", "'overflow_to_sample'", "]", "\n", "answers", "=", "examples", "[", "sample_index", "]", "[", "'answers'", "]", "\n", "answer_starts", "=", "examples", "[", "sample_index", "]", "[", "'answer_starts'", "]", "\n", "\n", "# If no answers are given, set the cls_index as answer.", "\n", "if", "len", "(", "answer_starts", ")", "==", "0", ":", "\n", "                ", "tokenized_examples", "[", "i", "]", "[", "\"start_positions\"", "]", "=", "cls_index", "\n", "tokenized_examples", "[", "i", "]", "[", "\"end_positions\"", "]", "=", "cls_index", "\n", "tokenized_examples", "[", "i", "]", "[", "'answerable_label'", "]", "=", "0", "\n", "", "else", ":", "\n", "# Start/end character index of the answer in the text.", "\n", "                ", "start_char", "=", "answer_starts", "[", "0", "]", "\n", "end_char", "=", "start_char", "+", "len", "(", "answers", "[", "0", "]", ")", "\n", "\n", "# Start token index of the current span in the text.", "\n", "token_start_index", "=", "0", "\n", "while", "sequence_ids", "[", "token_start_index", "]", "!=", "1", ":", "\n", "                    ", "token_start_index", "+=", "1", "\n", "\n", "# End token index of the current span in the text.", "\n", "", "token_end_index", "=", "len", "(", "input_ids", ")", "-", "2", "\n", "while", "sequence_ids", "[", "token_end_index", "]", "!=", "1", ":", "\n", "                    ", "token_end_index", "-=", "1", "\n", "\n", "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).", "\n", "", "if", "not", "(", "offsets", "[", "token_start_index", "]", "[", "0", "]", "<=", "start_char", "and", "\n", "offsets", "[", "token_end_index", "]", "[", "1", "]", ">=", "end_char", ")", ":", "\n", "                    ", "tokenized_examples", "[", "i", "]", "[", "\"start_positions\"", "]", "=", "cls_index", "\n", "tokenized_examples", "[", "i", "]", "[", "\"end_positions\"", "]", "=", "cls_index", "\n", "tokenized_examples", "[", "i", "]", "[", "'answerable_label'", "]", "=", "0", "\n", "", "else", ":", "\n", "# Otherwise move the token_start_index and token_end_index to the two ends of the answer.", "\n", "# Note: we could go after the last offset if the answer is the last word (edge case).", "\n", "                    ", "while", "token_start_index", "<", "len", "(", "offsets", ")", "and", "offsets", "[", "\n", "token_start_index", "]", "[", "0", "]", "<=", "start_char", ":", "\n", "                        ", "token_start_index", "+=", "1", "\n", "", "tokenized_examples", "[", "i", "]", "[", "\n", "\"start_positions\"", "]", "=", "token_start_index", "-", "1", "\n", "while", "offsets", "[", "token_end_index", "]", "[", "1", "]", ">=", "end_char", ":", "\n", "                        ", "token_end_index", "-=", "1", "\n", "", "tokenized_examples", "[", "i", "]", "[", "\"end_positions\"", "]", "=", "token_end_index", "+", "1", "\n", "tokenized_examples", "[", "i", "]", "[", "'answerable_label'", "]", "=", "1", "\n", "\n", "", "", "", "return", "tokenized_examples", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "assert", "args", ".", "train_file", "!=", "None", ",", "\"--train_file should be set when training!\"", "\n", "train_ds", "=", "DuReaderChecklist", "(", ")", ".", "read", "(", "args", ".", "train_file", ")", "\n", "train_ds", ".", "map", "(", "prepare_train_features", ",", "batched", "=", "True", ")", "\n", "\n", "train_batch_sampler", "=", "paddle", ".", "io", ".", "DistributedBatchSampler", "(", "\n", "train_ds", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "train_batchify_fn", "=", "lambda", "samples", ",", "fn", "=", "Dict", "(", "{", "\n", "\"input_ids\"", ":", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "tokenizer", ".", "pad_token_id", ")", ",", "\n", "\"token_type_ids\"", ":", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "tokenizer", ".", "pad_token_type_id", ")", ",", "\n", "\"start_positions\"", ":", "Stack", "(", "dtype", "=", "\"int64\"", ")", ",", "\n", "\"end_positions\"", ":", "Stack", "(", "dtype", "=", "\"int64\"", ")", ",", "\n", "\"answerable_label\"", ":", "Stack", "(", "dtype", "=", "\"int64\"", ")", "\n", "}", ")", ":", "fn", "(", "samples", ")", "\n", "train_data_loader", "=", "DataLoader", "(", "\n", "dataset", "=", "train_ds", ",", "\n", "batch_sampler", "=", "train_batch_sampler", ",", "\n", "collate_fn", "=", "train_batchify_fn", ",", "\n", "return_list", "=", "True", ")", "\n", "\n", "num_training_steps", "=", "args", ".", "max_steps", "if", "args", ".", "max_steps", ">", "0", "else", "len", "(", "\n", "train_data_loader", ")", "*", "args", ".", "num_train_epochs", "\n", "\n", "if", "paddle", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "            ", "dev_count", "=", "paddle", ".", "fluid", ".", "core", ".", "get_cuda_device_count", "(", ")", "\n", "print", "(", "\"Device count: %d\"", "%", "dev_count", ")", "\n", "print", "(", "\"Num train examples: %d\"", "%", "len", "(", "train_ds", ".", "data", ")", ")", "\n", "print", "(", "\"Max train steps: %d\"", "%", "num_training_steps", ")", "\n", "\n", "", "lr_scheduler", "=", "LinearDecayWithWarmup", "(", "\n", "args", ".", "learning_rate", ",", "num_training_steps", ",", "args", ".", "warmup_proportion", ")", "\n", "\n", "optimizer", "=", "paddle", ".", "optimizer", ".", "AdamW", "(", "\n", "learning_rate", "=", "lr_scheduler", ",", "\n", "epsilon", "=", "args", ".", "adam_epsilon", ",", "\n", "parameters", "=", "model", ".", "parameters", "(", ")", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", "apply_decay_param_fun", "=", "lambda", "x", ":", "x", "in", "[", "\n", "p", ".", "name", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "[", "\"bias\"", ",", "\"norm\"", "]", ")", "\n", "]", ")", "\n", "criterion", "=", "CrossEntropyLossForChecklist", "(", ")", "\n", "\n", "global_step", "=", "0", "\n", "tic_train", "=", "time", ".", "time", "(", ")", "\n", "for", "epoch", "in", "range", "(", "args", ".", "num_train_epochs", ")", ":", "\n", "            ", "for", "step", ",", "batch", "in", "enumerate", "(", "train_data_loader", ")", ":", "\n", "                ", "global_step", "+=", "1", "\n", "input_ids", ",", "segment_ids", ",", "start_positions", ",", "end_positions", ",", "answerable_label", "=", "batch", "\n", "\n", "logits", "=", "model", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "segment_ids", ")", "\n", "loss", "=", "criterion", "(", "logits", ",", "(", "start_positions", ",", "end_positions", ",", "answerable_label", ")", ")", "\n", "\n", "\n", "if", "global_step", "%", "args", ".", "logging_steps", "==", "0", ":", "\n", "                    ", "print", "(", "\n", "\"global step %d, epoch: %d, batch: %d, loss: %f, speed: %.2f step/s\"", "\n", "%", "(", "global_step", ",", "epoch", ",", "step", ",", "loss", ",", "\n", "args", ".", "logging_steps", "/", "(", "time", ".", "time", "(", ")", "-", "tic_train", ")", ")", ")", "\n", "tic_train", "=", "time", ".", "time", "(", ")", "\n", "", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "lr_scheduler", ".", "step", "(", ")", "\n", "optimizer", ".", "clear_gradients", "(", ")", "\n", "\n", "if", "global_step", "%", "args", ".", "save_steps", "==", "0", "or", "global_step", "==", "num_training_steps", ":", "\n", "                    ", "if", "paddle", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "                        ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\n", "\"model_%d\"", "%", "global_step", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_dir", ")", ":", "\n", "                            ", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "# need better way to get inner model of DataParallel", "\n", "", "model_to_save", "=", "model", ".", "_layers", "if", "isinstance", "(", "\n", "model", ",", "paddle", ".", "DataParallel", ")", "else", "model", "\n", "model_to_save", ".", "save_pretrained", "(", "output_dir", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "output_dir", ")", "\n", "print", "(", "'Saving checkpoint to:'", ",", "output_dir", ")", "\n", "\n", "", "", "", "", "", "def", "prepare_validation_features", "(", "examples", ")", ":", "\n", "# Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results", "\n", "# in one example possible giving several features when a context is long, each of those features having a", "\n", "# context that overlaps a bit the context of the previous feature.", "\n", "        ", "contexts", "=", "[", "examples", "[", "i", "]", "[", "'context'", "]", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", "]", "\n", "questions", "=", "[", "examples", "[", "i", "]", "[", "'question'", "]", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", "]", "\n", "\n", "tokenized_examples", "=", "tokenizer", "(", "\n", "questions", ",", "\n", "contexts", ",", "\n", "stride", "=", "args", ".", "doc_stride", ",", "\n", "max_seq_len", "=", "args", ".", "max_seq_length", ")", "\n", "\n", "# For validation, there is no need to compute start and end positions", "\n", "for", "i", ",", "tokenized_example", "in", "enumerate", "(", "tokenized_examples", ")", ":", "\n", "# Grab the sequence corresponding to that example (to know what is the context and what is the question).", "\n", "            ", "sequence_ids", "=", "tokenized_example", "[", "'token_type_ids'", "]", "\n", "\n", "# One example can give several spans, this is the index of the example containing this span of text.", "\n", "sample_index", "=", "tokenized_example", "[", "'overflow_to_sample'", "]", "\n", "tokenized_examples", "[", "i", "]", "[", "\"example_id\"", "]", "=", "examples", "[", "sample_index", "]", "[", "'id'", "]", "\n", "\n", "# Set to None the offset_mapping that are not part of the context so it's easy to determine if a token", "\n", "# position is part of the context or not.", "\n", "tokenized_examples", "[", "i", "]", "[", "\"offset_mapping\"", "]", "=", "[", "\n", "(", "o", "if", "sequence_ids", "[", "k", "]", "==", "1", "else", "None", ")", "\n", "for", "k", ",", "o", "in", "enumerate", "(", "tokenized_example", "[", "\"offset_mapping\"", "]", ")", "\n", "]", "\n", "\n", "", "return", "tokenized_examples", "\n", "\n", "", "if", "args", ".", "do_pred", ":", "\n", "        ", "input_files", "=", "[", "]", "\n", "assert", "args", ".", "predict_file", "!=", "None", ",", "\"--predict_file should be set when predicting!\"", "\n", "for", "input_pattern", "in", "args", ".", "predict_file", ":", "\n", "            ", "input_files", ".", "extend", "(", "glob", ".", "glob", "(", "input_pattern", ")", ")", "\n", "", "assert", "len", "(", "input_files", ")", ">", "0", ",", "'Can not find predict_file {}'", ".", "format", "(", "args", ".", "predict_file", ")", "\n", "for", "input_file", "in", "input_files", ":", "\n", "            ", "print", "(", "'Run prediction on {}'", ".", "format", "(", "input_file", ")", ")", "\n", "prefix", "=", "os", ".", "path", ".", "basename", "(", "input_file", ")", "\n", "prefix", "=", "re", ".", "sub", "(", "'.json'", ",", "''", ",", "prefix", ")", "\n", "dev_ds", "=", "DuReaderChecklist", "(", ")", ".", "read", "(", "input_file", ")", "\n", "dev_ds", ".", "map", "(", "prepare_validation_features", ",", "batched", "=", "True", ")", "\n", "\n", "dev_batch_sampler", "=", "paddle", ".", "io", ".", "BatchSampler", "(", "\n", "dev_ds", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "\n", "dev_batchify_fn", "=", "lambda", "samples", ",", "fn", "=", "Dict", "(", "{", "\n", "\"input_ids\"", ":", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "tokenizer", ".", "pad_token_id", ")", ",", "\n", "\"token_type_ids\"", ":", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "tokenizer", ".", "pad_token_type_id", ")", "\n", "}", ")", ":", "fn", "(", "samples", ")", "\n", "\n", "dev_data_loader", "=", "DataLoader", "(", "\n", "dataset", "=", "dev_ds", ",", "\n", "batch_sampler", "=", "dev_batch_sampler", ",", "\n", "collate_fn", "=", "dev_batchify_fn", ",", "\n", "return_list", "=", "True", ")", "\n", "if", "paddle", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "                ", "evaluate", "(", "model", ",", "dev_data_loader", ",", "args", ",", "prefix", "=", "prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.args.parse_args": [[3, 134], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args"], ["# Licensed under the Apache License, Version 2.0 (the \"License\");", "\n", "# you may not use this file except in compliance with the License.", "\n", "# You may obtain a copy of the License at", "\n", "#", "\n", "#     http://www.apache.org/licenses/LICENSE-2.0", "\n", "#", "\n", "# Unless required by applicable law or agreed to in writing, software", "\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,", "\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "\n", "# See the License for the specific language governing permissions and", "\n", "# limitations under the License.", "\n", "\"\"\"Arguments for configuration.\"\"\"", "\n", "\n", "from", "__future__", "import", "absolute_import", "\n", "from", "__future__", "import", "division", "\n", "from", "__future__", "import", "print_function", "\n", "\n", "import", "six", "\n", "import", "argparse", "\n", "\n", "\n", "def", "str2bool", "(", "v", ")", ":", "\n", "# because argparse does not support to parse \"true, False\" as python", "\n", "# boolean directly", "\n", "    ", "return", "v", ".", "lower", "(", ")", "in", "(", "\"true\"", ",", "\"t\"", ",", "\"1\"", ")", "\n", "\n", "\n", "", "class", "ArgumentGroup", "(", "object", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "parser", ",", "title", ",", "des", ")", ":", "\n", "        ", "self", ".", "_group", "=", "parser", ".", "add_argument_group", "(", "title", "=", "title", ",", "description", "=", "des", ")", "\n", "\n", "", "def", "add_arg", "(", "self", ",", "name", ",", "type", ",", "default", ",", "help", ",", "**", "kwargs", ")", ":", "\n", "        ", "type", "=", "str2bool", "if", "type", "==", "bool", "else", "type", "\n", "self", ".", "_group", ".", "add_argument", "(", "\n", "\"--\"", "+", "name", ",", "\n", "default", "=", "default", ",", "\n", "type", "=", "type", ",", "\n", "help", "=", "help", "+", "' Default: %(default)s.'", ",", "\n", "**", "kwargs", ")", "\n", "\n", "\n", "", "", "def", "print_arguments", "(", "args", ")", ":", "\n", "    ", "print", "(", "'-----------  Configuration Arguments -----------'", ")", "\n", "for", "arg", ",", "value", "in", "sorted", "(", "six", ".", "iteritems", "(", "vars", "(", "args", ")", ")", ")", ":", "\n", "        ", "print", "(", "'%s: %s'", "%", "(", "arg", ",", "value", ")", ")", "\n", "", "print", "(", "'------------------------------------------------'", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.src.squad.DuReaderChecklist._read": [[9, 42], ["open", "entry.get().strip", "json.load", "paragraph[].strip", "entry.get", "qa[].strip", "qa.keys", "answer[].strip", "qa.get", "qa.get"], "methods", ["None"], ["    ", "def", "_read", "(", "self", ",", "filename", ")", ":", "\n", "        ", "with", "open", "(", "filename", ",", "\"r\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "            ", "input_data", "=", "json", ".", "load", "(", "f", ")", "[", "\"data\"", "]", "\n", "\n", "", "for", "entry", "in", "input_data", ":", "\n", "            ", "title", "=", "entry", ".", "get", "(", "\"title\"", ",", "\"\"", ")", ".", "strip", "(", ")", "\n", "for", "paragraph", "in", "entry", "[", "\"paragraphs\"", "]", ":", "\n", "                ", "context", "=", "paragraph", "[", "\"context\"", "]", ".", "strip", "(", ")", "\n", "for", "qa", "in", "paragraph", "[", "\"qas\"", "]", ":", "\n", "                    ", "qas_id", "=", "qa", "[", "\"id\"", "]", "\n", "question", "=", "qa", "[", "\"question\"", "]", ".", "strip", "(", ")", "\n", "answer_starts", "=", "[", "]", "\n", "answers", "=", "[", "]", "\n", "is_impossible", "=", "False", "\n", "\n", "if", "\"is_impossible\"", "in", "qa", ".", "keys", "(", ")", ":", "\n", "                        ", "is_impossible", "=", "qa", "[", "\"is_impossible\"", "]", "\n", "\n", "", "answer_starts", "=", "[", "\n", "answer", "[", "\"answer_start\"", "]", "for", "answer", "in", "qa", ".", "get", "(", "\"answers\"", ",", "[", "]", ")", "\n", "]", "\n", "answers", "=", "[", "\n", "answer", "[", "\"text\"", "]", ".", "strip", "(", ")", "for", "answer", "in", "qa", ".", "get", "(", "\"answers\"", ",", "[", "]", ")", "\n", "]", "\n", "\n", "yield", "{", "\n", "'id'", ":", "qas_id", ",", "\n", "'title'", ":", "title", ",", "\n", "'context'", ":", "context", ",", "\n", "'question'", ":", "question", ",", "\n", "'answers'", ":", "answers", ",", "\n", "'answer_starts'", ":", "answer_starts", ",", "\n", "'is_impossible'", ":", "is_impossible", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.squad.compute_prediction_checklist": [[44, 231], ["collections.defaultdict", "enumerate", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "len", "len", "len", "features_per_example[].append", "numpy.array", "numpy.exp", "zip", "features[].get", "numpy.exp", "[].tolist", "[].tolist", "prelim_predictions.append", "numpy.argmax", "all_cls_predictions.append", "sorted", "predictions.append", "pred.pop", "predictions.insert", "np.exp.sum", "np.exp.sum", "numpy.array", "any", "len", "pred.pop", "numpy.max", "numpy.max", "prelim_predictions.append", "len", "isinstance", "float", "pred.items", "numpy.argsort", "numpy.argsort", "len", "len", "features[].get.get", "str"], "function", ["None"], ["", "", "", "", "", "def", "compute_prediction_checklist", "(", "examples", ",", "\n", "features", ",", "\n", "predictions", ",", "\n", "version_2_with_negative", ":", "bool", "=", "False", ",", "\n", "n_best_size", ":", "int", "=", "20", ",", "\n", "max_answer_length", ":", "int", "=", "30", ",", "\n", "cls_threshold", ":", "float", "=", "0.5", ")", ":", "\n", "    ", "\"\"\"\n    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\n    original contexts. This is the base postprocessing functions for models that only return start and end logits.\n\n    Args:\n        examples: The non-preprocessed dataset (see the main script for more information).\n        features: The processed dataset (see the main script for more information).\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n            first dimension must match the number of elements of :obj:`features`.\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n            Whether or not the underlying dataset contains examples with no answers.\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\n            The total number of n-best predictions to generate when looking for an answer.\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\n            are not conditioned on one another.\n        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\n            The threshold used to select the null answer: if the best answer has a score that is less than the score of\n            the null answer minus this threshold, the null answer is selected for this example (note that the score of\n            the null answer for an example giving several features is the minimum of the scores for the null answer on\n            each feature: all features must be aligned on the fact they `want` to predict a null answer).\n\n            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\n    \"\"\"", "\n", "assert", "len", "(", "\n", "predictions", "\n", ")", "==", "3", ",", "\"`predictions` should be a tuple with two elements (start_logits, end_logits, cls_logits).\"", "\n", "all_start_logits", ",", "all_end_logits", ",", "all_cls_logits", "=", "predictions", "\n", "\n", "assert", "len", "(", "predictions", "[", "0", "]", ")", "==", "len", "(", "\n", "features", ")", ",", "\"Number of predictions should be equal to number of features.\"", "\n", "\n", "# Build a map example to its corresponding features.", "\n", "features_per_example", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "i", ",", "feature", "in", "enumerate", "(", "features", ")", ":", "\n", "        ", "features_per_example", "[", "feature", "[", "\"example_id\"", "]", "]", ".", "append", "(", "\n", "i", ")", "\n", "\n", "# The dictionaries we have to fill.", "\n", "", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_nbest_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_cls_predictions", "=", "[", "]", "\n", "\n", "# Let's loop over all the examples!", "\n", "for", "example_index", ",", "example", "in", "enumerate", "(", "examples", ")", ":", "\n", "# Those are the indices of the features associated to the current example.", "\n", "        ", "feature_indices", "=", "features_per_example", "[", "example", "[", "'id'", "]", "]", "\n", "\n", "min_null_prediction", "=", "None", "\n", "prelim_predictions", "=", "[", "]", "\n", "score_answerable", "=", "-", "1", "\n", "# Looping through all the features associated to the current example.", "\n", "for", "feature_index", "in", "feature_indices", ":", "\n", "# We grab the predictions of the model for this feature.", "\n", "            ", "start_logits", "=", "all_start_logits", "[", "feature_index", "]", "\n", "end_logits", "=", "all_end_logits", "[", "feature_index", "]", "\n", "cls_logits", "=", "all_cls_logits", "[", "feature_index", "]", "\n", "# This is what will allow us to map some the positions in our logits to span of texts in the original", "\n", "# context.", "\n", "offset_mapping", "=", "features", "[", "feature_index", "]", "[", "\"offset_mapping\"", "]", "\n", "# Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context", "\n", "# available in the current feature.", "\n", "token_is_max_context", "=", "features", "[", "feature_index", "]", ".", "get", "(", "\n", "\"token_is_max_context\"", ",", "None", ")", "\n", "\n", "# Update minimum null prediction.", "\n", "feature_null_score", "=", "start_logits", "[", "0", "]", "+", "end_logits", "[", "0", "]", "\n", "exp_answerable_scores", "=", "np", ".", "exp", "(", "cls_logits", "-", "np", ".", "max", "(", "cls_logits", ")", ")", "\n", "feature_answerable_score", "=", "exp_answerable_scores", "/", "exp_answerable_scores", ".", "sum", "(", ")", "\n", "if", "feature_answerable_score", "[", "-", "1", "]", ">", "score_answerable", ":", "\n", "                ", "score_answerable", "=", "feature_answerable_score", "[", "-", "1", "]", "\n", "answerable_probs", "=", "feature_answerable_score", "\n", "", "if", "min_null_prediction", "is", "None", "or", "min_null_prediction", "[", "\n", "\"score\"", "]", ">", "feature_null_score", ":", "\n", "                ", "min_null_prediction", "=", "{", "\n", "\"offsets\"", ":", "(", "0", ",", "0", ")", ",", "\n", "\"score\"", ":", "feature_null_score", ",", "\n", "\"start_logit\"", ":", "start_logits", "[", "0", "]", ",", "\n", "\"end_logit\"", ":", "end_logits", "[", "0", "]", ",", "\n", "}", "\n", "# Go through all possibilities for the `n_best_size` greater start and end logits.", "\n", "", "start_indexes", "=", "np", ".", "argsort", "(", "start_logits", ")", "[", "-", "1", ":", "-", "n_best_size", "-", "1", ":", "\n", "-", "1", "]", ".", "tolist", "(", ")", "\n", "end_indexes", "=", "np", ".", "argsort", "(", "end_logits", ")", "[", "-", "1", ":", "-", "n_best_size", "-", "1", ":", "-", "1", "]", ".", "tolist", "(", "\n", ")", "\n", "for", "start_index", "in", "start_indexes", ":", "\n", "                ", "for", "end_index", "in", "end_indexes", ":", "\n", "# Don't consider out-of-scope answers, either because the indices are out of bounds or correspond", "\n", "# to part of the input_ids that are not in the context.", "\n", "                    ", "if", "(", "start_index", ">=", "len", "(", "offset_mapping", ")", "or", "\n", "end_index", ">=", "len", "(", "offset_mapping", ")", "or", "\n", "offset_mapping", "[", "start_index", "]", "is", "None", "or", "\n", "offset_mapping", "[", "end_index", "]", "is", "None", "or", "\n", "offset_mapping", "[", "start_index", "]", "==", "(", "0", ",", "0", ")", "or", "\n", "offset_mapping", "[", "end_index", "]", "==", "(", "0", ",", "0", ")", ")", ":", "\n", "                        ", "continue", "\n", "# Don't consider answers with a length that is either < 0 or > max_answer_length.", "\n", "", "if", "end_index", "<", "start_index", "or", "end_index", "-", "start_index", "+", "1", ">", "max_answer_length", ":", "\n", "                        ", "continue", "\n", "# Don't consider answer that don't have the maximum context available (if such information is", "\n", "# provided).", "\n", "", "if", "token_is_max_context", "is", "not", "None", "and", "not", "token_is_max_context", ".", "get", "(", "\n", "str", "(", "start_index", ")", ",", "False", ")", ":", "\n", "                        ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "{", "\n", "\"offsets\"", ":", "(", "offset_mapping", "[", "start_index", "]", "[", "0", "]", ",", "\n", "offset_mapping", "[", "end_index", "]", "[", "1", "]", ")", ",", "\n", "\"score\"", ":", "\n", "start_logits", "[", "start_index", "]", "+", "end_logits", "[", "end_index", "]", ",", "\n", "\"start_logit\"", ":", "start_logits", "[", "start_index", "]", ",", "\n", "\"end_logit\"", ":", "end_logits", "[", "end_index", "]", ",", "\n", "}", ")", "\n", "", "", "", "if", "version_2_with_negative", ":", "\n", "# Add the minimum null prediction", "\n", "            ", "prelim_predictions", ".", "append", "(", "min_null_prediction", ")", "\n", "pred_cls_label", "=", "np", ".", "argmax", "(", "np", ".", "array", "(", "answerable_probs", ")", ")", "\n", "all_cls_predictions", ".", "append", "(", "[", "example", "[", "'id'", "]", ",", "pred_cls_label", ",", "answerable_probs", "[", "0", "]", ",", "answerable_probs", "[", "1", "]", "]", ")", "\n", "\n", "\n", "# Only keep the best `n_best_size` predictions.", "\n", "", "predictions", "=", "sorted", "(", "\n", "prelim_predictions", ",", "key", "=", "lambda", "x", ":", "x", "[", "\"score\"", "]", ",", "\n", "reverse", "=", "True", ")", "[", ":", "n_best_size", "]", "\n", "\n", "# Add back the minimum null prediction if it was removed because of its low score.", "\n", "if", "version_2_with_negative", "and", "not", "any", "(", "p", "[", "\"offsets\"", "]", "==", "(", "0", ",", "0", ")", "\n", "for", "p", "in", "predictions", ")", ":", "\n", "            ", "predictions", ".", "append", "(", "min_null_prediction", ")", "\n", "\n", "# Use the offsets to gather the answer text in the original context.", "\n", "", "context", "=", "example", "[", "\"context\"", "]", "\n", "for", "pred", "in", "predictions", ":", "\n", "            ", "offsets", "=", "pred", ".", "pop", "(", "\"offsets\"", ")", "\n", "pred", "[", "\"text\"", "]", "=", "context", "[", "offsets", "[", "0", "]", ":", "offsets", "[", "1", "]", "]", "if", "context", "[", "offsets", "[", "0", "]", ":", "offsets", "[", "1", "]", "]", "!=", "\"\"", "else", "\"no answer\"", "\n", "\n", "# In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid", "\n", "# failure.", "\n", "", "if", "len", "(", "predictions", ")", "==", "0", "or", "(", "len", "(", "predictions", ")", "==", "1", "and", "\n", "predictions", "[", "0", "]", "[", "\"text\"", "]", "==", "\"no answer\"", ")", ":", "\n", "            ", "predictions", ".", "insert", "(", "0", ",", "{", "\n", "\"text\"", ":", "\"no answer\"", ",", "\n", "\"start_logit\"", ":", "0.0", ",", "\n", "\"end_logit\"", ":", "0.0", ",", "\n", "\"score\"", ":", "0.0", "\n", "}", ")", "\n", "\n", "# Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using", "\n", "# the LogSumExp trick).", "\n", "", "scores", "=", "np", ".", "array", "(", "[", "pred", ".", "pop", "(", "\"score\"", ")", "for", "pred", "in", "predictions", "]", ")", "\n", "exp_scores", "=", "np", ".", "exp", "(", "scores", "-", "np", ".", "max", "(", "scores", ")", ")", "\n", "probs", "=", "exp_scores", "/", "exp_scores", ".", "sum", "(", ")", "\n", "\n", "# Include the probabilities in our predictions.", "\n", "for", "prob", ",", "pred", "in", "zip", "(", "probs", ",", "predictions", ")", ":", "\n", "            ", "pred", "[", "\"probability\"", "]", "=", "prob", "\n", "\n", "# Pick the best prediction. If the null answer is not possible, this is easy.", "\n", "", "if", "not", "version_2_with_negative", ":", "\n", "            ", "all_predictions", "[", "example", "[", "\"id\"", "]", "]", "=", "predictions", "[", "0", "]", "[", "\"text\"", "]", "\n", "", "else", ":", "\n", "# Otherwise we first need to find the best non-empty prediction.", "\n", "            ", "i", "=", "0", "\n", "while", "predictions", "[", "i", "]", "[", "\"text\"", "]", "==", "\"no answer\"", ":", "\n", "                ", "i", "+=", "1", "\n", "", "best_non_null_pred", "=", "predictions", "[", "i", "]", "\n", "\n", "if", "answerable_probs", "[", "1", "]", "<", "cls_threshold", ":", "\n", "                ", "all_predictions", "[", "example", "[", "'id'", "]", "]", "=", "\"no answer\"", "\n", "", "else", ":", "\n", "                ", "all_predictions", "[", "example", "[", "'id'", "]", "]", "=", "best_non_null_pred", "[", "'text'", "]", "\n", "\n", "# Make `predictions` JSON-serializable by casting np.float back to float.", "\n", "", "", "all_nbest_json", "[", "example", "[", "\"id\"", "]", "]", "=", "[", "{", "\n", "k", ":", "(", "float", "(", "v", ")", "\n", "if", "isinstance", "(", "v", ",", "(", "np", ".", "float16", ",", "np", ".", "float32", ",", "np", ".", "float64", ")", ")", "else", "v", ")", "\n", "for", "k", ",", "v", "in", "pred", ".", "items", "(", ")", "\n", "}", "for", "pred", "in", "predictions", "]", "\n", "\n", "", "return", "all_predictions", ",", "all_nbest_json", ",", "all_cls_predictions", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad_twomemory.create_model": [[118, 262], ["model.bert.BertModel", "model.bert.BertModel.get_sequence_output", "logger.info", "paddle.layers.embedding", "paddle.layers.embedding", "paddle.layers.equal", "paddle.layers.cast", "paddle.layers.scale", "paddle.layers.reduce_sum", "paddle.layers.equal", "paddle.layers.cast", "paddle.layers.scale", "paddle.layers.reduce_sum", "model.layers.MemoryLayer", "model.layers.MemoryLayer.forward", "model.layers.MemoryLayer", "model.layers.MemoryLayer.forward", "paddle.layers.concat", "logger.info", "model.layers.TriLinearTwoTimeSelfAttentionLayer", "model.layers.TriLinearTwoTimeSelfAttentionLayer.forward", "paddle.layers.fc", "paddle.layers.transpose", "paddle.layers.unstack", "paddle.layers.fill_constant_batch_size_like", "paddle.layers.reduce_sum", "paddle.layers.py_reader", "paddle.layers.read_file", "paddle.layers.py_reader", "paddle.layers.read_file", "paddle.layers.fill_constant", "paddle.layers.elementwise_sub", "paddle.layers.fill_constant", "paddle.layers.elementwise_sub", "run_squad_twomemory.create_model.compute_loss"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.get_sequence_output", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.forward", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.forward", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.forward", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.read_file", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.read_file"], ["def", "create_model", "(", "pyreader_name", ",", "bert_config", ",", "max_wn_concept_length", ",", "max_nell_concept_length", ",", "wn_concept_embedding_mat", ",", "nell_concept_embedding_mat", ",", "is_training", "=", "False", ",", "freeze", "=", "False", ")", ":", "\n", "    ", "if", "is_training", ":", "\n", "        ", "pyreader", "=", "fluid", ".", "layers", ".", "py_reader", "(", "\n", "capacity", "=", "50", ",", "\n", "shapes", "=", "[", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "max_wn_concept_length", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "max_nell_concept_length", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "1", "]", ",", "[", "-", "1", ",", "1", "]", "]", ",", "\n", "dtypes", "=", "[", "\n", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'float32'", ",", "'int64'", ",", "'int64'", "]", ",", "\n", "lod_levels", "=", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "name", "=", "pyreader_name", ",", "\n", "use_double_buffer", "=", "True", ")", "\n", "(", "src_ids", ",", "pos_ids", ",", "sent_ids", ",", "wn_concept_ids", ",", "nell_concept_ids", ",", "input_mask", ",", "start_positions", ",", "\n", "end_positions", ")", "=", "fluid", ".", "layers", ".", "read_file", "(", "pyreader", ")", "\n", "", "else", ":", "\n", "        ", "pyreader", "=", "fluid", ".", "layers", ".", "py_reader", "(", "\n", "capacity", "=", "50", ",", "\n", "shapes", "=", "[", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "max_wn_concept_length", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "max_nell_concept_length", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "1", "]", "]", ",", "\n", "dtypes", "=", "[", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'float32'", ",", "'int64'", "]", ",", "\n", "lod_levels", "=", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "name", "=", "pyreader_name", ",", "\n", "use_double_buffer", "=", "True", ")", "\n", "(", "src_ids", ",", "pos_ids", ",", "sent_ids", ",", "wn_concept_ids", ",", "nell_concept_ids", ",", "input_mask", ",", "unique_id", ")", "=", "fluid", ".", "layers", ".", "read_file", "(", "pyreader", ")", "\n", "\n", "", "'''1st Layer: BERT Layer'''", "\n", "bert", "=", "BertModel", "(", "\n", "src_ids", "=", "src_ids", ",", "\n", "position_ids", "=", "pos_ids", ",", "\n", "sentence_ids", "=", "sent_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "config", "=", "bert_config", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "\n", "enc_out", "=", "bert", ".", "get_sequence_output", "(", ")", "\n", "if", "freeze", ":", "\n", "        ", "enc_out", ".", "stop_gradient", "=", "True", "\n", "", "logger", ".", "info", "(", "\"enc_out.stop_gradient: {}\"", ".", "format", "(", "enc_out", ".", "stop_gradient", ")", ")", "\n", "\n", "'''2nd layer: Memory Layer'''", "\n", "# get memory embedding", "\n", "wn_concept_vocab_size", "=", "wn_concept_embedding_mat", ".", "shape", "[", "0", "]", "\n", "wn_concept_dim", "=", "wn_concept_embedding_mat", ".", "shape", "[", "1", "]", "\n", "nell_concept_vocab_size", "=", "nell_concept_embedding_mat", ".", "shape", "[", "0", "]", "\n", "nell_concept_dim", "=", "nell_concept_embedding_mat", ".", "shape", "[", "1", "]", "\n", "wn_memory_embs", "=", "fluid", ".", "layers", ".", "embedding", "(", "wn_concept_ids", ",", "\n", "size", "=", "(", "wn_concept_vocab_size", ",", "wn_concept_dim", ")", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "\"wn_concept_emb_mat\"", ",", "\n", "do_model_average", "=", "False", ",", "\n", "trainable", "=", "False", ")", ",", "\n", "dtype", "=", "'float32'", ")", "\n", "nell_memory_embs", "=", "fluid", ".", "layers", ".", "embedding", "(", "nell_concept_ids", ",", "\n", "size", "=", "(", "nell_concept_vocab_size", ",", "nell_concept_dim", ")", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "\"nell_concept_emb_mat\"", ",", "\n", "do_model_average", "=", "False", ",", "\n", "trainable", "=", "False", ")", ",", "\n", "dtype", "=", "'float32'", ")", "\n", "\n", "# get memory length", "\n", "wn_concept_ids_reduced", "=", "fluid", ".", "layers", ".", "equal", "(", "wn_concept_ids", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "shape", "=", "[", "1", "]", ",", "value", "=", "0", ",", "dtype", "=", "\"int64\"", ")", ")", "# [batch_size, sent_size, concept_size, 1]", "\n", "wn_concept_ids_reduced", "=", "fluid", ".", "layers", ".", "cast", "(", "wn_concept_ids_reduced", ",", "dtype", "=", "\"float32\"", ")", "# [batch_size, sent_size, concept_size, 1]", "\n", "wn_concept_ids_reduced", "=", "fluid", ".", "layers", ".", "scale", "(", "\n", "fluid", ".", "layers", ".", "elementwise_sub", "(", "\n", "wn_concept_ids_reduced", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "[", "1", "]", ",", "\"float32\"", ",", "1", ")", "\n", ")", ",", "\n", "scale", "=", "-", "1", "\n", ")", "\n", "wn_mem_length", "=", "fluid", ".", "layers", ".", "reduce_sum", "(", "wn_concept_ids_reduced", ",", "dim", "=", "2", ")", "# [batch_size, sent_size, 1]    ", "\n", "\n", "nell_concept_ids_reduced", "=", "fluid", ".", "layers", ".", "equal", "(", "nell_concept_ids", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "shape", "=", "[", "1", "]", ",", "value", "=", "0", ",", "dtype", "=", "\"int64\"", ")", ")", "# [batch_size, sent_size, concept_size, 1]", "\n", "nell_concept_ids_reduced", "=", "fluid", ".", "layers", ".", "cast", "(", "nell_concept_ids_reduced", ",", "dtype", "=", "\"float32\"", ")", "# [batch_size, sent_size, concept_size, 1]", "\n", "nell_concept_ids_reduced", "=", "fluid", ".", "layers", ".", "scale", "(", "\n", "fluid", ".", "layers", ".", "elementwise_sub", "(", "\n", "nell_concept_ids_reduced", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "[", "1", "]", ",", "\"float32\"", ",", "1", ")", "\n", ")", ",", "\n", "scale", "=", "-", "1", "\n", ")", "\n", "nell_mem_length", "=", "fluid", ".", "layers", ".", "reduce_sum", "(", "nell_concept_ids_reduced", ",", "dim", "=", "2", ")", "# [batch_size, sent_size, 1]   ", "\n", "\n", "# select and integrate", "\n", "wn_memory_layer", "=", "MemoryLayer", "(", "bert_config", ",", "max_wn_concept_length", ",", "wn_concept_dim", ",", "mem_method", "=", "'raw'", ",", "prefix", "=", "'wn'", ")", "\n", "wn_memory_output", "=", "wn_memory_layer", ".", "forward", "(", "enc_out", ",", "wn_memory_embs", ",", "wn_mem_length", ",", "ignore_no_memory_token", "=", "True", ")", "\n", "\n", "nell_memory_layer", "=", "MemoryLayer", "(", "bert_config", ",", "max_nell_concept_length", ",", "nell_concept_dim", ",", "mem_method", "=", "'raw'", ",", "prefix", "=", "'nell'", ")", "\n", "nell_memory_output", "=", "nell_memory_layer", ".", "forward", "(", "enc_out", ",", "nell_memory_embs", ",", "nell_mem_length", ",", "ignore_no_memory_token", "=", "True", ")", "\n", "\n", "memory_output", "=", "fluid", ".", "layers", ".", "concat", "(", "[", "enc_out", ",", "wn_memory_output", ",", "nell_memory_output", "]", ",", "axis", "=", "2", ")", "\n", "\n", "'''3rd layer: Self-Matching Layer'''", "\n", "# calculate input dim for self-matching layer", "\n", "memory_output_size", "=", "bert_config", "[", "'hidden_size'", "]", "+", "wn_concept_dim", "+", "nell_concept_dim", "\n", "logger", ".", "info", "(", "\"memory_output_size: {}\"", ".", "format", "(", "memory_output_size", ")", ")", "\n", "\n", "# do matching", "\n", "self_att_layer", "=", "TriLinearTwoTimeSelfAttentionLayer", "(", "\n", "memory_output_size", ",", "dropout_rate", "=", "0.0", ",", "\n", "cat_mul", "=", "True", ",", "cat_sub", "=", "True", ",", "cat_twotime", "=", "True", ",", "\n", "cat_twotime_mul", "=", "False", ",", "cat_twotime_sub", "=", "True", ")", "# [bs, sq, concat_hs]", "\n", "att_output", "=", "self_att_layer", ".", "forward", "(", "memory_output", ",", "input_mask", ")", "# [bs, sq, concat_hs]", "\n", "\n", "'''4th layer: Output Layer'''", "\n", "logits", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "att_output", ",", "\n", "size", "=", "2", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"cls_squad_out_w\"", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "NormalInitializer", "(", "loc", "=", "0.0", ",", "scale", "=", "bert_config", "[", "'initializer_range'", "]", ")", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"cls_squad_out_b\"", ",", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", ")", "\n", "\n", "logits", "=", "fluid", ".", "layers", ".", "transpose", "(", "x", "=", "logits", ",", "perm", "=", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "start_logits", ",", "end_logits", "=", "fluid", ".", "layers", ".", "unstack", "(", "x", "=", "logits", ",", "axis", "=", "0", ")", "\n", "\n", "batch_ones", "=", "fluid", ".", "layers", ".", "fill_constant_batch_size_like", "(", "\n", "input", "=", "start_logits", ",", "dtype", "=", "'int64'", ",", "shape", "=", "[", "1", "]", ",", "value", "=", "1", ")", "\n", "num_seqs", "=", "fluid", ".", "layers", ".", "reduce_sum", "(", "input", "=", "batch_ones", ")", "\n", "\n", "if", "is_training", ":", "\n", "\n", "        ", "def", "compute_loss", "(", "logits", ",", "positions", ")", ":", "\n", "            ", "loss", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "logits", ",", "label", "=", "positions", ")", "\n", "loss", "=", "fluid", ".", "layers", ".", "mean", "(", "x", "=", "loss", ")", "\n", "return", "loss", "\n", "\n", "", "start_loss", "=", "compute_loss", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "compute_loss", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2.0", "\n", "if", "args", ".", "use_fp16", "and", "args", ".", "loss_scaling", ">", "1.0", ":", "\n", "            ", "total_loss", "=", "total_loss", "*", "args", ".", "loss_scaling", "\n", "\n", "", "return", "pyreader", ",", "total_loss", ",", "num_seqs", "\n", "", "else", ":", "\n", "        ", "return", "pyreader", ",", "unique_id", ",", "start_logits", ",", "end_logits", ",", "num_seqs", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad_twomemory.predict": [[268, 308], ["os.path.join", "os.path.join", "os.path.join", "os.path.join", "test_pyreader.start", "time.time", "time.time", "processor.get_features", "reader.squad_twomemory.write_predictions", "os.path.exists", "os.makedirs", "test_exe.run", "range", "int", "all_results.append", "test_pyreader.reset", "logger.info", "float", "float", "RawResult", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_features", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.write_predictions", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["def", "predict", "(", "test_exe", ",", "test_program", ",", "test_pyreader", ",", "fetch_list", ",", "processor", ",", "eval_concept_settings", ",", "eval_output_name", "=", "'eval_result.json'", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "checkpoints", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "checkpoints", ")", "\n", "", "output_prediction_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\"predictions.json\"", ")", "\n", "output_nbest_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\"nbest_predictions.json\"", ")", "\n", "output_null_log_odds_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\"null_odds.json\"", ")", "\n", "output_evaluation_result_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "eval_output_name", ")", "\n", "\n", "test_pyreader", ".", "start", "(", ")", "\n", "all_results", "=", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "np_unique_ids", ",", "np_start_logits", ",", "np_end_logits", ",", "np_num_seqs", "=", "test_exe", ".", "run", "(", "\n", "fetch_list", "=", "fetch_list", ",", "program", "=", "test_program", ")", "\n", "for", "idx", "in", "range", "(", "np_unique_ids", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "if", "len", "(", "all_results", ")", "%", "1000", "==", "0", ":", "\n", "                    ", "logger", ".", "info", "(", "\"Processing example: %d\"", "%", "len", "(", "all_results", ")", ")", "\n", "", "unique_id", "=", "int", "(", "np_unique_ids", "[", "idx", "]", ")", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "np_start_logits", "[", "idx", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "np_end_logits", "[", "idx", "]", ".", "flat", "]", "\n", "all_results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "", "", "except", "fluid", ".", "core", ".", "EOFException", ":", "\n", "            ", "test_pyreader", ".", "reset", "(", ")", "\n", "break", "\n", "", "", "time_end", "=", "time", ".", "time", "(", ")", "\n", "\n", "features", "=", "processor", ".", "get_features", "(", "\n", "processor", ".", "predict_examples", ",", "is_training", "=", "False", ",", "**", "eval_concept_settings", ")", "\n", "eval_result", "=", "write_predictions", "(", "processor", ".", "predict_examples", ",", "features", ",", "all_results", ",", "\n", "args", ".", "n_best_size", ",", "args", ".", "max_answer_length", ",", "\n", "args", ".", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "output_null_log_odds_file", ",", "\n", "args", ".", "version_2_with_negative", ",", "\n", "args", ".", "null_score_diff_threshold", ",", "args", ".", "verbose", ",", "args", ".", "predict_file", ",", "output_evaluation_result_file", ")", "\n", "return", "eval_result", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad_twomemory.read_concept_embedding": [[309, 329], ["open", "len", "len", "id2concept.append", "np.array.append", "numpy.array", "line.strip", "np.array.append", "len", "id2concept.append", "info[].split", "line.split", "float", "range", "len", "numpy.any", "line.split", "numpy.isnan"], "function", ["None"], ["", "def", "read_concept_embedding", "(", "embedding_path", ")", ":", "\n", "    ", "fin", "=", "open", "(", "embedding_path", ",", "encoding", "=", "'utf-8'", ")", "\n", "info", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "fin", "]", "\n", "dim", "=", "len", "(", "info", "[", "0", "]", ".", "split", "(", "' '", ")", "[", "1", ":", "]", ")", "\n", "n_concept", "=", "len", "(", "info", ")", "\n", "embedding_mat", "=", "[", "]", "\n", "id2concept", ",", "concept2id", "=", "[", "]", ",", "{", "}", "\n", "# add padding concept into vocab", "\n", "id2concept", ".", "append", "(", "'<pad_concept>'", ")", "\n", "concept2id", "[", "'<pad_concept>'", "]", "=", "0", "\n", "embedding_mat", ".", "append", "(", "[", "0.0", "for", "_", "in", "range", "(", "dim", ")", "]", ")", "\n", "for", "line", "in", "info", ":", "\n", "        ", "concept_name", "=", "line", ".", "split", "(", "' '", ")", "[", "0", "]", "\n", "embedding", "=", "[", "float", "(", "value_str", ")", "for", "value_str", "in", "line", ".", "split", "(", "' '", ")", "[", "1", ":", "]", "]", "\n", "assert", "len", "(", "embedding", ")", "==", "dim", "and", "not", "np", ".", "any", "(", "np", ".", "isnan", "(", "embedding", ")", ")", "\n", "embedding_mat", ".", "append", "(", "embedding", ")", "\n", "concept2id", "[", "concept_name", "]", "=", "len", "(", "id2concept", ")", "\n", "id2concept", ".", "append", "(", "concept_name", ")", "\n", "", "embedding_mat", "=", "np", ".", "array", "(", "embedding_mat", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "return", "id2concept", ",", "concept2id", ",", "embedding_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad_twomemory.train": [[330, 622], ["model.bert.BertConfig", "model.bert.BertConfig.print_config", "paddle.Executor", "run_squad_twomemory.read_concept_embedding", "run_squad_twomemory.read_concept_embedding", "reader.squad_twomemory.DataProcessor", "paddle.Program", "fluid.Executor.run", "ValueError", "paddle.CUDAPlace", "paddle.core.get_cuda_device_count", "paddle.CPUPlace", "int", "random.seed", "numpy.random.seed", "reader.squad_twomemory.DataProcessor.data_generator", "reader.squad_twomemory.DataProcessor.get_num_examples", "int", "logger.info", "logger.info", "logger.info", "logger.info", "paddle.Program", "reader.squad_twomemory.DataProcessor.data_generator", "paddle.Program", "test_prog.clone.clone", "logger.info", "paddle.global_scope().find_var().get_tensor().set", "paddle.global_scope().find_var().get_tensor().set", "paddle.ExecutionStrategy", "paddle.ParallelExecutor", "train_pyreader.decorate_tensor_provider", "train_pyreader.start", "time.time", "test_pyreader.decorate_tensor_provider", "logger.info", "os.environ.get", "paddle.program_guard", "logger.info", "paddle.program_guard", "logger.info", "utils.init.init_checkpoint", "utils.init.init_checkpoint", "run_squad_twomemory.predict", "multiprocessing.cpu_count", "paddle.unique_name.guard", "run_squad_twomemory.create_model", "optimization.optimization", "paddle.memory_optimize", "paddle.contrib.memory_usage", "paddle.contrib.memory_usage", "paddle.unique_name.guard", "run_squad_twomemory.create_model", "paddle.memory_optimize", "paddle.global_scope().find_var().get_tensor", "paddle.global_scope().find_var().get_tensor", "utils.init.init_pretraining_params", "ValueError", "fluid.ParallelExecutor.run", "fluid.optimizer.ExponentialMovingAverage.apply", "run_squad_twomemory.predict", "paddle.optimizer.ExponentialMovingAverage", "fluid.optimizer.ExponentialMovingAverage.update", "paddle.optimizer.ExponentialMovingAverage", "total_cost.extend", "total_num_seqs.extend", "time.time", "reader.squad_twomemory.DataProcessor.get_train_progress", "logger.info", "time.time", "os.path.join", "paddle.io.save_persistables", "os.path.join", "paddle.io.save_persistables", "train_pyreader.reset", "dir", "paddle.global_scope().find_var", "paddle.global_scope().find_var", "logger.info", "test_pyreader.decorate_tensor_provider", "run_squad_twomemory.predict", "logger.info", "train_pyreader.queue.size", "str", "reader.squad_twomemory.DataProcessor.data_generator", "paddle.global_scope", "paddle.global_scope", "str", "numpy.sum", "numpy.sum"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig.print_config", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad.read_concept_embedding", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad.read_concept_embedding", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_num_examples", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_checkpoint", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_checkpoint", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.create_model", "home.repos.pwc.inspect_result.baidu_DuReader.src.optimization.optimization", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.create_model", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_pretraining_params", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_train_progress", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator"], ["", "def", "train", "(", "args", ")", ":", "\n", "    ", "bert_config", "=", "BertConfig", "(", "args", ".", "bert_config_path", ")", "\n", "bert_config", ".", "print_config", "(", ")", "\n", "\n", "if", "not", "(", "args", ".", "do_train", "or", "args", ".", "do_predict", "or", "args", ".", "do_val", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"For args `do_train` and `do_predict`, at \"", "\n", "\"least one of them must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "use_cuda", ":", "\n", "        ", "place", "=", "fluid", ".", "CUDAPlace", "(", "0", ")", "\n", "dev_count", "=", "fluid", ".", "core", ".", "get_cuda_device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "place", "=", "fluid", ".", "CPUPlace", "(", ")", "\n", "dev_count", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'CPU_NUM'", ",", "multiprocessing", ".", "cpu_count", "(", ")", ")", ")", "\n", "", "exe", "=", "fluid", ".", "Executor", "(", "place", ")", "\n", "\n", "wn_id2concept", ",", "wn_concept2id", ",", "wn_concept_embedding_mat", "=", "read_concept_embedding", "(", "\n", "args", ".", "wn_concept_embedding_path", ")", "\n", "nell_id2concept", ",", "nell_concept2id", ",", "nell_concept_embedding_mat", "=", "read_concept_embedding", "(", "\n", "args", ".", "nell_concept_embedding_path", ")", "\n", "\n", "processor", "=", "DataProcessor", "(", "\n", "vocab_path", "=", "args", ".", "vocab_path", ",", "\n", "do_lower_case", "=", "args", ".", "do_lower_case", ",", "\n", "max_seq_length", "=", "args", ".", "max_seq_len", ",", "\n", "in_tokens", "=", "args", ".", "in_tokens", ",", "\n", "doc_stride", "=", "args", ".", "doc_stride", ",", "\n", "max_query_length", "=", "args", ".", "max_query_length", ")", "\n", "\n", "startup_prog", "=", "fluid", ".", "Program", "(", ")", "\n", "if", "args", ".", "random_seed", "is", "not", "None", ":", "\n", "        ", "startup_prog", ".", "random_seed", "=", "args", ".", "random_seed", "\n", "random", ".", "seed", "(", "args", ".", "random_seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "random_seed", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "train_concept_settings", "=", "{", "\n", "'tokenization_path'", ":", "'../retrieve_concepts/tokenization_squad/tokens/train.tokenization.{}.data'", ".", "format", "(", "'uncased'", "if", "args", ".", "do_lower_case", "else", "'cased'", ")", ",", "\n", "'wn_concept2id'", ":", "wn_concept2id", ",", "\n", "'nell_concept2id'", ":", "nell_concept2id", ",", "\n", "'use_wordnet'", ":", "args", ".", "use_wordnet", ",", "\n", "'retrieved_synset_path'", ":", "args", ".", "retrieved_synset_path", ",", "\n", "'use_nell'", ":", "args", ".", "use_nell", ",", "\n", "'retrieved_nell_concept_path'", ":", "args", ".", "train_retrieved_nell_concept_path", ",", "\n", "}", "\n", "train_data_generator", "=", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "train_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'train'", ",", "\n", "shuffle", "=", "True", ",", "\n", "dev_count", "=", "dev_count", ",", "\n", "version_2_with_negative", "=", "args", ".", "version_2_with_negative", ",", "\n", "epoch", "=", "args", ".", "epoch", ",", "\n", "**", "train_concept_settings", ")", "\n", "\n", "num_train_examples", "=", "processor", ".", "get_num_examples", "(", "phase", "=", "'train'", ")", "\n", "if", "args", ".", "in_tokens", ":", "\n", "            ", "max_train_steps", "=", "args", ".", "epoch", "*", "num_train_examples", "//", "(", "\n", "args", ".", "batch_size", "//", "args", ".", "max_seq_len", ")", "//", "dev_count", "\n", "", "else", ":", "\n", "            ", "max_train_steps", "=", "args", ".", "epoch", "*", "num_train_examples", "//", "(", "\n", "args", ".", "batch_size", ")", "//", "dev_count", "\n", "", "warmup_steps", "=", "int", "(", "max_train_steps", "*", "args", ".", "warmup_proportion", ")", "\n", "logger", ".", "info", "(", "\"Device count: %d\"", "%", "dev_count", ")", "\n", "logger", ".", "info", "(", "\"Num train examples: %d\"", "%", "num_train_examples", ")", "\n", "logger", ".", "info", "(", "\"Max train steps: %d\"", "%", "max_train_steps", ")", "\n", "logger", ".", "info", "(", "\"Num warmup steps: %d\"", "%", "warmup_steps", ")", "\n", "\n", "train_program", "=", "fluid", ".", "Program", "(", ")", "\n", "# if args.random_seed is not None:", "\n", "#     train_program.random_seed = args.random_seed", "\n", "with", "fluid", ".", "program_guard", "(", "train_program", ",", "startup_prog", ")", ":", "\n", "            ", "with", "fluid", ".", "unique_name", ".", "guard", "(", ")", ":", "\n", "                ", "train_pyreader", ",", "loss", ",", "num_seqs", "=", "create_model", "(", "\n", "pyreader_name", "=", "'train_reader'", ",", "\n", "bert_config", "=", "bert_config", ",", "\n", "max_wn_concept_length", "=", "processor", ".", "train_wn_max_concept_length", ",", "\n", "max_nell_concept_length", "=", "processor", ".", "train_nell_max_concept_length", ",", "\n", "wn_concept_embedding_mat", "=", "wn_concept_embedding_mat", ",", "\n", "nell_concept_embedding_mat", "=", "nell_concept_embedding_mat", ",", "\n", "is_training", "=", "True", ",", "\n", "freeze", "=", "args", ".", "freeze", ")", "\n", "\n", "scheduled_lr", "=", "optimization", "(", "\n", "loss", "=", "loss", ",", "\n", "warmup_steps", "=", "warmup_steps", ",", "\n", "num_train_steps", "=", "max_train_steps", ",", "\n", "learning_rate", "=", "args", ".", "learning_rate", ",", "\n", "train_program", "=", "train_program", ",", "\n", "startup_prog", "=", "startup_prog", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", "scheduler", "=", "args", ".", "lr_scheduler", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ",", "\n", "loss_scaling", "=", "args", ".", "loss_scaling", ")", "\n", "\n", "if", "args", ".", "use_ema", ":", "\n", "                    ", "ema", "=", "fluid", ".", "optimizer", ".", "ExponentialMovingAverage", "(", "args", ".", "ema_decay", ")", "\n", "ema", ".", "update", "(", ")", "\n", "\n", "", "fluid", ".", "memory_optimize", "(", "train_program", ",", "skip_opt_set", "=", "[", "loss", ".", "name", ",", "num_seqs", ".", "name", "]", ")", "\n", "\n", "", "", "if", "args", ".", "verbose", ":", "\n", "            ", "if", "args", ".", "in_tokens", ":", "\n", "                ", "lower_mem", ",", "upper_mem", ",", "unit", "=", "fluid", ".", "contrib", ".", "memory_usage", "(", "\n", "program", "=", "train_program", ",", "\n", "batch_size", "=", "args", ".", "batch_size", "//", "args", ".", "max_seq_len", ")", "\n", "", "else", ":", "\n", "                ", "lower_mem", ",", "upper_mem", ",", "unit", "=", "fluid", ".", "contrib", ".", "memory_usage", "(", "\n", "program", "=", "train_program", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "", "logger", ".", "info", "(", "\"Theoretical memory usage in training:  %.3f - %.3f %s\"", "%", "\n", "(", "lower_mem", ",", "upper_mem", ",", "unit", ")", ")", "\n", "\n", "", "", "if", "args", ".", "do_predict", "or", "args", ".", "do_val", ":", "\n", "        ", "eval_concept_settings", "=", "{", "\n", "'tokenization_path'", ":", "'../retrieve_concepts/tokenization_squad/tokens/dev.tokenization.{}.data'", ".", "format", "(", "'uncased'", "if", "args", ".", "do_lower_case", "else", "'cased'", ")", ",", "\n", "'wn_concept2id'", ":", "wn_concept2id", ",", "\n", "'nell_concept2id'", ":", "nell_concept2id", ",", "\n", "'use_wordnet'", ":", "args", ".", "use_wordnet", ",", "\n", "'retrieved_synset_path'", ":", "args", ".", "retrieved_synset_path", ",", "\n", "'use_nell'", ":", "args", ".", "use_nell", ",", "\n", "'retrieved_nell_concept_path'", ":", "args", ".", "dev_retrieved_nell_concept_path", ",", "\n", "}", "\n", "eval_data_generator", "=", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "predict_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'predict'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "epoch", "=", "1", ",", "\n", "**", "eval_concept_settings", ")", "\n", "\n", "test_prog", "=", "fluid", ".", "Program", "(", ")", "\n", "# if args.random_seed is not None:", "\n", "#     test_prog.random_seed = args.random_seed", "\n", "with", "fluid", ".", "program_guard", "(", "test_prog", ",", "startup_prog", ")", ":", "\n", "            ", "with", "fluid", ".", "unique_name", ".", "guard", "(", ")", ":", "\n", "                ", "test_pyreader", ",", "unique_ids", ",", "start_logits", ",", "end_logits", ",", "num_seqs", "=", "create_model", "(", "\n", "pyreader_name", "=", "'test_reader'", ",", "\n", "bert_config", "=", "bert_config", ",", "\n", "max_wn_concept_length", "=", "processor", ".", "predict_wn_max_concept_length", ",", "\n", "max_nell_concept_length", "=", "processor", ".", "predict_nell_max_concept_length", ",", "\n", "wn_concept_embedding_mat", "=", "wn_concept_embedding_mat", ",", "\n", "nell_concept_embedding_mat", "=", "nell_concept_embedding_mat", ",", "\n", "is_training", "=", "False", ")", "\n", "\n", "if", "args", ".", "use_ema", "and", "'ema'", "not", "in", "dir", "(", ")", ":", "\n", "                    ", "ema", "=", "fluid", ".", "optimizer", ".", "ExponentialMovingAverage", "(", "args", ".", "ema_decay", ")", "\n", "\n", "", "fluid", ".", "memory_optimize", "(", "test_prog", ",", "skip_opt_set", "=", "[", "unique_ids", ".", "name", ",", "\n", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "]", ")", "\n", "\n", "", "", "test_prog", "=", "test_prog", ".", "clone", "(", "for_test", "=", "True", ")", "\n", "# if args.random_seed is not None:", "\n", "#     test_prog.random_seed = args.random_seed", "\n", "\n", "", "exe", ".", "run", "(", "startup_prog", ")", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "info", "(", "'load pretrained concept embedding'", ")", "\n", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "'wn_concept_emb_mat'", ")", ".", "get_tensor", "(", ")", ".", "set", "(", "wn_concept_embedding_mat", ",", "place", ")", "\n", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "'nell_concept_emb_mat'", ")", ".", "get_tensor", "(", ")", ".", "set", "(", "nell_concept_embedding_mat", ",", "place", ")", "\n", "\n", "if", "args", ".", "init_checkpoint", "and", "args", ".", "init_pretraining_params", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"WARNING: args 'init_checkpoint' and 'init_pretraining_params' \"", "\n", "\"both are set! Only arg 'init_checkpoint' is made valid.\"", ")", "\n", "", "if", "args", ".", "init_checkpoint", ":", "\n", "            ", "init_checkpoint", "(", "\n", "exe", ",", "\n", "args", ".", "init_checkpoint", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "", "elif", "args", ".", "init_pretraining_params", ":", "\n", "            ", "init_pretraining_params", "(", "\n", "exe", ",", "\n", "args", ".", "init_pretraining_params", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "", "", "elif", "args", ".", "do_predict", "or", "args", ".", "do_val", ":", "\n", "        ", "if", "not", "args", ".", "init_checkpoint", ":", "\n", "            ", "raise", "ValueError", "(", "\"args 'init_checkpoint' should be set if\"", "\n", "\"only doing prediction!\"", ")", "\n", "", "init_checkpoint", "(", "\n", "exe", ",", "\n", "args", ".", "init_checkpoint", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "exec_strategy", "=", "fluid", ".", "ExecutionStrategy", "(", ")", "\n", "exec_strategy", ".", "use_experimental_executor", "=", "args", ".", "use_fast_executor", "\n", "exec_strategy", ".", "num_threads", "=", "dev_count", "\n", "exec_strategy", ".", "num_iteration_per_drop_scope", "=", "args", ".", "num_iteration_per_drop_scope", "\n", "\n", "train_exe", "=", "fluid", ".", "ParallelExecutor", "(", "\n", "use_cuda", "=", "args", ".", "use_cuda", ",", "\n", "loss_name", "=", "loss", ".", "name", ",", "\n", "exec_strategy", "=", "exec_strategy", ",", "\n", "main_program", "=", "train_program", ")", "\n", "\n", "train_pyreader", ".", "decorate_tensor_provider", "(", "train_data_generator", ")", "\n", "\n", "train_pyreader", ".", "start", "(", ")", "\n", "steps", "=", "0", "\n", "total_cost", ",", "total_num_seqs", "=", "[", "]", ",", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "while", "steps", "<", "max_train_steps", ":", "\n", "            ", "try", ":", "\n", "                ", "steps", "+=", "1", "\n", "if", "steps", "%", "args", ".", "skip_steps", "==", "0", ":", "\n", "                    ", "if", "warmup_steps", "<=", "0", ":", "\n", "                        ", "fetch_list", "=", "[", "loss", ".", "name", ",", "num_seqs", ".", "name", "]", "\n", "", "else", ":", "\n", "                        ", "fetch_list", "=", "[", "\n", "loss", ".", "name", ",", "scheduled_lr", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", "\n", "", "", "else", ":", "\n", "                    ", "fetch_list", "=", "[", "]", "\n", "\n", "", "outputs", "=", "train_exe", ".", "run", "(", "fetch_list", "=", "fetch_list", ")", "\n", "\n", "if", "steps", "%", "args", ".", "skip_steps", "==", "0", ":", "\n", "                    ", "if", "warmup_steps", "<=", "0", ":", "\n", "                        ", "np_loss", ",", "np_num_seqs", "=", "outputs", "\n", "", "else", ":", "\n", "                        ", "np_loss", ",", "np_lr", ",", "np_num_seqs", "=", "outputs", "\n", "", "total_cost", ".", "extend", "(", "np_loss", "*", "np_num_seqs", ")", "\n", "total_num_seqs", ".", "extend", "(", "np_num_seqs", ")", "\n", "\n", "if", "args", ".", "verbose", ":", "\n", "                        ", "verbose", "=", "\"train pyreader queue size: %d, \"", "%", "train_pyreader", ".", "queue", ".", "size", "(", "\n", ")", "\n", "verbose", "+=", "\"learning rate: %f\"", "%", "(", "\n", "np_lr", "[", "0", "]", "\n", "if", "warmup_steps", ">", "0", "else", "args", ".", "learning_rate", ")", "\n", "logger", ".", "info", "(", "verbose", ")", "\n", "\n", "", "time_end", "=", "time", ".", "time", "(", ")", "\n", "used_time", "=", "time_end", "-", "time_begin", "\n", "current_example", ",", "epoch", "=", "processor", ".", "get_train_progress", "(", ")", "\n", "\n", "logger", ".", "info", "(", "\"epoch: %d, progress: %d/%d, step: %d, loss: %f, \"", "\n", "\"speed: %f steps/s\"", "%", "\n", "(", "epoch", ",", "current_example", ",", "num_train_examples", ",", "steps", ",", "\n", "np", ".", "sum", "(", "total_cost", ")", "/", "np", ".", "sum", "(", "total_num_seqs", ")", ",", "\n", "args", ".", "skip_steps", "/", "used_time", ")", ")", "\n", "total_cost", ",", "total_num_seqs", "=", "[", "]", ",", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "steps", "%", "args", ".", "save_steps", "==", "0", "or", "steps", "==", "max_train_steps", ":", "\n", "                    ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\n", "\"step_\"", "+", "str", "(", "steps", ")", ")", "\n", "fluid", ".", "io", ".", "save_persistables", "(", "exe", ",", "save_path", ",", "train_program", ")", "\n", "\n", "", "if", "steps", "%", "args", ".", "validation_steps", "==", "0", "or", "steps", "==", "max_train_steps", ":", "\n", "                    ", "if", "args", ".", "do_val", ":", "\n", "                        ", "test_pyreader", ".", "decorate_tensor_provider", "(", "\n", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "predict_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'predict'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "epoch", "=", "1", ",", "\n", "**", "eval_concept_settings", ")", "\n", ")", "\n", "val_performance", "=", "predict", "(", "exe", ",", "test_prog", ",", "test_pyreader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "eval_concept_settings", ",", "'validate_result_step_{}.json'", ".", "format", "(", "steps", ")", ")", "\n", "logger", ".", "info", "(", "\"Validation performance after step {}:\\n* Exact_match: {}\\n* F1: {}\"", ".", "format", "(", "steps", ",", "val_performance", "[", "'exact_match'", "]", ",", "val_performance", "[", "'f1'", "]", ")", ")", "\n", "\n", "", "", "", "except", "fluid", ".", "core", ".", "EOFException", ":", "\n", "                ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\n", "\"step_\"", "+", "str", "(", "steps", ")", "+", "\"_final\"", ")", "\n", "fluid", ".", "io", ".", "save_persistables", "(", "exe", ",", "save_path", ",", "train_program", ")", "\n", "train_pyreader", ".", "reset", "(", ")", "\n", "break", "\n", "\n", "", "", "", "if", "args", ".", "do_predict", ":", "\n", "        ", "test_pyreader", ".", "decorate_tensor_provider", "(", "eval_data_generator", ")", "\n", "\n", "if", "args", ".", "use_ema", ":", "\n", "            ", "with", "ema", ".", "apply", "(", "exe", ")", ":", "\n", "                ", "eval_performance", "=", "predict", "(", "exe", ",", "test_prog", ",", "test_pyreader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "eval_concept_settings", ")", "\n", "", "", "else", ":", "\n", "            ", "eval_performance", "=", "predict", "(", "exe", ",", "test_prog", ",", "test_pyreader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "eval_concept_settings", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Eval performance:\\n* Exact_match: {}\\n* F1: {}\"", ".", "format", "(", "eval_performance", "[", "'exact_match'", "]", ",", "eval_performance", "[", "'f1'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.batching.mask": [[23, 75], ["max", "numpy.random.rand", "numpy.random.randint", "enumerate", "numpy.array().astype().reshape", "numpy.array().astype().reshape", "enumerate", "len", "len", "int", "numpy.array().astype", "numpy.array().astype", "numpy.random.randint", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "numpy.array", "numpy.array", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "len"], "function", ["None"], ["\n", "max_len", "=", "max", "(", "[", "len", "(", "sent", ")", "for", "sent", "in", "batch_tokens", "]", ")", "\n", "mask_label", "=", "[", "]", "\n", "mask_pos", "=", "[", "]", "\n", "prob_mask", "=", "np", ".", "random", ".", "rand", "(", "total_token_num", ")", "\n", "# Note: the first token is [CLS], so [low=1]", "\n", "replace_ids", "=", "np", ".", "random", ".", "randint", "(", "1", ",", "high", "=", "vocab_size", ",", "size", "=", "total_token_num", ")", "\n", "pre_sent_len", "=", "0", "\n", "prob_index", "=", "0", "\n", "for", "sent_index", ",", "sent", "in", "enumerate", "(", "batch_tokens", ")", ":", "\n", "        ", "mask_flag", "=", "False", "\n", "prob_index", "+=", "pre_sent_len", "\n", "for", "token_index", ",", "token", "in", "enumerate", "(", "sent", ")", ":", "\n", "            ", "prob", "=", "prob_mask", "[", "prob_index", "+", "token_index", "]", "\n", "if", "prob", ">", "0.15", ":", "\n", "                ", "continue", "\n", "", "elif", "0.03", "<", "prob", "<=", "0.15", ":", "\n", "# mask", "\n", "                ", "if", "token", "!=", "SEP", "and", "token", "!=", "CLS", ":", "\n", "                    ", "mask_label", ".", "append", "(", "sent", "[", "token_index", "]", ")", "\n", "sent", "[", "token_index", "]", "=", "MASK", "\n", "mask_flag", "=", "True", "\n", "mask_pos", ".", "append", "(", "sent_index", "*", "max_len", "+", "token_index", ")", "\n", "", "", "elif", "0.015", "<", "prob", "<=", "0.03", ":", "\n", "# random replace", "\n", "                ", "if", "token", "!=", "SEP", "and", "token", "!=", "CLS", ":", "\n", "                    ", "mask_label", ".", "append", "(", "sent", "[", "token_index", "]", ")", "\n", "sent", "[", "token_index", "]", "=", "replace_ids", "[", "prob_index", "+", "token_index", "]", "\n", "mask_flag", "=", "True", "\n", "mask_pos", ".", "append", "(", "sent_index", "*", "max_len", "+", "token_index", ")", "\n", "", "", "else", ":", "\n", "# keep the original token", "\n", "                ", "if", "token", "!=", "SEP", "and", "token", "!=", "CLS", ":", "\n", "                    ", "mask_label", ".", "append", "(", "sent", "[", "token_index", "]", ")", "\n", "mask_pos", ".", "append", "(", "sent_index", "*", "max_len", "+", "token_index", ")", "\n", "", "", "", "pre_sent_len", "=", "len", "(", "sent", ")", "\n", "# ensure at least mask one word in a sentence", "\n", "while", "not", "mask_flag", ":", "\n", "            ", "token_index", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "1", ",", "high", "=", "len", "(", "sent", ")", "-", "1", ",", "size", "=", "1", ")", ")", "\n", "if", "sent", "[", "token_index", "]", "!=", "SEP", "and", "sent", "[", "token_index", "]", "!=", "CLS", ":", "\n", "                ", "mask_label", ".", "append", "(", "sent", "[", "token_index", "]", ")", "\n", "sent", "[", "token_index", "]", "=", "MASK", "\n", "mask_flag", "=", "True", "\n", "mask_pos", ".", "append", "(", "sent_index", "*", "max_len", "+", "token_index", ")", "\n", "", "", "", "mask_label", "=", "np", ".", "array", "(", "mask_label", ")", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "1", "]", ")", "\n", "mask_pos", "=", "np", ".", "array", "(", "mask_pos", ")", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "1", "]", ")", "\n", "return", "batch_tokens", ",", "mask_label", ",", "mask_pos", "\n", "\n", "\n", "", "def", "prepare_batch_data", "(", "insts", ",", "\n", "total_token_num", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.batching.prepare_batch_data": [[77, 138], ["range", "batching.pad_batch_data", "batching.pad_batch_data", "batching.pad_batch_data", "len", "numpy.array().astype().reshape", "labels_list.append", "batching.mask", "len", "numpy.array().astype", "numpy.array"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.pad_batch_data", "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.pad_batch_data", "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.pad_batch_data", "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.mask"], ["voc_size", "=", "0", ",", "\n", "pad_id", "=", "None", ",", "\n", "cls_id", "=", "None", ",", "\n", "sep_id", "=", "None", ",", "\n", "mask_id", "=", "None", ",", "\n", "return_input_mask", "=", "True", ",", "\n", "return_max_len", "=", "True", ",", "\n", "return_num_token", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    1. generate Tensor of data\n    2. generate Tensor of position\n    3. generate self attention mask, [shape: batch_size *  max_len * max_len]\n    \"\"\"", "\n", "batch_src_ids", "=", "[", "inst", "[", "0", "]", "for", "inst", "in", "insts", "]", "\n", "batch_sent_ids", "=", "[", "inst", "[", "1", "]", "for", "inst", "in", "insts", "]", "\n", "batch_pos_ids", "=", "[", "inst", "[", "2", "]", "for", "inst", "in", "insts", "]", "\n", "labels_list", "=", "[", "]", "\n", "# compatible with mrqa, whose example includes start/end positions, ", "\n", "# or unique id", "\n", "for", "i", "in", "range", "(", "3", ",", "len", "(", "insts", "[", "0", "]", ")", ",", "1", ")", ":", "\n", "        ", "labels", "=", "[", "inst", "[", "i", "]", "for", "inst", "in", "insts", "]", "\n", "labels", "=", "np", ".", "array", "(", "labels", ")", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "1", "]", ")", "\n", "labels_list", ".", "append", "(", "labels", ")", "\n", "# First step: do mask without padding", "\n", "", "if", "mask_id", ">=", "0", ":", "\n", "        ", "out", ",", "mask_label", ",", "mask_pos", "=", "mask", "(", "\n", "batch_src_ids", ",", "\n", "total_token_num", ",", "\n", "vocab_size", "=", "voc_size", ",", "\n", "CLS", "=", "cls_id", ",", "\n", "SEP", "=", "sep_id", ",", "\n", "MASK", "=", "mask_id", ")", "\n", "", "else", ":", "\n", "        ", "out", "=", "batch_src_ids", "\n", "# Second step: padding", "\n", "", "src_id", ",", "self_input_mask", "=", "pad_batch_data", "(", "\n", "out", ",", "\n", "max_len", "=", "max_len", ",", "\n", "pad_idx", "=", "pad_id", ",", "return_input_mask", "=", "True", ")", "\n", "pos_id", "=", "pad_batch_data", "(", "\n", "batch_pos_ids", ",", "\n", "max_len", "=", "max_len", ",", "\n", "pad_idx", "=", "pad_id", ",", "\n", "return_pos", "=", "False", ",", "\n", "return_input_mask", "=", "False", ")", "\n", "sent_id", "=", "pad_batch_data", "(", "\n", "batch_sent_ids", ",", "\n", "max_len", "=", "max_len", ",", "\n", "pad_idx", "=", "pad_id", ",", "\n", "return_pos", "=", "False", ",", "\n", "return_input_mask", "=", "False", ")", "\n", "if", "mask_id", ">=", "0", ":", "\n", "        ", "return_list", "=", "[", "\n", "src_id", ",", "pos_id", ",", "sent_id", ",", "self_input_mask", ",", "mask_label", ",", "mask_pos", "\n", "]", "+", "labels_list", "\n", "", "else", ":", "\n", "        ", "return_list", "=", "[", "src_id", ",", "pos_id", ",", "sent_id", ",", "self_input_mask", "]", "+", "labels_list", "\n", "", "return", "return_list", "if", "len", "(", "return_list", ")", ">", "1", "else", "return_list", "[", "0", "]", "\n", "\n", "\n", "", "def", "pad_batch_data", "(", "insts", ",", "\n", "max_len", "=", "None", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.batching.pad_batch_data": [[140, 186], ["max", "numpy.array", "np.array.astype().reshape", "numpy.array", "numpy.array", "numpy.expand_dims", "len", "np.array.astype().reshape", "np.expand_dims.astype", "len", "len", "list", "list", "np.array.astype", "list", "np.array.astype", "range", "len", "len", "len", "len", "len"], "function", ["None"], ["return_pos", "=", "False", ",", "\n", "return_input_mask", "=", "False", ",", "\n", "return_max_len", "=", "False", ",", "\n", "return_num_token", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Pad the instances to the max sequence length in batch, and generate the\n    corresponding position data and input mask.\n    \"\"\"", "\n", "return_list", "=", "[", "]", "\n", "if", "max_len", "is", "None", ":", "\n", "        ", "max_len", "=", "max", "(", "len", "(", "inst", ")", "for", "inst", "in", "insts", ")", "\n", "# Any token included in dict can be used to pad, since the paddings' loss", "\n", "# will be masked out by weights and make no effect on parameter gradients.", "\n", "", "inst_data", "=", "np", ".", "array", "(", "[", "\n", "list", "(", "inst", ")", "+", "list", "(", "[", "pad_idx", "]", "*", "(", "max_len", "-", "len", "(", "inst", ")", ")", ")", "for", "inst", "in", "insts", "\n", "]", ")", "\n", "return_list", "+=", "[", "inst_data", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "max_len", ",", "1", "]", ")", "]", "\n", "# position data", "\n", "if", "return_pos", ":", "\n", "        ", "inst_pos", "=", "np", ".", "array", "(", "[", "\n", "list", "(", "range", "(", "0", ",", "len", "(", "inst", ")", ")", ")", "+", "[", "pad_idx", "]", "*", "(", "max_len", "-", "len", "(", "inst", ")", ")", "\n", "for", "inst", "in", "insts", "\n", "]", ")", "\n", "return_list", "+=", "[", "inst_pos", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "max_len", ",", "1", "]", ")", "]", "\n", "", "if", "return_input_mask", ":", "\n", "# This is used to avoid attention on paddings.", "\n", "        ", "input_mask_data", "=", "np", ".", "array", "(", "[", "[", "1", "]", "*", "len", "(", "inst", ")", "+", "[", "0", "]", "*", "\n", "(", "max_len", "-", "len", "(", "inst", ")", ")", "for", "inst", "in", "insts", "]", ")", "\n", "input_mask_data", "=", "np", ".", "expand_dims", "(", "input_mask_data", ",", "axis", "=", "-", "1", ")", "\n", "return_list", "+=", "[", "input_mask_data", ".", "astype", "(", "\"float32\"", ")", "]", "\n", "", "if", "return_max_len", ":", "\n", "        ", "return_list", "+=", "[", "max_len", "]", "\n", "", "if", "return_num_token", ":", "\n", "        ", "num_token", "=", "0", "\n", "for", "inst", "in", "insts", ":", "\n", "            ", "num_token", "+=", "len", "(", "inst", ")", "\n", "", "return_list", "+=", "[", "num_token", "]", "\n", "", "return", "return_list", "if", "len", "(", "return_list", ")", ">", "1", "else", "return_list", "[", "0", "]", "\n", "\n", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "pass", "\n", "\n", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.src.optimization.linear_warmup_decay": [[25, 51], ["paddle.default_main_program()._lr_schedule_guard", "paddle.layers.tensor.create_global_var", "paddle.layers.learning_rate_scheduler._decay_step_counter", "paddle.layers.control_flow.Switch", "paddle.default_main_program", "switch.case", "paddle.layers.tensor.assign", "switch.default", "paddle.layers.learning_rate_scheduler.polynomial_decay", "paddle.layers.tensor.assign"], "function", ["None"], ["def", "linear_warmup_decay", "(", "learning_rate", ",", "warmup_steps", ",", "num_train_steps", ")", ":", "\n", "    ", "\"\"\" Applies linear warmup of learning rate from 0 and decay to 0.\"\"\"", "\n", "with", "fluid", ".", "default_main_program", "(", ")", ".", "_lr_schedule_guard", "(", ")", ":", "\n", "        ", "lr", "=", "fluid", ".", "layers", ".", "tensor", ".", "create_global_var", "(", "\n", "shape", "=", "[", "1", "]", ",", "\n", "value", "=", "0.0", ",", "\n", "dtype", "=", "'float32'", ",", "\n", "persistable", "=", "True", ",", "\n", "name", "=", "\"scheduled_learning_rate\"", ")", "\n", "\n", "global_step", "=", "fluid", ".", "layers", ".", "learning_rate_scheduler", ".", "_decay_step_counter", "(", ")", "\n", "\n", "with", "fluid", ".", "layers", ".", "control_flow", ".", "Switch", "(", ")", "as", "switch", ":", "\n", "            ", "with", "switch", ".", "case", "(", "global_step", "<", "warmup_steps", ")", ":", "\n", "                ", "warmup_lr", "=", "learning_rate", "*", "(", "global_step", "/", "warmup_steps", ")", "\n", "fluid", ".", "layers", ".", "tensor", ".", "assign", "(", "warmup_lr", ",", "lr", ")", "\n", "", "with", "switch", ".", "default", "(", ")", ":", "\n", "                ", "decayed_lr", "=", "fluid", ".", "layers", ".", "learning_rate_scheduler", ".", "polynomial_decay", "(", "\n", "learning_rate", "=", "learning_rate", ",", "\n", "decay_steps", "=", "num_train_steps", ",", "\n", "end_learning_rate", "=", "0.0", ",", "\n", "power", "=", "1.0", ",", "\n", "cycle", "=", "False", ")", "\n", "fluid", ".", "layers", ".", "tensor", ".", "assign", "(", "decayed_lr", ",", "lr", ")", "\n", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.optimization.optimization": [[53, 178], ["paddle.optimizer.Adam", "paddle.clip.set_gradient_clip", "dict", "param.name.rstrip", "paddle.layers.create_global_var", "fluid.optimizer.Adam.backward", "utils.fp16.create_master_params_grads", "fluid.optimizer.Adam.apply_gradients", "utils.fp16.master_param_to_train_param", "fluid.optimizer.Adam.minimize", "paddle.layers.learning_rate_scheduler.noam_decay", "print", "paddle.layers.create_global_var", "ValueError", "paddle.clip.GradientClipByGlobalNorm", "param.name.rstrip.find", "param.name.rstrip.endswith", "utils.fp16.apply_dynamic_loss_scaling", "train_program.global_block().all_parameters", "optimization.linear_warmup_decay", "print", "paddle.layers.create_global_var", "paddle.unique_name.generate", "optimization.optimization.exclude_from_weight_decay"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.create_master_params_grads", "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.master_param_to_train_param", "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.apply_dynamic_loss_scaling", "home.repos.pwc.inspect_result.baidu_DuReader.src.optimization.linear_warmup_decay"], ["", "", "def", "optimization", "(", "loss", ",", "\n", "warmup_steps", ",", "\n", "num_train_steps", ",", "\n", "learning_rate", ",", "\n", "train_program", ",", "\n", "startup_prog", ",", "\n", "weight_decay", ",", "\n", "scheduler", "=", "'linear_warmup_decay'", ",", "\n", "use_fp16", "=", "False", ",", "\n", "loss_scaling", "=", "1.0", ")", ":", "\n", "    ", "if", "warmup_steps", ">", "0", ":", "\n", "        ", "if", "scheduler", "==", "'noam_decay'", ":", "\n", "            ", "scheduled_lr", "=", "fluid", ".", "layers", ".", "learning_rate_scheduler", ".", "noam_decay", "(", "1", "/", "(", "warmup_steps", "*", "(", "learning_rate", "**", "2", ")", ")", ",", "\n", "warmup_steps", ")", "\n", "", "elif", "scheduler", "==", "'linear_warmup_decay'", ":", "\n", "            ", "scheduled_lr", "=", "linear_warmup_decay", "(", "learning_rate", ",", "warmup_steps", ",", "\n", "num_train_steps", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unkown learning rate scheduler, should be \"", "\n", "\"'noam_decay' or 'linear_warmup_decay'\"", ")", "\n", "", "optimizer", "=", "fluid", ".", "optimizer", ".", "Adam", "(", "learning_rate", "=", "scheduled_lr", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "fluid", ".", "optimizer", ".", "Adam", "(", "learning_rate", "=", "learning_rate", ")", "\n", "scheduled_lr", "=", "learning_rate", "\n", "\n", "", "clip_norm_thres", "=", "1.0", "\n", "# When using mixed precision training, scale the gradient clip threshold", "\n", "# by loss_scaling", "\n", "if", "use_fp16", "and", "loss_scaling", ">", "1.0", ":", "\n", "        ", "clip_norm_thres", "*=", "loss_scaling", "\n", "", "fluid", ".", "clip", ".", "set_gradient_clip", "(", "\n", "clip", "=", "fluid", ".", "clip", ".", "GradientClipByGlobalNorm", "(", "clip_norm", "=", "clip_norm_thres", ")", ")", "\n", "\n", "def", "exclude_from_weight_decay", "(", "name", ")", ":", "\n", "        ", "if", "name", ".", "find", "(", "\"layer_norm\"", ")", ">", "-", "1", ":", "\n", "            ", "return", "True", "\n", "", "bias_suffix", "=", "[", "\"_bias\"", ",", "\"_b\"", ",", "\".b_0\"", "]", "\n", "for", "suffix", "in", "bias_suffix", ":", "\n", "            ", "if", "name", ".", "endswith", "(", "suffix", ")", ":", "\n", "                ", "return", "True", "\n", "", "", "return", "False", "\n", "\n", "", "param_list", "=", "dict", "(", ")", "\n", "\n", "if", "use_fp16", ":", "\n", "        ", "param_grads", "=", "optimizer", ".", "backward", "(", "loss", ")", "\n", "master_param_grads", "=", "create_master_params_grads", "(", "\n", "param_grads", ",", "train_program", ",", "startup_prog", ",", "loss_scaling", ")", "\n", "\n", "for", "param", ",", "_", "in", "master_param_grads", ":", "\n", "            ", "param_list", "[", "param", ".", "name", "]", "=", "param", "*", "1.0", "\n", "param_list", "[", "param", ".", "name", "]", ".", "stop_gradient", "=", "True", "\n", "\n", "", "optimizer", ".", "apply_gradients", "(", "master_param_grads", ")", "\n", "\n", "if", "weight_decay", ">", "0", ":", "\n", "            ", "for", "param", ",", "grad", "in", "master_param_grads", ":", "\n", "                ", "if", "exclude_from_weight_decay", "(", "param", ".", "name", ".", "rstrip", "(", "\".master\"", ")", ")", ":", "\n", "                    ", "continue", "\n", "", "with", "param", ".", "block", ".", "program", ".", "_optimized_guard", "(", "\n", "[", "param", ",", "grad", "]", ")", ",", "fluid", ".", "framework", ".", "name_scope", "(", "\"weight_decay\"", ")", ":", "\n", "                    ", "updated_param", "=", "param", "-", "param_list", "[", "\n", "param", ".", "name", "]", "*", "weight_decay", "*", "scheduled_lr", "\n", "fluid", ".", "layers", ".", "assign", "(", "output", "=", "param", ",", "input", "=", "updated_param", ")", "\n", "\n", "", "", "", "master_param_to_train_param", "(", "master_param_grads", ",", "param_grads", ",", "\n", "train_program", ")", "\n", "\n", "", "else", ":", "\n", "        ", "for", "param", "in", "train_program", ".", "global_block", "(", ")", ".", "all_parameters", "(", ")", ":", "\n", "            ", "param_list", "[", "param", ".", "name", "]", "=", "param", "*", "1.0", "\n", "param_list", "[", "param", ".", "name", "]", ".", "stop_gradient", "=", "True", "\n", "\n", "", "_", ",", "param_grads", "=", "optimizer", ".", "minimize", "(", "loss", ")", "\n", "\n", "if", "weight_decay", ">", "0", ":", "\n", "            ", "for", "param", ",", "grad", "in", "param_grads", ":", "\n", "                ", "if", "exclude_from_weight_decay", "(", "param", ".", "name", ")", ":", "\n", "                    ", "continue", "\n", "", "with", "param", ".", "block", ".", "program", ".", "_optimized_guard", "(", "\n", "[", "param", ",", "grad", "]", ")", ",", "fluid", ".", "framework", ".", "name_scope", "(", "\"weight_decay\"", ")", ":", "\n", "                    ", "updated_param", "=", "param", "-", "param_list", "[", "\n", "param", ".", "name", "]", "*", "weight_decay", "*", "scheduled_lr", "\n", "fluid", ".", "layers", ".", "assign", "(", "output", "=", "param", ",", "input", "=", "updated_param", ")", "\n", "\n", "", "", "", "", "return", "scheduled_lr", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.mask": [[23, 75], ["max", "numpy.random.rand", "numpy.random.randint", "enumerate", "numpy.array().astype().reshape", "numpy.array().astype().reshape", "enumerate", "len", "len", "int", "numpy.array().astype", "numpy.array().astype", "numpy.random.randint", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "numpy.array", "numpy.array", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "np.array().astype().reshape.append", "len"], "function", ["None"], ["def", "mask", "(", "batch_tokens", ",", "total_token_num", ",", "vocab_size", ",", "CLS", "=", "1", ",", "SEP", "=", "2", ",", "MASK", "=", "3", ")", ":", "\n", "    ", "\"\"\"\n    Add mask for batch_tokens, return out, mask_label, mask_pos;\n    Note: mask_pos responding the batch_tokens after padded;\n    \"\"\"", "\n", "max_len", "=", "max", "(", "[", "len", "(", "sent", ")", "for", "sent", "in", "batch_tokens", "]", ")", "\n", "mask_label", "=", "[", "]", "\n", "mask_pos", "=", "[", "]", "\n", "prob_mask", "=", "np", ".", "random", ".", "rand", "(", "total_token_num", ")", "\n", "# Note: the first token is [CLS], so [low=1]", "\n", "replace_ids", "=", "np", ".", "random", ".", "randint", "(", "1", ",", "high", "=", "vocab_size", ",", "size", "=", "total_token_num", ")", "\n", "pre_sent_len", "=", "0", "\n", "prob_index", "=", "0", "\n", "for", "sent_index", ",", "sent", "in", "enumerate", "(", "batch_tokens", ")", ":", "\n", "        ", "mask_flag", "=", "False", "\n", "prob_index", "+=", "pre_sent_len", "\n", "for", "token_index", ",", "token", "in", "enumerate", "(", "sent", ")", ":", "\n", "            ", "prob", "=", "prob_mask", "[", "prob_index", "+", "token_index", "]", "\n", "if", "prob", ">", "0.15", ":", "\n", "                ", "continue", "\n", "", "elif", "0.03", "<", "prob", "<=", "0.15", ":", "\n", "# mask", "\n", "                ", "if", "token", "!=", "SEP", "and", "token", "!=", "CLS", ":", "\n", "                    ", "mask_label", ".", "append", "(", "sent", "[", "token_index", "]", ")", "\n", "sent", "[", "token_index", "]", "=", "MASK", "\n", "mask_flag", "=", "True", "\n", "mask_pos", ".", "append", "(", "sent_index", "*", "max_len", "+", "token_index", ")", "\n", "", "", "elif", "0.015", "<", "prob", "<=", "0.03", ":", "\n", "# random replace", "\n", "                ", "if", "token", "!=", "SEP", "and", "token", "!=", "CLS", ":", "\n", "                    ", "mask_label", ".", "append", "(", "sent", "[", "token_index", "]", ")", "\n", "sent", "[", "token_index", "]", "=", "replace_ids", "[", "prob_index", "+", "token_index", "]", "\n", "mask_flag", "=", "True", "\n", "mask_pos", ".", "append", "(", "sent_index", "*", "max_len", "+", "token_index", ")", "\n", "", "", "else", ":", "\n", "# keep the original token", "\n", "                ", "if", "token", "!=", "SEP", "and", "token", "!=", "CLS", ":", "\n", "                    ", "mask_label", ".", "append", "(", "sent", "[", "token_index", "]", ")", "\n", "mask_pos", ".", "append", "(", "sent_index", "*", "max_len", "+", "token_index", ")", "\n", "", "", "", "pre_sent_len", "=", "len", "(", "sent", ")", "\n", "\n", "# ensure at least mask one word in a sentence", "\n", "while", "not", "mask_flag", ":", "\n", "            ", "token_index", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "1", ",", "high", "=", "len", "(", "sent", ")", "-", "1", ",", "size", "=", "1", ")", ")", "\n", "if", "sent", "[", "token_index", "]", "!=", "SEP", "and", "sent", "[", "token_index", "]", "!=", "CLS", ":", "\n", "                ", "mask_label", ".", "append", "(", "sent", "[", "token_index", "]", ")", "\n", "sent", "[", "token_index", "]", "=", "MASK", "\n", "mask_flag", "=", "True", "\n", "mask_pos", ".", "append", "(", "sent_index", "*", "max_len", "+", "token_index", ")", "\n", "", "", "", "mask_label", "=", "np", ".", "array", "(", "mask_label", ")", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "1", "]", ")", "\n", "mask_pos", "=", "np", ".", "array", "(", "mask_pos", ")", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "1", "]", ")", "\n", "return", "batch_tokens", ",", "mask_label", ",", "mask_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.prepare_batch_data": [[77, 148], ["range", "batching_twomemory.pad_batch_data", "batching_twomemory.pad_batch_data", "batching_twomemory.pad_batch_data", "batching_twomemory.pad_batch_data", "batching_twomemory.pad_batch_data", "len", "numpy.array().astype().reshape", "labels_list.append", "batching_twomemory.mask", "len", "numpy.array().astype", "numpy.array"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.pad_batch_data", "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.pad_batch_data", "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.pad_batch_data", "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.pad_batch_data", "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.pad_batch_data", "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.mask"], ["", "def", "prepare_batch_data", "(", "insts", ",", "\n", "total_token_num", ",", "\n", "voc_size", "=", "0", ",", "\n", "pad_id", "=", "None", ",", "\n", "cls_id", "=", "None", ",", "\n", "sep_id", "=", "None", ",", "\n", "mask_id", "=", "None", ",", "\n", "return_input_mask", "=", "True", ",", "\n", "return_max_len", "=", "True", ",", "\n", "return_num_token", "=", "False", ",", "\n", "max_wn_concept_length", "=", "50", ",", "\n", "max_nell_concept_length", "=", "50", ")", ":", "\n", "    ", "\"\"\"\n    1. generate Tensor of data\n    2. generate Tensor of position\n    3. generate self attention mask, [shape: batch_size *  max_len * max_len]\n    \"\"\"", "\n", "\n", "batch_src_ids", "=", "[", "inst", "[", "0", "]", "for", "inst", "in", "insts", "]", "\n", "batch_sent_ids", "=", "[", "inst", "[", "1", "]", "for", "inst", "in", "insts", "]", "\n", "batch_pos_ids", "=", "[", "inst", "[", "2", "]", "for", "inst", "in", "insts", "]", "\n", "batch_wn_concept_ids", "=", "[", "inst", "[", "3", "]", "for", "inst", "in", "insts", "]", "\n", "batch_nell_concept_ids", "=", "[", "inst", "[", "4", "]", "for", "inst", "in", "insts", "]", "\n", "labels_list", "=", "[", "]", "\n", "# compatible with squad, whose example includes start/end positions, ", "\n", "# or unique id", "\n", "\n", "for", "i", "in", "range", "(", "5", ",", "len", "(", "insts", "[", "0", "]", ")", ",", "1", ")", ":", "\n", "        ", "labels", "=", "[", "inst", "[", "i", "]", "for", "inst", "in", "insts", "]", "\n", "labels", "=", "np", ".", "array", "(", "labels", ")", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "1", "]", ")", "\n", "labels_list", ".", "append", "(", "labels", ")", "\n", "\n", "# First step: do mask without padding", "\n", "", "if", "mask_id", ">=", "0", ":", "\n", "        ", "out", ",", "mask_label", ",", "mask_pos", "=", "mask", "(", "\n", "batch_src_ids", ",", "\n", "total_token_num", ",", "\n", "vocab_size", "=", "voc_size", ",", "\n", "CLS", "=", "cls_id", ",", "\n", "SEP", "=", "sep_id", ",", "\n", "MASK", "=", "mask_id", ")", "\n", "", "else", ":", "\n", "        ", "out", "=", "batch_src_ids", "\n", "# Second step: padding", "\n", "", "src_id", ",", "self_input_mask", "=", "pad_batch_data", "(", "\n", "out", ",", "pad_idx", "=", "pad_id", ",", "return_input_mask", "=", "True", ")", "\n", "pos_id", "=", "pad_batch_data", "(", "\n", "batch_pos_ids", ",", "\n", "pad_idx", "=", "pad_id", ",", "\n", "return_pos", "=", "False", ",", "\n", "return_input_mask", "=", "False", ")", "\n", "sent_id", "=", "pad_batch_data", "(", "\n", "batch_sent_ids", ",", "\n", "pad_idx", "=", "pad_id", ",", "\n", "return_pos", "=", "False", ",", "\n", "return_input_mask", "=", "False", ")", "\n", "wn_concept_ids", "=", "pad_batch_data", "(", "\n", "batch_wn_concept_ids", ",", "pad_idx", "=", "[", "]", ",", "\n", "max_concept_length", "=", "max_wn_concept_length", ")", "# \u7528[0,0,..]\u6765pad      ", "\n", "nell_concept_ids", "=", "pad_batch_data", "(", "\n", "batch_nell_concept_ids", ",", "pad_idx", "=", "[", "]", ",", "\n", "max_concept_length", "=", "max_nell_concept_length", ")", "# \u7528[0,0,..]\u6765pad            ", "\n", "\n", "if", "mask_id", ">=", "0", ":", "\n", "        ", "return_list", "=", "[", "\n", "src_id", ",", "pos_id", ",", "sent_id", ",", "wn_concept_ids", ",", "nell_concept_ids", ",", "self_input_mask", ",", "mask_label", ",", "mask_pos", "\n", "]", "+", "labels_list", "\n", "", "else", ":", "\n", "        ", "return_list", "=", "[", "src_id", ",", "pos_id", ",", "sent_id", ",", "wn_concept_ids", ",", "nell_concept_ids", ",", "self_input_mask", "]", "+", "labels_list", "\n", "\n", "", "return", "return_list", "if", "len", "(", "return_list", ")", ">", "1", "else", "return_list", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.batching_twomemory.pad_batch_data": [[150, 203], ["type", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.expand_dims", "np.array.astype().reshape", "np.array.astype().reshape", "np.array.astype().reshape", "np.expand_dims.astype", "len", "len", "list", "np.array.astype", "list", "list", "np.array.astype", "list", "np.array.astype", "range", "len", "len", "len", "len", "range", "len", "len"], "function", ["None"], ["", "def", "pad_batch_data", "(", "insts", ",", "\n", "pad_idx", "=", "0", ",", "\n", "return_pos", "=", "False", ",", "\n", "return_input_mask", "=", "False", ",", "\n", "return_max_len", "=", "False", ",", "\n", "return_num_token", "=", "False", ",", "\n", "max_concept_length", "=", "50", ")", ":", "\n", "    ", "\"\"\"\n    Pad the instances to the max sequence length in batch, and generate the\n    corresponding position data and input mask.\n    \"\"\"", "\n", "return_list", "=", "[", "]", "\n", "# max_len = max(len(inst) for inst in insts)", "\n", "max_len", "=", "384", "\n", "# Any token included in dict can be used to pad, since the paddings' loss", "\n", "# will be masked out by weights and make no effect on parameter gradients.", "\n", "\n", "if", "type", "(", "pad_idx", ")", "==", "list", ":", "# padding list, for concept_ids", "\n", "        ", "inst_data", "=", "np", ".", "array", "(", "\n", "[", "inst", "+", "list", "(", "[", "0", "]", "*", "max_concept_length", "for", "x", "in", "range", "(", "max_len", "-", "len", "(", "inst", ")", ")", ")", "for", "inst", "in", "insts", "]", ")", "\n", "return_list", "+=", "[", "inst_data", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "max_len", ",", "max_concept_length", ",", "1", "]", ")", "]", "\n", "", "else", ":", "\n", "        ", "inst_data", "=", "np", ".", "array", "(", "[", "\n", "list", "(", "inst", ")", "+", "list", "(", "[", "pad_idx", "]", "*", "(", "max_len", "-", "len", "(", "inst", ")", ")", ")", "for", "inst", "in", "insts", "\n", "]", ")", "\n", "return_list", "+=", "[", "inst_data", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "max_len", ",", "1", "]", ")", "]", "\n", "\n", "# position data", "\n", "", "if", "return_pos", ":", "\n", "        ", "inst_pos", "=", "np", ".", "array", "(", "[", "\n", "list", "(", "range", "(", "0", ",", "len", "(", "inst", ")", ")", ")", "+", "[", "pad_idx", "]", "*", "(", "max_len", "-", "len", "(", "inst", ")", ")", "\n", "for", "inst", "in", "insts", "\n", "]", ")", "\n", "\n", "return_list", "+=", "[", "inst_pos", ".", "astype", "(", "\"int64\"", ")", ".", "reshape", "(", "[", "-", "1", ",", "max_len", ",", "1", "]", ")", "]", "\n", "\n", "", "if", "return_input_mask", ":", "\n", "# This is used to avoid attention on paddings.", "\n", "        ", "input_mask_data", "=", "np", ".", "array", "(", "[", "[", "1", "]", "*", "len", "(", "inst", ")", "+", "[", "0", "]", "*", "\n", "(", "max_len", "-", "len", "(", "inst", ")", ")", "for", "inst", "in", "insts", "]", ")", "\n", "input_mask_data", "=", "np", ".", "expand_dims", "(", "input_mask_data", ",", "axis", "=", "-", "1", ")", "\n", "return_list", "+=", "[", "input_mask_data", ".", "astype", "(", "\"float32\"", ")", "]", "\n", "\n", "", "if", "return_max_len", ":", "\n", "        ", "return_list", "+=", "[", "max_len", "]", "\n", "\n", "", "if", "return_num_token", ":", "\n", "        ", "num_token", "=", "0", "\n", "for", "inst", "in", "insts", ":", "\n", "            ", "num_token", "+=", "len", "(", "inst", ")", "\n", "", "return_list", "+=", "[", "num_token", "]", "\n", "\n", "", "return", "return_list", "if", "len", "(", "return_list", ")", ">", "1", "else", "return_list", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_record.create_model": [[117, 239], ["model.bert.BertModel", "model.bert.BertModel.get_sequence_output", "logger.info", "paddle.layers.embedding", "paddle.layers.equal", "paddle.layers.cast", "paddle.layers.scale", "paddle.layers.reduce_sum", "model.layers.MemoryLayer", "model.layers.MemoryLayer.forward", "logger.info", "model.layers.TriLinearTwoTimeSelfAttentionLayer", "model.layers.TriLinearTwoTimeSelfAttentionLayer.forward", "paddle.layers.fc", "paddle.layers.transpose", "paddle.layers.unstack", "paddle.layers.fill_constant_batch_size_like", "paddle.layers.reduce_sum", "paddle.layers.py_reader", "paddle.layers.read_file", "paddle.layers.py_reader", "paddle.layers.read_file", "paddle.layers.fill_constant", "paddle.layers.elementwise_sub", "run_record.create_model.compute_loss"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.get_sequence_output", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.forward", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.forward", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.read_file", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.read_file"], ["def", "create_model", "(", "pyreader_name", ",", "bert_config", ",", "max_concept_length", ",", "concept_embedding_mat", ",", "is_training", "=", "False", ",", "freeze", "=", "False", ")", ":", "\n", "    ", "if", "is_training", ":", "\n", "        ", "pyreader", "=", "fluid", ".", "layers", ".", "py_reader", "(", "\n", "capacity", "=", "50", ",", "\n", "shapes", "=", "[", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "max_concept_length", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "1", "]", ",", "[", "-", "1", ",", "1", "]", "]", ",", "\n", "dtypes", "=", "[", "\n", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'float32'", ",", "'int64'", ",", "'int64'", "]", ",", "\n", "lod_levels", "=", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "name", "=", "pyreader_name", ",", "\n", "use_double_buffer", "=", "True", ")", "\n", "(", "src_ids", ",", "pos_ids", ",", "sent_ids", ",", "concept_ids", ",", "input_mask", ",", "start_positions", ",", "\n", "end_positions", ")", "=", "fluid", ".", "layers", ".", "read_file", "(", "pyreader", ")", "\n", "", "else", ":", "\n", "        ", "pyreader", "=", "fluid", ".", "layers", ".", "py_reader", "(", "\n", "capacity", "=", "50", ",", "\n", "shapes", "=", "[", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "max_concept_length", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "1", "]", "]", ",", "\n", "dtypes", "=", "[", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'float32'", ",", "'int64'", "]", ",", "\n", "lod_levels", "=", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "name", "=", "pyreader_name", ",", "\n", "use_double_buffer", "=", "True", ")", "\n", "(", "src_ids", ",", "pos_ids", ",", "sent_ids", ",", "concept_ids", ",", "input_mask", ",", "unique_id", ")", "=", "fluid", ".", "layers", ".", "read_file", "(", "pyreader", ")", "\n", "\n", "", "'''1st Layer: BERT Layer'''", "\n", "bert", "=", "BertModel", "(", "\n", "src_ids", "=", "src_ids", ",", "\n", "position_ids", "=", "pos_ids", ",", "\n", "sentence_ids", "=", "sent_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "config", "=", "bert_config", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "\n", "enc_out", "=", "bert", ".", "get_sequence_output", "(", ")", "\n", "if", "freeze", ":", "\n", "        ", "enc_out", ".", "stop_gradient", "=", "True", "\n", "", "logger", ".", "info", "(", "\"enc_out.stop_gradient: {}\"", ".", "format", "(", "enc_out", ".", "stop_gradient", ")", ")", "\n", "\n", "'''2nd layer: Memory Layer'''", "\n", "# get memory embedding", "\n", "concept_vocab_size", "=", "concept_embedding_mat", ".", "shape", "[", "0", "]", "\n", "concept_dim", "=", "concept_embedding_mat", ".", "shape", "[", "1", "]", "\n", "memory_embs", "=", "fluid", ".", "layers", ".", "embedding", "(", "concept_ids", ",", "\n", "size", "=", "(", "concept_vocab_size", ",", "concept_dim", ")", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "\"concept_emb_mat\"", ",", "\n", "do_model_average", "=", "False", ",", "\n", "trainable", "=", "False", ")", ",", "\n", "dtype", "=", "'float32'", ")", "\n", "\n", "# get memory length", "\n", "concept_ids_reduced", "=", "fluid", ".", "layers", ".", "equal", "(", "concept_ids", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "shape", "=", "[", "1", "]", ",", "value", "=", "0", ",", "dtype", "=", "\"int64\"", ")", ")", "# [batch_size, sent_size, concept_size, 1]", "\n", "concept_ids_reduced", "=", "fluid", ".", "layers", ".", "cast", "(", "concept_ids_reduced", ",", "dtype", "=", "\"float32\"", ")", "# [batch_size, sent_size, concept_size, 1]", "\n", "concept_ids_reduced", "=", "fluid", ".", "layers", ".", "scale", "(", "\n", "fluid", ".", "layers", ".", "elementwise_sub", "(", "\n", "concept_ids_reduced", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "[", "1", "]", ",", "\"float32\"", ",", "1", ")", "\n", ")", ",", "\n", "scale", "=", "-", "1", "\n", ")", "\n", "mem_length", "=", "fluid", ".", "layers", ".", "reduce_sum", "(", "concept_ids_reduced", ",", "dim", "=", "2", ")", "# [batch_size, sent_size, 1]    ", "\n", "\n", "# select and integrate", "\n", "memory_layer", "=", "MemoryLayer", "(", "bert_config", ",", "max_concept_length", ",", "concept_dim", ",", "mem_method", "=", "'cat'", ")", "\n", "memory_output", "=", "memory_layer", ".", "forward", "(", "enc_out", ",", "memory_embs", ",", "mem_length", ",", "ignore_no_memory_token", "=", "True", ")", "\n", "\n", "'''3rd layer: Self-Matching Layer'''", "\n", "# calculate input dim for self-matching layer", "\n", "if", "memory_layer", ".", "mem_method", "==", "'add'", ":", "\n", "        ", "memory_output_size", "=", "bert_config", "[", "'hidden_size'", "]", "\n", "", "elif", "memory_layer", ".", "mem_method", "==", "'cat'", ":", "\n", "        ", "memory_output_size", "=", "bert_config", "[", "'hidden_size'", "]", "+", "concept_dim", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"memory_layer.mem_method must be 'add' or 'cat'\"", ")", "\n", "", "logger", ".", "info", "(", "\"memory_output_size: {}\"", ".", "format", "(", "memory_output_size", ")", ")", "\n", "\n", "# do matching", "\n", "self_att_layer", "=", "TriLinearTwoTimeSelfAttentionLayer", "(", "\n", "memory_output_size", ",", "dropout_rate", "=", "0.0", ",", "\n", "cat_mul", "=", "True", ",", "cat_sub", "=", "True", ",", "cat_twotime", "=", "True", ",", "\n", "cat_twotime_mul", "=", "False", ",", "cat_twotime_sub", "=", "True", ")", "# [bs, sq, concat_hs]", "\n", "att_output", "=", "self_att_layer", ".", "forward", "(", "memory_output", ",", "input_mask", ")", "# [bs, sq, concat_hs]", "\n", "\n", "'''4th layer: Output Layer'''", "\n", "logits", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "att_output", ",", "\n", "size", "=", "2", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"cls_squad_out_w\"", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "NormalInitializer", "(", "loc", "=", "0.0", ",", "scale", "=", "bert_config", "[", "'initializer_range'", "]", ")", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"cls_squad_out_b\"", ",", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", ")", "\n", "\n", "logits", "=", "fluid", ".", "layers", ".", "transpose", "(", "x", "=", "logits", ",", "perm", "=", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "start_logits", ",", "end_logits", "=", "fluid", ".", "layers", ".", "unstack", "(", "x", "=", "logits", ",", "axis", "=", "0", ")", "\n", "\n", "batch_ones", "=", "fluid", ".", "layers", ".", "fill_constant_batch_size_like", "(", "\n", "input", "=", "start_logits", ",", "dtype", "=", "'int64'", ",", "shape", "=", "[", "1", "]", ",", "value", "=", "1", ")", "\n", "num_seqs", "=", "fluid", ".", "layers", ".", "reduce_sum", "(", "input", "=", "batch_ones", ")", "\n", "\n", "if", "is_training", ":", "\n", "\n", "        ", "def", "compute_loss", "(", "logits", ",", "positions", ")", ":", "\n", "            ", "loss", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "logits", ",", "label", "=", "positions", ")", "\n", "loss", "=", "fluid", ".", "layers", ".", "mean", "(", "x", "=", "loss", ")", "\n", "return", "loss", "\n", "\n", "", "start_loss", "=", "compute_loss", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "compute_loss", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2.0", "\n", "if", "args", ".", "use_fp16", "and", "args", ".", "loss_scaling", ">", "1.0", ":", "\n", "            ", "total_loss", "=", "total_loss", "*", "args", ".", "loss_scaling", "\n", "\n", "", "return", "pyreader", ",", "total_loss", ",", "num_seqs", "\n", "", "else", ":", "\n", "        ", "return", "pyreader", ",", "unique_id", ",", "start_logits", ",", "end_logits", ",", "num_seqs", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_record.predict": [[245, 285], ["os.path.join", "os.path.join", "os.path.join", "os.path.join", "test_pyreader.start", "time.time", "time.time", "processor.get_features", "reader.record.write_predictions", "os.path.exists", "os.makedirs", "test_exe.run", "range", "int", "all_results.append", "test_pyreader.reset", "logger.info", "float", "float", "RawResult", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_features", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.write_predictions", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["def", "predict", "(", "test_exe", ",", "test_program", ",", "test_pyreader", ",", "fetch_list", ",", "processor", ",", "eval_concept_settings", ",", "eval_output_name", "=", "'eval_result.json'", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "checkpoints", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "checkpoints", ")", "\n", "", "output_prediction_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\"predictions.json\"", ")", "\n", "output_nbest_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\"nbest_predictions.json\"", ")", "\n", "output_null_log_odds_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\"null_odds.json\"", ")", "\n", "output_evaluation_result_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "eval_output_name", ")", "\n", "\n", "test_pyreader", ".", "start", "(", ")", "\n", "all_results", "=", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "np_unique_ids", ",", "np_start_logits", ",", "np_end_logits", ",", "np_num_seqs", "=", "test_exe", ".", "run", "(", "\n", "fetch_list", "=", "fetch_list", ",", "program", "=", "test_program", ")", "\n", "for", "idx", "in", "range", "(", "np_unique_ids", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "if", "len", "(", "all_results", ")", "%", "1000", "==", "0", ":", "\n", "                    ", "logger", ".", "info", "(", "\"Processing example: %d\"", "%", "len", "(", "all_results", ")", ")", "\n", "", "unique_id", "=", "int", "(", "np_unique_ids", "[", "idx", "]", ")", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "np_start_logits", "[", "idx", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "np_end_logits", "[", "idx", "]", ".", "flat", "]", "\n", "all_results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "", "", "except", "fluid", ".", "core", ".", "EOFException", ":", "\n", "            ", "test_pyreader", ".", "reset", "(", ")", "\n", "break", "\n", "", "", "time_end", "=", "time", ".", "time", "(", ")", "\n", "\n", "features", "=", "processor", ".", "get_features", "(", "\n", "processor", ".", "predict_examples", ",", "is_training", "=", "False", ",", "**", "eval_concept_settings", ")", "\n", "eval_result", "=", "write_predictions", "(", "processor", ".", "predict_examples", ",", "features", ",", "all_results", ",", "\n", "args", ".", "n_best_size", ",", "args", ".", "max_answer_length", ",", "\n", "args", ".", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "output_null_log_odds_file", ",", "\n", "args", ".", "version_2_with_negative", ",", "\n", "args", ".", "null_score_diff_threshold", ",", "args", ".", "verbose", ",", "args", ".", "predict_file", ",", "output_evaluation_result_file", ")", "\n", "return", "eval_result", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_record.read_concept_embedding": [[286, 306], ["open", "len", "len", "id2concept.append", "np.array.append", "numpy.array", "line.strip", "np.array.append", "len", "id2concept.append", "info[].split", "line.split", "float", "range", "len", "numpy.any", "line.split", "numpy.isnan"], "function", ["None"], ["", "def", "read_concept_embedding", "(", "embedding_path", ")", ":", "\n", "    ", "fin", "=", "open", "(", "embedding_path", ",", "encoding", "=", "'utf-8'", ")", "\n", "info", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "fin", "]", "\n", "dim", "=", "len", "(", "info", "[", "0", "]", ".", "split", "(", "' '", ")", "[", "1", ":", "]", ")", "\n", "n_concept", "=", "len", "(", "info", ")", "\n", "embedding_mat", "=", "[", "]", "\n", "id2concept", ",", "concept2id", "=", "[", "]", ",", "{", "}", "\n", "# add padding concept into vocab", "\n", "id2concept", ".", "append", "(", "'<pad_concept>'", ")", "\n", "concept2id", "[", "'<pad_concept>'", "]", "=", "0", "\n", "embedding_mat", ".", "append", "(", "[", "0.0", "for", "_", "in", "range", "(", "dim", ")", "]", ")", "\n", "for", "line", "in", "info", ":", "\n", "        ", "concept_name", "=", "line", ".", "split", "(", "' '", ")", "[", "0", "]", "\n", "embedding", "=", "[", "float", "(", "value_str", ")", "for", "value_str", "in", "line", ".", "split", "(", "' '", ")", "[", "1", ":", "]", "]", "\n", "assert", "len", "(", "embedding", ")", "==", "dim", "and", "not", "np", ".", "any", "(", "np", ".", "isnan", "(", "embedding", ")", ")", "\n", "embedding_mat", ".", "append", "(", "embedding", ")", "\n", "concept2id", "[", "concept_name", "]", "=", "len", "(", "id2concept", ")", "\n", "id2concept", ".", "append", "(", "concept_name", ")", "\n", "", "embedding_mat", "=", "np", ".", "array", "(", "embedding_mat", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "return", "id2concept", ",", "concept2id", ",", "embedding_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_record.train": [[307, 590], ["model.bert.BertConfig", "model.bert.BertConfig.print_config", "paddle.Executor", "run_record.read_concept_embedding", "reader.record.DataProcessor", "paddle.Program", "fluid.Executor.run", "ValueError", "paddle.CUDAPlace", "paddle.core.get_cuda_device_count", "paddle.CPUPlace", "int", "random.seed", "numpy.random.seed", "reader.record.DataProcessor.data_generator", "reader.record.DataProcessor.get_num_examples", "int", "logger.info", "logger.info", "logger.info", "logger.info", "paddle.Program", "reader.record.DataProcessor.data_generator", "paddle.Program", "test_prog.clone.clone", "logger.info", "paddle.global_scope().find_var().get_tensor().set", "paddle.ExecutionStrategy", "paddle.ParallelExecutor", "train_pyreader.decorate_tensor_provider", "train_pyreader.start", "time.time", "test_pyreader.decorate_tensor_provider", "logger.info", "os.environ.get", "paddle.program_guard", "logger.info", "paddle.program_guard", "logger.info", "utils.init.init_checkpoint", "utils.init.init_checkpoint", "run_record.predict", "multiprocessing.cpu_count", "paddle.unique_name.guard", "run_record.create_model", "optimization.optimization", "paddle.memory_optimize", "paddle.contrib.memory_usage", "paddle.contrib.memory_usage", "paddle.unique_name.guard", "run_record.create_model", "paddle.memory_optimize", "paddle.global_scope().find_var().get_tensor", "utils.init.init_pretraining_params", "ValueError", "fluid.ParallelExecutor.run", "fluid.optimizer.ExponentialMovingAverage.apply", "run_record.predict", "paddle.optimizer.ExponentialMovingAverage", "fluid.optimizer.ExponentialMovingAverage.update", "paddle.optimizer.ExponentialMovingAverage", "total_cost.extend", "total_num_seqs.extend", "time.time", "reader.record.DataProcessor.get_train_progress", "logger.info", "time.time", "os.path.join", "paddle.io.save_persistables", "os.path.join", "paddle.io.save_persistables", "train_pyreader.reset", "dir", "paddle.global_scope().find_var", "logger.info", "test_pyreader.decorate_tensor_provider", "run_record.predict", "logger.info", "train_pyreader.queue.size", "str", "reader.record.DataProcessor.data_generator", "paddle.global_scope", "str", "numpy.sum", "numpy.sum"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig.print_config", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad.read_concept_embedding", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_num_examples", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_checkpoint", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_checkpoint", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.create_model", "home.repos.pwc.inspect_result.baidu_DuReader.src.optimization.optimization", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.create_model", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_pretraining_params", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_train_progress", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator"], ["", "def", "train", "(", "args", ")", ":", "\n", "    ", "bert_config", "=", "BertConfig", "(", "args", ".", "bert_config_path", ")", "\n", "bert_config", ".", "print_config", "(", ")", "\n", "\n", "if", "not", "(", "args", ".", "do_train", "or", "args", ".", "do_predict", "or", "args", ".", "do_val", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"For args `do_train` and `do_predict`, at \"", "\n", "\"least one of them must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "use_cuda", ":", "\n", "        ", "place", "=", "fluid", ".", "CUDAPlace", "(", "0", ")", "\n", "dev_count", "=", "fluid", ".", "core", ".", "get_cuda_device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "place", "=", "fluid", ".", "CPUPlace", "(", ")", "\n", "dev_count", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'CPU_NUM'", ",", "multiprocessing", ".", "cpu_count", "(", ")", ")", ")", "\n", "", "exe", "=", "fluid", ".", "Executor", "(", "place", ")", "\n", "\n", "id2concept", ",", "concept2id", ",", "concept_embedding_mat", "=", "read_concept_embedding", "(", "\n", "args", ".", "concept_embedding_path", ")", "\n", "\n", "processor", "=", "DataProcessor", "(", "\n", "vocab_path", "=", "args", ".", "vocab_path", ",", "\n", "do_lower_case", "=", "args", ".", "do_lower_case", ",", "\n", "max_seq_length", "=", "args", ".", "max_seq_len", ",", "\n", "in_tokens", "=", "args", ".", "in_tokens", ",", "\n", "doc_stride", "=", "args", ".", "doc_stride", ",", "\n", "max_query_length", "=", "args", ".", "max_query_length", ")", "\n", "\n", "startup_prog", "=", "fluid", ".", "Program", "(", ")", "\n", "if", "args", ".", "random_seed", "is", "not", "None", ":", "\n", "        ", "startup_prog", ".", "random_seed", "=", "args", ".", "random_seed", "\n", "random", ".", "seed", "(", "args", ".", "random_seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "random_seed", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "train_concept_settings", "=", "{", "\n", "'tokenization_path'", ":", "'../retrieve_concepts/tokenization_record/tokens/train.tokenization.{}.data'", ".", "format", "(", "'uncased'", "if", "args", ".", "do_lower_case", "else", "'cased'", ")", ",", "\n", "'concept2id'", ":", "concept2id", ",", "\n", "'use_wordnet'", ":", "args", ".", "use_wordnet", ",", "\n", "'retrieved_synset_path'", ":", "args", ".", "retrieved_synset_path", ",", "\n", "'use_nell'", ":", "args", ".", "use_nell", ",", "\n", "'retrieved_nell_concept_path'", ":", "args", ".", "train_retrieved_nell_concept_path", ",", "\n", "}", "\n", "train_data_generator", "=", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "train_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'train'", ",", "\n", "shuffle", "=", "True", ",", "\n", "dev_count", "=", "dev_count", ",", "\n", "version_2_with_negative", "=", "args", ".", "version_2_with_negative", ",", "\n", "epoch", "=", "args", ".", "epoch", ",", "\n", "**", "train_concept_settings", ")", "\n", "\n", "num_train_examples", "=", "processor", ".", "get_num_examples", "(", "phase", "=", "'train'", ")", "\n", "if", "args", ".", "in_tokens", ":", "\n", "            ", "max_train_steps", "=", "args", ".", "epoch", "*", "num_train_examples", "//", "(", "\n", "args", ".", "batch_size", "//", "args", ".", "max_seq_len", ")", "//", "dev_count", "\n", "", "else", ":", "\n", "            ", "max_train_steps", "=", "args", ".", "epoch", "*", "num_train_examples", "//", "(", "\n", "args", ".", "batch_size", ")", "//", "dev_count", "\n", "", "warmup_steps", "=", "int", "(", "max_train_steps", "*", "args", ".", "warmup_proportion", ")", "\n", "logger", ".", "info", "(", "\"Device count: %d\"", "%", "dev_count", ")", "\n", "logger", ".", "info", "(", "\"Num train examples: %d\"", "%", "num_train_examples", ")", "\n", "logger", ".", "info", "(", "\"Max train steps: %d\"", "%", "max_train_steps", ")", "\n", "logger", ".", "info", "(", "\"Num warmup steps: %d\"", "%", "warmup_steps", ")", "\n", "\n", "train_program", "=", "fluid", ".", "Program", "(", ")", "\n", "# if args.random_seed is not None:", "\n", "#     train_program.random_seed = args.random_seed", "\n", "with", "fluid", ".", "program_guard", "(", "train_program", ",", "startup_prog", ")", ":", "\n", "            ", "with", "fluid", ".", "unique_name", ".", "guard", "(", ")", ":", "\n", "                ", "train_pyreader", ",", "loss", ",", "num_seqs", "=", "create_model", "(", "\n", "pyreader_name", "=", "'train_reader'", ",", "\n", "bert_config", "=", "bert_config", ",", "\n", "max_concept_length", "=", "processor", ".", "train_max_concept_length", ",", "\n", "concept_embedding_mat", "=", "concept_embedding_mat", ",", "\n", "is_training", "=", "True", ",", "\n", "freeze", "=", "args", ".", "freeze", ")", "\n", "\n", "scheduled_lr", "=", "optimization", "(", "\n", "loss", "=", "loss", ",", "\n", "warmup_steps", "=", "warmup_steps", ",", "\n", "num_train_steps", "=", "max_train_steps", ",", "\n", "learning_rate", "=", "args", ".", "learning_rate", ",", "\n", "train_program", "=", "train_program", ",", "\n", "startup_prog", "=", "startup_prog", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", "scheduler", "=", "args", ".", "lr_scheduler", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ",", "\n", "loss_scaling", "=", "args", ".", "loss_scaling", ")", "\n", "\n", "if", "args", ".", "use_ema", ":", "\n", "                    ", "ema", "=", "fluid", ".", "optimizer", ".", "ExponentialMovingAverage", "(", "args", ".", "ema_decay", ")", "\n", "ema", ".", "update", "(", ")", "\n", "\n", "", "fluid", ".", "memory_optimize", "(", "train_program", ",", "skip_opt_set", "=", "[", "loss", ".", "name", ",", "num_seqs", ".", "name", "]", ")", "\n", "\n", "", "", "if", "args", ".", "verbose", ":", "\n", "            ", "if", "args", ".", "in_tokens", ":", "\n", "                ", "lower_mem", ",", "upper_mem", ",", "unit", "=", "fluid", ".", "contrib", ".", "memory_usage", "(", "\n", "program", "=", "train_program", ",", "\n", "batch_size", "=", "args", ".", "batch_size", "//", "args", ".", "max_seq_len", ")", "\n", "", "else", ":", "\n", "                ", "lower_mem", ",", "upper_mem", ",", "unit", "=", "fluid", ".", "contrib", ".", "memory_usage", "(", "\n", "program", "=", "train_program", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "", "logger", ".", "info", "(", "\"Theoretical memory usage in training:  %.3f - %.3f %s\"", "%", "\n", "(", "lower_mem", ",", "upper_mem", ",", "unit", ")", ")", "\n", "\n", "", "", "if", "args", ".", "do_predict", "or", "args", ".", "do_val", ":", "\n", "        ", "eval_concept_settings", "=", "{", "\n", "'tokenization_path'", ":", "'../retrieve_concepts/tokenization_record/tokens/dev.tokenization.{}.data'", ".", "format", "(", "'uncased'", "if", "args", ".", "do_lower_case", "else", "'cased'", ")", ",", "\n", "'concept2id'", ":", "concept2id", ",", "\n", "'use_wordnet'", ":", "args", ".", "use_wordnet", ",", "\n", "'retrieved_synset_path'", ":", "args", ".", "retrieved_synset_path", ",", "\n", "'use_nell'", ":", "args", ".", "use_nell", ",", "\n", "'retrieved_nell_concept_path'", ":", "args", ".", "dev_retrieved_nell_concept_path", ",", "\n", "}", "\n", "eval_data_generator", "=", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "predict_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'predict'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "epoch", "=", "1", ",", "\n", "**", "eval_concept_settings", ")", "\n", "\n", "test_prog", "=", "fluid", ".", "Program", "(", ")", "\n", "# if args.random_seed is not None:", "\n", "#     test_prog.random_seed = args.random_seed", "\n", "with", "fluid", ".", "program_guard", "(", "test_prog", ",", "startup_prog", ")", ":", "\n", "            ", "with", "fluid", ".", "unique_name", ".", "guard", "(", ")", ":", "\n", "                ", "test_pyreader", ",", "unique_ids", ",", "start_logits", ",", "end_logits", ",", "num_seqs", "=", "create_model", "(", "\n", "pyreader_name", "=", "'test_reader'", ",", "\n", "bert_config", "=", "bert_config", ",", "\n", "max_concept_length", "=", "processor", ".", "predict_max_concept_length", ",", "\n", "concept_embedding_mat", "=", "concept_embedding_mat", ",", "\n", "is_training", "=", "False", ")", "\n", "\n", "if", "args", ".", "use_ema", "and", "'ema'", "not", "in", "dir", "(", ")", ":", "\n", "                    ", "ema", "=", "fluid", ".", "optimizer", ".", "ExponentialMovingAverage", "(", "args", ".", "ema_decay", ")", "\n", "\n", "", "fluid", ".", "memory_optimize", "(", "test_prog", ",", "skip_opt_set", "=", "[", "unique_ids", ".", "name", ",", "\n", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "]", ")", "\n", "\n", "", "", "test_prog", "=", "test_prog", ".", "clone", "(", "for_test", "=", "True", ")", "\n", "# if args.random_seed is not None:", "\n", "#     test_prog.random_seed = args.random_seed", "\n", "\n", "", "exe", ".", "run", "(", "startup_prog", ")", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "info", "(", "'load pretrained concept embedding'", ")", "\n", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "'concept_emb_mat'", ")", ".", "get_tensor", "(", ")", ".", "set", "(", "concept_embedding_mat", ",", "place", ")", "\n", "\n", "if", "args", ".", "init_checkpoint", "and", "args", ".", "init_pretraining_params", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"WARNING: args 'init_checkpoint' and 'init_pretraining_params' \"", "\n", "\"both are set! Only arg 'init_checkpoint' is made valid.\"", ")", "\n", "", "if", "args", ".", "init_checkpoint", ":", "\n", "            ", "init_checkpoint", "(", "\n", "exe", ",", "\n", "args", ".", "init_checkpoint", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "", "elif", "args", ".", "init_pretraining_params", ":", "\n", "            ", "init_pretraining_params", "(", "\n", "exe", ",", "\n", "args", ".", "init_pretraining_params", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "", "", "elif", "args", ".", "do_predict", "or", "args", ".", "do_val", ":", "\n", "        ", "if", "not", "args", ".", "init_checkpoint", ":", "\n", "            ", "raise", "ValueError", "(", "\"args 'init_checkpoint' should be set if\"", "\n", "\"only doing prediction!\"", ")", "\n", "", "init_checkpoint", "(", "\n", "exe", ",", "\n", "args", ".", "init_checkpoint", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "exec_strategy", "=", "fluid", ".", "ExecutionStrategy", "(", ")", "\n", "exec_strategy", ".", "use_experimental_executor", "=", "args", ".", "use_fast_executor", "\n", "exec_strategy", ".", "num_threads", "=", "dev_count", "\n", "exec_strategy", ".", "num_iteration_per_drop_scope", "=", "args", ".", "num_iteration_per_drop_scope", "\n", "\n", "train_exe", "=", "fluid", ".", "ParallelExecutor", "(", "\n", "use_cuda", "=", "args", ".", "use_cuda", ",", "\n", "loss_name", "=", "loss", ".", "name", ",", "\n", "exec_strategy", "=", "exec_strategy", ",", "\n", "main_program", "=", "train_program", ")", "\n", "\n", "train_pyreader", ".", "decorate_tensor_provider", "(", "train_data_generator", ")", "\n", "\n", "train_pyreader", ".", "start", "(", ")", "\n", "steps", "=", "0", "\n", "total_cost", ",", "total_num_seqs", "=", "[", "]", ",", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "while", "steps", "<", "max_train_steps", ":", "\n", "            ", "try", ":", "\n", "                ", "steps", "+=", "1", "\n", "if", "steps", "%", "args", ".", "skip_steps", "==", "0", ":", "\n", "                    ", "if", "warmup_steps", "<=", "0", ":", "\n", "                        ", "fetch_list", "=", "[", "loss", ".", "name", ",", "num_seqs", ".", "name", "]", "\n", "", "else", ":", "\n", "                        ", "fetch_list", "=", "[", "\n", "loss", ".", "name", ",", "scheduled_lr", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", "\n", "", "", "else", ":", "\n", "                    ", "fetch_list", "=", "[", "]", "\n", "\n", "", "outputs", "=", "train_exe", ".", "run", "(", "fetch_list", "=", "fetch_list", ")", "\n", "\n", "if", "steps", "%", "args", ".", "skip_steps", "==", "0", ":", "\n", "                    ", "if", "warmup_steps", "<=", "0", ":", "\n", "                        ", "np_loss", ",", "np_num_seqs", "=", "outputs", "\n", "", "else", ":", "\n", "                        ", "np_loss", ",", "np_lr", ",", "np_num_seqs", "=", "outputs", "\n", "", "total_cost", ".", "extend", "(", "np_loss", "*", "np_num_seqs", ")", "\n", "total_num_seqs", ".", "extend", "(", "np_num_seqs", ")", "\n", "\n", "if", "args", ".", "verbose", ":", "\n", "                        ", "verbose", "=", "\"train pyreader queue size: %d, \"", "%", "train_pyreader", ".", "queue", ".", "size", "(", "\n", ")", "\n", "verbose", "+=", "\"learning rate: %f\"", "%", "(", "\n", "np_lr", "[", "0", "]", "\n", "if", "warmup_steps", ">", "0", "else", "args", ".", "learning_rate", ")", "\n", "logger", ".", "info", "(", "verbose", ")", "\n", "\n", "", "time_end", "=", "time", ".", "time", "(", ")", "\n", "used_time", "=", "time_end", "-", "time_begin", "\n", "current_example", ",", "epoch", "=", "processor", ".", "get_train_progress", "(", ")", "\n", "\n", "logger", ".", "info", "(", "\"epoch: %d, progress: %d/%d, step: %d, loss: %f, \"", "\n", "\"speed: %f steps/s\"", "%", "\n", "(", "epoch", ",", "current_example", ",", "num_train_examples", ",", "steps", ",", "\n", "np", ".", "sum", "(", "total_cost", ")", "/", "np", ".", "sum", "(", "total_num_seqs", ")", ",", "\n", "args", ".", "skip_steps", "/", "used_time", ")", ")", "\n", "total_cost", ",", "total_num_seqs", "=", "[", "]", ",", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "steps", "%", "args", ".", "save_steps", "==", "0", "or", "steps", "==", "max_train_steps", ":", "\n", "                    ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\n", "\"step_\"", "+", "str", "(", "steps", ")", ")", "\n", "fluid", ".", "io", ".", "save_persistables", "(", "exe", ",", "save_path", ",", "train_program", ")", "\n", "\n", "", "if", "steps", "%", "args", ".", "validation_steps", "==", "0", "or", "steps", "==", "max_train_steps", ":", "\n", "                    ", "if", "args", ".", "do_val", ":", "\n", "                        ", "test_pyreader", ".", "decorate_tensor_provider", "(", "\n", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "predict_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'predict'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "epoch", "=", "1", ",", "\n", "**", "eval_concept_settings", ")", "\n", ")", "\n", "val_performance", "=", "predict", "(", "exe", ",", "test_prog", ",", "test_pyreader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "eval_concept_settings", ",", "'validate_result_step_{}.json'", ".", "format", "(", "steps", ")", ")", "\n", "logger", ".", "info", "(", "\"Validation performance after step {}:\\n* Exact_match: {}\\n* F1: {}\"", ".", "format", "(", "steps", ",", "val_performance", "[", "'exact_match'", "]", ",", "val_performance", "[", "'f1'", "]", ")", ")", "\n", "\n", "", "", "", "except", "fluid", ".", "core", ".", "EOFException", ":", "\n", "                ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\n", "\"step_\"", "+", "str", "(", "steps", ")", "+", "\"_final\"", ")", "\n", "fluid", ".", "io", ".", "save_persistables", "(", "exe", ",", "save_path", ",", "train_program", ")", "\n", "train_pyreader", ".", "reset", "(", ")", "\n", "break", "\n", "\n", "", "", "", "if", "args", ".", "do_predict", ":", "\n", "        ", "test_pyreader", ".", "decorate_tensor_provider", "(", "eval_data_generator", ")", "\n", "\n", "if", "args", ".", "use_ema", ":", "\n", "            ", "with", "ema", ".", "apply", "(", "exe", ")", ":", "\n", "                ", "eval_performance", "=", "predict", "(", "exe", ",", "test_prog", ",", "test_pyreader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "eval_concept_settings", ")", "\n", "", "", "else", ":", "\n", "            ", "eval_performance", "=", "predict", "(", "exe", ",", "test_prog", ",", "test_pyreader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "eval_concept_settings", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Eval performance:\\n* Exact_match: {}\\n* F1: {}\"", ".", "format", "(", "eval_performance", "[", "'exact_match'", "]", ",", "eval_performance", "[", "'f1'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.FullTokenizer.__init__": [[113, 118], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "tokenization.FullTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.load_vocab"], ["        ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.FullTokenizer.tokenize": [[119, 126], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n", "", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.FullTokenizer.convert_tokens_to_ids": [[127, 129], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab"], ["        ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n", "", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.FullTokenizer.convert_ids_to_tokens": [[130, 132], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab"], ["        ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.CharTokenizer.__init__": [[137, 141], ["tokenization.load_vocab", "tokenization.WordpieceTokenizer", "tokenization.CharTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.load_vocab"], ["        ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.CharTokenizer.tokenize": [[142, 149], ["text.lower().split", "tokenization.CharTokenizer.wordpiece_tokenizer.tokenize", "text.lower", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "text", ".", "lower", "(", ")", ".", "split", "(", "\" \"", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n", "", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.CharTokenizer.convert_tokens_to_ids": [[150, 152], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab"], ["        ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n", "", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.CharTokenizer.convert_ids_to_tokens": [[153, 155], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab"], ["        ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.BasicTokenizer.__init__": [[160, 167], ["None"], "methods", ["None"], ["        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n            do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "_never_lowercase", "=", "[", "'[UNK]'", ",", "'[SEP]'", ",", "'[PAD]'", ",", "'[CLS]'", ",", "'[MASK]'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.BasicTokenizer.tokenize": [[168, 191], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._tokenize_chinese_chars", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "_never_lowercase", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "if", "token", "in", "self", ".", "_never_lowercase", ":", "\n", "                ", "split_tokens", ".", "extend", "(", "[", "token", "]", ")", "\n", "", "else", ":", "\n", "                ", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.BasicTokenizer._run_strip_accents": [[192, 202], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.normalize"], ["", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n", "", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.BasicTokenizer._run_split_on_punc": [[203, 222], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_punctuation"], ["", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n", "", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.BasicTokenizer._tokenize_chinese_chars": [[223, 235], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._is_chinese_char"], ["\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n", "", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.BasicTokenizer._is_chinese_char": [[236, 257], ["None"], "methods", ["None"], ["                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n", "", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#     https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.BasicTokenizer._clean_text": [[258, 270], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_whitespace", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_control"], ["\n", "", "return", "False", "\n", "\n", "", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.WordpieceTokenizer.__init__": [[275, 279], ["None"], "methods", ["None"], ["", "", "class", "WordpieceTokenizer", "(", "object", ")", ":", "\n", "    ", "\"\"\"Runs WordPiece tokenziation.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.WordpieceTokenizer.tokenize": [[280, 332], ["tokenization.convert_to_unicode", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize"], ["self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n            input = \"unaffable\"\n            output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n            text: A single token or whitespace separated tokens. This should have\n                already been passed through `BasicTokenizer.\n\n        Returns:\n            A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_to_unicode": [[27, 45], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode", "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode"], ["    ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.printable_text": [[47, 68], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode"], ["    ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.load_vocab": [[70, 83], ["collections.OrderedDict", "io.open", "enumerate", "convert_to_unicode().split", "token.strip.strip", "int", "len", "tokenization.convert_to_unicode", "len", "line.strip"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode"], ["    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "fin", "=", "open", "(", "vocab_file", ")", "\n", "for", "num", ",", "line", "in", "enumerate", "(", "fin", ")", ":", "\n", "        ", "items", "=", "convert_to_unicode", "(", "line", ".", "strip", "(", ")", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "len", "(", "items", ")", ">", "2", ":", "\n", "            ", "break", "\n", "", "token", "=", "items", "[", "0", "]", "\n", "index", "=", "items", "[", "1", "]", "if", "len", "(", "items", ")", "==", "2", "else", "num", "\n", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "int", "(", "index", ")", "\n", "", "return", "vocab", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab": [[85, 91], ["output.append"], "function", ["None"], ["    ", "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "        ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "return", "output", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_tokens_to_ids": [[93, 95], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab"], ["    ", "return", "convert_by_vocab", "(", "vocab", ",", "tokens", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_ids_to_tokens": [[97, 99], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.convert_by_vocab"], ["    ", "return", "convert_by_vocab", "(", "inv_vocab", ",", "ids", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.whitespace_tokenize": [[101, 108], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["    ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization._is_whitespace": [[334, 344], ["unicodedata.category"], "function", ["None"], ["", "", "return", "output_tokens", "\n", "\n", "\n", "", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization._is_control": [[346, 356], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "return", "False", "\n", "\n", "\n", "", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization._is_punctuation": [[358, 372], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "return", "False", "\n", "\n", "\n", "", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_record_twomemory.create_model": [[118, 262], ["model.bert.BertModel", "model.bert.BertModel.get_sequence_output", "logger.info", "paddle.layers.embedding", "paddle.layers.embedding", "paddle.layers.equal", "paddle.layers.cast", "paddle.layers.scale", "paddle.layers.reduce_sum", "paddle.layers.equal", "paddle.layers.cast", "paddle.layers.scale", "paddle.layers.reduce_sum", "model.layers.MemoryLayer", "model.layers.MemoryLayer.forward", "model.layers.MemoryLayer", "model.layers.MemoryLayer.forward", "paddle.layers.concat", "logger.info", "model.layers.TriLinearTwoTimeSelfAttentionLayer", "model.layers.TriLinearTwoTimeSelfAttentionLayer.forward", "paddle.layers.fc", "paddle.layers.transpose", "paddle.layers.unstack", "paddle.layers.fill_constant_batch_size_like", "paddle.layers.reduce_sum", "paddle.layers.py_reader", "paddle.layers.read_file", "paddle.layers.py_reader", "paddle.layers.read_file", "paddle.layers.fill_constant", "paddle.layers.elementwise_sub", "paddle.layers.fill_constant", "paddle.layers.elementwise_sub", "run_record_twomemory.create_model.compute_loss"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.get_sequence_output", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.forward", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.forward", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.forward", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.read_file", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.read_file"], ["def", "create_model", "(", "pyreader_name", ",", "bert_config", ",", "max_wn_concept_length", ",", "max_nell_concept_length", ",", "wn_concept_embedding_mat", ",", "nell_concept_embedding_mat", ",", "is_training", "=", "False", ",", "freeze", "=", "False", ")", ":", "\n", "    ", "if", "is_training", ":", "\n", "        ", "pyreader", "=", "fluid", ".", "layers", ".", "py_reader", "(", "\n", "capacity", "=", "50", ",", "\n", "shapes", "=", "[", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "max_wn_concept_length", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "max_nell_concept_length", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "1", "]", ",", "[", "-", "1", ",", "1", "]", "]", ",", "\n", "dtypes", "=", "[", "\n", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'float32'", ",", "'int64'", ",", "'int64'", "]", ",", "\n", "lod_levels", "=", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "name", "=", "pyreader_name", ",", "\n", "use_double_buffer", "=", "True", ")", "\n", "(", "src_ids", ",", "pos_ids", ",", "sent_ids", ",", "wn_concept_ids", ",", "nell_concept_ids", ",", "input_mask", ",", "start_positions", ",", "\n", "end_positions", ")", "=", "fluid", ".", "layers", ".", "read_file", "(", "pyreader", ")", "\n", "", "else", ":", "\n", "        ", "pyreader", "=", "fluid", ".", "layers", ".", "py_reader", "(", "\n", "capacity", "=", "50", ",", "\n", "shapes", "=", "[", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "max_wn_concept_length", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "max_nell_concept_length", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "1", "]", "]", ",", "\n", "dtypes", "=", "[", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'float32'", ",", "'int64'", "]", ",", "\n", "lod_levels", "=", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "name", "=", "pyreader_name", ",", "\n", "use_double_buffer", "=", "True", ")", "\n", "(", "src_ids", ",", "pos_ids", ",", "sent_ids", ",", "wn_concept_ids", ",", "nell_concept_ids", ",", "input_mask", ",", "unique_id", ")", "=", "fluid", ".", "layers", ".", "read_file", "(", "pyreader", ")", "\n", "\n", "", "'''1st Layer: BERT Layer'''", "\n", "bert", "=", "BertModel", "(", "\n", "src_ids", "=", "src_ids", ",", "\n", "position_ids", "=", "pos_ids", ",", "\n", "sentence_ids", "=", "sent_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "config", "=", "bert_config", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "\n", "enc_out", "=", "bert", ".", "get_sequence_output", "(", ")", "\n", "if", "freeze", ":", "\n", "        ", "enc_out", ".", "stop_gradient", "=", "True", "\n", "", "logger", ".", "info", "(", "\"enc_out.stop_gradient: {}\"", ".", "format", "(", "enc_out", ".", "stop_gradient", ")", ")", "\n", "\n", "'''2nd layer: Memory Layer'''", "\n", "# get memory embedding", "\n", "wn_concept_vocab_size", "=", "wn_concept_embedding_mat", ".", "shape", "[", "0", "]", "\n", "wn_concept_dim", "=", "wn_concept_embedding_mat", ".", "shape", "[", "1", "]", "\n", "nell_concept_vocab_size", "=", "nell_concept_embedding_mat", ".", "shape", "[", "0", "]", "\n", "nell_concept_dim", "=", "nell_concept_embedding_mat", ".", "shape", "[", "1", "]", "\n", "wn_memory_embs", "=", "fluid", ".", "layers", ".", "embedding", "(", "wn_concept_ids", ",", "\n", "size", "=", "(", "wn_concept_vocab_size", ",", "wn_concept_dim", ")", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "\"wn_concept_emb_mat\"", ",", "\n", "do_model_average", "=", "False", ",", "\n", "trainable", "=", "False", ")", ",", "\n", "dtype", "=", "'float32'", ")", "\n", "nell_memory_embs", "=", "fluid", ".", "layers", ".", "embedding", "(", "nell_concept_ids", ",", "\n", "size", "=", "(", "nell_concept_vocab_size", ",", "nell_concept_dim", ")", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "\"nell_concept_emb_mat\"", ",", "\n", "do_model_average", "=", "False", ",", "\n", "trainable", "=", "False", ")", ",", "\n", "dtype", "=", "'float32'", ")", "\n", "\n", "# get memory length", "\n", "wn_concept_ids_reduced", "=", "fluid", ".", "layers", ".", "equal", "(", "wn_concept_ids", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "shape", "=", "[", "1", "]", ",", "value", "=", "0", ",", "dtype", "=", "\"int64\"", ")", ")", "# [batch_size, sent_size, concept_size, 1]", "\n", "wn_concept_ids_reduced", "=", "fluid", ".", "layers", ".", "cast", "(", "wn_concept_ids_reduced", ",", "dtype", "=", "\"float32\"", ")", "# [batch_size, sent_size, concept_size, 1]", "\n", "wn_concept_ids_reduced", "=", "fluid", ".", "layers", ".", "scale", "(", "\n", "fluid", ".", "layers", ".", "elementwise_sub", "(", "\n", "wn_concept_ids_reduced", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "[", "1", "]", ",", "\"float32\"", ",", "1", ")", "\n", ")", ",", "\n", "scale", "=", "-", "1", "\n", ")", "\n", "wn_mem_length", "=", "fluid", ".", "layers", ".", "reduce_sum", "(", "wn_concept_ids_reduced", ",", "dim", "=", "2", ")", "# [batch_size, sent_size, 1]    ", "\n", "\n", "nell_concept_ids_reduced", "=", "fluid", ".", "layers", ".", "equal", "(", "nell_concept_ids", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "shape", "=", "[", "1", "]", ",", "value", "=", "0", ",", "dtype", "=", "\"int64\"", ")", ")", "# [batch_size, sent_size, concept_size, 1]", "\n", "nell_concept_ids_reduced", "=", "fluid", ".", "layers", ".", "cast", "(", "nell_concept_ids_reduced", ",", "dtype", "=", "\"float32\"", ")", "# [batch_size, sent_size, concept_size, 1]", "\n", "nell_concept_ids_reduced", "=", "fluid", ".", "layers", ".", "scale", "(", "\n", "fluid", ".", "layers", ".", "elementwise_sub", "(", "\n", "nell_concept_ids_reduced", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "[", "1", "]", ",", "\"float32\"", ",", "1", ")", "\n", ")", ",", "\n", "scale", "=", "-", "1", "\n", ")", "\n", "nell_mem_length", "=", "fluid", ".", "layers", ".", "reduce_sum", "(", "nell_concept_ids_reduced", ",", "dim", "=", "2", ")", "# [batch_size, sent_size, 1]      ", "\n", "\n", "# select and integrate", "\n", "wn_memory_layer", "=", "MemoryLayer", "(", "bert_config", ",", "max_wn_concept_length", ",", "wn_concept_dim", ",", "mem_method", "=", "'raw'", ",", "prefix", "=", "'wn'", ")", "\n", "wn_memory_output", "=", "wn_memory_layer", ".", "forward", "(", "enc_out", ",", "wn_memory_embs", ",", "wn_mem_length", ",", "ignore_no_memory_token", "=", "True", ")", "\n", "\n", "nell_memory_layer", "=", "MemoryLayer", "(", "bert_config", ",", "max_nell_concept_length", ",", "nell_concept_dim", ",", "mem_method", "=", "'raw'", ",", "prefix", "=", "'nell'", ")", "\n", "nell_memory_output", "=", "nell_memory_layer", ".", "forward", "(", "enc_out", ",", "nell_memory_embs", ",", "nell_mem_length", ",", "ignore_no_memory_token", "=", "True", ")", "\n", "\n", "memory_output", "=", "fluid", ".", "layers", ".", "concat", "(", "[", "enc_out", ",", "wn_memory_output", ",", "nell_memory_output", "]", ",", "axis", "=", "2", ")", "\n", "\n", "'''3rd layer: Self-Matching Layer'''", "\n", "# calculate input dim for self-matching layer", "\n", "memory_output_size", "=", "bert_config", "[", "'hidden_size'", "]", "+", "wn_concept_dim", "+", "nell_concept_dim", "\n", "logger", ".", "info", "(", "\"memory_output_size: {}\"", ".", "format", "(", "memory_output_size", ")", ")", "\n", "\n", "# do matching", "\n", "self_att_layer", "=", "TriLinearTwoTimeSelfAttentionLayer", "(", "\n", "memory_output_size", ",", "dropout_rate", "=", "0.0", ",", "\n", "cat_mul", "=", "True", ",", "cat_sub", "=", "True", ",", "cat_twotime", "=", "True", ",", "\n", "cat_twotime_mul", "=", "False", ",", "cat_twotime_sub", "=", "True", ")", "# [bs, sq, concat_hs]", "\n", "att_output", "=", "self_att_layer", ".", "forward", "(", "memory_output", ",", "input_mask", ")", "# [bs, sq, concat_hs]", "\n", "\n", "'''4th layer: Output Layer'''", "\n", "logits", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "att_output", ",", "\n", "size", "=", "2", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"cls_squad_out_w\"", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "NormalInitializer", "(", "loc", "=", "0.0", ",", "scale", "=", "bert_config", "[", "'initializer_range'", "]", ")", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"cls_squad_out_b\"", ",", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", ")", "\n", "\n", "logits", "=", "fluid", ".", "layers", ".", "transpose", "(", "x", "=", "logits", ",", "perm", "=", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "start_logits", ",", "end_logits", "=", "fluid", ".", "layers", ".", "unstack", "(", "x", "=", "logits", ",", "axis", "=", "0", ")", "\n", "\n", "batch_ones", "=", "fluid", ".", "layers", ".", "fill_constant_batch_size_like", "(", "\n", "input", "=", "start_logits", ",", "dtype", "=", "'int64'", ",", "shape", "=", "[", "1", "]", ",", "value", "=", "1", ")", "\n", "num_seqs", "=", "fluid", ".", "layers", ".", "reduce_sum", "(", "input", "=", "batch_ones", ")", "\n", "\n", "if", "is_training", ":", "\n", "\n", "        ", "def", "compute_loss", "(", "logits", ",", "positions", ")", ":", "\n", "            ", "loss", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "logits", ",", "label", "=", "positions", ")", "\n", "loss", "=", "fluid", ".", "layers", ".", "mean", "(", "x", "=", "loss", ")", "\n", "return", "loss", "\n", "\n", "", "start_loss", "=", "compute_loss", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "compute_loss", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2.0", "\n", "if", "args", ".", "use_fp16", "and", "args", ".", "loss_scaling", ">", "1.0", ":", "\n", "            ", "total_loss", "=", "total_loss", "*", "args", ".", "loss_scaling", "\n", "\n", "", "return", "pyreader", ",", "total_loss", ",", "num_seqs", "\n", "", "else", ":", "\n", "        ", "return", "pyreader", ",", "unique_id", ",", "start_logits", ",", "end_logits", ",", "num_seqs", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_record_twomemory.predict": [[268, 308], ["os.path.join", "os.path.join", "os.path.join", "os.path.join", "test_pyreader.start", "time.time", "time.time", "processor.get_features", "reader.record_twomemory.write_predictions", "os.path.exists", "os.makedirs", "test_exe.run", "range", "int", "all_results.append", "test_pyreader.reset", "logger.info", "float", "float", "RawResult", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_features", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.write_predictions", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["def", "predict", "(", "test_exe", ",", "test_program", ",", "test_pyreader", ",", "fetch_list", ",", "processor", ",", "eval_concept_settings", ",", "eval_output_name", "=", "'eval_result.json'", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "checkpoints", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "checkpoints", ")", "\n", "", "output_prediction_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\"predictions.json\"", ")", "\n", "output_nbest_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\"nbest_predictions.json\"", ")", "\n", "output_null_log_odds_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\"null_odds.json\"", ")", "\n", "output_evaluation_result_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "eval_output_name", ")", "\n", "\n", "test_pyreader", ".", "start", "(", ")", "\n", "all_results", "=", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "np_unique_ids", ",", "np_start_logits", ",", "np_end_logits", ",", "np_num_seqs", "=", "test_exe", ".", "run", "(", "\n", "fetch_list", "=", "fetch_list", ",", "program", "=", "test_program", ")", "\n", "for", "idx", "in", "range", "(", "np_unique_ids", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "if", "len", "(", "all_results", ")", "%", "1000", "==", "0", ":", "\n", "                    ", "logger", ".", "info", "(", "\"Processing example: %d\"", "%", "len", "(", "all_results", ")", ")", "\n", "", "unique_id", "=", "int", "(", "np_unique_ids", "[", "idx", "]", ")", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "np_start_logits", "[", "idx", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "np_end_logits", "[", "idx", "]", ".", "flat", "]", "\n", "all_results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "", "", "except", "fluid", ".", "core", ".", "EOFException", ":", "\n", "            ", "test_pyreader", ".", "reset", "(", ")", "\n", "break", "\n", "", "", "time_end", "=", "time", ".", "time", "(", ")", "\n", "\n", "features", "=", "processor", ".", "get_features", "(", "\n", "processor", ".", "predict_examples", ",", "is_training", "=", "False", ",", "**", "eval_concept_settings", ")", "\n", "eval_result", "=", "write_predictions", "(", "processor", ".", "predict_examples", ",", "features", ",", "all_results", ",", "\n", "args", ".", "n_best_size", ",", "args", ".", "max_answer_length", ",", "\n", "args", ".", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "output_null_log_odds_file", ",", "\n", "args", ".", "version_2_with_negative", ",", "\n", "args", ".", "null_score_diff_threshold", ",", "args", ".", "verbose", ",", "args", ".", "predict_file", ",", "output_evaluation_result_file", ")", "\n", "return", "eval_result", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_record_twomemory.read_concept_embedding": [[309, 329], ["open", "len", "len", "id2concept.append", "np.array.append", "numpy.array", "line.strip", "np.array.append", "len", "id2concept.append", "info[].split", "line.split", "float", "range", "len", "numpy.any", "line.split", "numpy.isnan"], "function", ["None"], ["", "def", "read_concept_embedding", "(", "embedding_path", ")", ":", "\n", "    ", "fin", "=", "open", "(", "embedding_path", ",", "encoding", "=", "'utf-8'", ")", "\n", "info", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "fin", "]", "\n", "dim", "=", "len", "(", "info", "[", "0", "]", ".", "split", "(", "' '", ")", "[", "1", ":", "]", ")", "\n", "n_concept", "=", "len", "(", "info", ")", "\n", "embedding_mat", "=", "[", "]", "\n", "id2concept", ",", "concept2id", "=", "[", "]", ",", "{", "}", "\n", "# add padding concept into vocab", "\n", "id2concept", ".", "append", "(", "'<pad_concept>'", ")", "\n", "concept2id", "[", "'<pad_concept>'", "]", "=", "0", "\n", "embedding_mat", ".", "append", "(", "[", "0.0", "for", "_", "in", "range", "(", "dim", ")", "]", ")", "\n", "for", "line", "in", "info", ":", "\n", "        ", "concept_name", "=", "line", ".", "split", "(", "' '", ")", "[", "0", "]", "\n", "embedding", "=", "[", "float", "(", "value_str", ")", "for", "value_str", "in", "line", ".", "split", "(", "' '", ")", "[", "1", ":", "]", "]", "\n", "assert", "len", "(", "embedding", ")", "==", "dim", "and", "not", "np", ".", "any", "(", "np", ".", "isnan", "(", "embedding", ")", ")", "\n", "embedding_mat", ".", "append", "(", "embedding", ")", "\n", "concept2id", "[", "concept_name", "]", "=", "len", "(", "id2concept", ")", "\n", "id2concept", ".", "append", "(", "concept_name", ")", "\n", "", "embedding_mat", "=", "np", ".", "array", "(", "embedding_mat", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "return", "id2concept", ",", "concept2id", ",", "embedding_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_record_twomemory.train": [[330, 622], ["model.bert.BertConfig", "model.bert.BertConfig.print_config", "paddle.Executor", "run_record_twomemory.read_concept_embedding", "run_record_twomemory.read_concept_embedding", "reader.record_twomemory.DataProcessor", "paddle.Program", "fluid.Executor.run", "ValueError", "paddle.CUDAPlace", "paddle.core.get_cuda_device_count", "paddle.CPUPlace", "int", "random.seed", "numpy.random.seed", "reader.record_twomemory.DataProcessor.data_generator", "reader.record_twomemory.DataProcessor.get_num_examples", "int", "logger.info", "logger.info", "logger.info", "logger.info", "paddle.Program", "reader.record_twomemory.DataProcessor.data_generator", "paddle.Program", "test_prog.clone.clone", "logger.info", "paddle.global_scope().find_var().get_tensor().set", "paddle.global_scope().find_var().get_tensor().set", "paddle.ExecutionStrategy", "paddle.ParallelExecutor", "train_pyreader.decorate_tensor_provider", "train_pyreader.start", "time.time", "test_pyreader.decorate_tensor_provider", "logger.info", "os.environ.get", "paddle.program_guard", "logger.info", "paddle.program_guard", "logger.info", "utils.init.init_checkpoint", "utils.init.init_checkpoint", "run_record_twomemory.predict", "multiprocessing.cpu_count", "paddle.unique_name.guard", "run_record_twomemory.create_model", "optimization.optimization", "paddle.memory_optimize", "paddle.contrib.memory_usage", "paddle.contrib.memory_usage", "paddle.unique_name.guard", "run_record_twomemory.create_model", "paddle.memory_optimize", "paddle.global_scope().find_var().get_tensor", "paddle.global_scope().find_var().get_tensor", "utils.init.init_pretraining_params", "ValueError", "fluid.ParallelExecutor.run", "fluid.optimizer.ExponentialMovingAverage.apply", "run_record_twomemory.predict", "paddle.optimizer.ExponentialMovingAverage", "fluid.optimizer.ExponentialMovingAverage.update", "paddle.optimizer.ExponentialMovingAverage", "total_cost.extend", "total_num_seqs.extend", "time.time", "reader.record_twomemory.DataProcessor.get_train_progress", "logger.info", "time.time", "os.path.join", "paddle.io.save_persistables", "os.path.join", "paddle.io.save_persistables", "train_pyreader.reset", "dir", "paddle.global_scope().find_var", "paddle.global_scope().find_var", "logger.info", "test_pyreader.decorate_tensor_provider", "run_record_twomemory.predict", "logger.info", "train_pyreader.queue.size", "str", "reader.record_twomemory.DataProcessor.data_generator", "paddle.global_scope", "paddle.global_scope", "str", "numpy.sum", "numpy.sum"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig.print_config", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad.read_concept_embedding", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad.read_concept_embedding", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_num_examples", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_checkpoint", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_checkpoint", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.create_model", "home.repos.pwc.inspect_result.baidu_DuReader.src.optimization.optimization", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.create_model", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_pretraining_params", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_train_progress", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator"], ["", "def", "train", "(", "args", ")", ":", "\n", "    ", "bert_config", "=", "BertConfig", "(", "args", ".", "bert_config_path", ")", "\n", "bert_config", ".", "print_config", "(", ")", "\n", "\n", "if", "not", "(", "args", ".", "do_train", "or", "args", ".", "do_predict", "or", "args", ".", "do_val", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"For args `do_train` and `do_predict`, at \"", "\n", "\"least one of them must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "use_cuda", ":", "\n", "        ", "place", "=", "fluid", ".", "CUDAPlace", "(", "0", ")", "\n", "dev_count", "=", "fluid", ".", "core", ".", "get_cuda_device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "place", "=", "fluid", ".", "CPUPlace", "(", ")", "\n", "dev_count", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'CPU_NUM'", ",", "multiprocessing", ".", "cpu_count", "(", ")", ")", ")", "\n", "", "exe", "=", "fluid", ".", "Executor", "(", "place", ")", "\n", "\n", "wn_id2concept", ",", "wn_concept2id", ",", "wn_concept_embedding_mat", "=", "read_concept_embedding", "(", "\n", "args", ".", "wn_concept_embedding_path", ")", "\n", "nell_id2concept", ",", "nell_concept2id", ",", "nell_concept_embedding_mat", "=", "read_concept_embedding", "(", "\n", "args", ".", "nell_concept_embedding_path", ")", "\n", "\n", "processor", "=", "DataProcessor", "(", "\n", "vocab_path", "=", "args", ".", "vocab_path", ",", "\n", "do_lower_case", "=", "args", ".", "do_lower_case", ",", "\n", "max_seq_length", "=", "args", ".", "max_seq_len", ",", "\n", "in_tokens", "=", "args", ".", "in_tokens", ",", "\n", "doc_stride", "=", "args", ".", "doc_stride", ",", "\n", "max_query_length", "=", "args", ".", "max_query_length", ")", "\n", "\n", "startup_prog", "=", "fluid", ".", "Program", "(", ")", "\n", "if", "args", ".", "random_seed", "is", "not", "None", ":", "\n", "        ", "startup_prog", ".", "random_seed", "=", "args", ".", "random_seed", "\n", "random", ".", "seed", "(", "args", ".", "random_seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "random_seed", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "train_concept_settings", "=", "{", "\n", "'tokenization_path'", ":", "'../retrieve_concepts/tokenization_record/tokens/train.tokenization.{}.data'", ".", "format", "(", "'uncased'", "if", "args", ".", "do_lower_case", "else", "'cased'", ")", ",", "\n", "'wn_concept2id'", ":", "wn_concept2id", ",", "\n", "'nell_concept2id'", ":", "nell_concept2id", ",", "\n", "'use_wordnet'", ":", "args", ".", "use_wordnet", ",", "\n", "'retrieved_synset_path'", ":", "args", ".", "retrieved_synset_path", ",", "\n", "'use_nell'", ":", "args", ".", "use_nell", ",", "\n", "'retrieved_nell_concept_path'", ":", "args", ".", "train_retrieved_nell_concept_path", ",", "\n", "}", "\n", "train_data_generator", "=", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "train_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'train'", ",", "\n", "shuffle", "=", "True", ",", "\n", "dev_count", "=", "dev_count", ",", "\n", "version_2_with_negative", "=", "args", ".", "version_2_with_negative", ",", "\n", "epoch", "=", "args", ".", "epoch", ",", "\n", "**", "train_concept_settings", ")", "\n", "\n", "num_train_examples", "=", "processor", ".", "get_num_examples", "(", "phase", "=", "'train'", ")", "\n", "if", "args", ".", "in_tokens", ":", "\n", "            ", "max_train_steps", "=", "args", ".", "epoch", "*", "num_train_examples", "//", "(", "\n", "args", ".", "batch_size", "//", "args", ".", "max_seq_len", ")", "//", "dev_count", "\n", "", "else", ":", "\n", "            ", "max_train_steps", "=", "args", ".", "epoch", "*", "num_train_examples", "//", "(", "\n", "args", ".", "batch_size", ")", "//", "dev_count", "\n", "", "warmup_steps", "=", "int", "(", "max_train_steps", "*", "args", ".", "warmup_proportion", ")", "\n", "logger", ".", "info", "(", "\"Device count: %d\"", "%", "dev_count", ")", "\n", "logger", ".", "info", "(", "\"Num train examples: %d\"", "%", "num_train_examples", ")", "\n", "logger", ".", "info", "(", "\"Max train steps: %d\"", "%", "max_train_steps", ")", "\n", "logger", ".", "info", "(", "\"Num warmup steps: %d\"", "%", "warmup_steps", ")", "\n", "\n", "train_program", "=", "fluid", ".", "Program", "(", ")", "\n", "# if args.random_seed is not None:", "\n", "#     train_program.random_seed = args.random_seed", "\n", "with", "fluid", ".", "program_guard", "(", "train_program", ",", "startup_prog", ")", ":", "\n", "            ", "with", "fluid", ".", "unique_name", ".", "guard", "(", ")", ":", "\n", "                ", "train_pyreader", ",", "loss", ",", "num_seqs", "=", "create_model", "(", "\n", "pyreader_name", "=", "'train_reader'", ",", "\n", "bert_config", "=", "bert_config", ",", "\n", "max_wn_concept_length", "=", "processor", ".", "train_wn_max_concept_length", ",", "\n", "max_nell_concept_length", "=", "processor", ".", "train_nell_max_concept_length", ",", "\n", "wn_concept_embedding_mat", "=", "wn_concept_embedding_mat", ",", "\n", "nell_concept_embedding_mat", "=", "nell_concept_embedding_mat", ",", "\n", "is_training", "=", "True", ",", "\n", "freeze", "=", "args", ".", "freeze", ")", "\n", "\n", "scheduled_lr", "=", "optimization", "(", "\n", "loss", "=", "loss", ",", "\n", "warmup_steps", "=", "warmup_steps", ",", "\n", "num_train_steps", "=", "max_train_steps", ",", "\n", "learning_rate", "=", "args", ".", "learning_rate", ",", "\n", "train_program", "=", "train_program", ",", "\n", "startup_prog", "=", "startup_prog", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", "scheduler", "=", "args", ".", "lr_scheduler", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ",", "\n", "loss_scaling", "=", "args", ".", "loss_scaling", ")", "\n", "\n", "if", "args", ".", "use_ema", ":", "\n", "                    ", "ema", "=", "fluid", ".", "optimizer", ".", "ExponentialMovingAverage", "(", "args", ".", "ema_decay", ")", "\n", "ema", ".", "update", "(", ")", "\n", "\n", "", "fluid", ".", "memory_optimize", "(", "train_program", ",", "skip_opt_set", "=", "[", "loss", ".", "name", ",", "num_seqs", ".", "name", "]", ")", "\n", "\n", "", "", "if", "args", ".", "verbose", ":", "\n", "            ", "if", "args", ".", "in_tokens", ":", "\n", "                ", "lower_mem", ",", "upper_mem", ",", "unit", "=", "fluid", ".", "contrib", ".", "memory_usage", "(", "\n", "program", "=", "train_program", ",", "\n", "batch_size", "=", "args", ".", "batch_size", "//", "args", ".", "max_seq_len", ")", "\n", "", "else", ":", "\n", "                ", "lower_mem", ",", "upper_mem", ",", "unit", "=", "fluid", ".", "contrib", ".", "memory_usage", "(", "\n", "program", "=", "train_program", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "", "logger", ".", "info", "(", "\"Theoretical memory usage in training:  %.3f - %.3f %s\"", "%", "\n", "(", "lower_mem", ",", "upper_mem", ",", "unit", ")", ")", "\n", "\n", "", "", "if", "args", ".", "do_predict", "or", "args", ".", "do_val", ":", "\n", "        ", "eval_concept_settings", "=", "{", "\n", "'tokenization_path'", ":", "'../retrieve_concepts/tokenization_record/tokens/dev.tokenization.{}.data'", ".", "format", "(", "'uncased'", "if", "args", ".", "do_lower_case", "else", "'cased'", ")", ",", "\n", "'wn_concept2id'", ":", "wn_concept2id", ",", "\n", "'nell_concept2id'", ":", "nell_concept2id", ",", "\n", "'use_wordnet'", ":", "args", ".", "use_wordnet", ",", "\n", "'retrieved_synset_path'", ":", "args", ".", "retrieved_synset_path", ",", "\n", "'use_nell'", ":", "args", ".", "use_nell", ",", "\n", "'retrieved_nell_concept_path'", ":", "args", ".", "dev_retrieved_nell_concept_path", ",", "\n", "}", "\n", "eval_data_generator", "=", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "predict_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'predict'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "epoch", "=", "1", ",", "\n", "**", "eval_concept_settings", ")", "\n", "\n", "test_prog", "=", "fluid", ".", "Program", "(", ")", "\n", "# if args.random_seed is not None:", "\n", "#     test_prog.random_seed = args.random_seed", "\n", "with", "fluid", ".", "program_guard", "(", "test_prog", ",", "startup_prog", ")", ":", "\n", "            ", "with", "fluid", ".", "unique_name", ".", "guard", "(", ")", ":", "\n", "                ", "test_pyreader", ",", "unique_ids", ",", "start_logits", ",", "end_logits", ",", "num_seqs", "=", "create_model", "(", "\n", "pyreader_name", "=", "'test_reader'", ",", "\n", "bert_config", "=", "bert_config", ",", "\n", "max_wn_concept_length", "=", "processor", ".", "predict_wn_max_concept_length", ",", "\n", "max_nell_concept_length", "=", "processor", ".", "predict_nell_max_concept_length", ",", "\n", "wn_concept_embedding_mat", "=", "wn_concept_embedding_mat", ",", "\n", "nell_concept_embedding_mat", "=", "nell_concept_embedding_mat", ",", "\n", "is_training", "=", "False", ")", "\n", "\n", "if", "args", ".", "use_ema", "and", "'ema'", "not", "in", "dir", "(", ")", ":", "\n", "                    ", "ema", "=", "fluid", ".", "optimizer", ".", "ExponentialMovingAverage", "(", "args", ".", "ema_decay", ")", "\n", "\n", "", "fluid", ".", "memory_optimize", "(", "test_prog", ",", "skip_opt_set", "=", "[", "unique_ids", ".", "name", ",", "\n", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "]", ")", "\n", "\n", "", "", "test_prog", "=", "test_prog", ".", "clone", "(", "for_test", "=", "True", ")", "\n", "# if args.random_seed is not None:", "\n", "#     test_prog.random_seed = args.random_seed", "\n", "\n", "", "exe", ".", "run", "(", "startup_prog", ")", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "info", "(", "'load pretrained concept embedding'", ")", "\n", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "'wn_concept_emb_mat'", ")", ".", "get_tensor", "(", ")", ".", "set", "(", "wn_concept_embedding_mat", ",", "place", ")", "\n", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "'nell_concept_emb_mat'", ")", ".", "get_tensor", "(", ")", ".", "set", "(", "nell_concept_embedding_mat", ",", "place", ")", "\n", "\n", "if", "args", ".", "init_checkpoint", "and", "args", ".", "init_pretraining_params", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"WARNING: args 'init_checkpoint' and 'init_pretraining_params' \"", "\n", "\"both are set! Only arg 'init_checkpoint' is made valid.\"", ")", "\n", "", "if", "args", ".", "init_checkpoint", ":", "\n", "            ", "init_checkpoint", "(", "\n", "exe", ",", "\n", "args", ".", "init_checkpoint", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "", "elif", "args", ".", "init_pretraining_params", ":", "\n", "            ", "init_pretraining_params", "(", "\n", "exe", ",", "\n", "args", ".", "init_pretraining_params", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "", "", "elif", "args", ".", "do_predict", "or", "args", ".", "do_val", ":", "\n", "        ", "if", "not", "args", ".", "init_checkpoint", ":", "\n", "            ", "raise", "ValueError", "(", "\"args 'init_checkpoint' should be set if\"", "\n", "\"only doing prediction!\"", ")", "\n", "", "init_checkpoint", "(", "\n", "exe", ",", "\n", "args", ".", "init_checkpoint", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "exec_strategy", "=", "fluid", ".", "ExecutionStrategy", "(", ")", "\n", "exec_strategy", ".", "use_experimental_executor", "=", "args", ".", "use_fast_executor", "\n", "exec_strategy", ".", "num_threads", "=", "dev_count", "\n", "exec_strategy", ".", "num_iteration_per_drop_scope", "=", "args", ".", "num_iteration_per_drop_scope", "\n", "\n", "train_exe", "=", "fluid", ".", "ParallelExecutor", "(", "\n", "use_cuda", "=", "args", ".", "use_cuda", ",", "\n", "loss_name", "=", "loss", ".", "name", ",", "\n", "exec_strategy", "=", "exec_strategy", ",", "\n", "main_program", "=", "train_program", ")", "\n", "\n", "train_pyreader", ".", "decorate_tensor_provider", "(", "train_data_generator", ")", "\n", "\n", "train_pyreader", ".", "start", "(", ")", "\n", "steps", "=", "0", "\n", "total_cost", ",", "total_num_seqs", "=", "[", "]", ",", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "while", "steps", "<", "max_train_steps", ":", "\n", "            ", "try", ":", "\n", "                ", "steps", "+=", "1", "\n", "if", "steps", "%", "args", ".", "skip_steps", "==", "0", ":", "\n", "                    ", "if", "warmup_steps", "<=", "0", ":", "\n", "                        ", "fetch_list", "=", "[", "loss", ".", "name", ",", "num_seqs", ".", "name", "]", "\n", "", "else", ":", "\n", "                        ", "fetch_list", "=", "[", "\n", "loss", ".", "name", ",", "scheduled_lr", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", "\n", "", "", "else", ":", "\n", "                    ", "fetch_list", "=", "[", "]", "\n", "\n", "", "outputs", "=", "train_exe", ".", "run", "(", "fetch_list", "=", "fetch_list", ")", "\n", "\n", "if", "steps", "%", "args", ".", "skip_steps", "==", "0", ":", "\n", "                    ", "if", "warmup_steps", "<=", "0", ":", "\n", "                        ", "np_loss", ",", "np_num_seqs", "=", "outputs", "\n", "", "else", ":", "\n", "                        ", "np_loss", ",", "np_lr", ",", "np_num_seqs", "=", "outputs", "\n", "", "total_cost", ".", "extend", "(", "np_loss", "*", "np_num_seqs", ")", "\n", "total_num_seqs", ".", "extend", "(", "np_num_seqs", ")", "\n", "\n", "if", "args", ".", "verbose", ":", "\n", "                        ", "verbose", "=", "\"train pyreader queue size: %d, \"", "%", "train_pyreader", ".", "queue", ".", "size", "(", "\n", ")", "\n", "verbose", "+=", "\"learning rate: %f\"", "%", "(", "\n", "np_lr", "[", "0", "]", "\n", "if", "warmup_steps", ">", "0", "else", "args", ".", "learning_rate", ")", "\n", "logger", ".", "info", "(", "verbose", ")", "\n", "\n", "", "time_end", "=", "time", ".", "time", "(", ")", "\n", "used_time", "=", "time_end", "-", "time_begin", "\n", "current_example", ",", "epoch", "=", "processor", ".", "get_train_progress", "(", ")", "\n", "\n", "logger", ".", "info", "(", "\"epoch: %d, progress: %d/%d, step: %d, loss: %f, \"", "\n", "\"speed: %f steps/s\"", "%", "\n", "(", "epoch", ",", "current_example", ",", "num_train_examples", ",", "steps", ",", "\n", "np", ".", "sum", "(", "total_cost", ")", "/", "np", ".", "sum", "(", "total_num_seqs", ")", ",", "\n", "args", ".", "skip_steps", "/", "used_time", ")", ")", "\n", "total_cost", ",", "total_num_seqs", "=", "[", "]", ",", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "steps", "%", "args", ".", "save_steps", "==", "0", "or", "steps", "==", "max_train_steps", ":", "\n", "                    ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\n", "\"step_\"", "+", "str", "(", "steps", ")", ")", "\n", "fluid", ".", "io", ".", "save_persistables", "(", "exe", ",", "save_path", ",", "train_program", ")", "\n", "\n", "", "if", "steps", "%", "args", ".", "validation_steps", "==", "0", "or", "steps", "==", "max_train_steps", ":", "\n", "                    ", "if", "args", ".", "do_val", ":", "\n", "                        ", "test_pyreader", ".", "decorate_tensor_provider", "(", "\n", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "predict_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'predict'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "epoch", "=", "1", ",", "\n", "**", "eval_concept_settings", ")", "\n", ")", "\n", "val_performance", "=", "predict", "(", "exe", ",", "test_prog", ",", "test_pyreader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "eval_concept_settings", ",", "'validate_result_step_{}.json'", ".", "format", "(", "steps", ")", ")", "\n", "logger", ".", "info", "(", "\"Validation performance after step {}:\\n* Exact_match: {}\\n* F1: {}\"", ".", "format", "(", "steps", ",", "val_performance", "[", "'exact_match'", "]", ",", "val_performance", "[", "'f1'", "]", ")", ")", "\n", "\n", "", "", "", "except", "fluid", ".", "core", ".", "EOFException", ":", "\n", "                ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\n", "\"step_\"", "+", "str", "(", "steps", ")", "+", "\"_final\"", ")", "\n", "fluid", ".", "io", ".", "save_persistables", "(", "exe", ",", "save_path", ",", "train_program", ")", "\n", "train_pyreader", ".", "reset", "(", ")", "\n", "break", "\n", "\n", "", "", "", "if", "args", ".", "do_predict", ":", "\n", "        ", "test_pyreader", ".", "decorate_tensor_provider", "(", "eval_data_generator", ")", "\n", "\n", "if", "args", ".", "use_ema", ":", "\n", "            ", "with", "ema", ".", "apply", "(", "exe", ")", ":", "\n", "                ", "eval_performance", "=", "predict", "(", "exe", ",", "test_prog", ",", "test_pyreader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "eval_concept_settings", ")", "\n", "", "", "else", ":", "\n", "            ", "eval_performance", "=", "predict", "(", "exe", ",", "test_prog", ",", "test_pyreader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "eval_concept_settings", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Eval performance:\\n* Exact_match: {}\\n* F1: {}\"", ".", "format", "(", "eval_performance", "[", "'exact_match'", "]", ",", "eval_performance", "[", "'f1'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad.create_model": [[117, 239], ["model.bert.BertModel", "model.bert.BertModel.get_sequence_output", "logger.info", "paddle.layers.embedding", "paddle.layers.equal", "paddle.layers.cast", "paddle.layers.scale", "paddle.layers.reduce_sum", "model.layers.MemoryLayer", "model.layers.MemoryLayer.forward", "logger.info", "model.layers.TriLinearTwoTimeSelfAttentionLayer", "model.layers.TriLinearTwoTimeSelfAttentionLayer.forward", "paddle.layers.fc", "paddle.layers.transpose", "paddle.layers.unstack", "paddle.layers.fill_constant_batch_size_like", "paddle.layers.reduce_sum", "paddle.layers.py_reader", "paddle.layers.read_file", "paddle.layers.py_reader", "paddle.layers.read_file", "paddle.layers.fill_constant", "paddle.layers.elementwise_sub", "run_squad.create_model.compute_loss"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.get_sequence_output", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.forward", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.forward", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.read_file", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.read_file"], ["def", "create_model", "(", "pyreader_name", ",", "bert_config", ",", "max_concept_length", ",", "concept_embedding_mat", ",", "is_training", "=", "False", ",", "freeze", "=", "False", ")", ":", "\n", "    ", "if", "is_training", ":", "\n", "        ", "pyreader", "=", "fluid", ".", "layers", ".", "py_reader", "(", "\n", "capacity", "=", "50", ",", "\n", "shapes", "=", "[", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "max_concept_length", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "1", "]", ",", "[", "-", "1", ",", "1", "]", "]", ",", "\n", "dtypes", "=", "[", "\n", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'float32'", ",", "'int64'", ",", "'int64'", "]", ",", "\n", "lod_levels", "=", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "name", "=", "pyreader_name", ",", "\n", "use_double_buffer", "=", "True", ")", "\n", "(", "src_ids", ",", "pos_ids", ",", "sent_ids", ",", "concept_ids", ",", "input_mask", ",", "start_positions", ",", "\n", "end_positions", ")", "=", "fluid", ".", "layers", ".", "read_file", "(", "pyreader", ")", "\n", "", "else", ":", "\n", "        ", "pyreader", "=", "fluid", ".", "layers", ".", "py_reader", "(", "\n", "capacity", "=", "50", ",", "\n", "shapes", "=", "[", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "max_concept_length", ",", "1", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "max_seq_len", ",", "1", "]", ",", "[", "-", "1", ",", "1", "]", "]", ",", "\n", "dtypes", "=", "[", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'float32'", ",", "'int64'", "]", ",", "\n", "lod_levels", "=", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "name", "=", "pyreader_name", ",", "\n", "use_double_buffer", "=", "True", ")", "\n", "(", "src_ids", ",", "pos_ids", ",", "sent_ids", ",", "concept_ids", ",", "input_mask", ",", "unique_id", ")", "=", "fluid", ".", "layers", ".", "read_file", "(", "pyreader", ")", "\n", "\n", "", "'''1st Layer: BERT Layer'''", "\n", "bert", "=", "BertModel", "(", "\n", "src_ids", "=", "src_ids", ",", "\n", "position_ids", "=", "pos_ids", ",", "\n", "sentence_ids", "=", "sent_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "config", "=", "bert_config", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "\n", "enc_out", "=", "bert", ".", "get_sequence_output", "(", ")", "\n", "if", "freeze", ":", "\n", "        ", "enc_out", ".", "stop_gradient", "=", "True", "\n", "", "logger", ".", "info", "(", "\"enc_out.stop_gradient: {}\"", ".", "format", "(", "enc_out", ".", "stop_gradient", ")", ")", "\n", "\n", "'''2nd layer: Memory Layer'''", "\n", "# get memory embedding", "\n", "concept_vocab_size", "=", "concept_embedding_mat", ".", "shape", "[", "0", "]", "\n", "concept_dim", "=", "concept_embedding_mat", ".", "shape", "[", "1", "]", "\n", "memory_embs", "=", "fluid", ".", "layers", ".", "embedding", "(", "concept_ids", ",", "\n", "size", "=", "(", "concept_vocab_size", ",", "concept_dim", ")", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "name", "=", "\"concept_emb_mat\"", ",", "\n", "do_model_average", "=", "False", ",", "\n", "trainable", "=", "False", ")", ",", "\n", "dtype", "=", "'float32'", ")", "\n", "\n", "# get memory length", "\n", "concept_ids_reduced", "=", "fluid", ".", "layers", ".", "equal", "(", "concept_ids", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "shape", "=", "[", "1", "]", ",", "value", "=", "0", ",", "dtype", "=", "\"int64\"", ")", ")", "# [batch_size, sent_size, concept_size, 1]", "\n", "concept_ids_reduced", "=", "fluid", ".", "layers", ".", "cast", "(", "concept_ids_reduced", ",", "dtype", "=", "\"float32\"", ")", "# [batch_size, sent_size, concept_size, 1]", "\n", "concept_ids_reduced", "=", "fluid", ".", "layers", ".", "scale", "(", "\n", "fluid", ".", "layers", ".", "elementwise_sub", "(", "\n", "concept_ids_reduced", ",", "\n", "fluid", ".", "layers", ".", "fill_constant", "(", "[", "1", "]", ",", "\"float32\"", ",", "1", ")", "\n", ")", ",", "\n", "scale", "=", "-", "1", "\n", ")", "\n", "mem_length", "=", "fluid", ".", "layers", ".", "reduce_sum", "(", "concept_ids_reduced", ",", "dim", "=", "2", ")", "# [batch_size, sent_size, 1]    ", "\n", "\n", "# select and integrate", "\n", "memory_layer", "=", "MemoryLayer", "(", "bert_config", ",", "max_concept_length", ",", "concept_dim", ",", "mem_method", "=", "'cat'", ")", "\n", "memory_output", "=", "memory_layer", ".", "forward", "(", "enc_out", ",", "memory_embs", ",", "mem_length", ",", "ignore_no_memory_token", "=", "True", ")", "\n", "\n", "'''3rd layer: Self-Matching Layer'''", "\n", "# calculate input dim for self-matching layer", "\n", "if", "memory_layer", ".", "mem_method", "==", "'add'", ":", "\n", "        ", "memory_output_size", "=", "bert_config", "[", "'hidden_size'", "]", "\n", "", "elif", "memory_layer", ".", "mem_method", "==", "'cat'", ":", "\n", "        ", "memory_output_size", "=", "bert_config", "[", "'hidden_size'", "]", "+", "concept_dim", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"memory_layer.mem_method must be 'add' or 'cat'\"", ")", "\n", "", "logger", ".", "info", "(", "\"memory_output_size: {}\"", ".", "format", "(", "memory_output_size", ")", ")", "\n", "\n", "# do matching", "\n", "self_att_layer", "=", "TriLinearTwoTimeSelfAttentionLayer", "(", "\n", "memory_output_size", ",", "dropout_rate", "=", "0.0", ",", "\n", "cat_mul", "=", "True", ",", "cat_sub", "=", "True", ",", "cat_twotime", "=", "True", ",", "\n", "cat_twotime_mul", "=", "False", ",", "cat_twotime_sub", "=", "True", ")", "# [bs, sq, concat_hs]", "\n", "att_output", "=", "self_att_layer", ".", "forward", "(", "memory_output", ",", "input_mask", ")", "# [bs, sq, concat_hs]", "\n", "\n", "'''4th layer: Output Layer'''", "\n", "logits", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "att_output", ",", "\n", "size", "=", "2", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"cls_squad_out_w\"", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "NormalInitializer", "(", "loc", "=", "0.0", ",", "scale", "=", "bert_config", "[", "'initializer_range'", "]", ")", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"cls_squad_out_b\"", ",", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", ")", "\n", "\n", "logits", "=", "fluid", ".", "layers", ".", "transpose", "(", "x", "=", "logits", ",", "perm", "=", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "start_logits", ",", "end_logits", "=", "fluid", ".", "layers", ".", "unstack", "(", "x", "=", "logits", ",", "axis", "=", "0", ")", "\n", "\n", "batch_ones", "=", "fluid", ".", "layers", ".", "fill_constant_batch_size_like", "(", "\n", "input", "=", "start_logits", ",", "dtype", "=", "'int64'", ",", "shape", "=", "[", "1", "]", ",", "value", "=", "1", ")", "\n", "num_seqs", "=", "fluid", ".", "layers", ".", "reduce_sum", "(", "input", "=", "batch_ones", ")", "\n", "\n", "if", "is_training", ":", "\n", "\n", "        ", "def", "compute_loss", "(", "logits", ",", "positions", ")", ":", "\n", "            ", "loss", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "logits", ",", "label", "=", "positions", ")", "\n", "loss", "=", "fluid", ".", "layers", ".", "mean", "(", "x", "=", "loss", ")", "\n", "return", "loss", "\n", "\n", "", "start_loss", "=", "compute_loss", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "compute_loss", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2.0", "\n", "if", "args", ".", "use_fp16", "and", "args", ".", "loss_scaling", ">", "1.0", ":", "\n", "            ", "total_loss", "=", "total_loss", "*", "args", ".", "loss_scaling", "\n", "\n", "", "return", "pyreader", ",", "total_loss", ",", "num_seqs", "\n", "", "else", ":", "\n", "        ", "return", "pyreader", ",", "unique_id", ",", "start_logits", ",", "end_logits", ",", "num_seqs", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad.predict": [[245, 285], ["os.path.join", "os.path.join", "os.path.join", "os.path.join", "test_pyreader.start", "time.time", "time.time", "processor.get_features", "reader.squad.write_predictions", "os.path.exists", "os.makedirs", "test_exe.run", "range", "int", "all_results.append", "test_pyreader.reset", "logger.info", "float", "float", "RawResult", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_features", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.write_predictions", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["def", "predict", "(", "test_exe", ",", "test_program", ",", "test_pyreader", ",", "fetch_list", ",", "processor", ",", "eval_concept_settings", ",", "eval_output_name", "=", "'eval_result.json'", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "checkpoints", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "checkpoints", ")", "\n", "", "output_prediction_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\"predictions.json\"", ")", "\n", "output_nbest_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\"nbest_predictions.json\"", ")", "\n", "output_null_log_odds_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\"null_odds.json\"", ")", "\n", "output_evaluation_result_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "eval_output_name", ")", "\n", "\n", "test_pyreader", ".", "start", "(", ")", "\n", "all_results", "=", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "np_unique_ids", ",", "np_start_logits", ",", "np_end_logits", ",", "np_num_seqs", "=", "test_exe", ".", "run", "(", "\n", "fetch_list", "=", "fetch_list", ",", "program", "=", "test_program", ")", "\n", "for", "idx", "in", "range", "(", "np_unique_ids", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "if", "len", "(", "all_results", ")", "%", "1000", "==", "0", ":", "\n", "                    ", "logger", ".", "info", "(", "\"Processing example: %d\"", "%", "len", "(", "all_results", ")", ")", "\n", "", "unique_id", "=", "int", "(", "np_unique_ids", "[", "idx", "]", ")", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "np_start_logits", "[", "idx", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "np_end_logits", "[", "idx", "]", ".", "flat", "]", "\n", "all_results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "", "", "except", "fluid", ".", "core", ".", "EOFException", ":", "\n", "            ", "test_pyreader", ".", "reset", "(", ")", "\n", "break", "\n", "", "", "time_end", "=", "time", ".", "time", "(", ")", "\n", "\n", "features", "=", "processor", ".", "get_features", "(", "\n", "processor", ".", "predict_examples", ",", "is_training", "=", "False", ",", "**", "eval_concept_settings", ")", "\n", "eval_result", "=", "write_predictions", "(", "processor", ".", "predict_examples", ",", "features", ",", "all_results", ",", "\n", "args", ".", "n_best_size", ",", "args", ".", "max_answer_length", ",", "\n", "args", ".", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "output_null_log_odds_file", ",", "\n", "args", ".", "version_2_with_negative", ",", "\n", "args", ".", "null_score_diff_threshold", ",", "args", ".", "verbose", ",", "args", ".", "predict_file", ",", "output_evaluation_result_file", ")", "\n", "return", "eval_result", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad.read_concept_embedding": [[286, 306], ["open", "len", "len", "id2concept.append", "np.array.append", "numpy.array", "line.strip", "np.array.append", "len", "id2concept.append", "info[].split", "line.split", "float", "range", "len", "numpy.any", "line.split", "numpy.isnan"], "function", ["None"], ["", "def", "read_concept_embedding", "(", "embedding_path", ")", ":", "\n", "    ", "fin", "=", "open", "(", "embedding_path", ",", "encoding", "=", "'utf-8'", ")", "\n", "info", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "fin", "]", "\n", "dim", "=", "len", "(", "info", "[", "0", "]", ".", "split", "(", "' '", ")", "[", "1", ":", "]", ")", "\n", "n_concept", "=", "len", "(", "info", ")", "\n", "embedding_mat", "=", "[", "]", "\n", "id2concept", ",", "concept2id", "=", "[", "]", ",", "{", "}", "\n", "# add padding concept into vocab", "\n", "id2concept", ".", "append", "(", "'<pad_concept>'", ")", "\n", "concept2id", "[", "'<pad_concept>'", "]", "=", "0", "\n", "embedding_mat", ".", "append", "(", "[", "0.0", "for", "_", "in", "range", "(", "dim", ")", "]", ")", "\n", "for", "line", "in", "info", ":", "\n", "        ", "concept_name", "=", "line", ".", "split", "(", "' '", ")", "[", "0", "]", "\n", "embedding", "=", "[", "float", "(", "value_str", ")", "for", "value_str", "in", "line", ".", "split", "(", "' '", ")", "[", "1", ":", "]", "]", "\n", "assert", "len", "(", "embedding", ")", "==", "dim", "and", "not", "np", ".", "any", "(", "np", ".", "isnan", "(", "embedding", ")", ")", "\n", "embedding_mat", ".", "append", "(", "embedding", ")", "\n", "concept2id", "[", "concept_name", "]", "=", "len", "(", "id2concept", ")", "\n", "id2concept", ".", "append", "(", "concept_name", ")", "\n", "", "embedding_mat", "=", "np", ".", "array", "(", "embedding_mat", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "return", "id2concept", ",", "concept2id", ",", "embedding_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad.train": [[307, 590], ["model.bert.BertConfig", "model.bert.BertConfig.print_config", "paddle.Executor", "run_squad.read_concept_embedding", "reader.squad.DataProcessor", "paddle.Program", "fluid.Executor.run", "ValueError", "paddle.CUDAPlace", "paddle.core.get_cuda_device_count", "paddle.CPUPlace", "int", "random.seed", "numpy.random.seed", "reader.squad.DataProcessor.data_generator", "reader.squad.DataProcessor.get_num_examples", "int", "logger.info", "logger.info", "logger.info", "logger.info", "paddle.Program", "reader.squad.DataProcessor.data_generator", "paddle.Program", "test_prog.clone.clone", "logger.info", "paddle.global_scope().find_var().get_tensor().set", "paddle.ExecutionStrategy", "paddle.ParallelExecutor", "train_pyreader.decorate_tensor_provider", "train_pyreader.start", "time.time", "test_pyreader.decorate_tensor_provider", "logger.info", "os.environ.get", "paddle.program_guard", "logger.info", "paddle.program_guard", "logger.info", "utils.init.init_checkpoint", "utils.init.init_checkpoint", "run_squad.predict", "multiprocessing.cpu_count", "paddle.unique_name.guard", "run_squad.create_model", "optimization.optimization", "paddle.memory_optimize", "paddle.contrib.memory_usage", "paddle.contrib.memory_usage", "paddle.unique_name.guard", "run_squad.create_model", "paddle.memory_optimize", "paddle.global_scope().find_var().get_tensor", "utils.init.init_pretraining_params", "ValueError", "fluid.ParallelExecutor.run", "fluid.optimizer.ExponentialMovingAverage.apply", "run_squad.predict", "paddle.optimizer.ExponentialMovingAverage", "fluid.optimizer.ExponentialMovingAverage.update", "paddle.optimizer.ExponentialMovingAverage", "total_cost.extend", "total_num_seqs.extend", "time.time", "reader.squad.DataProcessor.get_train_progress", "logger.info", "time.time", "os.path.join", "paddle.io.save_persistables", "os.path.join", "paddle.io.save_persistables", "train_pyreader.reset", "dir", "paddle.global_scope().find_var", "logger.info", "test_pyreader.decorate_tensor_provider", "run_squad.predict", "logger.info", "train_pyreader.queue.size", "str", "reader.squad.DataProcessor.data_generator", "paddle.global_scope", "str", "numpy.sum", "numpy.sum"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig.print_config", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_squad.read_concept_embedding", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_num_examples", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_checkpoint", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_checkpoint", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.create_model", "home.repos.pwc.inspect_result.baidu_DuReader.src.optimization.optimization", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.create_model", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_pretraining_params", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_train_progress", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator"], ["", "def", "train", "(", "args", ")", ":", "\n", "    ", "bert_config", "=", "BertConfig", "(", "args", ".", "bert_config_path", ")", "\n", "bert_config", ".", "print_config", "(", ")", "\n", "\n", "if", "not", "(", "args", ".", "do_train", "or", "args", ".", "do_predict", "or", "args", ".", "do_val", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"For args `do_train` and `do_predict`, at \"", "\n", "\"least one of them must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "use_cuda", ":", "\n", "        ", "place", "=", "fluid", ".", "CUDAPlace", "(", "0", ")", "\n", "dev_count", "=", "fluid", ".", "core", ".", "get_cuda_device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "place", "=", "fluid", ".", "CPUPlace", "(", ")", "\n", "dev_count", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'CPU_NUM'", ",", "multiprocessing", ".", "cpu_count", "(", ")", ")", ")", "\n", "", "exe", "=", "fluid", ".", "Executor", "(", "place", ")", "\n", "\n", "id2concept", ",", "concept2id", ",", "concept_embedding_mat", "=", "read_concept_embedding", "(", "\n", "args", ".", "concept_embedding_path", ")", "\n", "\n", "processor", "=", "DataProcessor", "(", "\n", "vocab_path", "=", "args", ".", "vocab_path", ",", "\n", "do_lower_case", "=", "args", ".", "do_lower_case", ",", "\n", "max_seq_length", "=", "args", ".", "max_seq_len", ",", "\n", "in_tokens", "=", "args", ".", "in_tokens", ",", "\n", "doc_stride", "=", "args", ".", "doc_stride", ",", "\n", "max_query_length", "=", "args", ".", "max_query_length", ")", "\n", "\n", "startup_prog", "=", "fluid", ".", "Program", "(", ")", "\n", "if", "args", ".", "random_seed", "is", "not", "None", ":", "\n", "        ", "startup_prog", ".", "random_seed", "=", "args", ".", "random_seed", "\n", "random", ".", "seed", "(", "args", ".", "random_seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "random_seed", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "train_concept_settings", "=", "{", "\n", "'tokenization_path'", ":", "'../retrieve_concepts/tokenization_squad/tokens/train.tokenization.{}.data'", ".", "format", "(", "'uncased'", "if", "args", ".", "do_lower_case", "else", "'cased'", ")", ",", "\n", "'concept2id'", ":", "concept2id", ",", "\n", "'use_wordnet'", ":", "args", ".", "use_wordnet", ",", "\n", "'retrieved_synset_path'", ":", "args", ".", "retrieved_synset_path", ",", "\n", "'use_nell'", ":", "args", ".", "use_nell", ",", "\n", "'retrieved_nell_concept_path'", ":", "args", ".", "train_retrieved_nell_concept_path", ",", "\n", "}", "\n", "train_data_generator", "=", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "train_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'train'", ",", "\n", "shuffle", "=", "True", ",", "\n", "dev_count", "=", "dev_count", ",", "\n", "version_2_with_negative", "=", "args", ".", "version_2_with_negative", ",", "\n", "epoch", "=", "args", ".", "epoch", ",", "\n", "**", "train_concept_settings", ")", "\n", "\n", "num_train_examples", "=", "processor", ".", "get_num_examples", "(", "phase", "=", "'train'", ")", "\n", "if", "args", ".", "in_tokens", ":", "\n", "            ", "max_train_steps", "=", "args", ".", "epoch", "*", "num_train_examples", "//", "(", "\n", "args", ".", "batch_size", "//", "args", ".", "max_seq_len", ")", "//", "dev_count", "\n", "", "else", ":", "\n", "            ", "max_train_steps", "=", "args", ".", "epoch", "*", "num_train_examples", "//", "(", "\n", "args", ".", "batch_size", ")", "//", "dev_count", "\n", "", "warmup_steps", "=", "int", "(", "max_train_steps", "*", "args", ".", "warmup_proportion", ")", "\n", "logger", ".", "info", "(", "\"Device count: %d\"", "%", "dev_count", ")", "\n", "logger", ".", "info", "(", "\"Num train examples: %d\"", "%", "num_train_examples", ")", "\n", "logger", ".", "info", "(", "\"Max train steps: %d\"", "%", "max_train_steps", ")", "\n", "logger", ".", "info", "(", "\"Num warmup steps: %d\"", "%", "warmup_steps", ")", "\n", "\n", "train_program", "=", "fluid", ".", "Program", "(", ")", "\n", "# if args.random_seed is not None:", "\n", "#     train_program.random_seed = args.random_seed", "\n", "with", "fluid", ".", "program_guard", "(", "train_program", ",", "startup_prog", ")", ":", "\n", "            ", "with", "fluid", ".", "unique_name", ".", "guard", "(", ")", ":", "\n", "                ", "train_pyreader", ",", "loss", ",", "num_seqs", "=", "create_model", "(", "\n", "pyreader_name", "=", "'train_reader'", ",", "\n", "bert_config", "=", "bert_config", ",", "\n", "max_concept_length", "=", "processor", ".", "train_max_concept_length", ",", "\n", "concept_embedding_mat", "=", "concept_embedding_mat", ",", "\n", "is_training", "=", "True", ",", "\n", "freeze", "=", "args", ".", "freeze", ")", "\n", "\n", "scheduled_lr", "=", "optimization", "(", "\n", "loss", "=", "loss", ",", "\n", "warmup_steps", "=", "warmup_steps", ",", "\n", "num_train_steps", "=", "max_train_steps", ",", "\n", "learning_rate", "=", "args", ".", "learning_rate", ",", "\n", "train_program", "=", "train_program", ",", "\n", "startup_prog", "=", "startup_prog", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", "scheduler", "=", "args", ".", "lr_scheduler", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ",", "\n", "loss_scaling", "=", "args", ".", "loss_scaling", ")", "\n", "\n", "if", "args", ".", "use_ema", ":", "\n", "                    ", "ema", "=", "fluid", ".", "optimizer", ".", "ExponentialMovingAverage", "(", "args", ".", "ema_decay", ")", "\n", "ema", ".", "update", "(", ")", "\n", "\n", "", "fluid", ".", "memory_optimize", "(", "train_program", ",", "skip_opt_set", "=", "[", "loss", ".", "name", ",", "num_seqs", ".", "name", "]", ")", "\n", "\n", "", "", "if", "args", ".", "verbose", ":", "\n", "            ", "if", "args", ".", "in_tokens", ":", "\n", "                ", "lower_mem", ",", "upper_mem", ",", "unit", "=", "fluid", ".", "contrib", ".", "memory_usage", "(", "\n", "program", "=", "train_program", ",", "\n", "batch_size", "=", "args", ".", "batch_size", "//", "args", ".", "max_seq_len", ")", "\n", "", "else", ":", "\n", "                ", "lower_mem", ",", "upper_mem", ",", "unit", "=", "fluid", ".", "contrib", ".", "memory_usage", "(", "\n", "program", "=", "train_program", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "", "logger", ".", "info", "(", "\"Theoretical memory usage in training:  %.3f - %.3f %s\"", "%", "\n", "(", "lower_mem", ",", "upper_mem", ",", "unit", ")", ")", "\n", "\n", "", "", "if", "args", ".", "do_predict", "or", "args", ".", "do_val", ":", "\n", "        ", "eval_concept_settings", "=", "{", "\n", "'tokenization_path'", ":", "'../retrieve_concepts/tokenization_squad/tokens/dev.tokenization.{}.data'", ".", "format", "(", "'uncased'", "if", "args", ".", "do_lower_case", "else", "'cased'", ")", ",", "\n", "'concept2id'", ":", "concept2id", ",", "\n", "'use_wordnet'", ":", "args", ".", "use_wordnet", ",", "\n", "'retrieved_synset_path'", ":", "args", ".", "retrieved_synset_path", ",", "\n", "'use_nell'", ":", "args", ".", "use_nell", ",", "\n", "'retrieved_nell_concept_path'", ":", "args", ".", "dev_retrieved_nell_concept_path", ",", "\n", "}", "\n", "eval_data_generator", "=", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "predict_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'predict'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "epoch", "=", "1", ",", "\n", "**", "eval_concept_settings", ")", "\n", "\n", "test_prog", "=", "fluid", ".", "Program", "(", ")", "\n", "# if args.random_seed is not None:", "\n", "#     test_prog.random_seed = args.random_seed", "\n", "with", "fluid", ".", "program_guard", "(", "test_prog", ",", "startup_prog", ")", ":", "\n", "            ", "with", "fluid", ".", "unique_name", ".", "guard", "(", ")", ":", "\n", "                ", "test_pyreader", ",", "unique_ids", ",", "start_logits", ",", "end_logits", ",", "num_seqs", "=", "create_model", "(", "\n", "pyreader_name", "=", "'test_reader'", ",", "\n", "bert_config", "=", "bert_config", ",", "\n", "max_concept_length", "=", "processor", ".", "predict_max_concept_length", ",", "\n", "concept_embedding_mat", "=", "concept_embedding_mat", ",", "\n", "is_training", "=", "False", ")", "\n", "\n", "if", "args", ".", "use_ema", "and", "'ema'", "not", "in", "dir", "(", ")", ":", "\n", "                    ", "ema", "=", "fluid", ".", "optimizer", ".", "ExponentialMovingAverage", "(", "args", ".", "ema_decay", ")", "\n", "\n", "", "fluid", ".", "memory_optimize", "(", "test_prog", ",", "skip_opt_set", "=", "[", "unique_ids", ".", "name", ",", "\n", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "]", ")", "\n", "\n", "", "", "test_prog", "=", "test_prog", ".", "clone", "(", "for_test", "=", "True", ")", "\n", "# if args.random_seed is not None:", "\n", "#     test_prog.random_seed = args.random_seed", "\n", "\n", "", "exe", ".", "run", "(", "startup_prog", ")", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "info", "(", "'load pretrained concept embedding'", ")", "\n", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "'concept_emb_mat'", ")", ".", "get_tensor", "(", ")", ".", "set", "(", "concept_embedding_mat", ",", "place", ")", "\n", "\n", "if", "args", ".", "init_checkpoint", "and", "args", ".", "init_pretraining_params", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"WARNING: args 'init_checkpoint' and 'init_pretraining_params' \"", "\n", "\"both are set! Only arg 'init_checkpoint' is made valid.\"", ")", "\n", "", "if", "args", ".", "init_checkpoint", ":", "\n", "            ", "init_checkpoint", "(", "\n", "exe", ",", "\n", "args", ".", "init_checkpoint", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "", "elif", "args", ".", "init_pretraining_params", ":", "\n", "            ", "init_pretraining_params", "(", "\n", "exe", ",", "\n", "args", ".", "init_pretraining_params", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "", "", "elif", "args", ".", "do_predict", "or", "args", ".", "do_val", ":", "\n", "        ", "if", "not", "args", ".", "init_checkpoint", ":", "\n", "            ", "raise", "ValueError", "(", "\"args 'init_checkpoint' should be set if\"", "\n", "\"only doing prediction!\"", ")", "\n", "", "init_checkpoint", "(", "\n", "exe", ",", "\n", "args", ".", "init_checkpoint", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "exec_strategy", "=", "fluid", ".", "ExecutionStrategy", "(", ")", "\n", "exec_strategy", ".", "use_experimental_executor", "=", "args", ".", "use_fast_executor", "\n", "exec_strategy", ".", "num_threads", "=", "dev_count", "\n", "exec_strategy", ".", "num_iteration_per_drop_scope", "=", "args", ".", "num_iteration_per_drop_scope", "\n", "\n", "train_exe", "=", "fluid", ".", "ParallelExecutor", "(", "\n", "use_cuda", "=", "args", ".", "use_cuda", ",", "\n", "loss_name", "=", "loss", ".", "name", ",", "\n", "exec_strategy", "=", "exec_strategy", ",", "\n", "main_program", "=", "train_program", ")", "\n", "\n", "train_pyreader", ".", "decorate_tensor_provider", "(", "train_data_generator", ")", "\n", "\n", "train_pyreader", ".", "start", "(", ")", "\n", "steps", "=", "0", "\n", "total_cost", ",", "total_num_seqs", "=", "[", "]", ",", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "while", "steps", "<", "max_train_steps", ":", "\n", "            ", "try", ":", "\n", "                ", "steps", "+=", "1", "\n", "if", "steps", "%", "args", ".", "skip_steps", "==", "0", ":", "\n", "                    ", "if", "warmup_steps", "<=", "0", ":", "\n", "                        ", "fetch_list", "=", "[", "loss", ".", "name", ",", "num_seqs", ".", "name", "]", "\n", "", "else", ":", "\n", "                        ", "fetch_list", "=", "[", "\n", "loss", ".", "name", ",", "scheduled_lr", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", "\n", "", "", "else", ":", "\n", "                    ", "fetch_list", "=", "[", "]", "\n", "\n", "", "outputs", "=", "train_exe", ".", "run", "(", "fetch_list", "=", "fetch_list", ")", "\n", "\n", "if", "steps", "%", "args", ".", "skip_steps", "==", "0", ":", "\n", "                    ", "if", "warmup_steps", "<=", "0", ":", "\n", "                        ", "np_loss", ",", "np_num_seqs", "=", "outputs", "\n", "", "else", ":", "\n", "                        ", "np_loss", ",", "np_lr", ",", "np_num_seqs", "=", "outputs", "\n", "", "total_cost", ".", "extend", "(", "np_loss", "*", "np_num_seqs", ")", "\n", "total_num_seqs", ".", "extend", "(", "np_num_seqs", ")", "\n", "\n", "if", "args", ".", "verbose", ":", "\n", "                        ", "verbose", "=", "\"train pyreader queue size: %d, \"", "%", "train_pyreader", ".", "queue", ".", "size", "(", "\n", ")", "\n", "verbose", "+=", "\"learning rate: %f\"", "%", "(", "\n", "np_lr", "[", "0", "]", "\n", "if", "warmup_steps", ">", "0", "else", "args", ".", "learning_rate", ")", "\n", "logger", ".", "info", "(", "verbose", ")", "\n", "\n", "", "time_end", "=", "time", ".", "time", "(", ")", "\n", "used_time", "=", "time_end", "-", "time_begin", "\n", "current_example", ",", "epoch", "=", "processor", ".", "get_train_progress", "(", ")", "\n", "\n", "logger", ".", "info", "(", "\"epoch: %d, progress: %d/%d, step: %d, loss: %f, \"", "\n", "\"speed: %f steps/s\"", "%", "\n", "(", "epoch", ",", "current_example", ",", "num_train_examples", ",", "steps", ",", "\n", "np", ".", "sum", "(", "total_cost", ")", "/", "np", ".", "sum", "(", "total_num_seqs", ")", ",", "\n", "args", ".", "skip_steps", "/", "used_time", ")", ")", "\n", "total_cost", ",", "total_num_seqs", "=", "[", "]", ",", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "steps", "%", "args", ".", "save_steps", "==", "0", "or", "steps", "==", "max_train_steps", ":", "\n", "                    ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\n", "\"step_\"", "+", "str", "(", "steps", ")", ")", "\n", "fluid", ".", "io", ".", "save_persistables", "(", "exe", ",", "save_path", ",", "train_program", ")", "\n", "\n", "", "if", "steps", "%", "args", ".", "validation_steps", "==", "0", "or", "steps", "==", "max_train_steps", ":", "\n", "                    ", "if", "args", ".", "do_val", ":", "\n", "                        ", "test_pyreader", ".", "decorate_tensor_provider", "(", "\n", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "predict_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'predict'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "epoch", "=", "1", ",", "\n", "**", "eval_concept_settings", ")", "\n", ")", "\n", "val_performance", "=", "predict", "(", "exe", ",", "test_prog", ",", "test_pyreader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "eval_concept_settings", ",", "'validate_result_step_{}.json'", ".", "format", "(", "steps", ")", ")", "\n", "logger", ".", "info", "(", "\"Validation performance after step {}:\\n* Exact_match: {}\\n* F1: {}\"", ".", "format", "(", "steps", ",", "val_performance", "[", "'exact_match'", "]", ",", "val_performance", "[", "'f1'", "]", ")", ")", "\n", "\n", "", "", "", "except", "fluid", ".", "core", ".", "EOFException", ":", "\n", "                ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\n", "\"step_\"", "+", "str", "(", "steps", ")", "+", "\"_final\"", ")", "\n", "fluid", ".", "io", ".", "save_persistables", "(", "exe", ",", "save_path", ",", "train_program", ")", "\n", "train_pyreader", ".", "reset", "(", ")", "\n", "break", "\n", "\n", "", "", "", "if", "args", ".", "do_predict", ":", "\n", "        ", "test_pyreader", ".", "decorate_tensor_provider", "(", "eval_data_generator", ")", "\n", "\n", "if", "args", ".", "use_ema", ":", "\n", "            ", "with", "ema", ".", "apply", "(", "exe", ")", ":", "\n", "                ", "eval_performance", "=", "predict", "(", "exe", ",", "test_prog", ",", "test_pyreader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "eval_concept_settings", ")", "\n", "", "", "else", ":", "\n", "            ", "eval_performance", "=", "predict", "(", "exe", ",", "test_prog", ",", "test_pyreader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "eval_concept_settings", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Eval performance:\\n* Exact_match: {}\\n* F1: {}\"", ".", "format", "(", "eval_performance", "[", "'exact_match'", "]", ",", "eval_performance", "[", "'f1'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.dist_utils.nccl2_prepare": [[22, 32], ["paddle.DistributeTranspilerConfig", "paddle.DistributeTranspiler", "fluid.DistributeTranspiler.transpile", "os.environ.get", "os.environ.get"], "function", ["None"], ["def", "nccl2_prepare", "(", "trainer_id", ",", "startup_prog", ",", "main_prog", ")", ":", "\n", "    ", "config", "=", "fluid", ".", "DistributeTranspilerConfig", "(", ")", "\n", "config", ".", "mode", "=", "\"nccl2\"", "\n", "t", "=", "fluid", ".", "DistributeTranspiler", "(", "config", "=", "config", ")", "\n", "t", ".", "transpile", "(", "\n", "trainer_id", ",", "\n", "trainers", "=", "os", ".", "environ", ".", "get", "(", "'PADDLE_TRAINER_ENDPOINTS'", ")", ",", "\n", "current_endpoint", "=", "os", ".", "environ", ".", "get", "(", "'PADDLE_CURRENT_ENDPOINT'", ")", ",", "\n", "startup_program", "=", "startup_prog", ",", "\n", "program", "=", "main_prog", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.dist_utils.prepare_for_multi_process": [[34, 49], ["int", "int", "print", "print", "paddle.Program", "dist_utils.nccl2_prepare", "exe.run", "os.environ.get", "os.environ.get"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src.dist_utils.nccl2_prepare", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["", "def", "prepare_for_multi_process", "(", "exe", ",", "build_strategy", ",", "train_prog", ")", ":", "\n", "# prepare for multi-process", "\n", "    ", "trainer_id", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'PADDLE_TRAINER_ID'", ",", "0", ")", ")", "\n", "num_trainers", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'PADDLE_TRAINERS_NUM'", ",", "1", ")", ")", "\n", "if", "num_trainers", "<", "2", ":", "return", "\n", "print", "(", "\"PADDLE_TRAINERS_NUM\"", ",", "num_trainers", ")", "\n", "print", "(", "\"PADDLE_TRAINER_ID\"", ",", "trainer_id", ")", "\n", "build_strategy", ".", "num_trainers", "=", "num_trainers", "\n", "build_strategy", ".", "trainer_id", "=", "trainer_id", "\n", "# NOTE(zcd): use multi processes to train the model,", "\n", "# and each process use one GPU card.", "\n", "startup_prog", "=", "fluid", ".", "Program", "(", ")", "\n", "nccl2_prepare", "(", "trainer_id", ",", "startup_prog", ",", "train_prog", ")", "\n", "# the startup_prog are run two times, but it doesn't matter.", "\n", "exe", ".", "run", "(", "startup_prog", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.src.convert_params.parse_args": [[29, 44], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "__doc__", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--init_tf_checkpoint\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Initial TF checkpoint (a pre-trained BERT model).\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fluid_params_dir\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The directory to store converted Fluid parameters.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.convert_params.parse": [[46, 151], ["collections.OrderedDict", "collections.OrderedDict", "tensorflow.train.list_variables", "var_name.startswith", "key.startswith", "var_name.startswith", "key.endswith", "key.startswith", "key.endswith", "int", "key.endswith", "key.startswith", "key.endswith", "str", "key.endswith", "key.endswith", "print", "print", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "key.find", "key.endswith", "print", "print", "key.find", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "print", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "key.endswith", "print"], "function", ["None"], ["", "def", "parse", "(", "init_checkpoint", ")", ":", "\n", "    ", "tf_fluid_param_name_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "tf_param_name_shape_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "init_checkpoint", ")", "\n", "for", "(", "var_name", ",", "var_shape", ")", "in", "init_vars", ":", "\n", "        ", "fluid_param_name", "=", "''", "\n", "if", "var_name", ".", "startswith", "(", "'bert/'", ")", ":", "\n", "            ", "key", "=", "var_name", "[", "5", ":", "]", "\n", "if", "(", "key", ".", "startswith", "(", "'embeddings/'", ")", ")", ":", "\n", "                ", "if", "(", "key", ".", "endswith", "(", "'LayerNorm/gamma'", ")", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'pre_encoder_layer_norm_scale'", "\n", "", "elif", "(", "key", ".", "endswith", "(", "'LayerNorm/beta'", ")", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'pre_encoder_layer_norm_bias'", "\n", "", "elif", "(", "key", ".", "endswith", "(", "'position_embeddings'", ")", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'pos_embedding'", "\n", "", "elif", "(", "key", ".", "endswith", "(", "'word_embeddings'", ")", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'word_embedding'", "\n", "", "elif", "(", "key", ".", "endswith", "(", "'token_type_embeddings'", ")", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'sent_embedding'", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"ignored param: %s\"", "%", "var_name", ")", "\n", "", "", "elif", "(", "key", ".", "startswith", "(", "'encoder/'", ")", ")", ":", "\n", "                ", "key", "=", "key", "[", "8", ":", "]", "\n", "layer_num", "=", "int", "(", "key", "[", "key", ".", "find", "(", "'_'", ")", "+", "1", ":", "key", ".", "find", "(", "'/'", ")", "]", ")", "\n", "suffix", "=", "\"encoder_layer_\"", "+", "str", "(", "layer_num", ")", "\n", "if", "key", ".", "endswith", "(", "'attention/output/LayerNorm/beta'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_post_att_layer_norm_bias'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/output/LayerNorm/gamma'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_post_att_layer_norm_scale'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/output/dense/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_output_fc.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/output/dense/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_output_fc.w_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/self/key/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_key_fc.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/self/key/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_key_fc.w_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/self/query/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_query_fc.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/self/query/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_query_fc.w_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/self/value/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_value_fc.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'attention/self/value/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_multi_head_att_value_fc.w_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'intermediate/dense/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_ffn_fc_0.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'intermediate/dense/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_ffn_fc_0.w_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'output/LayerNorm/beta'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_post_ffn_layer_norm_bias'", "\n", "", "elif", "key", ".", "endswith", "(", "'output/LayerNorm/gamma'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_post_ffn_layer_norm_scale'", "\n", "", "elif", "key", ".", "endswith", "(", "'output/dense/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_ffn_fc_1.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'output/dense/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "suffix", "+", "'_ffn_fc_1.w_0'", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"ignored param: %s\"", "%", "var_name", ")", "\n", "", "", "elif", "(", "key", ".", "startswith", "(", "'pooler/'", ")", ")", ":", "\n", "                ", "if", "key", ".", "endswith", "(", "'dense/bias'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'pooled_fc.b_0'", "\n", "", "elif", "key", ".", "endswith", "(", "'dense/kernel'", ")", ":", "\n", "                    ", "fluid_param_name", "=", "'pooled_fc.w_0'", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"ignored param: %s\"", "%", "var_name", ")", "\n", "", "", "else", ":", "\n", "                ", "print", "(", "\"ignored param: %s\"", "%", "var_name", ")", "\n", "\n", "", "", "elif", "var_name", ".", "startswith", "(", "'cls/'", ")", ":", "\n", "            ", "if", "var_name", "==", "'cls/predictions/output_bias'", ":", "\n", "                ", "fluid_param_name", "=", "'mask_lm_out_fc.b_0'", "\n", "", "elif", "var_name", "==", "'cls/predictions/transform/LayerNorm/beta'", ":", "\n", "                ", "fluid_param_name", "=", "'mask_lm_trans_layer_norm_bias'", "\n", "", "elif", "var_name", "==", "'cls/predictions/transform/LayerNorm/gamma'", ":", "\n", "                ", "fluid_param_name", "=", "'mask_lm_trans_layer_norm_scale'", "\n", "", "elif", "var_name", "==", "'cls/predictions/transform/dense/bias'", ":", "\n", "                ", "fluid_param_name", "=", "'mask_lm_trans_fc.b_0'", "\n", "", "elif", "var_name", "==", "'cls/predictions/transform/dense/kernel'", ":", "\n", "                ", "fluid_param_name", "=", "'mask_lm_trans_fc.w_0'", "\n", "", "elif", "var_name", "==", "'cls/seq_relationship/output_bias'", ":", "\n", "                ", "fluid_param_name", "=", "'next_sent_fc.b_0'", "\n", "", "elif", "var_name", "==", "'cls/seq_relationship/output_weights'", ":", "\n", "                ", "fluid_param_name", "=", "'next_sent_fc.w_0'", "\n", "", "elif", "var_name", "==", "'cls/squad/output_weights'", ":", "\n", "                ", "fluid_param_name", "=", "'cls_squad_out_w'", "\n", "", "elif", "var_name", "==", "'cls/squad/output_bias'", ":", "\n", "                ", "fluid_param_name", "=", "'cls_squad_out_b'", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"ignored param: %s\"", "%", "var_name", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "var_name", "==", "'output_weights'", ":", "\n", "                ", "fluid_param_name", "=", "'cls_out_w'", "\n", "", "elif", "var_name", "==", "'output_bias'", ":", "\n", "                ", "fluid_param_name", "=", "'cls_out_b'", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"ignored param: %s\"", "%", "var_name", ")", "\n", "\n", "", "", "if", "fluid_param_name", "!=", "''", ":", "\n", "            ", "tf_fluid_param_name_map", "[", "var_name", "]", "=", "fluid_param_name", "\n", "tf_param_name_shape_map", "[", "var_name", "]", "=", "var_shape", "\n", "fluid_param_name", "=", "''", "\n", "\n", "", "", "return", "tf_fluid_param_name_map", ",", "tf_param_name_shape_map", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.convert_params.convert": [[153, 187], ["convert_params.parse", "paddle.Program", "fluid.Program.global_block", "paddle.core.CPUPlace", "paddle.Executor", "fluid.Executor.run", "print", "print", "print", "tensorflow.python.pywrap_tensorflow.NewCheckpointReader", "paddle.io.save_params", "program.global_block.create_parameter", "pywrap_tensorflow.NewCheckpointReader.get_tensor", "paddle.global_scope().find_var().get_tensor().set", "print", "numpy.transpose", "numpy.transpose", "numpy.transpose", "paddle.initializer.Constant", "paddle.global_scope().find_var().get_tensor", "paddle.global_scope().find_var", "paddle.global_scope"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src.convert_params.parse", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["", "def", "convert", "(", "args", ")", ":", "\n", "    ", "tf_fluid_param_name_map", ",", "tf_param_name_shape_map", "=", "parse", "(", "\n", "args", ".", "init_tf_checkpoint", ")", "\n", "program", "=", "fluid", ".", "Program", "(", ")", "\n", "global_block", "=", "program", ".", "global_block", "(", ")", "\n", "for", "param", "in", "tf_fluid_param_name_map", ":", "\n", "        ", "global_block", ".", "create_parameter", "(", "\n", "name", "=", "tf_fluid_param_name_map", "[", "param", "]", ",", "\n", "shape", "=", "tf_param_name_shape_map", "[", "param", "]", ",", "\n", "dtype", "=", "'float32'", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "value", "=", "0.0", ")", ")", "\n", "\n", "", "place", "=", "fluid", ".", "core", ".", "CPUPlace", "(", ")", "\n", "exe", "=", "fluid", ".", "Executor", "(", "place", ")", "\n", "exe", ".", "run", "(", "program", ")", "\n", "\n", "print", "(", "'---------------------- Converted Parameters -----------------------'", ")", "\n", "print", "(", "'###### [TF param name] --> [Fluid param name]  [param shape] ######'", ")", "\n", "print", "(", "'-------------------------------------------------------------------'", ")", "\n", "\n", "reader", "=", "pywrap_tensorflow", ".", "NewCheckpointReader", "(", "args", ".", "init_tf_checkpoint", ")", "\n", "for", "param", "in", "tf_fluid_param_name_map", ":", "\n", "        ", "value", "=", "reader", ".", "get_tensor", "(", "param", ")", "\n", "if", "param", "==", "'cls/seq_relationship/output_weights'", ":", "\n", "            ", "value", "=", "np", ".", "transpose", "(", "value", ")", "\n", "", "if", "param", "==", "'cls/squad/output_weights'", ":", "\n", "            ", "value", "=", "np", ".", "transpose", "(", "value", ")", "\n", "", "if", "param", "==", "'output_weights'", ":", "\n", "            ", "value", "=", "np", ".", "transpose", "(", "value", ")", "\n", "", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "tf_fluid_param_name_map", "[", "\n", "param", "]", ")", ".", "get_tensor", "(", ")", ".", "set", "(", "value", ",", "place", ")", "\n", "print", "(", "param", ",", "' --> '", ",", "tf_fluid_param_name_map", "[", "param", "]", ",", "'  '", ",", "value", ".", "shape", ")", "\n", "\n", "", "fluid", ".", "io", ".", "save_params", "(", "exe", ",", "args", ".", "fluid_params_dir", ",", "main_program", "=", "program", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.create_model": [[111, 184], ["paddle.io.DataLoader.from_generator", "model.ernie.ErnieModel", "model.ernie.ErnieModel.get_sequence_output", "paddle.layers.fc", "paddle.layers.transpose", "paddle.layers.unstack", "paddle.layers.fill_constant_batch_size_like", "paddle.layers.reduce_sum", "paddle.data", "run_mrc.create_model.compute_loss"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieModel.get_sequence_output"], ["def", "create_model", "(", "ernie_config", ",", "is_training", "=", "False", ")", ":", "\n", "    ", "if", "is_training", ":", "\n", "        ", "input_fields", "=", "{", "\n", "'names'", ":", "[", "'src_ids'", ",", "'pos_ids'", ",", "'sent_ids'", ",", "'input_mask'", ",", "'start_positions'", ",", "'end_positions'", "]", ",", "\n", "'shapes'", ":", "[", "[", "None", ",", "None", "]", ",", "[", "None", ",", "None", "]", ",", "[", "None", ",", "None", "]", ",", "\n", "[", "None", ",", "None", ",", "1", "]", ",", "[", "None", ",", "1", "]", ",", "[", "None", ",", "1", "]", "]", ",", "\n", "'dtypes'", ":", "[", "\n", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'float32'", ",", "'int64'", ",", "'int64'", "]", ",", "\n", "'lod_levels'", ":", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "}", "\n", "", "else", ":", "\n", "        ", "input_fields", "=", "{", "\n", "'names'", ":", "[", "'src_ids'", ",", "'pos_ids'", ",", "'sent_ids'", ",", "'input_mask'", ",", "'unique_id'", "]", ",", "\n", "'shapes'", ":", "[", "[", "None", ",", "None", "]", ",", "[", "None", ",", "None", "]", ",", "[", "None", ",", "None", "]", ",", "\n", "[", "None", ",", "None", ",", "1", "]", ",", "[", "None", ",", "1", "]", "]", ",", "\n", "'dtypes'", ":", "[", "\n", "'int64'", ",", "'int64'", ",", "'int64'", ",", "'float32'", ",", "'int64'", "]", ",", "\n", "'lod_levels'", ":", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "}", "\n", "\n", "", "inputs", "=", "[", "fluid", ".", "data", "(", "name", "=", "input_fields", "[", "'names'", "]", "[", "i", "]", ",", "\n", "shape", "=", "input_fields", "[", "'shapes'", "]", "[", "i", "]", ",", "\n", "dtype", "=", "input_fields", "[", "'dtypes'", "]", "[", "i", "]", ",", "\n", "lod_level", "=", "input_fields", "[", "'lod_levels'", "]", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "input_fields", "[", "'names'", "]", ")", ")", "]", "\n", "\n", "data_loader", "=", "fluid", ".", "io", ".", "DataLoader", ".", "from_generator", "(", "feed_list", "=", "inputs", ",", "capacity", "=", "50", ",", "iterable", "=", "False", ")", "\n", "\n", "if", "is_training", ":", "\n", "        ", "(", "src_ids", ",", "pos_ids", ",", "sent_ids", ",", "input_mask", ",", "start_positions", ",", "end_positions", ")", "=", "inputs", "\n", "", "else", ":", "\n", "        ", "(", "src_ids", ",", "pos_ids", ",", "sent_ids", ",", "input_mask", ",", "unique_id", ")", "=", "inputs", "\n", "\n", "", "ernie", "=", "ErnieModel", "(", "\n", "src_ids", "=", "src_ids", ",", "\n", "position_ids", "=", "pos_ids", ",", "\n", "sentence_ids", "=", "sent_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "config", "=", "ernie_config", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "\n", "enc_out", "=", "ernie", ".", "get_sequence_output", "(", ")", "\n", "\n", "logits", "=", "fluid", ".", "layers", ".", "fc", "(", "\n", "input", "=", "enc_out", ",", "\n", "size", "=", "2", ",", "\n", "num_flatten_dims", "=", "2", ",", "\n", "param_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"cls_squad_out_w\"", ",", "\n", "initializer", "=", "fluid", ".", "initializer", ".", "TruncatedNormal", "(", "scale", "=", "0.02", ")", ")", ",", "\n", "bias_attr", "=", "fluid", ".", "ParamAttr", "(", "\n", "name", "=", "\"cls_squad_out_b\"", ",", "initializer", "=", "fluid", ".", "initializer", ".", "Constant", "(", "0.", ")", ")", ")", "\n", "\n", "logits", "=", "fluid", ".", "layers", ".", "transpose", "(", "x", "=", "logits", ",", "perm", "=", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "start_logits", ",", "end_logits", "=", "fluid", ".", "layers", ".", "unstack", "(", "x", "=", "logits", ",", "axis", "=", "0", ")", "\n", "\n", "batch_ones", "=", "fluid", ".", "layers", ".", "fill_constant_batch_size_like", "(", "\n", "input", "=", "start_logits", ",", "dtype", "=", "'int64'", ",", "shape", "=", "[", "1", "]", ",", "value", "=", "1", ")", "\n", "num_seqs", "=", "fluid", ".", "layers", ".", "reduce_sum", "(", "input", "=", "batch_ones", ")", "\n", "\n", "if", "is_training", ":", "\n", "\n", "        ", "def", "compute_loss", "(", "logits", ",", "positions", ")", ":", "\n", "            ", "loss", "=", "fluid", ".", "layers", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "logits", ",", "label", "=", "positions", ")", "\n", "loss", "=", "fluid", ".", "layers", ".", "mean", "(", "x", "=", "loss", ")", "\n", "return", "loss", "\n", "\n", "", "start_loss", "=", "compute_loss", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "compute_loss", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2.0", "\n", "return", "data_loader", ",", "total_loss", ",", "num_seqs", "\n", "", "else", ":", "\n", "        ", "return", "data_loader", ",", "unique_id", ",", "start_logits", ",", "end_logits", ",", "num_seqs", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.predict": [[190, 228], ["os.path.join", "os.path.join", "os.path.join", "test_data_loader.start", "time.time", "time.time", "processor.get_features", "reader.squad.write_predictions", "os.path.exists", "os.makedirs", "test_exe.run", "range", "int", "all_results.append", "test_data_loader.reset", "print", "float", "float", "RawResult", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_features", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.write_predictions", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["def", "predict", "(", "test_exe", ",", "test_program", ",", "test_data_loader", ",", "fetch_list", ",", "processor", ",", "prefix", "=", "''", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "checkpoints", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "checkpoints", ")", "\n", "", "output_prediction_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "prefix", "+", "\"_predictions.json\"", ")", "\n", "output_nbest_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "prefix", "+", "\"_nbest_predictions.json\"", ")", "\n", "output_null_log_odds_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "prefix", "+", "\"_null_odds.json\"", ")", "\n", "\n", "test_data_loader", ".", "start", "(", ")", "\n", "all_results", "=", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "np_unique_ids", ",", "np_start_logits", ",", "np_end_logits", ",", "np_num_seqs", "=", "test_exe", ".", "run", "(", "\n", "fetch_list", "=", "fetch_list", ",", "program", "=", "test_program", ")", "\n", "for", "idx", "in", "range", "(", "np_unique_ids", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "if", "len", "(", "all_results", ")", "%", "1000", "==", "0", ":", "\n", "                    ", "print", "(", "\"Processing example: %d\"", "%", "len", "(", "all_results", ")", ")", "\n", "", "unique_id", "=", "int", "(", "np_unique_ids", "[", "idx", "]", ")", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "np_start_logits", "[", "idx", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "np_end_logits", "[", "idx", "]", ".", "flat", "]", "\n", "all_results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "", "", "except", "fluid", ".", "core", ".", "EOFException", ":", "\n", "            ", "test_data_loader", ".", "reset", "(", ")", "\n", "break", "\n", "", "", "time_end", "=", "time", ".", "time", "(", ")", "\n", "\n", "features", "=", "processor", ".", "get_features", "(", "\n", "processor", ".", "predict_examples", ",", "is_training", "=", "False", ")", "\n", "write_predictions", "(", "processor", ".", "predict_examples", ",", "features", ",", "all_results", ",", "\n", "args", ".", "n_best_size", ",", "args", ".", "max_answer_length", ",", "\n", "args", ".", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "output_null_log_odds_file", ",", "\n", "args", ".", "version_2_with_negative", ",", "\n", "args", ".", "null_score_diff_threshold", ",", "args", ".", "verbose", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.train": [[230, 433], ["model.ernie.ErnieConfig", "model.ernie.ErnieConfig.print_config", "paddle.Executor", "reader.squad.DataProcessor", "paddle.Program", "fluid.Executor.run", "ValueError", "paddle.CUDAPlace", "paddle.core.get_cuda_device_count", "paddle.CPUPlace", "int", "reader.squad.DataProcessor.data_generator", "reader.squad.DataProcessor.get_num_examples", "int", "print", "print", "print", "print", "paddle.Program", "paddle.Program", "test_prog.clone.clone", "paddle.ExecutionStrategy", "paddle.CompiledProgram().with_data_parallel", "train_data_loader.set_batch_generator", "train_data_loader.start", "time.time", "os.environ.get", "paddle.program_guard", "paddle.program_guard", "print", "utils.init.init_checkpoint", "utils.init.init_checkpoint", "input_files.extend", "len", "print", "os.path.basename", "re.sub", "test_data_loader.set_batch_generator", "run_mrc.predict", "multiprocessing.cpu_count", "paddle.unique_name.guard", "run_mrc.create_model", "optimization.optimization", "paddle.unique_name.guard", "run_mrc.create_model", "utils.init.init_pretraining_params", "ValueError", "paddle.CompiledProgram", "fluid.Executor.run", "glob.glob", "reader.squad.DataProcessor.data_generator", "total_cost.extend", "total_num_seqs.extend", "time.time", "reader.squad.DataProcessor.get_train_progress", "print", "time.time", "os.path.join", "paddle.io.save_persistables", "os.path.join", "paddle.io.save_persistables", "train_data_loader.reset", "print", "train_data_loader.queue.size", "str", "str", "numpy.sum", "numpy.sum"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.model.ernie.ErnieConfig.print_config", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_num_examples", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_checkpoint", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_checkpoint", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.create_model", "home.repos.pwc.inspect_result.baidu_DuReader.src.optimization.optimization", "home.repos.pwc.inspect_result.baidu_DuReader.src.run_mrc.create_model", "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_pretraining_params", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_train_progress", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size"], ["", "def", "train", "(", "args", ")", ":", "\n", "    ", "ernie_config", "=", "ErnieConfig", "(", "args", ".", "ernie_config", ")", "\n", "ernie_config", ".", "print_config", "(", ")", "\n", "\n", "if", "not", "(", "args", ".", "do_train", "or", "args", ".", "do_predict", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"For args `do_train` and `do_predict`, at \"", "\n", "\"least one of them must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "use_cuda", ":", "\n", "        ", "place", "=", "fluid", ".", "CUDAPlace", "(", "0", ")", "\n", "dev_count", "=", "fluid", ".", "core", ".", "get_cuda_device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "place", "=", "fluid", ".", "CPUPlace", "(", ")", "\n", "dev_count", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'CPU_NUM'", ",", "multiprocessing", ".", "cpu_count", "(", ")", ")", ")", "\n", "", "exe", "=", "fluid", ".", "Executor", "(", "place", ")", "\n", "\n", "processor", "=", "DataProcessor", "(", "\n", "vocab_path", "=", "args", ".", "vocab_path", ",", "\n", "do_lower_case", "=", "args", ".", "do_lower_case", ",", "\n", "max_seq_length", "=", "args", ".", "max_seq_len", ",", "\n", "in_tokens", "=", "args", ".", "in_tokens", ",", "\n", "doc_stride", "=", "args", ".", "doc_stride", ",", "\n", "max_query_length", "=", "args", ".", "max_query_length", ")", "\n", "\n", "startup_prog", "=", "fluid", ".", "Program", "(", ")", "\n", "if", "args", ".", "random_seed", "is", "not", "None", ":", "\n", "        ", "startup_prog", ".", "random_seed", "=", "args", ".", "random_seed", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "train_data_generator", "=", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "args", ".", "train_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'train'", ",", "\n", "shuffle", "=", "True", ",", "\n", "dev_count", "=", "dev_count", ",", "\n", "version_2_with_negative", "=", "args", ".", "version_2_with_negative", ",", "\n", "epoch", "=", "args", ".", "epoch", ")", "\n", "\n", "num_train_examples", "=", "processor", ".", "get_num_examples", "(", "phase", "=", "'train'", ")", "\n", "if", "args", ".", "in_tokens", ":", "\n", "            ", "max_train_steps", "=", "args", ".", "epoch", "*", "num_train_examples", "//", "(", "\n", "args", ".", "batch_size", "//", "args", ".", "max_seq_len", ")", "//", "dev_count", "\n", "", "else", ":", "\n", "            ", "max_train_steps", "=", "args", ".", "epoch", "*", "num_train_examples", "//", "(", "\n", "args", ".", "batch_size", ")", "//", "dev_count", "\n", "", "warmup_steps", "=", "int", "(", "max_train_steps", "*", "args", ".", "warmup_proportion", ")", "\n", "print", "(", "\"Device count: %d\"", "%", "dev_count", ")", "\n", "print", "(", "\"Num train examples: %d\"", "%", "num_train_examples", ")", "\n", "print", "(", "\"Max train steps: %d\"", "%", "max_train_steps", ")", "\n", "print", "(", "\"Num warmup steps: %d\"", "%", "warmup_steps", ")", "\n", "\n", "train_program", "=", "fluid", ".", "Program", "(", ")", "\n", "with", "fluid", ".", "program_guard", "(", "train_program", ",", "startup_prog", ")", ":", "\n", "            ", "with", "fluid", ".", "unique_name", ".", "guard", "(", ")", ":", "\n", "                ", "train_data_loader", ",", "loss", ",", "num_seqs", "=", "create_model", "(", "\n", "ernie_config", "=", "ernie_config", ",", "\n", "is_training", "=", "True", ")", "\n", "\n", "scheduled_lr", ",", "loss_scaling", "=", "optimization", "(", "\n", "loss", "=", "loss", ",", "\n", "warmup_steps", "=", "warmup_steps", ",", "\n", "num_train_steps", "=", "max_train_steps", ",", "\n", "learning_rate", "=", "args", ".", "learning_rate", ",", "\n", "train_program", "=", "train_program", ",", "\n", "startup_prog", "=", "startup_prog", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", "scheduler", "=", "args", ".", "lr_scheduler", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ",", "\n", "use_dynamic_loss_scaling", "=", "args", ".", "use_dynamic_loss_scaling", ",", "\n", "init_loss_scaling", "=", "args", ".", "init_loss_scaling", ",", "\n", "incr_every_n_steps", "=", "args", ".", "incr_every_n_steps", ",", "\n", "decr_every_n_nan_or_inf", "=", "args", ".", "decr_every_n_nan_or_inf", ",", "\n", "incr_ratio", "=", "args", ".", "incr_ratio", ",", "\n", "decr_ratio", "=", "args", ".", "decr_ratio", ")", "\n", "\n", "", "", "", "if", "args", ".", "do_predict", ":", "\n", "        ", "test_prog", "=", "fluid", ".", "Program", "(", ")", "\n", "with", "fluid", ".", "program_guard", "(", "test_prog", ",", "startup_prog", ")", ":", "\n", "            ", "with", "fluid", ".", "unique_name", ".", "guard", "(", ")", ":", "\n", "                ", "test_data_loader", ",", "unique_ids", ",", "start_logits", ",", "end_logits", ",", "num_seqs", "=", "create_model", "(", "\n", "ernie_config", "=", "ernie_config", ",", "\n", "is_training", "=", "False", ")", "\n", "\n", "", "", "test_prog", "=", "test_prog", ".", "clone", "(", "for_test", "=", "True", ")", "\n", "\n", "", "exe", ".", "run", "(", "startup_prog", ")", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "if", "args", ".", "init_checkpoint", "and", "args", ".", "init_pretraining_params", ":", "\n", "            ", "print", "(", "\n", "\"WARNING: args 'init_checkpoint' and 'init_pretraining_params' \"", "\n", "\"both are set! Only arg 'init_checkpoint' is made valid.\"", ")", "\n", "", "if", "args", ".", "init_checkpoint", ":", "\n", "            ", "init_checkpoint", "(", "\n", "exe", ",", "\n", "args", ".", "init_checkpoint", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "", "elif", "args", ".", "init_pretraining_params", ":", "\n", "            ", "init_pretraining_params", "(", "\n", "exe", ",", "\n", "args", ".", "init_pretraining_params", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "", "", "elif", "args", ".", "do_predict", ":", "\n", "        ", "if", "not", "args", ".", "init_checkpoint", ":", "\n", "            ", "raise", "ValueError", "(", "\"args 'init_checkpoint' should be set if\"", "\n", "\"only doing prediction!\"", ")", "\n", "", "init_checkpoint", "(", "\n", "exe", ",", "\n", "args", ".", "init_checkpoint", ",", "\n", "main_program", "=", "startup_prog", ",", "\n", "use_fp16", "=", "args", ".", "use_fp16", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "exec_strategy", "=", "fluid", ".", "ExecutionStrategy", "(", ")", "\n", "exec_strategy", ".", "use_experimental_executor", "=", "args", ".", "use_fast_executor", "\n", "exec_strategy", ".", "num_threads", "=", "dev_count", "\n", "exec_strategy", ".", "num_iteration_per_drop_scope", "=", "args", ".", "num_iteration_per_drop_scope", "\n", "\n", "train_compiled_program", "=", "fluid", ".", "CompiledProgram", "(", "train_program", ")", ".", "with_data_parallel", "(", "\n", "loss_name", "=", "loss", ".", "name", ",", "exec_strategy", "=", "exec_strategy", ")", "\n", "\n", "train_data_loader", ".", "set_batch_generator", "(", "train_data_generator", ",", "place", ")", "\n", "\n", "train_data_loader", ".", "start", "(", ")", "\n", "steps", "=", "0", "\n", "total_cost", ",", "total_num_seqs", "=", "[", "]", ",", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "while", "True", ":", "\n", "            ", "try", ":", "\n", "                ", "steps", "+=", "1", "\n", "if", "steps", "%", "args", ".", "skip_steps", "==", "0", ":", "\n", "                    ", "if", "args", ".", "use_fp16", ":", "\n", "                        ", "fetch_list", "=", "[", "loss", ".", "name", ",", "scheduled_lr", ".", "name", ",", "num_seqs", ".", "name", ",", "loss_scaling", ".", "name", "]", "\n", "", "else", ":", "\n", "                        ", "fetch_list", "=", "[", "loss", ".", "name", ",", "scheduled_lr", ".", "name", ",", "num_seqs", ".", "name", "]", "\n", "", "", "else", ":", "\n", "                    ", "fetch_list", "=", "[", "]", "\n", "\n", "", "outputs", "=", "exe", ".", "run", "(", "train_compiled_program", ",", "fetch_list", "=", "fetch_list", ")", "\n", "\n", "if", "steps", "%", "args", ".", "skip_steps", "==", "0", ":", "\n", "                    ", "if", "args", ".", "use_fp16", ":", "\n", "                        ", "np_loss", ",", "np_lr", ",", "np_num_seqs", ",", "np_scaling", "=", "outputs", "\n", "", "else", ":", "\n", "                        ", "np_loss", ",", "np_lr", ",", "np_num_seqs", "=", "outputs", "\n", "", "total_cost", ".", "extend", "(", "np_loss", "*", "np_num_seqs", ")", "\n", "total_num_seqs", ".", "extend", "(", "np_num_seqs", ")", "\n", "\n", "if", "args", ".", "verbose", ":", "\n", "                        ", "verbose", "=", "\"train data_loader queue size: %d, \"", "%", "train_data_loader", ".", "queue", ".", "size", "(", "\n", ")", "\n", "verbose", "+=", "\"learning rate: %f \"", "%", "np_lr", "[", "0", "]", "\n", "if", "args", ".", "use_fp16", ":", "\n", "                            ", "verbose", "+=", "\", loss scaling: %f\"", "%", "np_scaling", "[", "0", "]", "\n", "", "print", "(", "verbose", ")", "\n", "\n", "", "time_end", "=", "time", ".", "time", "(", ")", "\n", "used_time", "=", "time_end", "-", "time_begin", "\n", "current_example", ",", "epoch", "=", "processor", ".", "get_train_progress", "(", ")", "\n", "\n", "print", "(", "\"epoch: %d, progress: %d/%d, step: %d, loss: %f, \"", "\n", "\"speed: %f steps/s\"", "%", "\n", "(", "epoch", ",", "current_example", ",", "num_train_examples", ",", "steps", ",", "\n", "np", ".", "sum", "(", "total_cost", ")", "/", "np", ".", "sum", "(", "total_num_seqs", ")", ",", "\n", "args", ".", "skip_steps", "/", "used_time", ")", ")", "\n", "total_cost", ",", "total_num_seqs", "=", "[", "]", ",", "[", "]", "\n", "time_begin", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "if", "steps", "%", "args", ".", "save_steps", "==", "0", "or", "steps", "==", "max_train_steps", ":", "\n", "                    ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\n", "\"step_\"", "+", "str", "(", "steps", ")", ")", "\n", "fluid", ".", "io", ".", "save_persistables", "(", "exe", ",", "save_path", ",", "train_program", ")", "\n", "", "", "except", "fluid", ".", "core", ".", "EOFException", ":", "\n", "                ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoints", ",", "\n", "\"step_\"", "+", "str", "(", "steps", ")", "+", "\"_final\"", ")", "\n", "fluid", ".", "io", ".", "save_persistables", "(", "exe", ",", "save_path", ",", "train_program", ")", "\n", "train_data_loader", ".", "reset", "(", ")", "\n", "break", "\n", "\n", "", "", "", "if", "args", ".", "do_predict", ":", "\n", "        ", "input_files", "=", "[", "]", "\n", "for", "input_pattern", "in", "args", ".", "predict_file", ":", "\n", "            ", "input_files", ".", "extend", "(", "glob", ".", "glob", "(", "input_pattern", ")", ")", "\n", "", "assert", "len", "(", "input_files", ")", ">", "0", ",", "'Can not find predict_file {}'", ".", "format", "(", "args", ".", "predict_file", ")", "\n", "for", "input_file", "in", "input_files", ":", "\n", "            ", "print", "(", "'Run prediction on {}'", ".", "format", "(", "input_file", ")", ")", "\n", "prefix", "=", "os", ".", "path", ".", "basename", "(", "input_file", ")", "\n", "prefix", "=", "re", ".", "sub", "(", "'.json'", ",", "''", ",", "prefix", ")", "\n", "\n", "test_data_loader", ".", "set_batch_generator", "(", "\n", "processor", ".", "data_generator", "(", "\n", "data_path", "=", "input_file", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "phase", "=", "'predict'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "epoch", "=", "1", ")", ",", "place", ")", "\n", "\n", "predict", "(", "exe", ",", "test_prog", ",", "test_data_loader", ",", "[", "\n", "unique_ids", ".", "name", ",", "start_logits", ".", "name", ",", "end_logits", ".", "name", ",", "num_seqs", ".", "name", "\n", "]", ",", "processor", ",", "prefix", "=", "prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.tokenize_chinese_chars": [[373, 414], ["ord", "tokenization.tokenize_chinese_chars._is_chinese_char"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._is_chinese_char"], ["        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.src._ce.parse_log": [[33, 54], ["log.split", "line.strip().split", "print", "print", "float", "line.strip", "len"], "function", ["None"], ["def", "parse_log", "(", "log", ")", ":", "\n", "    ", "'''\n    This method should be implemented by model developers.\n    The suggestion:\n    each line in the log should be key, value, for example:\n    \"\n    train_cost\\t1.0\n    test_cost\\t1.0\n    train_cost\\t1.0\n    train_cost\\t1.0\n    train_acc\\t1.2\n    \"\n    '''", "\n", "for", "line", "in", "log", ".", "split", "(", "'\\n'", ")", ":", "\n", "        ", "fs", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "print", "(", "fs", ")", "\n", "if", "len", "(", "fs", ")", "==", "3", "and", "fs", "[", "0", "]", "==", "'kpis'", ":", "\n", "            ", "print", "(", "\"-----%s\"", "%", "fs", ")", "\n", "kpi_name", "=", "fs", "[", "1", "]", "\n", "kpi_value", "=", "float", "(", "fs", "[", "2", "]", ")", "\n", "yield", "kpi_name", ",", "kpi_value", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.src._ce.log_to_ce": [[56, 65], ["_ce.parse_log", "print", "kpi_tracker[].add_record", "kpi_tracker[].persist"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src._ce.parse_log"], ["", "", "", "def", "log_to_ce", "(", "log", ")", ":", "\n", "    ", "kpi_tracker", "=", "{", "}", "\n", "for", "kpi", "in", "tracking_kpis", ":", "\n", "        ", "kpi_tracker", "[", "kpi", ".", "name", "]", "=", "kpi", "\n", "\n", "", "for", "(", "kpi_name", ",", "kpi_value", ")", "in", "parse_log", "(", "log", ")", ":", "\n", "        ", "print", "(", "kpi_name", ",", "kpi_value", ")", "\n", "kpi_tracker", "[", "kpi_name", "]", ".", "add_record", "(", "kpi_value", ")", "\n", "kpi_tracker", "[", "kpi_name", "]", ".", "persist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.cast_fp32_to_fp16": [[26, 38], ["print", "main_program.global_block().all_parameters", "main_program.global_block", "param.name.endswith", "paddle.global_scope().find_var().get_tensor", "numpy.array", "paddle.global_scope().find_var", "param.name.find", "fluid.global_scope().find_var().get_tensor.set", "fluid.global_scope().find_var.get_tensor().set", "paddle.global_scope().find_var", "numpy.float16().view", "paddle.global_scope", "fluid.global_scope().find_var.get_tensor", "paddle.global_scope", "numpy.float16"], "function", ["None"], ["def", "cast_fp32_to_fp16", "(", "exe", ",", "main_program", ")", ":", "\n", "    ", "print", "(", "\"Cast parameters to float16 data format.\"", ")", "\n", "for", "param", "in", "main_program", ".", "global_block", "(", ")", ".", "all_parameters", "(", ")", ":", "\n", "        ", "if", "not", "param", ".", "name", ".", "endswith", "(", "\".master\"", ")", ":", "\n", "            ", "param_t", "=", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "param", ".", "name", ")", ".", "get_tensor", "(", ")", "\n", "data", "=", "np", ".", "array", "(", "param_t", ")", "\n", "if", "param", ".", "name", ".", "find", "(", "\"layer_norm\"", ")", "==", "-", "1", ":", "\n", "                ", "param_t", ".", "set", "(", "np", ".", "float16", "(", "data", ")", ".", "view", "(", "np", ".", "uint16", ")", ",", "exe", ".", "place", ")", "\n", "", "master_param_var", "=", "fluid", ".", "global_scope", "(", ")", ".", "find_var", "(", "param", ".", "name", "+", "\n", "\".master\"", ")", "\n", "if", "master_param_var", "is", "not", "None", ":", "\n", "                ", "master_param_var", ".", "get_tensor", "(", ")", ".", "set", "(", "data", ",", "exe", ".", "place", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_checkpoint": [[40, 60], ["os.path.exists", "paddle.io.load_vars", "print", "os.path.exists", "init.cast_fp32_to_fp16", "paddle.io.is_persistable", "os.path.join", "print"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.cast_fp32_to_fp16"], ["", "", "", "", "def", "init_checkpoint", "(", "exe", ",", "init_checkpoint_path", ",", "main_program", ",", "use_fp16", "=", "False", ",", "skip_list", "=", "[", "]", ")", ":", "\n", "    ", "assert", "os", ".", "path", ".", "exists", "(", "\n", "init_checkpoint_path", ")", ",", "\"[%s] cann't be found.\"", "%", "init_checkpoint_path", "\n", "\n", "def", "existed_persitables", "(", "var", ")", ":", "\n", "        ", "if", "not", "fluid", ".", "io", ".", "is_persistable", "(", "var", ")", ":", "\n", "            ", "return", "False", "\n", "", "if", "var", ".", "name", "in", "skip_list", ":", "\n", "            ", "return", "False", "\n", "", "return", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "init_checkpoint_path", ",", "var", ".", "name", ")", ")", "\n", "\n", "", "fluid", ".", "io", ".", "load_vars", "(", "\n", "exe", ",", "\n", "init_checkpoint_path", ",", "\n", "main_program", "=", "main_program", ",", "\n", "predicate", "=", "existed_persitables", ")", "\n", "print", "(", "\"Load model from {}\"", ".", "format", "(", "init_checkpoint_path", ")", ")", "\n", "\n", "if", "use_fp16", ":", "\n", "        ", "cast_fp32_to_fp16", "(", "exe", ",", "main_program", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.init.init_pretraining_params": [[62, 89], ["os.path.exists", "paddle.io.load_vars", "print", "os.path.exists", "init.cast_fp32_to_fp16", "isinstance", "os.path.join", "print", "print"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.cast_fp32_to_fp16"], ["", "", "def", "init_pretraining_params", "(", "exe", ",", "\n", "pretraining_params_path", ",", "\n", "main_program", ",", "\n", "use_fp16", "=", "False", ")", ":", "\n", "    ", "assert", "os", ".", "path", ".", "exists", "(", "pretraining_params_path", "\n", ")", ",", "\"[%s] cann't be found.\"", "%", "pretraining_params_path", "\n", "\n", "def", "existed_params", "(", "var", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "var", ",", "fluid", ".", "framework", ".", "Parameter", ")", ":", "\n", "            ", "return", "False", "\n", "", "return", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "pretraining_params_path", ",", "var", ".", "name", ")", ")", "\n", "\n", "", "fluid", ".", "io", ".", "load_vars", "(", "\n", "exe", ",", "\n", "pretraining_params_path", ",", "\n", "main_program", "=", "main_program", ",", "\n", "predicate", "=", "existed_params", ")", "\n", "print", "(", "\"Load pretraining parameters from {}.\"", ".", "format", "(", "\n", "pretraining_params_path", ")", ")", "\n", "\n", "if", "use_fp16", ":", "\n", "        ", "cast_fp32_to_fp16", "(", "exe", ",", "main_program", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.cast_fp16_to_fp32": [[20, 28], ["prog.global_block().append_op", "prog.global_block"], "function", ["None"], ["def", "cast_fp16_to_fp32", "(", "i", ",", "o", ",", "prog", ")", ":", "\n", "    ", "prog", ".", "global_block", "(", ")", ".", "append_op", "(", "\n", "type", "=", "\"cast\"", ",", "\n", "inputs", "=", "{", "\"X\"", ":", "i", "}", ",", "\n", "outputs", "=", "{", "\"Out\"", ":", "o", "}", ",", "\n", "attrs", "=", "{", "\n", "\"in_dtype\"", ":", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP16", ",", "\n", "\"out_dtype\"", ":", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP32", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.cast_fp32_to_fp16": [[31, 39], ["prog.global_block().append_op", "prog.global_block"], "function", ["None"], ["", "def", "cast_fp32_to_fp16", "(", "i", ",", "o", ",", "prog", ")", ":", "\n", "    ", "prog", ".", "global_block", "(", ")", ".", "append_op", "(", "\n", "type", "=", "\"cast\"", ",", "\n", "inputs", "=", "{", "\"X\"", ":", "i", "}", ",", "\n", "outputs", "=", "{", "\"Out\"", ":", "o", "}", ",", "\n", "attrs", "=", "{", "\n", "\"in_dtype\"", ":", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP32", ",", "\n", "\"out_dtype\"", ":", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP16", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.copy_to_master_param": [[36, 54], ["block.vars.get", "paddle.framework.Parameter", "ValueError"], "function", ["None"], ["attrs", "=", "{", "\n", "\"in_dtype\"", ":", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP32", ",", "\n", "\"out_dtype\"", ":", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP16", "\n", "}", ")", "\n", "\n", "\n", "", "def", "copy_to_master_param", "(", "p", ",", "block", ")", ":", "\n", "    ", "v", "=", "block", ".", "vars", ".", "get", "(", "p", ".", "name", ",", "None", ")", "\n", "if", "v", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"no param name %s found!\"", "%", "p", ".", "name", ")", "\n", "", "new_p", "=", "fluid", ".", "framework", ".", "Parameter", "(", "\n", "block", "=", "block", ",", "\n", "shape", "=", "v", ".", "shape", ",", "\n", "dtype", "=", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP32", ",", "\n", "type", "=", "v", ".", "type", ",", "\n", "lod_level", "=", "v", ".", "lod_level", ",", "\n", "stop_gradient", "=", "p", ".", "stop_gradient", ",", "\n", "trainable", "=", "p", ".", "trainable", ",", "\n", "optimize_attr", "=", "p", ".", "optimize_attr", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.create_master_params_grads": [[96, 117], ["main_prog._optimized_guard", "fp16.copy_to_master_param", "startup_prog.global_block()._clone_variable", "startup_prog.global_block().var", "fp16.append_cast_op", "paddle.layers.cast", "master_params_grads.append", "main_prog.global_block", "g.name.find", "master_params_grads.append", "startup_prog.global_block", "startup_prog.global_block"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.copy_to_master_param", "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.append_cast_op"], ["", "with", "main_prog", ".", "_optimized_guard", "(", "[", "m_p_g", "[", "0", "]", ",", "m_p_g", "[", "1", "]", "]", ")", ":", "\n", "            ", "cast_fp32_to_fp16", "(", "m_p_g", "[", "0", "]", ",", "train_p", ",", "main_prog", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.master_param_to_train_param": [[119, 127], ["enumerate", "main_prog._optimized_guard", "train_p.name.find", "paddle.layers.assign", "fp16.append_cast_op"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.append_cast_op"], []], "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.__init__": [[33, 35], ["parser.add_argument_group"], "methods", ["None"], ["\n", "", "def", "add_arg", "(", "self", ",", "name", ",", "type", ",", "default", ",", "help", ",", "**", "kwargs", ")", ":", "\n", "        ", "type", "=", "str2bool", "if", "type", "==", "bool", "else", "type", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.ArgumentGroup.add_arg": [[36, 44], ["args.ArgumentGroup._group.add_argument"], "methods", ["None"], ["self", ".", "_group", ".", "add_argument", "(", "\n", "\"--\"", "+", "name", ",", "\n", "default", "=", "default", ",", "\n", "type", "=", "type", ",", "\n", "help", "=", "help", "+", "' Default: %(default)s.'", ",", "\n", "**", "kwargs", ")", "\n", "\n", "\n", "", "", "def", "print_arguments", "(", "args", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.str2bool": [[26, 30], ["v.lower"], "function", ["None"], ["# boolean directly", "\n", "    ", "return", "v", ".", "lower", "(", ")", "in", "(", "\"true\"", ",", "\"t\"", ",", "\"1\"", ")", "\n", "\n", "\n", "", "class", "ArgumentGroup", "(", "object", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.print_arguments": [[46, 51], ["print", "sorted", "print", "six.iteritems", "print", "vars"], "function", ["None"], ["for", "arg", ",", "value", "in", "sorted", "(", "six", ".", "iteritems", "(", "vars", "(", "args", ")", ")", ")", ":", "\n", "        ", "print", "(", "'%s: %s'", "%", "(", "arg", ",", "value", ")", ")", "\n", "", "print", "(", "'------------------------------------------------'", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.cards.get_cards": [[18, 27], ["os.environ.get", "len", "os.environ.get.split"], "function", ["None"], ["def", "get_cards", "(", ")", ":", "\n", "    ", "\"\"\"\n    get gpu cards number\n    \"\"\"", "\n", "num", "=", "0", "\n", "cards", "=", "os", ".", "environ", ".", "get", "(", "'CUDA_VISIBLE_DEVICES'", ",", "''", ")", "\n", "if", "cards", "!=", "''", ":", "\n", "        ", "num", "=", "len", "(", "cards", ".", "split", "(", "\",\"", ")", ")", "\n", "", "return", "num", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.append_cast_op": [[20, 34], ["prog.global_block().append_op", "prog.global_block"], "function", ["None"], ["def", "cast_fp16_to_fp32", "(", "i", ",", "o", ",", "prog", ")", ":", "\n", "    ", "prog", ".", "global_block", "(", ")", ".", "append_op", "(", "\n", "type", "=", "\"cast\"", ",", "\n", "inputs", "=", "{", "\"X\"", ":", "i", "}", ",", "\n", "outputs", "=", "{", "\"Out\"", ":", "o", "}", ",", "\n", "attrs", "=", "{", "\n", "\"in_dtype\"", ":", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP16", ",", "\n", "\"out_dtype\"", ":", "fluid", ".", "core", ".", "VarDesc", ".", "VarType", ".", "FP32", "\n", "}", ")", "\n", "\n", "\n", "", "def", "cast_fp32_to_fp16", "(", "i", ",", "o", ",", "prog", ")", ":", "\n", "    ", "prog", ".", "global_block", "(", ")", ".", "append_op", "(", "\n", "type", "=", "\"cast\"", ",", "\n", "inputs", "=", "{", "\"X\"", ":", "i", "}", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.apply_dynamic_loss_scaling": [[56, 94], ["paddle.layers.fill_constant", "paddle.layers.fill_constant", "paddle.layers.create_global_var", "paddle.layers.create_global_var", "paddle.layers.concat", "paddle.layers.reduce_sum", "paddle.layers.isfinite", "fp16.update_loss_scaling", "paddle.layers.reduce_sum", "paddle.layers.Switch", "paddle.unique_name.generate", "paddle.unique_name.generate", "switch.case", "switch.default", "paddle.layers.assign", "paddle.layers.zeros_like"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.update_loss_scaling"], ["gradient_clip_attr", "=", "p", ".", "gradient_clip_attr", ",", "\n", "error_clip", "=", "p", ".", "error_clip", ",", "\n", "name", "=", "v", ".", "name", "+", "\".master\"", ")", "\n", "return", "new_p", "\n", "\n", "\n", "", "def", "create_master_params_grads", "(", "params_grads", ",", "main_prog", ",", "startup_prog", ",", "\n", "loss_scaling", ")", ":", "\n", "    ", "master_params_grads", "=", "[", "]", "\n", "tmp_role", "=", "main_prog", ".", "_current_role", "\n", "OpRole", "=", "fluid", ".", "core", ".", "op_proto_and_checker_maker", ".", "OpRole", "\n", "main_prog", ".", "_current_role", "=", "OpRole", ".", "Backward", "\n", "for", "p", ",", "g", "in", "params_grads", ":", "\n", "# create master parameters", "\n", "        ", "master_param", "=", "copy_to_master_param", "(", "p", ",", "main_prog", ".", "global_block", "(", ")", ")", "\n", "startup_master_param", "=", "startup_prog", ".", "global_block", "(", ")", ".", "_clone_variable", "(", "\n", "master_param", ")", "\n", "startup_p", "=", "startup_prog", ".", "global_block", "(", ")", ".", "var", "(", "p", ".", "name", ")", "\n", "cast_fp16_to_fp32", "(", "startup_p", ",", "startup_master_param", ",", "startup_prog", ")", "\n", "# cast fp16 gradients to fp32 before apply gradients", "\n", "if", "g", ".", "name", ".", "find", "(", "\"layer_norm\"", ")", ">", "-", "1", ":", "\n", "            ", "if", "loss_scaling", ">", "1", ":", "\n", "                ", "scaled_g", "=", "g", "/", "float", "(", "loss_scaling", ")", "\n", "", "else", ":", "\n", "                ", "scaled_g", "=", "g", "\n", "", "master_params_grads", ".", "append", "(", "[", "p", ",", "scaled_g", "]", ")", "\n", "continue", "\n", "", "master_grad", "=", "fluid", ".", "layers", ".", "cast", "(", "g", ",", "\"float32\"", ")", "\n", "if", "loss_scaling", ">", "1", ":", "\n", "            ", "master_grad", "=", "master_grad", "/", "float", "(", "loss_scaling", ")", "\n", "", "master_params_grads", ".", "append", "(", "[", "master_param", ",", "master_grad", "]", ")", "\n", "", "main_prog", ".", "_current_role", "=", "tmp_role", "\n", "return", "master_params_grads", "\n", "\n", "\n", "", "def", "master_param_to_train_param", "(", "master_params_grads", ",", "params_grads", ",", "main_prog", ")", ":", "\n", "    ", "for", "idx", ",", "m_p_g", "in", "enumerate", "(", "master_params_grads", ")", ":", "\n", "        ", "train_p", ",", "_", "=", "params_grads", "[", "idx", "]", "\n", "if", "train_p", ".", "name", ".", "find", "(", "\"layer_norm\"", ")", ">", "-", "1", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.fp16.update_loss_scaling": [[129, 203], ["paddle.layers.fill_constant", "paddle.layers.Switch", "switch.case", "paddle.layers.less_than", "switch.default", "paddle.layers.less_than", "paddle.layers.Switch", "paddle.layers.Switch", "switch1.case", "paddle.layers.isfinite", "paddle.layers.assign", "paddle.layers.assign", "switch1.default", "paddle.layers.increment", "paddle.layers.assign", "switch3.case", "paddle.layers.fill_constant", "paddle.layers.less_than", "paddle.layers.assign", "paddle.layers.assign", "switch3.default", "paddle.layers.assign", "paddle.layers.increment", "paddle.layers.Switch", "paddle.layers.Switch", "switch2.case", "paddle.layers.assign", "switch2.default", "switch4.case", "paddle.layers.assign", "switch4.default", "paddle.layers.assign"], "function", ["None"], []], "home.repos.pwc.inspect_result.baidu_DuReader.utils.args.check_cuda": [[52, 62], ["print", "sys.exit", "paddle.is_compiled_with_cuda"], "function", ["None"], []], "home.repos.pwc.inspect_result.baidu_DuReader.utils.get_vocab.get_vocab": [[29, 57], ["sorted", "open", "open", "json.loads", "itertools.chain", "vocab.items", "line.strip", "itertools.chain", "list", "w.encode", "vocab.get"], "function", ["None"], ["def", "get_vocab", "(", "files", ",", "vocab_file", ")", ":", "\n", "    ", "\"\"\"\n    Builds vocabulary file from field 'segmented_paragraphs'\n    and 'segmented_question'.\n\n    Args:\n        files: A list of file names.\n        vocab_file: The file that stores the vocabulary.\n    \"\"\"", "\n", "vocab", "=", "{", "}", "\n", "for", "f", "in", "files", ":", "\n", "        ", "with", "open", "(", "f", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "obj", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "paras", "=", "[", "\n", "chain", "(", "*", "d", "[", "'segmented_paragraphs'", "]", ")", "\n", "for", "d", "in", "obj", "[", "'documents'", "]", "]", "\n", "doc_tokens", "=", "chain", "(", "*", "paras", ")", "\n", "question_tokens", "=", "obj", "[", "'segmented_question'", "]", "\n", "for", "t", "in", "list", "(", "doc_tokens", ")", "+", "question_tokens", ":", "\n", "                    ", "vocab", "[", "t", "]", "=", "vocab", ".", "get", "(", "t", ",", "0", ")", "+", "1", "\n", "# output", "\n", "", "", "", "", "sorted_vocab", "=", "sorted", "(", "[", "(", "v", ",", "c", ")", "for", "v", ",", "c", "in", "vocab", ".", "items", "(", ")", "]", ",", "\n", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "\n", "reverse", "=", "True", ")", "\n", "with", "open", "(", "vocab_file", ",", "'w'", ")", "as", "outf", ":", "\n", "        ", "for", "w", ",", "c", "in", "sorted_vocab", ":", "\n", "            ", "print", ">>", "outf", ",", "'{}\\t{}'", ".", "format", "(", "w", ".", "encode", "(", "'utf8'", ")", ",", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.preprocess.precision_recall_f1": [[29, 56], ["sum", "isinstance", "prediction.split", "isinstance", "ground_truth.split", "collections.Counter", "collections.Counter", "common.values", "len", "len"], "function", ["None"], ["def", "precision_recall_f1", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "\"\"\"\n    This function calculates and returns the precision, recall and f1-score\n    Args:\n        prediction: prediction string or list to be matched\n        ground_truth: golden string or list reference\n    Returns:\n        floats of (p, r, f1)\n    Raises:\n        None\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "prediction", ",", "list", ")", ":", "\n", "        ", "prediction_tokens", "=", "prediction", ".", "split", "(", ")", "\n", "", "else", ":", "\n", "        ", "prediction_tokens", "=", "prediction", "\n", "", "if", "not", "isinstance", "(", "ground_truth", ",", "list", ")", ":", "\n", "        ", "ground_truth_tokens", "=", "ground_truth", ".", "split", "(", ")", "\n", "", "else", ":", "\n", "        ", "ground_truth_tokens", "=", "ground_truth", "\n", "", "common", "=", "Counter", "(", "prediction_tokens", ")", "&", "Counter", "(", "ground_truth_tokens", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "0", ",", "0", ",", "0", "\n", "", "p", "=", "1.0", "*", "num_same", "/", "len", "(", "prediction_tokens", ")", "\n", "r", "=", "1.0", "*", "num_same", "/", "len", "(", "ground_truth_tokens", ")", "\n", "f1", "=", "(", "2", "*", "p", "*", "r", ")", "/", "(", "p", "+", "r", ")", "\n", "return", "p", ",", "r", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.preprocess.recall": [[58, 70], ["preprocess.precision_recall_f1"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.precision_recall_f1"], ["", "def", "recall", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "\"\"\"\n    This function calculates and returns the recall\n    Args:\n        prediction: prediction string or list to be matched\n        ground_truth: golden string or list reference\n    Returns:\n        floats of recall\n    Raises:\n        None\n    \"\"\"", "\n", "return", "precision_recall_f1", "(", "prediction", ",", "ground_truth", ")", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.preprocess.f1_score": [[72, 84], ["preprocess.precision_recall_f1"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.precision_recall_f1"], ["", "def", "f1_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "\"\"\"\n    This function calculates and returns the f1-score\n    Args:\n        prediction: prediction string or list to be matched\n        ground_truth: golden string or list reference\n    Returns:\n        floats of f1\n    Raises:\n        None\n    \"\"\"", "\n", "return", "precision_recall_f1", "(", "prediction", ",", "ground_truth", ")", "[", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.preprocess.metric_max_over_ground_truths": [[86, 103], ["max", "scores_for_ground_truths.append", "preprocess.recall", "preprocess.recall", "preprocess.f1_score"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.recall", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.recall", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.f1_score"], ["", "def", "metric_max_over_ground_truths", "(", "metric_fn", ",", "prediction", ",", "ground_truths", ")", ":", "\n", "    ", "\"\"\"\n    This function calculates and returns the precision, recall and f1-score\n    Args:\n        metric_fn: metric function pointer which calculates scores according to corresponding logic.\n        prediction: prediction string or list to be matched\n        ground_truth: golden string or list reference\n    Returns:\n        floats of (p, r, f1)\n    Raises:\n        None\n    \"\"\"", "\n", "scores_for_ground_truths", "=", "[", "]", "\n", "for", "ground_truth", "in", "ground_truths", ":", "\n", "        ", "score", "=", "metric_fn", "(", "prediction", ",", "ground_truth", ")", "\n", "scores_for_ground_truths", ".", "append", "(", "score", ")", "\n", "", "return", "max", "(", "scores_for_ground_truths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.preprocess.find_best_question_match": [[105, 140], ["enumerate", "len", "preprocess.metric_max_over_ground_truths", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.metric_max_over_ground_truths"], ["", "def", "find_best_question_match", "(", "doc", ",", "question", ",", "with_score", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    For each document, find the paragraph that matches best to the question.\n    Args:\n        doc: The document object.\n        question: The question tokens.\n        with_score: If True then the match score will be returned,\n            otherwise False.\n    Returns:\n        The index of the best match paragraph, if with_score=False,\n        otherwise returns a tuple of the index of the best match paragraph\n        and the match score of that paragraph.\n    \"\"\"", "\n", "most_related_para", "=", "-", "1", "\n", "max_related_score", "=", "0", "\n", "most_related_para_len", "=", "0", "\n", "for", "p_idx", ",", "para_tokens", "in", "enumerate", "(", "doc", "[", "'segmented_paragraphs'", "]", ")", ":", "\n", "        ", "if", "len", "(", "question", ")", ">", "0", ":", "\n", "            ", "related_score", "=", "metric_max_over_ground_truths", "(", "recall", ",", "\n", "para_tokens", ",", "\n", "question", ")", "\n", "", "else", ":", "\n", "            ", "related_score", "=", "0", "\n", "\n", "", "if", "related_score", ">", "max_related_score", "or", "(", "related_score", "==", "max_related_score", "and", "len", "(", "para_tokens", ")", "<", "most_related_para_len", ")", ":", "\n", "            ", "most_related_para", "=", "p_idx", "\n", "max_related_score", "=", "related_score", "\n", "most_related_para_len", "=", "len", "(", "para_tokens", ")", "\n", "", "", "if", "most_related_para", "==", "-", "1", ":", "\n", "        ", "most_related_para", "=", "0", "\n", "", "if", "with_score", ":", "\n", "        ", "return", "most_related_para", ",", "max_related_score", "\n", "", "return", "most_related_para", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.preprocess.find_fake_answer": [[142, 212], ["set", "enumerate", "enumerate", "range", "sample[].append", "sample[].append", "sample[].append", "sample[].append", "set", "len", "range", "len", "preprocess.metric_max_over_ground_truths", "len", "len", "len", "preprocess.metric_max_over_ground_truths", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.metric_max_over_ground_truths", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.metric_max_over_ground_truths"], ["", "def", "find_fake_answer", "(", "sample", ")", ":", "\n", "    ", "\"\"\"\n    For each document, finds the most related paragraph based on recall,\n    then finds a span that maximize the f1_score compared with the gold answers\n    and uses this span as a fake answer span\n    Args:\n        sample: a sample in the dataset\n    Returns:\n        None\n    Raises:\n        None\n    \"\"\"", "\n", "for", "doc", "in", "sample", "[", "'documents'", "]", ":", "\n", "        ", "most_related_para", "=", "-", "1", "\n", "most_related_para_len", "=", "999999", "\n", "max_related_score", "=", "0", "\n", "for", "p_idx", ",", "para_tokens", "in", "enumerate", "(", "doc", "[", "'segmented_paragraphs'", "]", ")", ":", "\n", "            ", "if", "len", "(", "sample", "[", "'segmented_answers'", "]", ")", ">", "0", ":", "\n", "                ", "related_score", "=", "metric_max_over_ground_truths", "(", "recall", ",", "\n", "para_tokens", ",", "\n", "sample", "[", "'segmented_answers'", "]", ")", "\n", "", "else", ":", "\n", "                ", "continue", "\n", "", "if", "related_score", ">", "max_related_score", "or", "(", "related_score", "==", "max_related_score", "\n", "and", "len", "(", "para_tokens", ")", "<", "most_related_para_len", ")", ":", "\n", "                ", "most_related_para", "=", "p_idx", "\n", "most_related_para_len", "=", "len", "(", "para_tokens", ")", "\n", "max_related_score", "=", "related_score", "\n", "", "", "doc", "[", "'most_related_para'", "]", "=", "most_related_para", "\n", "\n", "", "sample", "[", "'answer_docs'", "]", "=", "[", "]", "\n", "sample", "[", "'answer_spans'", "]", "=", "[", "]", "\n", "sample", "[", "'fake_answers'", "]", "=", "[", "]", "\n", "sample", "[", "'match_scores'", "]", "=", "[", "]", "\n", "\n", "best_match_score", "=", "0", "\n", "best_match_d_idx", ",", "best_match_span", "=", "-", "1", ",", "[", "-", "1", ",", "-", "1", "]", "\n", "best_fake_answer", "=", "None", "\n", "answer_tokens", "=", "set", "(", ")", "\n", "for", "segmented_answer", "in", "sample", "[", "'segmented_answers'", "]", ":", "\n", "        ", "answer_tokens", "=", "answer_tokens", "|", "set", "(", "[", "token", "for", "token", "in", "segmented_answer", "]", ")", "\n", "", "for", "d_idx", ",", "doc", "in", "enumerate", "(", "sample", "[", "'documents'", "]", ")", ":", "\n", "        ", "if", "not", "doc", "[", "'is_selected'", "]", ":", "\n", "            ", "continue", "\n", "", "if", "doc", "[", "'most_related_para'", "]", "==", "-", "1", ":", "\n", "            ", "doc", "[", "'most_related_para'", "]", "=", "0", "\n", "", "most_related_para_tokens", "=", "doc", "[", "'segmented_paragraphs'", "]", "[", "doc", "[", "'most_related_para'", "]", "]", "[", ":", "1000", "]", "\n", "for", "start_tidx", "in", "range", "(", "len", "(", "most_related_para_tokens", ")", ")", ":", "\n", "            ", "if", "most_related_para_tokens", "[", "start_tidx", "]", "not", "in", "answer_tokens", ":", "\n", "                ", "continue", "\n", "", "for", "end_tidx", "in", "range", "(", "len", "(", "most_related_para_tokens", ")", "-", "1", ",", "start_tidx", "-", "1", ",", "-", "1", ")", ":", "\n", "                ", "span_tokens", "=", "most_related_para_tokens", "[", "start_tidx", ":", "end_tidx", "+", "1", "]", "\n", "if", "len", "(", "sample", "[", "'segmented_answers'", "]", ")", ">", "0", ":", "\n", "                    ", "match_score", "=", "metric_max_over_ground_truths", "(", "f1_score", ",", "span_tokens", ",", "\n", "sample", "[", "'segmented_answers'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "match_score", "=", "0", "\n", "", "if", "match_score", "==", "0", ":", "\n", "                    ", "break", "\n", "", "if", "match_score", ">", "best_match_score", ":", "\n", "                    ", "best_match_d_idx", "=", "d_idx", "\n", "best_match_span", "=", "[", "start_tidx", ",", "end_tidx", "]", "\n", "best_match_score", "=", "match_score", "\n", "best_fake_answer", "=", "''", ".", "join", "(", "span_tokens", ")", "\n", "", "", "", "", "if", "best_match_score", ">", "0", ":", "\n", "        ", "sample", "[", "'answer_docs'", "]", ".", "append", "(", "best_match_d_idx", ")", "\n", "sample", "[", "'answer_spans'", "]", ".", "append", "(", "best_match_span", ")", "\n", "sample", "[", "'fake_answers'", "]", ".", "append", "(", "best_fake_answer", ")", "\n", "sample", "[", "'match_scores'", "]", ".", "append", "(", "best_match_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.normalize": [[35, 52], ["normalized.append", "list", "len", "c.strip"], "function", ["None"], ["def", "normalize", "(", "s", ")", ":", "\n", "    ", "\"\"\"\n    Normalize strings to space joined chars.\n\n    Args:\n        s: a list of strings.\n\n    Returns:\n        A list of normalized strings.\n    \"\"\"", "\n", "if", "not", "s", ":", "\n", "        ", "return", "s", "\n", "", "normalized", "=", "[", "]", "\n", "for", "ss", "in", "s", ":", "\n", "        ", "tokens", "=", "[", "c", "for", "c", "in", "list", "(", "ss", ")", "if", "len", "(", "c", ".", "strip", "(", ")", ")", "!=", "0", "]", "\n", "normalized", ".", "append", "(", "' '", ".", "join", "(", "tokens", ")", ")", "\n", "", "return", "normalized", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.data_check": [[54, 78], ["isinstance", "isinstance", "len"], "function", ["None"], ["", "def", "data_check", "(", "obj", ",", "task", ")", ":", "\n", "    ", "\"\"\"\n    Check data.\n\n    Raises:\n        Raises AssertionError when data is not legal.\n    \"\"\"", "\n", "assert", "'question_id'", "in", "obj", ",", "\"Missing 'question_id' field.\"", "\n", "assert", "'question_type'", "in", "obj", ",", "\"Missing 'question_type' field. question_id: {}\"", ".", "format", "(", "obj", "[", "'question_type'", "]", ")", "\n", "\n", "assert", "'yesno_answers'", "in", "obj", ",", "\"Missing 'yesno_answers' field. question_id: {}\"", ".", "format", "(", "obj", "[", "'question_id'", "]", ")", "\n", "assert", "isinstance", "(", "obj", "[", "'yesno_answers'", "]", ",", "list", ")", ",", "r\"\"\"'yesno_answers' field must be a list, if the 'question_type' is not\n            'YES_NO', then this field should be an empty list.\n            question_id: {}\"\"\"", ".", "format", "(", "obj", "[", "'question_id'", "]", ")", "\n", "\n", "assert", "'entity_answers'", "in", "obj", ",", "\"Missing 'entity_answers' field. question_id: {}\"", ".", "format", "(", "obj", "[", "'question_id'", "]", ")", "\n", "assert", "isinstance", "(", "obj", "[", "'entity_answers'", "]", ",", "list", ")", "and", "len", "(", "obj", "[", "'entity_answers'", "]", ")", ">", "0", ",", "r\"\"\"'entity_answers' field must be a list, and has at least one element,\n            which can be a empty list. question_id: {}\"\"\"", ".", "format", "(", "obj", "[", "'question_id'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.read_file": [[80, 123], ["open", "file_name.endswith", "zipfile.ZipFile", "zf.namelist", "dureader_eval.read_file._open"], "function", ["None"], ["", "def", "read_file", "(", "file_name", ",", "task", ",", "is_ref", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Read predict answers or reference answers from file.\n\n    Args:\n        file_name: the name of the file containing predict result or reference\n                   result.\n\n    Returns:\n        A dictionary mapping question_id to the result information. The result\n        information itself is also a dictionary with has four keys:\n        - question_type: type of the query.\n        - yesno_answers: A list of yesno answers corresponding to 'answers'.\n        - answers: A list of predicted answers.\n        - entity_answers: A list, each element is also a list containing the entities\n                    tagged out from the corresponding answer string.\n    \"\"\"", "\n", "def", "_open", "(", "file_name", ",", "mode", ",", "zip_obj", "=", "None", ")", ":", "\n", "        ", "if", "zip_obj", "is", "not", "None", ":", "\n", "            ", "return", "zip_obj", ".", "open", "(", "file_name", ",", "mode", ")", "\n", "", "return", "open", "(", "file_name", ",", "mode", ")", "\n", "\n", "", "results", "=", "{", "}", "\n", "keys", "=", "[", "'answers'", ",", "'yesno_answers'", ",", "'entity_answers'", ",", "'question_type'", "]", "\n", "if", "is_ref", ":", "\n", "        ", "keys", "+=", "[", "'source'", "]", "\n", "\n", "", "zf", "=", "zipfile", ".", "ZipFile", "(", "file_name", ",", "'r'", ")", "if", "file_name", ".", "endswith", "(", "'.zip'", ")", "else", "None", "\n", "file_list", "=", "[", "file_name", "]", "if", "zf", "is", "None", "else", "zf", ".", "namelist", "(", ")", "\n", "\n", "for", "fn", "in", "file_list", ":", "\n", "        ", "for", "line", "in", "_open", "(", "fn", ",", "'r'", ",", "zip_obj", "=", "zf", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "obj", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "except", "ValueError", ":", "\n", "                ", "raise", "ValueError", "(", "\"Every line of data should be legal json\"", ")", "\n", "", "data_check", "(", "obj", ",", "task", ")", "\n", "qid", "=", "obj", "[", "'question_id'", "]", "\n", "assert", "qid", "not", "in", "results", ",", "\"Duplicate question_id: {}\"", ".", "format", "(", "qid", ")", "\n", "results", "[", "qid", "]", "=", "{", "}", "\n", "for", "k", "in", "keys", ":", "\n", "                ", "results", "[", "qid", "]", "[", "k", "]", "=", "obj", "[", "k", "]", "\n", "", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.compute_bleu_rouge": [[125, 138], ["bleu_metric.bleu.Bleu().compute_score", "enumerate", "rouge_metric.rouge.Rouge().compute_score", "set", "set", "pred_dict.keys", "ref_dict.keys", "set", "set", "bleu_metric.bleu.Bleu", "rouge_metric.rouge.Rouge", "ref_dict.keys", "pred_dict.keys"], "function", ["None"], ["", "def", "compute_bleu_rouge", "(", "pred_dict", ",", "ref_dict", ",", "bleu_order", "=", "4", ")", ":", "\n", "    ", "\"\"\"\n    Compute bleu and rouge scores.\n    \"\"\"", "\n", "assert", "set", "(", "pred_dict", ".", "keys", "(", ")", ")", "==", "set", "(", "ref_dict", ".", "keys", "(", ")", ")", ",", "\"missing keys: {}\"", ".", "format", "(", "set", "(", "ref_dict", ".", "keys", "(", ")", ")", "-", "set", "(", "pred_dict", ".", "keys", "(", ")", ")", ")", "\n", "scores", "=", "{", "}", "\n", "bleu_scores", ",", "_", "=", "Bleu", "(", "bleu_order", ")", ".", "compute_score", "(", "ref_dict", ",", "pred_dict", ")", "\n", "for", "i", ",", "bleu_score", "in", "enumerate", "(", "bleu_scores", ")", ":", "\n", "        ", "scores", "[", "'Bleu-%d'", "%", "(", "i", "+", "1", ")", "]", "=", "bleu_score", "\n", "", "rouge_score", ",", "_", "=", "Rouge", "(", ")", ".", "compute_score", "(", "ref_dict", ",", "pred_dict", ")", "\n", "scores", "[", "'Rouge-L'", "]", "=", "rouge_score", "\n", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.local_prf": [[140, 153], ["sum", "collections.Counter", "collections.Counter", "common.values", "len", "len"], "function", ["None"], ["", "def", "local_prf", "(", "pred_list", ",", "ref_list", ")", ":", "\n", "    ", "\"\"\"\n    Compute local precision recall and f1-score,\n    given only one prediction list and one reference list\n    \"\"\"", "\n", "common", "=", "Counter", "(", "pred_list", ")", "&", "Counter", "(", "ref_list", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "0", ",", "0", ",", "0", "\n", "", "p", "=", "1.0", "*", "num_same", "/", "len", "(", "pred_list", ")", "\n", "r", "=", "1.0", "*", "num_same", "/", "len", "(", "ref_list", ")", "\n", "f1", "=", "(", "2", "*", "p", "*", "r", ")", "/", "(", "p", "+", "r", ")", "\n", "return", "p", ",", "r", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.compute_prf": [[155, 190], ["set", "set", "pred_dict.keys", "ref_dict.keys", "pred_dict.get", "set", "set", "len", "len", "len", "len", "float", "float", "dureader_eval.local_prf", "len", "sorted", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.local_prf"], ["", "def", "compute_prf", "(", "pred_dict", ",", "ref_dict", ")", ":", "\n", "    ", "\"\"\"\n    Compute precision recall and f1-score.\n    \"\"\"", "\n", "pred_question_ids", "=", "set", "(", "pred_dict", ".", "keys", "(", ")", ")", "\n", "ref_question_ids", "=", "set", "(", "ref_dict", ".", "keys", "(", ")", ")", "\n", "correct_preds", ",", "total_correct", ",", "total_preds", "=", "0", ",", "0", ",", "0", "\n", "for", "question_id", "in", "ref_question_ids", ":", "\n", "        ", "pred_entity_list", "=", "pred_dict", ".", "get", "(", "question_id", ",", "[", "[", "]", "]", ")", "\n", "assert", "len", "(", "pred_entity_list", ")", "==", "1", ",", "'the number of entity list for question_id {} is not 1.'", ".", "format", "(", "question_id", ")", "\n", "pred_entity_list", "=", "pred_entity_list", "[", "0", "]", "\n", "all_ref_entity_lists", "=", "ref_dict", "[", "question_id", "]", "\n", "best_local_f1", "=", "0", "\n", "best_ref_entity_list", "=", "None", "\n", "for", "ref_entity_list", "in", "all_ref_entity_lists", ":", "\n", "            ", "local_f1", "=", "local_prf", "(", "pred_entity_list", ",", "ref_entity_list", ")", "[", "2", "]", "\n", "if", "local_f1", ">", "best_local_f1", ":", "\n", "                ", "best_ref_entity_list", "=", "ref_entity_list", "\n", "best_local_f1", "=", "local_f1", "\n", "", "", "if", "best_ref_entity_list", "is", "None", ":", "\n", "            ", "if", "len", "(", "all_ref_entity_lists", ")", ">", "0", ":", "\n", "                ", "best_ref_entity_list", "=", "sorted", "(", "all_ref_entity_lists", ",", "\n", "key", "=", "lambda", "x", ":", "len", "(", "x", ")", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "best_ref_entity_list", "=", "[", "]", "\n", "", "", "gold_entities", "=", "set", "(", "best_ref_entity_list", ")", "\n", "pred_entities", "=", "set", "(", "pred_entity_list", ")", "\n", "correct_preds", "+=", "len", "(", "gold_entities", "&", "pred_entities", ")", "\n", "total_preds", "+=", "len", "(", "pred_entities", ")", "\n", "total_correct", "+=", "len", "(", "gold_entities", ")", "\n", "", "p", "=", "float", "(", "correct_preds", ")", "/", "total_preds", "if", "correct_preds", ">", "0", "else", "0", "\n", "r", "=", "float", "(", "correct_preds", ")", "/", "total_correct", "if", "correct_preds", ">", "0", "else", "0", "\n", "f1", "=", "2", "*", "p", "*", "r", "/", "(", "p", "+", "r", ")", "if", "correct_preds", ">", "0", "else", "0", "\n", "return", "{", "'Precision'", ":", "p", ",", "'Recall'", ":", "r", ",", "'F1'", ":", "f1", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.prepare_prf": [[192, 199], ["pred_dict.items", "ref_dict.items"], "function", ["None"], ["", "def", "prepare_prf", "(", "pred_dict", ",", "ref_dict", ")", ":", "\n", "    ", "\"\"\"\n    Prepares data for calculation of prf scores.\n    \"\"\"", "\n", "preds", "=", "{", "k", ":", "v", "[", "'entity_answers'", "]", "for", "k", ",", "v", "in", "pred_dict", ".", "items", "(", ")", "}", "\n", "refs", "=", "{", "k", ":", "v", "[", "'entity_answers'", "]", "for", "k", ",", "v", "in", "ref_dict", ".", "items", "(", ")", "}", "\n", "return", "preds", ",", "refs", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.filter_dict": [[201, 210], ["result_dict.items", "k.endswith"], "function", ["None"], ["", "def", "filter_dict", "(", "result_dict", ",", "key_tag", ")", ":", "\n", "    ", "\"\"\"\n    Filter a subset of the result_dict, where keys ends with 'key_tag'.\n    \"\"\"", "\n", "filtered", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "result_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "k", ".", "endswith", "(", "key_tag", ")", ":", "\n", "            ", "filtered", "[", "k", "]", "=", "v", "\n", "", "", "return", "filtered", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_metrics": [[212, 263], ["ref_result.items", "dureader_eval.prepare_bleu", "dureader_eval.compute_bleu_rouge", "dureader_eval.prepare_bleu", "dureader_eval.compute_bleu_rouge", "zip", "dureader_eval.filter_dict", "dureader_eval.filter_dict", "dureader_eval.compute_bleu_rouge", "compute_prf.update", "dureader_eval.prepare_prf", "dureader_eval.prepare_bleu", "dureader_eval.compute_prf", "compute_prf.update", "ValueError", "dureader_eval.compute_bleu_rouge", "compute_bleu_rouge.items"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.prepare_bleu", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.compute_bleu_rouge", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.prepare_bleu", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.compute_bleu_rouge", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.filter_dict", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.filter_dict", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.compute_bleu_rouge", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.prepare_prf", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.prepare_bleu", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.compute_prf", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.compute_bleu_rouge"], ["", "def", "get_metrics", "(", "pred_result", ",", "ref_result", ",", "task", ",", "source", ")", ":", "\n", "    ", "\"\"\"\n    Computes metrics.\n    \"\"\"", "\n", "metrics", "=", "{", "}", "\n", "\n", "ref_result_filtered", "=", "{", "}", "\n", "pred_result_filtered", "=", "{", "}", "\n", "if", "source", "==", "'both'", ":", "\n", "        ", "ref_result_filtered", "=", "ref_result", "\n", "pred_result_filtered", "=", "pred_result", "\n", "", "else", ":", "\n", "        ", "for", "question_id", ",", "info", "in", "ref_result", ".", "items", "(", ")", ":", "\n", "            ", "if", "info", "[", "'source'", "]", "==", "source", ":", "\n", "                ", "ref_result_filtered", "[", "question_id", "]", "=", "info", "\n", "if", "question_id", "in", "pred_result", ":", "\n", "                    ", "pred_result_filtered", "[", "question_id", "]", "=", "pred_result", "[", "question_id", "]", "\n", "\n", "", "", "", "", "if", "task", "==", "'main'", "or", "task", "==", "'all'", "or", "task", "==", "'description'", ":", "\n", "        ", "pred_dict", ",", "ref_dict", "=", "prepare_bleu", "(", "pred_result_filtered", ",", "\n", "ref_result_filtered", ",", "\n", "task", ")", "\n", "metrics", "=", "compute_bleu_rouge", "(", "pred_dict", ",", "ref_dict", ")", "\n", "", "elif", "task", "==", "'yesno'", ":", "\n", "        ", "pred_dict", ",", "ref_dict", "=", "prepare_bleu", "(", "pred_result_filtered", ",", "\n", "ref_result_filtered", ",", "\n", "task", ")", "\n", "keys", "=", "[", "'Yes'", ",", "'No'", ",", "'Depends'", "]", "\n", "preds", "=", "[", "filter_dict", "(", "pred_dict", ",", "k", ")", "for", "k", "in", "keys", "]", "\n", "refs", "=", "[", "filter_dict", "(", "ref_dict", ",", "k", ")", "for", "k", "in", "keys", "]", "\n", "\n", "metrics", "=", "compute_bleu_rouge", "(", "pred_dict", ",", "ref_dict", ")", "\n", "\n", "for", "k", ",", "pred", ",", "ref", "in", "zip", "(", "keys", ",", "preds", ",", "refs", ")", ":", "\n", "            ", "m", "=", "compute_bleu_rouge", "(", "pred", ",", "ref", ")", "\n", "k_metric", "=", "[", "(", "k", "+", "'|'", "+", "key", ",", "v", ")", "for", "key", ",", "v", "in", "m", ".", "items", "(", ")", "]", "\n", "metrics", ".", "update", "(", "k_metric", ")", "\n", "\n", "", "", "elif", "task", "==", "'entity'", ":", "\n", "        ", "pred_dict", ",", "ref_dict", "=", "prepare_prf", "(", "pred_result_filtered", ",", "\n", "ref_result_filtered", ")", "\n", "pred_dict_bleu", ",", "ref_dict_bleu", "=", "prepare_bleu", "(", "pred_result_filtered", ",", "\n", "ref_result_filtered", ",", "\n", "task", ")", "\n", "metrics", "=", "compute_prf", "(", "pred_dict", ",", "ref_dict", ")", "\n", "metrics", ".", "update", "(", "compute_bleu_rouge", "(", "pred_dict_bleu", ",", "ref_dict_bleu", ")", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Illegal task name: {}\"", ".", "format", "(", "task", ")", ")", "\n", "\n", "", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.prepare_bleu": [[265, 300], ["ref_result.keys", "dict", "dict", "dict.items", "dict.items", "dureader_eval.normalize", "dureader_eval.normalize", "dureader_eval.get_main_result", "dict.get", "len", "dureader_eval.get_yesno_result", "dureader_eval.get_all_result", "dureader_eval.get_entity_result", "dureader_eval.get_desc_result", "ValueError"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.normalize", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.normalize", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_main_result", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_yesno_result", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_all_result", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_entity_result", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_desc_result"], ["", "def", "prepare_bleu", "(", "pred_result", ",", "ref_result", ",", "task", ")", ":", "\n", "    ", "\"\"\"\n    Prepares data for calculation of bleu and rouge scores.\n    \"\"\"", "\n", "pred_list", ",", "ref_list", "=", "[", "]", ",", "[", "]", "\n", "qids", "=", "ref_result", ".", "keys", "(", ")", "\n", "for", "qid", "in", "qids", ":", "\n", "        ", "if", "task", "==", "'main'", ":", "\n", "            ", "pred", ",", "ref", "=", "get_main_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", "\n", "", "elif", "task", "==", "'yesno'", ":", "\n", "            ", "pred", ",", "ref", "=", "get_yesno_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", "\n", "", "elif", "task", "==", "'all'", ":", "\n", "            ", "pred", ",", "ref", "=", "get_all_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", "\n", "", "elif", "task", "==", "'entity'", ":", "\n", "            ", "pred", ",", "ref", "=", "get_entity_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", "\n", "", "elif", "task", "==", "'description'", ":", "\n", "            ", "pred", ",", "ref", "=", "get_desc_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Illegal task name: {}\"", ".", "format", "(", "task", ")", ")", "\n", "", "if", "pred", "and", "ref", ":", "\n", "            ", "pred_list", "+=", "pred", "\n", "ref_list", "+=", "ref", "\n", "", "", "pred_dict", "=", "dict", "(", "pred_list", ")", "\n", "ref_dict", "=", "dict", "(", "ref_list", ")", "\n", "for", "qid", ",", "ans", "in", "ref_dict", ".", "items", "(", ")", ":", "\n", "        ", "ref_dict", "[", "qid", "]", "=", "normalize", "(", "ref_dict", "[", "qid", "]", ")", "\n", "pred_dict", "[", "qid", "]", "=", "normalize", "(", "pred_dict", ".", "get", "(", "qid", ",", "[", "EMPTY", "]", ")", ")", "\n", "if", "not", "ans", "or", "ans", "==", "[", "EMPTY", "]", ":", "\n", "            ", "del", "ref_dict", "[", "qid", "]", "\n", "del", "pred_dict", "[", "qid", "]", "\n", "\n", "", "", "for", "k", ",", "v", "in", "pred_dict", ".", "items", "(", ")", ":", "\n", "        ", "assert", "len", "(", "v", ")", "==", "1", ",", "\"There should be only one predict answer. question_id: {}\"", ".", "format", "(", "k", ")", "\n", "", "return", "pred_dict", ",", "ref_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_main_result": [[302, 325], ["pred_result.get().get", "pred_result.get"], "function", ["None"], ["", "def", "get_main_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", ":", "\n", "    ", "\"\"\"\n    Prepare answers for task 'main'.\n\n    Args:\n        qid: question_id.\n        pred_result: A dict include all question_id's result information read\n                     from args.pred_file.\n        ref_result: A dict incluce all question_id's result information read\n                    from args.ref_file.\n    Returns:\n        Two lists, the first one contains predict result, the second\n        one contains reference result of the same question_id. Each list has\n        elements of tuple (question_id, answers), 'answers' is a list of strings.\n    \"\"\"", "\n", "ref_ans", "=", "ref_result", "[", "qid", "]", "[", "'answers'", "]", "\n", "if", "not", "ref_ans", ":", "\n", "        ", "ref_ans", "=", "[", "EMPTY", "]", "\n", "", "pred_ans", "=", "pred_result", ".", "get", "(", "qid", ",", "{", "}", ")", ".", "get", "(", "'answers'", ",", "[", "]", ")", "[", ":", "1", "]", "\n", "if", "not", "pred_ans", ":", "\n", "        ", "pred_ans", "=", "[", "EMPTY", "]", "\n", "\n", "", "return", "[", "(", "qid", ",", "pred_ans", ")", "]", ",", "[", "(", "qid", ",", "ref_ans", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_entity_result": [[327, 345], ["dureader_eval.get_main_result"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_main_result"], ["", "def", "get_entity_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", ":", "\n", "    ", "\"\"\"\n    Prepare answers for task 'entity'.\n\n    Args:\n        qid: question_id.\n        pred_result: A dict include all question_id's result information read\n                     from args.pred_file.\n        ref_result: A dict incluce all question_id's result information read\n                    from args.ref_file.\n    Returns:\n        Two lists, the first one contains predict result, the second\n        one contains reference result of the same question_id. Each list has\n        elements of tuple (question_id, answers), 'answers' is a list of strings.\n    \"\"\"", "\n", "if", "ref_result", "[", "qid", "]", "[", "'question_type'", "]", "!=", "'ENTITY'", ":", "\n", "        ", "return", "None", ",", "None", "\n", "", "return", "get_main_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_desc_result": [[347, 365], ["dureader_eval.get_main_result"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_main_result"], ["", "def", "get_desc_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", ":", "\n", "    ", "\"\"\"\n    Prepare answers for task 'description'.\n\n    Args:\n        qid: question_id.\n        pred_result: A dict include all question_id's result information read\n                     from args.pred_file.\n        ref_result: A dict incluce all question_id's result information read\n                    from args.ref_file.\n    Returns:\n        Two lists, the first one contains predict result, the second\n        one contains reference result of the same question_id. Each list has\n        elements of tuple (question_id, answers), 'answers' is a list of strings.\n    \"\"\"", "\n", "if", "ref_result", "[", "qid", "]", "[", "'question_type'", "]", "!=", "'DESCRIPTION'", ":", "\n", "        ", "return", "None", ",", "None", "\n", "", "return", "get_main_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_yesno_result": [[367, 422], ["dureader_eval.get_yesno_result._get_yesno_ans"], "function", ["None"], ["", "def", "get_yesno_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", ":", "\n", "    ", "\"\"\"\n    Prepare answers for task 'yesno'.\n\n    Args:\n        qid: question_id.\n        pred_result: A dict include all question_id's result information read\n                     from args.pred_file.\n        ref_result: A dict incluce all question_id's result information read\n                    from args.ref_file.\n    Returns:\n        Two lists, the first one contains predict result, the second\n        one contains reference result of the same question_id. Each list has\n        elements of tuple (question_id, answers), 'answers' is a list of strings.\n    \"\"\"", "\n", "def", "_uniq", "(", "li", ",", "is_ref", ")", ":", "\n", "        ", "uniq_li", "=", "[", "]", "\n", "left", "=", "[", "]", "\n", "keys", "=", "set", "(", ")", "\n", "for", "k", ",", "v", "in", "li", ":", "\n", "            ", "if", "k", "not", "in", "keys", ":", "\n", "                ", "uniq_li", ".", "append", "(", "(", "k", ",", "v", ")", ")", "\n", "keys", ".", "add", "(", "k", ")", "\n", "", "else", ":", "\n", "                ", "left", ".", "append", "(", "(", "k", ",", "v", ")", ")", "\n", "\n", "", "", "if", "is_ref", ":", "\n", "            ", "dict_li", "=", "dict", "(", "uniq_li", ")", "\n", "for", "k", ",", "v", "in", "left", ":", "\n", "                ", "dict_li", "[", "k", "]", "+=", "v", "\n", "", "uniq_li", "=", "[", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "dict_li", ".", "items", "(", ")", "]", "\n", "", "return", "uniq_li", "\n", "\n", "", "def", "_expand_result", "(", "uniq_li", ")", ":", "\n", "        ", "expanded", "=", "uniq_li", "[", ":", "]", "\n", "keys", "=", "set", "(", "[", "x", "[", "0", "]", "for", "x", "in", "uniq_li", "]", ")", "\n", "for", "k", "in", "YESNO_LABELS", "-", "keys", ":", "\n", "            ", "expanded", ".", "append", "(", "(", "k", ",", "[", "EMPTY", "]", ")", ")", "\n", "", "return", "expanded", "\n", "\n", "", "def", "_get_yesno_ans", "(", "qid", ",", "result_dict", ",", "is_ref", "=", "False", ")", ":", "\n", "        ", "if", "qid", "not", "in", "result_dict", ":", "\n", "            ", "return", "[", "(", "str", "(", "qid", ")", "+", "'_'", "+", "k", ",", "v", ")", "for", "k", ",", "v", "in", "_expand_result", "(", "[", "]", ")", "]", "\n", "", "yesno_answers", "=", "result_dict", "[", "qid", "]", "[", "'yesno_answers'", "]", "\n", "answers", "=", "result_dict", "[", "qid", "]", "[", "'answers'", "]", "\n", "lbl_ans", "=", "_uniq", "(", "[", "(", "k", ",", "[", "v", "]", ")", "for", "k", ",", "v", "in", "zip", "(", "yesno_answers", ",", "answers", ")", "]", ",", "is_ref", ")", "\n", "ret", "=", "[", "(", "str", "(", "qid", ")", "+", "'_'", "+", "k", ",", "v", ")", "for", "k", ",", "v", "in", "_expand_result", "(", "lbl_ans", ")", "]", "\n", "return", "ret", "\n", "\n", "", "if", "ref_result", "[", "qid", "]", "[", "'question_type'", "]", "!=", "'YES_NO'", ":", "\n", "        ", "return", "None", ",", "None", "\n", "\n", "", "ref_ans", "=", "_get_yesno_ans", "(", "qid", ",", "ref_result", ",", "is_ref", "=", "True", ")", "\n", "pred_ans", "=", "_get_yesno_ans", "(", "qid", ",", "pred_result", ")", "\n", "return", "pred_ans", ",", "ref_ans", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_all_result": [[424, 442], ["dureader_eval.get_main_result", "dureader_eval.get_yesno_result"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_main_result", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_yesno_result"], ["", "def", "get_all_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", ":", "\n", "    ", "\"\"\"\n    Prepare answers for task 'all'.\n\n    Args:\n        qid: question_id.\n        pred_result: A dict include all question_id's result information read\n                     from args.pred_file.\n        ref_result: A dict incluce all question_id's result information read\n                    from args.ref_file.\n    Returns:\n        Two lists, the first one contains predict result, the second\n        one contains reference result of the same question_id. Each list has\n        elements of tuple (question_id, answers), 'answers' is a list of strings.\n    \"\"\"", "\n", "if", "ref_result", "[", "qid", "]", "[", "'question_type'", "]", "==", "'YES_NO'", ":", "\n", "        ", "return", "get_yesno_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", "\n", "", "return", "get_main_result", "(", "qid", ",", "pred_result", ",", "ref_result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.format_metrics": [[444, 510], ["str", "data.append", "data.append", "round", "round", "data.append", "data.append", "round", "round", "metrics[].get", "metrics[].get", "metrics[].get", "metrics[].get"], "function", ["None"], ["", "def", "format_metrics", "(", "metrics", ",", "task", ",", "err_msg", ")", ":", "\n", "    ", "\"\"\"\n    Format metrics. 'err' field returns any error occured during evaluation.\n\n    Args:\n        metrics: A dict object contains metrics for different tasks.\n        task: Task name.\n        err_msg: Exception raised during evaluation.\n    Returns:\n        Formatted result.\n    \"\"\"", "\n", "result", "=", "{", "}", "\n", "sources", "=", "[", "\"both\"", ",", "\"search\"", ",", "\"zhidao\"", "]", "\n", "if", "err_msg", "is", "not", "None", ":", "\n", "        ", "return", "{", "'errorMsg'", ":", "str", "(", "err_msg", ")", ",", "'errorCode'", ":", "1", ",", "'data'", ":", "[", "]", "}", "\n", "", "data", "=", "[", "]", "\n", "if", "task", "!=", "'all'", "and", "task", "!=", "'main'", ":", "\n", "        ", "sources", "=", "[", "\"both\"", "]", "\n", "\n", "", "if", "task", "==", "'entity'", ":", "\n", "        ", "metric_names", "=", "[", "\"Bleu-4\"", ",", "\"Rouge-L\"", "]", "\n", "metric_names_prf", "=", "[", "\"F1\"", ",", "\"Precision\"", ",", "\"Recall\"", "]", "\n", "for", "name", "in", "metric_names", "+", "metric_names_prf", ":", "\n", "            ", "for", "src", "in", "sources", ":", "\n", "                ", "obj", "=", "{", "\n", "\"name\"", ":", "name", ",", "\n", "\"value\"", ":", "round", "(", "metrics", "[", "src", "]", ".", "get", "(", "name", ",", "0", ")", "*", "100", ",", "2", ")", ",", "\n", "\"type\"", ":", "src", ",", "\n", "}", "\n", "data", ".", "append", "(", "obj", ")", "\n", "", "", "", "elif", "task", "==", "'yesno'", ":", "\n", "        ", "metric_names", "=", "[", "\"Bleu-4\"", ",", "\"Rouge-L\"", "]", "\n", "details", "=", "[", "\"Yes\"", ",", "\"No\"", ",", "\"Depends\"", "]", "\n", "src", "=", "sources", "[", "0", "]", "\n", "for", "name", "in", "metric_names", ":", "\n", "            ", "obj", "=", "{", "\n", "\"name\"", ":", "name", ",", "\n", "\"value\"", ":", "round", "(", "metrics", "[", "src", "]", ".", "get", "(", "name", ",", "0", ")", "*", "100", ",", "2", ")", ",", "\n", "\"type\"", ":", "'All'", ",", "\n", "}", "\n", "data", ".", "append", "(", "obj", ")", "\n", "for", "d", "in", "details", ":", "\n", "                ", "obj", "=", "{", "\n", "\"name\"", ":", "name", ",", "\n", "\"value\"", ":", "round", "(", "metrics", "[", "src", "]", ".", "get", "(", "d", "+", "'|'", "+", "name", ",", "0", ")", "*", "100", ",", "2", ")", ",", "\n", "\"type\"", ":", "d", ",", "\n", "}", "\n", "data", ".", "append", "(", "obj", ")", "\n", "", "", "", "else", ":", "\n", "        ", "metric_names", "=", "[", "\"Bleu-4\"", ",", "\"Rouge-L\"", "]", "\n", "for", "name", "in", "metric_names", ":", "\n", "            ", "for", "src", "in", "sources", ":", "\n", "                ", "obj", "=", "{", "\n", "\"name\"", ":", "name", ",", "\n", "\"value\"", ":", "round", "(", "metrics", "[", "src", "]", ".", "get", "(", "name", ",", "0", ")", "*", "100", ",", "2", ")", ",", "\n", "\"type\"", ":", "src", ",", "\n", "}", "\n", "data", ".", "append", "(", "obj", ")", "\n", "\n", "", "", "", "result", "[", "\"data\"", "]", "=", "data", "\n", "result", "[", "\"errorCode\"", "]", "=", "0", "\n", "result", "[", "\"errorMsg\"", "]", "=", "\"success\"", "\n", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.main": [[512, 535], ["print", "dureader_eval.read_file", "dureader_eval.read_file", "json.dumps().encode", "set", "dureader_eval.get_metrics", "json.dumps", "dureader_eval.format_metrics"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.read_file", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.read_file", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.get_metrics", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.format_metrics"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Do evaluation.\n    \"\"\"", "\n", "err", "=", "None", "\n", "metrics", "=", "{", "}", "\n", "try", ":", "\n", "        ", "pred_result", "=", "read_file", "(", "args", ".", "pred_file", ",", "args", ".", "task", ")", "\n", "ref_result", "=", "read_file", "(", "args", ".", "ref_file", ",", "args", ".", "task", ",", "is_ref", "=", "True", ")", "\n", "sources", "=", "[", "'both'", ",", "'search'", ",", "'zhidao'", "]", "\n", "if", "args", ".", "task", "not", "in", "set", "(", "[", "'main'", ",", "'all'", "]", ")", ":", "\n", "            ", "sources", "=", "sources", "[", ":", "1", "]", "\n", "", "for", "source", "in", "sources", ":", "\n", "            ", "metrics", "[", "source", "]", "=", "get_metrics", "(", "\n", "pred_result", ",", "ref_result", ",", "args", ".", "task", ",", "source", ")", "\n", "", "", "except", "ValueError", "as", "ve", ":", "\n", "        ", "err", "=", "ve", "\n", "", "except", "AssertionError", "as", "ae", ":", "\n", "        ", "err", "=", "ae", "\n", "\n", "", "print", "(", "json", ".", "dumps", "(", "\n", "format_metrics", "(", "metrics", ",", "args", ".", "task", ",", "err", ")", ",", "\n", "ensure_ascii", "=", "False", ")", ".", "encode", "(", "'utf8'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.marco_tokenize_data._nltk_tokenize": [[6, 17], ["nltk.word_tokenize", "sequence.find", "token_offsets.append", "token_words.append", "len"], "function", ["None"], ["def", "_nltk_tokenize", "(", "sequence", ")", ":", "\n", "    ", "tokens", "=", "nltk", ".", "word_tokenize", "(", "sequence", ")", "\n", "\n", "cur_char_offset", "=", "0", "\n", "token_offsets", "=", "[", "]", "\n", "token_words", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "cur_char_offset", "=", "sequence", ".", "find", "(", "token", ",", "cur_char_offset", ")", "\n", "token_offsets", ".", "append", "(", "[", "cur_char_offset", ",", "cur_char_offset", "+", "len", "(", "token", ")", "-", "1", "]", ")", "\n", "token_words", ".", "append", "(", "token", ")", "\n", "", "return", "token_offsets", ",", "token_words", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.marco_tokenize_data.segment": [[18, 31], ["marco_tokenize_data._nltk_tokenize", "enumerate", "enumerate", "enumerate", "marco_tokenize_data._nltk_tokenize", "doc[].append", "marco_tokenize_data._nltk_tokenize", "input_js[].append"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.marco_tokenize_data._nltk_tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.utils.marco_tokenize_data._nltk_tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.utils.marco_tokenize_data._nltk_tokenize"], ["", "def", "segment", "(", "input_js", ")", ":", "\n", "    ", "_", ",", "input_js", "[", "'segmented_question'", "]", "=", "_nltk_tokenize", "(", "input_js", "[", "'question'", "]", ")", "\n", "for", "doc_id", ",", "doc", "in", "enumerate", "(", "input_js", "[", "'documents'", "]", ")", ":", "\n", "        ", "doc", "[", "'segmented_title'", "]", "=", "[", "]", "\n", "doc", "[", "'segmented_paragraphs'", "]", "=", "[", "]", "\n", "for", "para_id", ",", "para", "in", "enumerate", "(", "doc", "[", "'paragraphs'", "]", ")", ":", "\n", "            ", "_", ",", "seg_para", "=", "_nltk_tokenize", "(", "para", ")", "\n", "doc", "[", "'segmented_paragraphs'", "]", ".", "append", "(", "seg_para", ")", "\n", "", "", "if", "'answers'", "in", "input_js", ":", "\n", "        ", "input_js", "[", "'segmented_answers'", "]", "=", "[", "]", "\n", "for", "answer_id", ",", "answer", "in", "enumerate", "(", "input_js", "[", "'answers'", "]", ")", ":", "\n", "            ", "_", ",", "seg_answer", "=", "_nltk_tokenize", "(", "answer", ")", "\n", "input_js", "[", "'segmented_answers'", "]", ".", "append", "(", "seg_answer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.utils.marcov1_to_dureader.trans": [[8, 26], ["enumerate", "output_js[].append"], "function", ["None"], ["def", "trans", "(", "input_js", ")", ":", "\n", "    ", "output_js", "=", "{", "}", "\n", "output_js", "[", "'question'", "]", "=", "input_js", "[", "'query'", "]", "\n", "output_js", "[", "'question_type'", "]", "=", "input_js", "[", "'query_type'", "]", "\n", "output_js", "[", "'question_id'", "]", "=", "input_js", "[", "'query_id'", "]", "\n", "output_js", "[", "'fact_or_opinion'", "]", "=", "\"\"", "\n", "output_js", "[", "'documents'", "]", "=", "[", "]", "\n", "for", "para_id", ",", "para", "in", "enumerate", "(", "input_js", "[", "'passages'", "]", ")", ":", "\n", "        ", "doc", "=", "{", "}", "\n", "doc", "[", "'title'", "]", "=", "\"\"", "\n", "if", "'is_selected'", "in", "para", ":", "\n", "            ", "doc", "[", "'is_selected'", "]", "=", "True", "if", "para", "[", "'is_selected'", "]", "!=", "0", "else", "False", "\n", "", "doc", "[", "'paragraphs'", "]", "=", "[", "para", "[", "'passage_text'", "]", "]", "\n", "output_js", "[", "'documents'", "]", ".", "append", "(", "doc", ")", "\n", "\n", "", "if", "'answers'", "in", "input_js", ":", "\n", "        ", "output_js", "[", "'answers'", "]", "=", "input_js", "[", "'answers'", "]", "\n", "", "return", "output_js", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.ReCoRDExample.__init__": [[42, 57], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "qas_id", ",", "\n", "question_text", ",", "\n", "doc_tokens", ",", "\n", "orig_answer_text", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "False", ")", ":", "\n", "        ", "self", ".", "qas_id", "=", "qas_id", "\n", "self", ".", "question_text", "=", "question_text", "\n", "self", ".", "doc_tokens", "=", "doc_tokens", "\n", "self", ".", "orig_answer_text", "=", "orig_answer_text", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.ReCoRDExample.__str__": [[58, 60], ["record_twomemory.ReCoRDExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.SQuADExample.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.ReCoRDExample.__repr__": [[61, 74], ["tokenization.printable_text", "tokenization.printable_text"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"\"", "\n", "s", "+=", "\"qas_id: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "self", ".", "qas_id", ")", ")", "\n", "s", "+=", "\", question_text: %s\"", "%", "(", "\n", "tokenization", ".", "printable_text", "(", "self", ".", "question_text", ")", ")", "\n", "s", "+=", "\", doc_tokens: [%s]\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "doc_tokens", ")", ")", "\n", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", start_position: %d\"", "%", "(", "self", ".", "start_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", end_position: %d\"", "%", "(", "self", ".", "end_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", is_impossible: %r\"", "%", "(", "self", ".", "is_impossible", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.InputFeatures.__init__": [[79, 108], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "unique_id", ",", "\n", "example_index", ",", "\n", "doc_span_index", ",", "\n", "tokens", ",", "\n", "token_to_orig_map", ",", "\n", "token_is_max_context", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "segment_ids", ",", "\n", "wn_concept_ids", ",", "\n", "nell_concept_ids", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "None", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "example_index", "=", "example_index", "\n", "self", ".", "doc_span_index", "=", "doc_span_index", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "token_to_orig_map", "=", "token_to_orig_map", "\n", "self", ".", "token_is_max_context", "=", "token_is_max_context", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "self", ".", "wn_concept_ids", "=", "wn_concept_ids", "\n", "self", ".", "nell_concept_ids", "=", "nell_concept_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.Examples_To_Features_Converter.__init__": [[192, 223], ["os.path.exists", "pickle.load", "open", "os.path.exists", "pickle.load", "max", "os.path.exists", "pickle.load", "max", "open", "open", "len", "max", "record_twomemory.Examples_To_Features_Converter.synsets_info.values", "record_twomemory.Examples_To_Features_Converter.nell_retrieve_info.items", "len", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "**", "concept_settings", ")", ":", "\n", "        ", "self", ".", "concept_settings", "=", "concept_settings", "\n", "\n", "# load necessary data files for mapping to related concepts", "\n", "# 1. mapping from subword-level tokenization to word-level tokenization", "\n", "tokenization_filepath", "=", "self", ".", "concept_settings", "[", "'tokenization_path'", "]", "\n", "assert", "os", ".", "path", ".", "exists", "(", "tokenization_filepath", ")", "\n", "self", ".", "all_tokenization_info", "=", "{", "}", "\n", "for", "item", "in", "pickle", ".", "load", "(", "open", "(", "tokenization_filepath", ",", "'rb'", ")", ")", ":", "\n", "            ", "self", ".", "all_tokenization_info", "[", "item", "[", "'id'", "]", "]", "=", "item", "\n", "\n", "# 2. mapping from concept name to concept id", "\n", "", "self", ".", "wn_concept2id", "=", "self", ".", "concept_settings", "[", "'wn_concept2id'", "]", "\n", "self", ".", "nell_concept2id", "=", "self", ".", "concept_settings", "[", "'nell_concept2id'", "]", "\n", "\n", "# 3. retrieved related wordnet concepts (if use_wordnet)", "\n", "if", "concept_settings", "[", "'use_wordnet'", "]", ":", "\n", "            ", "retrieved_synset_filepath", "=", "self", ".", "concept_settings", "[", "'retrieved_synset_path'", "]", "\n", "assert", "os", ".", "path", ".", "exists", "(", "retrieved_synset_filepath", ")", "\n", "self", ".", "synsets_info", "=", "pickle", ".", "load", "(", "open", "(", "retrieved_synset_filepath", ",", "'rb'", ")", ")", "# token to sysnet names", "\n", "self", ".", "max_wn_concept_length", "=", "max", "(", "[", "len", "(", "synsets", ")", "for", "synsets", "in", "self", ".", "synsets_info", ".", "values", "(", ")", "]", ")", "\n", "\n", "# 4. retrieved related nell concepts (if use_nell)", "\n", "", "if", "concept_settings", "[", "'use_nell'", "]", ":", "\n", "            ", "retrieved_nell_concept_filepath", "=", "self", ".", "concept_settings", "[", "'retrieved_nell_concept_path'", "]", "\n", "assert", "os", ".", "path", ".", "exists", "(", "retrieved_nell_concept_filepath", ")", "\n", "self", ".", "nell_retrieve_info", "=", "{", "}", "\n", "for", "item", "in", "pickle", ".", "load", "(", "open", "(", "retrieved_nell_concept_filepath", ",", "'rb'", ")", ")", ":", "\n", "                ", "self", ".", "nell_retrieve_info", "[", "item", "[", "'id'", "]", "]", "=", "item", "\n", "", "self", ".", "max_nell_concept_length", "=", "max", "(", "[", "max", "(", "[", "len", "(", "entity_info", "[", "'retrieved_concepts'", "]", ")", "for", "entity_info", "in", "item", "[", "'query_entities'", "]", "+", "item", "[", "'document_entities'", "]", "]", ")", "\n", "for", "qid", ",", "item", "in", "self", ".", "nell_retrieve_info", ".", "items", "(", ")", "if", "len", "(", "item", "[", "'query_entities'", "]", "+", "item", "[", "'document_entities'", "]", ")", ">", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.Examples_To_Features_Converter._lookup_wordnet_concept_ids": [[225, 236], ["range", "len", "tokenizer.basic_tokenizer._run_strip_accents", "concept_ids.append", "concept_ids.append", "original_token.lower"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_strip_accents"], ["", "", "def", "_lookup_wordnet_concept_ids", "(", "self", ",", "sub_tokens", ",", "sub_to_ori_index", ",", "tokens", ",", "tolower", ",", "tokenizer", ")", ":", "\n", "        ", "concept_ids", "=", "[", "]", "\n", "for", "index", "in", "range", "(", "len", "(", "sub_tokens", ")", ")", ":", "\n", "            ", "original_token", "=", "tokens", "[", "sub_to_ori_index", "[", "index", "]", "]", "\n", "# if tokens are in upper case, we must lower it for retrieving", "\n", "retrieve_token", "=", "tokenizer", ".", "basic_tokenizer", ".", "_run_strip_accents", "(", "original_token", ".", "lower", "(", ")", ")", "if", "tolower", "else", "original_token", "\n", "if", "retrieve_token", "in", "self", ".", "synsets_info", ":", "\n", "                ", "concept_ids", ".", "append", "(", "[", "self", ".", "wn_concept2id", "[", "synset_name", "]", "for", "synset_name", "in", "self", ".", "synsets_info", "[", "retrieve_token", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "concept_ids", ".", "append", "(", "[", "]", ")", "\n", "", "", "return", "concept_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.Examples_To_Features_Converter._lookup_nell_concept_ids": [[237, 246], ["range", "range", "len", "list", "range", "set", "range", "len", "len"], "methods", ["None"], ["", "def", "_lookup_nell_concept_ids", "(", "self", ",", "sub_tokens", ",", "sub_to_ori_index", ",", "tokens", ",", "nell_info", ")", ":", "\n", "        ", "original_concept_ids", "=", "[", "[", "]", "for", "_", "in", "range", "(", "len", "(", "tokens", ")", ")", "]", "\n", "for", "entity_info", "in", "nell_info", ":", "\n", "            ", "for", "pos", "in", "range", "(", "entity_info", "[", "'token_start'", "]", ",", "entity_info", "[", "'token_end'", "]", "+", "1", ")", ":", "\n", "                ", "original_concept_ids", "[", "pos", "]", "+=", "[", "self", ".", "nell_concept2id", "[", "category_name", "]", "for", "category_name", "in", "entity_info", "[", "'retrieved_concepts'", "]", "]", "\n", "", "", "for", "pos", "in", "range", "(", "len", "(", "original_concept_ids", ")", ")", ":", "\n", "            ", "original_concept_ids", "[", "pos", "]", "=", "list", "(", "set", "(", "original_concept_ids", "[", "pos", "]", ")", ")", "\n", "", "concept_ids", "=", "[", "original_concept_ids", "[", "sub_to_ori_index", "[", "index", "]", "]", "for", "index", "in", "range", "(", "len", "(", "sub_tokens", ")", ")", "]", "\n", "return", "concept_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.Examples_To_Features_Converter.__call__": [[247, 469], ["enumerate", "tokenizer.tokenize", "enumerate", "collections.namedtuple", "enumerate", "record_twomemory.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "record_twomemory.Examples_To_Features_Converter._lookup_nell_concept_ids", "len", "orig_to_tok_index.append", "tokenizer.tokenize", "record_twomemory.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "record_twomemory.Examples_To_Features_Converter._lookup_nell_concept_ids", "record_twomemory._improve_answer_span", "len", "doc_spans.append", "min", "tokens.append", "segment_ids.append", "wn_concept_ids.append", "nell_concept_ids.append", "zip", "tokens.append", "segment_ids.append", "wn_concept_ids.append", "nell_concept_ids.append", "range", "tokens.append", "segment_ids.append", "wn_concept_ids.append", "nell_concept_ids.append", "tokenizer.convert_tokens_to_ids", "zip", "record_twomemory.InputFeatures", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "len", "len", "collections.namedtuple.", "len", "tokens.append", "segment_ids.append", "wn_concept_ids.append", "nell_concept_ids.append", "record_twomemory._check_is_max_context", "tokens.append", "segment_ids.append", "wn_concept_ids.append", "nell_concept_ids.append", "len", "range", "all", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "len", "len", "len", "tokenization.printable_text", "len", "tokenization.printable_text", "str", "str", "str", "six.iteritems", "six.iteritems", "list", "enumerate", "list", "enumerate", "filter", "filter"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_nell_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_nell_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._improve_answer_span", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._check_is_max_context", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["", "def", "__call__", "(", "self", ",", "\n", "examples", ",", "\n", "tokenizer", ",", "\n", "max_seq_length", ",", "\n", "doc_stride", ",", "\n", "max_query_length", ",", "\n", "is_training", ")", ":", "\n", "        ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "unique_id", "=", "1000000000", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "            ", "tokenization_info", "=", "self", ".", "all_tokenization_info", "[", "example", ".", "qas_id", "]", "\n", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "# check online subword tokenization result is the same as offline result", "\n", "assert", "query_tokens", "==", "tokenization_info", "[", "'query_subtokens'", "]", "\n", "if", "self", ".", "concept_settings", "[", "'use_wordnet'", "]", ":", "\n", "                ", "query_wn_concepts", "=", "self", ".", "_lookup_wordnet_concept_ids", "(", "query_tokens", ",", "tokenization_info", "[", "'query_sub_to_ori_index'", "]", ",", "\n", "tokenization_info", "[", "'query_tokens'", "]", ",", "\n", "tolower", "=", "tokenizer", ".", "basic_tokenizer", ".", "do_lower_case", "==", "False", ",", "tokenizer", "=", "tokenizer", ")", "# if tolower is True, tokenizer must be given", "\n", "\n", "", "if", "self", ".", "concept_settings", "[", "'use_nell'", "]", ":", "\n", "                ", "query_nell_concepts", "=", "self", ".", "_lookup_nell_concept_ids", "(", "query_tokens", ",", "tokenization_info", "[", "'query_sub_to_ori_index'", "]", ",", "\n", "tokenization_info", "[", "'query_tokens'", "]", ",", "self", ".", "nell_retrieve_info", "[", "example", ".", "qas_id", "]", "[", "'query_entities'", "]", ")", "\n", "\n", "", "if", "len", "(", "query_tokens", ")", ">", "max_query_length", ":", "\n", "                ", "query_tokens", "=", "query_tokens", "[", "0", ":", "max_query_length", "]", "\n", "query_wn_concepts", "=", "query_wn_concepts", "[", "0", ":", "max_query_length", "]", "\n", "query_nell_concepts", "=", "query_nell_concepts", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "tok_to_orig_index", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "all_doc_tokens", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "doc_tokens", ")", ":", "\n", "                ", "orig_to_tok_index", ".", "append", "(", "len", "(", "all_doc_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "for", "sub_token", "in", "sub_tokens", ":", "\n", "                    ", "tok_to_orig_index", ".", "append", "(", "i", ")", "\n", "all_doc_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "assert", "all_doc_tokens", "==", "tokenization_info", "[", "'document_subtokens'", "]", "\n", "if", "self", ".", "concept_settings", "[", "'use_wordnet'", "]", ":", "\n", "                ", "doc_wn_concepts", "=", "self", ".", "_lookup_wordnet_concept_ids", "(", "all_doc_tokens", ",", "tokenization_info", "[", "'document_sub_to_ori_index'", "]", ",", "\n", "tokenization_info", "[", "'document_tokens'", "]", ",", "\n", "tolower", "=", "tokenizer", ".", "basic_tokenizer", ".", "do_lower_case", "==", "False", ",", "tokenizer", "=", "tokenizer", ")", "# if tolower is True, tokenizer must be given", "\n", "\n", "", "if", "self", ".", "concept_settings", "[", "'use_nell'", "]", ":", "\n", "                ", "doc_nell_concepts", "=", "self", ".", "_lookup_nell_concept_ids", "(", "all_doc_tokens", ",", "tokenization_info", "[", "'document_sub_to_ori_index'", "]", ",", "\n", "tokenization_info", "[", "'document_tokens'", "]", ",", "self", ".", "nell_retrieve_info", "[", "example", ".", "qas_id", "]", "[", "'document_entities'", "]", ")", "\n", "\n", "", "tok_start_position", "=", "None", "\n", "tok_end_position", "=", "None", "\n", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "                ", "tok_start_position", "=", "-", "1", "\n", "tok_end_position", "=", "-", "1", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "                ", "tok_start_position", "=", "orig_to_tok_index", "[", "example", ".", "start_position", "]", "\n", "if", "example", ".", "end_position", "<", "len", "(", "example", ".", "doc_tokens", ")", "-", "1", ":", "\n", "                    ", "tok_end_position", "=", "orig_to_tok_index", "[", "example", ".", "end_position", "+", "\n", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "                    ", "tok_end_position", "=", "len", "(", "all_doc_tokens", ")", "-", "1", "\n", "", "(", "tok_start_position", ",", "tok_end_position", ")", "=", "_improve_answer_span", "(", "\n", "all_doc_tokens", ",", "tok_start_position", ",", "tok_end_position", ",", "tokenizer", ",", "\n", "example", ".", "orig_answer_text", ")", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "", "max_tokens_for_doc", "=", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "# We can have documents that are longer than the maximum sequence length.", "\n", "# To deal with this we do a sliding window approach, where we take chunks", "\n", "# of the up to our max length with a stride of `doc_stride`.", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "                ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "                    ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "                    ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "                ", "tokens", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "token_is_max_context", "=", "{", "}", "\n", "segment_ids", "=", "[", "]", "\n", "wn_concept_ids", "=", "[", "]", "\n", "nell_concept_ids", "=", "[", "]", "\n", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "wn_concept_ids", ".", "append", "(", "[", "]", ")", "\n", "nell_concept_ids", ".", "append", "(", "[", "]", ")", "\n", "for", "token", ",", "query_wn_concept", ",", "query_nell_concept", "in", "zip", "(", "query_tokens", ",", "query_wn_concepts", ",", "query_nell_concepts", ")", ":", "\n", "                    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "wn_concept_ids", ".", "append", "(", "query_wn_concept", ")", "\n", "nell_concept_ids", ".", "append", "(", "query_nell_concept", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "wn_concept_ids", ".", "append", "(", "[", "]", ")", "\n", "nell_concept_ids", ".", "append", "(", "[", "]", ")", "\n", "\n", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "                    ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "tok_to_orig_index", "[", "split_token_index", "]", "\n", "\n", "is_max_context", "=", "_check_is_max_context", "(", "doc_spans", ",", "doc_span_index", ",", "\n", "split_token_index", ")", "\n", "token_is_max_context", "[", "len", "(", "tokens", ")", "]", "=", "is_max_context", "\n", "tokens", ".", "append", "(", "all_doc_tokens", "[", "split_token_index", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "wn_concept_ids", ".", "append", "(", "doc_wn_concepts", "[", "split_token_index", "]", ")", "\n", "nell_concept_ids", ".", "append", "(", "doc_nell_concepts", "[", "split_token_index", "]", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "wn_concept_ids", ".", "append", "(", "[", "]", ")", "\n", "nell_concept_ids", ".", "append", "(", "[", "]", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "#while len(input_ids) < max_seq_length:", "\n", "#  input_ids.append(0)", "\n", "#  input_mask.append(0)", "\n", "#  segment_ids.append(0)", "\n", "\n", "#assert len(input_ids) == max_seq_length", "\n", "#assert len(input_mask) == max_seq_length", "\n", "#assert len(segment_ids) == max_seq_length         ", "\n", "\n", "for", "concept_ids", ",", "max_concept_length", "in", "zip", "(", "(", "wn_concept_ids", ",", "nell_concept_ids", ")", ",", "(", "self", ".", "max_wn_concept_length", ",", "self", ".", "max_nell_concept_length", ")", ")", ":", "\n", "                    ", "for", "cindex", "in", "range", "(", "len", "(", "concept_ids", ")", ")", ":", "\n", "                        ", "concept_ids", "[", "cindex", "]", "=", "concept_ids", "[", "cindex", "]", "+", "[", "0", "]", "*", "(", "max_concept_length", "-", "len", "(", "concept_ids", "[", "cindex", "]", ")", ")", "\n", "concept_ids", "[", "cindex", "]", "=", "concept_ids", "[", "cindex", "]", "[", ":", "max_concept_length", "]", "\n", "", "assert", "all", "(", "[", "len", "(", "id_list", ")", "==", "max_concept_length", "for", "id_list", "in", "concept_ids", "]", ")", "\n", "\n", "", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "                    ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "# out_of_span = False", "\n", "if", "not", "(", "tok_start_position", ">=", "doc_start", "and", "\n", "tok_end_position", "<=", "doc_end", ")", ":", "\n", "                        ", "continue", "\n", "# out_of_span = True", "\n", "# if out_of_span:", "\n", "#     start_position = 0", "\n", "#     end_position = 0", "\n", "# else:", "\n", "#     doc_offset = len(query_tokens) + 2", "\n", "#     start_position = tok_start_position - doc_start + doc_offset", "\n", "#     end_position = tok_end_position - doc_start + doc_offset", "\n", "", "doc_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "start_position", "=", "tok_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "tok_end_position", "-", "doc_start", "+", "doc_offset", "\n", "\n", "", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "                    ", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "\n", "", "if", "example_index", "<", "3", ":", "\n", "                    ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"unique_id: %s\"", "%", "(", "unique_id", ")", ")", "\n", "logger", ".", "info", "(", "\"example_index: %s\"", "%", "(", "example_index", ")", ")", "\n", "logger", ".", "info", "(", "\"doc_span_index: %s\"", "%", "(", "doc_span_index", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"token_to_orig_map: %s\"", "%", "\" \"", ".", "join", "(", "[", "\n", "\"%d:%d\"", "%", "(", "x", ",", "y", ")", "\n", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_to_orig_map", ")", "\n", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"token_is_max_context: %s\"", "%", "\" \"", ".", "join", "(", "[", "\n", "\"%d:%s\"", "%", "(", "x", ",", "y", ")", "\n", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_is_max_context", ")", "\n", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"segment_ids: %s\"", "%", "\n", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"wordnet_concept_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "\"{}:{}\"", ".", "format", "(", "tidx", ",", "list", "(", "filter", "(", "lambda", "index", ":", "index", "!=", "0", ",", "x", ")", ")", ")", "for", "tidx", ",", "x", "in", "enumerate", "(", "wn_concept_ids", ")", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"nell_concept_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "\"{}:{}\"", ".", "format", "(", "tidx", ",", "list", "(", "filter", "(", "lambda", "index", ":", "index", "!=", "0", ",", "x", ")", ")", ")", "for", "tidx", ",", "x", "in", "enumerate", "(", "nell_concept_ids", ")", "]", ")", ")", "\n", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "                        ", "logger", ".", "info", "(", "\"impossible example\"", ")", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "                        ", "answer_text", "=", "\" \"", ".", "join", "(", "tokens", "[", "start_position", ":", "(", "end_position", "+", "\n", "1", ")", "]", ")", "\n", "logger", ".", "info", "(", "\"start_position: %d\"", "%", "(", "start_position", ")", ")", "\n", "logger", ".", "info", "(", "\"end_position: %d\"", "%", "(", "end_position", ")", ")", "\n", "logger", ".", "info", "(", "\"answer: %s\"", "%", "\n", "(", "tokenization", ".", "printable_text", "(", "answer_text", ")", ")", ")", "\n", "\n", "", "", "feature", "=", "InputFeatures", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "example_index", "=", "example_index", ",", "\n", "doc_span_index", "=", "doc_span_index", ",", "\n", "tokens", "=", "tokens", ",", "\n", "token_to_orig_map", "=", "token_to_orig_map", ",", "\n", "token_is_max_context", "=", "token_is_max_context", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "wn_concept_ids", "=", "wn_concept_ids", ",", "\n", "nell_concept_ids", "=", "nell_concept_ids", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "example", ".", "is_impossible", ")", "\n", "\n", "unique_id", "+=", "1", "\n", "\n", "yield", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.DataProcessor.__init__": [[547, 575], ["tokenization.FullTokenizer", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab_path", ",", "do_lower_case", ",", "max_seq_length", ",", "in_tokens", ",", "\n", "doc_stride", ",", "max_query_length", ")", ":", "\n", "        ", "self", ".", "_tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "vocab_path", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "_max_seq_length", "=", "max_seq_length", "\n", "self", ".", "_doc_stride", "=", "doc_stride", "\n", "self", ".", "_max_query_length", "=", "max_query_length", "\n", "self", ".", "_in_tokens", "=", "in_tokens", "\n", "\n", "self", ".", "vocab", "=", "self", ".", "_tokenizer", ".", "vocab", "\n", "self", ".", "vocab_size", "=", "len", "(", "self", ".", "vocab", ")", "\n", "self", ".", "pad_id", "=", "self", ".", "vocab", "[", "\"[PAD]\"", "]", "\n", "self", ".", "cls_id", "=", "self", ".", "vocab", "[", "\"[CLS]\"", "]", "\n", "self", ".", "sep_id", "=", "self", ".", "vocab", "[", "\"[SEP]\"", "]", "\n", "self", ".", "mask_id", "=", "self", ".", "vocab", "[", "\"[MASK]\"", "]", "\n", "\n", "self", ".", "current_train_example", "=", "-", "1", "\n", "self", ".", "num_train_examples", "=", "-", "1", "\n", "self", ".", "current_train_epoch", "=", "-", "1", "\n", "\n", "self", ".", "train_examples", "=", "None", "\n", "self", ".", "predict_examples", "=", "None", "\n", "self", ".", "num_examples", "=", "{", "'train'", ":", "-", "1", ",", "'predict'", ":", "-", "1", "}", "\n", "\n", "self", ".", "train_wn_max_concept_length", "=", "None", "\n", "self", ".", "predict_wn_max_concept_length", "=", "None", "\n", "self", ".", "train_nell_max_concept_length", "=", "None", "\n", "self", ".", "predict_nell_max_concept_length", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.DataProcessor.get_train_progress": [[576, 579], ["None"], "methods", ["None"], ["", "def", "get_train_progress", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets progress for training phase.\"\"\"", "\n", "return", "self", ".", "current_train_example", ",", "self", ".", "current_train_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.DataProcessor.get_examples": [[580, 589], ["record_twomemory.read_record_examples"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization.read_record_examples"], ["", "def", "get_examples", "(", "self", ",", "\n", "data_path", ",", "\n", "is_training", ",", "\n", "version_2_with_negative", "=", "False", ")", ":", "\n", "        ", "examples", "=", "read_record_examples", "(", "\n", "input_file", "=", "data_path", ",", "\n", "is_training", "=", "is_training", ",", "\n", "version_2_with_negative", "=", "version_2_with_negative", ")", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.DataProcessor.get_num_examples": [[590, 595], ["ValueError"], "methods", ["None"], ["", "def", "get_num_examples", "(", "self", ",", "phase", ")", ":", "\n", "        ", "if", "phase", "not", "in", "[", "'train'", ",", "'predict'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unknown phase, which should be in ['train', 'predict'].\"", ")", "\n", "", "return", "self", ".", "num_examples", "[", "phase", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.DataProcessor.get_features": [[596, 606], ["record_twomemory.Examples_To_Features_Converter", "Examples_To_Features_Converter."], "methods", ["None"], ["", "def", "get_features", "(", "self", ",", "examples", ",", "is_training", ",", "**", "concept_settings", ")", ":", "\n", "        ", "convert_examples_to_features", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", "\n", "features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "examples", ",", "\n", "tokenizer", "=", "self", ".", "_tokenizer", ",", "\n", "max_seq_length", "=", "self", ".", "_max_seq_length", ",", "\n", "doc_stride", "=", "self", ".", "_doc_stride", ",", "\n", "max_query_length", "=", "self", ".", "_max_query_length", ",", "\n", "is_training", "=", "is_training", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.DataProcessor.data_generator": [[607, 712], ["record_twomemory.DataProcessor.get_examples", "len", "enumerate", "range", "record_twomemory.DataProcessor.get_examples", "len", "ValueError", "len", "max", "len", "record_twomemory.Examples_To_Features_Converter", "record_twomemory.Examples_To_Features_Converter", "record_twomemory.Examples_To_Features_Converter", "record_twomemory.Examples_To_Features_Converter", "record_twomemory.DataProcessor.data_generator.batch_reader"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_examples", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_examples", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.batch_reader"], ["", "def", "data_generator", "(", "self", ",", "\n", "data_path", ",", "\n", "batch_size", ",", "\n", "phase", "=", "'train'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "version_2_with_negative", "=", "False", ",", "\n", "epoch", "=", "1", ",", "\n", "**", "concept_settings", ")", ":", "\n", "        ", "if", "phase", "==", "'train'", ":", "\n", "            ", "self", ".", "train_examples", "=", "self", ".", "get_examples", "(", "\n", "data_path", ",", "\n", "is_training", "=", "True", ",", "\n", "version_2_with_negative", "=", "version_2_with_negative", ")", "\n", "examples", "=", "self", ".", "train_examples", "\n", "self", ".", "num_examples", "[", "'train'", "]", "=", "len", "(", "self", ".", "train_examples", ")", "\n", "", "elif", "phase", "==", "'predict'", ":", "\n", "            ", "self", ".", "predict_examples", "=", "self", ".", "get_examples", "(", "\n", "data_path", ",", "\n", "is_training", "=", "False", ",", "\n", "version_2_with_negative", "=", "version_2_with_negative", ")", "\n", "examples", "=", "self", ".", "predict_examples", "\n", "self", ".", "num_examples", "[", "'predict'", "]", "=", "len", "(", "self", ".", "predict_examples", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unknown phase, which should be in ['train', 'predict'].\"", ")", "\n", "\n", "", "def", "batch_reader", "(", "features", ",", "batch_size", ",", "in_tokens", ")", ":", "\n", "            ", "batch", ",", "total_token_num", ",", "max_len", "=", "[", "]", ",", "0", ",", "0", "\n", "for", "(", "index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "                ", "if", "phase", "==", "'train'", ":", "\n", "                    ", "self", ".", "current_train_example", "=", "index", "+", "1", "\n", "", "seq_len", "=", "len", "(", "feature", ".", "input_ids", ")", "\n", "labels", "=", "[", "feature", ".", "unique_id", "\n", "]", "if", "feature", ".", "start_position", "is", "None", "else", "[", "\n", "feature", ".", "start_position", ",", "feature", ".", "end_position", "\n", "]", "\n", "example", "=", "[", "\n", "# feature.input_ids, feature.segment_ids, range(seq_len), feature.wn_concept_ids, feature.nell_concept_ids", "\n", "feature", ".", "input_ids", ",", "feature", ".", "segment_ids", ",", "range", "(", "384", ")", ",", "feature", ".", "wn_concept_ids", ",", "feature", ".", "nell_concept_ids", "\n", "]", "+", "labels", "\n", "max_len", "=", "max", "(", "max_len", ",", "seq_len", ")", "\n", "\n", "#max_len = max(max_len, len(token_ids))", "\n", "if", "in_tokens", ":", "\n", "                    ", "to_append", "=", "(", "len", "(", "batch", ")", "+", "1", ")", "*", "max_len", "<=", "batch_size", "\n", "", "else", ":", "\n", "                    ", "to_append", "=", "len", "(", "batch", ")", "<", "batch_size", "\n", "\n", "", "if", "to_append", ":", "\n", "                    ", "batch", ".", "append", "(", "example", ")", "\n", "total_token_num", "+=", "seq_len", "\n", "", "else", ":", "\n", "                    ", "yield", "batch", ",", "total_token_num", "\n", "batch", ",", "total_token_num", ",", "max_len", "=", "[", "example", "\n", "]", ",", "seq_len", ",", "seq_len", "\n", "", "", "if", "len", "(", "batch", ")", ">", "0", ":", "\n", "                ", "yield", "batch", ",", "total_token_num", "\n", "\n", "", "", "if", "phase", "==", "'train'", ":", "\n", "            ", "self", ".", "train_wn_max_concept_length", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", ".", "max_wn_concept_length", "\n", "self", ".", "train_nell_max_concept_length", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", ".", "max_nell_concept_length", "\n", "", "else", ":", "\n", "            ", "self", ".", "predict_wn_max_concept_length", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", ".", "max_wn_concept_length", "\n", "self", ".", "predict_nell_max_concept_length", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", ".", "max_nell_concept_length", "\n", "\n", "", "def", "wrapper", "(", ")", ":", "\n", "            ", "for", "epoch_index", "in", "range", "(", "epoch", ")", ":", "\n", "                ", "if", "shuffle", ":", "\n", "                    ", "random", ".", "shuffle", "(", "examples", ")", "\n", "", "if", "phase", "==", "'train'", ":", "\n", "                    ", "self", ".", "current_train_epoch", "=", "epoch_index", "\n", "features", "=", "self", ".", "get_features", "(", "examples", ",", "is_training", "=", "True", ",", "**", "concept_settings", ")", "\n", "max_wn_concept_length", "=", "self", ".", "train_wn_max_concept_length", "\n", "max_nell_concept_length", "=", "self", ".", "train_nell_max_concept_length", "\n", "", "else", ":", "\n", "                    ", "features", "=", "self", ".", "get_features", "(", "examples", ",", "is_training", "=", "False", ",", "**", "concept_settings", ")", "\n", "max_wn_concept_length", "=", "self", ".", "predict_wn_max_concept_length", "\n", "max_nell_concept_length", "=", "self", ".", "predict_nell_max_concept_length", "\n", "\n", "", "all_dev_batches", "=", "[", "]", "\n", "for", "batch_data", ",", "total_token_num", "in", "batch_reader", "(", "\n", "features", ",", "batch_size", ",", "self", ".", "_in_tokens", ")", ":", "\n", "                    ", "batch_data", "=", "prepare_batch_data", "(", "\n", "batch_data", ",", "\n", "total_token_num", ",", "\n", "voc_size", "=", "-", "1", ",", "\n", "pad_id", "=", "self", ".", "pad_id", ",", "\n", "cls_id", "=", "self", ".", "cls_id", ",", "\n", "sep_id", "=", "self", ".", "sep_id", ",", "\n", "mask_id", "=", "-", "1", ",", "\n", "return_input_mask", "=", "True", ",", "\n", "return_max_len", "=", "False", ",", "\n", "return_num_token", "=", "False", ",", "\n", "max_wn_concept_length", "=", "max_wn_concept_length", ",", "\n", "max_nell_concept_length", "=", "max_nell_concept_length", ")", "\n", "if", "len", "(", "all_dev_batches", ")", "<", "dev_count", ":", "\n", "                        ", "all_dev_batches", ".", "append", "(", "batch_data", ")", "\n", "\n", "", "if", "len", "(", "all_dev_batches", ")", "==", "dev_count", ":", "\n", "                        ", "for", "batch", "in", "all_dev_batches", ":", "\n", "                            ", "yield", "batch", "\n", "", "all_dev_batches", "=", "[", "]", "\n", "\n", "", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.read_record_examples": [[110, 190], ["open", "[].replace", "json.load", "record_twomemory.read_record_examples.is_whitespace"], "function", ["None"], ["", "", "def", "read_record_examples", "(", "input_file", ",", "is_training", ",", "version_2_with_negative", "=", "False", ")", ":", "\n", "    ", "\"\"\"Read a ReCoRD json file into a list of ReCoRDExample.\"\"\"", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "        ", "input_data", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "\n", "", "def", "is_whitespace", "(", "c", ")", ":", "\n", "        ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "examples", "=", "[", "]", "\n", "for", "entry", "in", "input_data", ":", "\n", "        ", "paragraph_text", "=", "entry", "[", "\"passage\"", "]", "[", "\"text\"", "]", ".", "replace", "(", "'\\xa0'", ",", "' '", ")", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "            ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "                ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "prev_is_whitespace", ":", "\n", "                    ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                    ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "", "for", "qa", "in", "entry", "[", "\"qas\"", "]", ":", "\n", "            ", "qas_id", "=", "qa", "[", "\"id\"", "]", "\n", "question_text", "=", "qa", "[", "\"query\"", "]", ".", "replace", "(", "'\\xa0'", ",", "' '", ")", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "is_impossible", "=", "False", "\n", "if", "is_training", ":", "\n", "\n", "                ", "if", "version_2_with_negative", ":", "\n", "                    ", "is_impossible", "=", "qa", "[", "\"is_impossible\"", "]", "\n", "# if (len(qa[\"answers\"]) != 1) and (not is_impossible):", "\n", "#     raise ValueError(", "\n", "#         \"For training, each question should have exactly 1 answer.\"", "\n", "#     )", "\n", "", "if", "not", "is_impossible", ":", "\n", "                    ", "answer", "=", "qa", "[", "\"answers\"", "]", "[", "0", "]", "\n", "orig_answer_text", "=", "answer", "[", "\"text\"", "]", "\n", "answer_offset", "=", "answer", "[", "\"start\"", "]", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "\n", "answer_length", "-", "1", "]", "\n", "# Only add answers where the text can be exactly recovered from the", "\n", "# document. If this CAN'T happen it's likely due to weird Unicode", "\n", "# stuff so we will just skip the example.", "\n", "#", "\n", "# Note that this means for training mode, every example is NOT", "\n", "# guaranteed to be preserved.", "\n", "actual_text", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "start_position", ":", "(", "\n", "end_position", "+", "1", ")", "]", ")", "\n", "cleaned_answer_text", "=", "\" \"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "if", "actual_text", ".", "find", "(", "cleaned_answer_text", ")", "==", "-", "1", ":", "\n", "                        ", "logger", ".", "info", "(", "\"Could not find answer: '%s' vs. '%s'\"", ",", "\n", "actual_text", ",", "cleaned_answer_text", ")", "\n", "continue", "\n", "", "", "else", ":", "\n", "                    ", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "orig_answer_text", "=", "\"\"", "\n", "\n", "", "", "example", "=", "ReCoRDExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "\n", "orig_answer_text", "=", "orig_answer_text", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "is_impossible", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory._improve_answer_span": [[471, 506], ["range", "tokenizer.tokenize", "range"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "", "", "", "def", "_improve_answer_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_answer_text", ")", ":", "\n", "    ", "\"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"", "\n", "\n", "# The ReCoRD annotations are character based. We first project them to", "\n", "# whitespace-tokenized words. But then after WordPiece tokenization, we can", "\n", "# often find a \"better match\". For example:", "\n", "#", "\n", "#   Question: What year was John Smith born?", "\n", "#   Context: The leader was John Smith (1895-1943).", "\n", "#   Answer: 1895", "\n", "#", "\n", "# The original whitespace-tokenized answer will be \"(1895-1943).\". However", "\n", "# after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match", "\n", "# the exact answer, 1895.", "\n", "#", "\n", "# However, this is not always possible. Consider the following:", "\n", "#", "\n", "#   Question: What country is the top exporter of electornics?", "\n", "#   Context: The Japanese electronics industry is the lagest in the world.", "\n", "#   Answer: Japan", "\n", "#", "\n", "# In this case, the annotator chose \"Japan\" as a character sub-span of", "\n", "# the word \"Japanese\". Since our WordPiece tokenizer does not split", "\n", "# \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare", "\n", "# in ReCoRD, but does happen.", "\n", "tok_answer_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_answer_text", ")", ")", "\n", "\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "        ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "text_span", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_answer_text", ":", "\n", "                ", "return", "(", "new_start", ",", "new_end", ")", "\n", "\n", "", "", "", "return", "(", "input_start", ",", "input_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory._check_is_max_context": [[508, 544], ["enumerate", "min"], "function", ["None"], ["", "def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "    ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "        ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "            ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "            ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "\n", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "            ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.write_predictions": [[714, 939], ["logger.info", "logger.info", "logger.info", "collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "eval.record_official_evaluate.evaluate", "open", "example_index_to_features[].append", "enumerate", "sorted", "collections.namedtuple", "record_twomemory._compute_softmax", "enumerate", "open", "writer.write", "open", "writer.write", "open", "writer.write", "json.load", "record_twomemory._get_best_indexes", "record_twomemory._get_best_indexes", "sorted.append", "nbest.append", "nbest.append", "len", "total_scores.append", "logger.info", "collections.OrderedDict", "nbest_json.append", "len", "range", "open", "writer.write", "candidates.append", "collections.namedtuple.", "len", "tok_text.strip.replace", "tok_text.strip.replace", "tok_text.strip.strip", "record_twomemory.get_final_text", "collections.namedtuple.", "nbest.append", "collections.namedtuple.", "len", "any", "json.dumps", "json.dumps", "json.dumps", "sorted.append", "tok_text.strip.split", "collections.namedtuple.", "json.dumps", "len", "len", "feature.token_is_max_context.get", "collections.namedtuple.", "eval.record_official_evaluate.f1_score"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.evaluate", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._compute_softmax", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.get_final_text", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.f1_score"], ["", "", "def", "write_predictions", "(", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "output_null_log_odds_file", ",", "\n", "version_2_with_negative", ",", "null_score_diff_threshold", ",", "\n", "verbose", ",", "predict_file", ",", "evaluation_result_file", ")", ":", "\n", "    ", "\"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"", "\n", "logger", ".", "info", "(", "\"Writing predictions to: %s\"", "%", "(", "output_prediction_file", ")", ")", "\n", "logger", ".", "info", "(", "\"Writing nbest to: %s\"", "%", "(", "output_nbest_file", ")", ")", "\n", "logger", ".", "info", "(", "\"Writing evaluation result to: %s\"", "%", "(", "evaluation_result_file", ")", ")", "\n", "\n", "# load ground truth file for evaluation and post-edit", "\n", "with", "open", "(", "predict_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "        ", "predict_json", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "all_candidates", "=", "{", "}", "\n", "for", "passage", "in", "predict_json", ":", "\n", "            ", "passage_text", "=", "passage", "[", "'passage'", "]", "[", "'text'", "]", "\n", "candidates", "=", "[", "]", "\n", "for", "entity_info", "in", "passage", "[", "'passage'", "]", "[", "'entities'", "]", ":", "\n", "                ", "start_offset", "=", "entity_info", "[", "'start'", "]", "\n", "end_offset", "=", "entity_info", "[", "'end'", "]", "\n", "candidates", ".", "append", "(", "passage_text", "[", "start_offset", ":", "end_offset", "+", "1", "]", ")", "\n", "", "for", "qa", "in", "passage", "[", "'qas'", "]", ":", "\n", "                ", "all_candidates", "[", "qa", "[", "'id'", "]", "]", "=", "candidates", "\n", "\n", "", "", "", "example_index_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "        ", "example_index_to_features", "[", "feature", ".", "example_index", "]", ".", "append", "(", "feature", ")", "\n", "\n", "", "unique_id_to_result", "=", "{", "}", "\n", "for", "result", "in", "all_results", ":", "\n", "        ", "unique_id_to_result", "[", "result", ".", "unique_id", "]", "=", "result", "\n", "\n", "", "_PrelimPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"PrelimPrediction\"", ",", "[", "\n", "\"feature_index\"", ",", "\"start_index\"", ",", "\"end_index\"", ",", "\"start_logit\"", ",", "\n", "\"end_logit\"", "\n", "]", ")", "\n", "\n", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_nbest_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "scores_diff_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "all_examples", ")", ":", "\n", "        ", "features", "=", "example_index_to_features", "[", "example_index", "]", "\n", "\n", "prelim_predictions", "=", "[", "]", "\n", "# keep track of the minimum score of null start+end of position 0", "\n", "score_null", "=", "1000000", "# large and positive", "\n", "min_null_feature_index", "=", "0", "# the paragraph slice with min mull score", "\n", "null_start_logit", "=", "0", "# the start logit at the slice with min null score", "\n", "null_end_logit", "=", "0", "# the end logit at the slice with min null score", "\n", "for", "(", "feature_index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "            ", "result", "=", "unique_id_to_result", "[", "feature", ".", "unique_id", "]", "\n", "start_indexes", "=", "_get_best_indexes", "(", "result", ".", "start_logits", ",", "n_best_size", ")", "\n", "end_indexes", "=", "_get_best_indexes", "(", "result", ".", "end_logits", ",", "n_best_size", ")", "\n", "# if we could have irrelevant answers, get the min score of irrelevant", "\n", "if", "version_2_with_negative", ":", "\n", "                ", "feature_null_score", "=", "result", ".", "start_logits", "[", "0", "]", "+", "result", ".", "end_logits", "[", "\n", "0", "]", "\n", "if", "feature_null_score", "<", "score_null", ":", "\n", "                    ", "score_null", "=", "feature_null_score", "\n", "min_null_feature_index", "=", "feature_index", "\n", "null_start_logit", "=", "result", ".", "start_logits", "[", "0", "]", "\n", "null_end_logit", "=", "result", ".", "end_logits", "[", "0", "]", "\n", "", "", "for", "start_index", "in", "start_indexes", ":", "\n", "                ", "for", "end_index", "in", "end_indexes", ":", "\n", "# We could hypothetically create invalid predictions, e.g., predict", "\n", "# that the start of the span is in the question. We throw out all", "\n", "# invalid predictions.", "\n", "                    ", "if", "start_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "start_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                        ", "continue", "\n", "", "if", "not", "feature", ".", "token_is_max_context", ".", "get", "(", "start_index", ",", "False", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "                        ", "continue", "\n", "", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "if", "length", ">", "max_answer_length", ":", "\n", "                        ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "feature_index", ",", "\n", "start_index", "=", "start_index", ",", "\n", "end_index", "=", "end_index", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "start_index", "]", ",", "\n", "end_logit", "=", "result", ".", "end_logits", "[", "end_index", "]", ")", ")", "\n", "\n", "", "", "", "if", "version_2_with_negative", ":", "\n", "            ", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "min_null_feature_index", ",", "\n", "start_index", "=", "0", ",", "\n", "end_index", "=", "0", ",", "\n", "start_logit", "=", "null_start_logit", ",", "\n", "end_logit", "=", "null_end_logit", ")", ")", "\n", "", "prelim_predictions", "=", "sorted", "(", "\n", "prelim_predictions", ",", "\n", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_logit", "+", "x", ".", "end_logit", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "_NbestPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"NbestPrediction\"", ",", "[", "\"text\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "seen_predictions", "=", "{", "}", "\n", "nbest", "=", "[", "]", "\n", "for", "pred", "in", "prelim_predictions", ":", "\n", "            ", "if", "len", "(", "nbest", ")", ">=", "n_best_size", ":", "\n", "                ", "break", "\n", "", "feature", "=", "features", "[", "pred", ".", "feature_index", "]", "\n", "if", "pred", ".", "start_index", ">", "0", ":", "# this is a non-null prediction", "\n", "                ", "tok_tokens", "=", "feature", ".", "tokens", "[", "pred", ".", "start_index", ":", "(", "pred", ".", "end_index", "+", "1", "\n", ")", "]", "\n", "orig_doc_start", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "start_index", "]", "\n", "orig_doc_end", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "end_index", "]", "\n", "orig_tokens", "=", "example", ".", "doc_tokens", "[", "orig_doc_start", ":", "(", "orig_doc_end", "+", "\n", "1", ")", "]", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_tokens", ")", "\n", "\n", "# De-tokenize WordPieces that have been split off.", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\"##\"", ",", "\"\"", ")", "\n", "\n", "# Clean whitespace", "\n", "tok_text", "=", "tok_text", ".", "strip", "(", ")", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_text", ".", "split", "(", ")", ")", "\n", "orig_text", "=", "\" \"", ".", "join", "(", "orig_tokens", ")", "\n", "\n", "final_text", "=", "get_final_text", "(", "tok_text", ",", "orig_text", ",", "do_lower_case", ",", "\n", "verbose", ")", "\n", "if", "final_text", "in", "seen_predictions", ":", "\n", "                    ", "continue", "\n", "\n", "", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "", "else", ":", "\n", "                ", "final_text", "=", "\"\"", "\n", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "\n", "", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "final_text", ",", "\n", "start_logit", "=", "pred", ".", "start_logit", ",", "\n", "end_logit", "=", "pred", ".", "end_logit", ")", ")", "\n", "\n", "# if we didn't inlude the empty option in the n-best, inlcude it", "\n", "", "if", "version_2_with_negative", ":", "\n", "            ", "if", "\"\"", "not", "in", "seen_predictions", ":", "\n", "                ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "\"\"", ",", "\n", "start_logit", "=", "null_start_logit", ",", "\n", "end_logit", "=", "null_end_logit", ")", ")", "\n", "# In very rare edge cases we could have no valid predictions. So we", "\n", "# just create a nonce prediction in this case to avoid failure.", "\n", "", "", "if", "not", "nbest", ":", "\n", "            ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "\"empty\"", ",", "start_logit", "=", "0.0", ",", "end_logit", "=", "0.0", ")", ")", "\n", "\n", "", "assert", "len", "(", "nbest", ")", ">=", "1", "\n", "\n", "total_scores", "=", "[", "]", "\n", "best_non_null_entry", "=", "None", "\n", "for", "entry", "in", "nbest", ":", "\n", "            ", "total_scores", ".", "append", "(", "entry", ".", "start_logit", "+", "entry", ".", "end_logit", ")", "\n", "if", "not", "best_non_null_entry", ":", "\n", "                ", "if", "entry", ".", "text", ":", "\n", "                    ", "best_non_null_entry", "=", "entry", "\n", "# debug", "\n", "", "", "", "if", "best_non_null_entry", "is", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\"Emmm..., sth wrong\"", ")", "\n", "\n", "", "probs", "=", "_compute_softmax", "(", "total_scores", ")", "\n", "\n", "nbest_json", "=", "[", "]", "\n", "for", "(", "i", ",", "entry", ")", "in", "enumerate", "(", "nbest", ")", ":", "\n", "            ", "output", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "output", "[", "\"text\"", "]", "=", "entry", ".", "text", "\n", "output", "[", "\"probability\"", "]", "=", "probs", "[", "i", "]", "\n", "output", "[", "\"start_logit\"", "]", "=", "entry", ".", "start_logit", "\n", "output", "[", "\"end_logit\"", "]", "=", "entry", ".", "end_logit", "\n", "nbest_json", ".", "append", "(", "output", ")", "\n", "\n", "", "assert", "len", "(", "nbest_json", ")", ">=", "1", "\n", "\n", "if", "not", "version_2_with_negative", ":", "\n", "# restrict the finally picked prediction to have overlap with at least one candidate", "\n", "            ", "picked_index", "=", "0", "\n", "for", "pred_index", "in", "range", "(", "len", "(", "nbest_json", ")", ")", ":", "\n", "                ", "if", "any", "(", "[", "f1_score", "(", "nbest_json", "[", "pred_index", "]", "[", "'text'", "]", ",", "candidate", ")", ">", "0.", "for", "candidate", "in", "all_candidates", "[", "example", ".", "qas_id", "]", "]", ")", ":", "\n", "                    ", "picked_index", "=", "pred_index", "\n", "break", "\n", "", "", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "[", "picked_index", "]", "[", "\"text\"", "]", "\n", "", "else", ":", "\n", "# predict \"\" iff the null score - the score of best non-null > threshold", "\n", "            ", "score_diff", "=", "score_null", "-", "best_non_null_entry", ".", "start_logit", "-", "(", "\n", "best_non_null_entry", ".", "end_logit", ")", "\n", "scores_diff_json", "[", "example", ".", "qas_id", "]", "=", "score_diff", "\n", "if", "score_diff", ">", "null_score_diff_threshold", ":", "\n", "                ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "\"\"", "\n", "", "else", ":", "\n", "                ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "best_non_null_entry", ".", "text", "\n", "\n", "", "", "all_nbest_json", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "\n", "\n", "", "with", "open", "(", "output_prediction_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_predictions", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "with", "open", "(", "output_nbest_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_nbest_json", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "if", "version_2_with_negative", ":", "\n", "        ", "with", "open", "(", "output_null_log_odds_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "scores_diff_json", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "", "eval_result", ",", "_", "=", "evaluate", "(", "predict_json", ",", "all_predictions", ")", "\n", "\n", "with", "open", "(", "evaluation_result_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "eval_result", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "return", "eval_result", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory.get_final_text": [[940, 1033], ["tokenization.BasicTokenizer", "tok_text.find", "record_twomemory.get_final_text._strip_spaces"], "function", ["None"], ["", "def", "get_final_text", "(", "pred_text", ",", "orig_text", ",", "do_lower_case", ",", "verbose", ")", ":", "\n", "    ", "\"\"\"Project the tokenized prediction back to the original text.\"\"\"", "\n", "\n", "# When we created the data, we kept track of the alignment between original", "\n", "# (whitespace tokenized) tokens and our WordPiece tokenized tokens. So", "\n", "# now `orig_text` contains the span of our original text corresponding to the", "\n", "# span that we predicted.", "\n", "#", "\n", "# However, `orig_text` may contain extra characters that we don't want in", "\n", "# our prediction.", "\n", "#", "\n", "# For example, let's say:", "\n", "#   pred_text = steve smith", "\n", "#   orig_text = Steve Smith's", "\n", "#", "\n", "# We don't want to return `orig_text` because it contains the extra \"'s\".", "\n", "#", "\n", "# We don't want to return `pred_text` because it's already been normalized", "\n", "# (the ReCoRD eval script also does punctuation stripping/lower casing but", "\n", "# our tokenizer does additional normalization like stripping accent", "\n", "# characters).", "\n", "#", "\n", "# What we really want to return is \"Steve Smith\".", "\n", "#", "\n", "# Therefore, we have to apply a semi-complicated alignment heruistic between", "\n", "# `pred_text` and `orig_text` to get a character-to-charcter alignment. This", "\n", "# can fail in certain cases in which case we just return `orig_text`.", "\n", "\n", "def", "_strip_spaces", "(", "text", ")", ":", "\n", "        ", "ns_chars", "=", "[", "]", "\n", "ns_to_s_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "i", ",", "c", ")", "in", "enumerate", "(", "text", ")", ":", "\n", "            ", "if", "c", "==", "\" \"", ":", "\n", "                ", "continue", "\n", "", "ns_to_s_map", "[", "len", "(", "ns_chars", ")", "]", "=", "i", "\n", "ns_chars", ".", "append", "(", "c", ")", "\n", "", "ns_text", "=", "\"\"", ".", "join", "(", "ns_chars", ")", "\n", "return", "(", "ns_text", ",", "ns_to_s_map", ")", "\n", "\n", "# We first tokenize `orig_text`, strip whitespace from the result", "\n", "# and `pred_text`, and check if they are the same length. If they are", "\n", "# NOT the same length, the heuristic has failed. If they are the same", "\n", "# length, we assume the characters are one-to-one aligned.", "\n", "", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_text", ")", ")", "\n", "\n", "start_position", "=", "tok_text", ".", "find", "(", "pred_text", ")", "\n", "if", "start_position", "==", "-", "1", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Unable to find text: '%s' in '%s'\"", "%", "(", "pred_text", ",", "orig_text", ")", ")", "\n", "", "return", "orig_text", "\n", "", "end_position", "=", "start_position", "+", "len", "(", "pred_text", ")", "-", "1", "\n", "\n", "(", "orig_ns_text", ",", "orig_ns_to_s_map", ")", "=", "_strip_spaces", "(", "orig_text", ")", "\n", "(", "tok_ns_text", ",", "tok_ns_to_s_map", ")", "=", "_strip_spaces", "(", "tok_text", ")", "\n", "\n", "if", "len", "(", "orig_ns_text", ")", "!=", "len", "(", "tok_ns_text", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Length not equal after stripping spaces: '%s' vs '%s'\"", ",", "\n", "orig_ns_text", ",", "tok_ns_text", ")", "\n", "", "return", "orig_text", "\n", "\n", "# We then project the characters in `pred_text` back to `orig_text` using", "\n", "# the character-to-character alignment.", "\n", "", "tok_s_to_ns_map", "=", "{", "}", "\n", "for", "(", "i", ",", "tok_index", ")", "in", "six", ".", "iteritems", "(", "tok_ns_to_s_map", ")", ":", "\n", "        ", "tok_s_to_ns_map", "[", "tok_index", "]", "=", "i", "\n", "\n", "", "orig_start_position", "=", "None", "\n", "if", "start_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_start_position", "=", "tok_s_to_ns_map", "[", "start_position", "]", "\n", "if", "ns_start_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_start_position", "=", "orig_ns_to_s_map", "[", "ns_start_position", "]", "\n", "\n", "", "", "if", "orig_start_position", "is", "None", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Couldn't map start position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "orig_end_position", "=", "None", "\n", "if", "end_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_end_position", "=", "tok_s_to_ns_map", "[", "end_position", "]", "\n", "if", "ns_end_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_end_position", "=", "orig_ns_to_s_map", "[", "ns_end_position", "]", "\n", "\n", "", "", "if", "orig_end_position", "is", "None", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Couldn't map end position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "output_text", "=", "orig_text", "[", "orig_start_position", ":", "(", "orig_end_position", "+", "1", ")", "]", "\n", "return", "output_text", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory._get_best_indexes": [[1035, 1046], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "_get_best_indexes", "(", "logits", ",", "n_best_size", ")", ":", "\n", "    ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "\n", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "        ", "if", "i", ">=", "n_best_size", ":", "\n", "            ", "break", "\n", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record_twomemory._compute_softmax": [[1048, 1069], ["math.exp", "exp_scores.append", "probs.append"], "function", ["None"], ["", "def", "_compute_softmax", "(", "scores", ")", ":", "\n", "    ", "\"\"\"Compute softmax probability over raw logits.\"\"\"", "\n", "if", "not", "scores", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "", "max_score", "=", "None", "\n", "for", "score", "in", "scores", ":", "\n", "        ", "if", "max_score", "is", "None", "or", "score", ">", "max_score", ":", "\n", "            ", "max_score", "=", "score", "\n", "\n", "", "", "exp_scores", "=", "[", "]", "\n", "total_sum", "=", "0.0", "\n", "for", "score", "in", "scores", ":", "\n", "        ", "x", "=", "math", ".", "exp", "(", "score", "-", "max_score", ")", "\n", "exp_scores", ".", "append", "(", "x", ")", "\n", "total_sum", "+=", "x", "\n", "\n", "", "probs", "=", "[", "]", "\n", "for", "score", "in", "exp_scores", ":", "\n", "        ", "probs", ".", "append", "(", "score", "/", "total_sum", ")", "\n", "", "return", "probs", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.SquadExample.__init__": [[42, 57], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "qas_id", ",", "\n", "question_text", ",", "\n", "doc_tokens", ",", "\n", "orig_answer_text", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "False", ")", ":", "\n", "        ", "self", ".", "qas_id", "=", "qas_id", "\n", "self", ".", "question_text", "=", "question_text", "\n", "self", ".", "doc_tokens", "=", "doc_tokens", "\n", "self", ".", "orig_answer_text", "=", "orig_answer_text", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.SquadExample.__str__": [[58, 60], ["squad_twomemory.SquadExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.SQuADExample.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.SquadExample.__repr__": [[61, 74], ["tokenization.printable_text", "tokenization.printable_text"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"\"", "\n", "s", "+=", "\"qas_id: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "self", ".", "qas_id", ")", ")", "\n", "s", "+=", "\", question_text: %s\"", "%", "(", "\n", "tokenization", ".", "printable_text", "(", "self", ".", "question_text", ")", ")", "\n", "s", "+=", "\", doc_tokens: [%s]\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "doc_tokens", ")", ")", "\n", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", start_position: %d\"", "%", "(", "self", ".", "start_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", end_position: %d\"", "%", "(", "self", ".", "end_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", is_impossible: %r\"", "%", "(", "self", ".", "is_impossible", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.InputFeatures.__init__": [[79, 108], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "unique_id", ",", "\n", "example_index", ",", "\n", "doc_span_index", ",", "\n", "tokens", ",", "\n", "token_to_orig_map", ",", "\n", "token_is_max_context", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "segment_ids", ",", "\n", "wn_concept_ids", ",", "\n", "nell_concept_ids", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "None", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "example_index", "=", "example_index", "\n", "self", ".", "doc_span_index", "=", "doc_span_index", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "token_to_orig_map", "=", "token_to_orig_map", "\n", "self", ".", "token_is_max_context", "=", "token_is_max_context", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "self", ".", "wn_concept_ids", "=", "wn_concept_ids", "\n", "self", ".", "nell_concept_ids", "=", "nell_concept_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.Examples_To_Features_Converter.__init__": [[193, 224], ["os.path.exists", "pickle.load", "open", "os.path.exists", "pickle.load", "max", "os.path.exists", "pickle.load", "max", "open", "open", "len", "max", "squad_twomemory.Examples_To_Features_Converter.synsets_info.values", "squad_twomemory.Examples_To_Features_Converter.nell_retrieve_info.items", "len", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "**", "concept_settings", ")", ":", "\n", "        ", "self", ".", "concept_settings", "=", "concept_settings", "\n", "\n", "# load necessary data files for mapping to related concepts", "\n", "# 1. mapping from subword-level tokenization to word-level tokenization", "\n", "tokenization_filepath", "=", "self", ".", "concept_settings", "[", "'tokenization_path'", "]", "\n", "assert", "os", ".", "path", ".", "exists", "(", "tokenization_filepath", ")", "\n", "self", ".", "all_tokenization_info", "=", "{", "}", "\n", "for", "item", "in", "pickle", ".", "load", "(", "open", "(", "tokenization_filepath", ",", "'rb'", ")", ")", ":", "\n", "            ", "self", ".", "all_tokenization_info", "[", "item", "[", "'id'", "]", "]", "=", "item", "\n", "\n", "# 2. mapping from concept name to concept id", "\n", "", "self", ".", "wn_concept2id", "=", "self", ".", "concept_settings", "[", "'wn_concept2id'", "]", "\n", "self", ".", "nell_concept2id", "=", "self", ".", "concept_settings", "[", "'nell_concept2id'", "]", "\n", "\n", "# 3. retrieved related wordnet concepts (if use_wordnet)", "\n", "if", "concept_settings", "[", "'use_wordnet'", "]", ":", "\n", "            ", "retrieved_synset_filepath", "=", "self", ".", "concept_settings", "[", "'retrieved_synset_path'", "]", "\n", "assert", "os", ".", "path", ".", "exists", "(", "retrieved_synset_filepath", ")", "\n", "self", ".", "synsets_info", "=", "pickle", ".", "load", "(", "open", "(", "retrieved_synset_filepath", ",", "'rb'", ")", ")", "# token to sysnet names", "\n", "self", ".", "max_wn_concept_length", "=", "max", "(", "[", "len", "(", "synsets", ")", "for", "synsets", "in", "self", ".", "synsets_info", ".", "values", "(", ")", "]", ")", "\n", "\n", "# 4. retrieved related nell concepts (if use_nell)", "\n", "", "if", "concept_settings", "[", "'use_nell'", "]", ":", "\n", "            ", "retrieved_nell_concept_filepath", "=", "self", ".", "concept_settings", "[", "'retrieved_nell_concept_path'", "]", "\n", "assert", "os", ".", "path", ".", "exists", "(", "retrieved_nell_concept_filepath", ")", "\n", "self", ".", "nell_retrieve_info", "=", "{", "}", "\n", "for", "item", "in", "pickle", ".", "load", "(", "open", "(", "retrieved_nell_concept_filepath", ",", "'rb'", ")", ")", ":", "\n", "                ", "self", ".", "nell_retrieve_info", "[", "item", "[", "'id'", "]", "]", "=", "item", "\n", "", "self", ".", "max_nell_concept_length", "=", "max", "(", "[", "max", "(", "[", "len", "(", "entity_info", "[", "'retrieved_concepts'", "]", ")", "for", "entity_info", "in", "item", "[", "'query_entities'", "]", "+", "item", "[", "'document_entities'", "]", "]", ")", "\n", "for", "qid", ",", "item", "in", "self", ".", "nell_retrieve_info", ".", "items", "(", ")", "if", "len", "(", "item", "[", "'query_entities'", "]", "+", "item", "[", "'document_entities'", "]", ")", ">", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.Examples_To_Features_Converter._lookup_wordnet_concept_ids": [[226, 237], ["range", "len", "tokenizer.basic_tokenizer._run_strip_accents", "concept_ids.append", "concept_ids.append", "original_token.lower"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_strip_accents"], ["", "", "def", "_lookup_wordnet_concept_ids", "(", "self", ",", "sub_tokens", ",", "sub_to_ori_index", ",", "tokens", ",", "tolower", ",", "tokenizer", ")", ":", "\n", "        ", "concept_ids", "=", "[", "]", "\n", "for", "index", "in", "range", "(", "len", "(", "sub_tokens", ")", ")", ":", "\n", "            ", "original_token", "=", "tokens", "[", "sub_to_ori_index", "[", "index", "]", "]", "\n", "# if tokens are in upper case, we must lower it for retrieving", "\n", "retrieve_token", "=", "tokenizer", ".", "basic_tokenizer", ".", "_run_strip_accents", "(", "original_token", ".", "lower", "(", ")", ")", "if", "tolower", "else", "original_token", "\n", "if", "retrieve_token", "in", "self", ".", "synsets_info", ":", "\n", "                ", "concept_ids", ".", "append", "(", "[", "self", ".", "wn_concept2id", "[", "synset_name", "]", "for", "synset_name", "in", "self", ".", "synsets_info", "[", "retrieve_token", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "concept_ids", ".", "append", "(", "[", "]", ")", "\n", "", "", "return", "concept_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.Examples_To_Features_Converter._lookup_nell_concept_ids": [[238, 247], ["range", "range", "len", "list", "range", "set", "range", "len", "len"], "methods", ["None"], ["", "def", "_lookup_nell_concept_ids", "(", "self", ",", "sub_tokens", ",", "sub_to_ori_index", ",", "tokens", ",", "nell_info", ")", ":", "\n", "        ", "original_concept_ids", "=", "[", "[", "]", "for", "_", "in", "range", "(", "len", "(", "tokens", ")", ")", "]", "\n", "for", "entity_info", "in", "nell_info", ":", "\n", "            ", "for", "pos", "in", "range", "(", "entity_info", "[", "'token_start'", "]", ",", "entity_info", "[", "'token_end'", "]", "+", "1", ")", ":", "\n", "                ", "original_concept_ids", "[", "pos", "]", "+=", "[", "self", ".", "nell_concept2id", "[", "category_name", "]", "for", "category_name", "in", "entity_info", "[", "'retrieved_concepts'", "]", "]", "\n", "", "", "for", "pos", "in", "range", "(", "len", "(", "original_concept_ids", ")", ")", ":", "\n", "            ", "original_concept_ids", "[", "pos", "]", "=", "list", "(", "set", "(", "original_concept_ids", "[", "pos", "]", ")", ")", "\n", "", "concept_ids", "=", "[", "original_concept_ids", "[", "sub_to_ori_index", "[", "index", "]", "]", "for", "index", "in", "range", "(", "len", "(", "sub_tokens", ")", ")", "]", "\n", "return", "concept_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.Examples_To_Features_Converter.__call__": [[248, 470], ["enumerate", "tokenizer.tokenize", "enumerate", "collections.namedtuple", "enumerate", "squad_twomemory.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "squad_twomemory.Examples_To_Features_Converter._lookup_nell_concept_ids", "len", "orig_to_tok_index.append", "tokenizer.tokenize", "squad_twomemory.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "squad_twomemory.Examples_To_Features_Converter._lookup_nell_concept_ids", "squad_twomemory._improve_answer_span", "len", "doc_spans.append", "min", "tokens.append", "segment_ids.append", "wn_concept_ids.append", "nell_concept_ids.append", "zip", "tokens.append", "segment_ids.append", "wn_concept_ids.append", "nell_concept_ids.append", "range", "tokens.append", "segment_ids.append", "wn_concept_ids.append", "nell_concept_ids.append", "tokenizer.convert_tokens_to_ids", "zip", "squad_twomemory.InputFeatures", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "len", "len", "collections.namedtuple.", "len", "tokens.append", "segment_ids.append", "wn_concept_ids.append", "nell_concept_ids.append", "squad_twomemory._check_is_max_context", "tokens.append", "segment_ids.append", "wn_concept_ids.append", "nell_concept_ids.append", "len", "range", "all", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "len", "len", "len", "tokenization.printable_text", "len", "tokenization.printable_text", "str", "str", "str", "six.iteritems", "six.iteritems", "list", "enumerate", "list", "enumerate", "filter", "filter"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_nell_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_nell_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._improve_answer_span", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._check_is_max_context", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["", "def", "__call__", "(", "self", ",", "\n", "examples", ",", "\n", "tokenizer", ",", "\n", "max_seq_length", ",", "\n", "doc_stride", ",", "\n", "max_query_length", ",", "\n", "is_training", ")", ":", "\n", "        ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "unique_id", "=", "1000000000", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "            ", "tokenization_info", "=", "self", ".", "all_tokenization_info", "[", "example", ".", "qas_id", "]", "\n", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "# check online subword tokenization result is the same as offline result", "\n", "assert", "query_tokens", "==", "tokenization_info", "[", "'query_subtokens'", "]", "\n", "if", "self", ".", "concept_settings", "[", "'use_wordnet'", "]", ":", "\n", "                ", "query_wn_concepts", "=", "self", ".", "_lookup_wordnet_concept_ids", "(", "query_tokens", ",", "tokenization_info", "[", "'query_sub_to_ori_index'", "]", ",", "\n", "tokenization_info", "[", "'query_tokens'", "]", ",", "\n", "tolower", "=", "tokenizer", ".", "basic_tokenizer", ".", "do_lower_case", "==", "False", ",", "tokenizer", "=", "tokenizer", ")", "# if tolower is True, tokenizer must be given", "\n", "\n", "", "if", "self", ".", "concept_settings", "[", "'use_nell'", "]", ":", "\n", "                ", "query_nell_concepts", "=", "self", ".", "_lookup_nell_concept_ids", "(", "query_tokens", ",", "tokenization_info", "[", "'query_sub_to_ori_index'", "]", ",", "\n", "tokenization_info", "[", "'query_tokens'", "]", ",", "self", ".", "nell_retrieve_info", "[", "example", ".", "qas_id", "]", "[", "'query_entities'", "]", ")", "\n", "\n", "", "if", "len", "(", "query_tokens", ")", ">", "max_query_length", ":", "\n", "                ", "query_tokens", "=", "query_tokens", "[", "0", ":", "max_query_length", "]", "\n", "query_wn_concepts", "=", "query_wn_concepts", "[", "0", ":", "max_query_length", "]", "\n", "query_nell_concepts", "=", "query_nell_concepts", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "tok_to_orig_index", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "all_doc_tokens", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "doc_tokens", ")", ":", "\n", "                ", "orig_to_tok_index", ".", "append", "(", "len", "(", "all_doc_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "for", "sub_token", "in", "sub_tokens", ":", "\n", "                    ", "tok_to_orig_index", ".", "append", "(", "i", ")", "\n", "all_doc_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "assert", "all_doc_tokens", "==", "tokenization_info", "[", "'document_subtokens'", "]", "\n", "if", "self", ".", "concept_settings", "[", "'use_wordnet'", "]", ":", "\n", "                ", "doc_wn_concepts", "=", "self", ".", "_lookup_wordnet_concept_ids", "(", "all_doc_tokens", ",", "tokenization_info", "[", "'document_sub_to_ori_index'", "]", ",", "\n", "tokenization_info", "[", "'document_tokens'", "]", ",", "\n", "tolower", "=", "tokenizer", ".", "basic_tokenizer", ".", "do_lower_case", "==", "False", ",", "tokenizer", "=", "tokenizer", ")", "# if tolower is True, tokenizer must be given", "\n", "\n", "", "if", "self", ".", "concept_settings", "[", "'use_nell'", "]", ":", "\n", "                ", "doc_nell_concepts", "=", "self", ".", "_lookup_nell_concept_ids", "(", "all_doc_tokens", ",", "tokenization_info", "[", "'document_sub_to_ori_index'", "]", ",", "\n", "tokenization_info", "[", "'document_tokens'", "]", ",", "self", ".", "nell_retrieve_info", "[", "example", ".", "qas_id", "]", "[", "'document_entities'", "]", ")", "\n", "\n", "", "tok_start_position", "=", "None", "\n", "tok_end_position", "=", "None", "\n", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "                ", "tok_start_position", "=", "-", "1", "\n", "tok_end_position", "=", "-", "1", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "                ", "tok_start_position", "=", "orig_to_tok_index", "[", "example", ".", "start_position", "]", "\n", "if", "example", ".", "end_position", "<", "len", "(", "example", ".", "doc_tokens", ")", "-", "1", ":", "\n", "                    ", "tok_end_position", "=", "orig_to_tok_index", "[", "example", ".", "end_position", "+", "\n", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "                    ", "tok_end_position", "=", "len", "(", "all_doc_tokens", ")", "-", "1", "\n", "", "(", "tok_start_position", ",", "tok_end_position", ")", "=", "_improve_answer_span", "(", "\n", "all_doc_tokens", ",", "tok_start_position", ",", "tok_end_position", ",", "tokenizer", ",", "\n", "example", ".", "orig_answer_text", ")", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "", "max_tokens_for_doc", "=", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "# We can have documents that are longer than the maximum sequence length.", "\n", "# To deal with this we do a sliding window approach, where we take chunks", "\n", "# of the up to our max length with a stride of `doc_stride`.", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "                ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "                    ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "                    ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "                ", "tokens", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "token_is_max_context", "=", "{", "}", "\n", "segment_ids", "=", "[", "]", "\n", "wn_concept_ids", "=", "[", "]", "\n", "nell_concept_ids", "=", "[", "]", "\n", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "wn_concept_ids", ".", "append", "(", "[", "]", ")", "\n", "nell_concept_ids", ".", "append", "(", "[", "]", ")", "\n", "for", "token", ",", "query_wn_concept", ",", "query_nell_concept", "in", "zip", "(", "query_tokens", ",", "query_wn_concepts", ",", "query_nell_concepts", ")", ":", "\n", "                    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "wn_concept_ids", ".", "append", "(", "query_wn_concept", ")", "\n", "nell_concept_ids", ".", "append", "(", "query_nell_concept", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "wn_concept_ids", ".", "append", "(", "[", "]", ")", "\n", "nell_concept_ids", ".", "append", "(", "[", "]", ")", "\n", "\n", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "                    ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "tok_to_orig_index", "[", "split_token_index", "]", "\n", "\n", "is_max_context", "=", "_check_is_max_context", "(", "doc_spans", ",", "doc_span_index", ",", "\n", "split_token_index", ")", "\n", "token_is_max_context", "[", "len", "(", "tokens", ")", "]", "=", "is_max_context", "\n", "tokens", ".", "append", "(", "all_doc_tokens", "[", "split_token_index", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "wn_concept_ids", ".", "append", "(", "doc_wn_concepts", "[", "split_token_index", "]", ")", "\n", "nell_concept_ids", ".", "append", "(", "doc_nell_concepts", "[", "split_token_index", "]", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "wn_concept_ids", ".", "append", "(", "[", "]", ")", "\n", "nell_concept_ids", ".", "append", "(", "[", "]", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "#while len(input_ids) < max_seq_length:", "\n", "#  input_ids.append(0)", "\n", "#  input_mask.append(0)", "\n", "#  segment_ids.append(0)", "\n", "\n", "#assert len(input_ids) == max_seq_length", "\n", "#assert len(input_mask) == max_seq_length", "\n", "#assert len(segment_ids) == max_seq_length         ", "\n", "\n", "for", "concept_ids", ",", "max_concept_length", "in", "zip", "(", "(", "wn_concept_ids", ",", "nell_concept_ids", ")", ",", "(", "self", ".", "max_wn_concept_length", ",", "self", ".", "max_nell_concept_length", ")", ")", ":", "\n", "                    ", "for", "cindex", "in", "range", "(", "len", "(", "concept_ids", ")", ")", ":", "\n", "                        ", "concept_ids", "[", "cindex", "]", "=", "concept_ids", "[", "cindex", "]", "+", "[", "0", "]", "*", "(", "max_concept_length", "-", "len", "(", "concept_ids", "[", "cindex", "]", ")", ")", "\n", "concept_ids", "[", "cindex", "]", "=", "concept_ids", "[", "cindex", "]", "[", ":", "max_concept_length", "]", "\n", "", "assert", "all", "(", "[", "len", "(", "id_list", ")", "==", "max_concept_length", "for", "id_list", "in", "concept_ids", "]", ")", "\n", "\n", "", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "                    ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "# out_of_span = False", "\n", "if", "not", "(", "tok_start_position", ">=", "doc_start", "and", "\n", "tok_end_position", "<=", "doc_end", ")", ":", "\n", "                        ", "continue", "\n", "# out_of_span = True", "\n", "# if out_of_span:", "\n", "#     start_position = 0", "\n", "#     end_position = 0", "\n", "# else:", "\n", "#     doc_offset = len(query_tokens) + 2", "\n", "#     start_position = tok_start_position - doc_start + doc_offset", "\n", "#     end_position = tok_end_position - doc_start + doc_offset", "\n", "", "doc_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "start_position", "=", "tok_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "tok_end_position", "-", "doc_start", "+", "doc_offset", "\n", "\n", "", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "                    ", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "\n", "", "if", "example_index", "<", "3", ":", "\n", "                    ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"unique_id: %s\"", "%", "(", "unique_id", ")", ")", "\n", "logger", ".", "info", "(", "\"example_index: %s\"", "%", "(", "example_index", ")", ")", "\n", "logger", ".", "info", "(", "\"doc_span_index: %s\"", "%", "(", "doc_span_index", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"token_to_orig_map: %s\"", "%", "\" \"", ".", "join", "(", "[", "\n", "\"%d:%d\"", "%", "(", "x", ",", "y", ")", "\n", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_to_orig_map", ")", "\n", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"token_is_max_context: %s\"", "%", "\" \"", ".", "join", "(", "[", "\n", "\"%d:%s\"", "%", "(", "x", ",", "y", ")", "\n", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_is_max_context", ")", "\n", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"segment_ids: %s\"", "%", "\n", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"wordnet_concept_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "\"{}:{}\"", ".", "format", "(", "tidx", ",", "list", "(", "filter", "(", "lambda", "index", ":", "index", "!=", "0", ",", "x", ")", ")", ")", "for", "tidx", ",", "x", "in", "enumerate", "(", "wn_concept_ids", ")", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"nell_concept_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "\"{}:{}\"", ".", "format", "(", "tidx", ",", "list", "(", "filter", "(", "lambda", "index", ":", "index", "!=", "0", ",", "x", ")", ")", ")", "for", "tidx", ",", "x", "in", "enumerate", "(", "nell_concept_ids", ")", "]", ")", ")", "\n", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "                        ", "logger", ".", "info", "(", "\"impossible example\"", ")", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "                        ", "answer_text", "=", "\" \"", ".", "join", "(", "tokens", "[", "start_position", ":", "(", "end_position", "+", "\n", "1", ")", "]", ")", "\n", "logger", ".", "info", "(", "\"start_position: %d\"", "%", "(", "start_position", ")", ")", "\n", "logger", ".", "info", "(", "\"end_position: %d\"", "%", "(", "end_position", ")", ")", "\n", "logger", ".", "info", "(", "\"answer: %s\"", "%", "\n", "(", "tokenization", ".", "printable_text", "(", "answer_text", ")", ")", ")", "\n", "\n", "", "", "feature", "=", "InputFeatures", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "example_index", "=", "example_index", ",", "\n", "doc_span_index", "=", "doc_span_index", ",", "\n", "tokens", "=", "tokens", ",", "\n", "token_to_orig_map", "=", "token_to_orig_map", ",", "\n", "token_is_max_context", "=", "token_is_max_context", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "wn_concept_ids", "=", "wn_concept_ids", ",", "\n", "nell_concept_ids", "=", "nell_concept_ids", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "example", ".", "is_impossible", ")", "\n", "\n", "unique_id", "+=", "1", "\n", "\n", "yield", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.DataProcessor.__init__": [[548, 576], ["tokenization.FullTokenizer", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab_path", ",", "do_lower_case", ",", "max_seq_length", ",", "in_tokens", ",", "\n", "doc_stride", ",", "max_query_length", ")", ":", "\n", "        ", "self", ".", "_tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "vocab_path", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "_max_seq_length", "=", "max_seq_length", "\n", "self", ".", "_doc_stride", "=", "doc_stride", "\n", "self", ".", "_max_query_length", "=", "max_query_length", "\n", "self", ".", "_in_tokens", "=", "in_tokens", "\n", "\n", "self", ".", "vocab", "=", "self", ".", "_tokenizer", ".", "vocab", "\n", "self", ".", "vocab_size", "=", "len", "(", "self", ".", "vocab", ")", "\n", "self", ".", "pad_id", "=", "self", ".", "vocab", "[", "\"[PAD]\"", "]", "\n", "self", ".", "cls_id", "=", "self", ".", "vocab", "[", "\"[CLS]\"", "]", "\n", "self", ".", "sep_id", "=", "self", ".", "vocab", "[", "\"[SEP]\"", "]", "\n", "self", ".", "mask_id", "=", "self", ".", "vocab", "[", "\"[MASK]\"", "]", "\n", "\n", "self", ".", "current_train_example", "=", "-", "1", "\n", "self", ".", "num_train_examples", "=", "-", "1", "\n", "self", ".", "current_train_epoch", "=", "-", "1", "\n", "\n", "self", ".", "train_examples", "=", "None", "\n", "self", ".", "predict_examples", "=", "None", "\n", "self", ".", "num_examples", "=", "{", "'train'", ":", "-", "1", ",", "'predict'", ":", "-", "1", "}", "\n", "\n", "self", ".", "train_wn_max_concept_length", "=", "None", "\n", "self", ".", "predict_wn_max_concept_length", "=", "None", "\n", "self", ".", "train_nell_max_concept_length", "=", "None", "\n", "self", ".", "predict_nell_max_concept_length", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.DataProcessor.get_train_progress": [[577, 580], ["None"], "methods", ["None"], ["", "def", "get_train_progress", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets progress for training phase.\"\"\"", "\n", "return", "self", ".", "current_train_example", ",", "self", ".", "current_train_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.DataProcessor.get_examples": [[581, 590], ["squad_twomemory.read_squad_examples"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.read_squad_examples"], ["", "def", "get_examples", "(", "self", ",", "\n", "data_path", ",", "\n", "is_training", ",", "\n", "version_2_with_negative", "=", "False", ")", ":", "\n", "        ", "examples", "=", "read_squad_examples", "(", "\n", "input_file", "=", "data_path", ",", "\n", "is_training", "=", "is_training", ",", "\n", "version_2_with_negative", "=", "version_2_with_negative", ")", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.DataProcessor.get_num_examples": [[591, 596], ["ValueError"], "methods", ["None"], ["", "def", "get_num_examples", "(", "self", ",", "phase", ")", ":", "\n", "        ", "if", "phase", "not", "in", "[", "'train'", ",", "'predict'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unknown phase, which should be in ['train', 'predict'].\"", ")", "\n", "", "return", "self", ".", "num_examples", "[", "phase", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.DataProcessor.get_features": [[597, 607], ["squad_twomemory.Examples_To_Features_Converter", "Examples_To_Features_Converter."], "methods", ["None"], ["", "def", "get_features", "(", "self", ",", "examples", ",", "is_training", ",", "**", "concept_settings", ")", ":", "\n", "        ", "convert_examples_to_features", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", "\n", "features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "examples", ",", "\n", "tokenizer", "=", "self", ".", "_tokenizer", ",", "\n", "max_seq_length", "=", "self", ".", "_max_seq_length", ",", "\n", "doc_stride", "=", "self", ".", "_doc_stride", ",", "\n", "max_query_length", "=", "self", ".", "_max_query_length", ",", "\n", "is_training", "=", "is_training", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.DataProcessor.data_generator": [[608, 713], ["squad_twomemory.DataProcessor.get_examples", "len", "enumerate", "range", "squad_twomemory.DataProcessor.get_examples", "len", "ValueError", "len", "max", "len", "squad_twomemory.Examples_To_Features_Converter", "squad_twomemory.Examples_To_Features_Converter", "squad_twomemory.Examples_To_Features_Converter", "squad_twomemory.Examples_To_Features_Converter", "squad_twomemory.DataProcessor.data_generator.batch_reader"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_examples", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_examples", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.batch_reader"], ["", "def", "data_generator", "(", "self", ",", "\n", "data_path", ",", "\n", "batch_size", ",", "\n", "phase", "=", "'train'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "version_2_with_negative", "=", "False", ",", "\n", "epoch", "=", "1", ",", "\n", "**", "concept_settings", ")", ":", "\n", "        ", "if", "phase", "==", "'train'", ":", "\n", "            ", "self", ".", "train_examples", "=", "self", ".", "get_examples", "(", "\n", "data_path", ",", "\n", "is_training", "=", "True", ",", "\n", "version_2_with_negative", "=", "version_2_with_negative", ")", "\n", "examples", "=", "self", ".", "train_examples", "\n", "self", ".", "num_examples", "[", "'train'", "]", "=", "len", "(", "self", ".", "train_examples", ")", "\n", "", "elif", "phase", "==", "'predict'", ":", "\n", "            ", "self", ".", "predict_examples", "=", "self", ".", "get_examples", "(", "\n", "data_path", ",", "\n", "is_training", "=", "False", ",", "\n", "version_2_with_negative", "=", "version_2_with_negative", ")", "\n", "examples", "=", "self", ".", "predict_examples", "\n", "self", ".", "num_examples", "[", "'predict'", "]", "=", "len", "(", "self", ".", "predict_examples", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unknown phase, which should be in ['train', 'predict'].\"", ")", "\n", "\n", "", "def", "batch_reader", "(", "features", ",", "batch_size", ",", "in_tokens", ")", ":", "\n", "            ", "batch", ",", "total_token_num", ",", "max_len", "=", "[", "]", ",", "0", ",", "0", "\n", "for", "(", "index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "                ", "if", "phase", "==", "'train'", ":", "\n", "                    ", "self", ".", "current_train_example", "=", "index", "+", "1", "\n", "", "seq_len", "=", "len", "(", "feature", ".", "input_ids", ")", "\n", "labels", "=", "[", "feature", ".", "unique_id", "\n", "]", "if", "feature", ".", "start_position", "is", "None", "else", "[", "\n", "feature", ".", "start_position", ",", "feature", ".", "end_position", "\n", "]", "\n", "example", "=", "[", "\n", "# feature.input_ids, feature.segment_ids, range(seq_len), feature.wn_concept_ids, feature.nell_concept_ids", "\n", "feature", ".", "input_ids", ",", "feature", ".", "segment_ids", ",", "range", "(", "384", ")", ",", "feature", ".", "wn_concept_ids", ",", "feature", ".", "nell_concept_ids", "\n", "]", "+", "labels", "\n", "max_len", "=", "max", "(", "max_len", ",", "seq_len", ")", "\n", "\n", "#max_len = max(max_len, len(token_ids))", "\n", "if", "in_tokens", ":", "\n", "                    ", "to_append", "=", "(", "len", "(", "batch", ")", "+", "1", ")", "*", "max_len", "<=", "batch_size", "\n", "", "else", ":", "\n", "                    ", "to_append", "=", "len", "(", "batch", ")", "<", "batch_size", "\n", "\n", "", "if", "to_append", ":", "\n", "                    ", "batch", ".", "append", "(", "example", ")", "\n", "total_token_num", "+=", "seq_len", "\n", "", "else", ":", "\n", "                    ", "yield", "batch", ",", "total_token_num", "\n", "batch", ",", "total_token_num", ",", "max_len", "=", "[", "example", "\n", "]", ",", "seq_len", ",", "seq_len", "\n", "", "", "if", "len", "(", "batch", ")", ">", "0", ":", "\n", "                ", "yield", "batch", ",", "total_token_num", "\n", "\n", "", "", "if", "phase", "==", "'train'", ":", "\n", "            ", "self", ".", "train_wn_max_concept_length", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", ".", "max_wn_concept_length", "\n", "self", ".", "train_nell_max_concept_length", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", ".", "max_nell_concept_length", "\n", "", "else", ":", "\n", "            ", "self", ".", "predict_wn_max_concept_length", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", ".", "max_wn_concept_length", "\n", "self", ".", "predict_nell_max_concept_length", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", ".", "max_nell_concept_length", "\n", "\n", "", "def", "wrapper", "(", ")", ":", "\n", "            ", "for", "epoch_index", "in", "range", "(", "epoch", ")", ":", "\n", "                ", "if", "shuffle", ":", "\n", "                    ", "random", ".", "shuffle", "(", "examples", ")", "\n", "", "if", "phase", "==", "'train'", ":", "\n", "                    ", "self", ".", "current_train_epoch", "=", "epoch_index", "\n", "features", "=", "self", ".", "get_features", "(", "examples", ",", "is_training", "=", "True", ",", "**", "concept_settings", ")", "\n", "max_wn_concept_length", "=", "self", ".", "train_wn_max_concept_length", "\n", "max_nell_concept_length", "=", "self", ".", "train_nell_max_concept_length", "\n", "", "else", ":", "\n", "                    ", "features", "=", "self", ".", "get_features", "(", "examples", ",", "is_training", "=", "False", ",", "**", "concept_settings", ")", "\n", "max_wn_concept_length", "=", "self", ".", "predict_wn_max_concept_length", "\n", "max_nell_concept_length", "=", "self", ".", "predict_nell_max_concept_length", "\n", "\n", "", "all_dev_batches", "=", "[", "]", "\n", "for", "batch_data", ",", "total_token_num", "in", "batch_reader", "(", "\n", "features", ",", "batch_size", ",", "self", ".", "_in_tokens", ")", ":", "\n", "                    ", "batch_data", "=", "prepare_batch_data", "(", "\n", "batch_data", ",", "\n", "total_token_num", ",", "\n", "voc_size", "=", "-", "1", ",", "\n", "pad_id", "=", "self", ".", "pad_id", ",", "\n", "cls_id", "=", "self", ".", "cls_id", ",", "\n", "sep_id", "=", "self", ".", "sep_id", ",", "\n", "mask_id", "=", "-", "1", ",", "\n", "return_input_mask", "=", "True", ",", "\n", "return_max_len", "=", "False", ",", "\n", "return_num_token", "=", "False", ",", "\n", "max_wn_concept_length", "=", "max_wn_concept_length", ",", "\n", "max_nell_concept_length", "=", "max_nell_concept_length", ")", "\n", "if", "len", "(", "all_dev_batches", ")", "<", "dev_count", ":", "\n", "                        ", "all_dev_batches", ".", "append", "(", "batch_data", ")", "\n", "\n", "", "if", "len", "(", "all_dev_batches", ")", "==", "dev_count", ":", "\n", "                        ", "for", "batch", "in", "all_dev_batches", ":", "\n", "                            ", "yield", "batch", "\n", "", "all_dev_batches", "=", "[", "]", "\n", "\n", "", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.read_squad_examples": [[110, 191], ["open", "json.load", "ord", "squad_twomemory.read_squad_examples.is_whitespace"], "function", ["None"], ["", "", "def", "read_squad_examples", "(", "input_file", ",", "is_training", ",", "version_2_with_negative", "=", "False", ")", ":", "\n", "    ", "\"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "        ", "input_data", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "\n", "", "def", "is_whitespace", "(", "c", ")", ":", "\n", "        ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "examples", "=", "[", "]", "\n", "for", "entry", "in", "input_data", ":", "\n", "        ", "for", "paragraph", "in", "entry", "[", "\"paragraphs\"", "]", ":", "\n", "            ", "paragraph_text", "=", "paragraph", "[", "\"context\"", "]", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "                ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "                    ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "                    ", "if", "prev_is_whitespace", ":", "\n", "                        ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                        ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "", "for", "qa", "in", "paragraph", "[", "\"qas\"", "]", ":", "\n", "                ", "qas_id", "=", "qa", "[", "\"id\"", "]", "\n", "question_text", "=", "qa", "[", "\"question\"", "]", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "is_impossible", "=", "False", "\n", "if", "is_training", ":", "\n", "\n", "                    ", "if", "version_2_with_negative", ":", "\n", "                        ", "is_impossible", "=", "qa", "[", "\"is_impossible\"", "]", "\n", "", "if", "(", "len", "(", "qa", "[", "\"answers\"", "]", ")", "!=", "1", ")", "and", "(", "not", "is_impossible", ")", ":", "\n", "                        ", "raise", "ValueError", "(", "\n", "\"For training, each question should have exactly 1 answer.\"", "\n", ")", "\n", "", "if", "not", "is_impossible", ":", "\n", "                        ", "answer", "=", "qa", "[", "\"answers\"", "]", "[", "0", "]", "\n", "orig_answer_text", "=", "answer", "[", "\"text\"", "]", "\n", "answer_offset", "=", "answer", "[", "\"answer_start\"", "]", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "\n", "answer_length", "-", "1", "]", "\n", "# Only add answers where the text can be exactly recovered from the", "\n", "# document. If this CAN'T happen it's likely due to weird Unicode", "\n", "# stuff so we will just skip the example.", "\n", "#", "\n", "# Note that this means for training mode, every example is NOT", "\n", "# guaranteed to be preserved.", "\n", "actual_text", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "start_position", ":", "(", "\n", "end_position", "+", "1", ")", "]", ")", "\n", "cleaned_answer_text", "=", "\" \"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "if", "actual_text", ".", "find", "(", "cleaned_answer_text", ")", "==", "-", "1", ":", "\n", "                            ", "logger", ".", "info", "(", "\"Could not find answer: '%s' vs. '%s'\"", ",", "\n", "actual_text", ",", "cleaned_answer_text", ")", "\n", "continue", "\n", "", "", "else", ":", "\n", "                        ", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "orig_answer_text", "=", "\"\"", "\n", "\n", "", "", "example", "=", "SquadExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "\n", "orig_answer_text", "=", "orig_answer_text", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "is_impossible", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory._improve_answer_span": [[472, 507], ["range", "tokenizer.tokenize", "range"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "", "", "", "def", "_improve_answer_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_answer_text", ")", ":", "\n", "    ", "\"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"", "\n", "\n", "# The SQuAD annotations are character based. We first project them to", "\n", "# whitespace-tokenized words. But then after WordPiece tokenization, we can", "\n", "# often find a \"better match\". For example:", "\n", "#", "\n", "#   Question: What year was John Smith born?", "\n", "#   Context: The leader was John Smith (1895-1943).", "\n", "#   Answer: 1895", "\n", "#", "\n", "# The original whitespace-tokenized answer will be \"(1895-1943).\". However", "\n", "# after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match", "\n", "# the exact answer, 1895.", "\n", "#", "\n", "# However, this is not always possible. Consider the following:", "\n", "#", "\n", "#   Question: What country is the top exporter of electornics?", "\n", "#   Context: The Japanese electronics industry is the lagest in the world.", "\n", "#   Answer: Japan", "\n", "#", "\n", "# In this case, the annotator chose \"Japan\" as a character sub-span of", "\n", "# the word \"Japanese\". Since our WordPiece tokenizer does not split", "\n", "# \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare", "\n", "# in SQuAD, but does happen.", "\n", "tok_answer_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_answer_text", ")", ")", "\n", "\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "        ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "text_span", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_answer_text", ":", "\n", "                ", "return", "(", "new_start", ",", "new_end", ")", "\n", "\n", "", "", "", "return", "(", "input_start", ",", "input_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory._check_is_max_context": [[509, 545], ["enumerate", "min"], "function", ["None"], ["", "def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "    ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "        ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "            ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "            ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "\n", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "            ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.write_predictions": [[715, 924], ["logger.info", "logger.info", "logger.info", "collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "eval.squad_v1_official_evaluate.evaluate", "open", "example_index_to_features[].append", "enumerate", "sorted", "collections.namedtuple", "squad_twomemory._compute_softmax", "enumerate", "open", "writer.write", "open", "writer.write", "open", "writer.write", "json.load", "squad_twomemory._get_best_indexes", "squad_twomemory._get_best_indexes", "sorted.append", "nbest.append", "nbest.append", "len", "total_scores.append", "logger.info", "collections.OrderedDict", "nbest_json.append", "len", "open", "writer.write", "collections.namedtuple.", "len", "tok_text.strip.replace", "tok_text.strip.replace", "tok_text.strip.strip", "squad_twomemory.get_final_text", "collections.namedtuple.", "nbest.append", "collections.namedtuple.", "json.dumps", "json.dumps", "json.dumps", "sorted.append", "tok_text.strip.split", "collections.namedtuple.", "json.dumps", "len", "len", "feature.token_is_max_context.get", "collections.namedtuple."], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.evaluate", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._compute_softmax", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.get_final_text"], ["", "", "def", "write_predictions", "(", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "output_null_log_odds_file", ",", "\n", "version_2_with_negative", ",", "null_score_diff_threshold", ",", "\n", "verbose", ",", "predict_file", ",", "evaluation_result_file", ")", ":", "\n", "    ", "\"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"", "\n", "logger", ".", "info", "(", "\"Writing predictions to: %s\"", "%", "(", "output_prediction_file", ")", ")", "\n", "logger", ".", "info", "(", "\"Writing nbest to: %s\"", "%", "(", "output_nbest_file", ")", ")", "\n", "logger", ".", "info", "(", "\"Writing evaluation result to: %s\"", "%", "(", "evaluation_result_file", ")", ")", "\n", "\n", "# load ground truth file for evaluation and post-edit", "\n", "with", "open", "(", "predict_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "        ", "predict_json", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "\n", "", "example_index_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "        ", "example_index_to_features", "[", "feature", ".", "example_index", "]", ".", "append", "(", "feature", ")", "\n", "\n", "", "unique_id_to_result", "=", "{", "}", "\n", "for", "result", "in", "all_results", ":", "\n", "        ", "unique_id_to_result", "[", "result", ".", "unique_id", "]", "=", "result", "\n", "\n", "", "_PrelimPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"PrelimPrediction\"", ",", "[", "\n", "\"feature_index\"", ",", "\"start_index\"", ",", "\"end_index\"", ",", "\"start_logit\"", ",", "\n", "\"end_logit\"", "\n", "]", ")", "\n", "\n", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_nbest_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "scores_diff_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "all_examples", ")", ":", "\n", "        ", "features", "=", "example_index_to_features", "[", "example_index", "]", "\n", "\n", "prelim_predictions", "=", "[", "]", "\n", "# keep track of the minimum score of null start+end of position 0", "\n", "score_null", "=", "1000000", "# large and positive", "\n", "min_null_feature_index", "=", "0", "# the paragraph slice with min mull score", "\n", "null_start_logit", "=", "0", "# the start logit at the slice with min null score", "\n", "null_end_logit", "=", "0", "# the end logit at the slice with min null score", "\n", "for", "(", "feature_index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "            ", "result", "=", "unique_id_to_result", "[", "feature", ".", "unique_id", "]", "\n", "start_indexes", "=", "_get_best_indexes", "(", "result", ".", "start_logits", ",", "n_best_size", ")", "\n", "end_indexes", "=", "_get_best_indexes", "(", "result", ".", "end_logits", ",", "n_best_size", ")", "\n", "# if we could have irrelevant answers, get the min score of irrelevant", "\n", "if", "version_2_with_negative", ":", "\n", "                ", "feature_null_score", "=", "result", ".", "start_logits", "[", "0", "]", "+", "result", ".", "end_logits", "[", "\n", "0", "]", "\n", "if", "feature_null_score", "<", "score_null", ":", "\n", "                    ", "score_null", "=", "feature_null_score", "\n", "min_null_feature_index", "=", "feature_index", "\n", "null_start_logit", "=", "result", ".", "start_logits", "[", "0", "]", "\n", "null_end_logit", "=", "result", ".", "end_logits", "[", "0", "]", "\n", "", "", "for", "start_index", "in", "start_indexes", ":", "\n", "                ", "for", "end_index", "in", "end_indexes", ":", "\n", "# We could hypothetically create invalid predictions, e.g., predict", "\n", "# that the start of the span is in the question. We throw out all", "\n", "# invalid predictions.", "\n", "                    ", "if", "start_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "start_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                        ", "continue", "\n", "", "if", "not", "feature", ".", "token_is_max_context", ".", "get", "(", "start_index", ",", "False", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "                        ", "continue", "\n", "", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "if", "length", ">", "max_answer_length", ":", "\n", "                        ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "feature_index", ",", "\n", "start_index", "=", "start_index", ",", "\n", "end_index", "=", "end_index", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "start_index", "]", ",", "\n", "end_logit", "=", "result", ".", "end_logits", "[", "end_index", "]", ")", ")", "\n", "\n", "", "", "", "if", "version_2_with_negative", ":", "\n", "            ", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "min_null_feature_index", ",", "\n", "start_index", "=", "0", ",", "\n", "end_index", "=", "0", ",", "\n", "start_logit", "=", "null_start_logit", ",", "\n", "end_logit", "=", "null_end_logit", ")", ")", "\n", "", "prelim_predictions", "=", "sorted", "(", "\n", "prelim_predictions", ",", "\n", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_logit", "+", "x", ".", "end_logit", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "_NbestPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"NbestPrediction\"", ",", "[", "\"text\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "seen_predictions", "=", "{", "}", "\n", "nbest", "=", "[", "]", "\n", "for", "pred", "in", "prelim_predictions", ":", "\n", "            ", "if", "len", "(", "nbest", ")", ">=", "n_best_size", ":", "\n", "                ", "break", "\n", "", "feature", "=", "features", "[", "pred", ".", "feature_index", "]", "\n", "if", "pred", ".", "start_index", ">", "0", ":", "# this is a non-null prediction", "\n", "                ", "tok_tokens", "=", "feature", ".", "tokens", "[", "pred", ".", "start_index", ":", "(", "pred", ".", "end_index", "+", "1", "\n", ")", "]", "\n", "orig_doc_start", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "start_index", "]", "\n", "orig_doc_end", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "end_index", "]", "\n", "orig_tokens", "=", "example", ".", "doc_tokens", "[", "orig_doc_start", ":", "(", "orig_doc_end", "+", "\n", "1", ")", "]", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_tokens", ")", "\n", "\n", "# De-tokenize WordPieces that have been split off.", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\"##\"", ",", "\"\"", ")", "\n", "\n", "# Clean whitespace", "\n", "tok_text", "=", "tok_text", ".", "strip", "(", ")", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_text", ".", "split", "(", ")", ")", "\n", "orig_text", "=", "\" \"", ".", "join", "(", "orig_tokens", ")", "\n", "\n", "final_text", "=", "get_final_text", "(", "tok_text", ",", "orig_text", ",", "do_lower_case", ",", "\n", "verbose", ")", "\n", "if", "final_text", "in", "seen_predictions", ":", "\n", "                    ", "continue", "\n", "\n", "", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "", "else", ":", "\n", "                ", "final_text", "=", "\"\"", "\n", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "\n", "", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "final_text", ",", "\n", "start_logit", "=", "pred", ".", "start_logit", ",", "\n", "end_logit", "=", "pred", ".", "end_logit", ")", ")", "\n", "\n", "# if we didn't inlude the empty option in the n-best, inlcude it", "\n", "", "if", "version_2_with_negative", ":", "\n", "            ", "if", "\"\"", "not", "in", "seen_predictions", ":", "\n", "                ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "\"\"", ",", "\n", "start_logit", "=", "null_start_logit", ",", "\n", "end_logit", "=", "null_end_logit", ")", ")", "\n", "# In very rare edge cases we could have no valid predictions. So we", "\n", "# just create a nonce prediction in this case to avoid failure.", "\n", "", "", "if", "not", "nbest", ":", "\n", "            ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "\"empty\"", ",", "start_logit", "=", "0.0", ",", "end_logit", "=", "0.0", ")", ")", "\n", "\n", "", "assert", "len", "(", "nbest", ")", ">=", "1", "\n", "\n", "total_scores", "=", "[", "]", "\n", "best_non_null_entry", "=", "None", "\n", "for", "entry", "in", "nbest", ":", "\n", "            ", "total_scores", ".", "append", "(", "entry", ".", "start_logit", "+", "entry", ".", "end_logit", ")", "\n", "if", "not", "best_non_null_entry", ":", "\n", "                ", "if", "entry", ".", "text", ":", "\n", "                    ", "best_non_null_entry", "=", "entry", "\n", "# debug", "\n", "", "", "", "if", "best_non_null_entry", "is", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\"Emmm..., sth wrong\"", ")", "\n", "\n", "", "probs", "=", "_compute_softmax", "(", "total_scores", ")", "\n", "\n", "nbest_json", "=", "[", "]", "\n", "for", "(", "i", ",", "entry", ")", "in", "enumerate", "(", "nbest", ")", ":", "\n", "            ", "output", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "output", "[", "\"text\"", "]", "=", "entry", ".", "text", "\n", "output", "[", "\"probability\"", "]", "=", "probs", "[", "i", "]", "\n", "output", "[", "\"start_logit\"", "]", "=", "entry", ".", "start_logit", "\n", "output", "[", "\"end_logit\"", "]", "=", "entry", ".", "end_logit", "\n", "nbest_json", ".", "append", "(", "output", ")", "\n", "\n", "", "assert", "len", "(", "nbest_json", ")", ">=", "1", "\n", "\n", "if", "not", "version_2_with_negative", ":", "\n", "            ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "[", "0", "]", "[", "\"text\"", "]", "\n", "", "else", ":", "\n", "# predict \"\" iff the null score - the score of best non-null > threshold", "\n", "            ", "score_diff", "=", "score_null", "-", "best_non_null_entry", ".", "start_logit", "-", "(", "\n", "best_non_null_entry", ".", "end_logit", ")", "\n", "scores_diff_json", "[", "example", ".", "qas_id", "]", "=", "score_diff", "\n", "if", "score_diff", ">", "null_score_diff_threshold", ":", "\n", "                ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "\"\"", "\n", "", "else", ":", "\n", "                ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "best_non_null_entry", ".", "text", "\n", "\n", "", "", "all_nbest_json", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "\n", "\n", "", "with", "open", "(", "output_prediction_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_predictions", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "with", "open", "(", "output_nbest_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_nbest_json", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "if", "version_2_with_negative", ":", "\n", "        ", "with", "open", "(", "output_null_log_odds_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "scores_diff_json", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "", "eval_result", "=", "evaluate", "(", "predict_json", ",", "all_predictions", ")", "\n", "\n", "with", "open", "(", "evaluation_result_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "eval_result", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "return", "eval_result", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory.get_final_text": [[925, 1018], ["tokenization.BasicTokenizer", "tok_text.find", "squad_twomemory.get_final_text._strip_spaces"], "function", ["None"], ["", "def", "get_final_text", "(", "pred_text", ",", "orig_text", ",", "do_lower_case", ",", "verbose", ")", ":", "\n", "    ", "\"\"\"Project the tokenized prediction back to the original text.\"\"\"", "\n", "\n", "# When we created the data, we kept track of the alignment between original", "\n", "# (whitespace tokenized) tokens and our WordPiece tokenized tokens. So", "\n", "# now `orig_text` contains the span of our original text corresponding to the", "\n", "# span that we predicted.", "\n", "#", "\n", "# However, `orig_text` may contain extra characters that we don't want in", "\n", "# our prediction.", "\n", "#", "\n", "# For example, let's say:", "\n", "#   pred_text = steve smith", "\n", "#   orig_text = Steve Smith's", "\n", "#", "\n", "# We don't want to return `orig_text` because it contains the extra \"'s\".", "\n", "#", "\n", "# We don't want to return `pred_text` because it's already been normalized", "\n", "# (the SQuAD eval script also does punctuation stripping/lower casing but", "\n", "# our tokenizer does additional normalization like stripping accent", "\n", "# characters).", "\n", "#", "\n", "# What we really want to return is \"Steve Smith\".", "\n", "#", "\n", "# Therefore, we have to apply a semi-complicated alignment heruistic between", "\n", "# `pred_text` and `orig_text` to get a character-to-charcter alignment. This", "\n", "# can fail in certain cases in which case we just return `orig_text`.", "\n", "\n", "def", "_strip_spaces", "(", "text", ")", ":", "\n", "        ", "ns_chars", "=", "[", "]", "\n", "ns_to_s_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "i", ",", "c", ")", "in", "enumerate", "(", "text", ")", ":", "\n", "            ", "if", "c", "==", "\" \"", ":", "\n", "                ", "continue", "\n", "", "ns_to_s_map", "[", "len", "(", "ns_chars", ")", "]", "=", "i", "\n", "ns_chars", ".", "append", "(", "c", ")", "\n", "", "ns_text", "=", "\"\"", ".", "join", "(", "ns_chars", ")", "\n", "return", "(", "ns_text", ",", "ns_to_s_map", ")", "\n", "\n", "# We first tokenize `orig_text`, strip whitespace from the result", "\n", "# and `pred_text`, and check if they are the same length. If they are", "\n", "# NOT the same length, the heuristic has failed. If they are the same", "\n", "# length, we assume the characters are one-to-one aligned.", "\n", "", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_text", ")", ")", "\n", "\n", "start_position", "=", "tok_text", ".", "find", "(", "pred_text", ")", "\n", "if", "start_position", "==", "-", "1", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Unable to find text: '%s' in '%s'\"", "%", "(", "pred_text", ",", "orig_text", ")", ")", "\n", "", "return", "orig_text", "\n", "", "end_position", "=", "start_position", "+", "len", "(", "pred_text", ")", "-", "1", "\n", "\n", "(", "orig_ns_text", ",", "orig_ns_to_s_map", ")", "=", "_strip_spaces", "(", "orig_text", ")", "\n", "(", "tok_ns_text", ",", "tok_ns_to_s_map", ")", "=", "_strip_spaces", "(", "tok_text", ")", "\n", "\n", "if", "len", "(", "orig_ns_text", ")", "!=", "len", "(", "tok_ns_text", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Length not equal after stripping spaces: '%s' vs '%s'\"", ",", "\n", "orig_ns_text", ",", "tok_ns_text", ")", "\n", "", "return", "orig_text", "\n", "\n", "# We then project the characters in `pred_text` back to `orig_text` using", "\n", "# the character-to-character alignment.", "\n", "", "tok_s_to_ns_map", "=", "{", "}", "\n", "for", "(", "i", ",", "tok_index", ")", "in", "six", ".", "iteritems", "(", "tok_ns_to_s_map", ")", ":", "\n", "        ", "tok_s_to_ns_map", "[", "tok_index", "]", "=", "i", "\n", "\n", "", "orig_start_position", "=", "None", "\n", "if", "start_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_start_position", "=", "tok_s_to_ns_map", "[", "start_position", "]", "\n", "if", "ns_start_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_start_position", "=", "orig_ns_to_s_map", "[", "ns_start_position", "]", "\n", "\n", "", "", "if", "orig_start_position", "is", "None", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Couldn't map start position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "orig_end_position", "=", "None", "\n", "if", "end_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_end_position", "=", "tok_s_to_ns_map", "[", "end_position", "]", "\n", "if", "ns_end_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_end_position", "=", "orig_ns_to_s_map", "[", "ns_end_position", "]", "\n", "\n", "", "", "if", "orig_end_position", "is", "None", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Couldn't map end position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "output_text", "=", "orig_text", "[", "orig_start_position", ":", "(", "orig_end_position", "+", "1", ")", "]", "\n", "return", "output_text", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory._get_best_indexes": [[1020, 1031], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "_get_best_indexes", "(", "logits", ",", "n_best_size", ")", ":", "\n", "    ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "\n", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "        ", "if", "i", ">=", "n_best_size", ":", "\n", "            ", "break", "\n", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad_twomemory._compute_softmax": [[1033, 1054], ["math.exp", "exp_scores.append", "probs.append"], "function", ["None"], ["", "def", "_compute_softmax", "(", "scores", ")", ":", "\n", "    ", "\"\"\"Compute softmax probability over raw logits.\"\"\"", "\n", "if", "not", "scores", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "", "max_score", "=", "None", "\n", "for", "score", "in", "scores", ":", "\n", "        ", "if", "max_score", "is", "None", "or", "score", ">", "max_score", ":", "\n", "            ", "max_score", "=", "score", "\n", "\n", "", "", "exp_scores", "=", "[", "]", "\n", "total_sum", "=", "0.0", "\n", "for", "score", "in", "scores", ":", "\n", "        ", "x", "=", "math", ".", "exp", "(", "score", "-", "max_score", ")", "\n", "exp_scores", ".", "append", "(", "x", ")", "\n", "total_sum", "+=", "x", "\n", "\n", "", "probs", "=", "[", "]", "\n", "for", "score", "in", "exp_scores", ":", "\n", "        ", "probs", ".", "append", "(", "score", "/", "total_sum", ")", "\n", "", "return", "probs", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.ReCoRDExample.__init__": [[42, 57], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "qas_id", ",", "\n", "question_text", ",", "\n", "doc_tokens", ",", "\n", "orig_answer_text", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "False", ")", ":", "\n", "        ", "self", ".", "qas_id", "=", "qas_id", "\n", "self", ".", "question_text", "=", "question_text", "\n", "self", ".", "doc_tokens", "=", "doc_tokens", "\n", "self", ".", "orig_answer_text", "=", "orig_answer_text", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.ReCoRDExample.__str__": [[58, 60], ["record.ReCoRDExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.SQuADExample.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.ReCoRDExample.__repr__": [[61, 74], ["tokenization.printable_text", "tokenization.printable_text"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"\"", "\n", "s", "+=", "\"qas_id: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "self", ".", "qas_id", ")", ")", "\n", "s", "+=", "\", question_text: %s\"", "%", "(", "\n", "tokenization", ".", "printable_text", "(", "self", ".", "question_text", ")", ")", "\n", "s", "+=", "\", doc_tokens: [%s]\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "doc_tokens", ")", ")", "\n", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", start_position: %d\"", "%", "(", "self", ".", "start_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", end_position: %d\"", "%", "(", "self", ".", "end_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", is_impossible: %r\"", "%", "(", "self", ".", "is_impossible", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.InputFeatures.__init__": [[79, 106], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "unique_id", ",", "\n", "example_index", ",", "\n", "doc_span_index", ",", "\n", "tokens", ",", "\n", "token_to_orig_map", ",", "\n", "token_is_max_context", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "segment_ids", ",", "\n", "concept_ids", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "None", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "example_index", "=", "example_index", "\n", "self", ".", "doc_span_index", "=", "doc_span_index", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "token_to_orig_map", "=", "token_to_orig_map", "\n", "self", ".", "token_is_max_context", "=", "token_is_max_context", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "self", ".", "concept_ids", "=", "concept_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.Examples_To_Features_Converter.__init__": [[190, 222], ["os.path.exists", "pickle.load", "open", "os.path.exists", "pickle.load", "max", "os.path.exists", "pickle.load", "max", "open", "open", "len", "max", "record.Examples_To_Features_Converter.synsets_info.values", "record.Examples_To_Features_Converter.nell_retrieve_info.items", "len", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "**", "concept_settings", ")", ":", "\n", "        ", "self", ".", "concept_settings", "=", "concept_settings", "\n", "\n", "# load necessary data files for mapping to related concepts", "\n", "# 1. mapping from subword-level tokenization to word-level tokenization", "\n", "tokenization_filepath", "=", "self", ".", "concept_settings", "[", "'tokenization_path'", "]", "\n", "assert", "os", ".", "path", ".", "exists", "(", "tokenization_filepath", ")", "\n", "self", ".", "all_tokenization_info", "=", "{", "}", "\n", "for", "item", "in", "pickle", ".", "load", "(", "open", "(", "tokenization_filepath", ",", "'rb'", ")", ")", ":", "\n", "            ", "self", ".", "all_tokenization_info", "[", "item", "[", "'id'", "]", "]", "=", "item", "\n", "\n", "# 2. mapping from concept name to concept id (currently only support one KB)", "\n", "", "self", ".", "concept2id", "=", "self", ".", "concept_settings", "[", "'concept2id'", "]", "\n", "\n", "# 3. retrieved related wordnet concepts (if use_wordnet)", "\n", "if", "concept_settings", "[", "'use_wordnet'", "]", ":", "\n", "            ", "assert", "not", "self", ".", "concept_settings", "[", "'use_nell'", "]", "\n", "retrieved_synset_filepath", "=", "self", ".", "concept_settings", "[", "'retrieved_synset_path'", "]", "\n", "assert", "os", ".", "path", ".", "exists", "(", "retrieved_synset_filepath", ")", "\n", "self", ".", "synsets_info", "=", "pickle", ".", "load", "(", "open", "(", "retrieved_synset_filepath", ",", "'rb'", ")", ")", "# token to sysnet names", "\n", "self", ".", "max_concept_length", "=", "max", "(", "[", "len", "(", "synsets", ")", "for", "synsets", "in", "self", ".", "synsets_info", ".", "values", "(", ")", "]", ")", "\n", "\n", "# 4. retrieved related nell concepts (if use_nell)", "\n", "", "if", "concept_settings", "[", "'use_nell'", "]", ":", "\n", "            ", "assert", "not", "self", ".", "concept_settings", "[", "'use_wordnet'", "]", "\n", "retrieved_nell_concept_filepath", "=", "self", ".", "concept_settings", "[", "'retrieved_nell_concept_path'", "]", "\n", "assert", "os", ".", "path", ".", "exists", "(", "retrieved_nell_concept_filepath", ")", "\n", "self", ".", "nell_retrieve_info", "=", "{", "}", "\n", "for", "item", "in", "pickle", ".", "load", "(", "open", "(", "retrieved_nell_concept_filepath", ",", "'rb'", ")", ")", ":", "\n", "                ", "self", ".", "nell_retrieve_info", "[", "item", "[", "'id'", "]", "]", "=", "item", "\n", "", "self", ".", "max_concept_length", "=", "max", "(", "[", "max", "(", "[", "len", "(", "entity_info", "[", "'retrieved_concepts'", "]", ")", "for", "entity_info", "in", "item", "[", "'query_entities'", "]", "+", "item", "[", "'document_entities'", "]", "]", ")", "\n", "for", "qid", ",", "item", "in", "self", ".", "nell_retrieve_info", ".", "items", "(", ")", "if", "len", "(", "item", "[", "'query_entities'", "]", "+", "item", "[", "'document_entities'", "]", ")", ">", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.Examples_To_Features_Converter._lookup_wordnet_concept_ids": [[224, 235], ["range", "len", "tokenizer.basic_tokenizer._run_strip_accents", "concept_ids.append", "concept_ids.append", "original_token.lower"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_strip_accents"], ["", "", "def", "_lookup_wordnet_concept_ids", "(", "self", ",", "sub_tokens", ",", "sub_to_ori_index", ",", "tokens", ",", "tolower", ",", "tokenizer", ")", ":", "\n", "        ", "concept_ids", "=", "[", "]", "\n", "for", "index", "in", "range", "(", "len", "(", "sub_tokens", ")", ")", ":", "\n", "            ", "original_token", "=", "tokens", "[", "sub_to_ori_index", "[", "index", "]", "]", "\n", "# if tokens are in upper case, we must lower it for retrieving", "\n", "retrieve_token", "=", "tokenizer", ".", "basic_tokenizer", ".", "_run_strip_accents", "(", "original_token", ".", "lower", "(", ")", ")", "if", "tolower", "else", "original_token", "\n", "if", "retrieve_token", "in", "self", ".", "synsets_info", ":", "\n", "                ", "concept_ids", ".", "append", "(", "[", "self", ".", "concept2id", "[", "synset_name", "]", "for", "synset_name", "in", "self", ".", "synsets_info", "[", "retrieve_token", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "concept_ids", ".", "append", "(", "[", "]", ")", "\n", "", "", "return", "concept_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.Examples_To_Features_Converter._lookup_nell_concept_ids": [[236, 245], ["range", "range", "len", "list", "range", "set", "range", "len", "len"], "methods", ["None"], ["", "def", "_lookup_nell_concept_ids", "(", "self", ",", "sub_tokens", ",", "sub_to_ori_index", ",", "tokens", ",", "nell_info", ")", ":", "\n", "        ", "original_concept_ids", "=", "[", "[", "]", "for", "_", "in", "range", "(", "len", "(", "tokens", ")", ")", "]", "\n", "for", "entity_info", "in", "nell_info", ":", "\n", "            ", "for", "pos", "in", "range", "(", "entity_info", "[", "'token_start'", "]", ",", "entity_info", "[", "'token_end'", "]", "+", "1", ")", ":", "\n", "                ", "original_concept_ids", "[", "pos", "]", "+=", "[", "self", ".", "concept2id", "[", "category_name", "]", "for", "category_name", "in", "entity_info", "[", "'retrieved_concepts'", "]", "]", "\n", "", "", "for", "pos", "in", "range", "(", "len", "(", "original_concept_ids", ")", ")", ":", "\n", "            ", "original_concept_ids", "[", "pos", "]", "=", "list", "(", "set", "(", "original_concept_ids", "[", "pos", "]", ")", ")", "\n", "", "concept_ids", "=", "[", "original_concept_ids", "[", "sub_to_ori_index", "[", "index", "]", "]", "for", "index", "in", "range", "(", "len", "(", "sub_tokens", ")", ")", "]", "\n", "return", "concept_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.Examples_To_Features_Converter.__call__": [[246, 457], ["enumerate", "tokenizer.tokenize", "enumerate", "collections.namedtuple", "enumerate", "record.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "record.Examples_To_Features_Converter._lookup_nell_concept_ids", "len", "orig_to_tok_index.append", "tokenizer.tokenize", "record.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "record.Examples_To_Features_Converter._lookup_nell_concept_ids", "record._improve_answer_span", "len", "doc_spans.append", "min", "tokens.append", "segment_ids.append", "concept_ids.append", "zip", "tokens.append", "segment_ids.append", "concept_ids.append", "range", "tokens.append", "segment_ids.append", "concept_ids.append", "tokenizer.convert_tokens_to_ids", "range", "all", "record.InputFeatures", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "len", "len", "collections.namedtuple.", "len", "tokens.append", "segment_ids.append", "concept_ids.append", "record._check_is_max_context", "tokens.append", "segment_ids.append", "concept_ids.append", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "len", "len", "len", "len", "tokenization.printable_text", "tokenization.printable_text", "str", "str", "str", "six.iteritems", "six.iteritems", "list", "enumerate", "filter"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_nell_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_nell_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._improve_answer_span", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._check_is_max_context", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["", "def", "__call__", "(", "self", ",", "\n", "examples", ",", "\n", "tokenizer", ",", "\n", "max_seq_length", ",", "\n", "doc_stride", ",", "\n", "max_query_length", ",", "\n", "is_training", ")", ":", "\n", "        ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "unique_id", "=", "1000000000", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "            ", "tokenization_info", "=", "self", ".", "all_tokenization_info", "[", "example", ".", "qas_id", "]", "\n", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "# check online subword tokenization result is the same as offline result", "\n", "assert", "query_tokens", "==", "tokenization_info", "[", "'query_subtokens'", "]", "\n", "if", "self", ".", "concept_settings", "[", "'use_wordnet'", "]", ":", "\n", "                ", "query_concepts", "=", "self", ".", "_lookup_wordnet_concept_ids", "(", "query_tokens", ",", "tokenization_info", "[", "'query_sub_to_ori_index'", "]", ",", "\n", "tokenization_info", "[", "'query_tokens'", "]", ",", "\n", "tolower", "=", "tokenizer", ".", "basic_tokenizer", ".", "do_lower_case", "==", "False", ",", "tokenizer", "=", "tokenizer", ")", "# if tolower is True, tokenizer must be given", "\n", "\n", "", "if", "self", ".", "concept_settings", "[", "'use_nell'", "]", ":", "\n", "                ", "query_concepts", "=", "self", ".", "_lookup_nell_concept_ids", "(", "query_tokens", ",", "tokenization_info", "[", "'query_sub_to_ori_index'", "]", ",", "\n", "tokenization_info", "[", "'query_tokens'", "]", ",", "self", ".", "nell_retrieve_info", "[", "example", ".", "qas_id", "]", "[", "'query_entities'", "]", ")", "\n", "\n", "", "if", "len", "(", "query_tokens", ")", ">", "max_query_length", ":", "\n", "                ", "query_tokens", "=", "query_tokens", "[", "0", ":", "max_query_length", "]", "\n", "query_concepts", "=", "query_concepts", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "tok_to_orig_index", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "all_doc_tokens", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "doc_tokens", ")", ":", "\n", "                ", "orig_to_tok_index", ".", "append", "(", "len", "(", "all_doc_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "for", "sub_token", "in", "sub_tokens", ":", "\n", "                    ", "tok_to_orig_index", ".", "append", "(", "i", ")", "\n", "all_doc_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "assert", "all_doc_tokens", "==", "tokenization_info", "[", "'document_subtokens'", "]", "\n", "if", "self", ".", "concept_settings", "[", "'use_wordnet'", "]", ":", "\n", "                ", "doc_concepts", "=", "self", ".", "_lookup_wordnet_concept_ids", "(", "all_doc_tokens", ",", "tokenization_info", "[", "'document_sub_to_ori_index'", "]", ",", "\n", "tokenization_info", "[", "'document_tokens'", "]", ",", "\n", "tolower", "=", "tokenizer", ".", "basic_tokenizer", ".", "do_lower_case", "==", "False", ",", "tokenizer", "=", "tokenizer", ")", "# if tolower is True, tokenizer must be given", "\n", "\n", "", "if", "self", ".", "concept_settings", "[", "'use_nell'", "]", ":", "\n", "                ", "doc_concepts", "=", "self", ".", "_lookup_nell_concept_ids", "(", "all_doc_tokens", ",", "tokenization_info", "[", "'document_sub_to_ori_index'", "]", ",", "\n", "tokenization_info", "[", "'document_tokens'", "]", ",", "self", ".", "nell_retrieve_info", "[", "example", ".", "qas_id", "]", "[", "'document_entities'", "]", ")", "\n", "\n", "", "tok_start_position", "=", "None", "\n", "tok_end_position", "=", "None", "\n", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "                ", "tok_start_position", "=", "-", "1", "\n", "tok_end_position", "=", "-", "1", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "                ", "tok_start_position", "=", "orig_to_tok_index", "[", "example", ".", "start_position", "]", "\n", "if", "example", ".", "end_position", "<", "len", "(", "example", ".", "doc_tokens", ")", "-", "1", ":", "\n", "                    ", "tok_end_position", "=", "orig_to_tok_index", "[", "example", ".", "end_position", "+", "\n", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "                    ", "tok_end_position", "=", "len", "(", "all_doc_tokens", ")", "-", "1", "\n", "", "(", "tok_start_position", ",", "tok_end_position", ")", "=", "_improve_answer_span", "(", "\n", "all_doc_tokens", ",", "tok_start_position", ",", "tok_end_position", ",", "tokenizer", ",", "\n", "example", ".", "orig_answer_text", ")", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "", "max_tokens_for_doc", "=", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "# We can have documents that are longer than the maximum sequence length.", "\n", "# To deal with this we do a sliding window approach, where we take chunks", "\n", "# of the up to our max length with a stride of `doc_stride`.", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "                ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "                    ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "                    ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "                ", "tokens", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "token_is_max_context", "=", "{", "}", "\n", "segment_ids", "=", "[", "]", "\n", "concept_ids", "=", "[", "]", "\n", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "concept_ids", ".", "append", "(", "[", "]", ")", "\n", "for", "token", ",", "query_concept", "in", "zip", "(", "query_tokens", ",", "query_concepts", ")", ":", "\n", "                    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "concept_ids", ".", "append", "(", "query_concept", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "concept_ids", ".", "append", "(", "[", "]", ")", "\n", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "                    ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "tok_to_orig_index", "[", "split_token_index", "]", "\n", "\n", "is_max_context", "=", "_check_is_max_context", "(", "doc_spans", ",", "doc_span_index", ",", "\n", "split_token_index", ")", "\n", "token_is_max_context", "[", "len", "(", "tokens", ")", "]", "=", "is_max_context", "\n", "tokens", ".", "append", "(", "all_doc_tokens", "[", "split_token_index", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "concept_ids", ".", "append", "(", "doc_concepts", "[", "split_token_index", "]", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "concept_ids", ".", "append", "(", "[", "]", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "#while len(input_ids) < max_seq_length:", "\n", "#  input_ids.append(0)", "\n", "#  input_mask.append(0)", "\n", "#  segment_ids.append(0)", "\n", "\n", "#assert len(input_ids) == max_seq_length", "\n", "#assert len(input_mask) == max_seq_length", "\n", "#assert len(segment_ids) == max_seq_length", "\n", "\n", "for", "cindex", "in", "range", "(", "len", "(", "concept_ids", ")", ")", ":", "\n", "                    ", "concept_ids", "[", "cindex", "]", "=", "concept_ids", "[", "cindex", "]", "+", "[", "0", "]", "*", "(", "self", ".", "max_concept_length", "-", "len", "(", "concept_ids", "[", "cindex", "]", ")", ")", "\n", "concept_ids", "[", "cindex", "]", "=", "concept_ids", "[", "cindex", "]", "[", ":", "self", ".", "max_concept_length", "]", "\n", "", "assert", "all", "(", "[", "len", "(", "id_list", ")", "==", "self", ".", "max_concept_length", "for", "id_list", "in", "concept_ids", "]", ")", "\n", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "                    ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "# out_of_span = False", "\n", "if", "not", "(", "tok_start_position", ">=", "doc_start", "and", "\n", "tok_end_position", "<=", "doc_end", ")", ":", "\n", "                        ", "continue", "\n", "# out_of_span = True", "\n", "# if out_of_span:", "\n", "#     start_position = 0", "\n", "#     end_position = 0", "\n", "# else:", "\n", "#     doc_offset = len(query_tokens) + 2", "\n", "#     start_position = tok_start_position - doc_start + doc_offset", "\n", "#     end_position = tok_end_position - doc_start + doc_offset", "\n", "", "doc_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "start_position", "=", "tok_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "tok_end_position", "-", "doc_start", "+", "doc_offset", "\n", "\n", "", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "                    ", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "\n", "", "if", "example_index", "<", "3", ":", "\n", "                    ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"unique_id: %s\"", "%", "(", "unique_id", ")", ")", "\n", "logger", ".", "info", "(", "\"example_index: %s\"", "%", "(", "example_index", ")", ")", "\n", "logger", ".", "info", "(", "\"doc_span_index: %s\"", "%", "(", "doc_span_index", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"token_to_orig_map: %s\"", "%", "\" \"", ".", "join", "(", "[", "\n", "\"%d:%d\"", "%", "(", "x", ",", "y", ")", "\n", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_to_orig_map", ")", "\n", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"token_is_max_context: %s\"", "%", "\" \"", ".", "join", "(", "[", "\n", "\"%d:%s\"", "%", "(", "x", ",", "y", ")", "\n", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_is_max_context", ")", "\n", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"segment_ids: %s\"", "%", "\n", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"concept_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "\"{}:{}\"", ".", "format", "(", "tidx", ",", "list", "(", "filter", "(", "lambda", "index", ":", "index", "!=", "0", ",", "x", ")", ")", ")", "for", "tidx", ",", "x", "in", "enumerate", "(", "concept_ids", ")", "]", ")", ")", "\n", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "                        ", "logger", ".", "info", "(", "\"impossible example\"", ")", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "                        ", "answer_text", "=", "\" \"", ".", "join", "(", "tokens", "[", "start_position", ":", "(", "end_position", "+", "\n", "1", ")", "]", ")", "\n", "logger", ".", "info", "(", "\"start_position: %d\"", "%", "(", "start_position", ")", ")", "\n", "logger", ".", "info", "(", "\"end_position: %d\"", "%", "(", "end_position", ")", ")", "\n", "logger", ".", "info", "(", "\"answer: %s\"", "%", "\n", "(", "tokenization", ".", "printable_text", "(", "answer_text", ")", ")", ")", "\n", "\n", "", "", "feature", "=", "InputFeatures", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "example_index", "=", "example_index", ",", "\n", "doc_span_index", "=", "doc_span_index", ",", "\n", "tokens", "=", "tokens", ",", "\n", "token_to_orig_map", "=", "token_to_orig_map", ",", "\n", "token_is_max_context", "=", "token_is_max_context", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "concept_ids", "=", "concept_ids", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "example", ".", "is_impossible", ")", "\n", "\n", "unique_id", "+=", "1", "\n", "\n", "yield", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.DataProcessor.__init__": [[535, 561], ["tokenization.FullTokenizer", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab_path", ",", "do_lower_case", ",", "max_seq_length", ",", "in_tokens", ",", "\n", "doc_stride", ",", "max_query_length", ")", ":", "\n", "        ", "self", ".", "_tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "vocab_path", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "_max_seq_length", "=", "max_seq_length", "\n", "self", ".", "_doc_stride", "=", "doc_stride", "\n", "self", ".", "_max_query_length", "=", "max_query_length", "\n", "self", ".", "_in_tokens", "=", "in_tokens", "\n", "\n", "self", ".", "vocab", "=", "self", ".", "_tokenizer", ".", "vocab", "\n", "self", ".", "vocab_size", "=", "len", "(", "self", ".", "vocab", ")", "\n", "self", ".", "pad_id", "=", "self", ".", "vocab", "[", "\"[PAD]\"", "]", "\n", "self", ".", "cls_id", "=", "self", ".", "vocab", "[", "\"[CLS]\"", "]", "\n", "self", ".", "sep_id", "=", "self", ".", "vocab", "[", "\"[SEP]\"", "]", "\n", "self", ".", "mask_id", "=", "self", ".", "vocab", "[", "\"[MASK]\"", "]", "\n", "\n", "self", ".", "current_train_example", "=", "-", "1", "\n", "self", ".", "num_train_examples", "=", "-", "1", "\n", "self", ".", "current_train_epoch", "=", "-", "1", "\n", "\n", "self", ".", "train_examples", "=", "None", "\n", "self", ".", "predict_examples", "=", "None", "\n", "self", ".", "num_examples", "=", "{", "'train'", ":", "-", "1", ",", "'predict'", ":", "-", "1", "}", "\n", "\n", "self", ".", "train_max_concept_length", "=", "None", "\n", "self", ".", "predict_max_concept_length", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.DataProcessor.get_train_progress": [[562, 565], ["None"], "methods", ["None"], ["", "def", "get_train_progress", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets progress for training phase.\"\"\"", "\n", "return", "self", ".", "current_train_example", ",", "self", ".", "current_train_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.DataProcessor.get_examples": [[566, 575], ["record.read_record_examples"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization.read_record_examples"], ["", "def", "get_examples", "(", "self", ",", "\n", "data_path", ",", "\n", "is_training", ",", "\n", "version_2_with_negative", "=", "False", ")", ":", "\n", "        ", "examples", "=", "read_record_examples", "(", "\n", "input_file", "=", "data_path", ",", "\n", "is_training", "=", "is_training", ",", "\n", "version_2_with_negative", "=", "version_2_with_negative", ")", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.DataProcessor.get_num_examples": [[576, 581], ["ValueError"], "methods", ["None"], ["", "def", "get_num_examples", "(", "self", ",", "phase", ")", ":", "\n", "        ", "if", "phase", "not", "in", "[", "'train'", ",", "'predict'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unknown phase, which should be in ['train', 'predict'].\"", ")", "\n", "", "return", "self", ".", "num_examples", "[", "phase", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.DataProcessor.get_features": [[582, 592], ["record.Examples_To_Features_Converter", "Examples_To_Features_Converter."], "methods", ["None"], ["", "def", "get_features", "(", "self", ",", "examples", ",", "is_training", ",", "**", "concept_settings", ")", ":", "\n", "        ", "convert_examples_to_features", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", "\n", "features", "=", "convert_examples_to_features", "(", "\n", "examples", "=", "examples", ",", "\n", "tokenizer", "=", "self", ".", "_tokenizer", ",", "\n", "max_seq_length", "=", "self", ".", "_max_seq_length", ",", "\n", "doc_stride", "=", "self", ".", "_doc_stride", ",", "\n", "max_query_length", "=", "self", ".", "_max_query_length", ",", "\n", "is_training", "=", "is_training", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.DataProcessor.data_generator": [[593, 693], ["record.DataProcessor.get_examples", "len", "enumerate", "range", "record.DataProcessor.get_examples", "len", "ValueError", "len", "max", "len", "record.Examples_To_Features_Converter", "record.Examples_To_Features_Converter", "record.DataProcessor.data_generator.batch_reader"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_examples", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_examples", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.batch_reader"], ["", "def", "data_generator", "(", "self", ",", "\n", "data_path", ",", "\n", "batch_size", ",", "\n", "phase", "=", "'train'", ",", "\n", "shuffle", "=", "False", ",", "\n", "dev_count", "=", "1", ",", "\n", "version_2_with_negative", "=", "False", ",", "\n", "epoch", "=", "1", ",", "\n", "**", "concept_settings", ")", ":", "\n", "        ", "if", "phase", "==", "'train'", ":", "\n", "            ", "self", ".", "train_examples", "=", "self", ".", "get_examples", "(", "\n", "data_path", ",", "\n", "is_training", "=", "True", ",", "\n", "version_2_with_negative", "=", "version_2_with_negative", ")", "\n", "examples", "=", "self", ".", "train_examples", "\n", "self", ".", "num_examples", "[", "'train'", "]", "=", "len", "(", "self", ".", "train_examples", ")", "\n", "", "elif", "phase", "==", "'predict'", ":", "\n", "            ", "self", ".", "predict_examples", "=", "self", ".", "get_examples", "(", "\n", "data_path", ",", "\n", "is_training", "=", "False", ",", "\n", "version_2_with_negative", "=", "version_2_with_negative", ")", "\n", "examples", "=", "self", ".", "predict_examples", "\n", "self", ".", "num_examples", "[", "'predict'", "]", "=", "len", "(", "self", ".", "predict_examples", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unknown phase, which should be in ['train', 'predict'].\"", ")", "\n", "\n", "", "def", "batch_reader", "(", "features", ",", "batch_size", ",", "in_tokens", ")", ":", "\n", "            ", "batch", ",", "total_token_num", ",", "max_len", "=", "[", "]", ",", "0", ",", "0", "\n", "for", "(", "index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "                ", "if", "phase", "==", "'train'", ":", "\n", "                    ", "self", ".", "current_train_example", "=", "index", "+", "1", "\n", "", "seq_len", "=", "len", "(", "feature", ".", "input_ids", ")", "\n", "labels", "=", "[", "feature", ".", "unique_id", "\n", "]", "if", "feature", ".", "start_position", "is", "None", "else", "[", "\n", "feature", ".", "start_position", ",", "feature", ".", "end_position", "\n", "]", "\n", "example", "=", "[", "\n", "# feature.input_ids, feature.segment_ids, range(seq_len), feature.concept_ids", "\n", "feature", ".", "input_ids", ",", "feature", ".", "segment_ids", ",", "range", "(", "384", ")", ",", "feature", ".", "concept_ids", "\n", "]", "+", "labels", "\n", "max_len", "=", "max", "(", "max_len", ",", "seq_len", ")", "\n", "\n", "#max_len = max(max_len, len(token_ids))", "\n", "if", "in_tokens", ":", "\n", "                    ", "to_append", "=", "(", "len", "(", "batch", ")", "+", "1", ")", "*", "max_len", "<=", "batch_size", "\n", "", "else", ":", "\n", "                    ", "to_append", "=", "len", "(", "batch", ")", "<", "batch_size", "\n", "\n", "", "if", "to_append", ":", "\n", "                    ", "batch", ".", "append", "(", "example", ")", "\n", "total_token_num", "+=", "seq_len", "\n", "", "else", ":", "\n", "                    ", "yield", "batch", ",", "total_token_num", "\n", "batch", ",", "total_token_num", ",", "max_len", "=", "[", "example", "\n", "]", ",", "seq_len", ",", "seq_len", "\n", "", "", "if", "len", "(", "batch", ")", ">", "0", ":", "\n", "                ", "yield", "batch", ",", "total_token_num", "\n", "\n", "", "", "if", "phase", "==", "'train'", ":", "\n", "            ", "self", ".", "train_max_concept_length", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", ".", "max_concept_length", "\n", "", "else", ":", "\n", "            ", "self", ".", "predict_max_concept_length", "=", "Examples_To_Features_Converter", "(", "**", "concept_settings", ")", ".", "max_concept_length", "\n", "\n", "", "def", "wrapper", "(", ")", ":", "\n", "            ", "for", "epoch_index", "in", "range", "(", "epoch", ")", ":", "\n", "                ", "if", "shuffle", ":", "\n", "                    ", "random", ".", "shuffle", "(", "examples", ")", "\n", "", "if", "phase", "==", "'train'", ":", "\n", "                    ", "self", ".", "current_train_epoch", "=", "epoch_index", "\n", "features", "=", "self", ".", "get_features", "(", "examples", ",", "is_training", "=", "True", ",", "**", "concept_settings", ")", "\n", "max_concept_length", "=", "self", ".", "train_max_concept_length", "\n", "", "else", ":", "\n", "                    ", "features", "=", "self", ".", "get_features", "(", "examples", ",", "is_training", "=", "False", ",", "**", "concept_settings", ")", "\n", "max_concept_length", "=", "self", ".", "predict_max_concept_length", "\n", "\n", "", "all_dev_batches", "=", "[", "]", "\n", "for", "batch_data", ",", "total_token_num", "in", "batch_reader", "(", "\n", "features", ",", "batch_size", ",", "self", ".", "_in_tokens", ")", ":", "\n", "                    ", "batch_data", "=", "prepare_batch_data", "(", "\n", "batch_data", ",", "\n", "total_token_num", ",", "\n", "voc_size", "=", "-", "1", ",", "\n", "pad_id", "=", "self", ".", "pad_id", ",", "\n", "cls_id", "=", "self", ".", "cls_id", ",", "\n", "sep_id", "=", "self", ".", "sep_id", ",", "\n", "mask_id", "=", "-", "1", ",", "\n", "return_input_mask", "=", "True", ",", "\n", "return_max_len", "=", "False", ",", "\n", "return_num_token", "=", "False", ",", "\n", "max_concept_length", "=", "max_concept_length", ")", "\n", "if", "len", "(", "all_dev_batches", ")", "<", "dev_count", ":", "\n", "                        ", "all_dev_batches", ".", "append", "(", "batch_data", ")", "\n", "\n", "", "if", "len", "(", "all_dev_batches", ")", "==", "dev_count", ":", "\n", "                        ", "for", "batch", "in", "all_dev_batches", ":", "\n", "                            ", "yield", "batch", "\n", "", "all_dev_batches", "=", "[", "]", "\n", "\n", "", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.read_record_examples": [[108, 188], ["open", "[].replace", "json.load", "record.read_record_examples.is_whitespace"], "function", ["None"], ["", "", "def", "read_record_examples", "(", "input_file", ",", "is_training", ",", "version_2_with_negative", "=", "False", ")", ":", "\n", "    ", "\"\"\"Read a ReCoRD json file into a list of ReCoRDExample.\"\"\"", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "        ", "input_data", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "\n", "", "def", "is_whitespace", "(", "c", ")", ":", "\n", "        ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "examples", "=", "[", "]", "\n", "for", "entry", "in", "input_data", ":", "\n", "        ", "paragraph_text", "=", "entry", "[", "\"passage\"", "]", "[", "\"text\"", "]", ".", "replace", "(", "'\\xa0'", ",", "' '", ")", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "            ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "                ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "prev_is_whitespace", ":", "\n", "                    ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                    ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "", "for", "qa", "in", "entry", "[", "\"qas\"", "]", ":", "\n", "            ", "qas_id", "=", "qa", "[", "\"id\"", "]", "\n", "question_text", "=", "qa", "[", "\"query\"", "]", ".", "replace", "(", "'\\xa0'", ",", "' '", ")", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "is_impossible", "=", "False", "\n", "if", "is_training", ":", "\n", "\n", "                ", "if", "version_2_with_negative", ":", "\n", "                    ", "is_impossible", "=", "qa", "[", "\"is_impossible\"", "]", "\n", "# if (len(qa[\"answers\"]) != 1) and (not is_impossible):", "\n", "#     raise ValueError(", "\n", "#         \"For training, each question should have exactly 1 answer.\"", "\n", "#     )", "\n", "", "if", "not", "is_impossible", ":", "\n", "                    ", "answer", "=", "qa", "[", "\"answers\"", "]", "[", "0", "]", "\n", "orig_answer_text", "=", "answer", "[", "\"text\"", "]", "\n", "answer_offset", "=", "answer", "[", "\"start\"", "]", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "\n", "answer_length", "-", "1", "]", "\n", "# Only add answers where the text can be exactly recovered from the", "\n", "# document. If this CAN'T happen it's likely due to weird Unicode", "\n", "# stuff so we will just skip the example.", "\n", "#", "\n", "# Note that this means for training mode, every example is NOT", "\n", "# guaranteed to be preserved.", "\n", "actual_text", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "start_position", ":", "(", "\n", "end_position", "+", "1", ")", "]", ")", "\n", "cleaned_answer_text", "=", "\" \"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "if", "actual_text", ".", "find", "(", "cleaned_answer_text", ")", "==", "-", "1", ":", "\n", "                        ", "logger", ".", "info", "(", "\"Could not find answer: '%s' vs. '%s'\"", ",", "\n", "actual_text", ",", "cleaned_answer_text", ")", "\n", "continue", "\n", "", "", "else", ":", "\n", "                    ", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "orig_answer_text", "=", "\"\"", "\n", "\n", "", "", "example", "=", "ReCoRDExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "\n", "orig_answer_text", "=", "orig_answer_text", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "is_impossible", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record._improve_answer_span": [[459, 494], ["range", "tokenizer.tokenize", "range"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "", "", "", "def", "_improve_answer_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_answer_text", ")", ":", "\n", "    ", "\"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"", "\n", "\n", "# The ReCoRD annotations are character based. We first project them to", "\n", "# whitespace-tokenized words. But then after WordPiece tokenization, we can", "\n", "# often find a \"better match\". For example:", "\n", "#", "\n", "#   Question: What year was John Smith born?", "\n", "#   Context: The leader was John Smith (1895-1943).", "\n", "#   Answer: 1895", "\n", "#", "\n", "# The original whitespace-tokenized answer will be \"(1895-1943).\". However", "\n", "# after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match", "\n", "# the exact answer, 1895.", "\n", "#", "\n", "# However, this is not always possible. Consider the following:", "\n", "#", "\n", "#   Question: What country is the top exporter of electornics?", "\n", "#   Context: The Japanese electronics industry is the lagest in the world.", "\n", "#   Answer: Japan", "\n", "#", "\n", "# In this case, the annotator chose \"Japan\" as a character sub-span of", "\n", "# the word \"Japanese\". Since our WordPiece tokenizer does not split", "\n", "# \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare", "\n", "# in ReCoRD, but does happen.", "\n", "tok_answer_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_answer_text", ")", ")", "\n", "\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "        ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "text_span", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_answer_text", ":", "\n", "                ", "return", "(", "new_start", ",", "new_end", ")", "\n", "\n", "", "", "", "return", "(", "input_start", ",", "input_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record._check_is_max_context": [[496, 532], ["enumerate", "min"], "function", ["None"], ["", "def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "    ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "        ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "            ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "            ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "\n", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "            ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.write_predictions": [[695, 920], ["logger.info", "logger.info", "logger.info", "collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "eval.record_official_evaluate.evaluate", "open", "example_index_to_features[].append", "enumerate", "sorted", "collections.namedtuple", "record._compute_softmax", "enumerate", "open", "writer.write", "open", "writer.write", "open", "writer.write", "json.load", "record._get_best_indexes", "record._get_best_indexes", "sorted.append", "nbest.append", "nbest.append", "len", "total_scores.append", "logger.info", "collections.OrderedDict", "nbest_json.append", "len", "range", "open", "writer.write", "candidates.append", "collections.namedtuple.", "len", "tok_text.strip.replace", "tok_text.strip.replace", "tok_text.strip.strip", "record.get_final_text", "collections.namedtuple.", "nbest.append", "collections.namedtuple.", "len", "any", "json.dumps", "json.dumps", "json.dumps", "sorted.append", "tok_text.strip.split", "collections.namedtuple.", "json.dumps", "len", "len", "feature.token_is_max_context.get", "collections.namedtuple.", "eval.record_official_evaluate.f1_score"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.evaluate", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._compute_softmax", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.get_final_text", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.f1_score"], ["", "", "def", "write_predictions", "(", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "output_null_log_odds_file", ",", "\n", "version_2_with_negative", ",", "null_score_diff_threshold", ",", "\n", "verbose", ",", "predict_file", ",", "evaluation_result_file", ")", ":", "\n", "    ", "\"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"", "\n", "logger", ".", "info", "(", "\"Writing predictions to: %s\"", "%", "(", "output_prediction_file", ")", ")", "\n", "logger", ".", "info", "(", "\"Writing nbest to: %s\"", "%", "(", "output_nbest_file", ")", ")", "\n", "logger", ".", "info", "(", "\"Writing evaluation result to: %s\"", "%", "(", "evaluation_result_file", ")", ")", "\n", "\n", "# load ground truth file for evaluation and post-edit", "\n", "with", "open", "(", "predict_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "        ", "predict_json", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "all_candidates", "=", "{", "}", "\n", "for", "passage", "in", "predict_json", ":", "\n", "            ", "passage_text", "=", "passage", "[", "'passage'", "]", "[", "'text'", "]", "\n", "candidates", "=", "[", "]", "\n", "for", "entity_info", "in", "passage", "[", "'passage'", "]", "[", "'entities'", "]", ":", "\n", "                ", "start_offset", "=", "entity_info", "[", "'start'", "]", "\n", "end_offset", "=", "entity_info", "[", "'end'", "]", "\n", "candidates", ".", "append", "(", "passage_text", "[", "start_offset", ":", "end_offset", "+", "1", "]", ")", "\n", "", "for", "qa", "in", "passage", "[", "'qas'", "]", ":", "\n", "                ", "all_candidates", "[", "qa", "[", "'id'", "]", "]", "=", "candidates", "\n", "\n", "", "", "", "example_index_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "        ", "example_index_to_features", "[", "feature", ".", "example_index", "]", ".", "append", "(", "feature", ")", "\n", "\n", "", "unique_id_to_result", "=", "{", "}", "\n", "for", "result", "in", "all_results", ":", "\n", "        ", "unique_id_to_result", "[", "result", ".", "unique_id", "]", "=", "result", "\n", "\n", "", "_PrelimPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"PrelimPrediction\"", ",", "[", "\n", "\"feature_index\"", ",", "\"start_index\"", ",", "\"end_index\"", ",", "\"start_logit\"", ",", "\n", "\"end_logit\"", "\n", "]", ")", "\n", "\n", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_nbest_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "scores_diff_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "all_examples", ")", ":", "\n", "        ", "features", "=", "example_index_to_features", "[", "example_index", "]", "\n", "\n", "prelim_predictions", "=", "[", "]", "\n", "# keep track of the minimum score of null start+end of position 0", "\n", "score_null", "=", "1000000", "# large and positive", "\n", "min_null_feature_index", "=", "0", "# the paragraph slice with min mull score", "\n", "null_start_logit", "=", "0", "# the start logit at the slice with min null score", "\n", "null_end_logit", "=", "0", "# the end logit at the slice with min null score", "\n", "for", "(", "feature_index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "            ", "result", "=", "unique_id_to_result", "[", "feature", ".", "unique_id", "]", "\n", "start_indexes", "=", "_get_best_indexes", "(", "result", ".", "start_logits", ",", "n_best_size", ")", "\n", "end_indexes", "=", "_get_best_indexes", "(", "result", ".", "end_logits", ",", "n_best_size", ")", "\n", "# if we could have irrelevant answers, get the min score of irrelevant", "\n", "if", "version_2_with_negative", ":", "\n", "                ", "feature_null_score", "=", "result", ".", "start_logits", "[", "0", "]", "+", "result", ".", "end_logits", "[", "\n", "0", "]", "\n", "if", "feature_null_score", "<", "score_null", ":", "\n", "                    ", "score_null", "=", "feature_null_score", "\n", "min_null_feature_index", "=", "feature_index", "\n", "null_start_logit", "=", "result", ".", "start_logits", "[", "0", "]", "\n", "null_end_logit", "=", "result", ".", "end_logits", "[", "0", "]", "\n", "", "", "for", "start_index", "in", "start_indexes", ":", "\n", "                ", "for", "end_index", "in", "end_indexes", ":", "\n", "# We could hypothetically create invalid predictions, e.g., predict", "\n", "# that the start of the span is in the question. We throw out all", "\n", "# invalid predictions.", "\n", "                    ", "if", "start_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "start_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "                        ", "continue", "\n", "", "if", "not", "feature", ".", "token_is_max_context", ".", "get", "(", "start_index", ",", "False", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "                        ", "continue", "\n", "", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "if", "length", ">", "max_answer_length", ":", "\n", "                        ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "feature_index", ",", "\n", "start_index", "=", "start_index", ",", "\n", "end_index", "=", "end_index", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "start_index", "]", ",", "\n", "end_logit", "=", "result", ".", "end_logits", "[", "end_index", "]", ")", ")", "\n", "\n", "", "", "", "if", "version_2_with_negative", ":", "\n", "            ", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "min_null_feature_index", ",", "\n", "start_index", "=", "0", ",", "\n", "end_index", "=", "0", ",", "\n", "start_logit", "=", "null_start_logit", ",", "\n", "end_logit", "=", "null_end_logit", ")", ")", "\n", "", "prelim_predictions", "=", "sorted", "(", "\n", "prelim_predictions", ",", "\n", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_logit", "+", "x", ".", "end_logit", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "_NbestPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"NbestPrediction\"", ",", "[", "\"text\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "seen_predictions", "=", "{", "}", "\n", "nbest", "=", "[", "]", "\n", "for", "pred", "in", "prelim_predictions", ":", "\n", "            ", "if", "len", "(", "nbest", ")", ">=", "n_best_size", ":", "\n", "                ", "break", "\n", "", "feature", "=", "features", "[", "pred", ".", "feature_index", "]", "\n", "if", "pred", ".", "start_index", ">", "0", ":", "# this is a non-null prediction", "\n", "                ", "tok_tokens", "=", "feature", ".", "tokens", "[", "pred", ".", "start_index", ":", "(", "pred", ".", "end_index", "+", "1", "\n", ")", "]", "\n", "orig_doc_start", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "start_index", "]", "\n", "orig_doc_end", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "end_index", "]", "\n", "orig_tokens", "=", "example", ".", "doc_tokens", "[", "orig_doc_start", ":", "(", "orig_doc_end", "+", "\n", "1", ")", "]", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_tokens", ")", "\n", "\n", "# De-tokenize WordPieces that have been split off.", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\"##\"", ",", "\"\"", ")", "\n", "\n", "# Clean whitespace", "\n", "tok_text", "=", "tok_text", ".", "strip", "(", ")", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_text", ".", "split", "(", ")", ")", "\n", "orig_text", "=", "\" \"", ".", "join", "(", "orig_tokens", ")", "\n", "\n", "final_text", "=", "get_final_text", "(", "tok_text", ",", "orig_text", ",", "do_lower_case", ",", "\n", "verbose", ")", "\n", "if", "final_text", "in", "seen_predictions", ":", "\n", "                    ", "continue", "\n", "\n", "", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "", "else", ":", "\n", "                ", "final_text", "=", "\"\"", "\n", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "\n", "", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "final_text", ",", "\n", "start_logit", "=", "pred", ".", "start_logit", ",", "\n", "end_logit", "=", "pred", ".", "end_logit", ")", ")", "\n", "\n", "# if we didn't inlude the empty option in the n-best, inlcude it", "\n", "", "if", "version_2_with_negative", ":", "\n", "            ", "if", "\"\"", "not", "in", "seen_predictions", ":", "\n", "                ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "\"\"", ",", "\n", "start_logit", "=", "null_start_logit", ",", "\n", "end_logit", "=", "null_end_logit", ")", ")", "\n", "# In very rare edge cases we could have no valid predictions. So we", "\n", "# just create a nonce prediction in this case to avoid failure.", "\n", "", "", "if", "not", "nbest", ":", "\n", "            ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "\"empty\"", ",", "start_logit", "=", "0.0", ",", "end_logit", "=", "0.0", ")", ")", "\n", "\n", "", "assert", "len", "(", "nbest", ")", ">=", "1", "\n", "\n", "total_scores", "=", "[", "]", "\n", "best_non_null_entry", "=", "None", "\n", "for", "entry", "in", "nbest", ":", "\n", "            ", "total_scores", ".", "append", "(", "entry", ".", "start_logit", "+", "entry", ".", "end_logit", ")", "\n", "if", "not", "best_non_null_entry", ":", "\n", "                ", "if", "entry", ".", "text", ":", "\n", "                    ", "best_non_null_entry", "=", "entry", "\n", "# debug", "\n", "", "", "", "if", "best_non_null_entry", "is", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\"Emmm..., sth wrong\"", ")", "\n", "\n", "", "probs", "=", "_compute_softmax", "(", "total_scores", ")", "\n", "\n", "nbest_json", "=", "[", "]", "\n", "for", "(", "i", ",", "entry", ")", "in", "enumerate", "(", "nbest", ")", ":", "\n", "            ", "output", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "output", "[", "\"text\"", "]", "=", "entry", ".", "text", "\n", "output", "[", "\"probability\"", "]", "=", "probs", "[", "i", "]", "\n", "output", "[", "\"start_logit\"", "]", "=", "entry", ".", "start_logit", "\n", "output", "[", "\"end_logit\"", "]", "=", "entry", ".", "end_logit", "\n", "nbest_json", ".", "append", "(", "output", ")", "\n", "\n", "", "assert", "len", "(", "nbest_json", ")", ">=", "1", "\n", "\n", "if", "not", "version_2_with_negative", ":", "\n", "# restrict the finally picked prediction to have overlap with at least one candidate", "\n", "            ", "picked_index", "=", "0", "\n", "for", "pred_index", "in", "range", "(", "len", "(", "nbest_json", ")", ")", ":", "\n", "                ", "if", "any", "(", "[", "f1_score", "(", "nbest_json", "[", "pred_index", "]", "[", "'text'", "]", ",", "candidate", ")", ">", "0.", "for", "candidate", "in", "all_candidates", "[", "example", ".", "qas_id", "]", "]", ")", ":", "\n", "                    ", "picked_index", "=", "pred_index", "\n", "break", "\n", "", "", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "[", "picked_index", "]", "[", "\"text\"", "]", "\n", "", "else", ":", "\n", "# predict \"\" iff the null score - the score of best non-null > threshold", "\n", "            ", "score_diff", "=", "score_null", "-", "best_non_null_entry", ".", "start_logit", "-", "(", "\n", "best_non_null_entry", ".", "end_logit", ")", "\n", "scores_diff_json", "[", "example", ".", "qas_id", "]", "=", "score_diff", "\n", "if", "score_diff", ">", "null_score_diff_threshold", ":", "\n", "                ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "\"\"", "\n", "", "else", ":", "\n", "                ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "best_non_null_entry", ".", "text", "\n", "\n", "", "", "all_nbest_json", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "\n", "\n", "", "with", "open", "(", "output_prediction_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_predictions", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "with", "open", "(", "output_nbest_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_nbest_json", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "if", "version_2_with_negative", ":", "\n", "        ", "with", "open", "(", "output_null_log_odds_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "scores_diff_json", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "", "eval_result", ",", "_", "=", "evaluate", "(", "predict_json", ",", "all_predictions", ")", "\n", "\n", "with", "open", "(", "evaluation_result_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "eval_result", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "return", "eval_result", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record.get_final_text": [[921, 1014], ["tokenization.BasicTokenizer", "tok_text.find", "record.get_final_text._strip_spaces"], "function", ["None"], ["", "def", "get_final_text", "(", "pred_text", ",", "orig_text", ",", "do_lower_case", ",", "verbose", ")", ":", "\n", "    ", "\"\"\"Project the tokenized prediction back to the original text.\"\"\"", "\n", "\n", "# When we created the data, we kept track of the alignment between original", "\n", "# (whitespace tokenized) tokens and our WordPiece tokenized tokens. So", "\n", "# now `orig_text` contains the span of our original text corresponding to the", "\n", "# span that we predicted.", "\n", "#", "\n", "# However, `orig_text` may contain extra characters that we don't want in", "\n", "# our prediction.", "\n", "#", "\n", "# For example, let's say:", "\n", "#   pred_text = steve smith", "\n", "#   orig_text = Steve Smith's", "\n", "#", "\n", "# We don't want to return `orig_text` because it contains the extra \"'s\".", "\n", "#", "\n", "# We don't want to return `pred_text` because it's already been normalized", "\n", "# (the ReCoRD eval script also does punctuation stripping/lower casing but", "\n", "# our tokenizer does additional normalization like stripping accent", "\n", "# characters).", "\n", "#", "\n", "# What we really want to return is \"Steve Smith\".", "\n", "#", "\n", "# Therefore, we have to apply a semi-complicated alignment heruistic between", "\n", "# `pred_text` and `orig_text` to get a character-to-charcter alignment. This", "\n", "# can fail in certain cases in which case we just return `orig_text`.", "\n", "\n", "def", "_strip_spaces", "(", "text", ")", ":", "\n", "        ", "ns_chars", "=", "[", "]", "\n", "ns_to_s_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "i", ",", "c", ")", "in", "enumerate", "(", "text", ")", ":", "\n", "            ", "if", "c", "==", "\" \"", ":", "\n", "                ", "continue", "\n", "", "ns_to_s_map", "[", "len", "(", "ns_chars", ")", "]", "=", "i", "\n", "ns_chars", ".", "append", "(", "c", ")", "\n", "", "ns_text", "=", "\"\"", ".", "join", "(", "ns_chars", ")", "\n", "return", "(", "ns_text", ",", "ns_to_s_map", ")", "\n", "\n", "# We first tokenize `orig_text`, strip whitespace from the result", "\n", "# and `pred_text`, and check if they are the same length. If they are", "\n", "# NOT the same length, the heuristic has failed. If they are the same", "\n", "# length, we assume the characters are one-to-one aligned.", "\n", "", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_text", ")", ")", "\n", "\n", "start_position", "=", "tok_text", ".", "find", "(", "pred_text", ")", "\n", "if", "start_position", "==", "-", "1", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Unable to find text: '%s' in '%s'\"", "%", "(", "pred_text", ",", "orig_text", ")", ")", "\n", "", "return", "orig_text", "\n", "", "end_position", "=", "start_position", "+", "len", "(", "pred_text", ")", "-", "1", "\n", "\n", "(", "orig_ns_text", ",", "orig_ns_to_s_map", ")", "=", "_strip_spaces", "(", "orig_text", ")", "\n", "(", "tok_ns_text", ",", "tok_ns_to_s_map", ")", "=", "_strip_spaces", "(", "tok_text", ")", "\n", "\n", "if", "len", "(", "orig_ns_text", ")", "!=", "len", "(", "tok_ns_text", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Length not equal after stripping spaces: '%s' vs '%s'\"", ",", "\n", "orig_ns_text", ",", "tok_ns_text", ")", "\n", "", "return", "orig_text", "\n", "\n", "# We then project the characters in `pred_text` back to `orig_text` using", "\n", "# the character-to-character alignment.", "\n", "", "tok_s_to_ns_map", "=", "{", "}", "\n", "for", "(", "i", ",", "tok_index", ")", "in", "six", ".", "iteritems", "(", "tok_ns_to_s_map", ")", ":", "\n", "        ", "tok_s_to_ns_map", "[", "tok_index", "]", "=", "i", "\n", "\n", "", "orig_start_position", "=", "None", "\n", "if", "start_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_start_position", "=", "tok_s_to_ns_map", "[", "start_position", "]", "\n", "if", "ns_start_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_start_position", "=", "orig_ns_to_s_map", "[", "ns_start_position", "]", "\n", "\n", "", "", "if", "orig_start_position", "is", "None", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Couldn't map start position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "orig_end_position", "=", "None", "\n", "if", "end_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_end_position", "=", "tok_s_to_ns_map", "[", "end_position", "]", "\n", "if", "ns_end_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_end_position", "=", "orig_ns_to_s_map", "[", "ns_end_position", "]", "\n", "\n", "", "", "if", "orig_end_position", "is", "None", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Couldn't map end position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "output_text", "=", "orig_text", "[", "orig_start_position", ":", "(", "orig_end_position", "+", "1", ")", "]", "\n", "return", "output_text", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record._get_best_indexes": [[1016, 1027], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "_get_best_indexes", "(", "logits", ",", "n_best_size", ")", ":", "\n", "    ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "\n", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "        ", "if", "i", ">=", "n_best_size", ":", "\n", "            ", "break", "\n", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.record._compute_softmax": [[1029, 1050], ["math.exp", "exp_scores.append", "probs.append"], "function", ["None"], ["", "def", "_compute_softmax", "(", "scores", ")", ":", "\n", "    ", "\"\"\"Compute softmax probability over raw logits.\"\"\"", "\n", "if", "not", "scores", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "", "max_score", "=", "None", "\n", "for", "score", "in", "scores", ":", "\n", "        ", "if", "max_score", "is", "None", "or", "score", ">", "max_score", ":", "\n", "            ", "max_score", "=", "score", "\n", "\n", "", "", "exp_scores", "=", "[", "]", "\n", "total_sum", "=", "0.0", "\n", "for", "score", "in", "scores", ":", "\n", "        ", "x", "=", "math", ".", "exp", "(", "score", "-", "max_score", ")", "\n", "exp_scores", ".", "append", "(", "x", ")", "\n", "total_sum", "+=", "x", "\n", "\n", "", "probs", "=", "[", "]", "\n", "for", "score", "in", "exp_scores", ":", "\n", "        ", "probs", ".", "append", "(", "score", "/", "total_sum", ")", "\n", "", "return", "probs", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.SquadExample.__init__": [[33, 48], ["None"], "methods", ["None"], ["\n", "yield", "{", "\n", "'id'", ":", "qas_id", ",", "\n", "'title'", ":", "title", ",", "\n", "'context'", ":", "context", ",", "\n", "'question'", ":", "question", ",", "\n", "'answers'", ":", "answers", ",", "\n", "'answer_starts'", ":", "answer_starts", ",", "\n", "'is_impossible'", ":", "is_impossible", "\n", "}", "\n", "\n", "", "", "", "", "", "def", "compute_prediction_checklist", "(", "examples", ",", "\n", "features", ",", "\n", "predictions", ",", "\n", "version_2_with_negative", ":", "bool", "=", "False", ",", "\n", "n_best_size", ":", "int", "=", "20", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.SquadExample.__str__": [[49, 51], ["squad.SquadExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.SQuADExample.__repr__"], ["max_answer_length", ":", "int", "=", "30", ",", "\n", "cls_threshold", ":", "float", "=", "0.5", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.SquadExample.__repr__": [[52, 65], ["tokenization.printable_text", "tokenization.printable_text"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.InputFeatures.__init__": [[70, 95], ["None"], "methods", ["None"], ["\n", "assert", "len", "(", "\n", "predictions", "\n", ")", "==", "3", ",", "\"`predictions` should be a tuple with two elements (start_logits, end_logits, cls_logits).\"", "\n", "all_start_logits", ",", "all_end_logits", ",", "all_cls_logits", "=", "predictions", "\n", "\n", "assert", "len", "(", "predictions", "[", "0", "]", ")", "==", "len", "(", "\n", "features", ")", ",", "\"Number of predictions should be equal to number of features.\"", "\n", "\n", "# Build a map example to its corresponding features.", "\n", "features_per_example", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "i", ",", "feature", "in", "enumerate", "(", "features", ")", ":", "\n", "        ", "features_per_example", "[", "feature", "[", "\"example_id\"", "]", "]", ".", "append", "(", "\n", "i", ")", "\n", "\n", "# The dictionaries we have to fill.", "\n", "", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_nbest_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_cls_predictions", "=", "[", "]", "\n", "\n", "# Let's loop over all the examples!", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter.__init__": [[191, 223], ["os.path.exists", "pickle.load", "open", "os.path.exists", "pickle.load", "max", "os.path.exists", "pickle.load", "max", "open", "open", "len", "max", "squad.Examples_To_Features_Converter.synsets_info.values", "squad.Examples_To_Features_Converter.nell_retrieve_info.items", "len", "len"], "methods", ["None"], ["            ", "predictions", ".", "insert", "(", "0", ",", "{", "\n", "\"text\"", ":", "\"no answer\"", ",", "\n", "\"start_logit\"", ":", "0.0", ",", "\n", "\"end_logit\"", ":", "0.0", ",", "\n", "\"score\"", ":", "0.0", "\n", "}", ")", "\n", "\n", "# Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using", "\n", "# the LogSumExp trick).", "\n", "", "scores", "=", "np", ".", "array", "(", "[", "pred", ".", "pop", "(", "\"score\"", ")", "for", "pred", "in", "predictions", "]", ")", "\n", "exp_scores", "=", "np", ".", "exp", "(", "scores", "-", "np", ".", "max", "(", "scores", ")", ")", "\n", "probs", "=", "exp_scores", "/", "exp_scores", ".", "sum", "(", ")", "\n", "\n", "# Include the probabilities in our predictions.", "\n", "for", "prob", ",", "pred", "in", "zip", "(", "probs", ",", "predictions", ")", ":", "\n", "            ", "pred", "[", "\"probability\"", "]", "=", "prob", "\n", "\n", "# Pick the best prediction. If the null answer is not possible, this is easy.", "\n", "", "if", "not", "version_2_with_negative", ":", "\n", "            ", "all_predictions", "[", "example", "[", "\"id\"", "]", "]", "=", "predictions", "[", "0", "]", "[", "\"text\"", "]", "\n", "", "else", ":", "\n", "# Otherwise we first need to find the best non-empty prediction.", "\n", "            ", "i", "=", "0", "\n", "while", "predictions", "[", "i", "]", "[", "\"text\"", "]", "==", "\"no answer\"", ":", "\n", "                ", "i", "+=", "1", "\n", "", "best_non_null_pred", "=", "predictions", "[", "i", "]", "\n", "\n", "if", "answerable_probs", "[", "1", "]", "<", "cls_threshold", ":", "\n", "                ", "all_predictions", "[", "example", "[", "'id'", "]", "]", "=", "\"no answer\"", "\n", "", "else", ":", "\n", "                ", "all_predictions", "[", "example", "[", "'id'", "]", "]", "=", "best_non_null_pred", "[", "'text'", "]", "\n", "\n", "# Make `predictions` JSON-serializable by casting np.float back to float.", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_wordnet_concept_ids": [[225, 236], ["range", "len", "tokenizer.basic_tokenizer._run_strip_accents", "concept_ids.append", "concept_ids.append", "original_token.lower"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_strip_accents"], ["k", ":", "(", "float", "(", "v", ")", "\n", "if", "isinstance", "(", "v", ",", "(", "np", ".", "float16", ",", "np", ".", "float32", ",", "np", ".", "float64", ")", ")", "else", "v", ")", "\n", "for", "k", ",", "v", "in", "pred", ".", "items", "(", ")", "\n", "}", "for", "pred", "in", "predictions", "]", "\n", "\n", "", "return", "all_predictions", ",", "all_nbest_json", ",", "all_cls_predictions", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_nell_concept_ids": [[237, 246], ["range", "range", "len", "list", "range", "set", "range", "len", "len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter.__call__": [[247, 458], ["enumerate", "tokenizer.tokenize", "enumerate", "collections.namedtuple", "enumerate", "squad.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "squad.Examples_To_Features_Converter._lookup_nell_concept_ids", "len", "orig_to_tok_index.append", "tokenizer.tokenize", "squad.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "squad.Examples_To_Features_Converter._lookup_nell_concept_ids", "squad._improve_answer_span", "len", "doc_spans.append", "min", "tokens.append", "segment_ids.append", "concept_ids.append", "zip", "tokens.append", "segment_ids.append", "concept_ids.append", "range", "tokens.append", "segment_ids.append", "concept_ids.append", "tokenizer.convert_tokens_to_ids", "range", "all", "squad.InputFeatures", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "len", "len", "collections.namedtuple.", "len", "tokens.append", "segment_ids.append", "concept_ids.append", "squad._check_is_max_context", "tokens.append", "segment_ids.append", "concept_ids.append", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "len", "len", "len", "len", "tokenization.printable_text", "tokenization.printable_text", "str", "str", "str", "six.iteritems", "six.iteritems", "list", "enumerate", "filter"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_nell_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_wordnet_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.Examples_To_Features_Converter._lookup_nell_concept_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._improve_answer_span", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._check_is_max_context", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.__init__": [[438, 461], ["tokenization.FullTokenizer", "len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_train_progress": [[462, 465], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_examples": [[466, 475], ["squad.read_squad_examples"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.read_squad_examples"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_num_examples": [[476, 481], ["ValueError"], "methods", ["None"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_features": [[482, 491], ["squad.convert_examples_to_features"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.convert_examples_to_features"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.data_generator": [[492, 582], ["squad.DataProcessor.get_examples", "len", "enumerate", "range", "squad.DataProcessor.get_examples", "len", "ValueError", "len", "max", "len", "squad.DataProcessor.data_generator.batch_reader"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_examples", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.DataProcessor.get_examples", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.batch_reader"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.read_squad_examples": [[97, 182], ["io.open", "json.load", "ord", "squad.SquadExample", "examples.append", "tokenization.tokenize_chinese_chars", "ValueError", "len", "len", "tokenization.whitespace_tokenize", "actual_text.find", "print"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.src.tokenization.tokenize_chinese_chars", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize"], ["# Those are the indices of the features associated to the current example.", "\n", "        ", "feature_indices", "=", "features_per_example", "[", "example", "[", "'id'", "]", "]", "\n", "\n", "min_null_prediction", "=", "None", "\n", "prelim_predictions", "=", "[", "]", "\n", "score_answerable", "=", "-", "1", "\n", "# Looping through all the features associated to the current example.", "\n", "for", "feature_index", "in", "feature_indices", ":", "\n", "# We grab the predictions of the model for this feature.", "\n", "            ", "start_logits", "=", "all_start_logits", "[", "feature_index", "]", "\n", "end_logits", "=", "all_end_logits", "[", "feature_index", "]", "\n", "cls_logits", "=", "all_cls_logits", "[", "feature_index", "]", "\n", "# This is what will allow us to map some the positions in our logits to span of texts in the original", "\n", "# context.", "\n", "offset_mapping", "=", "features", "[", "feature_index", "]", "[", "\"offset_mapping\"", "]", "\n", "# Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context", "\n", "# available in the current feature.", "\n", "token_is_max_context", "=", "features", "[", "feature_index", "]", ".", "get", "(", "\n", "\"token_is_max_context\"", ",", "None", ")", "\n", "\n", "# Update minimum null prediction.", "\n", "feature_null_score", "=", "start_logits", "[", "0", "]", "+", "end_logits", "[", "0", "]", "\n", "exp_answerable_scores", "=", "np", ".", "exp", "(", "cls_logits", "-", "np", ".", "max", "(", "cls_logits", ")", ")", "\n", "feature_answerable_score", "=", "exp_answerable_scores", "/", "exp_answerable_scores", ".", "sum", "(", ")", "\n", "if", "feature_answerable_score", "[", "-", "1", "]", ">", "score_answerable", ":", "\n", "                ", "score_answerable", "=", "feature_answerable_score", "[", "-", "1", "]", "\n", "answerable_probs", "=", "feature_answerable_score", "\n", "", "if", "min_null_prediction", "is", "None", "or", "min_null_prediction", "[", "\n", "\"score\"", "]", ">", "feature_null_score", ":", "\n", "                ", "min_null_prediction", "=", "{", "\n", "\"offsets\"", ":", "(", "0", ",", "0", ")", ",", "\n", "\"score\"", ":", "feature_null_score", ",", "\n", "\"start_logit\"", ":", "start_logits", "[", "0", "]", ",", "\n", "\"end_logit\"", ":", "end_logits", "[", "0", "]", ",", "\n", "}", "\n", "# Go through all possibilities for the `n_best_size` greater start and end logits.", "\n", "", "start_indexes", "=", "np", ".", "argsort", "(", "start_logits", ")", "[", "-", "1", ":", "-", "n_best_size", "-", "1", ":", "\n", "-", "1", "]", ".", "tolist", "(", ")", "\n", "end_indexes", "=", "np", ".", "argsort", "(", "end_logits", ")", "[", "-", "1", ":", "-", "n_best_size", "-", "1", ":", "-", "1", "]", ".", "tolist", "(", "\n", ")", "\n", "for", "start_index", "in", "start_indexes", ":", "\n", "                ", "for", "end_index", "in", "end_indexes", ":", "\n", "# Don't consider out-of-scope answers, either because the indices are out of bounds or correspond", "\n", "# to part of the input_ids that are not in the context.", "\n", "                    ", "if", "(", "start_index", ">=", "len", "(", "offset_mapping", ")", "or", "\n", "end_index", ">=", "len", "(", "offset_mapping", ")", "or", "\n", "offset_mapping", "[", "start_index", "]", "is", "None", "or", "\n", "offset_mapping", "[", "end_index", "]", "is", "None", "or", "\n", "offset_mapping", "[", "start_index", "]", "==", "(", "0", ",", "0", ")", "or", "\n", "offset_mapping", "[", "end_index", "]", "==", "(", "0", ",", "0", ")", ")", ":", "\n", "                        ", "continue", "\n", "# Don't consider answers with a length that is either < 0 or > max_answer_length.", "\n", "", "if", "end_index", "<", "start_index", "or", "end_index", "-", "start_index", "+", "1", ">", "max_answer_length", ":", "\n", "                        ", "continue", "\n", "# Don't consider answer that don't have the maximum context available (if such information is", "\n", "# provided).", "\n", "", "if", "token_is_max_context", "is", "not", "None", "and", "not", "token_is_max_context", ".", "get", "(", "\n", "str", "(", "start_index", ")", ",", "False", ")", ":", "\n", "                        ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "{", "\n", "\"offsets\"", ":", "(", "offset_mapping", "[", "start_index", "]", "[", "0", "]", ",", "\n", "offset_mapping", "[", "end_index", "]", "[", "1", "]", ")", ",", "\n", "\"score\"", ":", "\n", "start_logits", "[", "start_index", "]", "+", "end_logits", "[", "end_index", "]", ",", "\n", "\"start_logit\"", ":", "start_logits", "[", "start_index", "]", ",", "\n", "\"end_logit\"", ":", "end_logits", "[", "end_index", "]", ",", "\n", "}", ")", "\n", "", "", "", "if", "version_2_with_negative", ":", "\n", "# Add the minimum null prediction", "\n", "            ", "prelim_predictions", ".", "append", "(", "min_null_prediction", ")", "\n", "pred_cls_label", "=", "np", ".", "argmax", "(", "np", ".", "array", "(", "answerable_probs", ")", ")", "\n", "all_cls_predictions", ".", "append", "(", "[", "example", "[", "'id'", "]", ",", "pred_cls_label", ",", "answerable_probs", "[", "0", "]", ",", "answerable_probs", "[", "1", "]", "]", ")", "\n", "\n", "\n", "# Only keep the best `n_best_size` predictions.", "\n", "", "predictions", "=", "sorted", "(", "\n", "prelim_predictions", ",", "key", "=", "lambda", "x", ":", "x", "[", "\"score\"", "]", ",", "\n", "reverse", "=", "True", ")", "[", ":", "n_best_size", "]", "\n", "\n", "# Add back the minimum null prediction if it was removed because of its low score.", "\n", "if", "version_2_with_negative", "and", "not", "any", "(", "p", "[", "\"offsets\"", "]", "==", "(", "0", ",", "0", ")", "\n", "for", "p", "in", "predictions", ")", ":", "\n", "            ", "predictions", ".", "append", "(", "min_null_prediction", ")", "\n", "\n", "# Use the offsets to gather the answer text in the original context.", "\n", "", "context", "=", "example", "[", "\"context\"", "]", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._improve_answer_span": [[362, 397], ["range", "tokenizer.tokenize", "range"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._check_is_max_context": [[399, 435], ["enumerate", "min"], "function", ["None"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.write_predictions": [[584, 781], ["print", "print", "collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "example_index_to_features[].append", "enumerate", "sorted", "collections.namedtuple", "squad._compute_softmax", "enumerate", "io.open", "writer.write", "io.open", "writer.write", "squad._get_best_indexes", "squad._get_best_indexes", "sorted.append", "nbest.append", "nbest.append", "len", "total_scores.append", "print", "collections.OrderedDict", "nbest_json.append", "len", "io.open", "writer.write", "collections.namedtuple.", "len", "tok_text.strip.replace", "tok_text.strip.replace", "tok_text.strip.strip", "squad.get_final_text", "collections.namedtuple.", "nbest.append", "collections.namedtuple.", "json.dumps", "json.dumps", "sorted.append", "tok_text.strip.split", "collections.namedtuple.", "json.dumps", "len", "len", "feature.token_is_max_context.get", "collections.namedtuple."], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._compute_softmax", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.get_final_text"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.get_final_text": [[783, 876], ["tokenization.BasicTokenizer", "tok_text.find", "squad.get_final_text._strip_spaces"], "function", ["None"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._get_best_indexes": [[878, 889], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._compute_softmax": [[891, 912], ["math.exp", "exp_scores.append", "probs.append"], "function", ["None"], []], "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad.convert_examples_to_features": [[184, 360], ["enumerate", "tokenizer.tokenize", "enumerate", "collections.namedtuple", "enumerate", "len", "orig_to_tok_index.append", "tokenizer.tokenize", "squad._improve_answer_span", "len", "doc_spans.append", "min", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "range", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "squad.InputFeatures", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "len", "len", "collections.namedtuple.", "len", "tokens.append", "segment_ids.append", "squad._check_is_max_context", "tokens.append", "segment_ids.append", "len", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "len", "len", "print", "print", "print", "print", "len", "len", "len", "tokenization.printable_text", "tokenization.printable_text", "str", "str", "str", "six.iteritems", "six.iteritems"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._improve_answer_span", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.reader.squad._check_is_max_context", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["            ", "offsets", "=", "pred", ".", "pop", "(", "\"offsets\"", ")", "\n", "pred", "[", "\"text\"", "]", "=", "context", "[", "offsets", "[", "0", "]", ":", "offsets", "[", "1", "]", "]", "if", "context", "[", "offsets", "[", "0", "]", ":", "offsets", "[", "1", "]", "]", "!=", "\"\"", "else", "\"no answer\"", "\n", "\n", "# In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid", "\n", "# failure.", "\n", "", "if", "len", "(", "predictions", ")", "==", "0", "or", "(", "len", "(", "predictions", ")", "==", "1", "and", "\n", "predictions", "[", "0", "]", "[", "\"text\"", "]", "==", "\"no answer\"", ")", ":", "\n", "            ", "predictions", ".", "insert", "(", "0", ",", "{", "\n", "\"text\"", ":", "\"no answer\"", ",", "\n", "\"start_logit\"", ":", "0.0", ",", "\n", "\"end_logit\"", ":", "0.0", ",", "\n", "\"score\"", ":", "0.0", "\n", "}", ")", "\n", "\n", "# Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using", "\n", "# the LogSumExp trick).", "\n", "", "scores", "=", "np", ".", "array", "(", "[", "pred", ".", "pop", "(", "\"score\"", ")", "for", "pred", "in", "predictions", "]", ")", "\n", "exp_scores", "=", "np", ".", "exp", "(", "scores", "-", "np", ".", "max", "(", "scores", ")", ")", "\n", "probs", "=", "exp_scores", "/", "exp_scores", ".", "sum", "(", ")", "\n", "\n", "# Include the probabilities in our predictions.", "\n", "for", "prob", ",", "pred", "in", "zip", "(", "probs", ",", "predictions", ")", ":", "\n", "            ", "pred", "[", "\"probability\"", "]", "=", "prob", "\n", "\n", "# Pick the best prediction. If the null answer is not possible, this is easy.", "\n", "", "if", "not", "version_2_with_negative", ":", "\n", "            ", "all_predictions", "[", "example", "[", "\"id\"", "]", "]", "=", "predictions", "[", "0", "]", "[", "\"text\"", "]", "\n", "", "else", ":", "\n", "# Otherwise we first need to find the best non-empty prediction.", "\n", "            ", "i", "=", "0", "\n", "while", "predictions", "[", "i", "]", "[", "\"text\"", "]", "==", "\"no answer\"", ":", "\n", "                ", "i", "+=", "1", "\n", "", "best_non_null_pred", "=", "predictions", "[", "i", "]", "\n", "\n", "if", "answerable_probs", "[", "1", "]", "<", "cls_threshold", ":", "\n", "                ", "all_predictions", "[", "example", "[", "'id'", "]", "]", "=", "\"no answer\"", "\n", "", "else", ":", "\n", "                ", "all_predictions", "[", "example", "[", "'id'", "]", "]", "=", "best_non_null_pred", "[", "'text'", "]", "\n", "\n", "# Make `predictions` JSON-serializable by casting np.float back to float.", "\n", "", "", "all_nbest_json", "[", "example", "[", "\"id\"", "]", "]", "=", "[", "{", "\n", "k", ":", "(", "float", "(", "v", ")", "\n", "if", "isinstance", "(", "v", ",", "(", "np", ".", "float16", ",", "np", ".", "float32", ",", "np", ".", "float64", ")", ")", "else", "v", ")", "\n", "for", "k", ",", "v", "in", "pred", ".", "items", "(", ")", "\n", "}", "for", "pred", "in", "predictions", "]", "\n", "\n", "", "return", "all_predictions", ",", "all_nbest_json", ",", "all_cls_predictions", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.retrieve_wordnet.retrieve.main": [[32, 102], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "open", "logger.info", "logger.info", "pickle.load", "pickle.load", "logger.info", "set", "logger.info", "set", "logger.info", "logger.info", "dict", "tqdm.tqdm", "logger.info", "logger.info", "logger.info", "line.strip().split", "open", "open", "nltk.corpus.stopwords.words", "nltk.corpus.wordnet.synsets", "os.path.exists", "os.makedirs", "open", "pickle.dump", "set.add", "set", "logger.info", "logger.info", "logger.info", "str().zfill", "len", "len", "len", "os.path.join", "line.strip", "len", "wn18synset_names.append", "str", "synset.offset"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--train_token'", ",", "type", "=", "str", ",", "default", "=", "'../tokenization_record/tokens/train.tokenization.uncased.data'", ",", "help", "=", "'token file of train set'", ")", "\n", "parser", ".", "add_argument", "(", "'--eval_token'", ",", "type", "=", "str", ",", "default", "=", "'../tokenization_record/tokens/dev.tokenization.uncased.data'", ",", "help", "=", "'token file of dev set'", ")", "\n", "parser", ".", "add_argument", "(", "'--output_dir'", ",", "type", "=", "str", ",", "default", "=", "'output_record/'", ",", "help", "=", "'output directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_stopwords'", ",", "action", "=", "'store_true'", ",", "help", "=", "'ignore stopwords'", ")", "\n", "parser", ".", "add_argument", "(", "'--ignore_length'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'ignore words with length <= ignore_length'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# initialize mapping from offset id to wn18 synset name", "\n", "offset_to_wn18name_dict", "=", "{", "}", "\n", "fin", "=", "open", "(", "'wordnet-mlj12-definitions.txt'", ")", "\n", "for", "line", "in", "fin", ":", "\n", "        ", "info", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "offset_str", ",", "synset_name", "=", "info", "[", "0", "]", ",", "info", "[", "1", "]", "\n", "offset_to_wn18name_dict", "[", "offset_str", "]", "=", "synset_name", "\n", "", "logger", ".", "info", "(", "'Finished loading wn18 definition file.'", ")", "\n", "\n", "\n", "# load pickled samples", "\n", "logger", ".", "info", "(", "'Begin to load tokenization results...'", ")", "\n", "train_samples", "=", "pickle", ".", "load", "(", "open", "(", "args", ".", "train_token", ",", "'rb'", ")", ")", "\n", "dev_samples", "=", "pickle", ".", "load", "(", "open", "(", "args", ".", "eval_token", ",", "'rb'", ")", ")", "\n", "logger", ".", "info", "(", "'Finished loading tokenization results.'", ")", "\n", "\n", "# build token set", "\n", "all_token_set", "=", "set", "(", ")", "\n", "for", "sample", "in", "train_samples", "+", "dev_samples", ":", "\n", "        ", "for", "token", "in", "sample", "[", "'query_tokens'", "]", "+", "sample", "[", "'document_tokens'", "]", ":", "\n", "            ", "all_token_set", ".", "add", "(", "token", ")", "\n", "", "", "logger", ".", "info", "(", "'Finished making tokenization results into token set.'", ")", "\n", "\n", "# load stopwords", "\n", "stopwords", "=", "set", "(", "nltk", ".", "corpus", ".", "stopwords", ".", "words", "(", "'english'", ")", ")", "\n", "logger", ".", "info", "(", "'Finished loading stopwords list.'", ")", "\n", "\n", "# retrive synsets", "\n", "logger", ".", "info", "(", "'Begin to retrieve synsets...'", ")", "\n", "token2synset", "=", "dict", "(", ")", "\n", "stopword_cnt", "=", "0", "\n", "punctuation_cnt", "=", "0", "\n", "for", "token", "in", "tqdm", "(", "all_token_set", ")", ":", "\n", "        ", "if", "token", "in", "set", "(", "string", ".", "punctuation", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'{} is punctuation, skipped!'", ".", "format", "(", "token", ")", ")", "\n", "punctuation_cnt", "+=", "1", "\n", "continue", "\n", "", "if", "args", ".", "no_stopwords", "and", "token", "in", "stopwords", ":", "\n", "            ", "logger", ".", "info", "(", "'{} is stopword, skipped!'", ".", "format", "(", "token", ")", ")", "\n", "stopword_cnt", "+=", "1", "\n", "continue", "\n", "", "if", "args", ".", "ignore_length", ">", "0", "and", "len", "(", "token", ")", "<=", "args", ".", "ignore_length", ":", "\n", "            ", "logger", ".", "info", "(", "'{} is too short, skipped!'", ".", "format", "(", "token", ")", ")", "\n", "continue", "\n", "", "synsets", "=", "wn", ".", "synsets", "(", "token", ")", "\n", "wn18synset_names", "=", "[", "]", "\n", "for", "synset", "in", "synsets", ":", "\n", "            ", "offset_str", "=", "str", "(", "synset", ".", "offset", "(", ")", ")", ".", "zfill", "(", "8", ")", "\n", "if", "offset_str", "in", "offset_to_wn18name_dict", ":", "\n", "                ", "wn18synset_names", ".", "append", "(", "offset_to_wn18name_dict", "[", "offset_str", "]", ")", "\n", "", "", "if", "len", "(", "wn18synset_names", ")", ">", "0", ":", "\n", "            ", "token2synset", "[", "token", "]", "=", "wn18synset_names", "\n", "", "", "logger", ".", "info", "(", "'Finished retrieving sysnets.'", ")", "\n", "logger", ".", "info", "(", "'{} / {} tokens retrieved at lease 1 synset. {} stopwords and {} punctuations skipped.'", ".", "format", "(", "len", "(", "token2synset", ")", ",", "len", "(", "all_token_set", ")", ",", "stopword_cnt", ",", "punctuation_cnt", ")", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'retrived_synsets.data'", ")", ",", "'wb'", ")", "as", "fout", ":", "\n", "        ", "pickle", ".", "dump", "(", "token2synset", ",", "fout", ")", "\n", "", "logger", ".", "info", "(", "'Finished dumping retrieved synsets.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.FullTokenizer.__init__": [[104, 108], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.load_vocab"], ["        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.FullTokenizer.tokenize": [[109, 116], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "class", "FullTokenizer", "(", "object", ")", ":", "\n", "    ", "\"\"\"Runs end-to-end tokenziation.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.FullTokenizer.convert_tokens_to_ids": [[117, 119], ["tokenization.FullTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_tokens_to_ids"], ["\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.BasicTokenizer.__init__": [[124, 131], ["None"], "methods", ["None"], ["", "", "return", "split_tokens", "\n", "\n", "", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n", "", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.BasicTokenizer.tokenize": [[132, 153], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._tokenize_chinese_chars", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_split_on_punc"], ["\n", "", "", "class", "CharTokenizer", "(", "object", ")", ":", "\n", "    ", "\"\"\"Runs end-to-end tokenziation.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "text", ".", "lower", "(", ")", ".", "split", "(", "\" \"", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n", "", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n", "", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.BasicTokenizer._run_strip_accents": [[154, 164], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.normalize"], ["\n", "\n", "", "", "class", "BasicTokenizer", "(", "object", ")", ":", "\n", "    ", "\"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n            do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.BasicTokenizer._run_split_on_punc": [[165, 184], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_punctuation"], ["self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "_never_lowercase", "=", "[", "'[UNK]'", ",", "'[SEP]'", ",", "'[PAD]'", ",", "'[CLS]'", ",", "'[MASK]'", "]", "\n", "\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "_never_lowercase", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.BasicTokenizer._tokenize_chinese_chars": [[185, 197], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._is_chinese_char"], ["                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "if", "token", "in", "self", ".", "_never_lowercase", ":", "\n", "                ", "split_tokens", ".", "extend", "(", "[", "token", "]", ")", "\n", "", "else", ":", "\n", "                ", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n", "", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.BasicTokenizer._is_chinese_char": [[198, 219], ["None"], "methods", ["None"], ["output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n", "", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.BasicTokenizer._clean_text": [[220, 232], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_whitespace", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_control"], ["", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n", "", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.WordpieceTokenizer.__init__": [[237, 241], ["None"], "methods", ["None"], ["", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n", "", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.WordpieceTokenizer.tokenize": [[242, 294], ["tokenization.convert_to_unicode", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize"], ["#     https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n", "", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n", "\n", "", "", "class", "WordpieceTokenizer", "(", "object", ")", ":", "\n", "    ", "\"\"\"Runs WordPiece tokenziation.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.convert_to_unicode": [[26, 44], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode", "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode"], ["def", "convert_to_unicode", "(", "text", ")", ":", "\n", "    ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.printable_text": [[46, 67], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "    ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.load_vocab": [[69, 82], ["collections.OrderedDict", "open", "tokenization.convert_to_unicode", "token.strip.strip", "reader.readline"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "fin", "=", "open", "(", "vocab_file", ")", "\n", "for", "num", ",", "line", "in", "enumerate", "(", "fin", ")", ":", "\n", "        ", "items", "=", "convert_to_unicode", "(", "line", ".", "strip", "(", ")", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "len", "(", "items", ")", ">", "2", ":", "\n", "            ", "break", "\n", "", "token", "=", "items", "[", "0", "]", "\n", "index", "=", "items", "[", "1", "]", "if", "len", "(", "items", ")", "==", "2", "else", "num", "\n", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "int", "(", "index", ")", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.convert_tokens_to_ids": [[84, 90], ["ids.append"], "function", ["None"], ["", "def", "convert_by_vocab", "(", "vocab", ",", "items", ")", ":", "\n", "    ", "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "        ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization.whitespace_tokenize": [[92, 99], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "vocab", ",", "tokens", ")", "\n", "\n", "\n", "", "def", "convert_ids_to_tokens", "(", "inv_vocab", ",", "ids", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "inv_vocab", ",", "ids", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization._is_whitespace": [[296, 306], ["unicodedata.category"], "function", ["None"], ["\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization._is_control": [[308, 318], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.tokenization._is_punctuation": [[320, 334], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization.ReCoRDExample.__init__": [[38, 53], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "qas_id", ",", "\n", "question_text", ",", "\n", "doc_tokens", ",", "\n", "passage_entities", ",", "\n", "orig_answer_text", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ")", ":", "\n", "        ", "self", ".", "passage_entities", "=", "passage_entities", "\n", "self", ".", "qas_id", "=", "qas_id", "\n", "self", ".", "question_text", "=", "question_text", "\n", "self", ".", "doc_tokens", "=", "doc_tokens", "\n", "self", ".", "orig_answer_text", "=", "orig_answer_text", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization.ReCoRDExample.__str__": [[54, 56], ["do_tokenization.ReCoRDExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.SQuADExample.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization.ReCoRDExample.__repr__": [[57, 68], ["tokenization.printable_text", "tokenization.printable_text"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"\"", "\n", "s", "+=", "\"qas_id: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "self", ".", "qas_id", ")", ")", "\n", "s", "+=", "\", question_text: %s\"", "%", "(", "\n", "tokenization", ".", "printable_text", "(", "self", ".", "question_text", ")", ")", "\n", "s", "+=", "\", doc_tokens: [%s]\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "doc_tokens", ")", ")", "\n", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", start_position: %d\"", "%", "(", "self", ".", "start_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", end_position: %d\"", "%", "(", "self", ".", "end_position", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization.read_record_examples": [[70, 149], ["open", "[].replace", "json.load", "do_tokenization.read_record_examples.is_whitespace"], "function", ["None"], ["", "", "def", "read_record_examples", "(", "input_file", ",", "is_training", ")", ":", "\n", "    ", "\"\"\"Read a ReCoRD json file into a list of ReCoRDExample.\"\"\"", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "        ", "input_data", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "\n", "", "def", "is_whitespace", "(", "c", ")", ":", "\n", "        ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "examples", "=", "[", "]", "\n", "for", "entry", "in", "input_data", ":", "\n", "        ", "paragraph_text", "=", "entry", "[", "\"passage\"", "]", "[", "\"text\"", "]", ".", "replace", "(", "'\\xa0'", ",", "' '", ")", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "            ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "                ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "prev_is_whitespace", ":", "\n", "                    ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                    ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "# load entities in passage", "\n", "", "passage_entities", "=", "[", "]", "\n", "for", "entity", "in", "entry", "[", "'passage'", "]", "[", "'entities'", "]", ":", "\n", "            ", "entity_start_offset", "=", "entity", "[", "'start'", "]", "\n", "entity_end_offset", "=", "entity", "[", "'end'", "]", "\n", "if", "entity_end_offset", "<", "entity_start_offset", ":", "# some error labeled entities in record dataset", "\n", "                ", "continue", "\n", "", "entity_text", "=", "paragraph_text", "[", "entity_start_offset", ":", "entity_end_offset", "+", "1", "]", "\n", "passage_entities", ".", "append", "(", "{", "'orig_text'", ":", "entity_text", ",", "\n", "'start_position'", ":", "char_to_word_offset", "[", "entity_start_offset", "]", ",", "\n", "'end_position'", ":", "char_to_word_offset", "[", "entity_end_offset", "]", "}", ")", "\n", "\n", "", "for", "qa", "in", "entry", "[", "\"qas\"", "]", ":", "\n", "            ", "qas_id", "=", "qa", "[", "\"id\"", "]", "\n", "question_text", "=", "qa", "[", "\"query\"", "]", ".", "replace", "(", "'\\xa0'", ",", "' '", ")", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "if", "is_training", ":", "\n", "# if len(qa[\"answers\"]) != 1:", "\n", "#     raise ValueError(", "\n", "#         \"For training, each question should have exactly 1 answer.\")", "\n", "                ", "answer", "=", "qa", "[", "\"answers\"", "]", "[", "0", "]", "\n", "orig_answer_text", "=", "answer", "[", "\"text\"", "]", "\n", "answer_offset", "=", "answer", "[", "\"start\"", "]", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "answer_length", "-", "1", "]", "\n", "# Only add answers where the text can be exactly recovered from the", "\n", "# document. If this CAN'T happen it's likely due to weird Unicode", "\n", "# stuff so we will just skip the example.", "\n", "#", "\n", "# Note that this means for training mode, every example is NOT", "\n", "# guaranteed to be preserved.", "\n", "actual_text", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "start_position", ":", "(", "end_position", "+", "1", ")", "]", ")", "\n", "cleaned_answer_text", "=", "\" \"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "if", "actual_text", ".", "find", "(", "cleaned_answer_text", ")", "==", "-", "1", ":", "\n", "                    ", "logger", ".", "warning", "(", "\"Could not find answer: '%s' vs. '%s'\"", ",", "\n", "actual_text", ",", "cleaned_answer_text", ")", "\n", "continue", "\n", "\n", "", "", "example", "=", "ReCoRDExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "\n", "passage_entities", "=", "passage_entities", ",", "\n", "orig_answer_text", "=", "orig_answer_text", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization._improve_entity_span": [[150, 162], ["range", "tokenizer.basic_tokenizer.tokenize", "range"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "_improve_entity_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_entity_text", ")", ":", "\n", "    ", "\"\"\"Returns token-level tokenized entity spans that better match the annotated entity.\"\"\"", "\n", "tok_entity_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "basic_tokenizer", ".", "tokenize", "(", "orig_entity_text", ")", ")", "\n", "\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "        ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "text_span", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_entity_text", ":", "\n", "                ", "return", "(", "new_start", ",", "new_end", ")", "\n", "\n", "", "", "", "return", "(", "input_start", ",", "input_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization._is_real_subspan": [[163, 165], ["None"], "function", ["None"], ["", "def", "_is_real_subspan", "(", "start", ",", "end", ",", "other_start", ",", "other_end", ")", ":", "\n", "    ", "return", "(", "start", ">=", "other_start", "and", "end", "<", "other_end", ")", "or", "(", "start", ">", "other_start", "and", "end", "<=", "other_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization.match_query_entities": [[166, 204], ["set", "offset_to_tid_map.append", "set.add", "len", "len", "query_string.find", "any", "no_subspan_results.append", "set", "results.append", "len", "entity_string.count", "do_tokenization._is_real_subspan"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization._is_real_subspan"], ["", "def", "match_query_entities", "(", "query_tokens", ",", "document_entities", ",", "document_tokens", ")", ":", "\n", "# transform query_tokens list into a whitespace separated string", "\n", "    ", "query_string", "=", "\" \"", ".", "join", "(", "query_tokens", ")", "\n", "offset_to_tid_map", "=", "[", "]", "\n", "tid", "=", "0", "\n", "for", "char", "in", "query_string", ":", "\n", "        ", "offset_to_tid_map", ".", "append", "(", "tid", ")", "\n", "if", "char", "==", "' '", ":", "\n", "            ", "tid", "+=", "1", "\n", "\n", "# transform entity_tokens into whitespace separated strings", "\n", "", "", "entity_strings", "=", "set", "(", ")", "\n", "for", "document_entity", "in", "document_entities", ":", "\n", "        ", "entity_tokens", "=", "document_tokens", "[", "document_entity", "[", "1", "]", ":", "document_entity", "[", "2", "]", "+", "1", "]", "\n", "entity_strings", ".", "add", "(", "\" \"", ".", "join", "(", "entity_tokens", ")", ")", "\n", "\n", "# do matching", "\n", "", "results", "=", "[", "]", "\n", "for", "entity_string", "in", "entity_strings", ":", "\n", "        ", "start", "=", "0", "\n", "while", "True", ":", "\n", "            ", "pos", "=", "query_string", ".", "find", "(", "entity_string", ",", "start", ")", "\n", "if", "pos", "==", "-", "1", ":", "\n", "                ", "break", "\n", "", "token_start", ",", "token_end", "=", "offset_to_tid_map", "[", "pos", "]", ",", "offset_to_tid_map", "[", "pos", "]", "+", "entity_string", ".", "count", "(", "' '", ")", "\n", "# assure the match is not partial match (eg. \"ville\" matches to \"danville\")", "\n", "if", "\" \"", ".", "join", "(", "query_tokens", "[", "token_start", ":", "token_end", "+", "1", "]", ")", "==", "entity_string", ":", "\n", "                ", "results", ".", "append", "(", "(", "token_start", ",", "token_end", ")", ")", "\n", "", "start", "=", "pos", "+", "len", "(", "entity_string", ")", "\n", "\n", "# filter out a result span if it's a subspan of another span", "\n", "", "", "no_subspan_results", "=", "[", "]", "\n", "for", "result", "in", "results", ":", "\n", "        ", "if", "not", "any", "(", "[", "_is_real_subspan", "(", "result", "[", "0", "]", ",", "result", "[", "1", "]", ",", "other_result", "[", "0", "]", ",", "other_result", "[", "1", "]", ")", "for", "other_result", "in", "results", "]", ")", ":", "\n", "            ", "no_subspan_results", ".", "append", "(", "(", "\" \"", ".", "join", "(", "query_tokens", "[", "result", "[", "0", "]", ":", "result", "[", "1", "]", "+", "1", "]", ")", ",", "result", "[", "0", "]", ",", "result", "[", "1", "]", ")", ")", "\n", "", "", "assert", "len", "(", "no_subspan_results", ")", "==", "len", "(", "set", "(", "no_subspan_results", ")", ")", "\n", "\n", "return", "no_subspan_results", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization.tokenization_on_examples": [[207, 263], ["tqdm.tqdm", "tokenizer.basic_tokenizer.tokenize", "enumerate", "do_tokenization.match_query_entities", "tokenization_result.append", "tokenizer.wordpiece_tokenizer.tokenize", "tokenizer.basic_tokenizer.tokenize", "document_up_to_ori_index.append", "do_tokenization._improve_entity_span", "document_entities.append", "query_subtokens.append", "query_sub_to_ori_index.append", "len", "tokenizer.wordpiece_tokenizer.tokenize", "document_tokens.append", "document_subtokens.append", "document_sub_to_ori_index.append", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.match_query_entities", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization._improve_entity_span", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenization_on_examples", "(", "examples", ",", "tokenizer", ")", ":", "\n", "\n", "    ", "tokenization_result", "=", "[", "]", "\n", "for", "example", "in", "tqdm", "(", "examples", ")", ":", "\n", "# do tokenization on raw question text", "\n", "        ", "query_subtokens", "=", "[", "]", "\n", "query_sub_to_ori_index", "=", "[", "]", "# mapping from sub-token index to token index", "\n", "query_tokens", "=", "tokenizer", ".", "basic_tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "for", "index", ",", "token", "in", "enumerate", "(", "query_tokens", ")", ":", "\n", "            ", "for", "sub_token", "in", "tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "query_subtokens", ".", "append", "(", "sub_token", ")", "\n", "query_sub_to_ori_index", ".", "append", "(", "index", ")", "\n", "\n", "# do tokenization on whitespace tokenized document", "\n", "", "", "document_tokens", "=", "[", "]", "\n", "document_subtokens", "=", "[", "]", "\n", "document_sub_to_ori_index", "=", "[", "]", "\n", "document_up_to_ori_index", "=", "[", "]", "# map unpunc token index to tokenized token index", "\n", "for", "unpunc_tokenized_tokens", "in", "example", ".", "doc_tokens", ":", "\n", "            ", "tokens", "=", "tokenizer", ".", "basic_tokenizer", ".", "tokenize", "(", "unpunc_tokenized_tokens", ")", "# do punctuation tokenization", "\n", "document_up_to_ori_index", ".", "append", "(", "len", "(", "document_tokens", ")", ")", "\n", "for", "token", "in", "tokens", ":", "\n", "                ", "for", "sub_token", "in", "tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                    ", "document_subtokens", ".", "append", "(", "sub_token", ")", "\n", "document_sub_to_ori_index", ".", "append", "(", "len", "(", "document_tokens", ")", ")", "\n", "", "document_tokens", ".", "append", "(", "token", ")", "\n", "\n", "# generate token-level document entity index", "\n", "", "", "document_entities", "=", "[", "]", "\n", "for", "entity", "in", "example", ".", "passage_entities", ":", "\n", "            ", "entity_start_position", "=", "document_up_to_ori_index", "[", "entity", "[", "'start_position'", "]", "]", "\n", "entity_end_position", "=", "None", "\n", "if", "entity", "[", "'end_position'", "]", "<", "len", "(", "example", ".", "doc_tokens", ")", "-", "1", ":", "\n", "                ", "entity_end_position", "=", "document_up_to_ori_index", "[", "entity", "[", "'end_position'", "]", "+", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "                ", "entity_end_position", "=", "len", "(", "document_tokens", ")", "-", "1", "\n", "", "(", "entity_start_position", ",", "entity_end_position", ")", "=", "_improve_entity_span", "(", "\n", "document_tokens", ",", "entity_start_position", ",", "entity_end_position", ",", "tokenizer", ",", "entity", "[", "'orig_text'", "]", ")", "\n", "document_entities", ".", "append", "(", "(", "entity", "[", "'orig_text'", "]", ",", "entity_start_position", ",", "entity_end_position", ")", ")", "# ('Trump', 10, 10)", "\n", "\n", "# match query to passage entities", "\n", "", "query_entities", "=", "match_query_entities", "(", "query_tokens", ",", "document_entities", ",", "document_tokens", ")", "# [('trump', 10, 10)]", "\n", "\n", "tokenization_result", ".", "append", "(", "{", "\n", "'id'", ":", "example", ".", "qas_id", ",", "\n", "'query_tokens'", ":", "query_tokens", ",", "\n", "'query_subtokens'", ":", "query_subtokens", ",", "\n", "'query_sub_to_ori_index'", ":", "query_sub_to_ori_index", ",", "\n", "'query_entities'", ":", "query_entities", ",", "\n", "'document_tokens'", ":", "document_tokens", ",", "\n", "'document_subtokens'", ":", "document_subtokens", ",", "\n", "'document_entities'", ":", "document_entities", ",", "\n", "'document_sub_to_ori_index'", ":", "document_sub_to_ori_index", ",", "\n", "}", ")", "\n", "\n", "", "return", "tokenization_result", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization.main": [[265, 306], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.exists", "os.mkdir", "tokenization.FullTokenizer", "do_tokenization.read_record_examples", "do_tokenization.tokenization_on_examples", "logger.info", "do_tokenization.read_record_examples", "do_tokenization.tokenization_on_examples", "logger.info", "open", "pickle.dump", "open", "pickle.dump", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization.read_record_examples", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.tokenization_on_examples", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_record.do_tokenization.read_record_examples", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.tokenization_on_examples"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "default", "=", "'tokens'", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output directory to dump tokenization results.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_file\"", ",", "default", "=", "'../../data/ReCoRD/train.json'", ",", "type", "=", "str", ",", "help", "=", "\"ReCoRD json for training. E.g., train-v1.1.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--predict_file\"", ",", "default", "=", "'../../data/ReCoRD/dev.json'", ",", "type", "=", "str", ",", "\n", "help", "=", "\"ReCoRD json for predictions. E.g., dev-v1.1.json or test-v1.1.json\"", ")", "\n", "# parser.add_argument(\"--do_lower_case\", default=False, action='store_true',", "\n", "#                     help=\"Whether to lower case the input text. Should be True for uncased \"", "\n", "#                          \"models and False for cased models.\")", "\n", "# parser.add_argument('--dump_token', action='store_true', help='whether dump the token-level tokenization result')", "\n", "# parser.add_argument('--dump_subtoken', action='store_true', help='whether dump the subtoken-level tokenization result, with its mapping with token-level result')", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# make output directory if not exist", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "args", ".", "output_dir", ")", "\n", "\n", "# We do both cased and uncased tokenization", "\n", "", "for", "do_lower_case", "in", "(", "True", ",", "False", ")", ":", "\n", "        ", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "'vocab.{}.txt'", ".", "format", "(", "'uncased'", "if", "do_lower_case", "else", "'cased'", ")", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "read_record_examples", "(", "input_file", "=", "args", ".", "train_file", ",", "is_training", "=", "True", ")", "\n", "train_tokenization_result", "=", "tokenization_on_examples", "(", "\n", "examples", "=", "train_examples", ",", "\n", "tokenizer", "=", "tokenizer", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'train.tokenization.{}.data'", ".", "format", "(", "'uncased'", "if", "do_lower_case", "else", "'cased'", ")", ")", ",", "'wb'", ")", "as", "fout", ":", "\n", "            ", "pickle", ".", "dump", "(", "train_tokenization_result", ",", "fout", ")", "\n", "\n", "", "logger", ".", "info", "(", "'Finished {} tokenization for train set.'", ".", "format", "(", "'uncased'", "if", "do_lower_case", "else", "'cased'", ")", ")", "\n", "\n", "eval_examples", "=", "read_record_examples", "(", "input_file", "=", "args", ".", "predict_file", ",", "is_training", "=", "False", ")", "\n", "eval_tokenization_result", "=", "tokenization_on_examples", "(", "\n", "examples", "=", "eval_examples", ",", "\n", "tokenizer", "=", "tokenizer", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'dev.tokenization.{}.data'", ".", "format", "(", "'uncased'", "if", "do_lower_case", "else", "'cased'", ")", ")", ",", "'wb'", ")", "as", "fout", ":", "\n", "            ", "pickle", ".", "dump", "(", "eval_tokenization_result", ",", "fout", ")", "\n", "\n", "", "logger", ".", "info", "(", "'Finished {} tokenization for dev set.'", ".", "format", "(", "'uncased'", "if", "do_lower_case", "else", "'cased'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.FullTokenizer.__init__": [[104, 108], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.load_vocab"], ["        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.FullTokenizer.tokenize": [[109, 116], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["", "class", "FullTokenizer", "(", "object", ")", ":", "\n", "    ", "\"\"\"Runs end-to-end tokenziation.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.FullTokenizer.convert_tokens_to_ids": [[117, 119], ["tokenization.FullTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_tokens_to_ids"], ["\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer.__init__": [[124, 131], ["None"], "methods", ["None"], ["", "", "return", "split_tokens", "\n", "\n", "", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n", "", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer.tokenize": [[132, 153], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._tokenize_chinese_chars", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_split_on_punc"], ["\n", "", "", "class", "CharTokenizer", "(", "object", ")", ":", "\n", "    ", "\"\"\"Runs end-to-end tokenziation.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "text", ".", "lower", "(", ")", ".", "split", "(", "\" \"", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n", "", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n", "", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_strip_accents": [[154, 164], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.normalize"], ["\n", "\n", "", "", "class", "BasicTokenizer", "(", "object", ")", ":", "\n", "    ", "\"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n            do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._run_split_on_punc": [[165, 184], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_punctuation"], ["self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "_never_lowercase", "=", "[", "'[UNK]'", ",", "'[SEP]'", ",", "'[PAD]'", ",", "'[CLS]'", ",", "'[MASK]'", "]", "\n", "\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "_never_lowercase", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._tokenize_chinese_chars": [[185, 197], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._is_chinese_char"], ["                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "if", "token", "in", "self", ".", "_never_lowercase", ":", "\n", "                ", "split_tokens", ".", "extend", "(", "[", "token", "]", ")", "\n", "", "else", ":", "\n", "                ", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n", "", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._is_chinese_char": [[198, 219], ["None"], "methods", ["None"], ["output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n", "", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._clean_text": [[220, 232], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_whitespace", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_control"], ["", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n", "", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.__init__": [[237, 241], ["None"], "methods", ["None"], ["", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n", "", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize": [[242, 294], ["tokenization.convert_to_unicode", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize"], ["#     https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n", "", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n", "\n", "", "", "class", "WordpieceTokenizer", "(", "object", ")", ":", "\n", "    ", "\"\"\"Runs WordPiece tokenziation.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode": [[26, 44], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode", "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode"], ["def", "convert_to_unicode", "(", "text", ")", ":", "\n", "    ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text": [[46, 67], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "    ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "            ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "            ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "            ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.load_vocab": [[69, 82], ["collections.OrderedDict", "open", "tokenization.convert_to_unicode", "token.strip.strip", "reader.readline"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "fin", "=", "open", "(", "vocab_file", ")", "\n", "for", "num", ",", "line", "in", "enumerate", "(", "fin", ")", ":", "\n", "        ", "items", "=", "convert_to_unicode", "(", "line", ".", "strip", "(", ")", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "len", "(", "items", ")", ">", "2", ":", "\n", "            ", "break", "\n", "", "token", "=", "items", "[", "0", "]", "\n", "index", "=", "items", "[", "1", "]", "if", "len", "(", "items", ")", "==", "2", "else", "num", "\n", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "int", "(", "index", ")", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.convert_tokens_to_ids": [[84, 90], ["ids.append"], "function", ["None"], ["", "def", "convert_by_vocab", "(", "vocab", ",", "items", ")", ":", "\n", "    ", "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "        ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.whitespace_tokenize": [[92, 99], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "vocab", ",", "tokens", ")", "\n", "\n", "\n", "", "def", "convert_ids_to_tokens", "(", "inv_vocab", ",", "ids", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "inv_vocab", ",", "ids", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_whitespace": [[296, 306], ["unicodedata.category"], "function", ["None"], ["\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_control": [[308, 318], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization._is_punctuation": [[320, 334], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.SQuADExample.__init__": [[38, 55], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "qas_id", ",", "\n", "question_text", ",", "\n", "doc_tokens", ",", "\n", "passage_entities", ",", "\n", "orig_answer_text", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ")", ":", "\n", "        ", "self", ".", "passage_entities", "=", "passage_entities", "\n", "self", ".", "qas_id", "=", "qas_id", "\n", "self", ".", "question_text", "=", "question_text", "\n", "self", ".", "doc_tokens", "=", "doc_tokens", "\n", "self", ".", "orig_answer_text", "=", "orig_answer_text", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "\n", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.SQuADExample.__str__": [[56, 58], ["do_tokenization.SQuADExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.SQuADExample.__repr__"], ["\n", "", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"\"", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.SQuADExample.__repr__": [[59, 70], ["tokenization.printable_text", "tokenization.printable_text"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.printable_text"], ["s", "+=", "\"qas_id: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "self", ".", "qas_id", ")", ")", "\n", "s", "+=", "\", question_text: %s\"", "%", "(", "\n", "tokenization", ".", "printable_text", "(", "self", ".", "question_text", ")", ")", "\n", "s", "+=", "\", doc_tokens: [%s]\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "doc_tokens", ")", ")", "\n", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", start_position: %d\"", "%", "(", "self", ".", "start_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "            ", "s", "+=", "\", end_position: %d\"", "%", "(", "self", ".", "end_position", ")", "\n", "", "return", "s", "\n", "\n", "# the tokenization process when reading examples", "\n", "", "", "def", "read_record_examples", "(", "input_file", ",", "is_training", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.read_squad_examples": [[72, 153], ["open", "json.load", "ord", "do_tokenization.read_squad_examples.is_whitespace"], "function", ["None"], ["with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "        ", "input_data", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "\n", "", "def", "is_whitespace", "(", "c", ")", ":", "\n", "        ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "examples", "=", "[", "]", "\n", "for", "entry", "in", "input_data", ":", "\n", "        ", "paragraph_text", "=", "entry", "[", "\"passage\"", "]", "[", "\"text\"", "]", ".", "replace", "(", "'\\xa0'", ",", "' '", ")", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "            ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "                ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "prev_is_whitespace", ":", "\n", "                    ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                    ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "# load entities in passage", "\n", "", "passage_entities", "=", "[", "]", "\n", "for", "entity", "in", "entry", "[", "'passage'", "]", "[", "'entities'", "]", ":", "\n", "            ", "entity_start_offset", "=", "entity", "[", "'start'", "]", "\n", "entity_end_offset", "=", "entity", "[", "'end'", "]", "\n", "if", "entity_end_offset", "<", "entity_start_offset", ":", "# some error labeled entities in record dataset", "\n", "                ", "continue", "\n", "", "entity_text", "=", "paragraph_text", "[", "entity_start_offset", ":", "entity_end_offset", "+", "1", "]", "\n", "passage_entities", ".", "append", "(", "{", "'orig_text'", ":", "entity_text", ",", "\n", "'start_position'", ":", "char_to_word_offset", "[", "entity_start_offset", "]", ",", "\n", "'end_position'", ":", "char_to_word_offset", "[", "entity_end_offset", "]", "}", ")", "\n", "\n", "", "for", "qa", "in", "entry", "[", "\"qas\"", "]", ":", "\n", "            ", "qas_id", "=", "qa", "[", "\"id\"", "]", "\n", "question_text", "=", "qa", "[", "\"query\"", "]", ".", "replace", "(", "'\\xa0'", ",", "' '", ")", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "if", "is_training", ":", "\n", "# if len(qa[\"answers\"]) != 1:", "\n", "#     raise ValueError(", "\n", "#         \"For training, each question should have exactly 1 answer.\")", "\n", "                ", "answer", "=", "qa", "[", "\"answers\"", "]", "[", "0", "]", "\n", "orig_answer_text", "=", "answer", "[", "\"text\"", "]", "\n", "answer_offset", "=", "answer", "[", "\"start\"", "]", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "answer_length", "-", "1", "]", "\n", "# Only add answers where the text can be exactly recovered from the", "\n", "# document. If this CAN'T happen it's likely due to weird Unicode", "\n", "# stuff so we will just skip the example.", "\n", "#", "\n", "# Note that this means for training mode, every example is NOT", "\n", "# guaranteed to be preserved.", "\n", "actual_text", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "start_position", ":", "(", "end_position", "+", "1", ")", "]", ")", "\n", "cleaned_answer_text", "=", "\" \"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "if", "actual_text", ".", "find", "(", "cleaned_answer_text", ")", "==", "-", "1", ":", "\n", "                    ", "logger", ".", "warning", "(", "\"Could not find answer: '%s' vs. '%s'\"", ",", "\n", "actual_text", ",", "cleaned_answer_text", ")", "\n", "continue", "\n", "\n", "", "", "example", "=", "ReCoRDExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "\n", "passage_entities", "=", "passage_entities", ",", "\n", "orig_answer_text", "=", "orig_answer_text", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "", "", "return", "examples", "\n", "\n", "", "def", "_improve_entity_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_entity_text", ")", ":", "\n", "    ", "\"\"\"Returns token-level tokenized entity spans that better match the annotated entity.\"\"\"", "\n", "tok_entity_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "basic_tokenizer", ".", "tokenize", "(", "orig_entity_text", ")", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization._improve_entity_span": [[154, 166], ["range", "tokenizer.basic_tokenizer.tokenize", "range"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "        ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "text_span", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_entity_text", ":", "\n", "                ", "return", "(", "new_start", ",", "new_end", ")", "\n", "\n", "", "", "", "return", "(", "input_start", ",", "input_end", ")", "\n", "\n", "", "def", "_is_real_subspan", "(", "start", ",", "end", ",", "other_start", ",", "other_end", ")", ":", "\n", "    ", "return", "(", "start", ">=", "other_start", "and", "end", "<", "other_end", ")", "or", "(", "start", ">", "other_start", "and", "end", "<=", "other_end", ")", "\n", "\n", "", "def", "match_query_entities", "(", "query_tokens", ",", "document_entities", ",", "document_tokens", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization._is_real_subspan": [[167, 169], ["None"], "function", ["None"], ["# transform query_tokens list into a whitespace separated string", "\n", "    ", "query_string", "=", "\" \"", ".", "join", "(", "query_tokens", ")", "\n", "offset_to_tid_map", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.match_query_entities": [[170, 207], ["set", "offset_to_tid_map.append", "set.add", "len", "len", "query_string.find", "any", "no_subspan_results.append", "set", "results.append", "len", "entity_string.count", "do_tokenization._is_real_subspan"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization._is_real_subspan"], ["tid", "=", "0", "\n", "for", "char", "in", "query_string", ":", "\n", "        ", "offset_to_tid_map", ".", "append", "(", "tid", ")", "\n", "if", "char", "==", "' '", ":", "\n", "            ", "tid", "+=", "1", "\n", "\n", "# transform entity_tokens into whitespace separated strings", "\n", "", "", "entity_strings", "=", "set", "(", ")", "\n", "for", "document_entity", "in", "document_entities", ":", "\n", "        ", "entity_tokens", "=", "document_tokens", "[", "document_entity", "[", "1", "]", ":", "document_entity", "[", "2", "]", "+", "1", "]", "\n", "entity_strings", ".", "add", "(", "\" \"", ".", "join", "(", "entity_tokens", ")", ")", "\n", "\n", "# do matching", "\n", "", "results", "=", "[", "]", "\n", "for", "entity_string", "in", "entity_strings", ":", "\n", "        ", "start", "=", "0", "\n", "while", "True", ":", "\n", "            ", "pos", "=", "query_string", ".", "find", "(", "entity_string", ",", "start", ")", "\n", "if", "pos", "==", "-", "1", ":", "\n", "                ", "break", "\n", "", "token_start", ",", "token_end", "=", "offset_to_tid_map", "[", "pos", "]", ",", "offset_to_tid_map", "[", "pos", "]", "+", "entity_string", ".", "count", "(", "' '", ")", "\n", "# assure the match is not partial match (eg. \"ville\" matches to \"danville\")", "\n", "if", "\" \"", ".", "join", "(", "query_tokens", "[", "token_start", ":", "token_end", "+", "1", "]", ")", "==", "entity_string", ":", "\n", "                ", "results", ".", "append", "(", "(", "token_start", ",", "token_end", ")", ")", "\n", "", "start", "=", "pos", "+", "len", "(", "entity_string", ")", "\n", "\n", "# filter out a result span if it's a subspan of another span", "\n", "", "", "no_subspan_results", "=", "[", "]", "\n", "for", "result", "in", "results", ":", "\n", "        ", "if", "not", "any", "(", "[", "_is_real_subspan", "(", "result", "[", "0", "]", ",", "result", "[", "1", "]", ",", "other_result", "[", "0", "]", ",", "other_result", "[", "1", "]", ")", "for", "other_result", "in", "results", "]", ")", ":", "\n", "            ", "no_subspan_results", ".", "append", "(", "(", "\" \"", ".", "join", "(", "query_tokens", "[", "result", "[", "0", "]", ":", "result", "[", "1", "]", "+", "1", "]", ")", ",", "result", "[", "0", "]", ",", "result", "[", "1", "]", ")", ")", "\n", "", "", "assert", "len", "(", "no_subspan_results", ")", "==", "len", "(", "set", "(", "no_subspan_results", ")", ")", "\n", "\n", "return", "no_subspan_results", "\n", "\n", "\n", "# the further tokenization process when generating features", "\n", "", "def", "tokenization_on_examples", "(", "examples", ",", "tokenizer", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.tokenization_on_examples": [[209, 270], ["tqdm.tqdm", "tokenizer.basic_tokenizer.tokenize", "enumerate", "do_tokenization.match_query_entities", "tokenization_result.append", "tokenizer.wordpiece_tokenizer.tokenize", "tokenizer.basic_tokenizer.tokenize", "document_up_to_ori_index.append", "do_tokenization._improve_entity_span", "document_entities.append", "entities_tokens.append", "entities_tokens.append", "query_subtokens.append", "query_sub_to_ori_index.append", "len", "tokenizer.wordpiece_tokenizer.tokenize", "document_tokens.append", "tokenizer.basic_tokenizer.tokenize", "document_subtokens.append", "document_sub_to_ori_index.append", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.match_query_entities", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization._improve_entity_span", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.WordpieceTokenizer.tokenize"], ["    ", "tokenization_result", "=", "[", "]", "\n", "for", "example", "in", "tqdm", "(", "examples", ")", ":", "\n", "# do tokenization on raw question text", "\n", "        ", "query_subtokens", "=", "[", "]", "\n", "query_sub_to_ori_index", "=", "[", "]", "# mapping from sub-token index to token index", "\n", "query_tokens", "=", "tokenizer", ".", "basic_tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "for", "index", ",", "token", "in", "enumerate", "(", "query_tokens", ")", ":", "\n", "            ", "for", "sub_token", "in", "tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "query_subtokens", ".", "append", "(", "sub_token", ")", "\n", "query_sub_to_ori_index", ".", "append", "(", "index", ")", "\n", "\n", "# do tokenization on whitespace tokenized document", "\n", "", "", "document_tokens", "=", "[", "]", "\n", "document_subtokens", "=", "[", "]", "\n", "document_sub_to_ori_index", "=", "[", "]", "\n", "document_up_to_ori_index", "=", "[", "]", "# map unpunc token index to tokenized token index", "\n", "for", "unpunc_tokenized_tokens", "in", "example", ".", "doc_tokens", ":", "\n", "            ", "tokens", "=", "tokenizer", ".", "basic_tokenizer", ".", "tokenize", "(", "unpunc_tokenized_tokens", ")", "# do punctuation tokenization", "\n", "document_up_to_ori_index", ".", "append", "(", "len", "(", "document_tokens", ")", ")", "\n", "for", "token", "in", "tokens", ":", "\n", "                ", "for", "sub_token", "in", "tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                    ", "document_subtokens", ".", "append", "(", "sub_token", ")", "\n", "document_sub_to_ori_index", ".", "append", "(", "len", "(", "document_tokens", ")", ")", "\n", "", "document_tokens", ".", "append", "(", "token", ")", "\n", "\n", "# generate token-level document entity index", "\n", "", "", "document_entities", "=", "[", "]", "\n", "for", "entity", "in", "example", ".", "passage_entities", ":", "\n", "            ", "entity_start_position", "=", "document_up_to_ori_index", "[", "entity", "[", "'start_position'", "]", "]", "\n", "entity_end_position", "=", "None", "\n", "if", "entity", "[", "'end_position'", "]", "<", "len", "(", "example", ".", "doc_tokens", ")", "-", "1", ":", "\n", "                ", "entity_end_position", "=", "document_up_to_ori_index", "[", "entity", "[", "'end_position'", "]", "+", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "                ", "entity_end_position", "=", "len", "(", "document_tokens", ")", "-", "1", "\n", "", "(", "entity_start_position", ",", "entity_end_position", ")", "=", "_improve_entity_span", "(", "\n", "document_tokens", ",", "entity_start_position", ",", "entity_end_position", ",", "tokenizer", ",", "entity", "[", "'orig_text'", "]", ")", "\n", "document_entities", ".", "append", "(", "(", "entity", "[", "'orig_text'", "]", ",", "entity_start_position", ",", "entity_end_position", ")", ")", "# ('Trump', 10, 10)", "\n", "\n", "# match query to passage entities", "\n", "", "query_entities", "=", "match_query_entities", "(", "query_tokens", ",", "document_entities", ",", "document_tokens", ")", "# [('trump', 10, 10)]", "\n", "\n", "tokenization_result", ".", "append", "(", "{", "\n", "'id'", ":", "example", ".", "qas_id", ",", "\n", "'query_tokens'", ":", "query_tokens", ",", "\n", "'query_subtokens'", ":", "query_subtokens", ",", "\n", "'query_sub_to_ori_index'", ":", "query_sub_to_ori_index", ",", "\n", "'query_entities'", ":", "query_entities", ",", "\n", "'document_tokens'", ":", "document_tokens", ",", "\n", "'document_subtokens'", ":", "document_subtokens", ",", "\n", "'document_entities'", ":", "document_entities", ",", "\n", "'document_sub_to_ori_index'", ":", "document_sub_to_ori_index", ",", "\n", "}", ")", "\n", "\n", "", "return", "tokenization_result", "\n", "\n", "\n", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "default", "=", "'tokens'", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output directory to dump tokenization results.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_file\"", ",", "default", "=", "'../../data/ReCoRD/train.json'", ",", "type", "=", "str", ",", "help", "=", "\"ReCoRD json for training. E.g., train-v1.1.json\"", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.main": [[272, 313], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.exists", "os.mkdir", "tokenization.FullTokenizer", "do_tokenization.read_squad_examples", "do_tokenization.tokenization_on_examples", "logger.info", "do_tokenization.read_squad_examples", "do_tokenization.tokenization_on_examples", "logger.info", "open", "pickle.dump", "open", "pickle.dump", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.read_squad_examples", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.tokenization_on_examples", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.read_squad_examples", "home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.do_tokenization.tokenization_on_examples"], ["help", "=", "\"ReCoRD json for predictions. E.g., dev-v1.1.json or test-v1.1.json\"", ")", "\n", "# parser.add_argument(\"--do_lower_case\", default=False, action='store_true',", "\n", "#                     help=\"Whether to lower case the input text. Should be True for uncased \"", "\n", "#                          \"models and False for cased models.\")", "\n", "# parser.add_argument('--dump_token', action='store_true', help='whether dump the token-level tokenization result')", "\n", "# parser.add_argument('--dump_subtoken', action='store_true', help='whether dump the subtoken-level tokenization result, with its mapping with token-level result')", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# make output directory if not exist", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "args", ".", "output_dir", ")", "\n", "\n", "# We do both cased and uncased tokenization", "\n", "", "for", "do_lower_case", "in", "(", "True", ",", "False", ")", ":", "\n", "        ", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "'vocab.{}.txt'", ".", "format", "(", "'uncased'", "if", "do_lower_case", "else", "'cased'", ")", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "read_record_examples", "(", "input_file", "=", "args", ".", "train_file", ",", "is_training", "=", "True", ")", "\n", "train_tokenization_result", "=", "tokenization_on_examples", "(", "\n", "examples", "=", "train_examples", ",", "\n", "tokenizer", "=", "tokenizer", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'train.tokenization.{}.data'", ".", "format", "(", "'uncased'", "if", "do_lower_case", "else", "'cased'", ")", ")", ",", "'wb'", ")", "as", "fout", ":", "\n", "            ", "pickle", ".", "dump", "(", "train_tokenization_result", ",", "fout", ")", "\n", "\n", "", "logger", ".", "info", "(", "'Finished {} tokenization for train set.'", ".", "format", "(", "'uncased'", "if", "do_lower_case", "else", "'cased'", ")", ")", "\n", "\n", "eval_examples", "=", "read_record_examples", "(", "input_file", "=", "args", ".", "predict_file", ",", "is_training", "=", "False", ")", "\n", "eval_tokenization_result", "=", "tokenization_on_examples", "(", "\n", "examples", "=", "eval_examples", ",", "\n", "tokenizer", "=", "tokenizer", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'dev.tokenization.{}.data'", ".", "format", "(", "'uncased'", "if", "do_lower_case", "else", "'cased'", ")", ")", ",", "'wb'", ")", "as", "fout", ":", "\n", "            ", "pickle", ".", "dump", "(", "eval_tokenization_result", ",", "fout", ")", "\n", "\n", "", "logger", ".", "info", "(", "'Finished {} tokenization for dev set.'", ".", "format", "(", "'uncased'", "if", "do_lower_case", "else", "'cased'", ")", ")", "\n", "\n", "", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "main", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.ner_tagging_squad.tagging.parse_args": [[36, 44], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "default", "=", "'output'", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output directory to store tagging results.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_file\"", ",", "default", "=", "'../../data/SQuAD/train-v1.1.json'", ",", "type", "=", "str", ",", "help", "=", "\"SQuAD json for training. E.g., train-v1.1.json\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--predict_file\"", ",", "default", "=", "'../../data/SQuAD/dev-v1.1.json'", ",", "type", "=", "str", ",", "\n", "help", "=", "\"SQuAD json for predictions. E.g., dev-v1.1.json or test-v1.1.json\"", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.ner_tagging_squad.tagging.parse_output": [[47, 71], ["entities.append", "entities.append", "entities.append"], "function", ["None"], ["", "def", "parse_output", "(", "text", ",", "tagging_output", ",", "begin_offset", "=", "0", ")", ":", "\n", "    ", "entities", "=", "[", "]", "\n", "select_states", "=", "[", "'ORGANIZATION'", ",", "'PERSON'", ",", "'MISC'", ",", "'LOCATION'", "]", "\n", "for", "sent", "in", "tagging_output", "[", "'sentences'", "]", ":", "\n", "        ", "state", "=", "'O'", "\n", "start_pos", ",", "end_pos", "=", "-", "1", ",", "-", "1", "\n", "for", "token", "in", "sent", "[", "'tokens'", "]", ":", "\n", "            ", "tag", "=", "token", "[", "'ner'", "]", "\n", "if", "tag", "==", "'O'", "and", "state", "!=", "'O'", ":", "\n", "                ", "if", "state", "in", "select_states", ":", "\n", "                    ", "entities", ".", "append", "(", "{", "'text'", ":", "text", "[", "begin_offset", "+", "start_pos", ":", "begin_offset", "+", "end_pos", "]", ",", "'start'", ":", "begin_offset", "+", "start_pos", ",", "'end'", ":", "begin_offset", "+", "end_pos", "-", "1", "}", ")", "\n", "", "state", "=", "'O'", "\n", "", "elif", "tag", "!=", "'O'", ":", "\n", "                ", "if", "state", "==", "tag", ":", "\n", "                    ", "end_pos", "=", "token", "[", "'characterOffsetEnd'", "]", "\n", "", "else", ":", "\n", "                    ", "if", "state", "in", "select_states", ":", "\n", "                        ", "entities", ".", "append", "(", "{", "'text'", ":", "text", "[", "begin_offset", "+", "start_pos", ":", "begin_offset", "+", "end_pos", "]", ",", "'start'", ":", "begin_offset", "+", "start_pos", ",", "'end'", ":", "begin_offset", "+", "end_pos", "-", "1", "}", ")", "\n", "", "state", "=", "tag", "\n", "start_pos", "=", "token", "[", "'characterOffsetBegin'", "]", "\n", "end_pos", "=", "token", "[", "'characterOffsetEnd'", "]", "\n", "", "", "", "if", "state", "in", "select_states", ":", "\n", "            ", "entities", ".", "append", "(", "{", "'text'", ":", "text", "[", "begin_offset", "+", "start_pos", ":", "begin_offset", "+", "end_pos", "]", ",", "'start'", ":", "begin_offset", "+", "start_pos", ",", "'end'", ":", "begin_offset", "+", "end_pos", "-", "1", "}", ")", "\n", "", "", "return", "entities", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.ner_tagging_squad.tagging.tagging": [[72, 99], ["tqdm.tqdm", "logger.info", "tqdm.tqdm", "nlp.annotate", "tqdm.tqdm", "urllib.parse.quote", "len", "tagging.parse_output", "logger.info", "logger.info", "nlp.annotate", "context.strip", "urllib.parse.quote", "len", "tagging.parse_output", "logger.info", "logger.info", "len", "len", "question.strip", "context.lstrip", "len", "len", "context.lstrip"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.ner_tagging_squad.tagging.parse_output", "home.repos.pwc.inspect_result.baidu_DuReader.ner_tagging_squad.tagging.parse_output"], ["", "def", "tagging", "(", "dataset", ",", "nlp", ")", ":", "\n", "    ", "skip_context_cnt", ",", "skip_question_cnt", "=", "0", ",", "0", "\n", "for", "article", "in", "tqdm", "(", "dataset", "[", "'data'", "]", ")", ":", "\n", "        ", "for", "paragraph", "in", "tqdm", "(", "article", "[", "'paragraphs'", "]", ")", ":", "\n", "            ", "context", "=", "paragraph", "[", "'context'", "]", "\n", "context_tagging_output", "=", "nlp", ".", "annotate", "(", "urllib", ".", "parse", ".", "quote", "(", "context", ")", ",", "properties", "=", "{", "'annotators'", ":", "'ner'", ",", "'outputFormat'", ":", "'json'", "}", ")", "\n", "# assert the context length is not changed", "\n", "if", "len", "(", "context", ".", "strip", "(", ")", ")", "==", "context_tagging_output", "[", "'sentences'", "]", "[", "-", "1", "]", "[", "'tokens'", "]", "[", "-", "1", "]", "[", "'characterOffsetEnd'", "]", ":", "\n", "                ", "context_entities", "=", "parse_output", "(", "context", ",", "context_tagging_output", ",", "len", "(", "context", ")", "-", "len", "(", "context", ".", "lstrip", "(", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "context_entities", "=", "[", "]", "\n", "skip_context_cnt", "+=", "1", "\n", "logger", ".", "info", "(", "'Skipped context due to offset mismatch:'", ")", "\n", "logger", ".", "info", "(", "context", ")", "\n", "", "paragraph", "[", "'context_entities'", "]", "=", "context_entities", "\n", "for", "qa", "in", "tqdm", "(", "paragraph", "[", "'qas'", "]", ")", ":", "\n", "                ", "question", "=", "qa", "[", "'question'", "]", "\n", "question_tagging_output", "=", "nlp", ".", "annotate", "(", "urllib", ".", "parse", ".", "quote", "(", "question", ")", ",", "properties", "=", "{", "'annotators'", ":", "'ner'", ",", "'outputFormat'", ":", "'json'", "}", ")", "\n", "if", "len", "(", "question", ".", "strip", "(", ")", ")", "==", "question_tagging_output", "[", "'sentences'", "]", "[", "-", "1", "]", "[", "'tokens'", "]", "[", "-", "1", "]", "[", "'characterOffsetEnd'", "]", ":", "\n", "                    ", "question_entities", "=", "parse_output", "(", "question", ",", "question_tagging_output", ",", "len", "(", "context", ")", "-", "len", "(", "context", ".", "lstrip", "(", ")", ")", ")", "\n", "", "else", ":", "\n", "                    ", "question_entities", "=", "[", "]", "\n", "skip_question_cnt", "+=", "1", "\n", "logger", ".", "info", "(", "'Skipped question due to offset mismatch:'", ")", "\n", "logger", ".", "info", "(", "question", ")", "\n", "", "qa", "[", "'question_entities'", "]", "=", "question_entities", "\n", "", "", "", "logger", ".", "info", "(", "'In total, {} contexts and {} questions are skipped...'", ".", "format", "(", "skip_context_cnt", ",", "skip_question_cnt", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.retrieve_nell.retrieve.preprocess_nell_ent_name": [[41, 48], ["set", "raw_name.split", "ent_name.startswith", "all", "filter", "ent_name.split", "len", "ent_name.split"], "function", ["None"], ["# initialize mapping from offset id to wn18 synset name", "\n", "offset_to_wn18name_dict", "=", "{", "}", "\n", "fin", "=", "open", "(", "'wordnet-mlj12-definitions.txt'", ")", "\n", "for", "line", "in", "fin", ":", "\n", "        ", "info", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "offset_str", ",", "synset_name", "=", "info", "[", "0", "]", ",", "info", "[", "1", "]", "\n", "offset_to_wn18name_dict", "[", "offset_str", "]", "=", "synset_name", "\n", "", "logger", ".", "info", "(", "'Finished loading wn18 definition file.'", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.retrieve_nell.retrieve.preprocess_record_ent_name": [[50, 52], ["filter"], "function", ["None"], ["\n", "# load pickled samples", "\n", "logger", ".", "info", "(", "'Begin to load tokenization results...'", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.retrieve_nell.retrieve.main": [[53, 188], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "set", "logger.info", "open", "logger.info", "logger.info", "pickle.load", "pickle.load", "logger.info", "set", "logger.info", "tqdm.tqdm", "logger.info", "logger.info", "collections.namedtuple", "zip", "logger.info", "os.path.exists", "os.mkdir", "open", "line.strip.strip", "line.strip.split", "open", "open", "tqdm.tqdm", "pickle.dump", "line.strip.strip", "set.add", "retrieve.preprocess_nell_ent_name", "nell_ent_to_cpt[].add", "nell_ent_to_fullname[].add", "set.add", "set.add", "set", "set", "cpt.update", "nell_ent.update", "zip", "all_outputs.append", "open", "float", "set", "set", "retrieve.preprocess_record_ent_name", "retrieve.preprocess_record_ent_name", "nltk.corpus.wordnet.morphy", "retrieve.preprocess_record_ent_name", "doc_entities.append", "retrieve.preprocess_record_ent_name", "query_entities.append", "os.path.join", "cpt.update", "nell_ent.update", "collections.namedtuple.", "collections.namedtuple.", "entities_final.append", "set", "set", "len", "other_trt.entity_string.endswith", "new_nell_cpt_set.update", "new_nell_ent_set.update", "list", "list"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.retrieve_nell.retrieve.preprocess_nell_ent_name", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.retrieve_nell.retrieve.preprocess_record_ent_name", "home.repos.pwc.inspect_result.baidu_DuReader.retrieve_nell.retrieve.preprocess_record_ent_name", "home.repos.pwc.inspect_result.baidu_DuReader.retrieve_nell.retrieve.preprocess_record_ent_name", "home.repos.pwc.inspect_result.baidu_DuReader.retrieve_nell.retrieve.preprocess_record_ent_name"], ["train_samples", "=", "pickle", ".", "load", "(", "open", "(", "args", ".", "train_token", ",", "'rb'", ")", ")", "\n", "dev_samples", "=", "pickle", ".", "load", "(", "open", "(", "args", ".", "eval_token", ",", "'rb'", ")", ")", "\n", "logger", ".", "info", "(", "'Finished loading tokenization results.'", ")", "\n", "\n", "# build token set", "\n", "all_token_set", "=", "set", "(", ")", "\n", "for", "sample", "in", "train_samples", "+", "dev_samples", ":", "\n", "        ", "for", "token", "in", "sample", "[", "'query_tokens'", "]", "+", "sample", "[", "'document_tokens'", "]", ":", "\n", "            ", "all_token_set", ".", "add", "(", "token", ")", "\n", "", "", "logger", ".", "info", "(", "'Finished making tokenization results into token set.'", ")", "\n", "\n", "# load stopwords", "\n", "stopwords", "=", "set", "(", "nltk", ".", "corpus", ".", "stopwords", ".", "words", "(", "'english'", ")", ")", "\n", "logger", ".", "info", "(", "'Finished loading stopwords list.'", ")", "\n", "\n", "# retrive synsets", "\n", "logger", ".", "info", "(", "'Begin to retrieve synsets...'", ")", "\n", "token2synset", "=", "dict", "(", ")", "\n", "stopword_cnt", "=", "0", "\n", "punctuation_cnt", "=", "0", "\n", "for", "token", "in", "tqdm", "(", "all_token_set", ")", ":", "\n", "        ", "if", "token", "in", "set", "(", "string", ".", "punctuation", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'{} is punctuation, skipped!'", ".", "format", "(", "token", ")", ")", "\n", "punctuation_cnt", "+=", "1", "\n", "continue", "\n", "", "if", "args", ".", "no_stopwords", "and", "token", "in", "stopwords", ":", "\n", "            ", "logger", ".", "info", "(", "'{} is stopword, skipped!'", ".", "format", "(", "token", ")", ")", "\n", "stopword_cnt", "+=", "1", "\n", "continue", "\n", "", "if", "args", ".", "ignore_length", ">", "0", "and", "len", "(", "token", ")", "<=", "args", ".", "ignore_length", ":", "\n", "            ", "logger", ".", "info", "(", "'{} is too short, skipped!'", ".", "format", "(", "token", ")", ")", "\n", "continue", "\n", "", "synsets", "=", "wn", ".", "synsets", "(", "token", ")", "\n", "wn18synset_names", "=", "[", "]", "\n", "for", "synset", "in", "synsets", ":", "\n", "            ", "offset_str", "=", "str", "(", "synset", ".", "offset", "(", ")", ")", ".", "zfill", "(", "8", ")", "\n", "if", "offset_str", "in", "offset_to_wn18name_dict", ":", "\n", "                ", "wn18synset_names", ".", "append", "(", "offset_to_wn18name_dict", "[", "offset_str", "]", ")", "\n", "", "", "if", "len", "(", "wn18synset_names", ")", ">", "0", ":", "\n", "            ", "token2synset", "[", "token", "]", "=", "wn18synset_names", "\n", "", "", "logger", ".", "info", "(", "'Finished retrieving sysnets.'", ")", "\n", "logger", ".", "info", "(", "'{} / {} tokens retrieved at lease 1 synset. {} stopwords and {} punctuations skipped.'", ".", "format", "(", "len", "(", "token2synset", ")", ",", "len", "(", "all_token_set", ")", ",", "stopword_cnt", ",", "punctuation_cnt", ")", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'retrived_synsets.data'", ")", ",", "'wb'", ")", "as", "fout", ":", "\n", "        ", "pickle", ".", "dump", "(", "token2synset", ",", "fout", ")", "\n", "", "logger", ".", "info", "(", "'Finished dumping retrieved synsets.'", ")", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "main", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._tokenize_chinese_chars": [[19, 64], ["ord", "output.append", "evaluate._tokenize_chinese_chars._is_chinese_char"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.tokenization_squad.tokenization.BasicTokenizer._is_chinese_char"], ["def", "_tokenize_chinese_chars", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    :param text: input text, unicode string\n    :return:\n        tokenized text, list\n    \"\"\"", "\n", "\n", "def", "_is_chinese_char", "(", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#     https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n", "", "output", "=", "[", "]", "\n", "buff", "=", "\"\"", "\n", "for", "char", "in", "text", ":", "\n", "        ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "_is_chinese_char", "(", "cp", ")", "or", "char", "==", "\"=\"", ":", "\n", "            ", "if", "buff", "!=", "\"\"", ":", "\n", "                ", "output", ".", "append", "(", "buff", ")", "\n", "buff", "=", "\"\"", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "else", ":", "\n", "            ", "buff", "+=", "char", "\n", "\n", "", "", "if", "buff", "!=", "\"\"", ":", "\n", "        ", "output", ".", "append", "(", "buff", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._normalize": [[66, 83], ["in_str.lower.lower", "out_segs.append"], "function", ["None"], ["", "def", "_normalize", "(", "in_str", ")", ":", "\n", "    ", "\"\"\"\n    normalize the input unicode string\n    \"\"\"", "\n", "in_str", "=", "in_str", ".", "lower", "(", ")", "\n", "sp_char", "=", "[", "\n", "u':'", ",", "u'_'", ",", "u'`'", ",", "u'\uff0c'", ",", "u'\u3002'", ",", "u'\uff1a'", ",", "u'\uff1f'", ",", "u'\uff01'", ",", "u'('", ",", "u')'", ",", "\n", "u'\u201c'", ",", "u'\u201d'", ",", "u'\uff1b'", ",", "u'\u2019'", ",", "u'\u300a'", ",", "u'\u300b'", ",", "u'\u2026\u2026'", ",", "u'\u00b7'", ",", "u'\u3001'", ",", "u','", ",", "\n", "u'\u300c'", ",", "u'\u300d'", ",", "u'\uff08'", ",", "u'\uff09'", ",", "u'\uff0d'", ",", "u'\uff5e'", ",", "u'\u300e'", ",", "u'\u300f'", ",", "'|'", "\n", "]", "\n", "out_segs", "=", "[", "]", "\n", "for", "char", "in", "in_str", ":", "\n", "        ", "if", "char", "in", "sp_char", ":", "\n", "            ", "continue", "\n", "", "else", ":", "\n", "            ", "out_segs", ".", "append", "(", "char", ")", "\n", "", "", "return", "''", ".", "join", "(", "out_segs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate.find_lcs": [[85, 98], ["range", "len", "range", "range", "len", "range", "len", "len"], "function", ["None"], ["", "def", "find_lcs", "(", "s1", ",", "s2", ")", ":", "\n", "    ", "\"\"\"find the longest common subsequence between s1 ans s2\"\"\"", "\n", "m", "=", "[", "[", "0", "for", "i", "in", "range", "(", "len", "(", "s2", ")", "+", "1", ")", "]", "for", "j", "in", "range", "(", "len", "(", "s1", ")", "+", "1", ")", "]", "\n", "max_len", "=", "0", "\n", "p", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "s1", ")", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "len", "(", "s2", ")", ")", ":", "\n", "            ", "if", "s1", "[", "i", "]", "==", "s2", "[", "j", "]", ":", "\n", "                ", "m", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "=", "m", "[", "i", "]", "[", "j", "]", "+", "1", "\n", "if", "m", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", ">", "max_len", ":", "\n", "                    ", "max_len", "=", "m", "[", "i", "+", "1", "]", "[", "j", "+", "1", "]", "\n", "p", "=", "i", "+", "1", "\n", "", "", "", "", "return", "s1", "[", "p", "-", "max_len", ":", "p", "]", ",", "max_len", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate.evaluate": [[100, 147], ["document[].strip", "qa[].strip", "evaluate.calc_f1_score", "evaluate.calc_em_score", "print", "print", "print", "print", "print", "print", "str", "print", "print", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate.calc_f1_score", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate.calc_em_score"], ["", "def", "evaluate", "(", "ref_ans", ",", "pred_ans", ",", "verbose", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    ref_ans: reference answers, dict\n    pred_ans: predicted answer, dict\n    return:\n        f1_score: averaged F1 score\n        em_score: averaged EM score\n        total_count: number of samples in the reference dataset\n        skip_count: number of samples skipped in the calculation due to unknown errors\n    \"\"\"", "\n", "f1", "=", "0", "\n", "em", "=", "0", "\n", "total_count", "=", "0", "\n", "skip_count", "=", "0", "\n", "for", "query_id", ",", "sample", "in", "ref_ans", ".", "items", "(", ")", ":", "\n", "        ", "total_count", "+=", "1", "\n", "para", "=", "sample", "[", "'para'", "]", "\n", "query_text", "=", "sample", "[", "'question'", "]", "\n", "title", "=", "sample", "[", "'title'", "]", "\n", "answers", "=", "sample", "[", "'answers'", "]", "\n", "is_impossible", "=", "sample", "[", "'is_impossible'", "]", "\n", "try", ":", "\n", "            ", "prediction", "=", "pred_ans", "[", "str", "(", "query_id", ")", "]", "\n", "", "except", ":", "\n", "            ", "skip_count", "+=", "1", "\n", "if", "verbose", ":", "\n", "                ", "print", "(", "\"para: {}\"", ".", "format", "(", "para", ")", ")", "\n", "print", "(", "\"query: {}\"", ".", "format", "(", "query_text", ")", ")", "\n", "print", "(", "\"ref: {}\"", ".", "format", "(", "'#'", ".", "join", "(", "answers", ")", ")", ")", "\n", "print", "(", "\"Skipped\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "", "continue", "\n", "", "if", "is_impossible", ":", "\n", "            ", "if", "prediction", ".", "lower", "(", ")", "==", "'no answer'", ":", "\n", "                ", "_f1", "=", "1.0", "\n", "_em", "=", "1.0", "\n", "", "else", ":", "\n", "                ", "_f1", "=", "0.0", "\n", "_em", "=", "0.0", "\n", "", "", "else", ":", "\n", "            ", "_f1", "=", "calc_f1_score", "(", "answers", ",", "prediction", ")", "\n", "_em", "=", "calc_em_score", "(", "answers", ",", "prediction", ")", "\n", "", "f1", "+=", "_f1", "\n", "em", "+=", "_em", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "\"para: {}\"", ".", "format", "(", "para", ")", ")", "\n", "print", "(", "\"query: {}\"", ".", "format", "(", "query_text", ")", ")", "\n", "print", "(", "\"title: {}\"", ".", "format", "(", "title", ")", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate.calc_f1_score": [[149, 166], ["max", "evaluate._tokenize_chinese_chars", "evaluate._tokenize_chinese_chars", "evaluate.find_lcs", "f1_scores.append", "evaluate._normalize", "evaluate._normalize", "print", "print", "f1_scores.append", "len", "len", "json.dumps", "json.dumps"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._tokenize_chinese_chars", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._tokenize_chinese_chars", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate.find_lcs", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._normalize", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._normalize"], ["print", "(", "\"cand: {}\"", ".", "format", "(", "prediction", ")", ")", "\n", "print", "(", "\"score: {}\"", ".", "format", "(", "_f1", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "", "", "f1_score", "=", "100.0", "*", "f1", "/", "total_count", "\n", "em_score", "=", "100.0", "*", "em", "/", "total_count", "\n", "return", "f1_score", ",", "em_score", ",", "total_count", ",", "skip_count", "\n", "\n", "\n", "", "def", "calc_f1_score", "(", "answers", ",", "prediction", ")", ":", "\n", "    ", "f1_scores", "=", "[", "]", "\n", "for", "ans", "in", "answers", ":", "\n", "        ", "ans_segs", "=", "_tokenize_chinese_chars", "(", "_normalize", "(", "ans", ")", ")", "\n", "prediction_segs", "=", "_tokenize_chinese_chars", "(", "_normalize", "(", "prediction", ")", ")", "\n", "if", "args", ".", "debug", ":", "\n", "            ", "print", "(", "json", ".", "dumps", "(", "ans_segs", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "print", "(", "json", ".", "dumps", "(", "prediction_segs", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "", "lcs", ",", "lcs_len", "=", "find_lcs", "(", "ans_segs", ",", "prediction_segs", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate.calc_em_score": [[168, 177], ["evaluate._normalize", "evaluate._normalize"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._normalize", "home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._normalize"], ["            ", "f1_scores", ".", "append", "(", "0", ")", "\n", "continue", "\n", "", "prec", "=", "1.0", "*", "lcs_len", "/", "len", "(", "prediction_segs", ")", "\n", "rec", "=", "1.0", "*", "lcs_len", "/", "len", "(", "ans_segs", ")", "\n", "f1", "=", "(", "2", "*", "prec", "*", "rec", ")", "/", "(", "prec", "+", "rec", ")", "\n", "f1_scores", ".", "append", "(", "f1", ")", "\n", "", "return", "max", "(", "f1_scores", ")", "\n", "\n", "\n", "", "def", "calc_em_score", "(", "answers", ",", "prediction", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddlehub_baseline.demo_dataset.DuReader.__init__": [[30, 36], ["paddlehub.dataset.base_nlp_dataset.BaseNLPDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.__init__"], ["def", "__init__", "(", "self", ",", "dataset_path", ")", ":", "\n", "        ", "dataset_dir", "=", "dataset_path", "\n", "super", "(", "DuReader", ",", "self", ")", ".", "__init__", "(", "\n", "base_path", "=", "dataset_dir", ",", "\n", "train_file", "=", "\"train.json\"", ",", "\n", "dev_file", "=", "\"dev.json\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddlehub_baseline.demo_dataset.DuReader._read_file": [[38, 157], ["paddlehub.common.logger.logger.warning", "open", "ord", "json.load", "demo_dataset.DuReader._read_file._tokenize_chinese_chars"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.DuReader-Robust.evaluate._tokenize_chinese_chars"], ["", "def", "_read_file", "(", "self", ",", "input_file", ",", "phase", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        \u8bfb\u5165json\u683c\u5f0f\u6570\u636e\u96c6\n        \"\"\"", "\n", "def", "_is_chinese_char", "(", "cp", ")", ":", "\n", "            ", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "\n", "or", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "\n", "or", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "\n", "or", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "\n", "or", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "\n", "or", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "\n", "or", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "\n", "or", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "\n", "                ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "def", "_is_punctuation", "(", "c", ")", ":", "\n", "            ", "if", "c", "in", "[", "\n", "'\u3002'", ",", "'\uff0c'", ",", "'\uff01'", ",", "'\uff1f'", ",", "'\uff1b'", ",", "'\u3001'", ",", "'\uff1a'", ",", "'\uff08'", ",", "'\uff09'", ",", "'\uff0d'", ",", "'~'", ",", "'\u300c'", ",", "\n", "'\u300a'", ",", "'\u300b'", ",", "','", ",", "'\u300d'", ",", "'\"'", ",", "'\u201c'", ",", "'\u201d'", ",", "'$'", ",", "'\u300e'", ",", "'\u300f'", ",", "'\u2014'", ",", "';'", ",", "\n", "'\u3002'", ",", "'('", ",", "')'", ",", "'-'", ",", "'\uff5e'", ",", "'\u3002'", ",", "'\u2018'", ",", "'\u2019'", ",", "'\u2500'", ",", "':'", "\n", "]", ":", "\n", "                ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "def", "_tokenize_chinese_chars", "(", "text", ")", ":", "\n", "            ", "\"\"\"\n            \u4e2d\u6587\u6c49\u5b57\u5207\u5206\n            \"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "                ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "_is_chinese_char", "(", "cp", ")", "or", "_is_punctuation", "(", "char", ")", ":", "\n", "                    ", "if", "len", "(", "output", ")", ">", "0", "and", "output", "[", "-", "1", "]", "!=", "SPIECE_UNDERLINE", ":", "\n", "                        ", "output", ".", "append", "(", "SPIECE_UNDERLINE", ")", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "SPIECE_UNDERLINE", ")", "\n", "", "else", ":", "\n", "                    ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n", "", "def", "is_whitespace", "(", "c", ")", ":", "\n", "            ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "\n", "c", ")", "==", "0x202F", "or", "ord", "(", "c", ")", "==", "0x3000", "or", "c", "==", "SPIECE_UNDERLINE", ":", "\n", "                ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "examples", "=", "[", "]", "\n", "drop", "=", "0", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "            ", "input_data", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "", "for", "entry", "in", "input_data", ":", "\n", "            ", "for", "paragraph", "in", "entry", "[", "\"paragraphs\"", "]", ":", "\n", "                ", "paragraph_text", "=", "paragraph", "[", "\"context\"", "]", "\n", "context", "=", "_tokenize_chinese_chars", "(", "paragraph_text", ")", "\n", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "context", ":", "\n", "                    ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "                        ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "                        ", "if", "prev_is_whitespace", ":", "\n", "                            ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                            ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "if", "c", "!=", "SPIECE_UNDERLINE", ":", "\n", "                        ", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "", "", "for", "qa", "in", "paragraph", "[", "\"qas\"", "]", ":", "\n", "                    ", "qas_id", "=", "qa", "[", "\"id\"", "]", "\n", "question_text", "=", "qa", "[", "\"question\"", "]", "\n", "\n", "\n", "if", "phase", "==", "'predict'", ":", "\n", "# \u6d4b\u8bd5\u96c6\u90e8\u5206\u6ca1\u6709\u7b54\u6848", "\n", "                        ", "orig_answer_text", "=", "\"\"", "\n", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "", "else", ":", "\n", "# \u8bad\u7ec3\u96c6/\u9a8c\u8bc1\u96c6\u9009\u62e9\u7b2c\u4e00\u4e2a\u7b54\u6848\u4f5c\u4e3aground truth", "\n", "                        ", "answer", "=", "qa", "[", "\"answers\"", "]", "[", "0", "]", "\n", "orig_answer_text", "=", "answer", "[", "\"text\"", "]", "\n", "answer_offset", "=", "answer", "[", "\"answer_start\"", "]", "\n", "while", "paragraph_text", "[", "answer_offset", "]", "in", "[", "\n", "\" \"", ",", "\"\\t\"", ",", "\"\\r\"", ",", "\"\\n\"", ",", "\"\u3002\"", ",", "\"\uff0c\"", ",", "\"\uff1a\"", ",", "\":\"", ",", "\".\"", ",", "\",\"", "\n", "]", ":", "\n", "                            ", "answer_offset", "+=", "1", "\n", "", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "\n", "end_offset", "=", "answer_offset", "+", "answer_length", "-", "1", "\n", "if", "end_offset", ">=", "len", "(", "char_to_word_offset", ")", ":", "\n", "                            ", "end_offset", "=", "len", "(", "char_to_word_offset", ")", "-", "1", "\n", "", "end_position", "=", "char_to_word_offset", "[", "end_offset", "]", "\n", "\n", "", "if", "phase", "==", "\"train\"", ":", "\n", "                        ", "actual_text", "=", "\"\"", ".", "join", "(", "\n", "doc_tokens", "[", "start_position", ":", "(", "end_position", "+", "1", ")", "]", ")", "\n", "cleaned_answer_text", "=", "\"\"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "if", "actual_text", ".", "find", "(", "cleaned_answer_text", ")", "==", "-", "1", ":", "\n", "                            ", "drop", "+=", "1", "\n", "logger", ".", "warning", "(", "(", "actual_text", ",", "\" vs \"", ",", "\n", "cleaned_answer_text", ",", "\" in \"", ",", "qa", ")", ")", "\n", "continue", "\n", "", "", "example", "=", "CMRC2018Example", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "\n", "orig_answer_text", "=", "orig_answer_text", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "", "", "logger", ".", "warning", "(", "\"%i bad examples has been dropped\"", "%", "drop", ")", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.__init__": [[43, 77], ["logging.getLogger", "tensorflow.ConfigProto", "tensorflow.Session", "rc_model.RCModel._build_graph", "tensorflow.train.Saver", "rc_model.RCModel.sess.run", "tensorflow.global_variables_initializer"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._build_graph", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["def", "__init__", "(", "self", ",", "vocab", ",", "args", ")", ":", "\n", "\n", "# logging", "\n", "        ", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"brc\"", ")", "\n", "\n", "# basic config", "\n", "self", ".", "algo", "=", "args", ".", "algo", "\n", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "\n", "self", ".", "optim_type", "=", "args", ".", "optim", "\n", "self", ".", "learning_rate", "=", "args", ".", "learning_rate", "\n", "self", ".", "weight_decay", "=", "args", ".", "weight_decay", "\n", "self", ".", "use_dropout", "=", "args", ".", "dropout_keep_prob", "<", "1", "\n", "\n", "# length limit", "\n", "self", ".", "max_p_num", "=", "args", ".", "max_p_num", "\n", "self", ".", "max_p_len", "=", "args", ".", "max_p_len", "\n", "self", ".", "max_q_len", "=", "args", ".", "max_q_len", "\n", "self", ".", "max_a_len", "=", "args", ".", "max_a_len", "\n", "\n", "# the vocab", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n", "# session info", "\n", "sess_config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "sess_config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", "config", "=", "sess_config", ")", "\n", "\n", "self", ".", "_build_graph", "(", ")", "\n", "\n", "# save info", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "\n", "# initialize the model", "\n", "self", ".", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._build_graph": [[78, 94], ["time.time", "rc_model.RCModel._setup_placeholders", "rc_model.RCModel._embed", "rc_model.RCModel._encode", "rc_model.RCModel._match", "rc_model.RCModel._fuse", "rc_model.RCModel._decode", "rc_model.RCModel._compute_loss", "rc_model.RCModel._create_train_op", "rc_model.RCModel.logger.info", "sum", "rc_model.RCModel.logger.info", "numpy.prod", "time.time", "rc_model.RCModel.sess.run", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._setup_placeholders", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._embed", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._encode", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._match", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._fuse", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._decode", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._compute_loss", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._create_train_op", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["", "def", "_build_graph", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Builds the computation graph with Tensorflow\n        \"\"\"", "\n", "start_t", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "_setup_placeholders", "(", ")", "\n", "self", ".", "_embed", "(", ")", "\n", "self", ".", "_encode", "(", ")", "\n", "self", ".", "_match", "(", ")", "\n", "self", ".", "_fuse", "(", ")", "\n", "self", ".", "_decode", "(", ")", "\n", "self", ".", "_compute_loss", "(", ")", "\n", "self", ".", "_create_train_op", "(", ")", "\n", "self", ".", "logger", ".", "info", "(", "'Time to build graph: {} s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_t", ")", ")", "\n", "param_num", "=", "sum", "(", "[", "np", ".", "prod", "(", "self", ".", "sess", ".", "run", "(", "tf", ".", "shape", "(", "v", ")", ")", ")", "for", "v", "in", "self", ".", "all_params", "]", ")", "\n", "self", ".", "logger", ".", "info", "(", "'There are {} parameters in the model'", ".", "format", "(", "param_num", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._setup_placeholders": [[95, 106], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder"], "methods", ["None"], ["", "def", "_setup_placeholders", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Placeholders\n        \"\"\"", "\n", "self", ".", "p", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", "]", ")", "\n", "self", ".", "q", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", "]", ")", "\n", "self", ".", "p_length", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ")", "\n", "self", ".", "q_length", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ")", "\n", "self", ".", "start_label", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ")", "\n", "self", ".", "end_label", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ")", "\n", "self", ".", "dropout_keep_prob", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._embed": [[107, 120], ["tensorflow.device", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup", "tensorflow.constant_initializer", "rc_model.RCModel.vocab.size"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size"], ["", "def", "_embed", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The embedding layer, question and passage share embeddings\n        \"\"\"", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ",", "tf", ".", "variable_scope", "(", "'word_embedding'", ")", ":", "\n", "            ", "self", ".", "word_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "'word_embeddings'", ",", "\n", "shape", "=", "(", "self", ".", "vocab", ".", "size", "(", ")", ",", "self", ".", "vocab", ".", "embed_dim", ")", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "self", ".", "vocab", ".", "embeddings", ")", ",", "\n", "trainable", "=", "True", "\n", ")", "\n", "self", ".", "p_emb", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "word_embeddings", ",", "self", ".", "p", ")", "\n", "self", ".", "q_emb", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "word_embeddings", ",", "self", ".", "q", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._encode": [[121, 132], ["tensorflow.variable_scope", "layers.basic_rnn.rnn", "tensorflow.variable_scope", "layers.basic_rnn.rnn", "tensorflow.nn.dropout", "tensorflow.nn.dropout"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.basic_rnn.rnn", "home.repos.pwc.inspect_result.baidu_DuReader.layers.basic_rnn.rnn", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["", "", "def", "_encode", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Employs two Bi-LSTMs to encode passage and question separately\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'passage_encoding'", ")", ":", "\n", "            ", "self", ".", "sep_p_encodes", ",", "_", "=", "rnn", "(", "'bi-lstm'", ",", "self", ".", "p_emb", ",", "self", ".", "p_length", ",", "self", ".", "hidden_size", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'question_encoding'", ")", ":", "\n", "            ", "self", ".", "sep_q_encodes", ",", "_", "=", "rnn", "(", "'bi-lstm'", ",", "self", ".", "q_emb", ",", "self", ".", "q_length", ",", "self", ".", "hidden_size", ")", "\n", "", "if", "self", ".", "use_dropout", ":", "\n", "            ", "self", ".", "sep_p_encodes", "=", "tf", ".", "nn", ".", "dropout", "(", "self", ".", "sep_p_encodes", ",", "self", ".", "dropout_keep_prob", ")", "\n", "self", ".", "sep_q_encodes", "=", "tf", ".", "nn", ".", "dropout", "(", "self", ".", "sep_q_encodes", ",", "self", ".", "dropout_keep_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._match": [[133, 147], ["layers.match_layer.AttentionFlowMatchLayer.match", "layers.match_layer.MatchLSTMLayer", "tensorflow.nn.dropout", "layers.match_layer.AttentionFlowMatchLayer", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.match_layer.AttentionFlowMatchLayer.match", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["", "", "def", "_match", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The core of RC model, get the question-aware passage encoding with either BIDAF or MLSTM\n        \"\"\"", "\n", "if", "self", ".", "algo", "==", "'MLSTM'", ":", "\n", "            ", "match_layer", "=", "MatchLSTMLayer", "(", "self", ".", "hidden_size", ")", "\n", "", "elif", "self", ".", "algo", "==", "'BIDAF'", ":", "\n", "            ", "match_layer", "=", "AttentionFlowMatchLayer", "(", "self", ".", "hidden_size", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'The algorithm {} is not implemented.'", ".", "format", "(", "self", ".", "algo", ")", ")", "\n", "", "self", ".", "match_p_encodes", ",", "_", "=", "match_layer", ".", "match", "(", "self", ".", "sep_p_encodes", ",", "self", ".", "sep_q_encodes", ",", "\n", "self", ".", "p_length", ",", "self", ".", "q_length", ")", "\n", "if", "self", ".", "use_dropout", ":", "\n", "            ", "self", ".", "match_p_encodes", "=", "tf", ".", "nn", ".", "dropout", "(", "self", ".", "match_p_encodes", ",", "self", ".", "dropout_keep_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._fuse": [[148, 157], ["tensorflow.variable_scope", "layers.basic_rnn.rnn", "tensorflow.nn.dropout"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.basic_rnn.rnn", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["", "", "def", "_fuse", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Employs Bi-LSTM again to fuse the context information after match layer\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'fusion'", ")", ":", "\n", "            ", "self", ".", "fuse_p_encodes", ",", "_", "=", "rnn", "(", "'bi-lstm'", ",", "self", ".", "match_p_encodes", ",", "self", ".", "p_length", ",", "\n", "self", ".", "hidden_size", ",", "layer_num", "=", "1", ")", "\n", "if", "self", ".", "use_dropout", ":", "\n", "                ", "self", ".", "fuse_p_encodes", "=", "tf", ".", "nn", ".", "dropout", "(", "self", ".", "fuse_p_encodes", ",", "self", ".", "dropout_keep_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._decode": [[158, 178], ["layers.pointer_net.PointerNetDecoder", "layers.pointer_net.PointerNetDecoder.decode", "tensorflow.variable_scope", "tensorflow.reshape", "tensorflow.shape", "tensorflow.reshape", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode"], ["", "", "", "def", "_decode", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Employs Pointer Network to get the the probs of each position\n        to be the start or end of the predicted answer.\n        Note that we concat the fuse_p_encodes for the passages in the same document.\n        And since the encodes of queries in the same document is same, we select the first one.\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'same_question_concat'", ")", ":", "\n", "            ", "batch_size", "=", "tf", ".", "shape", "(", "self", ".", "start_label", ")", "[", "0", "]", "\n", "concat_passage_encodes", "=", "tf", ".", "reshape", "(", "\n", "self", ".", "fuse_p_encodes", ",", "\n", "[", "batch_size", ",", "-", "1", ",", "2", "*", "self", ".", "hidden_size", "]", "\n", ")", "\n", "no_dup_question_encodes", "=", "tf", ".", "reshape", "(", "\n", "self", ".", "sep_q_encodes", ",", "\n", "[", "batch_size", ",", "-", "1", ",", "tf", ".", "shape", "(", "self", ".", "sep_q_encodes", ")", "[", "1", "]", ",", "2", "*", "self", ".", "hidden_size", "]", "\n", ")", "[", "0", ":", ",", "0", ",", "0", ":", ",", "0", ":", "]", "\n", "", "decoder", "=", "PointerNetDecoder", "(", "self", ".", "hidden_size", ")", "\n", "self", ".", "start_probs", ",", "self", ".", "end_probs", "=", "decoder", ".", "decode", "(", "concat_passage_encodes", ",", "\n", "no_dup_question_encodes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._compute_loss": [[179, 201], ["rc_model.RCModel._compute_loss.sparse_nll_loss"], "methods", ["None"], ["", "def", "_compute_loss", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The loss function\n        \"\"\"", "\n", "\n", "def", "sparse_nll_loss", "(", "probs", ",", "labels", ",", "epsilon", "=", "1e-9", ",", "scope", "=", "None", ")", ":", "\n", "            ", "\"\"\"\n            negative log likelyhood loss\n            \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "scope", ",", "\"log_loss\"", ")", ":", "\n", "                ", "labels", "=", "tf", ".", "one_hot", "(", "labels", ",", "tf", ".", "shape", "(", "probs", ")", "[", "1", "]", ",", "axis", "=", "1", ")", "\n", "losses", "=", "-", "tf", ".", "reduce_sum", "(", "labels", "*", "tf", ".", "log", "(", "probs", "+", "epsilon", ")", ",", "1", ")", "\n", "", "return", "losses", "\n", "\n", "", "self", ".", "start_loss", "=", "sparse_nll_loss", "(", "probs", "=", "self", ".", "start_probs", ",", "labels", "=", "self", ".", "start_label", ")", "\n", "self", ".", "end_loss", "=", "sparse_nll_loss", "(", "probs", "=", "self", ".", "end_probs", ",", "labels", "=", "self", ".", "end_label", ")", "\n", "self", ".", "all_params", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "add", "(", "self", ".", "start_loss", ",", "self", ".", "end_loss", ")", ")", "\n", "if", "self", ".", "weight_decay", ">", "0", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "'l2_loss'", ")", ":", "\n", "                ", "l2_loss", "=", "tf", ".", "add_n", "(", "[", "tf", ".", "nn", ".", "l2_loss", "(", "v", ")", "for", "v", "in", "self", ".", "all_params", "]", ")", "\n", "", "self", ".", "loss", "+=", "self", ".", "weight_decay", "*", "l2_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._create_train_op": [[202, 217], ["rc_model.RCModel.optimizer.minimize", "tensorflow.train.AdagradOptimizer", "tensorflow.train.AdamOptimizer", "tensorflow.train.RMSPropOptimizer", "tensorflow.train.GradientDescentOptimizer", "NotImplementedError"], "methods", ["None"], ["", "", "def", "_create_train_op", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Selects the training algorithm and creates a train operation with it\n        \"\"\"", "\n", "if", "self", ".", "optim_type", "==", "'adagrad'", ":", "\n", "            ", "self", ".", "optimizer", "=", "tf", ".", "train", ".", "AdagradOptimizer", "(", "self", ".", "learning_rate", ")", "\n", "", "elif", "self", ".", "optim_type", "==", "'adam'", ":", "\n", "            ", "self", ".", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "learning_rate", ")", "\n", "", "elif", "self", ".", "optim_type", "==", "'rprop'", ":", "\n", "            ", "self", ".", "optimizer", "=", "tf", ".", "train", ".", "RMSPropOptimizer", "(", "self", ".", "learning_rate", ")", "\n", "", "elif", "self", ".", "optim_type", "==", "'sgd'", ":", "\n", "            ", "self", ".", "optimizer", "=", "tf", ".", "train", ".", "GradientDescentOptimizer", "(", "self", ".", "learning_rate", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Unsupported optimizer: {}'", ".", "format", "(", "self", ".", "optim_type", ")", ")", "\n", "", "self", ".", "train_op", "=", "self", ".", "optimizer", ".", "minimize", "(", "self", ".", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._train_epoch": [[218, 244], ["enumerate", "rc_model.RCModel.sess.run", "len", "len", "rc_model.RCModel.logger.info"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run"], ["", "def", "_train_epoch", "(", "self", ",", "train_batches", ",", "dropout_keep_prob", ")", ":", "\n", "        ", "\"\"\"\n        Trains the model for a single epoch.\n        Args:\n            train_batches: iterable batch data for training\n            dropout_keep_prob: float value indicating dropout keep probability\n        \"\"\"", "\n", "total_num", ",", "total_loss", "=", "0", ",", "0", "\n", "log_every_n_batch", ",", "n_batch_loss", "=", "50", ",", "0", "\n", "for", "bitx", ",", "batch", "in", "enumerate", "(", "train_batches", ",", "1", ")", ":", "\n", "            ", "feed_dict", "=", "{", "self", ".", "p", ":", "batch", "[", "'passage_token_ids'", "]", ",", "\n", "self", ".", "q", ":", "batch", "[", "'question_token_ids'", "]", ",", "\n", "self", ".", "p_length", ":", "batch", "[", "'passage_length'", "]", ",", "\n", "self", ".", "q_length", ":", "batch", "[", "'question_length'", "]", ",", "\n", "self", ".", "start_label", ":", "batch", "[", "'start_id'", "]", ",", "\n", "self", ".", "end_label", ":", "batch", "[", "'end_id'", "]", ",", "\n", "self", ".", "dropout_keep_prob", ":", "dropout_keep_prob", "}", "\n", "_", ",", "loss", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "train_op", ",", "self", ".", "loss", "]", ",", "feed_dict", ")", "\n", "total_loss", "+=", "loss", "*", "len", "(", "batch", "[", "'raw_data'", "]", ")", "\n", "total_num", "+=", "len", "(", "batch", "[", "'raw_data'", "]", ")", "\n", "n_batch_loss", "+=", "loss", "\n", "if", "log_every_n_batch", ">", "0", "and", "bitx", "%", "log_every_n_batch", "==", "0", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "'Average loss from batch {} to {} is {}'", ".", "format", "(", "\n", "bitx", "-", "log_every_n_batch", "+", "1", ",", "bitx", ",", "n_batch_loss", "/", "log_every_n_batch", ")", ")", "\n", "n_batch_loss", "=", "0", "\n", "", "", "return", "1.0", "*", "total_loss", "/", "total_num", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.train": [[245, 281], ["rc_model.RCModel.vocab.get_id", "range", "rc_model.RCModel.logger.info", "data.gen_mini_batches", "rc_model.RCModel._train_epoch", "rc_model.RCModel.logger.info", "rc_model.RCModel.logger.info", "rc_model.RCModel.save", "data.gen_mini_batches", "rc_model.RCModel.evaluate", "rc_model.RCModel.logger.info", "rc_model.RCModel.logger.info", "rc_model.RCModel.logger.warning", "rc_model.RCModel.save", "str"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_id", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.gen_mini_batches", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel._train_epoch", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.save", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.gen_mini_batches", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.evaluate", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.save"], ["", "def", "train", "(", "self", ",", "data", ",", "epochs", ",", "batch_size", ",", "save_dir", ",", "save_prefix", ",", "\n", "dropout_keep_prob", "=", "1.0", ",", "evaluate", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Train the model with data\n        Args:\n            data: the BRCDataset class implemented in dataset.py\n            epochs: number of training epochs\n            batch_size:\n            save_dir: the directory to save the model\n            save_prefix: the prefix indicating the model type\n            dropout_keep_prob: float value indicating dropout keep probability\n            evaluate: whether to evaluate the model on test set after each epoch\n        \"\"\"", "\n", "pad_id", "=", "self", ".", "vocab", ".", "get_id", "(", "self", ".", "vocab", ".", "pad_token", ")", "\n", "max_bleu_4", "=", "0", "\n", "for", "epoch", "in", "range", "(", "1", ",", "epochs", "+", "1", ")", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "'Training the model for epoch {}'", ".", "format", "(", "epoch", ")", ")", "\n", "train_batches", "=", "data", ".", "gen_mini_batches", "(", "'train'", ",", "batch_size", ",", "pad_id", ",", "shuffle", "=", "True", ")", "\n", "train_loss", "=", "self", ".", "_train_epoch", "(", "train_batches", ",", "dropout_keep_prob", ")", "\n", "self", ".", "logger", ".", "info", "(", "'Average train loss for epoch {} is {}'", ".", "format", "(", "epoch", ",", "train_loss", ")", ")", "\n", "\n", "if", "evaluate", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "'Evaluating the model after epoch {}'", ".", "format", "(", "epoch", ")", ")", "\n", "if", "data", ".", "dev_set", "is", "not", "None", ":", "\n", "                    ", "eval_batches", "=", "data", ".", "gen_mini_batches", "(", "'dev'", ",", "batch_size", ",", "pad_id", ",", "shuffle", "=", "False", ")", "\n", "eval_loss", ",", "bleu_rouge", "=", "self", ".", "evaluate", "(", "eval_batches", ")", "\n", "self", ".", "logger", ".", "info", "(", "'Dev eval loss {}'", ".", "format", "(", "eval_loss", ")", ")", "\n", "self", ".", "logger", ".", "info", "(", "'Dev eval result: {}'", ".", "format", "(", "bleu_rouge", ")", ")", "\n", "\n", "if", "bleu_rouge", "[", "'Bleu-4'", "]", ">", "max_bleu_4", ":", "\n", "                        ", "self", ".", "save", "(", "save_dir", ",", "save_prefix", ")", "\n", "max_bleu_4", "=", "bleu_rouge", "[", "'Bleu-4'", "]", "\n", "", "", "else", ":", "\n", "                    ", "self", ".", "logger", ".", "warning", "(", "'No dev set is loaded for evaluation in the dataset!'", ")", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "save", "(", "save_dir", ",", "save_prefix", "+", "'_'", "+", "str", "(", "epoch", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.evaluate": [[282, 350], ["enumerate", "rc_model.RCModel.sess.run", "len", "len", "zip", "os.path.join", "rc_model.RCModel.logger.info", "len", "zip", "utils.compute_bleu_rouge", "len", "rc_model.RCModel.find_best_answer", "open", "pred_answers.append", "pred_answers.append", "ref_answers.append", "fout.write", "len", "utils.normalize", "utils.normalize", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.compute_bleu_rouge", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.find_best_answer", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.normalize", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.normalize"], ["", "", "", "def", "evaluate", "(", "self", ",", "eval_batches", ",", "result_dir", "=", "None", ",", "result_prefix", "=", "None", ",", "save_full_info", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Evaluates the model performance on eval_batches and results are saved if specified\n        Args:\n            eval_batches: iterable batch data\n            result_dir: directory to save predicted answers, answers will not be saved if None\n            result_prefix: prefix of the file for saving predicted answers,\n                           answers will not be saved if None\n            save_full_info: if True, the pred_answers will be added to raw sample and saved\n        \"\"\"", "\n", "pred_answers", ",", "ref_answers", "=", "[", "]", ",", "[", "]", "\n", "total_loss", ",", "total_num", "=", "0", ",", "0", "\n", "for", "b_itx", ",", "batch", "in", "enumerate", "(", "eval_batches", ")", ":", "\n", "            ", "feed_dict", "=", "{", "self", ".", "p", ":", "batch", "[", "'passage_token_ids'", "]", ",", "\n", "self", ".", "q", ":", "batch", "[", "'question_token_ids'", "]", ",", "\n", "self", ".", "p_length", ":", "batch", "[", "'passage_length'", "]", ",", "\n", "self", ".", "q_length", ":", "batch", "[", "'question_length'", "]", ",", "\n", "self", ".", "start_label", ":", "batch", "[", "'start_id'", "]", ",", "\n", "self", ".", "end_label", ":", "batch", "[", "'end_id'", "]", ",", "\n", "self", ".", "dropout_keep_prob", ":", "1.0", "}", "\n", "start_probs", ",", "end_probs", ",", "loss", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "start_probs", ",", "\n", "self", ".", "end_probs", ",", "self", ".", "loss", "]", ",", "feed_dict", ")", "\n", "\n", "total_loss", "+=", "loss", "*", "len", "(", "batch", "[", "'raw_data'", "]", ")", "\n", "total_num", "+=", "len", "(", "batch", "[", "'raw_data'", "]", ")", "\n", "\n", "padded_p_len", "=", "len", "(", "batch", "[", "'passage_token_ids'", "]", "[", "0", "]", ")", "\n", "for", "sample", ",", "start_prob", ",", "end_prob", "in", "zip", "(", "batch", "[", "'raw_data'", "]", ",", "start_probs", ",", "end_probs", ")", ":", "\n", "\n", "                ", "best_answer", "=", "self", ".", "find_best_answer", "(", "sample", ",", "start_prob", ",", "end_prob", ",", "padded_p_len", ")", "\n", "if", "save_full_info", ":", "\n", "                    ", "sample", "[", "'pred_answers'", "]", "=", "[", "best_answer", "]", "\n", "pred_answers", ".", "append", "(", "sample", ")", "\n", "", "else", ":", "\n", "                    ", "pred_answers", ".", "append", "(", "{", "'question_id'", ":", "sample", "[", "'question_id'", "]", ",", "\n", "'question_type'", ":", "sample", "[", "'question_type'", "]", ",", "\n", "'answers'", ":", "[", "best_answer", "]", ",", "\n", "'entity_answers'", ":", "[", "[", "]", "]", ",", "\n", "'yesno_answers'", ":", "[", "]", "}", ")", "\n", "", "if", "'answers'", "in", "sample", ":", "\n", "                    ", "ref_answers", ".", "append", "(", "{", "'question_id'", ":", "sample", "[", "'question_id'", "]", ",", "\n", "'question_type'", ":", "sample", "[", "'question_type'", "]", ",", "\n", "'answers'", ":", "sample", "[", "'answers'", "]", ",", "\n", "'entity_answers'", ":", "[", "[", "]", "]", ",", "\n", "'yesno_answers'", ":", "[", "]", "}", ")", "\n", "\n", "", "", "", "if", "result_dir", "is", "not", "None", "and", "result_prefix", "is", "not", "None", ":", "\n", "            ", "result_file", "=", "os", ".", "path", ".", "join", "(", "result_dir", ",", "result_prefix", "+", "'.json'", ")", "\n", "with", "open", "(", "result_file", ",", "'w'", ")", "as", "fout", ":", "\n", "                ", "for", "pred_answer", "in", "pred_answers", ":", "\n", "                    ", "fout", ".", "write", "(", "json", ".", "dumps", "(", "pred_answer", ",", "ensure_ascii", "=", "False", ")", "+", "'\\n'", ")", "\n", "\n", "", "", "self", ".", "logger", ".", "info", "(", "'Saving {} results to {}'", ".", "format", "(", "result_prefix", ",", "result_file", ")", ")", "\n", "\n", "# this average loss is invalid on test set, since we don't have true start_id and end_id", "\n", "", "ave_loss", "=", "1.0", "*", "total_loss", "/", "total_num", "\n", "# compute the bleu and rouge scores if reference answers is provided", "\n", "if", "len", "(", "ref_answers", ")", ">", "0", ":", "\n", "            ", "pred_dict", ",", "ref_dict", "=", "{", "}", ",", "{", "}", "\n", "for", "pred", ",", "ref", "in", "zip", "(", "pred_answers", ",", "ref_answers", ")", ":", "\n", "                ", "question_id", "=", "ref", "[", "'question_id'", "]", "\n", "if", "len", "(", "ref", "[", "'answers'", "]", ")", ">", "0", ":", "\n", "                    ", "pred_dict", "[", "question_id", "]", "=", "normalize", "(", "pred", "[", "'answers'", "]", ")", "\n", "ref_dict", "[", "question_id", "]", "=", "normalize", "(", "ref", "[", "'answers'", "]", ")", "\n", "", "", "bleu_rouge", "=", "compute_bleu_rouge", "(", "pred_dict", ",", "ref_dict", ")", "\n", "", "else", ":", "\n", "            ", "bleu_rouge", "=", "None", "\n", "", "return", "ave_loss", ",", "bleu_rouge", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.find_best_answer": [[351, 375], ["enumerate", "min", "rc_model.RCModel.find_best_answer_for_passage", "len"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.find_best_answer_for_passage"], ["", "def", "find_best_answer", "(", "self", ",", "sample", ",", "start_prob", ",", "end_prob", ",", "padded_p_len", ")", ":", "\n", "        ", "\"\"\"\n        Finds the best answer for a sample given start_prob and end_prob for each position.\n        This will call find_best_answer_for_passage because there are multiple passages in a sample\n        \"\"\"", "\n", "best_p_idx", ",", "best_span", ",", "best_score", "=", "None", ",", "None", ",", "0", "\n", "for", "p_idx", ",", "passage", "in", "enumerate", "(", "sample", "[", "'passages'", "]", ")", ":", "\n", "            ", "if", "p_idx", ">=", "self", ".", "max_p_num", ":", "\n", "                ", "continue", "\n", "", "passage_len", "=", "min", "(", "self", ".", "max_p_len", ",", "len", "(", "passage", "[", "'passage_tokens'", "]", ")", ")", "\n", "answer_span", ",", "score", "=", "self", ".", "find_best_answer_for_passage", "(", "\n", "start_prob", "[", "p_idx", "*", "padded_p_len", ":", "(", "p_idx", "+", "1", ")", "*", "padded_p_len", "]", ",", "\n", "end_prob", "[", "p_idx", "*", "padded_p_len", ":", "(", "p_idx", "+", "1", ")", "*", "padded_p_len", "]", ",", "\n", "passage_len", ")", "\n", "if", "score", ">", "best_score", ":", "\n", "                ", "best_score", "=", "score", "\n", "best_p_idx", "=", "p_idx", "\n", "best_span", "=", "answer_span", "\n", "", "", "if", "best_p_idx", "is", "None", "or", "best_span", "is", "None", ":", "\n", "            ", "best_answer", "=", "''", "\n", "", "else", ":", "\n", "            ", "best_answer", "=", "''", ".", "join", "(", "\n", "sample", "[", "'passages'", "]", "[", "best_p_idx", "]", "[", "'passage_tokens'", "]", "[", "best_span", "[", "0", "]", ":", "best_span", "[", "1", "]", "+", "1", "]", ")", "\n", "", "return", "best_answer", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.find_best_answer_for_passage": [[376, 396], ["range", "len", "min", "range", "len"], "methods", ["None"], ["", "def", "find_best_answer_for_passage", "(", "self", ",", "start_probs", ",", "end_probs", ",", "passage_len", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Finds the best answer with the maximum start_prob * end_prob from a single passage\n        \"\"\"", "\n", "if", "passage_len", "is", "None", ":", "\n", "            ", "passage_len", "=", "len", "(", "start_probs", ")", "\n", "", "else", ":", "\n", "            ", "passage_len", "=", "min", "(", "len", "(", "start_probs", ")", ",", "passage_len", ")", "\n", "", "best_start", ",", "best_end", ",", "max_prob", "=", "-", "1", ",", "-", "1", ",", "0", "\n", "for", "start_idx", "in", "range", "(", "passage_len", ")", ":", "\n", "            ", "for", "ans_len", "in", "range", "(", "self", ".", "max_a_len", ")", ":", "\n", "                ", "end_idx", "=", "start_idx", "+", "ans_len", "\n", "if", "end_idx", ">=", "passage_len", ":", "\n", "                    ", "continue", "\n", "", "prob", "=", "start_probs", "[", "start_idx", "]", "*", "end_probs", "[", "end_idx", "]", "\n", "if", "prob", ">", "max_prob", ":", "\n", "                    ", "best_start", "=", "start_idx", "\n", "best_end", "=", "end_idx", "\n", "max_prob", "=", "prob", "\n", "", "", "", "return", "(", "best_start", ",", "best_end", ")", ",", "max_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.save": [[397, 403], ["rc_model.RCModel.saver.save", "rc_model.RCModel.logger.info", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.save"], ["", "def", "save", "(", "self", ",", "model_dir", ",", "model_prefix", ")", ":", "\n", "        ", "\"\"\"\n        Saves the model into model_dir with model_prefix as the model indicator\n        \"\"\"", "\n", "self", ".", "saver", ".", "save", "(", "self", ".", "sess", ",", "os", ".", "path", ".", "join", "(", "model_dir", ",", "model_prefix", ")", ")", "\n", "self", ".", "logger", ".", "info", "(", "'Model saved in {}, with prefix {}.'", ".", "format", "(", "model_dir", ",", "model_prefix", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.restore": [[404, 410], ["rc_model.RCModel.saver.restore", "rc_model.RCModel.logger.info", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.restore"], ["", "def", "restore", "(", "self", ",", "model_dir", ",", "model_prefix", ")", ":", "\n", "        ", "\"\"\"\n        Restores the model into model_dir from model_prefix as the model indicator\n        \"\"\"", "\n", "self", ".", "saver", ".", "restore", "(", "self", ".", "sess", ",", "os", ".", "path", ".", "join", "(", "model_dir", ",", "model_prefix", ")", ")", "\n", "self", ".", "logger", ".", "info", "(", "'Model restored from {}, with prefix {}'", ".", "format", "(", "model_dir", ",", "model_prefix", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.dataset.BRCDataset.__init__": [[32, 54], ["logging.getLogger", "dataset.BRCDataset.logger.info", "dataset.BRCDataset.logger.info", "dataset.BRCDataset.logger.info", "dataset.BRCDataset._load_dataset", "dataset.BRCDataset._load_dataset", "dataset.BRCDataset._load_dataset", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset._load_dataset", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset._load_dataset", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset._load_dataset"], ["def", "__init__", "(", "self", ",", "max_p_num", ",", "max_p_len", ",", "max_q_len", ",", "\n", "train_files", "=", "[", "]", ",", "dev_files", "=", "[", "]", ",", "test_files", "=", "[", "]", ")", ":", "\n", "        ", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"brc\"", ")", "\n", "self", ".", "max_p_num", "=", "max_p_num", "\n", "self", ".", "max_p_len", "=", "max_p_len", "\n", "self", ".", "max_q_len", "=", "max_q_len", "\n", "\n", "self", ".", "train_set", ",", "self", ".", "dev_set", ",", "self", ".", "test_set", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "if", "train_files", ":", "\n", "            ", "for", "train_file", "in", "train_files", ":", "\n", "                ", "self", ".", "train_set", "+=", "self", ".", "_load_dataset", "(", "train_file", ",", "train", "=", "True", ")", "\n", "", "self", ".", "logger", ".", "info", "(", "'Train set size: {} questions.'", ".", "format", "(", "len", "(", "self", ".", "train_set", ")", ")", ")", "\n", "\n", "", "if", "dev_files", ":", "\n", "            ", "for", "dev_file", "in", "dev_files", ":", "\n", "                ", "self", ".", "dev_set", "+=", "self", ".", "_load_dataset", "(", "dev_file", ")", "\n", "", "self", ".", "logger", ".", "info", "(", "'Dev set size: {} questions.'", ".", "format", "(", "len", "(", "self", ".", "dev_set", ")", ")", ")", "\n", "\n", "", "if", "test_files", ":", "\n", "            ", "for", "test_file", "in", "test_files", ":", "\n", "                ", "self", ".", "test_set", "+=", "self", ".", "_load_dataset", "(", "test_file", ")", "\n", "", "self", ".", "logger", ".", "info", "(", "'Test set size: {} questions.'", ".", "format", "(", "len", "(", "self", ".", "test_set", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.dataset.BRCDataset._load_dataset": [[55, 102], ["open", "enumerate", "json.loads", "enumerate", "data_set.append", "line.strip", "len", "sample[].append", "para_infos.sort", "sample[].append", "sum", "para_infos.append", "collections.Counter", "collections.Counter", "common_with_question.values", "float", "len", "len"], "methods", ["None"], ["", "", "def", "_load_dataset", "(", "self", ",", "data_path", ",", "train", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Loads the dataset\n        Args:\n            data_path: the data file to load\n        \"\"\"", "\n", "with", "open", "(", "data_path", ")", "as", "fin", ":", "\n", "            ", "data_set", "=", "[", "]", "\n", "for", "lidx", ",", "line", "in", "enumerate", "(", "fin", ")", ":", "\n", "                ", "sample", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "if", "train", ":", "\n", "                    ", "if", "len", "(", "sample", "[", "'answer_spans'", "]", ")", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "if", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "1", "]", ">=", "self", ".", "max_p_len", ":", "\n", "                        ", "continue", "\n", "\n", "", "", "if", "'answer_docs'", "in", "sample", ":", "\n", "                    ", "sample", "[", "'answer_passages'", "]", "=", "sample", "[", "'answer_docs'", "]", "\n", "\n", "", "sample", "[", "'question_tokens'", "]", "=", "sample", "[", "'segmented_question'", "]", "\n", "\n", "sample", "[", "'passages'", "]", "=", "[", "]", "\n", "for", "d_idx", ",", "doc", "in", "enumerate", "(", "sample", "[", "'documents'", "]", ")", ":", "\n", "                    ", "if", "train", ":", "\n", "                        ", "most_related_para", "=", "doc", "[", "'most_related_para'", "]", "\n", "sample", "[", "'passages'", "]", ".", "append", "(", "\n", "{", "'passage_tokens'", ":", "doc", "[", "'segmented_paragraphs'", "]", "[", "most_related_para", "]", ",", "\n", "'is_selected'", ":", "doc", "[", "'is_selected'", "]", "}", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "para_infos", "=", "[", "]", "\n", "for", "para_tokens", "in", "doc", "[", "'segmented_paragraphs'", "]", ":", "\n", "                            ", "question_tokens", "=", "sample", "[", "'segmented_question'", "]", "\n", "common_with_question", "=", "Counter", "(", "para_tokens", ")", "&", "Counter", "(", "question_tokens", ")", "\n", "correct_preds", "=", "sum", "(", "common_with_question", ".", "values", "(", ")", ")", "\n", "if", "correct_preds", "==", "0", ":", "\n", "                                ", "recall_wrt_question", "=", "0", "\n", "", "else", ":", "\n", "                                ", "recall_wrt_question", "=", "float", "(", "correct_preds", ")", "/", "len", "(", "question_tokens", ")", "\n", "", "para_infos", ".", "append", "(", "(", "para_tokens", ",", "recall_wrt_question", ",", "len", "(", "para_tokens", ")", ")", ")", "\n", "", "para_infos", ".", "sort", "(", "key", "=", "lambda", "x", ":", "(", "-", "x", "[", "1", "]", ",", "x", "[", "2", "]", ")", ")", "\n", "fake_passage_tokens", "=", "[", "]", "\n", "for", "para_info", "in", "para_infos", "[", ":", "1", "]", ":", "\n", "                            ", "fake_passage_tokens", "+=", "para_info", "[", "0", "]", "\n", "", "sample", "[", "'passages'", "]", ".", "append", "(", "{", "'passage_tokens'", ":", "fake_passage_tokens", "}", ")", "\n", "", "", "data_set", ".", "append", "(", "sample", ")", "\n", "", "", "return", "data_set", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.dataset.BRCDataset._one_mini_batch": [[103, 146], ["max", "min", "enumerate", "dataset.BRCDataset._dynamic_padding", "range", "len", "len", "batch_data[].append", "batch_data[].append", "batch_data[].append", "batch_data[].append", "len", "batch_data[].append", "batch_data[].append", "batch_data[].append", "batch_data[].append", "batch_data[].append", "batch_data[].append", "batch_data[].append", "batch_data[].append", "len", "min", "len"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.dataset.BRCDataset._dynamic_padding"], ["", "def", "_one_mini_batch", "(", "self", ",", "data", ",", "indices", ",", "pad_id", ")", ":", "\n", "        ", "\"\"\"\n        Get one mini batch\n        Args:\n            data: all data\n            indices: the indices of the samples to be selected\n            pad_id:\n        Returns:\n            one batch of data\n        \"\"\"", "\n", "batch_data", "=", "{", "'raw_data'", ":", "[", "data", "[", "i", "]", "for", "i", "in", "indices", "]", ",", "\n", "'question_token_ids'", ":", "[", "]", ",", "\n", "'question_length'", ":", "[", "]", ",", "\n", "'passage_token_ids'", ":", "[", "]", ",", "\n", "'passage_length'", ":", "[", "]", ",", "\n", "'start_id'", ":", "[", "]", ",", "\n", "'end_id'", ":", "[", "]", "}", "\n", "max_passage_num", "=", "max", "(", "[", "len", "(", "sample", "[", "'passages'", "]", ")", "for", "sample", "in", "batch_data", "[", "'raw_data'", "]", "]", ")", "\n", "max_passage_num", "=", "min", "(", "self", ".", "max_p_num", ",", "max_passage_num", ")", "\n", "for", "sidx", ",", "sample", "in", "enumerate", "(", "batch_data", "[", "'raw_data'", "]", ")", ":", "\n", "            ", "for", "pidx", "in", "range", "(", "max_passage_num", ")", ":", "\n", "                ", "if", "pidx", "<", "len", "(", "sample", "[", "'passages'", "]", ")", ":", "\n", "                    ", "batch_data", "[", "'question_token_ids'", "]", ".", "append", "(", "sample", "[", "'question_token_ids'", "]", ")", "\n", "batch_data", "[", "'question_length'", "]", ".", "append", "(", "len", "(", "sample", "[", "'question_token_ids'", "]", ")", ")", "\n", "passage_token_ids", "=", "sample", "[", "'passages'", "]", "[", "pidx", "]", "[", "'passage_token_ids'", "]", "\n", "batch_data", "[", "'passage_token_ids'", "]", ".", "append", "(", "passage_token_ids", ")", "\n", "batch_data", "[", "'passage_length'", "]", ".", "append", "(", "min", "(", "len", "(", "passage_token_ids", ")", ",", "self", ".", "max_p_len", ")", ")", "\n", "", "else", ":", "\n", "                    ", "batch_data", "[", "'question_token_ids'", "]", ".", "append", "(", "[", "]", ")", "\n", "batch_data", "[", "'question_length'", "]", ".", "append", "(", "0", ")", "\n", "batch_data", "[", "'passage_token_ids'", "]", ".", "append", "(", "[", "]", ")", "\n", "batch_data", "[", "'passage_length'", "]", ".", "append", "(", "0", ")", "\n", "", "", "", "batch_data", ",", "padded_p_len", ",", "padded_q_len", "=", "self", ".", "_dynamic_padding", "(", "batch_data", ",", "pad_id", ")", "\n", "for", "sample", "in", "batch_data", "[", "'raw_data'", "]", ":", "\n", "            ", "if", "'answer_passages'", "in", "sample", "and", "len", "(", "sample", "[", "'answer_passages'", "]", ")", ":", "\n", "                ", "gold_passage_offset", "=", "padded_p_len", "*", "sample", "[", "'answer_passages'", "]", "[", "0", "]", "\n", "batch_data", "[", "'start_id'", "]", ".", "append", "(", "gold_passage_offset", "+", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "0", "]", ")", "\n", "batch_data", "[", "'end_id'", "]", ".", "append", "(", "gold_passage_offset", "+", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "# fake span for some samples, only valid for testing", "\n", "                ", "batch_data", "[", "'start_id'", "]", ".", "append", "(", "0", ")", "\n", "batch_data", "[", "'end_id'", "]", ".", "append", "(", "0", ")", "\n", "", "", "return", "batch_data", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.dataset.BRCDataset._dynamic_padding": [[147, 158], ["min", "min", "max", "max", "len", "len"], "methods", ["None"], ["", "def", "_dynamic_padding", "(", "self", ",", "batch_data", ",", "pad_id", ")", ":", "\n", "        ", "\"\"\"\n        Dynamically pads the batch_data with pad_id\n        \"\"\"", "\n", "pad_p_len", "=", "min", "(", "self", ".", "max_p_len", ",", "max", "(", "batch_data", "[", "'passage_length'", "]", ")", ")", "\n", "pad_q_len", "=", "min", "(", "self", ".", "max_q_len", ",", "max", "(", "batch_data", "[", "'question_length'", "]", ")", ")", "\n", "batch_data", "[", "'passage_token_ids'", "]", "=", "[", "(", "ids", "+", "[", "pad_id", "]", "*", "(", "pad_p_len", "-", "len", "(", "ids", ")", ")", ")", "[", ":", "pad_p_len", "]", "\n", "for", "ids", "in", "batch_data", "[", "'passage_token_ids'", "]", "]", "\n", "batch_data", "[", "'question_token_ids'", "]", "=", "[", "(", "ids", "+", "[", "pad_id", "]", "*", "(", "pad_q_len", "-", "len", "(", "ids", ")", ")", ")", "[", ":", "pad_q_len", "]", "\n", "for", "ids", "in", "batch_data", "[", "'question_token_ids'", "]", "]", "\n", "return", "batch_data", ",", "pad_p_len", ",", "pad_q_len", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.dataset.BRCDataset.word_iter": [[159, 184], ["NotImplementedError"], "methods", ["None"], ["", "def", "word_iter", "(", "self", ",", "set_name", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Iterates over all the words in the dataset\n        Args:\n            set_name: if it is set, then the specific set will be used\n        Returns:\n            a generator\n        \"\"\"", "\n", "if", "set_name", "is", "None", ":", "\n", "            ", "data_set", "=", "self", ".", "train_set", "+", "self", ".", "dev_set", "+", "self", ".", "test_set", "\n", "", "elif", "set_name", "==", "'train'", ":", "\n", "            ", "data_set", "=", "self", ".", "train_set", "\n", "", "elif", "set_name", "==", "'dev'", ":", "\n", "            ", "data_set", "=", "self", ".", "dev_set", "\n", "", "elif", "set_name", "==", "'test'", ":", "\n", "            ", "data_set", "=", "self", ".", "test_set", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'No data set named as {}'", ".", "format", "(", "set_name", ")", ")", "\n", "", "if", "data_set", "is", "not", "None", ":", "\n", "            ", "for", "sample", "in", "data_set", ":", "\n", "                ", "for", "token", "in", "sample", "[", "'question_tokens'", "]", ":", "\n", "                    ", "yield", "token", "\n", "", "for", "passage", "in", "sample", "[", "'passages'", "]", ":", "\n", "                    ", "for", "token", "in", "passage", "[", "'passage_tokens'", "]", ":", "\n", "                        ", "yield", "token", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.dataset.BRCDataset.convert_to_ids": [[185, 198], ["vocab.convert_to_ids", "vocab.convert_to_ids"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.convert_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.convert_to_ids"], ["", "", "", "", "", "def", "convert_to_ids", "(", "self", ",", "vocab", ")", ":", "\n", "        ", "\"\"\"\n        Convert the question and passage in the original dataset to ids\n        Args:\n            vocab: the vocabulary on this dataset\n        \"\"\"", "\n", "for", "data_set", "in", "[", "self", ".", "train_set", ",", "self", ".", "dev_set", ",", "self", ".", "test_set", "]", ":", "\n", "            ", "if", "data_set", "is", "None", ":", "\n", "                ", "continue", "\n", "", "for", "sample", "in", "data_set", ":", "\n", "                ", "sample", "[", "'question_token_ids'", "]", "=", "vocab", ".", "convert_to_ids", "(", "sample", "[", "'question_tokens'", "]", ")", "\n", "for", "passage", "in", "sample", "[", "'passages'", "]", ":", "\n", "                    ", "passage", "[", "'passage_token_ids'", "]", "=", "vocab", ".", "convert_to_ids", "(", "passage", "[", "'passage_tokens'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.dataset.BRCDataset.gen_mini_batches": [[199, 225], ["len", "numpy.arange", "numpy.arange", "numpy.random.shuffle", "dataset.BRCDataset._one_mini_batch", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset._one_mini_batch"], ["", "", "", "", "def", "gen_mini_batches", "(", "self", ",", "set_name", ",", "batch_size", ",", "pad_id", ",", "shuffle", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Generate data batches for a specific dataset (train/dev/test)\n        Args:\n            set_name: train/dev/test to indicate the set\n            batch_size: number of samples in one batch\n            pad_id: pad id\n            shuffle: if set to be true, the data is shuffled.\n        Returns:\n            a generator for all batches\n        \"\"\"", "\n", "if", "set_name", "==", "'train'", ":", "\n", "            ", "data", "=", "self", ".", "train_set", "\n", "", "elif", "set_name", "==", "'dev'", ":", "\n", "            ", "data", "=", "self", ".", "dev_set", "\n", "", "elif", "set_name", "==", "'test'", ":", "\n", "            ", "data", "=", "self", ".", "test_set", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'No data set named as {}'", ".", "format", "(", "set_name", ")", ")", "\n", "", "data_size", "=", "len", "(", "data", ")", "\n", "indices", "=", "np", ".", "arange", "(", "data_size", ")", "\n", "if", "shuffle", ":", "\n", "            ", "np", ".", "random", ".", "shuffle", "(", "indices", ")", "\n", "", "for", "batch_start", "in", "np", ".", "arange", "(", "0", ",", "data_size", ",", "batch_size", ")", ":", "\n", "            ", "batch_indices", "=", "indices", "[", "batch_start", ":", "batch_start", "+", "batch_size", "]", "\n", "yield", "self", ".", "_one_mini_batch", "(", "data", ",", "batch_indices", ",", "pad_id", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.parse_args": [[35, 104], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "argparse.ArgumentParser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "argparse.ArgumentParser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args"], ["\"ernie\"", ":", "(", "ErnieForQuestionAnswering", ",", "ErnieTokenizer", ")", ",", "\n", "\"bert\"", ":", "(", "BertForQuestionAnswering", ",", "BertTokenizer", ")", ",", "\n", "\"roberta\"", ":", "(", "RobertaForQuestionAnswering", ",", "RobertaTokenizer", ")", "\n", "}", "\n", "\n", "def", "set_seed", "(", "args", ")", ":", "\n", "    ", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "paddle", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "\n", "", "class", "CrossEntropyLossForChecklist", "(", "paddle", ".", "nn", ".", "Layer", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "CrossEntropyLossForChecklist", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "y", ",", "label", ")", ":", "\n", "        ", "start_logits", ",", "end_logits", ",", "cls_logits", "=", "y", "\n", "start_position", ",", "end_position", ",", "answerable_label", "=", "label", "\n", "start_position", "=", "paddle", ".", "unsqueeze", "(", "start_position", ",", "axis", "=", "-", "1", ")", "\n", "end_position", "=", "paddle", ".", "unsqueeze", "(", "end_position", ",", "axis", "=", "-", "1", ")", "\n", "answerable_label", "=", "paddle", ".", "unsqueeze", "(", "answerable_label", ",", "axis", "=", "-", "1", ")", "\n", "start_loss", "=", "paddle", ".", "nn", ".", "functional", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "start_logits", ",", "label", "=", "start_position", ",", "soft_label", "=", "False", ")", "\n", "start_loss", "=", "paddle", ".", "mean", "(", "start_loss", ")", "\n", "end_loss", "=", "paddle", ".", "nn", ".", "functional", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "end_logits", ",", "label", "=", "end_position", ",", "soft_label", "=", "False", ")", "\n", "end_loss", "=", "paddle", ".", "mean", "(", "end_loss", ")", "\n", "cls_loss", "=", "paddle", ".", "nn", ".", "functional", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "cls_logits", ",", "label", "=", "answerable_label", ",", "soft_label", "=", "False", ")", "\n", "cls_loss", "=", "paddle", ".", "mean", "(", "cls_loss", ")", "\n", "mrc_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "loss", "=", "(", "mrc_loss", "+", "cls_loss", ")", "/", "2", "\n", "return", "loss", "\n", "\n", "\n", "", "", "def", "evaluate", "(", "model", ",", "data_loader", ",", "args", ",", "prefix", "=", "\"\"", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "all_start_logits", "=", "[", "]", "\n", "all_end_logits", "=", "[", "]", "\n", "all_cls_logits", "=", "[", "]", "\n", "tic_eval", "=", "time", ".", "time", "(", ")", "\n", "\n", "for", "batch", "in", "data_loader", ":", "\n", "        ", "input_ids", ",", "segment_ids", "=", "batch", "\n", "start_logits_tensor", ",", "end_logits_tensor", ",", "cls_logits_tensor", "=", "model", "(", "input_ids", ",", "segment_ids", ")", "\n", "\n", "for", "idx", "in", "range", "(", "start_logits_tensor", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "if", "len", "(", "all_start_logits", ")", "%", "1000", "==", "0", "and", "len", "(", "all_start_logits", ")", ":", "\n", "                ", "print", "(", "\"Processing example: %d\"", "%", "len", "(", "all_start_logits", ")", ")", "\n", "print", "(", "'time per 1000:'", ",", "time", ".", "time", "(", ")", "-", "tic_eval", ")", "\n", "tic_eval", "=", "time", ".", "time", "(", ")", "\n", "\n", "\n", "", "all_start_logits", ".", "append", "(", "start_logits_tensor", ".", "numpy", "(", ")", "[", "idx", "]", ")", "\n", "all_end_logits", ".", "append", "(", "end_logits_tensor", ".", "numpy", "(", ")", "[", "idx", "]", ")", "\n", "all_cls_logits", ".", "append", "(", "cls_logits_tensor", ".", "numpy", "(", ")", "[", "idx", "]", ")", "\n", "\n", "", "", "all_predictions", ",", "all_nbest_json", ",", "all_cls_predictions", "=", "compute_prediction_checklist", "(", "\n", "data_loader", ".", "dataset", ".", "data", ",", "data_loader", ".", "dataset", ".", "new_data", ",", "\n", "(", "all_start_logits", ",", "all_end_logits", ",", "all_cls_logits", ")", ",", "True", ",", "args", ".", "n_best_size", ",", "\n", "args", ".", "max_answer_length", ",", "args", ".", "cls_threshold", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "prefix", "+", "'_predictions.json'", ")", ",", "\"w\"", ",", "encoding", "=", "'utf-8'", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "\n", "json", ".", "dumps", "(", "\n", "all_predictions", ",", "ensure_ascii", "=", "False", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.prepare": [[106, 140], ["logging.getLogger", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "dataset.BRCDataset", "vocab.Vocab", "dataset.BRCDataset.word_iter", "vocab.Vocab.size", "vocab.Vocab.filter_tokens_by_cnt", "logging.getLogger.info", "logging.getLogger.info", "vocab.Vocab.randomly_init_embeddings", "logging.getLogger.info", "logging.getLogger.info", "os.path.exists", "vocab.Vocab.add", "vocab.Vocab.size", "open", "pickle.dump", "os.path.exists", "os.makedirs", "vocab.Vocab.size", "os.path.join"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.word_iter", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.filter_tokens_by_cnt", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.randomly_init_embeddings", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size"], ["", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "prefix", "+", "'_nbest_predictions.json'", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_nbest_json", ",", "indent", "=", "4", ",", "ensure_ascii", "=", "False", ")", "+", "u\"\\n\"", ")", "\n", "\n", "", "if", "all_cls_predictions", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "prefix", "+", "\"_cls_preditions.json\"", ")", ",", "\"w\"", ")", "as", "f_cls", ":", "\n", "            ", "for", "cls_predictions", "in", "all_cls_predictions", ":", "\n", "                ", "qas_id", ",", "pred_cls_label", ",", "no_answer_prob", ",", "answerable_prob", "=", "cls_predictions", "\n", "f_cls", ".", "write", "(", "'{}\\t{}\\t{}\\t{}\\n'", ".", "format", "(", "qas_id", ",", "pred_cls_label", ",", "no_answer_prob", ",", "answerable_prob", ")", ")", "\n", "\n", "", "", "", "model", ".", "train", "(", ")", "\n", "\n", "\n", "", "def", "run", "(", "args", ")", ":", "\n", "    ", "paddle", ".", "set_device", "(", "args", ".", "device", ")", "\n", "if", "paddle", ".", "distributed", ".", "get_world_size", "(", ")", ">", "1", ":", "\n", "        ", "paddle", ".", "distributed", ".", "init_parallel_env", "(", ")", "\n", "\n", "", "args", ".", "model_type", "=", "args", ".", "model_type", ".", "lower", "(", ")", "\n", "model_class", ",", "tokenizer_class", "=", "MODEL_CLASSES", "[", "args", ".", "model_type", "]", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ")", "\n", "set_seed", "(", "args", ")", "\n", "\n", "if", "paddle", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "model_name_or_path", ")", ":", "\n", "            ", "print", "(", "\"init checkpoint from %s\"", "%", "args", ".", "model_name_or_path", ")", "\n", "", "", "model", "=", "model_class", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ")", "\n", "\n", "if", "paddle", ".", "distributed", ".", "get_world_size", "(", ")", ">", "1", ":", "\n", "        ", "model", "=", "paddle", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "def", "prepare_train_features", "(", "examples", ")", ":", "\n", "# Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results", "\n", "# in one example possible giving several features when a context is long, each of those features having a", "\n", "# context that overlaps a bit the context of the previous feature.", "\n", "        ", "contexts", "=", "[", "examples", "[", "i", "]", "[", "'context'", "]", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", "]", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.train": [[142, 161], ["logging.getLogger", "logging.getLogger.info", "dataset.BRCDataset", "logging.getLogger.info", "dataset.BRCDataset.convert_to_ids", "logging.getLogger.info", "rc_model.RCModel", "logging.getLogger.info", "rc_model.RCModel.train", "logging.getLogger.info", "open", "pickle.load", "os.path.join"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.convert_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.train"], ["\n", "tokenized_examples", "=", "tokenizer", "(", "\n", "questions", ",", "\n", "contexts", ",", "\n", "stride", "=", "args", ".", "doc_stride", ",", "\n", "max_seq_len", "=", "args", ".", "max_seq_length", ")", "\n", "\n", "for", "i", ",", "tokenized_example", "in", "enumerate", "(", "tokenized_examples", ")", ":", "\n", "# We will label impossible answers with the index of the CLS token.", "\n", "            ", "input_ids", "=", "tokenized_example", "[", "\"input_ids\"", "]", "\n", "cls_index", "=", "input_ids", ".", "index", "(", "tokenizer", ".", "cls_token_id", ")", "\n", "\n", "# The offset mappings will give us a map from token to character position in the original context. This will", "\n", "# help us compute the start_positions and end_positions.", "\n", "offsets", "=", "tokenized_example", "[", "'offset_mapping'", "]", "\n", "\n", "# Grab the sequence corresponding to that example (to know what is the context and what is the question).", "\n", "sequence_ids", "=", "tokenized_example", "[", "'token_type_ids'", "]", "\n", "\n", "# One example can give several spans, this is the index of the example containing this span of text.", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.evaluate": [[163, 186], ["logging.getLogger", "logging.getLogger.info", "dataset.BRCDataset", "logging.getLogger.info", "dataset.BRCDataset.convert_to_ids", "logging.getLogger.info", "rc_model.RCModel", "rc_model.RCModel.restore", "logging.getLogger.info", "dataset.BRCDataset.gen_mini_batches", "rc_model.RCModel.evaluate", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "open", "pickle.load", "len", "os.path.join", "pickle.load.get_id", "os.path.join"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.convert_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.restore", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.gen_mini_batches", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.evaluate", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_id"], ["answers", "=", "examples", "[", "sample_index", "]", "[", "'answers'", "]", "\n", "answer_starts", "=", "examples", "[", "sample_index", "]", "[", "'answer_starts'", "]", "\n", "\n", "# If no answers are given, set the cls_index as answer.", "\n", "if", "len", "(", "answer_starts", ")", "==", "0", ":", "\n", "                ", "tokenized_examples", "[", "i", "]", "[", "\"start_positions\"", "]", "=", "cls_index", "\n", "tokenized_examples", "[", "i", "]", "[", "\"end_positions\"", "]", "=", "cls_index", "\n", "tokenized_examples", "[", "i", "]", "[", "'answerable_label'", "]", "=", "0", "\n", "", "else", ":", "\n", "# Start/end character index of the answer in the text.", "\n", "                ", "start_char", "=", "answer_starts", "[", "0", "]", "\n", "end_char", "=", "start_char", "+", "len", "(", "answers", "[", "0", "]", ")", "\n", "\n", "# Start token index of the current span in the text.", "\n", "token_start_index", "=", "0", "\n", "while", "sequence_ids", "[", "token_start_index", "]", "!=", "1", ":", "\n", "                    ", "token_start_index", "+=", "1", "\n", "\n", "# End token index of the current span in the text.", "\n", "", "token_end_index", "=", "len", "(", "input_ids", ")", "-", "2", "\n", "while", "sequence_ids", "[", "token_end_index", "]", "!=", "1", ":", "\n", "                    ", "token_end_index", "-=", "1", "\n", "\n", "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.predict": [[188, 209], ["logging.getLogger", "logging.getLogger.info", "dataset.BRCDataset", "logging.getLogger.info", "dataset.BRCDataset.convert_to_ids", "logging.getLogger.info", "rc_model.RCModel", "rc_model.RCModel.restore", "logging.getLogger.info", "dataset.BRCDataset.gen_mini_batches", "rc_model.RCModel.evaluate", "open", "pickle.load", "len", "os.path.join", "pickle.load.get_id"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.convert_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.restore", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.gen_mini_batches", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.evaluate", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_id"], ["offsets", "[", "token_end_index", "]", "[", "1", "]", ">=", "end_char", ")", ":", "\n", "                    ", "tokenized_examples", "[", "i", "]", "[", "\"start_positions\"", "]", "=", "cls_index", "\n", "tokenized_examples", "[", "i", "]", "[", "\"end_positions\"", "]", "=", "cls_index", "\n", "tokenized_examples", "[", "i", "]", "[", "'answerable_label'", "]", "=", "0", "\n", "", "else", ":", "\n", "# Otherwise move the token_start_index and token_end_index to the two ends of the answer.", "\n", "# Note: we could go after the last offset if the answer is the last word (edge case).", "\n", "                    ", "while", "token_start_index", "<", "len", "(", "offsets", ")", "and", "offsets", "[", "\n", "token_start_index", "]", "[", "0", "]", "<=", "start_char", ":", "\n", "                        ", "token_start_index", "+=", "1", "\n", "", "tokenized_examples", "[", "i", "]", "[", "\n", "\"start_positions\"", "]", "=", "token_start_index", "-", "1", "\n", "while", "offsets", "[", "token_end_index", "]", "[", "1", "]", ">=", "end_char", ":", "\n", "                        ", "token_end_index", "-=", "1", "\n", "", "tokenized_examples", "[", "i", "]", "[", "\"end_positions\"", "]", "=", "token_end_index", "+", "1", "\n", "tokenized_examples", "[", "i", "]", "[", "'answerable_label'", "]", "=", "1", "\n", "\n", "", "", "", "return", "tokenized_examples", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "assert", "args", ".", "train_file", "!=", "None", ",", "\"--train_file should be set when training!\"", "\n", "train_ds", "=", "DuReaderChecklist", "(", ")", ".", "read", "(", "args", ".", "train_file", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run": [[211, 244], ["run.parse_args", "logging.getLogger", "logging.getLogger.setLevel", "logging.Formatter", "logging.getLogger.info", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.FileHandler.setFormatter", "logging.getLogger.addHandler", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler", "run.prepare", "run.train", "run.evaluate", "run.predict"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.prepare", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.train", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.evaluate", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict"], ["\n", "train_batch_sampler", "=", "paddle", ".", "io", ".", "DistributedBatchSampler", "(", "\n", "train_ds", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "train_batchify_fn", "=", "lambda", "samples", ",", "fn", "=", "Dict", "(", "{", "\n", "\"input_ids\"", ":", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "tokenizer", ".", "pad_token_id", ")", ",", "\n", "\"token_type_ids\"", ":", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "tokenizer", ".", "pad_token_type_id", ")", ",", "\n", "\"start_positions\"", ":", "Stack", "(", "dtype", "=", "\"int64\"", ")", ",", "\n", "\"end_positions\"", ":", "Stack", "(", "dtype", "=", "\"int64\"", ")", ",", "\n", "\"answerable_label\"", ":", "Stack", "(", "dtype", "=", "\"int64\"", ")", "\n", "}", ")", ":", "fn", "(", "samples", ")", "\n", "train_data_loader", "=", "DataLoader", "(", "\n", "dataset", "=", "train_ds", ",", "\n", "batch_sampler", "=", "train_batch_sampler", ",", "\n", "collate_fn", "=", "train_batchify_fn", ",", "\n", "return_list", "=", "True", ")", "\n", "\n", "num_training_steps", "=", "args", ".", "max_steps", "if", "args", ".", "max_steps", ">", "0", "else", "len", "(", "\n", "train_data_loader", ")", "*", "args", ".", "num_train_epochs", "\n", "\n", "if", "paddle", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "            ", "dev_count", "=", "paddle", ".", "fluid", ".", "core", ".", "get_cuda_device_count", "(", ")", "\n", "print", "(", "\"Device count: %d\"", "%", "dev_count", ")", "\n", "print", "(", "\"Num train examples: %d\"", "%", "len", "(", "train_ds", ".", "data", ")", ")", "\n", "print", "(", "\"Max train steps: %d\"", "%", "num_training_steps", ")", "\n", "\n", "", "lr_scheduler", "=", "LinearDecayWithWarmup", "(", "\n", "args", ".", "learning_rate", ",", "num_training_steps", ",", "args", ".", "warmup_proportion", ")", "\n", "\n", "optimizer", "=", "paddle", ".", "optimizer", ".", "AdamW", "(", "\n", "learning_rate", "=", "lr_scheduler", ",", "\n", "epsilon", "=", "args", ".", "adam_epsilon", ",", "\n", "parameters", "=", "model", ".", "parameters", "(", ")", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", "apply_decay_param_fun", "=", "lambda", "x", ":", "x", "in", "[", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.vocab.Vocab.__init__": [[28, 47], ["vocab.Vocab.initial_tokens.extend", "vocab.Vocab.add", "vocab.Vocab.load_from_file"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.load_from_file"], ["def", "__init__", "(", "self", ",", "filename", "=", "None", ",", "initial_tokens", "=", "None", ",", "lower", "=", "False", ")", ":", "\n", "        ", "self", ".", "id2token", "=", "{", "}", "\n", "self", ".", "token2id", "=", "{", "}", "\n", "self", ".", "token_cnt", "=", "{", "}", "\n", "self", ".", "lower", "=", "lower", "\n", "\n", "self", ".", "embed_dim", "=", "None", "\n", "self", ".", "embeddings", "=", "None", "\n", "\n", "self", ".", "pad_token", "=", "'<blank>'", "\n", "self", ".", "unk_token", "=", "'<unk>'", "\n", "\n", "self", ".", "initial_tokens", "=", "initial_tokens", "if", "initial_tokens", "is", "not", "None", "else", "[", "]", "\n", "self", ".", "initial_tokens", ".", "extend", "(", "[", "self", ".", "pad_token", ",", "self", ".", "unk_token", "]", ")", "\n", "for", "token", "in", "self", ".", "initial_tokens", ":", "\n", "            ", "self", ".", "add", "(", "token", ")", "\n", "\n", "", "if", "filename", "is", "not", "None", ":", "\n", "            ", "self", ".", "load_from_file", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.vocab.Vocab.size": [[48, 55], ["len"], "methods", ["None"], ["", "", "def", "size", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        get the size of vocabulary\n        Returns:\n            an integer indicating the size\n        \"\"\"", "\n", "return", "len", "(", "self", ".", "id2token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.vocab.Vocab.load_from_file": [[56, 65], ["open", "line.rstrip", "vocab.Vocab.add"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add"], ["", "def", "load_from_file", "(", "self", ",", "file_path", ")", ":", "\n", "        ", "\"\"\"\n        loads the vocab from file_path\n        Args:\n            file_path: a file with a word in each line\n        \"\"\"", "\n", "for", "line", "in", "open", "(", "file_path", ",", "'r'", ")", ":", "\n", "            ", "token", "=", "line", ".", "rstrip", "(", "'\\n'", ")", "\n", "self", ".", "add", "(", "token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.vocab.Vocab.get_id": [[66, 79], ["token.lower"], "methods", ["None"], ["", "", "def", "get_id", "(", "self", ",", "token", ")", ":", "\n", "        ", "\"\"\"\n        gets the id of a token, returns the id of unk token if token is not in vocab\n        Args:\n            key: a string indicating the word\n        Returns:\n            an integer\n        \"\"\"", "\n", "token", "=", "token", ".", "lower", "(", ")", "if", "self", ".", "lower", "else", "token", "\n", "try", ":", "\n", "            ", "return", "self", ".", "token2id", "[", "token", "]", "\n", "", "except", "KeyError", ":", "\n", "            ", "return", "self", ".", "token2id", "[", "self", ".", "unk_token", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.vocab.Vocab.get_token": [[80, 92], ["None"], "methods", ["None"], ["", "", "def", "get_token", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"\n        gets the token corresponding to idx, returns unk token if idx is not in vocab\n        Args:\n            idx: an integer\n        returns:\n            a token string\n        \"\"\"", "\n", "try", ":", "\n", "            ", "return", "self", ".", "id2token", "[", "idx", "]", "\n", "", "except", "KeyError", ":", "\n", "            ", "return", "self", ".", "unk_token", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.vocab.Vocab.add": [[93, 113], ["token.lower", "len"], "methods", ["None"], ["", "", "def", "add", "(", "self", ",", "token", ",", "cnt", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        adds the token to vocab\n        Args:\n            token: a string\n            cnt: a num indicating the count of the token to add, default is 1\n        \"\"\"", "\n", "token", "=", "token", ".", "lower", "(", ")", "if", "self", ".", "lower", "else", "token", "\n", "if", "token", "in", "self", ".", "token2id", ":", "\n", "            ", "idx", "=", "self", ".", "token2id", "[", "token", "]", "\n", "", "else", ":", "\n", "            ", "idx", "=", "len", "(", "self", ".", "id2token", ")", "\n", "self", ".", "id2token", "[", "idx", "]", "=", "token", "\n", "self", ".", "token2id", "[", "token", "]", "=", "idx", "\n", "", "if", "cnt", ">", "0", ":", "\n", "            ", "if", "token", "in", "self", ".", "token_cnt", ":", "\n", "                ", "self", ".", "token_cnt", "[", "token", "]", "+=", "cnt", "\n", "", "else", ":", "\n", "                ", "self", ".", "token_cnt", "[", "token", "]", "=", "cnt", "\n", "", "", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.vocab.Vocab.filter_tokens_by_cnt": [[114, 128], ["vocab.Vocab.add", "vocab.Vocab.add"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add"], ["", "def", "filter_tokens_by_cnt", "(", "self", ",", "min_cnt", ")", ":", "\n", "        ", "\"\"\"\n        filter the tokens in vocab by their count\n        Args:\n            min_cnt: tokens with frequency less than min_cnt is filtered\n        \"\"\"", "\n", "filtered_tokens", "=", "[", "token", "for", "token", "in", "self", ".", "token2id", "if", "self", ".", "token_cnt", "[", "token", "]", ">=", "min_cnt", "]", "\n", "# rebuild the token x id map", "\n", "self", ".", "token2id", "=", "{", "}", "\n", "self", ".", "id2token", "=", "{", "}", "\n", "for", "token", "in", "self", ".", "initial_tokens", ":", "\n", "            ", "self", ".", "add", "(", "token", ",", "cnt", "=", "0", ")", "\n", "", "for", "token", "in", "filtered_tokens", ":", "\n", "            ", "self", ".", "add", "(", "token", ",", "cnt", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.vocab.Vocab.randomly_init_embeddings": [[129, 139], ["numpy.random.rand", "vocab.Vocab.size", "numpy.zeros", "vocab.Vocab.get_id"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_id"], ["", "", "def", "randomly_init_embeddings", "(", "self", ",", "embed_dim", ")", ":", "\n", "        ", "\"\"\"\n        randomly initializes the embeddings for each token\n        Args:\n            embed_dim: the size of the embedding for each token\n        \"\"\"", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "embeddings", "=", "np", ".", "random", ".", "rand", "(", "self", ".", "size", "(", ")", ",", "embed_dim", ")", "\n", "for", "token", "in", "[", "self", ".", "pad_token", ",", "self", ".", "unk_token", "]", ":", "\n", "            ", "self", ".", "embeddings", "[", "self", ".", "get_id", "(", "token", ")", "]", "=", "np", ".", "zeros", "(", "[", "self", ".", "embed_dim", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.vocab.Vocab.load_pretrained_embeddings": [[140, 170], ["trained_embeddings.keys", "numpy.zeros", "vocab.Vocab.token2id.keys", "open", "vocab.Vocab.add", "vocab.Vocab.add", "line.strip().split", "contents[].decode", "list", "vocab.Vocab.size", "map", "line.strip", "len", "vocab.Vocab.get_id"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_id"], ["", "", "def", "load_pretrained_embeddings", "(", "self", ",", "embedding_path", ")", ":", "\n", "        ", "\"\"\"\n        loads the pretrained embeddings from embedding_path,\n        tokens not in pretrained embeddings will be filtered\n        Args:\n            embedding_path: the path of the pretrained embedding file\n        \"\"\"", "\n", "trained_embeddings", "=", "{", "}", "\n", "with", "open", "(", "embedding_path", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "contents", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "token", "=", "contents", "[", "0", "]", ".", "decode", "(", "'utf8'", ")", "\n", "if", "token", "not", "in", "self", ".", "token2id", ":", "\n", "                    ", "continue", "\n", "", "trained_embeddings", "[", "token", "]", "=", "list", "(", "map", "(", "float", ",", "contents", "[", "1", ":", "]", ")", ")", "\n", "if", "self", ".", "embed_dim", "is", "None", ":", "\n", "                    ", "self", ".", "embed_dim", "=", "len", "(", "contents", ")", "-", "1", "\n", "", "", "", "filtered_tokens", "=", "trained_embeddings", ".", "keys", "(", ")", "\n", "# rebuild the token x id map", "\n", "self", ".", "token2id", "=", "{", "}", "\n", "self", ".", "id2token", "=", "{", "}", "\n", "for", "token", "in", "self", ".", "initial_tokens", ":", "\n", "            ", "self", ".", "add", "(", "token", ",", "cnt", "=", "0", ")", "\n", "", "for", "token", "in", "filtered_tokens", ":", "\n", "            ", "self", ".", "add", "(", "token", ",", "cnt", "=", "0", ")", "\n", "# load embeddings", "\n", "", "self", ".", "embeddings", "=", "np", ".", "zeros", "(", "[", "self", ".", "size", "(", ")", ",", "self", ".", "embed_dim", "]", ")", "\n", "for", "token", "in", "self", ".", "token2id", ".", "keys", "(", ")", ":", "\n", "            ", "if", "token", "in", "trained_embeddings", ":", "\n", "                ", "self", ".", "embeddings", "[", "self", ".", "get_id", "(", "token", ")", "]", "=", "trained_embeddings", "[", "token", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.vocab.Vocab.convert_to_ids": [[171, 181], ["vocab.Vocab.get_id"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_id"], ["", "", "", "def", "convert_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"\n        Convert a list of tokens to ids, use unk_token if the token is not in vocab.\n        Args:\n            tokens: a list of token\n        Returns:\n            a list of ids\n        \"\"\"", "\n", "vec", "=", "[", "self", ".", "get_id", "(", "label", ")", "for", "label", "in", "tokens", "]", "\n", "return", "vec", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.vocab.Vocab.recover_from_ids": [[182, 197], ["vocab.Vocab.get_token"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_token"], ["", "def", "recover_from_ids", "(", "self", ",", "ids", ",", "stop_id", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Convert a list of ids to tokens, stop converting if the stop_id is encountered\n        Args:\n            ids: a list of ids to convert\n            stop_id: the stop id, default is None\n        Returns:\n            a list of tokens\n        \"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", "+=", "[", "self", ".", "get_token", "(", "i", ")", "]", "\n", "if", "stop_id", "is", "not", "None", "and", "i", "==", "stop_id", ":", "\n", "                ", "break", "\n", "", "", "return", "tokens", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetLSTMCell.__init__": [[113, 119], ["super().__init__", "tensorflow.layers.fully_connected", "tensorflow.layers.fully_connected"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.__init__"], ["def", "__init__", "(", "self", ",", "num_units", ",", "context_to_point", ")", ":", "\n", "        ", "super", "(", "PointerNetLSTMCell", ",", "self", ")", ".", "__init__", "(", "num_units", ",", "state_is_tuple", "=", "True", ")", "\n", "self", ".", "context_to_point", "=", "context_to_point", "\n", "self", ".", "fc_context", "=", "tc", ".", "layers", ".", "fully_connected", "(", "self", ".", "context_to_point", ",", "\n", "num_outputs", "=", "self", ".", "_num_units", ",", "\n", "activation_fn", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetLSTMCell.__call__": [[120, 133], ["tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.layers.fully_connected", "tensorflow.layers.fully_connected", "tensorflow.nn.softmax", "tensorflow.nn.softmax", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "super().__call__", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.expand_dims", "tensorflow.expand_dims", "type", "tensorflow.layers.fully_connected", "tensorflow.layers.fully_connected"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.match_layer.MatchLSTMAttnCell.__call__"], ["", "def", "__call__", "(", "self", ",", "inputs", ",", "state", ",", "scope", "=", "None", ")", ":", "\n", "        ", "(", "c_prev", ",", "m_prev", ")", "=", "state", "\n", "with", "tf", ".", "variable_scope", "(", "scope", "or", "type", "(", "self", ")", ".", "__name__", ")", ":", "\n", "            ", "U", "=", "tf", ".", "tanh", "(", "self", ".", "fc_context", "\n", "+", "tf", ".", "expand_dims", "(", "tc", ".", "layers", ".", "fully_connected", "(", "m_prev", ",", "\n", "num_outputs", "=", "self", ".", "_num_units", ",", "\n", "activation_fn", "=", "None", ")", ",", "\n", "1", ")", ")", "\n", "logits", "=", "tc", ".", "layers", ".", "fully_connected", "(", "U", ",", "num_outputs", "=", "1", ",", "activation_fn", "=", "None", ")", "\n", "scores", "=", "tf", ".", "nn", ".", "softmax", "(", "logits", ",", "1", ")", "\n", "attended_context", "=", "tf", ".", "reduce_sum", "(", "self", ".", "context_to_point", "*", "scores", ",", "axis", "=", "1", ")", "\n", "lstm_out", ",", "lstm_state", "=", "super", "(", "PointerNetLSTMCell", ",", "self", ")", ".", "__call__", "(", "attended_context", ",", "state", ")", "\n", "", "return", "tf", ".", "squeeze", "(", "scores", ",", "-", "1", ")", ",", "lstm_state", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.__init__": [[139, 141], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "hidden_size", ")", ":", "\n", "        ", "self", ".", "hidden_size", "=", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode": [[142, 176], ["tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.tile", "tensorflow.tile", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.layers.fully_connected", "tensorflow.layers.fully_connected", "tensorflow.rnn.LSTMStateTuple", "tensorflow.rnn.LSTMStateTuple", "tensorflow.variable_scope", "tensorflow.variable_scope", "pointer_net.PointerNetLSTMCell", "pointer_net.custom_dynamic_rnn", "tensorflow.variable_scope", "tensorflow.variable_scope", "pointer_net.PointerNetLSTMCell", "pointer_net.custom_dynamic_rnn", "tensorflow.random_normal", "tensorflow.random_normal", "pointer_net.attend_pooling", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.custom_dynamic_rnn", "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.custom_dynamic_rnn", "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.attend_pooling"], ["", "def", "decode", "(", "self", ",", "passage_vectors", ",", "question_vectors", ",", "init_with_question", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Use Pointer Network to compute the probabilities of each position\n        to be start and end of the answer\n        Args:\n            passage_vectors: the encoded passage vectors\n            question_vectors: the encoded question vectors\n            init_with_question: if set to be true,\n                             we will use the question_vectors to init the state of Pointer Network\n        Returns:\n            the probs of evary position to be start and end of the answer\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'pn_decoder'", ")", ":", "\n", "            ", "fake_inputs", "=", "tf", ".", "zeros", "(", "[", "tf", ".", "shape", "(", "passage_vectors", ")", "[", "0", "]", ",", "2", ",", "1", "]", ")", "# not used", "\n", "sequence_len", "=", "tf", ".", "tile", "(", "[", "2", "]", ",", "[", "tf", ".", "shape", "(", "passage_vectors", ")", "[", "0", "]", "]", ")", "\n", "if", "init_with_question", ":", "\n", "                ", "random_attn_vector", "=", "tf", ".", "Variable", "(", "tf", ".", "random_normal", "(", "[", "1", ",", "self", ".", "hidden_size", "]", ")", ",", "\n", "trainable", "=", "True", ",", "name", "=", "\"random_attn_vector\"", ")", "\n", "pooled_question_rep", "=", "tc", ".", "layers", ".", "fully_connected", "(", "\n", "attend_pooling", "(", "question_vectors", ",", "random_attn_vector", ",", "self", ".", "hidden_size", ")", ",", "\n", "num_outputs", "=", "self", ".", "hidden_size", ",", "activation_fn", "=", "None", "\n", ")", "\n", "init_state", "=", "tc", ".", "rnn", ".", "LSTMStateTuple", "(", "pooled_question_rep", ",", "pooled_question_rep", ")", "\n", "", "else", ":", "\n", "                ", "init_state", "=", "None", "\n", "", "with", "tf", ".", "variable_scope", "(", "'fw'", ")", ":", "\n", "                ", "fw_cell", "=", "PointerNetLSTMCell", "(", "self", ".", "hidden_size", ",", "passage_vectors", ")", "\n", "fw_outputs", ",", "_", "=", "custom_dynamic_rnn", "(", "fw_cell", ",", "fake_inputs", ",", "sequence_len", ",", "init_state", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'bw'", ")", ":", "\n", "                ", "bw_cell", "=", "PointerNetLSTMCell", "(", "self", ".", "hidden_size", ",", "passage_vectors", ")", "\n", "bw_outputs", ",", "_", "=", "custom_dynamic_rnn", "(", "bw_cell", ",", "fake_inputs", ",", "sequence_len", ",", "init_state", ")", "\n", "", "start_prob", "=", "(", "fw_outputs", "[", "0", ":", ",", "0", ",", "0", ":", "]", "+", "bw_outputs", "[", "0", ":", ",", "1", ",", "0", ":", "]", ")", "/", "2", "\n", "end_prob", "=", "(", "fw_outputs", "[", "0", ":", ",", "1", ",", "0", ":", "]", "+", "bw_outputs", "[", "0", ":", ",", "0", ",", "0", ":", "]", ")", "/", "2", "\n", "return", "start_prob", ",", "end_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.custom_dynamic_rnn": [[26, 84], ["tensorflow.TensorArray", "inputs_ta.unstack.unstack", "tensorflow.TensorArray", "tensorflow.constant", "tensorflow.zeros", "tensorflow.while_loop", "tensorflow.transpose", "tensorflow.shape", "tensorflow.shape", "tensorflow.transpose", "cell.zero_state", "inputs_ta.unstack.read", "cell", "tensorflow.where", "isinstance", "emit_ta.write.write", "tensorflow.greater_equal", "emit_ta.write.stack", "tensorflow.zeros_like", "tensorflow.rnn.LSTMStateTuple", "tensorflow.where", "tensorflow.where", "tensorflow.where", "tensorflow.logical_not", "tensorflow.reduce_all"], "function", ["None"], ["def", "custom_dynamic_rnn", "(", "cell", ",", "inputs", ",", "inputs_len", ",", "initial_state", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Implements a dynamic rnn that can store scores in the pointer network,\n    the reason why we implements this is that the raw_rnn or dynamic_rnn function in Tensorflow\n    seem to require the hidden unit and memory unit has the same dimension, and we cannot\n    store the scores directly in the hidden unit.\n    Args:\n        cell: RNN cell\n        inputs: the input sequence to rnn\n        inputs_len: valid length\n        initial_state: initial_state of the cell\n    Returns:\n        outputs and state\n    \"\"\"", "\n", "batch_size", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "max_time", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "1", "]", "\n", "\n", "inputs_ta", "=", "tf", ".", "TensorArray", "(", "dtype", "=", "tf", ".", "float32", ",", "size", "=", "max_time", ")", "\n", "inputs_ta", "=", "inputs_ta", ".", "unstack", "(", "tf", ".", "transpose", "(", "inputs", ",", "[", "1", ",", "0", ",", "2", "]", ")", ")", "\n", "emit_ta", "=", "tf", ".", "TensorArray", "(", "dtype", "=", "tf", ".", "float32", ",", "dynamic_size", "=", "True", ",", "size", "=", "0", ")", "\n", "t0", "=", "tf", ".", "constant", "(", "0", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "if", "initial_state", "is", "not", "None", ":", "\n", "        ", "s0", "=", "initial_state", "\n", "", "else", ":", "\n", "        ", "s0", "=", "cell", ".", "zero_state", "(", "batch_size", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "", "f0", "=", "tf", ".", "zeros", "(", "[", "batch_size", "]", ",", "dtype", "=", "tf", ".", "bool", ")", "\n", "\n", "def", "loop_fn", "(", "t", ",", "prev_s", ",", "emit_ta", ",", "finished", ")", ":", "\n", "        ", "\"\"\"\n        the loop function of rnn\n        \"\"\"", "\n", "cur_x", "=", "inputs_ta", ".", "read", "(", "t", ")", "\n", "scores", ",", "cur_state", "=", "cell", "(", "cur_x", ",", "prev_s", ")", "\n", "\n", "# copy through", "\n", "scores", "=", "tf", ".", "where", "(", "finished", ",", "tf", ".", "zeros_like", "(", "scores", ")", ",", "scores", ")", "\n", "\n", "if", "isinstance", "(", "cell", ",", "tc", ".", "rnn", ".", "LSTMCell", ")", ":", "\n", "            ", "cur_c", ",", "cur_h", "=", "cur_state", "\n", "prev_c", ",", "prev_h", "=", "prev_s", "\n", "cur_state", "=", "tc", ".", "rnn", ".", "LSTMStateTuple", "(", "tf", ".", "where", "(", "finished", ",", "prev_c", ",", "cur_c", ")", ",", "\n", "tf", ".", "where", "(", "finished", ",", "prev_h", ",", "cur_h", ")", ")", "\n", "", "else", ":", "\n", "            ", "cur_state", "=", "tf", ".", "where", "(", "finished", ",", "prev_s", ",", "cur_state", ")", "\n", "\n", "", "emit_ta", "=", "emit_ta", ".", "write", "(", "t", ",", "scores", ")", "\n", "finished", "=", "tf", ".", "greater_equal", "(", "t", "+", "1", ",", "inputs_len", ")", "\n", "return", "[", "t", "+", "1", ",", "cur_state", ",", "emit_ta", ",", "finished", "]", "\n", "\n", "", "_", ",", "state", ",", "emit_ta", ",", "_", "=", "tf", ".", "while_loop", "(", "\n", "cond", "=", "lambda", "_1", ",", "_2", ",", "_3", ",", "finished", ":", "tf", ".", "logical_not", "(", "tf", ".", "reduce_all", "(", "finished", ")", ")", ",", "\n", "body", "=", "loop_fn", ",", "\n", "loop_vars", "=", "(", "t0", ",", "s0", ",", "emit_ta", ",", "f0", ")", ",", "\n", "parallel_iterations", "=", "32", ",", "\n", "swap_memory", "=", "False", ")", "\n", "\n", "outputs", "=", "tf", ".", "transpose", "(", "emit_ta", ".", "stack", "(", ")", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "return", "outputs", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.attend_pooling": [[86, 107], ["tensorflow.variable_scope", "tensorflow.tanh", "tensorflow.layers.fully_connected", "tensorflow.nn.softmax", "tensorflow.reduce_sum", "tensorflow.layers.fully_connected", "tensorflow.layers.fully_connected", "tensorflow.expand_dims"], "function", ["None"], ["", "def", "attend_pooling", "(", "pooling_vectors", ",", "ref_vector", ",", "hidden_size", ",", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Applies attend pooling to a set of vectors according to a reference vector.\n    Args:\n        pooling_vectors: the vectors to pool\n        ref_vector: the reference vector\n        hidden_size: the hidden size for attention function\n        scope: score name\n    Returns:\n        the pooled vector\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "scope", "or", "'attend_pooling'", ")", ":", "\n", "        ", "U", "=", "tf", ".", "tanh", "(", "tc", ".", "layers", ".", "fully_connected", "(", "pooling_vectors", ",", "num_outputs", "=", "hidden_size", ",", "\n", "activation_fn", "=", "None", ",", "biases_initializer", "=", "None", ")", "\n", "+", "tc", ".", "layers", ".", "fully_connected", "(", "tf", ".", "expand_dims", "(", "ref_vector", ",", "1", ")", ",", "\n", "num_outputs", "=", "hidden_size", ",", "\n", "activation_fn", "=", "None", ")", ")", "\n", "logits", "=", "tc", ".", "layers", ".", "fully_connected", "(", "U", ",", "num_outputs", "=", "1", ",", "activation_fn", "=", "None", ")", "\n", "scores", "=", "tf", ".", "nn", ".", "softmax", "(", "logits", ",", "1", ")", "\n", "pooled_vector", "=", "tf", ".", "reduce_sum", "(", "pooling_vectors", "*", "scores", ",", "axis", "=", "1", ")", "\n", "", "return", "pooled_vector", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.basic_rnn.rnn": [[25, 67], ["rnn_type.startswith", "basic_rnn.get_cell", "tensorflow.nn.dynamic_rnn", "rnn_type.endswith", "basic_rnn.get_cell", "basic_rnn.get_cell", "tensorflow.nn.bidirectional_dynamic_rnn", "rnn_type.endswith", "tensorflow.concat", "tensorflow.concat"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.basic_rnn.get_cell", "home.repos.pwc.inspect_result.baidu_DuReader.layers.basic_rnn.get_cell", "home.repos.pwc.inspect_result.baidu_DuReader.layers.basic_rnn.get_cell"], ["def", "rnn", "(", "rnn_type", ",", "inputs", ",", "length", ",", "hidden_size", ",", "layer_num", "=", "1", ",", "dropout_keep_prob", "=", "None", ",", "concat", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Implements (Bi-)LSTM, (Bi-)GRU and (Bi-)RNN\n    Args:\n        rnn_type: the type of rnn\n        inputs: padded inputs into rnn\n        length: the valid length of the inputs\n        hidden_size: the size of hidden units\n        layer_num: multiple rnn layer are stacked if layer_num > 1\n        dropout_keep_prob:\n        concat: When the rnn is bidirectional, the forward outputs and backward outputs are\n                concatenated if this is True, else we add them.\n    Returns:\n        RNN outputs and final state\n    \"\"\"", "\n", "if", "not", "rnn_type", ".", "startswith", "(", "'bi'", ")", ":", "\n", "        ", "cell", "=", "get_cell", "(", "rnn_type", ",", "hidden_size", ",", "layer_num", ",", "dropout_keep_prob", ")", "\n", "outputs", ",", "states", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", ",", "inputs", ",", "sequence_length", "=", "length", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "if", "rnn_type", ".", "endswith", "(", "'lstm'", ")", ":", "\n", "            ", "c", "=", "[", "state", ".", "c", "for", "state", "in", "states", "]", "\n", "h", "=", "[", "state", ".", "h", "for", "state", "in", "states", "]", "\n", "states", "=", "h", "\n", "", "", "else", ":", "\n", "        ", "cell_fw", "=", "get_cell", "(", "rnn_type", ",", "hidden_size", ",", "layer_num", ",", "dropout_keep_prob", ")", "\n", "cell_bw", "=", "get_cell", "(", "rnn_type", ",", "hidden_size", ",", "layer_num", ",", "dropout_keep_prob", ")", "\n", "outputs", ",", "states", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "\n", "cell_bw", ",", "cell_fw", ",", "inputs", ",", "sequence_length", "=", "length", ",", "dtype", "=", "tf", ".", "float32", "\n", ")", "\n", "states_fw", ",", "states_bw", "=", "states", "\n", "if", "rnn_type", ".", "endswith", "(", "'lstm'", ")", ":", "\n", "            ", "c_fw", "=", "[", "state_fw", ".", "c", "for", "state_fw", "in", "states_fw", "]", "\n", "h_fw", "=", "[", "state_fw", ".", "h", "for", "state_fw", "in", "states_fw", "]", "\n", "c_bw", "=", "[", "state_bw", ".", "c", "for", "state_bw", "in", "states_bw", "]", "\n", "h_bw", "=", "[", "state_bw", ".", "h", "for", "state_bw", "in", "states_bw", "]", "\n", "states_fw", ",", "states_bw", "=", "h_fw", ",", "h_bw", "\n", "", "if", "concat", ":", "\n", "            ", "outputs", "=", "tf", ".", "concat", "(", "outputs", ",", "2", ")", "\n", "states", "=", "tf", ".", "concat", "(", "[", "states_fw", ",", "states_bw", "]", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "outputs", "[", "0", "]", "+", "outputs", "[", "1", "]", "\n", "states", "=", "states_fw", "+", "states_bw", "\n", "", "", "return", "outputs", ",", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.basic_rnn.get_cell": [[69, 97], ["range", "tensorflow.rnn.MultiRNNCell", "rnn_type.endswith", "tc.rnn.MultiRNNCell.append", "tensorflow.rnn.LSTMCell", "rnn_type.endswith", "tensorflow.rnn.DropoutWrapper", "tensorflow.rnn.GRUCell", "rnn_type.endswith", "tensorflow.rnn.BasicRNNCell", "NotImplementedError"], "function", ["None"], ["", "def", "get_cell", "(", "rnn_type", ",", "hidden_size", ",", "layer_num", "=", "1", ",", "dropout_keep_prob", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Gets the RNN Cell\n    Args:\n        rnn_type: 'lstm', 'gru' or 'rnn'\n        hidden_size: The size of hidden units\n        layer_num: MultiRNNCell are used if layer_num > 1\n        dropout_keep_prob: dropout in RNN\n    Returns:\n        An RNN Cell\n    \"\"\"", "\n", "cells", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "layer_num", ")", ":", "\n", "        ", "if", "rnn_type", ".", "endswith", "(", "'lstm'", ")", ":", "\n", "            ", "cell", "=", "tc", ".", "rnn", ".", "LSTMCell", "(", "num_units", "=", "hidden_size", ",", "state_is_tuple", "=", "True", ")", "\n", "", "elif", "rnn_type", ".", "endswith", "(", "'gru'", ")", ":", "\n", "            ", "cell", "=", "tc", ".", "rnn", ".", "GRUCell", "(", "num_units", "=", "hidden_size", ")", "\n", "", "elif", "rnn_type", ".", "endswith", "(", "'rnn'", ")", ":", "\n", "            ", "cell", "=", "tc", ".", "rnn", ".", "BasicRNNCell", "(", "num_units", "=", "hidden_size", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Unsuported rnn type: {}'", ".", "format", "(", "rnn_type", ")", ")", "\n", "", "if", "dropout_keep_prob", "is", "not", "None", ":", "\n", "            ", "cell", "=", "tc", ".", "rnn", ".", "DropoutWrapper", "(", "cell", ",", "\n", "input_keep_prob", "=", "dropout_keep_prob", ",", "\n", "output_keep_prob", "=", "dropout_keep_prob", ")", "\n", "", "cells", ".", "append", "(", "cell", ")", "\n", "", "cells", "=", "tc", ".", "rnn", ".", "MultiRNNCell", "(", "cells", ",", "state_is_tuple", "=", "True", ")", "\n", "return", "cells", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.match_layer.MatchLSTMAttnCell.__init__": [[29, 35], ["super().__init__", "tensorflow.layers.fully_connected", "tensorflow.layers.fully_connected"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.__init__"], ["def", "__init__", "(", "self", ",", "num_units", ",", "context_to_attend", ")", ":", "\n", "        ", "super", "(", "MatchLSTMAttnCell", ",", "self", ")", ".", "__init__", "(", "num_units", ",", "state_is_tuple", "=", "True", ")", "\n", "self", ".", "context_to_attend", "=", "context_to_attend", "\n", "self", ".", "fc_context", "=", "tc", ".", "layers", ".", "fully_connected", "(", "self", ".", "context_to_attend", ",", "\n", "num_outputs", "=", "self", ".", "_num_units", ",", "\n", "activation_fn", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.match_layer.MatchLSTMAttnCell.__call__": [[36, 51], ["tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.concat", "tensorflow.concat", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.layers.fully_connected", "tensorflow.layers.fully_connected", "tensorflow.nn.softmax", "tensorflow.nn.softmax", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.concat", "tensorflow.concat", "super().__call__", "tensorflow.expand_dims", "tensorflow.expand_dims", "type", "tensorflow.layers.fully_connected", "tensorflow.layers.fully_connected"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.match_layer.MatchLSTMAttnCell.__call__"], ["", "def", "__call__", "(", "self", ",", "inputs", ",", "state", ",", "scope", "=", "None", ")", ":", "\n", "        ", "(", "c_prev", ",", "h_prev", ")", "=", "state", "\n", "with", "tf", ".", "variable_scope", "(", "scope", "or", "type", "(", "self", ")", ".", "__name__", ")", ":", "\n", "            ", "ref_vector", "=", "tf", ".", "concat", "(", "[", "inputs", ",", "h_prev", "]", ",", "-", "1", ")", "\n", "G", "=", "tf", ".", "tanh", "(", "self", ".", "fc_context", "\n", "+", "tf", ".", "expand_dims", "(", "tc", ".", "layers", ".", "fully_connected", "(", "ref_vector", ",", "\n", "num_outputs", "=", "self", ".", "_num_units", ",", "\n", "activation_fn", "=", "None", ")", ",", "1", ")", ")", "\n", "logits", "=", "tc", ".", "layers", ".", "fully_connected", "(", "G", ",", "num_outputs", "=", "1", ",", "activation_fn", "=", "None", ")", "\n", "scores", "=", "tf", ".", "nn", ".", "softmax", "(", "logits", ",", "1", ")", "\n", "attended_context", "=", "tf", ".", "reduce_sum", "(", "self", ".", "context_to_attend", "*", "scores", ",", "axis", "=", "1", ")", "\n", "new_inputs", "=", "tf", ".", "concat", "(", "[", "inputs", ",", "attended_context", ",", "\n", "inputs", "-", "attended_context", ",", "inputs", "*", "attended_context", "]", ",", "\n", "-", "1", ")", "\n", "return", "super", "(", "MatchLSTMAttnCell", ",", "self", ")", ".", "__call__", "(", "new_inputs", ",", "state", ",", "scope", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.match_layer.MatchLSTMLayer.__init__": [[57, 59], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "hidden_size", ")", ":", "\n", "        ", "self", ".", "hidden_size", "=", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.match_layer.MatchLSTMLayer.match": [[60, 77], ["tensorflow.variable_scope", "tensorflow.variable_scope", "match_layer.MatchLSTMAttnCell", "match_layer.MatchLSTMAttnCell", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat"], "methods", ["None"], ["", "def", "match", "(", "self", ",", "passage_encodes", ",", "question_encodes", ",", "p_length", ",", "q_length", ")", ":", "\n", "        ", "\"\"\"\n        Match the passage_encodes with question_encodes using Match-LSTM algorithm\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'match_lstm'", ")", ":", "\n", "            ", "cell_fw", "=", "MatchLSTMAttnCell", "(", "self", ".", "hidden_size", ",", "question_encodes", ")", "\n", "cell_bw", "=", "MatchLSTMAttnCell", "(", "self", ".", "hidden_size", ",", "question_encodes", ")", "\n", "outputs", ",", "state", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "cell_fw", ",", "cell_bw", ",", "\n", "inputs", "=", "passage_encodes", ",", "\n", "sequence_length", "=", "p_length", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "match_outputs", "=", "tf", ".", "concat", "(", "outputs", ",", "2", ")", "\n", "state_fw", ",", "state_bw", "=", "state", "\n", "c_fw", ",", "h_fw", "=", "state_fw", "\n", "c_bw", ",", "h_bw", "=", "state_bw", "\n", "match_state", "=", "tf", ".", "concat", "(", "[", "h_fw", ",", "h_bw", "]", ",", "1", ")", "\n", "", "return", "match_outputs", ",", "match_state", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.match_layer.AttentionFlowMatchLayer.__init__": [[84, 86], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "hidden_size", ")", ":", "\n", "        ", "self", ".", "hidden_size", "=", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.layers.match_layer.AttentionFlowMatchLayer.match": [[87, 101], ["tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.nn.softmax", "tensorflow.nn.softmax", "tensorflow.tile", "tensorflow.tile", "tensorflow.concat", "tensorflow.concat", "tensorflow.nn.softmax", "tensorflow.nn.softmax", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.reduce_max", "tensorflow.reduce_max", "tensorflow.shape", "tensorflow.shape"], "methods", ["None"], ["", "def", "match", "(", "self", ",", "passage_encodes", ",", "question_encodes", ",", "p_length", ",", "q_length", ")", ":", "\n", "        ", "\"\"\"\n        Match the passage_encodes with question_encodes using Attention Flow Match algorithm\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'bidaf'", ")", ":", "\n", "            ", "sim_matrix", "=", "tf", ".", "matmul", "(", "passage_encodes", ",", "question_encodes", ",", "transpose_b", "=", "True", ")", "\n", "context2question_attn", "=", "tf", ".", "matmul", "(", "tf", ".", "nn", ".", "softmax", "(", "sim_matrix", ",", "-", "1", ")", ",", "question_encodes", ")", "\n", "b", "=", "tf", ".", "nn", ".", "softmax", "(", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_max", "(", "sim_matrix", ",", "2", ")", ",", "1", ")", ",", "-", "1", ")", "\n", "question2context_attn", "=", "tf", ".", "tile", "(", "tf", ".", "matmul", "(", "b", ",", "passage_encodes", ")", ",", "\n", "[", "1", ",", "tf", ".", "shape", "(", "passage_encodes", ")", "[", "1", "]", ",", "1", "]", ")", "\n", "concat_outputs", "=", "tf", ".", "concat", "(", "[", "passage_encodes", ",", "context2question_attn", ",", "\n", "passage_encodes", "*", "context2question_attn", ",", "\n", "passage_encodes", "*", "question2context_attn", "]", ",", "-", "1", ")", "\n", "return", "concat_outputs", ",", "None", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout": [[24, 34], ["paddle.dropout"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["import", "os", "\n", "import", "time", "\n", "import", "logging", "\n", "import", "json", "\n", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "from", "utils", "import", "compute_bleu_rouge", "\n", "from", "utils", "import", "normalize", "\n", "from", "layers", ".", "basic_rnn", "import", "rnn", "\n", "from", "layers", ".", "match_layer", "import", "MatchLSTMLayer", "\n", "from", "layers", ".", "match_layer", "import", "AttentionFlowMatchLayer", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.bi_lstm_encoder": [[36, 72], ["paddle.fc", "paddle.fc", "paddle.dynamic_lstm", "paddle.dynamic_lstm", "paddle.concat", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.ParamAttr", "paddle.ParamAttr"], "function", ["None"], ["\n", "\n", "class", "RCModel", "(", "object", ")", ":", "\n", "    ", "\"\"\"\n    Implements the main reading comprehension model.\n    \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "vocab", ",", "args", ")", ":", "\n", "\n", "# logging", "\n", "        ", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"brc\"", ")", "\n", "\n", "# basic config", "\n", "self", ".", "algo", "=", "args", ".", "algo", "\n", "self", ".", "hidden_size", "=", "args", ".", "hidden_size", "\n", "self", ".", "optim_type", "=", "args", ".", "optim", "\n", "self", ".", "learning_rate", "=", "args", ".", "learning_rate", "\n", "self", ".", "weight_decay", "=", "args", ".", "weight_decay", "\n", "self", ".", "use_dropout", "=", "args", ".", "dropout_keep_prob", "<", "1", "\n", "\n", "# length limit", "\n", "self", ".", "max_p_num", "=", "args", ".", "max_p_num", "\n", "self", ".", "max_p_len", "=", "args", ".", "max_p_len", "\n", "self", ".", "max_q_len", "=", "args", ".", "max_q_len", "\n", "self", ".", "max_a_len", "=", "args", ".", "max_a_len", "\n", "\n", "# the vocab", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n", "# session info", "\n", "sess_config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "sess_config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", "config", "=", "sess_config", ")", "\n", "\n", "self", ".", "_build_graph", "(", ")", "\n", "\n", "# save info", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.get_data": [[74, 78], ["paddle.data"], "function", ["None"], ["\n", "# initialize the model", "\n", "self", ".", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n", "", "def", "_build_graph", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding": [[80, 89], ["paddle.embedding", "paddle.ParamAttr"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding"], ["\n", "start_t", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "_setup_placeholders", "(", ")", "\n", "self", ".", "_embed", "(", ")", "\n", "self", ".", "_encode", "(", ")", "\n", "self", ".", "_match", "(", ")", "\n", "self", ".", "_fuse", "(", ")", "\n", "self", ".", "_decode", "(", ")", "\n", "self", ".", "_compute_loss", "(", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.encoder": [[91, 99], ["rc_model.bi_lstm_encoder", "rc_model.dropout"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.bi_lstm_encoder", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["self", ".", "logger", ".", "info", "(", "'Time to build graph: {} s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_t", ")", ")", "\n", "param_num", "=", "sum", "(", "[", "np", ".", "prod", "(", "self", ".", "sess", ".", "run", "(", "tf", ".", "shape", "(", "v", ")", ")", ")", "for", "v", "in", "self", ".", "all_params", "]", ")", "\n", "self", ".", "logger", ".", "info", "(", "'There are {} parameters in the model'", ".", "format", "(", "param_num", ")", ")", "\n", "\n", "", "def", "_setup_placeholders", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Placeholders\n        \"\"\"", "\n", "self", ".", "p", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", "]", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.attn_flow": [[101, 130], ["paddle.DynamicRNN", "layers.DynamicRNN.", "paddle.sequence_softmax", "paddle.elementwise_mul", "paddle.sequence_pool", "paddle.sequence_expand", "paddle.lod_reset", "paddle.elementwise_mul", "paddle.elementwise_mul", "paddle.concat", "rc_model.dropout", "layers.DynamicRNN.block", "layers.DynamicRNN.step_input", "layers.DynamicRNN.static_input", "paddle.sequence_expand", "paddle.elementwise_mul", "paddle.reduce_sum", "paddle.reshape", "paddle.sequence_softmax", "paddle.elementwise_mul", "paddle.sequence_pool", "paddle.sequence_pool", "layers.DynamicRNN.output"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["self", ".", "p_length", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ")", "\n", "self", ".", "q_length", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ")", "\n", "self", ".", "start_label", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ")", "\n", "self", ".", "end_label", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ")", "\n", "self", ".", "dropout_keep_prob", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ")", "\n", "\n", "", "def", "_embed", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The embedding layer, question and passage share embeddings\n        \"\"\"", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ",", "tf", ".", "variable_scope", "(", "'word_embedding'", ")", ":", "\n", "            ", "self", ".", "word_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "'word_embeddings'", ",", "\n", "shape", "=", "(", "self", ".", "vocab", ".", "size", "(", ")", ",", "self", ".", "vocab", ".", "embed_dim", ")", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "self", ".", "vocab", ".", "embeddings", ")", ",", "\n", "trainable", "=", "True", "\n", ")", "\n", "self", ".", "p_emb", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "word_embeddings", ",", "self", ".", "p", ")", "\n", "self", ".", "q_emb", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "word_embeddings", ",", "self", ".", "q", ")", "\n", "\n", "", "", "def", "_encode", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Employs two Bi-LSTMs to encode passage and question separately\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'passage_encoding'", ")", ":", "\n", "            ", "self", ".", "sep_p_encodes", ",", "_", "=", "rnn", "(", "'bi-lstm'", ",", "self", ".", "p_emb", ",", "self", ".", "p_length", ",", "self", ".", "hidden_size", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'question_encoding'", ")", ":", "\n", "            ", "self", ".", "sep_q_encodes", ",", "_", "=", "rnn", "(", "'bi-lstm'", ",", "self", ".", "q_emb", ",", "self", ".", "q_length", ",", "self", ".", "hidden_size", ")", "\n", "", "if", "self", ".", "use_dropout", ":", "\n", "            ", "self", ".", "sep_p_encodes", "=", "tf", ".", "nn", ".", "dropout", "(", "self", ".", "sep_p_encodes", ",", "self", ".", "dropout_keep_prob", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.fusion": [[132, 137], ["rc_model.bi_lstm_encoder", "rc_model.dropout"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.bi_lstm_encoder", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["\n", "", "", "def", "_match", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The core of RC model, get the question-aware passage encoding with either BIDAF or MLSTM\n        \"\"\"", "\n", "if", "self", ".", "algo", "==", "'MLSTM'", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.lstm_step": [[139, 165], ["paddle.concat", "paddle.sigmoid", "paddle.sigmoid", "paddle.sigmoid", "paddle.tanh", "paddle.sums", "paddle.elementwise_mul", "paddle.fc", "rc_model.lstm_step.linear"], "function", ["None"], ["", "elif", "self", ".", "algo", "==", "'BIDAF'", ":", "\n", "            ", "match_layer", "=", "AttentionFlowMatchLayer", "(", "self", ".", "hidden_size", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'The algorithm {} is not implemented.'", ".", "format", "(", "self", ".", "algo", ")", ")", "\n", "", "self", ".", "match_p_encodes", ",", "_", "=", "match_layer", ".", "match", "(", "self", ".", "sep_p_encodes", ",", "self", ".", "sep_q_encodes", ",", "\n", "self", ".", "p_length", ",", "self", ".", "q_length", ")", "\n", "if", "self", ".", "use_dropout", ":", "\n", "            ", "self", ".", "match_p_encodes", "=", "tf", ".", "nn", ".", "dropout", "(", "self", ".", "match_p_encodes", ",", "self", ".", "dropout_keep_prob", ")", "\n", "\n", "", "", "def", "_fuse", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Employs Bi-LSTM again to fuse the context information after match layer\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'fusion'", ")", ":", "\n", "            ", "self", ".", "fuse_p_encodes", ",", "_", "=", "rnn", "(", "'bi-lstm'", ",", "self", ".", "match_p_encodes", ",", "self", ".", "p_length", ",", "\n", "self", ".", "hidden_size", ",", "layer_num", "=", "1", ")", "\n", "if", "self", ".", "use_dropout", ":", "\n", "                ", "self", ".", "fuse_p_encodes", "=", "tf", ".", "nn", ".", "dropout", "(", "self", ".", "fuse_p_encodes", ",", "self", ".", "dropout_keep_prob", ")", "\n", "\n", "", "", "", "def", "_decode", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Employs Pointer Network to get the the probs of each position\n        to be the start or end of the predicted answer.\n        Note that we concat the fuse_p_encodes for the passages in the same document.\n        And since the encodes of queries in the same document is same, we select the first one.\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'same_question_concat'", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.point_network_decoder": [[167, 273], ["paddle.initializer.Normal", "paddle.create_parameter", "paddle.fc", "paddle.reshape", "paddle.tanh", "paddle.fc", "paddle.sequence_softmax", "paddle.elementwise_mul", "paddle.sequence_pool", "paddle.fc", "rc_model.point_network_decoder.custom_dynamic_rnn"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.custom_dynamic_rnn"], ["concat_passage_encodes", "=", "tf", ".", "reshape", "(", "\n", "self", ".", "fuse_p_encodes", ",", "\n", "[", "batch_size", ",", "-", "1", ",", "2", "*", "self", ".", "hidden_size", "]", "\n", ")", "\n", "no_dup_question_encodes", "=", "tf", ".", "reshape", "(", "\n", "self", ".", "sep_q_encodes", ",", "\n", "[", "batch_size", ",", "-", "1", ",", "tf", ".", "shape", "(", "self", ".", "sep_q_encodes", ")", "[", "1", "]", ",", "2", "*", "self", ".", "hidden_size", "]", "\n", ")", "[", "0", ":", ",", "0", ",", "0", ":", ",", "0", ":", "]", "\n", "", "decoder", "=", "PointerNetDecoder", "(", "self", ".", "hidden_size", ")", "\n", "self", ".", "start_probs", ",", "self", ".", "end_probs", "=", "decoder", ".", "decode", "(", "concat_passage_encodes", ",", "\n", "no_dup_question_encodes", ")", "\n", "\n", "", "def", "_compute_loss", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The loss function\n        \"\"\"", "\n", "\n", "def", "sparse_nll_loss", "(", "probs", ",", "labels", ",", "epsilon", "=", "1e-9", ",", "scope", "=", "None", ")", ":", "\n", "            ", "\"\"\"\n            negative log likelyhood loss\n            \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "scope", ",", "\"log_loss\"", ")", ":", "\n", "                ", "labels", "=", "tf", ".", "one_hot", "(", "labels", ",", "tf", ".", "shape", "(", "probs", ")", "[", "1", "]", ",", "axis", "=", "1", ")", "\n", "losses", "=", "-", "tf", ".", "reduce_sum", "(", "labels", "*", "tf", ".", "log", "(", "probs", "+", "epsilon", ")", ",", "1", ")", "\n", "", "return", "losses", "\n", "\n", "", "self", ".", "start_loss", "=", "sparse_nll_loss", "(", "probs", "=", "self", ".", "start_probs", ",", "labels", "=", "self", ".", "start_label", ")", "\n", "self", ".", "end_loss", "=", "sparse_nll_loss", "(", "probs", "=", "self", ".", "end_probs", ",", "labels", "=", "self", ".", "end_label", ")", "\n", "self", ".", "all_params", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "add", "(", "self", ".", "start_loss", ",", "self", ".", "end_loss", ")", ")", "\n", "if", "self", ".", "weight_decay", ">", "0", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "'l2_loss'", ")", ":", "\n", "                ", "l2_loss", "=", "tf", ".", "add_n", "(", "[", "tf", ".", "nn", ".", "l2_loss", "(", "v", ")", "for", "v", "in", "self", ".", "all_params", "]", ")", "\n", "", "self", ".", "loss", "+=", "self", ".", "weight_decay", "*", "l2_loss", "\n", "\n", "", "", "def", "_create_train_op", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Selects the training algorithm and creates a train operation with it\n        \"\"\"", "\n", "if", "self", ".", "optim_type", "==", "'adagrad'", ":", "\n", "            ", "self", ".", "optimizer", "=", "tf", ".", "train", ".", "AdagradOptimizer", "(", "self", ".", "learning_rate", ")", "\n", "", "elif", "self", ".", "optim_type", "==", "'adam'", ":", "\n", "            ", "self", ".", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "learning_rate", ")", "\n", "", "elif", "self", ".", "optim_type", "==", "'rprop'", ":", "\n", "            ", "self", ".", "optimizer", "=", "tf", ".", "train", ".", "RMSPropOptimizer", "(", "self", ".", "learning_rate", ")", "\n", "", "elif", "self", ".", "optim_type", "==", "'sgd'", ":", "\n", "            ", "self", ".", "optimizer", "=", "tf", ".", "train", ".", "GradientDescentOptimizer", "(", "self", ".", "learning_rate", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Unsupported optimizer: {}'", ".", "format", "(", "self", ".", "optim_type", ")", ")", "\n", "", "self", ".", "train_op", "=", "self", ".", "optimizer", ".", "minimize", "(", "self", ".", "loss", ")", "\n", "\n", "", "def", "_train_epoch", "(", "self", ",", "train_batches", ",", "dropout_keep_prob", ")", ":", "\n", "        ", "\"\"\"\n        Trains the model for a single epoch.\n        Args:\n            train_batches: iterable batch data for training\n            dropout_keep_prob: float value indicating dropout keep probability\n        \"\"\"", "\n", "total_num", ",", "total_loss", "=", "0", ",", "0", "\n", "log_every_n_batch", ",", "n_batch_loss", "=", "50", ",", "0", "\n", "for", "bitx", ",", "batch", "in", "enumerate", "(", "train_batches", ",", "1", ")", ":", "\n", "            ", "feed_dict", "=", "{", "self", ".", "p", ":", "batch", "[", "'passage_token_ids'", "]", ",", "\n", "self", ".", "q", ":", "batch", "[", "'question_token_ids'", "]", ",", "\n", "self", ".", "p_length", ":", "batch", "[", "'passage_length'", "]", ",", "\n", "self", ".", "q_length", ":", "batch", "[", "'question_length'", "]", ",", "\n", "self", ".", "start_label", ":", "batch", "[", "'start_id'", "]", ",", "\n", "self", ".", "end_label", ":", "batch", "[", "'end_id'", "]", ",", "\n", "self", ".", "dropout_keep_prob", ":", "dropout_keep_prob", "}", "\n", "_", ",", "loss", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "train_op", ",", "self", ".", "loss", "]", ",", "feed_dict", ")", "\n", "total_loss", "+=", "loss", "*", "len", "(", "batch", "[", "'raw_data'", "]", ")", "\n", "total_num", "+=", "len", "(", "batch", "[", "'raw_data'", "]", ")", "\n", "n_batch_loss", "+=", "loss", "\n", "if", "log_every_n_batch", ">", "0", "and", "bitx", "%", "log_every_n_batch", "==", "0", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "'Average loss from batch {} to {} is {}'", ".", "format", "(", "\n", "bitx", "-", "log_every_n_batch", "+", "1", ",", "bitx", ",", "n_batch_loss", "/", "log_every_n_batch", ")", ")", "\n", "n_batch_loss", "=", "0", "\n", "", "", "return", "1.0", "*", "total_loss", "/", "total_num", "\n", "\n", "", "def", "train", "(", "self", ",", "data", ",", "epochs", ",", "batch_size", ",", "save_dir", ",", "save_prefix", ",", "\n", "dropout_keep_prob", "=", "1.0", ",", "evaluate", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Train the model with data\n        Args:\n            data: the BRCDataset class implemented in dataset.py\n            epochs: number of training epochs\n            batch_size:\n            save_dir: the directory to save the model\n            save_prefix: the prefix indicating the model type\n            dropout_keep_prob: float value indicating dropout keep probability\n            evaluate: whether to evaluate the model on test set after each epoch\n        \"\"\"", "\n", "pad_id", "=", "self", ".", "vocab", ".", "get_id", "(", "self", ".", "vocab", ".", "pad_token", ")", "\n", "max_bleu_4", "=", "0", "\n", "for", "epoch", "in", "range", "(", "1", ",", "epochs", "+", "1", ")", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "'Training the model for epoch {}'", ".", "format", "(", "epoch", ")", ")", "\n", "train_batches", "=", "data", ".", "gen_mini_batches", "(", "'train'", ",", "batch_size", ",", "pad_id", ",", "shuffle", "=", "True", ")", "\n", "train_loss", "=", "self", ".", "_train_epoch", "(", "train_batches", ",", "dropout_keep_prob", ")", "\n", "self", ".", "logger", ".", "info", "(", "'Average train loss for epoch {} is {}'", ".", "format", "(", "epoch", ",", "train_loss", ")", ")", "\n", "\n", "if", "evaluate", ":", "\n", "                ", "self", ".", "logger", ".", "info", "(", "'Evaluating the model after epoch {}'", ".", "format", "(", "epoch", ")", ")", "\n", "if", "data", ".", "dev_set", "is", "not", "None", ":", "\n", "                    ", "eval_batches", "=", "data", ".", "gen_mini_batches", "(", "'dev'", ",", "batch_size", ",", "pad_id", ",", "shuffle", "=", "False", ")", "\n", "eval_loss", ",", "bleu_rouge", "=", "self", ".", "evaluate", "(", "eval_batches", ")", "\n", "self", ".", "logger", ".", "info", "(", "'Dev eval loss {}'", ".", "format", "(", "eval_loss", ")", ")", "\n", "self", ".", "logger", ".", "info", "(", "'Dev eval result: {}'", ".", "format", "(", "bleu_rouge", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.rc_model": [[275, 331], ["paddle.data", "paddle.data", "rc_model.get_data", "rc_model.get_data", "rc_model.get_data", "rc_model.embedding", "rc_model.embedding", "paddle.DynamicRNN", "layers.DynamicRNN.", "paddle.lod_reset", "paddle.lod_reset", "rc_model.point_network_decoder", "paddle.sequence_pool", "paddle.sequence_pool", "paddle.mean", "paddle.mean", "vocab.size", "layers.DynamicRNN.block", "layers.DynamicRNN.step_input", "layers.DynamicRNN.step_input", "rc_model.encoder", "rc_model.encoder", "rc_model.attn_flow", "rc_model.fusion", "layers.DynamicRNN.output", "paddle.cross_entropy", "paddle.cross_entropy"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.get_data", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.get_data", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.get_data", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.embedding", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.point_network_decoder", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.encoder", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.encoder", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.attn_flow", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.fusion"], ["                        ", "self", ".", "save", "(", "save_dir", ",", "save_prefix", ")", "\n", "max_bleu_4", "=", "bleu_rouge", "[", "'Bleu-4'", "]", "\n", "", "", "else", ":", "\n", "                    ", "self", ".", "logger", ".", "warning", "(", "'No dev set is loaded for evaluation in the dataset!'", ")", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "save", "(", "save_dir", ",", "save_prefix", "+", "'_'", "+", "str", "(", "epoch", ")", ")", "\n", "\n", "", "", "", "def", "evaluate", "(", "self", ",", "eval_batches", ",", "result_dir", "=", "None", ",", "result_prefix", "=", "None", ",", "save_full_info", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Evaluates the model performance on eval_batches and results are saved if specified\n        Args:\n            eval_batches: iterable batch data\n            result_dir: directory to save predicted answers, answers will not be saved if None\n            result_prefix: prefix of the file for saving predicted answers,\n                           answers will not be saved if None\n            save_full_info: if True, the pred_answers will be added to raw sample and saved\n        \"\"\"", "\n", "pred_answers", ",", "ref_answers", "=", "[", "]", ",", "[", "]", "\n", "total_loss", ",", "total_num", "=", "0", ",", "0", "\n", "for", "b_itx", ",", "batch", "in", "enumerate", "(", "eval_batches", ")", ":", "\n", "            ", "feed_dict", "=", "{", "self", ".", "p", ":", "batch", "[", "'passage_token_ids'", "]", ",", "\n", "self", ".", "q", ":", "batch", "[", "'question_token_ids'", "]", ",", "\n", "self", ".", "p_length", ":", "batch", "[", "'passage_length'", "]", ",", "\n", "self", ".", "q_length", ":", "batch", "[", "'question_length'", "]", ",", "\n", "self", ".", "start_label", ":", "batch", "[", "'start_id'", "]", ",", "\n", "self", ".", "end_label", ":", "batch", "[", "'end_id'", "]", ",", "\n", "self", ".", "dropout_keep_prob", ":", "1.0", "}", "\n", "start_probs", ",", "end_probs", ",", "loss", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "start_probs", ",", "\n", "self", ".", "end_probs", ",", "self", ".", "loss", "]", ",", "feed_dict", ")", "\n", "\n", "total_loss", "+=", "loss", "*", "len", "(", "batch", "[", "'raw_data'", "]", ")", "\n", "total_num", "+=", "len", "(", "batch", "[", "'raw_data'", "]", ")", "\n", "\n", "padded_p_len", "=", "len", "(", "batch", "[", "'passage_token_ids'", "]", "[", "0", "]", ")", "\n", "for", "sample", ",", "start_prob", ",", "end_prob", "in", "zip", "(", "batch", "[", "'raw_data'", "]", ",", "start_probs", ",", "end_probs", ")", ":", "\n", "\n", "                ", "best_answer", "=", "self", ".", "find_best_answer", "(", "sample", ",", "start_prob", ",", "end_prob", ",", "padded_p_len", ")", "\n", "if", "save_full_info", ":", "\n", "                    ", "sample", "[", "'pred_answers'", "]", "=", "[", "best_answer", "]", "\n", "pred_answers", ".", "append", "(", "sample", ")", "\n", "", "else", ":", "\n", "                    ", "pred_answers", ".", "append", "(", "{", "'question_id'", ":", "sample", "[", "'question_id'", "]", ",", "\n", "'question_type'", ":", "sample", "[", "'question_type'", "]", ",", "\n", "'answers'", ":", "[", "best_answer", "]", ",", "\n", "'entity_answers'", ":", "[", "[", "]", "]", ",", "\n", "'yesno_answers'", ":", "[", "]", "}", ")", "\n", "", "if", "'answers'", "in", "sample", ":", "\n", "                    ", "ref_answers", ".", "append", "(", "{", "'question_id'", ":", "sample", "[", "'question_id'", "]", ",", "\n", "'question_type'", ":", "sample", "[", "'question_type'", "]", ",", "\n", "'answers'", ":", "sample", "[", "'answers'", "]", ",", "\n", "'entity_answers'", ":", "[", "[", "]", "]", ",", "\n", "'yesno_answers'", ":", "[", "]", "}", ")", "\n", "\n", "", "", "", "if", "result_dir", "is", "not", "None", "and", "result_prefix", "is", "not", "None", ":", "\n", "            ", "result_file", "=", "os", ".", "path", ".", "join", "(", "result_dir", ",", "result_prefix", "+", "'.json'", ")", "\n", "with", "open", "(", "result_file", ",", "'w'", ")", "as", "fout", ":", "\n", "                ", "for", "pred_answer", "in", "pred_answers", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.paragraph_extraction.compute_paragraph_score": [[13, 34], ["enumerate", "doc[].append", "len", "preprocess.metric_max_over_ground_truths"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.metric_max_over_ground_truths"], ["def", "compute_paragraph_score", "(", "sample", ")", ":", "\n", "    ", "\"\"\"\n    For each paragraph, compute the f1 score compared with the question\n    Args:\n        sample: a sample in the dataset.\n    Returns:\n        None\n    Raises:\n        None\n    \"\"\"", "\n", "question", "=", "sample", "[", "\"segmented_question\"", "]", "\n", "for", "doc", "in", "sample", "[", "'documents'", "]", ":", "\n", "        ", "doc", "[", "'segmented_paragraphs_scores'", "]", "=", "[", "]", "\n", "for", "p_idx", ",", "para_tokens", "in", "enumerate", "(", "doc", "[", "'segmented_paragraphs'", "]", ")", ":", "\n", "            ", "if", "len", "(", "question", ")", ">", "0", ":", "\n", "                ", "related_score", "=", "metric_max_over_ground_truths", "(", "f1_score", ",", "\n", "para_tokens", ",", "\n", "[", "question", "]", ")", "\n", "", "else", ":", "\n", "                ", "related_score", "=", "0.0", "\n", "", "doc", "[", "'segmented_paragraphs_scores'", "]", ".", "append", "(", "related_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.paragraph_extraction.dup_remove": [[36, 82], ["enumerate", "zip", "doc[].append", "len", "len", "del_ids.append", "doc[].append"], "function", ["None"], ["", "", "", "def", "dup_remove", "(", "doc", ")", ":", "\n", "    ", "\"\"\"\n    For each document, remove the duplicated paragraphs\n    Args:\n        doc: a doc in the sample\n    Returns:\n        bool\n    Raises:\n        None\n    \"\"\"", "\n", "paragraphs_his", "=", "{", "}", "\n", "del_ids", "=", "[", "]", "\n", "para_id", "=", "None", "\n", "if", "'most_related_para'", "in", "doc", ":", "\n", "        ", "para_id", "=", "doc", "[", "'most_related_para'", "]", "\n", "", "doc", "[", "'paragraphs_length'", "]", "=", "[", "]", "\n", "for", "p_idx", ",", "(", "segmented_paragraph", ",", "paragraph_score", ")", "in", "enumerate", "(", "zip", "(", "doc", "[", "\"segmented_paragraphs\"", "]", ",", "doc", "[", "\"segmented_paragraphs_scores\"", "]", ")", ")", ":", "\n", "        ", "doc", "[", "'paragraphs_length'", "]", ".", "append", "(", "len", "(", "segmented_paragraph", ")", ")", "\n", "paragraph", "=", "''", ".", "join", "(", "segmented_paragraph", ")", "\n", "if", "paragraph", "in", "paragraphs_his", ":", "\n", "            ", "del_ids", ".", "append", "(", "p_idx", ")", "\n", "if", "p_idx", "==", "para_id", ":", "\n", "                ", "para_id", "=", "paragraphs_his", "[", "paragraph", "]", "\n", "", "continue", "\n", "", "paragraphs_his", "[", "paragraph", "]", "=", "p_idx", "\n", "# delete", "\n", "", "prev_del_num", "=", "0", "\n", "del_num", "=", "0", "\n", "for", "p_idx", "in", "del_ids", ":", "\n", "        ", "if", "p_idx", "<", "para_id", ":", "\n", "            ", "prev_del_num", "+=", "1", "\n", "", "del", "doc", "[", "\"segmented_paragraphs\"", "]", "[", "p_idx", "-", "del_num", "]", "\n", "del", "doc", "[", "\"segmented_paragraphs_scores\"", "]", "[", "p_idx", "-", "del_num", "]", "\n", "del", "doc", "[", "'paragraphs_length'", "]", "[", "p_idx", "-", "del_num", "]", "\n", "del_num", "+=", "1", "\n", "", "if", "len", "(", "del_ids", ")", "!=", "0", ":", "\n", "        ", "if", "'most_related_para'", "in", "doc", ":", "\n", "            ", "doc", "[", "'most_related_para'", "]", "=", "para_id", "-", "prev_del_num", "\n", "", "doc", "[", "'paragraphs'", "]", "=", "[", "]", "\n", "for", "segmented_para", "in", "doc", "[", "\"segmented_paragraphs\"", "]", ":", "\n", "            ", "paragraph", "=", "''", ".", "join", "(", "segmented_para", ")", "\n", "doc", "[", "'paragraphs'", "]", ".", "append", "(", "paragraph", ")", "\n", "", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.paragraph_extraction.paragraph_selection": [[84, 182], ["enumerate", "paragraph_extraction.dup_remove", "len", "len", "enumerate", "para_infos.sort", "copy.deepcopy", "final_idx.sort", "len", "len", "sum", "copy.deepcopy", "enumerate", "zip", "para_infos.append", "topN_idx.append", "final_idx.append", "final_idx.append", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.paragraph_extraction.dup_remove"], ["", "", "def", "paragraph_selection", "(", "sample", ",", "mode", ")", ":", "\n", "    ", "\"\"\"\n    For each document, select paragraphs that includes as much information as possible\n    Args:\n        sample: a sample in the dataset.\n        mode: string of (\"train\", \"dev\", \"test\"), indicate the type of dataset to process.\n    Returns:\n        None\n    Raises:\n        None\n    \"\"\"", "\n", "# predefined maximum length of paragraph", "\n", "MAX_P_LEN", "=", "500", "\n", "# predefined splitter", "\n", "splitter", "=", "u'<splitter>'", "\n", "# topN of related paragraph to choose", "\n", "topN", "=", "3", "\n", "doc_id", "=", "None", "\n", "if", "'answer_docs'", "in", "sample", "and", "len", "(", "sample", "[", "'answer_docs'", "]", ")", ">", "0", ":", "\n", "        ", "doc_id", "=", "sample", "[", "'answer_docs'", "]", "[", "0", "]", "\n", "if", "doc_id", ">=", "len", "(", "sample", "[", "'documents'", "]", ")", ":", "\n", "# Data error, answer doc ID > number of documents, this sample", "\n", "# will be filtered by dataset.py", "\n", "            ", "return", "\n", "", "", "for", "d_idx", ",", "doc", "in", "enumerate", "(", "sample", "[", "'documents'", "]", ")", ":", "\n", "        ", "if", "'segmented_paragraphs_scores'", "not", "in", "doc", ":", "\n", "            ", "continue", "\n", "", "status", "=", "dup_remove", "(", "doc", ")", "\n", "segmented_title", "=", "doc", "[", "\"segmented_title\"", "]", "\n", "title_len", "=", "len", "(", "segmented_title", ")", "\n", "para_id", "=", "None", "\n", "if", "doc_id", "is", "not", "None", ":", "\n", "            ", "para_id", "=", "sample", "[", "'documents'", "]", "[", "doc_id", "]", "[", "'most_related_para'", "]", "\n", "", "total_len", "=", "title_len", "+", "sum", "(", "doc", "[", "'paragraphs_length'", "]", ")", "\n", "# add splitter", "\n", "para_num", "=", "len", "(", "doc", "[", "\"segmented_paragraphs\"", "]", ")", "\n", "total_len", "+=", "para_num", "\n", "if", "total_len", "<=", "MAX_P_LEN", ":", "\n", "            ", "incre_len", "=", "title_len", "\n", "total_segmented_content", "=", "copy", ".", "deepcopy", "(", "segmented_title", ")", "\n", "for", "p_idx", ",", "segmented_para", "in", "enumerate", "(", "doc", "[", "\"segmented_paragraphs\"", "]", ")", ":", "\n", "                ", "if", "doc_id", "==", "d_idx", "and", "para_id", ">", "p_idx", ":", "\n", "                    ", "incre_len", "+=", "len", "(", "[", "splitter", "]", "+", "segmented_para", ")", "\n", "", "if", "doc_id", "==", "d_idx", "and", "para_id", "==", "p_idx", ":", "\n", "                    ", "incre_len", "+=", "1", "\n", "", "total_segmented_content", "+=", "[", "splitter", "]", "+", "segmented_para", "\n", "", "if", "doc_id", "==", "d_idx", ":", "\n", "                ", "answer_start", "=", "incre_len", "+", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "0", "]", "\n", "answer_end", "=", "incre_len", "+", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "1", "]", "\n", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "0", "]", "=", "answer_start", "\n", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "1", "]", "=", "answer_end", "\n", "", "doc", "[", "\"segmented_paragraphs\"", "]", "=", "[", "total_segmented_content", "]", "\n", "doc", "[", "\"segmented_paragraphs_scores\"", "]", "=", "[", "1.0", "]", "\n", "doc", "[", "'paragraphs_length'", "]", "=", "[", "total_len", "]", "\n", "doc", "[", "'paragraphs'", "]", "=", "[", "''", ".", "join", "(", "total_segmented_content", ")", "]", "\n", "doc", "[", "'most_related_para'", "]", "=", "0", "\n", "continue", "\n", "# find topN paragraph id", "\n", "", "para_infos", "=", "[", "]", "\n", "for", "p_idx", ",", "(", "para_tokens", ",", "para_scores", ")", "in", "enumerate", "(", "zip", "(", "doc", "[", "'segmented_paragraphs'", "]", ",", "doc", "[", "'segmented_paragraphs_scores'", "]", ")", ")", ":", "\n", "            ", "para_infos", ".", "append", "(", "(", "para_tokens", ",", "para_scores", ",", "len", "(", "para_tokens", ")", ",", "p_idx", ")", ")", "\n", "", "para_infos", ".", "sort", "(", "key", "=", "lambda", "x", ":", "(", "-", "x", "[", "1", "]", ",", "x", "[", "2", "]", ")", ")", "\n", "topN_idx", "=", "[", "]", "\n", "for", "para_info", "in", "para_infos", "[", ":", "topN", "]", ":", "\n", "            ", "topN_idx", ".", "append", "(", "para_info", "[", "-", "1", "]", ")", "\n", "", "final_idx", "=", "[", "]", "\n", "total_len", "=", "title_len", "\n", "if", "doc_id", "==", "d_idx", ":", "\n", "            ", "if", "mode", "==", "\"train\"", ":", "\n", "                ", "final_idx", ".", "append", "(", "para_id", ")", "\n", "total_len", "=", "title_len", "+", "1", "+", "doc", "[", "'paragraphs_length'", "]", "[", "para_id", "]", "\n", "", "", "for", "id", "in", "topN_idx", ":", "\n", "            ", "if", "total_len", ">", "MAX_P_LEN", ":", "\n", "                ", "break", "\n", "", "if", "doc_id", "==", "d_idx", "and", "id", "==", "para_id", "and", "mode", "==", "\"train\"", ":", "\n", "                ", "continue", "\n", "", "total_len", "+=", "1", "+", "doc", "[", "'paragraphs_length'", "]", "[", "id", "]", "\n", "final_idx", ".", "append", "(", "id", ")", "\n", "", "total_segmented_content", "=", "copy", ".", "deepcopy", "(", "segmented_title", ")", "\n", "final_idx", ".", "sort", "(", ")", "\n", "incre_len", "=", "title_len", "\n", "for", "id", "in", "final_idx", ":", "\n", "            ", "if", "doc_id", "==", "d_idx", "and", "id", "<", "para_id", ":", "\n", "                ", "incre_len", "+=", "1", "+", "doc", "[", "'paragraphs_length'", "]", "[", "id", "]", "\n", "", "if", "doc_id", "==", "d_idx", "and", "id", "==", "para_id", ":", "\n", "                ", "incre_len", "+=", "1", "\n", "", "total_segmented_content", "+=", "[", "splitter", "]", "+", "doc", "[", "'segmented_paragraphs'", "]", "[", "id", "]", "\n", "", "if", "doc_id", "==", "d_idx", ":", "\n", "            ", "answer_start", "=", "incre_len", "+", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "0", "]", "\n", "answer_end", "=", "incre_len", "+", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "1", "]", "\n", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "0", "]", "=", "answer_start", "\n", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "1", "]", "=", "answer_end", "\n", "", "doc", "[", "\"segmented_paragraphs\"", "]", "=", "[", "total_segmented_content", "]", "\n", "doc", "[", "\"segmented_paragraphs_scores\"", "]", "=", "[", "1.0", "]", "\n", "doc", "[", "'paragraphs_length'", "]", "=", "[", "total_len", "]", "\n", "doc", "[", "'paragraphs'", "]", "=", "[", "''", ".", "join", "(", "total_segmented_content", ")", "]", "\n", "doc", "[", "'most_related_para'", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.__init__": [[34, 64], ["logging.getLogger", "dataset.BRCDataset.logger.info", "dataset.BRCDataset.logger.info", "dataset.BRCDataset.logger.info", "dataset.BRCDataset._load_dataset", "dataset.BRCDataset._load_dataset", "dataset.BRCDataset._load_dataset", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset._load_dataset", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset._load_dataset", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset._load_dataset"], ["        ", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "\"brc\"", ")", "\n", "self", ".", "max_p_num", "=", "max_p_num", "\n", "self", ".", "max_p_len", "=", "max_p_len", "\n", "self", ".", "max_q_len", "=", "max_q_len", "\n", "\n", "self", ".", "train_set", ",", "self", ".", "dev_set", ",", "self", ".", "test_set", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "if", "train_files", ":", "\n", "            ", "for", "train_file", "in", "train_files", ":", "\n", "                ", "self", ".", "train_set", "+=", "self", ".", "_load_dataset", "(", "train_file", ",", "train", "=", "True", ")", "\n", "", "self", ".", "logger", ".", "info", "(", "'Train set size: {} questions.'", ".", "format", "(", "len", "(", "self", ".", "train_set", ")", ")", ")", "\n", "\n", "", "if", "dev_files", ":", "\n", "            ", "for", "dev_file", "in", "dev_files", ":", "\n", "                ", "self", ".", "dev_set", "+=", "self", ".", "_load_dataset", "(", "dev_file", ")", "\n", "", "self", ".", "logger", ".", "info", "(", "'Dev set size: {} questions.'", ".", "format", "(", "len", "(", "self", ".", "dev_set", ")", ")", ")", "\n", "\n", "", "if", "test_files", ":", "\n", "            ", "for", "test_file", "in", "test_files", ":", "\n", "                ", "self", ".", "test_set", "+=", "self", ".", "_load_dataset", "(", "test_file", ")", "\n", "", "self", ".", "logger", ".", "info", "(", "'Test set size: {} questions.'", ".", "format", "(", "len", "(", "self", ".", "test_set", ")", ")", ")", "\n", "\n", "", "", "def", "_load_dataset", "(", "self", ",", "data_path", ",", "train", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Loads the dataset\n        Args:\n            data_path: the data file to load\n        \"\"\"", "\n", "with", "open", "(", "data_path", ")", "as", "fin", ":", "\n", "            ", "data_set", "=", "[", "]", "\n", "for", "lidx", ",", "line", "in", "enumerate", "(", "fin", ")", ":", "\n", "                ", "sample", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset._load_dataset": [[65, 118], ["io.open", "enumerate", "json.loads", "enumerate", "data_set.append", "line.strip", "len", "sample[].append", "para_infos.sort", "sample[].append", "sum", "para_infos.append", "collections.Counter", "collections.Counter", "common_with_question.values", "float", "len", "len"], "methods", ["None"], ["if", "train", ":", "\n", "                    ", "if", "len", "(", "sample", "[", "'answer_spans'", "]", ")", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "if", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "1", "]", ">=", "self", ".", "max_p_len", ":", "\n", "                        ", "continue", "\n", "\n", "", "", "if", "'answer_docs'", "in", "sample", ":", "\n", "                    ", "sample", "[", "'answer_passages'", "]", "=", "sample", "[", "'answer_docs'", "]", "\n", "\n", "", "sample", "[", "'question_tokens'", "]", "=", "sample", "[", "'segmented_question'", "]", "\n", "\n", "sample", "[", "'passages'", "]", "=", "[", "]", "\n", "for", "d_idx", ",", "doc", "in", "enumerate", "(", "sample", "[", "'documents'", "]", ")", ":", "\n", "                    ", "if", "train", ":", "\n", "                        ", "most_related_para", "=", "doc", "[", "'most_related_para'", "]", "\n", "sample", "[", "'passages'", "]", ".", "append", "(", "\n", "{", "'passage_tokens'", ":", "doc", "[", "'segmented_paragraphs'", "]", "[", "most_related_para", "]", ",", "\n", "'is_selected'", ":", "doc", "[", "'is_selected'", "]", "}", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "para_infos", "=", "[", "]", "\n", "for", "para_tokens", "in", "doc", "[", "'segmented_paragraphs'", "]", ":", "\n", "                            ", "question_tokens", "=", "sample", "[", "'segmented_question'", "]", "\n", "common_with_question", "=", "Counter", "(", "para_tokens", ")", "&", "Counter", "(", "question_tokens", ")", "\n", "correct_preds", "=", "sum", "(", "common_with_question", ".", "values", "(", ")", ")", "\n", "if", "correct_preds", "==", "0", ":", "\n", "                                ", "recall_wrt_question", "=", "0", "\n", "", "else", ":", "\n", "                                ", "recall_wrt_question", "=", "float", "(", "correct_preds", ")", "/", "len", "(", "question_tokens", ")", "\n", "", "para_infos", ".", "append", "(", "(", "para_tokens", ",", "recall_wrt_question", ",", "len", "(", "para_tokens", ")", ")", ")", "\n", "", "para_infos", ".", "sort", "(", "key", "=", "lambda", "x", ":", "(", "-", "x", "[", "1", "]", ",", "x", "[", "2", "]", ")", ")", "\n", "fake_passage_tokens", "=", "[", "]", "\n", "for", "para_info", "in", "para_infos", "[", ":", "1", "]", ":", "\n", "                            ", "fake_passage_tokens", "+=", "para_info", "[", "0", "]", "\n", "", "sample", "[", "'passages'", "]", ".", "append", "(", "{", "'passage_tokens'", ":", "fake_passage_tokens", "}", ")", "\n", "", "", "data_set", ".", "append", "(", "sample", ")", "\n", "", "", "return", "data_set", "\n", "\n", "", "def", "_one_mini_batch", "(", "self", ",", "data", ",", "indices", ",", "pad_id", ")", ":", "\n", "        ", "\"\"\"\n        Get one mini batch\n        Args:\n            data: all data\n            indices: the indices of the samples to be selected\n            pad_id:\n        Returns:\n            one batch of data\n        \"\"\"", "\n", "batch_data", "=", "{", "'raw_data'", ":", "[", "data", "[", "i", "]", "for", "i", "in", "indices", "]", ",", "\n", "'question_token_ids'", ":", "[", "]", ",", "\n", "'question_length'", ":", "[", "]", ",", "\n", "'passage_token_ids'", ":", "[", "]", ",", "\n", "'passage_length'", ":", "[", "]", ",", "\n", "'start_id'", ":", "[", "]", ",", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset._one_mini_batch": [[119, 174], ["max", "min", "enumerate", "range", "sum", "batch_data[].append", "len", "len", "range", "min", "min", "batch_data[].append", "batch_data[].append", "batch_data[].append", "batch_data[].append", "len", "batch_data[].append", "batch_data[].append", "batch_data[].append", "batch_data[].append", "len", "len", "min", "min", "len", "len"], "methods", ["None"], ["'end_id'", ":", "[", "]", "}", "\n", "max_passage_num", "=", "max", "(", "[", "len", "(", "sample", "[", "'passages'", "]", ")", "for", "sample", "in", "batch_data", "[", "'raw_data'", "]", "]", ")", "\n", "max_passage_num", "=", "min", "(", "self", ".", "max_p_num", ",", "max_passage_num", ")", "\n", "for", "sidx", ",", "sample", "in", "enumerate", "(", "batch_data", "[", "'raw_data'", "]", ")", ":", "\n", "            ", "for", "pidx", "in", "range", "(", "max_passage_num", ")", ":", "\n", "                ", "if", "pidx", "<", "len", "(", "sample", "[", "'passages'", "]", ")", ":", "\n", "                    ", "batch_data", "[", "'question_token_ids'", "]", ".", "append", "(", "sample", "[", "'question_token_ids'", "]", ")", "\n", "batch_data", "[", "'question_length'", "]", ".", "append", "(", "len", "(", "sample", "[", "'question_token_ids'", "]", ")", ")", "\n", "passage_token_ids", "=", "sample", "[", "'passages'", "]", "[", "pidx", "]", "[", "'passage_token_ids'", "]", "\n", "batch_data", "[", "'passage_token_ids'", "]", ".", "append", "(", "passage_token_ids", ")", "\n", "batch_data", "[", "'passage_length'", "]", ".", "append", "(", "min", "(", "len", "(", "passage_token_ids", ")", ",", "self", ".", "max_p_len", ")", ")", "\n", "", "else", ":", "\n", "                    ", "batch_data", "[", "'question_token_ids'", "]", ".", "append", "(", "[", "]", ")", "\n", "batch_data", "[", "'question_length'", "]", ".", "append", "(", "0", ")", "\n", "batch_data", "[", "'passage_token_ids'", "]", ".", "append", "(", "[", "]", ")", "\n", "batch_data", "[", "'passage_length'", "]", ".", "append", "(", "0", ")", "\n", "", "", "", "batch_data", ",", "padded_p_len", ",", "padded_q_len", "=", "self", ".", "_dynamic_padding", "(", "batch_data", ",", "pad_id", ")", "\n", "for", "sample", "in", "batch_data", "[", "'raw_data'", "]", ":", "\n", "            ", "if", "'answer_passages'", "in", "sample", "and", "len", "(", "sample", "[", "'answer_passages'", "]", ")", ":", "\n", "                ", "gold_passage_offset", "=", "padded_p_len", "*", "sample", "[", "'answer_passages'", "]", "[", "0", "]", "\n", "batch_data", "[", "'start_id'", "]", ".", "append", "(", "gold_passage_offset", "+", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "0", "]", ")", "\n", "batch_data", "[", "'end_id'", "]", ".", "append", "(", "gold_passage_offset", "+", "sample", "[", "'answer_spans'", "]", "[", "0", "]", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "# fake span for some samples, only valid for testing", "\n", "                ", "batch_data", "[", "'start_id'", "]", ".", "append", "(", "0", ")", "\n", "batch_data", "[", "'end_id'", "]", ".", "append", "(", "0", ")", "\n", "", "", "return", "batch_data", "\n", "\n", "", "def", "_dynamic_padding", "(", "self", ",", "batch_data", ",", "pad_id", ")", ":", "\n", "        ", "\"\"\"\n        Dynamically pads the batch_data with pad_id\n        \"\"\"", "\n", "pad_p_len", "=", "min", "(", "self", ".", "max_p_len", ",", "max", "(", "batch_data", "[", "'passage_length'", "]", ")", ")", "\n", "pad_q_len", "=", "min", "(", "self", ".", "max_q_len", ",", "max", "(", "batch_data", "[", "'question_length'", "]", ")", ")", "\n", "batch_data", "[", "'passage_token_ids'", "]", "=", "[", "(", "ids", "+", "[", "pad_id", "]", "*", "(", "pad_p_len", "-", "len", "(", "ids", ")", ")", ")", "[", ":", "pad_p_len", "]", "\n", "for", "ids", "in", "batch_data", "[", "'passage_token_ids'", "]", "]", "\n", "batch_data", "[", "'question_token_ids'", "]", "=", "[", "(", "ids", "+", "[", "pad_id", "]", "*", "(", "pad_q_len", "-", "len", "(", "ids", ")", ")", ")", "[", ":", "pad_q_len", "]", "\n", "for", "ids", "in", "batch_data", "[", "'question_token_ids'", "]", "]", "\n", "return", "batch_data", ",", "pad_p_len", ",", "pad_q_len", "\n", "\n", "", "def", "word_iter", "(", "self", ",", "set_name", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Iterates over all the words in the dataset\n        Args:\n            set_name: if it is set, then the specific set will be used\n        Returns:\n            a generator\n        \"\"\"", "\n", "if", "set_name", "is", "None", ":", "\n", "            ", "data_set", "=", "self", ".", "train_set", "+", "self", ".", "dev_set", "+", "self", ".", "test_set", "\n", "", "elif", "set_name", "==", "'train'", ":", "\n", "            ", "data_set", "=", "self", ".", "train_set", "\n", "", "elif", "set_name", "==", "'dev'", ":", "\n", "            ", "data_set", "=", "self", ".", "dev_set", "\n", "", "elif", "set_name", "==", "'test'", ":", "\n", "            ", "data_set", "=", "self", ".", "test_set", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.word_iter": [[175, 201], ["NotImplementedError"], "methods", ["None"], ["", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'No data set named as {}'", ".", "format", "(", "set_name", ")", ")", "\n", "", "if", "data_set", "is", "not", "None", ":", "\n", "            ", "for", "sample", "in", "data_set", ":", "\n", "                ", "for", "token", "in", "sample", "[", "'question_tokens'", "]", ":", "\n", "                    ", "yield", "token", "\n", "", "for", "passage", "in", "sample", "[", "'passages'", "]", ":", "\n", "                    ", "for", "token", "in", "passage", "[", "'passage_tokens'", "]", ":", "\n", "                        ", "yield", "token", "\n", "\n", "", "", "", "", "", "def", "convert_to_ids", "(", "self", ",", "vocab", ")", ":", "\n", "        ", "\"\"\"\n        Convert the question and passage in the original dataset to ids\n        Args:\n            vocab: the vocabulary on this dataset\n        \"\"\"", "\n", "for", "data_set", "in", "[", "self", ".", "train_set", ",", "self", ".", "dev_set", ",", "self", ".", "test_set", "]", ":", "\n", "            ", "if", "data_set", "is", "None", ":", "\n", "                ", "continue", "\n", "", "for", "sample", "in", "data_set", ":", "\n", "                ", "sample", "[", "'question_token_ids'", "]", "=", "vocab", ".", "convert_to_ids", "(", "sample", "[", "'question_tokens'", "]", ")", "\n", "for", "passage", "in", "sample", "[", "'passages'", "]", ":", "\n", "                    ", "passage", "[", "'passage_token_ids'", "]", "=", "vocab", ".", "convert_to_ids", "(", "passage", "[", "'passage_tokens'", "]", ")", "\n", "\n", "", "", "", "", "def", "gen_mini_batches", "(", "self", ",", "set_name", ",", "batch_size", ",", "pad_id", ",", "shuffle", "=", "True", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.convert_to_ids": [[202, 217], ["vocab.convert_to_ids", "vocab.convert_to_ids"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.convert_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.convert_to_ids"], ["\n", "if", "set_name", "==", "'train'", ":", "\n", "            ", "data", "=", "self", ".", "train_set", "\n", "", "elif", "set_name", "==", "'dev'", ":", "\n", "            ", "data", "=", "self", ".", "dev_set", "\n", "", "elif", "set_name", "==", "'test'", ":", "\n", "            ", "data", "=", "self", ".", "test_set", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'No data set named as {}'", ".", "format", "(", "set_name", ")", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.gen_mini_batches": [[218, 245], ["len", "numpy.arange", "numpy.arange", "numpy.random.shuffle", "dataset.BRCDataset._one_mini_batch", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset._one_mini_batch"], ["", "data_size", "=", "len", "(", "data", ")", "\n", "indices", "=", "np", ".", "arange", "(", "data_size", ")", "\n", "if", "shuffle", ":", "\n", "            ", "np", ".", "random", ".", "shuffle", "(", "indices", ")", "\n", "", "for", "batch_start", "in", "np", ".", "arange", "(", "0", ",", "data_size", ",", "batch_size", ")", ":", "\n", "            ", "batch_indices", "=", "indices", "[", "batch_start", ":", "batch_start", "+", "batch_size", "]", "\n", "yield", "self", ".", "_one_mini_batch", "(", "data", ",", "batch_indices", ",", "pad_id", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.precision_recall_f1": [[29, 56], ["sum", "isinstance", "prediction.split", "isinstance", "ground_truth.split", "collections.Counter", "collections.Counter", "common.values", "len", "len"], "function", ["None"], ["def", "precision_recall_f1", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "\"\"\"\n    This function calculates and returns the precision, recall and f1-score\n    Args:\n        prediction: prediction string or list to be matched\n        ground_truth: golden string or list reference\n    Returns:\n        floats of (p, r, f1)\n    Raises:\n        None\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "prediction", ",", "list", ")", ":", "\n", "        ", "prediction_tokens", "=", "prediction", ".", "split", "(", ")", "\n", "", "else", ":", "\n", "        ", "prediction_tokens", "=", "prediction", "\n", "", "if", "not", "isinstance", "(", "ground_truth", ",", "list", ")", ":", "\n", "        ", "ground_truth_tokens", "=", "ground_truth", ".", "split", "(", ")", "\n", "", "else", ":", "\n", "        ", "ground_truth_tokens", "=", "ground_truth", "\n", "", "common", "=", "Counter", "(", "prediction_tokens", ")", "&", "Counter", "(", "ground_truth_tokens", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "0", ",", "0", ",", "0", "\n", "", "p", "=", "1.0", "*", "num_same", "/", "len", "(", "prediction_tokens", ")", "\n", "r", "=", "1.0", "*", "num_same", "/", "len", "(", "ground_truth_tokens", ")", "\n", "f1", "=", "(", "2", "*", "p", "*", "r", ")", "/", "(", "p", "+", "r", ")", "\n", "return", "p", ",", "r", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.recall": [[58, 70], ["preprocess.precision_recall_f1"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.precision_recall_f1"], ["", "def", "recall", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "\"\"\"\n    This function calculates and returns the recall\n    Args:\n        prediction: prediction string or list to be matched\n        ground_truth: golden string or list reference\n    Returns:\n        floats of recall\n    Raises:\n        None\n    \"\"\"", "\n", "return", "precision_recall_f1", "(", "prediction", ",", "ground_truth", ")", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.f1_score": [[72, 84], ["preprocess.precision_recall_f1"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.precision_recall_f1"], ["", "def", "f1_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "\"\"\"\n    This function calculates and returns the f1-score\n    Args:\n        prediction: prediction string or list to be matched\n        ground_truth: golden string or list reference\n    Returns:\n        floats of f1\n    Raises:\n        None\n    \"\"\"", "\n", "return", "precision_recall_f1", "(", "prediction", ",", "ground_truth", ")", "[", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.metric_max_over_ground_truths": [[86, 103], ["max", "scores_for_ground_truths.append", "preprocess.recall", "preprocess.recall", "preprocess.f1_score"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.recall", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.recall", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.f1_score"], ["", "def", "metric_max_over_ground_truths", "(", "metric_fn", ",", "prediction", ",", "ground_truths", ")", ":", "\n", "    ", "\"\"\"\n    This function calculates and returns the precision, recall and f1-score\n    Args:\n        metric_fn: metric function pointer which calculates scores according to corresponding logic.\n        prediction: prediction string or list to be matched\n        ground_truth: golden string or list reference\n    Returns:\n        floats of (p, r, f1)\n    Raises:\n        None\n    \"\"\"", "\n", "scores_for_ground_truths", "=", "[", "]", "\n", "for", "ground_truth", "in", "ground_truths", ":", "\n", "        ", "score", "=", "metric_fn", "(", "prediction", ",", "ground_truth", ")", "\n", "scores_for_ground_truths", ".", "append", "(", "score", ")", "\n", "", "return", "max", "(", "scores_for_ground_truths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.find_best_question_match": [[105, 140], ["enumerate", "len", "preprocess.metric_max_over_ground_truths", "len", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.metric_max_over_ground_truths"], ["", "def", "find_best_question_match", "(", "doc", ",", "question", ",", "with_score", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    For each document, find the paragraph that matches best to the question.\n    Args:\n        doc: The document object.\n        question: The question tokens.\n        with_score: If True then the match score will be returned,\n            otherwise False.\n    Returns:\n        The index of the best match paragraph, if with_score=False,\n        otherwise returns a tuple of the index of the best match paragraph\n        and the match score of that paragraph.\n    \"\"\"", "\n", "most_related_para", "=", "-", "1", "\n", "max_related_score", "=", "0", "\n", "most_related_para_len", "=", "0", "\n", "for", "p_idx", ",", "para_tokens", "in", "enumerate", "(", "doc", "[", "'segmented_paragraphs'", "]", ")", ":", "\n", "        ", "if", "len", "(", "question", ")", ">", "0", ":", "\n", "            ", "related_score", "=", "metric_max_over_ground_truths", "(", "recall", ",", "\n", "para_tokens", ",", "\n", "question", ")", "\n", "", "else", ":", "\n", "            ", "related_score", "=", "0", "\n", "\n", "", "if", "related_score", ">", "max_related_score", "or", "(", "related_score", "==", "max_related_score", "and", "len", "(", "para_tokens", ")", "<", "most_related_para_len", ")", ":", "\n", "            ", "most_related_para", "=", "p_idx", "\n", "max_related_score", "=", "related_score", "\n", "most_related_para_len", "=", "len", "(", "para_tokens", ")", "\n", "", "", "if", "most_related_para", "==", "-", "1", ":", "\n", "        ", "most_related_para", "=", "0", "\n", "", "if", "with_score", ":", "\n", "        ", "return", "most_related_para", ",", "max_related_score", "\n", "", "return", "most_related_para", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.find_fake_answer": [[142, 212], ["set", "enumerate", "enumerate", "range", "sample[].append", "sample[].append", "sample[].append", "sample[].append", "set", "len", "range", "len", "preprocess.metric_max_over_ground_truths", "len", "len", "len", "preprocess.metric_max_over_ground_truths", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.metric_max_over_ground_truths", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.preprocess.metric_max_over_ground_truths"], ["", "def", "find_fake_answer", "(", "sample", ")", ":", "\n", "    ", "\"\"\"\n    For each document, finds the most related paragraph based on recall,\n    then finds a span that maximize the f1_score compared with the gold answers\n    and uses this span as a fake answer span\n    Args:\n        sample: a sample in the dataset\n    Returns:\n        None\n    Raises:\n        None\n    \"\"\"", "\n", "for", "doc", "in", "sample", "[", "'documents'", "]", ":", "\n", "        ", "most_related_para", "=", "-", "1", "\n", "most_related_para_len", "=", "999999", "\n", "max_related_score", "=", "0", "\n", "for", "p_idx", ",", "para_tokens", "in", "enumerate", "(", "doc", "[", "'segmented_paragraphs'", "]", ")", ":", "\n", "            ", "if", "len", "(", "sample", "[", "'segmented_answers'", "]", ")", ">", "0", ":", "\n", "                ", "related_score", "=", "metric_max_over_ground_truths", "(", "recall", ",", "\n", "para_tokens", ",", "\n", "sample", "[", "'segmented_answers'", "]", ")", "\n", "", "else", ":", "\n", "                ", "continue", "\n", "", "if", "related_score", ">", "max_related_score", "or", "(", "related_score", "==", "max_related_score", "\n", "and", "len", "(", "para_tokens", ")", "<", "most_related_para_len", ")", ":", "\n", "                ", "most_related_para", "=", "p_idx", "\n", "most_related_para_len", "=", "len", "(", "para_tokens", ")", "\n", "max_related_score", "=", "related_score", "\n", "", "", "doc", "[", "'most_related_para'", "]", "=", "most_related_para", "\n", "\n", "", "sample", "[", "'answer_docs'", "]", "=", "[", "]", "\n", "sample", "[", "'answer_spans'", "]", "=", "[", "]", "\n", "sample", "[", "'fake_answers'", "]", "=", "[", "]", "\n", "sample", "[", "'match_scores'", "]", "=", "[", "]", "\n", "\n", "best_match_score", "=", "0", "\n", "best_match_d_idx", ",", "best_match_span", "=", "-", "1", ",", "[", "-", "1", ",", "-", "1", "]", "\n", "best_fake_answer", "=", "None", "\n", "answer_tokens", "=", "set", "(", ")", "\n", "for", "segmented_answer", "in", "sample", "[", "'segmented_answers'", "]", ":", "\n", "        ", "answer_tokens", "=", "answer_tokens", "|", "set", "(", "[", "token", "for", "token", "in", "segmented_answer", "]", ")", "\n", "", "for", "d_idx", ",", "doc", "in", "enumerate", "(", "sample", "[", "'documents'", "]", ")", ":", "\n", "        ", "if", "not", "doc", "[", "'is_selected'", "]", ":", "\n", "            ", "continue", "\n", "", "if", "doc", "[", "'most_related_para'", "]", "==", "-", "1", ":", "\n", "            ", "doc", "[", "'most_related_para'", "]", "=", "0", "\n", "", "most_related_para_tokens", "=", "doc", "[", "'segmented_paragraphs'", "]", "[", "doc", "[", "'most_related_para'", "]", "]", "[", ":", "1000", "]", "\n", "for", "start_tidx", "in", "range", "(", "len", "(", "most_related_para_tokens", ")", ")", ":", "\n", "            ", "if", "most_related_para_tokens", "[", "start_tidx", "]", "not", "in", "answer_tokens", ":", "\n", "                ", "continue", "\n", "", "for", "end_tidx", "in", "range", "(", "len", "(", "most_related_para_tokens", ")", "-", "1", ",", "start_tidx", "-", "1", ",", "-", "1", ")", ":", "\n", "                ", "span_tokens", "=", "most_related_para_tokens", "[", "start_tidx", ":", "end_tidx", "+", "1", "]", "\n", "if", "len", "(", "sample", "[", "'segmented_answers'", "]", ")", ">", "0", ":", "\n", "                    ", "match_score", "=", "metric_max_over_ground_truths", "(", "f1_score", ",", "span_tokens", ",", "\n", "sample", "[", "'segmented_answers'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "match_score", "=", "0", "\n", "", "if", "match_score", "==", "0", ":", "\n", "                    ", "break", "\n", "", "if", "match_score", ">", "best_match_score", ":", "\n", "                    ", "best_match_d_idx", "=", "d_idx", "\n", "best_match_span", "=", "[", "start_tidx", ",", "end_tidx", "]", "\n", "best_match_score", "=", "match_score", "\n", "best_fake_answer", "=", "''", ".", "join", "(", "span_tokens", ")", "\n", "", "", "", "", "if", "best_match_score", ">", "0", ":", "\n", "        ", "sample", "[", "'answer_docs'", "]", ".", "append", "(", "best_match_d_idx", ")", "\n", "sample", "[", "'answer_spans'", "]", ".", "append", "(", "best_match_span", ")", "\n", "sample", "[", "'fake_answers'", "]", ".", "append", "(", "best_fake_answer", ")", "\n", "sample", "[", "'match_scores'", "]", ".", "append", "(", "best_match_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.prepare_batch_input": [[49, 89], ["len", "len", "range", "print", "range", "len", "run.prepare_batch_input._get_label"], "function", ["None"], ["\n", "", "def", "forward", "(", "self", ",", "y", ",", "label", ")", ":", "\n", "        ", "start_logits", ",", "end_logits", ",", "cls_logits", "=", "y", "\n", "start_position", ",", "end_position", ",", "answerable_label", "=", "label", "\n", "start_position", "=", "paddle", ".", "unsqueeze", "(", "start_position", ",", "axis", "=", "-", "1", ")", "\n", "end_position", "=", "paddle", ".", "unsqueeze", "(", "end_position", ",", "axis", "=", "-", "1", ")", "\n", "answerable_label", "=", "paddle", ".", "unsqueeze", "(", "answerable_label", ",", "axis", "=", "-", "1", ")", "\n", "start_loss", "=", "paddle", ".", "nn", ".", "functional", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "start_logits", ",", "label", "=", "start_position", ",", "soft_label", "=", "False", ")", "\n", "start_loss", "=", "paddle", ".", "mean", "(", "start_loss", ")", "\n", "end_loss", "=", "paddle", ".", "nn", ".", "functional", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "end_logits", ",", "label", "=", "end_position", ",", "soft_label", "=", "False", ")", "\n", "end_loss", "=", "paddle", ".", "mean", "(", "end_loss", ")", "\n", "cls_loss", "=", "paddle", ".", "nn", ".", "functional", ".", "softmax_with_cross_entropy", "(", "\n", "logits", "=", "cls_logits", ",", "label", "=", "answerable_label", ",", "soft_label", "=", "False", ")", "\n", "cls_loss", "=", "paddle", ".", "mean", "(", "cls_loss", ")", "\n", "mrc_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "loss", "=", "(", "mrc_loss", "+", "cls_loss", ")", "/", "2", "\n", "return", "loss", "\n", "\n", "\n", "", "", "def", "evaluate", "(", "model", ",", "data_loader", ",", "args", ",", "prefix", "=", "\"\"", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "all_start_logits", "=", "[", "]", "\n", "all_end_logits", "=", "[", "]", "\n", "all_cls_logits", "=", "[", "]", "\n", "tic_eval", "=", "time", ".", "time", "(", ")", "\n", "\n", "for", "batch", "in", "data_loader", ":", "\n", "        ", "input_ids", ",", "segment_ids", "=", "batch", "\n", "start_logits_tensor", ",", "end_logits_tensor", ",", "cls_logits_tensor", "=", "model", "(", "input_ids", ",", "segment_ids", ")", "\n", "\n", "for", "idx", "in", "range", "(", "start_logits_tensor", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "if", "len", "(", "all_start_logits", ")", "%", "1000", "==", "0", "and", "len", "(", "all_start_logits", ")", ":", "\n", "                ", "print", "(", "\"Processing example: %d\"", "%", "len", "(", "all_start_logits", ")", ")", "\n", "print", "(", "'time per 1000:'", ",", "time", ".", "time", "(", ")", "-", "tic_eval", ")", "\n", "tic_eval", "=", "time", ".", "time", "(", ")", "\n", "\n", "\n", "", "all_start_logits", ".", "append", "(", "start_logits_tensor", ".", "numpy", "(", ")", "[", "idx", "]", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.batch_reader": [[91, 96], ["res.append", "run.prepare_batch_input"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.prepare_batch_input"], ["all_cls_logits", ".", "append", "(", "cls_logits_tensor", ".", "numpy", "(", ")", "[", "idx", "]", ")", "\n", "\n", "", "", "all_predictions", ",", "all_nbest_json", ",", "all_cls_predictions", "=", "compute_prediction_checklist", "(", "\n", "data_loader", ".", "dataset", ".", "data", ",", "data_loader", ".", "dataset", ".", "new_data", ",", "\n", "(", "all_start_logits", ",", "all_end_logits", ",", "all_cls_logits", ")", ",", "True", ",", "args", ".", "n_best_size", ",", "\n", "args", ".", "max_answer_length", ",", "args", ".", "cls_threshold", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.read_multiple": [[98, 124], ["reader", "res.append", "len", "len", "len", "len", "range"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.scripts.convert_mrqa2squad.reader"], ["if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "prefix", "+", "'_predictions.json'", ")", ",", "\"w\"", ",", "encoding", "=", "'utf-8'", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "\n", "json", ".", "dumps", "(", "\n", "all_predictions", ",", "ensure_ascii", "=", "False", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "prefix", "+", "'_nbest_predictions.json'", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_nbest_json", ",", "indent", "=", "4", ",", "ensure_ascii", "=", "False", ")", "+", "u\"\\n\"", ")", "\n", "\n", "", "if", "all_cls_predictions", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "prefix", "+", "\"_cls_preditions.json\"", ")", ",", "\"w\"", ")", "as", "f_cls", ":", "\n", "            ", "for", "cls_predictions", "in", "all_cls_predictions", ":", "\n", "                ", "qas_id", ",", "pred_cls_label", ",", "no_answer_prob", ",", "answerable_prob", "=", "cls_predictions", "\n", "f_cls", ".", "write", "(", "'{}\\t{}\\t{}\\t{}\\n'", ".", "format", "(", "qas_id", ",", "pred_cls_label", ",", "no_answer_prob", ",", "answerable_prob", ")", ")", "\n", "\n", "", "", "", "model", ".", "train", "(", ")", "\n", "\n", "\n", "", "def", "run", "(", "args", ")", ":", "\n", "    ", "paddle", ".", "set_device", "(", "args", ".", "device", ")", "\n", "if", "paddle", ".", "distributed", ".", "get_world_size", "(", ")", ">", "1", ":", "\n", "        ", "paddle", ".", "distributed", ".", "init_parallel_env", "(", ")", "\n", "\n", "", "args", ".", "model_type", "=", "args", ".", "model_type", ".", "lower", "(", ")", "\n", "model_class", ",", "tokenizer_class", "=", "MODEL_CLASSES", "[", "args", ".", "model_type", "]", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.LodTensor_Array": [[126, 133], ["lod_tensor.lod", "numpy.array", "range", "new_array.append", "len"], "function", ["None"], ["set_seed", "(", "args", ")", "\n", "\n", "if", "paddle", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "model_name_or_path", ")", ":", "\n", "            ", "print", "(", "\"init checkpoint from %s\"", "%", "args", ".", "model_name_or_path", ")", "\n", "", "", "model", "=", "model_class", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ")", "\n", "\n", "if", "paddle", ".", "distributed", ".", "get_world_size", "(", ")", ">", "1", ":", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.print_para": [[135, 151], ["train_prog.block().all_parameters", "logger.info", "numpy.array", "numpy.prod", "logger.info", "train_prog.block", "train_exe.scope.find_var().get_tensor", "np.array.mean", "np.array.max", "np.array.min", "train_exe.scope.find_var"], "function", ["None"], ["\n", "", "def", "prepare_train_features", "(", "examples", ")", ":", "\n", "# Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results", "\n", "# in one example possible giving several features when a context is long, each of those features having a", "\n", "# context that overlaps a bit the context of the previous feature.", "\n", "        ", "contexts", "=", "[", "examples", "[", "i", "]", "[", "'context'", "]", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", "]", "\n", "questions", "=", "[", "examples", "[", "i", "]", "[", "'question'", "]", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", "]", "\n", "\n", "tokenized_examples", "=", "tokenizer", "(", "\n", "questions", ",", "\n", "contexts", ",", "\n", "stride", "=", "args", ".", "doc_stride", ",", "\n", "max_seq_len", "=", "args", ".", "max_seq_length", ")", "\n", "\n", "for", "i", ",", "tokenized_example", "in", "enumerate", "(", "tokenized_examples", ")", ":", "\n", "# We will label impossible answers with the index of the CLS token.", "\n", "            ", "input_ids", "=", "tokenized_example", "[", "\"input_ids\"", "]", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.find_best_answer_for_passage": [[153, 173], ["range", "len", "min", "range", "len"], "function", ["None"], ["\n", "# The offset mappings will give us a map from token to character position in the original context. This will", "\n", "# help us compute the start_positions and end_positions.", "\n", "offsets", "=", "tokenized_example", "[", "'offset_mapping'", "]", "\n", "\n", "# Grab the sequence corresponding to that example (to know what is the context and what is the question).", "\n", "sequence_ids", "=", "tokenized_example", "[", "'token_type_ids'", "]", "\n", "\n", "# One example can give several spans, this is the index of the example containing this span of text.", "\n", "sample_index", "=", "tokenized_example", "[", "'overflow_to_sample'", "]", "\n", "answers", "=", "examples", "[", "sample_index", "]", "[", "'answers'", "]", "\n", "answer_starts", "=", "examples", "[", "sample_index", "]", "[", "'answer_starts'", "]", "\n", "\n", "# If no answers are given, set the cls_index as answer.", "\n", "if", "len", "(", "answer_starts", ")", "==", "0", ":", "\n", "                ", "tokenized_examples", "[", "i", "]", "[", "\"start_positions\"", "]", "=", "cls_index", "\n", "tokenized_examples", "[", "i", "]", "[", "\"end_positions\"", "]", "=", "cls_index", "\n", "tokenized_examples", "[", "i", "]", "[", "'answerable_label'", "]", "=", "0", "\n", "", "else", ":", "\n", "# Start/end character index of the answer in the text.", "\n", "                ", "start_char", "=", "answer_starts", "[", "0", "]", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.find_best_answer_for_inst": [[175, 209], ["enumerate", "min", "run.find_best_answer_for_passage", "len", "len", "logger.info", "len"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.find_best_answer_for_passage"], ["\n", "# Start token index of the current span in the text.", "\n", "token_start_index", "=", "0", "\n", "while", "sequence_ids", "[", "token_start_index", "]", "!=", "1", ":", "\n", "                    ", "token_start_index", "+=", "1", "\n", "\n", "# End token index of the current span in the text.", "\n", "", "token_end_index", "=", "len", "(", "input_ids", ")", "-", "2", "\n", "while", "sequence_ids", "[", "token_end_index", "]", "!=", "1", ":", "\n", "                    ", "token_end_index", "-=", "1", "\n", "\n", "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).", "\n", "", "if", "not", "(", "offsets", "[", "token_start_index", "]", "[", "0", "]", "<=", "start_char", "and", "\n", "offsets", "[", "token_end_index", "]", "[", "1", "]", ">=", "end_char", ")", ":", "\n", "                    ", "tokenized_examples", "[", "i", "]", "[", "\"start_positions\"", "]", "=", "cls_index", "\n", "tokenized_examples", "[", "i", "]", "[", "\"end_positions\"", "]", "=", "cls_index", "\n", "tokenized_examples", "[", "i", "]", "[", "'answerable_label'", "]", "=", "0", "\n", "", "else", ":", "\n", "# Otherwise move the token_start_index and token_end_index to the two ends of the answer.", "\n", "# Note: we could go after the last offset if the answer is the last word (edge case).", "\n", "                    ", "while", "token_start_index", "<", "len", "(", "offsets", ")", "and", "offsets", "[", "\n", "token_start_index", "]", "[", "0", "]", "<=", "start_char", ":", "\n", "                        ", "token_start_index", "+=", "1", "\n", "", "tokenized_examples", "[", "i", "]", "[", "\n", "\"start_positions\"", "]", "=", "token_start_index", "-", "1", "\n", "while", "offsets", "[", "token_end_index", "]", "[", "1", "]", ">=", "end_char", ":", "\n", "                        ", "token_end_index", "-=", "1", "\n", "", "tokenized_examples", "[", "i", "]", "[", "\"end_positions\"", "]", "=", "token_end_index", "+", "1", "\n", "tokenized_examples", "[", "i", "]", "[", "'answerable_label'", "]", "=", "1", "\n", "\n", "", "", "", "return", "tokenized_examples", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "assert", "args", ".", "train_file", "!=", "None", ",", "\"--train_file should be set when training!\"", "\n", "train_ds", "=", "DuReaderChecklist", "(", ")", ".", "read", "(", "args", ".", "train_file", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.validation": [[211, 321], ["paddle.ParallelExecutor", "run.print_para", "paddle.DataFeeder", "vocab.get_id", "run.read_multiple", "enumerate", "inference_program.global_block().var", "brc_data.gen_mini_batches", "read_multiple.", "run.batch_reader", "fluid.ParallelExecutor.run", "numpy.array().sum", "run.LodTensor_Array", "run.LodTensor_Array", "val_fetch_outs[].lod", "len", "len", "numpy.array().sum", "enumerate", "os.path.join", "logger.info", "len", "zip", "utils.compute_bleu_rouge", "bool", "numpy.array", "numpy.array", "logger.info", "len", "zip", "os.path.exists", "os.makedirs", "open", "inference_program.global_block", "list", "numpy.array", "numpy.array", "run.find_best_answer_for_inst", "pred_answers.append", "fout.write", "len", "utils.normalize", "utils.normalize", "fluid.DataFeeder.feed_parallel", "range", "ref_answers.append", "len", "json.dumps"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.print_para", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_id", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.read_multiple", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.gen_mini_batches", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.batch_reader", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.LodTensor_Array", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.LodTensor_Array", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.compute_bleu_rouge", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.find_best_answer_for_inst", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.normalize", "home.repos.pwc.inspect_result.baidu_DuReader.utils.dureader_eval.normalize"], ["\n", "train_batch_sampler", "=", "paddle", ".", "io", ".", "DistributedBatchSampler", "(", "\n", "train_ds", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "train_batchify_fn", "=", "lambda", "samples", ",", "fn", "=", "Dict", "(", "{", "\n", "\"input_ids\"", ":", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "tokenizer", ".", "pad_token_id", ")", ",", "\n", "\"token_type_ids\"", ":", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "tokenizer", ".", "pad_token_type_id", ")", ",", "\n", "\"start_positions\"", ":", "Stack", "(", "dtype", "=", "\"int64\"", ")", ",", "\n", "\"end_positions\"", ":", "Stack", "(", "dtype", "=", "\"int64\"", ")", ",", "\n", "\"answerable_label\"", ":", "Stack", "(", "dtype", "=", "\"int64\"", ")", "\n", "}", ")", ":", "fn", "(", "samples", ")", "\n", "train_data_loader", "=", "DataLoader", "(", "\n", "dataset", "=", "train_ds", ",", "\n", "batch_sampler", "=", "train_batch_sampler", ",", "\n", "collate_fn", "=", "train_batchify_fn", ",", "\n", "return_list", "=", "True", ")", "\n", "\n", "num_training_steps", "=", "args", ".", "max_steps", "if", "args", ".", "max_steps", ">", "0", "else", "len", "(", "\n", "train_data_loader", ")", "*", "args", ".", "num_train_epochs", "\n", "\n", "if", "paddle", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "            ", "dev_count", "=", "paddle", ".", "fluid", ".", "core", ".", "get_cuda_device_count", "(", ")", "\n", "print", "(", "\"Device count: %d\"", "%", "dev_count", ")", "\n", "print", "(", "\"Num train examples: %d\"", "%", "len", "(", "train_ds", ".", "data", ")", ")", "\n", "print", "(", "\"Max train steps: %d\"", "%", "num_training_steps", ")", "\n", "\n", "", "lr_scheduler", "=", "LinearDecayWithWarmup", "(", "\n", "args", ".", "learning_rate", ",", "num_training_steps", ",", "args", ".", "warmup_proportion", ")", "\n", "\n", "optimizer", "=", "paddle", ".", "optimizer", ".", "AdamW", "(", "\n", "learning_rate", "=", "lr_scheduler", ",", "\n", "epsilon", "=", "args", ".", "adam_epsilon", ",", "\n", "parameters", "=", "model", ".", "parameters", "(", ")", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", "apply_decay_param_fun", "=", "lambda", "x", ":", "x", "in", "[", "\n", "p", ".", "name", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "[", "\"bias\"", ",", "\"norm\"", "]", ")", "\n", "]", ")", "\n", "criterion", "=", "CrossEntropyLossForChecklist", "(", ")", "\n", "\n", "global_step", "=", "0", "\n", "tic_train", "=", "time", ".", "time", "(", ")", "\n", "for", "epoch", "in", "range", "(", "args", ".", "num_train_epochs", ")", ":", "\n", "            ", "for", "step", ",", "batch", "in", "enumerate", "(", "train_data_loader", ")", ":", "\n", "                ", "global_step", "+=", "1", "\n", "input_ids", ",", "segment_ids", ",", "start_positions", ",", "end_positions", ",", "answerable_label", "=", "batch", "\n", "\n", "logits", "=", "model", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "segment_ids", ")", "\n", "loss", "=", "criterion", "(", "logits", ",", "(", "start_positions", ",", "end_positions", ",", "answerable_label", ")", ")", "\n", "\n", "\n", "if", "global_step", "%", "args", ".", "logging_steps", "==", "0", ":", "\n", "                    ", "print", "(", "\n", "\"global step %d, epoch: %d, batch: %d, loss: %f, speed: %.2f step/s\"", "\n", "%", "(", "global_step", ",", "epoch", ",", "step", ",", "loss", ",", "\n", "args", ".", "logging_steps", "/", "(", "time", ".", "time", "(", ")", "-", "tic_train", ")", ")", ")", "\n", "tic_train", "=", "time", ".", "time", "(", ")", "\n", "", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "lr_scheduler", ".", "step", "(", ")", "\n", "optimizer", ".", "clear_gradients", "(", ")", "\n", "\n", "if", "global_step", "%", "args", ".", "save_steps", "==", "0", "or", "global_step", "==", "num_training_steps", ":", "\n", "                    ", "if", "paddle", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "                        ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\n", "\"model_%d\"", "%", "global_step", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_dir", ")", ":", "\n", "                            ", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "# need better way to get inner model of DataParallel", "\n", "", "model_to_save", "=", "model", ".", "_layers", "if", "isinstance", "(", "\n", "model", ",", "paddle", ".", "DataParallel", ")", "else", "model", "\n", "model_to_save", ".", "save_pretrained", "(", "output_dir", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "output_dir", ")", "\n", "print", "(", "'Saving checkpoint to:'", ",", "output_dir", ")", "\n", "\n", "", "", "", "", "", "def", "prepare_validation_features", "(", "examples", ")", ":", "\n", "# Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results", "\n", "# in one example possible giving several features when a context is long, each of those features having a", "\n", "# context that overlaps a bit the context of the previous feature.", "\n", "        ", "contexts", "=", "[", "examples", "[", "i", "]", "[", "'context'", "]", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", "]", "\n", "questions", "=", "[", "examples", "[", "i", "]", "[", "'question'", "]", "for", "i", "in", "range", "(", "len", "(", "examples", ")", ")", "]", "\n", "\n", "tokenized_examples", "=", "tokenizer", "(", "\n", "questions", ",", "\n", "contexts", ",", "\n", "stride", "=", "args", ".", "doc_stride", ",", "\n", "max_seq_len", "=", "args", ".", "max_seq_length", ")", "\n", "\n", "# For validation, there is no need to compute start and end positions", "\n", "for", "i", ",", "tokenized_example", "in", "enumerate", "(", "tokenized_examples", ")", ":", "\n", "# Grab the sequence corresponding to that example (to know what is the context and what is the question).", "\n", "            ", "sequence_ids", "=", "tokenized_example", "[", "'token_type_ids'", "]", "\n", "\n", "# One example can give several spans, this is the index of the example containing this span of text.", "\n", "sample_index", "=", "tokenized_example", "[", "'overflow_to_sample'", "]", "\n", "tokenized_examples", "[", "i", "]", "[", "\"example_id\"", "]", "=", "examples", "[", "sample_index", "]", "[", "'id'", "]", "\n", "\n", "# Set to None the offset_mapping that are not part of the context so it's easy to determine if a token", "\n", "# position is part of the context or not.", "\n", "tokenized_examples", "[", "i", "]", "[", "\"offset_mapping\"", "]", "=", "[", "\n", "(", "o", "if", "sequence_ids", "[", "k", "]", "==", "1", "else", "None", ")", "\n", "for", "k", ",", "o", "in", "enumerate", "(", "tokenized_example", "[", "\"offset_mapping\"", "]", ")", "\n", "]", "\n", "\n", "", "return", "tokenized_examples", "\n", "\n", "", "if", "args", ".", "do_pred", ":", "\n", "        ", "input_files", "=", "[", "]", "\n", "assert", "args", ".", "predict_file", "!=", "None", ",", "\"--predict_file should be set when predicting!\"", "\n", "for", "input_pattern", "in", "args", ".", "predict_file", ":", "\n", "            ", "input_files", ".", "extend", "(", "glob", ".", "glob", "(", "input_pattern", ")", ")", "\n", "", "assert", "len", "(", "input_files", ")", ">", "0", ",", "'Can not find predict_file {}'", ".", "format", "(", "args", ".", "predict_file", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.l2_loss": [[323, 330], ["train_prog.block().all_parameters", "paddle.layers.elementwise_mul", "para_sum.append", "paddle.layers.sums", "train_prog.block", "paddle.layers.reduce_sum"], "function", ["None"], ["            ", "print", "(", "'Run prediction on {}'", ".", "format", "(", "input_file", ")", ")", "\n", "prefix", "=", "os", ".", "path", ".", "basename", "(", "input_file", ")", "\n", "prefix", "=", "re", ".", "sub", "(", "'.json'", ",", "''", ",", "prefix", ")", "\n", "dev_ds", "=", "DuReaderChecklist", "(", ")", ".", "read", "(", "input_file", ")", "\n", "dev_ds", ".", "map", "(", "prepare_validation_features", ",", "batched", "=", "True", ")", "\n", "\n", "dev_batch_sampler", "=", "paddle", ".", "io", ".", "BatchSampler", "(", "\n", "dev_ds", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "False", ")", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.train": [[332, 490], ["logger.info", "dataset.BRCDataset", "logger.info", "dataset.BRCDataset.convert_to_ids", "logger.info", "paddle.Program", "paddle.Program", "open", "logger.info", "paddle.CPUPlace", "int", "paddle.CUDAPlace", "paddle.core.get_cuda_device_count", "paddle.program_guard", "os.path.join", "pickle.load", "pickle.load", "os.environ.get", "paddle.unique_name.guard", "rc_model.rc_model", "fluid.Program.clone", "paddle.fluid.executor.Executor", "paddle.DataFeeder", "logger.info", "paddle.ParallelExecutor", "run.print_para", "range", "pickle.load.size", "multiprocessing.cpu_count", "paddle.optimizer.SGD", "fluid.optimizer.RMSPropOptimizer.minimize", "fluid.optimizer.RMSPropOptimizer.minimize", "paddle.CUDAPlace", "paddle.CPUPlace", "logger.info", "paddle.io.load_persistables", "paddle.fluid.executor.Executor.run", "paddle.global_scope().find_var().get_tensor", "fluid.global_scope().find_var().get_tensor.set", "fluid.Program.global_block().var", "time.time", "pickle.load.get_id", "run.read_multiple", "enumerate", "time.time", "logger.info", "logger.info", "logger.info", "paddle.optimizer.Adam", "pickle.load.embeddings.astype", "bool", "read_multiple.", "run.batch_reader", "fluid.ParallelExecutor.run", "numpy.array().mean", "run.validation", "logger.info", "logger.warning", "os.path.join", "paddle.io.save_persistables", "print", "print", "paddle.optimizer.RMSPropOptimizer", "logger.error", "exit", "run.l2_loss", "paddle.global_scope().find_var", "fluid.Program.global_block", "dataset.BRCDataset.gen_mini_batches", "dataset.BRCDataset.gen_mini_batches", "run.print_para", "logger.info", "str", "os.path.isdir", "os.makedirs", "print", "list", "numpy.array", "run.validation", "logger.info", "paddle.global_scope", "fluid.DataFeeder.feed_parallel"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.convert_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.rc_model", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.print_para", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_id", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.read_multiple", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.batch_reader", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.run.run", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.validation", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.l2_loss", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.gen_mini_batches", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.gen_mini_batches", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.print_para", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.validation"], ["dev_batchify_fn", "=", "lambda", "samples", ",", "fn", "=", "Dict", "(", "{", "\n", "\"input_ids\"", ":", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "tokenizer", ".", "pad_token_id", ")", ",", "\n", "\"token_type_ids\"", ":", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "tokenizer", ".", "pad_token_type_id", ")", "\n", "}", ")", ":", "fn", "(", "samples", ")", "\n", "\n", "dev_data_loader", "=", "DataLoader", "(", "\n", "dataset", "=", "dev_ds", ",", "\n", "batch_sampler", "=", "dev_batch_sampler", ",", "\n", "collate_fn", "=", "dev_batchify_fn", ",", "\n", "return_list", "=", "True", ")", "\n", "if", "paddle", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "                ", "evaluate", "(", "model", ",", "dev_data_loader", ",", "args", ",", "prefix", "=", "prefix", ")", "\n", "\n", "\n", "", "", "", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "assert", "args", ".", "device", "in", "[", "\n", "\"cpu\"", ",", "\"gpu\"", ",", "\"xpu\"", "\n", "]", ",", "\"Invalid device! Available device should be cpu, gpu, or xpu.\"", "\n", "\n", "run", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.evaluate": [[492, 537], ["logger.info", "dataset.BRCDataset", "logger.info", "dataset.BRCDataset.convert_to_ids", "logger.info", "paddle.Program", "paddle.Program", "open", "pickle.load", "logger.info", "paddle.program_guard", "os.path.join", "paddle.unique_name.guard", "rc_model.rc_model", "paddle.fluid.executor.Executor", "fluid.Program.clone", "run.validation", "logger.info", "logger.info", "pickle.load.size", "paddle.CPUPlace", "int", "paddle.CUDAPlace", "paddle.core.get_cuda_device_count", "logger.info", "paddle.io.load_persistables", "logger.error", "os.environ.get", "os.path.join", "multiprocessing.cpu_count"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.convert_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.rc_model", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.validation", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size"], []], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.predict": [[539, 581], ["logger.info", "dataset.BRCDataset", "logger.info", "dataset.BRCDataset.convert_to_ids", "logger.info", "paddle.Program", "paddle.Program", "open", "pickle.load", "logger.info", "paddle.program_guard", "os.path.join", "paddle.unique_name.guard", "rc_model.rc_model", "paddle.fluid.executor.Executor", "fluid.Program.clone", "run.validation", "pickle.load.size", "paddle.CPUPlace", "int", "paddle.CUDAPlace", "paddle.core.get_cuda_device_count", "logger.info", "paddle.io.load_persistables", "logger.error", "os.environ.get", "multiprocessing.cpu_count"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.convert_to_ids", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.rc_model", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.validation", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size"], []], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.prepare": [[583, 617], ["logger.info", "logger.info", "logger.info", "dataset.BRCDataset", "vocab.Vocab", "dataset.BRCDataset.word_iter", "vocab.Vocab.size", "vocab.Vocab.filter_tokens_by_cnt", "logger.info", "logger.info", "vocab.Vocab.randomly_init_embeddings", "logger.info", "logger.info", "os.path.exists", "vocab.Vocab.add", "vocab.Vocab.size", "open", "pickle.dump", "os.path.exists", "os.makedirs", "vocab.Vocab.size", "os.path.join"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.dataset.BRCDataset.word_iter", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.filter_tokens_by_cnt", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.randomly_init_embeddings", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size"], []], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args": [[23, 92], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.args.parse_args"], ["\n", "def", "str2bool", "(", "v", ")", ":", "\n", "# because argparse does not support to parse \"true, False\" as python", "\n", "# boolean directly", "\n", "    ", "return", "v", ".", "lower", "(", ")", "in", "(", "\"true\"", ",", "\"t\"", ",", "\"1\"", ")", "\n", "\n", "\n", "", "class", "ArgumentGroup", "(", "object", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "parser", ",", "title", ",", "des", ")", ":", "\n", "        ", "self", ".", "_group", "=", "parser", ".", "add_argument_group", "(", "title", "=", "title", ",", "description", "=", "des", ")", "\n", "\n", "", "def", "add_arg", "(", "self", ",", "name", ",", "type", ",", "default", ",", "help", ",", "**", "kwargs", ")", ":", "\n", "        ", "type", "=", "str2bool", "if", "type", "==", "bool", "else", "type", "\n", "self", ".", "_group", ".", "add_argument", "(", "\n", "\"--\"", "+", "name", ",", "\n", "default", "=", "default", ",", "\n", "type", "=", "type", ",", "\n", "help", "=", "help", "+", "' Default: %(default)s.'", ",", "\n", "**", "kwargs", ")", "\n", "\n", "\n", "", "", "def", "print_arguments", "(", "args", ")", ":", "\n", "    ", "print", "(", "'-----------  Configuration Arguments -----------'", ")", "\n", "for", "arg", ",", "value", "in", "sorted", "(", "six", ".", "iteritems", "(", "vars", "(", "args", ")", ")", ")", ":", "\n", "        ", "print", "(", "'%s: %s'", "%", "(", "arg", ",", "value", ")", ")", "\n", "", "print", "(", "'------------------------------------------------'", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.__init__": [[29, 49], ["vocab.Vocab.initial_tokens.extend", "vocab.Vocab.add", "vocab.Vocab.load_from_file"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.load_from_file"], ["        ", "self", ".", "id2token", "=", "{", "}", "\n", "self", ".", "token2id", "=", "{", "}", "\n", "self", ".", "token_cnt", "=", "{", "}", "\n", "self", ".", "lower", "=", "lower", "\n", "\n", "self", ".", "embed_dim", "=", "None", "\n", "self", ".", "embeddings", "=", "None", "\n", "\n", "self", ".", "pad_token", "=", "'<blank>'", "\n", "self", ".", "unk_token", "=", "'<unk>'", "\n", "\n", "self", ".", "initial_tokens", "=", "initial_tokens", "if", "initial_tokens", "is", "not", "None", "else", "[", "]", "\n", "self", ".", "initial_tokens", ".", "extend", "(", "[", "self", ".", "pad_token", ",", "self", ".", "unk_token", "]", ")", "\n", "for", "token", "in", "self", ".", "initial_tokens", ":", "\n", "            ", "self", ".", "add", "(", "token", ")", "\n", "\n", "", "if", "filename", "is", "not", "None", ":", "\n", "            ", "self", ".", "load_from_file", "(", "filename", ")", "\n", "\n", "", "", "def", "size", "(", "self", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size": [[50, 57], ["len"], "methods", ["None"], ["\n", "return", "len", "(", "self", ".", "id2token", ")", "\n", "\n", "", "def", "load_from_file", "(", "self", ",", "file_path", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.load_from_file": [[58, 67], ["open", "line.rstrip", "vocab.Vocab.add"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add"], ["\n", "for", "line", "in", "open", "(", "file_path", ",", "'r'", ")", ":", "\n", "            ", "token", "=", "line", ".", "rstrip", "(", "'\\n'", ")", "\n", "self", ".", "add", "(", "token", ")", "\n", "\n", "", "", "def", "get_id", "(", "self", ",", "token", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_id": [[68, 81], ["token.lower"], "methods", ["None"], ["\n", "token", "=", "token", ".", "lower", "(", ")", "if", "self", ".", "lower", "else", "token", "\n", "try", ":", "\n", "            ", "return", "self", ".", "token2id", "[", "token", "]", "\n", "", "except", "KeyError", ":", "\n", "            ", "return", "self", ".", "token2id", "[", "self", ".", "unk_token", "]", "\n", "\n", "", "", "def", "get_token", "(", "self", ",", "idx", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_token": [[82, 94], ["None"], "methods", ["None"], ["\n", "try", ":", "\n", "            ", "return", "self", ".", "id2token", "[", "idx", "]", "\n", "", "except", "KeyError", ":", "\n", "            ", "return", "self", ".", "unk_token", "\n", "\n", "", "", "def", "add", "(", "self", ",", "token", ",", "cnt", "=", "1", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add": [[95, 115], ["token.lower", "len"], "methods", ["None"], ["\n", "token", "=", "token", ".", "lower", "(", ")", "if", "self", ".", "lower", "else", "token", "\n", "if", "token", "in", "self", ".", "token2id", ":", "\n", "            ", "idx", "=", "self", ".", "token2id", "[", "token", "]", "\n", "", "else", ":", "\n", "            ", "idx", "=", "len", "(", "self", ".", "id2token", ")", "\n", "self", ".", "id2token", "[", "idx", "]", "=", "token", "\n", "self", ".", "token2id", "[", "token", "]", "=", "idx", "\n", "", "if", "cnt", ">", "0", ":", "\n", "            ", "if", "token", "in", "self", ".", "token_cnt", ":", "\n", "                ", "self", ".", "token_cnt", "[", "token", "]", "+=", "cnt", "\n", "", "else", ":", "\n", "                ", "self", ".", "token_cnt", "[", "token", "]", "=", "cnt", "\n", "", "", "return", "idx", "\n", "\n", "", "def", "filter_tokens_by_cnt", "(", "self", ",", "min_cnt", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.filter_tokens_by_cnt": [[116, 132], ["vocab.Vocab.add", "vocab.Vocab.add"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add"], ["\n", "filtered_tokens", "=", "[", "token", "for", "token", "in", "self", ".", "token2id", "if", "self", ".", "token_cnt", "[", "token", "]", ">=", "min_cnt", "]", "\n", "# rebuild the token x id map", "\n", "self", ".", "token2id", "=", "{", "}", "\n", "self", ".", "id2token", "=", "{", "}", "\n", "for", "token", "in", "self", ".", "initial_tokens", ":", "\n", "            ", "self", ".", "add", "(", "token", ",", "cnt", "=", "0", ")", "\n", "", "for", "token", "in", "filtered_tokens", ":", "\n", "            ", "self", ".", "add", "(", "token", ",", "cnt", "=", "0", ")", "\n", "\n", "", "", "def", "randomly_init_embeddings", "(", "self", ",", "embed_dim", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.randomly_init_embeddings": [[133, 143], ["numpy.random.rand", "vocab.Vocab.size", "numpy.zeros", "vocab.Vocab.get_id"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_id"], ["\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "embeddings", "=", "np", ".", "random", ".", "rand", "(", "self", ".", "size", "(", ")", ",", "embed_dim", ")", "\n", "for", "token", "in", "[", "self", ".", "pad_token", ",", "self", ".", "unk_token", "]", ":", "\n", "            ", "self", ".", "embeddings", "[", "self", ".", "get_id", "(", "token", ")", "]", "=", "np", ".", "zeros", "(", "[", "self", ".", "embed_dim", "]", ")", "\n", "\n", "", "", "def", "load_pretrained_embeddings", "(", "self", ",", "embedding_path", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.load_pretrained_embeddings": [[144, 174], ["trained_embeddings.keys", "numpy.zeros", "vocab.Vocab.token2id.keys", "open", "vocab.Vocab.add", "vocab.Vocab.add", "line.strip().split", "contents[].decode", "list", "vocab.Vocab.size", "map", "line.strip", "len", "vocab.Vocab.get_id"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.add", "home.repos.pwc.inspect_result.baidu_DuReader.layers.pointer_net.PointerNetDecoder.decode", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.size", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_id"], ["\n", "trained_embeddings", "=", "{", "}", "\n", "with", "open", "(", "embedding_path", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "contents", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "token", "=", "contents", "[", "0", "]", ".", "decode", "(", "'utf8'", ")", "\n", "if", "token", "not", "in", "self", ".", "token2id", ":", "\n", "                    ", "continue", "\n", "", "trained_embeddings", "[", "token", "]", "=", "list", "(", "map", "(", "float", ",", "contents", "[", "1", ":", "]", ")", ")", "\n", "if", "self", ".", "embed_dim", "is", "None", ":", "\n", "                    ", "self", ".", "embed_dim", "=", "len", "(", "contents", ")", "-", "1", "\n", "", "", "", "filtered_tokens", "=", "trained_embeddings", ".", "keys", "(", ")", "\n", "# rebuild the token x id map", "\n", "self", ".", "token2id", "=", "{", "}", "\n", "self", ".", "id2token", "=", "{", "}", "\n", "for", "token", "in", "self", ".", "initial_tokens", ":", "\n", "            ", "self", ".", "add", "(", "token", ",", "cnt", "=", "0", ")", "\n", "", "for", "token", "in", "filtered_tokens", ":", "\n", "            ", "self", ".", "add", "(", "token", ",", "cnt", "=", "0", ")", "\n", "# load embeddings", "\n", "", "self", ".", "embeddings", "=", "np", ".", "zeros", "(", "[", "self", ".", "size", "(", ")", ",", "self", ".", "embed_dim", "]", ")", "\n", "for", "token", "in", "self", ".", "token2id", ".", "keys", "(", ")", ":", "\n", "            ", "if", "token", "in", "trained_embeddings", ":", "\n", "                ", "self", ".", "embeddings", "[", "self", ".", "get_id", "(", "token", ")", "]", "=", "trained_embeddings", "[", "token", "]", "\n", "\n", "", "", "", "def", "convert_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.convert_to_ids": [[175, 185], ["vocab.Vocab.get_id"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_id"], ["\n", "vec", "=", "[", "self", ".", "get_id", "(", "label", ")", "for", "label", "in", "tokens", "]", "\n", "return", "vec", "\n", "\n", "", "def", "recover_from_ids", "(", "self", ",", "ids", ",", "stop_id", "=", "None", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.recover_from_ids": [[186, 201], ["vocab.Vocab.get_token"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.vocab.Vocab.get_token"], ["\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", "+=", "[", "self", ".", "get_token", "(", "i", ")", "]", "\n", "if", "stop_id", "is", "not", "None", "and", "i", "==", "stop_id", ":", "\n", "                ", "break", "\n", "", "", "return", "tokens", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.predict.predict": [[45, 74], ["model.eval", "paddle.no_grad", "paddle.no_grad", "numpy.concatenate", "paddle.to_tensor", "paddle.to_tensor", "paddle.to_tensor", "paddle.to_tensor", "model", "np.concatenate.append", "batch_logit.numpy"], "function", ["None"], ["def", "predict", "(", "model", ",", "data_loader", ")", ":", "\n", "    ", "\"\"\"\n    Predicts the data labels.\n\n    Args:\n        model (obj:`QuestionMatching`): A model to calculate whether the question pair is semantic similar or not.\n        data_loaer (obj:`List(Example)`): The processed data ids of text pair: [query_input_ids, query_token_type_ids, title_input_ids, title_token_type_ids]\n    Returns:\n        results(obj:`List`): cosine similarity of text pairs.\n    \"\"\"", "\n", "batch_logits", "=", "[", "]", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "with", "paddle", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch_data", "in", "data_loader", ":", "\n", "            ", "input_ids", ",", "token_type_ids", "=", "batch_data", "\n", "\n", "input_ids", "=", "paddle", ".", "to_tensor", "(", "input_ids", ")", "\n", "token_type_ids", "=", "paddle", ".", "to_tensor", "(", "token_type_ids", ")", "\n", "\n", "batch_logit", ",", "_", "=", "model", "(", "\n", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "\n", "batch_logits", ".", "append", "(", "batch_logit", ".", "numpy", "(", ")", ")", "\n", "\n", "", "batch_logits", "=", "np", ".", "concatenate", "(", "batch_logits", ",", "axis", "=", "0", ")", "\n", "\n", "return", "batch_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.set_seed": [[59, 64], ["random.seed", "numpy.random.seed", "paddle.seed", "paddle.seed"], "function", ["None"], ["def", "set_seed", "(", "seed", ")", ":", "\n", "    ", "\"\"\"sets random seed\"\"\"", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "paddle", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.evaluate": [[66, 96], ["paddle.no_grad", "paddle.no_grad", "model.eval", "metric.reset", "print", "model.train", "metric.reset", "len", "model", "criterion", "losses.append", "metric.compute", "metric.update", "metric.accumulate", "criterion.numpy", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.run.train"], ["", "@", "paddle", ".", "no_grad", "(", ")", "\n", "def", "evaluate", "(", "model", ",", "criterion", ",", "metric", ",", "data_loader", ")", ":", "\n", "    ", "\"\"\"\n    Given a dataset, it evals model and computes the metric.\n\n    Args:\n        model(obj:`paddle.nn.Layer`): A model to classify texts.\n        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n        criterion(obj:`paddle.nn.Layer`): It can compute the loss.\n        metric(obj:`paddle.metric.Metric`): The evaluation metric.\n    \"\"\"", "\n", "model", ".", "eval", "(", ")", "\n", "metric", ".", "reset", "(", ")", "\n", "losses", "=", "[", "]", "\n", "total_num", "=", "0", "\n", "\n", "for", "batch", "in", "data_loader", ":", "\n", "        ", "input_ids", ",", "token_type_ids", ",", "labels", "=", "batch", "\n", "total_num", "+=", "len", "(", "labels", ")", "\n", "logits", ",", "_", "=", "model", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "do_evaluate", "=", "True", ")", "\n", "loss", "=", "criterion", "(", "logits", ",", "labels", ")", "\n", "losses", ".", "append", "(", "loss", ".", "numpy", "(", ")", ")", "\n", "correct", "=", "metric", ".", "compute", "(", "logits", ",", "labels", ")", "\n", "metric", ".", "update", "(", "correct", ")", "\n", "accu", "=", "metric", ".", "accumulate", "(", ")", "\n", "\n", "", "print", "(", "\"dev_loss: {:.5}, accuracy: {:.5}, total_num:{}\"", ".", "format", "(", "np", ".", "mean", "(", "losses", ")", ",", "accu", ",", "total_num", ")", ")", "\n", "model", ".", "train", "(", ")", "\n", "metric", ".", "reset", "(", ")", "\n", "return", "accu", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.do_train": [[98, 215], ["paddle.set_device", "paddle.set_device", "paddle.distributed.get_rank", "paddle.distributed.get_rank", "train.set_seed", "paddlenlp.datasets.load_dataset", "paddlenlp.datasets.load_dataset", "paddlenlp.transformers.ErnieGramModel.from_pretrained", "paddlenlp.transformers.ErnieGramTokenizer.from_pretrained", "functools.partial", "data.create_dataloader", "data.create_dataloader", "model.QuestionMatching", "paddle.DataParallel", "paddle.DataParallel", "paddlenlp.transformers.LinearDecayWithWarmup", "paddle.optimizer.AdamW", "paddle.optimizer.AdamW", "paddle.nn.loss.CrossEntropyLoss", "paddle.nn.loss.CrossEntropyLoss", "paddle.metric.Accuracy", "paddle.metric.Accuracy", "time.time", "range", "paddle.distributed.get_world_size", "paddle.distributed.get_world_size", "paddle.distributed.init_parallel_env", "paddle.distributed.init_parallel_env", "os.path.isfile", "paddle.load", "paddle.load", "paddle.DataParallel.set_dict", "len", "enumerate", "paddlenlp.data.Tuple", "paddle.DataParallel.named_parameters", "paddle.DataParallel.parameters", "paddle.DataParallel.", "paddle.metric.Accuracy.compute", "paddle.metric.Accuracy.update", "paddle.metric.Accuracy.accumulate", "paddle.nn.loss.CrossEntropyLoss.", "loss.backward", "paddle.optimizer.AdamW.step", "paddlenlp.transformers.LinearDecayWithWarmup.step", "paddle.optimizer.AdamW.clear_grad", "paddlenlp.data.Pad", "paddlenlp.data.Pad", "paddlenlp.data.Stack", "fn", "any", "print", "time.time", "train.evaluate", "os.path.join", "os.path.join", "paddle.save", "paddle.save", "ppnlp.transformers.ErnieGramTokenizer.from_pretrained.save_pretrained", "os.path.exists", "os.makedirs", "paddle.DataParallel.state_dict", "time.time"], "function", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.set_seed", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.data.create_dataloader", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.data.create_dataloader", "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.train.evaluate", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.save", "home.repos.pwc.inspect_result.baidu_DuReader.tensorflow.rc_model.RCModel.save"], ["", "def", "do_train", "(", ")", ":", "\n", "    ", "paddle", ".", "set_device", "(", "args", ".", "device", ")", "\n", "rank", "=", "paddle", ".", "distributed", ".", "get_rank", "(", ")", "\n", "if", "paddle", ".", "distributed", ".", "get_world_size", "(", ")", ">", "1", ":", "\n", "        ", "paddle", ".", "distributed", ".", "init_parallel_env", "(", ")", "\n", "\n", "", "set_seed", "(", "args", ".", "seed", ")", "\n", "\n", "train_ds", "=", "load_dataset", "(", "\n", "read_text_pair", ",", "data_path", "=", "args", ".", "train_set", ",", "is_test", "=", "False", ",", "lazy", "=", "False", ")", "\n", "\n", "dev_ds", "=", "load_dataset", "(", "\n", "read_text_pair", ",", "data_path", "=", "args", ".", "dev_set", ",", "is_test", "=", "False", ",", "lazy", "=", "False", ")", "\n", "\n", "pretrained_model", "=", "ppnlp", ".", "transformers", ".", "ErnieGramModel", ".", "from_pretrained", "(", "\n", "'ernie-gram-zh'", ")", "\n", "tokenizer", "=", "ppnlp", ".", "transformers", ".", "ErnieGramTokenizer", ".", "from_pretrained", "(", "\n", "'ernie-gram-zh'", ")", "\n", "\n", "trans_func", "=", "partial", "(", "\n", "convert_example", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_seq_length", "=", "args", ".", "max_seq_length", ")", "\n", "\n", "batchify_fn", "=", "lambda", "samples", ",", "fn", "=", "Tuple", "(", "\n", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "tokenizer", ".", "pad_token_id", ")", ",", "# text_pair_input", "\n", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "tokenizer", ".", "pad_token_type_id", ")", ",", "# text_pair_segment", "\n", "Stack", "(", "dtype", "=", "\"int64\"", ")", "# label", "\n", ")", ":", "[", "data", "for", "data", "in", "fn", "(", "samples", ")", "]", "\n", "\n", "train_data_loader", "=", "create_dataloader", "(", "\n", "train_ds", ",", "\n", "mode", "=", "'train'", ",", "\n", "batch_size", "=", "args", ".", "train_batch_size", ",", "\n", "batchify_fn", "=", "batchify_fn", ",", "\n", "trans_fn", "=", "trans_func", ")", "\n", "\n", "dev_data_loader", "=", "create_dataloader", "(", "\n", "dev_ds", ",", "\n", "mode", "=", "'dev'", ",", "\n", "batch_size", "=", "args", ".", "eval_batch_size", ",", "\n", "batchify_fn", "=", "batchify_fn", ",", "\n", "trans_fn", "=", "trans_func", ")", "\n", "\n", "model", "=", "QuestionMatching", "(", "pretrained_model", ",", "rdrop_coef", "=", "args", ".", "rdrop_coef", ")", "\n", "\n", "if", "args", ".", "init_from_ckpt", "and", "os", ".", "path", ".", "isfile", "(", "args", ".", "init_from_ckpt", ")", ":", "\n", "        ", "state_dict", "=", "paddle", ".", "load", "(", "args", ".", "init_from_ckpt", ")", "\n", "model", ".", "set_dict", "(", "state_dict", ")", "\n", "\n", "", "model", "=", "paddle", ".", "DataParallel", "(", "model", ")", "\n", "\n", "num_training_steps", "=", "len", "(", "train_data_loader", ")", "*", "args", ".", "epochs", "\n", "\n", "lr_scheduler", "=", "LinearDecayWithWarmup", "(", "args", ".", "learning_rate", ",", "num_training_steps", ",", "\n", "args", ".", "warmup_proportion", ")", "\n", "\n", "# Generate parameter names needed to perform weight decay.", "\n", "# All bias and LayerNorm parameters are excluded.", "\n", "decay_params", "=", "[", "\n", "p", ".", "name", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "[", "\"bias\"", ",", "\"norm\"", "]", ")", "\n", "]", "\n", "optimizer", "=", "paddle", ".", "optimizer", ".", "AdamW", "(", "\n", "learning_rate", "=", "lr_scheduler", ",", "\n", "parameters", "=", "model", ".", "parameters", "(", ")", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", "apply_decay_param_fun", "=", "lambda", "x", ":", "x", "in", "decay_params", ")", "\n", "\n", "criterion", "=", "paddle", ".", "nn", ".", "loss", ".", "CrossEntropyLoss", "(", ")", "\n", "\n", "metric", "=", "paddle", ".", "metric", ".", "Accuracy", "(", ")", "\n", "\n", "global_step", "=", "0", "\n", "best_accuracy", "=", "0.0", "\n", "\n", "tic_train", "=", "time", ".", "time", "(", ")", "\n", "for", "epoch", "in", "range", "(", "1", ",", "args", ".", "epochs", "+", "1", ")", ":", "\n", "        ", "for", "step", ",", "batch", "in", "enumerate", "(", "train_data_loader", ",", "start", "=", "1", ")", ":", "\n", "            ", "input_ids", ",", "token_type_ids", ",", "labels", "=", "batch", "\n", "logits1", ",", "kl_loss", "=", "model", "(", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "correct", "=", "metric", ".", "compute", "(", "logits1", ",", "labels", ")", "\n", "metric", ".", "update", "(", "correct", ")", "\n", "acc", "=", "metric", ".", "accumulate", "(", ")", "\n", "\n", "ce_loss", "=", "criterion", "(", "logits1", ",", "labels", ")", "\n", "if", "kl_loss", ">", "0", ":", "\n", "                ", "loss", "=", "ce_loss", "+", "kl_loss", "*", "args", ".", "rdrop_coef", "\n", "", "else", ":", "\n", "                ", "loss", "=", "ce_loss", "\n", "\n", "", "global_step", "+=", "1", "\n", "if", "global_step", "%", "10", "==", "0", "and", "rank", "==", "0", ":", "\n", "                ", "print", "(", "\n", "\"global step %d, epoch: %d, batch: %d, loss: %.4f, ce_loss: %.4f., kl_loss: %.4f, accu: %.4f, speed: %.2f step/s\"", "\n", "%", "(", "global_step", ",", "epoch", ",", "step", ",", "loss", ",", "ce_loss", ",", "kl_loss", ",", "acc", ",", "\n", "10", "/", "(", "time", ".", "time", "(", ")", "-", "tic_train", ")", ")", ")", "\n", "tic_train", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "lr_scheduler", ".", "step", "(", ")", "\n", "optimizer", ".", "clear_grad", "(", ")", "\n", "\n", "if", "global_step", "%", "args", ".", "eval_step", "==", "0", "and", "rank", "==", "0", ":", "\n", "                ", "accuracy", "=", "evaluate", "(", "model", ",", "criterion", ",", "metric", ",", "dev_data_loader", ")", "\n", "if", "accuracy", ">", "best_accuracy", ":", "\n", "                    ", "save_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "\"model_%d\"", "%", "global_step", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "save_dir", ")", ":", "\n", "                        ", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "", "save_param_path", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'model_state.pdparams'", ")", "\n", "paddle", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "save_param_path", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "save_dir", ")", "\n", "best_accuracy", "=", "accuracy", "\n", "\n", "", "", "if", "global_step", "==", "args", ".", "max_steps", ":", "\n", "                ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.data.create_dataloader": [[21, 42], ["paddle.io.DataLoader", "dataset.map.map", "paddle.io.DistributedBatchSampler", "paddle.io.BatchSampler"], "function", ["None"], ["def", "create_dataloader", "(", "dataset", ",", "\n", "mode", "=", "'train'", ",", "\n", "batch_size", "=", "1", ",", "\n", "batchify_fn", "=", "None", ",", "\n", "trans_fn", "=", "None", ")", ":", "\n", "    ", "if", "trans_fn", ":", "\n", "        ", "dataset", "=", "dataset", ".", "map", "(", "trans_fn", ")", "\n", "\n", "", "shuffle", "=", "True", "if", "mode", "==", "'train'", "else", "False", "\n", "if", "mode", "==", "'train'", ":", "\n", "        ", "batch_sampler", "=", "paddle", ".", "io", ".", "DistributedBatchSampler", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "shuffle", ")", "\n", "", "else", ":", "\n", "        ", "batch_sampler", "=", "paddle", ".", "io", ".", "BatchSampler", "(", "\n", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "shuffle", ")", "\n", "\n", "", "return", "paddle", ".", "io", ".", "DataLoader", "(", "\n", "dataset", "=", "dataset", ",", "\n", "batch_sampler", "=", "batch_sampler", ",", "\n", "collate_fn", "=", "batchify_fn", ",", "\n", "return_list", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.data.read_text_pair": [[44, 57], ["open", "line.rstrip().split", "line.rstrip", "len", "len"], "function", ["None"], ["", "def", "read_text_pair", "(", "data_path", ",", "is_test", "=", "False", ")", ":", "\n", "    ", "\"\"\"Reads data.\"\"\"", "\n", "with", "open", "(", "data_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "data", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "is_test", "==", "False", ":", "\n", "                ", "if", "len", "(", "data", ")", "!=", "3", ":", "\n", "                    ", "continue", "\n", "", "yield", "{", "'query1'", ":", "data", "[", "0", "]", ",", "'query2'", ":", "data", "[", "1", "]", ",", "'label'", ":", "data", "[", "2", "]", "}", "\n", "", "else", ":", "\n", "                ", "if", "len", "(", "data", ")", "!=", "2", ":", "\n", "                    ", "continue", "\n", "", "yield", "{", "'query1'", ":", "data", "[", "0", "]", ",", "'query2'", ":", "data", "[", "1", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.data.convert_example": [[60, 75], ["tokenizer", "numpy.array"], "function", ["None"], ["", "", "", "", "def", "convert_example", "(", "example", ",", "tokenizer", ",", "max_seq_length", "=", "512", ",", "is_test", "=", "False", ")", ":", "\n", "\n", "    ", "query", ",", "title", "=", "example", "[", "\"query1\"", "]", ",", "example", "[", "\"query2\"", "]", "\n", "\n", "encoded_inputs", "=", "tokenizer", "(", "\n", "text", "=", "query", ",", "text_pair", "=", "title", ",", "max_seq_len", "=", "max_seq_length", ")", "\n", "\n", "input_ids", "=", "encoded_inputs", "[", "\"input_ids\"", "]", "\n", "token_type_ids", "=", "encoded_inputs", "[", "\"token_type_ids\"", "]", "\n", "\n", "if", "not", "is_test", ":", "\n", "        ", "label", "=", "np", ".", "array", "(", "[", "example", "[", "\"label\"", "]", "]", ",", "dtype", "=", "\"int64\"", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "label", "\n", "", "else", ":", "\n", "        ", "return", "input_ids", ",", "token_type_ids", "", "", "", ""]], "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.__init__": [[23, 32], ["paddle.Layer.__init__", "paddle.Dropout", "paddle.Dropout", "paddle.Dropout", "paddle.Linear", "paddle.Linear", "paddle.Linear", "paddlenlp.losses.RDropLoss"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pretrained_model", ",", "dropout", "=", "None", ",", "rdrop_coef", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ptm", "=", "pretrained_model", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", "if", "dropout", "is", "not", "None", "else", "0.1", ")", "\n", "\n", "# num_labels = 2 (similar or dissimilar)", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "self", ".", "ptm", ".", "config", "[", "\"hidden_size\"", "]", ",", "2", ")", "\n", "self", ".", "rdrop_coef", "=", "rdrop_coef", "\n", "self", ".", "rdrop_loss", "=", "ppnlp", ".", "losses", ".", "RDropLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baidu_DuReader.DuQM.model.QuestionMatching.forward": [[33, 57], ["model.QuestionMatching.ptm", "model.QuestionMatching.dropout", "model.QuestionMatching.classifier", "model.QuestionMatching.ptm", "model.QuestionMatching.dropout", "model.QuestionMatching.classifier", "model.QuestionMatching.rdrop_loss"], "methods", ["home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout", "home.repos.pwc.inspect_result.baidu_DuReader.paddle.rc_model.dropout"], ["", "def", "forward", "(", "self", ",", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "do_evaluate", "=", "False", ")", ":", "\n", "\n", "        ", "_", ",", "cls_embedding1", "=", "self", ".", "ptm", "(", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "\n", "attention_mask", ")", "\n", "cls_embedding1", "=", "self", ".", "dropout", "(", "cls_embedding1", ")", "\n", "logits1", "=", "self", ".", "classifier", "(", "cls_embedding1", ")", "\n", "\n", "# For more information about R-drop please refer to this paper: https://arxiv.org/abs/2106.14448", "\n", "# Original implementation please refer to this code: https://github.com/dropreg/R-Drop", "\n", "if", "self", ".", "rdrop_coef", ">", "0", "and", "not", "do_evaluate", ":", "\n", "            ", "_", ",", "cls_embedding2", "=", "self", ".", "ptm", "(", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "\n", "attention_mask", ")", "\n", "cls_embedding2", "=", "self", ".", "dropout", "(", "cls_embedding2", ")", "\n", "logits2", "=", "self", ".", "classifier", "(", "cls_embedding2", ")", "\n", "kl_loss", "=", "self", ".", "rdrop_loss", "(", "logits1", ",", "logits2", ")", "\n", "", "else", ":", "\n", "            ", "kl_loss", "=", "0.0", "\n", "\n", "", "return", "logits1", ",", "kl_loss", "\n", "", "", ""]]}