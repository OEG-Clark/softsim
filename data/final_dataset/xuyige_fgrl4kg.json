{"home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.data_preprocess_debug.sort_keyphrases_by_their_order_of_occurence_debug": [[4, 11], ["integrated_data_preprocess.get_tokens", "print", "source.lower", "integrated_data_preprocess.get_tokens", "integrated_data_preprocess.sort_keyphrases_by_their_order_of_occurence", "keyphrase.lower"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.sort_keyphrases_by_their_order_of_occurence"], ["def", "sort_keyphrases_by_their_order_of_occurence_debug", "(", ")", ":", "\n", "    ", "keyphrase_list", "=", "[", "\"svm\"", ",", "\"we\"", ",", "\"support vector machine\"", ",", "\"in this\"", ",", "\"pca\"", ",", "\"abcdefgh\"", ",", "\"\"", "]", "\n", "source", "=", "\"In this paper, we support propose a support vector machine to classify.\"", "\n", "src_tokens", "=", "get_tokens", "(", "source", ".", "lower", "(", ")", ",", "True", ",", "False", ")", "\n", "keyphrase_token_2dlist", "=", "[", "get_tokens", "(", "keyphrase", ".", "lower", "(", ")", ",", "True", ",", "False", ")", "for", "keyphrase", "in", "keyphrase_list", "]", "\n", "print", "(", "sort_keyphrases_by_their_order_of_occurence", "(", "keyphrase_list", ",", "src_tokens", ",", "keyphrase_token_2dlist", ",", "separate_present_absent", "=", "True", ")", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.data_preprocess_debug.check_present_idx_debug": [[13, 24], ["integrated_data_preprocess.get_tokens", "print", "len", "print", "print", "integrated_data_preprocess.get_tokens", "source.lower", "integrated_data_preprocess.batch_check_present_idx", "keyphrase.lower"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.batch_check_present_idx"], ["", "def", "check_present_idx_debug", "(", ")", ":", "\n", "    ", "keyphrase_list", "=", "[", "\"svm\"", ",", "\"support\"", ",", "\"support vector machine\"", ",", "\".\"", ",", "\"in this\"", ",", "\"pca\"", ",", "\"suppor\"", ",", "\"\"", "]", "\n", "# keyphrase_list = [\"svm\", \"we\", \"support vector machine\", \"in this\", \"pca\", \"abcdefgh\", \"\"]", "\n", "source", "=", "\"In this paper, we support propose a support vector machine to classify.\"", "\n", "keyphrase_list_tokenized", "=", "[", "get_tokens", "(", "keyphrase", ".", "lower", "(", ")", ",", "True", ",", "False", ")", "for", "keyphrase", "in", "keyphrase_list", "]", "\n", "source_tokens", "=", "get_tokens", "(", "source", ".", "lower", "(", ")", ",", "True", ",", "False", ")", "\n", "print", "(", "source_tokens", ")", "\n", "src_len", "=", "len", "(", "source_tokens", ")", "\n", "print", "(", "src_len", ")", "\n", "print", "(", "batch_check_present_idx", "(", "source_tokens", ",", "keyphrase_list_tokenized", ")", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.data_preprocess_debug.find_variations_from_wiki_debug": [[26, 33], ["integrated_data_preprocess.get_tokens", "source.lower", "print", "integrated_data_preprocess.find_variations_from_wiki"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.find_variations_from_wiki"], ["", "def", "find_variations_from_wiki_debug", "(", ")", ":", "\n", "    ", "keyphrase_list", "=", "[", "\"svm\"", ",", "\"support vector machine\"", ",", "\"principal component analysis\"", ",", "\"abcdefg\"", ",", "\"pca\"", "]", "\n", "source", "=", "\"In this paper, we propose support vector machines to classify.\"", "\n", "source_tokens", "=", "get_tokens", "(", "source", ".", "lower", "(", ")", ",", "True", ",", "False", ")", "\n", "for", "keyphrase", "in", "keyphrase_list", ":", "\n", "        ", "print", "(", "find_variations_from_wiki", "(", "keyphrase", ",", "source_tokens", ",", "True", ")", ")", "\n", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.data_preprocess_debug.find_variations_debug": [[35, 42], ["integrated_data_preprocess.get_tokens", "source.lower", "print", "integrated_data_preprocess.find_variations"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.find_variations"], ["", "def", "find_variations_debug", "(", ")", ":", "\n", "    ", "keyphrase_list", "=", "[", "\"svm\"", ",", "\"support vector machine\"", ",", "\"principal component analysis\"", ",", "\"abcdefg\"", ",", "\"pca\"", ",", "\"principal component analysis (pca)\"", ",", "\"apple (a b c d e f g h)\"", "]", "\n", "source", "=", "\"In this paper, we propose support vector machines to classify.\"", "\n", "source_tokens", "=", "get_tokens", "(", "source", ".", "lower", "(", ")", ",", "True", ",", "False", ")", "\n", "for", "keyphrase", "in", "keyphrase_list", ":", "\n", "        ", "print", "(", "find_variations", "(", "keyphrase", ",", "source_tokens", ",", "fine_grad", "=", "True", ",", "limit_num", "=", "True", ")", ")", "\n", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.data_preprocess_debug.process_keyphrase_debug_with_variation": [[44, 61], ["integrated_data_preprocess.get_tokens", "integrated_data_preprocess.process_keyphrase", "print", "source.lower", "print"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.process_keyphrase"], ["", "def", "process_keyphrase_debug_with_variation", "(", ")", ":", "\n", "    ", "keyphrase_list", "=", "[", "\"svm\"", ",", "\"support vector machine\"", ",", "\"principal component analysis\"", ",", "\"abcdefg\"", ",", "\"pca\"", ",", "\"principal component analysis (pca)\"", ",", "\"apple (a b c d e f g h)\"", ",", "\"\"", "]", "\n", "keyphrase_str", "=", "';'", ".", "join", "(", "keyphrase_list", ")", "\n", "source", "=", "\"In this paper, we propose support vector machines to classify.\"", "\n", "source_tokens", "=", "get_tokens", "(", "source", ".", "lower", "(", ")", ",", "True", ",", "False", ")", "\n", "variate_keyphrase_list", "=", "process_keyphrase", "(", "keyphrase_str", ",", "source_tokens", ",", "variations", "=", "True", ",", "fine_grad", "=", "True", ",", "limit_num", "=", "True", ")", "\n", "print", "(", "variate_keyphrase_list", ")", "\n", "for", "keyphrase", "in", "variate_keyphrase_list", ":", "\n", "        ", "print", "(", "keyphrase", ")", "\n", "#print(process_keyphrase(keyphrase_str, source_tokens, variations=True, fine_grad=True, limit_num=True))", "\n", "", "\"\"\"\n    variate_keyphrase_str = process_keyphrase(keyphrase_str, source_tokens, variations=True, fine_grad=True, limit_num=True)\n    variate_keyphrase_list = variate_keyphrase_str.split(';')\n    for keyphrase in variate_keyphrase_list:\n        print(keyphrase)\n    \"\"\"", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.data_preprocess_debug.process_keyphrase_debug_sort_keyphrases": [[63, 80], ["integrated_data_preprocess.get_tokens", "integrated_data_preprocess.process_keyphrase", "print", "source.lower", "print"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.process_keyphrase"], ["", "def", "process_keyphrase_debug_sort_keyphrases", "(", ")", ":", "\n", "    ", "keyphrase_list", "=", "[", "\"svm\"", ",", "\"we\"", ",", "\"support vector machine\"", ",", "\"in this\"", ",", "\"pca\"", ",", "\"abcdefgh\"", ",", "\"\"", "]", "\n", "keyphrase_str", "=", "';'", ".", "join", "(", "keyphrase_list", ")", "\n", "source", "=", "\"In this paper, we propose support vector machines to classify.\"", "\n", "source_tokens", "=", "get_tokens", "(", "source", ".", "lower", "(", ")", ",", "True", ",", "False", ")", "\n", "variate_keyphrase_list", "=", "process_keyphrase", "(", "keyphrase_str", ",", "source_tokens", ",", "variations", "=", "False", ",", "fine_grad", "=", "True", ",", "limit_num", "=", "True", ",", "sort_keyphrases", "=", "True", ")", "\n", "print", "(", "variate_keyphrase_list", ")", "\n", "for", "keyphrase", "in", "variate_keyphrase_list", ":", "\n", "        ", "print", "(", "keyphrase", ")", "\n", "#print(process_keyphrase(keyphrase_str, source_tokens, variations=True, fine_grad=True, limit_num=True))", "\n", "", "\"\"\"\n    variate_keyphrase_str = process_keyphrase(keyphrase_str, source_tokens, variations=True, fine_grad=True, limit_num=True)\n    variate_keyphrase_list = variate_keyphrase_str.split(';')\n    for keyphrase in variate_keyphrase_list:\n        print(keyphrase)\n    \"\"\"", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.config.init_logging": [[6, 30], ["logging.Formatter", "print", "print", "logging.FileHandler", "logging.FileHandler.setFormatter", "logging.FileHandler.setLevel", "logging.getLogger", "logging.getLogger.addHandler", "logging.getLogger.setLevel", "os.path.exists", "os.makedirs", "logging.StreamHandler", "logging.StreamHandler.setFormatter", "logging.StreamHandler.setLevel", "logging.getLogger.addHandler", "log_file.rfind", "log_file.rfind", "log_file.rfind"], "function", ["None"], ["def", "init_logging", "(", "log_file", ",", "stdout", "=", "False", ")", ":", "\n", "    ", "formatter", "=", "logging", ".", "Formatter", "(", "'%(asctime)s [%(levelname)s] %(module)s: %(message)s'", ",", "\n", "datefmt", "=", "'%m/%d/%Y %H:%M:%S'", ")", "\n", "\n", "print", "(", "'Making log output file: %s'", "%", "log_file", ")", "\n", "print", "(", "log_file", "[", ":", "log_file", ".", "rfind", "(", "os", ".", "sep", ")", "]", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "log_file", "[", ":", "log_file", ".", "rfind", "(", "os", ".", "sep", ")", "]", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "log_file", "[", ":", "log_file", ".", "rfind", "(", "os", ".", "sep", ")", "]", ")", "\n", "\n", "", "fh", "=", "logging", ".", "FileHandler", "(", "log_file", ")", "\n", "fh", ".", "setFormatter", "(", "formatter", ")", "\n", "fh", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "logger", ".", "addHandler", "(", "fh", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "\n", "if", "stdout", ":", "\n", "        ", "ch", "=", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "\n", "ch", ".", "setFormatter", "(", "formatter", ")", "\n", "ch", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "logger", ".", "addHandler", "(", "ch", ")", "\n", "\n", "", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.config.model_opts": [[31, 154], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "def", "model_opts", "(", "parser", ")", ":", "\n", "    ", "\"\"\"\n    These options are passed to the construction of the model.\n    Be careful with these as they will be used during translation.\n    \"\"\"", "\n", "# Embedding Options", "\n", "parser", ".", "add_argument", "(", "'-word_vec_size'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'Word embedding for both.'", ")", "\n", "\n", "#parser.add_argument('-position_encoding', action='store_true',", "\n", "#                    help='Use a sin to mark relative words positions.')", "\n", "parser", ".", "add_argument", "(", "'-share_embeddings'", ",", "default", "=", "True", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"\"\"Share the word embeddings between encoder\n                         and decoder.\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-use_target_encoder'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use target decoder\"", ")", "\n", "\n", "# RNN Options", "\n", "parser", ".", "add_argument", "(", "'-encoder_type'", ",", "type", "=", "str", ",", "default", "=", "'rnn'", ",", "\n", "choices", "=", "[", "'rnn'", ",", "'brnn'", ",", "'mean'", ",", "'transformer'", ",", "'cnn'", "]", ",", "\n", "help", "=", "\"\"\"Type of encoder layer to use.\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-decoder_type'", ",", "type", "=", "str", ",", "default", "=", "'rnn'", ",", "\n", "choices", "=", "[", "'rnn'", ",", "'transformer'", ",", "'cnn'", "]", ",", "\n", "help", "=", "'Type of decoder layer to use.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-enc_layers'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Number of layers in the encoder'", ")", "\n", "parser", ".", "add_argument", "(", "'-dec_layers'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Number of layers in the decoder'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-encoder_size'", ",", "type", "=", "int", ",", "default", "=", "150", ",", "\n", "help", "=", "'Size of encoder hidden states'", ")", "\n", "parser", ".", "add_argument", "(", "'-decoder_size'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "\n", "help", "=", "'Size of decoder hidden states'", ")", "\n", "parser", ".", "add_argument", "(", "'-target_encoder_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "\n", "help", "=", "'Size of target encoder hidden states'", ")", "\n", "parser", ".", "add_argument", "(", "'-source_representation_queue_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "'Size of queue for storing the encoder representation for training the target encoder'", ")", "\n", "parser", ".", "add_argument", "(", "'-source_representation_sample_size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'Sample size of encoder representation for training the target encoder.'", ")", "\n", "parser", ".", "add_argument", "(", "'-dropout'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "\"Dropout probability; applied in LSTM stacks.\"", ")", "\n", "# parser.add_argument('-input_feed', type=int, default=1,", "\n", "#                     help=\"\"\"Feed the context vector at each time step as", "\n", "#                     additional input (via concatenation with the word", "\n", "#                     embeddings) to the decoder.\"\"\")", "\n", "\n", "#parser.add_argument('-rnn_type', type=str, default='GRU',", "\n", "#                    choices=['LSTM', 'GRU'],", "\n", "#                    help=\"\"\"The gate type to use in the RNNs\"\"\")", "\n", "# parser.add_argument('-residual',   action=\"store_true\",", "\n", "#                     help=\"Add residual connections between RNN layers.\")", "\n", "\n", "#parser.add_argument('-input_feeding', action=\"store_true\",", "\n", "#                    help=\"Apply input feeding or not. Feed the updated hidden vector (after attention)\"", "\n", "#                         \"as new hidden vector to the decoder (Luong et al. 2015). \"", "\n", "#                         \"Feed the context vector at each time step  after normal attention\"", "\n", "#                         \"as additional input (via concatenation with the word\"", "\n", "#                         \"embeddings) to the decoder.\")", "\n", "\n", "parser", ".", "add_argument", "(", "'-bidirectional'", ",", "default", "=", "True", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"whether the encoder is bidirectional\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-bridge'", ",", "type", "=", "str", ",", "default", "=", "'copy'", ",", "\n", "choices", "=", "[", "'copy'", ",", "'dense'", ",", "'dense_nonlinear'", ",", "'none'", "]", ",", "\n", "help", "=", "\"An additional layer between the encoder and the decoder\"", ")", "\n", "\n", "# Attention options", "\n", "parser", ".", "add_argument", "(", "'-attn_mode'", ",", "type", "=", "str", ",", "default", "=", "'concat'", ",", "\n", "choices", "=", "[", "'general'", ",", "'concat'", "]", ",", "\n", "help", "=", "\"\"\"The attention type to use:\n                       dot or general (Luong) or concat (Bahdanau)\"\"\"", ")", "\n", "#parser.add_argument('-attention_mode', type=str, default='concat',", "\n", "#                    choices=['dot', 'general', 'concat'],", "\n", "#                    help=\"\"\"The attention type to use:", "\n", "#                    dot or general (Luong) or concat (Bahdanau)\"\"\")", "\n", "\n", "# Genenerator and loss options.", "\n", "parser", ".", "add_argument", "(", "'-copy_attention'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Train a copy model.'", ")", "\n", "\n", "#parser.add_argument('-copy_mode', type=str, default='concat',", "\n", "#                    choices=['dot', 'general', 'concat'],", "\n", "#                    help=\"\"\"The attention type to use: dot or general (Luong) or concat (Bahdanau)\"\"\")", "\n", "\n", "#parser.add_argument('-copy_input_feeding', action=\"store_true\",", "\n", "#                    help=\"Feed the context vector at each time step after copy attention\"", "\n", "#                         \"as additional input (via concatenation with the word\"", "\n", "#                         \"embeddings) to the decoder.\")", "\n", "\n", "#parser.add_argument('-reuse_copy_attn', action=\"store_true\",", "\n", "#                   help=\"Reuse standard attention for copy (see See et al.)\")", "\n", "\n", "#parser.add_argument('-copy_gate', action=\"store_true\",", "\n", "#                    help=\"A gate controling the flow from generative model and copy model (see See et al.)\")", "\n", "\n", "parser", ".", "add_argument", "(", "'-coverage_attn'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Train a coverage attention layer.'", ")", "\n", "parser", ".", "add_argument", "(", "'-review_attn'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Train a review attention layer'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-lambda_coverage'", ",", "type", "=", "float", ",", "default", "=", "1", ",", "\n", "help", "=", "'Lambda value for coverage by See et al.'", ")", "\n", "parser", ".", "add_argument", "(", "'-coverage_loss'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'whether to include coverage loss'", ")", "\n", "parser", ".", "add_argument", "(", "'-orthogonal_loss'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'whether to include orthogonal loss'", ")", "\n", "parser", ".", "add_argument", "(", "'-lambda_orthogonal'", ",", "type", "=", "float", ",", "default", "=", "0.03", ",", "\n", "help", "=", "'Lambda value for the orthogonal loss by Yuan et al.'", ")", "\n", "parser", ".", "add_argument", "(", "'-lambda_target_encoder'", ",", "type", "=", "float", ",", "default", "=", "0.03", ",", "\n", "help", "=", "'Lambda value for the target encoder loss by Yuan et al.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-separate_present_absent'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'whether to separate present keyphrase predictions and absnet keyphrase predictions as two sub-tasks'", ")", "\n", "parser", ".", "add_argument", "(", "'-manager_mode'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "choices", "=", "[", "1", "]", ",", "\n", "help", "=", "'Only effective in separate_present_absent. 1: two trainable vectors as the goal vectors;'", ")", "\n", "parser", ".", "add_argument", "(", "'-goal_vector_size'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "\n", "help", "=", "'size of goal vector'", ")", "\n", "parser", ".", "add_argument", "(", "'-goal_vector_mode'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", ",", "2", "]", ",", "\n", "help", "=", "'Only effective in separate_present_absent. 0: no goal vector; 1: goal vector act as an extra input to the decoder; 2: goal vector act as an extra input to p_gen'", ")", "\n", "parser", ".", "add_argument", "(", "'-title_guided'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'whether to use title-guided encoder'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.config.vocab_opts": [[168, 181], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "def", "vocab_opts", "(", "parser", ")", ":", "\n", "# Dictionary Options", "\n", "    ", "parser", ".", "add_argument", "(", "'-vocab_size'", ",", "type", "=", "int", ",", "default", "=", "50002", ",", "\n", "help", "=", "\"Size of the source vocabulary\"", ")", "\n", "# for copy model", "\n", "parser", ".", "add_argument", "(", "'-max_unk_words'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "\n", "help", "=", "\"Maximum number of unknown words the model supports (mainly for masking in loss)\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-words_min_frequency'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "\n", "# Options most relevant to summarization", "\n", "parser", ".", "add_argument", "(", "'-dynamic_dict'", ",", "default", "=", "True", ",", "\n", "action", "=", "'store_true'", ",", "help", "=", "\"Create dynamic dictionaries (for copy)\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.config.train_opts": [[182, 410], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "time.strftime", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "time.localtime", "time.time"], "function", ["None"], ["", "def", "train_opts", "(", "parser", ")", ":", "\n", "# Model loading/saving options", "\n", "    ", "parser", ".", "add_argument", "(", "'-data'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"\"\"Path prefix to the \"train.one2one.pt\" and\n                        \"train.one2many.pt\" file path from preprocess.py\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-vocab'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"\"\"Path prefix to the \"vocab.pt\"\n                        file path from preprocess.py\"\"\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-custom_data_filename_suffix'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-custom_vocab_filename_suffix'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-vocab_filename_suffix'", ",", "default", "=", "''", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-data_filename_suffix'", ",", "default", "=", "''", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-save_model'", ",", "default", "=", "'model'", ",", "\n", "help", "=", "\"\"\"Model filename (the model will be saved as\n                        <save_model>_epochN_PPL.pt where PPL is the\n                        validation perplexity\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-train_from'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "\n", "help", "=", "\"\"\"If training from a checkpoint then this is the\n                        path to the pretrained model's state_dict.\"\"\"", ")", "\n", "# GPU", "\n", "parser", ".", "add_argument", "(", "'-gpuid'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Use CUDA on the selected device.\"", ")", "\n", "#parser.add_argument('-gpuid', default=[0], nargs='+', type=int,", "\n", "#                    help=\"Use CUDA on the listed devices.\")", "\n", "parser", ".", "add_argument", "(", "'-seed'", ",", "type", "=", "int", ",", "default", "=", "9527", ",", "\n", "help", "=", "\"\"\"Random seed used for the experiments\n                        reproducibility.\"\"\"", ")", "\n", "\n", "# Init options", "\n", "parser", ".", "add_argument", "(", "'-epochs'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "'Number of training epochs'", ")", "\n", "parser", ".", "add_argument", "(", "'-start_epoch'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'The epoch from which to start'", ")", "\n", "parser", ".", "add_argument", "(", "'-param_init'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "\"\"\"Parameters are initialized over uniform distribution\n                        with support (-param_init, param_init).\n                        Use 0 to not use initialization\"\"\"", ")", "\n", "\n", "# Pretrained word vectors", "\n", "parser", ".", "add_argument", "(", "'-pre_word_vecs_enc'", ",", "\n", "help", "=", "\"\"\"If a valid path is specified, then this will load\n                        pretrained word embeddings on the encoder side.\n                        See README for specific formatting instructions.\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-pre_word_vecs_dec'", ",", "\n", "help", "=", "\"\"\"If a valid path is specified, then this will load\n                        pretrained word embeddings on the decoder side.\n                        See README for specific formatting instructions.\"\"\"", ")", "\n", "# Fixed word vectors", "\n", "parser", ".", "add_argument", "(", "'-fix_word_vecs_enc'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Fix word embeddings on the encoder side.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-fix_word_vecs_dec'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Fix word embeddings on the encoder side.\"", ")", "\n", "\n", "# Optimization options", "\n", "parser", ".", "add_argument", "(", "'-batch_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "\n", "help", "=", "'Maximum batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'-batch_workers'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "'Number of workers for generating batches'", ")", "\n", "parser", ".", "add_argument", "(", "'-optim'", ",", "default", "=", "'adam'", ",", "\n", "choices", "=", "[", "'sgd'", ",", "'adagrad'", ",", "'adadelta'", ",", "'adam'", "]", ",", "\n", "help", "=", "\"\"\"Optimization method.\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-max_grad_norm'", ",", "type", "=", "float", ",", "default", "=", "1", ",", "\n", "help", "=", "\"\"\"If the norm of the gradient vector exceeds this,\n                        renormalize it to have the norm equal to\n                        max_grad_norm\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-truncated_decoder'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"Truncated bptt.\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-loss_normalization'", ",", "default", "=", "\"tokens\"", ",", "choices", "=", "[", "'tokens'", ",", "'batches'", "]", ",", "\n", "help", "=", "\"Normalize the cross-entropy loss by the number of tokens or batch size\"", ")", "\n", "\n", "# Learning options", "\n", "parser", ".", "add_argument", "(", "'-train_ml'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'Train with Maximum Likelihood or not'", ")", "\n", "parser", ".", "add_argument", "(", "'-train_rl'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'Train with Reinforcement Learning or not'", ")", "\n", "\n", "# Reinforcement Learning options", "\n", "#parser.add_argument('-rl_method', default=0, type=int,", "\n", "#                    help=\"\"\"0: ori, 1: running average as baseline\"\"\")", "\n", "parser", ".", "add_argument", "(", "'-max_sample_length'", ",", "default", "=", "6", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The max length of sequence that can be sampled by the model\"", ")", "\n", "parser", ".", "add_argument", "(", "'-max_length'", ",", "type", "=", "int", ",", "default", "=", "6", ",", "\n", "help", "=", "'Maximum prediction length.'", ")", "\n", "parser", ".", "add_argument", "(", "'-topk'", ",", "type", "=", "str", ",", "default", "=", "'M'", ",", "\n", "help", "=", "'The only  pick the top k predictions in reward.'", ")", "\n", "parser", ".", "add_argument", "(", "'-reward_type'", ",", "default", "=", "'0'", ",", "type", "=", "int", ",", "\n", "# choices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],", "\n", "help", "=", "\"\"\"Type of reward. 0: f1, 1: recall, 2: ndcg, 3: accuracy, 4: alpha-ndcg, 5: alpha-dcg, 6: AP, 7: F1 penalize duplicate\n                        9: phrase reward   edit distance\uff0c 10: phrase reward    token f1, 11: phrase reward token f1 + edit distance\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-match_type'", ",", "default", "=", "'exact'", ",", "\n", "choices", "=", "[", "'exact'", ",", "'sub'", "]", ",", "\n", "help", "=", "\"\"\"Either exact matching or substring matching.\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-pretrained_model'", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"The path of pretrained model. Only effective in RL\"", ")", "\n", "parser", ".", "add_argument", "(", "'-reward_shaping'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use reward shaping in RL training\"", ")", "\n", "parser", ".", "add_argument", "(", "'-baseline'", ",", "default", "=", "\"self\"", ",", "choices", "=", "[", "\"none\"", ",", "\"self\"", "]", ",", "\n", "help", "=", "\"The baseline in RL training. none: no baseline; self: use greedy decoding as baseline\"", ")", "\n", "parser", ".", "add_argument", "(", "'-mc_rollouts'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use Monte Carlo rollouts to estimate q value. Not support yet.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-num_rollouts'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "\n", "help", "=", "\"The number of Monte Carlo rollouts. Only effective when mc_rollouts is True. Not supported yet\"", ")", "\n", "\n", "# One2many options", "\n", "parser", ".", "add_argument", "(", "'-delimiter_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "'If type is 0, use <sep> to separate keyphrases. If type is 1, use <eos> to separate keyphrases'", ")", "\n", "parser", ".", "add_argument", "(", "'-one2many'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'If true, it will not split a sample into multiple src-keyphrase pairs'", ")", "\n", "parser", ".", "add_argument", "(", "'-one2many_mode'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "1", ",", "2", ",", "3", "]", ",", "\n", "help", "=", "'Only effective when one2many=True. 1: concatenated the keyphrases by <sep>; 2: reset the inital state and input after each keyphrase; 3: reset the input after each keyphrase'", ")", "\n", "parser", ".", "add_argument", "(", "'-num_predictions'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Control the number of predictions when one2many_mode=2. If you set the one2many_mode to 1, the number of predictions should also be 1.'", ")", "\n", "\n", "#parser.add_argument('-loss_scale', type=float, default=0.5,", "\n", "#                    help='A scaling factor to merge the loss of ML and RL parts: L_mixed = \u03b3 * L_rl + (1 \u2212 \u03b3) * L_ml'", "\n", "#                         'The \u03b3 used by Metamind is 0.9984 in \"A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION\"'", "\n", "#                         'The \u03b1 used by Google is 0.017 in \"Google Translation\": O_Mixed(\u03b8) = \u03b1 \u2217 O_ML(\u03b8) + O_RL(\u03b8)'", "\n", "#                     )", "\n", "\n", "#parser.add_argument('-rl_start_epoch', default=2, type=int,", "\n", "#                    help=\"\"\"from which epoch rl training starts\"\"\")", "\n", "parser", ".", "add_argument", "(", "'-init_perturb_std'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Init std of gaussian perturbation vector to the hidden state of the GRU after generated each a keyphrase\"", ")", "\n", "parser", ".", "add_argument", "(", "'-final_perturb_std'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Final std of gaussian perturbation vector to the hidden state of the GRU after generated each a keyphrase. Only effective when perturb_decay=1\"", ")", "\n", "parser", ".", "add_argument", "(", "'-perturb_decay_mode'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "choices", "=", "[", "0", ",", "1", ",", "2", "]", ",", "\n", "help", "=", "'Specify how the std of perturbation vector decay. 0: no decay, 1: exponential decay, 2: iteration-wise decay'", ")", "\n", "parser", ".", "add_argument", "(", "'-perturb_decay_factor'", ",", "type", "=", "float", ",", "default", "=", "0.0001", ",", "\n", "help", "=", "\"Specify the decay factor, only effective when perturb_decay=1 or 2\"", ")", "\n", "parser", ".", "add_argument", "(", "'-perturb_baseline'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Whether to perturb the baseline or not\"", ")", "\n", "#parser.add_argument('-perturb_decay_along_phrases', action=\"store_true\", default=False,", "\n", "#                    help=\"Decay the perturbations along the predicted keyphrases, std=std/num_of_preds\")", "\n", "parser", ".", "add_argument", "(", "'-regularization_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", ",", "2", "]", ",", "\n", "help", "=", "'0: no regularization, 1: percentage of unique keyphrases, 2: entropy'", ")", "\n", "parser", ".", "add_argument", "(", "'-regularization_factor'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "\"Factor of regularization\"", ")", "\n", "parser", ".", "add_argument", "(", "'-replace_unk'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Replace the unk token with the token of highest attention score.'", ")", "\n", "parser", ".", "add_argument", "(", "'-remove_src_eos'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Remove the eos token at the end of src text'", ")", "\n", "\n", "# GPU", "\n", "\n", "# Teacher Forcing and Scheduled Sampling", "\n", "parser", ".", "add_argument", "(", "'-must_teacher_forcing'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Apply must_teacher_forcing or not\"", ")", "\n", "parser", ".", "add_argument", "(", "'-teacher_forcing_ratio'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"The ratio to apply teaching forcing ratio (default 0)\"", ")", "\n", "parser", ".", "add_argument", "(", "'-scheduled_sampling'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Apply scheduled sampling or not\"", ")", "\n", "parser", ".", "add_argument", "(", "'-scheduled_sampling_batches'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "\"The maximum number of batches to apply scheduled sampling\"", ")", "\n", "\n", "# learning rate", "\n", "parser", ".", "add_argument", "(", "'-learning_rate'", ",", "type", "=", "float", ",", "default", "=", "0.001", ",", "\n", "help", "=", "\"\"\"Starting learning rate.\n                        Recommended settings: sgd = 1, adagrad = 0.1,\n                        adadelta = 1, adam = 0.001\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-learning_rate_rl'", ",", "type", "=", "float", ",", "default", "=", "0.00005", ",", "\n", "help", "=", "\"\"\"Starting learning rate for Reinforcement Learning.\n                        Recommended settings: sgd = 1, adagrad = 0.1,\n                        adadelta = 1, adam = 0.001\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-learning_rate_decay_rl'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "\"\"\"A flag to use learning rate decay in rl training\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-learning_rate_decay'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n", "help", "=", "\"\"\"If update_learning_rate, decay learning rate by\n                        this much if (i) perplexity does not decrease on the\n                        validation set or (ii) epoch has gone past\n                        start_decay_at\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-start_decay_at'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "\"\"\"Start decaying every epoch after and including this\n                        epoch\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-start_checkpoint_at'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "\"\"\"Start checkpointing every epoch after and including\n                        this epoch\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-decay_method'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "choices", "=", "[", "'noam'", "]", ",", "help", "=", "\"Use a custom decay rate.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-warmup_steps'", ",", "type", "=", "int", ",", "default", "=", "4000", ",", "\n", "help", "=", "\"\"\"Number of warmup steps for custom decay.\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-checkpoint_interval'", ",", "type", "=", "int", ",", "default", "=", "4000", ",", "\n", "help", "=", "'Run validation and save model parameters at this interval.'", ")", "\n", "#parser.add_argument('-run_valid_every', type=int, default=4000,", "\n", "#                    help=\"Run validation test at this interval (every run_valid_every batches)\")", "\n", "parser", ".", "add_argument", "(", "'-disable_early_stop_rl'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "\"A flag to disable early stopping in rl training.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-early_stop_tolerance'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "\"Stop training if it doesn't improve any more for several rounds of validation\"", ")", "\n", "\n", "timemark", "=", "time", ".", "strftime", "(", "'%Y%m%d-%H%M%S'", ",", "time", ".", "localtime", "(", "time", ".", "time", "(", ")", ")", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-timemark'", ",", "type", "=", "str", ",", "default", "=", "timemark", ",", "\n", "help", "=", "\"The current time stamp.\"", ")", "\n", "\n", "#parser.add_argument('-save_model_every', type=int, default=2000,", "\n", "#                    help=\"Save checkpoint at this interval.\")", "\n", "\n", "parser", ".", "add_argument", "(", "'-report_every'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "\"Print stats at this interval.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-exp'", ",", "type", "=", "str", ",", "default", "=", "\"kp20k\"", ",", "\n", "help", "=", "\"Name of the experiment for logging.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-exp_path'", ",", "type", "=", "str", ",", "default", "=", "\"exp/%s.%s\"", ",", "\n", "help", "=", "\"Path of experiment log/plot.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-model_path'", ",", "type", "=", "str", ",", "default", "=", "\"model/%s.%s\"", ",", "\n", "help", "=", "\"Path of checkpoints.\"", ")", "\n", "\n", "# beam search setting", "\n", "'''\n    parser.add_argument('-beam_search_batch_example', type=int, default=8,\n                        help='Maximum of examples for one batch, should be disabled for training')\n    parser.add_argument('-beam_search_batch_size', type=int, default=8,\n                        help='Maximum batch size')\n    parser.add_argument('-beam_search_batch_workers', type=int, default=4,\n                        help='Number of workers for generating batches')\n\n    parser.add_argument('-beam_size',  type=int, default=150,\n                        help='Beam size')\n    parser.add_argument('-max_sent_length', type=int, default=6,\n                        help='Maximum sentence length.')\n    '''", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.config.predict_opts": [[411, 498], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "time.strftime", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "time.localtime", "time.time"], "function", ["None"], ["", "def", "predict_opts", "(", "parser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "'-model'", ",", "required", "=", "True", ",", "\n", "help", "=", "'Path to model .pt file'", ")", "\n", "parser", ".", "add_argument", "(", "'-verbose'", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Whether to log the results of every individual samples\"", ")", "\n", "parser", ".", "add_argument", "(", "'-attn_debug'", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Whether to print attn for each word\"", ")", "\n", "#parser.add_argument('-present_kp_only', action=\"store_true\", help=\"Only consider the keyphrases that present in the source text\")", "\n", "parser", ".", "add_argument", "(", "'-data'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"\"\"Path prefix to the \"test.one2many.pt\" file path from preprocess.py\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-vocab'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"\"\"Path prefix to the \"vocab.pt\"\n                            file path from preprocess.py\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-custom_data_filename_suffix'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-custom_vocab_filename_suffix'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-vocab_filename_suffix'", ",", "default", "=", "''", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-data_filename_suffix'", ",", "default", "=", "''", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-beam_size'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "'Beam size'", ")", "\n", "parser", ".", "add_argument", "(", "'-n_best'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'Pick the top n_best sequences from beam_search, if n_best < 0, then n_best=beam_size'", ")", "\n", "parser", ".", "add_argument", "(", "'-max_length'", ",", "type", "=", "int", ",", "default", "=", "6", ",", "\n", "help", "=", "'Maximum prediction length.'", ")", "\n", "parser", ".", "add_argument", "(", "'-length_penalty_factor'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "\"\"\"Google NMT length penalty parameter\n                            (higher = longer generation)\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-coverage_penalty_factor'", ",", "type", "=", "float", ",", "default", "=", "-", "0.", ",", "\n", "help", "=", "\"\"\"Coverage penalty parameter\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-length_penalty'", ",", "default", "=", "'none'", ",", "choices", "=", "[", "'none'", ",", "'wu'", ",", "'avg'", "]", ",", "\n", "help", "=", "\"\"\"Length Penalty to use.\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-coverage_penalty'", ",", "default", "=", "'none'", ",", "choices", "=", "[", "'none'", ",", "'wu'", ",", "'summary'", "]", ",", "\n", "help", "=", "\"\"\"Coverage Penalty to use.\"\"\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-gpuid'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Use CUDA on the selected device.\"", ")", "\n", "# parser.add_argument('-gpuid', default=[0], nargs='+', type=int,", "\n", "#                    help=\"Use CUDA on the listed devices.\")", "\n", "parser", ".", "add_argument", "(", "'-seed'", ",", "type", "=", "int", ",", "default", "=", "9527", ",", "\n", "help", "=", "\"\"\"Random seed used for the experiments\n                            reproducibility.\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-batch_size'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "'Maximum batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'-batch_workers'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "'Number of workers for generating batches'", ")", "\n", "\n", "timemark", "=", "time", ".", "strftime", "(", "'%Y%m%d-%H%M%S'", ",", "time", ".", "localtime", "(", "time", ".", "time", "(", ")", ")", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-timemark'", ",", "type", "=", "str", ",", "default", "=", "timemark", ",", "\n", "help", "=", "\"The current time stamp.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-include_attn_dist'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Whether to return the attention distribution, for the visualization of the attention weights, haven't implemented\"", ")", "\n", "parser", ".", "add_argument", "(", "'-pred_file_prefix'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Prefix of prediction file.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-pred_path'", ",", "type", "=", "str", ",", "default", "=", "\"pred/%s.%s\"", ",", "\n", "help", "=", "\"Path of outputs of predictions.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-exp'", ",", "type", "=", "str", ",", "default", "=", "\"kp20k\"", ",", "\n", "help", "=", "\"Name of the experiment for logging.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-exp_path'", ",", "type", "=", "str", ",", "default", "=", "\"exp/%s.%s\"", ",", "\n", "help", "=", "\"Path of experiment log/plot.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-one2many'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'If true, it will not split a sample into multiple src-keyphrase pairs'", ")", "\n", "#parser.add_argument('-greedy', action=\"store_true\", default=False,", "\n", "#                    help='Use greedy decoding instead of sampling in one2many mode')", "\n", "parser", ".", "add_argument", "(", "'-one2many_mode'", ",", "type", "=", "int", ",", "choices", "=", "[", "0", ",", "1", ",", "2", ",", "3", "]", ",", "default", "=", "0", ",", "\n", "help", "=", "'Only effective when one2many=True. 0 is a dummy option which takes no effect. 1: concatenated the keyphrases by <sep>; 2: reset the inital state and input after each keyphrase; 3: reset the input after each keyphrase'", ")", "\n", "parser", ".", "add_argument", "(", "'-delimiter_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "'If type is 0, use <sep> to separate keyphrases. If type is 1, use <eos> to separate keyphrases'", ")", "\n", "#parser.add_argument('-num_predictions', type=int, default=1,", "\n", "#                    help='Control the number of predictions when one2many_mode=2.')", "\n", "parser", ".", "add_argument", "(", "'-max_eos_per_output_seq'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "# max_eos_per_seq", "\n", "help", "=", "'Specify the max number of eos in one output sequences to control the number of keyphrases in one output sequence. Only effective when one2many_mode=3 or one2many_mode=2.'", ")", "\n", "parser", ".", "add_argument", "(", "'-sampling'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Use sampling instead of beam search to generate the predictions.'", ")", "\n", "parser", ".", "add_argument", "(", "'-replace_unk'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Replace the unk token with the token of highest attention score.'", ")", "\n", "parser", ".", "add_argument", "(", "'-remove_src_eos'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Remove the eos token at the end of src text'", ")", "\n", "parser", ".", "add_argument", "(", "'-block_ngram_repeat'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Block repeat of n-gram'", ")", "\n", "parser", ".", "add_argument", "(", "'-ignore_when_blocking'", ",", "nargs", "=", "'+'", ",", "type", "=", "str", ",", "\n", "default", "=", "[", "'<sep>'", "]", ",", "\n", "help", "=", "\"\"\"Ignore these strings when blocking repeats.\n                               You want to block sentence delimiters.\"\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.config.post_predict_opts": [[500, 547], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "def", "post_predict_opts", "(", "parser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "'-pred_file_path'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of the prediction file.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-src_file_path'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of the source text file.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-trg_file_path'", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Path of the target text file.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-export_filtered_pred'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Export the filtered predictions to a file or not\"", ")", "\n", "parser", ".", "add_argument", "(", "'-filtered_pred_path'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Path of the folder for storing the filtered prediction\"", ")", "\n", "parser", ".", "add_argument", "(", "'-exp'", ",", "type", "=", "str", ",", "default", "=", "\"kp20k\"", ",", "\n", "help", "=", "\"Name of the experiment for logging.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-exp_path'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Path of experiment log/plot.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-disable_extra_one_word_filter'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"If False, it will only keep the first one-word prediction\"", ")", "\n", "parser", ".", "add_argument", "(", "'-disable_valid_filter'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"If False, it will remove all the invalid predictions\"", ")", "\n", "parser", ".", "add_argument", "(", "'-num_preds'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "\n", "help", "=", "'It will only consider the first num_preds keyphrases in each line of the prediction file'", ")", "\n", "parser", ".", "add_argument", "(", "'-debug'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'Print out the metric at each step or not'", ")", "\n", "parser", ".", "add_argument", "(", "'-match_by_str'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'If false, match the words at word level when checking present keyphrase. Else, match the words at string level.'", ")", "\n", "parser", ".", "add_argument", "(", "'-invalidate_unk'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'Treat unk as invalid output'", ")", "\n", "parser", ".", "add_argument", "(", "'-target_separated'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'The targets has already been separated into present keyphrases and absent keyphrases'", ")", "\n", "parser", ".", "add_argument", "(", "'-prediction_separated'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'The predictions has already been separated into present keyphrases and absent keyphrases'", ")", "\n", "parser", ".", "add_argument", "(", "'-reverse_sorting'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'Only effective in target separated.'", ")", "\n", "parser", ".", "add_argument", "(", "'-tune_f1_v'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'For tuning the F1@V score.'", ")", "\n", "parser", ".", "add_argument", "(", "'-all_ks'", ",", "nargs", "=", "'+'", ",", "default", "=", "[", "'5'", ",", "'10'", ",", "'M'", "]", ",", "type", "=", "str", ",", "\n", "help", "=", "'only allow integer or M'", ")", "\n", "parser", ".", "add_argument", "(", "'-present_ks'", ",", "nargs", "=", "'+'", ",", "default", "=", "[", "'5'", ",", "'10'", ",", "'M'", "]", ",", "type", "=", "str", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-absent_ks'", ",", "nargs", "=", "'+'", ",", "default", "=", "[", "'5'", ",", "'10'", ",", "'50'", ",", "'M'", "]", ",", "type", "=", "str", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-target_already_stemmed'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'If it is true, it will not stem the target keyphrases.'", ")", "\n", "parser", ".", "add_argument", "(", "'-meng_rui_precision'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'If it is true, when computing precision, it will divided by the number pf predictions, instead of divided by k.'", ")", "\n", "parser", ".", "add_argument", "(", "'-use_name_variations'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'Match the ground-truth with name variations.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.config.interactive_predict_opts": [[548, 628], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "time.strftime", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "time.localtime", "time.time"], "function", ["None"], ["", "def", "interactive_predict_opts", "(", "parser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "'-model'", ",", "required", "=", "True", ",", "\n", "help", "=", "'Path to model .pt file'", ")", "\n", "parser", ".", "add_argument", "(", "'-attn_debug'", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Whether to print attn for each word\"", ")", "\n", "parser", ".", "add_argument", "(", "'-src_file'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"\"\"Path to source file\"\"\"", ")", "\n", "#parser.add_argument('-trg_file', required=True,", "\n", "#                    help=\"\"\"Path to target file\"\"\")", "\n", "parser", ".", "add_argument", "(", "'-vocab'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"\"\"Path prefix to the \"vocab.pt\"\n                            file path from preprocess.py\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-custom_vocab_filename_suffix'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-vocab_filename_suffix'", ",", "default", "=", "''", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-beam_size'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "'Beam size'", ")", "\n", "parser", ".", "add_argument", "(", "'-n_best'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Pick the top n_best sequences from beam_search, if n_best < 0, then n_best=beam_size'", ")", "\n", "parser", ".", "add_argument", "(", "'-max_length'", ",", "type", "=", "int", ",", "default", "=", "60", ",", "\n", "help", "=", "'Maximum prediction length.'", ")", "\n", "parser", ".", "add_argument", "(", "'-length_penalty_factor'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "\"\"\"Google NMT length penalty parameter\n                            (higher = longer generation)\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-coverage_penalty_factor'", ",", "type", "=", "float", ",", "default", "=", "-", "0.", ",", "\n", "help", "=", "\"\"\"Coverage penalty parameter\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-length_penalty'", ",", "default", "=", "'none'", ",", "choices", "=", "[", "'none'", ",", "'wu'", ",", "'avg'", "]", ",", "\n", "help", "=", "\"\"\"Length Penalty to use.\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-coverage_penalty'", ",", "default", "=", "'none'", ",", "choices", "=", "[", "'none'", ",", "'wu'", ",", "'summary'", "]", ",", "\n", "help", "=", "\"\"\"Coverage Penalty to use.\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-gpuid'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Use CUDA on the selected device.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-seed'", ",", "type", "=", "int", ",", "default", "=", "9527", ",", "\n", "help", "=", "\"\"\"Random seed used for the experiments\n                            reproducibility.\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'-batch_size'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "'Maximum batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'-batch_workers'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Number of workers for generating batches'", ")", "\n", "\n", "timemark", "=", "time", ".", "strftime", "(", "'%Y%m%d-%H%M%S'", ",", "time", ".", "localtime", "(", "time", ".", "time", "(", ")", ")", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-timemark'", ",", "type", "=", "str", ",", "default", "=", "timemark", ",", "\n", "help", "=", "\"The current time stamp.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-include_attn_dist'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Whether to return the attention distribution, for the visualization of the attention weights, haven't implemented\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-pred_path'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path of outputs of predictions.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-pred_file_prefix'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Prefix of prediction file.\"", ")", "\n", "parser", ".", "add_argument", "(", "'-exp'", ",", "type", "=", "str", ",", "default", "=", "\"kp20k\"", ",", "\n", "help", "=", "\"Name of the experiment for logging.\"", ")", "\n", "#parser.add_argument('-exp_path', type=str, default=\"exp/%s.%s\",", "\n", "#                    help=\"Path of experiment log/plot.\")", "\n", "parser", ".", "add_argument", "(", "'-one2many'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'If true, it will not split a sample into multiple src-keyphrase pairs'", ")", "\n", "#parser.add_argument('-greedy', action=\"store_true\", default=False,", "\n", "#                    help='Use greedy decoding instead of sampling in one2many mode')", "\n", "parser", ".", "add_argument", "(", "'-one2many_mode'", ",", "type", "=", "int", ",", "choices", "=", "[", "0", ",", "1", ",", "2", ",", "3", "]", ",", "default", "=", "0", ",", "\n", "help", "=", "'Only effective when one2many=True. 0 is a dummy option which takes no effect. 1: concatenated the keyphrases by <sep>; 2: reset the inital state and input after each keyphrase; 3: reset the input after each keyphrase'", ")", "\n", "parser", ".", "add_argument", "(", "'-delimiter_type'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "'If type is 0, use <sep> to separate keyphrases. If type is 1, use <eos> to separate keyphrases'", ")", "\n", "parser", ".", "add_argument", "(", "'-max_eos_per_output_seq'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "# max_eos_per_seq", "\n", "help", "=", "'Specify the max number of eos in one output sequences to control the number of keyphrases in one output sequence. Only effective when one2many_mode=3 or one2many_mode=2.'", ")", "\n", "parser", ".", "add_argument", "(", "'-sampling'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Use sampling instead of beam search to generate the predictions.'", ")", "\n", "parser", ".", "add_argument", "(", "'-replace_unk'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Replace the unk token with the token of highest attention score.'", ")", "\n", "parser", ".", "add_argument", "(", "'-remove_src_eos'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Remove the eos token at the end of src text'", ")", "\n", "parser", ".", "add_argument", "(", "'-remove_title_eos'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ",", "\n", "help", "=", "'Remove the eos token at the end of title'", ")", "\n", "parser", ".", "add_argument", "(", "'-block_ngram_repeat'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Block repeat of n-gram'", ")", "\n", "parser", ".", "add_argument", "(", "'-ignore_when_blocking'", ",", "nargs", "=", "'+'", ",", "type", "=", "str", ",", "\n", "default", "=", "[", "'<sep>'", "]", ",", "\n", "help", "=", "\"\"\"Ignore these strings when blocking repeats.\n                           You want to block sentence delimiters.\"\"\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sort_testing_data.find_present_idx_for_variation_list": [[10, 24], ["len", "utils.string_helper.stem_word_list", "variation_str.split", "utils.string_helper.stem_word_list", "integrated_data_preprocess.check_present_idx"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_word_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_word_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.check_present_idx"], ["def", "find_present_idx_for_variation_list", "(", "src_tokens", ",", "variations_str_list", ")", ":", "\n", "    ", "src_len", "=", "len", "(", "src_tokens", ")", "\n", "src_tokens_stemmed", "=", "string_helper", ".", "stem_word_list", "(", "src_tokens", ")", "\n", "present_flag", "=", "False", "\n", "min_present_idx", "=", "10", "*", "src_len", "\n", "for", "variation_str", "in", "variations_str_list", ":", "\n", "        ", "variation_tokens", "=", "variation_str", ".", "split", "(", "' '", ")", "\n", "variation_tokens_stemmed", "=", "string_helper", ".", "stem_word_list", "(", "variation_tokens", ")", "\n", "present_idx", ",", "is_present", "=", "check_present_idx", "(", "src_tokens_stemmed", ",", "variation_tokens_stemmed", ")", "\n", "if", "present_idx", "<", "min_present_idx", ":", "\n", "            ", "min_present_idx", "=", "present_idx", "\n", "", "if", "is_present", ":", "\n", "            ", "present_flag", "=", "True", "\n", "", "", "return", "min_present_idx", ",", "present_flag", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sort_testing_data.sort_keyphrases_with_variations": [[26, 40], ["len", "len", "enumerate", "numpy.argsort", "sorted_keyphrase_list.insert", "numpy.ones", "target_str.split", "sort_testing_data.find_present_idx_for_variation_list"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sort_testing_data.find_present_idx_for_variation_list"], ["", "def", "sort_keyphrases_with_variations", "(", "src_tokens", ",", "target_str_list", ")", ":", "\n", "    ", "src_len", "=", "len", "(", "src_tokens", ")", "\n", "num_trgs", "=", "len", "(", "target_str_list", ")", "\n", "present_indices_array", "=", "np", ".", "ones", "(", "num_trgs", ")", "*", "(", "src_len", "+", "1", ")", "\n", "num_present_keyphrases", "=", "0", "\n", "for", "i", ",", "target_str", "in", "enumerate", "(", "target_str_list", ")", ":", "\n", "        ", "variations_str_list", "=", "target_str", ".", "split", "(", "'|'", ")", "\n", "present_indices_array", "[", "i", "]", ",", "is_present", "=", "find_present_idx_for_variation_list", "(", "src_tokens", ",", "variations_str_list", ")", "\n", "if", "is_present", ":", "\n", "            ", "num_present_keyphrases", "+=", "1", "\n", "", "", "sorted_keyphrase_indices", "=", "np", ".", "argsort", "(", "present_indices_array", ")", "\n", "sorted_keyphrase_list", "=", "[", "target_str_list", "[", "idx", "]", "for", "idx", "in", "sorted_keyphrase_indices", "]", "\n", "sorted_keyphrase_list", ".", "insert", "(", "num_present_keyphrases", ",", "present_absent_segmenter", ")", "\n", "return", "sorted_keyphrase_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sort_testing_data.main": [[42, 53], ["open", "zip", "os.path.split", "os.path.splitext", "os.path.join", "open", "open", "src_line.strip().split", "trg_line.strip().split", "sort_testing_data.sort_keyphrases_with_variations", "open.write", "src_line.strip", "trg_line.strip"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sort_testing_data.sort_keyphrases_with_variations"], ["", "def", "main", "(", "src_file_path", ",", "trg_file_path", ",", "saved_home", ")", ":", "\n", "    ", "trg_file_name", "=", "os", ".", "path", ".", "split", "(", "trg_file_path", ")", "[", "1", "]", "\n", "trg_file_name", "=", "os", ".", "path", ".", "splitext", "(", "trg_file_name", ")", "[", "0", "]", "\n", "sorted_trg_file_name", "=", "\"{}_sorted_separated.txt\"", ".", "format", "(", "trg_file_name", ")", "\n", "sorted_trg_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "saved_home", ",", "'data_for_corenlp'", ",", "sorted_trg_file_name", ")", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "\n", "for", "src_line", ",", "trg_line", "in", "zip", "(", "open", "(", "src_file_path", ",", "'r'", ")", ",", "open", "(", "trg_file_path", ",", "'r'", ")", ")", ":", "\n", "        ", "src_tokens", "=", "src_line", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "target_str_list", "=", "trg_line", ".", "strip", "(", ")", ".", "split", "(", "';'", ")", "\n", "sorted_keyphrase_list", "=", "sort_keyphrases_with_variations", "(", "src_tokens", ",", "target_str_list", ")", "\n", "sorted_trg_file", ".", "write", "(", "';'", ".", "join", "(", "sorted_keyphrase_list", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_rl.train_model": [[22, 158], ["utils.statistics.RewardStatistics", "utils.statistics.RewardStatistics", "float", "sequence_generator.SequenceGenerator", "model.train", "fastNLP.Vocabulary", "fnlp.Vocabulary.add_word_lst", "train_predicted_bert.BertPredictModel.from_pretrained", "BertPredictModel.from_pretrained.eval", "range", "utils.report.export_train_and_valid_reward", "ValueError", "opt.word2idx.keys", "enumerate", "train_rl.train_one_batch", "utils.statistics.RewardStatistics.update", "utils.statistics.RewardStatistics.update", "str", "print", "sys.stdout.flush", "datetime.now().strftime", "evaluate.evaluate_reward", "model.train", "evaluate.evaluate_reward.reward", "print", "sys.stdout.flush", "utils.statistics.RewardStatistics.reward", "utils.statistics.RewardStatistics.loss", "logging.info", "logging.info", "report_train_reward.append", "report_valid_reward.append", "utils.statistics.RewardStatistics.clear", "print", "sys.stdout.flush", "os.path.join", "torch.save", "torch.save", "logging.info", "print", "sys.stdout.flush", "math.exp", "math.pow", "datetime.now", "model.state_dict", "open", "enumerate", "logging.info", "math.floor", "len", "float"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_predicted_bert.BertPredictModel.from_pretrained", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.report.export_train_and_valid_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_ml.train_one_batch", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.update", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.update", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate.evaluate_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.loss", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.clear"], ["def", "train_model", "(", "model", ",", "optimizer_ml", ",", "optimizer_rl", ",", "criterion", ",", "train_data_loader", ",", "valid_data_loader", ",", "opt", ")", ":", "\n", "    ", "total_batch", "=", "-", "1", "\n", "early_stop_flag", "=", "False", "\n", "\n", "report_train_reward_statistics", "=", "RewardStatistics", "(", ")", "\n", "total_train_reward_statistics", "=", "RewardStatistics", "(", ")", "\n", "report_train_reward", "=", "[", "]", "\n", "report_valid_reward", "=", "[", "]", "\n", "best_valid_reward", "=", "float", "(", "'-inf'", ")", "\n", "num_stop_increasing", "=", "0", "\n", "init_perturb_std", "=", "opt", ".", "init_perturb_std", "\n", "final_perturb_std", "=", "opt", ".", "final_perturb_std", "\n", "perturb_decay_factor", "=", "opt", ".", "perturb_decay_factor", "\n", "perturb_decay_mode", "=", "opt", ".", "perturb_decay_mode", "\n", "\n", "if", "opt", ".", "train_from", ":", "# opt.train_from:", "\n", "#TODO: load the training state", "\n", "        ", "raise", "ValueError", "(", "\"Not implemented the function of load from trained model\"", ")", "\n", "pass", "\n", "\n", "", "generator", "=", "SequenceGenerator", "(", "model", ",", "\n", "bos_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "BOS_WORD", "]", ",", "\n", "eos_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "EOS_WORD", "]", ",", "\n", "pad_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "PAD_WORD", "]", ",", "\n", "peos_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "PEOS_WORD", "]", ",", "\n", "beam_size", "=", "1", ",", "\n", "max_sequence_length", "=", "opt", ".", "max_length", ",", "\n", "copy_attn", "=", "opt", ".", "copy_attention", ",", "\n", "coverage_attn", "=", "opt", ".", "coverage_attn", ",", "\n", "review_attn", "=", "opt", ".", "review_attn", ",", "\n", "cuda", "=", "opt", ".", "gpuid", ">", "-", "1", "\n", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "\n", "###", "\n", "#bert score", "\n", "###", "\n", "fnlp_vocab", "=", "fnlp", ".", "Vocabulary", "(", "padding", "=", "None", ",", "unknown", "=", "None", ")", "\n", "fnlp_vocab", ".", "add_word_lst", "(", "opt", ".", "word2idx", ".", "keys", "(", ")", ")", "\n", "fnlp_vocab", ".", "padding", "=", "'<pad>'", "\n", "fnlp_vocab", ".", "unknown", "=", "'<unk>'", "\n", "\n", "bert_dir", "=", "'/remote-home/ygxu/workspace/KG/KGM/BERT/new-bert-base-uncased-50k'", "\n", "bert", "=", "BertPredictModel", ".", "from_pretrained", "(", "bert_dir", ",", "vocab", "=", "fnlp_vocab", ")", "\n", "bert", ".", "eval", "(", ")", "\n", "\n", "###", "\n", "\n", "for", "epoch", "in", "range", "(", "opt", ".", "start_epoch", ",", "opt", ".", "epochs", "+", "1", ")", ":", "\n", "        ", "if", "early_stop_flag", ":", "\n", "            ", "break", "\n", "\n", "# TODO: progress bar", "\n", "# progbar = Progbar(logger=logging, title='Training', target=len(train_data_loader), batch_size=train_data_loader.batch_size,total_examples=len(train_data_loader.dataset.examples))", "\n", "", "for", "batch_i", ",", "batch", "in", "enumerate", "(", "train_data_loader", ")", ":", "\n", "            ", "total_batch", "+=", "1", "\n", "if", "perturb_decay_mode", "==", "0", ":", "# do not decay", "\n", "                ", "perturb_std", "=", "init_perturb_std", "\n", "", "elif", "perturb_decay_mode", "==", "1", ":", "# exponential decay", "\n", "                ", "perturb_std", "=", "final_perturb_std", "+", "(", "init_perturb_std", "-", "final_perturb_std", ")", "*", "math", ".", "exp", "(", "-", "1.", "*", "total_batch", "*", "perturb_decay_factor", ")", "\n", "", "elif", "perturb_decay_mode", "==", "2", ":", "# steps decay", "\n", "                ", "perturb_std", "=", "init_perturb_std", "*", "math", ".", "pow", "(", "perturb_decay_factor", ",", "math", ".", "floor", "(", "(", "1", "+", "total_batch", ")", "/", "4000", ")", ")", "\n", "\n", "#no bert", "\n", "#batch_reward_stat, log_selected_token_dist = train_one_batch(batch, generator, optimizer_rl, opt, perturb_std)", "\n", "#bert", "\n", "", "batch_reward_stat", ",", "log_selected_token_dist", "=", "train_one_batch", "(", "batch", ",", "generator", ",", "optimizer_rl", ",", "opt", ",", "\n", "perturb_std", ",", "bert", ")", "\n", "report_train_reward_statistics", ".", "update", "(", "batch_reward_stat", ")", "\n", "total_train_reward_statistics", ".", "update", "(", "batch_reward_stat", ")", "\n", "\n", "# Checkpoint, decay the learning rate if validation loss stop dropping, apply early stopping if stop decreasing for several epochs.", "\n", "# Save the model parameters if the validation loss improved.", "\n", "if", "total_batch", "%", "4000", "==", "0", ":", "\n", "                ", "from", "datetime", "import", "datetime", "\n", "now_time", "=", "str", "(", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d-%H-%M-%S-%f'", ")", ")", "\n", "print", "(", "f'[{now_time}]Epoch {epoch}; batch: {batch_i}; total batch: {total_batch}'", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "if", "epoch", ">=", "opt", ".", "start_checkpoint_at", ":", "\n", "                ", "if", "(", "opt", ".", "checkpoint_interval", "==", "-", "1", "and", "batch_i", "==", "len", "(", "train_data_loader", ")", "-", "1", ")", "or", "(", "opt", ".", "checkpoint_interval", ">", "-", "1", "and", "total_batch", ">", "1", "and", "total_batch", "%", "opt", ".", "checkpoint_interval", "==", "0", ")", ":", "\n", "\n", "                    ", "valid_reward_stat", "=", "evaluate_reward", "(", "valid_data_loader", ",", "generator", ",", "opt", ",", "bert", "=", "bert", ")", "\n", "model", ".", "train", "(", ")", "\n", "current_valid_reward", "=", "valid_reward_stat", ".", "reward", "(", ")", "\n", "print", "(", "\"Enter check point!\"", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "current_train_reward", "=", "report_train_reward_statistics", ".", "reward", "(", ")", "\n", "current_train_pg_loss", "=", "report_train_reward_statistics", ".", "loss", "(", ")", "\n", "\n", "if", "current_valid_reward", ">", "best_valid_reward", ":", "\n", "                        ", "print", "(", "\"Valid reward increases\"", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "best_valid_reward", "=", "current_valid_reward", "\n", "num_stop_increasing", "=", "0", "\n", "\n", "check_pt_model_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "model_path", ",", "'%s.epoch=%d.batch=%d.total_batch=%d'", "%", "(", "\n", "opt", ".", "exp", ",", "epoch", ",", "batch_i", ",", "total_batch", ")", "+", "'.model'", ")", "\n", "torch", ".", "save", "(", "# save model parameters", "\n", "model", ".", "state_dict", "(", ")", ",", "\n", "open", "(", "check_pt_model_path", ",", "'wb'", ")", "\n", ")", "\n", "logging", ".", "info", "(", "'Saving checkpoint to %s'", "%", "check_pt_model_path", ")", "\n", "", "else", ":", "\n", "                        ", "print", "(", "\"Valid reward does not increase\"", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "num_stop_increasing", "+=", "1", "\n", "# decay the learning rate by the factor specified by opt.learning_rate_decay", "\n", "if", "opt", ".", "learning_rate_decay_rl", ":", "\n", "                            ", "for", "i", ",", "param_group", "in", "enumerate", "(", "optimizer_rl", ".", "param_groups", ")", ":", "\n", "                                ", "old_lr", "=", "float", "(", "param_group", "[", "'lr'", "]", ")", "\n", "new_lr", "=", "old_lr", "*", "opt", ".", "learning_rate_decay", "\n", "if", "old_lr", "-", "new_lr", ">", "EPS", ":", "\n", "                                    ", "param_group", "[", "'lr'", "]", "=", "new_lr", "\n", "\n", "", "", "", "", "logging", ".", "info", "(", "'Epoch: %d; batch idx: %d; total batches: %d'", "%", "(", "epoch", ",", "batch_i", ",", "total_batch", ")", ")", "\n", "logging", ".", "info", "(", "\n", "'avg training reward: %.4f; avg training loss: %.4f; avg validation reward: %.4f; best validation reward: %.4f'", "%", "(", "\n", "current_train_reward", ",", "current_train_pg_loss", ",", "current_valid_reward", ",", "best_valid_reward", ")", ")", "\n", "\n", "report_train_reward", ".", "append", "(", "current_train_reward", ")", "\n", "report_valid_reward", ".", "append", "(", "current_valid_reward", ")", "\n", "\n", "if", "not", "opt", ".", "disable_early_stop_rl", ":", "\n", "                        ", "if", "num_stop_increasing", ">=", "opt", ".", "early_stop_tolerance", ":", "\n", "                            ", "logging", ".", "info", "(", "'Have not increased for %d check points, early stop training'", "%", "num_stop_increasing", ")", "\n", "early_stop_flag", "=", "True", "\n", "break", "\n", "", "", "report_train_reward_statistics", ".", "clear", "(", ")", "\n", "\n", "# export the training curve", "\n", "", "", "", "", "train_valid_curve_path", "=", "opt", ".", "exp_path", "+", "'/train_valid_curve'", "\n", "export_train_and_valid_reward", "(", "report_train_reward", ",", "report_valid_reward", ",", "opt", ".", "checkpoint_interval", ",", "train_valid_curve_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_rl.train_one_batch": [[160, 350], ["src.to.to", "src_mask.to.to", "src_oov.to.to", "bert.to.to", "optimizer.zero_grad", "src.to.size", "time.time", "generator.sample", "pykp.reward.sample_list_to_str_2dlist", "utils.time_log.time_since", "log_selected_token_dist.size", "torch.from_numpy().type().to", "torch.from_numpy().type().to", "torch.from_numpy().type().to.requires_grad_", "utils.time_log.time_since", "pykp.reward.compute_pg_loss", "time.time", "pykp.reward.compute_pg_loss.backward", "utils.time_log.time_since", "optimizer.step", "utils.statistics.RewardStatistics", "title.to.to", "title_mask.to.to", "entropy.data.cpu().numpy", "generator.model.eval", "generator.model.train", "max", "pykp.reward.compute_phrase_reward", "pykp.reward.compute_batch_reward.sum", "pykp.reward.shape_reward", "pykp.reward.phrase_reward_to_stepwise_reward", "[].copy", "torch.utils.clip_grad_norm_", "pykp.reward.compute_pg_loss.item", "log_selected_token_dist.detach", "torch.no_grad", "torch.no_grad", "time.time", "generator.sample", "pykp.reward.sample_list_to_str_2dlist", "max", "pykp.reward.compute_phrase_reward", "pykp.reward.compute_present_absent_reward", "pykp.reward.compute_present_absent_reward.sum", "pykp.reward.compute_batch_reward.sum", "pykp.reward.present_absent_reward_to_stepwise_reward", "[].copy", "pykp.reward.compute_batch_reward", "pykp.reward.compute_batch_reward.sum", "numpy.tile", "torch.from_numpy().type", "torch.from_numpy().type", "generator.model.parameters", "entropy.data.cpu", "len", "pykp.reward.compute_present_absent_reward", "pykp.reward.compute_batch_reward", "pykp.reward.compute_batch_reward.reshape", "len", "numpy.cumsum", "torch.from_numpy", "torch.from_numpy", "numpy.cumsum"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.sample", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.sample_list_to_str_2dlist", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_pg_loss", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_phrase_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.shape_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.phrase_reward_to_stepwise_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.sample", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.sample_list_to_str_2dlist", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_phrase_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_present_absent_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.present_absent_reward_to_stepwise_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_batch_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_present_absent_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_batch_reward"], ["", "def", "train_one_batch", "(", "one2many_batch", ",", "generator", ",", "optimizer", ",", "opt", ",", "perturb_std", "=", "0", ",", "bert", "=", "None", ")", ":", "\n", "    ", "src", ",", "src_lens", ",", "src_mask", ",", "src_oov", ",", "oov_lists", ",", "src_str_list", ",", "trg_str_2dlist", ",", "trg", ",", "trg_oov", ",", "trg_lens", ",", "trg_mask", ",", "_", ",", "title", ",", "title_oov", ",", "title_lens", ",", "title_mask", "=", "one2many_batch", "\n", "\"\"\"\n    src: a LongTensor containing the word indices of source sentences, [batch, src_seq_len], with oov words replaced by unk idx\n    src_lens: a list containing the length of src sequences for each batch, with len=batch\n    src_mask: a FloatTensor, [batch, src_seq_len]\n    src_oov: a LongTensor containing the word indices of source sentences, [batch, src_seq_len], contains the index of oov words (used by copy)\n    oov_lists: a list of oov words for each src, 2dlist\n    \"\"\"", "\n", "\n", "one2many", "=", "opt", ".", "one2many", "\n", "one2many_mode", "=", "opt", ".", "one2many_mode", "\n", "if", "one2many", "and", "one2many_mode", ">", "1", ":", "\n", "        ", "num_predictions", "=", "opt", ".", "num_predictions", "\n", "", "else", ":", "\n", "        ", "num_predictions", "=", "1", "\n", "\n", "# move data to GPU if available", "\n", "", "src", "=", "src", ".", "to", "(", "opt", ".", "device", ")", "\n", "src_mask", "=", "src_mask", ".", "to", "(", "opt", ".", "device", ")", "\n", "src_oov", "=", "src_oov", ".", "to", "(", "opt", ".", "device", ")", "\n", "bert", "=", "bert", ".", "to", "(", "opt", ".", "device", ")", "\n", "# trg = trg.to(opt.device)", "\n", "# trg_mask = trg_mask.to(opt.device)", "\n", "# trg_oov = trg_oov.to(opt.device)", "\n", "\n", "if", "opt", ".", "title_guided", ":", "\n", "        ", "title", "=", "title", ".", "to", "(", "opt", ".", "device", ")", "\n", "title_mask", "=", "title_mask", ".", "to", "(", "opt", ".", "device", ")", "\n", "#title_oov = title_oov.to(opt.device)", "\n", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "eos_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "EOS_WORD", "]", "\n", "delimiter_word", "=", "opt", ".", "delimiter_word", "\n", "batch_size", "=", "src", ".", "size", "(", "0", ")", "\n", "topk", "=", "opt", ".", "topk", "\n", "reward_type", "=", "opt", ".", "reward_type", "\n", "reward_shaping", "=", "opt", ".", "reward_shaping", "\n", "baseline", "=", "opt", ".", "baseline", "\n", "match_type", "=", "opt", ".", "match_type", "\n", "regularization_type", "=", "opt", ".", "regularization_type", "\n", "regularization_factor", "=", "opt", ".", "regularization_factor", "\n", "\n", "if", "regularization_type", "==", "2", ":", "\n", "        ", "entropy_regularize", "=", "True", "\n", "", "else", ":", "\n", "        ", "entropy_regularize", "=", "False", "\n", "\n", "", "if", "opt", ".", "perturb_baseline", ":", "\n", "        ", "baseline_perturb_std", "=", "perturb_std", "\n", "", "else", ":", "\n", "        ", "baseline_perturb_std", "=", "0", "\n", "\n", "#generator.model.train()", "\n", "\n", "# sample a sequence from the model", "\n", "# sample_list is a list of dict, {\"prediction\": [], \"scores\": [], \"attention\": [], \"done\": True}, prediction is a list of 0 dim tensors", "\n", "# log_selected_token_dist: size: [batch, output_seq_len]", "\n", "", "start_time", "=", "time", ".", "time", "(", ")", "\n", "sample_list", ",", "log_selected_token_dist", ",", "output_mask", ",", "pred_eos_idx_mask", ",", "entropy", ",", "location_of_eos_for_each_batch", ",", "location_of_peos_for_each_batch", "=", "generator", ".", "sample", "(", "\n", "src", ",", "src_lens", ",", "src_oov", ",", "src_mask", ",", "oov_lists", ",", "opt", ".", "max_length", ",", "greedy", "=", "False", ",", "one2many", "=", "one2many", ",", "\n", "one2many_mode", "=", "one2many_mode", ",", "num_predictions", "=", "num_predictions", ",", "perturb_std", "=", "perturb_std", ",", "entropy_regularize", "=", "entropy_regularize", ",", "title", "=", "title", ",", "title_lens", "=", "title_lens", ",", "title_mask", "=", "title_mask", ")", "\n", "pred_str_2dlist", "=", "sample_list_to_str_2dlist", "(", "sample_list", ",", "oov_lists", ",", "opt", ".", "idx2word", ",", "opt", ".", "vocab_size", ",", "eos_idx", ",", "delimiter_word", ",", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "UNK_WORD", "]", ",", "opt", ".", "replace_unk", ",", "\n", "src_str_list", ",", "opt", ".", "separate_present_absent", ",", "pykp", ".", "io", ".", "PEOS_WORD", ")", "\n", "sample_time", "=", "time_since", "(", "start_time", ")", "\n", "max_pred_seq_len", "=", "log_selected_token_dist", ".", "size", "(", "1", ")", "\n", "\n", "if", "entropy_regularize", ":", "\n", "        ", "entropy_array", "=", "entropy", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "        ", "entropy_array", "=", "None", "\n", "\n", "# if use self critical as baseline, greedily decode a sequence from the model", "\n", "", "if", "baseline", "==", "'self'", ":", "\n", "        ", "generator", ".", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "greedy_sample_list", ",", "_", ",", "_", ",", "greedy_eos_idx_mask", ",", "_", ",", "_", ",", "_", "=", "generator", ".", "sample", "(", "src", ",", "src_lens", ",", "src_oov", ",", "src_mask", ",", "\n", "oov_lists", ",", "opt", ".", "max_length", ",", "\n", "greedy", "=", "True", ",", "one2many", "=", "one2many", ",", "\n", "one2many_mode", "=", "one2many_mode", ",", "\n", "num_predictions", "=", "num_predictions", ",", "\n", "perturb_std", "=", "baseline_perturb_std", ",", "\n", "title", "=", "title", ",", "\n", "title_lens", "=", "title_lens", ",", "\n", "title_mask", "=", "title_mask", ")", "\n", "greedy_str_2dlist", "=", "sample_list_to_str_2dlist", "(", "greedy_sample_list", ",", "oov_lists", ",", "opt", ".", "idx2word", ",", "opt", ".", "vocab_size", ",", "\n", "eos_idx", ",", "\n", "delimiter_word", ",", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "UNK_WORD", "]", ",", "opt", ".", "replace_unk", ",", "\n", "src_str_list", ",", "opt", ".", "separate_present_absent", ",", "pykp", ".", "io", ".", "PEOS_WORD", ")", "\n", "", "generator", ".", "model", ".", "train", "(", ")", "\n", "\n", "# Compute the reward for each predicted keyphrase", "\n", "# if using reward shaping, each keyphrase will have its own reward, else, only the last keyphrase will get a reward", "\n", "# In addition, we adds a regularization terms to the reward", "\n", "\n", "", "if", "reward_shaping", ":", "\n", "        ", "max_num_pred_phrases", "=", "max", "(", "[", "len", "(", "pred_str_list", ")", "for", "pred_str_list", "in", "pred_str_2dlist", "]", ")", "\n", "\n", "# compute the reward for each phrase, np array with size: [batch_size, num_predictions]", "\n", "phrase_reward", "=", "compute_phrase_reward", "(", "pred_str_2dlist", ",", "trg_str_2dlist", ",", "batch_size", ",", "max_num_pred_phrases", ",", "reward_shaping", ",", "\n", "reward_type", ",", "topk", ",", "match_type", ",", "regularization_factor", ",", "regularization_type", ",", "entropy_array", ")", "\n", "# store the sum of cumulative reward for the experiment log", "\n", "cumulative_reward", "=", "phrase_reward", "[", ":", ",", "-", "1", "]", "\n", "cumulative_reward_sum", "=", "cumulative_reward", ".", "sum", "(", "0", ")", "\n", "\n", "# Subtract reward by a baseline if needed", "\n", "if", "opt", ".", "baseline", "==", "'self'", ":", "\n", "            ", "max_num_greedy_phrases", "=", "max", "(", "[", "len", "(", "greedy_str_list", ")", "for", "greedy_str_list", "in", "greedy_str_2dlist", "]", ")", "\n", "assert", "max_num_pred_phrases", "==", "max_num_greedy_phrases", ",", "\"if you use self-critical training with reward shaping, make sure the number of phrases sampled from the policy and that decoded by greedy are the same.\"", "\n", "# use the reward of greedy decoding as baseline", "\n", "phrase_baseline", "=", "compute_phrase_reward", "(", "greedy_str_2dlist", ",", "trg_str_2dlist", ",", "batch_size", ",", "max_num_greedy_phrases", ",", "reward_shaping", ",", "\n", "reward_type", ",", "topk", ",", "match_type", ",", "regularization_factor", ",", "regularization_type", ",", "entropy_array", ")", "\n", "phrase_reward", "=", "phrase_reward", "-", "phrase_baseline", "\n", "\n", "# convert each phrase reward to its improvement in reward", "\n", "", "phrase_reward", "=", "shape_reward", "(", "phrase_reward", ")", "\n", "\n", "# convert to reward received at each decoding step", "\n", "stepwise_reward", "=", "phrase_reward_to_stepwise_reward", "(", "phrase_reward", ",", "pred_eos_idx_mask", ")", "\n", "q_value_estimate_array", "=", "np", ".", "cumsum", "(", "stepwise_reward", "[", ":", ",", ":", ":", "-", "1", "]", ",", "axis", "=", "1", ")", "[", ":", ",", ":", ":", "-", "1", "]", ".", "copy", "(", ")", "\n", "\n", "", "elif", "opt", ".", "separate_present_absent", ":", "\n", "\n", "####no bert", "\n", "#present_absent_reward = compute_present_absent_reward(pred_str_2dlist, trg_str_2dlist, reward_type=reward_type, topk=topk, match_type=match_type,", "\n", "#               regularization_factor=regularization_factor, regularization_type=regularization_type, entropy=entropy_array)", "\n", "####bert", "\n", "        ", "present_absent_reward", "=", "compute_present_absent_reward", "(", "pred_str_2dlist", ",", "trg_str_2dlist", ",", "reward_type", "=", "reward_type", ",", "topk", "=", "topk", ",", "match_type", "=", "match_type", ",", "\n", "regularization_factor", "=", "regularization_factor", ",", "regularization_type", "=", "regularization_type", ",", "entropy", "=", "entropy_array", ",", "bert", "=", "bert", ")", "\n", "\n", "cumulative_reward", "=", "present_absent_reward", ".", "sum", "(", "1", ")", "\n", "cumulative_reward_sum", "=", "cumulative_reward", ".", "sum", "(", "0", ")", "\n", "# Subtract reward by a baseline if needed", "\n", "if", "opt", ".", "baseline", "==", "'self'", ":", "\n", "\n", "####no bert", "\n", "#present_absent_baseline = compute_present_absent_reward(greedy_str_2dlist, trg_str_2dlist, reward_type=reward_type, topk=topk, match_type=match_type,", "\n", "#           regularization_factor=regularization_factor, regularization_type=regularization_type, entropy=entropy_array)", "\n", "####bert", "\n", "            ", "present_absent_baseline", "=", "compute_present_absent_reward", "(", "greedy_str_2dlist", ",", "trg_str_2dlist", ",", "reward_type", "=", "reward_type", ",", "topk", "=", "topk", ",", "match_type", "=", "match_type", ",", "\n", "regularization_factor", "=", "regularization_factor", ",", "regularization_type", "=", "regularization_type", ",", "entropy", "=", "entropy_array", ",", "bert", "=", "bert", ")", "\n", "\n", "\n", "present_absent_reward", "=", "present_absent_reward", "-", "present_absent_baseline", "\n", "", "stepwise_reward", "=", "present_absent_reward_to_stepwise_reward", "(", "present_absent_reward", ",", "max_pred_seq_len", ",", "location_of_peos_for_each_batch", ",", "location_of_eos_for_each_batch", ")", "\n", "q_value_estimate_array", "=", "np", ".", "cumsum", "(", "stepwise_reward", "[", ":", ",", ":", ":", "-", "1", "]", ",", "axis", "=", "1", ")", "[", ":", ",", ":", ":", "-", "1", "]", ".", "copy", "(", ")", "\n", "\n", "", "else", ":", "# neither using reward shaping", "\n", "# only receive reward at the end of whole sequence, np array: [batch_size]", "\n", "        ", "cumulative_reward", "=", "compute_batch_reward", "(", "pred_str_2dlist", ",", "trg_str_2dlist", ",", "batch_size", ",", "reward_type", "=", "reward_type", ",", "topk", "=", "topk", ",", "match_type", "=", "match_type", ",", "\n", "regularization_factor", "=", "regularization_factor", ",", "regularization_type", "=", "regularization_type", ",", "entropy", "=", "entropy_array", ")", "\n", "# store the sum of cumulative reward (before baseline) for the experiment log", "\n", "cumulative_reward_sum", "=", "cumulative_reward", ".", "sum", "(", "0", ")", "\n", "# Subtract the cumulative reward by a baseline if needed", "\n", "if", "opt", ".", "baseline", "==", "'self'", ":", "\n", "            ", "baseline", "=", "compute_batch_reward", "(", "greedy_str_2dlist", ",", "trg_str_2dlist", ",", "batch_size", ",", "reward_type", "=", "reward_type", ",", "topk", "=", "topk", ",", "match_type", "=", "match_type", ",", "\n", "regularization_factor", "=", "regularization_factor", ",", "regularization_type", "=", "regularization_type", ",", "entropy", "=", "entropy_array", ")", "\n", "cumulative_reward", "=", "cumulative_reward", "-", "baseline", "\n", "# q value estimation for each time step equals to the (baselined) cumulative reward", "\n", "", "q_value_estimate_array", "=", "np", ".", "tile", "(", "cumulative_reward", ".", "reshape", "(", "[", "-", "1", ",", "1", "]", ")", ",", "[", "1", ",", "max_pred_seq_len", "]", ")", "# [batch, max_pred_seq_len]", "\n", "\n", "#shapped_baselined_reward = torch.gather(shapped_baselined_phrase_reward, dim=1, index=pred_phrase_idx_mask)", "\n", "\n", "# use the return as the estimation of q_value at each step", "\n", "\n", "", "q_value_estimate", "=", "torch", ".", "from_numpy", "(", "q_value_estimate_array", ")", ".", "type", "(", "torch", ".", "FloatTensor", ")", ".", "to", "(", "src", ".", "device", ")", "\n", "q_value_estimate", ".", "requires_grad_", "(", "True", ")", "\n", "q_estimate_compute_time", "=", "time_since", "(", "start_time", ")", "\n", "\n", "# compute the policy gradient objective", "\n", "pg_loss", "=", "compute_pg_loss", "(", "log_selected_token_dist", ",", "output_mask", ",", "q_value_estimate", ")", "\n", "\n", "# back propagation to compute the gradient", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "pg_loss", ".", "backward", "(", ")", "\n", "backward_time", "=", "time_since", "(", "start_time", ")", "\n", "\n", "if", "opt", ".", "max_grad_norm", ">", "0", ":", "\n", "        ", "grad_norm_before_clipping", "=", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "generator", ".", "model", ".", "parameters", "(", ")", ",", "opt", ".", "max_grad_norm", ")", "\n", "\n", "# take a step of gradient descent", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "\n", "stat", "=", "RewardStatistics", "(", "cumulative_reward_sum", ",", "pg_loss", ".", "item", "(", ")", ",", "batch_size", ",", "sample_time", ",", "q_estimate_compute_time", ",", "backward_time", ")", "\n", "# (final_reward=0.0, pg_loss=0.0, n_batch=0, sample_time=0, q_estimate_compute_time=0, backward_time=0)", "\n", "# reward=0.0, pg_loss=0.0, n_batch=0, sample_time=0, q_estimate_compute_time=0, backward_time=0", "\n", "\n", "return", "stat", ",", "log_selected_token_dist", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.predict.init_pretrained_model": [[18, 33], ["pykp.model.Seq2SeqModel", "pykp.model.Seq2SeqModel.load_state_dict", "pykp.model.Seq2SeqModel.to", "pykp.model.Seq2SeqModel.eval", "torch.load"], "function", ["None"], ["def", "init_pretrained_model", "(", "opt", ")", ":", "\n", "    ", "model", "=", "Seq2SeqModel", "(", "opt", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "opt", ".", "model", ")", ")", "\n", "\"\"\"\n    pretrained_state_dict = torch.load(opt.model)\n    pretrained_state_dict_renamed = {}\n    for k, v in pretrained_state_dict.items():\n        if k.startswith(\"encoder.rnn.\"):\n            k = k.replace(\"encoder.rnn.\", \"encoder.encoder.rnn.\", 1)\n        pretrained_state_dict_renamed[k] = v\n    model.load_state_dict(pretrained_state_dict_renamed)\n    \"\"\"", "\n", "model", ".", "to", "(", "opt", ".", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.predict.process_opt": [[35, 101], ["torch.cuda.is_available", "torch.manual_seed", "torch.device", "torch.device", "print", "hasattr", "opt.exp_path.find", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "ValueError", "ValueError", "ValueError"], "function", ["None"], ["", "def", "process_opt", "(", "opt", ")", ":", "\n", "    ", "if", "opt", ".", "seed", ">", "0", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "opt", ".", "seed", ")", "\n", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "if", "not", "opt", ".", "gpuid", ":", "\n", "            ", "opt", ".", "gpuid", "=", "0", "\n", "", "opt", ".", "device", "=", "torch", ".", "device", "(", "\"cuda:%d\"", "%", "opt", ".", "gpuid", ")", "\n", "", "else", ":", "\n", "        ", "opt", ".", "device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "opt", ".", "gpuid", "=", "-", "1", "\n", "print", "(", "\"CUDA is not available, fall back to CPU.\"", ")", "\n", "\n", "", "opt", ".", "exp", "=", "'predict.'", "+", "opt", ".", "exp", "\n", "if", "opt", ".", "one2many", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.one2many'", "\n", "\n", "", "if", "opt", ".", "one2many_mode", "==", "1", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.cat'", "\n", "\n", "", "if", "opt", ".", "copy_attention", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.copy'", "\n", "\n", "", "if", "opt", ".", "coverage_attn", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.coverage'", "\n", "\n", "", "if", "opt", ".", "review_attn", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.review'", "\n", "\n", "", "if", "opt", ".", "orthogonal_loss", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.orthogonal'", "\n", "\n", "", "if", "opt", ".", "use_target_encoder", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.target_encode'", "\n", "\n", "", "if", "hasattr", "(", "opt", ",", "'bidirectional'", ")", "and", "opt", ".", "bidirectional", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.bi-directional'", "\n", "", "else", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.uni-directional'", "\n", "\n", "", "if", "opt", ".", "n_best", "<", "0", ":", "\n", "        ", "opt", ".", "n_best", "=", "opt", ".", "beam_size", "\n", "\n", "# fill time into the name", "\n", "", "if", "opt", ".", "exp_path", ".", "find", "(", "'%s'", ")", ">", "0", ":", "\n", "        ", "opt", ".", "exp_path", "=", "opt", ".", "exp_path", "%", "(", "opt", ".", "exp", ",", "opt", ".", "timemark", ")", "\n", "opt", ".", "pred_path", "=", "opt", ".", "pred_path", "%", "(", "opt", ".", "exp", ",", "opt", ".", "timemark", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "opt", ".", "exp_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "opt", ".", "exp_path", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "opt", ".", "pred_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "opt", ".", "pred_path", ")", "\n", "\n", "", "if", "not", "opt", ".", "one2many", "and", "opt", ".", "one2many_mode", ">", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"You cannot choose one2many mode without the -one2many options.\"", ")", "\n", "\n", "", "if", "opt", ".", "one2many", "and", "opt", ".", "one2many_mode", "==", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"If you choose one2many, you must specify the one2many mode.\"", ")", "\n", "\n", "#if opt.greedy and not opt.one2many:", "\n", "#    raise ValueError(\"Greedy sampling can only be used in one2many mode.\")", "\n", "\n", "", "if", "opt", ".", "one2many_mode", "not", "in", "[", "2", ",", "3", "]", "and", "opt", ".", "max_eos_per_output_seq", "!=", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"You cannot specify the max_eos_per_output_seq unless your are using one2many_mode 2 or 3\"", ")", "\n", "\n", "", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.predict.predict": [[103, 139], ["sequence_generator.SequenceGenerator", "ValueError", "evaluate.evaluate_beam_search"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate.evaluate_beam_search"], ["", "def", "predict", "(", "test_data_loader", ",", "model", ",", "opt", ")", ":", "\n", "    ", "if", "opt", ".", "delimiter_type", "==", "0", ":", "\n", "        ", "delimiter_word", "=", "pykp", ".", "io", ".", "SEP_WORD", "\n", "", "else", ":", "\n", "        ", "delimiter_word", "=", "pykp", ".", "io", ".", "EOS_WORD", "\n", "", "generator", "=", "SequenceGenerator", "(", "model", ",", "\n", "bos_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "BOS_WORD", "]", ",", "\n", "eos_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "EOS_WORD", "]", ",", "\n", "pad_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "PAD_WORD", "]", ",", "\n", "beam_size", "=", "opt", ".", "beam_size", ",", "\n", "max_sequence_length", "=", "opt", ".", "max_length", ",", "\n", "copy_attn", "=", "opt", ".", "copy_attention", ",", "\n", "coverage_attn", "=", "opt", ".", "coverage_attn", ",", "\n", "review_attn", "=", "opt", ".", "review_attn", ",", "\n", "include_attn_dist", "=", "opt", ".", "include_attn_dist", ",", "\n", "length_penalty_factor", "=", "opt", ".", "length_penalty_factor", ",", "\n", "coverage_penalty_factor", "=", "opt", ".", "coverage_penalty_factor", ",", "\n", "length_penalty", "=", "opt", ".", "length_penalty", ",", "\n", "coverage_penalty", "=", "opt", ".", "coverage_penalty", ",", "\n", "cuda", "=", "opt", ".", "gpuid", ">", "-", "1", ",", "\n", "n_best", "=", "opt", ".", "n_best", ",", "\n", "block_ngram_repeat", "=", "opt", ".", "block_ngram_repeat", ",", "\n", "ignore_when_blocking", "=", "opt", ".", "ignore_when_blocking", ",", "\n", "peos_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "PEOS_WORD", "]", "\n", ")", "\n", "\"\"\"\n    if opt.one2many and opt.one2many_mode > 1:\n        prediction_by_sampling(generator, test_data_loader, opt, delimiter_word)\n    else:\n        evaluate_beam_search(generator, test_data_loader, opt, delimiter_word)\n    \"\"\"", "\n", "if", "opt", ".", "sampling", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not support yet!\"", ")", "\n", "#prediction_by_sampling(generator, test_data_loader, opt, delimiter_word)", "\n", "", "else", ":", "\n", "        ", "evaluate_beam_search", "(", "generator", ",", "test_data_loader", ",", "opt", ",", "delimiter_word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.predict.main": [[141, 162], ["time.time", "utils.time_log.time_since", "utils.data_loader.load_data_and_vocab", "predict.init_pretrained_model", "logging.info", "time.time", "predict.predict", "utils.time_log.time_since", "logging.info", "print", "sys.stdout.flush", "logging.exception"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.data_loader.load_data_and_vocab", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.predict.init_pretrained_model", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.predict.predict", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since"], ["", "", "def", "main", "(", "opt", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "load_data_time", "=", "time_since", "(", "start_time", ")", "\n", "test_data_loader", ",", "word2idx", ",", "idx2word", ",", "vocab", "=", "load_data_and_vocab", "(", "opt", ",", "load_train", "=", "False", ")", "\n", "model", "=", "init_pretrained_model", "(", "opt", ")", "\n", "logging", ".", "info", "(", "'Time for loading the data and model: %.1f'", "%", "load_data_time", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "predict", "(", "test_data_loader", ",", "model", ",", "opt", ")", "\n", "\n", "total_testing_time", "=", "time_since", "(", "start_time", ")", "\n", "logging", ".", "info", "(", "'Time for a complete testing: %.1f'", "%", "total_testing_time", ")", "\n", "print", "(", "'Time for a complete testing: %.1f'", "%", "total_testing_time", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "logging", ".", "exception", "(", "\"message\"", ")", "\n", "", "return", "\n", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.Beam.__init__": [[6, 53], ["set", "beam.Beam.tt.FloatTensor().zero_", "torch.zeros().to", "beam.Beam.tt.LongTensor().fill_", "beam.Beam.tt.FloatTensor", "torch.zeros", "beam.Beam.tt.LongTensor"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "size", ",", "pad", ",", "bos", ",", "eos", ",", "\n", "n_best", "=", "1", ",", "cuda", "=", "False", ",", "\n", "global_scorer", "=", "None", ",", "\n", "min_length", "=", "0", ",", "\n", "stepwise_penalty", "=", "False", ",", "\n", "block_ngram_repeat", "=", "0", ",", "\n", "exclusion_tokens", "=", "set", "(", ")", ",", "max_eos_per_output_seq", "=", "1", ")", ":", "\n", "        ", "self", ".", "size", "=", "size", "\n", "self", ".", "tt", "=", "torch", ".", "cuda", "if", "cuda", "else", "torch", "\n", "\n", "# The score for each translation on the beam.", "\n", "self", ".", "scores", "=", "self", ".", "tt", ".", "FloatTensor", "(", "size", ")", ".", "zero_", "(", ")", "\n", "self", ".", "all_scores", "=", "[", "]", "\n", "\n", "# The backpointers at each time-step.", "\n", "self", ".", "prev_ks", "=", "[", "]", "\n", "\n", "# The outputs at each time-step.", "\n", "self", ".", "next_ys", "=", "[", "self", ".", "tt", ".", "LongTensor", "(", "size", ")", "\n", ".", "fill_", "(", "pad", ")", "]", "\n", "self", ".", "next_ys", "[", "0", "]", "[", "0", "]", "=", "bos", "\n", "\n", "# Has EOS topped the beam yet.", "\n", "self", ".", "_eos", "=", "eos", "\n", "self", ".", "eos_top", "=", "False", "\n", "\n", "# The attentions (matrix) for each time.", "\n", "self", ".", "attn", "=", "[", "]", "\n", "\n", "# Time and k pair for finished.", "\n", "self", ".", "finished", "=", "[", "]", "\n", "self", ".", "n_best", "=", "n_best", "\n", "\n", "# Information for global scoring.", "\n", "self", ".", "global_scorer", "=", "global_scorer", "\n", "self", ".", "global_state", "=", "{", "}", "\n", "\n", "# Minimum prediction length", "\n", "self", ".", "min_length", "=", "min_length", "\n", "\n", "# Apply Penalty at every step", "\n", "self", ".", "stepwise_penalty", "=", "stepwise_penalty", "\n", "self", ".", "block_ngram_repeat", "=", "block_ngram_repeat", "\n", "self", ".", "exclusion_tokens", "=", "exclusion_tokens", "\n", "\n", "self", ".", "eos_counters", "=", "torch", ".", "zeros", "(", "size", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "self", ".", "next_ys", "[", "0", "]", ".", "device", ")", "# Store the number of emitted eos token for each hypothesis sequence", "\n", "self", ".", "max_eos_per_output_seq", "=", "max_eos_per_output_seq", "# The max. number of eos token that a hypothesis sequence can have", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.Beam.get_current_tokens": [[54, 57], ["None"], "methods", ["None"], ["", "def", "get_current_tokens", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the outputs for the current timestep.\"\"\"", "\n", "return", "self", ".", "next_ys", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.Beam.get_current_origin": [[58, 61], ["None"], "methods", ["None"], ["", "def", "get_current_origin", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the backpointers for the current timestep.\"\"\"", "\n", "return", "self", ".", "prev_ks", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.Beam.done": [[62, 64], ["len"], "methods", ["None"], ["", "def", "done", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "eos_top", "and", "len", "(", "self", ".", "finished", ")", ">=", "self", ".", "n_best", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.Beam.get_hyp": [[65, 83], ["range", "hyp.append", "attn.append", "torch.stack", "len"], "methods", ["None"], ["", "def", "get_hyp", "(", "self", ",", "timestep", ",", "k", ")", ":", "\n", "        ", "\"\"\"\n        walk back to construct the full hypothesis given the finished time step and beam idx\n        :param timestep: int\n        :param k: int\n        :return:\n        \"\"\"", "\n", "hyp", ",", "attn", "=", "[", "]", ",", "[", "]", "\n", "# iterate from output sequence length (with eos but not bos) - 1 to 0f", "\n", "for", "j", "in", "range", "(", "len", "(", "self", ".", "prev_ks", "[", ":", "timestep", "]", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "hyp", ".", "append", "(", "self", ".", "next_ys", "[", "j", "+", "1", "]", "[", "k", "]", ")", "# j+1 so that it will iterate from the <eos> token, and end before the <bos>", "\n", "attn", ".", "append", "(", "self", ".", "attn", "[", "j", "]", "[", "k", "]", ")", "# since it does not has attn for bos, it will also iterate from the attn for <eos>", "\n", "# attn[j][k] Tensor with size [src_len]", "\n", "k", "=", "self", ".", "prev_ks", "[", "j", "]", "[", "k", "]", "# find the beam idx of the previous token", "\n", "\n", "# hyp[::-1]: a list of idx (zero dim tensor), with len = output sequence length", "\n", "# torch.stack(attn): FloatTensor, with size: [output sequence length, src_len]", "\n", "", "return", "hyp", "[", ":", ":", "-", "1", "]", ",", "torch", ".", "stack", "(", "attn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.Beam.advance": [[84, 166], ["word_logits.size", "len", "beam_scores.view", "beam_scores.view.topk", "beam.Beam.all_scores.append", "beam.Beam.prev_ks.append", "beam.Beam.next_ys.append", "beam.Beam.attn.append", "beam.Beam.global_scorer.update_global_state", "beam.Beam.update_eos_counter", "range", "range", "len", "range", "attn_dist.index_select", "beam.Beam.next_ys[].size", "beam.Beam.all_scores.append", "len", "beam.Beam.scores.unsqueeze().expand_as", "beam.Beam.next_ys[].size", "len", "range", "beam.Beam.next_ys[].size", "beam.Beam.get_hyp", "set", "range", "beam.Beam.global_scorer.score", "beam.Beam.finished.append", "beam.Beam.scores.unsqueeze", "set.add", "set", "tuple", "tuple", "len", "hyp[].item"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.GNMTGlobalScorer.update_global_state", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.Beam.update_eos_counter", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.Beam.get_hyp", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.attention.Attention.score"], ["", "def", "advance", "(", "self", ",", "word_logits", ",", "attn_dist", ")", ":", "\n", "        ", "\"\"\"\n        Given prob over words for every last beam `wordLk` and attention\n        `attn_out`: Compute and update the beam search.\n\n        Parameters:\n\n        * `word_logit`- probs of advancing from the last step [beam_size, vocab_size]\n        * `attn_dist`- attention at the last step [beam_size, src_len]\n\n        Returns: True if beam search is complete.\n        \"\"\"", "\n", "vocab_size", "=", "word_logits", ".", "size", "(", "1", ")", "\n", "# To be implemented: stepwise penalty", "\n", "\n", "# force the output to be longer than self.min_length", "\n", "cur_len", "=", "len", "(", "self", ".", "next_ys", ")", "\n", "if", "cur_len", "<", "self", ".", "min_length", ":", "\n", "            ", "for", "k", "in", "range", "(", "len", "(", "word_logits", ")", ")", ":", "\n", "                ", "word_logits", "[", "k", "]", "[", "self", ".", "_eos", "]", "=", "-", "1e20", "\n", "# Sum the previous scores", "\n", "", "", "if", "len", "(", "self", ".", "prev_ks", ")", ">", "0", ":", "\n", "            ", "beam_scores", "=", "word_logits", "+", "self", ".", "scores", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "word_logits", ")", "\n", "# Don't let EOS have children. If it have reached the max number of eos.", "\n", "for", "i", "in", "range", "(", "self", ".", "next_ys", "[", "-", "1", "]", ".", "size", "(", "0", ")", ")", ":", "\n", "                ", "if", "self", ".", "next_ys", "[", "-", "1", "]", "[", "i", "]", "==", "self", ".", "_eos", "and", "self", ".", "eos_counters", "[", "i", "]", ">=", "self", ".", "max_eos_per_output_seq", ":", "\n", "                    ", "beam_scores", "[", "i", "]", "=", "-", "1e20", "\n", "# To be implemented: block n-gram repeated", "\n", "", "", "if", "self", ".", "block_ngram_repeat", ">", "0", ":", "\n", "                ", "ngrams", "=", "[", "]", "\n", "le", "=", "len", "(", "self", ".", "next_ys", ")", "\n", "for", "j", "in", "range", "(", "self", ".", "next_ys", "[", "-", "1", "]", ".", "size", "(", "0", ")", ")", ":", "\n", "                    ", "hyp", ",", "_", "=", "self", ".", "get_hyp", "(", "le", "-", "1", ",", "j", ")", "\n", "ngrams", "=", "set", "(", ")", "\n", "fail", "=", "False", "\n", "gram", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "le", "-", "1", ")", ":", "\n", "# Last n tokens, n = block_ngram_repeat", "\n", "                        ", "gram", "=", "(", "gram", "+", "\n", "[", "hyp", "[", "i", "]", ".", "item", "(", ")", "]", ")", "[", "-", "self", ".", "block_ngram_repeat", ":", "]", "\n", "# Skip the blocking if it is in the exclusion list", "\n", "if", "set", "(", "gram", ")", "&", "self", ".", "exclusion_tokens", ":", "\n", "                            ", "continue", "\n", "", "if", "tuple", "(", "gram", ")", "in", "ngrams", ":", "\n", "                            ", "fail", "=", "True", "\n", "", "ngrams", ".", "add", "(", "tuple", "(", "gram", ")", ")", "\n", "", "if", "fail", ":", "\n", "                        ", "beam_scores", "[", "j", "]", "=", "-", "10e20", "\n", "\n", "", "", "", "", "else", ":", "# This is the first decoding step, every beam are the same", "\n", "            ", "beam_scores", "=", "word_logits", "[", "0", "]", "\n", "", "flat_beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "best_scores", ",", "best_scores_idx", "=", "flat_beam_scores", ".", "topk", "(", "self", ".", "size", ",", "0", ",", "True", ",", "True", ")", "# [beam_size]", "\n", "\n", "self", ".", "all_scores", ".", "append", "(", "self", ".", "scores", ")", "# list of tensor with size [beam_size]", "\n", "self", ".", "scores", "=", "best_scores", "\n", "\n", "# best_scores_idx indicate the idx in the flattened beam * vocab_size array, so need to convert", "\n", "# the idx back to which beam and word each score came from.", "\n", "prev_k", "=", "best_scores_idx", "/", "vocab_size", "# convert it to the beam indices that the top k scores came from, LongTensor, size: [beam_size]", "\n", "self", ".", "prev_ks", ".", "append", "(", "prev_k", ")", "\n", "self", ".", "next_ys", ".", "append", "(", "(", "best_scores_idx", "-", "prev_k", "*", "vocab_size", ")", ")", "# convert it to the vocab indices, LongTensor, size: [beam_size]", "\n", "self", ".", "attn", ".", "append", "(", "attn_dist", ".", "index_select", "(", "0", ",", "prev_k", ")", ")", "# select the attention dist from the corresponding beam, size: [beam_size, src_len]", "\n", "self", ".", "global_scorer", ".", "update_global_state", "(", "self", ")", "# update coverage vector, previous coverage penalty, and cov_total", "\n", "self", ".", "update_eos_counter", "(", ")", "# update the eos_counter according to prev_ks", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "next_ys", "[", "-", "1", "]", ".", "size", "(", "0", ")", ")", ":", "# For each generated token in the current step, check if it is EOS", "\n", "            ", "if", "self", ".", "next_ys", "[", "-", "1", "]", "[", "i", "]", "==", "self", ".", "_eos", ":", "\n", "                ", "self", ".", "eos_counters", "[", "i", "]", "+=", "1", "\n", "if", "self", ".", "eos_counters", "[", "i", "]", "==", "self", ".", "max_eos_per_output_seq", ":", "# compute the score penalize by length and coverage amd append add it to finished", "\n", "                    ", "global_scores", "=", "self", ".", "global_scorer", ".", "score", "(", "self", ",", "self", ".", "scores", ")", "\n", "s", "=", "global_scores", "[", "i", "]", "\n", "self", ".", "finished", ".", "append", "(", "(", "s", ",", "len", "(", "self", ".", "next_ys", ")", "-", "1", ",", "i", ")", ")", "# penalized score, length of sequence, beam_idx", "\n", "", "", "\"\"\"\n            elif self.next_ys[-1][i] == self._unk:  # if it is unk, replace it with the w\n                _, max_attn_score_idx = self.attn[-1][i].max(0)\n                self.next_ys[-1][i] = max_attn_score_idx\n            \"\"\"", "\n", "# End condition is when top-of-beam is EOS (and its number of EOS tokens reached the max) and no global score.", "\n", "", "if", "self", ".", "next_ys", "[", "-", "1", "]", "[", "0", "]", "==", "self", ".", "_eos", "and", "self", ".", "eos_counters", "[", "0", "]", "==", "self", ".", "max_eos_per_output_seq", ":", "\n", "            ", "self", ".", "all_scores", ".", "append", "(", "self", ".", "scores", ")", "\n", "self", ".", "eos_top", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.Beam.sort_finished": [[167, 181], ["beam.Beam.finished.sort", "len", "beam.Beam.global_scorer.score", "beam.Beam.finished.append", "len"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.attention.Attention.score"], ["", "", "def", "sort_finished", "(", "self", ",", "minimum", "=", "None", ")", ":", "\n", "        ", "if", "minimum", "is", "not", "None", ":", "\n", "            ", "i", "=", "0", "\n", "# Add from beam until we have minimum outputs in the finished list", "\n", "while", "len", "(", "self", ".", "finished", ")", "<", "minimum", ":", "\n", "                ", "global_scores", "=", "self", ".", "global_scorer", ".", "score", "(", "self", ",", "self", ".", "scores", ")", "\n", "s", "=", "global_scores", "[", "i", "]", "\n", "self", ".", "finished", ".", "append", "(", "(", "s", ",", "len", "(", "self", ".", "next_ys", ")", "-", "1", ",", "i", ")", ")", "# score, length of sequence (include eos but not bos), beam_idx", "\n", "i", "+=", "1", "\n", "\n", "", "", "self", ".", "finished", ".", "sort", "(", "key", "=", "lambda", "a", ":", "-", "a", "[", "0", "]", ")", "\n", "scores", "=", "[", "sc", "for", "sc", ",", "_", ",", "_", "in", "self", ".", "finished", "]", "\n", "ks", "=", "[", "(", "t", ",", "k", ")", "for", "_", ",", "t", ",", "k", "in", "self", ".", "finished", "]", "\n", "return", "scores", ",", "ks", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.Beam.update_eos_counter": [[182, 185], ["beam.Beam.eos_counters.index_select"], "methods", ["None"], ["", "def", "update_eos_counter", "(", "self", ")", ":", "\n", "# update the eos_counter according to prev_ks", "\n", "        ", "self", ".", "eos_counters", "=", "self", ".", "eos_counters", ".", "index_select", "(", "0", ",", "self", ".", "prev_ks", "[", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.GNMTGlobalScorer.__init__": [[197, 205], ["penalties.PenaltyBuilder", "penalties.PenaltyBuilder.coverage_penalty", "penalties.PenaltyBuilder.length_penalty"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.penalties.PenaltyBuilder.coverage_penalty", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.penalties.PenaltyBuilder.length_penalty"], ["def", "__init__", "(", "self", ",", "alpha", ",", "beta", ",", "cov_penalty", ",", "length_penalty", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "beta", "=", "beta", "\n", "penalty_builder", "=", "penalties", ".", "PenaltyBuilder", "(", "cov_penalty", ",", "length_penalty", ")", "\n", "# Term will be subtracted from probability", "\n", "self", ".", "cov_penalty", "=", "penalty_builder", ".", "coverage_penalty", "(", ")", "\n", "# Probability will be divided by this", "\n", "self", ".", "length_penalty", "=", "penalty_builder", ".", "length_penalty", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.GNMTGlobalScorer.score": [[206, 221], ["beam.GNMTGlobalScorer.length_penalty", "beam.GNMTGlobalScorer.cov_penalty"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.penalties.PenaltyBuilder.length_penalty"], ["", "def", "score", "(", "self", ",", "beam", ",", "logprobs", ")", ":", "\n", "        ", "\"\"\"\n        Rescores all the prediction scores of a beam based on penalty functions\n        Return: normalized_probs, size: [beam_size]\n        \"\"\"", "\n", "normalized_probs", "=", "self", ".", "length_penalty", "(", "beam", ",", "\n", "logprobs", ",", "\n", "self", ".", "alpha", ")", "\n", "if", "not", "beam", ".", "stepwise_penalty", ":", "\n", "            ", "penalty", "=", "self", ".", "cov_penalty", "(", "beam", ",", "\n", "beam", ".", "global_state", "[", "\"coverage\"", "]", ",", "\n", "self", ".", "beta", ")", "\n", "normalized_probs", "-=", "penalty", "\n", "\n", "", "return", "normalized_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.GNMTGlobalScorer.update_score": [[222, 232], ["beam.global_state.keys", "beam.scores.add_", "beam.GNMTGlobalScorer.cov_penalty", "beam.scores.sub_"], "methods", ["None"], ["", "def", "update_score", "(", "self", ",", "beam", ",", "attn", ")", ":", "\n", "        ", "\"\"\"\n        Function to update scores of a Beam that is not finished\n        \"\"\"", "\n", "if", "\"prev_penalty\"", "in", "beam", ".", "global_state", ".", "keys", "(", ")", ":", "\n", "            ", "beam", ".", "scores", ".", "add_", "(", "beam", ".", "global_state", "[", "\"prev_penalty\"", "]", ")", "\n", "penalty", "=", "self", ".", "cov_penalty", "(", "beam", ",", "\n", "beam", ".", "global_state", "[", "\"coverage\"", "]", "+", "attn", ",", "\n", "self", ".", "beta", ")", "\n", "beam", ".", "scores", ".", "sub_", "(", "penalty", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.GNMTGlobalScorer.update_global_state": [[233, 251], ["len", "beam.scores.clone().fill_", "beam.attn[].sum", "torch.min().sum", "beam.global_state[].index_select().add", "beam.GNMTGlobalScorer.cov_penalty", "beam.scores.clone", "torch.min", "beam.global_state[].index_select"], "methods", ["None"], ["", "", "def", "update_global_state", "(", "self", ",", "beam", ")", ":", "\n", "        ", "\"\"\"\n        Keeps the coverage vector as sum of attentions\n        \"\"\"", "\n", "if", "len", "(", "beam", ".", "prev_ks", ")", "==", "1", ":", "\n", "            ", "beam", ".", "global_state", "[", "\"prev_penalty\"", "]", "=", "beam", ".", "scores", ".", "clone", "(", ")", ".", "fill_", "(", "0.0", ")", "# [beam_size]", "\n", "beam", ".", "global_state", "[", "\"coverage\"", "]", "=", "beam", ".", "attn", "[", "-", "1", "]", "# [beam_size, src_len]", "\n", "self", ".", "cov_total", "=", "beam", ".", "attn", "[", "-", "1", "]", ".", "sum", "(", "1", ")", "# [beam_size], accumulate the penalty term for coverage", "\n", "", "else", ":", "\n", "            ", "self", ".", "cov_total", "+=", "torch", ".", "min", "(", "beam", ".", "attn", "[", "-", "1", "]", ",", "\n", "beam", ".", "global_state", "[", "'coverage'", "]", ")", ".", "sum", "(", "1", ")", "\n", "beam", ".", "global_state", "[", "\"coverage\"", "]", "=", "beam", ".", "global_state", "[", "\"coverage\"", "]", ".", "index_select", "(", "0", ",", "beam", ".", "prev_ks", "[", "-", "1", "]", ")", ".", "add", "(", "beam", ".", "attn", "[", "-", "1", "]", ")", "# accumulate coverage vector", "\n", "\n", "prev_penalty", "=", "self", ".", "cov_penalty", "(", "beam", ",", "\n", "beam", ".", "global_state", "[", "\"coverage\"", "]", ",", "\n", "self", ".", "beta", ")", "\n", "beam", ".", "global_state", "[", "\"prev_penalty\"", "]", "=", "prev_penalty", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate.evaluate_loss": [[21, 78], ["model.eval", "utils.statistics.LossStatistics", "torch.no_grad", "enumerate", "max", "src.to.size", "src.to.to", "src_mask.to.to", "trg.to.to", "trg_mask.to.to", "src_oov.to.to", "trg_oov.to.to", "time.time", "utils.time_log.time_since", "time.time", "utils.time_log.time_since", "pykp.masked_loss.masked_cross_entropy.item", "sum", "title.to.to", "title_mask.to.to", "model", "model", "pykp.masked_loss.masked_cross_entropy", "pykp.masked_loss.masked_cross_entropy", "len", "len"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.masked_cross_entropy", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.masked_cross_entropy"], ["def", "evaluate_loss", "(", "data_loader", ",", "model", ",", "opt", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "evaluation_loss_sum", "=", "0.0", "\n", "total_trg_tokens", "=", "0", "\n", "n_batch", "=", "0", "\n", "loss_compute_time_total", "=", "0.0", "\n", "forward_time_total", "=", "0.0", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch_i", ",", "batch", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "if", "not", "opt", ".", "one2many", ":", "# load one2one dataset", "\n", "                ", "src", ",", "src_lens", ",", "src_mask", ",", "trg", ",", "trg_lens", ",", "trg_mask", ",", "src_oov", ",", "trg_oov", ",", "oov_lists", ",", "title", ",", "title_oov", ",", "title_lens", ",", "title_mask", "=", "batch", "\n", "", "else", ":", "# load one2many dataset", "\n", "                ", "src", ",", "src_lens", ",", "src_mask", ",", "src_oov", ",", "oov_lists", ",", "src_str_list", ",", "trg_str_2dlist", ",", "trg", ",", "trg_oov", ",", "trg_lens", ",", "trg_mask", ",", "_", ",", "title", ",", "title_oov", ",", "title_lens", ",", "title_mask", "=", "batch", "\n", "num_trgs", "=", "[", "len", "(", "trg_str_list", ")", "for", "trg_str_list", "in", "\n", "trg_str_2dlist", "]", "# a list of num of targets in each batch, with len=batch_size", "\n", "\n", "", "max_num_oov", "=", "max", "(", "[", "len", "(", "oov", ")", "for", "oov", "in", "oov_lists", "]", ")", "# max number of oov for each batch", "\n", "\n", "batch_size", "=", "src", ".", "size", "(", "0", ")", "\n", "n_batch", "+=", "batch_size", "\n", "\n", "# move data to GPU if available", "\n", "src", "=", "src", ".", "to", "(", "opt", ".", "device", ")", "\n", "src_mask", "=", "src_mask", ".", "to", "(", "opt", ".", "device", ")", "\n", "trg", "=", "trg", ".", "to", "(", "opt", ".", "device", ")", "\n", "trg_mask", "=", "trg_mask", ".", "to", "(", "opt", ".", "device", ")", "\n", "src_oov", "=", "src_oov", ".", "to", "(", "opt", ".", "device", ")", "\n", "trg_oov", "=", "trg_oov", ".", "to", "(", "opt", ".", "device", ")", "\n", "if", "opt", ".", "title_guided", ":", "\n", "                ", "title", "=", "title", ".", "to", "(", "opt", ".", "device", ")", "\n", "title_mask", "=", "title_mask", ".", "to", "(", "opt", ".", "device", ")", "\n", "# title_oov = title_oov.to(opt.device)", "\n", "\n", "", "start_time", "=", "time", ".", "time", "(", ")", "\n", "if", "not", "opt", ".", "one2many", ":", "\n", "                ", "decoder_dist", ",", "h_t", ",", "attention_dist", ",", "encoder_final_state", ",", "coverage", ",", "_", ",", "_", ",", "_", "=", "model", "(", "src", ",", "src_lens", ",", "trg", ",", "src_oov", ",", "max_num_oov", ",", "src_mask", ",", "title", "=", "title", ",", "title_lens", "=", "title_lens", ",", "title_mask", "=", "title_mask", ")", "\n", "", "else", ":", "\n", "                ", "decoder_dist", ",", "h_t", ",", "attention_dist", ",", "encoder_final_state", ",", "coverage", ",", "_", ",", "_", ",", "_", "=", "model", "(", "src", ",", "src_lens", ",", "trg", ",", "src_oov", ",", "max_num_oov", ",", "src_mask", ",", "num_trgs", ",", "title", "=", "title", ",", "title_lens", "=", "title_lens", ",", "title_mask", "=", "title_mask", ")", "\n", "", "forward_time", "=", "time_since", "(", "start_time", ")", "\n", "forward_time_total", "+=", "forward_time", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "if", "opt", ".", "copy_attention", ":", "# Compute the loss using target with oov words", "\n", "                ", "loss", "=", "masked_cross_entropy", "(", "decoder_dist", ",", "trg_oov", ",", "trg_mask", ",", "trg_lens", ",", "\n", "opt", ".", "coverage_attn", ",", "coverage", ",", "attention_dist", ",", "opt", ".", "lambda_coverage", ",", "coverage_loss", "=", "False", ")", "\n", "", "else", ":", "# Compute the loss using target without oov words", "\n", "                ", "loss", "=", "masked_cross_entropy", "(", "decoder_dist", ",", "trg", ",", "trg_mask", ",", "trg_lens", ",", "\n", "opt", ".", "coverage_attn", ",", "coverage", ",", "attention_dist", ",", "opt", ".", "lambda_coverage", ",", "coverage_loss", "=", "False", ")", "\n", "", "loss_compute_time", "=", "time_since", "(", "start_time", ")", "\n", "loss_compute_time_total", "+=", "loss_compute_time", "\n", "\n", "evaluation_loss_sum", "+=", "loss", ".", "item", "(", ")", "\n", "total_trg_tokens", "+=", "sum", "(", "trg_lens", ")", "\n", "\n", "", "", "eval_loss_stat", "=", "LossStatistics", "(", "evaluation_loss_sum", ",", "total_trg_tokens", ",", "n_batch", ",", "forward_time", "=", "forward_time_total", ",", "loss_compute_time", "=", "loss_compute_time_total", ")", "\n", "return", "eval_loss_stat", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate.evaluate_reward": [[79, 141], ["generator.model.eval", "utils.statistics.RewardStatistics", "torch.no_grad", "enumerate", "src.to.size", "src.to.to", "src_mask.to.to", "src_oov.to.to", "time.time", "generator.sample", "pykp.reward.sample_list_to_str_2dlist", "utils.time_log.time_since", "pykp.reward.compute_batch_reward", "pykp.reward.compute_batch_reward.sum", "len", "title.to.to", "title_mask.to.to"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.sample", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.sample_list_to_str_2dlist", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_batch_reward"], ["", "def", "evaluate_reward", "(", "data_loader", ",", "generator", ",", "opt", ",", "bert", "=", "None", ")", ":", "\n", "    ", "\"\"\"Return the avg. reward in the validation dataset\"\"\"", "\n", "generator", ".", "model", ".", "eval", "(", ")", "\n", "final_reward_sum", "=", "0.0", "\n", "n_batch", "=", "0", "\n", "sample_time_total", "=", "0.0", "\n", "topk", "=", "opt", ".", "topk", "\n", "reward_type", "=", "opt", ".", "reward_type", "\n", "#reward_type = 7", "\n", "match_type", "=", "opt", ".", "match_type", "\n", "eos_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "EOS_WORD", "]", "\n", "delimiter_word", "=", "opt", ".", "delimiter_word", "\n", "one2many", "=", "opt", ".", "one2many", "\n", "one2many_mode", "=", "opt", ".", "one2many_mode", "\n", "if", "one2many", "and", "one2many_mode", ">", "1", ":", "\n", "        ", "num_predictions", "=", "opt", ".", "num_predictions", "\n", "", "else", ":", "\n", "        ", "num_predictions", "=", "1", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch_i", ",", "batch", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "# load one2many dataset", "\n", "            ", "src", ",", "src_lens", ",", "src_mask", ",", "src_oov", ",", "oov_lists", ",", "src_str_list", ",", "trg_str_2dlist", ",", "trg", ",", "trg_oov", ",", "trg_lens", ",", "trg_mask", ",", "_", ",", "title", ",", "title_oov", ",", "title_lens", ",", "title_mask", "=", "batch", "\n", "num_trgs", "=", "[", "len", "(", "trg_str_list", ")", "for", "trg_str_list", "in", "\n", "trg_str_2dlist", "]", "# a list of num of targets in each batch, with len=batch_size", "\n", "\n", "batch_size", "=", "src", ".", "size", "(", "0", ")", "\n", "n_batch", "+=", "batch_size", "\n", "\n", "# move data to GPU if available", "\n", "src", "=", "src", ".", "to", "(", "opt", ".", "device", ")", "\n", "src_mask", "=", "src_mask", ".", "to", "(", "opt", ".", "device", ")", "\n", "src_oov", "=", "src_oov", ".", "to", "(", "opt", ".", "device", ")", "\n", "#trg = trg.to(opt.device)", "\n", "#trg_mask = trg_mask.to(opt.device)", "\n", "#trg_oov = trg_oov.to(opt.device)", "\n", "if", "opt", ".", "title_guided", ":", "\n", "                ", "title", "=", "title", ".", "to", "(", "opt", ".", "device", ")", "\n", "title_mask", "=", "title_mask", ".", "to", "(", "opt", ".", "device", ")", "\n", "# title_oov = title_oov.to(opt.device)", "\n", "\n", "", "start_time", "=", "time", ".", "time", "(", ")", "\n", "# sample a sequence", "\n", "# sample_list is a list of dict, {\"prediction\": [], \"scores\": [], \"attention\": [], \"done\": True}, preidiction is a list of 0 dim tensors", "\n", "sample_list", ",", "log_selected_token_dist", ",", "output_mask", ",", "pred_idx_mask", ",", "_", ",", "_", ",", "_", "=", "generator", ".", "sample", "(", "\n", "src", ",", "src_lens", ",", "src_oov", ",", "src_mask", ",", "oov_lists", ",", "opt", ".", "max_length", ",", "greedy", "=", "True", ",", "one2many", "=", "one2many", ",", "\n", "one2many_mode", "=", "one2many_mode", ",", "num_predictions", "=", "num_predictions", ",", "perturb_std", "=", "0", ",", "title", "=", "title", ",", "title_lens", "=", "title_lens", ",", "title_mask", "=", "title_mask", ")", "\n", "#pred_str_2dlist = sample_list_to_str_2dlist(sample_list, oov_lists, opt.idx2word, opt.vocab_size, eos_idx, delimiter_word)", "\n", "pred_str_2dlist", "=", "sample_list_to_str_2dlist", "(", "sample_list", ",", "oov_lists", ",", "opt", ".", "idx2word", ",", "opt", ".", "vocab_size", ",", "eos_idx", ",", "\n", "delimiter_word", ",", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "UNK_WORD", "]", ",", "opt", ".", "replace_unk", ",", "\n", "src_str_list", ")", "\n", "#print(pred_str_2dlist)", "\n", "sample_time", "=", "time_since", "(", "start_time", ")", "\n", "sample_time_total", "+=", "sample_time", "\n", "\n", "final_reward", "=", "compute_batch_reward", "(", "pred_str_2dlist", ",", "trg_str_2dlist", ",", "batch_size", ",", "reward_type", ",", "topk", ",", "match_type", ",", "regularization_factor", "=", "0.0", ",", "bert", "=", "bert", ")", "# np.array, [batch_size]", "\n", "\n", "final_reward_sum", "+=", "final_reward", ".", "sum", "(", "0", ")", "\n", "\n", "", "", "eval_reward_stat", "=", "RewardStatistics", "(", "final_reward_sum", ",", "pg_loss", "=", "0", ",", "n_batch", "=", "n_batch", ",", "sample_time", "=", "sample_time_total", ")", "\n", "\n", "return", "eval_reward_stat", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate.preprocess_beam_search_result": [[292, 312], ["zip", "len", "zip", "pred_list.append", "utils.string_helper.prediction_to_sentence", "sentences_n_best.append"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.prediction_to_sentence"], ["def", "preprocess_beam_search_result", "(", "beam_search_result", ",", "idx2word", ",", "vocab_size", ",", "oov_lists", ",", "eos_idx", ",", "unk_idx", ",", "replace_unk", ",", "src_str_list", ")", ":", "\n", "    ", "batch_size", "=", "beam_search_result", "[", "'batch_size'", "]", "\n", "predictions", "=", "beam_search_result", "[", "'predictions'", "]", "\n", "scores", "=", "beam_search_result", "[", "'scores'", "]", "\n", "attention", "=", "beam_search_result", "[", "'attention'", "]", "\n", "assert", "len", "(", "predictions", ")", "==", "batch_size", "\n", "pred_list", "=", "[", "]", "# a list of dict, with len = batch_size", "\n", "for", "pred_n_best", ",", "score_n_best", ",", "attn_n_best", ",", "oov", ",", "src_word_list", "in", "zip", "(", "predictions", ",", "scores", ",", "attention", ",", "oov_lists", ",", "src_str_list", ")", ":", "\n", "# attn_n_best: list of tensor with size [trg_len, src_len], len=n_best", "\n", "        ", "pred_dict", "=", "{", "}", "\n", "sentences_n_best", "=", "[", "]", "\n", "for", "pred", ",", "attn", "in", "zip", "(", "pred_n_best", ",", "attn_n_best", ")", ":", "\n", "            ", "sentence", "=", "prediction_to_sentence", "(", "pred", ",", "idx2word", ",", "vocab_size", ",", "oov", ",", "eos_idx", ",", "unk_idx", ",", "replace_unk", ",", "src_word_list", ",", "attn", ")", "\n", "#sentence = [idx2word[int(idx.item())] if int(idx.item()) < vocab_size else oov[int(idx.item())-vocab_size] for idx in pred[:-1]]", "\n", "sentences_n_best", ".", "append", "(", "sentence", ")", "\n", "", "pred_dict", "[", "'sentences'", "]", "=", "sentences_n_best", "# a list of list of word, with len [n_best, out_seq_len], does not include tbe final <EOS>", "\n", "pred_dict", "[", "'scores'", "]", "=", "score_n_best", "# a list of zero dim tensor, with len [n_best]", "\n", "pred_dict", "[", "'attention'", "]", "=", "attn_n_best", "# a list of FloatTensor[output sequence length, src_len], with len = [n_best]", "\n", "pred_list", ".", "append", "(", "pred_dict", ")", "\n", "", "return", "pred_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate.evaluate_beam_search": [[314, 386], ["open.close", "print", "open", "open", "torch.no_grad", "time.time", "enumerate", "os.path.join", "os.path.join", "src.to.to", "src_mask.to.to", "src_oov.to.to", "generator.beam_search", "evaluate.preprocess_beam_search_result", "sorted", "zip", "zip", "print", "sys.stdout.flush", "time.time", "title.to.to", "title_mask.to.to", "zip", "enumerate", "open.write", "utils.string_helper.split_word_list_by_delimiter", "utils.time_log.time_since", "len"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sequence_generator.SequenceGenerator.beam_search", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate.preprocess_beam_search_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.split_word_list_by_delimiter", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since"], ["", "def", "evaluate_beam_search", "(", "generator", ",", "one2many_data_loader", ",", "opt", ",", "delimiter_word", "=", "'<sep>'", ")", ":", "\n", "#score_dict_all = defaultdict(list)  # {'precision@5':[],'recall@5':[],'f1_score@5':[],'num_matches@5':[],'precision@10':[],'recall@10':[],'f1score@10':[],'num_matches@10':[]}", "\n", "# file for storing the predicted keyphrases", "\n", "    ", "if", "opt", ".", "pred_file_prefix", "==", "\"\"", ":", "\n", "        ", "pred_output_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "pred_path", ",", "\"predictions.txt\"", ")", ",", "\"w\"", ")", "\n", "", "else", ":", "\n", "        ", "pred_output_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "pred_path", ",", "\"%s_predictions.txt\"", "%", "opt", ".", "pred_file_prefix", ")", ",", "\"w\"", ")", "\n", "# debug", "\n", "", "interval", "=", "1000", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "for", "batch_i", ",", "batch", "in", "enumerate", "(", "one2many_data_loader", ")", ":", "\n", "            ", "if", "(", "batch_i", "+", "1", ")", "%", "interval", "==", "0", ":", "\n", "                ", "print", "(", "\"Batch %d: Time for running beam search on %d batches : %.1f\"", "%", "(", "batch_i", "+", "1", ",", "interval", ",", "time_since", "(", "start_time", ")", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "", "src", ",", "src_lens", ",", "src_mask", ",", "src_oov", ",", "oov_lists", ",", "src_str_list", ",", "trg_str_2dlist", ",", "_", ",", "_", ",", "_", ",", "_", ",", "original_idx_list", ",", "title", ",", "title_oov", ",", "title_lens", ",", "title_mask", "=", "batch", "\n", "\"\"\"\n            src: a LongTensor containing the word indices of source sentences, [batch, src_seq_len], with oov words replaced by unk idx\n            src_lens: a list containing the length of src sequences for each batch, with len=batch\n            src_mask: a FloatTensor, [batch, src_seq_len]\n            src_oov: a LongTensor containing the word indices of source sentences, [batch, src_seq_len], contains the index of oov words (used by copy)\n            oov_lists: a list of oov words for each src, 2dlist\n            \"\"\"", "\n", "src", "=", "src", ".", "to", "(", "opt", ".", "device", ")", "\n", "src_mask", "=", "src_mask", ".", "to", "(", "opt", ".", "device", ")", "\n", "src_oov", "=", "src_oov", ".", "to", "(", "opt", ".", "device", ")", "\n", "if", "opt", ".", "title_guided", ":", "\n", "                ", "title", "=", "title", ".", "to", "(", "opt", ".", "device", ")", "\n", "title_mask", "=", "title_mask", ".", "to", "(", "opt", ".", "device", ")", "\n", "# title_oov = title_oov.to(opt.device)", "\n", "\n", "\n", "", "beam_search_result", "=", "generator", ".", "beam_search", "(", "src", ",", "src_lens", ",", "src_oov", ",", "src_mask", ",", "oov_lists", ",", "opt", ".", "word2idx", ",", "opt", ".", "max_eos_per_output_seq", ",", "title", "=", "title", ",", "title_lens", "=", "title_lens", ",", "title_mask", "=", "title_mask", ")", "\n", "pred_list", "=", "preprocess_beam_search_result", "(", "beam_search_result", ",", "opt", ".", "idx2word", ",", "opt", ".", "vocab_size", ",", "oov_lists", ",", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "EOS_WORD", "]", ",", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "UNK_WORD", "]", ",", "opt", ".", "replace_unk", ",", "src_str_list", ")", "\n", "# list of {\"sentences\": [], \"scores\": [], \"attention\": []}", "\n", "\n", "# recover the original order in the dataset", "\n", "seq_pairs", "=", "sorted", "(", "zip", "(", "original_idx_list", ",", "src_str_list", ",", "trg_str_2dlist", ",", "pred_list", ",", "oov_lists", ")", ",", "\n", "key", "=", "lambda", "p", ":", "p", "[", "0", "]", ")", "\n", "original_idx_list", ",", "src_str_list", ",", "trg_str_2dlist", ",", "pred_list", ",", "oov_lists", "=", "zip", "(", "*", "seq_pairs", ")", "\n", "\n", "# Process every src in the batch", "\n", "for", "src_str", ",", "trg_str_list", ",", "pred", ",", "oov", "in", "zip", "(", "src_str_list", ",", "trg_str_2dlist", ",", "pred_list", ",", "oov_lists", ")", ":", "\n", "# src_str: a list of words; trg_str: a list of keyphrases, each keyphrase is a list of words", "\n", "# pred_seq_list: a list of sequence objects, sorted by scores", "\n", "# oov: a list of oov words", "\n", "                ", "pred_str_list", "=", "pred", "[", "'sentences'", "]", "# predicted sentences from a single src, a list of list of word, with len=[beam_size, out_seq_len], does not include the final <EOS>", "\n", "pred_score_list", "=", "pred", "[", "'scores'", "]", "\n", "pred_attn_list", "=", "pred", "[", "'attention'", "]", "# a list of FloatTensor[output sequence length, src_len], with len = [n_best]", "\n", "\n", "if", "opt", ".", "one2many", ":", "\n", "                    ", "all_keyphrase_list", "=", "[", "]", "# a list of word list contains all the keyphrases in the top max_n sequences decoded by beam search", "\n", "for", "word_list", "in", "pred_str_list", ":", "\n", "                        ", "all_keyphrase_list", "+=", "split_word_list_by_delimiter", "(", "word_list", ",", "delimiter_word", ",", "opt", ".", "separate_present_absent", ",", "pykp", ".", "io", ".", "PEOS_WORD", ")", "\n", "#not_duplicate_mask = check_duplicate_keyphrases(all_keyphrase_list)", "\n", "#pred_str_list = [word_list for word_list, is_keep in zip(all_keyphrase_list, not_duplicate_mask) if is_keep]", "\n", "", "pred_str_list", "=", "all_keyphrase_list", "\n", "\n", "# output the predicted keyphrases to a file", "\n", "", "pred_print_out", "=", "''", "\n", "for", "word_list_i", ",", "word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "                    ", "if", "word_list_i", "<", "len", "(", "pred_str_list", ")", "-", "1", ":", "\n", "                        ", "pred_print_out", "+=", "'%s;'", "%", "' '", ".", "join", "(", "word_list", ")", "\n", "", "else", ":", "\n", "                        ", "pred_print_out", "+=", "'%s'", "%", "' '", ".", "join", "(", "word_list", ")", "\n", "", "", "pred_print_out", "+=", "'\\n'", "\n", "pred_output_file", ".", "write", "(", "pred_print_out", ")", "\n", "\n", "", "", "", "pred_output_file", ".", "close", "(", ")", "\n", "print", "(", "\"done!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.filter_duplications.filter_dups": [[6, 83], ["open", "open.readlines", "open", "open.readlines", "set", "print", "tqdm.tqdm", "print", "sorted", "open", "open.writelines", "open", "open.writelines", "open", "open.write", "open.write", "len", "len", "os.path.split", "os.path.splitext", "os.path.split", "os.path.splitext", "set", "open", "total_filtered_idx_set.union.union", "print", "range", "list", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "line.strip.strip", "line.strip.split", "dups.split", "int", "int", "len", "total_filtered_idx_set.union.add", "len", "str", "set.add", "len", "context_lines[].strip().split", "allkeys_lines[].strip().split", "len", "src_dup.strip().split", "filtered_dup.strip().split", "set.add", "str", "context_lines[].strip", "allkeys_lines[].strip", "src_dup.strip", "filtered_dup.strip"], "function", ["None"], ["def", "filter_dups", "(", "saved_home", ",", "dups_info_home", ",", "context_file_path", ",", "keyword_file_path", ")", ":", "\n", "    ", "\"\"\"\n    filter out the duplicates in the training data with the testing data according to the obtained duplication info file.\n    :param saved_home: non-filtered data home\n    :param dups_info_home: duplication information home\n    :return: None\n    \"\"\"", "\n", "#orig_context_file = open(os.path.join(saved_home, 'data_for_corenlp', 'kp20k_training_context_for_corenlp.txt'),", "\n", "#                         encoding='utf-8')", "\n", "#context_lines = orig_context_file.readlines()", "\n", "#orig_allkeys_file = open(os.path.join(saved_home, 'data_for_corenlp', 'kp20k_training_keyword_for_corenlp.txt'),", "\n", "#                         encoding='utf-8')", "\n", "\n", "orig_context_file", "=", "open", "(", "context_file_path", ",", "encoding", "=", "'utf-8'", ")", "\n", "context_lines", "=", "orig_context_file", ".", "readlines", "(", ")", "\n", "orig_allkeys_file", "=", "open", "(", "keyword_file_path", ",", "encoding", "=", "'utf-8'", ")", "\n", "allkeys_lines", "=", "orig_allkeys_file", ".", "readlines", "(", ")", "\n", "assert", "len", "(", "context_lines", ")", "==", "len", "(", "allkeys_lines", ")", "\n", "\n", "context_file_name", "=", "os", ".", "path", ".", "split", "(", "context_file_path", ")", "[", "1", "]", "\n", "context_file_name", "=", "os", ".", "path", ".", "splitext", "(", "context_file_name", ")", "[", "0", "]", "\n", "filtered_context_file_name", "=", "\"{}_filtered\"", ".", "format", "(", "context_file_name", ")", "\n", "keyword_file_name", "=", "os", ".", "path", ".", "split", "(", "keyword_file_path", ")", "[", "1", "]", "\n", "keyword_file_name", "=", "os", ".", "path", ".", "splitext", "(", "keyword_file_name", ")", "[", "0", "]", "\n", "filtered_keyword_file_name", "=", "\"{}_filtered\"", ".", "format", "(", "keyword_file_name", ")", "\n", "\n", "# filter out the duplicates in the validation and the testing datasets and the kp20k training dataset itself", "\n", "dups_info_datasets", "=", "[", "'kp20k_training'", ",", "'kp20k_validation'", ",", "'kp20k_testing'", ",", "\n", "'inspec_testing'", ",", "'krapivin_testing'", ",", "\n", "'nus_testing'", ",", "'semeval_testing'", "]", "\n", "total_filtered_idx_set", "=", "set", "(", ")", "\n", "for", "dataset", "in", "dups_info_datasets", ":", "\n", "        ", "filtered_idx_set", "=", "set", "(", ")", "\n", "dups_info_file", "=", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "dups_info_home", ",", "'{}_context_nstpws_dups_w_kp20k_training.txt'", ".", "format", "(", "dataset", ")", ")", ",", "encoding", "=", "'utf-8'", ")", "\n", "for", "line", "in", "dups_info_file", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "# inspec_testing_48 kp20k_training_433051 jc_sc:0.7368; affine invariants of convex polygons | affine invariants of convex polygons", "\n", "dups", ",", "titles", "=", "line", ".", "split", "(", "';'", ")", "\n", "src_dup", ",", "filtered_dup", ",", "_", "=", "dups", ".", "split", "(", ")", "\n", "src_idx", "=", "int", "(", "src_dup", ".", "strip", "(", ")", ".", "split", "(", "'_'", ")", "[", "-", "1", "]", ")", "\n", "filtered_idx", "=", "int", "(", "filtered_dup", ".", "strip", "(", ")", ".", "split", "(", "'_'", ")", "[", "-", "1", "]", ")", "\n", "if", "dataset", "!=", "'kp20k_training'", ":", "\n", "                ", "filtered_idx_set", ".", "add", "(", "filtered_idx", ")", "\n", "", "else", ":", "\n", "                ", "if", "src_idx", "not", "in", "filtered_idx_set", ":", "\n", "                    ", "filtered_idx_set", ".", "add", "(", "filtered_idx", ")", "\n", "", "", "", "total_filtered_idx_set", "=", "total_filtered_idx_set", ".", "union", "(", "filtered_idx_set", ")", "\n", "print", "(", "'Num of filtered kp20k training data: {}'", ".", "format", "(", "len", "(", "total_filtered_idx_set", ")", ")", ")", "\n", "\n", "# also filter out the invalid data samples", "\n", "", "print", "(", "'Finding the invalid data samples in the original kp20k training ...'", ")", "\n", "for", "corpus_idx", "in", "tqdm", "(", "range", "(", "len", "(", "context_lines", ")", ")", ")", ":", "\n", "        ", "if", "context_lines", "[", "corpus_idx", "]", ".", "strip", "(", ")", ".", "split", "(", ")", "==", "[", "''", "]", "or", "allkeys_lines", "[", "corpus_idx", "]", ".", "strip", "(", ")", ".", "split", "(", "' ; '", ")", "==", "[", "''", "]", ":", "\n", "            ", "total_filtered_idx_set", ".", "add", "(", "corpus_idx", ")", "\n", "", "", "print", "(", "'Num of filtered kp20k training data: {}'", ".", "format", "(", "len", "(", "total_filtered_idx_set", ")", ")", ")", "\n", "\n", "total_filtered_idxes", "=", "sorted", "(", "list", "(", "total_filtered_idx_set", ")", ")", "\n", "for", "filter_idx", "in", "total_filtered_idxes", ":", "\n", "        ", "context_lines", "[", "filter_idx", "]", "=", "'\\n'", "\n", "allkeys_lines", "[", "filter_idx", "]", "=", "'\\n'", "\n", "\n", "", "filtered_context_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "saved_home", ",", "'data_for_corenlp'", ",", "\n", "'{}.txt'", ".", "format", "(", "filtered_context_file_name", ")", ")", ",", "\n", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "filtered_context_file", ".", "writelines", "(", "context_lines", ")", "\n", "\n", "filtered_allkeys_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "saved_home", ",", "'data_for_corenlp'", ",", "\n", "'{}.txt'", ".", "format", "(", "filtered_keyword_file_name", ")", ")", ",", "\n", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "filtered_allkeys_file", ".", "writelines", "(", "allkeys_lines", ")", "\n", "\n", "orig_context_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "saved_home", ",", "'data_for_corenlp'", ",", "\n", "'kp20k_training_filtered_for_corenlp_idxes.txt'", ")", ",", "\n", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "orig_context_file", ".", "write", "(", "' '", ".", "join", "(", "[", "str", "(", "idx", ")", "for", "idx", "in", "total_filtered_idxes", "]", ")", "+", "'\\n'", ")", "\n", "orig_context_file", ".", "write", "(", "str", "(", "len", "(", "total_filtered_idxes", ")", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.batch_check_present_idx_backup": [[23, 58], ["len", "numpy.zeros", "len", "enumerate", "numpy.ones", "joined_keyphrase_str.strip", "range", "enumerate", "len", "len"], "function", ["None"], ["def", "batch_check_present_idx_backup", "(", "src_str", ",", "keyphrase_str_list", ")", ":", "\n", "    ", "\"\"\"\n    :param src_str: stemmed word list of source text\n    :param keyphrase_str_list: stemmed list of word list\n    :return: an np array that stores the keyphrase's start idx in the src if it present in src. else, the value is len(src) +1\n    \"\"\"", "\n", "num_keyphrases", "=", "len", "(", "keyphrase_str_list", ")", "\n", "is_present", "=", "np", ".", "zeros", "(", "num_keyphrases", ",", "dtype", "=", "bool", ")", "\n", "src_len", "=", "len", "(", "src_str", ")", "\n", "num_present_keyphrases", "=", "0", "\n", "\n", "present_indices", "=", "np", ".", "ones", "(", "num_keyphrases", ")", "*", "(", "src_len", "+", "1", ")", "\n", "\n", "for", "i", ",", "keyphrase_word_list", "in", "enumerate", "(", "keyphrase_str_list", ")", ":", "\n", "        ", "joined_keyphrase_str", "=", "' '", ".", "join", "(", "keyphrase_word_list", ")", "\n", "if", "joined_keyphrase_str", ".", "strip", "(", ")", "==", "\"\"", ":", "# if the keyphrase is an empty string, treat it as absent", "\n", "            ", "present_indices", "[", "i", "]", "=", "src_len", "+", "1", "\n", "", "else", ":", "\n", "# check if it appears in source text", "\n", "            ", "match", "=", "False", "\n", "for", "src_start_idx", "in", "range", "(", "len", "(", "src_str", ")", "-", "len", "(", "keyphrase_word_list", ")", "+", "1", ")", ":", "\n", "                ", "match", "=", "True", "\n", "for", "keyphrase_i", ",", "keyphrase_w", "in", "enumerate", "(", "keyphrase_word_list", ")", ":", "\n", "                    ", "src_w", "=", "src_str", "[", "src_start_idx", "+", "keyphrase_i", "]", "\n", "if", "src_w", "!=", "keyphrase_w", ":", "\n", "                        ", "match", "=", "False", "\n", "break", "\n", "", "", "if", "match", ":", "\n", "                    ", "present_indices", "[", "i", "]", "=", "src_start_idx", "\n", "num_present_keyphrases", "+=", "1", "\n", "break", "\n", "", "", "if", "not", "match", ":", "\n", "                ", "present_indices", "[", "i", "]", "=", "src_len", "+", "1", "\n", "\n", "", "", "", "return", "present_indices", ",", "num_present_keyphrases", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.batch_check_present_idx": [[60, 78], ["len", "len", "enumerate", "numpy.ones", "integrated_data_preprocess.check_present_idx"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.check_present_idx"], ["", "def", "batch_check_present_idx", "(", "src_str", ",", "keyphrase_str_list", ")", ":", "\n", "    ", "\"\"\"\n    :param src_str: stemmed word list of source text\n    :param keyphrase_str_list: stemmed list of word list\n    :return: an np array that stores the keyphrase's start idx in the src if it present in src. else, the value is len(src) +1\n    \"\"\"", "\n", "num_keyphrases", "=", "len", "(", "keyphrase_str_list", ")", "\n", "src_len", "=", "len", "(", "src_str", ")", "\n", "num_present_keyphrases", "=", "0", "\n", "\n", "present_indices", "=", "np", ".", "ones", "(", "num_keyphrases", ")", "*", "(", "src_len", "+", "1", ")", "\n", "\n", "for", "i", ",", "keyphrase_word_list", "in", "enumerate", "(", "keyphrase_str_list", ")", ":", "\n", "        ", "present_indices", "[", "i", "]", ",", "is_present", "=", "check_present_idx", "(", "src_str", ",", "keyphrase_word_list", ")", "\n", "if", "is_present", ":", "\n", "            ", "num_present_keyphrases", "+=", "1", "\n", "\n", "", "", "return", "present_indices", ",", "num_present_keyphrases", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.check_present_idx": [[80, 101], ["len", "joined_keyphrase_str.strip", "range", "enumerate", "len", "len"], "function", ["None"], ["", "def", "check_present_idx", "(", "src_str", ",", "keyphrase_word_list", ")", ":", "\n", "    ", "src_len", "=", "len", "(", "src_str", ")", "\n", "joined_keyphrase_str", "=", "' '", ".", "join", "(", "keyphrase_word_list", ")", "\n", "if", "joined_keyphrase_str", ".", "strip", "(", ")", "==", "\"\"", ":", "# if the keyphrase is an empty string, treat it as absent", "\n", "        ", "return", "src_len", "+", "1", ",", "False", "\n", "", "else", ":", "\n", "# check if it appears in source text", "\n", "        ", "match", "=", "False", "\n", "for", "src_start_idx", "in", "range", "(", "len", "(", "src_str", ")", "-", "len", "(", "keyphrase_word_list", ")", "+", "1", ")", ":", "\n", "            ", "match", "=", "True", "\n", "for", "keyphrase_i", ",", "keyphrase_w", "in", "enumerate", "(", "keyphrase_word_list", ")", ":", "\n", "                ", "src_w", "=", "src_str", "[", "src_start_idx", "+", "keyphrase_i", "]", "\n", "if", "src_w", "!=", "keyphrase_w", ":", "\n", "                    ", "match", "=", "False", "\n", "break", "\n", "", "", "if", "match", ":", "\n", "                ", "present_index", "=", "src_start_idx", "\n", "break", "\n", "", "", "if", "not", "match", ":", "\n", "            ", "present_index", "=", "src_len", "+", "1", "\n", "", "", "return", "present_index", ",", "match", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.find_variations": [[103, 156], ["re.sub().strip", "keyphrase_variations.append", "integrated_data_preprocess.find_variations_from_wiki", "utils.string_helper.stem_str_list", "evaluate_prediction.check_duplicate_keyphrases", "re.findall", "integrated_data_preprocess.get_tokens", "keyphrase_variations.append", "len", "re.sub", "zip", "integrated_data_preprocess.get_tokens", "len", "len"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.find_variations_from_wiki", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_str_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_duplicate_keyphrases", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens"], ["", "def", "find_variations", "(", "keyphrase", ",", "src_tokens", ",", "fine_grad", ",", "limit_num", ",", "match_ending_parenthesis", ",", "use_corenlp", ",", "find_redirections", ")", ":", "\n", "    ", "\"\"\"\n    :param keyphrase: must be stripped\n    :param src_tokens: tokenized src, a list of words\n    :return: a list of keyphrase variations\n    \"\"\"", "\n", "\n", "extract_acronym_flag", "=", "False", "\n", "acronym_tokens", "=", "None", "\n", "\n", "if", "keyphrase", "==", "\"\"", ":", "\n", "        ", "return", "\"\"", "\n", "\n", "", "keyphrase_variations", "=", "[", "]", "\n", "# insert the acronym as one of the variations if there is a () place at the end of the keyphrase", "\n", "if", "keyphrase", "[", "-", "1", "]", "==", "')'", ":", "\n", "        ", "match_list", "=", "re", ".", "findall", "(", "'\\((.*?)\\)'", ",", "keyphrase", ")", "# match all str inside parenthesis", "\n", "if", "len", "(", "match_list", ")", ">", "0", ":", "\n", "            ", "acronym", "=", "match_list", "[", "-", "1", "]", "# the last match should be an acronym", "\n", "if", "match_ending_parenthesis", "and", "len", "(", "acronym", ")", ">", "1", ":", "\n", "#keyphrase_variations.append(get_tokens(acronym, fine_grad, use_corenlp))", "\n", "                ", "acronym_tokens", "=", "get_tokens", "(", "acronym", ",", "fine_grad", ",", "use_corenlp", ")", "\n", "extract_acronym_flag", "=", "True", "\n", "#else:", "\n", "#    acronym_tokens = None", "\n", "# remove the parenthesis and insert the keyphrase as one of the variations", "\n", "", "", "", "keyphrase_filtered", "=", "re", ".", "sub", "(", "r'\\(.*?\\)'", ",", "''", ",", "keyphrase", ")", ".", "strip", "(", ")", "\n", "\n", "# debug, if after filtering, keyphrase becomes empty:", "\n", "if", "keyphrase_filtered", "==", "\"\"", ":", "\n", "# If the keyphrase becomes empty after removing parenthesis, replace with the value inside the paraenthesis.", "\n", "        ", "keyphrase_filtered", "=", "acronym", "\n", "extract_acronym_flag", "=", "False", "\n", "acronym_tokens", "=", "None", "\n", "#print(\"Keyphrase becomes empty after removing parenthesis\")", "\n", "#print(\"From {} to {}.\".format(keyphrase, keyphrase_filtered))", "\n", "#print(keyphrase)", "\n", "#exit()", "\n", "\n", "", "keyphrase_variations", ".", "append", "(", "get_tokens", "(", "keyphrase_filtered", ",", "fine_grad", ",", "use_corenlp", ")", ")", "\n", "if", "acronym_tokens", "is", "not", "None", ":", "\n", "        ", "keyphrase_variations", ".", "append", "(", "acronym_tokens", ")", "\n", "# find variations from wikipedia, wiki_variations: a list of word list", "\n", "", "wiki_variations", ",", "num_matched_disambiguation", ",", "num_redirections_found", "=", "find_variations_from_wiki", "(", "keyphrase_filtered", ",", "src_tokens", ",", "fine_grad", ",", "use_corenlp", ",", "find_redirections", ")", "\n", "keyphrase_variations", "+=", "wiki_variations", "\n", "# remove duplicates", "\n", "# keyphrase_variations contains the original keyphrase, the text within a () in the original keyphrase if any, and the variations from wiki", "\n", "# we need to remove duplicate variations, i.e., we only keep the variations that have unique word stems", "\n", "# first stem the variations, then remove the duplicates", "\n", "keyphrase_variations_stemmed", "=", "string_helper", ".", "stem_str_list", "(", "keyphrase_variations", ")", "# a list of word list", "\n", "not_duplicate", "=", "check_duplicate_keyphrases", "(", "keyphrase_variations_stemmed", ")", "# a boolean np array", "\n", "keyphrase_variations_unique", "=", "[", "' '", ".", "join", "(", "v", ")", "for", "v", ",", "is_keep", "in", "zip", "(", "keyphrase_variations", ",", "not_duplicate", ")", "if", "is_keep", "and", "(", "not", "limit_num", "or", "len", "(", "v", ")", "<=", "MAX_KEYWORD_LEN", ")", "]", "# ['v11 v12', 'v21 v22']", "\n", "return", "keyphrase_variations_unique", ",", "num_matched_disambiguation", ",", "extract_acronym_flag", ",", "num_redirections_found", "# 'v11 v12|v21 v22'", "\n", "#return '|'.join(keyphrase_variations_unique), match_disambiguation_flag, extract_acronym_flag  # 'v11 v12|v21 v22'", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.find_variations_from_wiki": [[159, 249], ["range", "integrated_data_preprocess.get_tokens", "wiki_variations.append", "entity_title.lower", "integrated_data_preprocess.find_redirected_titles", "len", "wikipedia.page", "print", "print", "exit", "utils.string_helper.stem_str_list", "utils.string_helper.stem_word_list", "evaluate_prediction.check_present_and_duplicate_keyphrases", "len", "time.sleep", "print", "print", "print", "time.sleep", "print", "print", "print", "time.sleep", "print", "print", "integrated_data_preprocess.get_tokens", "ValueError", "ValueError", "ValueError", "title.lower", "zip", "title.strip"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.find_redirected_titles", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_str_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_word_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_present_and_duplicate_keyphrases", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens"], ["", "def", "find_variations_from_wiki", "(", "keyphrase", ",", "src_tokens", ",", "fine_grad", ",", "use_corenlp", ",", "find_redirections", ")", ":", "\n", "    ", "\"\"\"\n    :param phrase:\n    :param src_str: tokenized source\n    :return: a list of tokenized phrase variations, contains the title of the entity as well as the titles that redirected to the entities; a flag for indicating that we find a disambiguation that match the source str\n    \"\"\"", "\n", "wiki_variations", "=", "[", "]", "\n", "num_matched_disambiguation", "=", "0", "\n", "num_redirections_found", "=", "0", "\n", "\n", "max_retry", "=", "100", "\n", "retry_flag", "=", "False", "\n", "for", "retry_i", "in", "range", "(", "max_retry", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "entity", "=", "wikipedia", ".", "page", "(", "title", "=", "keyphrase", ",", "auto_suggest", "=", "False", ",", "redirect", "=", "True", ")", "\n", "entity_title", "=", "entity", ".", "title", "# without lowercase", "\n", "#entity_title_tokens = get_tokens(entity.title.lower(), fine_grad)  # lowercase and tokenize", "\n", "stage", "=", "1", "\n", "retry_flag", "=", "False", "\n", "\n", "", "except", "wikipedia", ".", "exceptions", ".", "DisambiguationError", "as", "e", ":", "\n", "            ", "stage", "=", "2", "\n", "possible_titles", "=", "e", ".", "options", "# fetch all the possible entity titles, a list of str, without lowercase", "\n", "# lowercase and then tokenize possible titles, ignore it if a possible title is an empty string", "\n", "possible_titles_tokenized", "=", "[", "get_tokens", "(", "title", ".", "lower", "(", ")", ",", "fine_grad", ",", "use_corenlp", ")", "for", "title", "in", "possible_titles", "if", "title", ".", "strip", "(", ")", "!=", "''", "]", "# a list of word lists", "\n", "# stem possible titles", "\n", "possible_titles_stemmed", "=", "string_helper", ".", "stem_str_list", "(", "possible_titles_tokenized", ")", "# a list of word lists", "\n", "# stem src", "\n", "src_stemmed", "=", "string_helper", ".", "stem_word_list", "(", "src_tokens", ")", "# word list", "\n", "is_present", ",", "not_duplicate", "=", "check_present_and_duplicate_keyphrases", "(", "src_stemmed", ",", "possible_titles_stemmed", ")", "\n", "possible_titles_that_present_in_src", "=", "[", "title", "for", "title", ",", "is_keep", "in", "zip", "(", "possible_titles", ",", "is_present", ")", "if", "is_keep", "]", "\n", "num_matched_disambiguation", "=", "len", "(", "possible_titles_that_present_in_src", ")", "\n", "if", "num_matched_disambiguation", "==", "0", ":", "\n", "                ", "return", "[", "]", ",", "num_matched_disambiguation", ",", "num_redirections_found", "\n", "", "else", ":", "\n", "                ", "entity_title", "=", "possible_titles_that_present_in_src", "[", "0", "]", "\n", "retry_flag", "=", "False", "\n", "", "", "except", "wikipedia", ".", "exceptions", ".", "PageError", "as", "e", ":", "\n", "            ", "return", "[", "]", ",", "num_matched_disambiguation", ",", "num_redirections_found", "\n", "", "except", "wikipedia", ".", "exceptions", ".", "HTTPTimeoutError", "as", "e", ":", "\n", "            ", "if", "retry_i", "==", "max_retry", "-", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\"HTTP time out for {} times, still cannot call wikipedia API\"", ".", "format", "(", "max_retry", ")", ")", "\n", "", "retry_flag", "=", "True", "\n", "time", ".", "sleep", "(", "10", ")", "\n", "", "except", "wikipedia", ".", "exceptions", ".", "WikipediaException", "as", "e", ":", "\n", "            ", "print", "(", "keyphrase", ")", "\n", "print", "(", "e", ")", "\n", "print", "(", "\"base exceptions\"", ")", "\n", "if", "retry_i", "==", "max_retry", "-", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\"Retry for {} times, still cannot call wikipedia API\"", ".", "format", "(", "max_retry", ")", ")", "\n", "", "retry_flag", "=", "True", "\n", "time", ".", "sleep", "(", "10", ")", "\n", "", "except", "KeyError", "as", "e", ":", "\n", "            ", "return", "[", "]", ",", "num_matched_disambiguation", ",", "num_redirections_found", "\n", "\"\"\"\n            if e.args[0] == 'pages':\n                return []\n            else:\n                print(e)\n                exit()\n            \"\"\"", "\n", "", "except", "Exception", "as", "e", ":", "# catch *all* exceptions", "\n", "            ", "print", "(", "keyphrase", ")", "\n", "print", "(", "e", ")", "\n", "print", "(", "\"all exceptions\"", ")", "\n", "if", "retry_i", "==", "max_retry", "-", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\"Retry for {} times, still cannot call wikipedia API\"", ".", "format", "(", "max_retry", ")", ")", "\n", "", "retry_flag", "=", "True", "\n", "time", ".", "sleep", "(", "10", ")", "\n", "\n", "", "if", "entity_title", "==", "\"\"", ":", "\n", "            ", "print", "(", "\"Entity title is empty!\"", ")", "\n", "print", "(", "keyphrase", ")", "\n", "if", "stage", "==", "2", ":", "\n", "                ", "print", "(", "possible_titles", ")", "\n", "print", "(", "possible_titles_that_present_in_src", ")", "\n", "", "exit", "(", ")", "\n", "\n", "", "if", "not", "retry_flag", ":", "\n", "            ", "break", "\n", "\n", "", "", "entity_title_tokens", "=", "get_tokens", "(", "entity_title", ".", "lower", "(", ")", ",", "fine_grad", ",", "use_corenlp", ")", "# lowercase and tokenize", "\n", "wiki_variations", ".", "append", "(", "entity_title_tokens", ")", "\n", "if", "find_redirections", ":", "\n", "        ", "titles_that_redirected_to_the_entity", "=", "find_redirected_titles", "(", "entity_title", ",", "fine_grad", ",", "use_corenlp", ")", "# a list of word list", "\n", "num_redirections_found", "=", "len", "(", "titles_that_redirected_to_the_entity", ")", "\n", "wiki_variations", "+=", "titles_that_redirected_to_the_entity", "\n", "#wiki_variations += find_redirected_titles(entity_title, fine_grad, use_corenlp)  # a list of word list", "\n", "# wiki_variations contains the title of the entity as well as the titles that redirected to the entities", "\n", "", "return", "wiki_variations", ",", "num_matched_disambiguation", ",", "num_redirections_found", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.find_redirected_titles": [[251, 281], ["range", "json.loads", "json.loads", "requests.get", "integrated_data_preprocess.get_tokens", "print", "print", "print", "print", "exit", "time.sleep", "re.sub().strip", "ValueError", "re.sub", "entry[].lower"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens"], ["", "def", "find_redirected_titles", "(", "entity_title", ",", "fine_grad", ",", "use_corenlp", ")", ":", "\n", "    ", "\"\"\"\n    :param entity_title: without lowercase\n    :return: titles_that_redicted_to_the_entity: a list of list of words, tokenized\n    \"\"\"", "\n", "# find all the names that are redirected to this entity", "\n", "url", "=", "\"http://en.wikipedia.org/w/api.php?action=query&list=backlinks&bltitle={}&blfilterredir=redirects&format=json\"", ".", "format", "(", "\n", "entity_title", ")", "\n", "max_retry", "=", "100", "\n", "for", "i", "in", "range", "(", "max_retry", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "response", "=", "requests", ".", "get", "(", "url", ")", "\n", "break", "\n", "", "except", "requests", ".", "ConnectionError", ":", "\n", "            ", "if", "i", "==", "max_retry", "-", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\"Retry for {} times, still cannot get the redirected titles\"", ".", "format", "(", "max_retry", ")", ")", "\n", "", "time", ".", "sleep", "(", "10", ")", "\n", "\n", "", "", "response_json", "=", "json", ".", "loads", "(", "response", ".", "text", ")", "\n", "# lowercase and remove the parenthesis in the titles that are redirected to the entitiy, and then tokenize it", "\n", "try", ":", "\n", "        ", "titles_that_redirected_to_the_entity", "=", "[", "get_tokens", "(", "re", ".", "sub", "(", "r'\\(.*?\\)'", ",", "\"\"", ",", "entry", "[", "'title'", "]", ".", "lower", "(", ")", ")", ".", "strip", "(", ")", ",", "fine_grad", ",", "use_corenlp", ")", "for", "\n", "entry", "in", "response_json", "[", "'query'", "]", "[", "'backlinks'", "]", "]", "# a list of word list", "\n", "", "except", "KeyError", "as", "e", ":", "\n", "        ", "print", "(", "e", ")", "\n", "print", "(", "entity_title", ")", "\n", "print", "(", "url", ")", "\n", "print", "(", "response_json", ")", "\n", "exit", "(", ")", "\n", "", "return", "titles_that_redirected_to_the_entity", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens": [[283, 312], ["re.sub", "re.sub", "list", "filter", "re.sub.split", "CoreNLP.word_tokenize", "filter", "re.split", "len", "re.match", "re.match"], "function", ["None"], ["", "def", "get_tokens", "(", "text", ",", "fine_grad", "=", "True", ",", "use_corenlp", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Need use the same word tokenizer between keywords and source context\n    keep [_<>,\\(\\)\\.\\'%], replace digits to <digit>, split by [^a-zA-Z0-9_<>,\\(\\)\\.\\'%]\n    \"\"\"", "\n", "if", "replace_with_space", ":", "\n", "        ", "text", "=", "re", ".", "sub", "(", "r'[\\r\\n\\t]'", ",", "' '", ",", "text", ")", "\n", "", "else", ":", "\n", "        ", "text", "=", "re", ".", "sub", "(", "r'[\\r\\n\\t]'", ",", "''", ",", "text", ")", "\n", "", "text", "=", "''", ".", "join", "(", "list", "(", "filter", "(", "lambda", "x", ":", "x", "in", "PRINTABLE", ",", "text", ")", ")", ")", "\n", "if", "fine_grad", ":", "\n", "# tokenize by non-letters", "\n", "# Although we have will use corenlp for tokenizing,", "\n", "# we still use the following tokenizer for fine granularity", "\n", "        ", "tokens", "=", "filter", "(", "lambda", "w", ":", "len", "(", "w", ")", ">", "0", ",", "re", ".", "split", "(", "r'[^a-zA-Z0-9_<>,\\(\\)\\.\\'%]'", ",", "text", ")", ")", "\n", "", "else", ":", "\n", "        ", "tokens", "=", "text", ".", "split", "(", ")", "\n", "\n", "", "if", "use_corenlp", ":", "\n", "        ", "tokens", "=", "CoreNLP", ".", "word_tokenize", "(", "' '", ".", "join", "(", "tokens", ")", ")", "\n", "# c = ' '.join(CoreNLP.word_tokenize(c.strip())) + '\\n'", "\n", "\n", "# replace the digit terms with <digit>", "\n", "", "if", "fine_grad_digit_matching", ":", "\n", "        ", "tokens", "=", "[", "w", "if", "not", "re", ".", "match", "(", "'^[+-]?((\\d+(\\.\\d*)?)|(\\.\\d+))$'", ",", "w", ")", "else", "DIGIT", "for", "w", "in", "tokens", "]", "\n", "", "else", ":", "\n", "        ", "tokens", "=", "[", "w", "if", "not", "re", ".", "match", "(", "'^\\d+$'", ",", "w", ")", "else", "DIGIT", "for", "w", "in", "tokens", "]", "\n", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.remove_duplicate_from_str_list": [[314, 322], ["set", "set.add", "unique_str_list.append"], "function", ["None"], ["", "def", "remove_duplicate_from_str_list", "(", "str_list", ")", ":", "\n", "    ", "unique_str_list", "=", "[", "]", "\n", "str_set", "=", "set", "(", ")", "\n", "for", "a_str", "in", "str_list", ":", "\n", "        ", "if", "a_str", "not", "in", "str_set", ":", "\n", "            ", "str_set", ".", "add", "(", "a_str", ")", "\n", "unique_str_list", ".", "append", "(", "a_str", ")", "\n", "", "", "return", "unique_str_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.process_keyphrase": [[324, 384], ["re.sub", "re.sub.split", "integrated_data_preprocess.remove_duplicate_from_str_list", "ValueError", "keyphrase.strip.strip", "integrated_data_preprocess.sort_keyphrases_by_their_order_of_occurence", "len", "integrated_data_preprocess.find_variations", "re.sub().strip", "integrated_data_preprocess.get_tokens", "len", "sort_keyphrases_by_their_order_of_occurence.append", "len", "re.findall", "keyphrase.strip.strip", "len", "len", "re.sub", "keyphrase_token_2dlist.append", "sort_keyphrases_by_their_order_of_occurence.append", "len"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.remove_duplicate_from_str_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.sort_keyphrases_by_their_order_of_occurence", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.find_variations", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens"], ["", "def", "process_keyphrase", "(", "keyword_str", ",", "src_tokens", ",", "keyphrase_stat", ",", "variations", "=", "False", ",", "limit_num", "=", "True", ",", "fine_grad", "=", "True", ",", "sort_keyphrases", "=", "False", ",", "match_ending_parenthesis", "=", "False", ",", "use_corenlp", "=", "True", ",", "separate_present_absent", "=", "False", ",", "find_redirections", "=", "False", ")", ":", "\n", "    ", "if", "variations", "and", "sort_keyphrases", ":", "\n", "        ", "raise", "ValueError", "(", "\"You cannot use sort_keyphrases when you need to find the variations of each keyphrase\"", ")", "\n", "# remove question mark", "\n", "#keyword_str = keyword_str.replace('?', '')", "\n", "# remove the any '[' or ']' symbol", "\n", "#keyword_str = keyword_str.replace('[', '')", "\n", "#keyword_str = keyword_str.replace(']', '')", "\n", "#keyword_str = keyword_str.replace('|', '')", "\n", "# remove '?', '[', ']', '|', '\\\\' characters", "\n", "", "keyword_str", "=", "re", ".", "sub", "(", "r'[\\\\|\\[\\]?]'", ",", "''", ",", "keyword_str", ")", "\n", "keyphrase_list", "=", "[", "]", "\n", "keyphrase_token_2dlist", "=", "[", "]", "\n", "for", "keyphrase", "in", "keyword_str", ".", "split", "(", "';'", ")", ":", "\n", "        ", "keyphrase", "=", "keyphrase", ".", "strip", "(", ")", "\n", "if", "len", "(", "keyphrase", ")", ">", "0", ":", "# if keyphrase is not an empty string", "\n", "            ", "keyphrase_stat", "[", "'num_keyphrases'", "]", "+=", "1", "\n", "if", "variations", ":", "\n", "                ", "keyphrase_variations", ",", "num_matched_disambiguation", ",", "extract_acronym_flag", ",", "num_redirections_found", "=", "find_variations", "(", "keyphrase", ",", "src_tokens", ",", "fine_grad", ",", "limit_num", ",", "match_ending_parenthesis", ",", "use_corenlp", ",", "find_redirections", ")", "# str of variations, e.g., 'v11 v12|v21 v22'", "\n", "keyphrase_variations_str", "=", "'|'", ".", "join", "(", "keyphrase_variations", ")", "# serialize it into a string, each variation is separated by '|', e.g., 'v11 v12|v21 v22'", "\n", "if", "len", "(", "keyphrase_variations", ")", ">", "0", ":", "\n", "                    ", "keyphrase_list", ".", "append", "(", "keyphrase_variations_str", ")", "\n", "keyphrase_stat", "[", "'num_variations'", "]", "+=", "len", "(", "keyphrase_variations", ")", "\n", "if", "num_matched_disambiguation", ">", "0", ":", "\n", "                        ", "keyphrase_stat", "[", "'num_matched_disambiguation'", "]", "+=", "num_matched_disambiguation", "\n", "keyphrase_stat", "[", "'num_keyphrases_with_match_disambiguation'", "]", "+=", "1", "\n", "", "if", "extract_acronym_flag", ":", "\n", "                        ", "keyphrase_stat", "[", "'num_extracted_acronym'", "]", "+=", "1", "\n", "", "if", "len", "(", "keyphrase_variations", ")", ">", "1", ":", "\n", "                        ", "keyphrase_stat", "[", "'num_keyphrases_with_variations'", "]", "+=", "1", "\n", "", "if", "num_redirections_found", ">", "0", ":", "\n", "                        ", "keyphrase_stat", "[", "'num_keyphrases_with_redirections'", "]", "+=", "1", "\n", "keyphrase_stat", "[", "'num_redirections'", "]", "+=", "num_redirections_found", "\n", "\n", "", "", "", "else", ":", "\n", "                ", "keyphrase_filtered", "=", "re", ".", "sub", "(", "r'\\(.*?\\)'", ",", "''", ",", "keyphrase", ")", ".", "strip", "(", ")", "# remove text in parenthesis", "\n", "if", "keyphrase_filtered", "==", "\"\"", ":", "# if keyphrase is empty after removing parenthesis, just keep the text inside the parenthesis", "\n", "                    ", "match_list", "=", "re", ".", "findall", "(", "'\\((.*?)\\)'", ",", "keyphrase", ")", "# match all str inside parenthesis", "\n", "keyphrase", "=", "match_list", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                    ", "keyphrase", "=", "keyphrase_filtered", "\n", "# tokenize, then serialize and add to the keyphrase_list if it does not exceed MAX_KEYWORD_LEN", "\n", "", "keyphrase_tokens", "=", "get_tokens", "(", "keyphrase", ".", "strip", "(", ")", ",", "fine_grad", ",", "use_corenlp", ")", "# word list", "\n", "if", "len", "(", "keyphrase_tokens", ")", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "elif", "limit_num", "and", "len", "(", "keyphrase_tokens", ")", ">", "MAX_KEYWORD_LEN", ":", "\n", "                    ", "continue", "\n", "", "else", ":", "\n", "                    ", "keyphrase_token_2dlist", ".", "append", "(", "keyphrase_tokens", ")", "\n", "keyphrase", "=", "' '", ".", "join", "(", "keyphrase_tokens", ")", "# a keyphrase str, e.g., 'k11 k12'", "\n", "keyphrase_list", ".", "append", "(", "keyphrase", ")", "\n", "\n", "", "", "", "", "if", "sort_keyphrases", ":", "\n", "        ", "keyphrase_list", "=", "sort_keyphrases_by_their_order_of_occurence", "(", "keyphrase_list", ",", "src_tokens", ",", "keyphrase_token_2dlist", ",", "separate_present_absent", ")", "\n", "\n", "# remove duplicate keyphrases", "\n", "", "keyphrase_list", "=", "remove_duplicate_from_str_list", "(", "keyphrase_list", ")", "\n", "\n", "# a list of keyphrase str", "\n", "return", "keyphrase_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.sort_keyphrases_by_their_order_of_occurence": [[386, 402], ["len", "utils.string_helper.stem_word_list", "utils.string_helper.stem_str_list", "integrated_data_preprocess.batch_check_present_idx", "numpy.argsort", "len", "sorted_keyphrase_list.insert"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_word_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_str_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.batch_check_present_idx"], ["", "def", "sort_keyphrases_by_their_order_of_occurence", "(", "keyphrase_list", ",", "src_tokens", ",", "keyphrase_token_2dlist", ",", "separate_present_absent", ")", ":", "\n", "    ", "num_keyphrase", "=", "len", "(", "keyphrase_list", ")", "\n", "assert", "num_keyphrase", "==", "len", "(", "keyphrase_token_2dlist", ")", "\n", "# stem the token list and check the present idx", "\n", "src_tokens_stemmed", "=", "string_helper", ".", "stem_word_list", "(", "src_tokens", ")", "\n", "keyphrase_token_2dlist_stemmed", "=", "string_helper", ".", "stem_str_list", "(", "keyphrase_token_2dlist", ")", "\n", "present_idx_array", ",", "num_present_keyphrases", "=", "batch_check_present_idx", "(", "src_tokens_stemmed", ",", "keyphrase_token_2dlist_stemmed", ")", "\n", "# rearrange the order in keyphrase list", "\n", "sorted_keyphrase_indices", "=", "np", ".", "argsort", "(", "present_idx_array", ")", "\n", "sorted_keyphrase_list", "=", "[", "keyphrase_list", "[", "idx", "]", "for", "idx", "in", "sorted_keyphrase_indices", "]", "\n", "if", "separate_present_absent", ":", "\n", "        ", "if", "reverse_sorting", ":", "\n", "            ", "sorted_keyphrase_list", "=", "sorted_keyphrase_list", "[", "num_present_keyphrases", ":", "]", "+", "[", "present_absent_segmenter", "]", "+", "sorted_keyphrase_list", "[", ":", "num_present_keyphrases", "]", "\n", "", "else", ":", "\n", "            ", "sorted_keyphrase_list", ".", "insert", "(", "num_present_keyphrases", ",", "present_absent_segmenter", ")", "\n", "", "", "return", "sorted_keyphrase_list", "\n", "#return [keyphrase_list[idx] for idx in sorted_keyphrase_indices]", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.process_cross_doamin_file": [[405, 464], ["os.path.join", "os.path.join", "open", "open", "tqdm.tqdm", "open.writelines", "open.writelines", "range", "open", "open", "integrated_data_preprocess.get_tokens", "keywords_lines.append", "context_lines.append", "os.path.join", "os.path.join", "open.readlines", "line.strip", "context_i_line.replace.replace", "w.split", "context_i_line.replace.strip().split", "open.readlines", "integrated_data_preprocess.process_keyphrase", "context_i_line.replace.strip"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.process_keyphrase"], ["", "def", "process_cross_doamin_file", "(", "home_folder", ",", "dataset", ",", "saved_home", ",", "fine_grad", "=", "True", ",", "variations", "=", "False", ",", "sort_keyphrases", "=", "False", ",", "match_ending_parenthesis", "=", "False", ",", "use_corenlp", "=", "True", ",", "separate_present_absent", "=", "False", ",", "find_redirections", "=", "False", ")", ":", "\n", "    ", "processed_files_suffix", "=", "\"\"", "\n", "if", "variations", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_variations\"", "\n", "", "if", "find_redirections", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_redirections\"", "\n", "", "if", "sort_keyphrases", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_sorted\"", "\n", "", "if", "match_ending_parenthesis", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_parenthesis\"", "\n", "", "if", "separate_present_absent", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_separated\"", "\n", "", "if", "fine_grad_digit_matching", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_digit\"", "\n", "", "if", "reverse_sorting", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_reversed\"", "\n", "", "if", "replace_with_space", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_space\"", "\n", "\n", "", "context_file_path", "=", "os", ".", "path", ".", "join", "(", "saved_home", ",", "'data_for_corenlp'", ",", "'{}_testing_context_for_corenlp{}.txt'", ".", "format", "(", "dataset", ",", "processed_files_suffix", ")", ")", "\n", "trg_file_path", "=", "os", ".", "path", ".", "join", "(", "saved_home", ",", "'data_for_corenlp'", ",", "'{}_testing_keyword_for_corenlp{}.txt'", ".", "format", "(", "dataset", ",", "processed_files_suffix", ")", ")", "\n", "keywords_file", "=", "open", "(", "trg_file_path", ",", "'w'", ")", "\n", "context_file", "=", "open", "(", "context_file_path", ",", "'w'", ")", "\n", "keywords_lines", "=", "[", "]", "\n", "context_lines", "=", "[", "]", "\n", "keyphrase_stat", "=", "{", "'num_keyphrases_with_variations'", ":", "0", ",", "'num_keyphrases'", ":", "0", ",", "'num_variations'", ":", "0", ",", "\n", "'num_keyphrases_with_match_disambiguation'", ":", "0", ",", "'num_extracted_acronym'", ":", "0", ",", "\n", "'num_keyphrases_with_redirections'", ":", "0", ",", "\n", "'num_redirections'", ":", "0", ",", "'num_matched_disambiguation'", ":", "0", "}", "\n", "\n", "file_num", "=", "FILE_NUM", "[", "dataset", "]", "\n", "for", "i", "in", "tqdm", "(", "range", "(", "file_num", ")", ")", ":", "\n", "        ", "keywords_file_i", "=", "open", "(", "os", ".", "path", ".", "join", "(", "home_folder", ",", "dataset", ",", "'keyphrase'", ",", "'{}.txt'", ".", "format", "(", "i", ")", ")", ")", "\n", "context_file_i", "=", "open", "(", "os", ".", "path", ".", "join", "(", "home_folder", ",", "dataset", ",", "'text'", ",", "'{}.txt'", ".", "format", "(", "i", ")", ")", ")", "\n", "\n", "context_i_line", "=", "context_file_i", ".", "readlines", "(", ")", "[", "0", "]", "\n", "context_i_line", "=", "[", "w", ".", "split", "(", "'_'", ")", "[", "0", "]", "for", "w", "in", "context_i_line", ".", "strip", "(", ")", ".", "split", "(", ")", "]", "\n", "\n", "context_i_tokens", "=", "get_tokens", "(", "' '", ".", "join", "(", "context_i_line", ")", ",", "fine_grad", "=", "fine_grad", ",", "use_corenlp", "=", "use_corenlp", ")", "\n", "context_i_line", "=", "' '", ".", "join", "(", "context_i_tokens", ")", "+", "'\\n'", "\n", "\n", "keywords_i", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "keywords_file_i", ".", "readlines", "(", ")", "]", "\n", "keywords_i_line", "=", "';'", ".", "join", "(", "keywords_i", ")", "\n", "keywords_i_line", "=", "';'", ".", "join", "(", "\n", "process_keyphrase", "(", "keywords_i_line", ",", "context_i_tokens", ",", "keyphrase_stat", ",", "variations", "=", "variations", ",", "limit_num", "=", "False", ",", "\n", "fine_grad", "=", "fine_grad", ",", "sort_keyphrases", "=", "sort_keyphrases", ",", "\n", "match_ending_parenthesis", "=", "match_ending_parenthesis", ",", "use_corenlp", "=", "use_corenlp", ",", "\n", "separate_present_absent", "=", "separate_present_absent", ",", "find_redirections", "=", "find_redirections", ")", ")", "+", "'\\n'", "\n", "\n", "if", "dataset", "!=", "'krapivin'", ":", "\n", "            ", "context_i_line", "=", "context_i_line", ".", "replace", "(", "'<eos>'", ",", "'. <eos>'", ")", "\n", "\n", "", "keywords_lines", ".", "append", "(", "keywords_i_line", ")", "\n", "context_lines", ".", "append", "(", "context_i_line", ")", "\n", "\n", "", "keywords_file", ".", "writelines", "(", "keywords_lines", ")", "\n", "context_file", ".", "writelines", "(", "context_lines", ")", "\n", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.json2txt_for_corenlp": [[466, 568], ["print", "os.path.join", "os.path.join", "open", "open", "open", "open.readlines", "tqdm.tqdm", "open.close", "open.close", "print", "ValueError", "os.path.exists", "os.makedirs", "os.path.join", "os.path.join", "range", "json.loads", "json.loads", "line_dict[].strip", "line_dict[].strip", "line_dict[].strip", "title.lower.lower", "abstract.lower.lower", "keywords.lower.lower", "integrated_data_preprocess.get_tokens", "open.write", "open.write", "print", "print", "print", "print", "print", "print", "print", "len", "line.strip", "len", "len", "integrated_data_preprocess.process_keyphrase", "keywords.lower.strip().split", "keywords.lower.strip"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.get_tokens", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.process_keyphrase"], ["", "def", "json2txt_for_corenlp", "(", "json_home", ",", "dataset", ",", "data_type", ",", "saved_home", ",", "fine_grad", "=", "True", ",", "use_orig_keys", "=", "False", ",", "variations", "=", "False", ",", "sort_keyphrases", "=", "False", ",", "match_ending_parenthesis", "=", "False", ",", "use_corenlp", "=", "True", ",", "separate_present_absent", "=", "False", ",", "find_redirections", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    process the original json file into a txt file for corenlp tokenizing\n    :param json_home: the home directory of the json files of KP20k\n    :param data_type: training, testing, validation\n    :param saved_home: the directory to save the obtained txt file\n    :param use_orig_keys: Whether directly use the original keys (unprocessed).\n    :return: None\n    \"\"\"", "\n", "keyphrase_stat", "=", "{", "'num_keyphrases_with_variations'", ":", "0", ",", "'num_keyphrases'", ":", "0", ",", "'num_variations'", ":", "0", ",", "\n", "'num_keyphrases_with_match_disambiguation'", ":", "0", ",", "'num_extracted_acronym'", ":", "0", ",", "'num_keyphrases_with_redirections'", ":", "0", ",", "\n", "'num_redirections'", ":", "0", ",", "'num_matched_disambiguation'", ":", "0", "}", "\n", "#num_keyphrases_with_variations = 0", "\n", "#num_keyphrases = 0", "\n", "#num_variations = 0", "\n", "#num_keyphrases_with_match_disambiguation = 0", "\n", "\n", "if", "variations", "and", "sort_keyphrases", ":", "\n", "        ", "raise", "ValueError", "(", "\"You cannot use sort_keyphrases when you need to find the variations of each keyphrase\"", ")", "\n", "\n", "", "print", "(", "'\\nProcessing {} data...'", ".", "format", "(", "data_type", ")", ")", "\n", "saved_data_dir", "=", "os", ".", "path", ".", "join", "(", "saved_home", ",", "'data_for_corenlp'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "saved_data_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "saved_data_dir", ")", "\n", "", "json_file_name", "=", "os", ".", "path", ".", "join", "(", "json_home", ",", "\"{}_{}.json\"", ".", "format", "(", "dataset", ",", "data_type", ")", ")", "\n", "json_file", "=", "open", "(", "json_file_name", ",", "encoding", "=", "'utf-8'", ")", "\n", "processed_files_suffix", "=", "\"\"", "\n", "if", "variations", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_variations\"", "\n", "", "if", "find_redirections", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_redirections\"", "\n", "", "if", "sort_keyphrases", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_sorted\"", "\n", "", "if", "match_ending_parenthesis", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_parenthesis\"", "\n", "", "if", "separate_present_absent", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_separated\"", "\n", "", "if", "fine_grad_digit_matching", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_digit\"", "\n", "", "if", "reverse_sorting", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_reversed\"", "\n", "", "if", "replace_with_space", ":", "\n", "        ", "processed_files_suffix", "+=", "\"_space\"", "\n", "\n", "", "processed_keyword_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "saved_data_dir", ",", "\"{}_{}_keyword_for_corenlp{}.txt\"", ".", "format", "(", "dataset", ",", "data_type", ",", "processed_files_suffix", ")", ")", ",", "\n", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "# context = title + '.' + '<eos>' + abstract", "\n", "processed_context_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "saved_data_dir", ",", "\"{}_{}_context_for_corenlp{}.txt\"", ".", "format", "(", "dataset", ",", "data_type", ",", "processed_files_suffix", ")", ")", ",", "\n", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "lines", "=", "json_file", ".", "readlines", "(", ")", "\n", "for", "line_idx", "in", "tqdm", "(", "range", "(", "len", "(", "lines", ")", ")", ")", ":", "\n", "        ", "line", "=", "lines", "[", "line_idx", "]", "\n", "line_dict", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "# tokenization, lowercasing, replace all digits with '<digit>' symbol", "\n", "title", "=", "line_dict", "[", "'title'", "]", ".", "strip", "(", ")", "\n", "abstract", "=", "line_dict", "[", "'abstract'", "]", ".", "strip", "(", ")", "\n", "keywords", "=", "line_dict", "[", "'keyword'", "]", ".", "strip", "(", ")", "\n", "# lowercasing the text", "\n", "title", "=", "title", ".", "lower", "(", ")", "\n", "abstract", "=", "abstract", ".", "lower", "(", ")", "\n", "keywords", "=", "keywords", ".", "lower", "(", ")", "\n", "# filter out no-title or no-abstract data", "\n", "if", "len", "(", "title", ")", "==", "0", "or", "len", "(", "abstract", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "# concatenate title and abstract", "\n", "", "context", "=", "title", "+", "' . '", "+", "' <eos> '", "+", "abstract", "\n", "\n", "# for fine granularity tokenization", "\n", "context_tokens", "=", "get_tokens", "(", "context", ",", "fine_grad", "=", "fine_grad", ",", "use_corenlp", "=", "use_corenlp", ")", "\n", "context", "=", "' '", ".", "join", "(", "context_tokens", ")", "\n", "if", "not", "use_orig_keys", ":", "\n", "            ", "if", "data_type", "!=", "'testing'", ":", "\n", "                ", "limit_num", "=", "True", "\n", "", "else", ":", "\n", "                ", "limit_num", "=", "False", "\n", "", "keywords", "=", "';'", ".", "join", "(", "\n", "process_keyphrase", "(", "keywords", ",", "context_tokens", ",", "keyphrase_stat", ",", "variations", "=", "variations", ",", "limit_num", "=", "limit_num", ",", "\n", "fine_grad", "=", "fine_grad", ",", "sort_keyphrases", "=", "sort_keyphrases", ",", "\n", "match_ending_parenthesis", "=", "match_ending_parenthesis", ",", "use_corenlp", "=", "use_corenlp", ",", "\n", "separate_present_absent", "=", "separate_present_absent", ",", "find_redirections", "=", "find_redirections", ")", ")", "\n", "", "else", ":", "\n", "            ", "keywords", "=", "';'", ".", "join", "(", "keywords", ".", "strip", "(", ")", ".", "split", "(", "';'", ")", ")", "\n", "\n", "", "context_line", "=", "context", "+", "'\\n'", "\n", "keywords_line", "=", "keywords", "+", "'\\n'", "\n", "\n", "processed_keyword_file", ".", "write", "(", "keywords_line", ")", "\n", "processed_context_file", ".", "write", "(", "context_line", ")", "\n", "\n", "", "processed_keyword_file", ".", "close", "(", ")", "\n", "processed_context_file", ".", "close", "(", ")", "\n", "print", "(", "\"# keyphrases: {}\"", ".", "format", "(", "keyphrase_stat", "[", "'num_keyphrases'", "]", ")", ")", "\n", "if", "variations", ":", "\n", "        ", "print", "(", "\"# variations: {}\"", ".", "format", "(", "keyphrase_stat", "[", "'num_variations'", "]", ")", ")", "\n", "print", "(", "\"# keyphrases with variations: {}\"", ".", "format", "(", "keyphrase_stat", "[", "'num_keyphrases_with_variations'", "]", ")", ")", "\n", "print", "(", "\"# keyphrases with match disambiguation: {}\"", ".", "format", "(", "keyphrase_stat", "[", "'num_keyphrases_with_match_disambiguation'", "]", ")", ")", "\n", "print", "(", "\"# matched disambiguation: {}\"", ".", "format", "(", "keyphrase_stat", "[", "'num_matched_disambiguation'", "]", ")", ")", "\n", "", "if", "match_ending_parenthesis", ":", "\n", "        ", "print", "(", "\"# extracted acronyms: {}\"", ".", "format", "(", "keyphrase_stat", "[", "'num_extracted_acronym'", "]", ")", ")", "\n", "", "if", "find_redirections", ":", "\n", "        ", "print", "(", "'# redirections found: {}'", ".", "format", "(", "keyphrase_stat", "[", "'num_redirections'", "]", ")", ")", "\n", "print", "(", "'# keyphrases with redirections: {}'", ".", "format", "(", "keyphrase_stat", "[", "'num_keyphrases_with_redirections'", "]", ")", ")", "\n", "# keyphrase_stat = {'num_keyphrases_with_variations': 0, 'num_keyphrases': 0, 'num_variations': 0, 'num_keyphrases_with_match_disambiguation': 0}", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.filter_dups": [[571, 637], ["open", "open.readlines", "open", "open.readlines", "set", "print", "tqdm.tqdm", "print", "sorted", "open", "open.writelines", "open", "open.writelines", "open", "open.write", "open.write", "os.path.join", "os.path.join", "len", "len", "set", "open", "total_filtered_idx_set.union.union", "print", "range", "list", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "line.strip.strip", "line.strip.split", "dups.split", "int", "int", "len", "total_filtered_idx_set.union.add", "len", "str", "set.add", "len", "context_lines[].strip().split", "allkeys_lines[].strip().split", "len", "src_dup.strip().split", "filtered_dup.strip().split", "set.add", "str", "context_lines[].strip", "allkeys_lines[].strip", "src_dup.strip", "filtered_dup.strip"], "function", ["None"], ["", "", "def", "filter_dups", "(", "saved_home", ",", "dups_info_home", ")", ":", "\n", "    ", "\"\"\"\n    filter out the duplicates in the training data with the testing data according to the obtained duplication info file.\n    :param saved_home: non-filtered data home\n    :param dups_info_home: duplication information home\n    :return: None\n    \"\"\"", "\n", "orig_context_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "saved_home", ",", "'data_for_corenlp'", ",", "'kp20k_training_context_for_corenlp.txt'", ")", ",", "\n", "encoding", "=", "'utf-8'", ")", "\n", "context_lines", "=", "orig_context_file", ".", "readlines", "(", ")", "\n", "orig_allkeys_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "saved_home", ",", "'data_for_corenlp'", ",", "'kp20k_training_keyword_for_corenlp.txt'", ")", ",", "\n", "encoding", "=", "'utf-8'", ")", "\n", "allkeys_lines", "=", "orig_allkeys_file", ".", "readlines", "(", ")", "\n", "assert", "len", "(", "context_lines", ")", "==", "len", "(", "allkeys_lines", ")", "\n", "\n", "# filter out the duplicates in the validation and the testing datasets and the kp20k training dataset itself", "\n", "dups_info_datasets", "=", "[", "'kp20k_training'", ",", "'kp20k_validation'", ",", "'kp20k_testing'", ",", "\n", "'inspec_testing'", ",", "'krapivin_testing'", ",", "\n", "'nus_testing'", ",", "'semeval_testing'", "]", "\n", "total_filtered_idx_set", "=", "set", "(", ")", "\n", "for", "dataset", "in", "dups_info_datasets", ":", "\n", "        ", "filtered_idx_set", "=", "set", "(", ")", "\n", "dups_info_file", "=", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "dups_info_home", ",", "'{}_context_nstpws_dups_w_kp20k_training.txt'", ".", "format", "(", "dataset", ")", ")", ",", "encoding", "=", "'utf-8'", ")", "\n", "for", "line", "in", "dups_info_file", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "# inspec_testing_48 kp20k_training_433051 jc_sc:0.7368; affine invariants of convex polygons | affine invariants of convex polygons", "\n", "dups", ",", "titles", "=", "line", ".", "split", "(", "';'", ")", "\n", "src_dup", ",", "filtered_dup", ",", "_", "=", "dups", ".", "split", "(", ")", "\n", "src_idx", "=", "int", "(", "src_dup", ".", "strip", "(", ")", ".", "split", "(", "'_'", ")", "[", "-", "1", "]", ")", "\n", "filtered_idx", "=", "int", "(", "filtered_dup", ".", "strip", "(", ")", ".", "split", "(", "'_'", ")", "[", "-", "1", "]", ")", "\n", "if", "dataset", "!=", "'kp20k_training'", ":", "\n", "                ", "filtered_idx_set", ".", "add", "(", "filtered_idx", ")", "\n", "", "else", ":", "\n", "                ", "if", "src_idx", "not", "in", "filtered_idx_set", ":", "\n", "                    ", "filtered_idx_set", ".", "add", "(", "filtered_idx", ")", "\n", "", "", "", "total_filtered_idx_set", "=", "total_filtered_idx_set", ".", "union", "(", "filtered_idx_set", ")", "\n", "print", "(", "'Num of filtered kp20k training data: {}'", ".", "format", "(", "len", "(", "total_filtered_idx_set", ")", ")", ")", "\n", "\n", "# also filter out the invalid data samples", "\n", "", "print", "(", "'Finding the invalid data samples in the original kp20k training ...'", ")", "\n", "for", "corpus_idx", "in", "tqdm", "(", "range", "(", "len", "(", "context_lines", ")", ")", ")", ":", "\n", "        ", "if", "context_lines", "[", "corpus_idx", "]", ".", "strip", "(", ")", ".", "split", "(", ")", "==", "[", "''", "]", "or", "allkeys_lines", "[", "corpus_idx", "]", ".", "strip", "(", ")", ".", "split", "(", "' ; '", ")", "==", "[", "''", "]", ":", "\n", "            ", "total_filtered_idx_set", ".", "add", "(", "corpus_idx", ")", "\n", "", "", "print", "(", "'Num of filtered kp20k training data: {}'", ".", "format", "(", "len", "(", "total_filtered_idx_set", ")", ")", ")", "\n", "\n", "total_filtered_idxes", "=", "sorted", "(", "list", "(", "total_filtered_idx_set", ")", ")", "\n", "for", "filter_idx", "in", "total_filtered_idxes", ":", "\n", "        ", "context_lines", "[", "filter_idx", "]", "=", "'\\n'", "\n", "allkeys_lines", "[", "filter_idx", "]", "=", "'\\n'", "\n", "\n", "", "filtered_context_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "saved_home", ",", "'data_for_corenlp'", ",", "\n", "'kp20k_training_context_for_corenlp_filtered.txt'", ")", ",", "\n", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "filtered_context_file", ".", "writelines", "(", "context_lines", ")", "\n", "\n", "filtered_allkeys_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "saved_home", ",", "'data_for_corenlp'", ",", "\n", "'kp20k_training_keyword_for_corenlp_filtered.txt'", ")", ",", "\n", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "filtered_allkeys_file", ".", "writelines", "(", "allkeys_lines", ")", "\n", "\n", "orig_context_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "saved_home", ",", "'data_for_corenlp'", ",", "\n", "'kp20k_training_filtered_for_corenlp_idxes.txt'", ")", ",", "\n", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "orig_context_file", ".", "write", "(", "' '", ".", "join", "(", "[", "str", "(", "idx", ")", "for", "idx", "in", "total_filtered_idxes", "]", ")", "+", "'\\n'", ")", "\n", "orig_context_file", ".", "write", "(", "str", "(", "len", "(", "total_filtered_idxes", ")", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.integrated_data_preprocess.corenlp_tokenizing": [[639, 680], ["os.path.join", "os.path.join", "open", "open.readlines", "tqdm.tqdm", "os.path.join", "open", "open.writelines", "os.path.join", "open", "open.readlines", "tqdm.tqdm", "os.path.join", "open", "open.writelines", "os.path.exists", "os.makedirs", "range", "tokenized_context_lines.append", "range", "tokenized_key_lines.append", "len", "len", "CoreNLP.word_tokenize", "CoreNLP.word_tokenize", "c.strip", "c.strip"], "function", ["None"], ["", "def", "corenlp_tokenizing", "(", "data_home", ",", "dataset", "=", "'kp20k'", ",", "data_type", "=", "'validation'", ")", ":", "\n", "    ", "\"\"\"\n    Use corenlp to tokenize the text\n    Corenlp Installation: https://github.com/Lynten/stanford-corenlp\n    :param data_for_corenlp_home: the directory for the original data\n    :param dataset: dataset name ['kp20k', 'inspec', 'krapivin', 'nus', 'semeval']\n    :param data_type: ['training', 'validation', 'testing']\n    :return: None\n    \"\"\"", "\n", "suffix", "=", "''", "\n", "if", "dataset", "==", "'kp20k'", "and", "data_type", "==", "'training'", ":", "\n", "        ", "suffix", "=", "'_filtered'", "\n", "", "data_for_opennmt_home", "=", "os", ".", "path", ".", "join", "(", "data_home", ",", "'data_for_opennmt'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_for_opennmt_home", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "data_for_opennmt_home", ")", "\n", "\n", "", "context_file", "=", "os", ".", "path", ".", "join", "(", "data_home", ",", "'data_for_corenlp'", ",", "'{}_{}_context_for_corenlp{}.txt'", ".", "format", "(", "dataset", ",", "data_type", ",", "suffix", ")", ")", "\n", "context_file", "=", "open", "(", "context_file", ",", "encoding", "=", "'utf-8'", ")", "\n", "context_lines", "=", "context_file", ".", "readlines", "(", ")", "\n", "# tokenized_context_lines = [' '.join(CoreNLP.word_tokenize(c.strip())) + '\\n' for c in context_lines]", "\n", "tokenized_context_lines", "=", "[", "]", "\n", "for", "c_idx", "in", "tqdm", "(", "range", "(", "len", "(", "context_lines", ")", ")", ")", ":", "\n", "        ", "c", "=", "context_lines", "[", "c_idx", "]", "\n", "c", "=", "' '", ".", "join", "(", "CoreNLP", ".", "word_tokenize", "(", "c", ".", "strip", "(", ")", ")", ")", "+", "'\\n'", "\n", "tokenized_context_lines", ".", "append", "(", "c", ")", "\n", "", "saved_context_file", "=", "os", ".", "path", ".", "join", "(", "data_for_opennmt_home", ",", "'{}_{}_context{}.txt'", ".", "format", "(", "dataset", ",", "data_type", ",", "suffix", ")", ")", "\n", "saved_context_file", "=", "open", "(", "saved_context_file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "saved_context_file", ".", "writelines", "(", "tokenized_context_lines", ")", "\n", "\n", "key_file", "=", "os", ".", "path", ".", "join", "(", "data_home", ",", "'data_for_corenlp'", ",", "'{}_{}_keyword_for_corenlp{}.txt'", ".", "format", "(", "dataset", ",", "data_type", ",", "suffix", ")", ")", "\n", "key_file", "=", "open", "(", "key_file", ",", "encoding", "=", "'utf-8'", ")", "\n", "key_lines", "=", "key_file", ".", "readlines", "(", ")", "\n", "# tokenized_key_lines = [' '.join(CoreNLP.word_tokenize(c.strip())) + '\\n' for c in key_lines]", "\n", "tokenized_key_lines", "=", "[", "]", "\n", "for", "c_idx", "in", "tqdm", "(", "range", "(", "len", "(", "key_lines", ")", ")", ")", ":", "\n", "        ", "c", "=", "key_lines", "[", "c_idx", "]", "\n", "c", "=", "' '", ".", "join", "(", "CoreNLP", ".", "word_tokenize", "(", "c", ".", "strip", "(", ")", ")", ")", "+", "'\\n'", "\n", "tokenized_key_lines", ".", "append", "(", "c", ")", "\n", "", "saved_key_file", "=", "os", ".", "path", ".", "join", "(", "data_for_opennmt_home", ",", "'{}_{}_keyword{}.txt'", ".", "format", "(", "dataset", ",", "data_type", ",", "suffix", ")", ")", "\n", "saved_key_file", "=", "open", "(", "saved_key_file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "saved_key_file", ".", "writelines", "(", "tokenized_key_lines", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train.process_opt": [[22, 92], ["logging.info", "torch.manual_seed", "numpy.random.seed", "random.seed", "torch.cuda.is_available", "hasattr", "hasattr", "hasattr", "torch.load.exp_path.find", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "torch.load", "torch.save", "json.dump", "open", "open", "vars", "open", "os.path.join", "os.path.join", "os.path.join"], "function", ["None"], ["def", "process_opt", "(", "opt", ")", ":", "\n", "    ", "if", "opt", ".", "seed", ">", "0", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "opt", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "opt", ".", "seed", ")", "\n", "random", ".", "seed", "(", "opt", ".", "seed", ")", "\n", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "opt", ".", "gpuid", ":", "\n", "        ", "opt", ".", "gpuid", "=", "0", "\n", "\n", "", "if", "hasattr", "(", "opt", ",", "'train_ml'", ")", "and", "opt", ".", "train_ml", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.ml'", "\n", "\n", "", "if", "hasattr", "(", "opt", ",", "'train_rl'", ")", "and", "opt", ".", "train_rl", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.rl'", "\n", "\n", "", "if", "opt", ".", "one2many", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.one2many'", "\n", "\n", "", "if", "opt", ".", "one2many_mode", "==", "1", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.cat'", "\n", "\n", "", "if", "opt", ".", "copy_attention", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.copy'", "\n", "\n", "", "if", "opt", ".", "coverage_attn", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.coverage'", "\n", "\n", "", "if", "opt", ".", "review_attn", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.review'", "\n", "\n", "", "if", "opt", ".", "orthogonal_loss", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.orthogonal'", "\n", "\n", "", "if", "opt", ".", "use_target_encoder", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.target_encode'", "\n", "\n", "", "if", "hasattr", "(", "opt", ",", "'bidirectional'", ")", "and", "opt", ".", "bidirectional", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.bi-directional'", "\n", "", "else", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.uni-directional'", "\n", "\n", "", "if", "opt", ".", "delimiter_type", "==", "0", ":", "\n", "        ", "opt", ".", "delimiter_word", "=", "pykp", ".", "io", ".", "SEP_WORD", "\n", "", "else", ":", "\n", "        ", "opt", ".", "delimiter_word", "=", "pykp", ".", "io", ".", "EOS_WORD", "\n", "\n", "# fill time into the name", "\n", "", "if", "opt", ".", "exp_path", ".", "find", "(", "'%s'", ")", ">", "0", ":", "\n", "        ", "opt", ".", "exp_path", "=", "opt", ".", "exp_path", "%", "(", "opt", ".", "exp", ",", "opt", ".", "timemark", ")", "\n", "opt", ".", "model_path", "=", "opt", ".", "model_path", "%", "(", "opt", ".", "exp", ",", "opt", ".", "timemark", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "opt", ".", "exp_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "opt", ".", "exp_path", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "opt", ".", "model_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "opt", ".", "model_path", ")", "\n", "\n", "", "logging", ".", "info", "(", "'EXP_PATH : '", "+", "opt", ".", "exp_path", ")", "\n", "\n", "# dump the setting (opt) to disk in order to reuse easily", "\n", "if", "opt", ".", "train_from", ":", "\n", "        ", "opt", "=", "torch", ".", "load", "(", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "model_path", ",", "opt", ".", "exp", "+", "'.initial.config'", ")", ",", "'rb'", ")", "\n", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "save", "(", "opt", ",", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "model_path", ",", "opt", ".", "exp", "+", "'.initial.config'", ")", ",", "'wb'", ")", "\n", ")", "\n", "json", ".", "dump", "(", "vars", "(", "opt", ")", ",", "open", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "model_path", ",", "opt", ".", "exp", "+", "'.initial.json'", ")", ",", "'w'", ")", ")", "\n", "\n", "", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train.init_optimizer_criterion": [[94, 126], ["torch.nn.NLLLoss().to", "torch.optim.Adam", "torch.optim.Adam", "torch.nn.NLLLoss", "filter", "filter", "model.parameters", "model.parameters"], "function", ["None"], ["", "def", "init_optimizer_criterion", "(", "model", ",", "opt", ")", ":", "\n", "    ", "\"\"\"\n    mask the PAD <pad> when computing loss, before we used weight matrix, but not handy for copy-model, change to ignore_index\n    :param model:\n    :param opt:\n    :return:\n    \"\"\"", "\n", "'''\n    if not opt.copy_attention:\n        weight_mask = torch.ones(opt.vocab_size).cuda() if torch.cuda.is_available() else torch.ones(opt.vocab_size)\n    else:\n        weight_mask = torch.ones(opt.vocab_size + opt.max_unk_words).cuda() if torch.cuda.is_available() else torch.ones(opt.vocab_size + opt.max_unk_words)\n    weight_mask[opt.word2id[pykp.IO.PAD_WORD]] = 0\n    criterion = torch.nn.NLLLoss(weight=weight_mask)\n\n    optimizer = Adam(params=filter(lambda p: p.requires_grad, model.parameters()), lr=opt.learning_rate)\n    # optimizer = torch.optim.Adadelta(model.parameters(), lr=0.1)\n    # optimizer = torch.optim.RMSprop(model.parameters(), lr=0.1)\n    '''", "\n", "criterion", "=", "torch", ".", "nn", ".", "NLLLoss", "(", "ignore_index", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "PAD_WORD", "]", ")", ".", "to", "(", "opt", ".", "device", ")", "\n", "\n", "if", "opt", ".", "train_ml", ":", "\n", "        ", "optimizer_ml", "=", "Adam", "(", "params", "=", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", ",", "lr", "=", "opt", ".", "learning_rate", ")", "\n", "", "else", ":", "\n", "        ", "optimizer_ml", "=", "None", "\n", "\n", "", "if", "opt", ".", "train_rl", ":", "\n", "        ", "optimizer_rl", "=", "Adam", "(", "params", "=", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", ",", "lr", "=", "opt", ".", "learning_rate_rl", ")", "\n", "", "else", ":", "\n", "        ", "optimizer_rl", "=", "None", "\n", "\n", "", "return", "optimizer_ml", ",", "optimizer_rl", ",", "criterion", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train.init_model": [[128, 152], ["logging.info", "pykp.model.Seq2SeqModel", "pykp.model.Seq2SeqModel.to", "logging.info", "logging.info", "logging.info", "pykp.model.Seq2SeqModel.load_state_dict", "torch.load"], "function", ["None"], ["", "def", "init_model", "(", "opt", ")", ":", "\n", "    ", "logging", ".", "info", "(", "'======================  Model Parameters  ========================='", ")", "\n", "\n", "if", "opt", ".", "copy_attention", ":", "\n", "        ", "logging", ".", "info", "(", "'Training a seq2seq model with copy mechanism'", ")", "\n", "", "else", ":", "\n", "        ", "logging", ".", "info", "(", "'Training a seq2seq model'", ")", "\n", "", "model", "=", "Seq2SeqModel", "(", "opt", ")", "\n", "\n", "if", "opt", ".", "train_from", ":", "\n", "        ", "logging", ".", "info", "(", "\"loading previous checkpoint from %s\"", "%", "opt", ".", "train_from", ")", "\n", "# TODO: load the saved model and override the current one", "\n", "", "elif", "opt", ".", "train_rl", "and", "opt", ".", "pretrained_model", "!=", "\"\"", ":", "\n", "        ", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "opt", ".", "pretrained_model", ")", ")", "\n", "\"\"\"\n        pretrained_state_dict = torch.load(opt.pretrained_model)\n        pretrained_state_dict_renamed = {}\n        for k, v in pretrained_state_dict.items():\n            if k.startswith(\"encoder.rnn.\"):\n                k = k.replace(\"encoder.rnn.\", \"encoder.encoder.rnn.\", 1)\n            pretrained_state_dict_renamed[k] = v\n        model.load_state_dict(pretrained_state_dict_renamed)\n        \"\"\"", "\n", "", "return", "model", ".", "to", "(", "opt", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train.main": [[154, 172], ["time.time", "utils.data_loader.load_data_and_vocab", "utils.time_log.time_since", "logging.info", "time.time", "train.init_model", "train.init_optimizer_criterion", "utils.time_log.time_since", "logging.info", "train_ml.train_model", "train_rl.train_model", "logging.exception"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.data_loader.load_data_and_vocab", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train.init_model", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train.init_optimizer_criterion", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_ml.train_model", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_ml.train_model"], ["", "def", "main", "(", "opt", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "train_data_loader", ",", "valid_data_loader", ",", "word2idx", ",", "idx2word", ",", "vocab", "=", "load_data_and_vocab", "(", "opt", ",", "load_train", "=", "True", ")", "\n", "load_data_time", "=", "time_since", "(", "start_time", ")", "\n", "logging", ".", "info", "(", "'Time for loading the data: %.1f'", "%", "load_data_time", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "model", "=", "init_model", "(", "opt", ")", "\n", "optimizer_ml", ",", "optimizer_rl", ",", "criterion", "=", "init_optimizer_criterion", "(", "model", ",", "opt", ")", "\n", "if", "opt", ".", "train_ml", ":", "\n", "            ", "train_ml", ".", "train_model", "(", "model", ",", "optimizer_ml", ",", "optimizer_rl", ",", "criterion", ",", "train_data_loader", ",", "valid_data_loader", ",", "opt", ")", "\n", "", "else", ":", "\n", "            ", "train_rl", ".", "train_model", "(", "model", ",", "optimizer_ml", ",", "optimizer_rl", ",", "criterion", ",", "train_data_loader", ",", "valid_data_loader", ",", "opt", ")", "\n", "", "training_time", "=", "time_since", "(", "start_time", ")", "\n", "logging", ".", "info", "(", "'Time for training: %.1f'", "%", "training_time", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "logging", ".", "exception", "(", "\"message\"", ")", "\n", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_ml.train_model": [[18, 158], ["logging.info", "utils.statistics.LossStatistics", "utils.statistics.LossStatistics", "float", "float", "model.train", "range", "utils.report.export_train_and_valid_loss", "utils.source_representation_queue.SourceRepresentationQueue", "ValueError", "enumerate", "train_ml.train_one_batch", "utils.statistics.LossStatistics.update", "utils.statistics.LossStatistics.update", "print", "sys.stdout.flush", "evaluate.evaluate_loss", "model.train", "evaluate.evaluate_loss.xent", "evaluate.evaluate_loss.ppl", "print", "sys.stdout.flush", "utils.statistics.LossStatistics.ppl", "utils.statistics.LossStatistics.xent", "logging.info", "logging.info", "logging.info", "report_train_ppl.append", "report_valid_ppl.append", "report_train_loss.append", "report_valid_loss.append", "utils.statistics.LossStatistics.clear", "math.isnan", "math.isnan", "logging.info", "exit", "print", "sys.stdout.flush", "os.path.join", "torch.save", "torch.save", "logging.info", "print", "sys.stdout.flush", "enumerate", "logging.info", "len", "model.state_dict", "open", "float"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.report.export_train_and_valid_loss", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_ml.train_one_batch", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.update", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.update", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate.evaluate_loss", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.LossStatistics.xent", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.LossStatistics.ppl", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.LossStatistics.ppl", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.LossStatistics.xent", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.clear"], ["def", "train_model", "(", "model", ",", "optimizer_ml", ",", "optimizer_rl", ",", "criterion", ",", "train_data_loader", ",", "valid_data_loader", ",", "opt", ")", ":", "\n", "    ", "'''\n    generator = SequenceGenerator(model,\n                                  eos_idx=opt.word2idx[pykp.io.EOS_WORD],\n                                  beam_size=opt.beam_size,\n                                  max_sequence_length=opt.max_sent_length\n                                  )\n    '''", "\n", "logging", ".", "info", "(", "'======================  Start Training  ========================='", ")", "\n", "\n", "total_batch", "=", "-", "1", "\n", "early_stop_flag", "=", "False", "\n", "\n", "total_train_loss_statistics", "=", "LossStatistics", "(", ")", "\n", "report_train_loss_statistics", "=", "LossStatistics", "(", ")", "\n", "report_train_ppl", "=", "[", "]", "\n", "report_valid_ppl", "=", "[", "]", "\n", "report_train_loss", "=", "[", "]", "\n", "report_valid_loss", "=", "[", "]", "\n", "best_valid_ppl", "=", "float", "(", "'inf'", ")", "\n", "best_valid_loss", "=", "float", "(", "'inf'", ")", "\n", "num_stop_dropping", "=", "0", "\n", "\n", "if", "opt", ".", "use_target_encoder", ":", "\n", "        ", "source_representation_queue", "=", "SourceRepresentationQueue", "(", "opt", ".", "source_representation_queue_size", ")", "\n", "", "else", ":", "\n", "        ", "source_representation_queue", "=", "None", "\n", "\n", "", "if", "opt", ".", "train_from", ":", "# opt.train_from:", "\n", "#TODO: load the training state", "\n", "        ", "raise", "ValueError", "(", "\"Not implemented the function of load from trained model\"", ")", "\n", "pass", "\n", "\n", "", "model", ".", "train", "(", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "opt", ".", "start_epoch", ",", "opt", ".", "epochs", "+", "1", ")", ":", "\n", "        ", "if", "early_stop_flag", ":", "\n", "            ", "break", "\n", "\n", "# TODO: progress bar", "\n", "#progbar = Progbar(logger=logging, title='Training', target=len(train_data_loader), batch_size=train_data_loader.batch_size,total_examples=len(train_data_loader.dataset.examples))", "\n", "\n", "", "for", "batch_i", ",", "batch", "in", "enumerate", "(", "train_data_loader", ")", ":", "\n", "            ", "total_batch", "+=", "1", "\n", "\n", "# Training", "\n", "if", "opt", ".", "train_ml", ":", "\n", "                ", "batch_loss_stat", ",", "decoder_dist", "=", "train_one_batch", "(", "batch", ",", "model", ",", "optimizer_ml", ",", "opt", ",", "batch_i", ",", "source_representation_queue", ")", "\n", "report_train_loss_statistics", ".", "update", "(", "batch_loss_stat", ")", "\n", "total_train_loss_statistics", ".", "update", "(", "batch_loss_stat", ")", "\n", "#logging.info(\"one_batch\")", "\n", "#report_loss.append(('train_ml_loss', loss_ml))", "\n", "#report_loss.append(('PPL', loss_ml))", "\n", "\n", "# Brief report", "\n", "'''\n                if batch_i % opt.report_every == 0:\n                    brief_report(epoch, batch_i, one2one_batch, loss_ml, decoder_log_probs, opt)\n                '''", "\n", "\n", "#progbar.update(epoch, batch_i, report_loss)", "\n", "\n", "# Checkpoint, decay the learning rate if validation loss stop dropping, apply early stopping if stop decreasing for several epochs.", "\n", "# Save the model parameters if the validation loss improved.", "\n", "", "if", "total_batch", "%", "4000", "==", "0", ":", "\n", "                ", "print", "(", "\"Epoch %d; batch: %d; total batch: %d\"", "%", "(", "epoch", ",", "batch_i", ",", "total_batch", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "if", "epoch", ">=", "opt", ".", "start_checkpoint_at", ":", "\n", "                ", "if", "(", "opt", ".", "checkpoint_interval", "==", "-", "1", "and", "batch_i", "==", "len", "(", "train_data_loader", ")", "-", "1", ")", "or", "(", "opt", ".", "checkpoint_interval", ">", "-", "1", "and", "total_batch", ">", "1", "and", "total_batch", "%", "opt", ".", "checkpoint_interval", "==", "0", ")", ":", "\n", "                    ", "if", "opt", ".", "train_ml", ":", "\n", "# test the model on the validation dataset for one epoch", "\n", "                        ", "valid_loss_stat", "=", "evaluate_loss", "(", "valid_data_loader", ",", "model", ",", "opt", ")", "\n", "model", ".", "train", "(", ")", "\n", "current_valid_loss", "=", "valid_loss_stat", ".", "xent", "(", ")", "\n", "current_valid_ppl", "=", "valid_loss_stat", ".", "ppl", "(", ")", "\n", "print", "(", "\"Enter check point!\"", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "current_train_ppl", "=", "report_train_loss_statistics", ".", "ppl", "(", ")", "\n", "current_train_loss", "=", "report_train_loss_statistics", ".", "xent", "(", ")", "\n", "\n", "# debug", "\n", "if", "math", ".", "isnan", "(", "current_valid_loss", ")", "or", "math", ".", "isnan", "(", "current_train_loss", ")", ":", "\n", "                            ", "logging", ".", "info", "(", "\n", "\"NaN valid loss. Epoch: %d; batch_i: %d, total_batch: %d\"", "%", "(", "epoch", ",", "batch_i", ",", "total_batch", ")", ")", "\n", "exit", "(", ")", "\n", "\n", "", "if", "current_valid_loss", "<", "best_valid_loss", ":", "# update the best valid loss and save the model parameters", "\n", "                            ", "print", "(", "\"Valid loss drops\"", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "best_valid_loss", "=", "current_valid_loss", "\n", "best_valid_ppl", "=", "current_valid_ppl", "\n", "num_stop_dropping", "=", "0", "\n", "\n", "check_pt_model_path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "model_path", ",", "'%s.epoch=%d.batch=%d.total_batch=%d'", "%", "(", "\n", "opt", ".", "exp", ",", "epoch", ",", "batch_i", ",", "total_batch", ")", "+", "'.model'", ")", "\n", "torch", ".", "save", "(", "# save model parameters", "\n", "model", ".", "state_dict", "(", ")", ",", "\n", "open", "(", "check_pt_model_path", ",", "'wb'", ")", "\n", ")", "\n", "logging", ".", "info", "(", "'Saving checkpoint to %s'", "%", "check_pt_model_path", ")", "\n", "\n", "", "else", ":", "\n", "                            ", "print", "(", "\"Valid loss does not drop\"", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "num_stop_dropping", "+=", "1", "\n", "# decay the learning rate by a factor", "\n", "for", "i", ",", "param_group", "in", "enumerate", "(", "optimizer_ml", ".", "param_groups", ")", ":", "\n", "                                ", "old_lr", "=", "float", "(", "param_group", "[", "'lr'", "]", ")", "\n", "new_lr", "=", "old_lr", "*", "opt", ".", "learning_rate_decay", "\n", "if", "old_lr", "-", "new_lr", ">", "EPS", ":", "\n", "                                    ", "param_group", "[", "'lr'", "]", "=", "new_lr", "\n", "\n", "# log loss, ppl, and time", "\n", "#print(\"check point!\")", "\n", "#sys.stdout.flush()", "\n", "", "", "", "logging", ".", "info", "(", "'Epoch: %d; batch idx: %d; total batches: %d'", "%", "(", "epoch", ",", "batch_i", ",", "total_batch", ")", ")", "\n", "logging", ".", "info", "(", "\n", "'avg training ppl: %.3f; avg validation ppl: %.3f; best validation ppl: %.3f'", "%", "(", "\n", "current_train_ppl", ",", "current_valid_ppl", ",", "best_valid_ppl", ")", ")", "\n", "logging", ".", "info", "(", "\n", "'avg training loss: %.3f; avg validation loss: %.3f; best validation loss: %.3f'", "%", "(", "\n", "current_train_loss", ",", "current_valid_loss", ",", "best_valid_loss", ")", ")", "\n", "\n", "report_train_ppl", ".", "append", "(", "current_train_ppl", ")", "\n", "report_valid_ppl", ".", "append", "(", "current_valid_ppl", ")", "\n", "report_train_loss", ".", "append", "(", "current_train_loss", ")", "\n", "report_valid_loss", ".", "append", "(", "current_valid_loss", ")", "\n", "\n", "if", "num_stop_dropping", ">=", "opt", ".", "early_stop_tolerance", ":", "\n", "                            ", "logging", ".", "info", "(", "'Have not increased for %d check points, early stop training'", "%", "num_stop_dropping", ")", "\n", "early_stop_flag", "=", "True", "\n", "break", "\n", "", "report_train_loss_statistics", ".", "clear", "(", ")", "\n", "\n", "# export the training curve", "\n", "", "", "", "", "", "train_valid_curve_path", "=", "opt", ".", "exp_path", "+", "'/train_valid_curve'", "\n", "export_train_and_valid_loss", "(", "report_train_loss", ",", "report_valid_loss", ",", "report_train_ppl", ",", "report_valid_ppl", ",", "opt", ".", "checkpoint_interval", ",", "train_valid_curve_path", ")", "\n", "#logging.info('Overall average training loss: %.3f, ppl: %.3f' % (total_train_loss_statistics.xent(), total_train_loss_statistics.ppl()))", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_ml.train_one_batch": [[160, 333], ["src.to.size", "max", "src.to.to", "src_mask.to.to", "trg.to.to", "trg_mask.to.to", "src_oov.to.to", "trg_oov.to.to", "optimizer.zero_grad", "time.time", "utils.time_log.time_since", "time.time", "utils.time_log.time_since", "sum", "math.isnan", "time.time", "pykp.masked_loss.masked_cross_entropy.div().backward", "utils.time_log.time_since", "optimizer.step", "utils.statistics.LossStatistics", "title.to.to", "title_mask.to.to", "model", "model", "pykp.masked_loss.masked_cross_entropy", "pykp.masked_loss.masked_cross_entropy", "pykp.masked_loss.masked_cross_entropy.item", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "ValueError", "torch.utils.clip_grad_norm_", "time.time", "optimizer.zero_grad", "delimiter_decoder_states.size", "torch.LongTensor().to", "torch.LongTensor().to", "source_representation_target.view().repeat.view().repeat", "torch.zeros().to", "torch.zeros().to", "range", "pykp.masked_loss.masked_cross_entropy", "utils.time_log.time_since", "time.time", "pykp.masked_loss.masked_cross_entropy.div().backward", "utils.time_log.time_since", "optimizer.step", "pykp.masked_loss.masked_cross_entropy.item", "decoder_dist.detach", "len", "len", "len", "range", "source_representation_queue.put", "src.to.size", "ValueError", "pykp.masked_loss.masked_cross_entropy.div", "model.parameters", "source_representation_target_mask[].fill_", "torch.utils.clip_grad_norm_", "source_representation_queue.sample", "numpy.random.randint", "source_representation_queue.sample.insert", "source_representation_samples_2dlist.append", "source_representation_target_list.append", "encoder_final_state[].detach", "range", "torch.LongTensor", "torch.LongTensor", "source_representation_target.view().repeat.view", "torch.zeros", "torch.zeros", "pykp.masked_loss.masked_cross_entropy.div", "model.parameters"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.masked_cross_entropy", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.masked_cross_entropy", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.masked_cross_entropy", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.put", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.sample"], ["", "def", "train_one_batch", "(", "batch", ",", "model", ",", "optimizer", ",", "opt", ",", "batch_i", ",", "source_representation_queue", "=", "None", ")", ":", "\n", "    ", "if", "not", "opt", ".", "one2many", ":", "# load one2one data", "\n", "        ", "src", ",", "src_lens", ",", "src_mask", ",", "trg", ",", "trg_lens", ",", "trg_mask", ",", "src_oov", ",", "trg_oov", ",", "oov_lists", ",", "title", ",", "title_oov", ",", "title_lens", ",", "title_mask", "=", "batch", "\n", "\"\"\"\n        src: a LongTensor containing the word indices of source sentences, [batch, src_seq_len], with oov words replaced by unk idx\n        src_lens: a list containing the length of src sequences for each batch, with len=batch\n        src_mask: a FloatTensor, [batch, src_seq_len]\n        trg: a LongTensor containing the word indices of target sentences, [batch, trg_seq_len]\n        trg_lens: a list containing the length of trg sequences for each batch, with len=batch\n        trg_mask: a FloatTensor, [batch, trg_seq_len]\n        src_oov: a LongTensor containing the word indices of source sentences, [batch, src_seq_len], contains the index of oov words (used by copy)\n        trg_oov: a LongTensor containing the word indices of target sentences, [batch, src_seq_len], contains the index of oov words (used by copy)\n        \"\"\"", "\n", "", "else", ":", "# load one2many data", "\n", "        ", "src", ",", "src_lens", ",", "src_mask", ",", "src_oov", ",", "oov_lists", ",", "src_str_list", ",", "trg_str_2dlist", ",", "trg", ",", "trg_oov", ",", "trg_lens", ",", "trg_mask", ",", "_", ",", "title", ",", "title_oov", ",", "title_lens", ",", "title_mask", "=", "batch", "\n", "num_trgs", "=", "[", "len", "(", "trg_str_list", ")", "for", "trg_str_list", "in", "trg_str_2dlist", "]", "# a list of num of targets in each batch, with len=batch_size", "\n", "\"\"\"\n        trg: LongTensor [batch, trg_seq_len], each target trg[i] contains the indices of a set of concatenated keyphrases, separated by opt.word2idx[pykp.io.SEP_WORD]\n             if opt.delimiter_type = 0, SEP_WORD=<sep>, if opt.delimiter_type = 1, SEP_WORD=<eos>\n        trg_oov: same as trg_oov, but all unk words are replaced with temporary idx, e.g. 50000, 50001 etc.\n        \"\"\"", "\n", "", "batch_size", "=", "src", ".", "size", "(", "0", ")", "\n", "max_num_oov", "=", "max", "(", "[", "len", "(", "oov", ")", "for", "oov", "in", "oov_lists", "]", ")", "# max number of oov for each batch", "\n", "\n", "# move data to GPU if available", "\n", "src", "=", "src", ".", "to", "(", "opt", ".", "device", ")", "\n", "src_mask", "=", "src_mask", ".", "to", "(", "opt", ".", "device", ")", "\n", "trg", "=", "trg", ".", "to", "(", "opt", ".", "device", ")", "\n", "trg_mask", "=", "trg_mask", ".", "to", "(", "opt", ".", "device", ")", "\n", "src_oov", "=", "src_oov", ".", "to", "(", "opt", ".", "device", ")", "\n", "trg_oov", "=", "trg_oov", ".", "to", "(", "opt", ".", "device", ")", "\n", "if", "opt", ".", "title_guided", ":", "\n", "        ", "title", "=", "title", ".", "to", "(", "opt", ".", "device", ")", "\n", "title_mask", "=", "title_mask", ".", "to", "(", "opt", ".", "device", ")", "\n", "#title_oov = title_oov.to(opt.device)", "\n", "# title, title_oov, title_lens, title_mask", "\n", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "#if opt.one2many_mode == 0 or opt.one2many_mode == 1:", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "opt", ".", "use_target_encoder", ":", "# Sample encoder representations", "\n", "        ", "if", "len", "(", "source_representation_queue", ")", "<", "opt", ".", "source_representation_sample_size", ":", "\n", "            ", "source_representation_samples_2dlist", "=", "None", "\n", "source_representation_target_list", "=", "None", "\n", "", "else", ":", "\n", "            ", "source_representation_samples_2dlist", "=", "[", "]", "\n", "source_representation_target_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "# N encoder representation from the queue", "\n", "                ", "source_representation_samples_list", "=", "source_representation_queue", ".", "sample", "(", "opt", ".", "source_representation_sample_size", ")", "\n", "# insert a place-holder for the ground-truth source representation to a random index", "\n", "place_holder_idx", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "opt", ".", "source_representation_sample_size", "+", "1", ")", "\n", "source_representation_samples_list", ".", "insert", "(", "place_holder_idx", ",", "None", ")", "# len=N+1", "\n", "# insert the sample list of one batch to the 2d list", "\n", "source_representation_samples_2dlist", ".", "append", "(", "source_representation_samples_list", ")", "\n", "# store the idx of place-holder for that batch", "\n", "source_representation_target_list", ".", "append", "(", "place_holder_idx", ")", "\n", "", "", "", "else", ":", "\n", "        ", "source_representation_samples_2dlist", "=", "None", "\n", "source_representation_target_list", "=", "None", "\n", "\n", "\"\"\"\n        if encoder_representation_samples_2dlist[0] is None and batch_i > math.ceil(\n                opt.encoder_representation_sample_size / batch_size):\n            # a return value of none indicates we don't have sufficient samples\n            # it will only occurs in the first few training steps\n            raise ValueError(\"encoder_representation_samples should not be none at this batch!\")\n        \"\"\"", "\n", "\n", "", "if", "not", "opt", ".", "one2many", ":", "\n", "        ", "decoder_dist", ",", "h_t", ",", "attention_dist", ",", "encoder_final_state", ",", "coverage", ",", "delimiter_decoder_states", ",", "delimiter_decoder_states_lens", ",", "source_classification_dist", "=", "model", "(", "src", ",", "src_lens", ",", "trg", ",", "src_oov", ",", "max_num_oov", ",", "src_mask", ",", "sampled_source_representation_2dlist", "=", "source_representation_samples_2dlist", ",", "source_representation_target_list", "=", "source_representation_target_list", ",", "title", "=", "title", ",", "title_lens", "=", "title_lens", ",", "title_mask", "=", "title_mask", ")", "\n", "", "else", ":", "\n", "        ", "decoder_dist", ",", "h_t", ",", "attention_dist", ",", "encoder_final_state", ",", "coverage", ",", "delimiter_decoder_states", ",", "delimiter_decoder_states_lens", ",", "source_classification_dist", "=", "model", "(", "src", ",", "src_lens", ",", "trg", ",", "src_oov", ",", "max_num_oov", ",", "src_mask", ",", "num_trgs", "=", "num_trgs", ",", "sampled_source_representation_2dlist", "=", "source_representation_samples_2dlist", ",", "source_representation_target_list", "=", "source_representation_target_list", ",", "title", "=", "title", ",", "title_lens", "=", "title_lens", ",", "title_mask", "=", "title_mask", ")", "\n", "", "forward_time", "=", "time_since", "(", "start_time", ")", "\n", "\n", "if", "opt", ".", "use_target_encoder", ":", "# Put all the encoder final states to the queue. Need to call detach() first", "\n", "# encoder_final_state: [batch, memory_bank_size]", "\n", "        ", "[", "source_representation_queue", ".", "put", "(", "encoder_final_state", "[", "i", ",", ":", "]", ".", "detach", "(", ")", ")", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "", "start_time", "=", "time", ".", "time", "(", ")", "\n", "if", "opt", ".", "copy_attention", ":", "# Compute the loss using target with oov words", "\n", "        ", "loss", "=", "masked_cross_entropy", "(", "decoder_dist", ",", "trg_oov", ",", "trg_mask", ",", "trg_lens", ",", "\n", "opt", ".", "coverage_attn", ",", "coverage", ",", "attention_dist", ",", "opt", ".", "lambda_coverage", ",", "opt", ".", "coverage_loss", ",", "delimiter_decoder_states", ",", "opt", ".", "orthogonal_loss", ",", "opt", ".", "lambda_orthogonal", ",", "delimiter_decoder_states_lens", ")", "\n", "", "else", ":", "# Compute the loss using target without oov words", "\n", "        ", "loss", "=", "masked_cross_entropy", "(", "decoder_dist", ",", "trg", ",", "trg_mask", ",", "trg_lens", ",", "\n", "opt", ".", "coverage_attn", ",", "coverage", ",", "attention_dist", ",", "opt", ".", "lambda_coverage", ",", "opt", ".", "coverage_loss", ",", "delimiter_decoder_states", ",", "opt", ".", "orthogonal_loss", ",", "opt", ".", "lambda_orthogonal", ",", "delimiter_decoder_states_lens", ")", "\n", "\n", "", "loss_compute_time", "=", "time_since", "(", "start_time", ")", "\n", "\n", "#else:  # opt.one2many_mode == 2", "\n", "#    forward_time = 0", "\n", "#    loss_compute_time = 0", "\n", "#    # TODO: a for loop to accumulate loss for each keyphrase", "\n", "#    # TODO: meanwhile, accumulate the forward time and loss_compute time", "\n", "#    pass", "\n", "\n", "total_trg_tokens", "=", "sum", "(", "trg_lens", ")", "\n", "\n", "if", "math", ".", "isnan", "(", "loss", ".", "item", "(", ")", ")", ":", "\n", "        ", "print", "(", "\"Batch i: %d\"", "%", "batch_i", ")", "\n", "print", "(", "\"src\"", ")", "\n", "print", "(", "src", ")", "\n", "print", "(", "src_oov", ")", "\n", "print", "(", "src_str_list", ")", "\n", "print", "(", "src_lens", ")", "\n", "print", "(", "src_mask", ")", "\n", "print", "(", "\"trg\"", ")", "\n", "print", "(", "trg", ")", "\n", "print", "(", "trg_oov", ")", "\n", "print", "(", "trg_str_2dlist", ")", "\n", "print", "(", "trg_lens", ")", "\n", "print", "(", "trg_mask", ")", "\n", "print", "(", "\"oov list\"", ")", "\n", "print", "(", "oov_lists", ")", "\n", "print", "(", "\"Decoder\"", ")", "\n", "print", "(", "decoder_dist", ")", "\n", "print", "(", "h_t", ")", "\n", "print", "(", "attention_dist", ")", "\n", "raise", "ValueError", "(", "\"Loss is NaN\"", ")", "\n", "\n", "", "if", "opt", ".", "loss_normalization", "==", "\"tokens\"", ":", "# use number of target tokens to normalize the loss", "\n", "        ", "normalization", "=", "total_trg_tokens", "\n", "", "elif", "opt", ".", "loss_normalization", "==", "'batches'", ":", "# use batch_size to normalize the loss", "\n", "        ", "normalization", "=", "src", ".", "size", "(", "0", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'The type of loss normalization is invalid.'", ")", "\n", "\n", "", "assert", "normalization", ">", "0", ",", "'normalization should be a positive number'", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "# back propagation on the normalized loss", "\n", "loss", ".", "div", "(", "normalization", ")", ".", "backward", "(", ")", "\n", "backward_time", "=", "time_since", "(", "start_time", ")", "\n", "\n", "if", "opt", ".", "max_grad_norm", ">", "0", ":", "\n", "        ", "grad_norm_before_clipping", "=", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "opt", ".", "max_grad_norm", ")", "\n", "# grad_norm_after_clipping = (sum([p.grad.data.norm(2) ** 2 for p in model.parameters() if p.grad is not None])) ** (1.0 / 2)", "\n", "# logging.info('clip grad (%f -> %f)' % (grad_norm_before_clipping, grad_norm_after_clipping))", "\n", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "\n", "# Compute target encoder loss", "\n", "if", "opt", ".", "use_target_encoder", "and", "source_classification_dist", "is", "not", "None", ":", "\n", "        ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "# convert source_representation_target_list to a LongTensor with size=[batch_size, max_num_delimiters]", "\n", "max_num_delimiters", "=", "delimiter_decoder_states", ".", "size", "(", "2", ")", "\n", "source_representation_target", "=", "torch", ".", "LongTensor", "(", "source_representation_target_list", ")", ".", "to", "(", "trg", ".", "device", ")", "# [batch_size]", "\n", "# expand along the second dimension, since for the target for each delimiter states in the same batch are the same", "\n", "source_representation_target", "=", "source_representation_target", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "max_num_delimiters", ")", "# [batch_size, max_num_delimiters]", "\n", "# mask for source representation classification", "\n", "source_representation_target_mask", "=", "torch", ".", "zeros", "(", "batch_size", ",", "max_num_delimiters", ")", ".", "to", "(", "trg", ".", "device", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "source_representation_target_mask", "[", "i", ",", ":", "delimiter_decoder_states_lens", "[", "i", "]", "]", ".", "fill_", "(", "1", ")", "\n", "# compute the masked loss", "\n", "", "loss_te", "=", "masked_cross_entropy", "(", "source_classification_dist", ",", "source_representation_target", ",", "source_representation_target_mask", ")", "\n", "loss_compute_time", "+=", "time_since", "(", "start_time", ")", "\n", "# back propagation on the normalized loss", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "loss_te", ".", "div", "(", "normalization", ")", ".", "backward", "(", ")", "\n", "backward_time", "+=", "time_since", "(", "start_time", ")", "\n", "\n", "if", "opt", ".", "max_grad_norm", ">", "0", ":", "\n", "            ", "grad_norm_before_clipping", "=", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "opt", ".", "max_grad_norm", ")", "\n", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "\n", "# construct a statistic object for the loss", "\n", "", "stat", "=", "LossStatistics", "(", "loss", ".", "item", "(", ")", ",", "total_trg_tokens", ",", "n_batch", "=", "1", ",", "forward_time", "=", "forward_time", ",", "loss_compute_time", "=", "loss_compute_time", ",", "backward_time", "=", "backward_time", ")", "\n", "\n", "return", "stat", ",", "decoder_dist", ".", "detach", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.read_tokenized_src_file": [[35, 73], ["enumerate", "open", "src_line.strip().split", "tokenized_train_src.append", "len", "context.strip().split", "tokenized_train_title.append", "src_line.strip", "ValueError", "len", "title.strip().split", "context.strip().split", "ValueError", "context.strip", "title.strip", "context.strip"], "function", ["None"], ["", "def", "read_tokenized_src_file", "(", "path", ",", "remove_eos", "=", "True", ",", "title_guided", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    read tokenized source text file and convert them to list of list of words\n    :param path:\n    :param remove_eos: concatenate the words in title and content\n    :return: data, a 2d list, each item in the list is a list of words of a src text, len(data) = num_lines\n    \"\"\"", "\n", "tokenized_train_src", "=", "[", "]", "\n", "if", "title_guided", ":", "\n", "        ", "tokenized_train_title", "=", "[", "]", "\n", "", "filtered_cnt", "=", "0", "\n", "for", "line_idx", ",", "src_line", "in", "enumerate", "(", "open", "(", "path", ",", "'r'", ")", ")", ":", "\n", "# process source line", "\n", "        ", "title_and_context", "=", "src_line", ".", "strip", "(", ")", ".", "split", "(", "'<eos>'", ")", "\n", "if", "len", "(", "title_and_context", ")", "==", "1", ":", "# it only has context without title", "\n", "            ", "[", "context", "]", "=", "title_and_context", "\n", "src_word_list", "=", "context", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "if", "title_guided", ":", "\n", "                ", "raise", "ValueError", "(", "\"The source text does not contains any title, so you cannot return title.\"", ")", "\n", "", "", "elif", "len", "(", "title_and_context", ")", "==", "2", ":", "\n", "            ", "[", "title", ",", "context", "]", "=", "title_and_context", "\n", "title_word_list", "=", "title", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "context_word_list", "=", "context", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "if", "remove_eos", ":", "\n", "                ", "src_word_list", "=", "title_word_list", "+", "context_word_list", "\n", "", "else", ":", "\n", "                ", "src_word_list", "=", "title_word_list", "+", "[", "'<eos>'", "]", "+", "context_word_list", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"The source text contains more than one title\"", ")", "\n", "# Append the lines to the data", "\n", "", "tokenized_train_src", ".", "append", "(", "src_word_list", ")", "\n", "if", "title_guided", ":", "\n", "            ", "tokenized_train_title", ".", "append", "(", "title_word_list", ")", "\n", "\n", "", "", "if", "title_guided", ":", "\n", "        ", "return", "tokenized_train_src", ",", "tokenized_train_title", "\n", "", "else", ":", "\n", "        ", "return", "tokenized_train_src", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.read_tokenized_trg_file": [[75, 88], ["open", "line.strip().split", "data.append", "trg.split", "line.strip"], "function", ["None"], ["", "", "def", "read_tokenized_trg_file", "(", "path", ")", ":", "\n", "    ", "\"\"\"\n    read tokenized target text file and convert them to list of list of words\n    :param path:\n    :return: data, a 3d list, each item in the list is a list of target, each target is a list of words.\n    \"\"\"", "\n", "data", "=", "[", "]", "\n", "with", "open", "(", "path", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "trg_list", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "';'", ")", "# a list of target sequences", "\n", "trg_word_list", "=", "[", "trg", ".", "split", "(", "' '", ")", "for", "trg", "in", "trg_list", "]", "\n", "data", ".", "append", "(", "trg_word_list", ")", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.read_src_and_trg_files": [[90, 141], ["enumerate", "print", "list", "zip", "src_line.strip().split", "trg_line.strip().split", "tokenized_train_src.append", "tokenized_train_trg.append", "len", "len", "zip", "open", "open", "len", "context.strip().split", "trg.split", "tokenized_train_title.append", "len", "src_line.strip", "ValueError", "len", "title.strip().split", "context.strip().split", "ValueError", "trg_line.strip", "src_line.strip", "context.strip", "len", "len", "title.strip", "context.strip"], "function", ["None"], ["", "def", "read_src_and_trg_files", "(", "src_file", ",", "trg_file", ",", "is_train", ",", "remove_eos", "=", "True", ",", "title_guided", "=", "False", ")", ":", "\n", "    ", "tokenized_train_src", "=", "[", "]", "\n", "tokenized_train_trg", "=", "[", "]", "\n", "if", "title_guided", ":", "\n", "        ", "tokenized_train_title", "=", "[", "]", "\n", "", "filtered_cnt", "=", "0", "\n", "for", "line_idx", ",", "(", "src_line", ",", "trg_line", ")", "in", "enumerate", "(", "zip", "(", "open", "(", "src_file", ",", "'r'", ")", ",", "open", "(", "trg_file", ",", "'r'", ")", ")", ")", ":", "\n", "# process source line", "\n", "        ", "if", "(", "len", "(", "src_line", ".", "strip", "(", ")", ")", "==", "0", ")", "and", "is_train", ":", "\n", "            ", "continue", "\n", "", "title_and_context", "=", "src_line", ".", "strip", "(", ")", ".", "split", "(", "'<eos>'", ")", "\n", "if", "len", "(", "title_and_context", ")", "==", "1", ":", "# it only has context without title", "\n", "            ", "[", "context", "]", "=", "title_and_context", "\n", "src_word_list", "=", "context", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "if", "title_guided", ":", "\n", "                ", "raise", "ValueError", "(", "\"The source text does not contains any title, so you cannot return title.\"", ")", "\n", "", "", "elif", "len", "(", "title_and_context", ")", "==", "2", ":", "\n", "            ", "[", "title", ",", "context", "]", "=", "title_and_context", "\n", "title_word_list", "=", "title", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "context_word_list", "=", "context", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "if", "remove_eos", ":", "\n", "                ", "src_word_list", "=", "title_word_list", "+", "context_word_list", "\n", "", "else", ":", "\n", "                ", "src_word_list", "=", "title_word_list", "+", "[", "'<eos>'", "]", "+", "context_word_list", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"The source text contains more than one title\"", ")", "\n", "# process target line", "\n", "", "trg_list", "=", "trg_line", ".", "strip", "(", ")", ".", "split", "(", "';'", ")", "# a list of target sequences", "\n", "trg_word_list", "=", "[", "trg", ".", "split", "(", "' '", ")", "for", "trg", "in", "trg_list", "]", "\n", "# If it is training data, ignore the line with source length > 400 or target length > 60", "\n", "if", "is_train", ":", "\n", "            ", "if", "len", "(", "src_word_list", ")", ">", "400", "or", "len", "(", "trg_word_list", ")", ">", "14", ":", "\n", "                ", "filtered_cnt", "+=", "1", "\n", "continue", "\n", "# Append the lines to the data", "\n", "", "", "tokenized_train_src", ".", "append", "(", "src_word_list", ")", "\n", "tokenized_train_trg", ".", "append", "(", "trg_word_list", ")", "\n", "if", "title_guided", ":", "\n", "            ", "tokenized_train_title", ".", "append", "(", "title_word_list", ")", "\n", "\n", "", "", "assert", "len", "(", "tokenized_train_src", ")", "==", "len", "(", "\n", "tokenized_train_trg", ")", ",", "'the number of records in source and target are not the same'", "\n", "\n", "print", "(", "\"%d rows filtered\"", "%", "filtered_cnt", ")", "\n", "\n", "tokenized_train_pairs", "=", "list", "(", "zip", "(", "tokenized_train_src", ",", "tokenized_train_trg", ")", ")", "\n", "\n", "if", "title_guided", ":", "\n", "        ", "return", "tokenized_train_pairs", ",", "tokenized_train_title", "\n", "", "else", ":", "\n", "        ", "return", "tokenized_train_pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.build_vocab": [[143, 178], ["collections.Counter", "len", "dict", "dict", "enumerate", "sorted", "enumerate", "enumerate", "collections.Counter.update", "special_tokens.append", "collections.Counter.items", "collections.Counter.update"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.update", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.update"], ["", "", "def", "build_vocab", "(", "tokenized_src_trg_pairs", ",", "include_peos", ")", ":", "\n", "    ", "token_freq_counter", "=", "Counter", "(", ")", "\n", "for", "src_word_list", ",", "trg_word_lists", "in", "tokenized_src_trg_pairs", ":", "\n", "        ", "token_freq_counter", ".", "update", "(", "src_word_list", ")", "\n", "for", "word_list", "in", "trg_word_lists", ":", "\n", "            ", "token_freq_counter", ".", "update", "(", "word_list", ")", "\n", "\n", "# Discard special tokens if already present", "\n", "", "", "special_tokens", "=", "[", "'<pad>'", ",", "'<bos>'", ",", "'<eos>'", ",", "'<unk>'", ",", "'<sep>'", "]", "\n", "if", "include_peos", ":", "\n", "        ", "special_tokens", ".", "append", "(", "'<peos>'", ")", "\n", "", "num_special_tokens", "=", "len", "(", "special_tokens", ")", "\n", "\n", "for", "s_t", "in", "special_tokens", ":", "\n", "        ", "if", "s_t", "in", "token_freq_counter", ":", "\n", "            ", "del", "token_freq_counter", "[", "s_t", "]", "\n", "\n", "", "", "word2idx", "=", "dict", "(", ")", "\n", "idx2word", "=", "dict", "(", ")", "\n", "for", "idx", ",", "word", "in", "enumerate", "(", "special_tokens", ")", ":", "\n", "# '<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3", "\n", "        ", "word2idx", "[", "word", "]", "=", "idx", "\n", "idx2word", "[", "idx", "]", "=", "word", "\n", "\n", "", "sorted_word2idx", "=", "sorted", "(", "token_freq_counter", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "sorted_words", "=", "[", "x", "[", "0", "]", "for", "x", "in", "sorted_word2idx", "]", "\n", "\n", "for", "idx", ",", "word", "in", "enumerate", "(", "sorted_words", ")", ":", "\n", "        ", "word2idx", "[", "word", "]", "=", "idx", "+", "num_special_tokens", "\n", "\n", "", "for", "idx", ",", "word", "in", "enumerate", "(", "sorted_words", ")", ":", "\n", "        ", "idx2word", "[", "idx", "+", "num_special_tokens", "]", "=", "word", "\n", "\n", "", "return", "word2idx", ",", "idx2word", ",", "token_freq_counter", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.main": [[180, 310], ["preprocess.build_vocab", "pykp.io.build_dataset", "print", "torch.save", "len", "pykp.io.build_dataset", "print", "torch.save", "len", "pykp.io.build_dataset", "pykp.io.build_dataset", "print", "torch.save", "torch.save", "pykp.io.build_dataset", "pykp.io.build_dataset", "print", "torch.save", "torch.save", "print", "torch.save", "print", "print", "print", "print", "print", "print", "print", "preprocess.read_src_and_trg_files", "preprocess.read_src_and_trg_files", "open", "open", "preprocess.read_src_and_trg_files", "preprocess.read_src_and_trg_files", "open", "open", "preprocess.read_src_and_trg_files", "preprocess.read_src_and_trg_files", "open", "open", "open", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.build_vocab", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.build_dataset", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.build_dataset", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.build_dataset", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.build_dataset", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.build_dataset", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.build_dataset", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.read_src_and_trg_files", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.read_src_and_trg_files", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.read_src_and_trg_files", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.read_src_and_trg_files", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.read_src_and_trg_files", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.read_src_and_trg_files"], ["", "def", "main", "(", "opt", ")", ":", "\n", "# Preprocess training data", "\n", "    ", "\"\"\"\n    # Tokenize train_src and train_trg\n    tokenized_train_src = read_tokenized_src_file(opt.train_src, remove_eos=opt.remove_eos)\n    tokenized_train_trg = read_tokenized_trg_file(opt.train_trg)\n\n    assert len(tokenized_train_src) == len(tokenized_train_trg), 'the number of records in source and target are not the same'\n\n    tokenized_train_pairs = list(zip(tokenized_train_src, tokenized_train_trg))\n    # a list of tuple, (src_word_list, [trg_1_word_list, trg_2_word_list, ...])\n\n    del tokenized_train_src\n    del tokenized_train_trg\n    \"\"\"", "\n", "title_guided", "=", "opt", ".", "title_guided", "\n", "\n", "# Tokenize train_src and train_trg, return a list of tuple, (src_word_list, [trg_1_word_list, trg_2_word_list, ...])", "\n", "if", "title_guided", ":", "\n", "        ", "tokenized_train_pairs", ",", "tokenized_train_title", "=", "read_src_and_trg_files", "(", "opt", ".", "train_src", ",", "opt", ".", "train_trg", ",", "is_train", "=", "True", ",", "remove_eos", "=", "opt", ".", "remove_eos", ",", "title_guided", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "tokenized_train_pairs", "=", "read_src_and_trg_files", "(", "opt", ".", "train_src", ",", "opt", ".", "train_trg", ",", "is_train", "=", "True", ",", "remove_eos", "=", "opt", ".", "remove_eos", ",", "title_guided", "=", "False", ")", "\n", "tokenized_train_title", "=", "None", "\n", "\n", "# build vocab from training src", "\n", "# build word2id, id2word, and vocab, where vocab is a counter", "\n", "# with special tokens, '<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3", "\n", "# word2id, id2word are ordered by frequencies, includes all the tokens in the data", "\n", "# simply concatenate src and target when building vocab", "\n", "", "word2idx", ",", "idx2word", ",", "token_freq_counter", "=", "build_vocab", "(", "tokenized_train_pairs", ",", "opt", ".", "include_peos", ")", "\n", "\n", "# building preprocessed training set for one2one training mode", "\n", "train_one2one", "=", "pykp", ".", "io", ".", "build_dataset", "(", "tokenized_train_pairs", ",", "word2idx", ",", "idx2word", ",", "opt", ",", "mode", "=", "'one2one'", ",", "include_original", "=", "True", ",", "title_list", "=", "tokenized_train_title", ")", "\n", "# a list of dict, with fields src, trg, src_oov, oov_dict, oov_list, etc.", "\n", "\n", "print", "(", "\"Dumping train one2one to disk: %s\"", "%", "(", "opt", ".", "data_dir", "+", "'/train.one2one.pt'", ")", ")", "\n", "torch", ".", "save", "(", "train_one2one", ",", "open", "(", "opt", ".", "data_dir", "+", "'/train.one2one.pt'", ",", "'wb'", ")", ")", "\n", "len_train_one2one", "=", "len", "(", "train_one2one", ")", "\n", "del", "train_one2one", "\n", "# building preprocessed training set for one2many training mode", "\n", "train_one2many", "=", "pykp", ".", "io", ".", "build_dataset", "(", "tokenized_train_pairs", ",", "word2idx", ",", "idx2word", ",", "opt", ",", "mode", "=", "'one2many'", ",", "include_original", "=", "True", ",", "title_list", "=", "tokenized_train_title", ")", "\n", "print", "(", "\"Dumping train one2many to disk: %s\"", "%", "(", "opt", ".", "data_dir", "+", "'/train.one2many.pt'", ")", ")", "\n", "torch", ".", "save", "(", "train_one2many", ",", "open", "(", "opt", ".", "data_dir", "+", "'/train.one2many.pt'", ",", "'wb'", ")", ")", "\n", "len_train_one2many", "=", "len", "(", "train_one2many", ")", "\n", "del", "train_one2many", "\n", "\n", "# Preprocess validation data", "\n", "\"\"\"\n    # Tokenize\n    tokenized_valid_src = read_tokenized_src_file(opt.valid_src, remove_eos=opt.remove_eos)\n    tokenized_valid_trg = read_tokenized_trg_file(opt.valid_trg)\n    assert len(tokenized_valid_src) == len(\n        tokenized_valid_trg), 'the number of records in source and target are not the same'\n\n    tokenized_valid_pairs = list(zip(tokenized_valid_src, tokenized_valid_trg))\n    del tokenized_valid_src\n    del tokenized_valid_trg\n    \"\"\"", "\n", "# Tokenize valid_src and valid_trg, return a list of tuple, (src_word_list, [trg_1_word_list, trg_2_word_list, ...])", "\n", "if", "title_guided", ":", "\n", "        ", "tokenized_valid_pairs", ",", "tokenized_valid_title", "=", "read_src_and_trg_files", "(", "opt", ".", "valid_src", ",", "opt", ".", "valid_trg", ",", "is_train", "=", "False", ",", "remove_eos", "=", "opt", ".", "remove_eos", ",", "title_guided", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "tokenized_valid_pairs", "=", "read_src_and_trg_files", "(", "opt", ".", "valid_src", ",", "opt", ".", "valid_trg", ",", "is_train", "=", "False", ",", "remove_eos", "=", "opt", ".", "remove_eos", ",", "title_guided", "=", "False", ")", "\n", "tokenized_valid_title", "=", "None", "\n", "\n", "# building preprocessed validation set for one2one and one2many training mode", "\n", "", "valid_one2one", "=", "pykp", ".", "io", ".", "build_dataset", "(", "\n", "tokenized_valid_pairs", ",", "word2idx", ",", "idx2word", ",", "opt", ",", "mode", "=", "'one2one'", ",", "include_original", "=", "True", ",", "title_list", "=", "tokenized_valid_title", ")", "\n", "valid_one2many", "=", "pykp", ".", "io", ".", "build_dataset", "(", "\n", "tokenized_valid_pairs", ",", "word2idx", ",", "idx2word", ",", "opt", ",", "mode", "=", "'one2many'", ",", "include_original", "=", "True", ",", "title_list", "=", "tokenized_valid_title", ")", "\n", "\n", "print", "(", "\"Dumping valid to disk: %s\"", "%", "(", "opt", ".", "data_dir", "+", "'/valid.pt'", ")", ")", "\n", "torch", ".", "save", "(", "valid_one2one", ",", "open", "(", "opt", ".", "data_dir", "+", "'/valid.one2one.pt'", ",", "'wb'", ")", ")", "\n", "torch", ".", "save", "(", "valid_one2many", ",", "open", "(", "opt", ".", "data_dir", "+", "'/valid.one2many.pt'", ",", "'wb'", ")", ")", "\n", "\n", "# Preprocess test data", "\n", "\"\"\"\n    tokenized_test_src = read_tokenized_src_file(opt.test_src, remove_eos=opt.remove_eos)\n    tokenized_test_trg = read_tokenized_trg_file(opt.test_trg)\n    assert len(tokenized_test_src) == len(\n        tokenized_test_trg), 'the number of records in source and target are not the same'\n\n    tokenized_test_pairs = list(zip(tokenized_test_src, tokenized_test_trg))\n    del tokenized_test_src\n    del tokenized_test_trg\n    \"\"\"", "\n", "# Tokenize train_src and train_trg, return a list of tuple, (src_word_list, [trg_1_word_list, trg_2_word_list, ...])", "\n", "if", "title_guided", ":", "\n", "        ", "tokenized_test_pairs", ",", "tokenized_test_title", "=", "read_src_and_trg_files", "(", "opt", ".", "test_src", ",", "opt", ".", "test_trg", ",", "is_train", "=", "False", ",", "remove_eos", "=", "opt", ".", "remove_eos", ",", "title_guided", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "tokenized_test_pairs", "=", "read_src_and_trg_files", "(", "opt", ".", "test_src", ",", "opt", ".", "test_trg", ",", "is_train", "=", "False", ",", "\n", "remove_eos", "=", "opt", ".", "remove_eos", ",", "title_guided", "=", "False", ")", "\n", "tokenized_test_title", "=", "None", "\n", "\n", "# building preprocessed test set for one2one and one2many training mode", "\n", "", "test_one2one", "=", "pykp", ".", "io", ".", "build_dataset", "(", "\n", "tokenized_test_pairs", ",", "word2idx", ",", "idx2word", ",", "opt", ",", "mode", "=", "'one2one'", ",", "include_original", "=", "True", ",", "title_list", "=", "tokenized_test_title", ")", "\n", "test_one2many", "=", "pykp", ".", "io", ".", "build_dataset", "(", "\n", "tokenized_test_pairs", ",", "word2idx", ",", "idx2word", ",", "opt", ",", "mode", "=", "'one2many'", ",", "include_original", "=", "True", ",", "title_list", "=", "tokenized_test_title", ")", "\n", "\n", "print", "(", "\"Dumping test to disk: %s\"", "%", "(", "opt", ".", "data_dir", "+", "'/valid.pt'", ")", ")", "\n", "torch", ".", "save", "(", "test_one2one", ",", "open", "(", "opt", ".", "data_dir", "+", "'/test.one2one.pt'", ",", "'wb'", ")", ")", "\n", "torch", ".", "save", "(", "test_one2many", ",", "open", "(", "opt", ".", "data_dir", "+", "'/test.one2many.pt'", ",", "'wb'", ")", ")", "\n", "\n", "print", "(", "\"Dumping dict to disk: %s\"", "%", "opt", ".", "data_dir", "+", "'/vocab.pt'", ")", "\n", "torch", ".", "save", "(", "[", "word2idx", ",", "idx2word", ",", "token_freq_counter", "]", ",", "\n", "open", "(", "opt", ".", "data_dir", "+", "'/vocab.pt'", ",", "'wb'", ")", ")", "\n", "\n", "print", "(", "'#pairs of train_one2one  = %d'", "%", "len_train_one2one", ")", "\n", "print", "(", "'#pairs of train_one2many = %d'", "%", "len_train_one2many", ")", "\n", "print", "(", "'#pairs of valid_one2one  = %d'", "%", "len", "(", "valid_one2one", ")", ")", "\n", "print", "(", "'#pairs of valid_one2many = %d'", "%", "len", "(", "valid_one2many", ")", ")", "\n", "print", "(", "'#pairs of test_one2one   = %d'", "%", "len", "(", "test_one2one", ")", ")", "\n", "print", "(", "'#pairs of test_one2many  = %d'", "%", "len", "(", "test_one2many", ")", ")", "\n", "\n", "print", "(", "'Done!'", ")", "\n", "\n", "'''\n    special_tokens = ['<pad>']\n    vocab = []\n    vocab += special_tokens\n    vocab += [w for w, n in token_freq_counter.items() if n > opt['word_count_threshold']]\n    total_tokens = len(token_freq_counter)\n    vocab_size = len(vocab) - len(special_tokens)\n    OOV_words = total_tokens - vocab_size\n    print('Vocab size: %d' % vocab_size)\n    print('Number of OOV words: %d' % OOV_words)\n    print('OOV percentage: %.2f' % OOV_words/total_tokens * 100 )\n    '''", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.t_stat.main": [[7, 14], ["numpy.array", "numpy.array", "scipy.stats.ttest_rel", "print"], "function", ["None"], ["def", "main", "(", "score_dict_a", ",", "score_dict_b", ",", "k_list", ",", "tag_list", ")", ":", "\n", "    ", "for", "tag", "in", "tag_list", ":", "\n", "        ", "for", "k", "in", "k_list", ":", "\n", "            ", "f1_np_array_a", "=", "np", ".", "array", "(", "score_dict_a", "[", "'f1_score@{}_{}'", ".", "format", "(", "k", ",", "tag", ")", "]", ")", "\n", "f1_np_array_b", "=", "np", ".", "array", "(", "score_dict_b", "[", "'f1_score@{}_{}'", ".", "format", "(", "k", ",", "tag", ")", "]", ")", "\n", "t_stat", ",", "p_value", "=", "stats", ".", "ttest_rel", "(", "f1_np_array_a", ",", "f1_np_array_b", ")", "\n", "print", "(", "\"tag: {}, topk: {}, p-value: {}\"", ".", "format", "(", "tag", ",", "k", ",", "p_value", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.penalties.PenaltyBuilder.__init__": [[14, 17], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "cov_pen", ",", "length_pen", ")", ":", "\n", "        ", "self", ".", "length_pen", "=", "length_pen", "\n", "self", ".", "cov_pen", "=", "cov_pen", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.penalties.PenaltyBuilder.coverage_penalty": [[18, 25], ["None"], "methods", ["None"], ["", "def", "coverage_penalty", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "cov_pen", "==", "\"wu\"", ":", "\n", "            ", "return", "self", ".", "coverage_wu", "\n", "", "elif", "self", ".", "cov_pen", "==", "\"summary\"", ":", "\n", "            ", "return", "self", ".", "coverage_summary", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "coverage_none", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.penalties.PenaltyBuilder.length_penalty": [[26, 33], ["None"], "methods", ["None"], ["", "", "def", "length_penalty", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "length_pen", "==", "\"wu\"", ":", "\n", "            ", "return", "self", ".", "length_wu", "\n", "", "elif", "self", ".", "length_pen", "==", "\"avg\"", ":", "\n", "            ", "return", "self", ".", "length_average", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "length_none", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.penalties.PenaltyBuilder.coverage_wu": [[38, 45], ["torch.min().log().sum", "torch.min().log", "torch.min", "cov.clone().fill_", "cov.clone"], "methods", ["None"], ["def", "coverage_wu", "(", "self", ",", "beam", ",", "cov", ",", "beta", "=", "0.", ")", ":", "\n", "        ", "\"\"\"\n        NMT coverage re-ranking score from\n        \"Google's Neural Machine Translation System\" :cite:`wu2016google`.\n        \"\"\"", "\n", "penalty", "=", "-", "torch", ".", "min", "(", "cov", ",", "cov", ".", "clone", "(", ")", ".", "fill_", "(", "1.0", ")", ")", ".", "log", "(", ")", ".", "sum", "(", "1", ")", "\n", "return", "beta", "*", "penalty", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.penalties.PenaltyBuilder.coverage_summary": [[46, 53], ["torch.max().sum", "cov.size", "torch.max", "cov.clone().fill_", "cov.clone"], "methods", ["None"], ["", "def", "coverage_summary", "(", "self", ",", "beam", ",", "cov", ",", "beta", "=", "0.", ")", ":", "\n", "        ", "\"\"\"\n        Our summary penalty.\n        \"\"\"", "\n", "penalty", "=", "torch", ".", "max", "(", "cov", ",", "cov", ".", "clone", "(", ")", ".", "fill_", "(", "1.0", ")", ")", ".", "sum", "(", "1", ")", "\n", "penalty", "-=", "cov", ".", "size", "(", "1", ")", "\n", "return", "beta", "*", "penalty", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.penalties.PenaltyBuilder.coverage_none": [[54, 59], ["beam.scores.clone().fill_", "beam.scores.clone"], "methods", ["None"], ["", "def", "coverage_none", "(", "self", ",", "beam", ",", "cov", ",", "beta", "=", "0.", ")", ":", "\n", "        ", "\"\"\"\n        returns zero as penalty\n        \"\"\"", "\n", "return", "beam", ".", "scores", ".", "clone", "(", ")", ".", "fill_", "(", "0.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.penalties.PenaltyBuilder.length_wu": [[60, 69], ["len"], "methods", ["None"], ["", "def", "length_wu", "(", "self", ",", "beam", ",", "logprobs", ",", "alpha", "=", "0.", ")", ":", "\n", "        ", "\"\"\"\n        NMT length re-ranking score from\n        \"Google's Neural Machine Translation System\" :cite:`wu2016google`.\n        \"\"\"", "\n", "\n", "modifier", "=", "(", "(", "(", "5", "+", "len", "(", "beam", ".", "next_ys", ")", ")", "**", "alpha", ")", "/", "\n", "(", "(", "5", "+", "1", ")", "**", "alpha", ")", ")", "\n", "return", "(", "logprobs", "/", "modifier", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.penalties.PenaltyBuilder.length_average": [[70, 77], ["len"], "methods", ["None"], ["", "def", "length_average", "(", "self", ",", "beam", ",", "logprobs", ",", "alpha", "=", "0.", ")", ":", "\n", "        ", "\"\"\"\n        Returns the average probability of tokens in a sequence.\n        \"\"\"", "\n", "seq_len", "=", "len", "(", "beam", ".", "next_ys", ")", "-", "1", "\n", "assert", "seq_len", "!=", "0", "\n", "return", "logprobs", "/", "seq_len", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.penalties.PenaltyBuilder.length_none": [[78, 83], ["None"], "methods", ["None"], ["", "def", "length_none", "(", "self", ",", "beam", ",", "logprobs", ",", "alpha", "=", "0.", ",", "beta", "=", "0.", ")", ":", "\n", "        ", "\"\"\"\n        Returns unmodified scores.\n        \"\"\"", "\n", "return", "logprobs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sp.pearson_and_spearman": [[5, 12], ["scipy.stats.pearsonr", "scipy.stats.spearmanr", "float", "float", "float"], "function", ["None"], ["def", "pearson_and_spearman", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "pearson_corr", "=", "pearsonr", "(", "preds", ",", "labels", ")", "[", "0", "]", "\n", "spearman_corr", "=", "spearmanr", "(", "preds", ",", "labels", ")", "[", "0", "]", "\n", "return", "{", "\n", "\"pearson\"", ":", "float", "(", "pearson_corr", ")", ",", "\n", "\"spearmanr\"", ":", "float", "(", "spearman_corr", ")", ",", "\n", "\"corr\"", ":", "float", "(", "(", "pearson_corr", "+", "spearman_corr", ")", "/", "2", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_predicted_bert.BertPredictModel.__init__": [[10, 20], ["torch.Module.__init__", "transformers.BertTokenizer.from_pretrained", "transformers.BertModel.from_pretrained", "torch.Linear", "torch.Linear", "torch.Sigmoid", "torch.Sigmoid", "torch.Parameter", "torch.Parameter", "torch.MSELoss", "torch.MSELoss", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.__init__", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_predicted_bert.BertPredictModel.from_pretrained", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_predicted_bert.BertPredictModel.from_pretrained"], ["    ", "def", "__init__", "(", "self", ",", "from_pretrained", ":", "str", ",", "vocab", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab", "=", "vocab", "\n", "# self.bert = fnlp.embeddings.BertEmbedding(vocab, from_pretrained)", "\n", "self", ".", "tokenizer", "=", "trf", ".", "BertTokenizer", ".", "from_pretrained", "(", "from_pretrained", ")", "\n", "self", ".", "bert", "=", "trf", ".", "BertModel", ".", "from_pretrained", "(", "from_pretrained", ")", "\n", "self", ".", "predict", "=", "nn", ".", "Linear", "(", "768", ",", "3", ")", "\n", "self", ".", "activate", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "check_device", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ")", ",", "requires_grad", "=", "False", ")", "\n", "self", ".", "mse_loss", "=", "nn", ".", "MSELoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_predicted_bert.BertPredictModel.forward": [[21, 35], ["train_predicted_bert.BertPredictModel.tokenizer", "train_predicted_bert.BertPredictModel.bert", "train_predicted_bert.BertPredictModel.predict().squeeze", "train_predicted_bert.BertPredictModel.activate", "p.to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "train_predicted_bert.BertPredictModel.mse_loss", "train_predicted_bert.BertPredictModel.items", "train_predicted_bert.BertPredictModel.predict", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.predict.predict"], ["", "def", "forward", "(", "self", ",", "pred_str", ",", "target_str", "=", "None", ",", "score1", "=", "None", ",", "score2", "=", "None", ")", ":", "\n", "        ", "inputs", "=", "self", ".", "tokenizer", "(", "f'{target_str} [SEP] {pred_str}'", ",", "return_tensors", "=", "\"pt\"", ")", "\n", "inputs", "=", "{", "n", ":", "p", ".", "to", "(", "self", ".", "check_device", ".", "device", ")", "for", "n", ",", "p", "in", "inputs", ".", "items", "(", ")", "}", "\n", "outputs", "=", "self", ".", "bert", "(", "**", "inputs", ")", "\n", "predicted_logits", "=", "self", ".", "predict", "(", "outputs", "[", "1", "]", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "# predicted_logits = self.predict(outputs.pooler_output).squeeze(-1)", "\n", "predicted_score", "=", "self", ".", "activate", "(", "predicted_logits", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "score1", "is", "not", "None", ":", "\n", "            ", "score_tensor", "=", "torch", ".", "tensor", "(", "[", "[", "score1", ",", "score2", ",", "(", "score1", "+", "score2", ")", "/", "2", "]", "]", ")", ".", "to", "(", "self", ".", "check_device", ".", "device", ")", "\n", "loss", "=", "self", ".", "mse_loss", "(", "predicted_score", ",", "score_tensor", ")", "\n", "\n", "", "return", "predicted_score", ",", "loss", "\n", "# if not isinstance(pred_str, str):", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_predicted_bert.BertPredictModel.from_pretrained": [[52, 64], ["kwargs.pop", "cls", "cls.predict.load_state_dict", "print", "os.path.exists", "print", "open", "torch.load", "torch.load", "torch.load", "torch.load", "os.path.join", "os.path.join"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "model_dir_or_name", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "vocab", "=", "kwargs", ".", "pop", "(", "'vocab'", ",", "None", ")", "\n", "model", "=", "cls", "(", "model_dir_or_name", ",", "vocab", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "model_dir_or_name", ",", "'predict_tensor.pkl'", ")", ")", ":", "\n", "            ", "print", "(", "f'only load pretrain model from `{model_dir_or_name}`!'", ")", "\n", "return", "model", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "model_dir_or_name", ",", "'predict_tensor.pkl'", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "predict_state_dict", "=", "torch", ".", "load", "(", "f", ",", "map_location", "=", "'cpu'", ")", "\n", "", "model", ".", "predict", ".", "load_state_dict", "(", "predict_state_dict", ")", "\n", "print", "(", "f'load predict parameters from `{model_dir_or_name}`!'", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_predicted_bert.BertPredictModel.save_pretrained": [[65, 71], ["train_predicted_bert.BertPredictModel.bert.save_pretrained", "os.path.exists", "os.makedirs", "open", "torch.save", "torch.save", "torch.save", "torch.save", "os.path.join", "train_predicted_bert.BertPredictModel.predict.state_dict"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.train_predicted_bert.BertPredictModel.save_pretrained"], ["", "def", "save_pretrained", "(", "self", ",", "model_dir_or_name", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "model_dir_or_name", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "model_dir_or_name", ")", "\n", "", "self", ".", "bert", ".", "save_pretrained", "(", "model_dir_or_name", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "model_dir_or_name", ",", "'predict_tensor.pkl'", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "torch", ".", "save", "(", "self", ".", "predict", ".", "state_dict", "(", ")", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sequence_generator.SequenceGenerator.__init__": [[19, 82], ["beam.GNMTGlobalScorer"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "model", ",", "\n", "eos_idx", ",", "\n", "bos_idx", ",", "\n", "pad_idx", ",", "\n", "beam_size", ",", "\n", "max_sequence_length", ",", "\n", "copy_attn", "=", "False", ",", "\n", "coverage_attn", "=", "False", ",", "\n", "review_attn", "=", "False", ",", "\n", "include_attn_dist", "=", "True", ",", "\n", "length_penalty_factor", "=", "0.0", ",", "\n", "coverage_penalty_factor", "=", "0.0", ",", "\n", "length_penalty", "=", "'avg'", ",", "\n", "coverage_penalty", "=", "'none'", ",", "\n", "cuda", "=", "True", ",", "\n", "n_best", "=", "None", ",", "\n", "block_ngram_repeat", "=", "0", ",", "\n", "ignore_when_blocking", "=", "[", "]", ",", "\n", "peos_idx", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"Initializes the generator.\n\n        Args:\n          model: recurrent model, with inputs: (input, dec_hidden) and outputs len(vocab) values\n          eos_idx: the idx of the <eos> token\n          beam_size: Beam size to use when generating sequences.\n          max_sequence_length: The maximum sequence length before stopping the search.\n          coverage_attn: use coverage attention or not\n          include_attn_dist: include the attention distribution in the sequence obj or not.\n          length_normalization_factor: If != 0, a number x such that sequences are\n            scored by logprob/length^x, rather than logprob. This changes the\n            relative scores of sequences depending on their lengths. For example, if\n            x > 0 then longer sequences will be favored.\n            alpha in: https://arxiv.org/abs/1609.08144\n          length_normalization_const: 5 in https://arxiv.org/abs/1609.08144\n        \"\"\"", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "eos_idx", "=", "eos_idx", "\n", "self", ".", "bos_idx", "=", "bos_idx", "\n", "self", ".", "pad_idx", "=", "pad_idx", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "self", ".", "max_sequence_length", "=", "max_sequence_length", "\n", "self", ".", "length_penalty_factor", "=", "length_penalty_factor", "\n", "self", ".", "coverage_penalty_factor", "=", "coverage_penalty_factor", "\n", "self", ".", "coverage_attn", "=", "coverage_attn", "\n", "self", ".", "include_attn_dist", "=", "include_attn_dist", "\n", "#self.lambda_coverage = lambda_coverage", "\n", "self", ".", "coverage_penalty", "=", "coverage_penalty", "\n", "self", ".", "copy_attn", "=", "copy_attn", "\n", "self", ".", "global_scorer", "=", "GNMTGlobalScorer", "(", "length_penalty_factor", ",", "coverage_penalty_factor", ",", "coverage_penalty", ",", "length_penalty", ")", "\n", "self", ".", "cuda", "=", "cuda", "\n", "self", ".", "review_attn", "=", "review_attn", "\n", "self", ".", "block_ngram_repeat", "=", "block_ngram_repeat", "\n", "self", ".", "ignore_when_blocking", "=", "ignore_when_blocking", "\n", "if", "n_best", "is", "None", ":", "\n", "            ", "self", ".", "n_best", "=", "self", ".", "beam_size", "\n", "", "else", ":", "\n", "            ", "self", ".", "n_best", "=", "n_best", "\n", "", "self", ".", "peos_idx", "=", "peos_idx", "\n", "\n", "if", "self", ".", "model", ".", "separate_present_absent", ":", "\n", "            ", "assert", "self", ".", "peos_idx", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sequence_generator.SequenceGenerator.beam_search": [[83, 212], ["sequence_generator.SequenceGenerator.model.eval", "src.size", "src.size", "sequence_generator.SequenceGenerator.model.encoder", "max", "sequence_generator.SequenceGenerator.model.init_decoder_state", "memory_bank.repeat.repeat.repeat", "src_mask.repeat.repeat.repeat", "src_oov.repeat.repeat.repeat", "sequence_generator.SequenceGenerator.repeat", "set", "range", "sequence_generator.SequenceGenerator._from_beam", "src.new_zeros", "decoder_init_state[].unsqueeze", "torch.cat.repeat", "torch.zeros", "sequence_generator.SequenceGenerator.repeat.new_zeros", "beam.Beam", "torch.tensor", "all", "sequence_generator.SequenceGenerator.beam_search.var"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.init_decoder_state", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sequence_generator.SequenceGenerator._from_beam"], ["", "", "def", "beam_search", "(", "self", ",", "src", ",", "src_lens", ",", "src_oov", ",", "src_mask", ",", "oov_lists", ",", "word2idx", ",", "max_eos_per_output_seq", "=", "1", ",", "title", "=", "None", ",", "title_lens", "=", "None", ",", "title_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        :param src: a LongTensor containing the word indices of source sentences, [batch, src_seq_len], with oov words replaced by unk idx\n        :param src_lens: a list containing the length of src sequences for each batch, with len=batch\n        :param src_oov: a LongTensor containing the word indices of source sentences, [batch, src_seq_len], contains the index of oov words (used by copy)\n        :param src_mask: a FloatTensor, [batch, src_seq_len]\n        :param oov_lists: list of oov words (idx2word) for each batch, len=batch\n        :param word2idx: a dictionary\n        \"\"\"", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "batch_size", "=", "src", ".", "size", "(", "0", ")", "\n", "beam_size", "=", "self", ".", "beam_size", "\n", "max_src_len", "=", "src", ".", "size", "(", "1", ")", "\n", "\n", "# Encoding", "\n", "memory_bank", ",", "encoder_final_state", "=", "self", ".", "model", ".", "encoder", "(", "src", ",", "src_lens", ",", "src_mask", ",", "title", ",", "title_lens", ",", "title_mask", ")", "\n", "# [batch_size, max_src_len, memory_bank_size], [batch_size, memory_bank_size]", "\n", "\n", "max_num_oov", "=", "max", "(", "[", "len", "(", "oov", ")", "for", "oov", "in", "oov_lists", "]", ")", "# max number of oov for each batch", "\n", "\n", "# Init decoder state", "\n", "decoder_init_state", "=", "self", ".", "model", ".", "init_decoder_state", "(", "encoder_final_state", ")", "# [dec_layers, batch_size, decoder_size]", "\n", "\n", "# init initial_input to be BOS token", "\n", "#decoder_init_input = src.new_ones((batch_size * beam_size, 1)) * self.bos_idx  # [batch_size*beam_size, 1]", "\n", "\n", "if", "self", ".", "coverage_attn", ":", "# init coverage", "\n", "#coverage = torch.zeros_like(src, dtype=torch.float)  # [batch, src_len]", "\n", "            ", "coverage", "=", "src", ".", "new_zeros", "(", "(", "batch_size", "*", "beam_size", ",", "max_src_len", ")", ",", "dtype", "=", "torch", ".", "float", ")", "# [batch_size * beam_size, max_src_len]", "\n", "", "else", ":", "\n", "            ", "coverage", "=", "None", "\n", "\n", "", "if", "self", ".", "review_attn", ":", "\n", "            ", "decoder_memory_bank", "=", "decoder_init_state", "[", "-", "1", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", "# [batch, 1, decoder_size]", "\n", "decoder_memory_bank", "=", "decoder_memory_bank", ".", "repeat", "(", "beam_size", ",", "1", ",", "1", ")", "\n", "assert", "decoder_memory_bank", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", "*", "beam_size", ",", "1", ",", "self", ".", "model", ".", "decoder_size", "]", ")", "\n", "", "else", ":", "\n", "            ", "decoder_memory_bank", "=", "None", "\n", "\n", "", "if", "self", ".", "model", ".", "separate_present_absent", "and", "self", ".", "model", ".", "goal_vector_mode", ">", "0", ":", "\n", "            ", "is_absent", "=", "torch", ".", "zeros", "(", "batch_size", "*", "self", ".", "beam_size", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "\n", "# expand memory_bank, src_mask", "\n", "", "memory_bank", "=", "memory_bank", ".", "repeat", "(", "beam_size", ",", "1", ",", "1", ")", "# [batch * beam_size, max_src_len, memory_bank_size]", "\n", "src_mask", "=", "src_mask", ".", "repeat", "(", "beam_size", ",", "1", ")", "# [batch * beam_size, src_seq_len]", "\n", "src_oov", "=", "src_oov", ".", "repeat", "(", "self", ".", "beam_size", ",", "1", ")", "# [batch * beam_size, src_seq_len]", "\n", "decoder_state", "=", "decoder_init_state", ".", "repeat", "(", "1", ",", "self", ".", "beam_size", ",", "1", ")", "# [dec_layers, batch_size * beam_size, decoder_size]", "\n", "\n", "if", "self", ".", "model", ".", "use_target_encoder", ":", "\n", "# init the hidden state of target encoder to zero vector", "\n", "            ", "target_encoder_state", "=", "decoder_state", ".", "new_zeros", "(", "1", ",", "batch_size", "*", "self", ".", "beam_size", ",", "self", ".", "model", ".", "target_encoder_size", ")", "# [1, batch_size * beam_size, target_encoder_size]", "\n", "\n", "# exclusion_list = [\"<t>\", \"</t>\", \".\"]", "\n", "", "exclusion_tokens", "=", "set", "(", "[", "word2idx", "[", "t", "]", "\n", "for", "t", "in", "self", ".", "ignore_when_blocking", "]", ")", "\n", "\n", "beam_list", "=", "[", "Beam", "(", "beam_size", ",", "n_best", "=", "self", ".", "n_best", ",", "cuda", "=", "self", ".", "cuda", ",", "global_scorer", "=", "self", ".", "global_scorer", ",", "pad", "=", "self", ".", "pad_idx", ",", "eos", "=", "self", ".", "eos_idx", ",", "bos", "=", "self", ".", "bos_idx", ",", "max_eos_per_output_seq", "=", "max_eos_per_output_seq", ",", "block_ngram_repeat", "=", "self", ".", "block_ngram_repeat", ",", "exclusion_tokens", "=", "exclusion_tokens", ")", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "# Help functions for working with beams and batches", "\n", "def", "var", "(", "a", ")", ":", "\n", "            ", "return", "torch", ".", "tensor", "(", "a", ",", "requires_grad", "=", "False", ")", "\n", "\n", "", "'''\n        Run beam search.\n        '''", "\n", "for", "t", "in", "range", "(", "1", ",", "self", ".", "max_sequence_length", "+", "1", ")", ":", "\n", "            ", "if", "all", "(", "(", "b", ".", "done", "(", ")", "for", "b", "in", "beam_list", ")", ")", ":", "\n", "                ", "break", "\n", "\n", "# Construct batch x beam_size nxt words.", "\n", "# Get all the pending current beam words and arrange for forward.", "\n", "# b.get_current_tokens(): [beam_size]", "\n", "# torch.stack([ [beam of batch 1], [beam of batch 2], ... ]) -> [batch, beam]", "\n", "# after transpose -> [beam, batch]", "\n", "# After flatten, it becomes", "\n", "# [batch_1_beam_1, batch_2_beam_1,..., batch_N_beam_1, batch_1_beam_2, ..., batch_N_beam_2, ...]", "\n", "# this match the dimension of hidden state", "\n", "", "decoder_input", "=", "var", "(", "torch", ".", "stack", "(", "[", "b", ".", "get_current_tokens", "(", ")", "for", "b", "in", "beam_list", "]", ")", "\n", ".", "t", "(", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ")", "\n", "# decoder_input: [batch_size * beam_size]", "\n", "\n", "# Turn any copied words to UNKS", "\n", "if", "self", ".", "copy_attn", ":", "\n", "                ", "decoder_input", "=", "decoder_input", ".", "masked_fill", "(", "\n", "decoder_input", ".", "gt", "(", "self", ".", "model", ".", "vocab_size", "-", "1", ")", ",", "self", ".", "model", ".", "unk_idx", ")", "\n", "\n", "# Convert the generated eos token to bos token, only useful in one2many_mode=2 or one2many_mode=3", "\n", "", "decoder_input", "=", "decoder_input", ".", "masked_fill", "(", "decoder_input", "==", "self", ".", "eos_idx", ",", "self", ".", "bos_idx", ")", "\n", "\n", "if", "self", ".", "model", ".", "use_target_encoder", ":", "\n", "# encode the previous token using target encoder", "\n", "                ", "target_encoder_state_next", "=", "self", ".", "model", ".", "target_encoder", "(", "decoder_input", ".", "detach", "(", ")", ",", "target_encoder_state", ")", "\n", "target_encoder_state", "=", "target_encoder_state_next", "# [1, batch_size * beam_size, target_encoder_size]", "\n", "", "else", ":", "\n", "                ", "target_encoder_state", "=", "None", "\n", "# decoder_input = y_t", "\n", "\n", "", "if", "self", ".", "model", ".", "separate_present_absent", "and", "self", ".", "model", ".", "goal_vector_mode", ">", "0", ":", "\n", "# update the is_absent vector", "\n", "                ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "if", "decoder_input", "[", "i", "]", ".", "item", "(", ")", "==", "self", ".", "peos_idx", ":", "\n", "                        ", "is_absent", "[", "i", "]", "=", "1", "\n", "", "", "if", "self", ".", "model", ".", "manager_mode", "==", "1", ":", "\n", "                    ", "goal_vector", "=", "self", ".", "model", ".", "manager", "(", "is_absent", ")", "\n", "", "", "else", ":", "\n", "                ", "goal_vector", "=", "None", "\n", "\n", "# run one step of decoding", "\n", "# [flattened_batch, vocab_size], [dec_layers, flattened_batch, decoder_size], [flattened_batch, memory_bank_size], [flattened_batch, src_len], [flattened_batch, src_len]", "\n", "", "decoder_dist", ",", "decoder_state", ",", "context", ",", "attn_dist", ",", "_", ",", "coverage", "=", "self", ".", "model", ".", "decoder", "(", "decoder_input", ",", "decoder_state", ",", "memory_bank", ",", "src_mask", ",", "max_num_oov", ",", "src_oov", ",", "coverage", ",", "decoder_memory_bank", ",", "target_encoder_state", ",", "goal_vector", ")", "\n", "log_decoder_dist", "=", "torch", ".", "log", "(", "decoder_dist", "+", "EPS", ")", "\n", "\n", "if", "self", ".", "review_attn", ":", "\n", "                ", "decoder_memory_bank", "=", "torch", ".", "cat", "(", "[", "decoder_memory_bank", ",", "decoder_state", "[", "-", "1", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "1", ")", "# [batch_size * beam_size, t+1, decoder_size]", "\n", "\n", "# Compute a vector of batch x beam word scores", "\n", "", "log_decoder_dist", "=", "log_decoder_dist", ".", "view", "(", "beam_size", ",", "batch_size", ",", "-", "1", ")", "# [beam_size, batch_size, vocab_size]", "\n", "attn_dist", "=", "attn_dist", ".", "view", "(", "beam_size", ",", "batch_size", ",", "-", "1", ")", "# [beam_size, batch_size, src_seq_len]", "\n", "\n", "# Advance each beam", "\n", "for", "batch_idx", ",", "beam", "in", "enumerate", "(", "beam_list", ")", ":", "\n", "                ", "beam", ".", "advance", "(", "log_decoder_dist", "[", ":", ",", "batch_idx", "]", ",", "attn_dist", "[", ":", ",", "batch_idx", ",", ":", "src_lens", "[", "batch_idx", "]", "]", ")", "\n", "self", ".", "beam_decoder_state_update", "(", "batch_idx", ",", "beam", ".", "get_current_origin", "(", ")", ",", "decoder_state", ",", "decoder_memory_bank", ")", "\n", "\n", "# Extract sentences from beam.", "\n", "", "", "result_dict", "=", "self", ".", "_from_beam", "(", "beam_list", ")", "\n", "result_dict", "[", "'batch_size'", "]", "=", "batch_size", "\n", "return", "result_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sequence_generator.SequenceGenerator._from_beam": [[213, 231], ["b.sort_finished", "enumerate", "ret[].append", "ret[].append", "ret[].append", "b.get_hyp", "hyps.append", "attn.append"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.Beam.sort_finished", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.beam.Beam.get_hyp"], ["", "def", "_from_beam", "(", "self", ",", "beam_list", ")", ":", "\n", "        ", "ret", "=", "{", "\"predictions\"", ":", "[", "]", ",", "\"scores\"", ":", "[", "]", ",", "\"attention\"", ":", "[", "]", "}", "\n", "for", "b", "in", "beam_list", ":", "\n", "            ", "n_best", "=", "self", ".", "n_best", "\n", "scores", ",", "ks", "=", "b", ".", "sort_finished", "(", "minimum", "=", "n_best", ")", "\n", "hyps", ",", "attn", "=", "[", "]", ",", "[", "]", "\n", "# Collect all the decoded sentences in to hyps (list of list of idx) and attn (list of tensor)", "\n", "for", "i", ",", "(", "times", ",", "k", ")", "in", "enumerate", "(", "ks", "[", ":", "n_best", "]", ")", ":", "\n", "# Get the corresponding decoded sentence, and also the attn dist [seq_len, memory_bank_size].", "\n", "                ", "hyp", ",", "att", "=", "b", ".", "get_hyp", "(", "times", ",", "k", ")", "\n", "hyps", ".", "append", "(", "hyp", ")", "\n", "attn", ".", "append", "(", "att", ")", "\n", "", "ret", "[", "\"predictions\"", "]", ".", "append", "(", "hyps", ")", "# 3d list of idx (zero dim tensor), with len [batch_size, n_best, output_seq_len]", "\n", "ret", "[", "'scores'", "]", ".", "append", "(", "scores", ")", "# a 2d list of zero dim tensor, with len [batch_size, n_best]", "\n", "ret", "[", "\"attention\"", "]", ".", "append", "(", "attn", ")", "# a 2d list of FloatTensor[output sequence length, src_len] , with len [batch_size, n_best]", "\n", "# hyp[::-1]: a list of idx (zero dim tensor), with len = output sequence length", "\n", "# torch.stack(attn): FloatTensor, with size: [output sequence length, src_len]", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sequence_generator.SequenceGenerator.beam_decoder_state_update": [[232, 252], ["list", "decoder_state_transformed.data.copy_", "decoder_state.size", "decoder_state.view", "decoder_state_transformed.data.index_select", "decoder_memory_bank_transformed.data.copy_", "decoder_memory_bank.view", "decoder_memory_bank_transformed.data.index_select"], "methods", ["None"], ["", "def", "beam_decoder_state_update", "(", "self", ",", "batch_idx", ",", "beam_indices", ",", "decoder_state", ",", "decoder_memory_bank", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        :param batch_idx: int\n        :param beam_indices: a long tensor of previous beam indices, size: [beam_size]\n        :param decoder_state: [dec_layers, flattened_batch_size, decoder_size]\n        :return:\n        \"\"\"", "\n", "decoder_layers", ",", "flattened_batch_size", ",", "decoder_size", "=", "list", "(", "decoder_state", ".", "size", "(", ")", ")", "\n", "assert", "flattened_batch_size", "%", "self", ".", "beam_size", "==", "0", "\n", "original_batch_size", "=", "flattened_batch_size", "//", "self", ".", "beam_size", "\n", "# select the hidden states of a particular batch, [dec_layers, batch_size * beam_size, decoder_size] -> [dec_layers, beam_size, decoder_size]", "\n", "decoder_state_transformed", "=", "decoder_state", ".", "view", "(", "decoder_layers", ",", "self", ".", "beam_size", ",", "original_batch_size", ",", "decoder_size", ")", "[", ":", ",", ":", ",", "batch_idx", "]", "\n", "# select the hidden states of the beams specified by the beam_indices -> [dec_layers, beam_size, decoder_size]", "\n", "decoder_state_transformed", ".", "data", ".", "copy_", "(", "decoder_state_transformed", ".", "data", ".", "index_select", "(", "1", ",", "beam_indices", ")", ")", "\n", "\n", "if", "decoder_memory_bank", "is", "not", "None", ":", "\n", "# [batch_size * beam_size, t+1, decoder_size] -> [beam_size, t-1, decoder_size]", "\n", "            ", "decoder_memory_bank_transformed", "=", "decoder_memory_bank", ".", "view", "(", "self", ".", "beam_size", ",", "original_batch_size", ",", "-", "1", ",", "decoder_size", ")", "[", ":", ",", "batch_idx", ",", ":", ",", ":", "]", "\n", "# select the hidden states of the beams specified by the beam_indices -> [beam_size, t-1, decoder_size]", "\n", "decoder_memory_bank_transformed", ".", "data", ".", "copy_", "(", "decoder_memory_bank_transformed", ".", "data", ".", "index_select", "(", "0", ",", "beam_indices", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.sequence_generator.SequenceGenerator.sample": [[253, 475], ["list", "max", "sequence_generator.SequenceGenerator.model.encoder", "sequence_generator.SequenceGenerator.model.init_decoder_state", "torch.zeros", "src.new_ones", "src.new_zeros", "range", "torch.cat", "torch.cat().type_as", "torch.cat().to", "src.size", "memory_bank.size", "torch.Size", "encoder_final_state.size", "torch.Size", "ValueError", "sequence_generator.SequenceGenerator.new_zeros", "torch.zeros_like", "h_t_init[].unsqueeze", "torch.zeros", "src.new_ones", "re_init_indicators.unsqueeze", "torch.zeros().to", "sequence_generator.SequenceGenerator.model.decoder", "torch.log", "enumerate", "all", "torch.stack", "torch.cat.size", "torch.Size", "torch.cat().type_as.size", "torch.cat.size", "torch.cat().to.size", "torch.cat.size", "len", "torch.cat.size", "torch.Size", "torch.zeros", "range", "torch.cat().to.append", "unfinished_mask.unsqueeze.unsqueeze.unsqueeze", "torch.cat().type_as.append", "torch.cat.masked_fill", "sequence_generator.SequenceGenerator.model.target_encoder", "range", "torch.bmm().view", "torch.max", "selected_token_dist.unsqueeze.unsqueeze.unsqueeze", "torch.multinomial.unsqueeze", "torch.cat.append", "torch.multinomial", "torch.cat.append", "unfinished_mask.unsqueeze.unsqueeze.type_as", "torch.cat", "torch.cat", "torch.zeros", "re_init_indicators.unsqueeze", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat.gt", "torch.cat.detach", "torch.log", "torch.log.gather", "sample[].append", "sample[].append", "re_init_indicators.sum().item", "zip", "enumerate", "torch.cat", "y_t[].item", "sequence_generator.SequenceGenerator.model.manager", "torch.bmm", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "re_init_indicators.sum().item", "zip", "h_t[].unsqueeze", "decoder_dist.unsqueeze", "torch.log.unsqueeze", "int", "pred_counters[].item", "re_init_indicators.sum", "indicator.item", "pred_count.item", "h_t_init[].unsqueeze", "y_t_init[].unsqueeze", "h_t_next[].unsqueeze", "y_t_next[].unsqueeze", "torch.cat.append", "torch.cat.append", "[].item", "re_init_indicators.sum", "indicator.item", "pred_count.item", "y_t_init[].unsqueeze", "y_t_next[].unsqueeze", "pred_count.item", "torch.normal", "torch.ones_like"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.init_decoder_state"], ["", "", "def", "sample", "(", "self", ",", "src", ",", "src_lens", ",", "src_oov", ",", "src_mask", ",", "oov_lists", ",", "max_sample_length", ",", "greedy", "=", "False", ",", "one2many", "=", "False", ",", "one2many_mode", "=", "1", ",", "num_predictions", "=", "1", ",", "perturb_std", "=", "0", ",", "entropy_regularize", "=", "False", ",", "title", "=", "None", ",", "title_lens", "=", "None", ",", "title_mask", "=", "None", ")", ":", "\n", "# src, src_lens, src_oov, src_mask, oov_lists, word2idx", "\n", "        ", "\"\"\"\n        :param src: a LongTensor containing the word indices of source sentences, [batch, src_seq_len], with oov words replaced by unk idx\n        :param src_lens: a list containing the length of src sequences for each batch, with len=batch, with oov words replaced by unk idx\n        :param src_oov: a LongTensor containing the word indices of source sentences, [batch, src_seq_len], contains the index of oov words (used by copy)\n        :param src_mask: a FloatTensor, [batch, src_seq_len]\n        :param oov_lists: list of oov words (idx2word) for each batch, len=batch\n        :param max_sample_length: The max length of sequence that can be sampled by the model\n        :param greedy: whether to sample the word with max prob at each decoding step\n        :return:\n        \"\"\"", "\n", "batch_size", ",", "max_src_len", "=", "list", "(", "src", ".", "size", "(", ")", ")", "\n", "max_num_oov", "=", "max", "(", "[", "len", "(", "oov", ")", "for", "oov", "in", "oov_lists", "]", ")", "# max number of oov for each batch", "\n", "\n", "# Encoding", "\n", "memory_bank", ",", "encoder_final_state", "=", "self", ".", "model", ".", "encoder", "(", "src", ",", "src_lens", ",", "src_mask", ",", "title", ",", "title_lens", ",", "title_mask", ")", "\n", "assert", "memory_bank", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "max_src_len", ",", "self", ".", "model", ".", "num_directions", "*", "self", ".", "model", ".", "encoder_size", "]", ")", "\n", "assert", "encoder_final_state", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "self", ".", "model", ".", "num_directions", "*", "self", ".", "model", ".", "encoder_size", "]", ")", "\n", "if", "greedy", "and", "entropy_regularize", ":", "\n", "            ", "raise", "ValueError", "(", "\"When using greedy, should not use entropy regularization.\"", ")", "\n", "\n", "# Init decoder state", "\n", "", "h_t_init", "=", "self", ".", "model", ".", "init_decoder_state", "(", "encoder_final_state", ")", "# [dec_layers, batch_size, decoder_size]", "\n", "\n", "if", "self", ".", "model", ".", "use_target_encoder", ":", "\n", "# init the hidden state of target encoder to zero vector", "\n", "            ", "h_t_te", "=", "h_t_init", ".", "new_zeros", "(", "1", ",", "batch_size", ",", "self", ".", "model", ".", "target_encoder_size", ")", "# [1, batch_size, target_encoder_size]", "\n", "\n", "", "if", "self", ".", "coverage_attn", ":", "\n", "            ", "coverage", "=", "torch", ".", "zeros_like", "(", "src", ",", "dtype", "=", "torch", ".", "float", ")", "# [batch, max_src_seq]", "\n", "", "else", ":", "\n", "            ", "coverage", "=", "None", "\n", "\n", "", "if", "self", ".", "review_attn", ":", "\n", "            ", "decoder_memory_bank", "=", "h_t_init", "[", "-", "1", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", "# [batch, 1, decoder_size]", "\n", "assert", "decoder_memory_bank", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "1", ",", "self", ".", "model", ".", "decoder_size", "]", ")", "\n", "", "else", ":", "\n", "            ", "decoder_memory_bank", "=", "None", "\n", "\n", "", "location_of_eos_for_each_batch", "=", "torch", ".", "zeros", "(", "batch_size", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "if", "self", ".", "model", ".", "separate_present_absent", ":", "\n", "            ", "location_of_peos_for_each_batch", "=", "torch", ".", "zeros", "(", "batch_size", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "self", ".", "model", ".", "goal_vector_mode", ">", "0", ":", "\n", "# byte tensor with size=batch_size to keep track of which batch has been proceeded to absent prediction", "\n", "                ", "is_absent", "=", "torch", ".", "zeros", "(", "batch_size", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "", "", "else", ":", "\n", "            ", "location_of_peos_for_each_batch", "=", "None", "\n", "\n", "# init y_t to be BOS token", "\n", "", "y_t_init", "=", "src", ".", "new_ones", "(", "batch_size", ")", "*", "self", ".", "bos_idx", "# [batch_size]", "\n", "sample_list", "=", "[", "{", "\"prediction\"", ":", "[", "]", ",", "\"attention\"", ":", "[", "]", ",", "\"done\"", ":", "False", "}", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "log_selected_token_dist", "=", "[", "]", "\n", "#prediction_all = src.new_ones(batch_size, max_sample_length) * self.pad_idx", "\n", "\n", "unfinished_mask", "=", "src", ".", "new_ones", "(", "(", "batch_size", ",", "1", ")", ",", "dtype", "=", "torch", ".", "uint8", ")", "# all seqs in a batch are unfinished at the beginning", "\n", "unfinished_mask_all", "=", "[", "unfinished_mask", "]", "\n", "pred_counters", "=", "src", ".", "new_zeros", "(", "batch_size", ",", "dtype", "=", "torch", ".", "uint8", ")", "# [batch_size]", "\n", "#pred_idx_all = []  # store the idx of prediction (e.g., the i-th prediction) for each token", "\n", "re_init_indicators", "=", "y_t_init", "==", "self", ".", "eos_idx", "\n", "eos_idx_mask_all", "=", "[", "re_init_indicators", ".", "unsqueeze", "(", "1", ")", "]", "\n", "\n", "if", "entropy_regularize", ":", "\n", "            ", "entropy", "=", "torch", ".", "zeros", "(", "batch_size", ")", ".", "to", "(", "src", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "entropy", "=", "None", "\n", "\n", "", "for", "t", "in", "range", "(", "max_sample_length", ")", ":", "\n", "            ", "if", "t", ">", "0", ":", "\n", "                ", "re_init_indicators", "=", "(", "y_t_next", "==", "self", ".", "eos_idx", ")", "# [batch_size]", "\n", "pred_counters", "+=", "re_init_indicators", "\n", "eos_idx_mask_all", ".", "append", "(", "re_init_indicators", ".", "unsqueeze", "(", "1", ")", ")", "\n", "unfinished_mask", "=", "pred_counters", "<", "num_predictions", "\n", "unfinished_mask", "=", "unfinished_mask", ".", "unsqueeze", "(", "1", ")", "\n", "unfinished_mask_all", ".", "append", "(", "unfinished_mask", ")", "\n", "\n", "#pred_idx_all.append(pred_counters.clone().unsqueeze(1))", "\n", "\n", "", "if", "t", "==", "0", ":", "\n", "                ", "h_t", "=", "h_t_init", "\n", "y_t", "=", "y_t_init", "\n", "", "elif", "one2many", "and", "one2many_mode", "==", "2", "and", "re_init_indicators", ".", "sum", "(", ")", ".", "item", "(", ")", ">", "0", ":", "\n", "                ", "h_t", "=", "[", "]", "\n", "y_t", "=", "[", "]", "\n", "for", "batch_idx", ",", "(", "indicator", ",", "pred_count", ")", "in", "enumerate", "(", "\n", "zip", "(", "re_init_indicators", ",", "pred_counters", ")", ")", ":", "\n", "                    ", "if", "indicator", ".", "item", "(", ")", "==", "1", "and", "pred_count", ".", "item", "(", ")", "<", "num_predictions", ":", "\n", "# some examples complete one keyphrase", "\n", "                        ", "h_t", ".", "append", "(", "h_t_init", "[", ":", ",", "batch_idx", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", "\n", "y_t", ".", "append", "(", "y_t_init", "[", "batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "", "else", ":", "# indicator.item() == 0 or indicator.item() == 1 and pred_count.item() == num_predictions:", "\n", "                        ", "h_t", ".", "append", "(", "h_t_next", "[", ":", ",", "batch_idx", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", "\n", "y_t", ".", "append", "(", "y_t_next", "[", "batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "", "", "h_t", "=", "torch", ".", "cat", "(", "h_t", ",", "dim", "=", "1", ")", "# [dec_layers, batch_size, decoder_size]", "\n", "y_t", "=", "torch", ".", "cat", "(", "y_t", ",", "dim", "=", "0", ")", "# [batch_size]", "\n", "", "elif", "one2many", "and", "one2many_mode", "==", "3", "and", "re_init_indicators", ".", "sum", "(", ")", ".", "item", "(", ")", ">", "0", ":", "\n", "                ", "h_t", "=", "h_t_next", "\n", "y_t", "=", "[", "]", "\n", "for", "batch_idx", ",", "(", "indicator", ",", "pred_count", ")", "in", "enumerate", "(", "\n", "zip", "(", "re_init_indicators", ",", "pred_counters", ")", ")", ":", "\n", "                    ", "if", "indicator", ".", "item", "(", ")", "==", "1", "and", "pred_count", ".", "item", "(", ")", "<", "num_predictions", ":", "\n", "# some examples complete one keyphrase", "\n", "# reset input to <BOS>", "\n", "                        ", "y_t", ".", "append", "(", "y_t_init", "[", "batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "# add a noisy vector to hidden state", "\n", "if", "perturb_std", ">", "0", ":", "\n", "                            ", "'''\n                            if perturb_decay_along_phrases:\n                                perturb_std_at_t = perturb_std / pred_count.item()\n                            else:\n                                perturb_std_at_t = perturb_std\n                            '''", "\n", "perturb_std_at_t", "=", "perturb_std", "/", "pred_count", ".", "item", "(", ")", "\n", "h_t", "=", "h_t", "+", "torch", ".", "normal", "(", "mean", "=", "0.0", ",", "std", "=", "torch", ".", "ones_like", "(", "h_t", ")", "*", "perturb_std_at_t", ")", "# [dec_layers, batch_size, decoder_size]", "\n", "", "", "else", ":", "# indicator.item() == 0 or indicator.item() == 1 and pred_count.item() == num_predictions:", "\n", "                        ", "y_t", ".", "append", "(", "y_t_next", "[", "batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "", "", "y_t", "=", "torch", ".", "cat", "(", "y_t", ",", "dim", "=", "0", ")", "# [batch_size]", "\n", "", "else", ":", "\n", "                ", "h_t", "=", "h_t_next", "\n", "y_t", "=", "y_t_next", "\n", "\n", "", "if", "self", ".", "review_attn", ":", "\n", "                ", "if", "t", ">", "0", ":", "\n", "                    ", "decoder_memory_bank", "=", "torch", ".", "cat", "(", "[", "decoder_memory_bank", ",", "h_t", "[", "-", "1", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "1", ")", "# [batch, t+1, decoder_size]", "\n", "\n", "# Turn any copied words to UNKS", "\n", "", "", "if", "self", ".", "copy_attn", ":", "\n", "                ", "y_t", "=", "y_t", ".", "masked_fill", "(", "\n", "y_t", ".", "gt", "(", "self", ".", "model", ".", "vocab_size", "-", "1", ")", ",", "self", ".", "model", ".", "unk_idx", ")", "\n", "\n", "", "if", "self", ".", "model", ".", "use_target_encoder", ":", "\n", "# encode the previous token using target encoder", "\n", "                ", "h_t_te_next", "=", "self", ".", "model", ".", "target_encoder", "(", "y_t", ".", "detach", "(", ")", ",", "h_t_te", ")", "\n", "h_t_te", "=", "h_t_te_next", "# [1, batch_size * beam_size, target_encoder_size]", "\n", "", "else", ":", "\n", "                ", "h_t_te", "=", "None", "\n", "\n", "", "if", "self", ".", "model", ".", "separate_present_absent", ":", "\n", "# update the is_absent vector", "\n", "                ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "if", "y_t", "[", "i", "]", ".", "item", "(", ")", "==", "self", ".", "peos_idx", ":", "\n", "                        ", "location_of_peos_for_each_batch", "[", "i", "]", "=", "t", "-", "1", "\n", "if", "self", ".", "model", ".", "goal_vector_mode", ">", "0", ":", "\n", "                            ", "is_absent", "[", "i", "]", "=", "1", "\n", "#", "\n", "", "", "", "if", "self", ".", "model", ".", "goal_vector_mode", ">", "0", ":", "\n", "                    ", "if", "self", ".", "model", ".", "manager_mode", "==", "1", ":", "\n", "                        ", "g_t", "=", "self", ".", "model", ".", "manager", "(", "is_absent", ")", "\n", "", "", "else", ":", "\n", "                    ", "g_t", "=", "None", "\n", "", "", "else", ":", "\n", "                ", "g_t", "=", "None", "\n", "\n", "# [batch, vocab_size], [dec_layers, batch, decoder_size], [batch, memory_bank_size], [batch, src_len], [batch, src_len]", "\n", "", "decoder_dist", ",", "h_t_next", ",", "context", ",", "attn_dist", ",", "_", ",", "coverage", "=", "self", ".", "model", ".", "decoder", "(", "y_t", ",", "h_t", ",", "memory_bank", ",", "src_mask", ",", "max_num_oov", ",", "src_oov", ",", "coverage", ",", "decoder_memory_bank", ",", "h_t_te", ",", "g_t", ")", "\n", "\n", "log_decoder_dist", "=", "torch", ".", "log", "(", "decoder_dist", "+", "EPS", ")", "# [batch, vocab_size]", "\n", "\n", "if", "entropy_regularize", ":", "\n", "                ", "entropy", "-=", "torch", ".", "bmm", "(", "decoder_dist", ".", "unsqueeze", "(", "1", ")", ",", "log_decoder_dist", ".", "unsqueeze", "(", "2", ")", ")", ".", "view", "(", "batch_size", ")", "# [batch]", "\n", "\n", "", "if", "greedy", ":", "# greedy decoding, only use in self-critical", "\n", "                ", "selected_token_dist", ",", "prediction", "=", "torch", ".", "max", "(", "decoder_dist", ",", "1", ")", "\n", "selected_token_dist", "=", "selected_token_dist", ".", "unsqueeze", "(", "1", ")", "# [batch, 1]", "\n", "prediction", "=", "prediction", ".", "unsqueeze", "(", "1", ")", "# [batch, 1]", "\n", "log_selected_token_dist", ".", "append", "(", "torch", ".", "log", "(", "selected_token_dist", "+", "EPS", ")", ")", "\n", "", "else", ":", "# sampling according to the probability distribution from the decoder", "\n", "                ", "prediction", "=", "torch", ".", "multinomial", "(", "decoder_dist", ",", "1", ")", "# [batch, 1]", "\n", "# select the probability of sampled tokens, and then take log, size: [batch, 1], append to a list", "\n", "log_selected_token_dist", ".", "append", "(", "log_decoder_dist", ".", "gather", "(", "1", ",", "prediction", ")", ")", "\n", "\n", "", "for", "batch_idx", ",", "sample", "in", "enumerate", "(", "sample_list", ")", ":", "\n", "                ", "if", "not", "sample", "[", "'done'", "]", ":", "\n", "                    ", "sample", "[", "'prediction'", "]", ".", "append", "(", "prediction", "[", "batch_idx", "]", "[", "0", "]", ")", "# 0 dim tensor", "\n", "sample", "[", "'attention'", "]", ".", "append", "(", "attn_dist", "[", "batch_idx", "]", ")", "# [src_len] tensor", "\n", "if", "int", "(", "prediction", "[", "batch_idx", "]", "[", "0", "]", ".", "item", "(", ")", ")", "==", "self", ".", "model", ".", "eos_idx", "and", "pred_counters", "[", "batch_idx", "]", ".", "item", "(", ")", "==", "num_predictions", "-", "1", ":", "\n", "                        ", "sample", "[", "'done'", "]", "=", "True", "\n", "location_of_eos_for_each_batch", "[", "batch_idx", "]", "=", "t", "\n", "", "", "else", ":", "\n", "                    ", "pass", "\n", "\n", "", "", "prediction", "=", "prediction", "*", "unfinished_mask", ".", "type_as", "(", "prediction", ")", "\n", "\n", "# prediction_all[:, t] = prediction[:, 0]", "\n", "y_t_next", "=", "prediction", "[", ":", ",", "0", "]", "# [batch]", "\n", "\n", "if", "all", "(", "(", "s", "[", "'done'", "]", "for", "s", "in", "sample_list", ")", ")", ":", "\n", "                ", "break", "\n", "\n", "#if t < max_sample_length - 1:", "\n", "#    #unfinished_mask = unfinished_mask_all[-1] * torch.ne(prediction, self.eos_idx)", "\n", "#    unfinished_mask = pred_counters < num_predictions", "\n", "#    unfinished_mask_all.append(unfinished_mask)", "\n", "\n", "", "", "for", "sample", "in", "sample_list", ":", "\n", "            ", "sample", "[", "'attention'", "]", "=", "torch", ".", "stack", "(", "sample", "[", "'attention'", "]", ",", "dim", "=", "0", ")", "# [trg_len, src_len]", "\n", "\n", "", "log_selected_token_dist", "=", "torch", ".", "cat", "(", "log_selected_token_dist", ",", "dim", "=", "1", ")", "# [batch, t]", "\n", "assert", "log_selected_token_dist", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "t", "+", "1", "]", ")", "\n", "#output_mask = torch.ne(prediction_all, self.pad_idx)[:, :t+1]  # [batch, t]", "\n", "#output_mask = output_mask.type(torch.FloatTensor).to(src.device)", "\n", "\n", "unfinished_mask_all", "=", "torch", ".", "cat", "(", "unfinished_mask_all", ",", "dim", "=", "1", ")", ".", "type_as", "(", "log_selected_token_dist", ")", "\n", "assert", "unfinished_mask_all", ".", "size", "(", ")", "==", "log_selected_token_dist", ".", "size", "(", ")", "\n", "#assert output_mask.size() == log_selected_token_dist.size()", "\n", "\n", "#pred_idx_all = torch.cat(pred_idx_all, dim=1).type(torch.LongTensor).to(src.device)", "\n", "#assert pred_idx_all.size() == log_selected_token_dist.size()", "\n", "\n", "eos_idx_mask_all", "=", "torch", ".", "cat", "(", "eos_idx_mask_all", ",", "dim", "=", "1", ")", ".", "to", "(", "src", ".", "device", ")", "\n", "assert", "eos_idx_mask_all", ".", "size", "(", ")", "==", "log_selected_token_dist", ".", "size", "(", ")", "\n", "\n", "#return sample_list, log_selected_token_dist, unfinished_mask_all, pred_idx_all", "\n", "\"\"\"\n        if entropy_regularize:\n            return sample_list, log_selected_token_dist, unfinished_mask_all, eos_idx_mask_all, entropy\n        else:\n            return sample_list, log_selected_token_dist, unfinished_mask_all, eos_idx_mask_all\n        \"\"\"", "\n", "return", "sample_list", ",", "log_selected_token_dist", ",", "unfinished_mask_all", ",", "eos_idx_mask_all", ",", "entropy", ",", "location_of_eos_for_each_batch", ",", "location_of_peos_for_each_batch", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.interactive_predict.process_opt": [[13, 69], ["torch.cuda.is_available", "torch.manual_seed", "torch.device", "torch.device", "print", "hasattr", "opt.pred_path.find", "os.path.exists", "os.makedirs", "ValueError", "ValueError"], "function", ["None"], ["def", "process_opt", "(", "opt", ")", ":", "\n", "    ", "if", "opt", ".", "seed", ">", "0", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "opt", ".", "seed", ")", "\n", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "if", "not", "opt", ".", "gpuid", ":", "\n", "            ", "opt", ".", "gpuid", "=", "0", "\n", "", "opt", ".", "device", "=", "torch", ".", "device", "(", "\"cuda:%d\"", "%", "opt", ".", "gpuid", ")", "\n", "", "else", ":", "\n", "        ", "opt", ".", "device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "opt", ".", "gpuid", "=", "-", "1", "\n", "print", "(", "\"CUDA is not available, fall back to CPU.\"", ")", "\n", "\n", "", "opt", ".", "exp", "=", "'predict.'", "+", "opt", ".", "exp", "\n", "if", "opt", ".", "one2many", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.one2many'", "\n", "\n", "", "if", "opt", ".", "one2many_mode", "==", "1", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.cat'", "\n", "\n", "", "if", "opt", ".", "copy_attention", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.copy'", "\n", "\n", "", "if", "opt", ".", "coverage_attn", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.coverage'", "\n", "\n", "", "if", "opt", ".", "review_attn", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.review'", "\n", "\n", "", "if", "opt", ".", "orthogonal_loss", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.orthogonal'", "\n", "\n", "", "if", "opt", ".", "use_target_encoder", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.target_encode'", "\n", "\n", "", "if", "hasattr", "(", "opt", ",", "'bidirectional'", ")", "and", "opt", ".", "bidirectional", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.bi-directional'", "\n", "", "else", ":", "\n", "        ", "opt", ".", "exp", "+=", "'.uni-directional'", "\n", "\n", "# fill time into the name", "\n", "", "if", "opt", ".", "pred_path", ".", "find", "(", "'%s'", ")", ">", "0", ":", "\n", "        ", "opt", ".", "pred_path", "=", "opt", ".", "pred_path", "%", "(", "opt", ".", "exp", ",", "opt", ".", "timemark", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "opt", ".", "pred_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "opt", ".", "pred_path", ")", "\n", "\n", "", "if", "not", "opt", ".", "one2many", "and", "opt", ".", "one2many_mode", ">", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"You cannot choose one2many mode without the -one2many options.\"", ")", "\n", "\n", "", "if", "opt", ".", "one2many", "and", "opt", ".", "one2many_mode", "==", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"If you choose one2many, you must specify the one2many mode.\"", ")", "\n", "\n", "#if opt.greedy and not opt.one2many:", "\n", "#    raise ValueError(\"Greedy sampling can only be used in one2many mode.\")", "\n", "", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.interactive_predict.main": [[71, 103], ["utils.data_loader.load_vocab", "pykp.io.build_interactive_predict_dataset", "pykp.io.KeyphraseDataset", "torch.utils.data.DataLoader", "predict.init_pretrained_model", "print", "predict.predict", "preprocess.read_tokenized_src_file", "preprocess.read_tokenized_src_file"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.data_loader.load_vocab", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.build_interactive_predict_dataset", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.predict.init_pretrained_model", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.predict.predict", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.read_tokenized_src_file", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.preprocess.read_tokenized_src_file"], ["", "def", "main", "(", "opt", ")", ":", "\n", "# load vocab", "\n", "    ", "word2idx", ",", "idx2word", ",", "vocab", "=", "load_vocab", "(", "opt", ")", "\n", "# load data", "\n", "# read tokenized text file and convert them to 2d list of words", "\n", "src_file", "=", "opt", ".", "src_file", "\n", "#trg_file = opt.trg_file", "\n", "#tokenized_train_pairs = read_src_and_trg_files(src_file, trg_file, is_train=False, remove_eos=opt.remove_title_eos)  # 2d list of word", "\n", "if", "opt", ".", "title_guided", ":", "\n", "        ", "tokenized_src", ",", "tokenized_title", "=", "read_tokenized_src_file", "(", "src_file", ",", "remove_eos", "=", "opt", ".", "remove_title_eos", ",", "title_guided", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "tokenized_src", "=", "read_tokenized_src_file", "(", "src_file", ",", "remove_eos", "=", "opt", ".", "remove_title_eos", ",", "title_guided", "=", "False", ")", "\n", "tokenized_title", "=", "None", "\n", "# convert the 2d list of words to a list of dictionary, with keys 'src', 'src_oov', 'trg', 'trg_copy', 'src_str', 'trg_str', 'oov_dict', 'oov_list'", "\n", "# since we don't need the targets during testing, 'trg' and 'trg_copy' are some dummy variables", "\n", "#test_one2many = build_dataset(tokenized_train_pairs, word2idx, idx2word, opt, mode=\"one2many\", include_original=True)", "\n", "", "test_one2many", "=", "build_interactive_predict_dataset", "(", "tokenized_src", ",", "word2idx", ",", "idx2word", ",", "opt", ",", "tokenized_title", ")", "\n", "# build the data loader", "\n", "test_one2many_dataset", "=", "KeyphraseDataset", "(", "test_one2many", ",", "word2idx", "=", "word2idx", ",", "idx2word", "=", "idx2word", ",", "\n", "type", "=", "'one2many'", ",", "delimiter_type", "=", "opt", ".", "delimiter_type", ",", "load_train", "=", "False", ",", "remove_src_eos", "=", "opt", ".", "remove_src_eos", ",", "title_guided", "=", "opt", ".", "title_guided", ")", "\n", "test_loader", "=", "DataLoader", "(", "dataset", "=", "test_one2many_dataset", ",", "\n", "collate_fn", "=", "test_one2many_dataset", ".", "collate_fn_one2many", ",", "\n", "num_workers", "=", "opt", ".", "batch_workers", ",", "batch_size", "=", "opt", ".", "batch_size", ",", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "False", ")", "\n", "# init the pretrained model", "\n", "model", "=", "predict", ".", "init_pretrained_model", "(", "opt", ")", "\n", "\n", "# Print out predict path", "\n", "print", "(", "\"Prediction path: %s\"", "%", "opt", ".", "pred_path", ")", "\n", "\n", "# predict the keyphrases of the src file and output it to opt.pred_path/predictions.txt", "\n", "predict", ".", "predict", "(", "test_loader", ",", "model", ",", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_valid_keyphrases": [[13, 32], ["len", "numpy.zeros", "enumerate", "len"], "function", ["None"], ["def", "check_valid_keyphrases", "(", "str_list", ")", ":", "\n", "    ", "num_pred_seq", "=", "len", "(", "str_list", ")", "\n", "is_valid", "=", "np", ".", "zeros", "(", "num_pred_seq", ",", "dtype", "=", "bool", ")", "\n", "for", "i", ",", "word_list", "in", "enumerate", "(", "str_list", ")", ":", "\n", "        ", "keep_flag", "=", "True", "\n", "\n", "if", "len", "(", "word_list", ")", "==", "0", ":", "\n", "            ", "keep_flag", "=", "False", "\n", "\n", "", "for", "w", "in", "word_list", ":", "\n", "            ", "if", "opt", ".", "invalidate_unk", ":", "\n", "                ", "if", "w", "==", "pykp", ".", "io", ".", "UNK_WORD", "or", "w", "==", "','", "or", "w", "==", "'.'", ":", "\n", "                    ", "keep_flag", "=", "False", "\n", "", "", "else", ":", "\n", "                ", "if", "w", "==", "','", "or", "w", "==", "'.'", ":", "\n", "                    ", "keep_flag", "=", "False", "\n", "", "", "", "is_valid", "[", "i", "]", "=", "keep_flag", "\n", "\n", "", "return", "is_valid", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.dummy_filter": [[34, 37], ["len", "numpy.ones"], "function", ["None"], ["", "def", "dummy_filter", "(", "str_list", ")", ":", "\n", "    ", "num_pred_seq", "=", "len", "(", "str_list", ")", "\n", "return", "np", ".", "ones", "(", "num_pred_seq", ",", "dtype", "=", "bool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_extra_one_word_seqs_mask": [[39, 51], ["len", "numpy.zeros", "enumerate", "len"], "function", ["None"], ["", "def", "compute_extra_one_word_seqs_mask", "(", "str_list", ")", ":", "\n", "    ", "num_pred_seq", "=", "len", "(", "str_list", ")", "\n", "mask", "=", "np", ".", "zeros", "(", "num_pred_seq", ",", "dtype", "=", "bool", ")", "\n", "num_one_word_seqs", "=", "0", "\n", "for", "i", ",", "word_list", "in", "enumerate", "(", "str_list", ")", ":", "\n", "        ", "if", "len", "(", "word_list", ")", "==", "1", ":", "\n", "            ", "num_one_word_seqs", "+=", "1", "\n", "if", "num_one_word_seqs", ">", "1", ":", "\n", "                ", "mask", "[", "i", "]", "=", "False", "\n", "continue", "\n", "", "", "mask", "[", "i", "]", "=", "True", "\n", "", "return", "mask", ",", "num_one_word_seqs", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_duplicate_keyphrases": [[53, 68], ["len", "numpy.ones", "set", "enumerate", "set.add"], "function", ["None"], ["", "def", "check_duplicate_keyphrases", "(", "keyphrase_str_list", ")", ":", "\n", "    ", "\"\"\"\n    :param keyphrase_str_list: a 2d list of tokens\n    :return: a boolean np array indicate, 1 = unique, 0 = duplicate\n    \"\"\"", "\n", "num_keyphrases", "=", "len", "(", "keyphrase_str_list", ")", "\n", "not_duplicate", "=", "np", ".", "ones", "(", "num_keyphrases", ",", "dtype", "=", "bool", ")", "\n", "keyphrase_set", "=", "set", "(", ")", "\n", "for", "i", ",", "keyphrase_word_list", "in", "enumerate", "(", "keyphrase_str_list", ")", ":", "\n", "        ", "if", "'_'", ".", "join", "(", "keyphrase_word_list", ")", "in", "keyphrase_set", ":", "\n", "            ", "not_duplicate", "[", "i", "]", "=", "False", "\n", "", "else", ":", "\n", "            ", "not_duplicate", "[", "i", "]", "=", "True", "\n", "", "keyphrase_set", ".", "add", "(", "'_'", ".", "join", "(", "keyphrase_word_list", ")", ")", "\n", "", "return", "not_duplicate", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_present_keyphrases": [[70, 107], ["len", "numpy.zeros", "enumerate", "joined_keyphrase_str.strip", "range", "enumerate", "len", "len"], "function", ["None"], ["", "def", "check_present_keyphrases", "(", "src_str", ",", "keyphrase_str_list", ",", "match_by_str", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    :param src_str: stemmed word list of source text\n    :param keyphrase_str_list: stemmed list of word list\n    :return:\n    \"\"\"", "\n", "num_keyphrases", "=", "len", "(", "keyphrase_str_list", ")", "\n", "is_present", "=", "np", ".", "zeros", "(", "num_keyphrases", ",", "dtype", "=", "bool", ")", "\n", "\n", "for", "i", ",", "keyphrase_word_list", "in", "enumerate", "(", "keyphrase_str_list", ")", ":", "\n", "        ", "joined_keyphrase_str", "=", "' '", ".", "join", "(", "keyphrase_word_list", ")", "\n", "\n", "if", "joined_keyphrase_str", ".", "strip", "(", ")", "==", "\"\"", ":", "# if the keyphrase is an empty string", "\n", "            ", "is_present", "[", "i", "]", "=", "False", "\n", "", "else", ":", "\n", "            ", "if", "not", "match_by_str", ":", "# match by word", "\n", "# check if it appears in source text", "\n", "                ", "match", "=", "False", "\n", "for", "src_start_idx", "in", "range", "(", "len", "(", "src_str", ")", "-", "len", "(", "keyphrase_word_list", ")", "+", "1", ")", ":", "\n", "                    ", "match", "=", "True", "\n", "for", "keyphrase_i", ",", "keyphrase_w", "in", "enumerate", "(", "keyphrase_word_list", ")", ":", "\n", "                        ", "src_w", "=", "src_str", "[", "src_start_idx", "+", "keyphrase_i", "]", "\n", "if", "src_w", "!=", "keyphrase_w", ":", "\n", "                            ", "match", "=", "False", "\n", "break", "\n", "", "", "if", "match", ":", "\n", "                        ", "break", "\n", "", "", "if", "match", ":", "\n", "                    ", "is_present", "[", "i", "]", "=", "True", "\n", "", "else", ":", "\n", "                    ", "is_present", "[", "i", "]", "=", "False", "\n", "", "", "else", ":", "# match by str", "\n", "                ", "if", "joined_keyphrase_str", "in", "' '", ".", "join", "(", "src_str", ")", ":", "\n", "                    ", "is_present", "[", "i", "]", "=", "True", "\n", "", "else", ":", "\n", "                    ", "is_present", "[", "i", "]", "=", "False", "\n", "", "", "", "", "return", "is_present", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.find_present_and_absent_index": [[109, 148], ["len", "enumerate", "joined_keyphrase_str.strip", "absent_indices.append", "range", "enumerate", "present_indices.append", "absent_indices.append", "len", "len"], "function", ["None"], ["", "def", "find_present_and_absent_index", "(", "src_str", ",", "keyphrase_str_list", ",", "use_name_variations", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    :param src_str: stemmed word list of source text\n    :param keyphrase_str_list: stemmed list of word list\n    :return:\n    \"\"\"", "\n", "num_keyphrases", "=", "len", "(", "keyphrase_str_list", ")", "\n", "#is_present = np.zeros(num_keyphrases, dtype=bool)", "\n", "present_indices", "=", "[", "]", "\n", "absent_indices", "=", "[", "]", "\n", "\n", "for", "i", ",", "v", "in", "enumerate", "(", "keyphrase_str_list", ")", ":", "\n", "        ", "if", "use_name_variations", ":", "\n", "            ", "keyphrase_word_list", "=", "v", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "keyphrase_word_list", "=", "v", "\n", "", "joined_keyphrase_str", "=", "' '", ".", "join", "(", "keyphrase_word_list", ")", "\n", "if", "joined_keyphrase_str", ".", "strip", "(", ")", "==", "\"\"", ":", "# if the keyphrase is an empty string", "\n", "#is_present[i] = False", "\n", "            ", "absent_indices", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "# check if it appears in source text", "\n", "            ", "match", "=", "False", "\n", "for", "src_start_idx", "in", "range", "(", "len", "(", "src_str", ")", "-", "len", "(", "keyphrase_word_list", ")", "+", "1", ")", ":", "\n", "                ", "match", "=", "True", "\n", "for", "keyphrase_i", ",", "keyphrase_w", "in", "enumerate", "(", "keyphrase_word_list", ")", ":", "\n", "                    ", "src_w", "=", "src_str", "[", "src_start_idx", "+", "keyphrase_i", "]", "\n", "if", "src_w", "!=", "keyphrase_w", ":", "\n", "                        ", "match", "=", "False", "\n", "break", "\n", "", "", "if", "match", ":", "\n", "                    ", "break", "\n", "", "", "if", "match", ":", "\n", "#is_present[i] = True", "\n", "                ", "present_indices", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "#is_present[i] = False", "\n", "                ", "absent_indices", ".", "append", "(", "i", ")", "\n", "", "", "", "return", "present_indices", ",", "absent_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.separate_present_absent_by_source_with_variations": [[150, 202], ["len", "enumerate", "enumerate", "present_indices.append", "absent_indices.append", "joined_keyphrase_str.strip", "range", "present_indices.append", "enumerate", "absent_indices.append", "ValueError", "len", "len"], "function", ["None"], ["", "def", "separate_present_absent_by_source_with_variations", "(", "src_token_list", ",", "keyphrase_variation_token_3dlist", ",", "use_name_variations", "=", "True", ")", ":", "\n", "    ", "num_keyphrases", "=", "len", "(", "keyphrase_variation_token_3dlist", ")", "\n", "present_indices", "=", "[", "]", "\n", "absent_indices", "=", "[", "]", "\n", "\n", "for", "keyphrase_idx", ",", "v", "in", "enumerate", "(", "keyphrase_variation_token_3dlist", ")", ":", "\n", "        ", "if", "use_name_variations", ":", "\n", "            ", "keyphrase_variation_token_2dlist", "=", "v", "\n", "", "else", ":", "\n", "            ", "keyphrase_variation_token_2dlist", "=", "[", "v", "]", "\n", "", "present_flag", "=", "False", "\n", "absent_flag", "=", "False", "\n", "# iterate every variation of a keyphrase", "\n", "for", "variation_idx", ",", "keyphrase_token_list", "in", "enumerate", "(", "keyphrase_variation_token_2dlist", ")", ":", "\n", "            ", "joined_keyphrase_str", "=", "' '", ".", "join", "(", "keyphrase_token_list", ")", "\n", "if", "joined_keyphrase_str", ".", "strip", "(", ")", "==", "\"\"", ":", "# if the keyphrase is an empty string", "\n", "                ", "absent_flag", "=", "True", "\n", "", "else", ":", "# check if it appears in source text", "\n", "                ", "match", "=", "False", "\n", "for", "src_start_idx", "in", "range", "(", "len", "(", "src_token_list", ")", "-", "len", "(", "keyphrase_token_list", ")", "+", "1", ")", ":", "\n", "                    ", "match", "=", "True", "\n", "for", "keyphrase_i", ",", "keyphrase_w", "in", "enumerate", "(", "keyphrase_token_list", ")", ":", "\n", "                        ", "src_w", "=", "src_token_list", "[", "src_start_idx", "+", "keyphrase_i", "]", "\n", "if", "src_w", "!=", "keyphrase_w", ":", "\n", "                            ", "match", "=", "False", "\n", "break", "\n", "", "", "if", "match", ":", "\n", "                        ", "break", "\n", "", "", "if", "match", ":", "\n", "# is_present[i] = True", "\n", "# present_indices.append(i)", "\n", "                    ", "present_flag", "=", "True", "\n", "", "else", ":", "\n", "# is_present[i] = False", "\n", "# absent_indices.append(i)", "\n", "                    ", "absent_flag", "=", "True", "\n", "", "", "", "if", "present_flag", "and", "absent_flag", ":", "\n", "            ", "present_indices", ".", "append", "(", "keyphrase_idx", ")", "\n", "absent_indices", ".", "append", "(", "keyphrase_idx", ")", "\n", "", "elif", "present_flag", ":", "\n", "            ", "present_indices", ".", "append", "(", "keyphrase_idx", ")", "\n", "", "elif", "absent_flag", ":", "\n", "            ", "absent_indices", ".", "append", "(", "keyphrase_idx", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Problem occurs in present absent checking\"", ")", "\n", "\n", "", "", "present_keyphrase_variation_token_3dlist", "=", "[", "keyphrase_variation_token_3dlist", "[", "present_index", "]", "for", "present_index", "in", "\n", "present_indices", "]", "\n", "absent_keyphrase_variation_token_3dlist", "=", "[", "keyphrase_variation_token_3dlist", "[", "absent_index", "]", "for", "absent_index", "in", "\n", "absent_indices", "]", "\n", "\n", "return", "present_keyphrase_variation_token_3dlist", ",", "absent_keyphrase_variation_token_3dlist", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_present_and_duplicate_keyphrases": [[204, 249], ["len", "numpy.zeros", "numpy.ones", "set", "enumerate", "set.add", "joined_keyphrase_str.strip", "range", "enumerate", "len", "len"], "function", ["None"], ["", "def", "check_present_and_duplicate_keyphrases", "(", "src_str", ",", "keyphrase_str_list", ",", "match_by_str", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    :param src_str: stemmed word list of source text\n    :param keyphrase_str_list: stemmed list of word list\n    :return:\n    \"\"\"", "\n", "num_keyphrases", "=", "len", "(", "keyphrase_str_list", ")", "\n", "is_present", "=", "np", ".", "zeros", "(", "num_keyphrases", ",", "dtype", "=", "bool", ")", "\n", "not_duplicate", "=", "np", ".", "ones", "(", "num_keyphrases", ",", "dtype", "=", "bool", ")", "\n", "keyphrase_set", "=", "set", "(", ")", "\n", "\n", "for", "i", ",", "keyphrase_word_list", "in", "enumerate", "(", "keyphrase_str_list", ")", ":", "\n", "        ", "joined_keyphrase_str", "=", "' '", ".", "join", "(", "keyphrase_word_list", ")", "\n", "if", "joined_keyphrase_str", "in", "keyphrase_set", ":", "\n", "            ", "not_duplicate", "[", "i", "]", "=", "False", "\n", "", "else", ":", "\n", "            ", "not_duplicate", "[", "i", "]", "=", "True", "\n", "\n", "", "if", "joined_keyphrase_str", ".", "strip", "(", ")", "==", "\"\"", ":", "# if the keyphrase is an empty string", "\n", "            ", "is_present", "[", "i", "]", "=", "False", "\n", "", "else", ":", "\n", "            ", "if", "not", "match_by_str", ":", "# match by word", "\n", "# check if it appears in source text", "\n", "                ", "match", "=", "False", "\n", "for", "src_start_idx", "in", "range", "(", "len", "(", "src_str", ")", "-", "len", "(", "keyphrase_word_list", ")", "+", "1", ")", ":", "\n", "                    ", "match", "=", "True", "\n", "for", "keyphrase_i", ",", "keyphrase_w", "in", "enumerate", "(", "keyphrase_word_list", ")", ":", "\n", "                        ", "src_w", "=", "src_str", "[", "src_start_idx", "+", "keyphrase_i", "]", "\n", "if", "src_w", "!=", "keyphrase_w", ":", "\n", "                            ", "match", "=", "False", "\n", "break", "\n", "", "", "if", "match", ":", "\n", "                        ", "break", "\n", "", "", "if", "match", ":", "\n", "                    ", "is_present", "[", "i", "]", "=", "True", "\n", "", "else", ":", "\n", "                    ", "is_present", "[", "i", "]", "=", "False", "\n", "", "", "else", ":", "# match by str", "\n", "                ", "if", "joined_keyphrase_str", "in", "' '", ".", "join", "(", "src_str", ")", ":", "\n", "                    ", "is_present", "[", "i", "]", "=", "True", "\n", "", "else", ":", "\n", "                    ", "is_present", "[", "i", "]", "=", "False", "\n", "", "", "", "keyphrase_set", ".", "add", "(", "joined_keyphrase_str", ")", "\n", "\n", "", "return", "is_present", ",", "not_duplicate", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result_backup": [[251, 279], ["len", "len", "numpy.zeros", "enumerate", "enumerate", "zip", "enumerate", "len", "len"], "function", ["None"], ["", "def", "compute_match_result_backup", "(", "trg_str_list", ",", "pred_str_list", ",", "type", "=", "'exact'", ")", ":", "\n", "    ", "assert", "type", "in", "[", "'exact'", ",", "'sub'", "]", ",", "\"Right now only support exact matching and substring matching\"", "\n", "num_pred_str", "=", "len", "(", "pred_str_list", ")", "\n", "num_trg_str", "=", "len", "(", "trg_str_list", ")", "\n", "is_match", "=", "np", ".", "zeros", "(", "num_pred_str", ",", "dtype", "=", "bool", ")", "\n", "\n", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "        ", "if", "type", "==", "'exact'", ":", "# exact matching", "\n", "            ", "is_match", "[", "pred_idx", "]", "=", "False", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "                ", "if", "len", "(", "pred_word_list", ")", "!=", "len", "(", "trg_word_list", ")", ":", "# if length not equal, it cannot be a match", "\n", "                    ", "continue", "\n", "", "match", "=", "True", "\n", "for", "pred_w", ",", "trg_w", "in", "zip", "(", "pred_word_list", ",", "trg_word_list", ")", ":", "\n", "                    ", "if", "pred_w", "!=", "trg_w", ":", "\n", "                        ", "match", "=", "False", "\n", "break", "\n", "# If there is one exact match in the target, match succeeds, go the next prediction", "\n", "", "", "if", "match", ":", "\n", "                    ", "is_match", "[", "pred_idx", "]", "=", "True", "\n", "break", "\n", "", "", "", "elif", "type", "==", "'sub'", ":", "# consider a match if the prediction is a subset of the target", "\n", "            ", "joined_pred_word_list", "=", "' '", ".", "join", "(", "pred_word_list", ")", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "                ", "if", "joined_pred_word_list", "in", "' '", ".", "join", "(", "trg_word_list", ")", ":", "\n", "                    ", "is_match", "[", "pred_idx", "]", "=", "True", "\n", "break", "\n", "", "", "", "", "return", "is_match", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_edit_distance": [[281, 309], ["len", "len", "numpy.zeros", "range", "range", "enumerate", "max", "max", "enumerate", "max", "max", "min"], "function", ["None"], ["", "def", "compute_token_level_reward_edit_distance", "(", "trg_word_list", ",", "pred_word_list", ")", ":", "\n", "#\u7f16\u8f91\u8ddd\u79bb", "\n", "    ", "add_score", "=", "1.0", "\n", "del_score", "=", "1.0", "\n", "exc_score", "=", "1.0", "\n", "threshold_pos", "=", "1.0", "\n", "threshold_neg", "=", "0.0", "\n", "assert", "threshold_pos", ">", "threshold_neg", "\n", "len_trg", "=", "len", "(", "trg_word_list", ")", "\n", "len_pred", "=", "len", "(", "pred_word_list", ")", "\n", "max_score", "=", "max", "(", "add_score", ",", "del_score", ",", "exc_score", ")", "*", "max", "(", "len_trg", ",", "len_pred", ")", "\n", "dp", "=", "np", ".", "zeros", "(", "(", "len_pred", "+", "3", ",", "len_trg", "+", "3", ")", ")", "\n", "for", "idx", "in", "range", "(", "1", ",", "len_pred", "+", "1", ")", ":", "\n", "        ", "dp", "[", "idx", "]", "[", "0", "]", "=", "idx", "\n", "", "for", "idy", "in", "range", "(", "1", ",", "len_trg", "+", "1", ")", ":", "\n", "        ", "dp", "[", "0", "]", "[", "idy", "]", "=", "idy", "\n", "", "for", "pred_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "        ", "for", "trg_idx", ",", "trg_word", "in", "enumerate", "(", "trg_word_list", ")", ":", "\n", "            ", "if", "trg_word", "==", "pred_word", ":", "\n", "                ", "dp", "[", "pred_idx", "+", "1", "]", "[", "trg_idx", "+", "1", "]", "=", "dp", "[", "pred_idx", "]", "[", "trg_idx", "]", "\n", "", "else", ":", "\n", "                ", "dp", "[", "pred_idx", "+", "1", "]", "[", "trg_idx", "+", "1", "]", "=", "min", "(", "dp", "[", "pred_idx", "]", "[", "trg_idx", "+", "1", "]", "+", "add_score", ",", "\n", "dp", "[", "pred_idx", "+", "1", "]", "[", "trg_idx", "]", "+", "del_score", ",", "\n", "dp", "[", "pred_idx", "]", "[", "trg_idx", "]", "+", "exc_score", ")", "\n", "", "", "", "score", "=", "max", "(", "max_score", "-", "dp", "[", "len_pred", "]", "[", "len_trg", "]", ",", "0", ")", "/", "max", "(", "max_score", ",", "1e-5", ")", "\n", "score", "=", "1.0", "if", "score", ">=", "threshold_pos", "else", "score", "\n", "score", "=", "0.0", "if", "score", "<=", "threshold_neg", "else", "score", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_token_f1": [[310, 322], ["enumerate", "max", "max", "max", "rest_trg_word_list.index", "rest_trg_word_list.pop", "len", "len"], "function", ["None"], ["", "def", "compute_token_level_reward_token_f1", "(", "trg_word_list", ",", "pred_word_list", ")", ":", "\n", "    ", "num_accurary", "=", "0", "\n", "rest_trg_word_list", "=", "[", "trg", "for", "trg", "in", "trg_word_list", "]", "\n", "for", "pred_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "        ", "if", "pred_word", "in", "rest_trg_word_list", ":", "\n", "            ", "num_accurary", "+=", "1", "\n", "idx", "=", "rest_trg_word_list", ".", "index", "(", "pred_word", ")", "\n", "rest_trg_word_list", ".", "pop", "(", "idx", ")", "\n", "", "", "precision", "=", "1.0", "*", "num_accurary", "/", "max", "(", "len", "(", "pred_word_list", ")", ",", "1", ")", "\n", "recall", "=", "1.0", "*", "num_accurary", "/", "max", "(", "len", "(", "trg_word_list", ")", ",", "1", ")", "\n", "F1_score", "=", "2", "*", "precision", "*", "recall", "/", "max", "(", "precision", "+", "recall", ",", "1e-5", ")", "\n", "return", "F1_score", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_token_f1": [[324, 336], ["enumerate", "max", "max", "max", "rest_trg_word_list.index", "rest_trg_word_list.pop", "len", "len"], "function", ["None"], ["", "def", "compute_token_level_token_f1", "(", "trg_word_list", ",", "pred_word_list", ")", ":", "\n", "    ", "num_accurary", "=", "0", "\n", "rest_trg_word_list", "=", "[", "trg", "for", "trg", "in", "trg_word_list", "]", "\n", "for", "pred_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "        ", "if", "pred_word", "in", "rest_trg_word_list", ":", "\n", "            ", "num_accurary", "+=", "1", "\n", "idx", "=", "rest_trg_word_list", ".", "index", "(", "pred_word", ")", "\n", "rest_trg_word_list", ".", "pop", "(", "idx", ")", "\n", "", "", "precision", "=", "1.0", "*", "num_accurary", "/", "max", "(", "len", "(", "pred_word_list", ")", ",", "1", ")", "\n", "recall", "=", "1.0", "*", "num_accurary", "/", "max", "(", "len", "(", "trg_word_list", ")", ",", "1", ")", "\n", "F1_score", "=", "2", "*", "precision", "*", "recall", "/", "max", "(", "precision", "+", "recall", ",", "1e-5", ")", "\n", "return", "F1_score", ",", "precision", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_cp": [[337, 344], ["evaluate_prediction.compute_token_level_reward_edit_distance", "evaluate_prediction.compute_token_level_reward_token_f1"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_edit_distance", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_token_f1"], ["", "def", "compute_cp", "(", "trg_word_list", ",", "pred_word_list", ")", ":", "\n", "    ", "alpha", "=", "1.0", "\n", "beta", "=", "2.0", "\n", "cur_edit_distance_score", "=", "compute_token_level_reward_edit_distance", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "cur_token_f1_score", "=", "compute_token_level_reward_token_f1", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "cur_score", "=", "(", "alpha", "*", "cur_edit_distance_score", "+", "beta", "*", "cur_token_f1_score", ")", "/", "(", "alpha", "+", "beta", ")", "\n", "return", "cur_score", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward": [[346, 439], ["len", "len", "numpy.zeros", "enumerate", "torch.tensor", "torch.cuda.is_available", "enumerate", "numpy.sum", "max", "bert_input_padded_words.cuda.append", "bert_input_padded_words.cuda.cuda", "torch.no_grad", "predicted_score.cpu().detach().numpy", "numpy.sum", "max", "max", "max", "bert_input_words.append", "bert", "[].max", "evaluate_prediction.compute_token_level_reward_edit_distance", "max", "bert.vocab.to_index", "len", "predicted_score.cpu().detach", "max", "evaluate_prediction.compute_token_level_reward_token_f1", "bert_words.split", "len", "evaluate_prediction.compute_cp", "max", "predicted_score.view", "predicted_score.cpu", "max", "torch.no_grad", "bert", "float"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_edit_distance", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_token_f1", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_cp"], ["", "def", "compute_phrase_level_reward", "(", "trg_str_list", ",", "pred_str_list", ",", "token_reward_type", ",", "bert", "=", "None", ")", ":", "\n", "    ", "num_pred_str", "=", "len", "(", "pred_str_list", ")", "\n", "num_trg_str", "=", "len", "(", "trg_str_list", ")", "\n", "score", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "if", "num_pred_str", "==", "0", "or", "num_trg_str", "==", "0", ":", "\n", "        ", "if", "num_pred_str", ">", "0", ":", "\n", "            ", "return", "-", "0.01", "\n", "", "return", "0.", "\n", "###", "\n", "# need_to_write = True", "\n", "# file_size = os.path.getsize('/remote-home/ygxu/workspace/KG/KGM/score_data.txt')", "\n", "# if need_to_write:", "\n", "#     f = open('/remote-home/ygxu/workspace/KG/KGM/score_data.txt', 'a+')", "\n", "# else:", "\n", "#     f = None", "\n", "###", "\n", "", "if", "token_reward_type", "==", "5", ":", "\n", "        ", "bert_input_words", "=", "[", "]", "\n", "bert_input_padded_words", "=", "[", "]", "\n", "bert_max_len", "=", "0", "\n", "for", "target_list", "in", "trg_str_list", ":", "\n", "            ", "for", "predicted_list", "in", "pred_str_list", ":", "\n", "                ", "bert_words", "=", "f'{\" \".join(target_list)} [SEP] {\" \".join(predicted_list)}'", "\n", "bert_input_instance", "=", "[", "bert", ".", "vocab", ".", "to_index", "(", "w", ")", "for", "w", "in", "bert_words", ".", "split", "(", "' '", ")", "]", "\n", "bert_max_len", "=", "max", "(", "bert_max_len", ",", "len", "(", "bert_input_instance", ")", ")", "\n", "bert_input_words", ".", "append", "(", "bert_input_instance", ")", "\n", "", "", "for", "ins", "in", "bert_input_words", ":", "\n", "            ", "new_ins", "=", "ins", "+", "[", "bert", ".", "vocab", ".", "padding_idx", "]", "*", "(", "bert_max_len", "-", "len", "(", "ins", ")", ")", "\n", "bert_input_padded_words", ".", "append", "(", "new_ins", ")", "\n", "", "bert_input_padded_words", "=", "torch", ".", "tensor", "(", "bert_input_padded_words", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "bert_input_padded_words", "=", "bert_input_padded_words", ".", "cuda", "(", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "predicted_score", "=", "bert", "(", "bert_input_padded_words", ")", "[", "0", "]", "# [pred * trg, 3]", "\n", "predicted_score", "=", "predicted_score", ".", "view", "(", "num_pred_str", ",", "num_trg_str", ",", "-", "1", ")", "[", ":", ",", ":", ",", "0", "]", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", "\n", "score", "=", "predicted_score", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "original_score", "=", "np", ".", "sum", "(", "score", ")", "/", "max", "(", "num_pred_str", ",", "1.0", ")", "\n", "\n", "# corr = - ((num_trg_str - num_pred_str) ** 2) / (max(num_trg_str, 1) ** 2) + 1", "\n", "if", "num_pred_str", "<", "num_trg_str", ":", "\n", "# corr = num_pred_str / max(num_trg_str, 1)", "\n", "            ", "corr", "=", "(", "num_pred_str", "**", "2", ")", "/", "(", "max", "(", "num_trg_str", ",", "1", ")", "**", "2", ")", "\n", "", "elif", "num_pred_str", "==", "num_trg_str", ":", "\n", "            ", "corr", "=", "1.", "\n", "", "else", ":", "\n", "            ", "corr", "=", "-", "(", "(", "num_trg_str", "-", "num_pred_str", ")", "**", "2", ")", "/", "(", "max", "(", "num_trg_str", ",", "1", ")", "**", "2", ")", "+", "1", "\n", "\n", "", "return", "original_score", "*", "corr", "\n", "\n", "", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "        ", "max_score", "=", "0.0", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "            ", "if", "token_reward_type", "==", "1", ":", "\n", "                ", "cur_score", "=", "compute_token_level_reward_edit_distance", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "", "elif", "token_reward_type", "==", "2", ":", "\n", "                ", "cur_score", "=", "compute_token_level_reward_token_f1", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "", "elif", "token_reward_type", "==", "3", ":", "\n", "                ", "cur_score", "=", "compute_cp", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "", "elif", "token_reward_type", "==", "4", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "score_tmp", ",", "_", "=", "bert", "(", "' '", ".", "join", "(", "pred_word_list", ")", ",", "' '", ".", "join", "(", "trg_word_list", ")", ")", "\n", "cur_score", "=", "float", "(", "score_tmp", "[", "0", "]", "[", "0", "]", ")", "\n", "\n", "###", "\n", "# token_f1_score = compute_token_level_reward_token_f1(trg_word_list, pred_word_list)", "\n", "# example = f'{\" \".join(trg_word_list)}\\t{\" \".join(pred_word_list)}\\t{cur_score}\\t{token_f1_score}\\n'", "\n", "# if f is not None and file_size < 1024 ** 3:", "\n", "#     f.write(example)", "\n", "###", "\n", "\n", "", "", "max_score", "=", "max", "(", "max_score", ",", "cur_score", ")", "\n", "", "score", "[", "pred_idx", "]", "=", "max_score", "\n", "\n", "", "original_score", "=", "np", ".", "sum", "(", "score", ")", "/", "max", "(", "num_pred_str", ",", "1.0", ")", "\n", "\n", "\n", "###", "\n", "# if f is not None:", "\n", "#     f.close()", "\n", "###", "\n", "\n", "# corr = - ((num_trg_str - num_pred_str) ** 2) / (max(num_trg_str, 1) ** 2) + 1", "\n", "if", "num_pred_str", "<", "num_trg_str", ":", "\n", "#corr = num_pred_str / max(num_trg_str, 1)", "\n", "        ", "corr", "=", "(", "num_pred_str", "**", "2", ")", "/", "(", "max", "(", "num_trg_str", ",", "1", ")", "**", "2", ")", "\n", "", "elif", "num_pred_str", "==", "num_trg_str", ":", "\n", "        ", "corr", "=", "1.", "\n", "", "else", ":", "\n", "        ", "corr", "=", "-", "(", "(", "num_trg_str", "-", "num_pred_str", ")", "**", "2", ")", "/", "(", "max", "(", "num_trg_str", ",", "1", ")", "**", "2", ")", "+", "1.", "\n", "\n", "", "return", "original_score", "*", "corr", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward_15": [[441, 478], ["len", "len", "numpy.zeros", "enumerate", "enumerate", "numpy.sum", "max", "max", "evaluate_prediction.compute_token_level_reward_edit_distance", "max", "evaluate_prediction.compute_token_level_reward_token_f1", "max", "evaluate_prediction.compute_cp", "torch.no_grad", "bert", "float"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_edit_distance", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_token_f1", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_cp"], ["", "def", "compute_phrase_level_reward_15", "(", "trg_str_list", ",", "pred_str_list", ",", "token_reward_type", ",", "bert", "=", "None", ")", ":", "\n", "    ", "num_pred_str", "=", "len", "(", "pred_str_list", ")", "\n", "num_trg_str", "=", "len", "(", "trg_str_list", ")", "\n", "score", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "if", "num_pred_str", "==", "0", "or", "num_trg_str", "==", "0", ":", "\n", "        ", "if", "num_pred_str", ">", "0", ":", "\n", "            ", "return", "-", "0.01", "\n", "", "return", "0.", "\n", "\n", "", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "        ", "max_score", "=", "0.0", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "            ", "if", "token_reward_type", "==", "1", ":", "\n", "                ", "cur_score", "=", "compute_token_level_reward_edit_distance", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "", "elif", "token_reward_type", "==", "2", ":", "\n", "                ", "cur_score", "=", "compute_token_level_reward_token_f1", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "", "elif", "token_reward_type", "==", "3", ":", "\n", "                ", "cur_score", "=", "compute_cp", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "", "elif", "token_reward_type", "==", "4", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "score_tmp", ",", "_", "=", "bert", "(", "' '", ".", "join", "(", "pred_word_list", ")", ",", "' '", ".", "join", "(", "trg_word_list", ")", ")", "\n", "cur_score", "=", "float", "(", "score_tmp", "[", "0", "]", "[", "0", "]", ")", "\n", "\n", "\n", "", "", "max_score", "=", "max", "(", "max_score", ",", "cur_score", ")", "\n", "", "score", "[", "pred_idx", "]", "=", "max_score", "\n", "\n", "", "original_score", "=", "np", ".", "sum", "(", "score", ")", "/", "max", "(", "num_pred_str", ",", "1.0", ")", "\n", "\n", "# corr = - ((num_trg_str - num_pred_str) ** 2) / (max(num_trg_str, 1) ** 2) + 1", "\n", "if", "num_pred_str", "<=", "num_trg_str", "+", "1", ":", "\n", "# corr = num_pred_str / max(num_trg_str, 1)", "\n", "        ", "corr", "=", "(", "num_pred_str", "**", "2", ")", "/", "(", "max", "(", "num_trg_str", ",", "1", ")", "**", "2", ")", "\n", "", "else", ":", "\n", "        ", "corr", "=", "-", "(", "(", "num_trg_str", "-", "num_pred_str", ")", "**", "2", ")", "/", "(", "max", "(", "num_trg_str", ",", "1", ")", "**", "2", ")", "+", "1.", "\n", "\n", "", "return", "original_score", "*", "corr", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward_add": [[479, 508], ["len", "len", "numpy.zeros", "enumerate", "numpy.sum", "enumerate", "max", "evaluate_prediction.compute_token_level_reward_edit_distance", "evaluate_prediction.compute_token_level_reward_token_f1", "evaluate_prediction.compute_cp"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_edit_distance", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_token_f1", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_cp"], ["", "def", "compute_phrase_level_reward_add", "(", "trg_str_list", ",", "pred_str_list", ",", "token_reward_type", "=", "1", ")", ":", "\n", "    ", "num_pred_str", "=", "len", "(", "pred_str_list", ")", "\n", "num_trg_str", "=", "len", "(", "trg_str_list", ")", "\n", "if", "num_pred_str", "==", "0", ":", "#no trg or no pred", "\n", "        ", "if", "num_trg_str", "==", "0", ":", "\n", "            ", "return", "0.5", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "", "", "score", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "threshold", "=", "0.1", "\n", "punishment", "=", "-", "0.05", "\n", "\n", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "        ", "max_score", "=", "0.0", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "            ", "if", "token_reward_type", "==", "1", ":", "\n", "                ", "cur_score", "=", "compute_token_level_reward_edit_distance", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "", "elif", "token_reward_type", "==", "2", ":", "\n", "                ", "cur_score", "=", "compute_token_level_reward_token_f1", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "", "elif", "token_reward_type", "==", "3", ":", "\n", "                ", "cur_score", "=", "compute_cp", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "\n", "", "max_score", "=", "max", "(", "max_score", ",", "cur_score", ")", "\n", "", "score", "[", "pred_idx", "]", "=", "max_score", "\n", "if", "score", "[", "pred_idx", "]", "<=", "threshold", ":", "\n", "            ", "score", "[", "pred_idx", "]", "=", "punishment", "\n", "\n", "", "", "original_score", "=", "np", ".", "sum", "(", "score", ")", "\n", "return", "original_score", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward_add_coverage": [[509, 568], ["len", "len", "numpy.zeros", "enumerate", "enumerate", "numpy.sum", "enumerate", "enumerate", "enumerate", "evaluate_prediction.compute_token_level_reward_edit_distance", "max"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_edit_distance"], ["", "def", "compute_phrase_level_reward_add_coverage", "(", "trg_str_list", ",", "pred_str_list", ")", ":", "\n", "    ", "num_pred_str", "=", "len", "(", "pred_str_list", ")", "\n", "num_trg_str", "=", "len", "(", "trg_str_list", ")", "\n", "if", "num_pred_str", "==", "0", ":", "#no trg or no pred", "\n", "        ", "if", "num_trg_str", "==", "0", ":", "\n", "            ", "return", "0.5", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "", "", "score", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "threshold", "=", "0.1", "\n", "punishment", "=", "-", "0.05", "\n", "punishment_repeat", "=", "-", "0.3", "\n", "\n", "dict_trg", "=", "{", "}", "\n", "dict_pred", "=", "{", "}", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "        ", "for", "word_idx", ",", "trg_word", "in", "enumerate", "(", "trg_word_list", ")", ":", "\n", "            ", "if", "trg_word", "in", "dict_trg", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "=", "1", "\n", "\n", "", "", "", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "        ", "max_score", "=", "0.0", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "# if token_reward_type == 1:", "\n", "            ", "cur_score", "=", "compute_token_level_reward_edit_distance", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "# elif token_reward_type == 2:", "\n", "#     cur_score = compute_token_level_reward_token_f1(trg_word_list, pred_word_list)", "\n", "# elif token_reward_type == 3:", "\n", "#     cur_score = compute_cp(trg_word_list, pred_word_list)", "\n", "\n", "max_score", "=", "max", "(", "max_score", ",", "cur_score", ")", "\n", "", "score", "[", "pred_idx", "]", "=", "max_score", "\n", "if", "score", "[", "pred_idx", "]", "<=", "threshold", ":", "\n", "            ", "score", "[", "pred_idx", "]", "=", "punishment", "\n", "", "repeat_fa", "=", "0", "\n", "for", "pred_word_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "            ", "if", "pred_word", "not", "in", "dict_trg", ":", "\n", "                ", "pass", "\n", "# #\u4e0d\u51fa\u73b0\u5728trg\u91cc\u7684\u91cd\u590d\u60e9\u7f5a", "\n", "# if pred_word in dict_pred:", "\n", "#     repeat_fa = 1", "\n", "#     break", "\n", "# else:", "\n", "#     dict_pred[pred_word] = 1", "\n", "", "else", ":", "\n", "                ", "if", "pred_word", "not", "in", "dict_pred", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "+=", "1", "\n", "if", "(", "dict_pred", "[", "pred_word", "]", ">", "dict_trg", "[", "pred_word", "]", ")", ":", "\n", "                        ", "repeat_fa", "=", "1", "\n", "break", "\n", "\n", "", "", "", "", "if", "repeat_fa", "==", "1", ":", "\n", "            ", "score", "[", "pred_idx", "]", "=", "punishment_repeat", "\n", "", "", "original_score", "=", "np", ".", "sum", "(", "score", ")", "\n", "return", "original_score", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward_add_coverage_descending": [[569, 641], ["len", "len", "numpy.zeros", "numpy.zeros", "enumerate", "enumerate", "sorted", "sorted.reverse", "enumerate", "numpy.sum", "enumerate", "enumerate", "sorted.append", "enumerate", "evaluate_prediction.compute_token_level_reward_edit_distance", "max"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_edit_distance"], ["", "def", "compute_phrase_level_reward_add_coverage_descending", "(", "trg_str_list", ",", "pred_str_list", ")", ":", "\n", "    ", "num_pred_str", "=", "len", "(", "pred_str_list", ")", "\n", "num_trg_str", "=", "len", "(", "trg_str_list", ")", "\n", "if", "num_pred_str", "==", "0", ":", "#no trg or no pred", "\n", "        ", "if", "num_trg_str", "==", "0", ":", "\n", "            ", "return", "0.5", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "", "", "score", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "score_final", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "threshold", "=", "0.1", "\n", "punishment", "=", "-", "0.05", "\n", "punishment_repeat", "=", "-", "0.3", "\n", "\n", "dict_trg", "=", "{", "}", "\n", "dict_pred", "=", "{", "}", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "        ", "for", "word_idx", ",", "trg_word", "in", "enumerate", "(", "trg_word_list", ")", ":", "\n", "            ", "if", "trg_word", "in", "dict_trg", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "=", "1", "\n", "", "", "", "pred_score_tuple", "=", "[", "]", "\n", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "        ", "max_score", "=", "0.0", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "# if token_reward_type == 1:", "\n", "            ", "cur_score", "=", "compute_token_level_reward_edit_distance", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "# elif token_reward_type == 2:", "\n", "#     cur_score = compute_token_level_reward_token_f1(trg_word_list, pred_word_list)", "\n", "# elif token_reward_type == 3:", "\n", "#     cur_score = compute_cp(trg_word_list, pred_word_list)", "\n", "\n", "max_score", "=", "max", "(", "max_score", ",", "cur_score", ")", "\n", "", "score", "[", "pred_idx", "]", "=", "max_score", "\n", "if", "score", "[", "pred_idx", "]", "<=", "threshold", ":", "\n", "            ", "score", "[", "pred_idx", "]", "=", "punishment", "\n", "", "pred_score_tuple", ".", "append", "(", "(", "pred_str_list", "[", "pred_idx", "]", ",", "score", "[", "pred_idx", "]", ")", ")", "\n", "#pred_score_tuple \u9006\u5e8f\u6392\u5e8f", "\n", "", "pred_score_tuple", "=", "sorted", "(", "pred_score_tuple", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "\n", "pred_score_tuple", ".", "reverse", "(", ")", "\n", "\n", "for", "tp_idx", ",", "tp", "in", "enumerate", "(", "pred_score_tuple", ")", ":", "\n", "        ", "pred_word_list", ",", "score_tmp", "=", "tp", "\n", "#print(pred_word_list)", "\n", "#print(score_tmp)", "\n", "repeat_fa", "=", "0", "\n", "for", "pred_word_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "            ", "if", "pred_word", "not", "in", "dict_trg", ":", "\n", "                ", "pass", "\n", "# #\u4e0d\u51fa\u73b0\u5728trg\u91cc\u7684\u91cd\u590d\u60e9\u7f5a", "\n", "# if pred_word in dict_pred:", "\n", "#     repeat_fa = 1", "\n", "#     break", "\n", "# else:", "\n", "#     dict_pred[pred_word] = 1", "\n", "", "else", ":", "\n", "                ", "if", "pred_word", "not", "in", "dict_pred", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "+=", "1", "\n", "if", "(", "dict_pred", "[", "pred_word", "]", ">", "dict_trg", "[", "pred_word", "]", ")", ":", "\n", "                        ", "repeat_fa", "=", "1", "\n", "break", "\n", "\n", "", "", "", "", "if", "repeat_fa", "==", "1", ":", "\n", "            ", "score_final", "[", "tp_idx", "]", "=", "punishment_repeat", "\n", "", "else", ":", "\n", "            ", "score_final", "[", "tp_idx", "]", "=", "score_tmp", "\n", "#print(score_final)", "\n", "", "", "original_score", "=", "np", ".", "sum", "(", "score_final", ")", "\n", "return", "original_score", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_fg_score_k": [[642, 810], ["len", "len", "numpy.zeros", "numpy.zeros", "enumerate", "enumerate", "sorted", "sorted.reverse", "enumerate", "enumerate", "enumerate", "sorted.append", "enumerate", "numpy.sum", "evaluate_prediction.compute_token_level_reward_edit_distance", "evaluate_prediction.compute_token_level_reward_token_f1", "evaluate_prediction.compute_token_level_token_f1", "enumerate", "max"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_edit_distance", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_token_f1", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_token_f1"], ["", "def", "compute_fg_score_k", "(", "trg_str_list", ",", "pred_str_list", ",", "eva", "=", "0", ")", ":", "\n", "\n", "    ", "num_pred_str", "=", "len", "(", "pred_str_list", ")", "\n", "num_trg_str", "=", "len", "(", "trg_str_list", ")", "\n", "tmp_stat_token_f1", "=", "[", "0", "]", "*", "12", "\n", "# no trg or no pred", "\n", "if", "num_pred_str", "==", "0", "and", "num_trg_str", "==", "0", ":", "\n", "        ", "if", "eva", "==", "1", ":", "\n", "            ", "return", "1.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "tmp_stat_token_f1", ",", "num_pred_str", "\n", "", "else", ":", "\n", "            ", "return", "0.5", "\n", "", "", "if", "num_pred_str", "==", "0", ":", "\n", "        ", "if", "eva", "==", "1", ":", "\n", "            ", "return", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "tmp_stat_token_f1", ",", "num_pred_str", "\n", "", "else", ":", "\n", "            ", "return", "0.0", "\n", "", "", "if", "num_trg_str", "==", "0", ":", "\n", "        ", "if", "eva", "==", "1", ":", "\n", "            ", "return", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "tmp_stat_token_f1", ",", "num_pred_str", "\n", "", "else", ":", "\n", "            ", "return", "0.0", "\n", "###", "\n", "# need_to_write = True", "\n", "# file_size = os.path.getsize('/remote-home/ygxu/workspace/KG/KGM/score_data.txt')", "\n", "# if need_to_write:", "\n", "#     f = open('/remote-home/ygxu/workspace/KG/KGM/score_data.txt', 'a+')", "\n", "# else:", "\n", "#     f = None", "\n", "###", "\n", "##", "\n", "# need_to_write = True", "\n", "# file_size = os.path.getsize('/remote-home/ygxu/workspace/KG/KGM/score_data_new.txt')", "\n", "# if need_to_write:", "\n", "#     f = open('/remote-home/ygxu/workspace/KG/KGM/score_data_new.txt', 'a+')", "\n", "# else:", "\n", "#     f = None", "\n", "##", "\n", "\n", "\n", "", "", "score", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "score_final", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "threshold", "=", "0.1", "\n", "#punishment = -0.05", "\n", "#punishment_repeat = -0.3", "\n", "\n", "dict_trg", "=", "{", "}", "\n", "dict_pred", "=", "{", "}", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "        ", "for", "word_idx", ",", "trg_word", "in", "enumerate", "(", "trg_word_list", ")", ":", "\n", "            ", "if", "trg_word", "in", "dict_trg", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "=", "1", "\n", "", "", "", "pred_score_tuple", "=", "[", "]", "\n", "if", "eva", "==", "1", ":", "\n", "        ", "total_tk_f1", "=", "0.0", "\n", "total_tk_p", "=", "0.0", "\n", "total_tk_r", "=", "0.0", "\n", "", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "        ", "max_score", "=", "0.0", "\n", "max_idx", "=", "0", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "# if token_reward_type == 1:", "\n", "            ", "cur_score_ed", "=", "compute_token_level_reward_edit_distance", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "# elif token_reward_type == 2:", "\n", "cur_score_tf", "=", "compute_token_level_reward_token_f1", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "cur_score", "=", "(", "cur_score_ed", "+", "cur_score_tf", ")", "/", "2.0", "\n", "if", "(", "cur_score", ">", "max_score", ")", ":", "\n", "                ", "max_score", "=", "cur_score", "\n", "max_idx", "=", "trg_idx", "\n", "", "", "score", "[", "pred_idx", "]", "=", "max_score", "\n", "if", "eva", "==", "1", ":", "\n", "            ", "tk_f1", ",", "tk_p", ",", "tk_r", "=", "compute_token_level_token_f1", "(", "trg_str_list", "[", "max_idx", "]", ",", "pred_word_list", ")", "\n", "total_tk_f1", "+=", "tk_f1", "\n", "total_tk_p", "+=", "tk_p", "\n", "total_tk_r", "+=", "tk_r", "\n", "if", "tk_f1", "==", "0.", ":", "\n", "                ", "tmp_stat_token_f1", "[", "0", "]", "+=", "1", "\n", "", "elif", "0", "<", "tk_f1", "<=", "0.1", ":", "\n", "                ", "tmp_stat_token_f1", "[", "1", "]", "+=", "1", "\n", "", "elif", "0", "<", "tk_f1", "<=", "0.2", ":", "\n", "                ", "tmp_stat_token_f1", "[", "2", "]", "+=", "1", "\n", "", "elif", "0", "<", "tk_f1", "<=", "0.3", ":", "\n", "                ", "tmp_stat_token_f1", "[", "3", "]", "+=", "1", "\n", "", "elif", "0", "<", "tk_f1", "<=", "0.4", ":", "\n", "                ", "tmp_stat_token_f1", "[", "4", "]", "+=", "1", "\n", "", "elif", "0", "<", "tk_f1", "<=", "0.5", ":", "\n", "                ", "tmp_stat_token_f1", "[", "5", "]", "+=", "1", "\n", "", "elif", "0", "<", "tk_f1", "<=", "0.6", ":", "\n", "                ", "tmp_stat_token_f1", "[", "6", "]", "+=", "1", "\n", "", "elif", "0", "<", "tk_f1", "<=", "0.7", ":", "\n", "                ", "tmp_stat_token_f1", "[", "7", "]", "+=", "1", "\n", "", "elif", "0", "<", "tk_f1", "<=", "0.8", ":", "\n", "                ", "tmp_stat_token_f1", "[", "8", "]", "+=", "1", "\n", "", "elif", "0", "<", "tk_f1", "<=", "0.9", ":", "\n", "                ", "tmp_stat_token_f1", "[", "9", "]", "+=", "1", "\n", "", "elif", "0", "<", "tk_f1", "<", "1.", ":", "\n", "                ", "tmp_stat_token_f1", "[", "10", "]", "+=", "1", "\n", "", "elif", "tk_f1", "==", "1.", ":", "\n", "                ", "tmp_stat_token_f1", "[", "11", "]", "+=", "1", "\n", "###", "\n", "# if f is not None:", "\n", "#     if file_size > 1024 **2 * 20:", "\n", "#         exit(0)", "\n", "#     strs = f'{\" \".join(trg_str_list[max_idx])}\\t{\" \".join(pred_word_list)}\\n'", "\n", "#     f.write(strs)", "\n", "###", "\n", "\n", "", "", "if", "score", "[", "pred_idx", "]", "<=", "threshold", ":", "\n", "            ", "score", "[", "pred_idx", "]", "=", "0.0", "\n", "", "pred_score_tuple", ".", "append", "(", "(", "pred_str_list", "[", "pred_idx", "]", ",", "score", "[", "pred_idx", "]", ")", ")", "\n", "\n", "###", "\n", "# if f is not None:", "\n", "#     f.close()", "\n", "###", "\n", "#pred_score_tuple \u9006\u5e8f\u6392\u5e8f", "\n", "", "pred_score_tuple", "=", "sorted", "(", "pred_score_tuple", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "\n", "pred_score_tuple", ".", "reverse", "(", ")", "\n", "\n", "for", "tp_idx", ",", "tp", "in", "enumerate", "(", "pred_score_tuple", ")", ":", "\n", "        ", "pred_word_list", ",", "score_tmp", "=", "tp", "\n", "#print(pred_word_list)", "\n", "#print(score_tmp)", "\n", "repeat_fa", "=", "0", "\n", "for", "pred_word_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "            ", "if", "pred_word", "not", "in", "dict_trg", ":", "\n", "                ", "pass", "\n", "# #\u4e0d\u51fa\u73b0\u5728trg\u91cc\u7684\u91cd\u590d\u60e9\u7f5a", "\n", "# if pred_word in dict_pred:", "\n", "#     repeat_fa = 1", "\n", "#     break", "\n", "# else:", "\n", "#     dict_pred[pred_word] = 1", "\n", "", "else", ":", "\n", "                ", "if", "pred_word", "not", "in", "dict_pred", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "+=", "1", "\n", "if", "(", "dict_pred", "[", "pred_word", "]", ">", "dict_trg", "[", "pred_word", "]", ")", ":", "\n", "                        ", "repeat_fa", "=", "1", "\n", "\n", "\n", "", "", "", "", "if", "repeat_fa", "==", "1", ":", "\n", "            ", "score_final", "[", "tp_idx", "]", "=", "0.0", "\n", "for", "pred_word_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "                ", "if", "pred_word", "in", "dict_trg", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "-=", "1", "\n", "", "", "", "else", ":", "\n", "            ", "score_final", "[", "tp_idx", "]", "=", "score_tmp", "\n", "#print(score_final)", "\n", "", "", "original_score", "=", "np", ".", "sum", "(", "score_final", ")", "/", "num_pred_str", "\n", "corr", "=", "1.0", "-", "(", "(", "num_trg_str", "-", "num_pred_str", ")", "**", "2", ")", "/", "(", "max", "(", "num_trg_str", ",", "num_pred_str", ",", "1", ")", "**", "2", ")", "\n", "###", "\n", "# token_f1_score = compute_token_level_reward_token_f1(trg_word_list, pred_word_list)", "\n", "# example = f'{\" \".join(trg_word_list)}\\t{\" \".join(pred_word_list)}\\t{cur_score}\\t{token_f1_score}\\n'", "\n", "# if f is not None and file_size < 1024 ** 3:", "\n", "#     f.write(example)", "\n", "###", "\n", "\n", "if", "eva", "==", "0", ":", "\n", "        ", "return", "original_score", "*", "corr", "\n", "", "else", ":", "\n", "# final_token_f1 = total_tk_f1 / max(num_pred_str, 1)", "\n", "# final_token_p = total_tk_p / max(num_pred_str, 1)", "\n", "# final_token_r = total_tk_r / max(num_pred_str, 1)", "\n", "# return original_score * corr, final_token_f1, final_token_p, final_token_r, tmp_stat_token_f1", "\n", "        ", "return", "original_score", "*", "corr", ",", "total_tk_f1", ",", "total_tk_p", ",", "total_tk_r", ",", "tmp_stat_token_f1", ",", "num_pred_str", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_fg_score_k1": [[811, 917], ["len", "len", "numpy.zeros", "numpy.zeros", "enumerate", "enumerate", "sorted", "sorted.reverse", "enumerate", "enumerate", "enumerate", "sorted.append", "enumerate", "numpy.sum", "evaluate_prediction.compute_token_level_reward_token_f1", "enumerate", "max"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_token_f1"], ["", "", "def", "compute_fg_score_k1", "(", "trg_str_list", ",", "pred_str_list", ",", "eva", "=", "0", ")", ":", "\n", "#delete edit distance", "\n", "    ", "num_pred_str", "=", "len", "(", "pred_str_list", ")", "\n", "num_trg_str", "=", "len", "(", "trg_str_list", ")", "\n", "# no trg or no pred", "\n", "if", "num_pred_str", "==", "0", "and", "num_trg_str", "==", "0", ":", "\n", "        ", "if", "eva", "==", "1", ":", "\n", "            ", "return", "1.0", "\n", "", "else", ":", "\n", "            ", "return", "0.5", "\n", "", "", "if", "num_pred_str", "==", "0", ":", "\n", "        ", "return", "0.0", "\n", "", "if", "num_trg_str", "==", "0", ":", "\n", "        ", "return", "0.0", "\n", "\n", "", "score", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "score_final", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "threshold", "=", "0.1", "\n", "#punishment = -0.05", "\n", "#punishment_repeat = -0.3", "\n", "\n", "dict_trg", "=", "{", "}", "\n", "dict_pred", "=", "{", "}", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "        ", "for", "word_idx", ",", "trg_word", "in", "enumerate", "(", "trg_word_list", ")", ":", "\n", "            ", "if", "trg_word", "in", "dict_trg", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "=", "1", "\n", "", "", "", "pred_score_tuple", "=", "[", "]", "\n", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "        ", "max_score", "=", "0.0", "\n", "max_idx", "=", "0", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "# if token_reward_type == 1:", "\n", "#cur_score_ed = compute_token_level_reward_edit_distance(trg_word_list, pred_word_list)", "\n", "# elif token_reward_type == 2:", "\n", "            ", "cur_score_tf", "=", "compute_token_level_reward_token_f1", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "#cur_score = (cur_score_ed + cur_score_tf) / 2.0", "\n", "cur_score", "=", "cur_score_tf", "\n", "if", "(", "cur_score", ">", "max_score", ")", ":", "\n", "                ", "max_score", "=", "cur_score", "\n", "max_idx", "=", "trg_idx", "\n", "", "", "score", "[", "pred_idx", "]", "=", "max_score", "\n", "###", "\n", "# if f is not None:", "\n", "#     if file_size > 1024 **2 * 20:", "\n", "#         exit(0)", "\n", "#     strs = f'{\" \".join(trg_str_list[max_idx])}\\t{\" \".join(pred_word_list)}\\n'", "\n", "#     f.write(strs)", "\n", "###", "\n", "\n", "if", "score", "[", "pred_idx", "]", "<=", "threshold", ":", "\n", "            ", "score", "[", "pred_idx", "]", "=", "0.0", "\n", "", "pred_score_tuple", ".", "append", "(", "(", "pred_str_list", "[", "pred_idx", "]", ",", "score", "[", "pred_idx", "]", ")", ")", "\n", "\n", "###", "\n", "# if f is not None:", "\n", "#     f.close()", "\n", "###", "\n", "#pred_score_tuple \u9006\u5e8f\u6392\u5e8f", "\n", "", "pred_score_tuple", "=", "sorted", "(", "pred_score_tuple", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "\n", "pred_score_tuple", ".", "reverse", "(", ")", "\n", "\n", "for", "tp_idx", ",", "tp", "in", "enumerate", "(", "pred_score_tuple", ")", ":", "\n", "        ", "pred_word_list", ",", "score_tmp", "=", "tp", "\n", "#print(pred_word_list)", "\n", "#print(score_tmp)", "\n", "repeat_fa", "=", "0", "\n", "for", "pred_word_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "            ", "if", "pred_word", "not", "in", "dict_trg", ":", "\n", "                ", "pass", "\n", "# #\u4e0d\u51fa\u73b0\u5728trg\u91cc\u7684\u91cd\u590d\u60e9\u7f5a", "\n", "# if pred_word in dict_pred:", "\n", "#     repeat_fa = 1", "\n", "#     break", "\n", "# else:", "\n", "#     dict_pred[pred_word] = 1", "\n", "", "else", ":", "\n", "                ", "if", "pred_word", "not", "in", "dict_pred", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "+=", "1", "\n", "if", "(", "dict_pred", "[", "pred_word", "]", ">", "dict_trg", "[", "pred_word", "]", ")", ":", "\n", "                        ", "repeat_fa", "=", "1", "\n", "\n", "\n", "", "", "", "", "if", "repeat_fa", "==", "1", ":", "\n", "            ", "score_final", "[", "tp_idx", "]", "=", "0.0", "\n", "for", "pred_word_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "                ", "if", "pred_word", "in", "dict_trg", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "-=", "1", "\n", "", "", "", "else", ":", "\n", "            ", "score_final", "[", "tp_idx", "]", "=", "score_tmp", "\n", "#print(score_final)", "\n", "", "", "original_score", "=", "np", ".", "sum", "(", "score_final", ")", "/", "num_pred_str", "\n", "corr", "=", "1.0", "-", "(", "(", "num_trg_str", "-", "num_pred_str", ")", "**", "2", ")", "/", "(", "max", "(", "num_trg_str", ",", "num_pred_str", ",", "1", ")", "**", "2", ")", "\n", "###", "\n", "# token_f1_score = compute_token_level_reward_token_f1(trg_word_list, pred_word_list)", "\n", "# example = f'{\" \".join(trg_word_list)}\\t{\" \".join(pred_word_list)}\\t{cur_score}\\t{token_f1_score}\\n'", "\n", "# if f is not None and file_size < 1024 ** 3:", "\n", "#     f.write(example)", "\n", "###", "\n", "\n", "\n", "return", "original_score", "*", "corr", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_fg_score_k2": [[918, 1018], ["len", "len", "numpy.zeros", "numpy.zeros", "enumerate", "enumerate", "sorted", "sorted.reverse", "enumerate", "enumerate", "enumerate", "sorted.append", "enumerate", "numpy.sum", "evaluate_prediction.compute_token_level_reward_edit_distance", "enumerate", "max"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_edit_distance"], ["", "def", "compute_fg_score_k2", "(", "trg_str_list", ",", "pred_str_list", ",", "eva", "=", "0", ")", ":", "\n", "#delete TF1", "\n", "    ", "num_pred_str", "=", "len", "(", "pred_str_list", ")", "\n", "num_trg_str", "=", "len", "(", "trg_str_list", ")", "\n", "# no trg or no pred", "\n", "if", "num_pred_str", "==", "0", "and", "num_trg_str", "==", "0", ":", "\n", "        ", "if", "eva", "==", "1", ":", "\n", "            ", "return", "1.0", "\n", "", "else", ":", "\n", "            ", "return", "0.5", "\n", "", "", "if", "num_pred_str", "==", "0", ":", "\n", "        ", "return", "0.0", "\n", "", "if", "num_trg_str", "==", "0", ":", "\n", "        ", "return", "0.0", "\n", "\n", "", "score", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "score_final", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "threshold", "=", "0.1", "\n", "#punishment = -0.05", "\n", "#punishment_repeat = -0.3", "\n", "\n", "dict_trg", "=", "{", "}", "\n", "dict_pred", "=", "{", "}", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "        ", "for", "word_idx", ",", "trg_word", "in", "enumerate", "(", "trg_word_list", ")", ":", "\n", "            ", "if", "trg_word", "in", "dict_trg", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "=", "1", "\n", "", "", "", "pred_score_tuple", "=", "[", "]", "\n", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "        ", "max_score", "=", "0.0", "\n", "max_idx", "=", "0", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "# if token_reward_type == 1:", "\n", "            ", "cur_score_ed", "=", "compute_token_level_reward_edit_distance", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "# elif token_reward_type == 2:", "\n", "#cur_score_tf = compute_token_level_reward_token_f1(trg_word_list, pred_word_list)", "\n", "#cur_score = (cur_score_ed + cur_score_tf) / 2.0", "\n", "cur_score", "=", "cur_score_ed", "\n", "if", "(", "cur_score", ">", "max_score", ")", ":", "\n", "                ", "max_score", "=", "cur_score", "\n", "max_idx", "=", "trg_idx", "\n", "", "", "score", "[", "pred_idx", "]", "=", "max_score", "\n", "\n", "\n", "if", "score", "[", "pred_idx", "]", "<=", "threshold", ":", "\n", "            ", "score", "[", "pred_idx", "]", "=", "0.0", "\n", "", "pred_score_tuple", ".", "append", "(", "(", "pred_str_list", "[", "pred_idx", "]", ",", "score", "[", "pred_idx", "]", ")", ")", "\n", "\n", "###", "\n", "# if f is not None:", "\n", "#     f.close()", "\n", "###", "\n", "#pred_score_tuple \u9006\u5e8f\u6392\u5e8f", "\n", "", "pred_score_tuple", "=", "sorted", "(", "pred_score_tuple", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "\n", "pred_score_tuple", ".", "reverse", "(", ")", "\n", "\n", "for", "tp_idx", ",", "tp", "in", "enumerate", "(", "pred_score_tuple", ")", ":", "\n", "        ", "pred_word_list", ",", "score_tmp", "=", "tp", "\n", "#print(pred_word_list)", "\n", "#print(score_tmp)", "\n", "repeat_fa", "=", "0", "\n", "for", "pred_word_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "            ", "if", "pred_word", "not", "in", "dict_trg", ":", "\n", "                ", "pass", "\n", "# #\u4e0d\u51fa\u73b0\u5728trg\u91cc\u7684\u91cd\u590d\u60e9\u7f5a", "\n", "# if pred_word in dict_pred:", "\n", "#     repeat_fa = 1", "\n", "#     break", "\n", "# else:", "\n", "#     dict_pred[pred_word] = 1", "\n", "", "else", ":", "\n", "                ", "if", "pred_word", "not", "in", "dict_pred", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "+=", "1", "\n", "if", "(", "dict_pred", "[", "pred_word", "]", ">", "dict_trg", "[", "pred_word", "]", ")", ":", "\n", "                        ", "repeat_fa", "=", "1", "\n", "\n", "\n", "", "", "", "", "if", "repeat_fa", "==", "1", ":", "\n", "            ", "score_final", "[", "tp_idx", "]", "=", "0.0", "\n", "for", "pred_word_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "                ", "if", "pred_word", "in", "dict_trg", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "-=", "1", "\n", "", "", "", "else", ":", "\n", "            ", "score_final", "[", "tp_idx", "]", "=", "score_tmp", "\n", "#print(score_final)", "\n", "", "", "original_score", "=", "np", ".", "sum", "(", "score_final", ")", "/", "num_pred_str", "\n", "corr", "=", "1.0", "-", "(", "(", "num_trg_str", "-", "num_pred_str", ")", "**", "2", ")", "/", "(", "max", "(", "num_trg_str", ",", "num_pred_str", ",", "1", ")", "**", "2", ")", "\n", "###", "\n", "# token_f1_score = compute_token_level_reward_token_f1(trg_word_list, pred_word_list)", "\n", "# example = f'{\" \".join(trg_word_list)}\\t{\" \".join(pred_word_list)}\\t{cur_score}\\t{token_f1_score}\\n'", "\n", "# if f is not None and file_size < 1024 ** 3:", "\n", "#     f.write(example)", "\n", "###", "\n", "\n", "\n", "return", "original_score", "*", "corr", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_fg_score_k3": [[1019, 1122], ["len", "len", "numpy.zeros", "numpy.zeros", "enumerate", "enumerate", "enumerate", "enumerate", "numpy.sum", "evaluate_prediction.compute_token_level_reward_edit_distance", "evaluate_prediction.compute_token_level_reward_token_f1", "max"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_edit_distance", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_token_f1"], ["", "def", "compute_fg_score_k3", "(", "trg_str_list", ",", "pred_str_list", ",", "eva", "=", "0", ")", ":", "\n", "#repetition", "\n", "    ", "num_pred_str", "=", "len", "(", "pred_str_list", ")", "\n", "num_trg_str", "=", "len", "(", "trg_str_list", ")", "\n", "# no trg or no pred", "\n", "if", "num_pred_str", "==", "0", "and", "num_trg_str", "==", "0", ":", "\n", "        ", "if", "eva", "==", "1", ":", "\n", "            ", "return", "1.0", "\n", "", "else", ":", "\n", "            ", "return", "0.5", "\n", "", "", "if", "num_pred_str", "==", "0", ":", "\n", "        ", "return", "0.0", "\n", "", "if", "num_trg_str", "==", "0", ":", "\n", "        ", "return", "0.0", "\n", "\n", "\n", "", "score", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "score_final", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "threshold", "=", "0.1", "\n", "#punishment = -0.05", "\n", "#punishment_repeat = -0.3", "\n", "\n", "dict_trg", "=", "{", "}", "\n", "dict_pred", "=", "{", "}", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "        ", "for", "word_idx", ",", "trg_word", "in", "enumerate", "(", "trg_word_list", ")", ":", "\n", "            ", "if", "trg_word", "in", "dict_trg", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "=", "1", "\n", "", "", "", "pred_score_tuple", "=", "[", "]", "\n", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "        ", "max_score", "=", "0.0", "\n", "max_idx", "=", "0", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "# if token_reward_type == 1:", "\n", "            ", "cur_score_ed", "=", "compute_token_level_reward_edit_distance", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "# elif token_reward_type == 2:", "\n", "cur_score_tf", "=", "compute_token_level_reward_token_f1", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "cur_score", "=", "(", "cur_score_ed", "+", "cur_score_tf", ")", "/", "2.0", "\n", "if", "(", "cur_score", ">", "max_score", ")", ":", "\n", "                ", "max_score", "=", "cur_score", "\n", "max_idx", "=", "trg_idx", "\n", "", "", "score", "[", "pred_idx", "]", "=", "max_score", "\n", "###", "\n", "# if f is not None:", "\n", "#     if file_size > 1024 **2 * 20:", "\n", "#         exit(0)", "\n", "#     strs = f'{\" \".join(trg_str_list[max_idx])}\\t{\" \".join(pred_word_list)}\\n'", "\n", "#     f.write(strs)", "\n", "###", "\n", "\n", "if", "score", "[", "pred_idx", "]", "<=", "threshold", ":", "\n", "            ", "score", "[", "pred_idx", "]", "=", "0.0", "\n", "#     pred_score_tuple.append((pred_str_list[pred_idx], score[pred_idx]))", "\n", "#", "\n", "# #pred_score_tuple \u9006\u5e8f\u6392\u5e8f", "\n", "# pred_score_tuple = sorted(pred_score_tuple, key=lambda x: x[1])", "\n", "# pred_score_tuple.reverse()", "\n", "#", "\n", "# for tp_idx, tp in enumerate(pred_score_tuple):", "\n", "#     pred_word_list, score_tmp = tp", "\n", "#     #print(pred_word_list)", "\n", "#     #print(score_tmp)", "\n", "#     repeat_fa = 0", "\n", "#     for pred_word_idx, pred_word in enumerate(pred_word_list):", "\n", "#         if pred_word not in dict_trg:", "\n", "#             pass", "\n", "#             # #\u4e0d\u51fa\u73b0\u5728trg\u91cc\u7684\u91cd\u590d\u60e9\u7f5a", "\n", "#             # if pred_word in dict_pred:", "\n", "#             #     repeat_fa = 1", "\n", "#             #     break", "\n", "#             # else:", "\n", "#             #     dict_pred[pred_word] = 1", "\n", "#         else:", "\n", "#             if pred_word not in dict_pred:", "\n", "#                 dict_pred[pred_word] = 1", "\n", "#             else:", "\n", "#                 dict_pred[pred_word] += 1", "\n", "#                 if(dict_pred[pred_word] > dict_trg[pred_word]):", "\n", "#                     repeat_fa = 1", "\n", "#", "\n", "#", "\n", "#     if repeat_fa == 1:", "\n", "#         score_final[tp_idx] = 0.0", "\n", "#         for pred_word_idx, pred_word in enumerate(pred_word_list):", "\n", "#             if pred_word in dict_trg:", "\n", "#                 dict_pred[pred_word] -= 1", "\n", "#     else:", "\n", "#         score_final[tp_idx] = score_tmp", "\n", "#print(score_final)", "\n", "", "", "score_final", "=", "score", "\n", "original_score", "=", "np", ".", "sum", "(", "score_final", ")", "/", "num_pred_str", "\n", "corr", "=", "1.0", "-", "(", "(", "num_trg_str", "-", "num_pred_str", ")", "**", "2", ")", "/", "(", "max", "(", "num_trg_str", ",", "num_pred_str", ",", "1", ")", "**", "2", ")", "\n", "###", "\n", "# token_f1_score = compute_token_level_reward_token_f1(trg_word_list, pred_word_list)", "\n", "# example = f'{\" \".join(trg_word_list)}\\t{\" \".join(pred_word_list)}\\t{cur_score}\\t{token_f1_score}\\n'", "\n", "# if f is not None and file_size < 1024 ** 3:", "\n", "#     f.write(example)", "\n", "###", "\n", "\n", "\n", "return", "original_score", "*", "corr", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_fg_score_k4": [[1123, 1245], ["len", "len", "numpy.zeros", "numpy.zeros", "enumerate", "enumerate", "sorted", "sorted.reverse", "enumerate", "enumerate", "enumerate", "sorted.append", "enumerate", "numpy.sum", "evaluate_prediction.compute_token_level_reward_edit_distance", "evaluate_prediction.compute_token_level_reward_token_f1", "enumerate"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_edit_distance", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_token_level_reward_token_f1"], ["", "def", "compute_fg_score_k4", "(", "trg_str_list", ",", "pred_str_list", ",", "eva", "=", "0", ")", ":", "\n", "\n", "    ", "num_pred_str", "=", "len", "(", "pred_str_list", ")", "\n", "num_trg_str", "=", "len", "(", "trg_str_list", ")", "\n", "# no trg or no pred", "\n", "if", "num_pred_str", "==", "0", "and", "num_trg_str", "==", "0", ":", "\n", "        ", "if", "eva", "==", "1", ":", "\n", "            ", "return", "1.0", "\n", "", "else", ":", "\n", "            ", "return", "0.5", "\n", "", "", "if", "num_pred_str", "==", "0", ":", "\n", "        ", "return", "0.0", "\n", "", "if", "num_trg_str", "==", "0", ":", "\n", "        ", "return", "0.0", "\n", "###", "\n", "# need_to_write = True", "\n", "# file_size = os.path.getsize('/remote-home/ygxu/workspace/KG/KGM/score_data.txt')", "\n", "# if need_to_write:", "\n", "#     f = open('/remote-home/ygxu/workspace/KG/KGM/score_data.txt', 'a+')", "\n", "# else:", "\n", "#     f = None", "\n", "###", "\n", "##", "\n", "# need_to_write = True", "\n", "# file_size = os.path.getsize('/remote-home/ygxu/workspace/KG/KGM/score_data_new.txt')", "\n", "# if need_to_write:", "\n", "#     f = open('/remote-home/ygxu/workspace/KG/KGM/score_data_new.txt', 'a+')", "\n", "# else:", "\n", "#     f = None", "\n", "##", "\n", "\n", "\n", "", "score", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "score_final", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "threshold", "=", "0.1", "\n", "#punishment = -0.05", "\n", "#punishment_repeat = -0.3", "\n", "\n", "dict_trg", "=", "{", "}", "\n", "dict_pred", "=", "{", "}", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "        ", "for", "word_idx", ",", "trg_word", "in", "enumerate", "(", "trg_word_list", ")", ":", "\n", "            ", "if", "trg_word", "in", "dict_trg", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "dict_trg", "[", "trg_word", "]", "=", "1", "\n", "", "", "", "pred_score_tuple", "=", "[", "]", "\n", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "        ", "max_score", "=", "0.0", "\n", "max_idx", "=", "0", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "# if token_reward_type == 1:", "\n", "            ", "cur_score_ed", "=", "compute_token_level_reward_edit_distance", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "# elif token_reward_type == 2:", "\n", "cur_score_tf", "=", "compute_token_level_reward_token_f1", "(", "trg_word_list", ",", "pred_word_list", ")", "\n", "cur_score", "=", "(", "cur_score_ed", "+", "cur_score_tf", ")", "/", "2.0", "\n", "if", "(", "cur_score", ">", "max_score", ")", ":", "\n", "                ", "max_score", "=", "cur_score", "\n", "max_idx", "=", "trg_idx", "\n", "", "", "score", "[", "pred_idx", "]", "=", "max_score", "\n", "###", "\n", "# if f is not None:", "\n", "#     if file_size > 1024 **2 * 20:", "\n", "#         exit(0)", "\n", "#     strs = f'{\" \".join(trg_str_list[max_idx])}\\t{\" \".join(pred_word_list)}\\n'", "\n", "#     f.write(strs)", "\n", "###", "\n", "\n", "if", "score", "[", "pred_idx", "]", "<=", "threshold", ":", "\n", "            ", "score", "[", "pred_idx", "]", "=", "0.0", "\n", "", "pred_score_tuple", ".", "append", "(", "(", "pred_str_list", "[", "pred_idx", "]", ",", "score", "[", "pred_idx", "]", ")", ")", "\n", "\n", "###", "\n", "# if f is not None:", "\n", "#     f.close()", "\n", "###", "\n", "#pred_score_tuple \u9006\u5e8f\u6392\u5e8f", "\n", "", "pred_score_tuple", "=", "sorted", "(", "pred_score_tuple", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "\n", "pred_score_tuple", ".", "reverse", "(", ")", "\n", "\n", "for", "tp_idx", ",", "tp", "in", "enumerate", "(", "pred_score_tuple", ")", ":", "\n", "        ", "pred_word_list", ",", "score_tmp", "=", "tp", "\n", "#print(pred_word_list)", "\n", "#print(score_tmp)", "\n", "repeat_fa", "=", "0", "\n", "for", "pred_word_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "            ", "if", "pred_word", "not", "in", "dict_trg", ":", "\n", "                ", "pass", "\n", "# #\u4e0d\u51fa\u73b0\u5728trg\u91cc\u7684\u91cd\u590d\u60e9\u7f5a", "\n", "# if pred_word in dict_pred:", "\n", "#     repeat_fa = 1", "\n", "#     break", "\n", "# else:", "\n", "#     dict_pred[pred_word] = 1", "\n", "", "else", ":", "\n", "                ", "if", "pred_word", "not", "in", "dict_pred", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "+=", "1", "\n", "if", "(", "dict_pred", "[", "pred_word", "]", ">", "dict_trg", "[", "pred_word", "]", ")", ":", "\n", "                        ", "repeat_fa", "=", "1", "\n", "\n", "\n", "", "", "", "", "if", "repeat_fa", "==", "1", ":", "\n", "            ", "score_final", "[", "tp_idx", "]", "=", "0.0", "\n", "for", "pred_word_idx", ",", "pred_word", "in", "enumerate", "(", "pred_word_list", ")", ":", "\n", "                ", "if", "pred_word", "in", "dict_trg", ":", "\n", "                    ", "dict_pred", "[", "pred_word", "]", "-=", "1", "\n", "", "", "", "else", ":", "\n", "            ", "score_final", "[", "tp_idx", "]", "=", "score_tmp", "\n", "#print(score_final)", "\n", "", "", "original_score", "=", "np", ".", "sum", "(", "score_final", ")", "/", "num_pred_str", "\n", "#corr = 1.0 - ((num_trg_str - num_pred_str) ** 2) / (max(num_trg_str, num_pred_str, 1) ** 2)", "\n", "###", "\n", "# token_f1_score = compute_token_level_reward_token_f1(trg_word_list, pred_word_list)", "\n", "# example = f'{\" \".join(trg_word_list)}\\t{\" \".join(pred_word_list)}\\t{cur_score}\\t{token_f1_score}\\n'", "\n", "# if f is not None and file_size < 1024 ** 3:", "\n", "#     f.write(example)", "\n", "###", "\n", "\n", "\n", "return", "original_score", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result": [[1246, 1278], ["len", "len", "numpy.zeros", "enumerate", "numpy.zeros", "enumerate", "enumerate", "enumerate"], "function", ["None"], ["", "def", "compute_match_result", "(", "trg_str_list", ",", "pred_str_list", ",", "type", "=", "'exact'", ",", "dimension", "=", "1", ")", ":", "\n", "    ", "assert", "type", "in", "[", "'exact'", ",", "'sub'", "]", ",", "\"Right now only support exact matching and substring matching\"", "\n", "assert", "dimension", "in", "[", "1", ",", "2", "]", ",", "\"only support 1 or 2\"", "\n", "num_pred_str", "=", "len", "(", "pred_str_list", ")", "\n", "num_trg_str", "=", "len", "(", "trg_str_list", ")", "\n", "if", "dimension", "==", "1", ":", "\n", "        ", "is_match", "=", "np", ".", "zeros", "(", "num_pred_str", ",", "dtype", "=", "bool", ")", "\n", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "            ", "joined_pred_word_list", "=", "' '", ".", "join", "(", "pred_word_list", ")", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "                ", "joined_trg_word_list", "=", "' '", ".", "join", "(", "trg_word_list", ")", "\n", "if", "type", "==", "'exact'", ":", "\n", "                    ", "if", "joined_pred_word_list", "==", "joined_trg_word_list", ":", "\n", "                        ", "is_match", "[", "pred_idx", "]", "=", "True", "\n", "break", "\n", "", "", "elif", "type", "==", "'sub'", ":", "\n", "                    ", "if", "joined_pred_word_list", "in", "joined_trg_word_list", ":", "\n", "                        ", "is_match", "[", "pred_idx", "]", "=", "True", "\n", "break", "\n", "", "", "", "", "", "else", ":", "\n", "        ", "is_match", "=", "np", ".", "zeros", "(", "(", "num_trg_str", ",", "num_pred_str", ")", ",", "dtype", "=", "bool", ")", "\n", "for", "trg_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "            ", "joined_trg_word_list", "=", "' '", ".", "join", "(", "trg_word_list", ")", "\n", "for", "pred_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "                ", "joined_pred_word_list", "=", "' '", ".", "join", "(", "pred_word_list", ")", "\n", "if", "type", "==", "'exact'", ":", "\n", "                    ", "if", "joined_pred_word_list", "==", "joined_trg_word_list", ":", "\n", "                        ", "is_match", "[", "trg_idx", "]", "[", "pred_idx", "]", "=", "True", "\n", "", "", "elif", "type", "==", "'sub'", ":", "\n", "                    ", "if", "joined_pred_word_list", "in", "joined_trg_word_list", ":", "\n", "                        ", "is_match", "[", "trg_idx", "]", "[", "pred_idx", "]", "=", "True", "\n", "", "", "", "", "", "return", "is_match", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.prepare_classification_result_dict": [[1280, 1285], ["None"], "function", ["None"], ["", "def", "prepare_classification_result_dict", "(", "precision_k", ",", "recall_k", ",", "f1_k", ",", "num_matches_k", ",", "num_predictions_k", ",", "num_targets_k", ",", "topk", ",", "is_present", ")", ":", "\n", "    ", "present_tag", "=", "\"present\"", "if", "is_present", "else", "\"absent\"", "\n", "return", "{", "'precision@%d_%s'", "%", "(", "topk", ",", "present_tag", ")", ":", "precision_k", ",", "'recall@%d_%s'", "%", "(", "topk", ",", "present_tag", ")", ":", "recall_k", ",", "\n", "'f1_score@%d_%s'", "%", "(", "topk", ",", "present_tag", ")", ":", "f1_k", ",", "'num_matches@%d_%s'", "%", "(", "topk", ",", "present_tag", ")", ":", "num_matches_k", ",", "\n", "'num_predictions@%d_%s'", "%", "(", "topk", ",", "present_tag", ")", ":", "num_predictions_k", ",", "'num_targets@%d_%s'", "%", "(", "topk", ",", "present_tag", ")", ":", "num_targets_k", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics_at_k": [[1287, 1321], ["sum", "evaluate_prediction.compute_classification_metrics"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics"], ["", "def", "compute_classification_metrics_at_k", "(", "is_match", ",", "num_predictions", ",", "num_trgs", ",", "topk", "=", "5", ",", "meng_rui_precision", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    :param is_match: a boolean np array with size [num_predictions]\n    :param predicted_list:\n    :param true_list:\n    :param topk:\n    :return: {'precision@%d' % topk: precision_k, 'recall@%d' % topk: recall_k, 'f1_score@%d' % topk: f1, 'num_matches@%d': num_matches}\n    \"\"\"", "\n", "assert", "is_match", ".", "shape", "[", "0", "]", "==", "num_predictions", "\n", "if", "topk", "==", "'M'", ":", "\n", "        ", "topk", "=", "num_predictions", "\n", "", "elif", "topk", "==", "'G'", ":", "\n", "#topk = num_trgs", "\n", "        ", "if", "num_predictions", "<", "num_trgs", ":", "\n", "            ", "topk", "=", "num_trgs", "\n", "", "else", ":", "\n", "            ", "topk", "=", "num_predictions", "\n", "\n", "", "", "if", "meng_rui_precision", ":", "\n", "        ", "if", "num_predictions", ">", "topk", ":", "\n", "            ", "is_match", "=", "is_match", "[", ":", "topk", "]", "\n", "num_predictions_k", "=", "topk", "\n", "", "else", ":", "\n", "            ", "num_predictions_k", "=", "num_predictions", "\n", "", "", "else", ":", "\n", "        ", "if", "num_predictions", ">", "topk", ":", "\n", "            ", "is_match", "=", "is_match", "[", ":", "topk", "]", "\n", "", "num_predictions_k", "=", "topk", "\n", "\n", "", "num_matches_k", "=", "sum", "(", "is_match", ")", "\n", "\n", "precision_k", ",", "recall_k", ",", "f1_k", "=", "compute_classification_metrics", "(", "num_matches_k", ",", "num_predictions_k", ",", "num_trgs", ")", "\n", "\n", "return", "precision_k", ",", "recall_k", ",", "f1_k", ",", "num_matches_k", ",", "num_predictions_k", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics_at_ks": [[1323, 1377], ["numpy.cumsum", "len", "len", "len", "len", "len", "evaluate_prediction.compute_classification_metrics", "precision_ks.append", "recall_ks.append", "f1_ks.append", "num_matches_ks.append", "num_predictions_ks.append"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics"], ["", "def", "compute_classification_metrics_at_ks", "(", "is_match", ",", "num_predictions", ",", "num_trgs", ",", "k_list", "=", "[", "5", ",", "10", "]", ",", "meng_rui_precision", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    :param is_match: a boolean np array with size [num_predictions]\n    :param predicted_list:\n    :param true_list:\n    :param topk:\n    :return: {'precision@%d' % topk: precision_k, 'recall@%d' % topk: recall_k, 'f1_score@%d' % topk: f1, 'num_matches@%d': num_matches}\n    \"\"\"", "\n", "assert", "is_match", ".", "shape", "[", "0", "]", "==", "num_predictions", "\n", "#topk.sort()", "\n", "if", "num_predictions", "==", "0", ":", "\n", "        ", "precision_ks", "=", "[", "0", "]", "*", "len", "(", "k_list", ")", "\n", "recall_ks", "=", "[", "0", "]", "*", "len", "(", "k_list", ")", "\n", "f1_ks", "=", "[", "0", "]", "*", "len", "(", "k_list", ")", "\n", "num_matches_ks", "=", "[", "0", "]", "*", "len", "(", "k_list", ")", "\n", "num_predictions_ks", "=", "[", "0", "]", "*", "len", "(", "k_list", ")", "\n", "", "else", ":", "\n", "        ", "num_matches", "=", "np", ".", "cumsum", "(", "is_match", ")", "\n", "num_predictions_ks", "=", "[", "]", "\n", "num_matches_ks", "=", "[", "]", "\n", "precision_ks", "=", "[", "]", "\n", "recall_ks", "=", "[", "]", "\n", "f1_ks", "=", "[", "]", "\n", "for", "topk", "in", "k_list", ":", "\n", "            ", "if", "topk", "==", "'M'", ":", "\n", "                ", "topk", "=", "num_predictions", "\n", "", "elif", "topk", "==", "'G'", ":", "\n", "#topk = num_trgs", "\n", "                ", "if", "num_predictions", "<", "num_trgs", ":", "\n", "                    ", "topk", "=", "num_trgs", "\n", "", "else", ":", "\n", "                    ", "topk", "=", "num_predictions", "\n", "\n", "", "", "if", "meng_rui_precision", ":", "\n", "                ", "if", "num_predictions", ">", "topk", ":", "\n", "                    ", "num_matches_at_k", "=", "num_matches", "[", "topk", "-", "1", "]", "\n", "num_predictions_at_k", "=", "topk", "\n", "", "else", ":", "\n", "                    ", "num_matches_at_k", "=", "num_matches", "[", "-", "1", "]", "\n", "num_predictions_at_k", "=", "num_predictions", "\n", "", "", "else", ":", "\n", "                ", "if", "num_predictions", ">", "topk", ":", "\n", "                    ", "num_matches_at_k", "=", "num_matches", "[", "topk", "-", "1", "]", "\n", "", "else", ":", "\n", "                    ", "num_matches_at_k", "=", "num_matches", "[", "-", "1", "]", "\n", "", "num_predictions_at_k", "=", "topk", "\n", "\n", "", "precision_k", ",", "recall_k", ",", "f1_k", "=", "compute_classification_metrics", "(", "num_matches_at_k", ",", "num_predictions_at_k", ",", "num_trgs", ")", "\n", "precision_ks", ".", "append", "(", "precision_k", ")", "\n", "recall_ks", ".", "append", "(", "recall_k", ")", "\n", "f1_ks", ".", "append", "(", "f1_k", ")", "\n", "num_matches_ks", ".", "append", "(", "num_matches_at_k", ")", "\n", "num_predictions_ks", ".", "append", "(", "num_predictions_at_k", ")", "\n", "", "", "return", "precision_ks", ",", "recall_ks", ",", "f1_ks", ",", "num_matches_ks", ",", "num_predictions_ks", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics": [[1379, 1384], ["evaluate_prediction.compute_precision", "evaluate_prediction.compute_recall", "evaluate_prediction.compute_f1"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_precision", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_recall", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_f1"], ["", "def", "compute_classification_metrics", "(", "num_matches", ",", "num_predictions", ",", "num_trgs", ")", ":", "\n", "    ", "precision", "=", "compute_precision", "(", "num_matches", ",", "num_predictions", ")", "\n", "recall", "=", "compute_recall", "(", "num_matches", ",", "num_trgs", ")", "\n", "f1", "=", "compute_f1", "(", "precision", ",", "recall", ")", "\n", "return", "precision", ",", "recall", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_precision": [[1386, 1388], ["None"], "function", ["None"], ["", "def", "compute_precision", "(", "num_matches", ",", "num_predictions", ")", ":", "\n", "    ", "return", "num_matches", "/", "num_predictions", "if", "num_predictions", ">", "0", "else", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_recall": [[1390, 1392], ["None"], "function", ["None"], ["", "def", "compute_recall", "(", "num_matches", ",", "num_trgs", ")", ":", "\n", "    ", "return", "num_matches", "/", "num_trgs", "if", "num_trgs", ">", "0", "else", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_f1": [[1394, 1396], ["float"], "function", ["None"], ["", "def", "compute_f1", "(", "precision", ",", "recall", ")", ":", "\n", "    ", "return", "float", "(", "2", "*", "(", "precision", "*", "recall", ")", ")", "/", "(", "precision", "+", "recall", ")", "if", "precision", "+", "recall", ">", "0", "else", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.dcg_at_k": [[1398, 1439], ["numpy.sum", "numpy.sum", "ValueError", "numpy.log2", "numpy.log2", "numpy.arange", "numpy.arange"], "function", ["None"], ["", "def", "dcg_at_k", "(", "r", ",", "k", ",", "num_trgs", ",", "method", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    Reference from https://www.kaggle.com/wendykan/ndcg-example and https://gist.github.com/bwhite/3726239\n    Score is discounted cumulative gain (dcg)\n    Relevance is positive real values.  Can use binary\n    as the previous methods.\n    Example from\n    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n    Args:\n        r: Relevance scores (list or numpy) in rank order\n            (first element is the first item)\n        k: Number of results to consider\n        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n    Returns:\n        Discounted cumulative gain\n    \"\"\"", "\n", "num_predictions", "=", "r", ".", "shape", "[", "0", "]", "\n", "if", "k", "==", "'M'", ":", "\n", "        ", "k", "=", "num_predictions", "\n", "", "elif", "k", "==", "'G'", ":", "\n", "#k = num_trgs", "\n", "        ", "if", "num_predictions", "<", "num_trgs", ":", "\n", "            ", "k", "=", "num_trgs", "\n", "", "else", ":", "\n", "            ", "k", "=", "num_predictions", "\n", "\n", "", "", "if", "num_predictions", "==", "0", ":", "\n", "        ", "dcg", "=", "0.", "\n", "", "else", ":", "\n", "        ", "if", "num_predictions", ">", "k", ":", "\n", "            ", "r", "=", "r", "[", ":", "k", "]", "\n", "num_predictions", "=", "k", "\n", "", "if", "method", "==", "0", ":", "\n", "            ", "dcg", "=", "r", "[", "0", "]", "+", "np", ".", "sum", "(", "r", "[", "1", ":", "]", "/", "np", ".", "log2", "(", "np", ".", "arange", "(", "2", ",", "r", ".", "size", "+", "1", ")", ")", ")", "\n", "", "elif", "method", "==", "1", ":", "\n", "            ", "discounted_gain", "=", "r", "/", "np", ".", "log2", "(", "np", ".", "arange", "(", "2", ",", "r", ".", "size", "+", "2", ")", ")", "\n", "dcg", "=", "np", ".", "sum", "(", "discounted_gain", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'method must be 0 or 1.'", ")", "\n", "", "", "return", "dcg", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.dcg_at_ks": [[1441, 1482], ["numpy.array", "numpy.cumsum", "numpy.array", "ValueError", "len", "numpy.log2", "np.array.append", "numpy.arange"], "function", ["None"], ["", "def", "dcg_at_ks", "(", "r", ",", "k_list", ",", "num_trgs", ",", "method", "=", "1", ")", ":", "\n", "    ", "num_predictions", "=", "r", ".", "shape", "[", "0", "]", "\n", "if", "num_predictions", "==", "0", ":", "\n", "        ", "dcg_array", "=", "np", ".", "array", "(", "[", "0", "]", "*", "len", "(", "k_list", ")", ")", "\n", "", "else", ":", "\n", "        ", "k_max", "=", "-", "1", "\n", "for", "k", "in", "k_list", ":", "\n", "            ", "if", "k", "==", "'M'", ":", "\n", "                ", "k", "=", "num_predictions", "\n", "", "elif", "k", "==", "'G'", ":", "\n", "#k = num_trgs", "\n", "                ", "if", "num_predictions", "<", "num_trgs", ":", "\n", "                    ", "k", "=", "num_trgs", "\n", "", "else", ":", "\n", "                    ", "k", "=", "num_predictions", "\n", "\n", "", "", "if", "k", ">", "k_max", ":", "\n", "                ", "k_max", "=", "k", "\n", "", "", "if", "num_predictions", ">", "k_max", ":", "\n", "            ", "r", "=", "r", "[", ":", "k_max", "]", "\n", "num_predictions", "=", "k_max", "\n", "", "if", "method", "==", "1", ":", "\n", "            ", "discounted_gain", "=", "r", "/", "np", ".", "log2", "(", "np", ".", "arange", "(", "2", ",", "r", ".", "size", "+", "2", ")", ")", "\n", "dcg", "=", "np", ".", "cumsum", "(", "discounted_gain", ")", "\n", "return_indices", "=", "[", "]", "\n", "for", "k", "in", "k_list", ":", "\n", "                ", "if", "k", "==", "'M'", ":", "\n", "                    ", "k", "=", "num_predictions", "\n", "", "elif", "k", "==", "'G'", ":", "\n", "#k = num_trgs", "\n", "                    ", "if", "num_predictions", "<", "num_trgs", ":", "\n", "                        ", "k", "=", "num_trgs", "\n", "", "else", ":", "\n", "                        ", "k", "=", "num_predictions", "\n", "\n", "", "", "return_indices", ".", "append", "(", "(", "k", "-", "1", ")", "if", "k", "<=", "num_predictions", "else", "(", "num_predictions", "-", "1", ")", ")", "\n", "", "return_indices", "=", "np", ".", "array", "(", "return_indices", ",", "dtype", "=", "int", ")", "\n", "dcg_array", "=", "dcg", "[", "return_indices", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'method must 1.'", ")", "\n", "", "", "return", "dcg_array", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.ndcg_at_k": [[1484, 1513], ["evaluate_prediction.dcg_at_k", "numpy.array", "evaluate_prediction.dcg_at_k", "sorted"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.dcg_at_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.dcg_at_k"], ["", "def", "ndcg_at_k", "(", "r", ",", "k", ",", "num_trgs", ",", "method", "=", "1", ",", "include_dcg", "=", "False", ")", ":", "\n", "    ", "\"\"\"Score is normalized discounted cumulative gain (ndcg)\n    Relevance is positive real values.  Can use binary\n    as the previous methods.\n    Example from\n    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n    Args:\n        r: Relevance scores (list or numpy) in rank order\n            (first element is the first item)\n        k: Number of results to consider\n        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n    Returns:\n        Normalized discounted cumulative gain\n    \"\"\"", "\n", "if", "r", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "        ", "ndcg", "=", "0.0", "\n", "dcg", "=", "0.0", "\n", "", "else", ":", "\n", "        ", "dcg_max", "=", "dcg_at_k", "(", "np", ".", "array", "(", "sorted", "(", "r", ",", "reverse", "=", "True", ")", ")", ",", "k", ",", "num_trgs", ",", "method", ")", "\n", "if", "dcg_max", "<=", "0.0", ":", "\n", "            ", "ndcg", "=", "0.0", "\n", "", "else", ":", "\n", "            ", "dcg", "=", "dcg_at_k", "(", "r", ",", "k", ",", "num_trgs", ",", "method", ")", "\n", "ndcg", "=", "dcg", "/", "dcg_max", "\n", "", "", "if", "include_dcg", ":", "\n", "        ", "return", "ndcg", ",", "dcg", "\n", "", "else", ":", "\n", "        ", "return", "ndcg", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.ndcg_at_ks": [[1515, 1529], ["evaluate_prediction.dcg_at_ks", "numpy.array", "evaluate_prediction.dcg_at_ks", "numpy.nan_to_num", "len", "len", "sorted"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.dcg_at_ks", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.dcg_at_ks"], ["", "", "def", "ndcg_at_ks", "(", "r", ",", "k_list", ",", "num_trgs", ",", "method", "=", "1", ",", "include_dcg", "=", "False", ")", ":", "\n", "    ", "if", "r", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "        ", "ndcg_array", "=", "[", "0.0", "]", "*", "len", "(", "k_list", ")", "\n", "dcg_array", "=", "[", "0.0", "]", "*", "len", "(", "k_list", ")", "\n", "", "else", ":", "\n", "        ", "dcg_array", "=", "dcg_at_ks", "(", "r", ",", "k_list", ",", "num_trgs", ",", "method", ")", "\n", "ideal_r", "=", "np", ".", "array", "(", "sorted", "(", "r", ",", "reverse", "=", "True", ")", ")", "\n", "dcg_max_array", "=", "dcg_at_ks", "(", "ideal_r", ",", "k_list", ",", "num_trgs", ",", "method", ")", "\n", "ndcg_array", "=", "dcg_array", "/", "dcg_max_array", "\n", "ndcg_array", "=", "np", ".", "nan_to_num", "(", "ndcg_array", ")", "\n", "", "if", "include_dcg", ":", "\n", "        ", "return", "ndcg_array", ",", "dcg_array", "\n", "", "else", ":", "\n", "        ", "return", "ndcg_array", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.alpha_dcg_at_k": [[1531, 1561], ["numpy.zeros", "numpy.concatenate", "range", "evaluate_prediction.dcg_at_k", "numpy.ones", "numpy.dot", "numpy.zeros", "numpy.cumsum", "numpy.power"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.dcg_at_k"], ["", "", "def", "alpha_dcg_at_k", "(", "r_2d", ",", "k", ",", "method", "=", "1", ",", "alpha", "=", "0.5", ")", ":", "\n", "    ", "\"\"\"\n    :param r_2d: 2d relevance np array, shape: [num_trg_str, num_pred_str]\n    :param k:\n    :param method:\n    :param alpha:\n    :return:\n    \"\"\"", "\n", "if", "r_2d", ".", "shape", "[", "-", "1", "]", "==", "0", ":", "\n", "        ", "alpha_dcg", "=", "0.0", "\n", "", "else", ":", "\n", "# convert r_2d to gain vector", "\n", "        ", "num_trg_str", ",", "num_pred_str", "=", "r_2d", ".", "shape", "\n", "if", "k", "==", "'M'", ":", "\n", "            ", "k", "=", "num_pred_str", "\n", "", "elif", "k", "==", "'G'", ":", "\n", "#k = num_trg_str", "\n", "            ", "if", "num_pred_str", "<", "num_trg_str", ":", "\n", "                ", "k", "=", "num_trg_str", "\n", "", "else", ":", "\n", "                ", "k", "=", "num_pred_str", "\n", "", "", "if", "num_pred_str", ">", "k", ":", "\n", "            ", "num_pred_str", "=", "k", "\n", "", "gain_vector", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "one_minus_alpha_vec", "=", "np", ".", "ones", "(", "num_trg_str", ")", "*", "(", "1", "-", "alpha", ")", "# [num_trg_str]", "\n", "cum_r", "=", "np", ".", "concatenate", "(", "(", "np", ".", "zeros", "(", "(", "num_trg_str", ",", "1", ")", ")", ",", "np", ".", "cumsum", "(", "r_2d", ",", "axis", "=", "1", ")", ")", ",", "axis", "=", "1", ")", "\n", "for", "j", "in", "range", "(", "num_pred_str", ")", ":", "\n", "            ", "gain_vector", "[", "j", "]", "=", "np", ".", "dot", "(", "r_2d", "[", ":", ",", "j", "]", ",", "np", ".", "power", "(", "one_minus_alpha_vec", ",", "cum_r", "[", ":", ",", "j", "]", ")", ")", "\n", "", "alpha_dcg", "=", "dcg_at_k", "(", "gain_vector", ",", "k", ",", "num_trg_str", ",", "method", ")", "\n", "", "return", "alpha_dcg", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.alpha_dcg_at_ks": [[1563, 1597], ["numpy.zeros", "numpy.concatenate", "range", "evaluate_prediction.dcg_at_ks", "numpy.ones", "numpy.dot", "len", "numpy.zeros", "numpy.cumsum", "numpy.power"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.dcg_at_ks"], ["", "def", "alpha_dcg_at_ks", "(", "r_2d", ",", "k_list", ",", "method", "=", "1", ",", "alpha", "=", "0.5", ")", ":", "\n", "    ", "\"\"\"\n    :param r_2d: 2d relevance np array, shape: [num_trg_str, num_pred_str]\n    :param ks:\n    :param method:\n    :param alpha:\n    :return:\n    \"\"\"", "\n", "if", "r_2d", ".", "shape", "[", "-", "1", "]", "==", "0", ":", "\n", "        ", "return", "[", "0.0", "]", "*", "len", "(", "k_list", ")", "\n", "# convert r_2d to gain vector", "\n", "", "num_trg_str", ",", "num_pred_str", "=", "r_2d", ".", "shape", "\n", "# k_max = max(k_list)", "\n", "k_max", "=", "-", "1", "\n", "for", "k", "in", "k_list", ":", "\n", "        ", "if", "k", "==", "'M'", ":", "\n", "            ", "k", "=", "num_pred_str", "\n", "", "elif", "k", "==", "'G'", ":", "\n", "#k = num_trg_str", "\n", "            ", "if", "num_pred_str", "<", "num_trg_str", ":", "\n", "                ", "k", "=", "num_trg_str", "\n", "", "else", ":", "\n", "                ", "k", "=", "num_pred_str", "\n", "\n", "", "", "if", "k", ">", "k_max", ":", "\n", "            ", "k_max", "=", "k", "\n", "", "", "if", "num_pred_str", ">", "k_max", ":", "\n", "        ", "num_pred_str", "=", "k_max", "\n", "", "gain_vector", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "one_minus_alpha_vec", "=", "np", ".", "ones", "(", "num_trg_str", ")", "*", "(", "1", "-", "alpha", ")", "# [num_trg_str]", "\n", "cum_r", "=", "np", ".", "concatenate", "(", "(", "np", ".", "zeros", "(", "(", "num_trg_str", ",", "1", ")", ")", ",", "np", ".", "cumsum", "(", "r_2d", ",", "axis", "=", "1", ")", ")", ",", "axis", "=", "1", ")", "\n", "for", "j", "in", "range", "(", "num_pred_str", ")", ":", "\n", "        ", "gain_vector", "[", "j", "]", "=", "np", ".", "dot", "(", "r_2d", "[", ":", ",", "j", "]", ",", "np", ".", "power", "(", "one_minus_alpha_vec", ",", "cum_r", "[", ":", ",", "j", "]", ")", ")", "\n", "", "return", "dcg_at_ks", "(", "gain_vector", ",", "k_list", ",", "num_trg_str", ",", "method", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.alpha_ndcg_at_k": [[1599, 1634], ["evaluate_prediction.alpha_dcg_at_k", "evaluate_prediction.compute_ideal_r_2d", "evaluate_prediction.alpha_dcg_at_k", "numpy.nan_to_num"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.alpha_dcg_at_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_ideal_r_2d", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.alpha_dcg_at_k"], ["", "def", "alpha_ndcg_at_k", "(", "r_2d", ",", "k", ",", "method", "=", "1", ",", "alpha", "=", "0.5", ",", "include_dcg", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    :param r_2d: 2d relevance np array, shape: [num_trg_str, num_pred_str]\n    :param k:\n    :param method:\n    :param alpha:\n    :return:\n    \"\"\"", "\n", "if", "r_2d", ".", "shape", "[", "-", "1", "]", "==", "0", ":", "\n", "        ", "alpha_ndcg", "=", "0.0", "\n", "alpha_dcg", "=", "0.0", "\n", "", "else", ":", "\n", "        ", "num_trg_str", ",", "num_pred_str", "=", "r_2d", ".", "shape", "\n", "if", "k", "==", "'M'", ":", "\n", "            ", "k", "=", "num_pred_str", "\n", "", "elif", "k", "==", "'G'", ":", "\n", "#k = num_trg_str", "\n", "            ", "if", "num_pred_str", "<", "num_trg_str", ":", "\n", "                ", "k", "=", "num_trg_str", "\n", "", "else", ":", "\n", "                ", "k", "=", "num_pred_str", "\n", "# convert r to gain vector", "\n", "", "", "alpha_dcg", "=", "alpha_dcg_at_k", "(", "r_2d", ",", "k", ",", "method", ",", "alpha", ")", "\n", "# compute alpha_dcg_max", "\n", "r_2d_ideal", "=", "compute_ideal_r_2d", "(", "r_2d", ",", "k", ",", "alpha", ")", "\n", "alpha_dcg_max", "=", "alpha_dcg_at_k", "(", "r_2d_ideal", ",", "k", ",", "method", ",", "alpha", ")", "\n", "if", "alpha_dcg_max", "<=", "0.0", ":", "\n", "            ", "alpha_ndcg", "=", "0.0", "\n", "", "else", ":", "\n", "            ", "alpha_ndcg", "=", "alpha_dcg", "/", "alpha_dcg_max", "\n", "alpha_ndcg", "=", "np", ".", "nan_to_num", "(", "alpha_ndcg", ")", "\n", "", "", "if", "include_dcg", ":", "\n", "        ", "return", "alpha_ndcg", ",", "alpha_dcg", "\n", "", "else", ":", "\n", "        ", "return", "alpha_ndcg", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.alpha_ndcg_at_ks": [[1636, 1674], ["evaluate_prediction.alpha_dcg_at_ks", "evaluate_prediction.compute_ideal_r_2d", "evaluate_prediction.alpha_dcg_at_ks", "numpy.nan_to_num", "len", "len"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.alpha_dcg_at_ks", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_ideal_r_2d", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.alpha_dcg_at_ks"], ["", "", "def", "alpha_ndcg_at_ks", "(", "r_2d", ",", "k_list", ",", "method", "=", "1", ",", "alpha", "=", "0.5", ",", "include_dcg", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    :param r_2d: 2d relevance np array, shape: [num_trg_str, num_pred_str]\n    :param k:\n    :param method:\n    :param alpha:\n    :return:\n    \"\"\"", "\n", "if", "r_2d", ".", "shape", "[", "-", "1", "]", "==", "0", ":", "\n", "        ", "alpha_ndcg_array", "=", "[", "0", "]", "*", "len", "(", "k_list", ")", "\n", "alpha_dcg_array", "=", "[", "0", "]", "*", "len", "(", "k_list", ")", "\n", "", "else", ":", "\n", "# k_max = max(k_list)", "\n", "        ", "num_trg_str", ",", "num_pred_str", "=", "r_2d", ".", "shape", "\n", "k_max", "=", "-", "1", "\n", "for", "k", "in", "k_list", ":", "\n", "            ", "if", "k", "==", "'M'", ":", "\n", "                ", "k", "=", "num_pred_str", "\n", "", "elif", "k", "==", "'G'", ":", "\n", "#k = num_trg_str", "\n", "                ", "if", "num_pred_str", "<", "num_trg_str", ":", "\n", "                    ", "k", "=", "num_trg_str", "\n", "", "else", ":", "\n", "                    ", "k", "=", "num_pred_str", "\n", "\n", "", "", "if", "k", ">", "k_max", ":", "\n", "                ", "k_max", "=", "k", "\n", "# convert r to gain vector", "\n", "", "", "alpha_dcg_array", "=", "alpha_dcg_at_ks", "(", "r_2d", ",", "k_list", ",", "method", ",", "alpha", ")", "\n", "# compute alpha_dcg_max", "\n", "r_2d_ideal", "=", "compute_ideal_r_2d", "(", "r_2d", ",", "k_max", ",", "alpha", ")", "\n", "alpha_dcg_max_array", "=", "alpha_dcg_at_ks", "(", "r_2d_ideal", ",", "k_list", ",", "method", ",", "alpha", ")", "\n", "alpha_ndcg_array", "=", "alpha_dcg_array", "/", "alpha_dcg_max_array", "\n", "alpha_ndcg_array", "=", "np", ".", "nan_to_num", "(", "alpha_ndcg_array", ")", "\n", "", "if", "include_dcg", ":", "\n", "        ", "return", "alpha_ndcg_array", ",", "alpha_dcg_array", "\n", "", "else", ":", "\n", "        ", "return", "alpha_ndcg_array", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_ideal_r_2d": [[1676, 1694], ["numpy.zeros", "min", "range", "numpy.ones", "numpy.zeros", "range", "numpy.argmax", "ideal_ranking.append", "numpy.dot", "numpy.array", "numpy.power"], "function", ["None"], ["", "", "def", "compute_ideal_r_2d", "(", "r_2d", ",", "k", ",", "alpha", "=", "0.5", ")", ":", "\n", "    ", "num_trg_str", ",", "num_pred_str", "=", "r_2d", ".", "shape", "\n", "one_minus_alpha_vec", "=", "np", ".", "ones", "(", "num_trg_str", ")", "*", "(", "1", "-", "alpha", ")", "# [num_trg_str]", "\n", "cum_r_vector", "=", "np", ".", "zeros", "(", "(", "num_trg_str", ")", ")", "\n", "ideal_ranking", "=", "[", "]", "\n", "greedy_depth", "=", "min", "(", "num_pred_str", ",", "k", ")", "\n", "for", "rank", "in", "range", "(", "greedy_depth", ")", ":", "\n", "        ", "gain_vector", "=", "np", ".", "zeros", "(", "num_pred_str", ")", "\n", "for", "j", "in", "range", "(", "num_pred_str", ")", ":", "\n", "            ", "if", "j", "in", "ideal_ranking", ":", "\n", "                ", "gain_vector", "[", "j", "]", "=", "-", "1000.0", "\n", "", "else", ":", "\n", "                ", "gain_vector", "[", "j", "]", "=", "np", ".", "dot", "(", "r_2d", "[", ":", ",", "j", "]", ",", "np", ".", "power", "(", "one_minus_alpha_vec", ",", "cum_r_vector", ")", ")", "\n", "", "", "max_idx", "=", "np", ".", "argmax", "(", "gain_vector", ")", "\n", "ideal_ranking", ".", "append", "(", "max_idx", ")", "\n", "current_relevance_vector", "=", "r_2d", "[", ":", ",", "max_idx", "]", "\n", "cum_r_vector", "=", "cum_r_vector", "+", "current_relevance_vector", "\n", "", "return", "r_2d", "[", ":", ",", "np", ".", "array", "(", "ideal_ranking", ",", "dtype", "=", "int", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.average_precision": [[1696, 1711], ["numpy.cumsum", "sum", "evaluate_prediction.compute_precision", "range"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_precision"], ["", "def", "average_precision", "(", "r", ",", "num_predictions", ",", "num_trgs", ")", ":", "\n", "    ", "if", "num_predictions", "==", "0", "or", "num_trgs", "==", "0", ":", "\n", "        ", "return", "0", "\n", "", "r_cum_sum", "=", "np", ".", "cumsum", "(", "r", ",", "axis", "=", "0", ")", "\n", "precision_sum", "=", "sum", "(", "[", "compute_precision", "(", "r_cum_sum", "[", "k", "]", ",", "k", "+", "1", ")", "for", "k", "in", "range", "(", "num_predictions", ")", "if", "r", "[", "k", "]", "]", ")", "\n", "'''\n    precision_sum = 0\n    for k in range(num_predictions):\n        if r[k] is False:\n            continue\n        else:\n            precision_k = precision(r_cum_sum[k], k+1)\n            precision_sum += precision_k\n    '''", "\n", "return", "precision_sum", "/", "num_trgs", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.average_precision_at_k": [[1713, 1727], ["evaluate_prediction.average_precision"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.average_precision"], ["", "def", "average_precision_at_k", "(", "r", ",", "k", ",", "num_predictions", ",", "num_trgs", ")", ":", "\n", "    ", "if", "k", "==", "'M'", ":", "\n", "        ", "k", "=", "num_predictions", "\n", "", "elif", "k", "==", "'G'", ":", "\n", "#k = num_trgs", "\n", "        ", "if", "num_predictions", "<", "num_trgs", ":", "\n", "            ", "k", "=", "num_trgs", "\n", "", "else", ":", "\n", "            ", "k", "=", "num_predictions", "\n", "\n", "", "", "if", "k", "<", "num_predictions", ":", "\n", "        ", "num_predictions", "=", "k", "\n", "r", "=", "r", "[", ":", "k", "]", "\n", "", "return", "average_precision", "(", "r", ",", "num_predictions", ",", "num_trgs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.average_precision_at_ks": [[1729, 1765], ["numpy.cumsum", "numpy.cumsum", "numpy.array", "np.array.append", "len", "evaluate_prediction.compute_precision", "range"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_precision"], ["", "def", "average_precision_at_ks", "(", "r", ",", "k_list", ",", "num_predictions", ",", "num_trgs", ")", ":", "\n", "    ", "if", "num_predictions", "==", "0", "or", "num_trgs", "==", "0", ":", "\n", "        ", "return", "[", "0", "]", "*", "len", "(", "k_list", ")", "\n", "# k_max = max(k_list)", "\n", "", "k_max", "=", "-", "1", "\n", "for", "k", "in", "k_list", ":", "\n", "        ", "if", "k", "==", "'M'", ":", "\n", "            ", "k", "=", "num_predictions", "\n", "", "elif", "k", "==", "'G'", ":", "\n", "#k = num_trgs", "\n", "            ", "if", "num_predictions", "<", "num_trgs", ":", "\n", "                ", "k", "=", "num_trgs", "\n", "", "else", ":", "\n", "                ", "k", "=", "num_predictions", "\n", "", "", "if", "k", ">", "k_max", ":", "\n", "            ", "k_max", "=", "k", "\n", "", "", "if", "num_predictions", ">", "k_max", ":", "\n", "        ", "num_predictions", "=", "k_max", "\n", "r", "=", "r", "[", ":", "num_predictions", "]", "\n", "", "r_cum_sum", "=", "np", ".", "cumsum", "(", "r", ",", "axis", "=", "0", ")", "\n", "precision_array", "=", "[", "compute_precision", "(", "r_cum_sum", "[", "k", "]", ",", "k", "+", "1", ")", "*", "r", "[", "k", "]", "for", "k", "in", "range", "(", "num_predictions", ")", "]", "\n", "precision_cum_sum", "=", "np", ".", "cumsum", "(", "precision_array", ",", "axis", "=", "0", ")", "\n", "average_precision_array", "=", "precision_cum_sum", "/", "num_trgs", "\n", "return_indices", "=", "[", "]", "\n", "for", "k", "in", "k_list", ":", "\n", "        ", "if", "k", "==", "'M'", ":", "\n", "            ", "k", "=", "num_predictions", "\n", "", "elif", "k", "==", "'G'", ":", "\n", "#k = num_trgs", "\n", "            ", "if", "num_predictions", "<", "num_trgs", ":", "\n", "                ", "k", "=", "num_trgs", "\n", "", "else", ":", "\n", "                ", "k", "=", "num_predictions", "\n", "", "", "return_indices", ".", "append", "(", "(", "k", "-", "1", ")", "if", "k", "<=", "num_predictions", "else", "(", "num_predictions", "-", "1", ")", ")", "\n", "", "return_indices", "=", "np", ".", "array", "(", "return_indices", ",", "dtype", "=", "int", ")", "\n", "return", "average_precision_array", "[", "return_indices", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.find_v": [[1767, 1777], ["numpy.zeros", "enumerate", "print", "len", "numpy.argmax"], "function", ["None"], ["", "def", "find_v", "(", "f1_dict", ",", "num_samples", ",", "k_list", ",", "tag", ")", ":", "\n", "    ", "marco_f1_scores", "=", "np", ".", "zeros", "(", "len", "(", "k_list", ")", ")", "\n", "for", "i", ",", "topk", "in", "enumerate", "(", "k_list", ")", ":", "\n", "        ", "marco_avg_precision", "=", "f1_dict", "[", "'precision_sum@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", "/", "num_samples", "\n", "marco_avg_recall", "=", "f1_dict", "[", "'recall_sum@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", "/", "num_samples", "\n", "marco_f1_scores", "[", "i", "]", "=", "2", "*", "marco_avg_precision", "*", "marco_avg_recall", "/", "(", "marco_avg_precision", "+", "marco_avg_recall", ")", "if", "(", "marco_avg_precision", "+", "marco_avg_recall", ")", ">", "0", "else", "0", "\n", "# marco_f1_scores[i] = f1_dict['f1_score_sum@{}_{}'.format(topk, tag)] / num_samples", "\n", "# for debug", "\n", "", "print", "(", "marco_f1_scores", ")", "\n", "return", "k_list", "[", "np", ".", "argmax", "(", "marco_f1_scores", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_f1_dict": [[1779, 1791], ["len", "len", "evaluate_prediction.compute_match_result", "evaluate_prediction.compute_classification_metrics_at_ks", "zip"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics_at_ks"], ["", "def", "update_f1_dict", "(", "trg_token_2dlist_stemmed", ",", "pred_token_2dlist_stemmed", ",", "k_list", ",", "f1_dict", ",", "tag", ")", ":", "\n", "    ", "num_targets", "=", "len", "(", "trg_token_2dlist_stemmed", ")", "\n", "num_predictions", "=", "len", "(", "pred_token_2dlist_stemmed", ")", "\n", "is_match", "=", "compute_match_result", "(", "trg_token_2dlist_stemmed", ",", "pred_token_2dlist_stemmed", ",", "\n", "type", "=", "'exact'", ",", "dimension", "=", "1", ")", "\n", "# Classification metrics", "\n", "precision_ks", ",", "recall_ks", ",", "f1_ks", ",", "num_matches_ks", ",", "num_predictions_ks", "=", "compute_classification_metrics_at_ks", "(", "is_match", ",", "num_predictions", ",", "num_targets", ",", "k_list", "=", "k_list", ",", "meng_rui_precision", "=", "opt", ".", "meng_rui_precision", ")", "\n", "for", "topk", ",", "precision_k", ",", "recall_k", "in", "zip", "(", "k_list", ",", "precision_ks", ",", "recall_ks", ")", ":", "\n", "        ", "f1_dict", "[", "'precision_sum@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", "+=", "precision_k", "\n", "f1_dict", "[", "'recall_sum@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", "+=", "recall_k", "\n", "", "return", "f1_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_f1_dict_with_name_variation": [[1793, 1806], ["len", "len", "evaluate_prediction.compute_var_match_result", "evaluate_prediction.compute_classification_metrics_at_ks", "zip"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_var_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics_at_ks"], ["", "def", "update_f1_dict_with_name_variation", "(", "trg_variation_token_3dlist", ",", "pred_token_2dlist", ",", "k_list", ",", "f1_dict", ",", "tag", ")", ":", "\n", "    ", "num_targets", "=", "len", "(", "trg_variation_token_3dlist", ")", "\n", "num_predictions", "=", "len", "(", "pred_token_2dlist", ")", "\n", "is_match", "=", "compute_var_match_result", "(", "trg_variation_token_3dlist", ",", "pred_token_2dlist", ")", "\n", "\n", "# Classification metrics", "\n", "precision_ks", ",", "recall_ks", ",", "f1_ks", ",", "num_matches_ks", ",", "num_predictions_ks", "=", "compute_classification_metrics_at_ks", "(", "is_match", ",", "num_predictions", ",", "num_targets", ",", "k_list", "=", "k_list", ",", "\n", "meng_rui_precision", "=", "opt", ".", "meng_rui_precision", ")", "\n", "for", "topk", ",", "precision_k", ",", "recall_k", "in", "zip", "(", "k_list", ",", "precision_ks", ",", "recall_ks", ")", ":", "\n", "        ", "f1_dict", "[", "'precision_sum@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", "+=", "precision_k", "\n", "f1_dict", "[", "'recall_sum@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", "+=", "recall_k", "\n", "", "return", "f1_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_score_dict": [[1808, 1843], ["len", "len", "evaluate_prediction.compute_match_result", "evaluate_prediction.compute_match_result", "evaluate_prediction.compute_classification_metrics_at_ks", "evaluate_prediction.ndcg_at_ks", "evaluate_prediction.alpha_ndcg_at_ks", "evaluate_prediction.average_precision_at_ks", "zip", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics_at_ks", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.ndcg_at_ks", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.alpha_ndcg_at_ks", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.average_precision_at_ks"], ["", "def", "update_score_dict", "(", "trg_token_2dlist_stemmed", ",", "pred_token_2dlist_stemmed", ",", "k_list", ",", "score_dict", ",", "tag", ")", ":", "\n", "    ", "num_targets", "=", "len", "(", "trg_token_2dlist_stemmed", ")", "\n", "num_predictions", "=", "len", "(", "pred_token_2dlist_stemmed", ")", "\n", "\n", "is_match", "=", "compute_match_result", "(", "trg_token_2dlist_stemmed", ",", "pred_token_2dlist_stemmed", ",", "\n", "type", "=", "'exact'", ",", "dimension", "=", "1", ")", "\n", "is_match_substring_2d", "=", "compute_match_result", "(", "trg_token_2dlist_stemmed", ",", "\n", "pred_token_2dlist_stemmed", ",", "type", "=", "'sub'", ",", "dimension", "=", "2", ")", "\n", "# Classification metrics", "\n", "precision_ks", ",", "recall_ks", ",", "f1_ks", ",", "num_matches_ks", ",", "num_predictions_ks", "=", "compute_classification_metrics_at_ks", "(", "is_match", ",", "num_predictions", ",", "num_targets", ",", "k_list", "=", "k_list", ",", "meng_rui_precision", "=", "opt", ".", "meng_rui_precision", ")", "\n", "\n", "# Ranking metrics", "\n", "ndcg_ks", ",", "dcg_ks", "=", "ndcg_at_ks", "(", "is_match", ",", "k_list", "=", "k_list", ",", "num_trgs", "=", "num_targets", ",", "method", "=", "1", ",", "include_dcg", "=", "True", ")", "\n", "alpha_ndcg_ks", ",", "alpha_dcg_ks", "=", "alpha_ndcg_at_ks", "(", "is_match_substring_2d", ",", "k_list", "=", "k_list", ",", "method", "=", "1", ",", "\n", "alpha", "=", "0.5", ",", "include_dcg", "=", "True", ")", "\n", "ap_ks", "=", "average_precision_at_ks", "(", "is_match", ",", "k_list", "=", "k_list", ",", "\n", "num_predictions", "=", "num_predictions", ",", "num_trgs", "=", "num_targets", ")", "\n", "\n", "for", "topk", ",", "precision_k", ",", "recall_k", ",", "f1_k", ",", "num_matches_k", ",", "num_predictions_k", ",", "ndcg_k", ",", "dcg_k", ",", "alpha_ndcg_k", ",", "alpha_dcg_k", ",", "ap_k", "in", "zip", "(", "k_list", ",", "precision_ks", ",", "recall_ks", ",", "f1_ks", ",", "num_matches_ks", ",", "num_predictions_ks", ",", "ndcg_ks", ",", "dcg_ks", ",", "\n", "alpha_ndcg_ks", ",", "alpha_dcg_ks", ",", "ap_ks", ")", ":", "\n", "        ", "score_dict", "[", "'precision@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "precision_k", ")", "\n", "score_dict", "[", "'recall@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "recall_k", ")", "\n", "score_dict", "[", "'f1_score@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "f1_k", ")", "\n", "score_dict", "[", "'num_matches@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "num_matches_k", ")", "\n", "score_dict", "[", "'num_predictions@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "num_predictions_k", ")", "\n", "score_dict", "[", "'num_targets@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "num_targets", ")", "\n", "score_dict", "[", "'AP@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "ap_k", ")", "\n", "score_dict", "[", "'NDCG@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "ndcg_k", ")", "\n", "score_dict", "[", "'AlphaNDCG@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "alpha_ndcg_k", ")", "\n", "\n", "", "score_dict", "[", "'num_targets_{}'", ".", "format", "(", "tag", ")", "]", ".", "append", "(", "num_targets", ")", "\n", "score_dict", "[", "'num_predictions_{}'", ".", "format", "(", "tag", ")", "]", ".", "append", "(", "num_predictions", ")", "\n", "return", "score_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_score_dict_with_name_variation_backup": [[1846, 1862], ["evaluate_prediction.compute_classification_metrics_at_ks", "zip", "len", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics_at_ks"], ["", "def", "update_score_dict_with_name_variation_backup", "(", "is_match_all", ",", "pred_indices", ",", "num_predictions", ",", "num_targets", ",", "k_list", ",", "score_dict", ",", "tag", ")", ":", "\n", "    ", "assert", "len", "(", "pred_indices", ")", "==", "num_predictions", "\n", "is_match", "=", "is_match_all", "[", "pred_indices", "]", "\n", "# Classification metrics", "\n", "precision_ks", ",", "recall_ks", ",", "f1_ks", ",", "num_matches_ks", ",", "num_predictions_ks", "=", "compute_classification_metrics_at_ks", "(", "is_match", ",", "num_predictions", ",", "num_targets", ",", "k_list", "=", "k_list", ",", "\n", "meng_rui_precision", "=", "opt", ".", "meng_rui_precision", ")", "\n", "for", "topk", ",", "precision_k", ",", "recall_k", ",", "f1_k", ",", "num_matches_k", ",", "num_predictions_k", "in", "zip", "(", "k_list", ",", "precision_ks", ",", "recall_ks", ",", "f1_ks", ",", "num_matches_ks", ",", "num_predictions_ks", ")", ":", "\n", "        ", "score_dict", "[", "'precision@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "precision_k", ")", "\n", "score_dict", "[", "'recall@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "recall_k", ")", "\n", "score_dict", "[", "'f1_score@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "f1_k", ")", "\n", "score_dict", "[", "'num_matches@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "num_matches_k", ")", "\n", "score_dict", "[", "'num_predictions@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "num_predictions_k", ")", "\n", "score_dict", "[", "'num_targets@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "num_targets", ")", "\n", "", "return", "score_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_score_dict_with_name_variation": [[1864, 1881], ["len", "len", "evaluate_prediction.compute_var_match_result", "evaluate_prediction.compute_classification_metrics_at_ks", "zip", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append", "score_dict[].append"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_var_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics_at_ks"], ["", "def", "update_score_dict_with_name_variation", "(", "trg_variation_token_3dlist", ",", "pred_token_2dlist", ",", "k_list", ",", "score_dict", ",", "tag", ")", ":", "\n", "    ", "num_targets", "=", "len", "(", "trg_variation_token_3dlist", ")", "\n", "num_predictions", "=", "len", "(", "pred_token_2dlist", ")", "\n", "is_match", "=", "compute_var_match_result", "(", "trg_variation_token_3dlist", ",", "pred_token_2dlist", ")", "\n", "# Classification metrics", "\n", "precision_ks", ",", "recall_ks", ",", "f1_ks", ",", "num_matches_ks", ",", "num_predictions_ks", "=", "compute_classification_metrics_at_ks", "(", "is_match", ",", "num_predictions", ",", "num_targets", ",", "k_list", "=", "k_list", ",", "\n", "meng_rui_precision", "=", "opt", ".", "meng_rui_precision", ")", "\n", "for", "topk", ",", "precision_k", ",", "recall_k", ",", "f1_k", ",", "num_matches_k", ",", "num_predictions_k", "in", "zip", "(", "k_list", ",", "precision_ks", ",", "recall_ks", ",", "f1_ks", ",", "num_matches_ks", ",", "num_predictions_ks", ")", ":", "\n", "        ", "score_dict", "[", "'precision@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "precision_k", ")", "\n", "score_dict", "[", "'recall@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "recall_k", ")", "\n", "score_dict", "[", "'f1_score@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "f1_k", ")", "\n", "score_dict", "[", "'num_matches@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "num_matches_k", ")", "\n", "score_dict", "[", "'num_predictions@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "num_predictions_k", ")", "\n", "score_dict", "[", "'num_targets@{}_{}'", ".", "format", "(", "topk", ",", "tag", ")", "]", ".", "append", "(", "num_targets", ")", "\n", "", "return", "score_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_var_match_result": [[1883, 1901], ["len", "len", "numpy.zeros", "enumerate", "enumerate"], "function", ["None"], ["", "def", "compute_var_match_result", "(", "trg_variation_token_3dlist", ",", "pred_token_2dlist", ")", ":", "\n", "    ", "num_pred", "=", "len", "(", "pred_token_2dlist", ")", "\n", "num_trg", "=", "len", "(", "trg_variation_token_3dlist", ")", "\n", "is_match", "=", "np", ".", "zeros", "(", "num_pred", ",", "dtype", "=", "bool", ")", "\n", "\n", "for", "pred_idx", ",", "pred_token_list", "in", "enumerate", "(", "pred_token_2dlist", ")", ":", "\n", "        ", "joined_pred_token_list", "=", "' '", ".", "join", "(", "pred_token_list", ")", "\n", "match_flag", "=", "False", "\n", "for", "trg_idx", ",", "trg_variation_token_2dlist", "in", "enumerate", "(", "trg_variation_token_3dlist", ")", ":", "\n", "            ", "for", "trg_variation_token_list", "in", "trg_variation_token_2dlist", ":", "\n", "                ", "joined_trg_variation_token_list", "=", "' '", ".", "join", "(", "trg_variation_token_list", ")", "\n", "if", "joined_pred_token_list", "==", "joined_trg_variation_token_list", ":", "\n", "                    ", "is_match", "[", "pred_idx", "]", "=", "True", "\n", "match_flag", "=", "True", "\n", "break", "\n", "", "", "if", "match_flag", ":", "\n", "                ", "break", "\n", "", "", "", "return", "is_match", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.filter_prediction": [[1903, 1926], ["len", "evaluate_prediction.check_duplicate_keyphrases", "evaluate_prediction.check_valid_keyphrases", "evaluate_prediction.compute_extra_one_word_seqs_mask", "numpy.sum", "zip"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_duplicate_keyphrases", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_valid_keyphrases", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_extra_one_word_seqs_mask"], ["", "def", "filter_prediction", "(", "disable_valid_filter", ",", "disable_extra_one_word_filter", ",", "pred_token_2dlist_stemmed", ")", ":", "\n", "    ", "\"\"\"\n    Remove the duplicate predictions, can optionally remove invalid predictions and extra one word predictions\n    :param disable_valid_filter:\n    :param disable_extra_one_word_filter:\n    :param pred_token_2dlist_stemmed:\n    :param pred_token_2d_list:\n    :return:\n    \"\"\"", "\n", "num_predictions", "=", "len", "(", "pred_token_2dlist_stemmed", ")", "\n", "is_unique_mask", "=", "check_duplicate_keyphrases", "(", "pred_token_2dlist_stemmed", ")", "# boolean array, 1=unqiue, 0=duplicate", "\n", "pred_filter", "=", "is_unique_mask", "\n", "if", "not", "disable_valid_filter", ":", "\n", "        ", "is_valid_mask", "=", "check_valid_keyphrases", "(", "pred_token_2dlist_stemmed", ")", "\n", "pred_filter", "=", "pred_filter", "*", "is_valid_mask", "\n", "", "if", "not", "disable_extra_one_word_filter", ":", "\n", "        ", "extra_one_word_seqs_mask", ",", "num_one_word_seqs", "=", "compute_extra_one_word_seqs_mask", "(", "pred_token_2dlist_stemmed", ")", "\n", "pred_filter", "=", "pred_filter", "*", "extra_one_word_seqs_mask", "\n", "", "filtered_stemmed_pred_str_list", "=", "[", "word_list", "for", "word_list", ",", "is_keep", "in", "\n", "zip", "(", "pred_token_2dlist_stemmed", ",", "pred_filter", ")", "if", "\n", "is_keep", "]", "\n", "num_duplicated_predictions", "=", "num_predictions", "-", "np", ".", "sum", "(", "is_unique_mask", ")", "\n", "return", "filtered_stemmed_pred_str_list", ",", "num_duplicated_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.find_unique_target": [[1928, 1942], ["len", "evaluate_prediction.check_duplicate_keyphrases", "numpy.sum", "zip"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_duplicate_keyphrases"], ["", "def", "find_unique_target", "(", "trg_token_2dlist_stemmed", ")", ":", "\n", "    ", "\"\"\"\n    Remove the duplicate targets\n    :param trg_token_2dlist_stemmed:\n    :return:\n    \"\"\"", "\n", "num_trg", "=", "len", "(", "trg_token_2dlist_stemmed", ")", "\n", "is_unique_mask", "=", "check_duplicate_keyphrases", "(", "trg_token_2dlist_stemmed", ")", "# boolean array, 1=unqiue, 0=duplicate", "\n", "trg_filter", "=", "is_unique_mask", "\n", "filtered_stemmed_trg_str_list", "=", "[", "word_list", "for", "word_list", ",", "is_keep", "in", "\n", "zip", "(", "trg_token_2dlist_stemmed", ",", "trg_filter", ")", "if", "\n", "is_keep", "]", "\n", "num_duplicated_trg", "=", "num_trg", "-", "np", ".", "sum", "(", "is_unique_mask", ")", "\n", "return", "filtered_stemmed_trg_str_list", ",", "num_duplicated_trg", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.separate_present_absent_by_source": [[1944, 1954], ["evaluate_prediction.check_present_keyphrases", "zip", "present_keyphrase_token2dlist.append", "absent_keyphrase_token2dlist.append"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_present_keyphrases"], ["", "def", "separate_present_absent_by_source", "(", "src_token_list_stemmed", ",", "keyphrase_token_2dlist_stemmed", ",", "match_by_str", ")", ":", "\n", "    ", "is_present_mask", "=", "check_present_keyphrases", "(", "src_token_list_stemmed", ",", "keyphrase_token_2dlist_stemmed", ",", "match_by_str", ")", "\n", "present_keyphrase_token2dlist", "=", "[", "]", "\n", "absent_keyphrase_token2dlist", "=", "[", "]", "\n", "for", "keyphrase_token_list", ",", "is_present", "in", "zip", "(", "keyphrase_token_2dlist_stemmed", ",", "is_present_mask", ")", ":", "\n", "        ", "if", "is_present", ":", "\n", "            ", "present_keyphrase_token2dlist", ".", "append", "(", "keyphrase_token_list", ")", "\n", "", "else", ":", "\n", "            ", "absent_keyphrase_token2dlist", ".", "append", "(", "keyphrase_token_list", ")", "\n", "", "", "return", "present_keyphrase_token2dlist", ",", "absent_keyphrase_token2dlist", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.separate_present_absent_by_segmenter": [[1956, 1970], ["absent_keyphrase_token2dlist.append", "present_keyphrase_token2dlist.append"], "function", ["None"], ["", "def", "separate_present_absent_by_segmenter", "(", "keyphrase_token_2dlist", ",", "segmenter", ")", ":", "\n", "    ", "present_keyphrase_token2dlist", "=", "[", "]", "\n", "absent_keyphrase_token2dlist", "=", "[", "]", "\n", "absent_flag", "=", "False", "\n", "for", "keyphrase_token_list", "in", "keyphrase_token_2dlist", ":", "\n", "        ", "if", "keyphrase_token_list", "[", "0", "]", "==", "segmenter", ":", "\n", "            ", "absent_flag", "=", "True", "\n", "# skip the segmenter token, because it should not be included in the evaluation", "\n", "continue", "\n", "", "if", "absent_flag", ":", "\n", "            ", "absent_keyphrase_token2dlist", ".", "append", "(", "keyphrase_token_list", ")", "\n", "", "else", ":", "\n", "            ", "present_keyphrase_token2dlist", ".", "append", "(", "keyphrase_token_list", ")", "\n", "", "", "return", "present_keyphrase_token2dlist", ",", "absent_keyphrase_token2dlist", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.process_input_ks": [[1972, 1979], ["ks_list.append", "int"], "function", ["None"], ["", "def", "process_input_ks", "(", "ks", ")", ":", "\n", "    ", "ks_list", "=", "[", "]", "\n", "for", "k", "in", "ks", ":", "\n", "        ", "if", "k", "!=", "'M'", "and", "k", "!=", "'G'", ":", "\n", "            ", "k", "=", "int", "(", "k", ")", "\n", "", "ks_list", ".", "append", "(", "k", ")", "\n", "", "return", "ks_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.main": [[1981, 2387], ["enumerate", "open", "collections.defaultdict", "collections.defaultdict", "evaluate_prediction.process_input_ks", "evaluate_prediction.process_input_ks", "evaluate_prediction.process_input_ks", "zip", "pred_l.strip().split", "trg_l.strip().split", "src_l.strip().split", "len", "utils.string_helper.stem_word_list", "utils.string_helper.stem_str_list", "evaluate_prediction.filter_prediction", "len", "open.close", "evaluate_prediction.find_v", "print", "evaluate_prediction.find_v", "print", "evaluate_prediction.find_v", "print", "open", "open.write", "open.write", "open.write", "open.close", "evaluate_prediction.report_stat_and_scores", "evaluate_prediction.report_stat_and_scores", "evaluate_prediction.report_stat_and_scores", "open", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "evaluate_prediction.mae", "evaluate_prediction.mae", "evaluate_prediction.mae", "open.write", "open.close", "open", "open.write", "open.write", "open.close", "open", "pickle.dump", "open.close", "os.path.join", "list", "list", "list", "open", "open", "open", "pred_str.strip().split", "title.strip().split", "context.strip().split", "sum", "len", "evaluate_prediction.find_unique_target", "len", "evaluate_prediction.separate_present_absent_by_source_with_variations", "evaluate_prediction.separate_present_absent_by_source_with_variations", "len", "len", "len", "len", "evaluate_prediction.separate_present_absent_by_source", "len", "len", "len", "len", "open.write", "os.path.join", "os.path.join", "max", "max", "max", "max", "os.path.join", "os.path.join", "range", "range", "range", "pred_l.strip", "trg_l.strip", "trg_str.strip().split", "trg_variation_token_3dlist.append", "trg_str.strip().split", "src_l.strip", "utils.string_helper.stem_str_2d_list", "utils.string_helper.stem_str_list", "evaluate_prediction.separate_present_absent_by_segmenter", "evaluate_prediction.separate_present_absent_by_segmenter", "len", "sum", "evaluate_prediction.check_present_keyphrases", "evaluate_prediction.update_f1_dict", "evaluate_prediction.update_f1_dict", "evaluate_prediction.update_f1_dict", "evaluate_prediction.update_score_dict_with_name_variation", "evaluate_prediction.update_score_dict_with_name_variation", "evaluate_prediction.update_score_dict_with_name_variation", "evaluate_prediction.separate_present_absent_by_source", "len", "len", "evaluate_prediction.update_f1_dict", "evaluate_prediction.update_f1_dict", "evaluate_prediction.update_f1_dict", "evaluate_prediction.update_score_dict", "evaluate_prediction.update_score_dict", "evaluate_prediction.update_score_dict", "evaluate_prediction.compute_fg_score_k", "final_pred_str_list.append", "str", "str", "str", "pred_str.strip", "name_variation_tokens_2dlist.append", "title.strip", "context.strip", "evaluate_prediction.check_present_keyphrases", "len", "len", "len", "len", "evaluate_prediction.separate_present_absent_by_segmenter", "evaluate_prediction.separate_present_absent_by_segmenter", "trg_str.strip", "name_variation.strip().split", "trg_str.strip", "zip", "name_variation.strip"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.process_input_ks", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.process_input_ks", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.process_input_ks", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_word_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_str_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.filter_prediction", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.find_v", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.find_v", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.find_v", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.report_stat_and_scores", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.report_stat_and_scores", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.report_stat_and_scores", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.mae", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.mae", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.mae", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.find_unique_target", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.separate_present_absent_by_source_with_variations", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.separate_present_absent_by_source_with_variations", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.separate_present_absent_by_source", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_str_2d_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_str_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.separate_present_absent_by_segmenter", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.separate_present_absent_by_segmenter", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_present_keyphrases", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_f1_dict", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_f1_dict", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_f1_dict", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_score_dict_with_name_variation", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_score_dict_with_name_variation", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_score_dict_with_name_variation", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.separate_present_absent_by_source", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_f1_dict", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_f1_dict", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_f1_dict", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_score_dict", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_score_dict", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.update_score_dict", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_fg_score_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_present_keyphrases", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.separate_present_absent_by_segmenter", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.separate_present_absent_by_segmenter"], ["", "def", "main", "(", "opt", ")", ":", "\n", "    ", "src_file_path", "=", "opt", ".", "src_file_path", "\n", "trg_file_path", "=", "opt", ".", "trg_file_path", "\n", "pred_file_path", "=", "opt", ".", "pred_file_path", "\n", "\n", "if", "opt", ".", "export_filtered_pred", ":", "\n", "        ", "pred_output_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "filtered_pred_path", ",", "\"predictions_filtered.txt\"", ")", ",", "\"w\"", ")", "\n", "\n", "", "if", "opt", ".", "tune_f1_v", ":", "\n", "        ", "f1_dict", "=", "defaultdict", "(", "lambda", ":", "0", ")", "\n", "max_k", "=", "20", "\n", "topk_dict", "=", "{", "'present'", ":", "list", "(", "range", "(", "1", ",", "max_k", ")", ")", ",", "'absent'", ":", "list", "(", "range", "(", "1", ",", "max_k", ")", ")", ",", "'all'", ":", "list", "(", "range", "(", "1", ",", "max_k", ")", ")", "}", "\n", "", "else", ":", "\n", "        ", "score_dict", "=", "defaultdict", "(", "list", ")", "\n", "all_ks", "=", "process_input_ks", "(", "opt", ".", "all_ks", ")", "\n", "present_ks", "=", "process_input_ks", "(", "opt", ".", "present_ks", ")", "\n", "absent_ks", "=", "process_input_ks", "(", "opt", ".", "absent_ks", ")", "\n", "topk_dict", "=", "{", "'present'", ":", "present_ks", ",", "'absent'", ":", "absent_ks", ",", "'all'", ":", "all_ks", "}", "\n", "# topk_dict = {'present': [5, 10, 'M'], 'absent': [5, 10, 50, 'M'], 'all': [5, 10, 'M']}", "\n", "\n", "", "total_num_src", "=", "0", "\n", "total_num_src_with_present_keyphrases", "=", "0", "\n", "total_num_src_with_absent_keyphrases", "=", "0", "\n", "total_num_unique_predictions", "=", "0", "\n", "total_num_present_filtered_predictions", "=", "0", "\n", "total_num_present_unique_targets", "=", "0", "\n", "total_num_absent_filtered_predictions", "=", "0", "\n", "total_num_absent_unique_targets", "=", "0", "\n", "max_unique_targets", "=", "0", "\n", "\n", "\n", "if", "opt", ".", "prediction_separated", ":", "\n", "        ", "sum_incorrect_fraction_for_identifying_present", "=", "0", "\n", "sum_incorrect_fraction_for_identifying_absent", "=", "0", "\n", "\n", "", "stat_total", "=", "[", "0", "]", "*", "12", "\n", "stat_token_f1", "=", "[", "0", "]", "*", "12", "\n", "total_fg_score", "=", "0.0", "\n", "total_token_f1", "=", "0.0", "\n", "total_token_p", "=", "0.0", "\n", "total_token_r", "=", "0.0", "\n", "\n", "fg_calc_times", "=", "0", "\n", "tot_phrase", "=", "0", "\n", "\n", "for", "data_idx", ",", "(", "src_l", ",", "trg_l", ",", "pred_l", ")", "in", "enumerate", "(", "zip", "(", "open", "(", "src_file_path", ")", ",", "open", "(", "trg_file_path", ")", ",", "open", "(", "pred_file_path", ")", ")", ")", ":", "\n", "        ", "total_num_src", "+=", "1", "\n", "# convert the str to token list", "\n", "pred_str_list", "=", "pred_l", ".", "strip", "(", ")", ".", "split", "(", "';'", ")", "\n", "pred_str_list", "=", "pred_str_list", "[", ":", "opt", ".", "num_preds", "]", "\n", "pred_token_2dlist", "=", "[", "pred_str", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "for", "pred_str", "in", "pred_str_list", "]", "\n", "trg_str_list", "=", "trg_l", ".", "strip", "(", ")", ".", "split", "(", "';'", ")", "\n", "if", "opt", ".", "use_name_variations", ":", "\n", "#trg_token_2dlist = [trg_str.strip().split('|') for trg_str in trg_str_list]", "\n", "            ", "trg_variation_token_3dlist", "=", "[", "]", "\n", "for", "trg_str", "in", "trg_str_list", ":", "\n", "                ", "name_variation_list", "=", "trg_str", ".", "strip", "(", ")", ".", "split", "(", "'|'", ")", "\n", "name_variation_tokens_2dlist", "=", "[", "]", "\n", "for", "name_variation", "in", "name_variation_list", ":", "\n", "                    ", "name_variation_tokens_2dlist", ".", "append", "(", "name_variation", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "", "trg_variation_token_3dlist", ".", "append", "(", "name_variation_tokens_2dlist", ")", "\n", "", "", "else", ":", "\n", "            ", "trg_token_2dlist", "=", "[", "trg_str", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "for", "trg_str", "in", "trg_str_list", "]", "\n", "\n", "# TODO: test name_variation_tokens_3dlist", "\n", "", "[", "title", ",", "context", "]", "=", "src_l", ".", "strip", "(", ")", ".", "split", "(", "'<eos>'", ")", "\n", "src_token_list", "=", "title", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "+", "context", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "\n", "num_predictions", "=", "len", "(", "pred_str_list", ")", "\n", "\n", "# perform stemming", "\n", "stemmed_src_token_list", "=", "stem_word_list", "(", "src_token_list", ")", "\n", "\n", "if", "opt", ".", "use_name_variations", ":", "\n", "            ", "if", "opt", ".", "target_already_stemmed", ":", "\n", "                ", "stemmed_trg_variation_token_3dlist", "=", "trg_variation_token_3dlist", "\n", "", "else", ":", "\n", "                ", "stemmed_trg_variation_token_3dlist", "=", "stem_str_2d_list", "(", "trg_variation_token_3dlist", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "opt", ".", "target_already_stemmed", ":", "\n", "                ", "stemmed_trg_token_2dlist", "=", "trg_token_2dlist", "\n", "", "else", ":", "\n", "                ", "stemmed_trg_token_2dlist", "=", "stem_str_list", "(", "trg_token_2dlist", ")", "\n", "# TODO: test stemmed_trg_variation_token_3dlist", "\n", "\n", "", "", "stemmed_pred_token_2dlist", "=", "stem_str_list", "(", "pred_token_2dlist", ")", "\n", "\n", "# remove peos in predictions, then check if the model can successfuly separate present and absent keyphrases by segmenter", "\n", "if", "opt", ".", "prediction_separated", ":", "\n", "            ", "if", "opt", ".", "reverse_sorting", ":", "\n", "                ", "absent_stemmed_pred_token_2dlist_by_segmenter", ",", "present_stemmed_pred_token_2dlist_by_segmenter", "=", "separate_present_absent_by_segmenter", "(", "\n", "stemmed_pred_token_2dlist", ",", "present_absent_segmenter", ")", "\n", "", "else", ":", "\n", "                ", "present_stemmed_pred_token_2dlist_by_segmenter", ",", "absent_stemmed_pred_token_2dlist_by_segmenter", "=", "separate_present_absent_by_segmenter", "(", "stemmed_pred_token_2dlist", ",", "present_absent_segmenter", ")", "\n", "", "stemmed_pred_token_2dlist", "=", "present_stemmed_pred_token_2dlist_by_segmenter", "+", "absent_stemmed_pred_token_2dlist_by_segmenter", "# remove all the peos token", "\n", "# check present absent", "\n", "num_absent_before_segmenter", "=", "len", "(", "present_stemmed_pred_token_2dlist_by_segmenter", ")", "-", "sum", "(", "check_present_keyphrases", "(", "stemmed_src_token_list", ",", "present_stemmed_pred_token_2dlist_by_segmenter", ")", ")", "\n", "num_present_after_segmenter", "=", "sum", "(", "check_present_keyphrases", "(", "stemmed_src_token_list", ",", "absent_stemmed_pred_token_2dlist_by_segmenter", ")", ")", "\n", "incorrect_fraction_for_identifying_present", "=", "num_absent_before_segmenter", "/", "len", "(", "present_stemmed_pred_token_2dlist_by_segmenter", ")", "if", "len", "(", "present_stemmed_pred_token_2dlist_by_segmenter", ")", ">", "0", "else", "0", "\n", "incorrect_fraction_for_identifying_absent", "=", "num_present_after_segmenter", "/", "len", "(", "absent_stemmed_pred_token_2dlist_by_segmenter", ")", "if", "len", "(", "absent_stemmed_pred_token_2dlist_by_segmenter", ")", ">", "0", "else", "0", "\n", "sum_incorrect_fraction_for_identifying_present", "+=", "incorrect_fraction_for_identifying_present", "\n", "sum_incorrect_fraction_for_identifying_absent", "+=", "incorrect_fraction_for_identifying_absent", "\n", "\n", "# Filter out duplicate, invalid, and extra one word predictions", "\n", "", "filtered_stemmed_pred_token_2dlist", ",", "num_duplicated_predictions", "=", "filter_prediction", "(", "opt", ".", "disable_valid_filter", ",", "opt", ".", "disable_extra_one_word_filter", ",", "stemmed_pred_token_2dlist", ")", "\n", "total_num_unique_predictions", "+=", "(", "num_predictions", "-", "num_duplicated_predictions", ")", "\n", "num_filtered_predictions", "=", "len", "(", "filtered_stemmed_pred_token_2dlist", ")", "\n", "\n", "# Remove duplicated targets", "\n", "if", "opt", ".", "use_name_variations", ":", "# testing set with name variation have removed all duplicates during preprocessing", "\n", "            ", "num_unique_targets", "=", "len", "(", "stemmed_trg_variation_token_3dlist", ")", "\n", "unique_stemmed_trg_variation_token_3dlist", "=", "stemmed_trg_variation_token_3dlist", "\n", "", "else", ":", "\n", "            ", "unique_stemmed_trg_token_2dlist", ",", "num_duplicated_trg", "=", "find_unique_target", "(", "stemmed_trg_token_2dlist", ")", "\n", "#unique_stemmed_trg_token_2dlist = stemmed_trg_token_2dlist", "\n", "num_unique_targets", "=", "len", "(", "unique_stemmed_trg_token_2dlist", ")", "\n", "# max_unique_targets += (num_trg - num_duplicated_trg)", "\n", "\n", "", "if", "num_unique_targets", ">", "max_unique_targets", ":", "\n", "            ", "max_unique_targets", "=", "num_unique_targets", "\n", "\n", "# separate present and absent keyphrases", "\n", "", "if", "opt", ".", "use_name_variations", ":", "\n", "# separate prediction", "\n", "            ", "present_filtered_stemmed_pred_token_2dlist", ",", "absent_filtered_stemmed_pred_token_2dlist", "=", "separate_present_absent_by_source_with_variations", "(", "stemmed_src_token_list", ",", "\n", "filtered_stemmed_pred_token_2dlist", ",", "\n", "use_name_variations", "=", "False", ")", "\n", "# separate target", "\n", "present_unique_stemmed_trg_variation_token_3dlist", ",", "absent_unique_stemmed_trg_variation_token_3dlist", "=", "separate_present_absent_by_source_with_variations", "(", "stemmed_src_token_list", ",", "\n", "unique_stemmed_trg_variation_token_3dlist", ",", "\n", "use_name_variations", "=", "True", ")", "\n", "\n", "num_present_filtered_predictions", "=", "len", "(", "present_filtered_stemmed_pred_token_2dlist", ")", "\n", "num_present_unique_targets", "=", "len", "(", "present_unique_stemmed_trg_variation_token_3dlist", ")", "\n", "num_absent_filtered_predictions", "=", "len", "(", "absent_filtered_stemmed_pred_token_2dlist", ")", "\n", "num_absent_unique_targets", "=", "len", "(", "absent_unique_stemmed_trg_variation_token_3dlist", ")", "\n", "\n", "total_num_present_filtered_predictions", "+=", "num_present_filtered_predictions", "\n", "total_num_present_unique_targets", "+=", "num_present_unique_targets", "\n", "total_num_absent_filtered_predictions", "+=", "num_absent_filtered_predictions", "\n", "total_num_absent_unique_targets", "+=", "num_absent_unique_targets", "\n", "\n", "if", "num_present_unique_targets", ">", "0", ":", "\n", "                ", "total_num_src_with_present_keyphrases", "+=", "1", "\n", "", "if", "num_absent_unique_targets", ">", "0", ":", "\n", "                ", "total_num_src_with_absent_keyphrases", "+=", "1", "\n", "\n", "", "if", "opt", ".", "tune_f1_v", ":", "\n", "# compute F1 score all", "\n", "                ", "f1_dict", "=", "update_f1_dict", "(", "unique_stemmed_trg_variation_token_3dlist", ",", "filtered_stemmed_pred_token_2dlist", ",", "\n", "topk_dict", "[", "'all'", "]", ",", "f1_dict", ",", "'all'", ")", "\n", "# compute F1 score present", "\n", "f1_dict", "=", "update_f1_dict", "(", "present_unique_stemmed_trg_variation_token_3dlist", ",", "\n", "present_filtered_stemmed_pred_token_2dlist", ",", "\n", "topk_dict", "[", "'present'", "]", ",", "f1_dict", ",", "'present'", ")", "\n", "# compute F1 score absent", "\n", "f1_dict", "=", "update_f1_dict", "(", "absent_unique_stemmed_trg_variation_token_3dlist", ",", "\n", "absent_filtered_stemmed_pred_token_2dlist", ",", "\n", "topk_dict", "[", "'absent'", "]", ",", "f1_dict", ",", "'absent'", ")", "\n", "", "else", ":", "\n", "# compute all the metrics and update the score_dict", "\n", "                ", "score_dict", "=", "update_score_dict_with_name_variation", "(", "unique_stemmed_trg_variation_token_3dlist", ",", "\n", "filtered_stemmed_pred_token_2dlist", ",", "\n", "topk_dict", "[", "'all'", "]", ",", "score_dict", ",", "'all'", ")", "\n", "# compute all the metrics and update the score_dict for present keyphrase", "\n", "score_dict", "=", "update_score_dict_with_name_variation", "(", "present_unique_stemmed_trg_variation_token_3dlist", ",", "\n", "present_filtered_stemmed_pred_token_2dlist", ",", "\n", "topk_dict", "[", "'present'", "]", ",", "score_dict", ",", "'present'", ")", "\n", "# compute all the metrics and update the score_dict for present keyphrase", "\n", "score_dict", "=", "update_score_dict_with_name_variation", "(", "absent_unique_stemmed_trg_variation_token_3dlist", ",", "\n", "absent_filtered_stemmed_pred_token_2dlist", ",", "\n", "topk_dict", "[", "'absent'", "]", ",", "score_dict", ",", "'absent'", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "present_filtered_stemmed_pred_token_2dlist", ",", "absent_filtered_stemmed_pred_token_2dlist", "=", "separate_present_absent_by_source", "(", "stemmed_src_token_list", ",", "filtered_stemmed_pred_token_2dlist", ",", "opt", ".", "match_by_str", ")", "\n", "if", "opt", ".", "target_separated", ":", "\n", "                ", "if", "opt", ".", "reverse_sorting", ":", "\n", "                    ", "absent_unique_stemmed_trg_token_2dlist", ",", "present_unique_stemmed_trg_token_2dlist", "=", "separate_present_absent_by_segmenter", "(", "\n", "unique_stemmed_trg_token_2dlist", ",", "present_absent_segmenter", ")", "\n", "", "else", ":", "\n", "                    ", "present_unique_stemmed_trg_token_2dlist", ",", "absent_unique_stemmed_trg_token_2dlist", "=", "separate_present_absent_by_segmenter", "(", "\n", "unique_stemmed_trg_token_2dlist", ",", "present_absent_segmenter", ")", "\n", "", "", "else", ":", "\n", "                ", "present_unique_stemmed_trg_token_2dlist", ",", "absent_unique_stemmed_trg_token_2dlist", "=", "separate_present_absent_by_source", "(", "\n", "stemmed_src_token_list", ",", "unique_stemmed_trg_token_2dlist", ",", "opt", ".", "match_by_str", ")", "\n", "\n", "", "total_num_present_filtered_predictions", "+=", "len", "(", "present_filtered_stemmed_pred_token_2dlist", ")", "\n", "total_num_present_unique_targets", "+=", "len", "(", "present_unique_stemmed_trg_token_2dlist", ")", "\n", "total_num_absent_filtered_predictions", "+=", "len", "(", "absent_filtered_stemmed_pred_token_2dlist", ")", "\n", "total_num_absent_unique_targets", "+=", "len", "(", "absent_unique_stemmed_trg_token_2dlist", ")", "\n", "if", "len", "(", "present_unique_stemmed_trg_token_2dlist", ")", ">", "0", ":", "\n", "                ", "total_num_src_with_present_keyphrases", "+=", "1", "\n", "", "if", "len", "(", "absent_unique_stemmed_trg_token_2dlist", ")", ">", "0", ":", "\n", "                ", "total_num_src_with_absent_keyphrases", "+=", "1", "\n", "\n", "", "if", "opt", ".", "tune_f1_v", ":", "\n", "# compute F1 score all", "\n", "                ", "f1_dict", "=", "update_f1_dict", "(", "unique_stemmed_trg_token_2dlist", ",", "filtered_stemmed_pred_token_2dlist", ",", "\n", "topk_dict", "[", "'all'", "]", ",", "f1_dict", ",", "'all'", ")", "\n", "# compute F1 score present", "\n", "f1_dict", "=", "update_f1_dict", "(", "present_unique_stemmed_trg_token_2dlist", ",", "\n", "present_filtered_stemmed_pred_token_2dlist", ",", "\n", "topk_dict", "[", "'present'", "]", ",", "f1_dict", ",", "'present'", ")", "\n", "# compute F1 score absent", "\n", "f1_dict", "=", "update_f1_dict", "(", "absent_unique_stemmed_trg_token_2dlist", ",", "\n", "absent_filtered_stemmed_pred_token_2dlist", ",", "\n", "topk_dict", "[", "'absent'", "]", ",", "f1_dict", ",", "'absent'", ")", "\n", "", "else", ":", "\n", "# compute all the metrics and update the score_dict", "\n", "                ", "score_dict", "=", "update_score_dict", "(", "unique_stemmed_trg_token_2dlist", ",", "filtered_stemmed_pred_token_2dlist", ",", "\n", "topk_dict", "[", "'all'", "]", ",", "score_dict", ",", "'all'", ")", "\n", "# compute all the metrics and update the score_dict for present keyphrase", "\n", "score_dict", "=", "update_score_dict", "(", "present_unique_stemmed_trg_token_2dlist", ",", "\n", "present_filtered_stemmed_pred_token_2dlist", ",", "\n", "topk_dict", "[", "'present'", "]", ",", "score_dict", ",", "'present'", ")", "\n", "# compute all the metrics and update the score_dict for present keyphrase", "\n", "score_dict", "=", "update_score_dict", "(", "absent_unique_stemmed_trg_token_2dlist", ",", "\n", "absent_filtered_stemmed_pred_token_2dlist", ",", "\n", "topk_dict", "[", "'absent'", "]", ",", "score_dict", ",", "'absent'", ")", "\n", "# compute fg metrics and update the score_dict for present keyphrase", "\n", "#print(\"@@@@@@@@@@@@@@@@@@@@@@@@over fg score@@@@@@@@@@@@@@@@@@@@@@\\n\")", "\n", "fg_score", ",", "token_f1", ",", "token_p", ",", "token_r", ",", "tmp_stat", ",", "npp", "=", "compute_fg_score_k", "(", "present_unique_stemmed_trg_token_2dlist", ",", "present_filtered_stemmed_pred_token_2dlist", ",", "eva", "=", "1", ")", "\n", "total_fg_score", "+=", "fg_score", "\n", "total_token_f1", "+=", "token_f1", "\n", "total_token_p", "+=", "token_p", "\n", "total_token_r", "+=", "token_r", "\n", "cur_score", "=", "fg_score", "\n", "if", "cur_score", "==", "0.", ":", "\n", "                    ", "stat_total", "[", "0", "]", "+=", "1", "\n", "", "elif", "0", "<", "cur_score", "<=", "0.1", ":", "\n", "                    ", "stat_total", "[", "1", "]", "+=", "1", "\n", "", "elif", "0", "<", "cur_score", "<=", "0.2", ":", "\n", "                    ", "stat_total", "[", "2", "]", "+=", "1", "\n", "", "elif", "0", "<", "cur_score", "<=", "0.3", ":", "\n", "                    ", "stat_total", "[", "3", "]", "+=", "1", "\n", "", "elif", "0", "<", "cur_score", "<=", "0.4", ":", "\n", "                    ", "stat_total", "[", "4", "]", "+=", "1", "\n", "", "elif", "0", "<", "cur_score", "<=", "0.5", ":", "\n", "                    ", "stat_total", "[", "5", "]", "+=", "1", "\n", "", "elif", "0", "<", "cur_score", "<=", "0.6", ":", "\n", "                    ", "stat_total", "[", "6", "]", "+=", "1", "\n", "", "elif", "0", "<", "cur_score", "<=", "0.7", ":", "\n", "                    ", "stat_total", "[", "7", "]", "+=", "1", "\n", "", "elif", "0", "<", "cur_score", "<=", "0.8", ":", "\n", "                    ", "stat_total", "[", "8", "]", "+=", "1", "\n", "", "elif", "0", "<", "cur_score", "<=", "0.9", ":", "\n", "                    ", "stat_total", "[", "9", "]", "+=", "1", "\n", "", "elif", "0", "<", "cur_score", "<", "1.", ":", "\n", "                    ", "stat_total", "[", "10", "]", "+=", "1", "\n", "", "elif", "cur_score", "==", "1.", ":", "\n", "                    ", "stat_total", "[", "11", "]", "+=", "1", "\n", "\n", "", "stat_token_f1", "=", "[", "i", "+", "j", "for", "i", ",", "j", "in", "zip", "(", "stat_token_f1", ",", "tmp_stat", ")", "]", "\n", "\n", "# if token_f1 == 0.:", "\n", "#     stat_token_f1[0] += 1", "\n", "# elif 0 < token_f1 <= 0.1:", "\n", "#     stat_token_f1[1] += 1", "\n", "# elif 0 < token_f1 <= 0.2:", "\n", "#     stat_token_f1[2] += 1", "\n", "# elif 0 < token_f1 <= 0.3:", "\n", "#     stat_token_f1[3] += 1", "\n", "# elif 0 < token_f1 <= 0.4:", "\n", "#     stat_token_f1[4] += 1", "\n", "# elif 0 < token_f1 <= 0.5:", "\n", "#     stat_token_f1[5] += 1", "\n", "# elif 0 < token_f1 <= 0.6:", "\n", "#     stat_token_f1[6] += 1", "\n", "# elif 0 < token_f1 <= 0.7:", "\n", "#     stat_token_f1[7] += 1", "\n", "# elif 0 < token_f1 <= 0.8:", "\n", "#     stat_token_f1[8] += 1", "\n", "# elif 0 < token_f1 <= 0.9:", "\n", "#     stat_token_f1[9] += 1", "\n", "# elif 0 < token_f1 < 1.:", "\n", "#     stat_token_f1[10] += 1", "\n", "# elif token_f1 == 1.:", "\n", "#     stat_token_f1[11] += 1", "\n", "\n", "\n", "\n", "\n", "fg_calc_times", "+=", "1", "\n", "tot_phrase", "+=", "npp", "\n", "\n", "", "", "\"\"\"\n        if opt.tune_f1_v:\n            # compute F1 score all\n            f1_dict = update_f1_dict(unique_stemmed_trg_token_2dlist, filtered_stemmed_pred_token_2dlist,\n                                           topk_dict['all'], f1_dict, 'all')\n            # compute F1 score present\n            f1_dict = update_f1_dict(present_unique_stemmed_trg_token_2dlist, present_filtered_stemmed_pred_token_2dlist,\n                                           topk_dict['present'], f1_dict, 'present')\n            # compute F1 score absent\n            f1_dict = update_f1_dict(absent_unique_stemmed_trg_token_2dlist, absent_filtered_stemmed_pred_token_2dlist,\n                                           topk_dict['absent'], f1_dict, 'absent')\n        else:\n            # compute all the metrics and update the score_dict\n            score_dict = update_score_dict(unique_stemmed_trg_token_2dlist, filtered_stemmed_pred_token_2dlist,\n                                           topk_dict['all'], score_dict, 'all')\n            # compute all the metrics and update the score_dict for present keyphrase\n            score_dict = update_score_dict(present_unique_stemmed_trg_token_2dlist, present_filtered_stemmed_pred_token_2dlist,\n                                           topk_dict['present'], score_dict, 'present')\n            # compute all the metrics and update the score_dict for present keyphrase\n            score_dict = update_score_dict(absent_unique_stemmed_trg_token_2dlist, absent_filtered_stemmed_pred_token_2dlist,\n                                           topk_dict['absent'], score_dict, 'absent')\n        \"\"\"", "\n", "if", "opt", ".", "export_filtered_pred", ":", "\n", "            ", "final_pred_str_list", "=", "[", "]", "\n", "for", "word_list", "in", "filtered_stemmed_pred_token_2dlist", ":", "\n", "                ", "final_pred_str_list", ".", "append", "(", "' '", ".", "join", "(", "word_list", ")", ")", "\n", "", "pred_print_out", "=", "';'", ".", "join", "(", "final_pred_str_list", ")", "+", "'\\n'", "\n", "pred_output_file", ".", "write", "(", "pred_print_out", ")", "\n", "\n", "", "", "if", "opt", ".", "export_filtered_pred", ":", "\n", "        ", "pred_output_file", ".", "close", "(", ")", "\n", "\n", "", "if", "opt", ".", "tune_f1_v", ":", "\n", "        ", "v_all", "=", "find_v", "(", "f1_dict", ",", "total_num_src", ",", "topk_dict", "[", "'all'", "]", ",", "'all'", ")", "\n", "print", "(", "\"V for all {}\"", ".", "format", "(", "v_all", ")", ")", "\n", "v_present", "=", "find_v", "(", "f1_dict", ",", "total_num_src", ",", "topk_dict", "[", "'present'", "]", ",", "'present'", ")", "\n", "print", "(", "\"V for present {}\"", ".", "format", "(", "v_present", ")", ")", "\n", "v_absent", "=", "find_v", "(", "f1_dict", ",", "total_num_src", ",", "topk_dict", "[", "'absent'", "]", ",", "'absent'", ")", "\n", "print", "(", "\"V for absent {}\"", ".", "format", "(", "v_absent", ")", ")", "\n", "v_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "exp_path", ",", "\"tune_v_output.txt\"", ")", ",", "\"w\"", ")", "\n", "v_file", ".", "write", "(", "\"V for all {}\\n\"", ".", "format", "(", "v_all", ")", ")", "\n", "v_file", ".", "write", "(", "\"V for present {}\\n\"", ".", "format", "(", "v_present", ")", ")", "\n", "v_file", ".", "write", "(", "\"V for absent {}\\n\"", ".", "format", "(", "v_absent", ")", ")", "\n", "v_file", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "        ", "total_num_unique_targets", "=", "total_num_present_unique_targets", "+", "total_num_absent_unique_targets", "\n", "total_num_filtered_predictions", "=", "total_num_present_filtered_predictions", "+", "total_num_absent_filtered_predictions", "\n", "\n", "result_txt_str", "=", "\"\"", "\n", "\n", "# report global statistics", "\n", "result_txt_str", "+=", "(", "'Total #samples: %d\\t # samples with present keyphrases: %d\\t # samples with absent keyphrases: %d\\n'", "%", "(", "total_num_src", ",", "total_num_src_with_present_keyphrases", ",", "total_num_src_with_absent_keyphrases", ")", ")", "\n", "result_txt_str", "+=", "(", "'Max. unique targets per src: %d\\n'", "%", "(", "max_unique_targets", ")", ")", "\n", "result_txt_str", "+=", "(", "'Total #unique predictions: %d\\n'", "%", "total_num_unique_predictions", ")", "\n", "\n", "# report statistics and scores for all predictions and targets", "\n", "result_txt_str_all", ",", "field_list_all", ",", "result_list_all", "=", "report_stat_and_scores", "(", "total_num_filtered_predictions", ",", "total_num_unique_targets", ",", "total_num_src", ",", "score_dict", ",", "topk_dict", "[", "'all'", "]", ",", "'all'", ",", "opt", ".", "use_name_variations", ")", "\n", "result_txt_str_present", ",", "field_list_present", ",", "result_list_present", "=", "report_stat_and_scores", "(", "total_num_present_filtered_predictions", ",", "total_num_present_unique_targets", ",", "total_num_src", ",", "score_dict", ",", "topk_dict", "[", "'present'", "]", ",", "'present'", ",", "opt", ".", "use_name_variations", ")", "\n", "result_txt_str_absent", ",", "field_list_absent", ",", "result_list_absent", "=", "report_stat_and_scores", "(", "total_num_absent_filtered_predictions", ",", "total_num_absent_unique_targets", ",", "total_num_src", ",", "score_dict", ",", "topk_dict", "[", "'absent'", "]", ",", "'absent'", ",", "opt", ".", "use_name_variations", ")", "\n", "result_txt_str", "+=", "(", "result_txt_str_all", "+", "result_txt_str_present", "+", "result_txt_str_absent", ")", "\n", "field_list", "=", "field_list_all", "+", "field_list_present", "+", "field_list_absent", "\n", "result_list", "=", "result_list_all", "+", "result_list_present", "+", "result_list_absent", "\n", "\n", "# Write to files", "\n", "# topk_dict = {'present': [5, 10, 'M'], 'absent': [5, 10, 50, 'M'], 'all': [5, 10, 'M']}", "\n", "k_list", "=", "topk_dict", "[", "'all'", "]", "+", "topk_dict", "[", "'present'", "]", "+", "topk_dict", "[", "'absent'", "]", "\n", "result_file_suffix", "=", "'_'", ".", "join", "(", "[", "str", "(", "k", ")", "for", "k", "in", "k_list", "]", ")", "\n", "if", "opt", ".", "meng_rui_precision", ":", "\n", "            ", "result_file_suffix", "+=", "'_meng_rui_precision'", "\n", "", "if", "opt", ".", "use_name_variations", ":", "\n", "            ", "result_file_suffix", "+=", "'_name_variations'", "\n", "", "results_txt_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "exp_path", ",", "\"results_log_{}.txt\"", ".", "format", "(", "result_file_suffix", ")", ")", ",", "\"w\"", ")", "\n", "if", "opt", ".", "prediction_separated", ":", "\n", "            ", "result_txt_str", "+=", "\"===================================Separation====================================\\n\"", "\n", "result_txt_str", "+=", "\"Avg error fraction for identifying present keyphrases: {:.5}\\n\"", ".", "format", "(", "sum_incorrect_fraction_for_identifying_present", "/", "total_num_src", ")", "\n", "result_txt_str", "+=", "\"Avg error fraction for identifying absent keyphrases: {:.5}\\n\"", ".", "format", "(", "sum_incorrect_fraction_for_identifying_absent", "/", "total_num_src", ")", "\n", "\n", "# Report MAE on lengths", "\n", "", "result_txt_str", "+=", "\"===================================MAE stat====================================\\n\"", "\n", "\n", "num_targets_present_array", "=", "np", ".", "array", "(", "score_dict", "[", "'num_targets_present'", "]", ")", "\n", "num_predictions_present_array", "=", "np", ".", "array", "(", "score_dict", "[", "'num_predictions_present'", "]", ")", "\n", "num_targets_absent_array", "=", "np", ".", "array", "(", "score_dict", "[", "'num_targets_absent'", "]", ")", "\n", "num_predictions_absent_array", "=", "np", ".", "array", "(", "score_dict", "[", "'num_predictions_absent'", "]", ")", "\n", "\n", "all_mae", "=", "mae", "(", "num_targets_present_array", "+", "num_targets_absent_array", ",", "num_predictions_present_array", "+", "num_predictions_absent_array", ")", "\n", "present_mae", "=", "mae", "(", "num_targets_present_array", ",", "num_predictions_present_array", ")", "\n", "absent_mae", "=", "mae", "(", "num_targets_absent_array", ",", "num_predictions_absent_array", ")", "\n", "\n", "result_txt_str", "+=", "\"MAE on keyphrase numbers (all): {:.5}\\n\"", ".", "format", "(", "all_mae", ")", "\n", "result_txt_str", "+=", "\"MAE on keyphrase numbers (present): {:.5}\\n\"", ".", "format", "(", "present_mae", ")", "\n", "result_txt_str", "+=", "\"MAE on keyphrase numbers (absent): {:.5}\\n\"", ".", "format", "(", "absent_mae", ")", "\n", "\n", "### fg_score", "\n", "final_fg_score", "=", "total_fg_score", "/", "max", "(", "fg_calc_times", ",", "1", ")", "\n", "result_txt_str", "+=", "\"FG_score:{:.5}\\n\"", ".", "format", "(", "final_fg_score", ")", "\n", "result_txt_str", "+=", "f\"FG dis: {str(stat_total)}\\n\"", "\n", "final_token_f1", "=", "total_token_f1", "/", "max", "(", "tot_phrase", ",", "1", ")", "\n", "final_token_p", "=", "total_token_p", "/", "max", "(", "tot_phrase", ",", "1", ")", "\n", "final_token_r", "=", "total_token_r", "/", "max", "(", "tot_phrase", ",", "1", ")", "\n", "result_txt_str", "+=", "\"token_f1:{:.5}\\n\"", ".", "format", "(", "final_token_f1", ")", "\n", "result_txt_str", "+=", "\"token_p:{:.5}\\n\"", ".", "format", "(", "final_token_p", ")", "\n", "result_txt_str", "+=", "\"token_r:{:.5}\\n\"", ".", "format", "(", "final_token_r", ")", "\n", "result_txt_str", "+=", "f\"FG dis: {str(stat_token_f1)}\\n\"", "\n", "###", "\n", "results_txt_file", ".", "write", "(", "result_txt_str", ")", "\n", "results_txt_file", ".", "close", "(", ")", "\n", "\n", "results_tsv_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "exp_path", ",", "\"results_log_{}.tsv\"", ".", "format", "(", "result_file_suffix", ")", ")", ",", "\"w\"", ")", "\n", "results_tsv_file", ".", "write", "(", "'\\t'", ".", "join", "(", "field_list", ")", "+", "'\\n'", ")", "\n", "results_tsv_file", ".", "write", "(", "'\\t'", ".", "join", "(", "'%.5f'", "%", "result", "for", "result", "in", "result_list", ")", "+", "'\\n'", ")", "\n", "results_tsv_file", ".", "close", "(", ")", "\n", "\n", "# save score dict for future use", "\n", "score_dict_pickle", "=", "open", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "exp_path", ",", "\"score_dict_{}.pickle\"", ".", "format", "(", "result_file_suffix", ")", ")", ",", "\"wb\"", ")", "\n", "pickle", ".", "dump", "(", "score_dict", ",", "score_dict_pickle", ")", "\n", "score_dict_pickle", ".", "close", "(", ")", "\n", "\n", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.rmse": [[2389, 2391], ["numpy.sqrt"], "function", ["None"], ["", "def", "rmse", "(", "a", ",", "b", ")", ":", "\n", "    ", "return", "np", ".", "sqrt", "(", "(", "(", "a", "-", "b", ")", "**", "2", ")", ".", "mean", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.mae": [[2393, 2395], ["numpy.abs().mean", "numpy.abs"], "function", ["None"], ["", "def", "mae", "(", "a", ",", "b", ")", ":", "\n", "    ", "return", "(", "np", ".", "abs", "(", "a", "-", "b", ")", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.report_stat_and_scores": [[2397, 2418], ["evaluate_prediction.report_classification_scores", "evaluate_prediction.report_ranking_scores"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.report_classification_scores", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.report_ranking_scores"], ["", "def", "report_stat_and_scores", "(", "total_num_filtered_predictions", ",", "num_unique_trgs", ",", "total_num_src", ",", "score_dict", ",", "topk_list", ",", "present_tag", ",", "use_name_variations", "=", "False", ")", ":", "\n", "    ", "result_txt_str", "=", "\"===================================%s====================================\\n\"", "%", "(", "present_tag", ")", "\n", "result_txt_str", "+=", "\"#predictions after filtering: %d\\t #predictions after filtering per src:%.3f\\n\"", "%", "(", "total_num_filtered_predictions", ",", "total_num_filtered_predictions", "/", "total_num_src", ")", "\n", "result_txt_str", "+=", "\"#unique targets: %d\\t #unique targets per src:%.3f\\n\"", "%", "(", "num_unique_trgs", ",", "num_unique_trgs", "/", "total_num_src", ")", "\n", "\n", "classification_output_str", ",", "classification_field_list", ",", "classification_result_list", "=", "report_classification_scores", "(", "\n", "score_dict", ",", "topk_list", ",", "present_tag", ")", "\n", "result_txt_str", "+=", "classification_output_str", "\n", "field_list", "=", "classification_field_list", "\n", "result_list", "=", "classification_result_list", "\n", "\n", "if", "not", "use_name_variations", ":", "\n", "        ", "ranking_output_str", ",", "ranking_field_list", ",", "ranking_result_list", "=", "report_ranking_scores", "(", "score_dict", ",", "\n", "topk_list", ",", "\n", "present_tag", ")", "\n", "result_txt_str", "+=", "ranking_output_str", "\n", "field_list", "+=", "ranking_field_list", "\n", "result_list", "+=", "ranking_result_list", "\n", "", "return", "result_txt_str", ",", "field_list", ",", "result_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.report_classification_scores": [[2420, 2444], ["sum", "sum", "sum", "evaluate_prediction.compute_classification_metrics", "sum", "len", "sum", "len"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics"], ["", "def", "report_classification_scores", "(", "score_dict", ",", "topk_list", ",", "present_tag", ")", ":", "\n", "    ", "output_str", "=", "\"\"", "\n", "result_list", "=", "[", "]", "\n", "field_list", "=", "[", "]", "\n", "for", "topk", "in", "topk_list", ":", "\n", "        ", "total_predictions_k", "=", "sum", "(", "score_dict", "[", "'num_predictions@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "\n", "total_targets_k", "=", "sum", "(", "score_dict", "[", "'num_targets@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "\n", "total_num_matches_k", "=", "sum", "(", "score_dict", "[", "'num_matches@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "\n", "# Compute the micro averaged recall, precision and F-1 score", "\n", "micro_avg_precision_k", ",", "micro_avg_recall_k", ",", "micro_avg_f1_score_k", "=", "compute_classification_metrics", "(", "\n", "total_num_matches_k", ",", "total_predictions_k", ",", "total_targets_k", ")", "\n", "# Compute the macro averaged recall, precision and F-1 score", "\n", "macro_avg_precision_k", "=", "sum", "(", "score_dict", "[", "'precision@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "/", "len", "(", "\n", "score_dict", "[", "'precision@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "\n", "macro_avg_recall_k", "=", "sum", "(", "score_dict", "[", "'recall@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "/", "len", "(", "\n", "score_dict", "[", "'recall@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "\n", "macro_avg_f1_score_k", "=", "(", "2", "*", "macro_avg_precision_k", "*", "macro_avg_recall_k", ")", "/", "(", "macro_avg_precision_k", "+", "macro_avg_recall_k", ")", "if", "(", "macro_avg_precision_k", "+", "macro_avg_recall_k", ")", ">", "0", "else", "0.0", "\n", "output_str", "+=", "(", "\"Begin===============classification metrics {}@{}===============Begin\\n\"", ".", "format", "(", "present_tag", ",", "topk", ")", ")", "\n", "output_str", "+=", "(", "\"#target: {}, #predictions: {}, #corrects: {}\\n\"", ".", "format", "(", "total_predictions_k", ",", "total_targets_k", ",", "total_num_matches_k", ")", ")", "\n", "output_str", "+=", "\"Micro:\\tP@{}={:.5}\\tR@{}={:.5}\\tF1@{}={:.5}\\n\"", ".", "format", "(", "topk", ",", "micro_avg_precision_k", ",", "topk", ",", "micro_avg_recall_k", ",", "topk", ",", "micro_avg_f1_score_k", ")", "\n", "output_str", "+=", "\"Macro:\\tP@{}={:.5}\\tR@{}={:.5}\\tF1@{}={:.5}\\n\"", ".", "format", "(", "topk", ",", "macro_avg_precision_k", ",", "topk", ",", "macro_avg_recall_k", ",", "topk", ",", "macro_avg_f1_score_k", ")", "\n", "field_list", "+=", "[", "'macro_avg_p@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", ",", "'macro_avg_r@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", ",", "'macro_avg_f1@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", "\n", "result_list", "+=", "[", "macro_avg_precision_k", ",", "macro_avg_recall_k", ",", "macro_avg_f1_score_k", "]", "\n", "", "return", "output_str", ",", "field_list", ",", "result_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.report_ranking_scores": [[2446, 2472], ["sum", "len", "sum", "len", "sum", "len"], "function", ["None"], ["", "def", "report_ranking_scores", "(", "score_dict", ",", "topk_list", ",", "present_tag", ")", ":", "\n", "    ", "output_str", "=", "\"\"", "\n", "result_list", "=", "[", "]", "\n", "field_list", "=", "[", "]", "\n", "for", "topk", "in", "topk_list", ":", "\n", "        ", "map_k", "=", "sum", "(", "score_dict", "[", "'AP@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "/", "len", "(", "\n", "score_dict", "[", "'AP@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "\n", "#avg_dcg_k = sum(score_dict['DCG@{}_{}'.format(topk, present_tag)]) / len(", "\n", "#    score_dict['DCG@{}_{}'.format(topk, present_tag)])", "\n", "avg_ndcg_k", "=", "sum", "(", "score_dict", "[", "'NDCG@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "/", "len", "(", "\n", "score_dict", "[", "'NDCG@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "\n", "#avg_alpha_dcg_k = sum(score_dict['AlphaDCG@{}_{}'.format(topk, present_tag)]) / len(", "\n", "#    score_dict['AlphaDCG@{}_{}'.format(topk, present_tag)])", "\n", "avg_alpha_ndcg_k", "=", "sum", "(", "score_dict", "[", "'AlphaNDCG@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "/", "len", "(", "\n", "score_dict", "[", "'AlphaNDCG@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", ")", "\n", "output_str", "+=", "(", "\"Begin==================Ranking metrics {}@{}==================Begin\\n\"", ".", "format", "(", "present_tag", ",", "topk", ")", ")", "\n", "#output_str += \"Relevance:\\tDCG@{}={:.5}\\tNDCG@{}={:.5}\\tMAP@{}={:.5}\\n\".format(topk, avg_dcg_k, topk, avg_ndcg_k, topk, map_k)", "\n", "#output_str += \"Diversity:\\tAlphaDCG@{}={:.5}\\tAlphaNDCG@{}={:.5}\\n\".format(topk, avg_alpha_dcg_k, topk, avg_alpha_ndcg_k)", "\n", "#field_list += ['avg_DCG@{}_{}'.format(topk, present_tag), 'avg_NDCG@{}_{}'.format(topk, present_tag),", "\n", "#               'MAP@{}_{}'.format(topk, present_tag), 'AlphaDCG@{}_{}'.format(topk, present_tag), 'AlphaNDCG@{}_{}'.format(topk, present_tag)]", "\n", "#result_list += [avg_dcg_k, avg_ndcg_k, map_k, avg_alpha_dcg_k, avg_alpha_ndcg_k]", "\n", "output_str", "+=", "\"\\tMAP@{}={:.5}\\tNDCG@{}={:.5}\\tAlphaNDCG@{}={:.5}\\n\"", ".", "format", "(", "topk", ",", "map_k", ",", "topk", ",", "avg_ndcg_k", ",", "topk", ",", "avg_alpha_ndcg_k", ")", "\n", "field_list", "+=", "[", "'MAP@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", ",", "'avg_NDCG@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", ",", "'AlphaNDCG@{}_{}'", ".", "format", "(", "topk", ",", "present_tag", ")", "]", "\n", "result_list", "+=", "[", "map_k", ",", "avg_ndcg_k", ",", "avg_alpha_ndcg_k", "]", "\n", "\n", "", "return", "output_str", ",", "field_list", ",", "result_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.LossStatistics.__init__": [[10, 21], ["math.isnan", "type", "ValueError", "type", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "loss", "=", "0.0", ",", "n_tokens", "=", "0", ",", "n_batch", "=", "0", ",", "forward_time", "=", "0.0", ",", "loss_compute_time", "=", "0.0", ",", "backward_time", "=", "0.0", ")", ":", "\n", "        ", "assert", "type", "(", "loss", ")", "is", "float", "or", "type", "(", "loss", ")", "is", "int", "\n", "assert", "type", "(", "n_tokens", ")", "is", "int", "\n", "self", ".", "loss", "=", "loss", "\n", "if", "math", ".", "isnan", "(", "loss", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Loss is NaN\"", ")", "\n", "", "self", ".", "n_tokens", "=", "n_tokens", "\n", "self", ".", "n_batch", "=", "n_batch", "\n", "self", ".", "forward_time", "=", "forward_time", "\n", "self", ".", "loss_compute_time", "=", "loss_compute_time", "\n", "self", ".", "backward_time", "=", "backward_time", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.LossStatistics.update": [[22, 37], ["math.isnan", "ValueError"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "stat", ",", "update_n_src_words", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Update statistics by suming values with another `LossStatistics` object\n\n        Args:\n            stat: another statistic object\n        \"\"\"", "\n", "self", ".", "loss", "+=", "stat", ".", "loss", "\n", "if", "math", ".", "isnan", "(", "stat", ".", "loss", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Loss is NaN\"", ")", "\n", "", "self", ".", "n_tokens", "+=", "stat", ".", "n_tokens", "\n", "self", ".", "n_batch", "+=", "stat", ".", "n_batch", "\n", "self", ".", "forward_time", "+=", "stat", ".", "forward_time", "\n", "self", ".", "loss_compute_time", "+=", "stat", ".", "loss_compute_time", "\n", "self", ".", "backward_time", "+=", "stat", ".", "backward_time", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.LossStatistics.xent": [[38, 42], ["None"], "methods", ["None"], ["", "def", "xent", "(", "self", ")", ":", "\n", "        ", "\"\"\" compute normalized cross entropy \"\"\"", "\n", "assert", "self", ".", "n_tokens", ">", "0", ",", "\"n_tokens must be larger than 0\"", "\n", "return", "self", ".", "loss", "/", "self", ".", "n_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.LossStatistics.ppl": [[43, 47], ["math.exp", "min"], "methods", ["None"], ["", "def", "ppl", "(", "self", ")", ":", "\n", "        ", "\"\"\" compute normalized perplexity \"\"\"", "\n", "assert", "self", ".", "n_tokens", ">", "0", ",", "\"n_tokens must be larger than 0\"", "\n", "return", "math", ".", "exp", "(", "min", "(", "self", ".", "loss", "/", "self", ".", "n_tokens", ",", "100", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.LossStatistics.total_time": [[48, 50], ["None"], "methods", ["None"], ["", "def", "total_time", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "forward_time", ",", "self", ".", "loss_compute_time", ",", "self", ".", "backward_time", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.LossStatistics.clear": [[51, 58], ["None"], "methods", ["None"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "self", ".", "loss", "=", "0.0", "\n", "self", ".", "n_tokens", "=", "0", "\n", "self", ".", "n_batch", "=", "0", "\n", "self", ".", "forward_time", "=", "0.0", "\n", "self", ".", "loss_compute_time", "=", "0.0", "\n", "self", ".", "backward_time", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.__init__": [[64, 73], ["math.isnan", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "final_reward", "=", "0.0", ",", "pg_loss", "=", "0.0", ",", "n_batch", "=", "0", ",", "sample_time", "=", "0", ",", "q_estimate_compute_time", "=", "0", ",", "backward_time", "=", "0", ")", ":", "\n", "        ", "self", ".", "final_reward", "=", "final_reward", "\n", "self", ".", "pg_loss", "=", "pg_loss", "\n", "if", "math", ".", "isnan", "(", "pg_loss", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Policy gradient loss is NaN\"", ")", "\n", "", "self", ".", "n_batch", "=", "n_batch", "\n", "self", ".", "sample_time", "=", "sample_time", "\n", "self", ".", "q_estimate_compute_time", "=", "q_estimate_compute_time", "\n", "self", ".", "backward_time", "=", "backward_time", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.update": [[74, 83], ["math.isnan", "ValueError"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "stat", ")", ":", "\n", "        ", "self", ".", "final_reward", "+=", "stat", ".", "final_reward", "\n", "if", "math", ".", "isnan", "(", "stat", ".", "pg_loss", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Policy gradient loss is NaN\"", ")", "\n", "", "self", ".", "pg_loss", "+=", "stat", ".", "pg_loss", "\n", "self", ".", "n_batch", "+=", "stat", ".", "n_batch", "\n", "self", ".", "sample_time", "+=", "stat", ".", "sample_time", "\n", "self", ".", "q_estimate_compute_time", "+=", "stat", ".", "q_estimate_compute_time", "\n", "self", ".", "backward_time", "+=", "stat", ".", "backward_time", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.reward": [[84, 87], ["None"], "methods", ["None"], ["", "def", "reward", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "n_batch", ">", "0", ",", "\"n_batch must be positive\"", "\n", "return", "self", ".", "final_reward", "/", "self", ".", "n_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.loss": [[88, 91], ["None"], "methods", ["None"], ["", "def", "loss", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "n_batch", ">", "0", ",", "\"n_batch must be positive\"", "\n", "return", "self", ".", "pg_loss", "/", "self", ".", "n_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.total_time": [[92, 94], ["None"], "methods", ["None"], ["", "def", "total_time", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sample_time", ",", "self", ".", "q_estimate_compute_time", ",", "self", ".", "backward_time", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.statistics.RewardStatistics.clear": [[95, 102], ["None"], "methods", ["None"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "self", ".", "final_reward", "=", "0.0", "\n", "self", ".", "pg_loss", "=", "0.0", "\n", "self", ".", "n_batch", "=", "0.0", "\n", "self", ".", "sample_time", "=", "0.0", "\n", "self", ".", "q_estimate_compute_time", "=", "0.0", "\n", "self", ".", "backward_time", "=", "0.0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.__init__": [[5, 9], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "capacity", ")", ":", "\n", "        ", "self", ".", "capacity", "=", "capacity", "\n", "self", ".", "queue", "=", "[", "]", "\n", "self", ".", "position", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.put": [[10, 15], ["len", "source_representation_queue.SourceRepresentationQueue.queue.append"], "methods", ["None"], ["", "def", "put", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "queue", ")", "<", "self", ".", "capacity", ":", "\n", "            ", "self", ".", "queue", ".", "append", "(", "None", ")", "\n", "", "self", ".", "queue", "[", "self", ".", "position", "]", "=", "tensor", "\n", "self", ".", "position", "=", "(", "self", ".", "position", "+", "1", ")", "%", "self", ".", "capacity", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.sample": [[16, 22], ["numpy.random.choice", "len", "len"], "methods", ["None"], ["", "def", "sample", "(", "self", ",", "sample_size", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "queue", ")", "<", "sample_size", ":", "\n", "            ", "return", "None", "\n", "# return random.sample(self.queue, sample_size)", "\n", "", "idxs", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "self", ".", "queue", ")", ",", "sample_size", ",", "replace", "=", "False", ")", "\n", "return", "[", "self", ".", "queue", "[", "i", "]", "for", "i", "in", "idxs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.__len__": [[23, 25], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "queue", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.data_loader.load_vocab": [[7, 22], ["logging.info", "logging.info", "logging.info", "torch.load", "torch.load", "len"], "function", ["None"], ["def", "load_vocab", "(", "opt", ")", ":", "\n", "# load vocab", "\n", "    ", "logging", ".", "info", "(", "\"Loading vocab from disk: %s\"", "%", "(", "opt", ".", "vocab", ")", ")", "\n", "if", "not", "opt", ".", "custom_vocab_filename_suffix", ":", "\n", "        ", "word2idx", ",", "idx2word", ",", "vocab", "=", "torch", ".", "load", "(", "opt", ".", "vocab", "+", "'/vocab.pt'", ",", "'wb'", ")", "\n", "", "else", ":", "\n", "        ", "word2idx", ",", "idx2word", ",", "vocab", "=", "torch", ".", "load", "(", "opt", ".", "vocab", "+", "'/vocab.%s.pt'", "%", "opt", ".", "vocab_filename_suffix", ",", "'wb'", ")", "\n", "# assign vocab to opt", "\n", "", "opt", ".", "word2idx", "=", "word2idx", "\n", "opt", ".", "idx2word", "=", "idx2word", "\n", "opt", ".", "vocab", "=", "vocab", "\n", "logging", ".", "info", "(", "'#(vocab)=%d'", "%", "len", "(", "vocab", ")", ")", "\n", "logging", ".", "info", "(", "'#(vocab used)=%d'", "%", "opt", ".", "vocab_size", ")", "\n", "\n", "return", "word2idx", ",", "idx2word", ",", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.data_loader.load_data_and_vocab": [[24, 96], ["data_loader.load_vocab", "logging.info", "pykp.io.KeyphraseDataset", "torch.utils.data.DataLoader", "logging.info", "pykp.io.KeyphraseDataset", "torch.utils.data.DataLoader", "logging.info", "pykp.io.KeyphraseDataset", "torch.utils.data.DataLoader", "logging.info", "pykp.io.KeyphraseDataset", "torch.utils.data.DataLoader", "logging.info", "pykp.io.KeyphraseDataset", "torch.utils.data.DataLoader", "logging.info", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.data_loader.load_vocab"], ["", "def", "load_data_and_vocab", "(", "opt", ",", "load_train", "=", "True", ")", ":", "\n", "# load vocab", "\n", "    ", "word2idx", ",", "idx2word", ",", "vocab", "=", "load_vocab", "(", "opt", ")", "\n", "\n", "# constructor data loader", "\n", "logging", ".", "info", "(", "\"Loading train and validate data from '%s'\"", "%", "opt", ".", "data", ")", "\n", "\n", "if", "load_train", ":", "# load training dataset", "\n", "        ", "if", "not", "opt", ".", "one2many", ":", "# load one2one dataset", "\n", "            ", "if", "not", "opt", ".", "custom_data_filename_suffix", ":", "\n", "                ", "train_one2one", "=", "torch", ".", "load", "(", "opt", ".", "data", "+", "'/train.one2one.pt'", ",", "'wb'", ")", "\n", "", "else", ":", "\n", "                ", "train_one2one", "=", "torch", ".", "load", "(", "opt", ".", "data", "+", "'/train.one2one.%s.pt'", "%", "opt", ".", "data_filename_suffix", ",", "'wb'", ")", "\n", "", "train_one2one_dataset", "=", "KeyphraseDataset", "(", "train_one2one", ",", "word2idx", "=", "word2idx", ",", "idx2word", "=", "idx2word", ",", "type", "=", "'one2one'", ",", "load_train", "=", "load_train", ",", "remove_src_eos", "=", "opt", ".", "remove_src_eos", ",", "title_guided", "=", "opt", ".", "title_guided", ")", "\n", "train_loader", "=", "DataLoader", "(", "dataset", "=", "train_one2one_dataset", ",", "\n", "collate_fn", "=", "train_one2one_dataset", ".", "collate_fn_one2one", ",", "\n", "num_workers", "=", "opt", ".", "batch_workers", ",", "batch_size", "=", "opt", ".", "batch_size", ",", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "True", ")", "\n", "logging", ".", "info", "(", "'#(train data size: #(batch)=%d'", "%", "(", "len", "(", "train_loader", ")", ")", ")", "\n", "\n", "if", "not", "opt", ".", "custom_data_filename_suffix", ":", "\n", "                ", "valid_one2one", "=", "torch", ".", "load", "(", "opt", ".", "data", "+", "'/valid.one2one.pt'", ",", "'wb'", ")", "\n", "", "else", ":", "\n", "                ", "valid_one2one", "=", "torch", ".", "load", "(", "opt", ".", "data", "+", "'/valid.one2one.%s.pt'", "%", "opt", ".", "data_filename_suffix", ",", "'wb'", ")", "\n", "", "valid_one2one_dataset", "=", "KeyphraseDataset", "(", "valid_one2one", ",", "word2idx", "=", "word2idx", ",", "idx2word", "=", "idx2word", ",", "\n", "type", "=", "'one2one'", ",", "load_train", "=", "load_train", ",", "remove_src_eos", "=", "opt", ".", "remove_src_eos", ",", "title_guided", "=", "opt", ".", "title_guided", ")", "\n", "valid_loader", "=", "DataLoader", "(", "dataset", "=", "valid_one2one_dataset", ",", "\n", "collate_fn", "=", "valid_one2one_dataset", ".", "collate_fn_one2one", ",", "\n", "num_workers", "=", "opt", ".", "batch_workers", ",", "batch_size", "=", "opt", ".", "batch_size", ",", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "False", ")", "\n", "logging", ".", "info", "(", "'#(valid data size: #(batch)=%d'", "%", "(", "len", "(", "valid_loader", ")", ")", ")", "\n", "\n", "\n", "", "else", ":", "# load one2many dataset", "\n", "            ", "if", "not", "opt", ".", "custom_data_filename_suffix", ":", "\n", "                ", "train_one2many", "=", "torch", ".", "load", "(", "opt", ".", "data", "+", "'/train.one2many.pt'", ",", "'wb'", ")", "\n", "", "else", ":", "\n", "                ", "train_one2many", "=", "torch", ".", "load", "(", "opt", ".", "data", "+", "'/train.one2many.%s.pt'", "%", "opt", ".", "data_filename_suffix", ",", "'wb'", ")", "\n", "", "train_one2many_dataset", "=", "KeyphraseDataset", "(", "train_one2many", ",", "word2idx", "=", "word2idx", ",", "idx2word", "=", "idx2word", ",", "type", "=", "'one2many'", ",", "delimiter_type", "=", "opt", ".", "delimiter_type", ",", "load_train", "=", "load_train", ",", "remove_src_eos", "=", "opt", ".", "remove_src_eos", ",", "title_guided", "=", "opt", ".", "title_guided", ")", "\n", "train_loader", "=", "DataLoader", "(", "dataset", "=", "train_one2many_dataset", ",", "\n", "collate_fn", "=", "train_one2many_dataset", ".", "collate_fn_one2many", ",", "\n", "num_workers", "=", "opt", ".", "batch_workers", ",", "batch_size", "=", "opt", ".", "batch_size", ",", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "True", ")", "\n", "logging", ".", "info", "(", "'#(train data size: #(batch)=%d'", "%", "(", "len", "(", "train_loader", ")", ")", ")", "\n", "\n", "if", "not", "opt", ".", "custom_data_filename_suffix", ":", "\n", "                ", "valid_one2many", "=", "torch", ".", "load", "(", "opt", ".", "data", "+", "'/valid.one2many.pt'", ",", "'wb'", ")", "\n", "", "else", ":", "\n", "                ", "valid_one2many", "=", "torch", ".", "load", "(", "opt", ".", "data", "+", "'/valid.one2many.%s.pt'", "%", "opt", ".", "data_filename_suffix", ",", "'wb'", ")", "\n", "#valid_one2many = valid_one2many[:2000]", "\n", "", "valid_one2many_dataset", "=", "KeyphraseDataset", "(", "valid_one2many", ",", "word2idx", "=", "word2idx", ",", "idx2word", "=", "idx2word", ",", "\n", "type", "=", "'one2many'", ",", "delimiter_type", "=", "opt", ".", "delimiter_type", ",", "load_train", "=", "load_train", ",", "remove_src_eos", "=", "opt", ".", "remove_src_eos", ",", "title_guided", "=", "opt", ".", "title_guided", ")", "\n", "valid_loader", "=", "DataLoader", "(", "dataset", "=", "valid_one2many_dataset", ",", "\n", "collate_fn", "=", "valid_one2many_dataset", ".", "collate_fn_one2many", ",", "\n", "num_workers", "=", "opt", ".", "batch_workers", ",", "batch_size", "=", "opt", ".", "batch_size", ",", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "False", ")", "\n", "logging", ".", "info", "(", "'#(valid data size: #(batch)=%d'", "%", "(", "len", "(", "valid_loader", ")", ")", ")", "\n", "", "return", "train_loader", ",", "valid_loader", ",", "word2idx", ",", "idx2word", ",", "vocab", "\n", "", "else", ":", "\n", "        ", "if", "not", "opt", ".", "custom_data_filename_suffix", ":", "\n", "            ", "test_one2many", "=", "torch", ".", "load", "(", "opt", ".", "data", "+", "'/test.one2many.pt'", ",", "'wb'", ")", "\n", "", "else", ":", "\n", "            ", "test_one2many", "=", "torch", ".", "load", "(", "opt", ".", "data", "+", "'/test.one2many.%s.pt'", "%", "opt", ".", "data_filename_suffix", ",", "'wb'", ")", "\n", "", "test_one2many_dataset", "=", "KeyphraseDataset", "(", "test_one2many", ",", "word2idx", "=", "word2idx", ",", "idx2word", "=", "idx2word", ",", "\n", "type", "=", "'one2many'", ",", "delimiter_type", "=", "opt", ".", "delimiter_type", ",", "load_train", "=", "load_train", ",", "remove_src_eos", "=", "opt", ".", "remove_src_eos", ",", "title_guided", "=", "opt", ".", "title_guided", ")", "\n", "test_loader", "=", "DataLoader", "(", "dataset", "=", "test_one2many_dataset", ",", "\n", "collate_fn", "=", "test_one2many_dataset", ".", "collate_fn_one2many", ",", "\n", "num_workers", "=", "opt", ".", "batch_workers", ",", "batch_size", "=", "opt", ".", "batch_size", ",", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "False", ")", "\n", "logging", ".", "info", "(", "'#(test data size: #(batch)=%d'", "%", "(", "len", "(", "test_loader", ")", ")", ")", "\n", "\n", "return", "test_loader", ",", "word2idx", ",", "idx2word", ",", "vocab", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.prediction_to_sentence": [[6, 34], ["enumerate", "int", "sentence.append", "pred.item", "attn_dist[].topk", "len", "len", "int", "int", "max_attn_idx[].item", "max_attn_idx[].item"], "function", ["None"], ["def", "prediction_to_sentence", "(", "prediction", ",", "idx2word", ",", "vocab_size", ",", "oov", ",", "eos_idx", ",", "unk_idx", "=", "None", ",", "replace_unk", "=", "False", ",", "src_word_list", "=", "None", ",", "attn_dist", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    :param prediction: a list of 0 dim tensor\n    :param attn_dist: tensor with size [trg_len, src_len]\n    :return: a list of words, does not include the final EOS\n    \"\"\"", "\n", "sentence", "=", "[", "]", "\n", "for", "i", ",", "pred", "in", "enumerate", "(", "prediction", ")", ":", "\n", "        ", "_pred", "=", "int", "(", "pred", ".", "item", "(", ")", ")", "# convert zero dim tensor to int", "\n", "if", "i", "==", "len", "(", "prediction", ")", "-", "1", "and", "_pred", "==", "eos_idx", ":", "# ignore the final EOS token", "\n", "            ", "break", "\n", "", "if", "_pred", "<", "vocab_size", ":", "\n", "            ", "if", "_pred", "==", "unk_idx", "and", "replace_unk", ":", "\n", "                ", "assert", "src_word_list", "is", "not", "None", "and", "attn_dist", "is", "not", "None", ",", "\"If you need to replace unk, you must supply src_word_list and attn_dist\"", "\n", "#_, max_attn_idx = attn_dist[i].max(0)", "\n", "_", ",", "max_attn_idx", "=", "attn_dist", "[", "i", "]", ".", "topk", "(", "2", ",", "dim", "=", "0", ")", "\n", "if", "max_attn_idx", "[", "0", "]", "<", "len", "(", "src_word_list", ")", ":", "\n", "                    ", "word", "=", "src_word_list", "[", "int", "(", "max_attn_idx", "[", "0", "]", ".", "item", "(", ")", ")", "]", "\n", "", "else", ":", "\n", "                    ", "word", "=", "src_word_list", "[", "int", "(", "max_attn_idx", "[", "1", "]", ".", "item", "(", ")", ")", "]", "\n", "#word = pykp.io.EOS_WORD", "\n", "", "", "else", ":", "\n", "                ", "word", "=", "idx2word", "[", "_pred", "]", "\n", "", "", "else", ":", "\n", "            ", "word", "=", "oov", "[", "_pred", "-", "vocab_size", "]", "\n", "", "sentence", ".", "append", "(", "word", ")", "\n", "\n", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_str_2d_list": [[36, 44], ["stemmed_str_2dlist.append", "string_helper.stem_word_list"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_word_list"], ["", "def", "stem_str_2d_list", "(", "str_2dlist", ")", ":", "\n", "# stem every word in a list of word list", "\n", "# str_list is a list of word list", "\n", "    ", "stemmed_str_2dlist", "=", "[", "]", "\n", "for", "str_list", "in", "str_2dlist", ":", "\n", "        ", "stemmed_str_list", "=", "[", "stem_word_list", "(", "word_list", ")", "for", "word_list", "in", "str_list", "]", "\n", "stemmed_str_2dlist", ".", "append", "(", "stemmed_str_list", ")", "\n", "", "return", "stemmed_str_2dlist", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_str_list": [[46, 54], ["string_helper.stem_word_list", "stemmed_str_list.append"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_word_list"], ["", "def", "stem_str_list", "(", "str_list", ")", ":", "\n", "# stem every word in a list of word list", "\n", "# str_list is a list of word list", "\n", "    ", "stemmed_str_list", "=", "[", "]", "\n", "for", "word_list", "in", "str_list", ":", "\n", "        ", "stemmed_word_list", "=", "stem_word_list", "(", "word_list", ")", "\n", "stemmed_str_list", ".", "append", "(", "stemmed_word_list", ")", "\n", "", "return", "stemmed_str_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_word_list": [[56, 58], ["stemmer.stem", "w.strip().lower", "w.strip"], "function", ["None"], ["", "def", "stem_word_list", "(", "word_list", ")", ":", "\n", "    ", "return", "[", "stemmer", ".", "stem", "(", "w", ".", "strip", "(", ")", ".", "lower", "(", ")", ")", "for", "w", "in", "word_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.split_word_list_by_delimiter": [[75, 104], ["len", "tmp_pred_str_list.append", "len", "tmp_pred_str_list.append", "tmp_pred_str_list.append", "tmp_word_list.append", "len", "tmp_pred_str_list.append"], "function", ["None"], ["def", "split_word_list_by_delimiter", "(", "word_list", ",", "keyphrase_delimiter", ",", "include_present_absent_delimiter", "=", "False", ",", "present_absent_delimiter", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert a word list into a list of keyphrase, each keyphrase is a word list.\n    :param word_list: word list of concated keyprhases, separated by a delimiter\n    :param keyphrase_delimiter\n    :param include_present_absent_delimiter: if it is true, the final list of keyphrase will include the present_absent_delimiter as one of the keyphrase, e.g., [['kp11', 'kp12'], ['<peos>'], ['kp21', 'kp22']]\n    :param present_absent_delimiter\n    :return: a list of keyphrase, each keyphrase is a word list.\n    \"\"\"", "\n", "if", "include_present_absent_delimiter", ":", "\n", "        ", "assert", "present_absent_delimiter", "is", "not", "None", "\n", "", "tmp_pred_str_list", "=", "[", "]", "\n", "tmp_word_list", "=", "[", "]", "\n", "for", "word", "in", "word_list", ":", "\n", "        ", "if", "word", "==", "keyphrase_delimiter", ":", "\n", "            ", "if", "len", "(", "tmp_word_list", ")", ">", "0", ":", "\n", "                ", "tmp_pred_str_list", ".", "append", "(", "tmp_word_list", ")", "\n", "tmp_word_list", "=", "[", "]", "\n", "", "", "elif", "word", "==", "present_absent_delimiter", "and", "include_present_absent_delimiter", ":", "\n", "            ", "if", "len", "(", "tmp_word_list", ")", ">", "0", ":", "\n", "                ", "tmp_pred_str_list", ".", "append", "(", "tmp_word_list", ")", "\n", "tmp_word_list", "=", "[", "]", "\n", "", "tmp_pred_str_list", ".", "append", "(", "[", "present_absent_delimiter", "]", ")", "\n", "", "else", ":", "\n", "            ", "tmp_word_list", ".", "append", "(", "word", ")", "\n", "\n", "", "", "if", "len", "(", "tmp_word_list", ")", ">", "0", ":", "# append the final keyphrase to the pred_str_list", "\n", "        ", "tmp_pred_str_list", ".", "append", "(", "tmp_word_list", ")", "\n", "", "return", "tmp_pred_str_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.report.export_train_and_valid_reward": [[7, 16], ["report.plot_train_valid_curve", "open", "range", "len", "result_csv.write", "report.concat_float_list"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.report.plot_train_valid_curve", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.report.concat_float_list"], ["def", "export_train_and_valid_reward", "(", "train_reward", ",", "valid_reward", ",", "plot_every", ",", "path", ")", ":", "\n", "# Export the results to a csv file", "\n", "    ", "labels", "=", "[", "'Training reward:,'", ",", "'Validation reward:,'", "]", "\n", "float_lists", "=", "[", "train_reward", ",", "valid_reward", "]", "\n", "with", "open", "(", "path", "+", "'.csv'", ",", "'w'", ")", "as", "result_csv", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "labels", ")", ")", ":", "\n", "            ", "result_csv", ".", "write", "(", "labels", "[", "i", "]", "+", "concat_float_list", "(", "float_lists", "[", "i", "]", ",", "','", ")", "+", "'\\n'", ")", "\n", "# Export the plots to pdf file", "\n", "", "", "plot_train_valid_curve", "(", "train_reward", ",", "valid_reward", ",", "plot_every", ",", "path", ",", "'Reward'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.report.export_train_and_valid_loss": [[18, 37], ["report.plot_train_valid_curve", "report.plot_train_valid_curve", "open", "range", "len", "result_csv.write", "report.concat_float_list"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.report.plot_train_valid_curve", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.report.plot_train_valid_curve", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.report.concat_float_list"], ["", "def", "export_train_and_valid_loss", "(", "train_loss", ",", "valid_loss", ",", "train_ppl", ",", "valid_ppl", ",", "plot_every", ",", "path", ")", ":", "\n", "    ", "\"\"\"\n    :param train_loss: a list of float\n    :param valid_loss: a list of float\n    :param train_ppl: a list of float\n    :param valid_ppl: a list of float\n    :param plot_every: int\n    :param path: str\n    :return:\n    \"\"\"", "\n", "# Export the results to a csv file", "\n", "labels", "=", "[", "'Training loss:,'", ",", "'Validation loss:,'", ",", "'Training perplexity:,'", ",", "'Validation Perplexity:,'", "]", "\n", "float_lists", "=", "[", "train_loss", ",", "valid_loss", ",", "train_ppl", ",", "valid_ppl", "]", "\n", "with", "open", "(", "path", "+", "'.csv'", ",", "'w'", ")", "as", "result_csv", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "labels", ")", ")", ":", "\n", "            ", "result_csv", ".", "write", "(", "labels", "[", "i", "]", "+", "concat_float_list", "(", "float_lists", "[", "i", "]", ",", "','", ")", "+", "'\\n'", ")", "\n", "# Export the plots to pdf file", "\n", "", "", "plot_train_valid_curve", "(", "train_loss", ",", "valid_loss", ",", "plot_every", ",", "path", ",", "'Loss'", ")", "\n", "plot_train_valid_curve", "(", "train_ppl", ",", "valid_ppl", ",", "plot_every", ",", "path", ",", "'Perplexity'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.report.concat_float_list": [[39, 41], ["delimiter.join", "str"], "function", ["None"], ["", "def", "concat_float_list", "(", "list", ",", "delimiter", "=", "','", ")", ":", "\n", "    ", "return", "delimiter", ".", "join", "(", "[", "str", "(", "l", ")", "for", "l", "in", "list", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.report.plot_train_valid_curve": [[42, 55], ["matplotlib.pyplot.figure", "matplotlib.pyplot.title", "matplotlib.pyplot.xlabel", "matplotlib.pyplot.ylabel", "len", "list", "matplotlib.pyplot.plot", "matplotlib.pyplot.plot", "matplotlib.pyplot.legend", "matplotlib.pyplot.savefig", "range", "loss_label.lower", "loss_label.lower"], "function", ["None"], ["", "def", "plot_train_valid_curve", "(", "train_loss", ",", "valid_loss", ",", "plot_every", ",", "path", ",", "loss_label", ")", ":", "\n", "#plt.ioff()", "\n", "    ", "title", "=", "\"Training and validation %s for every %d iterations\"", "%", "(", "loss_label", ".", "lower", "(", ")", ",", "plot_every", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "title", "(", "title", ")", "\n", "plt", ".", "xlabel", "(", "\"Checkpoints\"", ")", "\n", "plt", ".", "ylabel", "(", "loss_label", ")", "\n", "num_checkpoints", "=", "len", "(", "train_loss", ")", "\n", "X", "=", "list", "(", "range", "(", "num_checkpoints", ")", ")", "\n", "plt", ".", "plot", "(", "X", ",", "train_loss", ",", "label", "=", "\"training\"", ")", "\n", "plt", ".", "plot", "(", "X", ",", "valid_loss", ",", "label", "=", "\"validation\"", ")", "\n", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "savefig", "(", "\"%s_%s.pdf\"", "%", "(", "path", ",", "loss_label", ".", "lower", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.time_log.time_since": [[3, 5], ["time.time"], "function", ["None"], ["def", "time_since", "(", "start_time", ")", ":", "\n", "    ", "return", "time", ".", "time", "(", ")", "-", "start_time", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.rnn_encoder.RNNEncoder.forward": [[13, 15], ["None"], "methods", ["None"], ["def", "forward", "(", "self", ",", "src", ",", "src_lens", ",", "src_mask", "=", "None", ",", "title", "=", "None", ",", "title_lens", "=", "None", ",", "title_mask", "=", "None", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.rnn_encoder.RNNEncoderBasic.__init__": [[18, 34], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.GRU", "torch.GRU"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "embed_size", ",", "hidden_size", ",", "num_layers", ",", "bidirectional", ",", "pad_token", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "RNNEncoderBasic", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "embed_size", "=", "embed_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "self", ".", "pad_token", "=", "pad_token", "\n", "#self.dropout = nn.Dropout(dropout)", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "\n", "self", ".", "vocab_size", ",", "\n", "self", ".", "embed_size", ",", "\n", "self", ".", "pad_token", "\n", ")", "\n", "self", ".", "rnn", "=", "nn", ".", "GRU", "(", "input_size", "=", "embed_size", ",", "hidden_size", "=", "hidden_size", ",", "num_layers", "=", "num_layers", ",", "\n", "bidirectional", "=", "bidirectional", ",", "batch_first", "=", "True", ",", "dropout", "=", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.rnn_encoder.RNNEncoderBasic.forward": [[35, 58], ["rnn_encoder.RNNEncoderBasic.embedding", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "rnn_encoder.RNNEncoderBasic.rnn", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "memory_bank.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ",", "src_lens", ",", "src_mask", "=", "None", ",", "title", "=", "None", ",", "title_lens", "=", "None", ",", "title_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        :param src: [batch, src_seq_len]\n        :param src_lens: a list containing the length of src sequences for each batch, with len=batch\n        Other parameters will not be used in the RNNENcoderBasic class, they are here because we want to have a unify interface\n        :return:\n        \"\"\"", "\n", "# Debug", "\n", "#if math.isnan(self.rnn.weight_hh_l0[0,0].item()):", "\n", "#    logging.info('nan encoder parameter')", "\n", "src_embed", "=", "self", ".", "embedding", "(", "src", ")", "# [batch, src_len, embed_size]", "\n", "packed_input_src", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "src_embed", ",", "src_lens", ",", "batch_first", "=", "True", ")", "\n", "memory_bank", ",", "encoder_final_state", "=", "self", ".", "rnn", "(", "packed_input_src", ")", "\n", "# ([batch, seq_len, num_directions*hidden_size], [num_layer * num_directions, batch, hidden_size])", "\n", "memory_bank", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "memory_bank", ",", "batch_first", "=", "True", ")", "# unpack (back to padded)", "\n", "\n", "# only extract the final state in the last layer", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "encoder_last_layer_final_state", "=", "torch", ".", "cat", "(", "(", "encoder_final_state", "[", "-", "1", ",", ":", ",", ":", "]", ",", "encoder_final_state", "[", "-", "2", ",", ":", ",", ":", "]", ")", ",", "1", ")", "# [batch, hidden_size*2]", "\n", "", "else", ":", "\n", "            ", "encoder_last_layer_final_state", "=", "encoder_final_state", "[", "-", "1", ",", ":", ",", ":", "]", "# [batch, hidden_size]", "\n", "\n", "", "return", "memory_bank", ".", "contiguous", "(", ")", ",", "encoder_last_layer_final_state", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.rnn_encoder.RNNEncoderTG.__init__": [[61, 90], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.GRU", "torch.GRU", "torch.GRU", "torch.GRU", "torch.GRU", "torch.GRU", "pykp.masked_softmax.MaskedSoftmax", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "embed_size", ",", "hidden_size", ",", "num_layers", ",", "bidirectional", ",", "pad_token", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "RNNEncoderTG", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "embed_size", "=", "embed_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "self", ".", "pad_token", "=", "pad_token", "\n", "#self.dropout = nn.Dropout(dropout)", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "\n", "self", ".", "vocab_size", ",", "\n", "self", ".", "embed_size", ",", "\n", "self", ".", "pad_token", "\n", ")", "\n", "if", "bidirectional", ":", "\n", "            ", "self", ".", "num_directions", "=", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_directions", "=", "1", "\n", "\n", "", "self", ".", "merging_layer", "=", "nn", ".", "GRU", "(", "input_size", "=", "2", "*", "self", ".", "num_directions", "*", "hidden_size", ",", "hidden_size", "=", "hidden_size", ",", "num_layers", "=", "num_layers", ",", "\n", "bidirectional", "=", "bidirectional", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "title_encoder", "=", "nn", ".", "GRU", "(", "input_size", "=", "embed_size", ",", "hidden_size", "=", "hidden_size", ",", "num_layers", "=", "num_layers", ",", "\n", "bidirectional", "=", "bidirectional", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "source_encoder", "=", "nn", ".", "GRU", "(", "input_size", "=", "embed_size", ",", "hidden_size", "=", "hidden_size", ",", "num_layers", "=", "num_layers", ",", "\n", "bidirectional", "=", "bidirectional", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "softmax", "=", "MaskedSoftmax", "(", "dim", "=", "2", ")", "\n", "self", ".", "match_fc", "=", "nn", ".", "Linear", "(", "self", ".", "num_directions", "*", "hidden_size", ",", "self", ".", "num_directions", "*", "hidden_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "self", ".", "res_ratio", "=", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.rnn_encoder.RNNEncoderTG.forward": [[91, 142], ["rnn_encoder.RNNEncoderTG.embedding", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "rnn_encoder.RNNEncoderTG.source_encoder", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "rnn_encoder.RNNEncoderTG.embedding", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "rnn_encoder.RNNEncoderTG.index_select", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "rnn_encoder.RNNEncoderTG.title_encoder", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "title_context.index_select.index_select.index_select", "rnn_encoder.RNNEncoderTG.attn_matched_seq", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rnn_encoder.RNNEncoderTG.dropout", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "rnn_encoder.RNNEncoderTG.merging_layer", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "sorted_title_lens_tensor.tolist", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "final_src_context.contiguous", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.rnn_encoder.RNNEncoderTG.attn_matched_seq"], ["", "def", "forward", "(", "self", ",", "src", ",", "src_lens", ",", "src_mask", ",", "title", ",", "title_lens", ",", "title_mask", ")", ":", "\n", "        ", "\"\"\"\n        :param src: [batch, src_seq_len]\n        :param src_lens: a list containing the length of src sequences for each batch, with len=batch\n        :return:\n        \"\"\"", "\n", "# the src text pass through the source encoder to produce source context", "\n", "src_embed", "=", "self", ".", "embedding", "(", "src", ")", "# [batch, src_len, embed_size]", "\n", "packed_src_embed", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "src_embed", ",", "src_lens", ",", "batch_first", "=", "True", ")", "\n", "src_context", ",", "_", "=", "self", ".", "source_encoder", "(", "packed_src_embed", ")", "\n", "# ([batch, src_seq_len, num_directions*hidden_size], [num_layer * num_directions, batch, hidden_size])", "\n", "src_context", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "src_context", ",", "batch_first", "=", "True", ")", "# unpack (back to padded)", "\n", "\n", "# add for residual connection", "\n", "res_src_context", "=", "src_context", "\n", "\n", "# the title text pass through the title encoder to produce title context", "\n", "title_embed", "=", "self", ".", "embedding", "(", "title", ")", "# [batch, title_len, embed_size]", "\n", "# sort title according to length in a descending order", "\n", "title_lens_tensor", "=", "torch", ".", "LongTensor", "(", "title_lens", ")", ".", "to", "(", "title", ".", "device", ")", "\n", "sorted_title_lens_tensor", ",", "title_idx_sorted", "=", "torch", ".", "sort", "(", "title_lens_tensor", ",", "dim", "=", "0", ",", "descending", "=", "True", ")", "\n", "_", ",", "title_idx_original", "=", "torch", ".", "sort", "(", "title_idx_sorted", ",", "dim", "=", "0", ")", "\n", "sorted_title_embed", "=", "title_embed", ".", "index_select", "(", "0", ",", "title_idx_sorted", ")", "\n", "packed_sorted_title_embed", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "sorted_title_embed", ",", "sorted_title_lens_tensor", ".", "tolist", "(", ")", ",", "batch_first", "=", "True", ")", "\n", "title_context", ",", "_", "=", "self", ".", "title_encoder", "(", "packed_sorted_title_embed", ")", "\n", "# ([batch, src_seq_len, num_directions*hidden_size], [num_layer * num_directions, batch, hidden_size])", "\n", "title_context", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "title_context", ",", "batch_first", "=", "True", ")", "# unpack (back to padded)", "\n", "title_context", "=", "title_context", ".", "index_select", "(", "0", ",", "title_idx_original", ")", "\n", "\n", "attn_matched_seq", "=", "self", ".", "attn_matched_seq", "(", "src_context", ",", "title_context", ",", "title_mask", ")", "\n", "\n", "src_context", "=", "torch", ".", "cat", "(", "[", "src_context", ",", "attn_matched_seq", "]", ",", "dim", "=", "-", "1", ")", "\n", "src_context_dropouted", "=", "self", ".", "dropout", "(", "src_context", ")", "\n", "\n", "# final merge layer", "\n", "packed_src_context", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "src_context_dropouted", ",", "src_lens", ",", "batch_first", "=", "True", ")", "\n", "merged_src_context", ",", "encoder_final_state", "=", "self", ".", "merging_layer", "(", "packed_src_context", ")", "\n", "# ([batch, src_seq_len, num_directions*hidden_size], [num_layer * num_directions, batch, hidden_size])", "\n", "merged_src_context", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "merged_src_context", ",", "batch_first", "=", "True", ")", "# unpack (back to padded)", "\n", "\n", "# add residual connection", "\n", "final_src_context", "=", "self", ".", "res_ratio", "*", "res_src_context", "+", "(", "1", "-", "self", ".", "res_ratio", ")", "*", "merged_src_context", "\n", "\n", "# only extract the final state in the last layer", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "encoder_last_layer_final_state", "=", "torch", ".", "cat", "(", "(", "encoder_final_state", "[", "-", "1", ",", ":", ",", ":", "]", ",", "encoder_final_state", "[", "-", "2", ",", ":", ",", ":", "]", ")", ",", "\n", "1", ")", "# [batch, hidden_size*2]", "\n", "", "else", ":", "\n", "            ", "encoder_last_layer_final_state", "=", "encoder_final_state", "[", "-", "1", ",", ":", ",", ":", "]", "# [batch, hidden_size]", "\n", "\n", "", "return", "final_src_context", ".", "contiguous", "(", ")", ",", "encoder_last_layer_final_state", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.rnn_encoder.RNNEncoderTG.attn_matched_seq": [[143, 162], ["src_context.size", "rnn_encoder.RNNEncoderTG.match_fc", "src_context.bmm", "title_mask.unsqueeze().expand", "rnn_encoder.RNNEncoderTG.softmax", "rnn_encoder.RNNEncoderTG.bmm", "rnn_encoder.RNNEncoderTG.transpose", "title_mask.unsqueeze"], "methods", ["None"], ["", "def", "attn_matched_seq", "(", "self", ",", "src_context", ",", "title_context", ",", "title_mask", ")", ":", "\n", "        ", "\"\"\"\n        :param src_context: [batch, src_seq_len, num_directions*hidden_size]\n        :param title_context: [batch, title_seq_len, num_directions*hidden_size]\n        :return:\n        \"\"\"", "\n", "src_seq_len", "=", "src_context", ".", "size", "(", "1", ")", "\n", "# compute score", "\n", "matched_title_context", "=", "self", ".", "match_fc", "(", "title_context", ")", "# [batch, title_seq_len, num_directions * hidden_size]", "\n", "scores", "=", "src_context", ".", "bmm", "(", "matched_title_context", ".", "transpose", "(", "2", ",", "1", ")", ")", "# [batch, src_seq_len, title_seq_len]", "\n", "expanded_title_mask", "=", "title_mask", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "src_seq_len", ",", "-", "1", ")", "# [batch, src_seq_len, title_seq_len]", "\n", "\n", "# normalize the score", "\n", "attn_dist", "=", "self", ".", "softmax", "(", "scores", ",", "mask", "=", "expanded_title_mask", ")", "# [batch, src_seq_len, title_seq_len]", "\n", "\n", "# compute weighted sum", "\n", "matched_src_context", "=", "attn_dist", ".", "bmm", "(", "title_context", ")", "# [batch, src_seq_len, num_directions * hidden_size]", "\n", "\n", "return", "matched_src_context", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.target_encoder.TargetEncoder.__init__": [[7, 19], ["torch.Module.__init__", "torch.GRU", "torch.GRU", "torch.Embedding", "torch.Embedding"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embed_size", ",", "hidden_size", ",", "vocab_size", ",", "pad_idx", ")", ":", "\n", "        ", "super", "(", "TargetEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_size", "=", "embed_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "pad_token", "=", "pad_idx", "\n", "self", ".", "rnn", "=", "nn", ".", "GRU", "(", "input_size", "=", "embed_size", ",", "hidden_size", "=", "hidden_size", ",", "num_layers", "=", "1", ",", "\n", "bidirectional", "=", "False", ")", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "\n", "self", ".", "vocab_size", ",", "\n", "self", ".", "embed_size", ",", "\n", "self", ".", "pad_token", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.target_encoder.TargetEncoder.forward": [[21, 30], ["target_encoder.TargetEncoder.embedding().unsqueeze", "target_encoder.TargetEncoder.rnn", "target_encoder.TargetEncoder.embedding"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "y", ",", "h", ")", ":", "\n", "        ", "\"\"\"\n        :param y: [batch_size]\n        :param h: [1, batch_size, target_encoder_size]\n        :return:\n        \"\"\"", "\n", "y_emb", "=", "self", ".", "embedding", "(", "y", ")", ".", "unsqueeze", "(", "0", ")", "# [1, batch_size, embed_size]", "\n", "_", ",", "h_next", "=", "self", ".", "rnn", "(", "y_emb", ",", "h", ")", "# [1, batch_size, target_encoder_size]", "\n", "return", "h_next", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.rnn_decoder.RNNDecoder.__init__": [[12, 76], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Embedding", "torch.Embedding", "torch.GRU", "torch.GRU", "pykp.attention.Attention", "torch.Sigmoid", "torch.Sigmoid", "torch.Linear", "torch.Linear", "pykp.masked_softmax.MaskedSoftmax", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "pykp.attention.Attention", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "embed_size", ",", "hidden_size", ",", "num_layers", ",", "memory_bank_size", ",", "coverage_attn", ",", "copy_attn", ",", "\n", "review_attn", ",", "pad_idx", ",", "attn_mode", ",", "dropout", "=", "0.0", ",", "use_target_encoder", "=", "False", ",", "target_encoder_size", "=", "64", ",", "\n", "goal_vector_mode", "=", "0", ",", "goal_vector_size", "=", "16", ")", ":", "\n", "        ", "super", "(", "RNNDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "#self.input_size = input_size", "\n", "#self.input_size = embed_size + memory_bank_size", "\n", "self", ".", "embed_size", "=", "embed_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "memory_bank_size", "=", "memory_bank_size", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "coverage_attn", "=", "coverage_attn", "\n", "self", ".", "copy_attn", "=", "copy_attn", "\n", "self", ".", "review_attn", "=", "review_attn", "\n", "self", ".", "pad_token", "=", "pad_idx", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "\n", "self", ".", "vocab_size", ",", "\n", "self", ".", "embed_size", ",", "\n", "self", ".", "pad_token", "\n", ")", "\n", "self", ".", "input_size", "=", "embed_size", "\n", "self", ".", "use_target_encoder", "=", "use_target_encoder", "\n", "if", "use_target_encoder", ":", "\n", "            ", "self", ".", "input_size", "+=", "target_encoder_size", "\n", "self", ".", "target_encoder_size", "=", "target_encoder_size", "\n", "\n", "", "self", ".", "goal_vector_mode", "=", "goal_vector_mode", "\n", "self", ".", "goal_vector_size", "=", "goal_vector_size", "\n", "if", "goal_vector_mode", "==", "1", ":", "\n", "            ", "self", ".", "input_size", "+=", "goal_vector_size", "\n", "\n", "", "self", ".", "rnn", "=", "nn", ".", "GRU", "(", "input_size", "=", "self", ".", "input_size", ",", "hidden_size", "=", "hidden_size", ",", "num_layers", "=", "num_layers", ",", "\n", "bidirectional", "=", "False", ",", "batch_first", "=", "False", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "attention_layer", "=", "Attention", "(", "\n", "decoder_size", "=", "hidden_size", ",", "\n", "memory_bank_size", "=", "memory_bank_size", ",", "\n", "coverage_attn", "=", "coverage_attn", ",", "\n", "attn_mode", "=", "attn_mode", "\n", ")", "\n", "if", "copy_attn", ":", "\n", "            ", "p_gen_input_size", "=", "embed_size", "+", "hidden_size", "+", "memory_bank_size", "\n", "if", "goal_vector_mode", "==", "2", ":", "\n", "                ", "p_gen_input_size", "+=", "goal_vector_size", "\n", "", "self", ".", "p_gen_linear", "=", "nn", ".", "Linear", "(", "p_gen_input_size", ",", "1", ")", "\n", "\n", "", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "#self.p_gen_linear = nn.Linear(input_size + hidden_size, 1)", "\n", "#self.sigmoid = nn.Sigmoid()", "\n", "# self.vocab_dist_network = nn.Sequential(nn.Linear(hidden_size + memory_bank_size, hidden_size), nn.Linear(hidden_size, vocab_size), nn.Softmax(dim=1))", "\n", "\n", "if", "review_attn", ":", "\n", "            ", "self", ".", "vocab_dist_linear_1", "=", "nn", ".", "Linear", "(", "2", "*", "hidden_size", "+", "memory_bank_size", ",", "hidden_size", ")", "\n", "self", ".", "review_attention_layer", "=", "Attention", "(", "\n", "decoder_size", "=", "hidden_size", ",", "\n", "memory_bank_size", "=", "hidden_size", ",", "\n", "coverage_attn", "=", "False", ",", "\n", "attn_mode", "=", "attn_mode", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "vocab_dist_linear_1", "=", "nn", ".", "Linear", "(", "hidden_size", "+", "memory_bank_size", ",", "hidden_size", ")", "\n", "\n", "", "self", ".", "vocab_dist_linear_2", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "vocab_size", ")", "\n", "self", ".", "softmax", "=", "MaskedSoftmax", "(", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.rnn_decoder.RNNDecoder.forward": [[77, 171], ["list", "rnn_decoder.RNNDecoder.embedding().unsqueeze", "rnn_decoder.RNNDecoder.rnn", "rnn_decoder.RNNDecoder.attention_layer", "rnn_decoder.RNNDecoder.softmax", "src_oov.size", "y.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "h.contiguous.contiguous.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "h.contiguous.contiguous.contiguous", "h_next.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "context.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "attn_dist.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "rnn_decoder.RNNDecoder.review_attention_layer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rnn_decoder.RNNDecoder.vocab_dist_linear_2", "rnn_decoder.RNNDecoder.sigmoid", "torch.cat.scatter_add", "torch.cat.scatter_add", "rnn_decoder.RNNDecoder.embedding", "coverage.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "review_context.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "rnn_decoder.RNNDecoder.dropout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rnn_decoder.RNNDecoder.p_gen_linear", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.scatter_add.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.cat.scatter_add.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "target_encoder_state.detach", "rnn_decoder.RNNDecoder.vocab_dist_linear_1", "rnn_decoder.RNNDecoder.squeeze", "goal_vector.squeeze", "rnn_decoder.RNNDecoder.squeeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "y", ",", "h", ",", "memory_bank", ",", "src_mask", ",", "max_num_oovs", ",", "src_oov", ",", "coverage", ",", "decoder_memory_bank", "=", "None", ",", "target_encoder_state", "=", "None", ",", "goal_vector", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        :param y: [batch_size]\n        :param h: [num_layers, batch_size, decoder_size]\n        :param memory_bank: [batch_size, max_src_seq_len, memory_bank_size]\n        :param src_mask: [batch_size, max_src_seq_len]\n        :param max_num_oovs: int\n        :param src_oov: [batch_size, max_src_seq_len]\n        :param coverage: [batch_size, max_src_seq_len]\n        :param decoder_memory_bank: [batch_size, t-1, decoder_size]\n        :param target_encoder_state: [1, batch_size, target_encoder_size]\n        :param goal_vector: [1, batch_size, goal_vector_size]\n        :return:\n        \"\"\"", "\n", "batch_size", ",", "max_src_seq_len", "=", "list", "(", "src_oov", ".", "size", "(", ")", ")", "\n", "assert", "y", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", "]", ")", "\n", "assert", "h", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_size", "]", ")", "\n", "\n", "# init input embedding", "\n", "y_emb", "=", "self", ".", "embedding", "(", "y", ")", ".", "unsqueeze", "(", "0", ")", "# [1, batch_size, embed_size]", "\n", "# pass the concatenation of the input embedding and context vector to the RNN", "\n", "# insert one dimension to the context tensor", "\n", "#rnn_input = torch.cat((y_emb, context.unsqueeze(0)), 2)  # [1, batch_size, embed_size + num_directions * encoder_size]", "\n", "\n", "rnn_input", "=", "y_emb", "\n", "\n", "if", "self", ".", "use_target_encoder", ":", "\n", "            ", "assert", "target_encoder_state", "is", "not", "None", ",", "'If you use target encoder, you must supply the target encoder state to the decoder'", "\n", "rnn_input", "=", "torch", ".", "cat", "(", "[", "rnn_input", ",", "target_encoder_state", ".", "detach", "(", ")", "]", ",", "dim", "=", "2", ")", "# [1, batch_size, embed_size+target_encoder_size]", "\n", "\n", "", "if", "self", ".", "goal_vector_mode", "==", "1", ":", "\n", "            ", "assert", "goal_vector", "is", "not", "None", "\n", "rnn_input", "=", "torch", ".", "cat", "(", "[", "rnn_input", ",", "goal_vector", "]", ",", "dim", "=", "2", ")", "# [1, batch_size, embed_size+goal_vector_size]", "\n", "\n", "", "if", "self", ".", "num_layers", ">", "1", ":", "\n", "            ", "h", "=", "h", ".", "contiguous", "(", ")", "\n", "\n", "", "_", ",", "h_next", "=", "self", ".", "rnn", "(", "rnn_input", ",", "h", ")", "\n", "\n", "assert", "h_next", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_size", "]", ")", "\n", "\n", "last_layer_h_next", "=", "h_next", "[", "-", "1", ",", ":", ",", ":", "]", "# [batch, decoder_size]", "\n", "\n", "# apply attention, get input-aware context vector, attention distribution and update the coverage vector", "\n", "context", ",", "attn_dist", ",", "coverage", "=", "self", ".", "attention_layer", "(", "last_layer_h_next", ",", "memory_bank", ",", "src_mask", ",", "coverage", ")", "\n", "# context: [batch_size, memory_bank_size], attn_dist: [batch_size, max_input_seq_len], coverage: [batch_size, max_input_seq_len]", "\n", "assert", "context", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "self", ".", "memory_bank_size", "]", ")", "\n", "assert", "attn_dist", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "max_src_seq_len", "]", ")", "\n", "if", "self", ".", "coverage_attn", ":", "\n", "            ", "assert", "coverage", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "max_src_seq_len", "]", ")", "\n", "\n", "# apply review mechanism", "\n", "", "if", "self", ".", "review_attn", ":", "\n", "            ", "assert", "decoder_memory_bank", "is", "not", "None", "\n", "review_context", ",", "review_attn_dist", ",", "_", "=", "self", ".", "review_attention_layer", "(", "last_layer_h_next", ",", "decoder_memory_bank", ",", "src_mask", "=", "None", ",", "coverage", "=", "None", ")", "\n", "# review_context: [batch_size, decoder_size], attn_dist: [batch_size, t-1]", "\n", "assert", "review_context", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "self", ".", "hidden_size", "]", ")", "\n", "vocab_dist_input", "=", "torch", ".", "cat", "(", "(", "context", ",", "last_layer_h_next", ",", "review_context", ")", ",", "dim", "=", "1", ")", "# [B, memory_bank_size + decoder_size + decoder_size]", "\n", "", "else", ":", "\n", "            ", "vocab_dist_input", "=", "torch", ".", "cat", "(", "(", "context", ",", "last_layer_h_next", ")", ",", "dim", "=", "1", ")", "# [B, memory_bank_size + decoder_size]", "\n", "\n", "# Debug", "\n", "#if math.isnan(attn_dist[0,0].item()):", "\n", "#    logging.info('nan attention distribution')", "\n", "\n", "", "vocab_dist", "=", "self", ".", "softmax", "(", "self", ".", "vocab_dist_linear_2", "(", "self", ".", "dropout", "(", "self", ".", "vocab_dist_linear_1", "(", "vocab_dist_input", ")", ")", ")", ")", "\n", "#logit_1 = self.vocab_dist_linear_1(vocab_dist_input)", "\n", "#logit_2 = self.vocab_dist_linear_2(logit_1)", "\n", "#vocab_dist = self.softmax(logit_2)", "\n", "\n", "p_gen", "=", "None", "\n", "if", "self", ".", "copy_attn", ":", "\n", "            ", "if", "self", ".", "goal_vector_mode", "==", "2", ":", "\n", "                ", "p_gen_input", "=", "torch", ".", "cat", "(", "(", "context", ",", "last_layer_h_next", ",", "y_emb", ".", "squeeze", "(", "0", ")", ",", "goal_vector", ".", "squeeze", "(", "0", ")", ")", ",", "dim", "=", "1", ")", "# [B, memory_bank_size + decoder_size + embed_size + goal_vector]", "\n", "", "else", ":", "\n", "                ", "p_gen_input", "=", "torch", ".", "cat", "(", "(", "context", ",", "last_layer_h_next", ",", "y_emb", ".", "squeeze", "(", "0", ")", ")", ",", "dim", "=", "1", ")", "# [B, memory_bank_size + decoder_size + embed_size]", "\n", "#p_gen = self.sigmoid(self.p_gen_linear(p_gen_input))", "\n", "", "p_gen", "=", "self", ".", "sigmoid", "(", "self", ".", "p_gen_linear", "(", "p_gen_input", ")", ")", "\n", "\n", "vocab_dist_", "=", "p_gen", "*", "vocab_dist", "\n", "attn_dist_", "=", "(", "1", "-", "p_gen", ")", "*", "attn_dist", "\n", "\n", "if", "max_num_oovs", ">", "0", ":", "\n", "#extra_zeros = Variable(torch.zeros((batch_size, batch.max_art_oovs)))", "\n", "                ", "extra_zeros", "=", "vocab_dist_", ".", "new_zeros", "(", "(", "batch_size", ",", "max_num_oovs", ")", ")", "\n", "vocab_dist_", "=", "torch", ".", "cat", "(", "(", "vocab_dist_", ",", "extra_zeros", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "final_dist", "=", "vocab_dist_", ".", "scatter_add", "(", "1", ",", "src_oov", ",", "attn_dist_", ")", "\n", "assert", "final_dist", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "self", ".", "vocab_size", "+", "max_num_oovs", "]", ")", "\n", "", "else", ":", "\n", "            ", "final_dist", "=", "vocab_dist", "\n", "assert", "final_dist", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "self", ".", "vocab_size", "]", ")", "\n", "\n", "", "return", "final_dist", ",", "h_next", ",", "context", ",", "attn_dist", ",", "p_gen", ",", "coverage", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.rnn_decoder.RNNDecoder.forward_backup": [[172, 236], ["list", "rnn_decoder.RNNDecoder.embedding().unsqueeze", "rnn_decoder.RNNDecoder.rnn", "rnn_decoder.RNNDecoder.attention_layer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rnn_decoder.RNNDecoder.softmax", "src_oov.size", "y.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "h.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "h_next.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "context.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "attn_dist.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "rnn_decoder.RNNDecoder.vocab_dist_linear_2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rnn_decoder.RNNDecoder.sigmoid", "torch.cat.scatter_add", "torch.cat.scatter_add", "rnn_decoder.RNNDecoder.embedding", "coverage.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "rnn_decoder.RNNDecoder.dropout", "rnn_decoder.RNNDecoder.p_gen_linear", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.scatter_add.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.cat.scatter_add.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "rnn_decoder.RNNDecoder.vocab_dist_linear_1", "rnn_decoder.RNNDecoder.squeeze"], "methods", ["None"], ["", "def", "forward_backup", "(", "self", ",", "y", ",", "h", ",", "memory_bank", ",", "src_mask", ",", "max_num_oovs", ",", "src_oov", ",", "coverage", ")", ":", "\n", "        ", "\"\"\"\n        :param y: [batch_size]\n        :param h: [num_layers, batch_size, decoder_size]\n        :param memory_bank: [batch_size, max_src_seq_Len, memory_bank_size]\n        :param src_mask: [batch_size, max_src_seq_len]\n        :param max_num_oovs: int\n        :param src_oov: [batch_size, max_src_seq_len]\n        :param coverage: [batch_size, max_src_seq_len]\n        :return:\n        \"\"\"", "\n", "batch_size", ",", "max_src_seq_len", "=", "list", "(", "src_oov", ".", "size", "(", ")", ")", "\n", "assert", "y", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", "]", ")", "\n", "assert", "h", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_size", "]", ")", "\n", "\n", "# init input embedding", "\n", "y_emb", "=", "self", ".", "embedding", "(", "y", ")", ".", "unsqueeze", "(", "0", ")", "# [1, batch_size, embed_size]", "\n", "# pass the concatenation of the input embedding and context vector to the RNN", "\n", "# insert one dimension to the context tensor", "\n", "#rnn_input = torch.cat((y_emb, context.unsqueeze(0)), 2)  # [1, batch_size, embed_size + num_directions * encoder_size]", "\n", "_", ",", "h_next", "=", "self", ".", "rnn", "(", "y_emb", ",", "h", ")", "# [num_layers, batch, decoder_size]", "\n", "assert", "h_next", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_size", "]", ")", "\n", "\n", "last_layer_h_next", "=", "h_next", "[", "-", "1", ",", ":", ",", ":", "]", "# [batch, decoder_size]", "\n", "\n", "# apply attention, get input-aware context vector, attention distribution and update the coverage vector", "\n", "context", ",", "attn_dist", ",", "coverage", "=", "self", ".", "attention_layer", "(", "last_layer_h_next", ",", "memory_bank", ",", "src_mask", ",", "coverage", ")", "\n", "# context: [batch_size, memory_bank_size], attn_dist: [batch_size, max_input_seq_len], coverage: [batch_size, max_input_seq_len]", "\n", "assert", "context", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "self", ".", "memory_bank_size", "]", ")", "\n", "assert", "attn_dist", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "max_src_seq_len", "]", ")", "\n", "if", "self", ".", "coverage_attn", ":", "\n", "            ", "assert", "coverage", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "max_src_seq_len", "]", ")", "\n", "\n", "# Debug", "\n", "#if math.isnan(attn_dist[0,0].item()):", "\n", "#    logging.info('nan attention distribution')", "\n", "\n", "", "vocab_dist_input", "=", "torch", ".", "cat", "(", "(", "context", ",", "last_layer_h_next", ")", ",", "dim", "=", "1", ")", "# [B, memory_bank_size + decoder_size]", "\n", "\n", "vocab_dist", "=", "self", ".", "softmax", "(", "self", ".", "vocab_dist_linear_2", "(", "self", ".", "dropout", "(", "self", ".", "vocab_dist_linear_1", "(", "vocab_dist_input", ")", ")", ")", ")", "\n", "#logit_1 = self.vocab_dist_linear_1(vocab_dist_input)", "\n", "#logit_2 = self.vocab_dist_linear_2(logit_1)", "\n", "#vocab_dist = self.softmax(logit_2)", "\n", "\n", "p_gen", "=", "None", "\n", "if", "self", ".", "copy_attn", ":", "\n", "            ", "p_gen_input", "=", "torch", ".", "cat", "(", "(", "context", ",", "last_layer_h_next", ",", "y_emb", ".", "squeeze", "(", "0", ")", ")", ",", "dim", "=", "1", ")", "# [B, memory_bank_size + decoder_size + embed_size]", "\n", "#p_gen = self.sigmoid(self.p_gen_linear(p_gen_input))", "\n", "p_gen", "=", "self", ".", "sigmoid", "(", "self", ".", "p_gen_linear", "(", "p_gen_input", ")", ")", "\n", "\n", "vocab_dist_", "=", "p_gen", "*", "vocab_dist", "\n", "attn_dist_", "=", "(", "1", "-", "p_gen", ")", "*", "attn_dist", "\n", "\n", "if", "max_num_oovs", ">", "0", ":", "\n", "#extra_zeros = Variable(torch.zeros((batch_size, batch.max_art_oovs)))", "\n", "                ", "extra_zeros", "=", "vocab_dist_", ".", "new_zeros", "(", "(", "batch_size", ",", "max_num_oovs", ")", ")", "\n", "vocab_dist_", "=", "torch", ".", "cat", "(", "(", "vocab_dist_", ",", "extra_zeros", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "final_dist", "=", "vocab_dist_", ".", "scatter_add", "(", "1", ",", "src_oov", ",", "attn_dist_", ")", "\n", "assert", "final_dist", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "self", ".", "vocab_size", "+", "max_num_oovs", "]", ")", "\n", "", "else", ":", "\n", "            ", "final_dist", "=", "vocab_dist", "\n", "assert", "final_dist", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "self", ".", "vocab_size", "]", ")", "\n", "", "return", "final_dist", ",", "h_next", ",", "context", ",", "attn_dist", ",", "p_gen", ",", "coverage", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.rnn_decoder.RNNDecoder.forward_bah": [[237, 301], ["list", "rnn_decoder.RNNDecoder.attention_layer", "rnn_decoder.RNNDecoder.embedding().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rnn_decoder.RNNDecoder.rnn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rnn_decoder.RNNDecoder.softmax", "src_oov.size", "y.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "h.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "context.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "attn_dist.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "h_next.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "rnn_decoder.RNNDecoder.vocab_dist_linear_2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rnn_decoder.RNNDecoder.sigmoid", "torch.cat.scatter_add", "torch.cat.scatter_add", "coverage.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "rnn_decoder.RNNDecoder.embedding", "context.unsqueeze", "rnn_decoder.RNNDecoder.vocab_dist_linear_1", "rnn_decoder.RNNDecoder.p_gen_linear", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.scatter_add.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.cat.scatter_add.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "rnn_decoder.RNNDecoder.squeeze"], "methods", ["None"], ["", "def", "forward_bah", "(", "self", ",", "y", ",", "h", ",", "memory_bank", ",", "src_mask", ",", "max_num_oovs", ",", "src_oov", ",", "coverage", ")", ":", "\n", "        ", "\"\"\"\n        :param y: [batch_size]\n        :param h: [1, batch_size, decoder_size]\n        :param memory_bank: [batch_size, max_src_seq_Len, memory_bank_size]\n        :param src_mask: [batch_size, max_src_seq_len]\n        :param max_num_oovs: int\n        :param src_oov: [batch_size, max_src_seq_len]\n        :param coverage: [batch_size, max_src_seq_len]\n        :return:\n        \"\"\"", "\n", "batch_size", ",", "max_src_seq_len", "=", "list", "(", "src_oov", ".", "size", "(", ")", ")", "\n", "assert", "y", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", "]", ")", "\n", "assert", "h", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "1", ",", "batch_size", ",", "self", ".", "hidden_size", "]", ")", "\n", "\n", "# apply attention, get input-aware context vector, attention distribution and update the coverage vector", "\n", "context", ",", "attn_dist", ",", "coverage", "=", "self", ".", "attention_layer", "(", "h", "[", "0", "]", ",", "memory_bank", ",", "src_mask", ",", "coverage", ")", "\n", "# context: [batch_size, memory_bank_size], attn_dist: [batch_size, max_input_seq_len], coverage: [batch_size, max_input_seq_len]", "\n", "assert", "context", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "self", ".", "memory_bank_size", "]", ")", "\n", "assert", "attn_dist", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "max_src_seq_len", "]", ")", "\n", "if", "self", ".", "coverage_attn", ":", "\n", "            ", "assert", "coverage", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "max_src_seq_len", "]", ")", "\n", "\n", "# init input embedding", "\n", "", "y_emb", "=", "self", ".", "embedding", "(", "y", ")", ".", "unsqueeze", "(", "0", ")", "# [1, batch_size, embed_size]", "\n", "# pass the concatenation of the input embedding and context vector to the RNN", "\n", "# insert one dimension to the context tensor", "\n", "rnn_input", "=", "torch", ".", "cat", "(", "(", "y_emb", ",", "context", ".", "unsqueeze", "(", "0", ")", ")", ",", "2", ")", "# [1, batch_size, embed_size + num_directions * encoder_size]", "\n", "_", ",", "h_next", "=", "self", ".", "rnn", "(", "rnn_input", ",", "h", ")", "# [num_layers, batch, decoder_size]", "\n", "assert", "h_next", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "1", ",", "batch_size", ",", "self", ".", "hidden_size", "]", ")", "\n", "\n", "last_layer_h_next", "=", "h_next", "[", "-", "1", ",", ":", ",", ":", "]", "# [batch, decoder_size]", "\n", "\n", "# Debug", "\n", "#if math.isnan(attn_dist[0,0].item()):", "\n", "#    logging.info('nan attention distribution')", "\n", "\n", "vocab_dist_input", "=", "torch", ".", "cat", "(", "(", "context", ",", "last_layer_h_next", ")", ",", "dim", "=", "1", ")", "# [B, memory_bank_size + decoder_size]", "\n", "\n", "vocab_dist", "=", "self", ".", "softmax", "(", "self", ".", "vocab_dist_linear_2", "(", "self", ".", "vocab_dist_linear_1", "(", "vocab_dist_input", ")", ")", ")", "\n", "#logit_1 = self.vocab_dist_linear_1(vocab_dist_input)", "\n", "#logit_2 = self.vocab_dist_linear_2(logit_1)", "\n", "#vocab_dist = self.softmax(logit_2)", "\n", "\n", "p_gen", "=", "None", "\n", "if", "self", ".", "copy_attn", ":", "\n", "            ", "p_gen_input", "=", "torch", ".", "cat", "(", "(", "context", ",", "last_layer_h_next", ",", "y_emb", ".", "squeeze", "(", "0", ")", ")", ",", "dim", "=", "1", ")", "# [B, memory_bank_size + decoder_size + embed_size]", "\n", "#p_gen = self.sigmoid(self.p_gen_linear(p_gen_input))", "\n", "p_gen", "=", "self", ".", "sigmoid", "(", "self", ".", "p_gen_linear", "(", "p_gen_input", ")", ")", "\n", "\n", "vocab_dist_", "=", "p_gen", "*", "vocab_dist", "\n", "attn_dist_", "=", "(", "1", "-", "p_gen", ")", "*", "attn_dist", "\n", "\n", "if", "max_num_oovs", ">", "0", ":", "\n", "#extra_zeros = Variable(torch.zeros((batch_size, batch.max_art_oovs)))", "\n", "                ", "extra_zeros", "=", "vocab_dist_", ".", "new_zeros", "(", "(", "batch_size", ",", "max_num_oovs", ")", ")", "\n", "vocab_dist_", "=", "torch", ".", "cat", "(", "(", "vocab_dist_", ",", "extra_zeros", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "final_dist", "=", "vocab_dist_", ".", "scatter_add", "(", "1", ",", "src_oov", ",", "attn_dist_", ")", "\n", "assert", "final_dist", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "self", ".", "vocab_size", "+", "max_num_oovs", "]", ")", "\n", "", "else", ":", "\n", "            ", "final_dist", "=", "vocab_dist", "\n", "assert", "final_dist", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "self", ".", "vocab_size", "]", ")", "\n", "", "return", "final_dist", ",", "h_next", ",", "context", ",", "attn_dist", ",", "p_gen", ",", "coverage", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset.__init__": [[31, 72], ["filtered_examples.append", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "examples", ",", "word2idx", ",", "idx2word", ",", "type", "=", "'one2many'", ",", "delimiter_type", "=", "0", ",", "load_train", "=", "True", ",", "remove_src_eos", "=", "False", ",", "title_guided", "=", "False", ")", ":", "\n", "# keys of matter. `src_oov_map` is for mapping pointed word to dict, `oov_dict` is for determining the dim of predicted logit: dim=vocab_size+max_oov_dict_in_batch", "\n", "        ", "assert", "type", "in", "[", "'one2one'", ",", "'one2many'", "]", "\n", "if", "type", "==", "'one2one'", ":", "\n", "            ", "keys", "=", "[", "'src'", ",", "'trg'", ",", "'trg_copy'", ",", "'src_oov'", ",", "'oov_dict'", ",", "'oov_list'", "]", "\n", "", "elif", "type", "==", "'one2many'", ":", "\n", "            ", "keys", "=", "[", "'src'", ",", "'src_oov'", ",", "'oov_dict'", ",", "'oov_list'", ",", "'src_str'", ",", "'trg_str'", ",", "'trg'", ",", "'trg_copy'", "]", "\n", "\n", "", "if", "title_guided", ":", "\n", "            ", "keys", "+=", "[", "'title'", ",", "'title_oov'", "]", "\n", "\n", "", "filtered_examples", "=", "[", "]", "\n", "\n", "for", "e", "in", "examples", ":", "\n", "            ", "filtered_example", "=", "{", "}", "\n", "for", "k", "in", "keys", ":", "\n", "                ", "filtered_example", "[", "k", "]", "=", "e", "[", "k", "]", "\n", "", "if", "'oov_list'", "in", "filtered_example", ":", "\n", "                ", "filtered_example", "[", "'oov_number'", "]", "=", "len", "(", "filtered_example", "[", "'oov_list'", "]", ")", "\n", "'''\n                if type == 'one2one':\n                    filtered_example['oov_number'] = len(filtered_example['oov_list'])\n                elif type == 'one2many':\n                    # TODO: check the oov_number field in one2many example\n                    filtered_example['oov_number'] = [len(oov) for oov in filtered_example['oov_list']]\n                '''", "\n", "\n", "", "filtered_examples", ".", "append", "(", "filtered_example", ")", "\n", "\n", "", "self", ".", "examples", "=", "filtered_examples", "\n", "self", ".", "word2idx", "=", "word2idx", "\n", "self", ".", "id2xword", "=", "idx2word", "\n", "self", ".", "pad_idx", "=", "word2idx", "[", "PAD_WORD", "]", "\n", "self", ".", "type", "=", "type", "\n", "if", "delimiter_type", "==", "0", ":", "\n", "            ", "self", ".", "delimiter", "=", "self", ".", "word2idx", "[", "SEP_WORD", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "delimiter", "=", "self", ".", "word2idx", "[", "EOS_WORD", "]", "\n", "", "self", ".", "load_train", "=", "load_train", "\n", "self", ".", "remove_src_eos", "=", "remove_src_eos", "\n", "self", ".", "title_guided", "=", "title_guided", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset.__getitem__": [[73, 75], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "examples", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset.__len__": [[76, 78], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad": [[79, 94], ["max", "range", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.ne", "torch.ne", "torch.ne", "torch.ne", "input_mask.type.type.type", "len", "numpy.ones", "len", "len"], "methods", ["None"], ["", "def", "_pad", "(", "self", ",", "input_list", ")", ":", "\n", "        ", "input_list_lens", "=", "[", "len", "(", "l", ")", "for", "l", "in", "input_list", "]", "\n", "max_seq_len", "=", "max", "(", "input_list_lens", ")", "\n", "padded_batch", "=", "self", ".", "pad_idx", "*", "np", ".", "ones", "(", "(", "len", "(", "input_list", ")", ",", "max_seq_len", ")", ")", "\n", "\n", "for", "j", "in", "range", "(", "len", "(", "input_list", ")", ")", ":", "\n", "            ", "current_len", "=", "input_list_lens", "[", "j", "]", "\n", "padded_batch", "[", "j", "]", "[", ":", "current_len", "]", "=", "input_list", "[", "j", "]", "\n", "\n", "", "padded_batch", "=", "torch", ".", "LongTensor", "(", "padded_batch", ")", "\n", "\n", "input_mask", "=", "torch", ".", "ne", "(", "padded_batch", ",", "self", ".", "pad_idx", ")", "\n", "input_mask", "=", "input_mask", ".", "type", "(", "torch", ".", "FloatTensor", ")", "\n", "\n", "return", "padded_batch", ",", "input_list_lens", ",", "input_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset.collate_fn_one2one": [[95, 152], ["io.KeyphraseDataset._pad", "io.KeyphraseDataset._pad", "io.KeyphraseDataset._pad", "io.KeyphraseDataset._pad", "sorted", "zip", "io.KeyphraseDataset._pad", "io.KeyphraseDataset._pad", "sorted", "zip", "zip", "zip", "len", "len"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad"], ["", "def", "collate_fn_one2one", "(", "self", ",", "batches", ")", ":", "\n", "        ", "'''\n        Puts each data field into a tensor with outer dimension batch size\"\n        '''", "\n", "assert", "self", ".", "type", "==", "'one2one'", ",", "'The type of dataset should be one2one.'", "\n", "if", "self", ".", "remove_src_eos", ":", "\n", "# source with oov words replaced by <unk>", "\n", "            ", "src", "=", "[", "b", "[", "'src'", "]", "for", "b", "in", "batches", "]", "\n", "# extended src (oov words are replaced with temporary idx, e.g. 50000, 50001 etc.)", "\n", "src_oov", "=", "[", "b", "[", "'src_oov'", "]", "for", "b", "in", "batches", "]", "\n", "", "else", ":", "\n", "# source with oov words replaced by <unk>", "\n", "            ", "src", "=", "[", "b", "[", "'src'", "]", "+", "[", "self", ".", "word2idx", "[", "EOS_WORD", "]", "]", "for", "b", "in", "batches", "]", "\n", "# extended src (oov words are replaced with temporary idx, e.g. 50000, 50001 etc.)", "\n", "src_oov", "=", "[", "b", "[", "'src_oov'", "]", "+", "[", "self", ".", "word2idx", "[", "EOS_WORD", "]", "]", "for", "b", "in", "batches", "]", "\n", "\n", "", "if", "self", ".", "title_guided", ":", "\n", "            ", "title", "=", "[", "b", "[", "'title'", "]", "for", "b", "in", "batches", "]", "\n", "title_oov", "=", "[", "b", "[", "'title_oov'", "]", "for", "b", "in", "batches", "]", "\n", "", "else", ":", "\n", "            ", "title", ",", "title_oov", ",", "title_lens", ",", "title_mask", "=", "None", ",", "None", ",", "None", ",", "None", "\n", "\n", "", "\"\"\"\n        src = [b['src'] + [self.word2idx[EOS_WORD]] for b in batches]\n        # src = [[self.word2idx[BOS_WORD]] + b['src'] + [self.word2idx[EOS_WORD]] for b in batches]\n        # extended src (unk words are replaced with temporary idx, e.g. 50000, 50001 etc.)\n        src_oov = [b['src_oov'] + [self.word2idx[EOS_WORD]] for b in batches]\n        # src_oov = [[self.word2idx[BOS_WORD]] + b['src_oov'] + [self.word2idx[EOS_WORD]] for b in batches]\n        \"\"\"", "\n", "\n", "# target_input: input to decoder, ends with <eos> and oovs are replaced with <unk>", "\n", "trg", "=", "[", "b", "[", "'trg'", "]", "+", "[", "self", ".", "word2idx", "[", "EOS_WORD", "]", "]", "for", "b", "in", "batches", "]", "\n", "\n", "# target for copy model, ends with <eos>, oovs are replaced with temporary idx, e.g. 50000, 50001 etc.)", "\n", "trg_oov", "=", "[", "b", "[", "'trg_copy'", "]", "+", "[", "self", ".", "word2idx", "[", "EOS_WORD", "]", "]", "for", "b", "in", "batches", "]", "\n", "\n", "oov_lists", "=", "[", "b", "[", "'oov_list'", "]", "for", "b", "in", "batches", "]", "\n", "\n", "# sort all the sequences in the order of source lengths, to meet the requirement of pack_padded_sequence", "\n", "if", "self", ".", "title_guided", ":", "\n", "            ", "seq_pairs", "=", "sorted", "(", "zip", "(", "src", ",", "trg", ",", "trg_oov", ",", "src_oov", ",", "oov_lists", ",", "title", ",", "title_oov", ")", ",", "key", "=", "lambda", "p", ":", "len", "(", "p", "[", "0", "]", ")", ",", "reverse", "=", "True", ")", "\n", "src", ",", "trg", ",", "trg_oov", ",", "src_oov", ",", "oov_lists", ",", "title", ",", "title_oov", "=", "zip", "(", "*", "seq_pairs", ")", "\n", "title", ",", "title_lens", ",", "title_mask", "=", "self", ".", "_pad", "(", "title", ")", "\n", "title_oov", ",", "_", ",", "_", "=", "self", ".", "_pad", "(", "title_oov", ")", "\n", "", "else", ":", "\n", "            ", "seq_pairs", "=", "sorted", "(", "zip", "(", "src", ",", "trg", ",", "trg_oov", ",", "src_oov", ",", "oov_lists", ")", ",", "key", "=", "lambda", "p", ":", "len", "(", "p", "[", "0", "]", ")", ",", "reverse", "=", "True", ")", "\n", "src", ",", "trg", ",", "trg_oov", ",", "src_oov", ",", "oov_lists", "=", "zip", "(", "*", "seq_pairs", ")", "\n", "\n", "# pad the src and target sequences with <pad> token and convert to LongTensor", "\n", "", "src", ",", "src_lens", ",", "src_mask", "=", "self", ".", "_pad", "(", "src", ")", "\n", "trg", ",", "trg_lens", ",", "trg_mask", "=", "self", ".", "_pad", "(", "trg", ")", "\n", "\n", "#trg_target, _, _ = self._pad(trg_target)", "\n", "trg_oov", ",", "_", ",", "_", "=", "self", ".", "_pad", "(", "trg_oov", ")", "\n", "src_oov", ",", "_", ",", "_", "=", "self", ".", "_pad", "(", "src_oov", ")", "\n", "\n", "return", "src", ",", "src_lens", ",", "src_mask", ",", "trg", ",", "trg_lens", ",", "trg_mask", ",", "src_oov", ",", "trg_oov", ",", "oov_lists", ",", "title", ",", "title_oov", ",", "title_lens", ",", "title_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset.collate_fn_one2many": [[153, 254], ["len", "list", "io.KeyphraseDataset._pad", "io.KeyphraseDataset._pad", "range", "io.KeyphraseDataset._pad", "io.KeyphraseDataset._pad", "io.KeyphraseDataset._pad", "io.KeyphraseDataset._pad", "len", "enumerate", "trg.append", "trg_oov.append", "sorted", "zip", "sorted", "zip", "sorted", "zip", "sorted", "zip", "len", "len", "zip", "zip", "zip", "zip", "zip", "len", "len", "len", "len", "trg_concat.append", "trg_oov_concat.append"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad"], ["", "def", "collate_fn_one2many", "(", "self", ",", "batches", ")", ":", "\n", "        ", "assert", "self", ".", "type", "==", "'one2many'", ",", "'The type of dataset should be one2many.'", "\n", "if", "self", ".", "remove_src_eos", ":", "\n", "# source with oov words replaced by <unk>", "\n", "            ", "src", "=", "[", "b", "[", "'src'", "]", "for", "b", "in", "batches", "]", "\n", "# extended src (oov words are replaced with temporary idx, e.g. 50000, 50001 etc.)", "\n", "src_oov", "=", "[", "b", "[", "'src_oov'", "]", "for", "b", "in", "batches", "]", "\n", "", "else", ":", "\n", "# source with oov words replaced by <unk>", "\n", "            ", "src", "=", "[", "b", "[", "'src'", "]", "+", "[", "self", ".", "word2idx", "[", "EOS_WORD", "]", "]", "for", "b", "in", "batches", "]", "\n", "# extended src (oov words are replaced with temporary idx, e.g. 50000, 50001 etc.)", "\n", "src_oov", "=", "[", "b", "[", "'src_oov'", "]", "+", "[", "self", ".", "word2idx", "[", "EOS_WORD", "]", "]", "for", "b", "in", "batches", "]", "\n", "\n", "", "if", "self", ".", "title_guided", ":", "\n", "            ", "title", "=", "[", "b", "[", "'title'", "]", "for", "b", "in", "batches", "]", "\n", "title_oov", "=", "[", "b", "[", "'title_oov'", "]", "for", "b", "in", "batches", "]", "\n", "", "else", ":", "\n", "            ", "title", ",", "title_oov", ",", "title_lens", ",", "title_mask", "=", "None", ",", "None", ",", "None", ",", "None", "\n", "\n", "", "batch_size", "=", "len", "(", "src", ")", "\n", "\n", "# trg: a list of concatenated targets, the targets in a concatenated target are separated by a delimiter, oov replaced by UNK", "\n", "# trg_oov: a list of concatenated targets, the targets in a concatenated target are separated by a delimiter, oovs are replaced with temporary idx, e.g. 50000, 50001 etc.)", "\n", "if", "self", ".", "load_train", ":", "\n", "            ", "trg", "=", "[", "]", "\n", "trg_oov", "=", "[", "]", "\n", "for", "b", "in", "batches", ":", "\n", "                ", "trg_concat", "=", "[", "]", "\n", "trg_oov_concat", "=", "[", "]", "\n", "trg_size", "=", "len", "(", "b", "[", "'trg'", "]", ")", "\n", "assert", "len", "(", "b", "[", "'trg'", "]", ")", "==", "len", "(", "b", "[", "'trg_copy'", "]", ")", "\n", "for", "trg_idx", ",", "(", "trg_phase", ",", "trg_phase_oov", ")", "in", "enumerate", "(", "zip", "(", "b", "[", "'trg'", "]", ",", "b", "[", "'trg_copy'", "]", ")", ")", ":", "\n", "# b['trg'] contains a list of targets (keyphrase), each target is a list of indices, 2d list of idx", "\n", "#for trg_idx, a in enumerate(zip(b['trg'], b['trg_copy'])):", "\n", "#trg_phase, trg_phase_oov are list of idx", "\n", "                    ", "if", "trg_phase", "[", "0", "]", "==", "self", ".", "word2idx", "[", "PEOS_WORD", "]", ":", "\n", "                        ", "if", "trg_idx", "==", "0", ":", "\n", "                            ", "trg_concat", "+=", "trg_phase", "\n", "trg_oov_concat", "+=", "trg_phase_oov", "\n", "", "else", ":", "\n", "                            ", "trg_concat", "[", "-", "1", "]", "=", "trg_phase", "[", "0", "]", "\n", "trg_oov_concat", "[", "-", "1", "]", "=", "trg_phase_oov", "[", "0", "]", "\n", "if", "trg_idx", "==", "trg_size", "-", "1", ":", "\n", "                                ", "trg_concat", ".", "append", "(", "self", ".", "word2idx", "[", "EOS_WORD", "]", ")", "\n", "trg_oov_concat", ".", "append", "(", "self", ".", "word2idx", "[", "EOS_WORD", "]", ")", "\n", "", "", "", "else", ":", "\n", "                        ", "if", "trg_idx", "==", "trg_size", "-", "1", ":", "# if this is the last keyphrase, end with <eos>", "\n", "                            ", "trg_concat", "+=", "trg_phase", "+", "[", "self", ".", "word2idx", "[", "EOS_WORD", "]", "]", "\n", "trg_oov_concat", "+=", "trg_phase_oov", "+", "[", "self", ".", "word2idx", "[", "EOS_WORD", "]", "]", "\n", "", "else", ":", "\n", "                            ", "trg_concat", "+=", "trg_phase", "+", "[", "self", ".", "delimiter", "]", "# trg_concat = [target_1] + [delimiter] + [target_2] + [delimiter] + ...", "\n", "trg_oov_concat", "+=", "trg_phase_oov", "+", "[", "self", ".", "delimiter", "]", "\n", "", "", "", "trg", ".", "append", "(", "trg_concat", ")", "\n", "trg_oov", ".", "append", "(", "trg_oov_concat", ")", "\n", "", "", "else", ":", "\n", "            ", "trg", ",", "trg_oov", "=", "None", ",", "None", "\n", "#trg = [[t + [self.word2idx[EOS_WORD]] for t in b['trg']] for b in batches]", "\n", "#trg_oov = [[t + [self.word2idx[EOS_WORD]] for t in b['trg_copy']] for b in batches]", "\n", "\n", "", "oov_lists", "=", "[", "b", "[", "'oov_list'", "]", "for", "b", "in", "batches", "]", "\n", "\n", "# b['src_str'] is a word_list for source text, b['trg_str'] is a list of word list", "\n", "src_str", "=", "[", "b", "[", "'src_str'", "]", "for", "b", "in", "batches", "]", "\n", "trg_str", "=", "[", "b", "[", "'trg_str'", "]", "for", "b", "in", "batches", "]", "\n", "\n", "original_indices", "=", "list", "(", "range", "(", "batch_size", ")", ")", "\n", "\n", "# sort all the sequences in the order of source lengths, to meet the requirement of pack_padded_sequence", "\n", "if", "self", ".", "load_train", ":", "\n", "            ", "if", "self", ".", "title_guided", ":", "\n", "                ", "seq_pairs", "=", "sorted", "(", "zip", "(", "src", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "trg", ",", "trg_oov", ",", "original_indices", ",", "title", ",", "title_oov", ")", ",", "\n", "key", "=", "lambda", "p", ":", "len", "(", "p", "[", "0", "]", ")", ",", "reverse", "=", "True", ")", "\n", "src", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "trg", ",", "trg_oov", ",", "original_indices", ",", "title", ",", "title_oov", "=", "zip", "(", "*", "seq_pairs", ")", "\n", "", "else", ":", "\n", "                ", "seq_pairs", "=", "sorted", "(", "zip", "(", "src", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "trg", ",", "trg_oov", ",", "original_indices", ")", ",", "\n", "key", "=", "lambda", "p", ":", "len", "(", "p", "[", "0", "]", ")", ",", "reverse", "=", "True", ")", "\n", "src", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "trg", ",", "trg_oov", ",", "original_indices", "=", "zip", "(", "*", "seq_pairs", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "title_guided", ":", "\n", "                ", "seq_pairs", "=", "sorted", "(", "zip", "(", "src", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "original_indices", ",", "title", ",", "title_oov", ")", ",", "\n", "key", "=", "lambda", "p", ":", "len", "(", "p", "[", "0", "]", ")", ",", "reverse", "=", "True", ")", "\n", "src", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "original_indices", ",", "title", ",", "title_oov", "=", "zip", "(", "*", "seq_pairs", ")", "\n", "", "else", ":", "\n", "                ", "seq_pairs", "=", "sorted", "(", "zip", "(", "src", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "original_indices", ")", ",", "\n", "key", "=", "lambda", "p", ":", "len", "(", "p", "[", "0", "]", ")", ",", "reverse", "=", "True", ")", "\n", "src", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "original_indices", "=", "zip", "(", "*", "seq_pairs", ")", "\n", "\n", "# pad the src and target sequences with <pad> token and convert to LongTensor", "\n", "", "", "src", ",", "src_lens", ",", "src_mask", "=", "self", ".", "_pad", "(", "src", ")", "\n", "src_oov", ",", "_", ",", "_", "=", "self", ".", "_pad", "(", "src_oov", ")", "\n", "if", "self", ".", "load_train", ":", "\n", "            ", "trg", ",", "trg_lens", ",", "trg_mask", "=", "self", ".", "_pad", "(", "trg", ")", "\n", "trg_oov", ",", "_", ",", "_", "=", "self", ".", "_pad", "(", "trg_oov", ")", "\n", "", "else", ":", "\n", "            ", "trg_lens", ",", "trg_mask", "=", "None", ",", "None", "\n", "\n", "", "if", "self", ".", "title_guided", ":", "\n", "            ", "title", ",", "title_lens", ",", "title_mask", "=", "self", ".", "_pad", "(", "title", ")", "\n", "title_oov", ",", "_", ",", "_", "=", "self", ".", "_pad", "(", "title_oov", ")", "\n", "\n", "", "return", "src", ",", "src_lens", ",", "src_mask", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "trg", ",", "trg_oov", ",", "trg_lens", ",", "trg_mask", ",", "original_indices", ",", "title", ",", "title_oov", ",", "title_lens", ",", "title_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset.collate_fn_one2many_hier": [[255, 320], ["len", "list", "io.KeyphraseDataset._pad", "io.KeyphraseDataset._pad", "range", "sorted", "zip", "sorted", "zip", "io.KeyphraseDataset._pad", "io.KeyphraseDataset._pad", "len", "enumerate", "trg.append", "trg_oov.append", "zip", "zip", "len", "len", "zip", "len", "len"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.KeyphraseDataset._pad"], ["", "def", "collate_fn_one2many_hier", "(", "self", ",", "batches", ")", ":", "\n", "        ", "assert", "self", ".", "type", "==", "'one2many'", ",", "'The type of dataset should be one2many.'", "\n", "# source with oov words replaced by <unk>", "\n", "src", "=", "[", "b", "[", "'src'", "]", "+", "[", "self", ".", "word2idx", "[", "EOS_WORD", "]", "]", "for", "b", "in", "batches", "]", "\n", "# extended src (oov words are replaced with temporary idx, e.g. 50000, 50001 etc.)", "\n", "src_oov", "=", "[", "b", "[", "'src_oov'", "]", "+", "[", "self", ".", "word2idx", "[", "EOS_WORD", "]", "]", "for", "b", "in", "batches", "]", "\n", "\n", "batch_size", "=", "len", "(", "src", ")", "\n", "\n", "# trg: a list of concatenated targets, the targets in a concatenated target are separated by a delimiter, oov replaced by UNK", "\n", "# trg_oov: a list of concatenated targets, the targets in a concatenated target are separated by a delimiter, oovs are replaced with temporary idx, e.g. 50000, 50001 etc.)", "\n", "if", "self", ".", "load_train", ":", "\n", "            ", "trg", "=", "[", "]", "\n", "trg_oov", "=", "[", "]", "\n", "for", "b", "in", "batches", ":", "\n", "                ", "trg_concat", "=", "[", "]", "\n", "trg_oov_concat", "=", "[", "]", "\n", "trg_size", "=", "len", "(", "b", "[", "'trg'", "]", ")", "\n", "assert", "len", "(", "b", "[", "'trg'", "]", ")", "==", "len", "(", "b", "[", "'trg_copy'", "]", ")", "\n", "for", "trg_idx", ",", "(", "trg_phase", ",", "trg_phase_oov", ")", "in", "enumerate", "(", "zip", "(", "b", "[", "'trg'", "]", ",", "b", "[", "\n", "'trg_copy'", "]", ")", ")", ":", "# b['trg'] contains a list of targets, each target is a list of indices", "\n", "# for trg_idx, a in enumerate(zip(b['trg'], b['trg_copy'])):", "\n", "# trg_phase, trg_phase_oov = a", "\n", "                    ", "if", "trg_idx", "==", "trg_size", "-", "1", ":", "# if this is the last keyphrase, end with <eos>", "\n", "                        ", "trg_concat", "+=", "trg_phase", "+", "[", "self", ".", "word2idx", "[", "EOS_WORD", "]", "]", "\n", "trg_oov_concat", "+=", "trg_phase_oov", "+", "[", "self", ".", "word2idx", "[", "EOS_WORD", "]", "]", "\n", "", "else", ":", "\n", "                        ", "trg_concat", "+=", "trg_phase", "+", "[", "\n", "self", ".", "delimiter", "]", "# trg_concat = [target_1] + [delimiter] + [target_2] + [delimiter] + ...", "\n", "trg_oov_concat", "+=", "trg_phase_oov", "+", "[", "self", ".", "delimiter", "]", "\n", "", "", "trg", ".", "append", "(", "trg_concat", ")", "\n", "trg_oov", ".", "append", "(", "trg_oov_concat", ")", "\n", "", "", "else", ":", "\n", "            ", "trg", ",", "trg_oov", "=", "None", ",", "None", "\n", "# trg = [[t + [self.word2idx[EOS_WORD]] for t in b['trg']] for b in batches]", "\n", "# trg_oov = [[t + [self.word2idx[EOS_WORD]] for t in b['trg_copy']] for b in batches]", "\n", "\n", "", "oov_lists", "=", "[", "b", "[", "'oov_list'", "]", "for", "b", "in", "batches", "]", "\n", "\n", "# b['src_str'] is a word_list for source text, b['trg_str'] is a list of word list", "\n", "src_str", "=", "[", "b", "[", "'src_str'", "]", "for", "b", "in", "batches", "]", "\n", "trg_str", "=", "[", "b", "[", "'trg_str'", "]", "for", "b", "in", "batches", "]", "\n", "\n", "original_indices", "=", "list", "(", "range", "(", "batch_size", ")", ")", "\n", "\n", "# sort all the sequences in the order of source lengths, to meet the requirement of pack_padded_sequence", "\n", "if", "self", ".", "load_train", ":", "\n", "            ", "seq_pairs", "=", "sorted", "(", "zip", "(", "src", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "trg", ",", "trg_oov", ",", "original_indices", ")", ",", "\n", "key", "=", "lambda", "p", ":", "len", "(", "p", "[", "0", "]", ")", ",", "reverse", "=", "True", ")", "\n", "src", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "trg", ",", "trg_oov", ",", "original_indices", "=", "zip", "(", "*", "seq_pairs", ")", "\n", "", "else", ":", "\n", "            ", "seq_pairs", "=", "sorted", "(", "zip", "(", "src", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "original_indices", ")", ",", "\n", "key", "=", "lambda", "p", ":", "len", "(", "p", "[", "0", "]", ")", ",", "reverse", "=", "True", ")", "\n", "src", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "original_indices", "=", "zip", "(", "*", "seq_pairs", ")", "\n", "\n", "# pad the src and target sequences with <pad> token and convert to LongTensor", "\n", "", "src", ",", "src_lens", ",", "src_mask", "=", "self", ".", "_pad", "(", "src", ")", "\n", "src_oov", ",", "_", ",", "_", "=", "self", ".", "_pad", "(", "src_oov", ")", "\n", "if", "self", ".", "load_train", ":", "\n", "            ", "trg", ",", "trg_lens", ",", "trg_mask", "=", "self", ".", "_pad", "(", "trg", ")", "\n", "trg_oov", ",", "_", ",", "_", "=", "self", ".", "_pad", "(", "trg_oov", ")", "\n", "", "else", ":", "\n", "            ", "trg_lens", ",", "trg_mask", "=", "None", ",", "None", "\n", "\n", "", "return", "src", ",", "src_lens", ",", "src_mask", ",", "src_oov", ",", "oov_lists", ",", "src_str", ",", "trg_str", ",", "trg", ",", "trg_oov", ",", "trg_lens", ",", "trg_mask", ",", "original_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.load_json_data": [[345, 372], ["codecs.open", "enumerate", "json.loads", "src_trgs_pairs.append", "trg_strs.extend", "re.split"], "function", ["None"], ["def", "load_json_data", "(", "path", ",", "name", "=", "'kp20k'", ",", "src_fields", "=", "[", "'title'", ",", "'abstract'", "]", ",", "trg_fields", "=", "[", "'keyword'", "]", ",", "trg_delimiter", "=", "';'", ")", ":", "\n", "    ", "'''\n    To load keyphrase data from file, generate src by concatenating the contents in src_fields\n    Input file should be json format, one document per line\n    return pairs of (src_str, [trg_str_1, trg_str_2 ... trg_str_m])\n    default data is 'kp20k'\n    :param train_path:\n    :param name:\n    :param src_fields:\n    :param trg_fields:\n    :param trg_delimiter:\n    :return:\n    '''", "\n", "src_trgs_pairs", "=", "[", "]", "\n", "with", "codecs", ".", "open", "(", "path", ",", "\"r\"", ",", "\"utf-8\"", ")", "as", "corpus_file", ":", "\n", "        ", "for", "idx", ",", "line", "in", "enumerate", "(", "corpus_file", ")", ":", "\n", "# if(idx == 20000):", "\n", "#     break", "\n", "# print(line)", "\n", "            ", "json_", "=", "json", ".", "loads", "(", "line", ")", "\n", "\n", "trg_strs", "=", "[", "]", "\n", "src_str", "=", "'.'", ".", "join", "(", "[", "json_", "[", "f", "]", "for", "f", "in", "src_fields", "]", ")", "\n", "[", "trg_strs", ".", "extend", "(", "re", ".", "split", "(", "trg_delimiter", ",", "json_", "[", "f", "]", ")", ")", "for", "f", "in", "trg_fields", "]", "\n", "src_trgs_pairs", ".", "append", "(", "(", "src_str", ",", "trg_strs", ")", ")", "\n", "\n", "", "", "return", "src_trgs_pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.copyseq_tokenize": [[374, 393], ["re.sub", "re.sub", "filter", "re.split", "len", "re.match"], "function", ["None"], ["", "def", "copyseq_tokenize", "(", "text", ")", ":", "\n", "    ", "'''\n    The tokenizer used in Meng et al. ACL 2017\n    parse the feed-in text, filtering and tokenization\n    keep [_<>,\\(\\)\\.\\'%], replace digits to <digit>, split by [^a-zA-Z0-9_<>,\\(\\)\\.\\'%]\n    :param text:\n    :return: a list of tokens\n    '''", "\n", "# remove line breakers", "\n", "text", "=", "re", ".", "sub", "(", "r'[\\r\\n\\t]'", ",", "' '", ",", "text", ")", "\n", "# pad spaces to the left and right of special punctuations", "\n", "text", "=", "re", ".", "sub", "(", "r'[_<>,\\(\\)\\.\\'%]'", ",", "' \\g<0> '", ",", "text", ")", "\n", "# tokenize by non-letters (new-added + # & *, but don't pad spaces, to make them as one whole word)", "\n", "tokens", "=", "filter", "(", "lambda", "w", ":", "len", "(", "w", ")", ">", "0", ",", "re", ".", "split", "(", "r'[^a-zA-Z0-9_<>,#&\\+\\*\\(\\)\\.\\'%]'", ",", "text", ")", ")", "\n", "\n", "# replace the digit terms with <digit>", "\n", "tokens", "=", "[", "w", "if", "not", "re", ".", "match", "(", "'^\\d+$'", ",", "w", ")", "else", "DIGIT", "for", "w", "in", "tokens", "]", "\n", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.tokenize_filter_data": [[395, 491], ["enumerate", "tokenize", "return_pairs.append", "src.lower", "re.sub", "re.sub", "re.sub", "re.findall", "tokenize", "trgs_tokens.append", "print", "print", "print", "print", "print", "len", "len", "len", "re.sub.lower", "len", "print", "print", "print", "len", "set", "print", "print", "print", "print", "print", "print", "len", "len", "len", "len", "print", "print", "re.match", "re.match", "str", "len", "str", "str", "len", "trg_tokens[].strip", "len", "trg_tokens[].strip", "len", "len", "inspect.getframeinfo", "inspect.currentframe"], "function", ["None"], ["", "def", "tokenize_filter_data", "(", "\n", "src_trgs_pairs", ",", "tokenize", ",", "opt", ",", "valid_check", "=", "False", ")", ":", "\n", "    ", "'''\n    tokenize and truncate data, filter examples that exceed the length limit\n    :param src_trgs_pairs:\n    :param tokenize:\n    :param src_seq_length:\n    :param trg_seq_length:\n    :param src_seq_length_trunc:\n    :param trg_seq_length_trunc:\n    :return:\n    '''", "\n", "return_pairs", "=", "[", "]", "\n", "for", "idx", ",", "(", "src", ",", "trgs", ")", "in", "enumerate", "(", "src_trgs_pairs", ")", ":", "\n", "        ", "src_filter_flag", "=", "False", "\n", "\n", "src", "=", "src", ".", "lower", "(", ")", "if", "opt", ".", "lower", "else", "src", "\n", "src_tokens", "=", "tokenize", "(", "src", ")", "\n", "if", "opt", ".", "src_seq_length_trunc", "and", "len", "(", "src", ")", ">", "opt", ".", "src_seq_length_trunc", ":", "\n", "            ", "src_tokens", "=", "src_tokens", "[", ":", "opt", ".", "src_seq_length_trunc", "]", "\n", "\n", "# FILTER 3.1: if length of src exceeds limit, discard", "\n", "", "if", "opt", ".", "max_src_seq_length", "and", "len", "(", "src_tokens", ")", ">", "opt", ".", "max_src_seq_length", ":", "\n", "            ", "src_filter_flag", "=", "True", "\n", "", "if", "opt", ".", "min_src_seq_length", "and", "len", "(", "src_tokens", ")", "<", "opt", ".", "min_src_seq_length", ":", "\n", "            ", "src_filter_flag", "=", "True", "\n", "\n", "", "if", "valid_check", "and", "src_filter_flag", ":", "\n", "            ", "continue", "\n", "\n", "", "trgs_tokens", "=", "[", "]", "\n", "for", "trg", "in", "trgs", ":", "\n", "            ", "trg_filter_flag", "=", "False", "\n", "trg", "=", "trg", ".", "lower", "(", ")", "if", "src", ".", "lower", "else", "trg", "\n", "\n", "# FILTER 1: remove all the abbreviations/acronyms in parentheses in keyphrases", "\n", "trg", "=", "re", ".", "sub", "(", "r'\\(.*?\\)'", ",", "''", ",", "trg", ")", "\n", "trg", "=", "re", ".", "sub", "(", "r'\\[.*?\\]'", ",", "''", ",", "trg", ")", "\n", "trg", "=", "re", ".", "sub", "(", "r'\\{.*?\\}'", ",", "''", ",", "trg", ")", "\n", "\n", "# FILTER 2: ingore all the phrases that contains strange punctuations, very DIRTY data!", "\n", "puncts", "=", "re", ".", "findall", "(", "r'[,_\\\"<>\\(\\){}\\[\\]\\?~`!@$%\\^=]'", ",", "trg", ")", "\n", "\n", "trg_tokens", "=", "tokenize", "(", "trg", ")", "\n", "\n", "if", "len", "(", "puncts", ")", ">", "0", ":", "\n", "                ", "print", "(", "'-'", "*", "50", ")", "\n", "print", "(", "'Find punctuations in keyword: %s'", "%", "trg", ")", "\n", "print", "(", "'- tokens: %s'", "%", "str", "(", "trg_tokens", ")", ")", "\n", "continue", "\n", "\n", "# FILTER 3.2: if length of trg exceeds limit, discard", "\n", "", "if", "opt", ".", "trg_seq_length_trunc", "and", "len", "(", "trg", ")", ">", "opt", ".", "trg_seq_length_trunc", ":", "\n", "                ", "trg_tokens", "=", "trg_tokens", "[", ":", "src", ".", "trg_seq_length_trunc", "]", "\n", "", "if", "opt", ".", "max_trg_seq_length", "and", "len", "(", "trg_tokens", ")", ">", "opt", ".", "max_trg_seq_length", ":", "\n", "                ", "trg_filter_flag", "=", "True", "\n", "", "if", "opt", ".", "min_trg_seq_length", "and", "len", "(", "trg_tokens", ")", "<", "opt", ".", "min_trg_seq_length", ":", "\n", "                ", "trg_filter_flag", "=", "True", "\n", "\n", "", "filtered_by_heuristic_rule", "=", "False", "\n", "\n", "# FILTER 4: check the quality of long keyphrases (>5 words) with a heuristic rule", "\n", "if", "len", "(", "trg_tokens", ")", ">", "5", ":", "\n", "                ", "trg_set", "=", "set", "(", "trg_tokens", ")", "\n", "if", "len", "(", "trg_set", ")", "*", "2", "<", "len", "(", "trg_tokens", ")", ":", "\n", "                    ", "filtered_by_heuristic_rule", "=", "True", "\n", "\n", "", "", "if", "valid_check", "and", "(", "trg_filter_flag", "or", "filtered_by_heuristic_rule", ")", ":", "\n", "                ", "print", "(", "'*'", "*", "50", ")", "\n", "if", "filtered_by_heuristic_rule", ":", "\n", "                    ", "print", "(", "'INVALID by heuristic_rule'", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "'VALID by heuristic_rule'", ")", "\n", "", "print", "(", "'length of src/trg exceeds limit: len(src)=%d, len(trg)=%d'", "%", "(", "len", "(", "src_tokens", ")", ",", "len", "(", "trg_tokens", ")", ")", ")", "\n", "print", "(", "'src: %s'", "%", "str", "(", "src", ")", ")", "\n", "print", "(", "'trg: %s'", "%", "str", "(", "trg", ")", ")", "\n", "print", "(", "'*'", "*", "50", ")", "\n", "continue", "\n", "\n", "# FILTER 5: filter keywords like primary 75v05;secondary 76m10;65n30", "\n", "", "if", "(", "len", "(", "trg_tokens", ")", ">", "0", "and", "re", ".", "match", "(", "r'\\d\\d[a-zA-Z\\-]\\d\\d'", ",", "trg_tokens", "[", "0", "]", ".", "strip", "(", ")", ")", ")", "or", "(", "len", "(", "trg_tokens", ")", ">", "1", "and", "re", ".", "match", "(", "r'\\d\\d\\w\\d\\d'", ",", "trg_tokens", "[", "1", "]", ".", "strip", "(", ")", ")", ")", ":", "\n", "                ", "print", "(", "'Find dirty keyword of type \\d\\d[a-z]\\d\\d: %s'", "%", "trg", ")", "\n", "continue", "\n", "\n", "", "trgs_tokens", ".", "append", "(", "trg_tokens", ")", "\n", "\n", "", "return_pairs", ".", "append", "(", "(", "src_tokens", ",", "trgs_tokens", ")", ")", "\n", "\n", "if", "idx", "%", "2000", "==", "0", ":", "\n", "            ", "print", "(", "'-------------------- %s: %d ---------------------------'", "%", "(", "inspect", ".", "getframeinfo", "(", "inspect", ".", "currentframe", "(", ")", ")", ".", "function", ",", "idx", ")", ")", "\n", "print", "(", "src", ")", "\n", "print", "(", "src_tokens", ")", "\n", "print", "(", "trgs", ")", "\n", "print", "(", "trgs_tokens", ")", "\n", "\n", "", "", "return", "return_pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.build_interactive_predict_dataset": [[493, 499], ["len", "list", "io.build_dataset", "zip"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.build_dataset"], ["", "def", "build_interactive_predict_dataset", "(", "tokenized_src", ",", "word2idx", ",", "idx2word", ",", "opt", ",", "title_list", "=", "None", ")", ":", "\n", "# build a dummy trg list, and then combine it with src, and pass it to the build_dataset method", "\n", "    ", "num_lines", "=", "len", "(", "tokenized_src", ")", "\n", "tokenized_trg", "=", "[", "[", "'.'", "]", "]", "*", "num_lines", "# create a dummy tokenized_trg", "\n", "tokenized_src_trg_pairs", "=", "list", "(", "zip", "(", "tokenized_src", ",", "tokenized_trg", ")", ")", "\n", "return", "build_dataset", "(", "tokenized_src_trg_pairs", ",", "word2idx", ",", "idx2word", ",", "opt", ",", "mode", "=", "'one2many'", ",", "include_original", "=", "True", ",", "title_list", "=", "title_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.build_dataset": [[501, 655], ["enumerate", "print", "print", "print", "io.extend_vocab_OOV", "len", "len", "any", "examples[].keys", "return_examples.append", "str", "len", "len", "print", "print", "print", "print", "print", "print", "print", "print", "print", "any", "return_examples.append", "examples.append", "len", "len", "trg_copy.append", "len", "print", "print", "key.startswith", "key.startswith", "key.startswith", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "trg_copy.append", "trg_copy.append", "title_oov.append", "title_oov.append", "title_oov.append", "len", "len", "len", "len", "len", "len", "len", "len", "inspect.getframeinfo", "inspect.currentframe"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.extend_vocab_OOV"], ["", "def", "build_dataset", "(", "src_trgs_pairs", ",", "word2idx", ",", "idx2word", ",", "opt", ",", "mode", "=", "'one2one'", ",", "include_original", "=", "False", ",", "title_list", "=", "None", ")", ":", "\n", "    ", "'''\n    Standard process for copy model\n    :param mode: one2one or one2many\n    :param include_original: keep the original texts of source and target\n    :return:\n    '''", "\n", "return_examples", "=", "[", "]", "\n", "oov_target", "=", "0", "\n", "max_oov_len", "=", "0", "\n", "max_oov_sent", "=", "''", "\n", "if", "title_list", "!=", "None", ":", "\n", "        ", "assert", "len", "(", "title_list", ")", "==", "len", "(", "src_trgs_pairs", ")", "\n", "\n", "", "for", "idx", ",", "(", "source", ",", "targets", ")", "in", "enumerate", "(", "src_trgs_pairs", ")", ":", "\n", "# if w is not seen in training data vocab (word2idx, size could be larger than opt.vocab_size), replace with <unk>", "\n", "#src_all = [word2idx[w] if w in word2idx else word2idx[UNK_WORD] for w in source]", "\n", "# if w's id is larger than opt.vocab_size, replace with <unk>", "\n", "        ", "src", "=", "[", "word2idx", "[", "w", "]", "if", "w", "in", "word2idx", "and", "word2idx", "[", "w", "]", "<", "opt", ".", "vocab_size", "else", "word2idx", "[", "UNK_WORD", "]", "for", "w", "in", "source", "]", "\n", "\n", "if", "title_list", "is", "not", "None", ":", "\n", "            ", "title_word_list", "=", "title_list", "[", "idx", "]", "\n", "#title_all = [word2idx[w] if w in word2idx else word2idx[UNK_WORD] for w in title_word_list]", "\n", "title", "=", "[", "word2idx", "[", "w", "]", "if", "w", "in", "word2idx", "and", "word2idx", "[", "w", "]", "<", "opt", ".", "vocab_size", "else", "word2idx", "[", "UNK_WORD", "]", "for", "w", "in", "title_word_list", "]", "\n", "\n", "# create a local vocab for the current source text. If there're V words in the vocab of this string, len(itos)=V+2 (including <unk> and <pad>), len(stoi)=V+1 (including <pad>)", "\n", "", "src_oov", ",", "oov_dict", ",", "oov_list", "=", "extend_vocab_OOV", "(", "source", ",", "word2idx", ",", "opt", ".", "vocab_size", ",", "opt", ".", "max_unk_words", ")", "\n", "examples", "=", "[", "]", "# for one-to-many", "\n", "\n", "for", "target", "in", "targets", ":", "\n", "            ", "example", "=", "{", "}", "\n", "\n", "if", "include_original", ":", "\n", "                ", "example", "[", "'src_str'", "]", "=", "source", "\n", "example", "[", "'trg_str'", "]", "=", "target", "\n", "\n", "", "example", "[", "'src'", "]", "=", "src", "\n", "# example['src_input'] = [word2idx[BOS_WORD]] + src + [word2idx[EOS_WORD]] # target input, requires BOS at the beginning", "\n", "# example['src_all']   = src_all", "\n", "\n", "if", "title_list", "is", "not", "None", ":", "\n", "                ", "example", "[", "'title'", "]", "=", "title", "\n", "\n", "", "trg", "=", "[", "word2idx", "[", "w", "]", "if", "w", "in", "word2idx", "and", "word2idx", "[", "w", "]", "<", "opt", ".", "vocab_size", "else", "word2idx", "[", "UNK_WORD", "]", "for", "w", "in", "target", "]", "\n", "example", "[", "'trg'", "]", "=", "trg", "\n", "# example['trg_input']   = [word2idx[BOS_WORD]] + trg + [word2idx[EOS_WORD]] # target input, requires BOS at the beginning", "\n", "# example['trg_all']   = [word2idx[w] if w in word2idx else word2idx[UNK_WORD] for w in target]", "\n", "# example['trg_loss']  = example['trg'] + [word2idx[EOS_WORD]] # target for loss computation, ignore BOS", "\n", "\n", "example", "[", "'src_oov'", "]", "=", "src_oov", "\n", "example", "[", "'oov_dict'", "]", "=", "oov_dict", "\n", "example", "[", "'oov_list'", "]", "=", "oov_list", "\n", "if", "len", "(", "oov_list", ")", ">", "max_oov_len", ":", "\n", "                ", "max_oov_len", "=", "len", "(", "oov_list", ")", "\n", "max_oov_sent", "=", "source", "\n", "\n", "# oov words are replaced with new index", "\n", "", "trg_copy", "=", "[", "]", "\n", "for", "w", "in", "target", ":", "\n", "                ", "if", "w", "in", "word2idx", "and", "word2idx", "[", "w", "]", "<", "opt", ".", "vocab_size", ":", "\n", "                    ", "trg_copy", ".", "append", "(", "word2idx", "[", "w", "]", ")", "\n", "", "elif", "w", "in", "oov_dict", ":", "\n", "                    ", "trg_copy", ".", "append", "(", "oov_dict", "[", "w", "]", ")", "\n", "", "else", ":", "\n", "                    ", "trg_copy", ".", "append", "(", "word2idx", "[", "UNK_WORD", "]", ")", "\n", "", "", "example", "[", "'trg_copy'", "]", "=", "trg_copy", "\n", "\n", "if", "title_list", "is", "not", "None", ":", "\n", "                ", "title_oov", "=", "[", "]", "\n", "for", "w", "in", "title_word_list", ":", "\n", "                    ", "if", "w", "in", "word2idx", "and", "word2idx", "[", "w", "]", "<", "opt", ".", "vocab_size", ":", "\n", "                        ", "title_oov", ".", "append", "(", "word2idx", "[", "w", "]", ")", "\n", "", "elif", "w", "in", "oov_dict", ":", "\n", "                        ", "title_oov", ".", "append", "(", "oov_dict", "[", "w", "]", ")", "\n", "", "else", ":", "\n", "                        ", "title_oov", ".", "append", "(", "word2idx", "[", "UNK_WORD", "]", ")", "\n", "", "", "example", "[", "'title_oov'", "]", "=", "title_oov", "\n", "\n", "# example['trg_copy_input'] = [word2idx[BOS_WORD]] + trg_copy + [word2idx[EOS_WORD]] # target input, requires BOS at the beginning", "\n", "# example['trg_copy_loss']  = example['trg_copy'] + [word2idx[EOS_WORD]] # target for loss computation, ignore BOS", "\n", "\n", "# example['copy_martix'] = copy_martix(source, target)", "\n", "# C = [0 if w not in source else source.index(w) + opt.vocab_size for w in target]", "\n", "# example[\"copy_index\"] = C", "\n", "# A = [word2idx[w] if w in word2idx else word2idx['<unk>'] for w in source]", "\n", "# B = [[word2idx[w] if w in word2idx else word2idx['<unk>'] for w in p] for p in target]", "\n", "# C = [[0 if w not in source else source.index(w) + Lmax for w in p] for p in target]", "\n", "\n", "", "if", "any", "(", "[", "w", ">=", "opt", ".", "vocab_size", "for", "w", "in", "trg_copy", "]", ")", ":", "\n", "                ", "oov_target", "+=", "1", "\n", "\n", "", "if", "idx", "%", "100000", "==", "0", ":", "\n", "                ", "print", "(", "'-------------------- %s: %d ---------------------------'", "%", "(", "inspect", ".", "getframeinfo", "(", "inspect", ".", "currentframe", "(", ")", ")", ".", "function", ",", "idx", ")", ")", "\n", "print", "(", "'source    \\n\\t\\t[len=%d]: %s'", "%", "(", "len", "(", "source", ")", ",", "source", ")", ")", "\n", "print", "(", "'target    \\n\\t\\t[len=%d]: %s'", "%", "(", "len", "(", "target", ")", ",", "target", ")", ")", "\n", "# print('src_all   \\n\\t\\t[len=%d]: %s' % (len(example['src_all']), example['src_all']))", "\n", "# print('trg_all   \\n\\t\\t[len=%d]: %s' % (len(example['trg_all']), example['trg_all']))", "\n", "print", "(", "'src       \\n\\t\\t[len=%d]: %s'", "%", "(", "len", "(", "example", "[", "'src'", "]", ")", ",", "example", "[", "'src'", "]", ")", ")", "\n", "# print('src_input \\n\\t\\t[len=%d]: %s' % (len(example['src_input']), example['src_input']))", "\n", "print", "(", "'trg       \\n\\t\\t[len=%d]: %s'", "%", "(", "len", "(", "example", "[", "'trg'", "]", ")", ",", "example", "[", "'trg'", "]", ")", ")", "\n", "# print('trg_input \\n\\t\\t[len=%d]: %s' % (len(example['trg_input']), example['trg_input']))", "\n", "\n", "print", "(", "'src_oov   \\n\\t\\t[len=%d]: %s'", "%", "(", "len", "(", "src_oov", ")", ",", "src_oov", ")", ")", "\n", "\n", "print", "(", "'oov_dict         \\n\\t\\t[len=%d]: %s'", "%", "(", "len", "(", "oov_dict", ")", ",", "oov_dict", ")", ")", "\n", "print", "(", "'oov_list         \\n\\t\\t[len=%d]: %s'", "%", "(", "len", "(", "oov_list", ")", ",", "oov_list", ")", ")", "\n", "if", "len", "(", "oov_dict", ")", ">", "0", ":", "\n", "                    ", "print", "(", "'Find OOV in source'", ")", "\n", "\n", "", "print", "(", "'trg_copy         \\n\\t\\t[len=%d]: %s'", "%", "(", "len", "(", "trg_copy", ")", ",", "trg_copy", ")", ")", "\n", "# print('trg_copy_input   \\n\\t\\t[len=%d]: %s' % (len(example[\"trg_copy_input\"]), example[\"trg_copy_input\"]))", "\n", "\n", "if", "any", "(", "[", "w", ">=", "opt", ".", "vocab_size", "for", "w", "in", "trg_copy", "]", ")", ":", "\n", "                    ", "print", "(", "'Find OOV in target'", ")", "\n", "\n", "# print('copy_martix      \\n\\t\\t[len=%d]: %s' % (len(example[\"copy_martix\"]), example[\"copy_martix\"]))", "\n", "# print('copy_index  \\n\\t\\t[len=%d]: %s' % (len(example[\"copy_index\"]), example[\"copy_index\"]))", "\n", "\n", "", "", "if", "mode", "==", "'one2one'", ":", "\n", "                ", "return_examples", ".", "append", "(", "example", ")", "\n", "'''\n                For debug\n                if len(oov_list) > 0:\n                    print(\"Found oov\")\n                '''", "\n", "", "else", ":", "\n", "                ", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "", "if", "mode", "==", "'one2many'", "and", "len", "(", "examples", ")", ">", "0", ":", "\n", "            ", "o2m_example", "=", "{", "}", "\n", "keys", "=", "examples", "[", "0", "]", ".", "keys", "(", ")", "\n", "for", "key", "in", "keys", ":", "\n", "                ", "if", "key", ".", "startswith", "(", "'src'", ")", "or", "key", ".", "startswith", "(", "'oov'", ")", "or", "key", ".", "startswith", "(", "'title'", ")", ":", "\n", "                    ", "o2m_example", "[", "key", "]", "=", "examples", "[", "0", "]", "[", "key", "]", "\n", "", "else", ":", "\n", "                    ", "o2m_example", "[", "key", "]", "=", "[", "e", "[", "key", "]", "for", "e", "in", "examples", "]", "\n", "", "", "if", "include_original", ":", "\n", "                ", "assert", "len", "(", "o2m_example", "[", "'src'", "]", ")", "==", "len", "(", "o2m_example", "[", "'src_oov'", "]", ")", "==", "len", "(", "o2m_example", "[", "'src_str'", "]", ")", "\n", "assert", "len", "(", "o2m_example", "[", "'oov_dict'", "]", ")", "==", "len", "(", "o2m_example", "[", "'oov_list'", "]", ")", "\n", "assert", "len", "(", "o2m_example", "[", "'trg'", "]", ")", "==", "len", "(", "o2m_example", "[", "'trg_copy'", "]", ")", "==", "len", "(", "o2m_example", "[", "'trg_str'", "]", ")", "\n", "", "else", ":", "\n", "                ", "assert", "len", "(", "o2m_example", "[", "'src'", "]", ")", "==", "len", "(", "o2m_example", "[", "'src_oov'", "]", ")", "\n", "assert", "len", "(", "o2m_example", "[", "'oov_dict'", "]", ")", "==", "len", "(", "o2m_example", "[", "'oov_list'", "]", ")", "\n", "assert", "len", "(", "o2m_example", "[", "'trg'", "]", ")", "==", "len", "(", "o2m_example", "[", "'trg_copy'", "]", ")", "\n", "", "if", "title_list", "is", "not", "None", ":", "\n", "                ", "assert", "len", "(", "o2m_example", "[", "'title'", "]", ")", "==", "len", "(", "o2m_example", "[", "'title_oov'", "]", ")", "\n", "\n", "", "return_examples", ".", "append", "(", "o2m_example", ")", "\n", "\n", "", "", "print", "(", "'Find #(oov_target)/#(all) = %d/%d'", "%", "(", "oov_target", ",", "len", "(", "return_examples", ")", ")", ")", "\n", "print", "(", "'Find max_oov_len = %d'", "%", "(", "max_oov_len", ")", ")", "\n", "print", "(", "'max_oov sentence: %s'", "%", "str", "(", "max_oov_sent", ")", ")", "\n", "\n", "return", "return_examples", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.extend_vocab_OOV": [[657, 687], ["src_oov.append", "sorted", "len", "oov_dict.get", "src_oov.append", "src_oov.append", "oov_dict.items", "len"], "function", ["None"], ["", "def", "extend_vocab_OOV", "(", "source_words", ",", "word2idx", ",", "vocab_size", ",", "max_unk_words", ")", ":", "\n", "    ", "\"\"\"\n    Map source words to their ids, including OOV words. Also return a list of OOVs in the article.\n    WARNING: if the number of oovs in the source text is more than max_unk_words, ignore and replace them as <unk>\n    Args:\n        source_words: list of words (strings)\n        word2idx: vocab word2idx\n        vocab_size: the maximum acceptable index of word in vocab\n    Returns:\n        ids: A list of word ids (integers); OOVs are represented by their temporary article OOV number. If the vocabulary size is 50k and the article has 3 OOVs, then these temporary OOV numbers will be 50000, 50001, 50002.\n        oovs: A list of the OOV words in the article (strings), in the order corresponding to their temporary article OOV numbers.\n    \"\"\"", "\n", "src_oov", "=", "[", "]", "\n", "oov_dict", "=", "{", "}", "\n", "for", "w", "in", "source_words", ":", "\n", "        ", "if", "w", "in", "word2idx", "and", "word2idx", "[", "w", "]", "<", "vocab_size", ":", "# a OOV can be either outside the vocab or id>=vocab_size", "\n", "            ", "src_oov", ".", "append", "(", "word2idx", "[", "w", "]", ")", "\n", "", "else", ":", "\n", "            ", "if", "len", "(", "oov_dict", ")", "<", "max_unk_words", ":", "\n", "# e.g. 50000 for the first article OOV, 50001 for the second...", "\n", "                ", "word_id", "=", "oov_dict", ".", "get", "(", "w", ",", "len", "(", "oov_dict", ")", "+", "vocab_size", ")", "\n", "oov_dict", "[", "w", "]", "=", "word_id", "\n", "src_oov", ".", "append", "(", "word_id", ")", "\n", "", "else", ":", "\n", "# exceeds the maximum number of acceptable oov words, replace it with <unk>", "\n", "                ", "word_id", "=", "word2idx", "[", "UNK_WORD", "]", "\n", "src_oov", ".", "append", "(", "word_id", ")", "\n", "\n", "", "", "", "oov_list", "=", "[", "w", "for", "w", ",", "w_id", "in", "sorted", "(", "oov_dict", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "]", "\n", "return", "src_oov", ",", "oov_dict", ",", "oov_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.io.copy_martix": [[689, 701], ["numpy.zeros", "range", "len", "range", "len", "len", "len"], "function", ["None"], ["", "def", "copy_martix", "(", "source", ",", "target", ")", ":", "\n", "    ", "'''\n    For reproduce Gu's method\n    return the copy matrix, size = [nb_sample, max_len_source, max_len_target]\n    cc_matrix[i][j]=1 if i-th word in target matches the i-th word in source\n    '''", "\n", "cc", "=", "np", ".", "zeros", "(", "(", "len", "(", "target", ")", ",", "len", "(", "source", ")", ")", ",", "dtype", "=", "'float32'", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "target", ")", ")", ":", "# go over each word in target (all target have same length after padding)", "\n", "        ", "for", "j", "in", "range", "(", "len", "(", "source", ")", ")", ":", "# go over each word in source", "\n", "            ", "if", "source", "[", "j", "]", "==", "target", "[", "i", "]", ":", "# if word match, set cc[k][j][i] = 1. Don't count non-word(source[k, i]=0)", "\n", "                ", "cc", "[", "i", "]", "[", "j", "]", "=", "1.", "\n", "", "", "", "return", "cc", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.sample_list_to_str_2dlist": [[7, 18], ["zip", "prediction_to_sentence", "split_word_list_by_delimiter", "pred_str_2dlist.append"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.prediction_to_sentence", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.split_word_list_by_delimiter"], ["def", "sample_list_to_str_2dlist", "(", "sample_list", ",", "oov_lists", ",", "idx2word", ",", "vocab_size", ",", "eos_idx", ",", "delimiter_word", ",", "unk_idx", "=", "None", ",", "replace_unk", "=", "False", ",", "src_str_list", "=", "None", ",", "separate_present_absent", "=", "False", ",", "present_absent_delimiter_word", "=", "None", ")", ":", "\n", "    ", "\"\"\"Convert a list of sample dict to a 2d list of predicted keyphrases\"\"\"", "\n", "pred_str_2dlist", "=", "[", "]", "#  a 2dlist, len(pred_str_2d_list)=batch_size, len(pred_str_2d_list[0])=", "\n", "for", "sample", ",", "oov", ",", "src_word_list", "in", "zip", "(", "sample_list", ",", "oov_lists", ",", "src_str_list", ")", ":", "\n", "# sample['prediction']: list of 0-dim tensor, len=trg_len", "\n", "# sample['attention']: tensor with size [trg_len, src_len]", "\n", "        ", "word_list", "=", "prediction_to_sentence", "(", "sample", "[", "'prediction'", "]", ",", "idx2word", ",", "vocab_size", ",", "oov", ",", "eos_idx", ",", "unk_idx", ",", "replace_unk", ",", "src_word_list", ",", "sample", "[", "'attention'", "]", ")", "\n", "pred_str_list", "=", "split_word_list_by_delimiter", "(", "word_list", ",", "delimiter_word", ",", "separate_present_absent", ",", "present_absent_delimiter_word", ")", "\n", "#pred_str_list = split_concated_keyphrases(word_list, delimiter_word)", "\n", "pred_str_2dlist", ".", "append", "(", "pred_str_list", ")", "\n", "", "return", "pred_str_2dlist", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_batch_reward": [[20, 39], ["numpy.zeros", "enumerate", "len", "len", "zip", "reward.compute_reward", "ValueError"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_reward"], ["", "def", "compute_batch_reward", "(", "pred_str_2dlist", ",", "trg_str_2dlist", ",", "batch_size", ",", "reward_type", "=", "'f1'", ",", "topk", "=", "10", ",", "match_type", "=", "\"exact\"", ",", "regularization_factor", "=", "0.0", ",", "regularization_type", "=", "0", ",", "entropy", "=", "None", ",", "bert", "=", "None", ")", ":", "\n", "    ", "assert", "len", "(", "trg_str_2dlist", ")", "==", "batch_size", "\n", "assert", "len", "(", "pred_str_2dlist", ")", "==", "batch_size", "\n", "reward", "=", "np", ".", "zeros", "(", "batch_size", ")", "\n", "\n", "if", "regularization_type", "==", "2", ":", "\n", "        ", "if", "entropy", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Entropy should not be none when regularization type is 2'", ")", "\n", "", "assert", "reward", ".", "shape", "[", "0", "]", "==", "entropy", ".", "shape", "[", "0", "]", "\n", "\n", "", "for", "idx", ",", "(", "trg_str_list", ",", "pred_str_list", ")", "in", "enumerate", "(", "zip", "(", "trg_str_2dlist", ",", "pred_str_2dlist", ")", ")", ":", "\n", "# trg_str_list, list of word list, len = number of target keyphrases, trg_str_list[i] = word list of i-th target keyphrase", "\n", "# pred_str_list, list of word list, len = number of predicted keyphrases", "\n", "        ", "if", "entropy", "is", "None", ":", "\n", "            ", "entropy_idx", "=", "None", "\n", "", "else", ":", "\n", "            ", "entropy_idx", "=", "entropy", "[", "idx", "]", "\n", "", "reward", "[", "idx", "]", "=", "compute_reward", "(", "pred_str_list", ",", "trg_str_list", ",", "reward_type", ",", "topk", ",", "match_type", ",", "regularization_factor", ",", "regularization_type", ",", "entropy_idx", ",", "bert", "=", "bert", ")", "\n", "", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_reward": [[41, 214], ["len", "stem_str_list", "stem_str_list", "check_duplicate_keyphrases", "check_duplicate_keyphrases", "len", "len", "zip", "compute_match_result", "compute_classification_metrics_at_k", "zip", "zip", "penalized_stemmed_pred_str_list.append", "penalized_stemmed_pred_str_list.append", "compute_match_result", "compute_classification_metrics_at_k", "compute_match_result", "ndcg_at_k", "compute_match_result", "sum", "compute_match_result", "alpha_ndcg_at_k", "compute_match_result", "alpha_dcg_at_k", "compute_match_result", "average_precision_at_k", "compute_match_result", "compute_classification_metrics_at_k", "compute_match_result", "average_precision_at_k", "compute_phrase_level_reward", "compute_phrase_level_reward", "compute_phrase_level_reward", "compute_phrase_level_reward_add", "compute_phrase_level_reward_add_coverage", "compute_phrase_level_reward_add_coverage_descending", "compute_phrase_level_reward_15", "compute_match_result", "compute_classification_metrics_at_k", "compute_phrase_level_reward", "compute_match_result", "compute_classification_metrics_at_k", "compute_phrase_level_reward_15", "compute_fg_score_k", "compute_fg_score_k1", "compute_fg_score_k2", "compute_fg_score_k3", "compute_fg_score_k4", "compute_phrase_level_reward", "compute_phrase_level_reward"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_str_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.string_helper.stem_str_list", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_duplicate_keyphrases", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.check_duplicate_keyphrases", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics_at_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics_at_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.ndcg_at_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.alpha_ndcg_at_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.alpha_dcg_at_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.average_precision_at_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics_at_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.average_precision_at_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward_add", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward_add_coverage", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward_add_coverage_descending", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward_15", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics_at_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_match_result", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_classification_metrics_at_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward_15", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_fg_score_k", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_fg_score_k1", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_fg_score_k2", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_fg_score_k3", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_fg_score_k4", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.None.evaluate_prediction.compute_phrase_level_reward"], ["", "def", "compute_reward", "(", "pred_str_list", ",", "trg_str_list", ",", "reward_type", ",", "topk", ",", "match_type", "=", "\"exact\"", ",", "regularization_factor", "=", "0.0", ",", "regularization_type", "=", "0", ",", "entropy", "=", "None", ",", "bert", "=", "None", ")", ":", "\n", "    ", "num_predictions", "=", "len", "(", "pred_str_list", ")", "\n", "# perform stemming", "\n", "stemmed_trg_str_list", "=", "stem_str_list", "(", "trg_str_list", ")", "\n", "stemmed_pred_str_list", "=", "stem_str_list", "(", "pred_str_list", ")", "\n", "\n", "trg_str_unique_filter", "=", "check_duplicate_keyphrases", "(", "\n", "stemmed_trg_str_list", ")", "# a boolean nparray, true if not duplicated", "\n", "pred_str_unique_filter", "=", "check_duplicate_keyphrases", "(", "stemmed_pred_str_list", ")", "\n", "\n", "unique_stemmed_trg_str_list", "=", "[", "word_list", "for", "word_list", ",", "is_keep", "in", "zip", "(", "stemmed_trg_str_list", ",", "trg_str_unique_filter", ")", "\n", "if", "\n", "is_keep", "]", "\n", "unique_stemmed_pred_str_list", "=", "[", "word_list", "for", "word_list", ",", "is_keep", "in", "\n", "zip", "(", "stemmed_pred_str_list", ",", "pred_str_unique_filter", ")", "if", "\n", "is_keep", "]", "\n", "num_unique_targets", "=", "len", "(", "unique_stemmed_trg_str_list", ")", "\n", "num_unique_predictions", "=", "len", "(", "unique_stemmed_pred_str_list", ")", "\n", "\n", "# replace all duplicate keyphrases by a <PAD> token, i.e., treat it as a incorrect keyphrase", "\n", "penalized_stemmed_pred_str_list", "=", "[", "]", "\n", "for", "pred_word_list", ",", "is_unique", "in", "zip", "(", "stemmed_pred_str_list", ",", "pred_str_unique_filter", ")", ":", "\n", "        ", "if", "is_unique", ":", "\n", "            ", "penalized_stemmed_pred_str_list", ".", "append", "(", "pred_word_list", ")", "\n", "", "else", ":", "\n", "            ", "penalized_stemmed_pred_str_list", ".", "append", "(", "[", "'<pad>'", "]", ")", "\n", "# ============", "\n", "\n", "", "", "if", "regularization_type", "==", "1", ":", "\n", "        ", "\"\"\"\n        if num_predictions > 0:\n            duplicate_predictions_fraction = 1 - num_unique_predictions/num_predictions\n        else:\n            duplicate_predictions_fraction = 1.0\n        regularization = -duplicate_predictions_fraction\n        \"\"\"", "\n", "if", "num_predictions", ">", "0", ":", "\n", "            ", "unique_prediction_fraction", "=", "num_unique_predictions", "/", "num_predictions", "\n", "", "else", ":", "\n", "            ", "unique_prediction_fraction", "=", "0", "\n", "", "regularization", "=", "unique_prediction_fraction", "\n", "", "elif", "regularization_type", "==", "2", ":", "\n", "        ", "regularization", "=", "entropy", "\n", "", "else", ":", "\n", "        ", "regularization", "=", "0.0", "\n", "\n", "# regularization = regularization_factor * regularization", "\n", "\n", "", "if", "reward_type", "==", "0", ":", "# f1", "\n", "# boolean np array to indicate which prediction matches the target", "\n", "        ", "is_match", "=", "compute_match_result", "(", "trg_str_list", "=", "unique_stemmed_trg_str_list", ",", "\n", "pred_str_list", "=", "unique_stemmed_pred_str_list", ",", "type", "=", "match_type", ",", "dimension", "=", "1", ")", "\n", "precision_k", ",", "recall_k", ",", "f1_k", ",", "_", ",", "_", "=", "compute_classification_metrics_at_k", "(", "is_match", ",", "num_unique_predictions", ",", "\n", "num_unique_targets", ",", "topk", "=", "topk", ")", "\n", "tmp_reward", "=", "f1_k", "\n", "", "elif", "reward_type", "==", "1", ":", "# recall", "\n", "# boolean np array to indicate which prediction matches the target", "\n", "        ", "is_match", "=", "compute_match_result", "(", "trg_str_list", "=", "unique_stemmed_trg_str_list", ",", "\n", "pred_str_list", "=", "unique_stemmed_pred_str_list", ",", "type", "=", "match_type", ",", "dimension", "=", "1", ")", "\n", "precision_k", ",", "recall_k", ",", "f1_k", ",", "_", ",", "_", "=", "compute_classification_metrics_at_k", "(", "is_match", ",", "num_unique_predictions", ",", "\n", "num_unique_targets", ",", "topk", "=", "topk", ")", "\n", "tmp_reward", "=", "recall_k", "\n", "", "elif", "reward_type", "==", "2", ":", "# ndcg", "\n", "# boolean np array to indicate which prediction matches the target", "\n", "        ", "is_match", "=", "compute_match_result", "(", "trg_str_list", "=", "unique_stemmed_trg_str_list", ",", "\n", "pred_str_list", "=", "unique_stemmed_pred_str_list", ",", "type", "=", "match_type", ",", "dimension", "=", "1", ")", "\n", "ndcg_k", "=", "ndcg_at_k", "(", "is_match", ",", "topk", ",", "method", "=", "1", ")", "\n", "tmp_reward", "=", "ndcg_k", "\n", "", "elif", "reward_type", "==", "3", ":", "# accuracy", "\n", "# boolean np array to indicate which prediction matches the target", "\n", "        ", "is_match", "=", "compute_match_result", "(", "trg_str_list", "=", "unique_stemmed_trg_str_list", ",", "\n", "pred_str_list", "=", "unique_stemmed_pred_str_list", ",", "type", "=", "match_type", ",", "dimension", "=", "1", ")", "\n", "acc", "=", "sum", "(", "is_match", ")", "/", "is_match", ".", "shape", "[", "0", "]", "\n", "tmp_reward", "=", "acc", "\n", "", "elif", "reward_type", "==", "4", ":", "# alpha-ndcg", "\n", "# boolean np array to indicate which prediction matches the target", "\n", "        ", "is_match_2d", "=", "compute_match_result", "(", "trg_str_list", "=", "unique_stemmed_trg_str_list", ",", "\n", "pred_str_list", "=", "unique_stemmed_pred_str_list", ",", "type", "=", "match_type", ",", "dimension", "=", "2", ")", "\n", "# is_match_2d: [num_trg_str, num_pred_str]", "\n", "tmp_reward", "=", "alpha_ndcg_at_k", "(", "is_match_2d", ",", "topk", ",", "method", "=", "1", ",", "alpha", "=", "0.5", ")", "\n", "", "elif", "reward_type", "==", "5", ":", "# alpha-dcg", "\n", "# boolean np array to indicate which prediction matches the target", "\n", "        ", "is_match_2d", "=", "compute_match_result", "(", "trg_str_list", "=", "unique_stemmed_trg_str_list", ",", "\n", "pred_str_list", "=", "unique_stemmed_pred_str_list", ",", "type", "=", "match_type", ",", "dimension", "=", "2", ")", "\n", "# is_match_2d: [num_trg_str, num_pred_str]", "\n", "tmp_reward", "=", "alpha_dcg_at_k", "(", "is_match_2d", ",", "topk", ",", "method", "=", "1", ",", "alpha", "=", "0.5", ")", "\n", "", "elif", "reward_type", "==", "6", ":", "# average precision (AP)", "\n", "# boolean np array to indicate which prediction matches the target", "\n", "        ", "is_match", "=", "compute_match_result", "(", "trg_str_list", "=", "unique_stemmed_trg_str_list", ",", "\n", "pred_str_list", "=", "unique_stemmed_pred_str_list", ",", "type", "=", "match_type", ",", "dimension", "=", "1", ")", "\n", "tmp_reward", "=", "average_precision_at_k", "(", "is_match", ",", "topk", ",", "num_unique_predictions", ",", "num_unique_targets", ")", "\n", "", "elif", "reward_type", "==", "7", ":", "# f1 while treating all duplication as incorrect guess", "\n", "# boolean np array to indicate which prediction matches the target", "\n", "        ", "is_match", "=", "compute_match_result", "(", "trg_str_list", "=", "unique_stemmed_trg_str_list", ",", "\n", "pred_str_list", "=", "penalized_stemmed_pred_str_list", ",", "type", "=", "match_type", ",", "dimension", "=", "1", ")", "\n", "precision_k", ",", "recall_k", ",", "f1_k", ",", "_", ",", "_", "=", "compute_classification_metrics_at_k", "(", "is_match", ",", "num_predictions", ",", "\n", "num_unique_targets", ",", "topk", "=", "topk", ")", "\n", "tmp_reward", "=", "f1_k", "\n", "", "elif", "reward_type", "==", "8", ":", "# AP while treating all duplication as incorrect guess", "\n", "        ", "is_match", "=", "compute_match_result", "(", "trg_str_list", "=", "unique_stemmed_trg_str_list", ",", "\n", "pred_str_list", "=", "penalized_stemmed_pred_str_list", ",", "type", "=", "match_type", ",", "dimension", "=", "1", ")", "\n", "tmp_reward", "=", "average_precision_at_k", "(", "is_match", ",", "topk", ",", "num_predictions", ",", "num_unique_targets", ")", "\n", "", "elif", "reward_type", "==", "9", ":", "#phrase reward   edit distance", "\n", "        ", "tmp_reward", "=", "compute_phrase_level_reward", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "\n", "token_reward_type", "=", "1", ")", "\n", "", "elif", "reward_type", "==", "10", ":", "#phrase reward    token f1", "\n", "        ", "tmp_reward", "=", "compute_phrase_level_reward", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "\n", "token_reward_type", "=", "2", ")", "\n", "", "elif", "reward_type", "==", "11", ":", "#phrase reward token f1 + edit distance", "\n", "        ", "tmp_reward", "=", "compute_phrase_level_reward", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "\n", "token_reward_type", "=", "3", ")", "\n", "", "elif", "reward_type", "==", "12", ":", "#\u7d2f\u52a0reward \u9519\u8bef\u4f1a\u5012\u6263", "\n", "        ", "tmp_reward", "=", "compute_phrase_level_reward_add", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ")", "\n", "", "elif", "reward_type", "==", "13", ":", "\n", "        ", "tmp_reward", "=", "compute_phrase_level_reward_add_coverage", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ")", "\n", "", "elif", "reward_type", "==", "14", ":", "\n", "        ", "tmp_reward", "=", "compute_phrase_level_reward_add_coverage_descending", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ")", "\n", "", "elif", "reward_type", "==", "15", ":", "\n", "        ", "tmp_reward", "=", "compute_phrase_level_reward_15", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "\n", "token_reward_type", "=", "1", ")", "\n", "", "elif", "reward_type", "==", "16", ":", "\n", "        ", "is_match", "=", "compute_match_result", "(", "trg_str_list", "=", "unique_stemmed_trg_str_list", ",", "\n", "pred_str_list", "=", "penalized_stemmed_pred_str_list", ",", "type", "=", "match_type", ",", "dimension", "=", "1", ")", "\n", "precision_k", ",", "recall_k", ",", "f1_k", ",", "_", ",", "_", "=", "compute_classification_metrics_at_k", "(", "is_match", ",", "num_predictions", ",", "\n", "num_unique_targets", ",", "topk", "=", "topk", ")", "\n", "tmp_reward", "=", "compute_phrase_level_reward", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "\n", "token_reward_type", "=", "1", ")", "\n", "tmp_reward", "=", "(", "tmp_reward", "+", "f1_k", ")", "/", "2.0", "\n", "\n", "", "elif", "reward_type", "==", "17", ":", "\n", "        ", "is_match", "=", "compute_match_result", "(", "trg_str_list", "=", "unique_stemmed_trg_str_list", ",", "\n", "pred_str_list", "=", "penalized_stemmed_pred_str_list", ",", "type", "=", "match_type", ",", "dimension", "=", "1", ")", "\n", "precision_k", ",", "recall_k", ",", "f1_k", ",", "_", ",", "_", "=", "compute_classification_metrics_at_k", "(", "is_match", ",", "num_predictions", ",", "\n", "num_unique_targets", ",", "topk", "=", "topk", ")", "\n", "tmp_reward", "=", "compute_phrase_level_reward_15", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "\n", "token_reward_type", "=", "1", ")", "\n", "tmp_reward", "=", "(", "tmp_reward", "+", "f1_k", ")", "/", "2.0", "\n", "", "elif", "reward_type", "==", "18", ":", "\n", "        ", "tmp_reward", "=", "compute_fg_score_k", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "eva", "=", "0", ")", "\n", "#\u6d88\u878d1", "\n", "", "elif", "reward_type", "==", "31", ":", "\n", "        ", "tmp_reward", "=", "compute_fg_score_k1", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "eva", "=", "0", ")", "\n", "#\u6d88\u878d2", "\n", "", "elif", "reward_type", "==", "32", ":", "\n", "        ", "tmp_reward", "=", "compute_fg_score_k2", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "eva", "=", "0", ")", "\n", "#\u6d88\u878d3", "\n", "", "elif", "reward_type", "==", "33", ":", "\n", "        ", "tmp_reward", "=", "compute_fg_score_k3", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "eva", "=", "0", ")", "\n", "#\u6d88\u878d4", "\n", "", "elif", "reward_type", "==", "34", ":", "\n", "        ", "tmp_reward", "=", "compute_fg_score_k4", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "eva", "=", "0", ")", "\n", "\n", "", "elif", "reward_type", "==", "20", ":", "\n", "        ", "tmp_reward", "=", "compute_phrase_level_reward", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "\n", "token_reward_type", "=", "4", ",", "bert", "=", "bert", ")", "\n", "", "elif", "reward_type", "==", "21", ":", "\n", "        ", "tmp_reward", "=", "compute_phrase_level_reward", "(", "unique_stemmed_trg_str_list", ",", "penalized_stemmed_pred_str_list", ",", "\n", "token_reward_type", "=", "5", ",", "bert", "=", "bert", ")", "\n", "", "else", ":", "\n", "        ", "pass", "\n", "\n", "\n", "\n", "#print(is_match.shape)", "\n", "#exit(0)", "\n", "\n", "# Add the regularization term to the reward only if regularization type != 0", "\n", "", "if", "regularization_type", "==", "0", "or", "regularization_factor", "==", "0", ":", "\n", "        ", "reward", "=", "tmp_reward", "\n", "", "else", ":", "\n", "        ", "reward", "=", "(", "1", "-", "regularization_factor", ")", "*", "tmp_reward", "+", "regularization_factor", "*", "regularization", "\n", "# reward[idx] += regularization", "\n", "", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_present_absent_reward": [[216, 250], ["len", "numpy.zeros", "enumerate", "zip", "enumerate", "enumerate", "reward.compute_reward", "reward.compute_reward"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_reward"], ["", "def", "compute_present_absent_reward", "(", "pred_str_2dlist", ",", "trg_str_2dlist", ",", "reward_type", "=", "'f1'", ",", "topk", "=", "10", ",", "match_type", "=", "\"exact\"", ",", "regularization_factor", "=", "0.0", ",", "regularization_type", "=", "0", ",", "entropy", "=", "None", ",", "bert", "=", "None", ")", ":", "\n", "    ", "batch_size", "=", "len", "(", "pred_str_2dlist", ")", "\n", "present_absent_reward", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "2", ")", ")", "\n", "\n", "for", "batch_idx", ",", "(", "pred_str_list", ",", "trg_str_list", ")", "in", "enumerate", "(", "zip", "(", "pred_str_2dlist", ",", "trg_str_2dlist", ")", ")", ":", "\n", "        ", "pred_peos_position", "=", "0", "\n", "trg_peos_position", "=", "0", "\n", "pred_peos_counter", "=", "0", "\n", "\n", "for", "pred_phrase_idx", ",", "pred_word_list", "in", "enumerate", "(", "pred_str_list", ")", ":", "\n", "            ", "if", "pred_word_list", "[", "0", "]", "==", "pykp", ".", "io", ".", "PEOS_WORD", ":", "\n", "                ", "if", "pred_peos_counter", "==", "0", ":", "# only consider the first peos", "\n", "                    ", "pred_peos_position", "=", "pred_phrase_idx", "\n", "", "pred_peos_counter", "+=", "1", "\n", "\n", "", "", "present_pred_str_list", "=", "pred_str_list", "[", "0", ":", "pred_peos_position", "]", "\n", "absent_pred_str_list", "=", "pred_str_list", "[", "pred_peos_position", "+", "1", ":", "]", "\n", "\n", "for", "trg_phrase_idx", ",", "trg_word_list", "in", "enumerate", "(", "trg_str_list", ")", ":", "\n", "            ", "if", "trg_word_list", "[", "0", "]", "==", "pykp", ".", "io", ".", "PEOS_WORD", ":", "\n", "                ", "trg_peos_position", "=", "trg_phrase_idx", "\n", "", "", "present_trg_str_list", "=", "trg_str_list", "[", "0", ":", "trg_peos_position", "]", "\n", "absent_trg_str_list", "=", "trg_str_list", "[", "trg_peos_position", "+", "1", ":", "]", "\n", "\n", "present_reward", "=", "compute_reward", "(", "present_pred_str_list", ",", "present_trg_str_list", ",", "reward_type", ",", "topk", ",", "match_type", ",", "regularization_factor", ",", "regularization_type", ",", "entropy", ",", "bert", ")", "\n", "absent_reward", "=", "compute_reward", "(", "absent_pred_str_list", ",", "absent_trg_str_list", ",", "reward_type", ",", "topk", ",", "match_type", ",", "regularization_factor", ",", "regularization_type", ",", "entropy", ",", "bert", ")", "\n", "\n", "present_absent_reward", "[", "batch_idx", ",", "0", "]", "=", "present_reward", "\n", "present_absent_reward", "[", "batch_idx", ",", "1", "]", "=", "absent_reward", "\n", "\n", "# insert the reward of present prediction at the location of peos in the prediction str", "\n", "#stepwise_reward[batch_idx, location_of_peos_for_each_batch[batch_idx]] = present_reward", "\n", "\n", "", "return", "present_absent_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.present_absent_reward_to_stepwise_reward": [[252, 262], ["numpy.zeros", "range"], "function", ["None"], ["", "def", "present_absent_reward_to_stepwise_reward", "(", "present_absent_reward", ",", "max_pred_seq_len", ",", "location_of_peos_for_each_batch", ",", "location_of_eos_for_each_batch", ")", ":", "\n", "    ", "batch_size", "=", "present_absent_reward", ".", "shape", "[", "0", "]", "\n", "stepwise_reward", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "max_pred_seq_len", ")", ")", "\n", "for", "batch_i", "in", "range", "(", "batch_size", ")", ":", "\n", "# insert present reward to the location of <peos>", "\n", "        ", "stepwise_reward", "[", "batch_i", ",", "location_of_peos_for_each_batch", "[", "batch_i", "]", "]", "=", "present_absent_reward", "[", "batch_i", ",", "0", "]", "\n", "# insert absent reward to the location of <eos>", "\n", "stepwise_reward", "[", "batch_i", ",", "location_of_eos_for_each_batch", "[", "batch_i", "]", "]", "=", "present_absent_reward", "[", "batch_i", ",", "1", "]", "\n", "\n", "", "return", "stepwise_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_phrase_reward": [[264, 274], ["numpy.zeros", "range", "reward.compute_batch_reward", "reward.compute_batch_reward"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_batch_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_batch_reward"], ["", "def", "compute_phrase_reward", "(", "pred_str_2dlist", ",", "trg_str_2dlist", ",", "batch_size", ",", "max_num_phrases", ",", "reward_shaping", ",", "reward_type", ",", "topk", ",", "match_type", "=", "\"exact\"", ",", "regularization_factor", "=", "0.0", ",", "regularization_type", "=", "0", ",", "entropy", "=", "None", ")", ":", "\n", "    ", "phrase_reward", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "max_num_phrases", ")", ")", "\n", "if", "reward_shaping", ":", "\n", "        ", "for", "t", "in", "range", "(", "max_num_phrases", ")", ":", "\n", "            ", "pred_str_2dlist_at_t", "=", "[", "pred_str_list", "[", ":", "t", "+", "1", "]", "for", "pred_str_list", "in", "pred_str_2dlist", "]", "\n", "phrase_reward", "[", ":", ",", "t", "]", "=", "compute_batch_reward", "(", "pred_str_2dlist_at_t", ",", "trg_str_2dlist", ",", "batch_size", ",", "reward_type", ",", "topk", ",", "match_type", ",", "regularization_factor", ",", "regularization_type", ",", "entropy", ")", "\n", "", "", "else", ":", "\n", "        ", "phrase_reward", "[", ":", ",", "-", "1", "]", "=", "compute_batch_reward", "(", "pred_str_2dlist", ",", "trg_str_2dlist", ",", "batch_size", ",", "reward_type", ",", "\n", "topk", ",", "match_type", ",", "regularization_factor", ",", "regularization_type", ",", "entropy", ")", "\n", "", "return", "phrase_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_phrase_reward_backup": [[276, 286], ["numpy.zeros", "range", "reward.compute_batch_reward", "reward.compute_batch_reward"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_batch_reward", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_batch_reward"], ["", "def", "compute_phrase_reward_backup", "(", "pred_str_2dlist", ",", "trg_str_2dlist", ",", "batch_size", ",", "num_predictions", ",", "reward_shaping", ",", "reward_type", ",", "topk", ",", "match_type", "=", "\"exact\"", ",", "regularization_factor", "=", "0.0", ",", "regularization_type", "=", "0", ",", "entropy", "=", "None", ")", ":", "\n", "    ", "phrase_reward", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "num_predictions", ")", ")", "\n", "if", "reward_shaping", ":", "\n", "        ", "for", "t", "in", "range", "(", "num_predictions", ")", ":", "\n", "            ", "pred_str_2dlist_at_t", "=", "[", "pred_str_list", "[", ":", "t", "+", "1", "]", "for", "pred_str_list", "in", "pred_str_2dlist", "]", "\n", "phrase_reward", "[", ":", ",", "t", "]", "=", "compute_batch_reward", "(", "pred_str_2dlist_at_t", ",", "trg_str_2dlist", ",", "batch_size", ",", "reward_type", ",", "topk", ",", "match_type", ",", "regularization_factor", ",", "regularization_type", ",", "entropy", ")", "\n", "", "", "else", ":", "\n", "        ", "phrase_reward", "[", ":", ",", "num_predictions", "-", "1", "]", "=", "compute_batch_reward", "(", "pred_str_2dlist", ",", "trg_str_2dlist", ",", "batch_size", ",", "reward_type", ",", "\n", "topk", ",", "match_type", ",", "regularization_factor", ",", "regularization_type", ",", "entropy", ")", "\n", "", "return", "phrase_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.shape_reward": [[288, 293], ["numpy.zeros", "numpy.concatenate", "numpy.diff"], "function", ["None"], ["", "def", "shape_reward", "(", "reward_np_array", ")", ":", "\n", "    ", "batch_size", ",", "seq_len", "=", "reward_np_array", ".", "shape", "\n", "left_padding", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "1", ")", ")", "\n", "left_padded_reward", "=", "np", ".", "concatenate", "(", "[", "left_padding", ",", "reward_np_array", "]", ",", "axis", "=", "1", ")", "\n", "return", "np", ".", "diff", "(", "left_padded_reward", ",", "n", "=", "1", ",", "axis", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.phrase_reward_to_stepwise_reward": [[295, 307], ["eos_idx_mask.size", "numpy.zeros", "range", "range", "eos_idx_mask[].item"], "function", ["None"], ["", "def", "phrase_reward_to_stepwise_reward", "(", "phrase_reward", ",", "eos_idx_mask", ")", ":", "\n", "    ", "batch_size", ",", "seq_len", "=", "eos_idx_mask", ".", "size", "(", ")", "\n", "stepwise_reward", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "seq_len", ")", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "pred_cnt", "=", "0", "\n", "for", "j", "in", "range", "(", "seq_len", ")", ":", "\n", "            ", "if", "eos_idx_mask", "[", "i", ",", "j", "]", ".", "item", "(", ")", "==", "1", ":", "\n", "                ", "stepwise_reward", "[", "i", ",", "j", "]", "=", "phrase_reward", "[", "i", ",", "pred_cnt", "]", "\n", "pred_cnt", "+=", "1", "\n", "#elif j == seq_len:", "\n", "#    pass", "\n", "", "", "", "return", "stepwise_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.reward.compute_pg_loss": [[309, 322], ["log_likelihood.view.view", "output_mask.view.view", "q_val_sample.view.view", "torch.sum", "torch.sum"], "function", ["None"], ["", "def", "compute_pg_loss", "(", "log_likelihood", ",", "output_mask", ",", "q_val_sample", ")", ":", "\n", "    ", "\"\"\"\n    :param log_likelihood: [batch_size, prediction_seq_len]\n    :param input_mask: [batch_size, prediction_seq_len]\n    :param q_val_sample: [batch_size, prediction_seq_len]\n    :return:\n    \"\"\"", "\n", "log_likelihood", "=", "log_likelihood", ".", "view", "(", "-", "1", ")", "# [batch_size * prediction_seq_len]", "\n", "output_mask", "=", "output_mask", ".", "view", "(", "-", "1", ")", "# [batch_size * prediction_seq_len]", "\n", "q_val_sample", "=", "q_val_sample", ".", "view", "(", "-", "1", ")", "# [batch_size * prediction_seq_len]", "\n", "objective", "=", "-", "log_likelihood", "*", "output_mask", "*", "q_val_sample", "\n", "objective", "=", "torch", ".", "sum", "(", "objective", ")", "/", "torch", ".", "sum", "(", "output_mask", ")", "\n", "return", "objective", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.attention.Attention.__init__": [[8, 22], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "pykp.masked_softmax.MaskedSoftmax", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.__init__"], ["    ", "def", "__init__", "(", "self", ",", "decoder_size", ",", "memory_bank_size", ",", "coverage_attn", ",", "attn_mode", ")", ":", "\n", "        ", "super", "(", "Attention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# attention", "\n", "if", "attn_mode", "==", "\"concat\"", ":", "\n", "            ", "self", ".", "v", "=", "nn", ".", "Linear", "(", "decoder_size", ",", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "decode_project", "=", "nn", ".", "Linear", "(", "decoder_size", ",", "decoder_size", ")", "\n", "", "self", ".", "memory_project", "=", "nn", ".", "Linear", "(", "memory_bank_size", ",", "decoder_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "coverage_attn", "=", "coverage_attn", "\n", "if", "coverage_attn", ":", "\n", "            ", "self", ".", "coverage_project", "=", "nn", ".", "Linear", "(", "1", ",", "decoder_size", ",", "bias", "=", "False", ")", "\n", "", "self", ".", "softmax", "=", "MaskedSoftmax", "(", "dim", "=", "1", ")", "\n", "# self.softmax = nn.Softmax(dim=1)", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "attn_mode", "=", "attn_mode", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.attention.Attention.score": [[23, 88], ["list", "decoder_state.size", "attention.Attention.view", "memory_bank.size", "memory_bank.view", "attention.Attention.memory_project", "decoder_state.unsqueeze().expand().contiguous", "decoder_state_expanded.view.view.view", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "memory_bank.view", "attention.Attention.memory_project", "attention.Attention.decode_project", "attention.Attention.unsqueeze().expand().contiguous", "dec_feature_expanded.view.view.view", "attention.Attention.tanh", "attention.Attention.v", "coverage.view", "attention.Attention.coverage_project", "attention.Attention.tanh", "decoder_state_expanded.view.view.unsqueeze", "attention.Attention.unsqueeze", "coverage.view", "attention.Attention.coverage_project", "decoder_state.unsqueeze().expand", "attention.Attention.unsqueeze().expand", "decoder_state.unsqueeze", "attention.Attention.unsqueeze"], "methods", ["None"], ["", "def", "score", "(", "self", ",", "memory_bank", ",", "decoder_state", ",", "coverage", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        :param memory_bank: [batch_size, max_input_seq_len, self.num_directions * self.encoder_size]\n        :param decoder_state: [batch_size, decoder_size]\n        :param coverage: [batch_size, max_input_seq_len]\n        :return: score: [batch_size, max_input_seq_len]\n        \"\"\"", "\n", "batch_size", ",", "max_input_seq_len", ",", "memory_bank_size", "=", "list", "(", "memory_bank", ".", "size", "(", ")", ")", "\n", "decoder_size", "=", "decoder_state", ".", "size", "(", "1", ")", "\n", "\n", "if", "self", ".", "attn_mode", "==", "\"general\"", ":", "\n", "# project memory_bank", "\n", "            ", "memory_bank_", "=", "memory_bank", ".", "view", "(", "-", "1", ",", "\n", "memory_bank_size", ")", "# [batch_size*max_input_seq_len, memory_bank_size]", "\n", "\"\"\"\n            if self.coverage_attn:\n                coverage_input = coverage.view(-1, 1)  # [batch_size*max_input_seq_len, 1]\n                memory_bank_ += self.coverage_project(coverage_input)  # [batch_size*max_input_seq_len, decoder_size]\n                memory_bank_ = self.tanh(memory_bank_)\n\n            encoder_feature = self.memory_project(memory_bank_)  # [batch_size*max_input_seq_len, decoder size]\n            \"\"\"", "\n", "\n", "encoder_feature", "=", "self", ".", "memory_project", "(", "memory_bank_", ")", "# [batch_size*max_input_seq_len, decoder size]", "\n", "if", "self", ".", "coverage_attn", ":", "\n", "                ", "coverage_input", "=", "coverage", ".", "view", "(", "-", "1", ",", "1", ")", "# [batch_size*max_input_seq_len, 1]", "\n", "encoder_feature", "+=", "self", ".", "coverage_project", "(", "coverage_input", ")", "# [batch_size*max_input_seq_len, decoder_size]", "\n", "encoder_feature", "=", "self", ".", "tanh", "(", "encoder_feature", ")", "\n", "\n", "# expand decoder state", "\n", "", "decoder_state_expanded", "=", "decoder_state", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "batch_size", ",", "max_input_seq_len", ",", "\n", "decoder_size", ")", ".", "contiguous", "(", ")", "\n", "decoder_state_expanded", "=", "decoder_state_expanded", ".", "view", "(", "-", "1", ",", "\n", "decoder_size", ")", "# [batch_size*max_input_seq_len, decoder_size]", "\n", "# Perform bi-linear operation", "\n", "scores", "=", "torch", ".", "bmm", "(", "decoder_state_expanded", ".", "unsqueeze", "(", "1", ")", ",", "encoder_feature", ".", "unsqueeze", "(", "2", ")", ")", "# [batch_size*max_input_seq_len, 1, 1]", "\n", "\n", "", "else", ":", "# Bahdanau style attention", "\n", "# project memory_bank", "\n", "            ", "memory_bank_", "=", "memory_bank", ".", "view", "(", "-", "1", ",", "memory_bank_size", ")", "# [batch_size*max_input_seq_len, memory_bank_size]", "\n", "encoder_feature", "=", "self", ".", "memory_project", "(", "memory_bank_", ")", "# [batch_size*max_input_seq_len, decoder size]", "\n", "# project decoder state", "\n", "dec_feature", "=", "self", ".", "decode_project", "(", "decoder_state", ")", "# [batch_size, decoder_size]", "\n", "dec_feature_expanded", "=", "dec_feature", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "batch_size", ",", "max_input_seq_len", ",", "\n", "decoder_size", ")", ".", "contiguous", "(", ")", "\n", "dec_feature_expanded", "=", "dec_feature_expanded", ".", "view", "(", "-", "1", ",", "\n", "decoder_size", ")", "# [batch_size*max_input_seq_len, decoder_size]", "\n", "# sum up attention features", "\n", "att_features", "=", "encoder_feature", "+", "dec_feature_expanded", "# [batch_size*max_input_seq_len, decoder_size]", "\n", "\n", "# Apply coverage", "\n", "if", "self", ".", "coverage_attn", ":", "\n", "                ", "coverage_input", "=", "coverage", ".", "view", "(", "-", "1", ",", "1", ")", "# [batch_size*max_input_seq_len, 1]", "\n", "coverage_feature", "=", "self", ".", "coverage_project", "(", "coverage_input", ")", "# [batch_size*max_input_seq_len, decoder_size]", "\n", "# print(coverage.size())", "\n", "# print(coverage_feature.size())", "\n", "# print(att_features.size())", "\n", "att_features", "=", "att_features", "+", "coverage_feature", "\n", "\n", "# compute attention score and normalize them", "\n", "", "e", "=", "self", ".", "tanh", "(", "att_features", ")", "# [batch_size*max_input_seq_len, decoder_size]", "\n", "scores", "=", "self", ".", "v", "(", "e", ")", "# [batch_size*max_input_seq_len, 1]", "\n", "\n", "", "scores", "=", "scores", ".", "view", "(", "-", "1", ",", "max_input_seq_len", ")", "# [batch_size, max_input_seq_len]", "\n", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.attention.Attention.forward": [[89, 152], ["list", "attention.Attention.score", "attention.Attention.softmax", "attn_dist.squeeze.squeeze.unsqueeze", "memory_bank.view.view.view", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "context.squeeze.squeeze.squeeze", "attn_dist.squeeze.squeeze.squeeze", "memory_bank.view.view.size", "memory_bank.view.view.new_ones", "coverage.view.view.view", "attn_dist.squeeze.squeeze.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "context.squeeze.squeeze.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "coverage.view.view.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.attention.Attention.score"], ["", "def", "forward", "(", "self", ",", "decoder_state", ",", "memory_bank", ",", "src_mask", "=", "None", ",", "coverage", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        :param decoder_state: [batch_size, decoder_size]\n        :param memory_bank: [batch_size, max_input_seq_len, self.num_directions * self.encoder_size]\n        :param src_mask: [batch_size, max_input_seq_len]\n        :param coverage: [batch_size, max_input_seq_len]\n        :return: context: [batch_size, self.num_directions * self.encoder_size], attn_dist: [batch_size, max_input_seq_len], coverage: [batch_size, max_input_seq_len]\n        \"\"\"", "\n", "# init dimension info", "\n", "batch_size", ",", "max_input_seq_len", ",", "memory_bank_size", "=", "list", "(", "memory_bank", ".", "size", "(", ")", ")", "\n", "#decoder_size = decoder_state.size(1)", "\n", "\n", "if", "src_mask", "is", "None", ":", "# if it does not supply a source mask, create a dummy mask with all ones", "\n", "            ", "src_mask", "=", "memory_bank", ".", "new_ones", "(", "batch_size", ",", "max_input_seq_len", ")", "\n", "\n", "", "\"\"\"\n        # project memory_bank\n        memory_bank = memory_bank.view(-1, memory_bank_size)  # [batch_size*max_input_seq_len, memory_bank_size]\n        encoder_feature = self.memory_project(memory_bank)  # [batch_size*max_input_seq_len, decoder size]\n\n        # project decoder state\n        dec_feature = self.decode_project(decoder_state)  # [batch_size, decoder_size]\n        dec_feature_expanded = dec_feature.unsqueeze(1).expand(batch_size, max_input_seq_len, decoder_size).contiguous()\n        dec_feature_expanded = dec_feature_expanded.view(-1, decoder_size)  # [batch_size*max_input_seq_len, decoder_size]\n\n        # sum up attention features\n        att_features = encoder_feature + dec_feature_expanded  # [batch_size*max_input_seq_len, decoder_size]\n\n        # Apply coverage\n        if self.coverage_attn:\n            coverage_input = coverage.view(-1, 1)  # [batch_size*max_input_seq_len, 1]\n            coverage_feature = self.coverage_project(coverage_input)  # [batch_size*max_input_seq_len, decoder_size]\n            #print(coverage.size())\n            #print(coverage_feature.size())\n            #print(att_features.size())\n            att_features = att_features + coverage_feature\n\n        # compute attention score and normalize them\n        e = self.tanh(att_features)  # [batch_size*max_input_seq_len, decoder_size]\n        scores = self.v(e)  # [batch_size*max_input_seq_len, 1]\n        scores = scores.view(-1, max_input_seq_len)  # [batch_size, max_input_seq_len]\n        \"\"\"", "\n", "\n", "scores", "=", "self", ".", "score", "(", "memory_bank", ",", "decoder_state", ",", "coverage", ")", "# [batch_size, max_input_seq_len]", "\n", "attn_dist", "=", "self", ".", "softmax", "(", "scores", ",", "mask", "=", "src_mask", ")", "# src_mask: [batch_size, max_input_seq_len]", "\n", "\n", "# Compute weighted sum of memory bank features", "\n", "attn_dist", "=", "attn_dist", ".", "unsqueeze", "(", "1", ")", "# [batch_size, 1, max_input_seq_len]", "\n", "memory_bank", "=", "memory_bank", ".", "view", "(", "-", "1", ",", "max_input_seq_len", ",", "memory_bank_size", ")", "# batch_size, max_input_seq_len, memory_bank_size]", "\n", "context", "=", "torch", ".", "bmm", "(", "attn_dist", ",", "memory_bank", ")", "# [batch_size, 1, memory_bank_size]", "\n", "context", "=", "context", ".", "squeeze", "(", "1", ")", "# [batch_size, memory_bank_size]", "\n", "attn_dist", "=", "attn_dist", ".", "squeeze", "(", "1", ")", "# [batch_size, max_input_seq_len]", "\n", "\n", "# Update coverage", "\n", "if", "self", ".", "coverage_attn", ":", "\n", "            ", "coverage", "=", "coverage", ".", "view", "(", "-", "1", ",", "max_input_seq_len", ")", "\n", "coverage", "=", "coverage", "+", "attn_dist", "\n", "assert", "coverage", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "max_input_seq_len", "]", ")", "\n", "\n", "", "assert", "attn_dist", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "max_input_seq_len", "]", ")", "\n", "assert", "context", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "memory_bank_size", "]", ")", "\n", "\n", "return", "context", ",", "attn_dist", ",", "coverage", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_softmax.MaskedSoftmax.__init__": [[6, 9], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ")", ":", "\n", "        ", "super", "(", "MaskedSoftmax", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_softmax.MaskedSoftmax.forward": [[10, 18], ["torch.softmax", "torch.softmax", "torch.softmax", "dist_.sum", "torch.softmax", "torch.softmax", "torch.softmax", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "logit", ",", "mask", "=", "None", ")", ":", "\n", "        ", "if", "mask", "is", "None", ":", "\n", "            ", "dist", "=", "F", ".", "softmax", "(", "logit", "-", "torch", ".", "max", "(", "logit", ",", "dim", "=", "self", ".", "dim", ",", "keepdim", "=", "True", ")", "[", "0", "]", ",", "dim", "=", "self", ".", "dim", ")", "\n", "", "else", ":", "\n", "            ", "dist_", "=", "F", ".", "softmax", "(", "logit", "-", "torch", ".", "max", "(", "logit", ",", "dim", "=", "self", ".", "dim", ",", "keepdim", "=", "True", ")", "[", "0", "]", ",", "dim", "=", "self", ".", "dim", ")", "*", "mask", "\n", "normalization_factor", "=", "dist_", ".", "sum", "(", "self", ".", "dim", ",", "keepdim", "=", "True", ")", "\n", "dist", "=", "dist_", "/", "normalization_factor", "\n", "", "return", "dist", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.__init__": [[17, 184], ["torch.Module.__init__", "pykp.rnn_decoder.RNNDecoder", "model.Seq2SeqModel.init_weights", "RNNEncoderTG", "RNNEncoderBasic", "pykp.target_encoder.TargetEncoder", "pykp.attention.Attention", "torch.Linear", "torch.Linear", "torch.tanh", "torch.tanh", "torch.GRU", "torch.GRU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "pykp.manager.ManagerBasic"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.__init__", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.init_weights"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Initialize model.\"\"\"", "\n", "super", "(", "Seq2SeqModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "vocab_size", "=", "opt", ".", "vocab_size", "\n", "self", ".", "emb_dim", "=", "opt", ".", "word_vec_size", "\n", "self", ".", "num_directions", "=", "2", "if", "opt", ".", "bidirectional", "else", "1", "\n", "self", ".", "encoder_size", "=", "opt", ".", "encoder_size", "\n", "self", ".", "decoder_size", "=", "opt", ".", "decoder_size", "\n", "#self.ctx_hidden_dim = opt.rnn_size", "\n", "self", ".", "batch_size", "=", "opt", ".", "batch_size", "\n", "self", ".", "bidirectional", "=", "opt", ".", "bidirectional", "\n", "self", ".", "enc_layers", "=", "opt", ".", "enc_layers", "\n", "self", ".", "dec_layers", "=", "opt", ".", "dec_layers", "\n", "self", ".", "dropout", "=", "opt", ".", "dropout", "\n", "\n", "self", ".", "bridge", "=", "opt", ".", "bridge", "\n", "self", ".", "one2many_mode", "=", "opt", ".", "one2many_mode", "\n", "self", ".", "one2many", "=", "opt", ".", "one2many", "\n", "\n", "self", ".", "coverage_attn", "=", "opt", ".", "coverage_attn", "\n", "self", ".", "copy_attn", "=", "opt", ".", "copy_attention", "\n", "\n", "self", ".", "pad_idx_src", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "PAD_WORD", "]", "\n", "self", ".", "pad_idx_trg", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "PAD_WORD", "]", "\n", "self", ".", "bos_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "BOS_WORD", "]", "\n", "self", ".", "eos_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "EOS_WORD", "]", "\n", "self", ".", "unk_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "UNK_WORD", "]", "\n", "self", ".", "sep_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "SEP_WORD", "]", "\n", "self", ".", "orthogonal_loss", "=", "opt", ".", "orthogonal_loss", "\n", "\n", "self", ".", "share_embeddings", "=", "opt", ".", "share_embeddings", "\n", "self", ".", "review_attn", "=", "opt", ".", "review_attn", "\n", "\n", "self", ".", "attn_mode", "=", "opt", ".", "attn_mode", "\n", "\n", "self", ".", "use_target_encoder", "=", "opt", ".", "use_target_encoder", "\n", "self", ".", "target_encoder_size", "=", "opt", ".", "target_encoder_size", "\n", "\n", "self", ".", "device", "=", "opt", ".", "device", "\n", "\n", "self", ".", "separate_present_absent", "=", "opt", ".", "separate_present_absent", "\n", "self", ".", "goal_vector_mode", "=", "opt", ".", "goal_vector_mode", "\n", "self", ".", "goal_vector_size", "=", "opt", ".", "goal_vector_size", "\n", "self", ".", "manager_mode", "=", "opt", ".", "manager_mode", "\n", "self", ".", "title_guided", "=", "opt", ".", "title_guided", "\n", "\n", "if", "self", ".", "separate_present_absent", ":", "\n", "            ", "self", ".", "peos_idx", "=", "opt", ".", "word2idx", "[", "pykp", ".", "io", ".", "PEOS_WORD", "]", "\n", "\n", "", "'''\n        self.attention_mode = opt.attention_mode    # 'dot', 'general', 'concat'\n        self.input_feeding = opt.input_feeding\n\n        self.copy_attention = opt.copy_attention    # bool, enable copy attention or not\n        self.copy_mode = opt.copy_mode         # same to `attention_mode`\n        self.copy_input_feeding = opt.copy_input_feeding\n        self.reuse_copy_attn = opt.reuse_copy_attn\n        self.copy_gate = opt.copy_gate\n\n        self.must_teacher_forcing = opt.must_teacher_forcing\n        self.teacher_forcing_ratio = opt.teacher_forcing_ratio\n        self.scheduled_sampling = opt.scheduled_sampling\n        self.scheduled_sampling_batches = opt.scheduled_sampling_batches\n        self.scheduled_sampling_type = 'inverse_sigmoid'  # decay curve type: linear or inverse_sigmoid\n        self.current_batch = 0  # for scheduled sampling\n\n        self.device = opt.device\n\n        if self.scheduled_sampling:\n            logging.info(\"Applying scheduled sampling with %s decay for the first %d batches\" % (self.scheduled_sampling_type, self.scheduled_sampling_batches))\n        if self.must_teacher_forcing or self.teacher_forcing_ratio >= 1:\n            logging.info(\"Training with All Teacher Forcing\")\n        elif self.teacher_forcing_ratio <= 0:\n            logging.info(\"Training with All Sampling\")\n        else:\n            logging.info(\"Training with Teacher Forcing with static rate=%f\" % self.teacher_forcing_ratio)\n\n        self.get_mask = GetMask(self.pad_idx_src)\n        '''", "\n", "'''\n        self.embedding = nn.Embedding(\n            self.vocab_size,\n            self.emb_dim,\n            self.pad_idx_src\n        )\n        '''", "\n", "if", "self", ".", "title_guided", ":", "\n", "            ", "self", ".", "encoder", "=", "RNNEncoderTG", "(", "\n", "vocab_size", "=", "self", ".", "vocab_size", ",", "\n", "embed_size", "=", "self", ".", "emb_dim", ",", "\n", "hidden_size", "=", "self", ".", "encoder_size", ",", "\n", "num_layers", "=", "self", ".", "enc_layers", ",", "\n", "bidirectional", "=", "self", ".", "bidirectional", ",", "\n", "pad_token", "=", "self", ".", "pad_idx_src", ",", "\n", "dropout", "=", "self", ".", "dropout", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder", "=", "RNNEncoderBasic", "(", "\n", "vocab_size", "=", "self", ".", "vocab_size", ",", "\n", "embed_size", "=", "self", ".", "emb_dim", ",", "\n", "hidden_size", "=", "self", ".", "encoder_size", ",", "\n", "num_layers", "=", "self", ".", "enc_layers", ",", "\n", "bidirectional", "=", "self", ".", "bidirectional", ",", "\n", "pad_token", "=", "self", ".", "pad_idx_src", ",", "\n", "dropout", "=", "self", ".", "dropout", "\n", ")", "\n", "\n", "", "self", ".", "decoder", "=", "RNNDecoder", "(", "\n", "vocab_size", "=", "self", ".", "vocab_size", ",", "\n", "embed_size", "=", "self", ".", "emb_dim", ",", "\n", "hidden_size", "=", "self", ".", "decoder_size", ",", "\n", "num_layers", "=", "self", ".", "dec_layers", ",", "\n", "memory_bank_size", "=", "self", ".", "num_directions", "*", "self", ".", "encoder_size", ",", "\n", "coverage_attn", "=", "self", ".", "coverage_attn", ",", "\n", "copy_attn", "=", "self", ".", "copy_attn", ",", "\n", "review_attn", "=", "self", ".", "review_attn", ",", "\n", "pad_idx", "=", "self", ".", "pad_idx_trg", ",", "\n", "attn_mode", "=", "self", ".", "attn_mode", ",", "\n", "dropout", "=", "self", ".", "dropout", ",", "\n", "use_target_encoder", "=", "self", ".", "use_target_encoder", ",", "\n", "target_encoder_size", "=", "self", ".", "target_encoder_size", ",", "\n", "goal_vector_mode", "=", "self", ".", "goal_vector_mode", ",", "\n", "goal_vector_size", "=", "self", ".", "goal_vector_size", "\n", ")", "\n", "\n", "if", "self", ".", "use_target_encoder", ":", "\n", "            ", "self", ".", "target_encoder", "=", "TargetEncoder", "(", "\n", "embed_size", "=", "self", ".", "emb_dim", ",", "\n", "hidden_size", "=", "self", ".", "target_encoder_size", ",", "\n", "vocab_size", "=", "self", ".", "vocab_size", ",", "\n", "pad_idx", "=", "self", ".", "pad_idx_trg", "\n", ")", "\n", "# use the same embedding layer as that in the decoder", "\n", "self", ".", "target_encoder", ".", "embedding", ".", "weight", "=", "self", ".", "decoder", ".", "embedding", ".", "weight", "\n", "self", ".", "target_encoder_attention", "=", "Attention", "(", "\n", "self", ".", "target_encoder_size", ",", "\n", "memory_bank_size", "=", "self", ".", "num_directions", "*", "self", ".", "encoder_size", ",", "\n", "coverage_attn", "=", "False", ",", "\n", "attn_mode", "=", "\"general\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "bridge", "==", "'dense'", ":", "\n", "            ", "self", ".", "bridge_layer", "=", "nn", ".", "Linear", "(", "self", ".", "encoder_size", "*", "self", ".", "num_directions", ",", "self", ".", "decoder_size", ")", "\n", "", "elif", "opt", ".", "bridge", "==", "'dense_nonlinear'", ":", "\n", "            ", "self", ".", "bridge_layer", "=", "nn", ".", "tanh", "(", "nn", ".", "Linear", "(", "self", ".", "encoder_size", "*", "self", ".", "num_directions", ",", "self", ".", "decoder_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bridge_layer", "=", "None", "\n", "\n", "", "if", "self", ".", "bridge", "==", "'copy'", ":", "\n", "            ", "assert", "self", ".", "encoder_size", "*", "self", ".", "num_directions", "==", "self", ".", "decoder_size", ",", "'encoder hidden size and decoder hidden size are not match, please use a bridge layer'", "\n", "\n", "", "if", "self", ".", "separate_present_absent", "and", "self", ".", "goal_vector_mode", ">", "0", ":", "\n", "            ", "if", "self", ".", "manager_mode", "==", "2", ":", "# use GRU as a manager", "\n", "                ", "self", ".", "manager", "=", "nn", ".", "GRU", "(", "input_size", "=", "self", ".", "decoder_size", ",", "hidden_size", "=", "self", ".", "goal_vector_size", ",", "num_layers", "=", "1", ",", "bidirectional", "=", "False", ",", "batch_first", "=", "False", ",", "dropout", "=", "self", ".", "dropout", ")", "\n", "self", ".", "bridge_manager", "=", "opt", ".", "bridge_manager", "\n", "if", "self", ".", "bridge_manager", ":", "\n", "                    ", "self", ".", "manager_bridge_layer", "=", "nn", ".", "Linear", "(", "self", ".", "encoder_size", "*", "self", ".", "num_directions", ",", "self", ".", "goal_vector_size", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "manager_bridge_layer", "=", "None", "\n", "", "", "elif", "self", ".", "manager_mode", "==", "1", ":", "# use two trainable vectors only", "\n", "                ", "self", ".", "manager", "=", "ManagerBasic", "(", "self", ".", "goal_vector_size", ")", "\n", "\n", "", "", "if", "self", ".", "share_embeddings", ":", "\n", "            ", "self", ".", "encoder", ".", "embedding", ".", "weight", "=", "self", ".", "decoder", ".", "embedding", ".", "weight", "\n", "\n", "", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.init_weights": [[185, 191], ["model.Seq2SeqModel.encoder.embedding.weight.data.uniform_", "model.Seq2SeqModel.decoder.embedding.weight.data.uniform_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initialize weights.\"\"\"", "\n", "initrange", "=", "0.1", "\n", "self", ".", "encoder", ".", "embedding", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "initrange", ",", "initrange", ")", "\n", "if", "not", "self", ".", "share_embeddings", ":", "\n", "            ", "self", ".", "decoder", ".", "embedding", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "initrange", ",", "initrange", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.forward": [[199, 456], ["list", "model.Seq2SeqModel.encoder", "model.Seq2SeqModel.init_decoder_state", "trg.size", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "src.size", "memory_bank.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "encoder_final_state.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "range", "len", "model.Seq2SeqModel.tensor_2dlist_to_tensor", "torch.transpose().contiguous", "torch.transpose().contiguous", "torch.transpose().contiguous", "torch.transpose().contiguous", "torch.zeros_like().requires_grad_", "torch.zeros_like().requires_grad_", "torch.zeros_like().requires_grad_", "torch.zeros_like().requires_grad_", "h_t_init[].unsqueeze", "model.Seq2SeqModel.new_zeros", "trg.new_ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "model.Seq2SeqModel.decoder", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.size", "torch.cat.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "model.Seq2SeqModel.tensor_2dlist_to_tensor", "model.Seq2SeqModel.tensor_2dlist_to_tensor", "model.Seq2SeqModel.size", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "encoder_final_state[].detach", "torch.transpose().contiguous.size", "torch.transpose().contiguous.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.cat.size", "torch.cat.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "trg.new_zeros", "model.Seq2SeqModel.target_encoder", "range", "range", "decoder_dist.unsqueeze", "attn_dist.unsqueeze", "torch.cat.append", "torch.cat.append", "range", "torch.cat.size", "torch.cat.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.cat.size", "torch.cat.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.cat.size", "torch.cat.size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "len", "len", "len", "len", "model.Seq2SeqModel.target_encoder_attention", "torch.cat.append", "torch.cat.append", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "range", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.detach", "torch.cat.detach", "model.Seq2SeqModel.manager", "torch.zeros_like().requires_grad_.unsqueeze", "torch.zeros_like().requires_grad_.unsqueeze", "range", "range", "source_classification_dist.unsqueeze", "re_init_indicators.sum().item", "zip", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "y_t[].item", "delimiter_target_encoder_states_2dlist[].append", "y_t[].item", "y_t_next[].item", "delimiter_decoder_states_2dlist[].append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "re_init_indicators.sum().item", "zip", "h_t[].unsqueeze", "re_init_indicators.sum", "indicator.item", "pred_count.item", "h_t_init[].unsqueeze", "y_t_init[].unsqueeze", "h_t_next[].unsqueeze", "y_t_next[].unsqueeze", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "re_init_indicators.sum", "indicator.item", "pred_count.item", "y_t_init[].unsqueeze", "y_t_next[].unsqueeze"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.init_decoder_state", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.tensor_2dlist_to_tensor", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.tensor_2dlist_to_tensor", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.tensor_2dlist_to_tensor"], ["", "", "def", "forward", "(", "self", ",", "src", ",", "src_lens", ",", "trg", ",", "src_oov", ",", "max_num_oov", ",", "src_mask", ",", "num_trgs", "=", "None", ",", "sampled_source_representation_2dlist", "=", "None", ",", "source_representation_target_list", "=", "None", ",", "title", "=", "None", ",", "title_lens", "=", "None", ",", "title_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        :param src: a LongTensor containing the word indices of source sentences, [batch, src_seq_len], with oov words replaced by unk idx\n        :param src_lens: a list containing the length of src sequences for each batch, with len=batch, with oov words replaced by unk idx\n        :param trg: a LongTensor containing the word indices of target sentences, [batch, trg_seq_len]\n        :param src_oov: a LongTensor containing the word indices of source sentences, [batch, src_seq_len], contains the index of oov words (used by copy)\n        :param max_num_oov: int, max number of oov for each batch\n        :param src_mask: a FloatTensor, [batch, src_seq_len]\n        :param num_trgs: only effective in one2many mode 2, a list of num of targets in each batch, with len=batch_size\n        :param sampled_source_representation_2dlist: only effective when using target encoder, a 2dlist of tensor with dim=[memory_bank_size]\n        :param source_representation_target_list: a list that store the index of ground truth source representation for each batch, dim=[batch_size]\n        :return:\n        \"\"\"", "\n", "batch_size", ",", "max_src_len", "=", "list", "(", "src", ".", "size", "(", ")", ")", "\n", "\n", "# Encoding", "\n", "memory_bank", ",", "encoder_final_state", "=", "self", ".", "encoder", "(", "src", ",", "src_lens", ",", "src_mask", ",", "title", ",", "title_lens", ",", "title_mask", ")", "\n", "assert", "memory_bank", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "max_src_len", ",", "self", ".", "num_directions", "*", "self", ".", "encoder_size", "]", ")", "\n", "assert", "encoder_final_state", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "self", ".", "num_directions", "*", "self", ".", "encoder_size", "]", ")", "\n", "\n", "if", "self", ".", "one2many", "and", "self", ".", "one2many_mode", ">", "1", ":", "\n", "            ", "assert", "num_trgs", "is", "not", "None", ",", "\"If one2many mode is 2, you must supply the number of targets in each sample.\"", "\n", "assert", "len", "(", "num_trgs", ")", "==", "batch_size", ",", "\"The length of num_trgs is incorrect\"", "\n", "\n", "", "if", "self", ".", "use_target_encoder", "and", "sampled_source_representation_2dlist", "is", "not", "None", ":", "\n", "# put the ground-truth encoder representation, need to call detach() first", "\n", "            ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "sampled_source_representation_2dlist", "[", "i", "]", "[", "source_representation_target_list", "[", "i", "]", "]", "=", "encoder_final_state", "[", "i", ",", ":", "]", ".", "detach", "(", ")", "# [memory_bank_size]", "\n", "", "source_representation_sample_size", "=", "len", "(", "sampled_source_representation_2dlist", "[", "0", "]", ")", "\n", "sampled_source_representation", "=", "self", ".", "tensor_2dlist_to_tensor", "(", "\n", "sampled_source_representation_2dlist", ",", "batch_size", ",", "self", ".", "num_directions", "*", "self", ".", "encoder_size", ",", "[", "source_representation_sample_size", "]", "*", "batch_size", ")", "\n", "sampled_source_representation", "=", "torch", ".", "transpose", "(", "sampled_source_representation", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "assert", "sampled_source_representation", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "source_representation_sample_size", ",", "self", ".", "num_directions", "*", "self", ".", "encoder_size", "]", ")", "\n", "# sampled_source_representation: [batch_size, source_representation_sample_size, memory_bank_size]", "\n", "\n", "# Decoding", "\n", "", "h_t_init", "=", "self", ".", "init_decoder_state", "(", "encoder_final_state", ")", "# [dec_layers, batch_size, decoder_size]", "\n", "max_target_length", "=", "trg", ".", "size", "(", "1", ")", "\n", "#context = self.init_context(memory_bank)  # [batch, memory_bank_size]", "\n", "\n", "decoder_dist_all", "=", "[", "]", "\n", "attention_dist_all", "=", "[", "]", "\n", "\n", "if", "self", ".", "coverage_attn", ":", "\n", "            ", "coverage", "=", "torch", ".", "zeros_like", "(", "src", ",", "dtype", "=", "torch", ".", "float", ")", ".", "requires_grad_", "(", ")", "# [batch, max_src_seq]", "\n", "#coverage_all = coverage.new_zeros((max_target_length, batch_size, max_src_len), dtype=torch.float)  # [max_trg_len, batch_size, max_src_len]", "\n", "coverage_all", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "coverage", "=", "None", "\n", "coverage_all", "=", "None", "\n", "\n", "", "if", "self", ".", "review_attn", ":", "\n", "            ", "decoder_memory_bank", "=", "h_t_init", "[", "-", "1", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", "# [batch, 1, decoder_size]", "\n", "assert", "decoder_memory_bank", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size", ",", "1", ",", "self", ".", "decoder_size", "]", ")", "\n", "", "else", ":", "\n", "            ", "decoder_memory_bank", "=", "None", "\n", "\n", "", "if", "self", ".", "orthogonal_loss", ":", "# create a list of batch_size empty list", "\n", "            ", "delimiter_decoder_states_2dlist", "=", "[", "[", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "", "if", "self", ".", "use_target_encoder", ":", "\n", "# init the hidden state of target encoder to zero vector", "\n", "            ", "h_te_t", "=", "h_t_init", ".", "new_zeros", "(", "1", ",", "batch_size", ",", "self", ".", "target_encoder_size", ")", "\n", "# create a list of batch_size empty list", "\n", "delimiter_target_encoder_states_2dlist", "=", "[", "[", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "# init y_t to be BOS token", "\n", "#y_t = trg.new_ones(batch_size) * self.bos_idx  # [batch_size]", "\n", "", "y_t_init", "=", "trg", ".", "new_ones", "(", "batch_size", ")", "*", "self", ".", "bos_idx", "# [batch_size]", "\n", "\n", "if", "self", ".", "separate_present_absent", "and", "self", ".", "goal_vector_mode", ">", "0", ":", "\n", "# byte tensor with size=batch_size to keep track of which batch has been proceeded to absent prediction", "\n", "            ", "is_absent", "=", "torch", ".", "zeros", "(", "batch_size", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "\n", "#print(y_t[:5])", "\n", "", "'''\n        for t in range(max_target_length):\n            # determine the hidden state that will be feed into the next step\n            # according to the time step or the target input\n            re_init_indicators = (y_t == self.sep_idx)  # [batch]\n\n            if t == 0:\n                h_t = h_t_init\n            elif self.one2many_mode == 2 and re_init_indicators.sum().item() != 0:\n                h_t = []\n                # h_t_next [dec_layers, batch_size, decoder_size]\n                # h_t_init [dec_layers, batch_size, decoder_size]\n                for batch_idx, indicator in enumerate(re_init_indicators):\n                    if indicator.item() == 0:\n                        h_t.append(h_t_next[:, batch_idx, :].unsqueeze(1))\n                    else:\n                        # some examples complete one keyphrase\n                        h_t.append(h_t_init[:, batch_idx, :].unsqueeze(1))\n                h_t = torch.cat(h_t, dim=1)  # [dec_layers, batch_size, decoder_size]\n            else:\n                h_t = h_t_next\n\n            decoder_dist, h_t_next, _, attn_dist, p_gen, coverage = \\\n                self.decoder(y_t, h_t, memory_bank, src_mask, max_num_oov, src_oov, coverage)\n            decoder_dist_all.append(decoder_dist.unsqueeze(1))  # [batch, 1, vocab_size]\n            attention_dist_all.append(attn_dist.unsqueeze(1))  # [batch, 1, src_seq_len]\n            if self.coverage_attn:\n                coverage_all.append(coverage.unsqueeze(1))  # [batch, 1, src_seq_len]\n            y_t = trg[:, t]\n            #y_t_emb = trg_emb[:, t, :].unsqueeze(0)  # [1, batch, embed_size]\n        '''", "\n", "#print(t)", "\n", "#print(trg_emb.size(1))", "\n", "\n", "#pred_counters = trg.new_zeros(batch_size, dtype=torch.uint8)  # [batch_size]", "\n", "\n", "for", "t", "in", "range", "(", "max_target_length", ")", ":", "\n", "# determine the hidden state that will be feed into the next step", "\n", "# according to the time step or the target input", "\n", "#re_init_indicators = (y_t == self.eos_idx)  # [batch]", "\n", "            ", "if", "t", "==", "0", ":", "\n", "                ", "pred_counters", "=", "trg", ".", "new_zeros", "(", "batch_size", ",", "dtype", "=", "torch", ".", "uint8", ")", "# [batch_size]", "\n", "", "else", ":", "\n", "                ", "re_init_indicators", "=", "(", "y_t_next", "==", "self", ".", "eos_idx", ")", "# [batch_size]", "\n", "pred_counters", "+=", "re_init_indicators", "\n", "\n", "", "if", "t", "==", "0", ":", "\n", "                ", "h_t", "=", "h_t_init", "\n", "y_t", "=", "y_t_init", "\n", "#re_init_indicators = (y_t == self.eos_idx)  # [batch]", "\n", "#pred_counters = re_init_indicators", "\n", "#pred_counters = trg.new_zeros(batch_size, dtype=torch.uint8)  # [batch_size]", "\n", "\n", "", "elif", "self", ".", "one2many", "and", "self", ".", "one2many_mode", "==", "2", "and", "re_init_indicators", ".", "sum", "(", ")", ".", "item", "(", ")", ">", "0", ":", "\n", "#re_init_indicators = (y_t_next == self.eos_idx)  # [batch]", "\n", "#pred_counters += re_init_indicators", "\n", "                ", "h_t", "=", "[", "]", "\n", "y_t", "=", "[", "]", "\n", "# h_t_next [dec_layers, batch_size, decoder_size]", "\n", "# h_t_init [dec_layers, batch_size, decoder_size]", "\n", "for", "batch_idx", ",", "(", "indicator", ",", "pred_count", ",", "trg_count", ")", "in", "enumerate", "(", "zip", "(", "re_init_indicators", ",", "pred_counters", ",", "num_trgs", ")", ")", ":", "\n", "                    ", "if", "indicator", ".", "item", "(", ")", "==", "1", "and", "pred_count", ".", "item", "(", ")", "<", "trg_count", ":", "\n", "# some examples complete one keyphrase", "\n", "                        ", "h_t", ".", "append", "(", "h_t_init", "[", ":", ",", "batch_idx", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", "\n", "y_t", ".", "append", "(", "y_t_init", "[", "batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "", "else", ":", "# indicator.item() == 0 or indicator.item() == 1 and pred_count.item() == trg_count:", "\n", "                        ", "h_t", ".", "append", "(", "h_t_next", "[", ":", ",", "batch_idx", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", "\n", "y_t", ".", "append", "(", "y_t_next", "[", "batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "", "", "h_t", "=", "torch", ".", "cat", "(", "h_t", ",", "dim", "=", "1", ")", "# [dec_layers, batch_size, decoder_size]", "\n", "y_t", "=", "torch", ".", "cat", "(", "y_t", ",", "dim", "=", "0", ")", "# [batch_size]", "\n", "", "elif", "self", ".", "one2many", "and", "self", ".", "one2many_mode", "==", "3", "and", "re_init_indicators", ".", "sum", "(", ")", ".", "item", "(", ")", ">", "0", ":", "\n", "# re_init_indicators = (y_t_next == self.eos_idx)  # [batch]", "\n", "# pred_counters += re_init_indicators", "\n", "                ", "h_t", "=", "h_t_next", "\n", "y_t", "=", "[", "]", "\n", "# h_t_next [dec_layers, batch_size, decoder_size]", "\n", "# h_t_init [dec_layers, batch_size, decoder_size]", "\n", "for", "batch_idx", ",", "(", "indicator", ",", "pred_count", ",", "trg_count", ")", "in", "enumerate", "(", "\n", "zip", "(", "re_init_indicators", ",", "pred_counters", ",", "num_trgs", ")", ")", ":", "\n", "                    ", "if", "indicator", ".", "item", "(", ")", "==", "1", "and", "pred_count", ".", "item", "(", ")", "<", "trg_count", ":", "\n", "# some examples complete one keyphrase", "\n", "                        ", "y_t", ".", "append", "(", "y_t_init", "[", "batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "", "else", ":", "# indicator.item() == 0 or indicator.item() == 1 and pred_count.item() == trg_count:", "\n", "                        ", "y_t", ".", "append", "(", "y_t_next", "[", "batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "", "", "y_t", "=", "torch", ".", "cat", "(", "y_t", ",", "dim", "=", "0", ")", "# [batch_size]", "\n", "", "else", ":", "\n", "                ", "h_t", "=", "h_t_next", "\n", "y_t", "=", "y_t_next", "\n", "\n", "", "if", "self", ".", "review_attn", ":", "\n", "                ", "if", "t", ">", "0", ":", "\n", "                    ", "decoder_memory_bank", "=", "torch", ".", "cat", "(", "[", "decoder_memory_bank", ",", "h_t", "[", "-", "1", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "1", ")", "# [batch, t+1, decoder_size]", "\n", "\n", "", "", "if", "self", ".", "use_target_encoder", ":", "\n", "# encode the previous token", "\n", "                ", "h_te_t_next", "=", "self", ".", "target_encoder", "(", "y_t", ".", "detach", "(", ")", ",", "h_te_t", ")", "\n", "h_te_t", "=", "h_te_t_next", "# [1, batch_size, target_encoder_size]", "\n", "# decoder_input = (y_t, h_te_t)", "\n", "# if this target encoder state corresponds to the delimiter, stack it", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "if", "y_t", "[", "i", "]", ".", "item", "(", ")", "==", "self", ".", "sep_idx", ":", "\n", "                        ", "delimiter_target_encoder_states_2dlist", "[", "i", "]", ".", "append", "(", "h_te_t", "[", "0", ",", "i", ",", ":", "]", ")", "# [target_encoder_size]", "\n", "", "", "", "else", ":", "\n", "                ", "h_te_t", "=", "None", "\n", "# decoder_input = y_t", "\n", "\n", "", "if", "self", ".", "separate_present_absent", "and", "self", ".", "goal_vector_mode", ">", "0", ":", "\n", "# update the is_absent vector", "\n", "                ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "if", "y_t", "[", "i", "]", ".", "item", "(", ")", "==", "self", ".", "peos_idx", ":", "\n", "                        ", "is_absent", "[", "i", "]", "=", "1", "\n", "#", "\n", "", "", "if", "self", ".", "manager_mode", "==", "1", ":", "\n", "                    ", "g_t", "=", "self", ".", "manager", "(", "is_absent", ")", "\n", "", "", "else", ":", "\n", "                ", "g_t", "=", "None", "\n", "\n", "", "decoder_dist", ",", "h_t_next", ",", "_", ",", "attn_dist", ",", "p_gen", ",", "coverage", "=", "self", ".", "decoder", "(", "y_t", ",", "h_t", ",", "memory_bank", ",", "src_mask", ",", "max_num_oov", ",", "src_oov", ",", "coverage", ",", "decoder_memory_bank", ",", "h_te_t", ",", "g_t", ")", "\n", "decoder_dist_all", ".", "append", "(", "decoder_dist", ".", "unsqueeze", "(", "1", ")", ")", "# [batch, 1, vocab_size]", "\n", "attention_dist_all", ".", "append", "(", "attn_dist", ".", "unsqueeze", "(", "1", ")", ")", "# [batch, 1, src_seq_len]", "\n", "if", "self", ".", "coverage_attn", ":", "\n", "                ", "coverage_all", ".", "append", "(", "coverage", ".", "unsqueeze", "(", "1", ")", ")", "# [batch, 1, src_seq_len]", "\n", "", "y_t_next", "=", "trg", "[", ":", ",", "t", "]", "# [batch]", "\n", "\n", "# if this hidden state corresponds to the delimiter, stack it", "\n", "if", "self", ".", "orthogonal_loss", ":", "\n", "                ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "if", "y_t_next", "[", "i", "]", ".", "item", "(", ")", "==", "self", ".", "sep_idx", ":", "\n", "                        ", "delimiter_decoder_states_2dlist", "[", "i", "]", ".", "append", "(", "h_t_next", "[", "-", "1", ",", "i", ",", ":", "]", ")", "# [decoder_size]", "\n", "\n", "", "", "", "", "decoder_dist_all", "=", "torch", ".", "cat", "(", "decoder_dist_all", ",", "dim", "=", "1", ")", "# [batch_size, trg_len, vocab_size]", "\n", "attention_dist_all", "=", "torch", ".", "cat", "(", "attention_dist_all", ",", "dim", "=", "1", ")", "# [batch_size, trg_len, src_len]", "\n", "if", "self", ".", "coverage_attn", ":", "\n", "            ", "coverage_all", "=", "torch", ".", "cat", "(", "coverage_all", ",", "dim", "=", "1", ")", "# [batch_size, trg_len, src_len]", "\n", "assert", "coverage_all", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "(", "batch_size", ",", "max_target_length", ",", "max_src_len", ")", ")", "\n", "\n", "", "if", "self", ".", "copy_attn", ":", "\n", "            ", "assert", "decoder_dist_all", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "(", "batch_size", ",", "max_target_length", ",", "self", ".", "vocab_size", "+", "max_num_oov", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "decoder_dist_all", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "(", "batch_size", ",", "max_target_length", ",", "self", ".", "vocab_size", ")", ")", "\n", "", "assert", "attention_dist_all", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "(", "batch_size", ",", "max_target_length", ",", "max_src_len", ")", ")", "\n", "\n", "# Pad delimiter_decoder_states_2dlist with zero vectors", "\n", "if", "self", ".", "orthogonal_loss", ":", "\n", "            ", "assert", "len", "(", "delimiter_decoder_states_2dlist", ")", "==", "batch_size", "\n", "delimiter_decoder_states_lens", "=", "[", "len", "(", "delimiter_decoder_states_2dlist", "[", "i", "]", ")", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "# [batch_size, decoder_size, max_num_delimiters]", "\n", "delimiter_decoder_states", "=", "self", ".", "tensor_2dlist_to_tensor", "(", "delimiter_decoder_states_2dlist", ",", "batch_size", ",", "self", ".", "decoder_size", ",", "delimiter_decoder_states_lens", ")", "\n", "\"\"\"\n            max_num_delimiters = max(delimiter_decoder_states_lens)\n            for i in range(batch_size):\n                for j in range(max_num_delimiters - delimiter_decoder_states_lens[i]):\n                    delimiter_decoder_states_2dlist[i].append(torch.zeros_like(h_t_next[-1, 0, :]))  # [decoder_size]\n                delimiter_decoder_states_2dlist[i] = torch.stack(delimiter_decoder_states_2dlist[i], dim=1)  # [decoder_size, max_num_delimiters]\n            delimiter_decoder_states = torch.stack(delimiter_decoder_states_2dlist, dim=0)  # [batch_size, deocder_size, max_num_delimiters]\n            \"\"\"", "\n", "", "else", ":", "\n", "            ", "delimiter_decoder_states_lens", "=", "None", "\n", "delimiter_decoder_states", "=", "None", "\n", "\n", "# Pad the target_encoder_states_2dlist with zero vectors", "\n", "", "if", "self", ".", "use_target_encoder", "and", "sampled_source_representation_2dlist", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "delimiter_target_encoder_states_2dlist", ")", "==", "batch_size", "\n", "# Pad the delimiter_target_encoder_states_2dlist with zeros and convert it to a tensor", "\n", "delimiter_target_encoder_states_lens", "=", "[", "len", "(", "delimiter_target_encoder_states_2dlist", "[", "i", "]", ")", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "# [batch_size, target_encoder_size, max_num_delimiters]", "\n", "delimiter_target_encoder_states", "=", "self", ".", "tensor_2dlist_to_tensor", "(", "delimiter_target_encoder_states_2dlist", ",", "batch_size", ",", "self", ".", "target_encoder_size", ",", "delimiter_target_encoder_states_lens", ")", "\n", "max_num_delimiters", "=", "delimiter_target_encoder_states", ".", "size", "(", "2", ")", "\n", "# Perform attention step by step", "\n", "source_classification_dist_all", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "max_num_delimiters", ")", ":", "\n", "# delimiter_target_encoder_states[:, :, i]: [batch_size, target_encoder_size]", "\n", "# sampled_source_representation: [batch_size, source_representation_sample_size, memory_bank_size]", "\n", "                ", "_", ",", "source_classification_dist", ",", "_", "=", "self", ".", "target_encoder_attention", "(", "delimiter_target_encoder_states", "[", ":", ",", ":", ",", "i", "]", ",", "sampled_source_representation", ")", "\n", "# source_classification_dist: [batch_size, source_representation_sample_size]", "\n", "source_classification_dist_all", ".", "append", "(", "source_classification_dist", ".", "unsqueeze", "(", "1", ")", ")", "# [batch_size, 1, source_representation_sample_size]", "\n", "", "source_classification_dist_all", "=", "torch", ".", "cat", "(", "source_classification_dist_all", ",", "dim", "=", "1", ")", "# [batch_size, max_num_delimiters, source_representation_sample_size]", "\n", "", "else", ":", "\n", "            ", "source_classification_dist_all", "=", "None", "\n", "\n", "", "return", "decoder_dist_all", ",", "h_t_next", ",", "attention_dist_all", ",", "encoder_final_state", ",", "coverage_all", ",", "delimiter_decoder_states", ",", "delimiter_decoder_states_lens", ",", "source_classification_dist_all", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.tensor_2dlist_to_tensor": [[457, 473], ["max", "range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "tensor_2d_list[].append", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["None"], ["", "def", "tensor_2dlist_to_tensor", "(", "self", ",", "tensor_2d_list", ",", "batch_size", ",", "hidden_size", ",", "seq_lens", ")", ":", "\n", "        ", "\"\"\"\n        :param tensor_2d_list: a 2d list of tensor with size=[hidden_size], len(tensor_2d_list)=batch_size, len(tensor_2d_list[i])=seq_len[i]\n        :param batch_size:\n        :param hidden_size:\n        :param seq_lens: a list that store the seq len of each batch, with len=batch_size\n        :return: [batch_size, hidden_size, max_seq_len]\n        \"\"\"", "\n", "# assert tensor_2d_list[0][0].size() == torch.Size([hidden_size])", "\n", "max_seq_len", "=", "max", "(", "seq_lens", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "max_seq_len", "-", "seq_lens", "[", "i", "]", ")", ":", "\n", "                ", "tensor_2d_list", "[", "i", "]", ".", "append", "(", "torch", ".", "ones", "(", "hidden_size", ")", ".", "to", "(", "self", ".", "device", ")", "*", "self", ".", "pad_idx_trg", ")", "# [hidden_size]", "\n", "", "tensor_2d_list", "[", "i", "]", "=", "torch", ".", "stack", "(", "tensor_2d_list", "[", "i", "]", ",", "dim", "=", "1", ")", "# [hidden_size, max_seq_len]", "\n", "", "tensor_3d", "=", "torch", ".", "stack", "(", "tensor_2d_list", ",", "dim", "=", "0", ")", "# [batch_size, hidden_size, max_seq_len]", "\n", "return", "tensor_3d", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.init_decoder_state": [[474, 489], ["encoder_final_state.size", "model.Seq2SeqModel.unsqueeze().expand", "model.Seq2SeqModel.bridge_layer", "model.Seq2SeqModel.unsqueeze"], "methods", ["None"], ["", "def", "init_decoder_state", "(", "self", ",", "encoder_final_state", ")", ":", "\n", "        ", "\"\"\"\n        :param encoder_final_state: [batch_size, self.num_directions * self.encoder_size]\n        :return: [1, batch_size, decoder_size]\n        \"\"\"", "\n", "batch_size", "=", "encoder_final_state", ".", "size", "(", "0", ")", "\n", "if", "self", ".", "bridge", "==", "'none'", ":", "\n", "            ", "decoder_init_state", "=", "None", "\n", "", "elif", "self", ".", "bridge", "==", "'copy'", ":", "\n", "            ", "decoder_init_state", "=", "encoder_final_state", "\n", "", "else", ":", "\n", "            ", "decoder_init_state", "=", "self", ".", "bridge_layer", "(", "encoder_final_state", ")", "\n", "", "decoder_init_state", "=", "decoder_init_state", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "(", "self", ".", "dec_layers", ",", "batch_size", ",", "self", ".", "decoder_size", ")", ")", "\n", "# [dec_layers, batch_size, decoder_size]", "\n", "return", "decoder_init_state", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.model.Seq2SeqModel.init_context": [[490, 494], ["memory_bank.max"], "methods", ["None"], ["", "def", "init_context", "(", "self", ",", "memory_bank", ")", ":", "\n", "# Init by max pooling, may support other initialization later", "\n", "        ", "context", ",", "_", "=", "memory_bank", ".", "max", "(", "dim", "=", "1", ")", "\n", "return", "context", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_loss.masked_cross_entropy": [[7, 56], ["class_dist.size", "class_dist.view", "torch.log", "target.view", "losses_flat.view", "losses_flat.view.sum", "loss.sum.sum", "math.isnan", "torch.gather", "masked_loss.compute_coverage_losses", "masked_loss.compute_orthogonal_loss", "loss.sum.item", "print", "print", "print", "print", "target.size"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_loss.compute_coverage_losses", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_loss.compute_orthogonal_loss"], ["def", "masked_cross_entropy", "(", "class_dist", ",", "target", ",", "trg_mask", ",", "trg_lens", "=", "None", ",", "\n", "coverage_attn", "=", "False", ",", "coverage", "=", "None", ",", "attn_dist", "=", "None", ",", "lambda_coverage", "=", "0", ",", "coverage_loss", "=", "False", ",", "\n", "delimiter_hidden_states", "=", "None", ",", "orthogonal_loss", "=", "False", ",", "lambda_orthogonal", "=", "0", ",", "delimiter_hidden_states_lens", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    :param class_dist: [batch_size, trg_seq_len, num_classes]\n    :param target: [batch_size, trg_seq_len]\n    :param trg_mask: [batch_size, trg_seq_len]\n    :param trg_lens: a list with len of batch_size\n    :param coverage_attn: boolean, whether to include coverage loss\n    :param coverage: [batch_size, trg_seq_len, src_seq_len]\n    :param attn_dist: [batch_size, trg_seq_len, src_seq_len]\n    :param lambda_coverage: scalar, coefficient for coverage loss\n    :param delimiter_hidden_states: [batch_size, decoder_size, num_delimiter]\n    :return:\n    \"\"\"", "\n", "num_classes", "=", "class_dist", ".", "size", "(", "2", ")", "\n", "class_dist_flat", "=", "class_dist", ".", "view", "(", "-", "1", ",", "num_classes", ")", "# [batch_size*trg_seq_len, num_classes]", "\n", "log_dist_flat", "=", "torch", ".", "log", "(", "class_dist_flat", "+", "EPS", ")", "\n", "target_flat", "=", "target", ".", "view", "(", "-", "1", ",", "1", ")", "# [batch*trg_seq_len, 1]", "\n", "losses_flat", "=", "-", "torch", ".", "gather", "(", "log_dist_flat", ",", "dim", "=", "1", ",", "index", "=", "target_flat", ")", "# [batch * trg_seq_len, 1]", "\n", "losses", "=", "losses_flat", ".", "view", "(", "*", "target", ".", "size", "(", ")", ")", "# [batch, trg_seq_len]", "\n", "if", "coverage_attn", "and", "coverage_loss", ":", "\n", "        ", "coverage_losses", "=", "compute_coverage_losses", "(", "coverage", ",", "attn_dist", ")", "\n", "losses", "=", "losses", "+", "lambda_coverage", "*", "coverage_losses", "\n", "", "if", "trg_mask", "is", "not", "None", ":", "\n", "        ", "losses", "=", "losses", "*", "trg_mask", "\n", "", "'''\n    if divided_by_seq_len:\n        trg_lens_tensor = torch.FloatTensor(trg_lens).to(target.device).requires_grad_()\n        loss = losses.sum(dim=1)   # [batch_size]\n        loss = loss / trg_lens_tensor\n    else:\n        loss = losses.sum(dim=1) # [batch_size]\n    '''", "\n", "loss", "=", "losses", ".", "sum", "(", "dim", "=", "1", ")", "# [batch_size]", "\n", "if", "orthogonal_loss", ":", "\n", "        ", "orthogonal_loss", "=", "compute_orthogonal_loss", "(", "delimiter_hidden_states", ",", "delimiter_hidden_states_lens", ")", "# [batch_size]", "\n", "loss", "=", "loss", "+", "lambda_orthogonal", "*", "orthogonal_loss", "\n", "", "loss", "=", "loss", ".", "sum", "(", ")", "\n", "\n", "# Debug", "\n", "if", "math", ".", "isnan", "(", "loss", ".", "item", "(", ")", ")", ":", "\n", "        ", "print", "(", "\"class distribution\"", ")", "\n", "print", "(", "class_dist", ")", "\n", "print", "(", "\"log dist flat\"", ")", "\n", "print", "(", "log_dist_flat", ")", "\n", "#raise ValueError(\"Loss is NaN\")", "\n", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_loss.compute_coverage_losses": [[57, 71], ["coverage.size", "coverage.size", "attn_dist.size", "coverage.view", "attn_dist.view", "torch.sum", "torch.sum.view", "torch.min"], "function", ["None"], ["", "def", "compute_coverage_losses", "(", "coverage", ",", "attn_dist", ")", ":", "\n", "    ", "\"\"\"\n    :param coverage: [batch_size, trg_seq_len, src_seq_len]\n    :param attn_dist: [batch_size, trg_seq_len, src_seq_len]\n    :return: coverage_losses: [batch, trg_seq_len]\n    \"\"\"", "\n", "batch_size", "=", "coverage", ".", "size", "(", "0", ")", "\n", "trg_seq_len", "=", "coverage", ".", "size", "(", "1", ")", "\n", "src_seq_len", "=", "attn_dist", ".", "size", "(", "2", ")", "\n", "coverage_flat", "=", "coverage", ".", "view", "(", "-", "1", ",", "src_seq_len", ")", "# [batch_size * trg_seq_len, src_seq_len]", "\n", "attn_dist_flat", "=", "attn_dist", ".", "view", "(", "-", "1", ",", "src_seq_len", ")", "# [batch_size * trg_seq_len, src_seq_len]", "\n", "coverage_losses_flat", "=", "torch", ".", "sum", "(", "torch", ".", "min", "(", "attn_dist_flat", ",", "coverage_flat", ")", ",", "dim", "=", "1", ")", "# [batch_size * trg_seq_len]", "\n", "coverage_losses", "=", "coverage_losses_flat", ".", "view", "(", "batch_size", ",", "trg_seq_len", ")", "# [batch, trg_seq_len]", "\n", "return", "coverage_losses", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_loss.masked_coverage_loss": [[73, 88], ["attn_dist.size", "coverage.view", "attn_dist.view", "torch.sum", "torch.sum.view", "coverage_losses_flat.view.sum", "torch.min", "trg_mask.size"], "function", ["None"], ["", "def", "masked_coverage_loss", "(", "coverage", ",", "attn_dist", ",", "trg_mask", ")", ":", "\n", "    ", "\"\"\"\n    :param coverage: [batch_size, trg_seq_len, src_seq_len]\n    :param attn_dist: [batch_size, trg_seq_len, src_seq_len]\n    :param trg_mask: [batch_size, trg_seq_len]\n    :return:\n    \"\"\"", "\n", "src_seq_len", "=", "attn_dist", ".", "size", "(", "2", ")", "\n", "coverage_flat", "=", "coverage", ".", "view", "(", "-", "1", ",", "src_seq_len", ")", "# [batch_size * trg_seq_len, src_seq_len]", "\n", "attn_dist_flat", "=", "attn_dist", ".", "view", "(", "-", "1", ",", "src_seq_len", ")", "# [batch_size * trg_seq_len, src_seq_len]", "\n", "coverage_losses_flat", "=", "torch", ".", "sum", "(", "torch", ".", "min", "(", "attn_dist_flat", ",", "coverage_flat", ")", ",", "1", ")", "# [batch_size * trg_seq_len]", "\n", "coverage_losses", "=", "coverage_losses_flat", ".", "view", "(", "*", "trg_mask", ".", "size", "(", ")", ")", "# [batch, trg_seq_len]", "\n", "if", "trg_mask", "is", "not", "None", ":", "\n", "        ", "coverage_losses", "=", "coverage_losses", "*", "trg_mask", "\n", "", "return", "coverage_losses", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_loss.compute_orthogonal_loss": [[90, 105], ["delimiter_hidden_states.size", "torch.eye().unsqueeze().repeat().to", "torch.norm", "range", "torch.bmm", "orthogonal_loss_.view", "torch.eye().unsqueeze().repeat", "len", "range", "torch.transpose", "identity[].fill_", "torch.eye().unsqueeze", "torch.eye"], "function", ["None"], ["", "def", "compute_orthogonal_loss", "(", "delimiter_hidden_states", ",", "delimiter_hidden_states_lens", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    :param delimiter_hidden_states: [batch_size, decoder_size, max_num_delimiters]\n    :return:\n    \"\"\"", "\n", "batch_size", ",", "decoder_size", ",", "max_num_delimiters", "=", "delimiter_hidden_states", ".", "size", "(", ")", "\n", "identity", "=", "torch", ".", "eye", "(", "max_num_delimiters", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "batch_size", ",", "1", ",", "1", ")", ".", "to", "(", "delimiter_hidden_states", ".", "device", ")", "# [batch, max_num_delimiters, max_num_delimiters]", "\n", "if", "delimiter_hidden_states_lens", "is", "not", "None", ":", "\n", "        ", "assert", "len", "(", "delimiter_hidden_states", ")", "==", "batch_size", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "max_num_delimiters", "-", "1", ",", "delimiter_hidden_states_lens", "[", "i", "]", "-", "1", ",", "-", "1", ")", ":", "\n", "                ", "identity", "[", "i", ",", "j", ",", "j", "]", ".", "fill_", "(", "0.0", ")", "\n", "", "", "", "orthogonal_loss_", "=", "torch", ".", "bmm", "(", "torch", ".", "transpose", "(", "delimiter_hidden_states", ",", "1", ",", "2", ")", ",", "delimiter_hidden_states", ")", "-", "identity", "# [batch, num_delimiter, num_delimiter]", "\n", "orthogonal_loss", "=", "torch", ".", "norm", "(", "orthogonal_loss_", ".", "view", "(", "batch_size", ",", "-", "1", ")", ",", "p", "=", "2", ",", "dim", "=", "1", ")", "# [batch]", "\n", "return", "orthogonal_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_loss.loss_debug": [[120, 172], ["torch.manual_seed", "np.random.seed", "torch.randint", "F.softmax", "np.random.randint", "torch.LongTensor", "np.ones", "torch.FloatTensor", "torch.randint", "F.softmax", "torch.randn", "masked_loss.masked_cross_entropy", "print", "torch.rand"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.masked_cross_entropy"], ["def", "loss_debug", "(", ")", ":", "\n", "    ", "import", "torch", ".", "nn", ".", "functional", "as", "F", "\n", "import", "numpy", "as", "np", "\n", "torch", ".", "manual_seed", "(", "1234", ")", "\n", "np", ".", "random", ".", "seed", "(", "1234", ")", "\n", "\n", "num_classes", "=", "5000", "\n", "batch_size", "=", "5", "\n", "trg_seq_len", "=", "6", "\n", "src_seq_len", "=", "30", "\n", "class_dist", "=", "torch", ".", "randint", "(", "0", ",", "5", ",", "(", "batch_size", ",", "trg_seq_len", ",", "num_classes", ")", ")", "\n", "class_dist", "=", "F", ".", "softmax", "(", "class_dist", ",", "dim", "=", "-", "1", ")", "\n", "\n", "target", "=", "np", ".", "random", ".", "randint", "(", "2", ",", "300", ",", "(", "batch_size", ",", "trg_seq_len", ")", ")", "\n", "target", "[", "batch_size", "-", "1", ",", "trg_seq_len", "-", "1", "]", "=", "0", "\n", "target", "[", "batch_size", "-", "1", ",", "trg_seq_len", "-", "2", "]", "=", "0", "\n", "target", "[", "batch_size", "-", "2", ",", "trg_seq_len", "-", "1", "]", "=", "0", "\n", "target", "=", "torch", ".", "LongTensor", "(", "target", ")", "\n", "\n", "trg_mask", "=", "np", ".", "ones", "(", "(", "batch_size", ",", "trg_seq_len", ")", ")", "\n", "target", "[", "batch_size", "-", "1", ",", "trg_seq_len", "-", "1", "]", "=", "0", "\n", "target", "[", "batch_size", "-", "1", ",", "trg_seq_len", "-", "2", "]", "=", "0", "\n", "target", "[", "batch_size", "-", "2", ",", "trg_seq_len", "-", "1", "]", "=", "0", "\n", "trg_mask", "=", "torch", ".", "FloatTensor", "(", "trg_mask", ")", "\n", "\n", "divided_by_seq_len", "=", "True", "\n", "trg_lens", "=", "[", "trg_seq_len", "]", "*", "batch_size", "\n", "trg_lens", "[", "batch_size", "-", "1", "]", "=", "trg_seq_len", "-", "2", "\n", "trg_lens", "[", "batch_size", "-", "2", "]", "=", "trg_seq_len", "-", "1", "\n", "\n", "coverage_attn", "=", "True", "\n", "coverage", "=", "torch", ".", "rand", "(", "(", "batch_size", ",", "trg_seq_len", ",", "src_seq_len", ")", ")", "*", "5", "\n", "attn_dist", "=", "torch", ".", "randint", "(", "0", ",", "5", ",", "(", "batch_size", ",", "trg_seq_len", ",", "src_seq_len", ")", ")", "\n", "attn_dist", "=", "F", ".", "softmax", "(", "attn_dist", ",", "dim", "=", "-", "1", ")", "\n", "lambda_coverage", "=", "1", "\n", "coverage_loss", "=", "True", "\n", "\n", "decoder_size", "=", "100", "\n", "num_delimiter", "=", "5", "\n", "delimiter_hidden_states", "=", "torch", ".", "randn", "(", "batch_size", ",", "decoder_size", ",", "num_delimiter", ")", "\n", "lambda_orthogonal", "=", "0.03", "\n", "orthogonal_loss", "=", "True", "\n", "\n", "delimiter_hidden_states_lens", "=", "[", "3", ",", "5", ",", "2", ",", "1", ",", "5", "]", "\n", "\n", "loss", "=", "masked_cross_entropy", "(", "class_dist", ",", "target", ",", "trg_mask", ",", "trg_lens", "=", "trg_lens", ",", "\n", "coverage_attn", "=", "coverage_attn", ",", "coverage", "=", "coverage", ",", "attn_dist", "=", "attn_dist", ",", "\n", "lambda_coverage", "=", "lambda_coverage", ",", "coverage_loss", "=", "coverage_loss", ",", "\n", "delimiter_hidden_states", "=", "delimiter_hidden_states", ",", "orthogonal_loss", "=", "orthogonal_loss", ",", "\n", "lambda_orthogonal", "=", "lambda_orthogonal", ",", "delimiter_hidden_states_lens", "=", "delimiter_hidden_states_lens", ")", "\n", "print", "(", "loss", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_loss.compute_orthogonal_loss_debug": [[173, 221], ["torch.randn", "masked_loss.compute_orthogonal_loss", "print", "torch.zeros", "delimiter_hidden_states_2[].fill_", "delimiter_hidden_states_2[].fill_", "delimiter_hidden_states_2[].fill_", "delimiter_hidden_states_2[].fill_", "delimiter_hidden_states_2[].fill_", "delimiter_hidden_states_2[].fill_", "masked_loss.compute_orthogonal_loss", "print", "torch.zeros", "delimiter_hidden_states_3[].fill_", "delimiter_hidden_states_3[].fill_", "delimiter_hidden_states_3[].fill_", "delimiter_hidden_states_3[].fill_", "delimiter_hidden_states_3[].fill_", "delimiter_hidden_states_3[].fill_", "delimiter_hidden_states_3[].fill_", "delimiter_hidden_states_3[].fill_", "delimiter_hidden_states_3[].fill_", "masked_loss.compute_orthogonal_loss", "print", "print", "compute_orthogonal_loss.size", "torch.Size", "compute_orthogonal_loss.size", "torch.Size", "ortho_loss_2[].item", "math.fabs", "compute_orthogonal_loss.size", "torch.Size", "ortho_loss_2[].item", "math.fabs", "math.fabs", "ortho_loss_2[].item", "math.sqrt", "ortho_loss_2[].item", "math.sqrt", "ortho_loss_3[].item", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_loss.compute_orthogonal_loss", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_loss.compute_orthogonal_loss", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.masked_loss.compute_orthogonal_loss"], ["", "def", "compute_orthogonal_loss_debug", "(", ")", ":", "\n", "    ", "import", "math", "\n", "\n", "batch_size_1", "=", "12", "\n", "decoder_size_1", "=", "100", "\n", "num_delimiter_1", "=", "5", "\n", "delimiter_hidden_states_1", "=", "torch", ".", "randn", "(", "batch_size_1", ",", "decoder_size_1", ",", "num_delimiter_1", ")", "# [batch_size, decoder_size, num_delimiter]", "\n", "ortho_loss_1", "=", "compute_orthogonal_loss", "(", "delimiter_hidden_states_1", ")", "\n", "print", "(", "ortho_loss_1", ")", "\n", "assert", "ortho_loss_1", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size_1", "]", ")", "\n", "\n", "batch_size_2", "=", "2", "\n", "decoder_size_2", "=", "10", "\n", "num_delimiter_2", "=", "3", "\n", "delimiter_hidden_states_2", "=", "torch", ".", "zeros", "(", "batch_size_2", ",", "decoder_size_2", ",", "num_delimiter_2", ")", "\n", "delimiter_hidden_states_2", "[", "0", ",", "0", ",", "0", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_2", "[", "0", ",", "1", ",", "1", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_2", "[", "0", ",", "6", ",", "2", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_2", "[", "1", ",", "5", ",", "0", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_2", "[", "1", ",", "2", ",", "1", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_2", "[", "1", ",", "2", ",", "2", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_2_lens", "=", "[", "3", ",", "3", "]", "\n", "ortho_loss_2", "=", "compute_orthogonal_loss", "(", "delimiter_hidden_states_2", ",", "delimiter_hidden_states_2_lens", ")", "\n", "#print(delimiter_hidden_states_2[0])", "\n", "print", "(", "ortho_loss_2", ")", "\n", "assert", "ortho_loss_2", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size_2", "]", ")", "and", "ortho_loss_2", "[", "0", "]", ".", "item", "(", ")", "==", "0.0", "and", "math", ".", "fabs", "(", "ortho_loss_2", "[", "1", "]", ".", "item", "(", ")", "-", "math", ".", "sqrt", "(", "2", ")", ")", "<", "1e-3", "\n", "\n", "batch_size_3", "=", "3", "\n", "decoder_size_3", "=", "10", "\n", "num_delimiter_3", "=", "4", "\n", "delimiter_hidden_states_3", "=", "torch", ".", "zeros", "(", "batch_size_3", ",", "decoder_size_3", ",", "num_delimiter_3", ")", "\n", "delimiter_hidden_states_3", "[", "0", ",", "0", ",", "0", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_3", "[", "0", ",", "1", ",", "1", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_3", "[", "0", ",", "6", ",", "2", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_3", "[", "0", ",", "7", ",", "3", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_3", "[", "1", ",", "5", ",", "0", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_3", "[", "1", ",", "2", ",", "1", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_3", "[", "1", ",", "2", ",", "2", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_3", "[", "2", ",", "3", ",", "0", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_3", "[", "2", ",", "3", ",", "1", "]", ".", "fill_", "(", "1", ")", "\n", "delimiter_hidden_states_3_lens", "=", "[", "4", ",", "3", ",", "2", "]", "\n", "ortho_loss_3", "=", "compute_orthogonal_loss", "(", "delimiter_hidden_states_3", ",", "delimiter_hidden_states_3_lens", ")", "\n", "# print(delimiter_hidden_states_2[0])", "\n", "print", "(", "ortho_loss_3", ")", "\n", "assert", "ortho_loss_2", ".", "size", "(", ")", "==", "torch", ".", "Size", "(", "[", "batch_size_2", "]", ")", "and", "ortho_loss_2", "[", "0", "]", ".", "item", "(", ")", "==", "0.0", "and", "math", ".", "fabs", "(", "\n", "ortho_loss_2", "[", "1", "]", ".", "item", "(", ")", "-", "math", ".", "sqrt", "(", "2", ")", ")", "<", "1e-3", "and", "math", ".", "fabs", "(", "\n", "ortho_loss_3", "[", "1", "]", ".", "item", "(", ")", "-", "math", ".", "sqrt", "(", "2", ")", ")", "<", "1e-3", "\n", "print", "(", "\"Pass!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.ExceptionWrapper.__init__": [[38, 41], ["traceback.format_exception"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "exc_info", ")", ":", "\n", "        ", "self", ".", "exc_type", "=", "exc_info", "[", "0", "]", "\n", "self", ".", "exc_msg", "=", "\"\"", ".", "join", "(", "traceback", ".", "format_exception", "(", "*", "exc_info", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter.__init__": [[153, 194], ["threading.Event", "iter", "torch.SimpleQueue", "torch.SimpleQueue", "torch.SimpleQueue", "torch.SimpleQueue", "range", "torch.Process", "torch.Process", "w.start", "queue.Queue", "threading.Thread", "dataloader.DataLoaderIter.pin_thread.start", "dataloader.DataLoaderIter._put_indices", "range"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter._put_indices"], ["def", "__init__", "(", "self", ",", "loader", ")", ":", "\n", "        ", "self", ".", "dataset", "=", "loader", ".", "dataset", "\n", "self", ".", "collate_fn", "=", "loader", ".", "collate_fn", "\n", "self", ".", "batch_sampler", "=", "loader", ".", "batch_sampler", "\n", "self", ".", "num_workers", "=", "loader", ".", "num_workers", "\n", "self", ".", "pin_memory", "=", "loader", ".", "pin_memory", "\n", "self", ".", "done_event", "=", "threading", ".", "Event", "(", ")", "\n", "\n", "self", ".", "sample_iter", "=", "iter", "(", "self", ".", "batch_sampler", ")", "\n", "\n", "if", "self", ".", "num_workers", ">", "0", ":", "\n", "            ", "self", ".", "index_queue", "=", "multiprocessing", ".", "SimpleQueue", "(", ")", "\n", "self", ".", "data_queue", "=", "multiprocessing", ".", "SimpleQueue", "(", ")", "\n", "self", ".", "batches_outstanding", "=", "0", "\n", "self", ".", "shutdown", "=", "False", "\n", "self", ".", "send_idx", "=", "0", "\n", "self", ".", "rcvd_idx", "=", "0", "\n", "self", ".", "reorder_dict", "=", "{", "}", "\n", "\n", "self", ".", "workers", "=", "[", "\n", "multiprocessing", ".", "Process", "(", "\n", "target", "=", "_worker_loop", ",", "\n", "args", "=", "(", "self", ".", "dataset", ",", "self", ".", "index_queue", ",", "self", ".", "data_queue", ",", "self", ".", "collate_fn", ")", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "num_workers", ")", "]", "\n", "\n", "for", "w", "in", "self", ".", "workers", ":", "\n", "                ", "w", ".", "daemon", "=", "True", "# ensure that the worker exits on process exit", "\n", "w", ".", "start", "(", ")", "\n", "\n", "", "if", "self", ".", "pin_memory", ":", "\n", "                ", "in_data", "=", "self", ".", "data_queue", "\n", "self", ".", "data_queue", "=", "queue", ".", "Queue", "(", ")", "\n", "self", ".", "pin_thread", "=", "threading", ".", "Thread", "(", "\n", "target", "=", "_pin_memory_loop", ",", "\n", "args", "=", "(", "in_data", ",", "self", ".", "data_queue", ",", "self", ".", "done_event", ")", ")", "\n", "self", ".", "pin_thread", ".", "daemon", "=", "True", "\n", "self", ".", "pin_thread", ".", "start", "(", ")", "\n", "\n", "# prime the prefetch loop", "\n", "", "for", "_", "in", "range", "(", "2", "*", "self", ".", "num_workers", ")", ":", "\n", "                ", "self", ".", "_put_indices", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter.__len__": [[195, 197], ["len"], "methods", ["None"], ["", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "batch_sampler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter.__next__": [[198, 224], ["next", "dataloader.DataLoaderIter.collate_fn", "dataloader.DataLoaderIter.reorder_dict.pop", "dataloader.DataLoaderIter._process_next_batch", "dataloader.DataLoaderIter._shutdown_workers", "dataloader.DataLoaderIter.data_queue.get", "dataloader.DataLoaderIter._process_next_batch", "dataloader.pin_memory_batch"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter._process_next_batch", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter._shutdown_workers", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter._process_next_batch", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.pin_memory_batch"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "num_workers", "==", "0", ":", "# same-process loading", "\n", "            ", "indices", "=", "next", "(", "self", ".", "sample_iter", ")", "# may raise StopIteration", "\n", "batch", "=", "self", ".", "collate_fn", "(", "[", "self", ".", "dataset", "[", "i", "]", "for", "i", "in", "indices", "]", ")", "\n", "if", "self", ".", "pin_memory", ":", "\n", "                ", "batch", "=", "pin_memory_batch", "(", "batch", ")", "\n", "", "return", "batch", "\n", "\n", "# check if the next sample has already been generated", "\n", "", "if", "self", ".", "rcvd_idx", "in", "self", ".", "reorder_dict", ":", "\n", "            ", "batch", "=", "self", ".", "reorder_dict", ".", "pop", "(", "self", ".", "rcvd_idx", ")", "\n", "return", "self", ".", "_process_next_batch", "(", "batch", ")", "\n", "\n", "", "if", "self", ".", "batches_outstanding", "==", "0", ":", "\n", "            ", "self", ".", "_shutdown_workers", "(", ")", "\n", "raise", "StopIteration", "\n", "\n", "", "while", "True", ":", "\n", "            ", "assert", "(", "not", "self", ".", "shutdown", "and", "self", ".", "batches_outstanding", ">", "0", ")", "\n", "idx", ",", "batch", "=", "self", ".", "data_queue", ".", "get", "(", ")", "\n", "self", ".", "batches_outstanding", "-=", "1", "\n", "if", "idx", "!=", "self", ".", "rcvd_idx", ":", "\n", "# store out-of-order samples", "\n", "                ", "self", ".", "reorder_dict", "[", "idx", "]", "=", "batch", "\n", "continue", "\n", "", "return", "self", ".", "_process_next_batch", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter.__iter__": [[227, 229], ["None"], "methods", ["None"], ["def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter._put_indices": [[230, 238], ["next", "dataloader.DataLoaderIter.index_queue.put"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.put"], ["", "def", "_put_indices", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "batches_outstanding", "<", "2", "*", "self", ".", "num_workers", "\n", "indices", "=", "next", "(", "self", ".", "sample_iter", ",", "None", ")", "\n", "if", "indices", "is", "None", ":", "\n", "            ", "return", "\n", "", "self", ".", "index_queue", ".", "put", "(", "(", "self", ".", "send_idx", ",", "indices", ")", ")", "\n", "self", ".", "batches_outstanding", "+=", "1", "\n", "self", ".", "send_idx", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter._process_next_batch": [[239, 245], ["dataloader.DataLoaderIter._put_indices", "isinstance", "batch.exc_type"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter._put_indices"], ["", "def", "_process_next_batch", "(", "self", ",", "batch", ")", ":", "\n", "        ", "self", ".", "rcvd_idx", "+=", "1", "\n", "self", ".", "_put_indices", "(", ")", "\n", "if", "isinstance", "(", "batch", ",", "ExceptionWrapper", ")", ":", "\n", "            ", "raise", "batch", ".", "exc_type", "(", "batch", ".", "exc_msg", ")", "\n", "", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter.__getstate__": [[246, 252], ["NotImplementedError"], "methods", ["None"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "# across multiple threads for HOGWILD.", "\n", "# Probably the best way to do this is by moving the sample pushing", "\n", "# to a separate thread and then just sharing the data queue", "\n", "# but signalling the end is tricky without a non-blocking API", "\n", "        ", "raise", "NotImplementedError", "(", "\"DataLoaderIterator cannot be pickled\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter._shutdown_workers": [[253, 259], ["dataloader.DataLoaderIter.done_event.set", "dataloader.DataLoaderIter.index_queue.put"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.put"], ["", "def", "_shutdown_workers", "(", "self", ")", ":", "\n", "        ", "if", "not", "self", ".", "shutdown", ":", "\n", "            ", "self", ".", "shutdown", "=", "True", "\n", "self", ".", "done_event", ".", "set", "(", ")", "\n", "for", "_", "in", "self", ".", "workers", ":", "\n", "                ", "self", ".", "index_queue", ".", "put", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter.__del__": [[260, 263], ["dataloader.DataLoaderIter._shutdown_workers"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.DataLoaderIter._shutdown_workers"], ["", "", "", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "num_workers", ">", "0", ":", "\n", "            ", "self", ".", "_shutdown_workers", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.KeyphraseDataLoader.__init__": [[292, 323], ["dataloader.One2ManyBatchSampler", "len", "ValueError", "ValueError", "torch.utils.data.sampler.RandomSampler", "torch.utils.data.sampler.RandomSampler", "torch.utils.data.sampler.SequentialSampler", "torch.utils.data.sampler.SequentialSampler"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", ",", "max_batch_example", "=", "5", ",", "max_batch_pair", "=", "1", ",", "shuffle", "=", "False", ",", "sampler", "=", "None", ",", "batch_sampler", "=", "None", ",", "\n", "num_workers", "=", "0", ",", "collate_fn", "=", "default_collate", ",", "pin_memory", "=", "False", ",", "drop_last", "=", "False", ")", ":", "\n", "        ", "self", ".", "dataset", "=", "dataset", "\n", "# used for generating one2many batches", "\n", "self", ".", "num_trgs", "=", "[", "len", "(", "e", "[", "'trg'", "]", ")", "for", "e", "in", "dataset", ".", "examples", "]", "\n", "self", ".", "batch_size", "=", "max_batch_pair", "\n", "self", ".", "max_example_number", "=", "max_batch_example", "\n", "self", ".", "num_workers", "=", "num_workers", "\n", "self", ".", "collate_fn", "=", "collate_fn", "\n", "self", ".", "pin_memory", "=", "pin_memory", "\n", "self", ".", "drop_last", "=", "drop_last", "\n", "\n", "if", "batch_sampler", "is", "not", "None", ":", "\n", "            ", "if", "max_batch_pair", ">", "1", "or", "shuffle", "or", "sampler", "is", "not", "None", "or", "drop_last", ":", "\n", "                ", "raise", "ValueError", "(", "'batch_sampler is mutually exclusive with '", "\n", "'batch_size, shuffle, sampler, and drop_last'", ")", "\n", "\n", "", "", "if", "sampler", "is", "not", "None", "and", "shuffle", ":", "\n", "            ", "raise", "ValueError", "(", "'sampler is mutually exclusive with shuffle'", ")", "\n", "\n", "", "if", "batch_sampler", "is", "None", ":", "\n", "            ", "if", "sampler", "is", "None", ":", "\n", "                ", "if", "shuffle", ":", "\n", "                    ", "sampler", "=", "RandomSampler", "(", "dataset", ")", "\n", "", "else", ":", "\n", "                    ", "sampler", "=", "SequentialSampler", "(", "dataset", ")", "\n", "\n", "", "", "", "batch_sampler", "=", "One2ManyBatchSampler", "(", "sampler", ",", "self", ".", "num_trgs", ",", "max_batch_example", "=", "max_batch_example", ",", "max_batch_pair", "=", "max_batch_pair", ",", "drop_last", "=", "drop_last", ")", "\n", "\n", "self", ".", "sampler", "=", "sampler", "\n", "self", ".", "batch_sampler", "=", "batch_sampler", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.KeyphraseDataLoader.__iter__": [[324, 326], ["dataloader.DataLoaderIter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "DataLoaderIter", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.KeyphraseDataLoader.__len__": [[327, 329], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "batch_sampler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.KeyphraseDataLoader.one2one_number": [[330, 332], ["sum"], "methods", ["None"], ["", "def", "one2one_number", "(", "self", ")", ":", "\n", "        ", "return", "sum", "(", "self", ".", "num_trgs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.One2ManyBatchSampler.__init__": [[352, 382], ["len", "sum", "batches.append", "batch.append", "len", "len", "len", "batch.append", "batches.append", "batches.append", "batch.append"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sampler", ",", "num_trgs", ",", "max_batch_example", ",", "max_batch_pair", ",", "drop_last", ")", ":", "\n", "        ", "self", ".", "sampler", "=", "sampler", "\n", "self", ".", "num_trgs", "=", "num_trgs", "\n", "self", ".", "max_batch_pair", "=", "max_batch_pair", "\n", "self", ".", "max_batch_example", "=", "max_batch_example", "\n", "self", ".", "drop_last", "=", "drop_last", "\n", "\n", "batches", "=", "[", "]", "\n", "batch", "=", "[", "]", "\n", "for", "idx", "in", "self", ".", "sampler", ":", "\n", "# number of targets sequences in current batch", "\n", "            ", "number_trgs", "=", "sum", "(", "[", "self", ".", "num_trgs", "[", "id", "]", "for", "id", "in", "batch", "]", ")", "\n", "if", "len", "(", "batch", ")", "<", "self", ".", "max_batch_example", "and", "number_trgs", "+", "self", ".", "num_trgs", "[", "idx", "]", "<", "self", ".", "max_batch_pair", ":", "\n", "                ", "batch", ".", "append", "(", "idx", ")", "\n", "", "elif", "len", "(", "batch", ")", "==", "0", ":", "# if the batch_size is very small, return a batch of only one data sample", "\n", "                ", "batch", ".", "append", "(", "idx", ")", "\n", "batches", ".", "append", "(", "batch", ")", "\n", "# print('batch %d: #(src)=%d, #(trg)=%d \\t\\t %s' % (len(batches), len(batch), number_trgs, str(batch)))", "\n", "batch", "=", "[", "]", "\n", "", "else", ":", "\n", "                ", "batches", ".", "append", "(", "batch", ")", "\n", "# print('batch %d: #(src)=%d, #(trg)=%d \\t\\t %s' % (len(batches), len(batch), number_trgs, str(batch)))", "\n", "batch", "=", "[", "]", "\n", "batch", ".", "append", "(", "idx", ")", "\n", "\n", "", "", "if", "len", "(", "batch", ")", ">", "0", "and", "not", "self", ".", "drop_last", ":", "\n", "            ", "batches", ".", "append", "(", "batch", ")", "\n", "\n", "", "self", ".", "batches", "=", "batches", "\n", "self", ".", "final_num_batch", "=", "len", "(", "batches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.One2ManyBatchSampler.__iter__": [[383, 385], ["dataloader.One2ManyBatchSampler.batches.__iter__"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.One2ManyBatchSampler.__iter__"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "batches", ".", "__iter__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.One2ManyBatchSampler.__len__": [[386, 388], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "final_num_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader._worker_loop": [[43, 60], ["torch.set_num_threads", "torch.set_num_threads", "index_queue.get", "data_queue.put", "collate_fn", "data_queue.put", "data_queue.put", "dataloader.ExceptionWrapper", "sys.exc_info"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.put", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.put", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.put"], ["", "", "def", "_worker_loop", "(", "dataset", ",", "index_queue", ",", "data_queue", ",", "collate_fn", ")", ":", "\n", "    ", "global", "_use_shared_memory", "\n", "_use_shared_memory", "=", "True", "\n", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "while", "True", ":", "\n", "        ", "r", "=", "index_queue", ".", "get", "(", ")", "\n", "if", "r", "is", "None", ":", "\n", "            ", "data_queue", ".", "put", "(", "None", ")", "\n", "break", "\n", "", "idx", ",", "batch_indices", "=", "r", "\n", "try", ":", "\n", "            ", "samples", "=", "collate_fn", "(", "[", "dataset", "[", "i", "]", "for", "i", "in", "batch_indices", "]", ")", "\n", "", "except", "Exception", ":", "\n", "            ", "data_queue", ".", "put", "(", "(", "idx", ",", "ExceptionWrapper", "(", "sys", ".", "exc_info", "(", ")", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "data_queue", ".", "put", "(", "(", "idx", ",", "samples", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader._pin_memory_loop": [[62, 82], ["isinstance", "in_queue.get", "out_queue.put", "dataloader.pin_memory_batch", "out_queue.put", "done_event.is_set", "out_queue.put", "dataloader.ExceptionWrapper", "sys.exc_info"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.put", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.pin_memory_batch", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.put", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.utils.source_representation_queue.SourceRepresentationQueue.put"], ["", "", "", "def", "_pin_memory_loop", "(", "in_queue", ",", "out_queue", ",", "done_event", ")", ":", "\n", "    ", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "r", "=", "in_queue", ".", "get", "(", ")", "\n", "", "except", "Exception", ":", "\n", "            ", "if", "done_event", ".", "is_set", "(", ")", ":", "\n", "                ", "return", "\n", "", "raise", "\n", "", "if", "r", "is", "None", ":", "\n", "            ", "break", "\n", "", "if", "isinstance", "(", "r", "[", "1", "]", ",", "ExceptionWrapper", ")", ":", "\n", "            ", "out_queue", ".", "put", "(", "r", ")", "\n", "continue", "\n", "", "idx", ",", "batch", "=", "r", "\n", "try", ":", "\n", "            ", "batch", "=", "pin_memory_batch", "(", "batch", ")", "\n", "", "except", "Exception", ":", "\n", "            ", "out_queue", ".", "put", "(", "(", "idx", ",", "ExceptionWrapper", "(", "sys", ".", "exc_info", "(", ")", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "out_queue", ".", "put", "(", "(", "idx", ",", "batch", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.default_collate": [[96, 135], ["type", "torch.is_tensor", "torch.is_tensor", "TypeError", "torch.stack", "torch.stack", "error_msg.format", "sum", "batch[].storage()._new_shared", "batch[].new", "isinstance", "type", "torch.stack", "torch.stack", "torch.LongTensor", "torch.LongTensor", "isinstance", "x.numel", "batch[].storage", "re.search", "TypeError", "elem.dtype.name.startswith", "list", "torch.DoubleTensor", "torch.DoubleTensor", "isinstance", "error_msg.format", "torch.from_numpy", "torch.from_numpy", "map", "isinstance", "isinstance", "dataloader.default_collate", "zip", "dataloader.default_collate"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.default_collate", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.default_collate"], ["def", "default_collate", "(", "batch", ")", ":", "\n", "    ", "\"Puts each data field into a tensor with outer dimension batch size\"", "\n", "\n", "error_msg", "=", "\"batch must contain tensors, numbers, dicts or lists; found {}\"", "\n", "elem_type", "=", "type", "(", "batch", "[", "0", "]", ")", "\n", "if", "torch", ".", "is_tensor", "(", "batch", "[", "0", "]", ")", ":", "\n", "        ", "out", "=", "None", "\n", "if", "_use_shared_memory", ":", "\n", "# If we're in a background process, concatenate directly into a", "\n", "# shared memory tensor to avoid an extra copy", "\n", "            ", "numel", "=", "sum", "(", "[", "x", ".", "numel", "(", ")", "for", "x", "in", "batch", "]", ")", "\n", "storage", "=", "batch", "[", "0", "]", ".", "storage", "(", ")", ".", "_new_shared", "(", "numel", ")", "\n", "out", "=", "batch", "[", "0", "]", ".", "new", "(", "storage", ")", "\n", "", "return", "torch", ".", "stack", "(", "batch", ",", "0", ",", "out", "=", "out", ")", "\n", "", "elif", "elem_type", ".", "__module__", "==", "'numpy'", "and", "elem_type", ".", "__name__", "!=", "'str_'", "and", "elem_type", ".", "__name__", "!=", "'string_'", ":", "\n", "        ", "elem", "=", "batch", "[", "0", "]", "\n", "if", "elem_type", ".", "__name__", "==", "'ndarray'", ":", "\n", "# array of string classes and object", "\n", "            ", "if", "re", ".", "search", "(", "'[SaUO]'", ",", "elem", ".", "dtype", ".", "str", ")", "is", "not", "None", ":", "\n", "                ", "raise", "TypeError", "(", "error_msg", ".", "format", "(", "elem", ".", "dtype", ")", ")", "\n", "\n", "", "return", "torch", ".", "stack", "(", "[", "torch", ".", "from_numpy", "(", "b", ")", "for", "b", "in", "batch", "]", ",", "0", ")", "\n", "", "if", "elem", ".", "shape", "==", "(", ")", ":", "# scalars", "\n", "            ", "py_type", "=", "float", "if", "elem", ".", "dtype", ".", "name", ".", "startswith", "(", "'float'", ")", "else", "int", "\n", "return", "numpy_type_map", "[", "elem", ".", "dtype", ".", "name", "]", "(", "list", "(", "map", "(", "py_type", ",", "batch", ")", ")", ")", "\n", "", "", "elif", "isinstance", "(", "batch", "[", "0", "]", ",", "int", ")", ":", "\n", "        ", "return", "torch", ".", "LongTensor", "(", "batch", ")", "\n", "", "elif", "isinstance", "(", "batch", "[", "0", "]", ",", "float", ")", ":", "\n", "        ", "return", "torch", ".", "DoubleTensor", "(", "batch", ")", "\n", "", "elif", "isinstance", "(", "batch", "[", "0", "]", ",", "string_classes", ")", ":", "\n", "        ", "return", "batch", "\n", "", "elif", "isinstance", "(", "batch", "[", "0", "]", ",", "collections", ".", "Mapping", ")", ":", "\n", "        ", "return", "{", "key", ":", "default_collate", "(", "[", "d", "[", "key", "]", "for", "d", "in", "batch", "]", ")", "for", "key", "in", "batch", "[", "0", "]", "}", "\n", "", "elif", "isinstance", "(", "batch", "[", "0", "]", ",", "collections", ".", "Sequence", ")", ":", "\n", "        ", "transposed", "=", "zip", "(", "*", "batch", ")", "\n", "return", "[", "default_collate", "(", "samples", ")", "for", "samples", "in", "transposed", "]", "\n", "\n", "", "raise", "TypeError", "(", "(", "error_msg", ".", "format", "(", "type", "(", "batch", "[", "0", "]", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.pin_memory_batch": [[137, 148], ["torch.is_tensor", "torch.is_tensor", "batch.pin_memory", "isinstance", "isinstance", "isinstance", "dataloader.pin_memory_batch", "batch.items", "dataloader.pin_memory_batch"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.pin_memory_batch", "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.dataloader.pin_memory_batch"], ["", "def", "pin_memory_batch", "(", "batch", ")", ":", "\n", "    ", "if", "torch", ".", "is_tensor", "(", "batch", ")", ":", "\n", "        ", "return", "batch", ".", "pin_memory", "(", ")", "\n", "", "elif", "isinstance", "(", "batch", ",", "string_classes", ")", ":", "\n", "        ", "return", "batch", "\n", "", "elif", "isinstance", "(", "batch", ",", "collections", ".", "Mapping", ")", ":", "\n", "        ", "return", "{", "k", ":", "pin_memory_batch", "(", "sample", ")", "for", "k", ",", "sample", "in", "batch", ".", "items", "(", ")", "}", "\n", "", "elif", "isinstance", "(", "batch", ",", "collections", ".", "Sequence", ")", ":", "\n", "        ", "return", "[", "pin_memory_batch", "(", "sample", ")", "for", "sample", "in", "batch", "]", "\n", "", "else", ":", "\n", "        ", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.manager.ManagerBasic.__init__": [[5, 18], ["torch.Module.__init__", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros.uniform_", "torch.zeros.uniform_", "torch.zeros.uniform_", "torch.zeros.uniform_", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.__init__"], ["    ", "def", "__init__", "(", "self", ",", "goal_vector_size", ")", ":", "\n", "        ", "super", "(", "ManagerBasic", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "goal_vector_size", "=", "goal_vector_size", "\n", "\n", "present_goal_vector", "=", "torch", ".", "zeros", "(", "self", ".", "goal_vector_size", ")", "\n", "absent_goal_vector", "=", "torch", ".", "zeros", "(", "self", ".", "goal_vector_size", ")", "\n", "# init uniformly", "\n", "initrange", "=", "0.1", "\n", "present_goal_vector", ".", "uniform_", "(", "-", "initrange", ",", "initrange", ")", "\n", "absent_goal_vector", ".", "uniform_", "(", "-", "initrange", ",", "initrange", ")", "\n", "# set them to module parameters", "\n", "self", ".", "present_goal_vector", "=", "nn", ".", "Parameter", "(", "present_goal_vector", ",", "True", ")", "\n", "self", ".", "absent_goal_vector", "=", "nn", ".", "Parameter", "(", "absent_goal_vector", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.manager.ManagerBasic.forward": [[19, 36], ["range", "torch.stack().unsqueeze", "torch.stack().unsqueeze", "torch.stack().unsqueeze", "torch.stack().unsqueeze", "is_absent.size", "int", "torch.stack().unsqueeze.append", "torch.stack().unsqueeze.append", "torch.stack().unsqueeze.append", "torch.stack().unsqueeze.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "is_absent[].item"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "is_absent", ")", ":", "\n", "        ", "\"\"\"\n        :param is_absent: tensor with size [batch_size]\n        :return:\n        \"\"\"", "\n", "batch_size", "=", "is_absent", ".", "size", "(", ")", "[", "0", "]", "\n", "g_t", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "if", "int", "(", "is_absent", "[", "i", "]", ".", "item", "(", ")", ")", "==", "1", ":", "\n", "                ", "g_t", ".", "append", "(", "self", ".", "absent_goal_vector", ")", "\n", "", "else", ":", "\n", "                ", "g_t", ".", "append", "(", "self", ".", "present_goal_vector", ")", "\n", "\n", "", "", "g_t", "=", "torch", ".", "stack", "(", "g_t", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "0", ")", "# [1, batch_size, goal_vector_size]", "\n", "\n", "return", "g_t", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.GetMask.__init__": [[11, 14], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.__init__"], ["def", "__init__", "(", "self", ",", "pad_idx", "=", "0", ")", ":", "\n", "        ", "super", "(", "GetMask", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pad_idx", "=", "pad_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.GetMask.forward": [[15, 18], ["torch.ne().float", "torch.ne().float", "torch.ne().float", "torch.ne().float", "torch.ne", "torch.ne", "torch.ne", "torch.ne"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "mask", "=", "torch", ".", "ne", "(", "x", ",", "self", ".", "pad_idx", ")", ".", "float", "(", ")", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.StandardNLL.forward": [[50, 57], ["mask.float.float.float", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "log_P.view.view.view", "log_prob.view", "y_true.contiguous().view", "y_true.size", "y_true.size", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "log_prob.size", "y_true.contiguous"], "methods", ["None"], ["def", "forward", "(", "self", ",", "log_prob", ",", "y_true", ",", "mask", ")", ":", "\n", "        ", "mask", "=", "mask", ".", "float", "(", ")", "\n", "log_P", "=", "torch", ".", "gather", "(", "log_prob", ".", "view", "(", "-", "1", ",", "log_prob", ".", "size", "(", "2", ")", ")", ",", "1", ",", "y_true", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", ")", "# batch*time x 1", "\n", "log_P", "=", "log_P", ".", "view", "(", "y_true", ".", "size", "(", "0", ")", ",", "y_true", ".", "size", "(", "1", ")", ")", "# batch x time", "\n", "log_P", "=", "log_P", "*", "mask", "# batch x time", "\n", "sum_log_P", "=", "torch", ".", "sum", "(", "log_P", ",", "dim", "=", "1", ")", "/", "torch", ".", "sum", "(", "mask", ",", "dim", "=", "1", ")", "# batch", "\n", "return", "-", "sum_log_P", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.__init__": [[68, 71], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.__init__"], ["def", "__init__", "(", "self", ",", "mlp", ")", ":", "\n", "        ", "super", "(", "TimeDistributedDense", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mlp", "=", "mlp", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.forward": [[72, 81], ["x.view.view.size", "x.view.view.view", "mask.TimeDistributedDense.mlp.forward", "y.view.view.view", "mask.unsqueeze", "y.view.view.size"], "methods", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.TimeDistributedDense.forward"], ["", "def", "forward", "(", "self", ",", "x", ",", "mask", "=", "None", ")", ":", "\n", "\n", "        ", "x_size", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "x_size", "[", "-", "1", "]", ")", "# batch*time x a", "\n", "y", "=", "self", ".", "mlp", ".", "forward", "(", "x", ")", "# batch*time x b", "\n", "y", "=", "y", ".", "view", "(", "x_size", "[", ":", "-", "1", "]", "+", "(", "y", ".", "size", "(", "-", "1", ")", ",", ")", ")", "# batch x time x b", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "y", "=", "y", "*", "mask", ".", "unsqueeze", "(", "-", "1", ")", "# batch x time x b", "\n", "", "return", "y", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.masked_softmax": [[20, 33], ["torch.clamp", "torch.clamp", "torch.exp", "torch.exp", "m.float.float", "torch.sum", "torch.sum", "torch.max", "torch.max"], "function", ["None"], ["", "", "def", "masked_softmax", "(", "x", ",", "m", "=", "None", ",", "axis", "=", "-", "1", ")", ":", "\n", "    ", "'''\n    Softmax with mask (optional)\n    '''", "\n", "x", "=", "torch", ".", "clamp", "(", "x", ",", "min", "=", "-", "15.0", ",", "max", "=", "15.0", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "        ", "m", "=", "m", ".", "float", "(", ")", "\n", "x", "=", "x", "*", "m", "\n", "", "e_x", "=", "torch", ".", "exp", "(", "x", "-", "torch", ".", "max", "(", "x", ",", "dim", "=", "axis", ",", "keepdim", "=", "True", ")", "[", "0", "]", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "        ", "e_x", "=", "e_x", "*", "m", "\n", "", "softmax", "=", "e_x", "/", "(", "torch", ".", "sum", "(", "e_x", ",", "dim", "=", "axis", ",", "keepdim", "=", "True", ")", "+", "1e-6", ")", "\n", "return", "softmax", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.masked_log_softmax": [[34, 39], ["torch.log", "torch.log", "mask.masked_softmax"], "function", ["home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.masked_softmax"], ["", "def", "masked_log_softmax", "(", "x", ",", "m", "=", "None", ",", "axis", "=", "-", "1", ")", ":", "\n", "    ", "'''\n    Log softmax with mask (optional), might be numerically unstable?\n    '''", "\n", "return", "torch", ".", "log", "(", "masked_softmax", "(", "x", ",", "m", ",", "axis", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xuyige_fgrl4kg.pykp.mask.masked_cross_entropy": [[58, 60], ["None"], "function", ["None"], ["", "", "def", "masked_cross_entropy", "(", "logits", ",", "targets", ",", "length", ")", ":", "\n", "    ", "return", "\n", "\n"]]}