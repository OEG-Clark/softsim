{"home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_cql.get_data_sampler_func": [[15, 17], ["egpo_utils.common.CQLInputReader"], "function", ["None"], ["def", "get_data_sampler_func", "(", "ioctx", ")", ":", "\n", "    ", "return", "CQLInputReader", "(", "data_set_file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.evaluate_egpo_with_human.get_function": [[9, 46], ["egpo_utils.egpo.egpo.EGPOTrainer", "egpo_utils.egpo.egpo.EGPOTrainer.restore", "dict", "egpo_utils.egpo.egpo.EGPOTrainer.compute_actions", "dict"], "function", ["None"], ["def", "get_function", "(", "ckpt", ")", ":", "\n", "    ", "trainer", "=", "EGPOTrainer", "(", "dict", "(", "\n", "\n", "env", "=", "HumanInTheLoopEnv", ",", "\n", "\n", "# ===== Training =====", "\n", "takeover_data_discard", "=", "False", ",", "\n", "alpha", "=", "10.0", ",", "\n", "recent_episode_num", "=", "5", ",", "\n", "normalize", "=", "True", ",", "\n", "twin_cost_q", "=", "True", ",", "\n", "k_i", "=", "0.01", ",", "\n", "k_p", "=", "5", ",", "\n", "# search > 0", "\n", "k_d", "=", "0.1", ",", "\n", "\n", "# expected max takeover num", "\n", "cost_limit", "=", "300", ",", "\n", "optimization", "=", "dict", "(", "actor_learning_rate", "=", "1e-4", ",", "critic_learning_rate", "=", "1e-4", ",", "entropy_learning_rate", "=", "1e-4", ")", ",", "\n", "prioritized_replay", "=", "False", ",", "\n", "horizon", "=", "400", ",", "\n", "target_network_update_freq", "=", "1", ",", "\n", "timesteps_per_iteration", "=", "100", ",", "\n", "metrics_smoothing_episodes", "=", "10", ",", "\n", "learning_starts", "=", "100", ",", "\n", "clip_actions", "=", "False", ",", "\n", "normalize_actions", "=", "True", ",", "\n", "\n", ")", ")", "\n", "\n", "trainer", ".", "restore", "(", "ckpt", ")", "\n", "\n", "def", "_f", "(", "obs", ")", ":", "\n", "        ", "ret", "=", "trainer", ".", "compute_actions", "(", "{", "\"default_policy\"", ":", "obs", "}", ")", "\n", "return", "ret", "\n", "\n", "", "return", "_f", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.GAILExpertGuidedEnv._reset_agents": [[38, 42], ["super()._reset_agents", "metadrive.utils.get_np_random().rand", "metadrive.utils.get_np_random"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.GAILExpertGuidedEnv._reset_agents"], ["def", "_reset_agents", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "config", "[", "\"random_spawn\"", "]", ":", "\n", "            ", "self", ".", "vehicle", ".", "vehicle_config", "[", "\"spawn_longitude\"", "]", "=", "get_np_random", "(", ")", ".", "rand", "(", ")", "*", "40", "+", "10", "\n", "", "super", "(", "GAILExpertGuidedEnv", ",", "self", ")", ".", "_reset_agents", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner.__init__": [[77, 109], ["int", "train_gail.Learner._init_env", "train_gail.Learner._load_expert_traj", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "egpo_utils.gail.mlp.Policy().to().float", "egpo_utils.gail.mlp.Value().to().float", "egpo_utils.expert_guided_env.ExpertGuidedEnv", "egpo_utils.gail.mlp.Policy().to", "egpo_utils.gail.mlp.Value().to", "egpo_utils.gail.mlp.Policy", "egpo_utils.gail.mlp.Value"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner._init_env", "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner._load_expert_traj"], ["    ", "def", "__init__", "(", "self", ",", "cfg", ":", "EasyDict", ")", ":", "\n", "# self._init_cfg(cfg)", "\n", "# self._process_cfg()", "\n", "        ", "self", ".", "cfg", "=", "cfg", "\n", "self", ".", "env_num", "=", "20", "\n", "\n", "# hyper para", "\n", "self", ".", "g_optim_num", "=", "5", "\n", "self", ".", "d_optim_num", "=", "2000", "\n", "self", ".", "sgd_batch_size", "=", "64", "\n", "self", ".", "ppo_iterations", "=", "200", "\n", "self", ".", "g_learning_rate", "=", "1e-4", "\n", "self", ".", "d_learning_rate", "=", "5e-3", "# 1e-2", "\n", "self", ".", "eval_interval", "=", "5", "\n", "self", ".", "eval_episodes", "=", "30", "\n", "self", ".", "clip_epsilon", "=", "0.2", "\n", "\n", "# auto calculate", "\n", "self", ".", "ppo_train_batch_size", "=", "self", ".", "sgd_batch_size", "*", "self", ".", "ppo_iterations", "\n", "self", ".", "buffer_length", "=", "int", "(", "self", ".", "sgd_batch_size", "*", "self", ".", "ppo_iterations", "/", "self", ".", "env_num", ")", "\n", "self", ".", "buffer", "=", "None", "\n", "self", ".", "_init_env", "(", ")", "\n", "self", ".", "_load_expert_traj", "(", ")", "\n", "tm_stamp", "=", "\"%s-%s-%s-%s-%s-%s\"", "%", "(", "tm", ".", "tm_year", ",", "tm", ".", "tm_mon", ",", "tm", ".", "tm_mday", ",", "tm", ".", "tm_hour", ",", "tm", ".", "tm_min", ",", "tm", ".", "tm_sec", ")", "\n", "self", ".", "cfg", ".", "log_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "\"gail_iter_{}_g_{}_d_{}_bs_{}_lr_d_{}\"", ".", "format", "(", "self", ".", "ppo_iterations", ",", "self", ".", "g_optim_num", ",", "self", ".", "d_optim_num", ",", "\n", "self", ".", "sgd_batch_size", ",", "self", ".", "d_learning_rate", ")", ",", "\n", "tm_stamp", ")", "\n", "self", ".", "policy_net", "=", "Policy", "(", "state_dim", "=", "259", ",", "action_dim", "=", "2", ")", ".", "to", "(", "self", ".", "cfg", ".", "device", ")", ".", "float", "(", ")", "\n", "self", ".", "value_net", "=", "Value", "(", "state_dim", "=", "259", "+", "2", ")", ".", "to", "(", "self", ".", "cfg", ".", "device", ")", ".", "float", "(", ")", "\n", "self", ".", "eval_env", "=", "ExpertGuidedEnv", "(", "eval_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner._load_expert_traj": [[110, 120], ["open", "json.load", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "ValueError", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load"], ["", "def", "_load_expert_traj", "(", "self", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "file", "=", "open", "(", "expert_data_path", ")", "\n", "traj", "=", "json", ".", "load", "(", "file", ")", "\n", "obs", "=", "[", "i", "[", "\"obs\"", "]", "for", "i", "in", "traj", "]", "\n", "action", "=", "[", "i", "[", "\"actions\"", "]", "for", "i", "in", "traj", "]", "\n", "self", ".", "exp_obs", "=", "torch", ".", "tensor", "(", "obs", ")", ".", "to", "(", "self", ".", "cfg", ".", "device", ")", ".", "float", "(", ")", "\n", "self", ".", "exp_action", "=", "torch", ".", "tensor", "(", "action", ")", ".", "to", "(", "self", ".", "cfg", ".", "device", ")", ".", "float", "(", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "raise", "ValueError", "(", "\"Please collect dataset by using collect_dataset.py at first\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner._init_env": [[121, 125], ["stable_baselines3.common.vec_env.SubprocVecEnv", "train_gail.make_env", "range"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_dagger.make_env"], ["", "", "def", "_init_env", "(", "self", ")", ":", "\n", "# self.env = PGDriveEnv(dict(environment_num=1))", "\n", "        ", "self", ".", "env", "=", "SubprocVecEnv", "(", "\n", "[", "make_env", "(", "GAILExpertGuidedEnv", ",", "i", ",", "config", "=", "training_config", ")", "for", "i", "in", "range", "(", "self", ".", "env_num", ")", "]", ")", "\n", "# self.env = make_vec_env('PGDrive-v0', n_envs=self.env_num, seed=0)", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner._process_cfg": [[127, 131], ["isinstance", "train_gail.Learner.cfg.get", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device"], "methods", ["None"], ["", "def", "_process_cfg", "(", "self", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "cfg", ".", "get", "(", "'device'", ",", "torch", ".", "device", "(", "'cpu'", ")", ")", ",", "str", ")", ":", "\n", "            ", "assert", "self", ".", "cfg", ".", "device", "in", "[", "'cpu'", ",", "'cuda'", "]", "\n", "self", ".", "cfg", ".", "device", "=", "torch", ".", "device", "(", "self", ".", "cfg", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner._collect_samples": [[132, 186], ["train_gail.Learner.env.reset", "range", "numpy.arange", "numpy.random.shuffle", "collections.OrderedDict", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "batch_obs.append", "batch_prob.append", "batch_action.append", "train_gail.Learner.env.step", "batch_reward.append", "enumerate", "numpy.mean", "range", "range", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "train_gail.Learner.policy_net.select_action", "action.cpu().numpy", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "range", "range", "range", "train_gail.Learner.env.remotes[].send", "train_gail.Learner.env.remotes[].recv", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "action.cpu", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.PIDController.reset", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.step", "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Policy.select_action"], ["", "", "def", "_collect_samples", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "batch_obs", "=", "[", "]", "\n", "batch_prob", "=", "[", "]", "\n", "batch_action", "=", "[", "]", "\n", "batch_reward", "=", "[", "]", "\n", "\n", "# training metric", "\n", "done_num", "=", "1", "\n", "success_num", "=", "0", "\n", "episode_reward_mean", "=", "[", "0", "for", "_", "in", "range", "(", "self", ".", "env_num", ")", "]", "\n", "episode_cost_mean", "=", "[", "0", "for", "_", "in", "range", "(", "self", ".", "env_num", ")", "]", "\n", "total_episode_reward", "=", "0", "\n", "total_episode_cost", "=", "0", "\n", "total_episode_velocity", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "buffer_length", ")", ":", "\n", "            ", "obs", "=", "torch", ".", "tensor", "(", "obs", ")", ".", "to", "(", "self", ".", "cfg", ".", "device", ")", ".", "float", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "action", ",", "prob", "=", "self", ".", "policy_net", ".", "select_action", "(", "obs", ")", "\n", "", "batch_obs", ".", "append", "(", "obs", ")", "\n", "batch_prob", ".", "append", "(", "prob", ")", "\n", "batch_action", ".", "append", "(", "action", ")", "\n", "obs", ",", "reward", ",", "dones", ",", "info", ",", "=", "self", ".", "env", ".", "step", "(", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "batch_reward", ".", "append", "(", "torch", ".", "tensor", "(", "reward", ")", ")", "\n", "\n", "total_episode_velocity", "+=", "[", "info", "[", "idx", "]", "[", "\"velocity\"", "]", "for", "idx", "in", "range", "(", "self", ".", "env_num", ")", "]", "\n", "episode_reward_mean", "=", "[", "episode_reward_mean", "[", "i", "]", "+", "reward", "[", "i", "]", "for", "i", "in", "range", "(", "self", ".", "env_num", ")", "]", "\n", "episode_cost_mean", "=", "[", "episode_cost_mean", "[", "i", "]", "+", "info", "[", "i", "]", "[", "\"native_cost\"", "]", "for", "i", "in", "range", "(", "self", ".", "env_num", ")", "]", "\n", "\n", "# asyn done", "\n", "for", "idx", ",", "done", "in", "enumerate", "(", "dones", ")", ":", "\n", "                ", "if", "done", ":", "\n", "                    ", "done_num", "+=", "1", "\n", "success_num", "+=", "1", "if", "info", "[", "idx", "]", "[", "\"arrive_dest\"", "]", "else", "0", "\n", "total_episode_reward", "+=", "episode_reward_mean", "[", "idx", "]", "\n", "total_episode_cost", "+=", "episode_cost_mean", "[", "idx", "]", "\n", "episode_reward_mean", "[", "idx", "]", "=", "0", "\n", "episode_cost_mean", "[", "idx", "]", "=", "0", "\n", "self", ".", "env", ".", "remotes", "[", "idx", "]", ".", "send", "(", "(", "\"reset\"", ",", "None", ")", ")", "\n", "this_obs", "=", "self", ".", "env", ".", "remotes", "[", "idx", "]", ".", "recv", "(", ")", "\n", "obs", "[", "idx", "]", "=", "this_obs", "\n", "# return data", "\n", "", "", "", "perm", "=", "np", ".", "arange", "(", "self", ".", "buffer_length", "*", "self", ".", "env_num", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "perm", ")", "\n", "self", ".", "buffer", "=", "OrderedDict", "(", "{", "\n", "'obs'", ":", "torch", ".", "cat", "(", "batch_obs", ",", "0", ")", "[", "perm", "]", ",", "\n", "'action'", ":", "torch", ".", "cat", "(", "batch_action", ",", "0", ")", "[", "perm", "]", ",", "\n", "'prob'", ":", "torch", ".", "cat", "(", "batch_prob", ",", "0", ")", "[", "perm", "]", ",", "\n", "'reward'", ":", "torch", ".", "cat", "(", "batch_reward", ")", "[", "perm", "]", ",", "\n", "}", ")", "\n", "return", "{", "\"episode_reward_mean\"", ":", "total_episode_reward", "/", "done_num", ",", "\n", "\"success_rate_mean\"", ":", "success_num", "/", "done_num", ",", "\n", "\"episode_cost_mean\"", ":", "total_episode_cost", "/", "done_num", ",", "\n", "\"episode_velocity\"", ":", "np", ".", "mean", "(", "total_episode_velocity", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner._sample_from_buffer": [[187, 191], ["min", "train_gail.Learner.buffer.items"], "methods", ["None"], ["", "def", "_sample_from_buffer", "(", "self", ",", "batch_size", ",", "cnt", ")", ":", "\n", "        ", "start", "=", "batch_size", "*", "cnt", "\n", "end", "=", "min", "(", "batch_size", "*", "(", "cnt", "+", "1", ")", ",", "self", ".", "buffer_length", "*", "self", ".", "env_num", ")", "\n", "return", "(", "v", "[", "start", ":", "end", "]", "for", "k", ",", "v", "in", "self", ".", "buffer", ".", "items", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner.evaluation": [[192, 225], ["print", "env.reset", "dict", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "torch.tensor().to().float", "env.step", "velocity.append", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "train_gail.Learner.policy_net.select_action", "env.reset", "episode_overtake.append", "numpy.mean", "numpy.mean", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "action.cpu().numpy", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "action.cpu"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.PIDController.reset", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.step", "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Policy.select_action", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.PIDController.reset"], ["", "def", "evaluation", "(", "self", ",", "evaluation_episode_num", "=", "30", ")", ":", "\n", "        ", "env", "=", "self", ".", "eval_env", "\n", "print", "(", "\"... evaluation\"", ")", "\n", "episode_reward", "=", "0", "\n", "success_num", "=", "0", "\n", "episode_num", "=", "0", "\n", "episode_cost", "=", "0", "\n", "velocity", "=", "[", "]", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "episode_overtake", "=", "[", "]", "\n", "while", "episode_num", "<", "evaluation_episode_num", ":", "\n", "            ", "state", "=", "torch", ".", "tensor", "(", "[", "state", "]", ")", ".", "to", "(", "self", ".", "cfg", ".", "device", ")", ".", "float", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "action", ",", "prob", "=", "self", ".", "policy_net", ".", "select_action", "(", "state", ")", "\n", "", "next_state", ",", "r", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", ")", "\n", "velocity", ".", "append", "(", "info", "[", "\"velocity\"", "]", ")", "\n", "state", "=", "next_state", "\n", "episode_reward", "+=", "r", "\n", "episode_cost", "+=", "info", "[", "\"native_cost\"", "]", "\n", "if", "done", ":", "\n", "                ", "episode_num", "+=", "1", "\n", "env", ".", "reset", "(", ")", "\n", "if", "info", "[", "\"arrive_dest\"", "]", ":", "\n", "                    ", "success_num", "+=", "1", "\n", "", "episode_overtake", ".", "append", "(", "info", "[", "\"overtake_vehicle_num\"", "]", ")", "\n", "", "", "res", "=", "dict", "(", "\n", "mean_episode_reward", "=", "episode_reward", "/", "evaluation_episode_num", ",", "\n", "mean_success_rate", "=", "success_num", "/", "evaluation_episode_num", ",", "\n", "mean_episode_cost", "=", "episode_cost", "/", "evaluation_episode_num", ",", "\n", "mean_velocity", "=", "np", ".", "mean", "(", "velocity", ")", ",", "\n", "mean_episode_overtake_num", "=", "np", ".", "mean", "(", "episode_overtake", ")", "\n", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner.train": [[226, 290], ["train_gail.Learner.policy_net.train", "train_gail.Learner.value_net.train", "time.time", "train_gail.Learner._collect_samples", "range", "range", "dict", "numpy.mean", "exp_log.scalar", "exp_log.scalar", "train_gail.Learner.value_net", "train_gail.Learner.value_net", "train_gail.Learner.optim_d.zero_grad", "discrim_loss.backward", "train_gail.Learner.optim_d.step", "sum", "len", "range", "sum", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "d_loss_list.append", "train_gail.Learner._sample_from_buffer", "obs.to().float.to().float.to().float", "action.to().float.to().float.to().float", "prob.to().float.to().float.to().float", "train_gail.Learner.value_net", "train_gail.Learner.detach", "train_gail.Learner.policy_net.get_log_prob", "rl_loss_list.append", "train_gail.Learner.optim_g.zero_grad", "rl_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "train_gail.Learner.optim_g.step", "torch.BCELoss().float", "torch.BCELoss().float", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.BCELoss().float", "torch.BCELoss().float", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "discrim_loss.item", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ratio.clamp", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "rl_loss.item", "list", "obs.to().float.to().float.to", "action.to().float.to().float.to", "prob.to().float.to().float.to", "train_gail.Learner.policy_net.parameters", "time.time", "torch.BCELoss", "torch.BCELoss", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.BCELoss", "torch.BCELoss", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.min", "torch.min", "torch.min", "torch.min"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.train.train.train", "home.repos.pwc.inspect_result.decisionforce_EGPO.train.train.train", "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner._collect_samples", "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver.Experiment.scalar", "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver.Experiment.scalar", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.step", "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner._sample_from_buffer", "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Policy.get_log_prob", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.step"], ["", "def", "train", "(", "self", ",", "is_train", ")", ":", "\n", "        ", "self", ".", "policy_net", ".", "train", "(", ")", "\n", "self", ".", "value_net", ".", "train", "(", ")", "\n", "tick", "=", "time", ".", "time", "(", ")", "\n", "sample_result", "=", "self", ".", "_collect_samples", "(", ")", "\n", "\n", "# train discriminator", "\n", "d_loss_list", "=", "[", "]", "\n", "obs", "=", "self", ".", "buffer", "[", "\"obs\"", "]", "\n", "action", "=", "self", ".", "buffer", "[", "\"action\"", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "d_optim_num", ")", ":", "\n", "            ", "g_o", "=", "self", ".", "value_net", "(", "torch", ".", "cat", "(", "[", "obs", ",", "action", "]", ",", "1", ")", ")", "\n", "e_o", "=", "self", ".", "value_net", "(", "torch", ".", "cat", "(", "[", "self", ".", "exp_obs", ",", "self", ".", "exp_action", "]", ",", "1", ")", ")", "\n", "discrim_loss", "=", "nn", ".", "BCELoss", "(", ")", ".", "float", "(", ")", "(", "g_o", ",", "torch", ".", "zeros", "(", "(", "obs", ".", "shape", "[", "0", "]", ",", "1", ")", ")", ".", "cuda", "(", ")", ")", "+", "nn", ".", "BCELoss", "(", ")", ".", "float", "(", ")", "(", "e_o", ",", "torch", ".", "ones", "(", "(", "self", ".", "exp_obs", ".", "shape", "[", "0", "]", ",", "1", ")", ")", ".", "cuda", "(", ")", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "d_loss_list", ".", "append", "(", "discrim_loss", ".", "item", "(", ")", ")", "\n", "# update d", "\n", "", "self", ".", "optim_d", ".", "zero_grad", "(", ")", "\n", "discrim_loss", ".", "backward", "(", ")", "\n", "self", ".", "optim_d", ".", "step", "(", ")", "\n", "", "d_loss_mean", "=", "sum", "(", "d_loss_list", ")", "/", "len", "(", "d_loss_list", ")", "\n", "\n", "# train generator", "\n", "rl_loss_list", "=", "[", "]", "\n", "step_reward", "=", "[", "]", "\n", "for", "opt_idx", "in", "range", "(", "self", ".", "g_optim_num", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "self", ".", "ppo_iterations", ")", ":", "\n", "# obs, action, prob = self._collect_samples()", "\n", "                ", "obs", ",", "action", ",", "prob", ",", "real_reward", "=", "self", ".", "_sample_from_buffer", "(", "self", ".", "sgd_batch_size", ",", "i", ")", "\n", "step_reward", "+=", "real_reward", "\n", "obs", "=", "obs", ".", "to", "(", "self", ".", "cfg", ".", "device", ")", ".", "float", "(", ")", "\n", "action", "=", "action", ".", "to", "(", "self", ".", "cfg", ".", "device", ")", ".", "float", "(", ")", "\n", "prob", "=", "prob", ".", "to", "(", "self", ".", "cfg", ".", "device", ")", ".", "float", "(", ")", "\n", "g_o", "=", "self", ".", "value_net", "(", "torch", ".", "cat", "(", "[", "obs", ",", "action", "]", ",", "1", ")", ")", "\n", "\n", "# update g", "\n", "reward", "=", "g_o", ".", "detach", "(", ")", "\n", "obs_s", ",", "action_s", ",", "log_p_old_s", ",", "reward_s", "=", "obs", ",", "action", ",", "prob", ",", "reward", "\n", "\n", "# perform ppo step", "\n", "log_p", "=", "self", ".", "policy_net", ".", "get_log_prob", "(", "obs_s", ",", "action_s", ")", "\n", "ratio", "=", "(", "log_p", "-", "log_p_old_s", ")", ".", "exp", "(", ")", ".", "float", "(", ")", "\n", "surr1", "=", "ratio", "*", "reward_s", "\n", "surr2", "=", "ratio", ".", "clamp", "(", "1.0", "-", "self", ".", "clip_epsilon", ",", "1.0", "+", "self", ".", "clip_epsilon", ")", "*", "reward_s", "\n", "rl_loss", "=", "-", "torch", ".", "min", "(", "surr1", ",", "surr2", ")", ".", "mean", "(", ")", "\n", "rl_loss_list", ".", "append", "(", "rl_loss", ".", "item", "(", ")", ")", "\n", "\n", "self", ".", "optim_g", ".", "zero_grad", "(", ")", "\n", "rl_loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "list", "(", "self", ".", "policy_net", ".", "parameters", "(", ")", ")", ",", "10", ")", "\n", "self", ".", "optim_g", ".", "step", "(", ")", "\n", "", "", "loss_mean", "=", "sum", "(", "rl_loss_list", ")", "/", "len", "(", "rl_loss_list", ")", "\n", "\n", "metrics", "=", "dict", "(", ")", "\n", "metrics", "[", "'generator_loss'", "]", "=", "loss_mean", "\n", "metrics", "[", "'discriminator_loss'", "]", "=", "d_loss_mean", "\n", "metrics", "[", "'step_reward'", "]", "=", "np", ".", "mean", "(", "step_reward", ")", "\n", "metrics", "[", "\"episode_reward\"", "]", "=", "sample_result", "[", "\"episode_reward_mean\"", "]", "\n", "metrics", "[", "\"episode_cost\"", "]", "=", "sample_result", "[", "\"episode_cost_mean\"", "]", "\n", "metrics", "[", "\"success_rate\"", "]", "=", "sample_result", "[", "\"success_rate_mean\"", "]", "\n", "\n", "exp_log", ".", "scalar", "(", "is_train", "=", "is_train", ",", "**", "metrics", ")", "\n", "exp_log", ".", "scalar", "(", "is_train", "=", "is_train", ",", "fps", "=", "self", ".", "sgd_batch_size", "*", "self", ".", "ppo_iterations", "/", "(", "time", ".", "time", "(", ")", "-", "tick", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.Learner.learn": [[291, 319], ["exp_log.init", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "range", "pathlib.Path", "list", "list", "str", "str", "print", "train_gail.Learner.policy_net.load_state_dict", "train_gail.Learner.value_net.load_state_dict", "train_gail.Learner.value_net.parameters", "train_gail.Learner.policy_net.parameters", "train_gail.Learner.train", "exp_log.end_epoch", "pathlib.Path.glob", "pathlib.Path.glob", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "train_gail.Learner.evaluation", "exp_log.scalar", "train_gail.Learner.value_net.state_dict", "str", "train_gail.Learner.policy_net.state_dict", "str", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver.Experiment.init", "home.repos.pwc.inspect_result.decisionforce_EGPO.train.train.train", "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver.Experiment.end_epoch", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.utils.evaluation", "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver.Experiment.scalar"], ["", "def", "learn", "(", "self", ")", ":", "\n", "        ", "exp_log", ".", "init", "(", "self", ".", "cfg", ".", "log_dir", ")", "\n", "if", "self", ".", "cfg", ".", "resume", ":", "\n", "            ", "log_dir", "=", "Path", "(", "self", ".", "cfg", ".", "log_dir", ")", "\n", "checkpoints_d", "=", "list", "(", "log_dir", ".", "glob", "(", "'model_d_*.th'", ")", ")", "\n", "checkpoints_g", "=", "list", "(", "log_dir", ".", "glob", "(", "'model_g_*.th'", ")", ")", "\n", "checkpoint_d", "=", "str", "(", "checkpoints_d", "[", "-", "1", "]", ")", "\n", "checkpoint_g", "=", "str", "(", "checkpoints_g", "[", "-", "1", "]", ")", "\n", "print", "(", "\"load {} {}\"", ".", "format", "(", "checkpoint_d", ",", "checkpoint_g", ")", ")", "\n", "self", ".", "policy_net", ".", "load_state_dict", "(", "torch", ".", "load", "(", "checkpoint_d", ")", ")", "\n", "self", ".", "value_net", ".", "load_state_dict", "(", "torch", ".", "load", "(", "checkpoint_g", ")", ")", "\n", "\n", "", "self", ".", "optim_d", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "value_net", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "d_learning_rate", ")", "\n", "self", ".", "optim_g", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "policy_net", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "g_learning_rate", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "self", ".", "cfg", ".", "max_epoch", "+", "1", ")", ":", "\n", "            ", "self", ".", "train", "(", "True", ")", "\n", "if", "epoch", "%", "self", ".", "cfg", ".", "save_freq", "==", "0", ":", "\n", "                ", "torch", ".", "save", "(", "\n", "self", ".", "value_net", ".", "state_dict", "(", ")", ",", "\n", "str", "(", "Path", "(", "self", ".", "cfg", ".", "log_dir", ")", "/", "(", "'model_d_%d.th'", "%", "epoch", ")", ")", ")", "\n", "torch", ".", "save", "(", "\n", "self", ".", "policy_net", ".", "state_dict", "(", ")", ",", "\n", "str", "(", "Path", "(", "self", ".", "cfg", ".", "log_dir", ")", "/", "(", "'model_g_%d.th'", "%", "epoch", ")", ")", ")", "\n", "", "if", "epoch", "%", "self", ".", "eval_interval", "==", "0", ":", "\n", "                ", "res", "=", "self", ".", "evaluation", "(", "self", ".", "eval_episodes", ")", "\n", "exp_log", ".", "scalar", "(", "is_train", "=", "False", ",", "**", "res", ")", "\n", "", "exp_log", ".", "end_epoch", "(", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_gail.make_env": [[66, 74], ["stable_baselines3.common.utils.set_random_seed", "env_cls", "env_cls.seed"], "function", ["None"], ["def", "make_env", "(", "env_cls", ",", "rank", ",", "config", ",", "seed", "=", "0", ")", ":", "\n", "    ", "def", "_init", "(", ")", ":", "\n", "        ", "env", "=", "env_cls", "(", "config", ")", "\n", "env", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "return", "env", "\n", "\n", "", "set_random_seed", "(", "seed", ")", "\n", "return", "_init", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_bc.get_data_sampler_func": [[15, 17], ["egpo_utils.common.CQLInputReader"], "function", ["None"], ["def", "get_data_sampler_func", "(", "ioctx", ")", ":", "\n", "    ", "return", "CQLInputReader", "(", "data_set_file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_egpo_with_human.get_time_str": [[15, 17], ["datetime.datetime.now().strftime", "datetime.datetime.now"], "function", ["None"], ["", "def", "get_time_str", "(", ")", ":", "\n", "    ", "return", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%y%m%d-%H%M%S\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.train_dagger.make_env": [[43, 49], ["env_cls"], "function", ["None"], ["def", "make_env", "(", "env_cls", ",", "config", ",", "seed", "=", "0", ")", ":", "\n", "    ", "def", "_init", "(", ")", ":", "\n", "        ", "env", "=", "env_cls", "(", "config", ")", "\n", "return", "env", "\n", "\n", "", "return", "_init", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.training_script.collect_dataset.process_info": [[8, 15], ["info.items"], "function", ["None"], ["def", "process_info", "(", "info", ")", ":", "\n", "    ", "ret", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "info", ".", "items", "(", ")", ":", "\n", "# filter float 32", "\n", "        ", "if", "k", "!=", "\"raw_action\"", ":", "\n", "            ", "ret", "[", "k", "]", "=", "v", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.human_in_the_loop_env.HumanInTheLoopEnv.default_config": [[12, 27], ["super().default_config", "super().default_config.update"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.default_config"], ["def", "default_config", "(", "self", ")", ":", "\n", "        ", "config", "=", "super", "(", "HumanInTheLoopEnv", ",", "self", ")", ".", "default_config", "(", ")", "\n", "config", ".", "update", "(", "\n", "{", "\n", "\"environment_num\"", ":", "1", ",", "\n", "\"start_seed\"", ":", "10", ",", "\n", "\"map\"", ":", "\"Cr\"", ",", "\n", "\"cost_to_reward\"", ":", "True", ",", "\n", "\"manual_control\"", ":", "True", ",", "\n", "\"controller\"", ":", "\"joystick\"", ",", "\n", "\"agent_policy\"", ":", "TakeoverPolicy", "\n", "}", ",", "\n", "allow_add_new_key", "=", "True", "\n", ")", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.human_in_the_loop_env.HumanInTheLoopEnv.reset": [[28, 32], ["super().reset"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.PIDController.reset"], ["", "def", "reset", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "t_o", "=", "False", "\n", "self", ".", "total_takeover_cost", "=", "0", "\n", "return", "super", "(", "HumanInTheLoopEnv", ",", "self", ")", ".", "reset", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.human_in_the_loop_env.HumanInTheLoopEnv._get_step_return": [[33, 46], ["super()._get_step_return", "human_in_the_loop_env.HumanInTheLoopEnv.engine.get_policy", "hasattr"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.human_in_the_loop_env.HumanInTheLoopEnv._get_step_return"], ["", "def", "_get_step_return", "(", "self", ",", "actions", ",", "step_infos", ")", ":", "\n", "        ", "o", ",", "r", ",", "d", ",", "step_infos", "=", "super", "(", "HumanInTheLoopEnv", ",", "self", ")", ".", "_get_step_return", "(", "actions", ",", "step_infos", ")", "\n", "controller", "=", "self", ".", "engine", ".", "get_policy", "(", "self", ".", "vehicle", ".", "id", ")", "\n", "last_t_o", "=", "self", ".", "t_o", "\n", "self", ".", "t_o", "=", "controller", ".", "takeover", "if", "hasattr", "(", "controller", ",", "\"takeover\"", ")", "else", "False", "\n", "step_infos", "[", "\"takeover\"", "]", "=", "self", ".", "t_o", "\n", "if", "step_infos", "[", "\"takeover\"", "]", "and", "not", "last_t_o", ":", "\n", "            ", "self", ".", "total_takeover_cost", "+=", "1", "\n", "", "step_infos", "[", "\"takeover_cost\"", "]", "=", "1", "if", "step_infos", "[", "\"takeover\"", "]", "else", "0", "\n", "step_infos", "[", "\"total_takeover_cost\"", "]", "=", "self", ".", "total_takeover_cost", "\n", "step_infos", "[", "\"native_cost\"", "]", "=", "step_infos", "[", "\"cost\"", "]", "\n", "step_infos", "[", "\"total_native_cost\"", "]", "=", "self", ".", "episode_cost", "\n", "return", "o", ",", "r", ",", "d", ",", "step_infos", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.human_in_the_loop_env.HumanInTheLoopEnv.step": [[47, 56], ["super().step", "super().render"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.step"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "        ", "ret", "=", "super", "(", "HumanInTheLoopEnv", ",", "self", ")", ".", "step", "(", "actions", ")", "\n", "if", "self", ".", "config", "[", "\"use_render\"", "]", ":", "\n", "            ", "super", "(", "HumanInTheLoopEnv", ",", "self", ")", ".", "render", "(", "text", "=", "{", "\n", "\"Total Cost\"", ":", "self", ".", "episode_cost", ",", "\n", "\"Total Takeover Cost\"", ":", "self", ".", "total_takeover_cost", ",", "\n", "\"Takeover\"", ":", "self", ".", "t_o", "\n", "}", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.DrivingCallbacks.on_episode_start": [[21, 30], ["None"], "methods", ["None"], ["    ", "def", "on_episode_start", "(", "\n", "self", ",", "*", ",", "worker", ":", "RolloutWorker", ",", "base_env", ":", "BaseEnv", ",", "policies", ":", "Dict", "[", "str", ",", "Policy", "]", ",", "episode", ":", "MultiAgentEpisode", ",", "\n", "env_index", ":", "int", ",", "**", "kwargs", "\n", ")", ":", "\n", "        ", "episode", ".", "user_data", "[", "\"velocity\"", "]", "=", "[", "]", "\n", "episode", ".", "user_data", "[", "\"steering\"", "]", "=", "[", "]", "\n", "episode", ".", "user_data", "[", "\"step_reward\"", "]", "=", "[", "]", "\n", "episode", ".", "user_data", "[", "\"acceleration\"", "]", "=", "[", "]", "\n", "episode", ".", "user_data", "[", "\"cost\"", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.DrivingCallbacks.on_episode_step": [[31, 41], ["episode.last_info_for", "episode.user_data[].append", "episode.user_data[].append", "episode.user_data[].append", "episode.user_data[].append", "episode.user_data[].append"], "methods", ["None"], ["", "def", "on_episode_step", "(", "\n", "self", ",", "*", ",", "worker", ":", "RolloutWorker", ",", "base_env", ":", "BaseEnv", ",", "episode", ":", "MultiAgentEpisode", ",", "env_index", ":", "int", ",", "**", "kwargs", "\n", ")", ":", "\n", "        ", "info", "=", "episode", ".", "last_info_for", "(", ")", "\n", "if", "info", "is", "not", "None", ":", "\n", "            ", "episode", ".", "user_data", "[", "\"velocity\"", "]", ".", "append", "(", "info", "[", "\"velocity\"", "]", ")", "\n", "episode", ".", "user_data", "[", "\"steering\"", "]", ".", "append", "(", "info", "[", "\"steering\"", "]", ")", "\n", "episode", ".", "user_data", "[", "\"step_reward\"", "]", ".", "append", "(", "info", "[", "\"step_reward\"", "]", ")", "\n", "episode", ".", "user_data", "[", "\"acceleration\"", "]", ".", "append", "(", "info", "[", "\"acceleration\"", "]", ")", "\n", "episode", ".", "user_data", "[", "\"cost\"", "]", ".", "append", "(", "info", "[", "\"cost\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.DrivingCallbacks.on_episode_end": [[42, 67], ["float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "episode.last_info_for", "episode.last_info_for", "episode.last_info_for", "numpy.max", "numpy.mean", "numpy.min", "numpy.max", "numpy.mean", "numpy.min", "numpy.min", "numpy.mean", "numpy.max", "numpy.max", "numpy.mean", "numpy.min", "sum"], "methods", ["None"], ["", "", "def", "on_episode_end", "(", "\n", "self", ",", "worker", ":", "RolloutWorker", ",", "base_env", ":", "BaseEnv", ",", "policies", ":", "Dict", "[", "str", ",", "Policy", "]", ",", "episode", ":", "MultiAgentEpisode", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "arrive_dest", "=", "episode", ".", "last_info_for", "(", ")", "[", "\"arrive_dest\"", "]", "\n", "crash", "=", "episode", ".", "last_info_for", "(", ")", "[", "\"crash\"", "]", "\n", "out_of_road", "=", "episode", ".", "last_info_for", "(", ")", "[", "\"out_of_road\"", "]", "\n", "max_step_rate", "=", "not", "(", "arrive_dest", "or", "crash", "or", "out_of_road", ")", "\n", "episode", ".", "custom_metrics", "[", "\"success_rate\"", "]", "=", "float", "(", "arrive_dest", ")", "\n", "episode", ".", "custom_metrics", "[", "\"crash_rate\"", "]", "=", "float", "(", "crash", ")", "\n", "episode", ".", "custom_metrics", "[", "\"out_of_road_rate\"", "]", "=", "float", "(", "out_of_road", ")", "\n", "episode", ".", "custom_metrics", "[", "\"max_step_rate\"", "]", "=", "float", "(", "max_step_rate", ")", "\n", "episode", ".", "custom_metrics", "[", "\"velocity_max\"", "]", "=", "float", "(", "np", ".", "max", "(", "episode", ".", "user_data", "[", "\"velocity\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"velocity_mean\"", "]", "=", "float", "(", "np", ".", "mean", "(", "episode", ".", "user_data", "[", "\"velocity\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"velocity_min\"", "]", "=", "float", "(", "np", ".", "min", "(", "episode", ".", "user_data", "[", "\"velocity\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"steering_max\"", "]", "=", "float", "(", "np", ".", "max", "(", "episode", ".", "user_data", "[", "\"steering\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"steering_mean\"", "]", "=", "float", "(", "np", ".", "mean", "(", "episode", ".", "user_data", "[", "\"steering\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"steering_min\"", "]", "=", "float", "(", "np", ".", "min", "(", "episode", ".", "user_data", "[", "\"steering\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"acceleration_min\"", "]", "=", "float", "(", "np", ".", "min", "(", "episode", ".", "user_data", "[", "\"acceleration\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"acceleration_mean\"", "]", "=", "float", "(", "np", ".", "mean", "(", "episode", ".", "user_data", "[", "\"acceleration\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"acceleration_max\"", "]", "=", "float", "(", "np", ".", "max", "(", "episode", ".", "user_data", "[", "\"acceleration\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"step_reward_max\"", "]", "=", "float", "(", "np", ".", "max", "(", "episode", ".", "user_data", "[", "\"step_reward\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"step_reward_mean\"", "]", "=", "float", "(", "np", ".", "mean", "(", "episode", ".", "user_data", "[", "\"step_reward\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"step_reward_min\"", "]", "=", "float", "(", "np", ".", "min", "(", "episode", ".", "user_data", "[", "\"step_reward\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"cost\"", "]", "=", "float", "(", "sum", "(", "episode", ".", "user_data", "[", "\"cost\"", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.DrivingCallbacks.on_train_result": [[68, 85], ["None"], "methods", ["None"], ["", "def", "on_train_result", "(", "self", ",", "*", ",", "trainer", ",", "result", ":", "dict", ",", "**", "kwargs", ")", ":", "\n", "        ", "result", "[", "\"success\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"crash\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"out\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"max_step\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"length\"", "]", "=", "result", "[", "\"episode_len_mean\"", "]", "\n", "result", "[", "\"cost\"", "]", "=", "np", ".", "nan", "\n", "if", "\"custom_metrics\"", "not", "in", "result", ":", "\n", "            ", "return", "\n", "\n", "", "if", "\"success_rate_mean\"", "in", "result", "[", "\"custom_metrics\"", "]", ":", "\n", "            ", "result", "[", "\"success\"", "]", "=", "result", "[", "\"custom_metrics\"", "]", "[", "\"success_rate_mean\"", "]", "\n", "result", "[", "\"crash\"", "]", "=", "result", "[", "\"custom_metrics\"", "]", "[", "\"crash_rate_mean\"", "]", "\n", "result", "[", "\"out\"", "]", "=", "result", "[", "\"custom_metrics\"", "]", "[", "\"out_of_road_rate_mean\"", "]", "\n", "result", "[", "\"max_step\"", "]", "=", "result", "[", "\"custom_metrics\"", "]", "[", "\"max_step_rate_mean\"", "]", "\n", "", "if", "\"cost_mean\"", "in", "result", "[", "\"custom_metrics\"", "]", ":", "\n", "            ", "result", "[", "\"cost\"", "]", "=", "result", "[", "\"custom_metrics\"", "]", "[", "\"cost_mean\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.EGPOCallbacks.on_episode_start": [[88, 106], ["None"], "methods", ["None"], ["    ", "def", "on_episode_start", "(", "\n", "self", ",", "*", ",", "worker", ":", "RolloutWorker", ",", "base_env", ":", "BaseEnv", ",", "policies", ":", "Dict", "[", "str", ",", "Policy", "]", ",", "episode", ":", "MultiAgentEpisode", ",", "\n", "env_index", ":", "int", ",", "**", "kwargs", "\n", ")", ":", "\n", "        ", "episode", ".", "user_data", "[", "\"velocity\"", "]", "=", "[", "]", "\n", "episode", ".", "user_data", "[", "\"steering\"", "]", "=", "[", "]", "\n", "episode", ".", "user_data", "[", "\"step_reward\"", "]", "=", "[", "]", "\n", "episode", ".", "user_data", "[", "\"acceleration\"", "]", "=", "[", "]", "\n", "episode", ".", "user_data", "[", "\"takeover\"", "]", "=", "0", "\n", "episode", ".", "user_data", "[", "\"raw_episode_reward\"", "]", "=", "0", "\n", "episode", ".", "user_data", "[", "\"episode_crash_rate\"", "]", "=", "0", "\n", "episode", ".", "user_data", "[", "\"episode_out_of_road_rate\"", "]", "=", "0", "\n", "episode", ".", "user_data", "[", "\"high_speed_rate\"", "]", "=", "0", "\n", "episode", ".", "user_data", "[", "\"total_takeover_cost\"", "]", "=", "0", "\n", "episode", ".", "user_data", "[", "\"total_native_cost\"", "]", "=", "0", "\n", "episode", ".", "user_data", "[", "\"cost\"", "]", "=", "0", "\n", "episode", ".", "user_data", "[", "\"episode_crash_vehicle\"", "]", "=", "0", "\n", "episode", ".", "user_data", "[", "\"episode_crash_object\"", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.EGPOCallbacks.on_episode_step": [[107, 127], ["episode.last_info_for", "episode.user_data[].append", "episode.user_data[].append", "episode.user_data[].append", "episode.user_data[].append"], "methods", ["None"], ["", "def", "on_episode_step", "(", "\n", "self", ",", "*", ",", "worker", ":", "RolloutWorker", ",", "base_env", ":", "BaseEnv", ",", "episode", ":", "MultiAgentEpisode", ",", "env_index", ":", "int", ",", "**", "kwargs", "\n", ")", ":", "\n", "        ", "info", "=", "episode", ".", "last_info_for", "(", ")", "\n", "if", "info", "is", "not", "None", ":", "\n", "            ", "episode", ".", "user_data", "[", "\"velocity\"", "]", ".", "append", "(", "info", "[", "\"velocity\"", "]", ")", "\n", "episode", ".", "user_data", "[", "\"steering\"", "]", ".", "append", "(", "info", "[", "\"steering\"", "]", ")", "\n", "episode", ".", "user_data", "[", "\"step_reward\"", "]", ".", "append", "(", "info", "[", "\"step_reward\"", "]", ")", "\n", "episode", ".", "user_data", "[", "\"acceleration\"", "]", ".", "append", "(", "info", "[", "\"acceleration\"", "]", ")", "\n", "episode", ".", "user_data", "[", "\"takeover\"", "]", "+=", "1", "if", "info", "[", "\"takeover\"", "]", "else", "0", "\n", "episode", ".", "user_data", "[", "\"raw_episode_reward\"", "]", "+=", "info", "[", "\"step_reward\"", "]", "\n", "episode", ".", "user_data", "[", "\"episode_crash_rate\"", "]", "+=", "1", "if", "info", "[", "\"crash\"", "]", "else", "0", "\n", "episode", ".", "user_data", "[", "\"episode_out_of_road_rate\"", "]", "+=", "1", "if", "info", "[", "\"out_of_road\"", "]", "else", "0", "\n", "# episode.user_data[\"high_speed_rate\"] += 1 if info[\"high_speed\"] else 0", "\n", "episode", ".", "user_data", "[", "\"total_takeover_cost\"", "]", "+=", "info", "[", "\"takeover_cost\"", "]", "\n", "episode", ".", "user_data", "[", "\"total_native_cost\"", "]", "+=", "info", "[", "\"native_cost\"", "]", "\n", "episode", ".", "user_data", "[", "\"cost\"", "]", "+=", "info", "[", "\"cost\"", "]", "if", "\"cost\"", "in", "info", "else", "info", "[", "\"native_cost\"", "]", "\n", "\n", "episode", ".", "user_data", "[", "\"episode_crash_vehicle\"", "]", "+=", "1", "if", "info", "[", "\"crash_vehicle\"", "]", "else", "0", "\n", "episode", ".", "user_data", "[", "\"episode_crash_object\"", "]", "+=", "1", "if", "info", "[", "\"crash_object\"", "]", "else", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.EGPOCallbacks.on_episode_end": [[128, 166], ["float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "int", "float", "float", "episode.last_info_for", "episode.last_info_for", "episode.last_info_for", "numpy.max", "numpy.mean", "numpy.min", "numpy.max", "numpy.mean", "numpy.min", "numpy.min", "numpy.mean", "numpy.max", "numpy.max", "numpy.mean", "numpy.min", "episode.last_info_for"], "methods", ["None"], ["", "", "def", "on_episode_end", "(", "\n", "self", ",", "worker", ":", "RolloutWorker", ",", "base_env", ":", "BaseEnv", ",", "policies", ":", "Dict", "[", "str", ",", "Policy", "]", ",", "episode", ":", "MultiAgentEpisode", ",", "\n", "**", "kwargs", ")", "->", "None", ":", "\n", "        ", "arrive_dest", "=", "episode", ".", "last_info_for", "(", ")", "[", "\"arrive_dest\"", "]", "\n", "crash", "=", "episode", ".", "last_info_for", "(", ")", "[", "\"crash\"", "]", "\n", "out_of_road", "=", "episode", ".", "last_info_for", "(", ")", "[", "\"out_of_road\"", "]", "\n", "max_step_rate", "=", "not", "(", "arrive_dest", "or", "crash", "or", "out_of_road", ")", "\n", "episode", ".", "custom_metrics", "[", "\"success_rate\"", "]", "=", "float", "(", "arrive_dest", ")", "\n", "episode", ".", "custom_metrics", "[", "\"crash_rate\"", "]", "=", "float", "(", "crash", ")", "\n", "episode", ".", "custom_metrics", "[", "\"out_of_road_rate\"", "]", "=", "float", "(", "out_of_road", ")", "\n", "episode", ".", "custom_metrics", "[", "\"max_step_rate\"", "]", "=", "float", "(", "max_step_rate", ")", "\n", "episode", ".", "custom_metrics", "[", "\"velocity_max\"", "]", "=", "float", "(", "np", ".", "max", "(", "episode", ".", "user_data", "[", "\"velocity\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"velocity_mean\"", "]", "=", "float", "(", "np", ".", "mean", "(", "episode", ".", "user_data", "[", "\"velocity\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"velocity_min\"", "]", "=", "float", "(", "np", ".", "min", "(", "episode", ".", "user_data", "[", "\"velocity\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"steering_max\"", "]", "=", "float", "(", "np", ".", "max", "(", "episode", ".", "user_data", "[", "\"steering\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"steering_mean\"", "]", "=", "float", "(", "np", ".", "mean", "(", "episode", ".", "user_data", "[", "\"steering\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"steering_min\"", "]", "=", "float", "(", "np", ".", "min", "(", "episode", ".", "user_data", "[", "\"steering\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"acceleration_min\"", "]", "=", "float", "(", "np", ".", "min", "(", "episode", ".", "user_data", "[", "\"acceleration\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"acceleration_mean\"", "]", "=", "float", "(", "np", ".", "mean", "(", "episode", ".", "user_data", "[", "\"acceleration\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"acceleration_max\"", "]", "=", "float", "(", "np", ".", "max", "(", "episode", ".", "user_data", "[", "\"acceleration\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"step_reward_max\"", "]", "=", "float", "(", "np", ".", "max", "(", "episode", ".", "user_data", "[", "\"step_reward\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"step_reward_mean\"", "]", "=", "float", "(", "np", ".", "mean", "(", "episode", ".", "user_data", "[", "\"step_reward\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"step_reward_min\"", "]", "=", "float", "(", "np", ".", "min", "(", "episode", ".", "user_data", "[", "\"step_reward\"", "]", ")", ")", "\n", "episode", ".", "custom_metrics", "[", "\"takeover_rate\"", "]", "=", "float", "(", "episode", ".", "user_data", "[", "\"takeover\"", "]", "/", "episode", ".", "length", ")", "\n", "episode", ".", "custom_metrics", "[", "\"takeover_count\"", "]", "=", "float", "(", "episode", ".", "user_data", "[", "\"takeover\"", "]", ")", "\n", "episode", ".", "custom_metrics", "[", "\"raw_episode_reward\"", "]", "=", "float", "(", "episode", ".", "user_data", "[", "\"raw_episode_reward\"", "]", ")", "\n", "episode", ".", "custom_metrics", "[", "\"episode_crash_num\"", "]", "=", "float", "(", "episode", ".", "user_data", "[", "\"episode_crash_rate\"", "]", ")", "\n", "episode", ".", "custom_metrics", "[", "\"episode_out_of_road_num\"", "]", "=", "float", "(", "episode", ".", "user_data", "[", "\"episode_out_of_road_rate\"", "]", ")", "\n", "episode", ".", "custom_metrics", "[", "\"high_speed_rate\"", "]", "=", "float", "(", "episode", ".", "user_data", "[", "\"high_speed_rate\"", "]", "/", "episode", ".", "length", ")", "\n", "\n", "episode", ".", "custom_metrics", "[", "\"total_takeover_cost\"", "]", "=", "float", "(", "episode", ".", "user_data", "[", "\"total_takeover_cost\"", "]", ")", "\n", "episode", ".", "custom_metrics", "[", "\"total_native_cost\"", "]", "=", "float", "(", "episode", ".", "user_data", "[", "\"total_native_cost\"", "]", ")", "\n", "\n", "episode", ".", "custom_metrics", "[", "\"cost\"", "]", "=", "float", "(", "episode", ".", "user_data", "[", "\"cost\"", "]", ")", "\n", "episode", ".", "custom_metrics", "[", "\"overtake_num\"", "]", "=", "int", "(", "episode", ".", "last_info_for", "(", ")", "[", "\"overtake_vehicle_num\"", "]", ")", "\n", "\n", "episode", ".", "custom_metrics", "[", "\"episode_crash_vehicle_num\"", "]", "=", "float", "(", "episode", ".", "user_data", "[", "\"episode_crash_vehicle\"", "]", ")", "\n", "episode", ".", "custom_metrics", "[", "\"episode_crash_object_num\"", "]", "=", "float", "(", "episode", ".", "user_data", "[", "\"episode_crash_object\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.EGPOCallbacks.on_train_result": [[167, 185], ["None"], "methods", ["None"], ["", "def", "on_train_result", "(", "self", ",", "*", ",", "trainer", ",", "result", ":", "dict", ",", "**", "kwargs", ")", ":", "\n", "        ", "result", "[", "\"success\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"crash\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"out\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"max_step\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"cost\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"length\"", "]", "=", "result", "[", "\"episode_len_mean\"", "]", "\n", "result", "[", "\"takeover\"", "]", "=", "np", ".", "nan", "\n", "if", "\"success_rate_mean\"", "in", "result", "[", "\"custom_metrics\"", "]", ":", "\n", "            ", "result", "[", "\"success\"", "]", "=", "result", "[", "\"custom_metrics\"", "]", "[", "\"success_rate_mean\"", "]", "\n", "result", "[", "\"crash\"", "]", "=", "result", "[", "\"custom_metrics\"", "]", "[", "\"crash_rate_mean\"", "]", "\n", "result", "[", "\"out\"", "]", "=", "result", "[", "\"custom_metrics\"", "]", "[", "\"out_of_road_rate_mean\"", "]", "\n", "result", "[", "\"max_step\"", "]", "=", "result", "[", "\"custom_metrics\"", "]", "[", "\"max_step_rate_mean\"", "]", "\n", "result", "[", "\"native_cost\"", "]", "=", "result", "[", "\"custom_metrics\"", "]", "[", "\"total_native_cost_mean\"", "]", "\n", "", "if", "\"cost_mean\"", "in", "result", "[", "\"custom_metrics\"", "]", ":", "\n", "            ", "result", "[", "\"cost\"", "]", "=", "result", "[", "\"custom_metrics\"", "]", "[", "\"cost_mean\"", "]", "\n", "", "if", "\"takeover_count_mean\"", "in", "result", "[", "\"custom_metrics\"", "]", ":", "\n", "            ", "result", "[", "\"takeover\"", "]", "=", "result", "[", "'custom_metrics'", "]", "[", "\"takeover_count_mean\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ILCallBack.on_train_result": [[198, 223], ["None"], "methods", ["None"], ["    ", "def", "on_train_result", "(", "self", ",", "*", ",", "trainer", ",", "result", ":", "dict", ",", "**", "kwargs", ")", ":", "\n", "        ", "result", "[", "\"success\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"crash\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"out\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"max_step\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"cost\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"length\"", "]", "=", "np", ".", "nan", "\n", "result", "[", "\"takeover\"", "]", "=", "np", ".", "nan", "\n", "if", "\"evaluation\"", "in", "result", ":", "\n", "            ", "eval", "=", "result", "[", "\"evaluation\"", "]", "\n", "if", "\"success_rate_mean\"", "in", "eval", "[", "\"custom_metrics\"", "]", ":", "\n", "                ", "result", "[", "\"success\"", "]", "=", "eval", "[", "\"custom_metrics\"", "]", "[", "\"success_rate_mean\"", "]", "\n", "result", "[", "\"crash\"", "]", "=", "eval", "[", "\"custom_metrics\"", "]", "[", "\"crash_rate_mean\"", "]", "\n", "result", "[", "\"out\"", "]", "=", "eval", "[", "\"custom_metrics\"", "]", "[", "\"out_of_road_rate_mean\"", "]", "\n", "result", "[", "\"max_step\"", "]", "=", "eval", "[", "\"custom_metrics\"", "]", "[", "\"max_step_rate_mean\"", "]", "\n", "result", "[", "\"native_cost\"", "]", "=", "eval", "[", "\"custom_metrics\"", "]", "[", "\"total_native_cost_mean\"", "]", "\n", "", "if", "\"cost_mean\"", "in", "eval", "[", "\"custom_metrics\"", "]", ":", "\n", "                ", "result", "[", "\"cost\"", "]", "=", "eval", "[", "\"custom_metrics\"", "]", "[", "\"cost_mean\"", "]", "\n", "", "if", "\"takeover_count_mean\"", "in", "eval", "[", "\"custom_metrics\"", "]", ":", "\n", "                ", "result", "[", "\"takeover\"", "]", "=", "eval", "[", "'custom_metrics'", "]", "[", "\"takeover_count_mean\"", "]", "\n", "", "if", "\"episode_reward_mean\"", "in", "eval", ":", "\n", "                ", "result", "[", "\"episode_reward\"", "]", "=", "eval", "[", "\"episode_reward_mean\"", "]", "\n", "result", "[", "\"episode_reward_mean\"", "]", "=", "eval", "[", "\"episode_reward_mean\"", "]", "\n", "result", "[", "\"reward\"", "]", "=", "eval", "[", "\"episode_reward_mean\"", "]", "\n", "result", "[", "\"length\"", "]", "=", "eval", "[", "\"episode_len_mean\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.CQLInputReader.__init__": [[264, 272], ["ray.rllib.offline.InputReader.__init__", "len", "numpy.random.shuffle", "open", "json.load"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load"], ["    ", "def", "__init__", "(", "self", ",", "data_set_path", "=", "None", ")", ":", "\n", "        ", "super", "(", "CQLInputReader", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "data_set_path", "is", "not", "None", "\n", "with", "open", "(", "data_set_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "self", ".", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "", "self", ".", "data_len", "=", "len", "(", "self", ".", "data", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "self", ".", "data", ")", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.CQLInputReader.next": [[273, 289], ["ray.rllib.SampleBatch", "numpy.random.shuffle"], "methods", ["None"], ["", "def", "next", "(", "self", ")", "->", "SampleBatch", ":", "\n", "        ", "if", "self", ".", "count", "==", "self", ".", "data_len", ":", "\n", "            ", "np", ".", "random", ".", "shuffle", "(", "self", ".", "data", ")", "\n", "self", ".", "count", "=", "0", "\n", "", "index", "=", "self", ".", "count", "\n", "dp", "=", "self", ".", "data", "[", "index", "]", "\n", "# o,a,d,r,i", "\n", "batch", "=", "SampleBatch", "(", "{", "SampleBatch", ".", "OBS", ":", "[", "dp", "[", "SampleBatch", ".", "OBS", "]", "]", ",", "\n", "SampleBatch", ".", "ACTIONS", ":", "[", "dp", "[", "SampleBatch", ".", "ACTIONS", "]", "]", ",", "\n", "SampleBatch", ".", "DONES", ":", "[", "dp", "[", "SampleBatch", ".", "DONES", "]", "]", ",", "\n", "SampleBatch", ".", "REWARDS", ":", "[", "dp", "[", "SampleBatch", ".", "REWARDS", "]", "]", ",", "\n", "SampleBatch", ".", "NEXT_OBS", ":", "[", "dp", "[", "SampleBatch", ".", "NEXT_OBS", "]", "]", ",", "\n", "# SampleBatch.INFOS: [dp[SampleBatch.INFOS]]", "\n", "}", ")", "\n", "self", ".", "count", "+=", "1", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.StateObservation.__init__": [[292, 294], ["metadrive.obs.observation_base.ObservationBase.__init__"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "StateObservation", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.StateObservation.observation_space": [[295, 300], ["gym.spaces.Box"], "methods", ["None"], ["", "@", "property", "\n", "def", "observation_space", "(", "self", ")", ":", "\n", "# Navi info + Other states", "\n", "        ", "shape", "=", "19", "\n", "return", "gym", ".", "spaces", ".", "Box", "(", "-", "0.0", ",", "1.0", ",", "shape", "=", "(", "shape", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.StateObservation.observe": [[301, 306], ["vehicle.navigation.get_navi_info", "common.StateObservation.vehicle_state", "numpy.concatenate", "numpy.concatenate.astype"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.StateObservation.vehicle_state"], ["", "def", "observe", "(", "self", ",", "vehicle", ")", ":", "\n", "        ", "navi_info", "=", "vehicle", ".", "navigation", ".", "get_navi_info", "(", ")", "\n", "ego_state", "=", "self", ".", "vehicle_state", "(", "vehicle", ")", "\n", "ret", "=", "np", ".", "concatenate", "(", "[", "ego_state", ",", "navi_info", "]", ")", "\n", "return", "ret", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.StateObservation.vehicle_state": [[307, 338], ["float", "numpy.arccos", "info.append", "vehicle.lane.local_coordinates", "info.append", "metadrive.utils.clip", "metadrive.utils.clip", "vehicle.heading_diff", "metadrive.utils.clip", "metadrive.utils.clip", "metadrive.utils.clip", "metadrive.utils.clip", "heading_dir_now.dot", "metadrive.utils.clip", "metadrive.utils.clip", "metadrive.utils.clip", "numpy.linalg.norm", "numpy.linalg.norm"], "methods", ["None"], ["", "def", "vehicle_state", "(", "self", ",", "vehicle", ")", ":", "\n", "# update out of road", "\n", "        ", "current_reference_lane", "=", "vehicle", ".", "navigation", ".", "current_ref_lanes", "[", "-", "1", "]", "\n", "lateral_to_left", ",", "lateral_to_right", "=", "vehicle", ".", "dist_to_left_side", ",", "vehicle", ".", "dist_to_right_side", "\n", "total_width", "=", "float", "(", "\n", "(", "vehicle", ".", "navigation", ".", "map", ".", "config", "[", "\"lane_num\"", "]", "+", "1", ")", "*", "vehicle", ".", "navigation", ".", "map", ".", "config", "[", "\"lane_width\"", "]", "\n", ")", "\n", "info", "=", "[", "\n", "clip", "(", "lateral_to_left", "/", "total_width", ",", "0.0", ",", "1.0", ")", ",", "\n", "clip", "(", "lateral_to_right", "/", "total_width", ",", "0.0", ",", "1.0", ")", ",", "\n", "vehicle", ".", "heading_diff", "(", "current_reference_lane", ")", ",", "\n", "# Note: speed can be negative denoting free fall. This happen when emergency brake.", "\n", "clip", "(", "(", "vehicle", ".", "speed", "+", "1", ")", "/", "(", "vehicle", ".", "max_speed", "+", "1", ")", ",", "0.0", ",", "1.0", ")", ",", "\n", "clip", "(", "(", "vehicle", ".", "steering", "/", "vehicle", ".", "max_steering", "+", "1", ")", "/", "2", ",", "0.0", ",", "1.0", ")", ",", "\n", "clip", "(", "(", "vehicle", ".", "last_current_action", "[", "0", "]", "[", "0", "]", "+", "1", ")", "/", "2", ",", "0.0", ",", "1.0", ")", ",", "\n", "clip", "(", "(", "vehicle", ".", "last_current_action", "[", "0", "]", "[", "1", "]", "+", "1", ")", "/", "2", ",", "0.0", ",", "1.0", ")", "\n", "]", "\n", "heading_dir_last", "=", "vehicle", ".", "last_heading_dir", "\n", "heading_dir_now", "=", "vehicle", ".", "heading", "\n", "cos_beta", "=", "heading_dir_now", ".", "dot", "(", "heading_dir_last", "\n", ")", "/", "(", "np", ".", "linalg", ".", "norm", "(", "heading_dir_now", ")", "*", "np", ".", "linalg", ".", "norm", "(", "heading_dir_last", ")", ")", "\n", "\n", "beta_diff", "=", "np", ".", "arccos", "(", "clip", "(", "cos_beta", ",", "0.0", ",", "1.0", ")", ")", "\n", "\n", "# print(beta)", "\n", "yaw_rate", "=", "beta_diff", "/", "0.1", "\n", "# print(yaw_rate)", "\n", "info", ".", "append", "(", "clip", "(", "yaw_rate", ",", "0.0", ",", "1.0", ")", ")", "\n", "_", ",", "lateral", "=", "vehicle", ".", "lane", ".", "local_coordinates", "(", "vehicle", ".", "position", ")", "\n", "info", ".", "append", "(", "clip", "(", "(", "lateral", "*", "2", "/", "vehicle", ".", "navigation", ".", "map", ".", "config", "[", "\"lane_width\"", "]", "+", "1.0", ")", "/", "2.0", ",", "0.0", ",", "1.0", ")", ")", "\n", "return", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.__init__": [[341, 346], ["common.StateObservation", "metadrive.obs.observation_base.ObservationBase.__init__"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vehicle_config", ")", ":", "\n", "        ", "self", ".", "state_obs", "=", "StateObservation", "(", "vehicle_config", ")", "\n", "super", "(", "ExpertObservation", ",", "self", ")", ".", "__init__", "(", "vehicle_config", ")", "\n", "self", ".", "cloud_points", "=", "None", "\n", "self", ".", "detected_objects", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.observation_space": [[347, 354], ["list", "gym.spaces.Box", "tuple"], "methods", ["None"], ["", "@", "property", "\n", "def", "observation_space", "(", "self", ")", ":", "\n", "        ", "shape", "=", "list", "(", "self", ".", "state_obs", ".", "observation_space", ".", "shape", ")", "\n", "if", "self", ".", "config", "[", "\"lidar\"", "]", "[", "\"num_lasers\"", "]", ">", "0", "and", "self", ".", "config", "[", "\"lidar\"", "]", "[", "\"distance\"", "]", ">", "0", ":", "\n", "# Number of lidar rays and distance should be positive!", "\n", "            ", "shape", "[", "0", "]", "+=", "self", ".", "config", "[", "\"lidar\"", "]", "[", "\"num_lasers\"", "]", "+", "self", ".", "config", "[", "\"lidar\"", "]", "[", "\"num_others\"", "]", "*", "4", "\n", "", "return", "gym", ".", "spaces", ".", "Box", "(", "-", "0.0", ",", "1.0", ",", "shape", "=", "tuple", "(", "shape", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.observe": [[355, 361], ["common.ExpertObservation.state_observe", "common.ExpertObservation.lidar_observe", "numpy.concatenate", "ret.astype", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.state_observe", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.lidar_observe"], ["", "def", "observe", "(", "self", ",", "vehicle", ")", ":", "\n", "        ", "state", "=", "self", ".", "state_observe", "(", "vehicle", ")", "\n", "other_v_info", "=", "self", ".", "lidar_observe", "(", "vehicle", ")", "\n", "self", ".", "current_observation", "=", "np", ".", "concatenate", "(", "(", "state", ",", "np", ".", "asarray", "(", "other_v_info", ")", ")", ")", "\n", "ret", "=", "self", ".", "current_observation", "\n", "return", "ret", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.state_observe": [[362, 364], ["common.ExpertObservation.state_obs.observe"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.observe"], ["", "def", "state_observe", "(", "self", ",", "vehicle", ")", ":", "\n", "        ", "return", "self", ".", "state_obs", ".", "observe", "(", "vehicle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.lidar_observe": [[365, 375], ["vehicle.lidar.perceive", "vehicle.lidar.get_surrounding_vehicles_info"], "methods", ["None"], ["", "def", "lidar_observe", "(", "self", ",", "vehicle", ")", ":", "\n", "        ", "other_v_info", "=", "[", "]", "\n", "if", "vehicle", ".", "lidar", ".", "available", ":", "\n", "            ", "cloud_points", ",", "detected_objects", "=", "vehicle", ".", "lidar", ".", "perceive", "(", "vehicle", ",", ")", "\n", "other_v_info", "+=", "vehicle", ".", "lidar", ".", "get_surrounding_vehicles_info", "(", "\n", "vehicle", ",", "detected_objects", ",", "4", ")", "\n", "other_v_info", "+=", "cloud_points", "\n", "self", ".", "cloud_points", "=", "cloud_points", "\n", "self", ".", "detected_objects", "=", "detected_objects", "\n", "", "return", "other_v_info", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.normpdf": [[225, 230], ["math.exp", "float", "float", "float"], "function", ["None"], ["", "", "", "", "def", "normpdf", "(", "x", ",", "mean", ",", "sd", ")", ":", "\n", "    ", "var", "=", "float", "(", "sd", ")", "**", "2", "\n", "denom", "=", "(", "2", "*", "math", ".", "pi", "*", "var", ")", "**", ".5", "\n", "num", "=", "math", ".", "exp", "(", "-", "(", "float", "(", "x", ")", "-", "float", "(", "mean", ")", ")", "**", "2", "/", "(", "2", "*", "var", ")", ")", "\n", "return", "num", "/", "denom", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.expert_action_prob": [[232, 246], ["obs.reshape.reshape", "numpy.tanh", "numpy.tanh", "x.reshape.reshape", "numpy.split", "numpy.exp", "common.normpdf", "common.normpdf", "numpy.matmul", "numpy.matmul", "numpy.matmul", "numpy.random.normal"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.normpdf", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.normpdf"], ["", "def", "expert_action_prob", "(", "action", ",", "obs", ",", "weights", ",", "deterministic", "=", "False", ")", ":", "\n", "    ", "obs", "=", "obs", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "x", "=", "np", ".", "matmul", "(", "obs", ",", "weights", "[", "\"default_policy/fc_1/kernel\"", "]", ")", "+", "weights", "[", "\"default_policy/fc_1/bias\"", "]", "\n", "x", "=", "np", ".", "tanh", "(", "x", ")", "\n", "x", "=", "np", ".", "matmul", "(", "x", ",", "weights", "[", "\"default_policy/fc_2/kernel\"", "]", ")", "+", "weights", "[", "\"default_policy/fc_2/bias\"", "]", "\n", "x", "=", "np", ".", "tanh", "(", "x", ")", "\n", "x", "=", "np", ".", "matmul", "(", "x", ",", "weights", "[", "\"default_policy/fc_out/kernel\"", "]", ")", "+", "weights", "[", "\"default_policy/fc_out/bias\"", "]", "\n", "x", "=", "x", ".", "reshape", "(", "-", "1", ")", "\n", "mean", ",", "log_std", "=", "np", ".", "split", "(", "x", ",", "2", ")", "\n", "std", "=", "np", ".", "exp", "(", "log_std", ")", "\n", "a_0_p", "=", "normpdf", "(", "action", "[", "0", "]", ",", "mean", "[", "0", "]", ",", "std", "[", "0", "]", ")", "\n", "a_1_p", "=", "normpdf", "(", "action", "[", "1", "]", ",", "mean", "[", "1", "]", ",", "std", "[", "1", "]", ")", "\n", "expert_action", "=", "np", ".", "random", ".", "normal", "(", "mean", ",", "std", ")", "if", "not", "deterministic", "else", "mean", "\n", "return", "expert_action", ",", "a_0_p", ",", "a_1_p", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.load_weights": [[248, 257], ["numpy.load"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load"], ["", "def", "load_weights", "(", "path", ":", "str", ")", ":", "\n", "    ", "\"\"\"\n    Load NN weights\n    :param path: weights file path path\n    :return: NN weights object\n    \"\"\"", "\n", "# try:", "\n", "model", "=", "np", ".", "load", "(", "path", ")", "\n", "return", "model", "\n", "# except FileNotFoundError:", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.get_expert_action": [[377, 385], ["isinstance", "env.expert_observation.observe", "common.expert_action_prob", "env.env_method"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.observe", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.expert_action_prob"], ["", "", "def", "get_expert_action", "(", "env", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "env", ",", "SubprocVecEnv", ")", ":", "\n", "        ", "obs", "=", "env", ".", "expert_observation", ".", "observe", "(", "env", ".", "vehicle", ")", "\n", "saver_a", ",", "a_0_p", ",", "a_1_p", "=", "expert_action_prob", "(", "[", "0", ",", "0", "]", ",", "obs", ",", "env", ".", "expert_weights", ",", "\n", "deterministic", "=", "False", ")", "\n", "return", "saver_a", "\n", "", "else", ":", "\n", "        ", "return", "env", ".", "env_method", "(", "\"get_expert_action\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.default_config": [[34, 72], ["super().default_config", "super().default_config.update", "dict", "dict", "os.join", "os.dirname"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.default_config"], ["    ", "def", "default_config", "(", "self", ")", "->", "Config", ":", "\n", "        ", "\"\"\"\n        Train/Test set both contain 10 maps\n        :return: PGConfig\n        \"\"\"", "\n", "config", "=", "super", "(", "ExpertGuidedEnv", ",", "self", ")", ".", "default_config", "(", ")", "\n", "config", ".", "update", "(", "dict", "(", "\n", "environment_num", "=", "100", ",", "\n", "start_seed", "=", "100", ",", "\n", "safe_rl_env_v2", "=", "False", ",", "# If True, then DO NOT done even out of the road!", "\n", "# _disable_detector_mask=True,  # default False to acc Lidar detection", "\n", "\n", "# traffic setting", "\n", "random_traffic", "=", "False", ",", "\n", "# traffic_density=0.1,", "\n", "\n", "# special setting", "\n", "rule_takeover", "=", "False", ",", "\n", "takeover_cost", "=", "1", ",", "\n", "cost_info", "=", "\"native\"", ",", "# or takeover", "\n", "random_spawn", "=", "False", ",", "# used to collect dataset", "\n", "cost_to_reward", "=", "True", ",", "# for egpo, it accesses the ENV reward by penalty", "\n", "horizon", "=", "1000", ",", "\n", "\n", "crash_vehicle_penalty", "=", "1.", ",", "\n", "crash_object_penalty", "=", "0.5", ",", "\n", "out_of_road_penalty", "=", "1.", ",", "\n", "\n", "vehicle_config", "=", "dict", "(", "# saver config, free_level:0 = expert", "\n", "use_saver", "=", "False", ",", "\n", "free_level", "=", "100", ",", "\n", "expert_deterministic", "=", "False", ",", "\n", "release_threshold", "=", "100", ",", "# the save will be released when level < this threshold", "\n", "overtake_stat", "=", "False", ")", ",", "# set to True only when evaluate", "\n", "\n", "expert_value_weights", "=", "osp", ".", "join", "(", "osp", ".", "dirname", "(", "__file__", ")", ",", "\"expert.npz\"", ")", "\n", ")", ",", "allow_add_new_key", "=", "True", ")", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.__init__": [[73, 90], ["config.get", "metadrive.envs.safe_metadrive_env.SafeMetaDriveEnv.__init__", "egpo_utils.common.ExpertObservation", "expert_guided_env.load_weights"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.load_weights"], ["", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "# if (\"safe_rl_env\" in config) and (not config[\"safe_rl_env\"]):", "\n", "#     raise ValueError(\"You should always set safe_rl_env to True!\")", "\n", "# config[\"safe_rl_env\"] = True", "\n", "        ", "if", "config", ".", "get", "(", "\"safe_rl_env_v2\"", ",", "False", ")", ":", "\n", "            ", "config", "[", "\"out_of_road_penalty\"", "]", "=", "0", "\n", "", "super", "(", "ExpertGuidedEnv", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "expert_observation", "=", "ExpertObservation", "(", "self", ".", "config", "[", "\"vehicle_config\"", "]", ")", "\n", "assert", "self", ".", "config", "[", "\"expert_value_weights\"", "]", "is", "not", "None", "\n", "self", ".", "total_takeover_cost", "=", "0", "\n", "self", ".", "total_native_cost", "=", "0", "\n", "self", ".", "state_value", "=", "0", "\n", "self", ".", "expert_weights", "=", "load_weights", "(", "self", ".", "config", "[", "\"expert_value_weights\"", "]", ")", "\n", "if", "self", ".", "config", "[", "\"cost_to_reward\"", "]", ":", "\n", "            ", "self", ".", "config", "[", "\"out_of_road_penalty\"", "]", "=", "self", ".", "config", "[", "\"out_of_road_cost\"", "]", "\n", "self", ".", "config", "[", "\"crash_vehicle_penalty\"", "]", "=", "self", ".", "config", "[", "\"crash_vehicle_cost\"", "]", "\n", "self", ".", "config", "[", "\"crash_object_penalty\"", "]", "=", "self", ".", "config", "[", "\"crash_object_cost\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.expert_observe": [[91, 93], ["expert_guided_env.ExpertGuidedEnv.expert_observation.observe"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.observe"], ["", "", "def", "expert_observe", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "expert_observation", ".", "observe", "(", "self", ".", "vehicle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.get_expert_action": [[94, 98], ["expert_guided_env.ExpertGuidedEnv.expert_observation.observe", "egpo_utils.common.expert_action_prob"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.observe", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.expert_action_prob"], ["", "def", "get_expert_action", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "expert_observation", ".", "observe", "(", "self", ".", "vehicle", ")", "\n", "return", "expert_action_prob", "(", "[", "0", ",", "0", "]", ",", "obs", ",", "self", ".", "expert_weights", ",", "\n", "deterministic", "=", "False", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv._get_reset_return": [[99, 107], ["super()._get_reset_return"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv._get_reset_return"], ["", "def", "_get_reset_return", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "num_agents", "==", "1", "\n", "self", ".", "total_takeover_cost", "=", "0", "\n", "self", ".", "total_native_cost", "=", "0", "\n", "if", "self", ".", "config", "[", "\"vehicle_config\"", "]", "[", "\"free_level\"", "]", "<", "1e-3", ":", "\n", "# 1.0 full takeover", "\n", "            ", "self", ".", "vehicle", ".", "takeover_start", "=", "True", "\n", "", "return", "super", "(", "ExpertGuidedEnv", ",", "self", ")", ".", "_get_reset_return", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.step": [[108, 114], ["expert_guided_env.ExpertGuidedEnv.expert_takeover", "super().step", "saver_info.update", "expert_guided_env.ExpertGuidedEnv.extra_step_info"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.expert_takeover", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.step", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.extra_step_info"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "        ", "actions", ",", "saver_info", "=", "self", ".", "expert_takeover", "(", "\"default_agent\"", ",", "actions", ")", "\n", "obs", ",", "r", ",", "d", ",", "info", ",", "=", "super", "(", "ExpertGuidedEnv", ",", "self", ")", ".", "step", "(", "actions", ")", "\n", "saver_info", ".", "update", "(", "info", ")", "\n", "info", "=", "self", ".", "extra_step_info", "(", "saver_info", ")", "\n", "return", "obs", ",", "r", ",", "d", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.extra_step_info": [[115, 137], ["None"], "methods", ["None"], ["", "def", "extra_step_info", "(", "self", ",", "step_info", ")", ":", "\n", "# step_info = step_infos[self.DEFAULT_AGENT]", "\n", "\n", "        ", "step_info", "[", "\"native_cost\"", "]", "=", "step_info", "[", "\"cost\"", "]", "\n", "# if step_info[\"out_of_road\"] and not step_info[\"arrive_dest\"]:", "\n", "# out of road will be done now", "\n", "step_info", "[", "\"high_speed\"", "]", "=", "True", "if", "self", ".", "vehicle", ".", "speed", ">=", "50", "else", "False", "\n", "step_info", "[", "\"takeover_cost\"", "]", "=", "self", ".", "config", "[", "\"takeover_cost\"", "]", "if", "step_info", "[", "\"takeover_start\"", "]", "else", "0", "\n", "self", ".", "total_takeover_cost", "+=", "step_info", "[", "\"takeover_cost\"", "]", "\n", "self", ".", "total_native_cost", "+=", "step_info", "[", "\"native_cost\"", "]", "\n", "step_info", "[", "\"total_takeover_cost\"", "]", "=", "self", ".", "total_takeover_cost", "\n", "step_info", "[", "\"total_native_cost\"", "]", "=", "self", ".", "total_native_cost", "\n", "\n", "if", "self", ".", "config", "[", "\"cost_info\"", "]", "==", "\"native\"", ":", "\n", "            ", "step_info", "[", "\"cost\"", "]", "=", "step_info", "[", "\"native_cost\"", "]", "\n", "step_info", "[", "\"total_cost\"", "]", "=", "self", ".", "total_native_cost", "\n", "", "elif", "self", ".", "config", "[", "\"cost_info\"", "]", "==", "\"takeover\"", ":", "\n", "            ", "step_info", "[", "\"cost\"", "]", "=", "step_info", "[", "\"takeover_cost\"", "]", "\n", "step_info", "[", "\"total_cost\"", "]", "=", "self", ".", "total_takeover_cost", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "", "return", "step_info", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.done_function": [[138, 146], ["super().done_function"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.done_function"], ["", "def", "done_function", "(", "self", ",", "v_id", ")", ":", "\n", "        ", "\"\"\"This function is a little bit different compared to the SafePGDriveEnv in PGDrive!\"\"\"", "\n", "done", ",", "done_info", "=", "super", "(", "ExpertGuidedEnv", ",", "self", ")", ".", "done_function", "(", "v_id", ")", "\n", "if", "self", ".", "config", "[", "\"safe_rl_env_v2\"", "]", ":", "\n", "            ", "assert", "self", ".", "config", "[", "\"out_of_road_cost\"", "]", ">", "0", "\n", "if", "done_info", "[", "\"out_of_road\"", "]", ":", "\n", "                ", "done", "=", "False", "\n", "", "", "return", "done", ",", "done_info", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv._is_out_of_road": [[147, 149], ["None"], "methods", ["None"], ["", "def", "_is_out_of_road", "(", "self", ",", "vehicle", ")", ":", "\n", "        ", "return", "vehicle", ".", "out_of_route", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.expert_takeover": [[150, 190], ["expert_guided_env.ExpertGuidedEnv.rule_takeover", "expert_guided_env.ExpertGuidedEnv.expert_observation.observe", "egpo_utils.common.expert_action_prob", "print"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.rule_takeover", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.observe", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.expert_action_prob"], ["", "def", "expert_takeover", "(", "self", ",", "v_id", ":", "str", ",", "actions", ")", ":", "\n", "        ", "\"\"\"\n        Action prob takeover\n        \"\"\"", "\n", "if", "self", ".", "config", "[", "\"rule_takeover\"", "]", ":", "\n", "            ", "return", "self", ".", "rule_takeover", "(", "v_id", ",", "actions", ")", "\n", "", "vehicle", "=", "self", ".", "vehicles", "[", "v_id", "]", "\n", "action", "=", "actions", "\n", "steering", "=", "action", "[", "0", "]", "\n", "throttle", "=", "action", "[", "1", "]", "\n", "self", ".", "state_value", "=", "0", "\n", "pre_save", "=", "vehicle", ".", "takeover", "\n", "if", "vehicle", ".", "config", "[", "\"use_saver\"", "]", "or", "vehicle", ".", "expert_takeover", ":", "\n", "# saver can be used for human or another AI", "\n", "            ", "free_level", "=", "vehicle", ".", "config", "[", "\"free_level\"", "]", "if", "not", "vehicle", ".", "expert_takeover", "else", "1.0", "\n", "obs", "=", "self", ".", "expert_observation", ".", "observe", "(", "vehicle", ")", "\n", "try", ":", "\n", "                ", "saver_a", ",", "a_0_p", ",", "a_1_p", "=", "expert_action_prob", "(", "action", ",", "obs", ",", "self", ".", "expert_weights", ",", "\n", "deterministic", "=", "vehicle", ".", "config", "[", "\"expert_deterministic\"", "]", ")", "\n", "", "except", "ValueError", ":", "\n", "                ", "print", "(", "\"Expert can not takeover, due to observation space mismathing!\"", ")", "\n", "saver_a", "=", "action", "\n", "", "else", ":", "\n", "                ", "if", "free_level", "<=", "1e-3", ":", "\n", "                    ", "steering", "=", "saver_a", "[", "0", "]", "\n", "throttle", "=", "saver_a", "[", "1", "]", "\n", "", "elif", "free_level", ">", "1e-3", ":", "\n", "                    ", "if", "a_0_p", "*", "a_1_p", "<", "1", "-", "vehicle", ".", "config", "[", "\"free_level\"", "]", ":", "\n", "                        ", "steering", ",", "throttle", "=", "saver_a", "[", "0", "]", ",", "saver_a", "[", "1", "]", "\n", "\n", "# indicate if current frame is takeover step", "\n", "", "", "", "", "vehicle", ".", "takeover", "=", "True", "if", "action", "[", "0", "]", "!=", "steering", "or", "action", "[", "1", "]", "!=", "throttle", "else", "False", "\n", "saver_info", "=", "{", "\n", "\"takeover_start\"", ":", "True", "if", "not", "pre_save", "and", "vehicle", ".", "takeover", "else", "False", ",", "\n", "\"takeover_end\"", ":", "True", "if", "pre_save", "and", "not", "vehicle", ".", "takeover", "else", "False", ",", "\n", "\"takeover\"", ":", "vehicle", ".", "takeover", "if", "pre_save", "else", "False", "\n", "}", "\n", "if", "saver_info", "[", "\"takeover\"", "]", ":", "\n", "            ", "saver_info", "[", "\"raw_action\"", "]", "=", "[", "steering", ",", "throttle", "]", "\n", "", "return", "(", "steering", ",", "throttle", ")", "if", "saver_info", "[", "\"takeover\"", "]", "else", "action", ",", "saver_info", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.rule_takeover": [[191, 245], ["expert_guided_env.ExpertGuidedEnv.observations[].observe", "egpo_utils.common.expert_action_prob", "print", "min", "vehicle.lidar.get_cloud_points", "int", "int", "vehicle.heading_diff", "min", "min", "min", "min", "min", "abs"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.ExpertObservation.observe", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.common.expert_action_prob"], ["", "def", "rule_takeover", "(", "self", ",", "v_id", ",", "actions", ")", ":", "\n", "        ", "vehicle", "=", "self", ".", "vehicles", "[", "v_id", "]", "\n", "action", "=", "actions", "[", "v_id", "]", "\n", "steering", "=", "action", "[", "0", "]", "\n", "throttle", "=", "action", "[", "1", "]", "\n", "if", "vehicle", ".", "config", "[", "\"use_saver\"", "]", "or", "vehicle", ".", "expert_takeover", ":", "\n", "# saver can be used for human or another AI", "\n", "            ", "save_level", "=", "vehicle", ".", "config", "[", "\"save_level\"", "]", "if", "not", "vehicle", ".", "expert_takeover", "else", "1.0", "\n", "obs", "=", "self", ".", "observations", "[", "v_id", "]", ".", "observe", "(", "vehicle", ")", "\n", "try", ":", "\n", "                ", "saver_a", ",", "a_0_p", ",", "a_1_p", "=", "expert_action_prob", "(", "action", ",", "obs", ",", "self", ".", "expert_weights", ",", "\n", "deterministic", "=", "vehicle", ".", "config", "[", "\"expert_deterministic\"", "]", ")", "\n", "", "except", "ValueError", ":", "\n", "                ", "print", "(", "\"Expert can not takeover, due to observation space mismathing!\"", ")", "\n", "", "else", ":", "\n", "                ", "if", "save_level", ">", "0.9", ":", "\n", "                    ", "steering", "=", "saver_a", "[", "0", "]", "\n", "throttle", "=", "saver_a", "[", "1", "]", "\n", "", "elif", "save_level", ">", "1e-3", ":", "\n", "                    ", "heading_diff", "=", "vehicle", ".", "heading_diff", "(", "vehicle", ".", "lane", ")", "-", "0.5", "\n", "f", "=", "min", "(", "1", "+", "abs", "(", "heading_diff", ")", "*", "vehicle", ".", "speed", "*", "vehicle", ".", "max_speed", ",", "save_level", "*", "10", ")", "\n", "# for out of road", "\n", "if", "(", "obs", "[", "0", "]", "<", "0.04", "*", "f", "and", "heading_diff", "<", "0", ")", "or", "(", "obs", "[", "1", "]", "<", "0.04", "*", "f", "and", "heading_diff", ">", "0", ")", "or", "obs", "[", "\n", "0", "]", "<=", "1e-3", "or", "obs", "[", "\n", "1", "]", "<=", "1e-3", ":", "\n", "                        ", "steering", "=", "saver_a", "[", "0", "]", "\n", "throttle", "=", "saver_a", "[", "1", "]", "\n", "if", "vehicle", ".", "speed", "<", "5", ":", "\n", "                            ", "throttle", "=", "0.5", "\n", "# if saver_a[1] * vehicle.speed < -40 and action[1] > 0:", "\n", "#     throttle = saver_a[1]", "\n", "\n", "# for collision", "\n", "", "", "lidar_p", "=", "vehicle", ".", "lidar", ".", "get_cloud_points", "(", ")", "\n", "left", "=", "int", "(", "vehicle", ".", "lidar", ".", "num_lasers", "/", "4", ")", "\n", "right", "=", "int", "(", "vehicle", ".", "lidar", ".", "num_lasers", "/", "4", "*", "3", ")", "\n", "if", "min", "(", "lidar_p", "[", "left", "-", "4", ":", "left", "+", "6", "]", ")", "<", "(", "save_level", "+", "0.1", ")", "/", "10", "or", "min", "(", "lidar_p", "[", "right", "-", "4", ":", "right", "+", "6", "]", "\n", ")", "<", "(", "save_level", "+", "0.1", ")", "/", "10", ":", "\n", "# lateral safe distance 2.0m", "\n", "                        ", "steering", "=", "saver_a", "[", "0", "]", "\n", "", "if", "action", "[", "1", "]", ">=", "0", "and", "saver_a", "[", "1", "]", "<=", "0", "and", "min", "(", "min", "(", "lidar_p", "[", "0", ":", "10", "]", ")", ",", "min", "(", "lidar_p", "[", "-", "10", ":", "]", ")", ")", "<", "save_level", ":", "\n", "# longitude safe distance 15 m", "\n", "                        ", "throttle", "=", "saver_a", "[", "1", "]", "\n", "\n", "# indicate if current frame is takeover step", "\n", "", "", "", "", "pre_save", "=", "vehicle", ".", "takeover", "\n", "vehicle", ".", "takeover", "=", "True", "if", "action", "[", "0", "]", "!=", "steering", "or", "action", "[", "1", "]", "!=", "throttle", "else", "False", "\n", "saver_info", "=", "{", "\n", "\"takeover_start\"", ":", "True", "if", "not", "pre_save", "and", "vehicle", ".", "takeover", "else", "False", ",", "\n", "\"takeover_end\"", ":", "True", "if", "pre_save", "and", "not", "vehicle", ".", "takeover", "else", "False", ",", "\n", "\"takeover\"", ":", "vehicle", ".", "takeover", "if", "pre_save", "else", "False", "\n", "}", "\n", "return", "(", "steering", ",", "throttle", ")", "if", "saver_info", "[", "\"takeover\"", "]", "else", "action", ",", "saver_info", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.normpdf": [[11, 16], ["math.exp", "float", "float", "float"], "function", ["None"], ["def", "normpdf", "(", "x", ",", "mean", ",", "sd", ")", ":", "\n", "    ", "var", "=", "float", "(", "sd", ")", "**", "2", "\n", "denom", "=", "(", "2", "*", "math", ".", "pi", "*", "var", ")", "**", ".5", "\n", "num", "=", "math", ".", "exp", "(", "-", "(", "float", "(", "x", ")", "-", "float", "(", "mean", ")", ")", "**", "2", "/", "(", "2", "*", "var", ")", ")", "\n", "return", "num", "/", "denom", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.load_weights": [[18, 27], ["numpy.load"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load"], ["", "def", "load_weights", "(", "path", ":", "str", ")", ":", "\n", "    ", "\"\"\"\n    Load NN weights\n    :param path: weights file path path\n    :return: NN weights object\n    \"\"\"", "\n", "# try:", "\n", "model", "=", "np", ".", "load", "(", "path", ")", "\n", "return", "model", "\n", "# except FileNotFoundError:", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.CentralizedCostAdvantage.__call__": [[246, 260], ["isinstance", "ray.rllib.policy.sample_batch.MultiAgentBatch.policy_batches.items", "ray.rllib.policy.sample_batch.MultiAgentBatch", "batch[].mean"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "samples", ")", ":", "\n", "        ", "wrapped", "=", "False", "\n", "if", "isinstance", "(", "samples", ",", "SampleBatch", ")", ":", "\n", "            ", "samples", "=", "MultiAgentBatch", "(", "{", "DEFAULT_POLICY_ID", ":", "samples", "}", ",", "samples", ".", "count", ")", "\n", "wrapped", "=", "True", "\n", "\n", "", "for", "policy_id", ",", "batch", "in", "samples", ".", "policy_batches", ".", "items", "(", ")", ":", "\n", "            ", "cost_adv_mean", "=", "batch", "[", "COST_ADVANTAGE", "]", ".", "mean", "(", ")", "\n", "batch", "[", "COST_ADVANTAGE", "]", "-=", "cost_adv_mean", "\n", "\n", "", "if", "wrapped", ":", "\n", "            ", "samples", "=", "samples", ".", "policy_batches", "[", "DEFAULT_POLICY_ID", "]", "\n", "\n", "", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.UpdatePenalty.__init__": [[263, 265], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "workers", ")", ":", "\n", "        ", "self", ".", "workers", "=", "workers", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.UpdatePenalty.__call__": [[266, 277], ["ppo_lag.UpdatePenalty.workers.local_worker().foreach_trainable_policy", "ray.rllib.execution.common._get_shared_metrics", "pi.update_penalty", "ppo_lag.UpdatePenalty.workers.local_worker"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.UpdatePenaltyMixin.update_penalty"], ["", "def", "__call__", "(", "self", ",", "batch", ")", ":", "\n", "        ", "def", "update", "(", "pi", ",", "pi_id", ")", ":", "\n", "            ", "res", "=", "pi", ".", "update_penalty", "(", "batch", ")", "\n", "return", "(", "pi_id", ",", "res", ")", "\n", "\n", "", "res", "=", "self", ".", "workers", ".", "local_worker", "(", ")", ".", "foreach_trainable_policy", "(", "update", ")", "\n", "\n", "metrics", "=", "_get_shared_metrics", "(", ")", "\n", "metrics", ".", "info", "[", "\"penalty_loss\"", "]", "=", "res", "[", "0", "]", "[", "1", "]", "\n", "\n", "return", "batch", "# , fetch", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.UpdatePenaltyMixin.__init__": [[366, 384], ["hasattr", "tf.placeholder", "tf.train.AdamOptimizer", "ppo_lag.UpdatePenaltyMixin._penalty_optimizer.minimize", "tf.control_dependencies", "tf.print", "tf.reduce_mean"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "if", "hasattr", "(", "self", ",", "\"_penalty_param\"", ")", "and", "self", ".", "config", "[", "\"worker_index\"", "]", "==", "0", ":", "\n", "            ", "ep_cost", "=", "tf", ".", "placeholder", "(", "\n", "tf", ".", "float32", ",", "\n", "shape", "=", "[", "None", ",", "]", ",", "\n", "name", "=", "\"ep_cost\"", ")", "\n", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "tf", ".", "print", "(", "\"Cost: \"", ",", "ep_cost", ",", "\" Param: \"", ",", "self", ".", "_penalty_param", ",", "self", ".", "_penalty", ")", "]", ")", ":", "\n", "                ", "penalty_loss", "=", "-", "self", ".", "_penalty_param", "*", "(", "tf", ".", "reduce_mean", "(", "ep_cost", ")", "-", "self", ".", "config", "[", "COST_LIMIT", "]", ")", "\n", "\n", "", "self", ".", "_ep_cost_ph", "=", "ep_cost", "\n", "self", ".", "_penalty_loss", "=", "penalty_loss", "\n", "\n", "self", ".", "_penalty_optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "config", "[", "PENALTY_LR", "]", ")", "\n", "self", ".", "_train_penalty_op", "=", "self", ".", "_penalty_optimizer", ".", "minimize", "(", "\n", "self", ".", "_penalty_loss", ",", "\n", "var_list", "=", "[", "self", ".", "_penalty_param", "]", ",", "\n", "name", "=", "\"penalty_loss\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.UpdatePenaltyMixin.update_penalty": [[386, 393], ["ppo_lag.UpdatePenaltyMixin._sess.run", "batch[].sum().reshape", "batch[].sum", "batch[].sum"], "methods", ["None"], ["", "", "def", "update_penalty", "(", "self", ",", "batch", ")", ":", "\n", "        ", "feed_dict", "=", "{", "\n", "self", ".", "_is_training", ":", "True", ",", "\n", "self", ".", "_ep_cost_ph", ":", "batch", "[", "COST", "]", ".", "sum", "(", ")", ".", "reshape", "(", "-", "1", ",", ")", "/", "batch", "[", "SampleBatch", ".", "DONES", "]", ".", "sum", "(", ")", ",", "\n", "}", "\n", "_", ",", "penalty_loss", "=", "self", ".", "_sess", ".", "run", "(", "[", "self", ".", "_train_penalty_op", ",", "self", ".", "_penalty_loss", "]", ",", "feed_dict", ")", "\n", "return", "penalty_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.compute_cost_advantages": [[52, 59], ["numpy.concatenate", "discount", "rollout[].copy().astype", "numpy.array", "rollout[].copy"], "function", ["None"], ["def", "compute_cost_advantages", "(", "rollout", ":", "SampleBatch", ",", "last_r", ":", "float", ",", "gamma", ":", "float", "=", "0.9", ",", "lambda_", ":", "float", "=", "1.0", ")", ":", "\n", "    ", "vpred_t", "=", "np", ".", "concatenate", "(", "[", "rollout", "[", "COST_VALUES", "]", ",", "np", ".", "array", "(", "[", "last_r", "]", ")", "]", ")", "\n", "delta_t", "=", "(", "rollout", "[", "COST", "]", "+", "gamma", "*", "vpred_t", "[", "1", ":", "]", "-", "vpred_t", "[", ":", "-", "1", "]", ")", "\n", "rollout", "[", "COST_ADVANTAGE", "]", "=", "discount", "(", "delta_t", ",", "gamma", "*", "lambda_", ")", "\n", "rollout", "[", "COST_TARGET", "]", "=", "(", "rollout", "[", "COST_ADVANTAGE", "]", "+", "rollout", "[", "COST_VALUES", "]", ")", ".", "copy", "(", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "rollout", "[", "COST_ADVANTAGE", "]", "=", "rollout", "[", "COST_ADVANTAGE", "]", ".", "copy", "(", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "return", "rollout", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.postprocess_ppo_cost": [[61, 83], ["ppo_lag.compute_cost_advantages", "range", "policy._cost_value", "policy.num_state_tensors", "next_state.append"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.compute_cost_advantages"], ["", "def", "postprocess_ppo_cost", "(", "policy", ":", "Policy", ",", "sample_batch", ":", "SampleBatch", ")", "->", "SampleBatch", ":", "\n", "# Trajectory is actually complete -> last r=0.0.", "\n", "    ", "if", "sample_batch", "[", "SampleBatch", ".", "DONES", "]", "[", "-", "1", "]", ":", "\n", "        ", "last_r", "=", "0.0", "\n", "# Trajectory has been truncated -> last r=VF estimate of last obs.", "\n", "", "else", ":", "\n", "        ", "next_state", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "policy", ".", "num_state_tensors", "(", ")", ")", ":", "\n", "            ", "next_state", ".", "append", "(", "sample_batch", "[", "\"state_out_{}\"", ".", "format", "(", "i", ")", "]", "[", "-", "1", "]", ")", "\n", "", "last_r", "=", "policy", ".", "_cost_value", "(", "\n", "sample_batch", "[", "SampleBatch", ".", "NEXT_OBS", "]", "[", "-", "1", "]", ",", "sample_batch", "[", "SampleBatch", ".", "ACTIONS", "]", "[", "-", "1", "]", ",", "sample_batch", "[", "COST", "]", "[", "-", "1", "]", ",", "\n", "*", "next_state", "\n", ")", "\n", "\n", "# Adds the policy logits, VF preds, and advantages to the batch,", "\n", "# using GAE (\"generalized advantage estimation\") or not.", "\n", "", "batch", "=", "compute_cost_advantages", "(", "\n", "sample_batch", ",", "\n", "last_r", ",", "\n", "policy", ".", "config", "[", "\"gamma\"", "]", ",", "\n", "policy", ".", "config", "[", "\"lambda\"", "]", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.post_process_fn": [[85, 98], ["ray.rllib.agents.ppo.ppo_tf_policy.postprocess_ppo_gae.get", "ppo_lag.postprocess_ppo_cost", "ray.rllib.agents.ppo.ppo_tf_policy.postprocess_ppo_gae", "numpy.array", "numpy.zeros_like"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.postprocess_ppo_cost"], ["", "def", "post_process_fn", "(", "policy", ":", "Policy", ",", "\n", "sample_batch", ":", "SampleBatch", ",", "\n", "other_agent_batches", ":", "Optional", "[", "Dict", "[", "AgentID", ",", "SampleBatch", "]", "]", "=", "None", ",", "\n", "episode", ":", "Optional", "[", "MultiAgentEpisode", "]", "=", "None", ")", "->", "SampleBatch", ":", "\n", "# Put the actions to batch", "\n", "    ", "infos", "=", "sample_batch", ".", "get", "(", "SampleBatch", ".", "INFOS", ")", "\n", "if", "infos", "is", "not", "None", ":", "\n", "        ", "sample_batch", "[", "COST", "]", "=", "np", ".", "array", "(", "[", "info", "[", "\"cost\"", "]", "for", "info", "in", "infos", "]", ")", "\n", "", "else", ":", "# Fill the elements if not initialized", "\n", "        ", "sample_batch", "[", "COST", "]", "=", "np", ".", "zeros_like", "(", "sample_batch", "[", "SampleBatch", ".", "REWARDS", "]", ")", "\n", "", "sample_batch", "=", "postprocess_ppo_cost", "(", "policy", ",", "sample_batch", ")", "\n", "sample_batch", "=", "postprocess_ppo_gae", "(", "policy", ",", "sample_batch", ",", "other_agent_batches", ",", "episode", ")", "\n", "return", "sample_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.ppo_lag_surrogate_loss": [[100, 216], ["tf.nn.softplus", "model.from_batch", "dist_class", "dist_class", "tf.exp", "dist_class.kl", "ppo_lag.ppo_lag_surrogate_loss.reduce_mean_valid"], "function", ["None"], ["", "def", "ppo_lag_surrogate_loss", "(", "\n", "policy", ":", "Policy", ",", "model", ":", "ModelV2", ",", "dist_class", ":", "Type", "[", "TFActionDistribution", "]", ",", "\n", "train_batch", ":", "SampleBatch", ")", "->", "Union", "[", "TensorType", ",", "List", "[", "TensorType", "]", "]", ":", "\n", "    ", "\"\"\"Constructs the loss for Proximal Policy Objective.\n\n    Args:\n        policy (Policy): The Policy to calculate the loss for.\n        model (ModelV2): The Model to calculate the loss for.\n        dist_class (Type[ActionDistribution]: The action distr. class.\n        train_batch (SampleBatch): The training data.\n\n    Returns:\n        Union[TensorType, List[TensorType]]: A single loss tensor or a list\n            of loss tensors.\n    \"\"\"", "\n", "# Setup the lambda multiplier.", "\n", "# _ = train_batch[COST]  # touch", "\n", "# _ = train_batch[SampleBatch.DONES]  # touch", "\n", "# _ = train_batch[SampleBatch.NEXT_OBS]  # touch", "\n", "# _ = train_batch[COST_ADVANTAGE]  # touch", "\n", "# _ = train_batch[COST_TARGET]  # touch", "\n", "penalty_init", "=", "1.0", "\n", "# penalty_init = 0.1", "\n", "with", "tf", ".", "variable_scope", "(", "'penalty'", ")", ":", "\n", "        ", "param_init", "=", "np", ".", "log", "(", "max", "(", "np", ".", "exp", "(", "penalty_init", ")", "-", "1", ",", "1e-8", ")", ")", "\n", "penalty_param", "=", "tf", ".", "get_variable", "(", "\n", "'penalty_param'", ",", "\n", "initializer", "=", "float", "(", "param_init", ")", ",", "\n", "trainable", "=", "True", ",", "\n", "dtype", "=", "tf", ".", "float32", "\n", ")", "\n", "", "penalty", "=", "tf", ".", "nn", ".", "softplus", "(", "penalty_param", ")", "\n", "policy", ".", "_penalty", "=", "penalty", "\n", "policy", ".", "_penalty_param", "=", "penalty_param", "\n", "\n", "logits", ",", "state", "=", "model", ".", "from_batch", "(", "train_batch", ")", "\n", "curr_action_dist", "=", "dist_class", "(", "logits", ",", "model", ")", "\n", "\n", "# RNN case: Mask away 0-padded chunks at end of time axis.", "\n", "if", "state", ":", "\n", "        ", "max_seq_len", "=", "tf", ".", "reduce_max", "(", "train_batch", "[", "\"seq_lens\"", "]", ")", "\n", "mask", "=", "tf", ".", "sequence_mask", "(", "train_batch", "[", "\"seq_lens\"", "]", ",", "max_seq_len", ")", "\n", "mask", "=", "tf", ".", "reshape", "(", "mask", ",", "[", "-", "1", "]", ")", "\n", "\n", "def", "reduce_mean_valid", "(", "t", ")", ":", "\n", "            ", "return", "tf", ".", "reduce_mean", "(", "tf", ".", "boolean_mask", "(", "t", ",", "mask", ")", ")", "\n", "\n", "# non-RNN case: No masking.", "\n", "", "", "else", ":", "\n", "        ", "mask", "=", "None", "\n", "reduce_mean_valid", "=", "tf", ".", "reduce_mean", "\n", "\n", "", "prev_action_dist", "=", "dist_class", "(", "train_batch", "[", "SampleBatch", ".", "ACTION_DIST_INPUTS", "]", ",", "model", ")", "\n", "logp_ratio", "=", "tf", ".", "exp", "(", "\n", "curr_action_dist", ".", "logp", "(", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ")", "-", "\n", "train_batch", "[", "SampleBatch", ".", "ACTION_LOGP", "]", ")", "\n", "action_kl", "=", "prev_action_dist", ".", "kl", "(", "curr_action_dist", ")", "\n", "mean_kl", "=", "reduce_mean_valid", "(", "action_kl", ")", "\n", "\n", "curr_entropy", "=", "curr_action_dist", ".", "entropy", "(", ")", "\n", "mean_entropy", "=", "reduce_mean_valid", "(", "curr_entropy", ")", "\n", "\n", "surrogate_loss", "=", "tf", ".", "minimum", "(", "\n", "train_batch", "[", "Postprocessing", ".", "ADVANTAGES", "]", "*", "logp_ratio", ",", "\n", "train_batch", "[", "Postprocessing", ".", "ADVANTAGES", "]", "*", "tf", ".", "clip_by_value", "(", "\n", "logp_ratio", ",", "1", "-", "policy", ".", "config", "[", "\"clip_param\"", "]", ",", "\n", "1", "+", "policy", ".", "config", "[", "\"clip_param\"", "]", ")", ")", "\n", "\n", "cost_adv", "=", "train_batch", "[", "COST_ADVANTAGE", "]", "\n", "surrogate_cost", "=", "cost_adv", "*", "tf", ".", "clip_by_value", "(", "logp_ratio", ",", "0.", ",", "1", "+", "policy", ".", "config", "[", "\"clip_param\"", "]", ")", "\n", "\n", "mean_policy_loss", "=", "reduce_mean_valid", "(", "-", "surrogate_loss", ")", "\n", "\n", "mean_cost_loss", "=", "reduce_mean_valid", "(", "surrogate_cost", ")", "\n", "\n", "cost_value_loss", "=", "tf", ".", "math", ".", "square", "(", "model", ".", "get_cost_value", "(", ")", "-", "train_batch", "[", "COST_TARGET", "]", ")", "\n", "\n", "if", "policy", ".", "config", "[", "\"use_gae\"", "]", ":", "\n", "        ", "prev_value_fn_out", "=", "train_batch", "[", "SampleBatch", ".", "VF_PREDS", "]", "\n", "value_fn_out", "=", "model", ".", "value_function", "(", ")", "\n", "vf_loss1", "=", "tf", ".", "math", ".", "square", "(", "value_fn_out", "-", "train_batch", "[", "Postprocessing", ".", "VALUE_TARGETS", "]", ")", "\n", "vf_clipped", "=", "prev_value_fn_out", "+", "tf", ".", "clip_by_value", "(", "\n", "value_fn_out", "-", "prev_value_fn_out", ",", "-", "policy", ".", "config", "[", "\"vf_clip_param\"", "]", ",", "\n", "policy", ".", "config", "[", "\"vf_clip_param\"", "]", ")", "\n", "vf_loss2", "=", "tf", ".", "math", ".", "square", "(", "vf_clipped", "-", "\n", "train_batch", "[", "Postprocessing", ".", "VALUE_TARGETS", "]", ")", "\n", "vf_loss", "=", "tf", ".", "maximum", "(", "vf_loss1", ",", "vf_loss2", ")", "\n", "mean_vf_loss", "=", "reduce_mean_valid", "(", "vf_loss", ")", "\n", "total_loss", "=", "reduce_mean_valid", "(", "\n", "-", "surrogate_loss", "+", "policy", ".", "kl_coeff", "*", "action_kl", "+", "\n", "policy", ".", "config", "[", "\"vf_loss_coeff\"", "]", "*", "vf_loss", "-", "\n", "policy", ".", "entropy_coeff", "*", "curr_entropy", "\n", ")", "+", "penalty", "*", "mean_cost_loss", "\n", "total_loss", "=", "total_loss", "/", "(", "1", "+", "penalty", ")", "\n", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", ")", "\n", "mean_vf_loss", "=", "tf", ".", "constant", "(", "0.0", ")", "\n", "total_loss", "=", "reduce_mean_valid", "(", "-", "surrogate_loss", "+", "\n", "policy", ".", "kl_coeff", "*", "action_kl", "-", "\n", "policy", ".", "entropy_coeff", "*", "curr_entropy", ")", "\n", "\n", "", "policy", ".", "_mean_cost_loss", "=", "mean_cost_loss", "\n", "policy", ".", "_mean_cost_value_loss", "=", "reduce_mean_valid", "(", "cost_value_loss", ")", "\n", "\n", "# total_loss += penalty_loss  # Do not add it to total loss!", "\n", "total_loss", "+=", "policy", ".", "_mean_cost_value_loss", "\n", "\n", "# Store stats in policy for stats_fn.", "\n", "policy", ".", "_total_loss", "=", "total_loss", "\n", "policy", ".", "_mean_policy_loss", "=", "mean_policy_loss", "\n", "policy", ".", "_mean_vf_loss", "=", "mean_vf_loss", "\n", "policy", ".", "_mean_entropy", "=", "mean_entropy", "\n", "policy", ".", "_mean_kl", "=", "mean_kl", "\n", "\n", "return", "total_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.new_stats": [[218, 225], ["ray.rllib.agents.ppo.ppo_tf_policy.kl_and_loss_stats"], "function", ["None"], ["", "def", "new_stats", "(", "policy", ",", "batch", ")", ":", "\n", "    ", "ret", "=", "kl_and_loss_stats", "(", "policy", ",", "batch", ")", "\n", "ret", "[", "\"penalty\"", "]", "=", "policy", ".", "_penalty", "\n", "ret", "[", "\"penalty_param\"", "]", "=", "policy", ".", "_penalty_param", "\n", "ret", "[", "\"cost_loss\"", "]", "=", "policy", ".", "_mean_cost_loss", "\n", "ret", "[", "\"cost_value_loss\"", "]", "=", "policy", ".", "_mean_cost_value_loss", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.gradient_fn": [[227, 241], ["policy.model.trainable_variables", "optimizer.compute_gradients", "len", "tf.clip_by_global_norm", "list", "zip"], "function", ["None"], ["", "def", "gradient_fn", "(", "policy", ",", "optimizer", ",", "loss", ")", ":", "\n", "    ", "variables", "=", "policy", ".", "model", ".", "trainable_variables", "(", ")", "\n", "\n", "assert", "len", "(", "[", "v", "for", "v", "in", "variables", "if", "\"penalty_param\"", "in", "v", ".", "name", "]", ")", "==", "0", "\n", "grads_and_vars", "=", "optimizer", ".", "compute_gradients", "(", "loss", ",", "variables", ")", "\n", "\n", "# Clip by global norm, if necessary.", "\n", "if", "policy", ".", "config", "[", "\"grad_clip\"", "]", "is", "not", "None", ":", "\n", "        ", "grads", "=", "[", "g", "for", "(", "g", ",", "v", ")", "in", "grads_and_vars", "]", "\n", "policy", ".", "grads", ",", "_", "=", "tf", ".", "clip_by_global_norm", "(", "grads", ",", "policy", ".", "config", "[", "\"grad_clip\"", "]", ")", "\n", "clipped_grads_and_vars", "=", "list", "(", "zip", "(", "policy", ".", "grads", ",", "variables", ")", ")", "\n", "return", "clipped_grads_and_vars", "\n", "", "else", ":", "\n", "        ", "return", "grads_and_vars", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.execution_plan": [[279, 333], ["ray.rllib.execution.rollout_ops.ParallelRollouts", "rollouts.for_each.for_each", "rollouts.for_each.combine", "rollouts.for_each.for_each", "rollouts.for_each.for_each", "rollouts.for_each.for_each().for_each", "ray.rllib.execution.metric_ops.StandardMetricsReporting().for_each", "ray.rllib.execution.rollout_ops.SelectExperiences", "ray.rllib.execution.rollout_ops.ConcatBatches", "ray.rllib.execution.rollout_ops.StandardizeFields", "ppo_lag.UpdatePenalty", "rollouts.for_each.for_each", "rollouts.for_each.for_each", "ray.rllib.agents.ppo.ppo.UpdateKL", "workers.trainable_policies", "ray.rllib.execution.train_ops.TrainOneStep", "ray.rllib.execution.train_ops.TrainTFMultiGPU", "rollouts.for_each.for_each", "ray.rllib.execution.metric_ops.StandardMetricsReporting", "ray.rllib.agents.ppo.ppo.warn_about_bad_reward_scales", "config.get"], "function", ["None"], ["", "", "def", "execution_plan", "(", "workers", ":", "WorkerSet", ",", "config", ":", "TrainerConfigDict", ")", "->", "LocalIterator", "[", "dict", "]", ":", "\n", "    ", "\"\"\"Execution plan of the PPO algorithm. Defines the distributed dataflow.\n\n    Args:\n        workers (WorkerSet): The WorkerSet for training the Polic(y/ies)\n            of the Trainer.\n        config (TrainerConfigDict): The trainer's configuration dict.\n\n    Returns:\n        LocalIterator[dict]: The Policy class to use with PPOTrainer.\n            If None, use `default_policy` provided in build_trainer().\n    \"\"\"", "\n", "rollouts", "=", "ParallelRollouts", "(", "workers", ",", "mode", "=", "\"bulk_sync\"", ")", "\n", "\n", "# Collect batches for the trainable policies.", "\n", "rollouts", "=", "rollouts", ".", "for_each", "(", "\n", "SelectExperiences", "(", "workers", ".", "trainable_policies", "(", ")", ")", ")", "\n", "# Concatenate the SampleBatches into one.", "\n", "rollouts", "=", "rollouts", ".", "combine", "(", "\n", "ConcatBatches", "(", "min_batch_size", "=", "config", "[", "\"train_batch_size\"", "]", ")", ")", "\n", "# Standardize advantages.", "\n", "# <<<<< We add the cost advantage to normalization too! >>>>>", "\n", "rollouts", "=", "rollouts", ".", "for_each", "(", "StandardizeFields", "(", "[", "\"advantages\"", ",", "COST_ADVANTAGE", "]", ")", ")", "\n", "\n", "# Update penalty", "\n", "rollouts", "=", "rollouts", ".", "for_each", "(", "UpdatePenalty", "(", "workers", ")", ")", "\n", "\n", "# Perform one training step on the combined + standardized batch.", "\n", "if", "config", "[", "\"simple_optimizer\"", "]", ":", "\n", "        ", "train_op", "=", "rollouts", ".", "for_each", "(", "\n", "TrainOneStep", "(", "\n", "workers", ",", "\n", "num_sgd_iter", "=", "config", "[", "\"num_sgd_iter\"", "]", ",", "\n", "sgd_minibatch_size", "=", "config", "[", "\"sgd_minibatch_size\"", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "train_op", "=", "rollouts", ".", "for_each", "(", "\n", "TrainTFMultiGPU", "(", "\n", "workers", ",", "\n", "sgd_minibatch_size", "=", "config", "[", "\"sgd_minibatch_size\"", "]", ",", "\n", "num_sgd_iter", "=", "config", "[", "\"num_sgd_iter\"", "]", ",", "\n", "num_gpus", "=", "config", "[", "\"num_gpus\"", "]", ",", "\n", "rollout_fragment_length", "=", "config", "[", "\"rollout_fragment_length\"", "]", ",", "\n", "num_envs_per_worker", "=", "config", "[", "\"num_envs_per_worker\"", "]", ",", "\n", "train_batch_size", "=", "config", "[", "\"train_batch_size\"", "]", ",", "\n", "shuffle_sequences", "=", "config", "[", "\"shuffle_sequences\"", "]", ",", "\n", "_fake_gpus", "=", "config", "[", "\"_fake_gpus\"", "]", ",", "\n", "framework", "=", "config", ".", "get", "(", "\"framework\"", ")", ")", ")", "\n", "\n", "# Update KL after each round of training.", "\n", "", "train_op", "=", "train_op", ".", "for_each", "(", "lambda", "t", ":", "t", "[", "1", "]", ")", ".", "for_each", "(", "UpdateKL", "(", "workers", ")", ")", "\n", "\n", "# Warn about bad reward scales and return training metrics.", "\n", "return", "StandardMetricsReporting", "(", "train_op", ",", "workers", ",", "config", ")", ".", "for_each", "(", "lambda", "result", ":", "warn_about_bad_reward_scales", "(", "config", ",", "result", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.make_model": [[335, 344], ["ray.rllib.models.ModelCatalog.get_action_dist", "ray.rllib.models.ModelCatalog.get_model_v2"], "function", ["None"], ["", "def", "make_model", "(", "policy", ",", "obs_space", ",", "action_space", ",", "config", ")", ":", "\n", "    ", "dist_class", ",", "logit_dim", "=", "ModelCatalog", ".", "get_action_dist", "(", "action_space", ",", "config", "[", "\"model\"", "]", ")", "\n", "return", "ModelCatalog", ".", "get_model_v2", "(", "\n", "obs_space", "=", "obs_space", ",", "\n", "action_space", "=", "action_space", ",", "\n", "num_outputs", "=", "logit_dim", ",", "\n", "model_config", "=", "config", "[", "\"model\"", "]", ",", "\n", "framework", "=", "\"tf\"", ",", "\n", "model_interface", "=", "CostValueNetwork", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.setup_mixins_ppo_lag": [[347, 350], ["ray.rllib.agents.ppo.ppo_tf_policy.setup_mixins", "egpo_utils.ppo_lag.ppo_lag_model.CostValueNetworkMixin.__init__"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["", "def", "setup_mixins_ppo_lag", "(", "policy", ",", "obs_space", ",", "action_space", ",", "config", ")", ":", "\n", "    ", "setup_mixins", "(", "policy", ",", "obs_space", ",", "action_space", ",", "config", ")", "\n", "CostValueNetworkMixin", ".", "__init__", "(", "policy", ",", "obs_space", ",", "action_space", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.vf_preds_fetches": [[352, 356], ["policy.model.value_function", "policy.model.get_cost_value"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag_model.CostValueNetwork.get_cost_value"], ["", "def", "vf_preds_fetches", "(", "policy", ":", "Policy", ")", "->", "Dict", "[", "str", ",", "TensorType", "]", ":", "\n", "    ", "return", "{", "\n", "SampleBatch", ".", "VF_PREDS", ":", "policy", ".", "model", ".", "value_function", "(", ")", ",", "\n", "COST_VALUES", ":", "policy", ".", "model", ".", "get_cost_value", "(", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.validate_config": [[359, 362], ["ray.rllib.agents.ppo.ppo.validate_config"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql.validate_config"], ["", "def", "validate_config", "(", "config", ")", ":", "\n", "    ", "original_validate_config", "(", "config", ")", "\n", "assert", "config", "[", "\"batch_mode\"", "]", "==", "\"complete_episodes\"", ",", "\"We need to compute episode cost!\"", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag.after_init": [[395, 397], ["ppo_lag.UpdatePenaltyMixin.__init__"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["", "", "def", "after_init", "(", "policy", ",", "obs_space", ",", "action_space", ",", "config", ")", ":", "\n", "    ", "UpdatePenaltyMixin", ".", "__init__", "(", "policy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag_model.CostValueNetworkMixin.__init__": [[13, 34], ["config.get", "ray.rllib.utils.tf_ops.make_tf_callable", "ray.rllib.utils.tf_ops.make_tf_callable", "ppo_lag_model.CostValueNetworkMixin.model", "ppo_lag_model.CostValueNetworkMixin.get_session", "tf.constant", "ppo_lag_model.CostValueNetworkMixin.get_session", "tf.convert_to_tensor", "ppo_lag_model.CostValueNetworkMixin.model.get_cost_value", "tf.convert_to_tensor", "tf.convert_to_tensor", "tf.convert_to_tensor", "tf.convert_to_tensor", "tf.convert_to_tensor"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag_model.CostValueNetwork.get_cost_value"], ["    ", "def", "__init__", "(", "self", ",", "obs_space", ",", "action_space", ",", "config", ")", ":", "\n", "        ", "if", "config", ".", "get", "(", "\"use_gae\"", ")", ":", "\n", "\n", "            ", "@", "make_tf_callable", "(", "self", ".", "get_session", "(", ")", ")", "\n", "def", "cost_value", "(", "ob", ",", "prev_action", ",", "prev_reward", ",", "*", "state", ")", ":", "\n", "                ", "model_out", ",", "_", "=", "self", ".", "model", "(", "\n", "{", "\n", "SampleBatch", ".", "CUR_OBS", ":", "tf", ".", "convert_to_tensor", "(", "[", "ob", "]", ")", ",", "\n", "SampleBatch", ".", "PREV_ACTIONS", ":", "tf", ".", "convert_to_tensor", "(", "[", "prev_action", "]", ")", ",", "\n", "SampleBatch", ".", "PREV_REWARDS", ":", "tf", ".", "convert_to_tensor", "(", "[", "prev_reward", "]", ")", ",", "\n", "\"is_training\"", ":", "tf", ".", "convert_to_tensor", "(", "False", ")", ",", "\n", "}", ",", "[", "tf", ".", "convert_to_tensor", "(", "[", "s", "]", ")", "for", "s", "in", "state", "]", ",", "tf", ".", "convert_to_tensor", "(", "[", "1", "]", ")", "\n", ")", "\n", "return", "self", ".", "model", ".", "get_cost_value", "(", ")", "[", "0", "]", "\n", "", "", "else", ":", "\n", "\n", "            ", "@", "make_tf_callable", "(", "self", ".", "get_session", "(", ")", ")", "\n", "def", "cost_value", "(", "ob", ",", "prev_action", ",", "prev_reward", ",", "*", "state", ")", ":", "\n", "                ", "return", "tf", ".", "constant", "(", "0.0", ")", "\n", "\n", "", "", "self", ".", "_cost_value", "=", "cost_value", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag_model.CostValueNetwork.__init__": [[37, 62], ["ray.rllib.models.tf.tf_modelv2.TFModelV2.__init__", "ray.rllib.utils.framework.get_activation_fn", "model_config.get", "tf.keras.layers.Input", "tf.keras.Model", "ppo_lag_model.CostValueNetwork.register_variables", "model_config.get", "tf.keras.layers.Dense", "tf.keras.layers.Dense", "numpy.product", "ray.rllib.models.tf.misc.normc_initializer", "ray.rllib.models.tf.misc.normc_initializer"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["    ", "def", "__init__", "(", "self", ",", "obs_space", ",", "action_space", ",", "num_outputs", ",", "model_config", ",", "name", ")", ":", "\n", "        ", "super", "(", "CostValueNetwork", ",", "self", ")", ".", "__init__", "(", "obs_space", ",", "action_space", ",", "num_outputs", ",", "model_config", ",", "name", ")", "\n", "activation", "=", "get_activation_fn", "(", "model_config", ".", "get", "(", "\"fcnet_activation\"", ")", ")", "\n", "hiddens", "=", "model_config", ".", "get", "(", "\"fcnet_hiddens\"", ")", "\n", "# we are using obs_flat, so take the flattened shape as input", "\n", "inputs", "=", "tf", ".", "keras", ".", "layers", ".", "Input", "(", "shape", "=", "(", "np", ".", "product", "(", "obs_space", ".", "shape", ")", ",", ")", ",", "name", "=", "\"observations\"", ")", "\n", "# build the value network for cost", "\n", "last_layer", "=", "inputs", "\n", "i", "=", "1", "\n", "for", "size", "in", "hiddens", ":", "\n", "            ", "last_layer", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "size", ",", "\n", "name", "=", "\"fc_value_cost_{}\"", ".", "format", "(", "i", ")", ",", "\n", "activation", "=", "activation", ",", "\n", "kernel_initializer", "=", "normc_initializer", "(", "1.0", ")", "\n", ")", "(", "last_layer", ")", "\n", "i", "+=", "1", "\n", "", "value_out_cost", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "1", ",", "name", "=", "\"value_out_cost\"", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "normc_initializer", "(", "0.01", ")", "\n", ")", "(", "last_layer", ")", "\n", "\n", "# Register the network", "\n", "self", ".", "cost_value_network", "=", "tf", ".", "keras", ".", "Model", "(", "inputs", ",", "value_out_cost", ")", "\n", "self", ".", "register_variables", "(", "self", ".", "cost_value_network", ".", "variables", ")", "\n", "self", ".", "_last_cost_value", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag_model.CostValueNetwork.forward": [[63, 67], ["super().forward", "ppo_lag_model.CostValueNetwork.cost_value_network"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.forward"], ["", "def", "forward", "(", "self", ",", "input_dict", ",", "state", ",", "seq_lens", ")", ":", "\n", "        ", "ret", "=", "super", "(", "CostValueNetwork", ",", "self", ")", ".", "forward", "(", "input_dict", ",", "state", ",", "seq_lens", ")", "\n", "self", ".", "_last_cost_value", "=", "self", ".", "cost_value_network", "(", "input_dict", "[", "\"obs_flat\"", "]", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.ppo_lag.ppo_lag_model.CostValueNetwork.get_cost_value": [[68, 70], ["tf.reshape"], "methods", ["None"], ["", "def", "get_cost_value", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "reshape", "(", "self", ".", "_last_cost_value", ",", "[", "-", "1", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.PIDController.__init__": [[51, 58], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "k_p", ":", "float", ",", "k_i", ":", "float", ",", "k_d", ":", "float", ")", ":", "\n", "        ", "self", ".", "k_p", "=", "k_p", "\n", "self", ".", "k_i", "=", "k_i", "\n", "self", ".", "k_d", "=", "k_d", "\n", "self", ".", "p_error", "=", "0", "\n", "self", ".", "i_error", "=", "0", "\n", "self", ".", "d_error", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.PIDController._update_error": [[59, 63], ["None"], "methods", ["None"], ["", "def", "_update_error", "(", "self", ",", "current_error", ":", "float", ")", ":", "\n", "        ", "self", ".", "i_error", "+=", "current_error", "\n", "self", ".", "d_error", "=", "current_error", "-", "self", ".", "p_error", "\n", "self", ".", "p_error", "=", "current_error", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.PIDController.get_result": [[64, 67], ["sac_pid_policy.PIDController._update_error"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.PIDController._update_error"], ["", "def", "get_result", "(", "self", ",", "current_error", ":", "float", ",", "make_up_coefficient", "=", "1.0", ")", ":", "\n", "        ", "self", ".", "_update_error", "(", "current_error", ")", "\n", "return", "(", "-", "self", ".", "k_p", "*", "self", ".", "p_error", "-", "self", ".", "k_i", "*", "self", ".", "i_error", "-", "self", ".", "k_d", "*", "self", ".", "d_error", ")", "*", "make_up_coefficient", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.PIDController.reset": [[68, 72], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "p_error", "=", "0", "\n", "self", ".", "i_error", "=", "0", "\n", "self", ".", "d_error", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.UpdatePenalty.__init__": [[489, 491], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "workers", ")", ":", "\n", "        ", "self", ".", "workers", "=", "workers", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.UpdatePenalty.__call__": [[492, 504], ["sac_pid_policy.UpdatePenalty.workers.local_worker().foreach_trainable_policy", "ray.rllib.execution.common._get_shared_metrics", "pi.update_penalty", "sac_pid_policy.UpdatePenalty.workers.local_worker"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.UpdatePenaltyMixin.update_penalty"], ["", "def", "__call__", "(", "self", ",", "batch", ")", ":", "\n", "        ", "def", "update", "(", "pi", ",", "pi_id", ")", ":", "\n", "            ", "res", "=", "pi", ".", "update_penalty", "(", "batch", ")", "\n", "return", "(", "pi_id", ",", "res", ")", "\n", "\n", "", "res", "=", "self", ".", "workers", ".", "local_worker", "(", ")", ".", "foreach_trainable_policy", "(", "update", ")", "\n", "\n", "metrics", "=", "_get_shared_metrics", "(", ")", "\n", "metrics", ".", "info", "[", "\"pid_error\"", "]", "=", "res", "[", "0", "]", "[", "1", "]", "[", "0", "]", "\n", "metrics", ".", "info", "[", "\"mean_online_episode_cost\"", "]", "=", "res", "[", "0", "]", "[", "1", "]", "[", "1", "]", "\n", "\n", "return", "batch", "# , fetch", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.UpdatePenaltyMixin.__init__": [[509, 515], ["hasattr", "collections.deque", "sac_pid_policy.PIDController"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "if", "hasattr", "(", "self", ",", "\"lambda_value\"", ")", "and", "self", ".", "config", "[", "\"worker_index\"", "]", "==", "0", ":", "\n", "            ", "self", ".", "recent_episode_cost", "=", "deque", "(", "maxlen", "=", "self", ".", "config", "[", "\"recent_episode_num\"", "]", ")", "\n", "self", ".", "pid_controller", "=", "PIDController", "(", "self", ".", "config", "[", "\"k_p\"", "]", ",", "self", ".", "config", "[", "\"k_i\"", "]", ",", "self", ".", "config", "[", "\"k_d\"", "]", ")", "\n", "self", ".", "new_error", "=", "0", "\n", "self", ".", "online_cost", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.UpdatePenaltyMixin.update_penalty": [[516, 540], ["range", "sac_pid_policy.UpdatePenaltyMixin.recent_episode_cost.append", "numpy.array", "sac_pid_policy.UpdatePenaltyMixin.pid_controller.get_result", "print", "numpy.log", "sac_pid_policy.UpdatePenaltyMixin.lambda_value.assign", "sac_pid_policy.UpdatePenaltyMixin._sess.run", "numpy.sum", "len", "numpy.exp"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.PIDController.get_result"], ["", "", "def", "update_penalty", "(", "self", ",", "batch", ":", "SampleBatch", ")", ":", "\n", "        ", "assert", "self", ".", "config", "[", "\"info_total_cost_key\"", "]", "in", "batch", "\n", "# if batch.get(SampleBatch.INFOS) is None:", "\n", "#     return 0, 0", "\n", "for", "i", "in", "range", "(", "batch", ".", "count", ")", ":", "\n", "            ", "if", "batch", "[", "SampleBatch", ".", "DONES", "]", "[", "i", "]", ":", "\n", "                ", "self", ".", "recent_episode_cost", ".", "append", "(", "batch", "[", "self", ".", "config", "[", "\"info_total_cost_key\"", "]", "]", "[", "i", "]", ")", "\n", "self", ".", "online_cost", "=", "mean_episode_cost", "=", "np", ".", "array", "(", "\n", "[", "np", ".", "sum", "(", "self", ".", "recent_episode_cost", ")", "/", "len", "(", "self", ".", "recent_episode_cost", ")", "]", ")", "\n", "self", ".", "new_error", "=", "mean_episode_cost", "-", "self", ".", "config", "[", "\"cost_limit\"", "]", "\n", "# new_lambda = np.exp(-self.pid_controller.get_result(self.new_error))[0]", "\n", "\n", "pid_result", "=", "self", ".", "pid_controller", ".", "get_result", "(", "self", ".", "new_error", ")", "\n", "print", "(", "\"PIDRESULT: {}, Error {}, P {}, I {}, D {}\"", ".", "format", "(", "pid_result", ",", "\n", "self", ".", "new_error", ",", "\n", "self", ".", "pid_controller", ".", "p_error", ",", "\n", "self", ".", "pid_controller", ".", "i_error", ",", "\n", "self", ".", "pid_controller", ".", "d_error", ",", "\n", ")", ")", "\n", "\n", "new_lambda", "=", "np", ".", "log", "(", "np", ".", "exp", "(", "-", "pid_result", ")", "[", "0", "]", "+", "1", ")", "\n", "assign_op", "=", "self", ".", "lambda_value", ".", "assign", "(", "new_lambda", ")", "\n", "self", ".", "_sess", ".", "run", "(", "assign_op", ")", "\n", "", "", "return", "self", ".", "new_error", ",", "self", ".", "online_cost", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.ActorCriticOptimizerMixin.__init__": [[678, 728], ["get_variable", "tf.keras.optimizers.Adam", "tf.keras.optimizers.Adam", "sac_pid_policy.ActorCriticOptimizerMixin._critic_optimizer.append", "tf1.train.get_or_create_global_step", "tf1.train.AdamOptimizer", "tf1.train.AdamOptimizer", "sac_pid_policy.ActorCriticOptimizerMixin._critic_optimizer.append", "tf.keras.optimizers.Adam", "sac_pid_policy.ActorCriticOptimizerMixin._critic_optimizer.append", "tf.keras.optimizers.Adam", "sac_pid_policy.ActorCriticOptimizerMixin._critic_optimizer.append", "tf1.train.AdamOptimizer", "sac_pid_policy.ActorCriticOptimizerMixin._critic_optimizer.append", "tf1.train.AdamOptimizer", "sac_pid_policy.ActorCriticOptimizerMixin._critic_optimizer.append", "tf.keras.optimizers.Adam", "tf.keras.optimizers.Adam", "tf1.train.AdamOptimizer", "tf1.train.AdamOptimizer"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "# - Create global step for counting the number of update operations.", "\n", "# - Use separate optimizers for actor & critic.", "\n", "        ", "if", "config", "[", "\"framework\"", "]", "in", "[", "\"tf2\"", ",", "\"tfe\"", "]", ":", "\n", "            ", "raise", "ValueError", "\n", "self", ".", "global_step", "=", "get_variable", "(", "0", ",", "tf_name", "=", "\"global_step\"", ")", "\n", "self", ".", "_actor_optimizer", "=", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "\n", "learning_rate", "=", "config", "[", "\"optimization\"", "]", "[", "\"actor_learning_rate\"", "]", ")", "\n", "self", ".", "_critic_optimizer", "=", "[", "\n", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "learning_rate", "=", "config", "[", "\"optimization\"", "]", "[", "\n", "\"critic_learning_rate\"", "]", ")", "\n", "]", "\n", "if", "config", "[", "\"twin_q\"", "]", ":", "\n", "                ", "self", ".", "_critic_optimizer", ".", "append", "(", "\n", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "learning_rate", "=", "config", "[", "\n", "\"optimization\"", "]", "[", "\"critic_learning_rate\"", "]", ")", ")", "\n", "", "self", ".", "_alpha_optimizer", "=", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "\n", "learning_rate", "=", "config", "[", "\"optimization\"", "]", "[", "\"entropy_learning_rate\"", "]", ")", "\n", "\n", "# add optimizer for cost value", "\n", "self", ".", "_critic_optimizer", ".", "append", "(", "\n", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "learning_rate", "=", "config", "[", "\n", "\"optimization\"", "]", "[", "\"critic_learning_rate\"", "]", ")", ")", "\n", "if", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "                ", "self", ".", "_critic_optimizer", ".", "append", "(", "\n", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "learning_rate", "=", "config", "[", "\n", "\"optimization\"", "]", "[", "\"critic_learning_rate\"", "]", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "global_step", "=", "tf1", ".", "train", ".", "get_or_create_global_step", "(", ")", "\n", "self", ".", "_actor_optimizer", "=", "tf1", ".", "train", ".", "AdamOptimizer", "(", "\n", "learning_rate", "=", "config", "[", "\"optimization\"", "]", "[", "\"actor_learning_rate\"", "]", ")", "\n", "self", ".", "_critic_optimizer", "=", "[", "\n", "tf1", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "config", "[", "\"optimization\"", "]", "[", "\n", "\"critic_learning_rate\"", "]", ")", "\n", "]", "\n", "if", "config", "[", "\"twin_q\"", "]", ":", "\n", "                ", "self", ".", "_critic_optimizer", ".", "append", "(", "\n", "tf1", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "config", "[", "\n", "\"optimization\"", "]", "[", "\"critic_learning_rate\"", "]", ")", ")", "\n", "", "self", ".", "_alpha_optimizer", "=", "tf1", ".", "train", ".", "AdamOptimizer", "(", "\n", "learning_rate", "=", "config", "[", "\"optimization\"", "]", "[", "\"entropy_learning_rate\"", "]", ")", "\n", "\n", "# add optimizer for cost value", "\n", "self", ".", "_critic_optimizer", ".", "append", "(", "\n", "tf1", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "config", "[", "\n", "\"optimization\"", "]", "[", "\"critic_learning_rate\"", "]", ")", ")", "\n", "if", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "                ", "self", ".", "_critic_optimizer", ".", "append", "(", "\n", "tf1", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "config", "[", "\n", "\"optimization\"", "]", "[", "\"critic_learning_rate\"", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.build_sac_model": [[74, 129], ["ray.rllib.models.ModelCatalog.get_model_v2", "ray.rllib.models.ModelCatalog.get_model_v2", "logger.warning"], "function", ["None"], ["", "", "def", "build_sac_model", "(", "policy", ",", "obs_space", ",", "action_space", ",", "config", ")", ":", "\n", "# 2 cases:", "\n", "# 1) with separate state-preprocessor (before obs+action concat).", "\n", "# 2) no separate state-preprocessor: concat obs+actions right away.", "\n", "    ", "if", "config", "[", "\"use_state_preprocessor\"", "]", ":", "\n", "        ", "num_outputs", "=", "256", "# Flatten last Conv2D to this many nodes.", "\n", "", "else", ":", "\n", "        ", "num_outputs", "=", "0", "\n", "# No state preprocessor: fcnet_hiddens should be empty.", "\n", "if", "config", "[", "\"model\"", "]", "[", "\"fcnet_hiddens\"", "]", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"When not using a state-preprocessor with SAC, `fcnet_hiddens`\"", "\n", "\" will be set to an empty list! Any hidden layer sizes are \"", "\n", "\"defined via `policy_model.fcnet_hiddens` and \"", "\n", "\"`Q_model.fcnet_hiddens`.\"", ")", "\n", "config", "[", "\"model\"", "]", "[", "\"fcnet_hiddens\"", "]", "=", "[", "]", "\n", "\n", "# Force-ignore any additionally provided hidden layer sizes.", "\n", "# Everything should be configured using SAC's \"Q_model\" and \"policy_model\"", "\n", "# settings.", "\n", "", "", "policy", ".", "model", "=", "ModelCatalog", ".", "get_model_v2", "(", "\n", "obs_space", "=", "obs_space", ",", "\n", "action_space", "=", "action_space", ",", "\n", "num_outputs", "=", "num_outputs", ",", "\n", "model_config", "=", "config", "[", "\"model\"", "]", ",", "\n", "framework", "=", "config", "[", "\"framework\"", "]", ",", "\n", "model_interface", "=", "ConstrainedSACModel", ",", "\n", "name", "=", "\"sac_model\"", ",", "\n", "actor_hidden_activation", "=", "config", "[", "\"policy_model\"", "]", "[", "\"fcnet_activation\"", "]", ",", "\n", "actor_hiddens", "=", "config", "[", "\"policy_model\"", "]", "[", "\"fcnet_hiddens\"", "]", ",", "\n", "critic_hidden_activation", "=", "config", "[", "\"Q_model\"", "]", "[", "\"fcnet_activation\"", "]", ",", "\n", "critic_hiddens", "=", "config", "[", "\"Q_model\"", "]", "[", "\"fcnet_hiddens\"", "]", ",", "\n", "twin_q", "=", "config", "[", "\"twin_q\"", "]", ",", "\n", "twin_cost_q", "=", "config", "[", "\"twin_cost_q\"", "]", ",", "\n", "initial_alpha", "=", "config", "[", "\"initial_alpha\"", "]", ",", "\n", "target_entropy", "=", "config", "[", "\"target_entropy\"", "]", ")", "\n", "\n", "policy", ".", "target_model", "=", "ModelCatalog", ".", "get_model_v2", "(", "\n", "obs_space", "=", "obs_space", ",", "\n", "action_space", "=", "action_space", ",", "\n", "num_outputs", "=", "num_outputs", ",", "\n", "model_config", "=", "config", "[", "\"model\"", "]", ",", "\n", "framework", "=", "config", "[", "\"framework\"", "]", ",", "\n", "model_interface", "=", "ConstrainedSACModel", ",", "\n", "name", "=", "\"target_sac_model\"", ",", "\n", "actor_hidden_activation", "=", "config", "[", "\"policy_model\"", "]", "[", "\"fcnet_activation\"", "]", ",", "\n", "actor_hiddens", "=", "config", "[", "\"policy_model\"", "]", "[", "\"fcnet_hiddens\"", "]", ",", "\n", "critic_hidden_activation", "=", "config", "[", "\"Q_model\"", "]", "[", "\"fcnet_activation\"", "]", ",", "\n", "critic_hiddens", "=", "config", "[", "\"Q_model\"", "]", "[", "\"fcnet_hiddens\"", "]", ",", "\n", "twin_q", "=", "config", "[", "\"twin_q\"", "]", ",", "\n", "twin_cost_q", "=", "config", "[", "\"twin_cost_q\"", "]", ",", "\n", "initial_alpha", "=", "config", "[", "\"initial_alpha\"", "]", ",", "\n", "target_entropy", "=", "config", "[", "\"target_entropy\"", "]", ")", "\n", "\n", "return", "policy", ".", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.postprocess_trajectory": [[131, 158], ["sample_batch.get", "ray.rllib.agents.dqn.dqn_tf_policy.postprocess_nstep_and_prio", "numpy.array().astype", "numpy.array().astype", "numpy.zeros_like", "numpy.zeros_like", "numpy.array", "numpy.array", "numpy.array"], "function", ["None"], ["", "def", "postprocess_trajectory", "(", "policy", ",", "\n", "sample_batch", ",", "\n", "other_agent_batches", "=", "None", ",", "\n", "episode", "=", "None", ")", ":", "\n", "# if sample_batch.count > 1:", "\n", "#     raise ValueError", "\n", "# Put the actions to batch", "\n", "    ", "infos", "=", "sample_batch", ".", "get", "(", "SampleBatch", ".", "INFOS", ")", "\n", "if", "(", "infos", "is", "not", "None", ")", "and", "(", "infos", "[", "0", "]", "==", "0.0", ")", ":", "\n", "        ", "sample_batch", "[", "SampleBatch", ".", "INFOS", "]", "+=", "0.0", "\n", "", "if", "(", "infos", "is", "not", "None", ")", "and", "(", "infos", "[", "0", "]", "!=", "0.0", ")", ":", "\n", "        ", "if", "\"raw_action\"", "in", "infos", "[", "0", "]", ":", "\n", "            ", "sample_batch", "[", "SampleBatch", ".", "ACTIONS", "]", "=", "np", ".", "array", "(", "[", "info", "[", "\"raw_action\"", "]", "for", "info", "in", "infos", "]", ")", "\n", "", "sample_batch", "[", "policy", ".", "config", "[", "\"info_cost_key\"", "]", "]", "=", "np", ".", "array", "(", "\n", "[", "info", "[", "policy", ".", "config", "[", "\"info_cost_key\"", "]", "]", "for", "info", "in", "sample_batch", "[", "SampleBatch", ".", "INFOS", "]", "]", "\n", ")", ".", "astype", "(", "sample_batch", "[", "SampleBatch", ".", "REWARDS", "]", ".", "dtype", ")", "\n", "sample_batch", "[", "policy", ".", "config", "[", "\"info_total_cost_key\"", "]", "]", "=", "np", ".", "array", "(", "\n", "[", "info", "[", "policy", ".", "config", "[", "\"info_total_cost_key\"", "]", "]", "for", "info", "in", "sample_batch", "[", "SampleBatch", ".", "INFOS", "]", "]", "\n", ")", ".", "astype", "(", "sample_batch", "[", "SampleBatch", ".", "REWARDS", "]", ".", "dtype", ")", "\n", "", "else", ":", "\n", "        ", "assert", "episode", "is", "None", ",", "\"Only during initialization, can we see empty infos.\"", "\n", "sample_batch", "[", "policy", ".", "config", "[", "\"info_cost_key\"", "]", "]", "=", "np", ".", "zeros_like", "(", "sample_batch", "[", "SampleBatch", ".", "REWARDS", "]", ")", "\n", "sample_batch", "[", "policy", ".", "config", "[", "\"info_total_cost_key\"", "]", "]", "=", "np", ".", "zeros_like", "(", "sample_batch", "[", "SampleBatch", ".", "REWARDS", "]", ")", "\n", "", "batch", "=", "postprocess_nstep_and_prio", "(", "policy", ",", "sample_batch", ")", "\n", "assert", "policy", ".", "config", "[", "\"info_cost_key\"", "]", "in", "batch", "\n", "assert", "policy", ".", "config", "[", "\"info_total_cost_key\"", "]", "in", "batch", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.get_dist_class": [[160, 171], ["isinstance", "ValueError", "ValueError"], "function", ["None"], ["", "def", "get_dist_class", "(", "config", ",", "action_space", ")", ":", "\n", "    ", "if", "isinstance", "(", "action_space", ",", "Discrete", ")", ":", "\n", "        ", "raise", "ValueError", "(", ")", "\n", "return", "Categorical", "\n", "", "else", ":", "\n", "        ", "if", "config", "[", "\"normalize_actions\"", "]", ":", "\n", "            ", "return", "SquashedGaussian", "if", "not", "config", "[", "\"_use_beta_distribution\"", "]", "else", "Beta", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", ")", "\n", "return", "DiagGaussian", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.get_distribution_inputs_and_class": [[173, 188], ["model", "model.get_policy_output", "sac_pid_policy.get_dist_class", "policy._get_is_training_placeholder"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_policy_output", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.get_dist_class"], ["", "", "", "def", "get_distribution_inputs_and_class", "(", "policy", ",", "\n", "model", ",", "\n", "obs_batch", ",", "\n", "*", ",", "\n", "explore", "=", "True", ",", "\n", "**", "kwargs", ")", ":", "\n", "# Get base-model output.", "\n", "    ", "model_out", ",", "state_out", "=", "model", "(", "{", "\n", "\"obs\"", ":", "obs_batch", ",", "\n", "\"is_training\"", ":", "policy", ".", "_get_is_training_placeholder", "(", ")", ",", "\n", "}", ",", "[", "]", ",", "None", ")", "\n", "# Get action model output from base-model output.", "\n", "distribution_inputs", "=", "model", ".", "get_policy_output", "(", "model_out", ")", "\n", "action_dist_class", "=", "get_dist_class", "(", "policy", ".", "config", ",", "policy", ".", "action_space", ")", "\n", "return", "distribution_inputs", ",", "action_dist_class", ",", "state_out", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.sac_actor_critic_loss": [[190, 432], ["model", "model", "policy.target_model", "tf.squeeze", "tf.stop_gradient", "tf.stop_gradient", "tf.math.abs", "tf.math.abs", "critic_loss.append", "tf.variable_scope", "tf.get_variable", "ValueError", "tf.nn.log_softmax", "tf.math.exp", "tf.nn.log_softmax", "tf.math.exp", "model.get_q_values", "policy.target_model.get_q_values", "tf.one_hot", "tf.reduce_sum", "tf.reduce_sum", "sac_pid_policy.get_dist_class", "get_dist_class.", "tf.expand_dims", "get_dist_class.", "tf.expand_dims", "model.get_q_values", "model.get_cost_q_values", "model.get_q_values", "model.get_cost_q_values", "policy.target_model.get_q_values", "policy.target_model.get_cost_q_values", "tf.squeeze", "tf.squeeze", "tf.squeeze", "tf.math.abs", "tf.math.abs", "critic_loss.append", "critic_loss.append", "ValueError", "tf.reduce_mean", "tf.reduce_mean", "policy._get_is_training_placeholder", "policy._get_is_training_placeholder", "policy._get_is_training_placeholder", "model.get_policy_output", "model.get_policy_output", "model.get_twin_q_values", "policy.target_model.get_twin_q_values", "tf.reduce_min", "tf.reduce_sum", "tf.multiply", "model.get_policy_output", "action_dist_class.sample", "action_dist_class.deterministic_sample", "action_dist_class.logp", "model.get_policy_output", "action_dist_class.sample", "action_dist_class.deterministic_sample", "action_dist_class.logp", "model.get_twin_q_values", "model.get_twin_cost_q_values", "model.get_twin_q_values", "tf.reduce_min", "model.get_twin_cost_q_values", "tf.reduce_min", "policy.target_model.get_twin_q_values", "tf.reduce_min", "policy.target_model.get_twin_cost_q_values", "tf.reduce_min", "tf.squeeze", "tf.squeeze", "tf.cast", "tf.keras.losses.MSE", "tf.keras.losses.MSE", "tf.reduce_sum", "tf.reduce_sum", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.math.add_n", "float", "tf.cast", "tf.cast", "len", "tf.keras.losses.MSE", "tf.keras.losses.MSE", "tf.multiply", "tf.multiply", "model.get_q_values.shape.as_list", "len", "len", "len", "tf.stop_gradient", "tf.stop_gradient", "len", "len", "tf.stop_gradient", "tf.stop_gradient"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.get_dist_class", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_cost_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_cost_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_cost_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_policy_output", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_policy_output", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_policy_output", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_policy_output", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_cost_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_cost_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_cost_q_values"], ["", "def", "sac_actor_critic_loss", "(", "policy", ",", "model", ",", "_", ",", "train_batch", ")", ":", "\n", "    ", "_", "=", "train_batch", "[", "policy", ".", "config", "[", "\"info_total_cost_key\"", "]", "]", "# Touch this item, this is helpful in ray 1.2.0", "\n", "\n", "# Setup the lambda multiplier.", "\n", "with", "tf", ".", "variable_scope", "(", "'lambda'", ")", ":", "\n", "        ", "param_init", "=", "1e-8", "\n", "lambda_param", "=", "tf", ".", "get_variable", "(", "\n", "'lambda_value'", ",", "\n", "initializer", "=", "float", "(", "param_init", ")", ",", "\n", "trainable", "=", "False", ",", "\n", "dtype", "=", "tf", ".", "float32", "\n", ")", "\n", "", "policy", ".", "lambda_value", "=", "lambda_param", "\n", "\n", "# Should be True only for debugging purposes (e.g. test cases)!", "\n", "deterministic", "=", "policy", ".", "config", "[", "\"_deterministic_loss\"", "]", "\n", "\n", "model_out_t", ",", "_", "=", "model", "(", "{", "\n", "\"obs\"", ":", "train_batch", "[", "SampleBatch", ".", "CUR_OBS", "]", ",", "\n", "\"is_training\"", ":", "policy", ".", "_get_is_training_placeholder", "(", ")", ",", "\n", "}", ",", "[", "]", ",", "None", ")", "\n", "\n", "model_out_tp1", ",", "_", "=", "model", "(", "{", "\n", "\"obs\"", ":", "train_batch", "[", "SampleBatch", ".", "NEXT_OBS", "]", ",", "\n", "\"is_training\"", ":", "policy", ".", "_get_is_training_placeholder", "(", ")", ",", "\n", "}", ",", "[", "]", ",", "None", ")", "\n", "\n", "target_model_out_tp1", ",", "_", "=", "policy", ".", "target_model", "(", "{", "\n", "\"obs\"", ":", "train_batch", "[", "SampleBatch", ".", "NEXT_OBS", "]", ",", "\n", "\"is_training\"", ":", "policy", ".", "_get_is_training_placeholder", "(", ")", ",", "\n", "}", ",", "[", "]", ",", "None", ")", "\n", "\n", "# Discrete case.", "\n", "if", "model", ".", "discrete", ":", "\n", "        ", "raise", "ValueError", "(", "\"Doesn't support yet\"", ")", "\n", "# Get all action probs directly from pi and form their logp.", "\n", "log_pis_t", "=", "tf", ".", "nn", ".", "log_softmax", "(", "model", ".", "get_policy_output", "(", "model_out_t", ")", ",", "-", "1", ")", "\n", "policy_t", "=", "tf", ".", "math", ".", "exp", "(", "log_pis_t", ")", "\n", "log_pis_tp1", "=", "tf", ".", "nn", ".", "log_softmax", "(", "\n", "model", ".", "get_policy_output", "(", "model_out_tp1", ")", ",", "-", "1", ")", "\n", "policy_tp1", "=", "tf", ".", "math", ".", "exp", "(", "log_pis_tp1", ")", "\n", "# Q-values.", "\n", "q_t", "=", "model", ".", "get_q_values", "(", "model_out_t", ")", "\n", "# Target Q-values.", "\n", "q_tp1", "=", "policy", ".", "target_model", ".", "get_q_values", "(", "target_model_out_tp1", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "            ", "twin_q_t", "=", "model", ".", "get_twin_q_values", "(", "model_out_t", ")", "\n", "twin_q_tp1", "=", "policy", ".", "target_model", ".", "get_twin_q_values", "(", "\n", "target_model_out_tp1", ")", "\n", "q_tp1", "=", "tf", ".", "reduce_min", "(", "(", "q_tp1", ",", "twin_q_tp1", ")", ",", "axis", "=", "0", ")", "\n", "", "q_tp1", "-=", "model", ".", "alpha", "*", "log_pis_tp1", "\n", "\n", "# Actually selected Q-values (from the actions batch).", "\n", "one_hot", "=", "tf", ".", "one_hot", "(", "\n", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ",", "depth", "=", "q_t", ".", "shape", ".", "as_list", "(", ")", "[", "-", "1", "]", ")", "\n", "q_t_selected", "=", "tf", ".", "reduce_sum", "(", "q_t", "*", "one_hot", ",", "axis", "=", "-", "1", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "            ", "twin_q_t_selected", "=", "tf", ".", "reduce_sum", "(", "twin_q_t", "*", "one_hot", ",", "axis", "=", "-", "1", ")", "\n", "# Discrete case: \"Best\" means weighted by the policy (prob) outputs.", "\n", "", "q_tp1_best", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "policy_tp1", ",", "q_tp1", ")", ",", "axis", "=", "-", "1", ")", "\n", "q_tp1_best_masked", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "train_batch", "[", "SampleBatch", ".", "DONES", "]", ",", "tf", ".", "float32", ")", ")", "*", "q_tp1_best", "\n", "# Continuous actions case.", "\n", "", "else", ":", "\n", "# Sample simgle actions from distribution.", "\n", "        ", "action_dist_class", "=", "get_dist_class", "(", "policy", ".", "config", ",", "policy", ".", "action_space", ")", "\n", "action_dist_t", "=", "action_dist_class", "(", "\n", "model", ".", "get_policy_output", "(", "model_out_t", ")", ",", "policy", ".", "model", ")", "\n", "policy_t", "=", "action_dist_t", ".", "sample", "(", ")", "if", "not", "deterministic", "else", "action_dist_t", ".", "deterministic_sample", "(", ")", "\n", "log_pis_t", "=", "tf", ".", "expand_dims", "(", "action_dist_t", ".", "logp", "(", "policy_t", ")", ",", "-", "1", ")", "\n", "action_dist_tp1", "=", "action_dist_class", "(", "\n", "model", ".", "get_policy_output", "(", "model_out_tp1", ")", ",", "policy", ".", "model", ")", "\n", "policy_tp1", "=", "action_dist_tp1", ".", "sample", "(", ")", "if", "not", "deterministic", "else", "action_dist_tp1", ".", "deterministic_sample", "(", ")", "\n", "log_pis_tp1", "=", "tf", ".", "expand_dims", "(", "action_dist_tp1", ".", "logp", "(", "policy_tp1", ")", ",", "-", "1", ")", "\n", "\n", "# Q-values for the actually selected actions.", "\n", "q_t", "=", "model", ".", "get_q_values", "(", "model_out_t", ",", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "            ", "twin_q_t", "=", "model", ".", "get_twin_q_values", "(", "\n", "model_out_t", ",", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ")", "\n", "\n", "# Cost Q-Value for actually selected actions", "\n", "", "c_q_t", "=", "model", ".", "get_cost_q_values", "(", "model_out_t", ",", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "            ", "twin_c_q_t", "=", "model", ".", "get_twin_cost_q_values", "(", "\n", "model_out_t", ",", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ")", "\n", "\n", "# Q-values for current policy in given current state.", "\n", "", "q_t_det_policy", "=", "model", ".", "get_q_values", "(", "model_out_t", ",", "policy_t", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "            ", "twin_q_t_det_policy", "=", "model", ".", "get_twin_q_values", "(", "\n", "model_out_t", ",", "policy_t", ")", "\n", "q_t_det_policy", "=", "tf", ".", "reduce_min", "(", "\n", "(", "q_t_det_policy", ",", "twin_q_t_det_policy", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# Cost Q-values for current policy in given current state.", "\n", "", "c_q_t_det_policy", "=", "model", ".", "get_cost_q_values", "(", "model_out_t", ",", "policy_t", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "            ", "twin_c_q_t_det_policy", "=", "model", ".", "get_twin_cost_q_values", "(", "\n", "model_out_t", ",", "policy_t", ")", "\n", "c_q_t_det_policy", "=", "tf", ".", "reduce_min", "(", "\n", "(", "c_q_t_det_policy", ",", "twin_c_q_t_det_policy", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# target q network evaluation", "\n", "", "q_tp1", "=", "policy", ".", "target_model", ".", "get_q_values", "(", "target_model_out_tp1", ",", "\n", "policy_tp1", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "            ", "twin_q_tp1", "=", "policy", ".", "target_model", ".", "get_twin_q_values", "(", "\n", "target_model_out_tp1", ",", "policy_tp1", ")", "\n", "# Take min over both twin-NNs.", "\n", "q_tp1", "=", "tf", ".", "reduce_min", "(", "(", "q_tp1", ",", "twin_q_tp1", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# target c-q network evaluation", "\n", "", "c_q_tp1", "=", "policy", ".", "target_model", ".", "get_cost_q_values", "(", "target_model_out_tp1", ",", "\n", "policy_tp1", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "            ", "twin_c_q_tp1", "=", "policy", ".", "target_model", ".", "get_twin_cost_q_values", "(", "\n", "target_model_out_tp1", ",", "policy_tp1", ")", "\n", "# Take min over both twin-NNs.", "\n", "c_q_tp1", "=", "tf", ".", "reduce_min", "(", "(", "c_q_tp1", ",", "twin_c_q_tp1", ")", ",", "axis", "=", "0", ")", "\n", "\n", "", "q_t_selected", "=", "tf", ".", "squeeze", "(", "q_t", ",", "axis", "=", "len", "(", "q_t", ".", "shape", ")", "-", "1", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "            ", "twin_q_t_selected", "=", "tf", ".", "squeeze", "(", "twin_q_t", ",", "axis", "=", "len", "(", "twin_q_t", ".", "shape", ")", "-", "1", ")", "\n", "\n", "# c_q_t selected", "\n", "", "c_q_t_selected", "=", "tf", ".", "squeeze", "(", "c_q_t", ",", "axis", "=", "len", "(", "c_q_t", ".", "shape", ")", "-", "1", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "            ", "twin_c_q_t_selected", "=", "tf", ".", "squeeze", "(", "twin_c_q_t", ",", "axis", "=", "len", "(", "twin_c_q_t", ".", "shape", ")", "-", "1", ")", "\n", "\n", "", "q_tp1", "-=", "model", ".", "alpha", "*", "log_pis_tp1", "\n", "\n", "q_tp1_best", "=", "tf", ".", "squeeze", "(", "input", "=", "q_tp1", ",", "axis", "=", "len", "(", "q_tp1", ".", "shape", ")", "-", "1", ")", "\n", "q_tp1_best_masked", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "train_batch", "[", "SampleBatch", ".", "DONES", "]", ",", "\n", "tf", ".", "float32", ")", ")", "*", "q_tp1_best", "\n", "\n", "", "c_q_tp1_best", "=", "tf", ".", "squeeze", "(", "input", "=", "c_q_tp1", ",", "axis", "=", "len", "(", "c_q_tp1", ".", "shape", ")", "-", "1", ")", "\n", "c_q_tp1_best_masked", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "train_batch", "[", "SampleBatch", ".", "DONES", "]", ",", "tf", ".", "float32", ")", ")", "*", "c_q_tp1_best", "\n", "\n", "# compute RHS of bellman equation", "\n", "q_t_selected_target", "=", "tf", ".", "stop_gradient", "(", "\n", "train_batch", "[", "SampleBatch", ".", "REWARDS", "]", "+", "\n", "policy", ".", "config", "[", "\"gamma\"", "]", "**", "policy", ".", "config", "[", "\"n_step\"", "]", "*", "q_tp1_best_masked", ")", "\n", "\n", "# Compute Cost of bellman equation.", "\n", "c_q_t_selected_target", "=", "tf", ".", "stop_gradient", "(", "train_batch", "[", "policy", ".", "config", "[", "\"info_cost_key\"", "]", "]", "+", "\n", "policy", ".", "config", "[", "\"gamma\"", "]", "**", "policy", ".", "config", "[", "\"n_step\"", "]", "*", "c_q_tp1_best_masked", ")", "\n", "\n", "# Compute the TD-error (potentially clipped).", "\n", "base_td_error", "=", "tf", ".", "math", ".", "abs", "(", "q_t_selected", "-", "q_t_selected_target", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "        ", "twin_td_error", "=", "tf", ".", "math", ".", "abs", "(", "twin_q_t_selected", "-", "q_t_selected_target", ")", "\n", "td_error", "=", "0.5", "*", "(", "base_td_error", "+", "twin_td_error", ")", "\n", "", "else", ":", "\n", "        ", "td_error", "=", "base_td_error", "\n", "\n", "# Compute the Cost TD-error (potentially clipped).", "\n", "", "base_c_td_error", "=", "tf", ".", "math", ".", "abs", "(", "c_q_t_selected", "-", "c_q_t_selected_target", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "        ", "twin_c_td_error", "=", "tf", ".", "math", ".", "abs", "(", "twin_c_q_t_selected", "-", "c_q_t_selected_target", ")", "\n", "c_td_error", "=", "0.5", "*", "(", "base_c_td_error", "+", "twin_c_td_error", ")", "\n", "", "else", ":", "\n", "        ", "c_td_error", "=", "base_c_td_error", "\n", "\n", "", "critic_loss", "=", "[", "\n", "0.5", "*", "tf", ".", "keras", ".", "losses", ".", "MSE", "(", "\n", "y_true", "=", "q_t_selected_target", ",", "y_pred", "=", "q_t_selected", ")", "\n", "]", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "        ", "critic_loss", ".", "append", "(", "0.5", "*", "tf", ".", "keras", ".", "losses", ".", "MSE", "(", "\n", "y_true", "=", "q_t_selected_target", ",", "y_pred", "=", "twin_q_t_selected", ")", ")", "\n", "\n", "# add cost critic", "\n", "", "critic_loss", ".", "append", "(", "\n", "0.5", "*", "tf", ".", "keras", ".", "losses", ".", "MSE", "(", "\n", "y_true", "=", "c_q_t_selected_target", ",", "y_pred", "=", "c_q_t_selected", ")", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "        ", "critic_loss", ".", "append", "(", "0.5", "*", "tf", ".", "keras", ".", "losses", ".", "MSE", "(", "\n", "y_true", "=", "c_q_t_selected_target", ",", "y_pred", "=", "twin_c_q_t_selected", ")", ")", "\n", "\n", "# Alpha- and actor losses.", "\n", "# Note: In the papers, alpha is used directly, here we take the log.", "\n", "# Discrete case: Multiply the action probs as weights with the original", "\n", "# loss terms (no expectations needed).", "\n", "", "if", "model", ".", "discrete", ":", "\n", "        ", "raise", "ValueError", "(", "\"Didn't support discrete mode yet\"", ")", "\n", "alpha_loss", "=", "tf", ".", "reduce_mean", "(", "\n", "tf", ".", "reduce_sum", "(", "\n", "tf", ".", "multiply", "(", "\n", "tf", ".", "stop_gradient", "(", "policy_t", ")", ",", "-", "model", ".", "log_alpha", "*", "\n", "tf", ".", "stop_gradient", "(", "log_pis_t", "+", "model", ".", "target_entropy", ")", ")", ",", "\n", "axis", "=", "-", "1", ")", ")", "\n", "actor_loss", "=", "tf", ".", "reduce_mean", "(", "\n", "tf", ".", "reduce_sum", "(", "\n", "tf", ".", "multiply", "(", "\n", "# NOTE: No stop_grad around policy output here", "\n", "# (compare with q_t_det_policy for continuous case).", "\n", "policy_t", ",", "\n", "model", ".", "alpha", "*", "log_pis_t", "-", "tf", ".", "stop_gradient", "(", "q_t", ")", ")", ",", "\n", "axis", "=", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "        ", "alpha_loss", "=", "-", "tf", ".", "reduce_mean", "(", "\n", "model", ".", "log_alpha", "*", "\n", "tf", ".", "stop_gradient", "(", "log_pis_t", "+", "model", ".", "target_entropy", ")", ")", "\n", "if", "policy", ".", "config", "[", "\"only_evaluate_cost\"", "]", ":", "\n", "            ", "actor_loss", "=", "tf", ".", "reduce_mean", "(", "\n", "model", ".", "alpha", "*", "log_pis_t", "-", "q_t_det_policy", ")", "\n", "cost_loss", "=", "0", "\n", "reward_loss", "=", "actor_loss", "\n", "", "else", ":", "\n", "            ", "reward_loss", "=", "tf", ".", "reduce_mean", "(", "\n", "model", ".", "alpha", "*", "log_pis_t", "-", "q_t_det_policy", ")", "\n", "cost_loss", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "lambda_value", "*", "c_q_t_det_policy", ")", "\n", "actor_loss", "=", "tf", ".", "reduce_mean", "(", "\n", "model", ".", "alpha", "*", "log_pis_t", "-", "q_t_det_policy", "+", "policy", ".", "lambda_value", "*", "c_q_t_det_policy", ")", "\n", "", "actor_loss", "=", "actor_loss", "/", "(", "1", "+", "policy", ".", "lambda_value", ")", "if", "policy", ".", "config", "[", "\"normalize\"", "]", "else", "actor_loss", "\n", "\n", "# save for stats function", "\n", "", "policy", ".", "policy_t", "=", "policy_t", "\n", "policy", ".", "cost_loss", "=", "cost_loss", "\n", "policy", ".", "reward_loss", "=", "reward_loss", "\n", "policy", ".", "mean_batch_cost", "=", "train_batch", "[", "policy", ".", "config", "[", "\"info_cost_key\"", "]", "]", "\n", "policy", ".", "q_t", "=", "q_t", "\n", "policy", ".", "c_q_tp1", "=", "c_q_tp1", "\n", "policy", ".", "c_q_t", "=", "c_q_t", "\n", "policy", ".", "td_error", "=", "td_error", "\n", "policy", ".", "c_td_error", "=", "c_td_error", "\n", "policy", ".", "actor_loss", "=", "actor_loss", "\n", "policy", ".", "critic_loss", "=", "critic_loss", "\n", "policy", ".", "c_td_target", "=", "c_q_t_selected_target", "\n", "policy", ".", "alpha_loss", "=", "alpha_loss", "\n", "policy", ".", "alpha_value", "=", "model", ".", "alpha", "\n", "policy", ".", "target_entropy", "=", "model", ".", "target_entropy", "\n", "\n", "# in a custom apply op we handle the losses separately, but return them", "\n", "# combined in one loss for now", "\n", "return", "actor_loss", "+", "tf", ".", "math", ".", "add_n", "(", "critic_loss", ")", "+", "alpha_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.stats": [[434, 459], ["tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.constant", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_max", "tf.reduce_max", "tf.reduce_min", "tf.reduce_min", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean"], "function", ["None"], ["", "def", "stats", "(", "policy", ",", "train_batch", ")", ":", "\n", "    ", "return", "{", "\n", "# \"policy_t\": policy.policy_t,", "\n", "# \"td_error\": policy.td_error,", "\n", "\"mean_td_error\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "td_error", ")", ",", "\n", "\"mean_c_td_error\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "c_td_error", ")", ",", "\n", "\"actor_loss\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "actor_loss", ")", ",", "\n", "\"critic_loss\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "critic_loss", "[", ":", "2", "]", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", "else", "policy", ".", "critic_loss", "[", "0", "]", ")", ",", "\n", "\"cost_critic_loss\"", ":", "tf", ".", "reduce_mean", "(", "\n", "policy", ".", "critic_loss", "[", "2", ":", "]", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", "else", "policy", ".", "critic_loss", "[", "1", "]", ")", ",", "\n", "\"alpha_loss\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "alpha_loss", ")", ",", "\n", "\"lambda_value\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "lambda_value", ")", ",", "\n", "\"alpha_value\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "alpha_value", ")", ",", "\n", "\"target_entropy\"", ":", "tf", ".", "constant", "(", "policy", ".", "target_entropy", ")", ",", "\n", "\"c_td_target\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "c_td_target", ")", ",", "\n", "\"mean_q\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "q_t", ")", ",", "\n", "\"mean_c_q\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "c_q_t", ")", ",", "\n", "\"max_q\"", ":", "tf", ".", "reduce_max", "(", "policy", ".", "q_t", ")", ",", "\n", "\"max_c_q\"", ":", "tf", ".", "reduce_max", "(", "policy", ".", "c_q_t", ")", ",", "\n", "\"min_q\"", ":", "tf", ".", "reduce_min", "(", "policy", ".", "q_t", ")", ",", "\n", "\"min_c_q\"", ":", "tf", ".", "reduce_min", "(", "policy", ".", "c_q_t", ")", ",", "\n", "\"c_q_tp1\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "c_q_tp1", ")", ",", "\n", "\"mean_batch_cost\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "mean_batch_cost", ")", ",", "\n", "\"reward_loss\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "reward_loss", ")", ",", "\n", "\"cost_loss\"", ":", "tf", ".", "reduce_mean", "(", "policy", ".", "cost_loss", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.setup_early_mixins": [[462, 464], ["sac_pid_policy.ActorCriticOptimizerMixin.__init__"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["", "def", "setup_early_mixins", "(", "policy", ",", "obs_space", ",", "action_space", ",", "config", ")", ":", "\n", "    ", "ActorCriticOptimizerMixin", ".", "__init__", "(", "policy", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.setup_mid_mixins": [[466, 468], ["ray.rllib.agents.ddpg.ddpg_tf_policy.ComputeTDErrorMixin.__init__"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["", "def", "setup_mid_mixins", "(", "policy", ",", "obs_space", ",", "action_space", ",", "config", ")", ":", "\n", "    ", "ComputeTDErrorMixin", ".", "__init__", "(", "policy", ",", "sac_actor_critic_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.setup_late_mixins": [[470, 473], ["ray.rllib.agents.ddpg.ddpg_tf_policy.TargetNetworkMixin.__init__", "sac_pid_policy.UpdatePenaltyMixin.__init__"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__", "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["", "def", "setup_late_mixins", "(", "policy", ",", "obs_space", ",", "action_space", ",", "config", ")", ":", "\n", "    ", "TargetNetworkMixin", ".", "__init__", "(", "policy", ",", "config", ")", "\n", "UpdatePenaltyMixin", ".", "__init__", "(", "policy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.validate_spaces": [[475, 485], ["isinstance", "ray.rllib.utils.error.UnsupportedSpaceException", "isinstance", "ray.rllib.utils.error.UnsupportedSpaceException", "len"], "function", ["None"], ["", "def", "validate_spaces", "(", "pid", ",", "observation_space", ",", "action_space", ",", "config", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "action_space", ",", "(", "Box", ",", "Discrete", ")", ")", ":", "\n", "        ", "raise", "UnsupportedSpaceException", "(", "\n", "\"Action space ({}) of {} is not supported for \"", "\n", "\"SAC.\"", ".", "format", "(", "action_space", ",", "pid", ")", ")", "\n", "", "if", "isinstance", "(", "action_space", ",", "Box", ")", "and", "len", "(", "action_space", ".", "shape", ")", ">", "1", ":", "\n", "        ", "raise", "UnsupportedSpaceException", "(", "\n", "\"Action space ({}) of {} has multiple dimensions \"", "\n", "\"{}. \"", ".", "format", "(", "action_space", ",", "pid", ",", "action_space", ".", "shape", ")", "+", "\n", "\"Consider reshaping this into a single dimension, \"", "\n", "\"using a Tuple action space, or the multi-agent API.\"", ")", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.gradients_fn": [[542, 643], ["ValueError", "policy.model.policy_variables", "list", "policy.model.q_variables", "policy.model.cost_q_variables", "list", "policy._actor_optimizer.compute_gradients", "policy.model.q_variables", "policy.model.cost_q_variables", "policy._alpha_optimizer.compute_gradients", "zip", "tape.gradient", "tape.gradient", "list", "tape.gradient", "tape.gradient", "list", "zip", "policy._critic_optimizer[].compute_gradients", "policy._critic_optimizer[].compute_gradients", "clip_func", "clip_func", "clip_func", "clip_func", "tape.gradient", "len", "list", "list", "zip", "len", "list", "list", "zip", "tape.gradient", "policy.model.policy_variables", "len", "base_q_optimizer.compute_gradients", "twin_q_optimizer.compute_gradients", "len", "base_c_q_optimizer.compute_gradients", "twin_q_optimizer.compute_gradients", "zip", "zip", "tape.gradient", "zip", "zip", "tape.gradient"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.policy_variables", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.q_variables", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.cost_q_variables", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.q_variables", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.cost_q_variables", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.policy_variables"], ["", "", "def", "gradients_fn", "(", "policy", ",", "optimizer", ",", "loss", ")", ":", "\n", "# Eager: Use GradientTape.", "\n", "    ", "if", "policy", ".", "config", "[", "\"framework\"", "]", "in", "[", "\"tf2\"", ",", "\"tfe\"", "]", ":", "\n", "        ", "raise", "ValueError", "(", ")", "\n", "tape", "=", "optimizer", ".", "tape", "\n", "pol_weights", "=", "policy", ".", "model", ".", "policy_variables", "(", ")", "\n", "actor_grads_and_vars", "=", "list", "(", "\n", "zip", "(", "tape", ".", "gradient", "(", "policy", ".", "actor_loss", ",", "pol_weights", ")", ",", "pol_weights", ")", ")", "\n", "q_weights", "=", "policy", ".", "model", ".", "q_variables", "(", ")", "\n", "c_q_weights", "=", "policy", ".", "model", ".", "cost_q_variables", "(", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "            ", "half_cutoff", "=", "len", "(", "q_weights", ")", "//", "2", "\n", "grads_1", "=", "tape", ".", "gradient", "(", "policy", ".", "critic_loss", "[", "0", "]", ",", "\n", "q_weights", "[", ":", "half_cutoff", "]", ")", "\n", "grads_2", "=", "tape", ".", "gradient", "(", "policy", ".", "critic_loss", "[", "1", "]", ",", "\n", "q_weights", "[", "half_cutoff", ":", "]", ")", "\n", "\n", "critic_grads_and_vars", "=", "list", "(", "zip", "(", "grads_1", ",", "q_weights", "[", ":", "half_cutoff", "]", ")", ")", "+", "list", "(", "zip", "(", "grads_2", ",", "q_weights", "[", "half_cutoff", ":", "]", ")", ")", "\n", "\n", "", "else", ":", "\n", "            ", "critic_grads_and_vars", "=", "list", "(", "\n", "zip", "(", "\n", "tape", ".", "gradient", "(", "policy", ".", "critic_loss", "[", "0", "]", ",", "q_weights", ")", ",", "\n", "q_weights", ")", ")", "\n", "\n", "", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "            ", "c_half_cutoff", "=", "len", "(", "c_q_weights", ")", "//", "2", "\n", "grads_3", "=", "tape", ".", "gradient", "(", "policy", ".", "critic_loss", "[", "-", "2", "]", ",", "\n", "c_q_weights", "[", ":", "c_half_cutoff", "]", ")", "\n", "grads_4", "=", "tape", ".", "gradient", "(", "policy", ".", "critic_loss", "[", "-", "1", "]", ",", "\n", "c_q_weights", "[", "c_half_cutoff", ":", "]", ")", "\n", "\n", "c_critic_grads_and_vars", "=", "list", "(", "zip", "(", "grads_3", ",", "c_q_weights", "[", ":", "c_half_cutoff", "]", ")", ")", "+", "list", "(", "zip", "(", "grads_4", ",", "c_q_weights", "[", "c_half_cutoff", ":", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "c_critic_grads_and_vars", "=", "list", "(", "zip", "(", "tape", ".", "gradient", "(", "policy", ".", "critic_loss", "[", "-", "1", "]", ",", "c_q_weights", ")", ",", "c_q_weights", ")", ")", "\n", "\n", "", "alpha_vars", "=", "[", "policy", ".", "model", ".", "log_alpha", "]", "\n", "alpha_grads_and_vars", "=", "list", "(", "\n", "zip", "(", "tape", ".", "gradient", "(", "policy", ".", "alpha_loss", ",", "alpha_vars", ")", ",", "alpha_vars", ")", ")", "\n", "# Tf1.x: Use optimizer.compute_gradients()", "\n", "", "else", ":", "\n", "        ", "actor_grads_and_vars", "=", "policy", ".", "_actor_optimizer", ".", "compute_gradients", "(", "\n", "policy", ".", "actor_loss", ",", "var_list", "=", "policy", ".", "model", ".", "policy_variables", "(", ")", ")", "\n", "\n", "q_weights", "=", "policy", ".", "model", ".", "q_variables", "(", ")", "\n", "c_q_weights", "=", "policy", ".", "model", ".", "cost_q_variables", "(", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "            ", "half_cutoff", "=", "len", "(", "q_weights", ")", "//", "2", "\n", "base_q_optimizer", ",", "twin_q_optimizer", "=", "policy", ".", "_critic_optimizer", "[", "0", ":", "2", "]", "\n", "critic_grads_and_vars", "=", "base_q_optimizer", ".", "compute_gradients", "(", "\n", "policy", ".", "critic_loss", "[", "0", "]", ",", "var_list", "=", "q_weights", "[", ":", "half_cutoff", "]", "\n", ")", "+", "twin_q_optimizer", ".", "compute_gradients", "(", "\n", "policy", ".", "critic_loss", "[", "1", "]", ",", "var_list", "=", "q_weights", "[", "half_cutoff", ":", "]", ")", "\n", "", "else", ":", "\n", "            ", "critic_grads_and_vars", "=", "policy", ".", "_critic_optimizer", "[", "\n", "0", "]", ".", "compute_gradients", "(", "\n", "policy", ".", "critic_loss", "[", "0", "]", ",", "var_list", "=", "q_weights", ")", "\n", "", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "            ", "c_half_cutoff", "=", "len", "(", "c_q_weights", ")", "//", "2", "\n", "base_c_q_optimizer", ",", "twin_c_q_optimizer", "=", "policy", ".", "_critic_optimizer", "[", "-", "2", ":", "]", "\n", "c_critic_grads_and_vars", "=", "base_c_q_optimizer", ".", "compute_gradients", "(", "\n", "policy", ".", "critic_loss", "[", "-", "2", "]", ",", "var_list", "=", "c_q_weights", "[", ":", "c_half_cutoff", "]", "\n", ")", "+", "twin_q_optimizer", ".", "compute_gradients", "(", "\n", "policy", ".", "critic_loss", "[", "-", "1", "]", ",", "var_list", "=", "c_q_weights", "[", "c_half_cutoff", ":", "]", ")", "\n", "", "else", ":", "\n", "            ", "c_critic_grads_and_vars", "=", "policy", ".", "_critic_optimizer", "[", "\n", "-", "1", "]", ".", "compute_gradients", "(", "\n", "policy", ".", "critic_loss", "[", "-", "1", "]", ",", "var_list", "=", "c_q_weights", ")", "\n", "", "alpha_grads_and_vars", "=", "policy", ".", "_alpha_optimizer", ".", "compute_gradients", "(", "\n", "policy", ".", "alpha_loss", ",", "var_list", "=", "[", "policy", ".", "model", ".", "log_alpha", "]", ")", "\n", "\n", "# Clip if necessary.", "\n", "", "if", "policy", ".", "config", "[", "\"grad_clip\"", "]", ":", "\n", "        ", "clip_func", "=", "tf", ".", "clip_by_norm", "\n", "", "else", ":", "\n", "        ", "clip_func", "=", "tf", ".", "identity", "\n", "\n", "# Save grads and vars for later use in `build_apply_op`.", "\n", "", "policy", ".", "_actor_grads_and_vars", "=", "[", "(", "clip_func", "(", "g", ")", ",", "v", ")", "\n", "for", "(", "g", ",", "v", ")", "in", "actor_grads_and_vars", "\n", "if", "g", "is", "not", "None", "]", "\n", "policy", ".", "_critic_grads_and_vars", "=", "[", "(", "clip_func", "(", "g", ")", ",", "v", ")", "\n", "for", "(", "g", ",", "v", ")", "in", "critic_grads_and_vars", "\n", "if", "g", "is", "not", "None", "]", "\n", "# for cost critic", "\n", "policy", ".", "_c_critic_grads_and_vars", "=", "[", "(", "clip_func", "(", "g", ")", ",", "v", ")", "\n", "for", "(", "g", ",", "v", ")", "in", "c_critic_grads_and_vars", "\n", "if", "g", "is", "not", "None", "]", "\n", "\n", "policy", ".", "_alpha_grads_and_vars", "=", "[", "(", "clip_func", "(", "g", ")", ",", "v", ")", "\n", "for", "(", "g", ",", "v", ")", "in", "alpha_grads_and_vars", "\n", "if", "g", "is", "not", "None", "]", "\n", "\n", "grads_and_vars", "=", "(", "\n", "policy", ".", "_actor_grads_and_vars", "+", "policy", ".", "_critic_grads_and_vars", "+", "policy", ".", "_c_critic_grads_and_vars", "+", "\n", "policy", ".", "_alpha_grads_and_vars", ")", "\n", "return", "grads_and_vars", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.apply_gradients": [[645, 675], ["policy._actor_optimizer.apply_gradients", "critic_apply_ops.append", "policy._alpha_optimizer.apply_gradients", "policy._alpha_optimizer.apply_gradients", "tf.group", "len", "policy._critic_optimizer[].apply_gradients", "policy._critic_optimizer[].apply_gradients", "policy._critic_optimizer[].apply_gradients", "len", "policy._critic_optimizer[].apply_gradients", "policy._critic_optimizer[].apply_gradients", "policy._critic_optimizer[].apply_gradients", "tf1.train.get_or_create_global_step"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.apply_gradients", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.apply_gradients", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.apply_gradients", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.apply_gradients", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.apply_gradients", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.apply_gradients", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.apply_gradients", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.apply_gradients", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.apply_gradients"], ["", "def", "apply_gradients", "(", "policy", ",", "optimizer", ",", "grads_and_vars", ")", ":", "\n", "    ", "actor_apply_ops", "=", "policy", ".", "_actor_optimizer", ".", "apply_gradients", "(", "policy", ".", "_actor_grads_and_vars", ")", "\n", "\n", "cgrads", "=", "policy", ".", "_critic_grads_and_vars", "\n", "c_cgrads", "=", "policy", ".", "_c_critic_grads_and_vars", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "        ", "half_cutoff", "=", "len", "(", "cgrads", ")", "//", "2", "\n", "critic_apply_ops", "=", "[", "\n", "policy", ".", "_critic_optimizer", "[", "0", "]", ".", "apply_gradients", "(", "cgrads", "[", ":", "half_cutoff", "]", ")", ",", "\n", "policy", ".", "_critic_optimizer", "[", "1", "]", ".", "apply_gradients", "(", "cgrads", "[", "half_cutoff", ":", "]", ")", "]", "\n", "\n", "", "else", ":", "\n", "        ", "critic_apply_ops", "=", "[", "\n", "policy", ".", "_critic_optimizer", "[", "0", "]", ".", "apply_gradients", "(", "cgrads", ")", "]", "\n", "", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "        ", "c_half_cutoff", "=", "len", "(", "c_cgrads", ")", "//", "2", "\n", "critic_apply_ops", "+=", "[", "policy", ".", "_critic_optimizer", "[", "-", "2", "]", ".", "apply_gradients", "(", "c_cgrads", "[", ":", "c_half_cutoff", "]", ")", ",", "\n", "policy", ".", "_critic_optimizer", "[", "-", "1", "]", ".", "apply_gradients", "(", "c_cgrads", "[", "c_half_cutoff", ":", "]", ")", "]", "\n", "", "else", ":", "\n", "        ", "critic_apply_ops", ".", "append", "(", "policy", ".", "_critic_optimizer", "[", "-", "1", "]", ".", "apply_gradients", "(", "c_cgrads", ")", ")", "\n", "\n", "", "if", "policy", ".", "config", "[", "\"framework\"", "]", "in", "[", "\"tf2\"", ",", "\"tfe\"", "]", ":", "\n", "        ", "policy", ".", "_alpha_optimizer", ".", "apply_gradients", "(", "policy", ".", "_alpha_grads_and_vars", ")", "\n", "return", "\n", "", "else", ":", "\n", "        ", "alpha_apply_ops", "=", "policy", ".", "_alpha_optimizer", ".", "apply_gradients", "(", "\n", "policy", ".", "_alpha_grads_and_vars", ",", "\n", "global_step", "=", "tf1", ".", "train", ".", "get_or_create_global_step", "(", ")", ")", "\n", "\n", "return", "tf", ".", "group", "(", "[", "actor_apply_ops", ",", "alpha_apply_ops", "]", "+", "critic_apply_ops", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.__init__": [[24, 168], ["ray.rllib.models.tf.tf_modelv2.TFModelV2.__init__", "isinstance", "tf.keras.layers.Input", "tf.keras.Sequential", "sac_pid_model.ConstrainedSACModel.action_model", "sac_pid_model.ConstrainedSACModel.register_variables", "sac_pid_model.ConstrainedSACModel.__init__.build_q_net"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "obs_space", ",", "\n", "action_space", ",", "\n", "num_outputs", ",", "\n", "model_config", ",", "\n", "name", ",", "\n", "actor_hidden_activation", "=", "\"relu\"", ",", "\n", "actor_hiddens", "=", "(", "256", ",", "256", ")", ",", "\n", "critic_hidden_activation", "=", "\"relu\"", ",", "\n", "critic_hiddens", "=", "(", "256", ",", "256", ")", ",", "\n", "twin_q", "=", "False", ",", "\n", "twin_cost_q", "=", "False", ",", "\n", "initial_alpha", "=", "1.0", ",", "\n", "target_entropy", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"Initialize variables of this model.\n\n        Extra model kwargs:\n            actor_hidden_activation (str): activation for actor network\n            actor_hiddens (list): hidden layers sizes for actor network\n            critic_hidden_activation (str): activation for critic network\n            critic_hiddens (list): hidden layers sizes for critic network\n            twin_q (bool): build twin Q networks.\n            initial_alpha (float): The initial value for the to-be-optimized\n                alpha parameter (default: 1.0).\n\n        Note that the core layers for forward() are not defined here, this\n        only defines the layers for the output heads. Those layers for\n        forward() should be defined in subclasses of SACModel.\n        \"\"\"", "\n", "super", "(", "ConstrainedSACModel", ",", "self", ")", ".", "__init__", "(", "\n", "obs_space", ",", "action_space", ",", "num_outputs", ",", "model_config", ",", "name", "\n", ")", "\n", "self", ".", "discrete", "=", "False", "\n", "if", "isinstance", "(", "action_space", ",", "Discrete", ")", ":", "\n", "            ", "self", ".", "action_dim", "=", "action_space", ".", "n", "\n", "self", ".", "discrete", "=", "True", "\n", "action_outs", "=", "q_outs", "=", "self", ".", "action_dim", "\n", "", "else", ":", "\n", "            ", "self", ".", "action_dim", "=", "np", ".", "product", "(", "action_space", ".", "shape", ")", "\n", "action_outs", "=", "2", "*", "self", ".", "action_dim", "\n", "q_outs", "=", "1", "\n", "\n", "", "self", ".", "model_out", "=", "tf", ".", "keras", ".", "layers", ".", "Input", "(", "\n", "shape", "=", "(", "self", ".", "num_outputs", ",", ")", ",", "name", "=", "\"model_out\"", "\n", ")", "\n", "self", ".", "action_model", "=", "tf", ".", "keras", ".", "Sequential", "(", "\n", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "units", "=", "hidden", ",", "\n", "activation", "=", "getattr", "(", "tf", ".", "nn", ",", "actor_hidden_activation", ",", "None", ")", ",", "\n", "name", "=", "\"action_{}\"", ".", "format", "(", "i", "+", "1", ")", "\n", ")", "for", "i", ",", "hidden", "in", "enumerate", "(", "actor_hiddens", ")", "\n", "]", "+", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "\n", "Dense", "(", "units", "=", "action_outs", ",", "activation", "=", "None", ",", "name", "=", "\"action_out\"", ")", "\n", "]", "\n", ")", "\n", "self", ".", "shift_and_log_scale_diag", "=", "self", ".", "action_model", "(", "self", ".", "model_out", ")", "\n", "\n", "self", ".", "register_variables", "(", "self", ".", "action_model", ".", "variables", ")", "\n", "\n", "self", ".", "actions_input", "=", "None", "\n", "if", "not", "self", ".", "discrete", ":", "\n", "            ", "self", ".", "actions_input", "=", "tf", ".", "keras", ".", "layers", ".", "Input", "(", "\n", "shape", "=", "(", "self", ".", "action_dim", ",", ")", ",", "name", "=", "\"actions\"", "\n", ")", "\n", "\n", "", "def", "build_q_net", "(", "name", ",", "observations", ",", "actions", ")", ":", "\n", "# For continuous actions: Feed obs and actions (concatenated)", "\n", "# through the NN. For discrete actions, only obs.", "\n", "            ", "q_net", "=", "tf", ".", "keras", ".", "Sequential", "(", "\n", "(", "\n", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Concatenate", "(", "axis", "=", "1", ")", ",", "\n", "]", "if", "not", "self", ".", "discrete", "else", "[", "]", "\n", ")", "+", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "units", "=", "units", ",", "\n", "activation", "=", "getattr", "(", "\n", "tf", ".", "nn", ",", "critic_hidden_activation", ",", "None", "\n", ")", ",", "\n", "kernel_initializer", "=", "normc_initializer", "(", "1.0", ")", ",", "\n", "name", "=", "\"{}_hidden_{}\"", ".", "format", "(", "name", ",", "i", ")", "\n", ")", "for", "i", ",", "units", "in", "enumerate", "(", "critic_hiddens", ")", "\n", "]", "+", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "units", "=", "q_outs", ",", "\n", "activation", "=", "None", ",", "\n", "kernel_initializer", "=", "normc_initializer", "(", "1.0", ")", ",", "\n", "name", "=", "\"{}_out\"", ".", "format", "(", "name", ")", "\n", ")", "\n", "]", "\n", ")", "\n", "\n", "if", "self", ".", "discrete", ":", "\n", "                ", "q_net", "=", "tf", ".", "keras", ".", "Model", "(", "observations", ",", "q_net", "(", "observations", ")", ")", "\n", "", "else", ":", "\n", "                ", "q_net", "=", "tf", ".", "keras", ".", "Model", "(", "\n", "[", "observations", ",", "actions", "]", ",", "q_net", "(", "[", "observations", ",", "actions", "]", ")", "\n", ")", "\n", "", "return", "q_net", "\n", "\n", "", "self", ".", "q_net", "=", "build_q_net", "(", "\"q\"", ",", "self", ".", "model_out", ",", "self", ".", "actions_input", ")", "\n", "self", ".", "cost_q_net", "=", "build_q_net", "(", "\n", "\"cost_q\"", ",", "self", ".", "model_out", ",", "self", ".", "actions_input", "\n", ")", "\n", "self", ".", "register_variables", "(", "self", ".", "q_net", ".", "variables", ")", "\n", "self", ".", "register_variables", "(", "self", ".", "cost_q_net", ".", "variables", ")", "\n", "\n", "if", "twin_q", ":", "\n", "            ", "self", ".", "twin_q_net", "=", "build_q_net", "(", "\n", "\"twin_q\"", ",", "self", ".", "model_out", ",", "self", ".", "actions_input", "\n", ")", "\n", "self", ".", "register_variables", "(", "self", ".", "twin_q_net", ".", "variables", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "twin_q_net", "=", "None", "\n", "", "if", "twin_cost_q", ":", "\n", "            ", "self", ".", "cost_twin_q_net", "=", "build_q_net", "(", "\n", "\"cost_twin_q\"", ",", "self", ".", "model_out", ",", "self", ".", "actions_input", "\n", ")", "\n", "self", ".", "register_variables", "(", "self", ".", "cost_twin_q_net", ".", "variables", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "cost_twin_q_net", "=", "None", "\n", "\n", "", "self", ".", "log_alpha", "=", "tf", ".", "Variable", "(", "\n", "np", ".", "log", "(", "initial_alpha", ")", ",", "dtype", "=", "tf", ".", "float32", ",", "name", "=", "\"log_alpha\"", "\n", ")", "\n", "self", ".", "alpha", "=", "tf", ".", "exp", "(", "self", ".", "log_alpha", ")", "\n", "\n", "# Auto-calculate the target entropy.", "\n", "if", "target_entropy", "is", "None", "or", "target_entropy", "==", "\"auto\"", ":", "\n", "# See hyperparams in [2] (README.md).", "\n", "            ", "if", "self", ".", "discrete", ":", "\n", "                ", "target_entropy", "=", "0.98", "*", "np", ".", "array", "(", "\n", "-", "np", ".", "log", "(", "1.0", "/", "action_space", ".", "n", ")", ",", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "# See [1] (README.md).", "\n", "", "else", ":", "\n", "                ", "target_entropy", "=", "-", "np", ".", "prod", "(", "action_space", ".", "shape", ")", "\n", "", "", "self", ".", "target_entropy", "=", "target_entropy", "\n", "\n", "self", ".", "register_variables", "(", "[", "self", ".", "log_alpha", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values": [[169, 188], ["sac_pid_model.ConstrainedSACModel.q_net", "sac_pid_model.ConstrainedSACModel.q_net"], "methods", ["None"], ["", "def", "get_q_values", "(", "self", ",", "model_out", ",", "actions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Return the Q estimates for the most recent forward pass.\n\n        This implements Q(s, a).\n\n        Arguments:\n            model_out (Tensor): obs embeddings from the model layers, of shape\n                [BATCH_SIZE, num_outputs].\n            actions (Optional[Tensor]): Actions to return the Q-values for.\n                Shape: [BATCH_SIZE, action_dim]. If None (discrete action\n                case), return Q-values for all actions.\n\n        Returns:\n            tensor of shape [BATCH_SIZE].\n        \"\"\"", "\n", "if", "actions", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "q_net", "(", "[", "model_out", ",", "actions", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "q_net", "(", "model_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_cost_q_values": [[189, 194], ["sac_pid_model.ConstrainedSACModel.cost_q_net", "sac_pid_model.ConstrainedSACModel.cost_q_net"], "methods", ["None"], ["", "", "def", "get_cost_q_values", "(", "self", ",", "model_out", ",", "actions", "=", "None", ")", ":", "\n", "        ", "if", "actions", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "cost_q_net", "(", "[", "model_out", ",", "actions", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "cost_q_net", "(", "model_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_cost_q_values": [[195, 200], ["sac_pid_model.ConstrainedSACModel.cost_twin_q_net", "sac_pid_model.ConstrainedSACModel.cost_twin_q_net"], "methods", ["None"], ["", "", "def", "get_twin_cost_q_values", "(", "self", ",", "model_out", ",", "actions", "=", "None", ")", ":", "\n", "        ", "if", "actions", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "cost_twin_q_net", "(", "[", "model_out", ",", "actions", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "cost_twin_q_net", "(", "model_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values": [[201, 220], ["sac_pid_model.ConstrainedSACModel.twin_q_net", "sac_pid_model.ConstrainedSACModel.twin_q_net"], "methods", ["None"], ["", "", "def", "get_twin_q_values", "(", "self", ",", "model_out", ",", "actions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Same as get_q_values but using the twin Q net.\n\n        This implements the twin Q(s, a).\n\n        Arguments:\n            model_out (Tensor): obs embeddings from the model layers, of shape\n                [BATCH_SIZE, num_outputs].\n            actions (Optional[Tensor]): Actions to return the Q-values for.\n                Shape: [BATCH_SIZE, action_dim]. If None (discrete action\n                case), return Q-values for all actions.\n\n        Returns:\n            tensor of shape [BATCH_SIZE].\n        \"\"\"", "\n", "if", "actions", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "twin_q_net", "(", "[", "model_out", ",", "actions", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "twin_q_net", "(", "model_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_policy_output": [[221, 235], ["sac_pid_model.ConstrainedSACModel.action_model"], "methods", ["None"], ["", "", "def", "get_policy_output", "(", "self", ",", "model_out", ")", ":", "\n", "        ", "\"\"\"Return the action output for the most recent forward pass.\n\n        This outputs the support for pi(s). For continuous action spaces, this\n        is the action directly. For discrete, is is the mean / std dev.\n\n        Arguments:\n            model_out (Tensor): obs embeddings from the model layers, of shape\n                [BATCH_SIZE, num_outputs].\n\n        Returns:\n            tensor of shape [BATCH_SIZE, action_out_size]\n        \"\"\"", "\n", "return", "self", ".", "action_model", "(", "model_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.policy_variables": [[236, 240], ["list"], "methods", ["None"], ["", "def", "policy_variables", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the list of variables for the policy net.\"\"\"", "\n", "\n", "return", "list", "(", "self", ".", "action_model", ".", "variables", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.q_variables": [[241, 246], ["None"], "methods", ["None"], ["", "def", "q_variables", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the list of variables for Q / twin Q nets.\"\"\"", "\n", "\n", "return", "self", ".", "q_net", ".", "variables", "+", "(", "\n", "self", ".", "twin_q_net", ".", "variables", "if", "self", ".", "twin_q_net", "else", "[", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.cost_q_variables": [[248, 252], ["None"], "methods", ["None"], ["", "def", "cost_q_variables", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cost_q_net", ".", "variables", "+", "(", "\n", "self", ".", "cost_twin_q_net", ".", "variables", "\n", "if", "self", ".", "cost_twin_q_net", "else", "[", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid.get_policy_class": [[32, 37], ["None"], "function", ["None"], ["def", "get_policy_class", "(", "config", ")", ":", "\n", "    ", "if", "config", "[", "\"framework\"", "]", "==", "\"torch\"", ":", "\n", "        ", "raise", "ValueError", "\n", "", "else", ":", "\n", "        ", "return", "SACPIDPolicy", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid.validate_config": [[39, 48], ["config[].get", "logger.warning", "ValueError"], "function", ["None"], ["", "", "def", "validate_config", "(", "config", ")", ":", "\n", "    ", "if", "config", "[", "\"model\"", "]", ".", "get", "(", "\"custom_model\"", ")", ":", "\n", "        ", "logger", ".", "warning", "(", "\n", "\"Setting use_state_preprocessor=True since a custom model \"", "\n", "\"was specified.\"", ")", "\n", "config", "[", "\"use_state_preprocessor\"", "]", "=", "True", "\n", "\n", "", "if", "config", "[", "\"grad_clip\"", "]", "is", "not", "None", "and", "config", "[", "\"grad_clip\"", "]", "<=", "0.0", ":", "\n", "        ", "raise", "ValueError", "(", "\"`grad_clip` value must be > 0.0!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid.execution_plan": [[50, 116], ["config.get", "ray.rllib.execution.replay_buffer.LocalReplayBuffer", "ray.rllib.execution.rollout_ops.ParallelRollouts", "rollouts.for_each.for_each", "rollouts.for_each.for_each", "ray.rllib.execution.replay_ops.Replay().for_each().for_each().for_each().for_each", "ray.rllib.execution.concurrency_ops.Concurrently", "ray.rllib.execution.metric_ops.StandardMetricsReporting", "egpo_utils.sac_pid.sac_pid_policy.UpdatePenalty", "ray.rllib.execution.replay_ops.StoreToReplayBuffer", "config.get", "config.get", "ray.rllib.execution.train_ops.UpdateTargetNetwork", "info_dict.items", "ray.rllib.execution.replay_buffer.LocalReplayBuffer.update_priorities", "ray.rllib.execution.replay_ops.Replay().for_each().for_each().for_each", "ray.rllib.agents.dqn.dqn.calculate_rr_weights", "info.get", "info[].get", "samples.policy_batches[].data.get", "ray.rllib.execution.replay_ops.Replay().for_each().for_each", "ray.rllib.execution.train_ops.TrainOneStep", "ray.rllib.execution.replay_ops.Replay().for_each", "ray.rllib.execution.replay_ops.Replay", "post_fn"], "function", ["None"], ["", "", "def", "execution_plan", "(", "workers", ":", "WorkerSet", ",", "\n", "config", ":", "TrainerConfigDict", ")", "->", "LocalIterator", "[", "dict", "]", ":", "\n", "    ", "if", "config", ".", "get", "(", "\"prioritized_replay\"", ")", ":", "\n", "        ", "prio_args", "=", "{", "\n", "\"prioritized_replay_alpha\"", ":", "config", "[", "\"prioritized_replay_alpha\"", "]", ",", "\n", "\"prioritized_replay_beta\"", ":", "config", "[", "\"prioritized_replay_beta\"", "]", ",", "\n", "\"prioritized_replay_eps\"", ":", "config", "[", "\"prioritized_replay_eps\"", "]", ",", "\n", "}", "\n", "", "else", ":", "\n", "        ", "prio_args", "=", "{", "}", "\n", "\n", "", "local_replay_buffer", "=", "LocalReplayBuffer", "(", "\n", "num_shards", "=", "1", ",", "\n", "learning_starts", "=", "config", "[", "\"learning_starts\"", "]", ",", "\n", "buffer_size", "=", "config", "[", "\"buffer_size\"", "]", ",", "\n", "replay_batch_size", "=", "config", "[", "\"train_batch_size\"", "]", ",", "\n", "replay_mode", "=", "config", "[", "\"multiagent\"", "]", "[", "\"replay_mode\"", "]", ",", "\n", "replay_sequence_length", "=", "config", "[", "\"replay_sequence_length\"", "]", ",", "\n", "**", "prio_args", ")", "\n", "\n", "rollouts", "=", "ParallelRollouts", "(", "workers", ",", "mode", "=", "\"bulk_sync\"", ")", "\n", "\n", "# Update penalty", "\n", "rollouts", "=", "rollouts", ".", "for_each", "(", "UpdatePenalty", "(", "workers", ")", ")", "\n", "\n", "# We execute the following steps concurrently:", "\n", "# (1) Generate rollouts and store them in our local replay buffer. Calling", "\n", "# next() on store_op drives this.", "\n", "store_op", "=", "rollouts", ".", "for_each", "(", "StoreToReplayBuffer", "(", "local_buffer", "=", "local_replay_buffer", ")", ")", "\n", "\n", "def", "update_prio", "(", "item", ")", ":", "\n", "        ", "samples", ",", "info_dict", "=", "item", "\n", "if", "config", ".", "get", "(", "\"prioritized_replay\"", ")", ":", "\n", "            ", "prio_dict", "=", "{", "}", "\n", "for", "policy_id", ",", "info", "in", "info_dict", ".", "items", "(", ")", ":", "\n", "# TODO(sven): This is currently structured differently for", "\n", "#  torch/tf. Clean up these results/info dicts across", "\n", "#  policies (note: fixing this in torch_policy.py will", "\n", "#  break e.g. DDPPO!).", "\n", "                ", "td_error", "=", "info", ".", "get", "(", "\"td_error\"", ",", "\n", "info", "[", "LEARNER_STATS_KEY", "]", ".", "get", "(", "\"td_error\"", ")", ")", "\n", "prio_dict", "[", "policy_id", "]", "=", "(", "samples", ".", "policy_batches", "[", "policy_id", "]", "\n", ".", "data", ".", "get", "(", "\"batch_indexes\"", ")", ",", "td_error", ")", "\n", "", "local_replay_buffer", ".", "update_priorities", "(", "prio_dict", ")", "\n", "", "return", "info_dict", "\n", "\n", "# (2) Read and train on experiences from the replay buffer. Every batch", "\n", "# returned from the LocalReplay() iterator is passed to TrainOneStep to", "\n", "# take a SGD step, and then we decide whether to update the target network.", "\n", "", "post_fn", "=", "config", ".", "get", "(", "\"before_learn_on_batch\"", ")", "or", "(", "lambda", "b", ",", "*", "a", ":", "b", ")", "\n", "replay_op", "=", "Replay", "(", "local_buffer", "=", "local_replay_buffer", ")", ".", "for_each", "(", "lambda", "x", ":", "post_fn", "(", "x", ",", "workers", ",", "config", ")", ")", ".", "for_each", "(", "TrainOneStep", "(", "workers", ")", ")", ".", "for_each", "(", "update_prio", ")", ".", "for_each", "(", "UpdateTargetNetwork", "(", "\n", "workers", ",", "config", "[", "\"target_network_update_freq\"", "]", ")", ")", "\n", "\n", "# Alternate deterministically between (1) and (2). Only return the output", "\n", "# of (2) since training metrics are not available until (2) runs.", "\n", "train_op", "=", "Concurrently", "(", "\n", "[", "store_op", ",", "replay_op", "]", ",", "\n", "mode", "=", "\"round_robin\"", ",", "\n", "output_indexes", "=", "[", "1", "]", ",", "\n", "round_robin_weights", "=", "calculate_rr_weights", "(", "config", ")", ")", "\n", "\n", "return", "StandardMetricsReporting", "(", "train_op", ",", "workers", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.policy_actions_repeat": [[34, 41], ["obs.unsqueeze().repeat().view", "action_dist", "action_dist.sample", "torch.unsqueeze", "model.get_policy_output", "action_dist.logp", "torch.unsqueeze.squeeze", "obs.unsqueeze().repeat", "obs.unsqueeze"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_policy_output"], ["def", "policy_actions_repeat", "(", "model", ",", "action_dist", ",", "obs", ",", "num_repeat", "=", "1", ")", ":", "\n", "    ", "obs_temp", "=", "obs", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "num_repeat", ",", "1", ")", ".", "view", "(", "\n", "obs", ".", "shape", "[", "0", "]", "*", "num_repeat", ",", "obs", ".", "shape", "[", "1", "]", ")", "\n", "policy_dist", "=", "action_dist", "(", "model", ".", "get_policy_output", "(", "obs_temp", ")", ",", "model", ")", "\n", "actions", "=", "policy_dist", ".", "sample", "(", ")", "\n", "log_p", "=", "torch", ".", "unsqueeze", "(", "policy_dist", ".", "logp", "(", "actions", ")", ",", "-", "1", ")", "\n", "return", "actions", ",", "log_p", ".", "squeeze", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.q_values_repeat": [[43, 55], ["int", "obs.unsqueeze().repeat().view", "model.get_twin_q_values.view", "model.get_q_values", "model.get_twin_q_values", "obs.unsqueeze().repeat", "obs.unsqueeze"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values"], ["", "def", "q_values_repeat", "(", "model", ",", "obs", ",", "actions", ",", "twin", "=", "False", ")", ":", "\n", "    ", "action_shape", "=", "actions", ".", "shape", "[", "0", "]", "\n", "obs_shape", "=", "obs", ".", "shape", "[", "0", "]", "\n", "num_repeat", "=", "int", "(", "action_shape", "/", "obs_shape", ")", "\n", "obs_temp", "=", "obs", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "num_repeat", ",", "1", ")", ".", "view", "(", "\n", "obs", ".", "shape", "[", "0", "]", "*", "num_repeat", ",", "obs", ".", "shape", "[", "1", "]", ")", "\n", "if", "twin", ":", "\n", "        ", "preds", "=", "model", ".", "get_q_values", "(", "obs_temp", ",", "actions", ")", "\n", "", "else", ":", "\n", "        ", "preds", "=", "model", ".", "get_twin_q_values", "(", "obs_temp", ",", "actions", ")", "\n", "", "preds", "=", "preds", ".", "view", "(", "obs", ".", "shape", "[", "0", "]", ",", "num_repeat", ",", "1", ")", "\n", "return", "preds", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.cql_loss": [[57, 245], ["print", "model", "model", "policy.target_model", "ray.rllib.agents.sac.sac_torch_policy._get_dist_class", "ray.rllib.agents.sac.sac_torch_policy._get_dist_class.", "torch.unsqueeze", "torch.exp", "ray.rllib.agents.sac.sac_torch_policy._get_dist_class.", "model.get_q_values", "torch.squeeze", "policy.target_model.get_q_values", "torch.squeeze", "torch.abs", "ray.rllib.utils.torch_ops.convert_to_torch_tensor", "cql_torch_policy.policy_actions_repeat", "cql_torch_policy.policy_actions_repeat", "curr_logp.view.view", "next_logp.view.view", "cql_torch_policy.q_values_repeat", "cql_torch_policy.q_values_repeat", "cql_torch_policy.q_values_repeat", "numpy.log", "torch.cat", "tuple", "model.get_policy_output", "action_dist_class.sample", "action_dist_class.deterministic_sample", "action_dist_class.logp", "model.get_q_values", "action_dist_class.logp", "model.get_policy_output", "action_dist_class.sample", "action_dist_class.deterministic_sample", "model.get_twin_q_values", "torch.squeeze", "policy.target_model.get_twin_q_values", "torch.min", "torch.abs", "critic_loss.append", "torch.FloatTensor().uniform_", "cql_torch_policy.q_values_repeat", "cql_torch_policy.q_values_repeat", "cql_torch_policy.q_values_repeat", "torch.cat", "cql_loss.append", "tuple", "model.get_twin_q_values", "torch.min", "terminals.float", "nn.MSELoss", "torch.logsumexp().mean", "torch.squeeze.mean", "torch.clamp", "nn.MSELoss", "torch.FloatTensor", "next_logp.view.detach", "curr_logp.view.detach", "torch.logsumexp().mean", "torch.squeeze.mean", "model.log_alpha_prime.exp", "next_logp.view.detach", "curr_logp.view.detach", "torch.logsumexp", "torch.exp.detach", "torch.logsumexp"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.policy_actions_repeat", "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.policy_actions_repeat", "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.q_values_repeat", "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.q_values_repeat", "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.q_values_repeat", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_policy_output", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_policy_output", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.q_values_repeat", "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.q_values_repeat", "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.q_values_repeat", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values"], ["", "def", "cql_loss", "(", "policy", ":", "Policy", ",", "model", ":", "ModelV2", ",", "\n", "dist_class", ":", "Type", "[", "TorchDistributionWrapper", "]", ",", "\n", "train_batch", ":", "SampleBatch", ")", "->", "Union", "[", "TensorType", ",", "List", "[", "TensorType", "]", "]", ":", "\n", "    ", "print", "(", "\"current_iter:{}\"", ".", "format", "(", "policy", ".", "cur_iter", ")", ")", "\n", "policy", ".", "cur_iter", "+=", "1", "\n", "# For best performance, turn deterministic off", "\n", "deterministic", "=", "policy", ".", "config", "[", "\"_deterministic_loss\"", "]", "\n", "twin_q", "=", "policy", ".", "config", "[", "\"twin_q\"", "]", "\n", "discount", "=", "policy", ".", "config", "[", "\"gamma\"", "]", "\n", "action_low", "=", "model", ".", "action_space", ".", "low", "[", "0", "]", "\n", "action_high", "=", "model", ".", "action_space", ".", "high", "[", "0", "]", "\n", "\n", "# CQL Parameters", "\n", "bc_iters", "=", "policy", ".", "config", "[", "\"bc_iters\"", "]", "\n", "cql_temp", "=", "policy", ".", "config", "[", "\"temperature\"", "]", "\n", "num_actions", "=", "policy", ".", "config", "[", "\"num_actions\"", "]", "\n", "min_q_weight", "=", "policy", ".", "config", "[", "\"min_q_weight\"", "]", "\n", "use_lagrange", "=", "policy", ".", "config", "[", "\"lagrangian\"", "]", "\n", "target_action_gap", "=", "policy", ".", "config", "[", "\"lagrangian_thresh\"", "]", "\n", "\n", "obs", "=", "train_batch", "[", "SampleBatch", ".", "CUR_OBS", "]", "\n", "actions", "=", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", "\n", "rewards", "=", "train_batch", "[", "SampleBatch", ".", "REWARDS", "]", "\n", "next_obs", "=", "train_batch", "[", "SampleBatch", ".", "NEXT_OBS", "]", "\n", "terminals", "=", "train_batch", "[", "SampleBatch", ".", "DONES", "]", "\n", "\n", "model_out_t", ",", "_", "=", "model", "(", "{", "\n", "\"obs\"", ":", "obs", ",", "\n", "\"is_training\"", ":", "True", ",", "\n", "}", ",", "[", "]", ",", "None", ")", "\n", "\n", "model_out_tp1", ",", "_", "=", "model", "(", "{", "\n", "\"obs\"", ":", "next_obs", ",", "\n", "\"is_training\"", ":", "True", ",", "\n", "}", ",", "[", "]", ",", "None", ")", "\n", "\n", "target_model_out_tp1", ",", "_", "=", "policy", ".", "target_model", "(", "{", "\n", "\"obs\"", ":", "next_obs", ",", "\n", "\"is_training\"", ":", "True", ",", "\n", "}", ",", "[", "]", ",", "None", ")", "\n", "\n", "action_dist_class", "=", "_get_dist_class", "(", "policy", ".", "config", ",", "policy", ".", "action_space", ")", "\n", "action_dist_t", "=", "action_dist_class", "(", "\n", "model", ".", "get_policy_output", "(", "model_out_t", ")", ",", "policy", ".", "model", ")", "\n", "policy_t", "=", "action_dist_t", ".", "sample", "(", ")", "if", "not", "deterministic", "else", "action_dist_t", ".", "deterministic_sample", "(", ")", "\n", "log_pis_t", "=", "torch", ".", "unsqueeze", "(", "action_dist_t", ".", "logp", "(", "policy_t", ")", ",", "-", "1", ")", "\n", "\n", "# Unlike original SAC, Alpha and Actor Loss are computed first.", "\n", "# Alpha Loss", "\n", "alpha_loss", "=", "-", "(", "model", ".", "log_alpha", "*", "\n", "(", "log_pis_t", "+", "model", ".", "target_entropy", ")", ".", "detach", "(", ")", ")", ".", "mean", "(", ")", "\n", "\n", "# Policy Loss (Either Behavior Clone Loss or SAC Loss)", "\n", "alpha", "=", "torch", ".", "exp", "(", "model", ".", "log_alpha", ")", "\n", "if", "policy", ".", "cur_iter", ">=", "bc_iters", ":", "\n", "        ", "min_q", "=", "model", ".", "get_q_values", "(", "model_out_t", ",", "policy_t", ")", "\n", "if", "twin_q", ":", "\n", "            ", "twin_q", "=", "model", ".", "get_twin_q_values", "(", "model_out_t", ",", "policy_t", ")", "\n", "min_q", "=", "torch", ".", "min", "(", "min_q", ",", "twin_q", ")", "\n", "", "actor_loss", "=", "(", "alpha", ".", "detach", "(", ")", "*", "log_pis_t", "-", "min_q", ")", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "        ", "bc_logp", "=", "action_dist_t", ".", "logp", "(", "actions", ")", "\n", "actor_loss", "=", "(", "alpha", "*", "log_pis_t", "-", "bc_logp", ")", ".", "mean", "(", ")", "\n", "", "twin_q", "=", "policy", ".", "config", "[", "\"twin_q\"", "]", "\n", "# Critic Loss (Standard SAC Critic L2 Loss + CQL Entropy Loss)", "\n", "# SAC Loss", "\n", "action_dist_tp1", "=", "action_dist_class", "(", "\n", "model", ".", "get_policy_output", "(", "model_out_tp1", ")", ",", "policy", ".", "model", ")", "\n", "policy_tp1", "=", "action_dist_tp1", ".", "sample", "(", ")", "if", "not", "deterministic", "else", "action_dist_tp1", ".", "deterministic_sample", "(", ")", "\n", "\n", "# Q-values for the batched actions.", "\n", "q_t", "=", "model", ".", "get_q_values", "(", "model_out_t", ",", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ")", "\n", "q_t", "=", "torch", ".", "squeeze", "(", "q_t", ",", "dim", "=", "-", "1", ")", "\n", "if", "twin_q", ":", "\n", "        ", "twin_q_t", "=", "model", ".", "get_twin_q_values", "(", "model_out_t", ",", "\n", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ")", "\n", "twin_q_t", "=", "torch", ".", "squeeze", "(", "twin_q_t", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Target q network evaluation.", "\n", "", "q_tp1", "=", "policy", ".", "target_model", ".", "get_q_values", "(", "target_model_out_tp1", ",", "policy_tp1", ")", "\n", "if", "twin_q", ":", "\n", "        ", "twin_q_tp1", "=", "policy", ".", "target_model", ".", "get_twin_q_values", "(", "\n", "target_model_out_tp1", ",", "policy_tp1", ")", "\n", "# Take min over both twin-NNs.", "\n", "q_tp1", "=", "torch", ".", "min", "(", "q_tp1", ",", "twin_q_tp1", ")", "\n", "", "q_tp1", "=", "torch", ".", "squeeze", "(", "input", "=", "q_tp1", ",", "dim", "=", "-", "1", ")", "\n", "q_tp1", "=", "(", "1.0", "-", "terminals", ".", "float", "(", ")", ")", "*", "q_tp1", "\n", "\n", "# compute RHS of bellman equation", "\n", "q_t_target", "=", "(", "\n", "rewards", "+", "(", "discount", "**", "policy", ".", "config", "[", "\"n_step\"", "]", ")", "*", "q_tp1", ")", ".", "detach", "(", ")", "\n", "\n", "# Compute the TD-error (potentially clipped), for priority replay buffer", "\n", "base_td_error", "=", "torch", ".", "abs", "(", "q_t", "-", "q_t_target", ")", "\n", "if", "twin_q", ":", "\n", "        ", "twin_td_error", "=", "torch", ".", "abs", "(", "twin_q_t", "-", "q_t_target", ")", "\n", "td_error", "=", "0.5", "*", "(", "base_td_error", "+", "twin_td_error", ")", "\n", "", "else", ":", "\n", "        ", "td_error", "=", "base_td_error", "\n", "", "critic_loss", "=", "[", "nn", ".", "MSELoss", "(", ")", "(", "q_t", ",", "q_t_target", ")", "]", "\n", "if", "twin_q", ":", "\n", "        ", "critic_loss", ".", "append", "(", "nn", ".", "MSELoss", "(", ")", "(", "twin_q_t", ",", "q_t_target", ")", ")", "\n", "\n", "# CQL Loss (We are using Entropy version of CQL (the best version))", "\n", "", "rand_actions", "=", "convert_to_torch_tensor", "(", "\n", "torch", ".", "FloatTensor", "(", "actions", ".", "shape", "[", "0", "]", "*", "num_actions", ",", "\n", "actions", ".", "shape", "[", "-", "1", "]", ")", ".", "uniform_", "(", "action_low", ",", "action_high", ")", ")", "\n", "curr_actions", ",", "curr_logp", "=", "policy_actions_repeat", "(", "model", ",", "action_dist_class", ",", "\n", "obs", ",", "num_actions", ")", "\n", "next_actions", ",", "next_logp", "=", "policy_actions_repeat", "(", "model", ",", "action_dist_class", ",", "\n", "next_obs", ",", "num_actions", ")", "\n", "curr_logp", "=", "curr_logp", ".", "view", "(", "actions", ".", "shape", "[", "0", "]", ",", "num_actions", ",", "1", ")", "\n", "next_logp", "=", "next_logp", ".", "view", "(", "actions", ".", "shape", "[", "0", "]", ",", "num_actions", ",", "1", ")", "\n", "\n", "q1_rand", "=", "q_values_repeat", "(", "model", ",", "model_out_t", ",", "rand_actions", ")", "\n", "q1_curr_actions", "=", "q_values_repeat", "(", "model", ",", "model_out_t", ",", "curr_actions", ")", "\n", "q1_next_actions", "=", "q_values_repeat", "(", "model", ",", "model_out_t", ",", "next_actions", ")", "\n", "\n", "if", "twin_q", ":", "\n", "        ", "q2_rand", "=", "q_values_repeat", "(", "model", ",", "model_out_t", ",", "rand_actions", ",", "twin", "=", "True", ")", "\n", "q2_curr_actions", "=", "q_values_repeat", "(", "\n", "model", ",", "model_out_t", ",", "curr_actions", ",", "twin", "=", "True", ")", "\n", "q2_next_actions", "=", "q_values_repeat", "(", "\n", "model", ",", "model_out_t", ",", "next_actions", ",", "twin", "=", "True", ")", "\n", "\n", "", "random_density", "=", "np", ".", "log", "(", "0.5", "**", "curr_actions", ".", "shape", "[", "-", "1", "]", ")", "\n", "cat_q1", "=", "torch", ".", "cat", "(", "[", "\n", "q1_rand", "-", "random_density", ",", "q1_next_actions", "-", "next_logp", ".", "detach", "(", ")", ",", "\n", "q1_curr_actions", "-", "curr_logp", ".", "detach", "(", ")", "\n", "]", ",", "1", ")", "\n", "if", "twin_q", ":", "\n", "        ", "cat_q2", "=", "torch", ".", "cat", "(", "[", "\n", "q2_rand", "-", "random_density", ",", "q2_next_actions", "-", "next_logp", ".", "detach", "(", ")", ",", "\n", "q2_curr_actions", "-", "curr_logp", ".", "detach", "(", ")", "\n", "]", ",", "1", ")", "\n", "\n", "", "min_qf1_loss", "=", "torch", ".", "logsumexp", "(", "\n", "cat_q1", "/", "cql_temp", ",", "dim", "=", "1", ")", ".", "mean", "(", ")", "*", "min_q_weight", "*", "cql_temp", "\n", "min_qf1_loss", "=", "min_qf1_loss", "-", "q_t", ".", "mean", "(", ")", "*", "min_q_weight", "\n", "if", "twin_q", ":", "\n", "        ", "min_qf2_loss", "=", "torch", ".", "logsumexp", "(", "\n", "cat_q2", "/", "cql_temp", ",", "dim", "=", "1", ")", ".", "mean", "(", ")", "*", "min_q_weight", "*", "cql_temp", "\n", "min_qf2_loss", "=", "min_qf2_loss", "-", "twin_q_t", ".", "mean", "(", ")", "*", "min_q_weight", "\n", "\n", "", "if", "use_lagrange", ":", "\n", "        ", "alpha_prime", "=", "torch", ".", "clamp", "(", "\n", "model", ".", "log_alpha_prime", ".", "exp", "(", ")", ",", "min", "=", "0.0", ",", "max", "=", "1000000.0", ")", "[", "0", "]", "\n", "min_qf1_loss", "=", "alpha_prime", "*", "(", "min_qf1_loss", "-", "target_action_gap", ")", "\n", "if", "twin_q", ":", "\n", "            ", "min_qf2_loss", "=", "alpha_prime", "*", "(", "min_qf2_loss", "-", "target_action_gap", ")", "\n", "alpha_prime_loss", "=", "0.5", "*", "(", "-", "min_qf1_loss", "-", "min_qf2_loss", ")", "\n", "", "else", ":", "\n", "            ", "alpha_prime_loss", "=", "-", "min_qf1_loss", "\n", "\n", "", "", "cql_loss", "=", "[", "min_qf2_loss", "]", "\n", "if", "twin_q", ":", "\n", "        ", "cql_loss", ".", "append", "(", "min_qf2_loss", ")", "\n", "\n", "", "critic_loss", "[", "0", "]", "+=", "min_qf1_loss", "\n", "if", "twin_q", ":", "\n", "        ", "critic_loss", "[", "1", "]", "+=", "min_qf2_loss", "\n", "\n", "# Save for stats function.", "\n", "", "policy", ".", "q_t", "=", "q_t", "\n", "policy", ".", "policy_t", "=", "policy_t", "\n", "policy", ".", "log_pis_t", "=", "log_pis_t", "\n", "policy", ".", "td_error", "=", "td_error", "\n", "policy", ".", "actor_loss", "=", "actor_loss", "\n", "policy", ".", "critic_loss", "=", "critic_loss", "\n", "policy", ".", "alpha_loss", "=", "alpha_loss", "\n", "policy", ".", "log_alpha_value", "=", "model", ".", "log_alpha", "\n", "policy", ".", "alpha_value", "=", "alpha", "\n", "policy", ".", "target_entropy", "=", "model", ".", "target_entropy", "\n", "# CQL Stats", "\n", "policy", ".", "cql_loss", "=", "cql_loss", "\n", "if", "use_lagrange", ":", "\n", "        ", "policy", ".", "log_alpha_prime_value", "=", "model", ".", "log_alpha_prime", "[", "0", "]", "\n", "policy", ".", "alpha_prime_value", "=", "alpha_prime", "\n", "policy", ".", "alpha_prime_loss", "=", "alpha_prime_loss", "\n", "\n", "# Return all loss terms corresponding to our optimizers.", "\n", "", "if", "use_lagrange", ":", "\n", "        ", "return", "tuple", "(", "[", "policy", ".", "actor_loss", "]", "+", "policy", ".", "critic_loss", "+", "\n", "[", "policy", ".", "alpha_loss", "]", "+", "[", "policy", ".", "alpha_prime_loss", "]", ")", "\n", "", "return", "tuple", "(", "[", "policy", ".", "actor_loss", "]", "+", "policy", ".", "critic_loss", "+", "\n", "[", "policy", ".", "alpha_loss", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.cql_stats": [[247, 257], ["ray.rllib.agents.sac.sac_torch_policy.stats", "torch.mean", "torch.stack"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.stats"], ["", "def", "cql_stats", "(", "policy", ":", "Policy", ",", "\n", "train_batch", ":", "SampleBatch", ")", "->", "Dict", "[", "str", ",", "TensorType", "]", ":", "\n", "    ", "sac_dict", "=", "stats", "(", "policy", ",", "train_batch", ")", "\n", "sac_dict", "[", "\"cql_loss\"", "]", "=", "torch", ".", "mean", "(", "torch", ".", "stack", "(", "policy", ".", "cql_loss", ")", ")", "\n", "sac_dict", "[", "\"cur_iter\"", "]", "=", "policy", ".", "cur_iter", "\n", "if", "policy", ".", "config", "[", "\"lagrangian\"", "]", ":", "\n", "        ", "sac_dict", "[", "\"log_alpha_prime_value\"", "]", "=", "policy", ".", "log_alpha_prime_value", "\n", "sac_dict", "[", "\"alpha_prime_value\"", "]", "=", "policy", ".", "alpha_prime_value", "\n", "sac_dict", "[", "\"alpha_prime_loss\"", "]", "=", "policy", ".", "alpha_prime_loss", "\n", "", "return", "sac_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.cql_optimizer_fn": [[259, 275], ["ray.rllib.agents.sac.sac_torch_policy.optimizer_fn", "nn.Parameter", "policy.model.register_parameter", "torch.optim.Adam", "tuple", "torch.zeros().float", "torch.zeros"], "function", ["None"], ["", "def", "cql_optimizer_fn", "(", "policy", ":", "Policy", ",", "config", ":", "TrainerConfigDict", ")", "->", "Tuple", "[", "LocalOptimizer", "]", ":", "\n", "    ", "policy", ".", "cur_iter", "=", "0", "\n", "opt_list", "=", "optimizer_fn", "(", "policy", ",", "config", ")", "\n", "if", "config", "[", "\"lagrangian\"", "]", ":", "\n", "        ", "log_alpha_prime", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "zeros", "(", "1", ",", "requires_grad", "=", "True", ")", ".", "float", "(", ")", ")", "\n", "policy", ".", "model", ".", "register_parameter", "(", "\"log_alpha_prime\"", ",", "log_alpha_prime", ")", "\n", "policy", ".", "alpha_prime_optim", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "params", "=", "[", "policy", ".", "model", ".", "log_alpha_prime", "]", ",", "\n", "lr", "=", "config", "[", "\"optimization\"", "]", "[", "\"critic_learning_rate\"", "]", ",", "\n", "eps", "=", "1e-7", ",", "# to match tf.keras.optimizers.Adam's epsilon default", "\n", ")", "\n", "return", "tuple", "(", "[", "policy", ".", "actor_optim", "]", "+", "policy", ".", "critic_optims", "+", "\n", "[", "policy", ".", "alpha_optim", "]", "+", "[", "policy", ".", "alpha_prime_optim", "]", ")", "\n", "", "return", "opt_list", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql_torch_policy.cql_setup_late_mixins": [[277, 284], ["ray.rllib.agents.sac.sac_torch_policy.setup_late_mixins", "policy.model.log_alpha_prime.to"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.setup_late_mixins"], ["", "def", "cql_setup_late_mixins", "(", "policy", ":", "Policy", ",", "obs_space", ":", "gym", ".", "spaces", ".", "Space", ",", "\n", "action_space", ":", "gym", ".", "spaces", ".", "Space", ",", "\n", "config", ":", "TrainerConfigDict", ")", "->", "None", ":", "\n", "    ", "setup_late_mixins", "(", "policy", ",", "obs_space", ",", "action_space", ",", "config", ")", "\n", "if", "config", "[", "\"lagrangian\"", "]", ":", "\n", "        ", "policy", ".", "model", ".", "log_alpha_prime", "=", "policy", ".", "model", ".", "log_alpha_prime", ".", "to", "(", "\n", "policy", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql.validate_config": [[37, 40], ["ValueError"], "function", ["None"], ["def", "validate_config", "(", "config", ":", "TrainerConfigDict", ")", ":", "\n", "    ", "if", "config", "[", "\"framework\"", "]", "==", "\"tf\"", ":", "\n", "        ", "raise", "ValueError", "(", "\"Tensorflow CQL not implemented yet!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql.get_policy_class": [[42, 45], ["None"], "function", ["None"], ["", "", "def", "get_policy_class", "(", "config", ":", "TrainerConfigDict", ")", "->", "Optional", "[", "Type", "[", "Policy", "]", "]", ":", "\n", "    ", "if", "config", "[", "\"framework\"", "]", "==", "\"torch\"", ":", "\n", "        ", "return", "CQLTorchPolicy", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.exp_saver.Experiment.init": [[25, 48], ["collections.OrderedDict", "pathlib.Path().resolve", "exp_saver.Experiment.log_dir.mkdir", "tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter", "exp_saver.Experiment._log.add", "str", "str", "str", "exp_saver.Experiment._log.info", "pathlib.Path", "exp_saver._format"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver._format"], ["    ", "def", "init", "(", "self", ",", "log_dir", ")", ":", "\n", "        ", "\"\"\"\n        This MUST be called.\n        \"\"\"", "\n", "self", ".", "_log", "=", "logger", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "scalars", "=", "OrderedDict", "(", ")", "\n", "\n", "self", ".", "log_dir", "=", "Path", "(", "log_dir", ")", ".", "resolve", "(", ")", "\n", "self", ".", "log_dir", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# for i in self._log._handlers:", "\n", "#     self._log.remove(i)", "\n", "\n", "self", ".", "_writer_train", "=", "SummaryWriter", "(", "str", "(", "self", ".", "log_dir", "/", "'train'", ")", ")", "\n", "self", ".", "_writer_val", "=", "SummaryWriter", "(", "str", "(", "self", ".", "log_dir", "/", "'val'", ")", ")", "\n", "self", ".", "_log", ".", "add", "(", "\n", "str", "(", "self", ".", "log_dir", "/", "'log.txt'", ")", ",", "\n", "format", "=", "'{time:MM/DD/YY HH:mm:ss} {level}\\t{message}'", ")", "\n", "\n", "# Functions.", "\n", "self", ".", "debug", "=", "self", ".", "_log", ".", "debug", "\n", "self", ".", "info", "=", "lambda", "**", "kwargs", ":", "self", ".", "_log", ".", "info", "(", "_format", "(", "**", "kwargs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.exp_saver.Experiment.load_config": [[49, 54], ["pathlib.Path", "open", "json.load", "str"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load"], ["", "def", "load_config", "(", "self", ",", "model_path", ")", ":", "\n", "        ", "log_dir", "=", "Path", "(", "model_path", ")", ".", "parent", "\n", "\n", "with", "open", "(", "str", "(", "log_dir", "/", "'config.json'", ")", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "return", "json", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.exp_saver.Experiment.save_config": [[55, 69], ["copy.deepcopy", "exp_saver.Experiment.save_config._process"], "methods", ["None"], ["", "", "def", "save_config", "(", "self", ",", "config_dict", ")", ":", "\n", "        ", "def", "_process", "(", "x", ")", ":", "\n", "            ", "for", "key", ",", "val", "in", "x", ".", "items", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "val", ",", "dict", ")", ":", "\n", "                    ", "_process", "(", "val", ")", "\n", "", "elif", "not", "isinstance", "(", "val", ",", "float", ")", "and", "not", "isinstance", "(", "val", ",", "int", ")", ":", "\n", "                    ", "x", "[", "key", "]", "=", "str", "(", "val", ")", "\n", "\n", "", "", "", "config", "=", "copy", ".", "deepcopy", "(", "config_dict", ")", "\n", "\n", "_process", "(", "config", ")", "\n", "\n", "with", "open", "(", "str", "(", "self", ".", "log_dir", "/", "'config.json'", ")", ",", "'w+'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "config", ",", "f", ",", "indent", "=", "4", ",", "sort_keys", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.exp_saver.Experiment.scalar": [[70, 78], ["sorted", "kwargs.items", "exp_saver.Experiment.scalars[].append", "list"], "methods", ["None"], ["", "", "def", "scalar", "(", "self", ",", "is_train", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "sorted", "(", "kwargs", ".", "items", "(", ")", ")", ":", "\n", "            ", "key", "=", "(", "is_train", ",", "k", ")", "\n", "\n", "if", "key", "not", "in", "self", ".", "scalars", ":", "\n", "                ", "self", ".", "scalars", "[", "key", "]", "=", "list", "(", ")", "\n", "\n", "", "self", ".", "scalars", "[", "key", "]", ".", "append", "(", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.exp_saver.Experiment.end_epoch": [[79, 104], ["exp_saver.Experiment.info", "exp_saver.Experiment.scalars.items", "exp_saver.Experiment.scalars.clear", "collections.OrderedDict", "numpy.mean", "numpy.std", "numpy.min", "numpy.max", "exp_saver.Experiment.info", "torch.save", "exp_saver.Experiment._writer_train.add_scalar", "exp_saver.Experiment._writer_val.add_scalar", "torch.save", "net.state_dict", "str", "numpy.mean", "numpy.mean", "net.state_dict", "str"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save"], ["", "", "def", "end_epoch", "(", "self", ",", "epoch", ",", "net", "=", "None", ")", ":", "\n", "        ", "self", ".", "info", "(", "Epoch", "=", "epoch", ")", "\n", "for", "(", "is_train", ",", "k", ")", ",", "v", "in", "self", ".", "scalars", ".", "items", "(", ")", ":", "\n", "            ", "info", "=", "OrderedDict", "(", ")", "\n", "info", "[", "'%s_%s'", "%", "(", "'Train'", "if", "is_train", "else", "'Eval'", ",", "k", ")", "]", "=", "np", ".", "mean", "(", "v", ")", "\n", "info", "[", "'std'", "]", "=", "np", ".", "std", "(", "v", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "info", "[", "'min'", "]", "=", "np", ".", "min", "(", "v", ")", "\n", "info", "[", "'max'", "]", "=", "np", ".", "max", "(", "v", ")", "\n", "\n", "self", ".", "info", "(", "**", "info", ")", "\n", "\n", "if", "is_train", ":", "\n", "                ", "self", ".", "_writer_train", ".", "add_scalar", "(", "k", ",", "np", ".", "mean", "(", "v", ")", ",", "epoch", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_writer_val", ".", "add_scalar", "(", "k", ",", "np", ".", "mean", "(", "v", ")", ",", "epoch", ")", "\n", "\n", "", "", "self", ".", "scalars", ".", "clear", "(", ")", "\n", "\n", "if", "net", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "epoch", "%", "10", "==", "0", ":", "\n", "                ", "torch", ".", "save", "(", "net", ".", "state_dict", "(", ")", ",", "str", "(", "self", ".", "log_dir", "/", "(", "'model_%03d.t7'", "%", "self", ".", "epoch", ")", ")", ")", "\n", "\n", "", "torch", ".", "save", "(", "net", ".", "state_dict", "(", ")", ",", "str", "(", "self", ".", "log_dir", "/", "'latest.t7'", ")", ")", "\n", "\n", "", "self", ".", "epoch", "+=", "1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.exp_saver._format": [[12, 22], ["list", "kwargs.items", "isinstance", "isinstance", "list.append", "list.append"], "function", ["None"], ["def", "_format", "(", "**", "kwargs", ")", ":", "\n", "    ", "result", "=", "list", "(", ")", "\n", "\n", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "v", ",", "float", ")", "or", "isinstance", "(", "v", ",", "np", ".", "float32", ")", ":", "\n", "            ", "result", ".", "append", "(", "'%s: %.2f'", "%", "(", "k", ",", "v", ")", ")", "\n", "", "else", ":", "\n", "            ", "result", ".", "append", "(", "'%s: %s'", "%", "(", "k", ",", "v", ")", ")", "\n", "\n", "", "", "return", "'\\t'", ".", "join", "(", "result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.__init__": [[7, 17], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.Linear", "torch.Linear", "model.Model.affine_layers.append", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", "Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "activation", "=", "torch", ".", "relu", "\n", "self", ".", "affine_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "last_dim", "=", "state_dim", "\n", "for", "nh", "in", "hidden_size", ":", "\n", "            ", "self", ".", "affine_layers", ".", "append", "(", "nn", ".", "Linear", "(", "last_dim", ",", "nh", ")", ")", "\n", "last_dim", "=", "nh", "\n", "\n", "", "self", ".", "action_mean", "=", "nn", ".", "Linear", "(", "last_dim", ",", "action_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.forward": [[18, 24], ["model.Model.action_mean", "model.Model.activation", "affine"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "affine", "in", "self", ".", "affine_layers", ":", "\n", "            ", "x", "=", "self", ".", "activation", "(", "affine", "(", "x", ")", ")", "\n", "\n", "", "action_mean", "=", "self", ".", "action_mean", "(", "x", ")", "\n", "return", "action_mean", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load": [[25, 27], ["model.Model.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load"], ["", "def", "load", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "load_state_dict", "(", "torch", ".", "load", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save": [[28, 30], ["torch.save", "torch.save", "torch.save", "torch.save", "model.Model.state_dict"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save"], ["", "def", "save", "(", "self", ",", "path", ")", ":", "\n", "        ", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.utils.save_results": [[12, 29], ["dict", "len", "numpy.array().mean", "numpy.array().std", "os.path.join", "open", "json.dump", "print", "os.path.exists", "os.mkdir", "numpy.array", "numpy.array"], "function", ["None"], ["def", "save_results", "(", "episode_rewards", ",", "results_dir", "=", "\"./results\"", ",", "result_file_name", "=", "\"training_result\"", ")", ":", "\n", "# save results", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "results_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "results_dir", ")", "\n", "\n", "# save statistics in a dictionary and write them into a .json file", "\n", "", "results", "=", "dict", "(", ")", "\n", "results", "[", "\"number_episodes\"", "]", "=", "len", "(", "episode_rewards", ")", "\n", "results", "[", "\"episode_rewards\"", "]", "=", "episode_rewards", "\n", "\n", "results", "[", "\"mean_all_episodes\"", "]", "=", "np", ".", "array", "(", "episode_rewards", ")", ".", "mean", "(", ")", "\n", "results", "[", "\"std_all_episodes\"", "]", "=", "np", ".", "array", "(", "episode_rewards", ")", ".", "std", "(", ")", "\n", "\n", "fname", "=", "os", ".", "path", ".", "join", "(", "results_dir", ",", "result_file_name", ")", "\n", "fh", "=", "open", "(", "fname", ",", "\"w\"", ")", "\n", "json", ".", "dump", "(", "results", ",", "fh", ")", "\n", "print", "(", "'... finished'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.utils.store_data": [[31, 38], ["os.path.join", "gzip.open", "pickle.dump", "os.path.exists", "os.mkdir"], "function", ["None"], ["", "def", "store_data", "(", "data", ",", "datasets_dir", "=", "\"./data\"", ")", ":", "\n", "# save data", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "datasets_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "datasets_dir", ")", "\n", "", "data_file", "=", "os", ".", "path", ".", "join", "(", "datasets_dir", ",", "'data_dagger.pkl.gzip'", ")", "\n", "f", "=", "gzip", ".", "open", "(", "data_file", ",", "'wb'", ")", "\n", "pickle", ".", "dump", "(", "data", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.utils.read_data": [[40, 54], ["os.path.join", "gzip.open", "pickle.load", "numpy.array().astype", "numpy.array().astype", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load"], ["", "def", "read_data", "(", "datasets_dir", "=", "\"./data\"", ",", "path", "=", "'data.pkl.gzip'", ",", "frac", "=", "0.1", ")", ":", "\n", "    ", "\"\"\"\n    This method reads the states and actions recorded in drive_manually.py\n    and splits it into training/ validation set.\n    \"\"\"", "\n", "data_file", "=", "os", ".", "path", ".", "join", "(", "datasets_dir", ",", "path", ")", "\n", "\n", "f", "=", "gzip", ".", "open", "(", "data_file", ",", "'rb'", ")", "\n", "data", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "# get images as features and actions as targets", "\n", "X", "=", "np", ".", "array", "(", "data", "[", "\"state\"", "]", ")", ".", "astype", "(", "'float32'", ")", "\n", "y", "=", "np", ".", "array", "(", "data", "[", "\"action\"", "]", ")", ".", "astype", "(", "'float32'", ")", "\n", "return", "X", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.utils.train_model": [[56, 90], ["torch.nn.MSELoss", "torch.optim.SGD", "numpy.arange", "numpy.random.shuffle", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "range", "model.save", "model.parameters", "len", "len", "range", "torch.from_numpy().to", "torch.from_numpy().to", "len", "model", "torch.nn.MSELoss.", "torch.optim.SGD.zero_grad", "criterion.backward", "torch.optim.SGD.step", "numpy.sum", "len", "torch.no_grad", "total_loss.append", "criterion.item", "torch.from_numpy", "torch.from_numpy", "criterion.item"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.step"], ["", "def", "train_model", "(", "model", ",", "X_train", ",", "y_train", ",", "path", ",", "num_epochs", "=", "50", ",", "learning_rate", "=", "1e-3", ",", "lambda_l2", "=", "1e-5", ",", "\n", "early_terminate_loss_threshold", "=", "0.2", ",", "\n", "batch_size", "=", "32", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "    ", "criterion", "=", "torch", ".", "nn", ".", "MSELoss", "(", ")", "\n", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "learning_rate", ",", "weight_decay", "=", "lambda_l2", ")", "# built-in L2", "\n", "# shuffle", "\n", "perm", "=", "np", ".", "arange", "(", "len", "(", "X_train", ")", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "perm", ")", "\n", "X_train", "=", "X_train", "[", "perm", "]", "\n", "y_train", "=", "y_train", "[", "perm", "]", "\n", "\n", "X_train_torch", "=", "torch", ".", "from_numpy", "(", "X_train", ")", ".", "to", "(", "device", ")", ".", "float", "(", ")", "\n", "y_train_torch", "=", "torch", ".", "from_numpy", "(", "y_train", ")", ".", "to", "(", "device", ")", ".", "float", "(", ")", "\n", "total_loss", "=", "[", "]", "\n", "sgd_num", "=", "len", "(", "X_train_torch", ")", "/", "batch_size", "\n", "t", "=", "0", ",", "\n", "epoch_loss", "=", "0", "\n", "for", "t", "in", "range", "(", "num_epochs", ")", ":", "\n", "        ", "epoch_loss", "=", "0", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "X_train_torch", ")", ",", "batch_size", ")", ":", "\n", "            ", "curr_X", "=", "X_train_torch", "[", "i", ":", "i", "+", "batch_size", "]", "\n", "curr_Y", "=", "y_train_torch", "[", "i", ":", "i", "+", "batch_size", "]", "\n", "preds", "=", "model", "(", "curr_X", ")", "\n", "loss", "=", "criterion", "(", "preds", ",", "curr_Y", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "total_loss", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "epoch_loss", "+=", "loss", ".", "item", "(", ")", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "", "if", "epoch_loss", "/", "sgd_num", "<", "early_terminate_loss_threshold", ":", "\n", "            ", "break", "\n", "", "", "model", ".", "save", "(", "path", ")", "\n", "return", "np", ".", "sum", "(", "total_loss", ")", "/", "len", "(", "total_loss", ")", ",", "epoch_loss", "/", "sgd_num", ",", "t", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.utils.evaluation": [[92, 123], ["torch.no_grad", "print", "env.reset", "dict", "model", "env.step", "velocity.append", "torch.tensor().to().float", "model.detach().cpu().numpy().flatten", "episode_overtake.append", "env.reset", "numpy.mean", "numpy.mean", "torch.tensor().to", "model.detach().cpu().numpy", "torch.tensor", "model.detach().cpu", "model.detach"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.PIDController.reset", "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo_utils.expert_guided_env.ExpertGuidedEnv.step", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.PIDController.reset"], ["", "def", "evaluation", "(", "env", ",", "model", ",", "evaluation_episode_num", "=", "30", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "    ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "print", "(", "\"... evaluation\"", ")", "\n", "episode_reward", "=", "0", "\n", "episode_cost", "=", "0", "\n", "success_num", "=", "0", "\n", "episode_num", "=", "0", "\n", "velocity", "=", "[", "]", "\n", "episode_overtake", "=", "[", "]", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "while", "episode_num", "<", "evaluation_episode_num", ":", "\n", "            ", "prediction", "=", "model", "(", "torch", ".", "tensor", "(", "state", ")", ".", "to", "(", "device", ")", ".", "float", "(", ")", ")", "\n", "next_state", ",", "r", ",", "done", ",", "info", "=", "env", ".", "step", "(", "prediction", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", ")", "\n", "state", "=", "next_state", "\n", "episode_reward", "+=", "r", "\n", "episode_cost", "+=", "info", "[", "\"native_cost\"", "]", "\n", "velocity", ".", "append", "(", "info", "[", "\"velocity\"", "]", ")", "\n", "if", "done", ":", "\n", "                ", "episode_overtake", ".", "append", "(", "info", "[", "\"overtake_vehicle_num\"", "]", ")", "\n", "episode_num", "+=", "1", "\n", "if", "info", "[", "\"arrive_dest\"", "]", ":", "\n", "                    ", "success_num", "+=", "1", "\n", "", "env", ".", "reset", "(", ")", "\n", "", "", "res", "=", "dict", "(", "\n", "mean_episode_reward", "=", "episode_reward", "/", "episode_num", ",", "\n", "mean_episode_cost", "=", "episode_cost", "/", "episode_num", ",", "\n", "mean_success_rate", "=", "success_num", "/", "episode_num", ",", "\n", "mean_velocity", "=", "np", ".", "mean", "(", "velocity", ")", ",", "\n", "mean_episode_overtake_num", "=", "np", ".", "mean", "(", "episode_overtake", ")", "\n", ")", "\n", "return", "res", "\n", "", "", ""]], "home.repos.pwc.inspect_result.decisionforce_EGPO.train.train.train": [[11, 114], ["copy.deepcopy", "isinstance", "ray.tune.CLIReporter.DEFAULT_COLUMNS.copy", "ray.tune.CLIReporter", "ray.tune.CLIReporter.add_metric_column", "ray.tune.CLIReporter.add_metric_column", "ray.tune.CLIReporter.add_metric_column", "ray.tune.CLIReporter.add_metric_column", "ray.tune.CLIReporter.add_metric_column", "ray.tune.CLIReporter.add_metric_column", "ray.tune.CLIReporter.add_metric_column", "ray.tune.run", "dict", "os.environ.get", "egpo_utils.train.utils.initialize_ray", "os.environ.get", "os.environ.get", "print", "egpo_utils.train.utils.initialize_ray", "used_config.pop", "used_config.update", "hasattr", "numpy.isscalar", "isinstance", "print", "ray.tune.grid_search", "isinstance", "int", "open", "tune.run.fetch_trial_dataframes", "pickle.dump", "print", "range"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.train.utils.initialize_ray", "home.repos.pwc.inspect_result.decisionforce_EGPO.train.utils.initialize_ray"], ["def", "train", "(", "\n", "trainer", ",", "\n", "config", ",", "\n", "stop", ",", "\n", "exp_name", ",", "\n", "num_seeds", "=", "1", ",", "\n", "num_gpus", "=", "0", ",", "\n", "test_mode", "=", "False", ",", "\n", "suffix", "=", "\"\"", ",", "\n", "checkpoint_freq", "=", "10", ",", "\n", "keep_checkpoints_num", "=", "None", ",", "\n", "start_seed", "=", "0", ",", "\n", "local_mode", "=", "False", ",", "\n", "save_pkl", "=", "True", ",", "\n", "custom_callback", "=", "None", ",", "\n", "max_failures", "=", "5", ",", "\n", "init_kws", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "    ", "init_kws", "=", "init_kws", "or", "dict", "(", ")", "\n", "# initialize ray", "\n", "if", "not", "os", ".", "environ", ".", "get", "(", "\"redis_password\"", ")", ":", "\n", "        ", "initialize_ray", "(", "test_mode", "=", "test_mode", ",", "local_mode", "=", "local_mode", ",", "num_gpus", "=", "num_gpus", ",", "**", "init_kws", ")", "\n", "", "else", ":", "\n", "        ", "password", "=", "os", ".", "environ", ".", "get", "(", "\"redis_password\"", ")", "\n", "assert", "os", ".", "environ", ".", "get", "(", "\"ip_head\"", ")", "\n", "print", "(", "\n", "\"We detect redis_password ({}) exists in environment! So \"", "\n", "\"we will start a ray cluster!\"", ".", "format", "(", "password", ")", "\n", ")", "\n", "if", "num_gpus", ":", "\n", "            ", "print", "(", "\n", "\"We are in cluster mode! So GPU specification is disable and\"", "\n", "\" should be done when submitting task to cluster! You are \"", "\n", "\"requiring {} GPU for each machine!\"", ".", "format", "(", "num_gpus", ")", "\n", ")", "\n", "", "initialize_ray", "(", "address", "=", "os", ".", "environ", "[", "\"ip_head\"", "]", ",", "test_mode", "=", "test_mode", ",", "redis_password", "=", "password", ",", "**", "init_kws", ")", "\n", "\n", "# prepare config", "\n", "", "used_config", "=", "{", "\n", "\"seed\"", ":", "tune", ".", "grid_search", "(", "[", "i", "*", "100", "+", "start_seed", "for", "i", "in", "range", "(", "num_seeds", ")", "]", ")", "if", "num_seeds", "is", "not", "None", "else", "None", ",", "\n", "\"log_level\"", ":", "\"DEBUG\"", "if", "test_mode", "else", "\"INFO\"", ",", "\n", "\"callbacks\"", ":", "custom_callback", "if", "custom_callback", "else", "False", ",", "# Must Have!", "\n", "}", "\n", "if", "custom_callback", "is", "False", ":", "\n", "        ", "used_config", ".", "pop", "(", "\"callbacks\"", ")", "\n", "", "if", "config", ":", "\n", "        ", "used_config", ".", "update", "(", "config", ")", "\n", "", "config", "=", "copy", ".", "deepcopy", "(", "used_config", ")", "\n", "\n", "if", "isinstance", "(", "trainer", ",", "str", ")", ":", "\n", "        ", "trainer_name", "=", "trainer", "\n", "", "elif", "hasattr", "(", "trainer", ",", "\"_name\"", ")", ":", "\n", "        ", "trainer_name", "=", "trainer", ".", "_name", "\n", "", "else", ":", "\n", "        ", "trainer_name", "=", "trainer", ".", "__name__", "\n", "\n", "", "if", "not", "isinstance", "(", "stop", ",", "dict", ")", "and", "stop", "is", "not", "None", ":", "\n", "        ", "assert", "np", ".", "isscalar", "(", "stop", ")", "\n", "stop", "=", "{", "\"timesteps_total\"", ":", "int", "(", "stop", ")", "}", "\n", "\n", "", "if", "keep_checkpoints_num", "is", "not", "None", "and", "not", "test_mode", ":", "\n", "        ", "assert", "isinstance", "(", "keep_checkpoints_num", ",", "int", ")", "\n", "kwargs", "[", "\"keep_checkpoints_num\"", "]", "=", "keep_checkpoints_num", "\n", "kwargs", "[", "\"checkpoint_score_attr\"", "]", "=", "\"episode_reward_mean\"", "\n", "\n", "", "if", "\"verbose\"", "not", "in", "kwargs", ":", "\n", "        ", "kwargs", "[", "\"verbose\"", "]", "=", "1", "if", "not", "test_mode", "else", "2", "\n", "\n", "# This functionality is not supported yet!", "\n", "", "metric_columns", "=", "CLIReporter", ".", "DEFAULT_COLUMNS", ".", "copy", "(", ")", "\n", "progress_reporter", "=", "CLIReporter", "(", "metric_columns", ")", "\n", "progress_reporter", ".", "add_metric_column", "(", "\"success\"", ")", "\n", "progress_reporter", ".", "add_metric_column", "(", "\"crash\"", ")", "\n", "progress_reporter", ".", "add_metric_column", "(", "\"out\"", ")", "\n", "progress_reporter", ".", "add_metric_column", "(", "\"max_step\"", ")", "\n", "progress_reporter", ".", "add_metric_column", "(", "\"length\"", ")", "\n", "progress_reporter", ".", "add_metric_column", "(", "\"cost\"", ")", "\n", "progress_reporter", ".", "add_metric_column", "(", "\"takeover\"", ")", "\n", "kwargs", "[", "\"progress_reporter\"", "]", "=", "progress_reporter", "\n", "\n", "# start training", "\n", "analysis", "=", "tune", ".", "run", "(", "\n", "trainer", ",", "\n", "name", "=", "exp_name", ",", "\n", "checkpoint_freq", "=", "checkpoint_freq", ",", "\n", "checkpoint_at_end", "=", "True", ",", "\n", "stop", "=", "stop", ",", "\n", "config", "=", "config", ",", "\n", "max_failures", "=", "max_failures", "if", "not", "test_mode", "else", "0", ",", "\n", "reuse_actors", "=", "False", ",", "\n", "local_dir", "=", "\"./\"", ",", "\n", "**", "kwargs", "\n", ")", "\n", "\n", "# save training progress as insurance", "\n", "if", "save_pkl", ":", "\n", "        ", "pkl_path", "=", "\"{}-{}{}.pkl\"", ".", "format", "(", "exp_name", ",", "trainer_name", ",", "\"\"", "if", "not", "suffix", "else", "\"-\"", "+", "suffix", ")", "\n", "with", "open", "(", "pkl_path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "            ", "data", "=", "analysis", ".", "fetch_trial_dataframes", "(", ")", "\n", "pickle", ".", "dump", "(", "data", ",", "f", ")", "\n", "print", "(", "\"Result is saved at: <{}>\"", ".", "format", "(", "pkl_path", ")", ")", "\n", "", "", "return", "analysis", "\n", "", ""]], "home.repos.pwc.inspect_result.decisionforce_EGPO.train.utils.initialize_ray": [[8, 29], ["ray.init", "print", "print", "ray.__version__.split", "kwargs.pop", "ray.available_resources"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver.Experiment.init"], ["import", "numpy", "as", "np", "\n", "import", "torch", "\n", "\n", "\n", "def", "save_results", "(", "episode_rewards", ",", "results_dir", "=", "\"./results\"", ",", "result_file_name", "=", "\"training_result\"", ")", ":", "\n", "# save results", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "results_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "results_dir", ")", "\n", "\n", "# save statistics in a dictionary and write them into a .json file", "\n", "", "results", "=", "dict", "(", ")", "\n", "results", "[", "\"number_episodes\"", "]", "=", "len", "(", "episode_rewards", ")", "\n", "results", "[", "\"episode_rewards\"", "]", "=", "episode_rewards", "\n", "\n", "results", "[", "\"mean_all_episodes\"", "]", "=", "np", ".", "array", "(", "episode_rewards", ")", ".", "mean", "(", ")", "\n", "results", "[", "\"std_all_episodes\"", "]", "=", "np", ".", "array", "(", "episode_rewards", ")", ".", "std", "(", ")", "\n", "\n", "fname", "=", "os", ".", "path", ".", "join", "(", "results_dir", ",", "result_file_name", ")", "\n", "fh", "=", "open", "(", "fname", ",", "\"w\"", ")", "\n", "json", ".", "dump", "(", "results", ",", "fh", ")", "\n", "print", "(", "'... finished'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.train.utils.get_train_parser": [[31, 40], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["", "def", "store_data", "(", "data", ",", "datasets_dir", "=", "\"./data\"", ")", ":", "\n", "# save data", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "datasets_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "datasets_dir", ")", "\n", "", "data_file", "=", "os", ".", "path", ".", "join", "(", "datasets_dir", ",", "'data_dagger.pkl.gzip'", ")", "\n", "f", "=", "gzip", ".", "open", "(", "data_file", ",", "'wb'", ")", "\n", "pickle", ".", "dump", "(", "data", ",", "f", ")", "\n", "\n", "\n", "", "def", "read_data", "(", "datasets_dir", "=", "\"./data\"", ",", "path", "=", "'data.pkl.gzip'", ",", "frac", "=", "0.1", ")", ":", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.train.utils.setup_logger": [[42, 47], ["logging.basicConfig"], "function", ["None"], ["\n", "data_file", "=", "os", ".", "path", ".", "join", "(", "datasets_dir", ",", "path", ")", "\n", "\n", "f", "=", "gzip", ".", "open", "(", "data_file", ",", "'rb'", ")", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo.egpo.UpdateSaverPenalty.__init__": [[49, 52], ["egpo_utils.sac_pid.sac_pid_policy.UpdatePenalty.__init__", "egpo.UpdateSaverPenalty.workers.local_worker().get_policy", "egpo.UpdateSaverPenalty.workers.local_worker"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["    ", "def", "__init__", "(", "self", ",", "workers", ")", ":", "\n", "        ", "super", "(", "UpdateSaverPenalty", ",", "self", ")", ".", "__init__", "(", "workers", ")", "\n", "self", ".", "takeover_data_discard", "=", "self", ".", "workers", ".", "local_worker", "(", ")", ".", "get_policy", "(", ")", ".", "config", "[", "\"takeover_data_discard\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo.egpo.UpdateSaverPenalty.__call__": [[53, 65], ["super().__call__", "sample_batch.slice.slice.get", "sample_batch.slice.slice.slice"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.egpo.egpo.UpdateSaverPenalty.__call__"], ["", "def", "__call__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "sample_batch", "=", "super", "(", "UpdateSaverPenalty", ",", "self", ")", ".", "__call__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "infos", "=", "sample_batch", ".", "get", "(", "SampleBatch", ".", "INFOS", ")", "\n", "\n", "if", "infos", "is", "None", ":", "\n", "            ", "return", "sample_batch", "\n", "", "if", "self", ".", "takeover_data_discard", "and", "infos", "[", "0", "]", "[", "\"takeover\"", "]", "and", "not", "sample_batch", "[", "SampleBatch", ".", "DONES", "]", "[", "0", "]", ":", "\n", "# discard takeover data", "\n", "            ", "sample_batch", "=", "sample_batch", ".", "slice", "(", "0", ",", "0", ")", "\n", "# for key in batch.keys():", "\n", "#     batch[key] = batch[key][:-1]", "\n", "", "return", "sample_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo.egpo.validate_saver_config": [[43, 46], ["egpo_utils.sac_pid.sac_pid.validate_config"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.cql.cql.validate_config"], ["def", "validate_saver_config", "(", "config", ")", ":", "\n", "    ", "validate_config", "(", "config", ")", "\n", "assert", "config", "[", "\"info_cost_key\"", "]", "==", "\"takeover_cost\"", "and", "config", "[", "\"info_total_cost_key\"", "]", "==", "\"total_takeover_cost\"", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo.egpo.execution_plan": [[67, 133], ["config.get", "ray.rllib.execution.replay_buffer.LocalReplayBuffer", "ray.rllib.execution.rollout_ops.ParallelRollouts", "rollouts.for_each.for_each", "rollouts.for_each.for_each", "ray.rllib.execution.replay_ops.Replay().for_each().for_each().for_each().for_each", "ray.rllib.execution.concurrency_ops.Concurrently", "ray.rllib.execution.metric_ops.StandardMetricsReporting", "egpo.UpdateSaverPenalty", "ray.rllib.execution.replay_ops.StoreToReplayBuffer", "config.get", "config.get", "ray.rllib.execution.train_ops.UpdateTargetNetwork", "info_dict.items", "ray.rllib.execution.replay_buffer.LocalReplayBuffer.update_priorities", "ray.rllib.execution.replay_ops.Replay().for_each().for_each().for_each", "ray.rllib.agents.dqn.dqn.calculate_rr_weights", "info.get", "info[].get", "samples.policy_batches[].data.get", "ray.rllib.execution.replay_ops.Replay().for_each().for_each", "ray.rllib.execution.train_ops.TrainOneStep", "ray.rllib.execution.replay_ops.Replay().for_each", "ray.rllib.execution.replay_ops.Replay", "post_fn"], "function", ["None"], ["", "", "def", "execution_plan", "(", "workers", ":", "WorkerSet", ",", "\n", "config", ":", "TrainerConfigDict", ")", "->", "LocalIterator", "[", "dict", "]", ":", "\n", "    ", "if", "config", ".", "get", "(", "\"prioritized_replay\"", ")", ":", "\n", "        ", "prio_args", "=", "{", "\n", "\"prioritized_replay_alpha\"", ":", "config", "[", "\"prioritized_replay_alpha\"", "]", ",", "\n", "\"prioritized_replay_beta\"", ":", "config", "[", "\"prioritized_replay_beta\"", "]", ",", "\n", "\"prioritized_replay_eps\"", ":", "config", "[", "\"prioritized_replay_eps\"", "]", ",", "\n", "}", "\n", "", "else", ":", "\n", "        ", "prio_args", "=", "{", "}", "\n", "\n", "", "local_replay_buffer", "=", "LocalReplayBuffer", "(", "\n", "num_shards", "=", "1", ",", "\n", "learning_starts", "=", "config", "[", "\"learning_starts\"", "]", ",", "\n", "buffer_size", "=", "config", "[", "\"buffer_size\"", "]", ",", "\n", "replay_batch_size", "=", "config", "[", "\"train_batch_size\"", "]", ",", "\n", "replay_mode", "=", "config", "[", "\"multiagent\"", "]", "[", "\"replay_mode\"", "]", ",", "\n", "replay_sequence_length", "=", "config", "[", "\"replay_sequence_length\"", "]", ",", "\n", "**", "prio_args", ")", "\n", "\n", "rollouts", "=", "ParallelRollouts", "(", "workers", ",", "mode", "=", "\"bulk_sync\"", ")", "\n", "\n", "# Update penalty", "\n", "rollouts", "=", "rollouts", ".", "for_each", "(", "UpdateSaverPenalty", "(", "workers", ")", ")", "\n", "\n", "# We execute the following steps concurrently:", "\n", "# (1) Generate rollouts and store them in our local replay buffer. Calling", "\n", "# next() on store_op drives this.", "\n", "store_op", "=", "rollouts", ".", "for_each", "(", "StoreToReplayBuffer", "(", "local_buffer", "=", "local_replay_buffer", ")", ")", "\n", "\n", "def", "update_prio", "(", "item", ")", ":", "\n", "        ", "samples", ",", "info_dict", "=", "item", "\n", "if", "config", ".", "get", "(", "\"prioritized_replay\"", ")", ":", "\n", "            ", "prio_dict", "=", "{", "}", "\n", "for", "policy_id", ",", "info", "in", "info_dict", ".", "items", "(", ")", ":", "\n", "# TODO(sven): This is currently structured differently for", "\n", "#  torch/tf. Clean up these results/info dicts across", "\n", "#  policies (note: fixing this in torch_policy.py will", "\n", "#  break e.g. DDPPO!).", "\n", "                ", "td_error", "=", "info", ".", "get", "(", "\"td_error\"", ",", "\n", "info", "[", "LEARNER_STATS_KEY", "]", ".", "get", "(", "\"td_error\"", ")", ")", "\n", "prio_dict", "[", "policy_id", "]", "=", "(", "samples", ".", "policy_batches", "[", "policy_id", "]", "\n", ".", "data", ".", "get", "(", "\"batch_indexes\"", ")", ",", "td_error", ")", "\n", "", "local_replay_buffer", ".", "update_priorities", "(", "prio_dict", ")", "\n", "", "return", "info_dict", "\n", "\n", "# (2) Read and train on experiences from the replay buffer. Every batch", "\n", "# returned from the LocalReplay() iterator is passed to TrainOneStep to", "\n", "# take a SGD step, and then we decide whether to update the target network.", "\n", "", "post_fn", "=", "config", ".", "get", "(", "\"before_learn_on_batch\"", ")", "or", "(", "lambda", "b", ",", "*", "a", ":", "b", ")", "\n", "replay_op", "=", "Replay", "(", "local_buffer", "=", "local_replay_buffer", ")", ".", "for_each", "(", "lambda", "x", ":", "post_fn", "(", "x", ",", "workers", ",", "config", ")", ")", ".", "for_each", "(", "TrainOneStep", "(", "workers", ")", ")", ".", "for_each", "(", "update_prio", ")", ".", "for_each", "(", "UpdateTargetNetwork", "(", "\n", "workers", ",", "config", "[", "\"target_network_update_freq\"", "]", ")", ")", "\n", "\n", "# Alternate deterministically between (1) and (2). Only return the output", "\n", "# of (2) since training metrics are not available until (2) runs.", "\n", "train_op", "=", "Concurrently", "(", "\n", "[", "store_op", ",", "replay_op", "]", ",", "\n", "mode", "=", "\"round_robin\"", ",", "\n", "output_indexes", "=", "[", "1", "]", ",", "\n", "round_robin_weights", "=", "calculate_rr_weights", "(", "config", ")", ")", "\n", "\n", "return", "StandardMetricsReporting", "(", "train_op", ",", "workers", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo.egpo.postprocess_trajectory": [[135, 168], ["sample_batch.get", "ray.rllib.agents.dqn.dqn_tf_policy.postprocess_nstep_and_prio", "numpy.array", "numpy.array", "numpy.array().astype", "numpy.array().astype", "numpy.zeros_like", "numpy.zeros_like", "numpy.zeros_like", "numpy.zeros_like", "sample_batch.copy", "numpy.zeros_like", "numpy.array", "numpy.array"], "function", ["None"], ["", "def", "postprocess_trajectory", "(", "policy", ",", "\n", "sample_batch", ",", "\n", "other_agent_batches", "=", "None", ",", "\n", "episode", "=", "None", ")", ":", "\n", "# if sample_batch.count > 1:", "\n", "#     raise ValueError", "\n", "# Put the actions to batch", "\n", "    ", "infos", "=", "sample_batch", ".", "get", "(", "SampleBatch", ".", "INFOS", ")", "\n", "if", "(", "infos", "is", "not", "None", ")", "and", "(", "infos", "[", "0", "]", "!=", "0.0", ")", ":", "\n", "        ", "sample_batch", "[", "NEWBIE_ACTION", "]", "=", "sample_batch", ".", "copy", "(", ")", "[", "SampleBatch", ".", "ACTIONS", "]", "\n", "sample_batch", "[", "SampleBatch", ".", "ACTIONS", "]", "=", "np", ".", "array", "(", "[", "info", "[", "\"raw_action\"", "]", "for", "info", "in", "infos", "]", ")", "\n", "sample_batch", "[", "TAKEOVER", "]", "=", "np", ".", "array", "(", "\n", "[", "info", "[", "TAKEOVER", "]", "for", "info", "in", "sample_batch", "[", "SampleBatch", ".", "INFOS", "]", "]", ")", "\n", "sample_batch", "[", "policy", ".", "config", "[", "\"info_cost_key\"", "]", "]", "=", "np", ".", "array", "(", "\n", "[", "info", "[", "policy", ".", "config", "[", "\"info_cost_key\"", "]", "]", "for", "info", "in", "sample_batch", "[", "SampleBatch", ".", "INFOS", "]", "]", "\n", ")", ".", "astype", "(", "sample_batch", "[", "SampleBatch", ".", "REWARDS", "]", ".", "dtype", ")", "\n", "sample_batch", "[", "policy", ".", "config", "[", "\"info_total_cost_key\"", "]", "]", "=", "np", ".", "array", "(", "\n", "[", "info", "[", "policy", ".", "config", "[", "\"info_total_cost_key\"", "]", "]", "for", "info", "in", "sample_batch", "[", "SampleBatch", ".", "INFOS", "]", "]", "\n", ")", ".", "astype", "(", "sample_batch", "[", "SampleBatch", ".", "REWARDS", "]", ".", "dtype", ")", "\n", "if", "policy", ".", "config", "[", "\"no_reward\"", "]", ":", "\n", "            ", "sample_batch", "[", "SampleBatch", ".", "REWARDS", "]", "=", "np", ".", "zeros_like", "(", "sample_batch", "[", "SampleBatch", ".", "REWARDS", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "assert", "episode", "is", "None", ",", "\"Only during initialization, can we see empty infos.\"", "\n", "sample_batch", "[", "policy", ".", "config", "[", "\"info_cost_key\"", "]", "]", "=", "np", ".", "zeros_like", "(", "sample_batch", "[", "SampleBatch", ".", "REWARDS", "]", ")", "\n", "sample_batch", "[", "policy", ".", "config", "[", "\"info_total_cost_key\"", "]", "]", "=", "np", ".", "zeros_like", "(", "sample_batch", "[", "SampleBatch", ".", "REWARDS", "]", ")", "\n", "sample_batch", "[", "NEWBIE_ACTION", "]", "=", "np", ".", "zeros_like", "(", "sample_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ")", "\n", "sample_batch", "[", "TAKEOVER", "]", "=", "np", ".", "zeros_like", "(", "sample_batch", "[", "SampleBatch", ".", "DONES", "]", ")", "\n", "", "batch", "=", "postprocess_nstep_and_prio", "(", "policy", ",", "sample_batch", ")", "\n", "assert", "policy", ".", "config", "[", "\"info_cost_key\"", "]", "in", "batch", "\n", "assert", "policy", ".", "config", "[", "\"info_total_cost_key\"", "]", "in", "batch", "\n", "assert", "TAKEOVER", "in", "batch", "\n", "assert", "NEWBIE_ACTION", "in", "batch", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.egpo.egpo.sac_actor_critic_loss": [[170, 390], ["model", "model", "policy.target_model", "tf.squeeze", "tf.stop_gradient", "tf.stop_gradient", "tf.math.abs", "tf.math.abs", "model.get_q_values", "tf.squeeze", "critic_loss.append", "tf.variable_scope", "tf.get_variable", "ValueError", "egpo_utils.sac_pid.sac_pid_policy.get_dist_class", "egpo_utils.sac_pid.sac_pid_policy.get_dist_class.", "tf.expand_dims", "egpo_utils.sac_pid.sac_pid_policy.get_dist_class.", "tf.expand_dims", "model.get_q_values", "model.get_cost_q_values", "model.get_q_values", "model.get_cost_q_values", "policy.target_model.get_q_values", "policy.target_model.get_cost_q_values", "tf.squeeze", "tf.squeeze", "tf.squeeze", "tf.math.abs", "tf.math.abs", "model.get_twin_q_values", "tf.squeeze", "critic_loss.append", "critic_loss.append", "ValueError", "policy._get_is_training_placeholder", "policy._get_is_training_placeholder", "policy._get_is_training_placeholder", "model.get_policy_output", "action_dist_class.sample", "action_dist_class.deterministic_sample", "action_dist_class.logp", "model.get_policy_output", "action_dist_class.sample", "action_dist_class.deterministic_sample", "action_dist_class.logp", "model.get_twin_q_values", "model.get_twin_cost_q_values", "model.get_twin_q_values", "tf.reduce_min", "model.get_twin_cost_q_values", "tf.reduce_min", "policy.target_model.get_twin_q_values", "tf.reduce_min", "policy.target_model.get_twin_cost_q_values", "tf.reduce_min", "tf.squeeze", "tf.squeeze", "tf.cast", "tf.reduce_mean", "tf.reduce_mean", "tf.keras.losses.MSE", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.reduce_mean", "tf.math.add_n", "float", "tf.cast", "len", "len", "tf.keras.losses.MSE", "tf.keras.losses.MSE", "tf.keras.losses.MSE", "len", "len", "len", "len", "tf.stop_gradient", "len", "len", "tf.cast", "tf.cast"], "function", ["home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_policy.get_dist_class", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_cost_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_cost_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_cost_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_policy_output", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_policy_output", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_cost_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_cost_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_q_values", "home.repos.pwc.inspect_result.decisionforce_EGPO.sac_pid.sac_pid_model.ConstrainedSACModel.get_twin_cost_q_values"], ["", "def", "sac_actor_critic_loss", "(", "policy", ",", "model", ",", "_", ",", "train_batch", ")", ":", "\n", "    ", "_", "=", "train_batch", "[", "policy", ".", "config", "[", "\"info_total_cost_key\"", "]", "]", "# Touch this item, this is helpful in ray 1.2.0", "\n", "\n", "# Setup the lambda multiplier.", "\n", "with", "tf", ".", "variable_scope", "(", "'lambda'", ")", ":", "\n", "        ", "param_init", "=", "1e-8", "\n", "lambda_param", "=", "tf", ".", "get_variable", "(", "\n", "'lambda_value'", ",", "\n", "initializer", "=", "float", "(", "param_init", ")", ",", "\n", "trainable", "=", "False", ",", "\n", "dtype", "=", "tf", ".", "float32", "\n", ")", "\n", "", "policy", ".", "lambda_value", "=", "lambda_param", "\n", "\n", "# Should be True only for debugging purposes (e.g. test cases)!", "\n", "deterministic", "=", "policy", ".", "config", "[", "\"_deterministic_loss\"", "]", "\n", "\n", "model_out_t", ",", "_", "=", "model", "(", "{", "\n", "\"obs\"", ":", "train_batch", "[", "SampleBatch", ".", "CUR_OBS", "]", ",", "\n", "\"is_training\"", ":", "policy", ".", "_get_is_training_placeholder", "(", ")", ",", "\n", "}", ",", "[", "]", ",", "None", ")", "\n", "\n", "model_out_tp1", ",", "_", "=", "model", "(", "{", "\n", "\"obs\"", ":", "train_batch", "[", "SampleBatch", ".", "NEXT_OBS", "]", ",", "\n", "\"is_training\"", ":", "policy", ".", "_get_is_training_placeholder", "(", ")", ",", "\n", "}", ",", "[", "]", ",", "None", ")", "\n", "\n", "target_model_out_tp1", ",", "_", "=", "policy", ".", "target_model", "(", "{", "\n", "\"obs\"", ":", "train_batch", "[", "SampleBatch", ".", "NEXT_OBS", "]", ",", "\n", "\"is_training\"", ":", "policy", ".", "_get_is_training_placeholder", "(", ")", ",", "\n", "}", ",", "[", "]", ",", "None", ")", "\n", "\n", "# Discrete case.", "\n", "if", "model", ".", "discrete", ":", "\n", "        ", "raise", "ValueError", "(", "\"Doesn't support yet\"", ")", "\n", "# Continuous actions case.", "\n", "", "else", ":", "\n", "# Sample simgle actions from distribution.", "\n", "        ", "action_dist_class", "=", "get_dist_class", "(", "policy", ".", "config", ",", "policy", ".", "action_space", ")", "\n", "action_dist_t", "=", "action_dist_class", "(", "\n", "model", ".", "get_policy_output", "(", "model_out_t", ")", ",", "policy", ".", "model", ")", "\n", "policy_t", "=", "action_dist_t", ".", "sample", "(", ")", "if", "not", "deterministic", "else", "action_dist_t", ".", "deterministic_sample", "(", ")", "\n", "log_pis_t", "=", "tf", ".", "expand_dims", "(", "action_dist_t", ".", "logp", "(", "policy_t", ")", ",", "-", "1", ")", "\n", "action_dist_tp1", "=", "action_dist_class", "(", "\n", "model", ".", "get_policy_output", "(", "model_out_tp1", ")", ",", "policy", ".", "model", ")", "\n", "policy_tp1", "=", "action_dist_tp1", ".", "sample", "(", ")", "if", "not", "deterministic", "else", "action_dist_tp1", ".", "deterministic_sample", "(", ")", "\n", "log_pis_tp1", "=", "tf", ".", "expand_dims", "(", "action_dist_tp1", ".", "logp", "(", "policy_tp1", ")", ",", "-", "1", ")", "\n", "\n", "# Q-values for the actually selected actions.", "\n", "q_t", "=", "model", ".", "get_q_values", "(", "model_out_t", ",", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "            ", "twin_q_t", "=", "model", ".", "get_twin_q_values", "(", "\n", "model_out_t", ",", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ")", "\n", "\n", "# Cost Q-Value for actually selected actions", "\n", "", "c_q_t", "=", "model", ".", "get_cost_q_values", "(", "model_out_t", ",", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "            ", "twin_c_q_t", "=", "model", ".", "get_twin_cost_q_values", "(", "\n", "model_out_t", ",", "train_batch", "[", "SampleBatch", ".", "ACTIONS", "]", ")", "\n", "\n", "# Q-values for current policy in given current state.", "\n", "", "q_t_det_policy", "=", "model", ".", "get_q_values", "(", "model_out_t", ",", "policy_t", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "            ", "twin_q_t_det_policy", "=", "model", ".", "get_twin_q_values", "(", "\n", "model_out_t", ",", "policy_t", ")", "\n", "q_t_det_policy", "=", "tf", ".", "reduce_min", "(", "\n", "(", "q_t_det_policy", ",", "twin_q_t_det_policy", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# Cost Q-values for current policy in given current state.", "\n", "", "c_q_t_det_policy", "=", "model", ".", "get_cost_q_values", "(", "model_out_t", ",", "policy_t", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "            ", "twin_c_q_t_det_policy", "=", "model", ".", "get_twin_cost_q_values", "(", "\n", "model_out_t", ",", "policy_t", ")", "\n", "c_q_t_det_policy", "=", "tf", ".", "reduce_min", "(", "\n", "(", "c_q_t_det_policy", ",", "twin_c_q_t_det_policy", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# target q network evaluation", "\n", "", "q_tp1", "=", "policy", ".", "target_model", ".", "get_q_values", "(", "target_model_out_tp1", ",", "\n", "policy_tp1", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "            ", "twin_q_tp1", "=", "policy", ".", "target_model", ".", "get_twin_q_values", "(", "\n", "target_model_out_tp1", ",", "policy_tp1", ")", "\n", "# Take min over both twin-NNs.", "\n", "q_tp1", "=", "tf", ".", "reduce_min", "(", "(", "q_tp1", ",", "twin_q_tp1", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# target c-q network evaluation", "\n", "", "c_q_tp1", "=", "policy", ".", "target_model", ".", "get_cost_q_values", "(", "target_model_out_tp1", ",", "\n", "policy_tp1", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "            ", "twin_c_q_tp1", "=", "policy", ".", "target_model", ".", "get_twin_cost_q_values", "(", "\n", "target_model_out_tp1", ",", "policy_tp1", ")", "\n", "# Take min over both twin-NNs.", "\n", "c_q_tp1", "=", "tf", ".", "reduce_min", "(", "(", "c_q_tp1", ",", "twin_c_q_tp1", ")", ",", "axis", "=", "0", ")", "\n", "\n", "", "q_t_selected", "=", "tf", ".", "squeeze", "(", "q_t", ",", "axis", "=", "len", "(", "q_t", ".", "shape", ")", "-", "1", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "            ", "twin_q_t_selected", "=", "tf", ".", "squeeze", "(", "twin_q_t", ",", "axis", "=", "len", "(", "twin_q_t", ".", "shape", ")", "-", "1", ")", "\n", "\n", "# c_q_t selected", "\n", "", "c_q_t_selected", "=", "tf", ".", "squeeze", "(", "c_q_t", ",", "axis", "=", "len", "(", "c_q_t", ".", "shape", ")", "-", "1", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "            ", "twin_c_q_t_selected", "=", "tf", ".", "squeeze", "(", "twin_c_q_t", ",", "axis", "=", "len", "(", "twin_c_q_t", ".", "shape", ")", "-", "1", ")", "\n", "\n", "", "q_tp1", "-=", "model", ".", "alpha", "*", "log_pis_tp1", "\n", "\n", "q_tp1_best", "=", "tf", ".", "squeeze", "(", "input", "=", "q_tp1", ",", "axis", "=", "len", "(", "q_tp1", ".", "shape", ")", "-", "1", ")", "\n", "q_tp1_best_masked", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "train_batch", "[", "SampleBatch", ".", "DONES", "]", ",", "\n", "tf", ".", "float32", ")", ")", "*", "q_tp1_best", "\n", "\n", "", "c_q_tp1_best", "=", "tf", ".", "squeeze", "(", "input", "=", "c_q_tp1", ",", "axis", "=", "len", "(", "c_q_tp1", ".", "shape", ")", "-", "1", ")", "\n", "c_q_tp1_best_masked", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "train_batch", "[", "SampleBatch", ".", "DONES", "]", ",", "tf", ".", "float32", ")", ")", "*", "c_q_tp1_best", "\n", "\n", "# compute RHS of bellman equation", "\n", "q_t_selected_target", "=", "tf", ".", "stop_gradient", "(", "\n", "train_batch", "[", "SampleBatch", ".", "REWARDS", "]", "+", "\n", "policy", ".", "config", "[", "\"gamma\"", "]", "**", "policy", ".", "config", "[", "\"n_step\"", "]", "*", "q_tp1_best_masked", ")", "\n", "\n", "# Compute Cost of bellman equation.", "\n", "c_q_t_selected_target", "=", "tf", ".", "stop_gradient", "(", "train_batch", "[", "policy", ".", "config", "[", "\"info_cost_key\"", "]", "]", "+", "\n", "policy", ".", "config", "[", "\"gamma\"", "]", "**", "policy", ".", "config", "[", "\"n_step\"", "]", "*", "c_q_tp1_best_masked", ")", "\n", "\n", "# Compute the TD-error (potentially clipped).", "\n", "base_td_error", "=", "tf", ".", "math", ".", "abs", "(", "q_t_selected", "-", "q_t_selected_target", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "        ", "twin_td_error", "=", "tf", ".", "math", ".", "abs", "(", "twin_q_t_selected", "-", "q_t_selected_target", ")", "\n", "td_error", "=", "0.5", "*", "(", "base_td_error", "+", "twin_td_error", ")", "\n", "", "else", ":", "\n", "        ", "td_error", "=", "base_td_error", "\n", "\n", "# Compute the Cost TD-error (potentially clipped).", "\n", "", "base_c_td_error", "=", "tf", ".", "math", ".", "abs", "(", "c_q_t_selected", "-", "c_q_t_selected_target", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "        ", "twin_c_td_error", "=", "tf", ".", "math", ".", "abs", "(", "twin_c_q_t_selected", "-", "c_q_t_selected_target", ")", "\n", "c_td_error", "=", "0.5", "*", "(", "base_c_td_error", "+", "twin_c_td_error", ")", "\n", "", "else", ":", "\n", "        ", "c_td_error", "=", "base_c_td_error", "\n", "\n", "# conservative loss", "\n", "", "newbie_q_t", "=", "model", ".", "get_q_values", "(", "model_out_t", ",", "train_batch", "[", "NEWBIE_ACTION", "]", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "        ", "newbie_twin_q_t", "=", "model", ".", "get_twin_q_values", "(", "\n", "model_out_t", ",", "train_batch", "[", "NEWBIE_ACTION", "]", ")", "\n", "\n", "", "newbie_q_t_selected", "=", "tf", ".", "squeeze", "(", "newbie_q_t", ",", "axis", "=", "len", "(", "newbie_q_t", ".", "shape", ")", "-", "1", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "        ", "newbie_twin_q_t_selected", "=", "tf", ".", "squeeze", "(", "newbie_twin_q_t", ",", "axis", "=", "len", "(", "newbie_twin_q_t", ".", "shape", ")", "-", "1", ")", "\n", "\n", "# add conservative loss", "\n", "", "critic_loss", "=", "[", "\n", "0.5", "*", "tf", ".", "keras", ".", "losses", ".", "MSE", "(", "\n", "y_true", "=", "q_t_selected_target", ",", "y_pred", "=", "q_t_selected", ")", "-", "tf", ".", "reduce_mean", "(", "(", "tf", ".", "cast", "(", "train_batch", "[", "TAKEOVER", "]", ",", "\n", "tf", ".", "float32", ")", ")", "*", "policy", ".", "config", "[", "\n", "\"alpha\"", "]", "*", "(", "\n", "q_t_selected", "-", "newbie_q_t_selected", ")", ")", "]", "\n", "if", "policy", ".", "config", "[", "\"twin_q\"", "]", ":", "\n", "        ", "loss", "=", "0.5", "*", "tf", ".", "keras", ".", "losses", ".", "MSE", "(", "y_true", "=", "q_t_selected_target", ",", "y_pred", "=", "twin_q_t_selected", ")", "-", "tf", ".", "reduce_mean", "(", "(", "\n", "tf", ".", "cast", "(", "\n", "train_batch", "[", "\n", "TAKEOVER", "]", ",", "\n", "tf", ".", "float32", ")", ")", "*", "\n", "policy", ".", "config", "[", "\n", "\"alpha\"", "]", "*", "(", "\n", "twin_q_t_selected", "-", "newbie_twin_q_t_selected", ")", ")", "\n", "critic_loss", ".", "append", "(", "loss", ")", "\n", "\n", "# add cost critic", "\n", "", "critic_loss", ".", "append", "(", "\n", "0.5", "*", "tf", ".", "keras", ".", "losses", ".", "MSE", "(", "\n", "y_true", "=", "c_q_t_selected_target", ",", "y_pred", "=", "c_q_t_selected", ")", ")", "\n", "if", "policy", ".", "config", "[", "\"twin_cost_q\"", "]", ":", "\n", "        ", "critic_loss", ".", "append", "(", "0.5", "*", "tf", ".", "keras", ".", "losses", ".", "MSE", "(", "\n", "y_true", "=", "c_q_t_selected_target", ",", "y_pred", "=", "twin_c_q_t_selected", ")", ")", "\n", "\n", "# Alpha- and actor losses.", "\n", "# Note: In the papers, alpha is used directly, here we take the log.", "\n", "# Discrete case: Multiply the action probs as weights with the original", "\n", "# loss terms (no expectations needed).", "\n", "", "if", "model", ".", "discrete", ":", "\n", "        ", "raise", "ValueError", "(", "\"Didn't support discrete mode yet\"", ")", "\n", "", "else", ":", "\n", "        ", "alpha_loss", "=", "-", "tf", ".", "reduce_mean", "(", "\n", "model", ".", "log_alpha", "*", "\n", "tf", ".", "stop_gradient", "(", "log_pis_t", "+", "model", ".", "target_entropy", ")", ")", "\n", "if", "policy", ".", "config", "[", "\"only_evaluate_cost\"", "]", ":", "\n", "            ", "actor_loss", "=", "tf", ".", "reduce_mean", "(", "\n", "model", ".", "alpha", "*", "log_pis_t", "-", "q_t_det_policy", ")", "\n", "cost_loss", "=", "0", "\n", "reward_loss", "=", "actor_loss", "\n", "", "else", ":", "\n", "            ", "reward_loss", "=", "tf", ".", "reduce_mean", "(", "\n", "model", ".", "alpha", "*", "log_pis_t", "-", "q_t_det_policy", ")", "\n", "cost_loss", "=", "tf", ".", "reduce_mean", "(", "policy", ".", "lambda_value", "*", "c_q_t_det_policy", ")", "\n", "actor_loss", "=", "tf", ".", "reduce_mean", "(", "\n", "model", ".", "alpha", "*", "log_pis_t", "-", "q_t_det_policy", "+", "policy", ".", "lambda_value", "*", "c_q_t_det_policy", ")", "\n", "", "actor_loss", "=", "actor_loss", "/", "(", "1", "+", "policy", ".", "lambda_value", ")", "if", "policy", ".", "config", "[", "\"normalize\"", "]", "else", "actor_loss", "\n", "\n", "# save for stats function", "\n", "", "policy", ".", "policy_t", "=", "policy_t", "\n", "policy", ".", "cost_loss", "=", "cost_loss", "\n", "policy", ".", "reward_loss", "=", "reward_loss", "\n", "policy", ".", "mean_batch_cost", "=", "train_batch", "[", "policy", ".", "config", "[", "\"info_cost_key\"", "]", "]", "\n", "policy", ".", "q_t", "=", "q_t", "\n", "policy", ".", "c_q_tp1", "=", "c_q_tp1", "\n", "policy", ".", "c_q_t", "=", "c_q_t", "\n", "policy", ".", "td_error", "=", "td_error", "\n", "policy", ".", "c_td_error", "=", "c_td_error", "\n", "policy", ".", "actor_loss", "=", "actor_loss", "\n", "policy", ".", "critic_loss", "=", "critic_loss", "\n", "policy", ".", "c_td_target", "=", "c_q_t_selected_target", "\n", "policy", ".", "alpha_loss", "=", "alpha_loss", "\n", "policy", ".", "alpha_value", "=", "model", ".", "alpha", "\n", "policy", ".", "target_entropy", "=", "model", ".", "target_entropy", "\n", "\n", "# in a custom apply op we handle the losses separately, but return them", "\n", "# combined in one loss for now", "\n", "return", "actor_loss", "+", "tf", ".", "math", ".", "add_n", "(", "critic_loss", ")", "+", "alpha_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver.Experiment.init": [[25, 48], ["collections.OrderedDict", "pathlib.Path().resolve", "exp_saver.Experiment.log_dir.mkdir", "tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter", "exp_saver.Experiment._log.add", "str", "str", "str", "exp_saver.Experiment._log.info", "pathlib.Path", "exp_saver._format"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver._format"], ["    ", "def", "init", "(", "self", ",", "log_dir", ")", ":", "\n", "        ", "\"\"\"\n        This MUST be called.\n        \"\"\"", "\n", "self", ".", "_log", "=", "logger", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "scalars", "=", "OrderedDict", "(", ")", "\n", "\n", "self", ".", "log_dir", "=", "Path", "(", "log_dir", ")", ".", "resolve", "(", ")", "\n", "self", ".", "log_dir", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# for i in self._log._handlers:", "\n", "#     self._log.remove(i)", "\n", "\n", "self", ".", "_writer_train", "=", "SummaryWriter", "(", "str", "(", "self", ".", "log_dir", "/", "'train'", ")", ")", "\n", "self", ".", "_writer_val", "=", "SummaryWriter", "(", "str", "(", "self", ".", "log_dir", "/", "'val'", ")", ")", "\n", "self", ".", "_log", ".", "add", "(", "\n", "str", "(", "self", ".", "log_dir", "/", "'log.txt'", ")", ",", "\n", "format", "=", "'{time:MM/DD/YY HH:mm:ss} {level}\\t{message}'", ")", "\n", "\n", "# Functions.", "\n", "self", ".", "debug", "=", "self", ".", "_log", ".", "debug", "\n", "self", ".", "info", "=", "lambda", "**", "kwargs", ":", "self", ".", "_log", ".", "info", "(", "_format", "(", "**", "kwargs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver.Experiment.load_config": [[49, 54], ["pathlib.Path", "open", "json.load", "str"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.load"], ["", "def", "load_config", "(", "self", ",", "model_path", ")", ":", "\n", "        ", "log_dir", "=", "Path", "(", "model_path", ")", ".", "parent", "\n", "\n", "with", "open", "(", "str", "(", "log_dir", "/", "'config.json'", ")", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "return", "json", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver.Experiment.save_config": [[55, 69], ["copy.deepcopy", "exp_saver.Experiment.save_config._process"], "methods", ["None"], ["", "", "def", "save_config", "(", "self", ",", "config_dict", ")", ":", "\n", "        ", "def", "_process", "(", "x", ")", ":", "\n", "            ", "for", "key", ",", "val", "in", "x", ".", "items", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "val", ",", "dict", ")", ":", "\n", "                    ", "_process", "(", "val", ")", "\n", "", "elif", "not", "isinstance", "(", "val", ",", "float", ")", "and", "not", "isinstance", "(", "val", ",", "int", ")", ":", "\n", "                    ", "x", "[", "key", "]", "=", "str", "(", "val", ")", "\n", "\n", "", "", "", "config", "=", "copy", ".", "deepcopy", "(", "config_dict", ")", "\n", "\n", "_process", "(", "config", ")", "\n", "\n", "with", "open", "(", "str", "(", "self", ".", "log_dir", "/", "'config.json'", ")", ",", "'w+'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "config", ",", "f", ",", "indent", "=", "4", ",", "sort_keys", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver.Experiment.scalar": [[70, 78], ["sorted", "kwargs.items", "exp_saver.Experiment.scalars[].append", "list"], "methods", ["None"], ["", "", "def", "scalar", "(", "self", ",", "is_train", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "sorted", "(", "kwargs", ".", "items", "(", ")", ")", ":", "\n", "            ", "key", "=", "(", "is_train", ",", "k", ")", "\n", "\n", "if", "key", "not", "in", "self", ".", "scalars", ":", "\n", "                ", "self", ".", "scalars", "[", "key", "]", "=", "list", "(", ")", "\n", "\n", "", "self", ".", "scalars", "[", "key", "]", ".", "append", "(", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver.Experiment.end_epoch": [[79, 104], ["exp_saver.Experiment.info", "exp_saver.Experiment.scalars.items", "exp_saver.Experiment.scalars.clear", "collections.OrderedDict", "numpy.mean", "numpy.std", "numpy.min", "numpy.max", "exp_saver.Experiment.info", "torch.save", "exp_saver.Experiment._writer_train.add_scalar", "exp_saver.Experiment._writer_val.add_scalar", "torch.save", "net.state_dict", "str", "numpy.mean", "numpy.mean", "net.state_dict", "str"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save", "home.repos.pwc.inspect_result.decisionforce_EGPO.dagger.model.Model.save"], ["", "", "def", "end_epoch", "(", "self", ",", "epoch", ",", "net", "=", "None", ")", ":", "\n", "        ", "self", ".", "info", "(", "Epoch", "=", "epoch", ")", "\n", "for", "(", "is_train", ",", "k", ")", ",", "v", "in", "self", ".", "scalars", ".", "items", "(", ")", ":", "\n", "            ", "info", "=", "OrderedDict", "(", ")", "\n", "info", "[", "'%s_%s'", "%", "(", "'Train'", "if", "is_train", "else", "'Eval'", ",", "k", ")", "]", "=", "np", ".", "mean", "(", "v", ")", "\n", "info", "[", "'std'", "]", "=", "np", ".", "std", "(", "v", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "info", "[", "'min'", "]", "=", "np", ".", "min", "(", "v", ")", "\n", "info", "[", "'max'", "]", "=", "np", ".", "max", "(", "v", ")", "\n", "\n", "self", ".", "info", "(", "**", "info", ")", "\n", "\n", "if", "is_train", ":", "\n", "                ", "self", ".", "_writer_train", ".", "add_scalar", "(", "k", ",", "np", ".", "mean", "(", "v", ")", ",", "epoch", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_writer_val", ".", "add_scalar", "(", "k", ",", "np", ".", "mean", "(", "v", ")", ",", "epoch", ")", "\n", "\n", "", "", "self", ".", "scalars", ".", "clear", "(", ")", "\n", "\n", "if", "net", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "epoch", "%", "10", "==", "0", ":", "\n", "                ", "torch", ".", "save", "(", "net", ".", "state_dict", "(", ")", ",", "str", "(", "self", ".", "log_dir", "/", "(", "'model_%03d.t7'", "%", "self", ".", "epoch", ")", ")", ")", "\n", "\n", "", "torch", ".", "save", "(", "net", ".", "state_dict", "(", ")", ",", "str", "(", "self", ".", "log_dir", "/", "'latest.t7'", ")", ")", "\n", "\n", "", "self", ".", "epoch", "+=", "1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.exp_saver._format": [[12, 22], ["list", "kwargs.items", "isinstance", "isinstance", "list.append", "list.append"], "function", ["None"], ["def", "_format", "(", "**", "kwargs", ")", ":", "\n", "    ", "result", "=", "list", "(", ")", "\n", "\n", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "v", ",", "float", ")", "or", "isinstance", "(", "v", ",", "np", ".", "float32", ")", ":", "\n", "            ", "result", ".", "append", "(", "'%s: %.2f'", "%", "(", "k", ",", "v", ")", ")", "\n", "", "else", ":", "\n", "            ", "result", ".", "append", "(", "'%s: %s'", "%", "(", "k", ",", "v", ")", ")", "\n", "\n", "", "", "return", "'\\t'", ".", "join", "(", "result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Policy.__init__": [[11, 32], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "mlp.Policy.affine_layers.append", "torch.Linear", "torch.Linear", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "hidden_size", "=", "(", "128", ",", "256", ",", "128", ")", ",", "activation", "=", "'tanh'", ",", "log_std", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "is_disc_action", "=", "False", "\n", "if", "activation", "==", "'tanh'", ":", "\n", "            ", "self", ".", "activation", "=", "torch", ".", "tanh", "\n", "", "elif", "activation", "==", "'relu'", ":", "\n", "            ", "self", ".", "activation", "=", "torch", ".", "relu", "\n", "", "elif", "activation", "==", "'sigmoid'", ":", "\n", "            ", "self", ".", "activation", "=", "torch", ".", "sigmoid", "\n", "\n", "", "self", ".", "affine_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "last_dim", "=", "state_dim", "\n", "for", "nh", "in", "hidden_size", ":", "\n", "            ", "self", ".", "affine_layers", ".", "append", "(", "nn", ".", "Linear", "(", "last_dim", ",", "nh", ")", ")", "\n", "last_dim", "=", "nh", "\n", "\n", "", "self", ".", "action_mean", "=", "nn", ".", "Linear", "(", "last_dim", ",", "action_dim", ")", "\n", "# self.action_mean.weight.data.mul_(0.1)", "\n", "# self.action_mean.bias.data.mul_(0.0)", "\n", "\n", "self", ".", "action_log_std", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "1", ",", "action_dim", ")", "*", "log_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Policy.forward": [[33, 42], ["mlp.Policy.action_mean", "mlp.Policy.action_log_std.expand_as", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "mlp.Policy.activation", "affine"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "affine", "in", "self", ".", "affine_layers", ":", "\n", "            ", "x", "=", "self", ".", "activation", "(", "affine", "(", "x", ")", ")", "\n", "\n", "", "action_mean", "=", "self", ".", "action_mean", "(", "x", ")", "\n", "action_log_std", "=", "self", ".", "action_log_std", ".", "expand_as", "(", "action_mean", ")", "\n", "action_std", "=", "torch", ".", "exp", "(", "action_log_std", ")", "\n", "\n", "return", "action_mean", ",", "action_log_std", ",", "action_std", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Policy.select_action": [[43, 47], ["mlp.Policy.forward", "torch.normal", "torch.normal", "torch.normal", "torch.normal", "mlp.normal_log_density"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.forward", "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.normal_log_density"], ["", "def", "select_action", "(", "self", ",", "x", ")", ":", "\n", "        ", "action_mean", ",", "action_log_std", ",", "action_std", "=", "self", ".", "forward", "(", "x", ")", "\n", "action", "=", "torch", ".", "normal", "(", "action_mean", ",", "action_std", ")", "\n", "return", "action", ",", "normal_log_density", "(", "action", ",", "action_mean", ",", "action_log_std", ",", "action_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Policy.get_kl": [[48, 56], ["mlp.Policy.forward", "mean1.detach", "log_std1.detach", "std1.detach", "kl.sum", "std1.detach.pow", "std1.pow"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.forward"], ["", "def", "get_kl", "(", "self", ",", "x", ")", ":", "\n", "        ", "mean1", ",", "log_std1", ",", "std1", "=", "self", ".", "forward", "(", "x", ")", "\n", "\n", "mean0", "=", "mean1", ".", "detach", "(", ")", "\n", "log_std0", "=", "log_std1", ".", "detach", "(", ")", "\n", "std0", "=", "std1", ".", "detach", "(", ")", "\n", "kl", "=", "log_std1", "-", "log_std0", "+", "(", "std0", ".", "pow", "(", "2", ")", "+", "(", "mean0", "-", "mean1", ")", ".", "pow", "(", "2", ")", ")", "/", "(", "2.0", "*", "std1", ".", "pow", "(", "2", ")", ")", "-", "0.5", "\n", "return", "kl", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Policy.get_log_prob": [[57, 60], ["mlp.Policy.forward", "mlp.normal_log_density"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.forward", "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.normal_log_density"], ["", "def", "get_log_prob", "(", "self", ",", "x", ",", "actions", ")", ":", "\n", "        ", "action_mean", ",", "action_log_std", ",", "action_std", "=", "self", ".", "forward", "(", "x", ")", "\n", "return", "normal_log_density", "(", "actions", ",", "action_mean", ",", "action_log_std", ",", "action_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Policy.get_fim": [[61, 74], ["mlp.Policy.forward", "mlp.Policy.action_log_std.exp().pow().squeeze().repeat", "mlp.Policy.named_parameters", "x.size", "mlp.Policy.detach", "mlp.Policy.action_log_std.exp().pow().squeeze", "param.view", "mlp.Policy.action_log_std.exp().pow", "mlp.Policy.action_log_std.exp"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.forward"], ["", "def", "get_fim", "(", "self", ",", "x", ")", ":", "\n", "        ", "mean", ",", "_", ",", "_", "=", "self", ".", "forward", "(", "x", ")", "\n", "cov_inv", "=", "self", ".", "action_log_std", ".", "exp", "(", ")", ".", "pow", "(", "-", "2", ")", ".", "squeeze", "(", "0", ")", ".", "repeat", "(", "x", ".", "size", "(", "0", ")", ")", "\n", "param_count", "=", "0", "\n", "std_index", "=", "0", "\n", "id", "=", "0", "\n", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "name", "==", "\"action_log_std\"", ":", "\n", "                ", "std_id", "=", "id", "\n", "std_index", "=", "param_count", "\n", "", "param_count", "+=", "param", ".", "view", "(", "-", "1", ")", ".", "shape", "[", "0", "]", "\n", "id", "+=", "1", "\n", "", "return", "cov_inv", ".", "detach", "(", ")", ",", "mean", ",", "{", "'std_id'", ":", "std_id", ",", "'std_index'", ":", "std_index", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__": [[76, 94], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.Linear", "torch.Linear", "mlp.Value.value_head.weight.data.mul_", "mlp.Value.value_head.bias.data.mul_", "mlp.Value.affine_layers.append", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.__init__"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "hidden_size", "=", "(", "128", ",", "128", ")", ",", "activation", "=", "'tanh'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "activation", "==", "'tanh'", ":", "\n", "            ", "self", ".", "activation", "=", "torch", ".", "tanh", "\n", "", "elif", "activation", "==", "'relu'", ":", "\n", "            ", "self", ".", "activation", "=", "torch", ".", "relu", "\n", "", "elif", "activation", "==", "'sigmoid'", ":", "\n", "            ", "self", ".", "activation", "=", "torch", ".", "sigmoid", "\n", "\n", "", "self", ".", "affine_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "last_dim", "=", "state_dim", "\n", "for", "nh", "in", "hidden_size", ":", "\n", "            ", "self", ".", "affine_layers", ".", "append", "(", "nn", ".", "Linear", "(", "last_dim", ",", "nh", ")", ")", "\n", "last_dim", "=", "nh", "\n", "\n", "", "self", ".", "value_head", "=", "nn", ".", "Linear", "(", "last_dim", ",", "1", ")", "\n", "self", ".", "value_head", ".", "weight", ".", "data", ".", "mul_", "(", "0.1", ")", "\n", "self", ".", "value_head", ".", "bias", ".", "data", ".", "mul_", "(", "0.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.Value.forward": [[95, 102], ["mlp.Value.value_head", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "mlp.Value.activation", "affine"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "affine", "in", "self", ".", "affine_layers", ":", "\n", "            ", "x", "=", "self", ".", "activation", "(", "affine", "(", "x", ")", ")", "\n", "\n", "", "value", "=", "self", ".", "value_head", "(", "x", ")", "\n", "prob", "=", "torch", ".", "sigmoid", "(", "value", ")", "\n", "return", "prob", "\n", "", "", ""]], "home.repos.pwc.inspect_result.decisionforce_EGPO.gail.mlp.normal_log_density": [[5, 9], ["std.pow", "log_density.sum", "math.log"], "function", ["None"], ["def", "normal_log_density", "(", "x", ",", "mean", ",", "log_std", ",", "std", ")", ":", "\n", "    ", "var", "=", "std", ".", "pow", "(", "2", ")", "\n", "log_density", "=", "-", "(", "x", "-", "mean", ")", ".", "pow", "(", "2", ")", "/", "(", "2", "*", "var", ")", "-", "0.5", "*", "math", ".", "log", "(", "2", "*", "math", ".", "pi", ")", "-", "log_std", "\n", "return", "log_density", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "\n"]]}