{"home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string.test.load_closest_match_dataset": [[12, 19], ["closest_string.task.dataset.ReferenceDataset", "closest_string.task.dataset.QueryDataset", "open", "pickle.load"], "function", ["None"], ["def", "load_closest_match_dataset", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "sequences_references", ",", "sequences_queries", ",", "labels", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "reference_dataset", "=", "ReferenceDataset", "(", "sequences_references", ")", "\n", "query_dataset", "=", "QueryDataset", "(", "sequences_queries", ",", "labels", ")", "\n", "return", "reference_dataset", ",", "query_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string.test.execute_test": [[21, 44], ["print", "numpy.random.seed", "torch.manual_seed", "torch.load", "model_class", "print", "model_class.load_state_dict", "model_class.eval", "test.closest_string_testing", "torch.cuda.is_available", "torch.cuda.manual_seed", "vars"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string.test.closest_string_testing"], ["", "def", "execute_test", "(", "args", ")", ":", "\n", "    ", "\"\"\" Run when pretrained model is saved and needs to be restored \"\"\"", "\n", "# set device", "\n", "args", ".", "cuda", "=", "not", "args", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "device", "=", "'cuda'", "if", "args", ".", "cuda", "else", "'cpu'", "\n", "print", "(", "'Using device:'", ",", "device", ")", "\n", "\n", "# set the random seed", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "# model", "\n", "", "model_class", ",", "model_args", ",", "state_dict", ",", "distance", "=", "torch", ".", "load", "(", "args", ".", "encoder_path", ")", "\n", "encoder_model", "=", "model_class", "(", "**", "vars", "(", "model_args", ")", ")", "\n", "\n", "# Restore best model", "\n", "print", "(", "'Loading model '", "+", "args", ".", "encoder_path", ")", "\n", "encoder_model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "encoder_model", ".", "eval", "(", ")", "\n", "\n", "closest_string_testing", "(", "encoder_model", ",", "args", ".", "data", ",", "args", ".", "batch_size", ",", "device", ",", "distance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string.test.closest_string_testing": [[46, 64], ["time.time", "test.load_closest_match_dataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "test.embed_strings", "test.test", "print", "print", "print", "time.time"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string.test.load_closest_match_dataset", "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string.test.embed_strings", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.test"], ["", "def", "closest_string_testing", "(", "encoder_model", ",", "data_path", ",", "batch_size", ",", "device", ",", "distance", ")", ":", "\n", "    ", "\"\"\" Main routine: computes performance given pretrained model, dataset path and other arguments \"\"\"", "\n", "t_total", "=", "time", ".", "time", "(", ")", "\n", "\n", "# load data", "\n", "reference_dataset", ",", "query_dataset", "=", "load_closest_match_dataset", "(", "data_path", ")", "\n", "reference_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "reference_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "query_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "query_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "distance_function", "=", "DISTANCE_MATRIX", "[", "distance", "]", "\n", "\n", "# Testing", "\n", "embedded_reference", "=", "embed_strings", "(", "reference_loader", ",", "encoder_model", ",", "device", ")", "\n", "avg_acc", "=", "test", "(", "query_loader", ",", "encoder_model", ",", "embedded_reference", ",", "distance_function", ",", "device", ")", "\n", "print", "(", "'Results: accuracy {:.3f} {:.3f} {:.3f} {:.3f} {:.3f} {:.3f} {:.3f} {:.3f} {:.3f} {:.3f}'", "\n", ".", "format", "(", "*", "avg_acc", ")", ")", "\n", "print", "(", "'Top1: {:.3f}  Top5: {:.3f}  Top10: {:.3f}'", ".", "format", "(", "avg_acc", "[", "0", "]", ",", "avg_acc", "[", "4", "]", ",", "avg_acc", "[", "9", "]", ")", ")", "\n", "\n", "print", "(", "'Total time elapsed: {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t_total", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string.test.embed_strings": [[66, 77], ["torch.cat", "sequences.to.to", "model.encode", "embedded_list.append", "model.encode.cpu().detach", "model.encode.cpu"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.encode"], ["", "def", "embed_strings", "(", "loader", ",", "model", ",", "device", ")", ":", "\n", "    ", "\"\"\" Embeds the sequences of a dataset one batch at the time given an encoder \"\"\"", "\n", "embedded_list", "=", "[", "]", "\n", "\n", "for", "sequences", "in", "loader", ":", "\n", "        ", "sequences", "=", "sequences", ".", "to", "(", "device", ")", "\n", "embedded", "=", "model", ".", "encode", "(", "sequences", ")", "\n", "embedded_list", ".", "append", "(", "embedded", ".", "cpu", "(", ")", ".", "detach", "(", ")", ")", "\n", "\n", "", "embedded_reference", "=", "torch", ".", "cat", "(", "embedded_list", ",", "axis", "=", "0", ")", "\n", "return", "embedded_reference", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string.test.test": [[79, 97], ["util.ml_and_math.loss_functions.AverageMeter", "embedded_reference.to.to", "model.encode", "distance", "torch.sum", "util.ml_and_math.loss_functions.AverageMeter.update", "sequences.to", "labels.to", "torch.le().float", "torch.mean", "range", "labels.long", "torch.arange", "torch.le", "label_distances.unsqueeze"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.encode", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update"], ["", "def", "test", "(", "loader", ",", "model", ",", "embedded_reference", ",", "distance", ",", "device", ",", "slots", "=", "10", ")", ":", "\n", "    ", "\"\"\" Given the embedding of the references, embeds and checks the performance for one batch of queries at a time \"\"\"", "\n", "avg_acc", "=", "AverageMeter", "(", "len_tuple", "=", "slots", ")", "\n", "embedded_reference", "=", "embedded_reference", ".", "to", "(", "device", ")", "\n", "\n", "for", "sequences", ",", "labels", "in", "loader", ":", "\n", "        ", "sequences", ",", "labels", "=", "sequences", ".", "to", "(", "device", ")", ",", "labels", ".", "to", "(", "device", ")", "\n", "embedded", "=", "model", ".", "encode", "(", "sequences", ")", "\n", "\n", "distance_matrix", "=", "distance", "(", "embedded_reference", ",", "embedded", ",", "model", ".", "scaling", ")", "\n", "\n", "label_distances", "=", "distance_matrix", "[", "labels", ".", "long", "(", ")", ",", "torch", ".", "arange", "(", "0", ",", "distance_matrix", ".", "shape", "[", "1", "]", ")", "]", "\n", "rank", "=", "torch", ".", "sum", "(", "torch", ".", "le", "(", "distance_matrix", ",", "label_distances", ".", "unsqueeze", "(", "0", ")", ")", ".", "float", "(", ")", ",", "dim", "=", "0", ")", "\n", "\n", "acc", "=", "[", "torch", ".", "mean", "(", "(", "rank", "<=", "i", "+", "1", ")", ".", "float", "(", ")", ")", "for", "i", "in", "range", "(", "slots", ")", "]", "\n", "avg_acc", ".", "update", "(", "acc", ",", "sequences", ".", "shape", "[", "0", "]", ")", "\n", "\n", "", "return", "avg_acc", ".", "avg", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_genomic.ClosestStringGenomicDatasetGenerator.__init__": [[14, 41], ["max", "util.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads", "numpy.min", "numpy.sum", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().float", "print", "util.data_handling.string_generator.string_to_list", "util.data_handling.string_generator.string_to_list", "matplotlib.hist", "matplotlib.show", "len", "numpy.argmin", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.asarray", "numpy.min", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.string_to_list", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.string_to_list"], ["    ", "def", "__init__", "(", "self", ",", "strings_reference", ",", "strings_query", ",", "n_queries", ",", "plot", "=", "False", ")", ":", "\n", "# compute maximum length and transform sequences into list of integers", "\n", "        ", "length", "=", "max", "(", "len", "(", "s", ")", "for", "s", "in", "strings_reference", "+", "strings_query", ")", "\n", "sequences_references", "=", "[", "string_to_list", "(", "s", ",", "length", "=", "length", ")", "for", "s", "in", "strings_reference", "]", "\n", "sequences_queries", "=", "[", "string_to_list", "(", "s", ",", "length", "=", "length", ")", "for", "s", "in", "strings_query", "]", "\n", "\n", "# compute distances and find reference with minimum distance", "\n", "distances", "=", "cross_distance_matrix_threads", "(", "strings_reference", ",", "strings_query", ",", "5", ")", "\n", "minimum", "=", "np", ".", "min", "(", "distances", ",", "axis", "=", "0", ",", "keepdims", "=", "True", ")", "\n", "\n", "# queries are only valid if there is a unique answer (no exaequo)", "\n", "counts", "=", "np", ".", "sum", "(", "(", "minimum", "+", "0.5", ">", "distances", ")", ".", "astype", "(", "float", ")", ",", "axis", "=", "0", ")", "\n", "valid", "=", "counts", "==", "1", "\n", "labels", "=", "np", ".", "argmin", "(", "distances", ",", "axis", "=", "0", ")", "[", "valid", "]", "[", ":", "n_queries", "]", "\n", "\n", "# print an histogram of the minimum distances", "\n", "if", "plot", ":", "\n", "            ", "plt", ".", "hist", "(", "x", "=", "np", ".", "min", "(", "distances", ",", "axis", "=", "0", ")", "[", "valid", "]", ",", "bins", "=", "'auto'", ",", "color", "=", "'#0504aa'", ",", "alpha", "=", "0.7", ",", "rwidth", "=", "0.85", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "# convert to torch", "\n", "", "self", ".", "sequences_references", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "sequences_references", ")", ")", ".", "long", "(", ")", "\n", "self", ".", "sequences_queries", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "sequences_queries", ")", "[", "valid", "]", "[", ":", "n_queries", "]", ")", ".", "long", "(", ")", "\n", "self", ".", "labels", "=", "torch", ".", "from_numpy", "(", "labels", ")", ".", "float", "(", ")", "\n", "\n", "print", "(", "\"Shapes:\"", ",", "\"References\"", ",", "self", ".", "sequences_references", ".", "shape", ",", "\" Queries\"", ",", "self", ".", "sequences_queries", ".", "shape", ",", "\n", "\" Labels\"", ",", "self", ".", "labels", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_genomic.ClosestStringGenomicDatasetGenerator.save_as_pickle": [[42, 49], ["os.path.dirname", "os.makedirs", "open", "pickle.dump", "os.path.exists"], "methods", ["None"], ["", "def", "save_as_pickle", "(", "self", ",", "filename", ")", ":", "\n", "        ", "directory", "=", "os", ".", "path", ".", "dirname", "(", "filename", ")", "\n", "if", "directory", "!=", "''", "and", "not", "os", ".", "path", ".", "exists", "(", "directory", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "directory", ")", "\n", "\n", "", "with", "open", "(", "filename", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "(", "self", ".", "sequences_references", ",", "self", ".", "sequences_queries", ",", "self", ".", "labels", ")", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.ReferenceDataset.__init__": [[6, 8], ["util.data_handling.data_loader.index_to_one_hot"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot"], ["    ", "def", "__init__", "(", "self", ",", "sequences", ")", ":", "\n", "        ", "self", ".", "sequences", "=", "index_to_one_hot", "(", "sequences", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.ReferenceDataset.__len__": [[9, 11], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sequences", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.ReferenceDataset.__getitem__": [[12, 14], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sequences", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.QueryDataset.__init__": [[17, 20], ["util.data_handling.data_loader.index_to_one_hot"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot"], ["    ", "def", "__init__", "(", "self", ",", "sequences", ",", "labels", ")", ":", "\n", "        ", "self", ".", "sequences", "=", "index_to_one_hot", "(", "sequences", ")", "\n", "self", ".", "labels", "=", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.QueryDataset.__len__": [[21, 23], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sequences", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.QueryDataset.__getitem__": [[24, 26], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sequences", "[", "index", "]", ",", "self", ".", "labels", "[", "index", "]", "", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.ClosestStringDatasetGenerator.__init__": [[15, 52], ["random.seed", "range", "range", "util.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads", "numpy.argmin", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().float", "string_generator.generate", "util.data_handling.string_generator.k_mutations", "sequences_references.append", "util.data_handling.string_generator.k_mutations", "sequences_queries.append", "matplotlib.hist", "matplotlib.show", "range", "numpy.random.randint", "numpy.random.randint", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "random.randint", "random.randint", "chr", "chr", "numpy.min", "numpy.asarray", "numpy.asarray", "len"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.k_mutations", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.k_mutations"], ["    ", "def", "__init__", "(", "self", ",", "N_reference", ",", "N_query", ",", "len_sequence", ",", "min_changes", ",", "max_changes", ",", "string_generator", ",", "initials", ",", "\n", "seed", "=", "0", ",", "plot", "=", "False", ")", ":", "\n", "        ", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "# generate some sequences independently at random", "\n", "sequences_references", "=", "[", "string_generator", ".", "generate", "(", "len_sequence", ")", "for", "_", "in", "range", "(", "initials", ")", "]", "\n", "sequences_queries", "=", "[", "]", "\n", "\n", "# generate the rest of references from previously generated ones", "\n", "for", "i", "in", "range", "(", "N_reference", "-", "initials", ")", ":", "\n", "            ", "S_ref", "=", "sequences_references", "[", "random", ".", "randint", "(", "0", ",", "len", "(", "sequences_references", ")", "-", "1", ")", "]", "\n", "S", "=", "k_mutations", "(", "S_ref", ",", "np", ".", "random", ".", "randint", "(", "min_changes", ",", "max_changes", ")", ")", "\n", "sequences_references", ".", "append", "(", "S", ")", "\n", "\n", "# generate queries via random mutations", "\n", "", "for", "i", "in", "range", "(", "N_query", ")", ":", "\n", "            ", "S_ref", "=", "sequences_references", "[", "random", ".", "randint", "(", "0", ",", "N_reference", "-", "1", ")", "]", "\n", "S", "=", "k_mutations", "(", "S_ref", ",", "np", ".", "random", ".", "randint", "(", "min_changes", ",", "max_changes", ")", ")", "\n", "sequences_queries", ".", "append", "(", "S", ")", "\n", "\n", "# convert to strings for distance routine", "\n", "", "sequences_references_str", "=", "[", "\"\"", ".", "join", "(", "chr", "(", "s", "+", "97", ")", "for", "s", "in", "S", ")", "for", "S", "in", "sequences_references", "]", "\n", "sequences_queries_str", "=", "[", "\"\"", ".", "join", "(", "chr", "(", "s", "+", "97", ")", "for", "s", "in", "S", ")", "for", "S", "in", "sequences_queries", "]", "\n", "\n", "# compute distances and reference with minimum distance", "\n", "distances", "=", "cross_distance_matrix_threads", "(", "sequences_references_str", ",", "sequences_queries_str", ",", "5", ")", "\n", "labels", "=", "np", ".", "argmin", "(", "distances", ",", "axis", "=", "0", ")", "\n", "\n", "# plot histogram of minimum distances", "\n", "if", "plot", ":", "\n", "            ", "plt", ".", "hist", "(", "x", "=", "np", ".", "min", "(", "distances", ",", "axis", "=", "0", ")", ",", "bins", "=", "'auto'", ",", "color", "=", "'#0504aa'", ",", "alpha", "=", "0.7", ",", "rwidth", "=", "0.85", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "# convert to torch", "\n", "", "self", ".", "sequences_references", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "sequences_references", ")", ")", ".", "long", "(", ")", "\n", "self", ".", "sequences_queries", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "sequences_queries", ")", ")", ".", "long", "(", ")", "\n", "self", ".", "labels", "=", "torch", ".", "from_numpy", "(", "labels", ")", ".", "float", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.ClosestStringDatasetGenerator.save_as_pickle": [[53, 60], ["os.path.dirname", "os.makedirs", "open", "pickle.dump", "os.path.exists"], "methods", ["None"], ["", "def", "save_as_pickle", "(", "self", ",", "filename", ")", ":", "\n", "        ", "directory", "=", "os", ".", "path", ".", "dirname", "(", "filename", ")", "\n", "if", "directory", "!=", "''", "and", "not", "os", ".", "path", ".", "exists", "(", "directory", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "directory", ")", "\n", "\n", "", "with", "open", "(", "filename", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "(", "self", ".", "sequences_references", ",", "self", ".", "sequences_queries", ",", "self", ".", "labels", ")", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_genomic.HierarchicalClusteringGenomicDatasetGenerator.__init__": [[12, 24], ["max", "numpy.asarray", "util.data_handling.string_generator.string_to_list", "util.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads", "len"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.string_to_list", "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads"], ["class", "ClosestStringGenomicDatasetGenerator", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "strings_reference", ",", "strings_query", ",", "n_queries", ",", "plot", "=", "False", ")", ":", "\n", "# compute maximum length and transform sequences into list of integers", "\n", "        ", "length", "=", "max", "(", "len", "(", "s", ")", "for", "s", "in", "strings_reference", "+", "strings_query", ")", "\n", "sequences_references", "=", "[", "string_to_list", "(", "s", ",", "length", "=", "length", ")", "for", "s", "in", "strings_reference", "]", "\n", "sequences_queries", "=", "[", "string_to_list", "(", "s", ",", "length", "=", "length", ")", "for", "s", "in", "strings_query", "]", "\n", "\n", "# compute distances and find reference with minimum distance", "\n", "distances", "=", "cross_distance_matrix_threads", "(", "strings_reference", ",", "strings_query", ",", "5", ")", "\n", "minimum", "=", "np", ".", "min", "(", "distances", ",", "axis", "=", "0", ",", "keepdims", "=", "True", ")", "\n", "\n", "# queries are only valid if there is a unique answer (no exaequo)", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_genomic.HierarchicalClusteringGenomicDatasetGenerator.save_as_pickle": [[25, 32], ["os.path.dirname", "os.makedirs", "open", "pickle.dump", "os.path.exists"], "methods", ["None"], ["counts", "=", "np", ".", "sum", "(", "(", "minimum", "+", "0.5", ">", "distances", ")", ".", "astype", "(", "float", ")", ",", "axis", "=", "0", ")", "\n", "valid", "=", "counts", "==", "1", "\n", "labels", "=", "np", ".", "argmin", "(", "distances", ",", "axis", "=", "0", ")", "[", "valid", "]", "[", ":", "n_queries", "]", "\n", "\n", "# print an histogram of the minimum distances", "\n", "if", "plot", ":", "\n", "            ", "plt", ".", "hist", "(", "x", "=", "np", ".", "min", "(", "distances", ",", "axis", "=", "0", ")", "[", "valid", "]", ",", "bins", "=", "'auto'", ",", "color", "=", "'#0504aa'", ",", "alpha", "=", "0.7", ",", "rwidth", "=", "0.85", ")", "\n", "plt", ".", "show", "(", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_example.HierarchicalClusteringGenomicDatasetGenerator.__init__": [[12, 21], ["max", "numpy.asarray", "util.data_handling.string_generator.string_to_list", "util.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads", "len"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.string_to_list", "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads"], ["    ", "def", "__init__", "(", "self", ",", "sequences", ")", ":", "\n", "        ", "length", "=", "max", "(", "len", "(", "s", ")", "for", "s", "in", "sequences", ")", "\n", "texts", "=", "[", "string_to_list", "(", "s", ",", "length", "=", "length", ",", "alphabet", "=", "PROTEIN_ALPHABET", ")", "for", "s", "in", "sequences", "]", "\n", "\n", "distances", "=", "cross_distance_matrix_threads", "(", "sequences", ",", "sequences", ",", "5", ")", "/", "length", "\n", "similarities", "=", "1", "-", "distances", "\n", "\n", "self", ".", "texts_leaves", "=", "np", ".", "asarray", "(", "texts", ")", "\n", "self", ".", "similarities", "=", "similarities", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_example.HierarchicalClusteringGenomicDatasetGenerator.save_as_pickle": [[22, 29], ["os.path.dirname", "os.makedirs", "open", "pickle.dump", "os.path.exists"], "methods", ["None"], ["", "def", "save_as_pickle", "(", "self", ",", "filename", ")", ":", "\n", "        ", "directory", "=", "os", ".", "path", ".", "dirname", "(", "filename", ")", "\n", "if", "directory", "!=", "''", "and", "not", "os", ".", "path", ".", "exists", "(", "directory", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "directory", ")", "\n", "\n", "", "with", "open", "(", "filename", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "(", "self", ".", "texts_leaves", ",", "self", ".", "similarities", ")", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.HierarchicalClusteringDatasetGenerator.__init__": [[14, 43], ["random.seed", "range", "range", "numpy.asarray", "string_generator.generate", "util.data_handling.string_generator.k_mutations", "sequences_references.append", "util.data_handling.string_generator.k_mutations", "sequences_leaves.append", "util.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads", "numpy.random.randint", "numpy.random.randint", "random.randint", "random.randint", "chr", "len"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.k_mutations", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.k_mutations", "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads"], ["\n", "    ", "def", "__init__", "(", "self", ",", "N_reference", ",", "N_query", ",", "len_sequence", ",", "min_changes", ",", "max_changes", ",", "string_generator", ",", "initials", ",", "\n", "seed", "=", "0", ",", "plot", "=", "False", ")", ":", "\n", "        ", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "# generate some sequences independently at random", "\n", "sequences_references", "=", "[", "string_generator", ".", "generate", "(", "len_sequence", ")", "for", "_", "in", "range", "(", "initials", ")", "]", "\n", "sequences_queries", "=", "[", "]", "\n", "\n", "# generate the rest of references from previously generated ones", "\n", "for", "i", "in", "range", "(", "N_reference", "-", "initials", ")", ":", "\n", "            ", "S_ref", "=", "sequences_references", "[", "random", ".", "randint", "(", "0", ",", "len", "(", "sequences_references", ")", "-", "1", ")", "]", "\n", "S", "=", "k_mutations", "(", "S_ref", ",", "np", ".", "random", ".", "randint", "(", "min_changes", ",", "max_changes", ")", ")", "\n", "sequences_references", ".", "append", "(", "S", ")", "\n", "\n", "# generate queries via random mutations", "\n", "", "for", "i", "in", "range", "(", "N_query", ")", ":", "\n", "            ", "S_ref", "=", "sequences_references", "[", "random", ".", "randint", "(", "0", ",", "N_reference", "-", "1", ")", "]", "\n", "S", "=", "k_mutations", "(", "S_ref", ",", "np", ".", "random", ".", "randint", "(", "min_changes", ",", "max_changes", ")", ")", "\n", "sequences_queries", ".", "append", "(", "S", ")", "\n", "\n", "# convert to strings for distance routine", "\n", "", "sequences_references_str", "=", "[", "\"\"", ".", "join", "(", "chr", "(", "s", "+", "97", ")", "for", "s", "in", "S", ")", "for", "S", "in", "sequences_references", "]", "\n", "sequences_queries_str", "=", "[", "\"\"", ".", "join", "(", "chr", "(", "s", "+", "97", ")", "for", "s", "in", "S", ")", "for", "S", "in", "sequences_queries", "]", "\n", "\n", "# compute distances and reference with minimum distance", "\n", "distances", "=", "cross_distance_matrix_threads", "(", "sequences_references_str", ",", "sequences_queries_str", ",", "5", ")", "\n", "labels", "=", "np", ".", "argmin", "(", "distances", ",", "axis", "=", "0", ")", "\n", "\n", "# plot histogram of minimum distances", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.HierarchicalClusteringDatasetGenerator.save_as_pickle": [[44, 51], ["os.path.dirname", "os.makedirs", "open", "pickle.dump", "os.path.exists"], "methods", ["None"], ["if", "plot", ":", "\n", "            ", "plt", ".", "hist", "(", "x", "=", "np", ".", "min", "(", "distances", ",", "axis", "=", "0", ")", ",", "bins", "=", "'auto'", ",", "color", "=", "'#0504aa'", ",", "alpha", "=", "0.7", ",", "rwidth", "=", "0.85", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "# convert to torch", "\n", "", "self", ".", "sequences_references", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "sequences_references", ")", ")", ".", "long", "(", ")", "\n", "self", ".", "sequences_queries", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "sequences_queries", ")", ")", ".", "long", "(", ")", "\n", "self", ".", "labels", "=", "torch", ".", "from_numpy", "(", "labels", ")", ".", "float", "(", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.PairDistanceDatasetSampled.__init__": [[6, 20], ["print", "dataset.PairDistanceDatasetSampled.__len__"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.hc_dataset.HCTripletDataset.__len__"], ["    ", "def", "__init__", "(", "self", ",", "sequences", ")", ":", "\n", "        ", "self", ".", "sequences", "=", "index_to_one_hot", "(", "sequences", ")", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sequences", ".", "shape", "[", "0", "]", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sequences", "[", "index", "]", "\n", "\n", "\n", "", "", "class", "QueryDataset", "(", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "sequences", ",", "labels", ")", ":", "\n", "        ", "self", ".", "sequences", "=", "index_to_one_hot", "(", "sequences", ")", "\n", "self", ".", "labels", "=", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.PairDistanceDatasetSampled.__len__": [[21, 23], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sequences", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.PairDistanceDatasetSampled.__getitem__": [[24, 35], ["random.randint", "torch.cat", "dataset.PairDistanceDatasetSampled.sequences[].unsqueeze", "dataset.PairDistanceDatasetSampled.sequences[].unsqueeze"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sequences", "[", "index", "]", ",", "self", ".", "labels", "[", "index", "]", "", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.PairDistanceDatasetFull.__init__": [[38, 52], ["print", "dataset.PairDistanceDatasetFull.__len__"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.hc_dataset.HCTripletDataset.__len__"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.PairDistanceDatasetFull.__len__": [[53, 55], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.PairDistanceDatasetFull.__getitem__": [[56, 66], ["torch.cat", "dataset.PairDistanceDatasetFull.sequences[].unsqueeze", "dataset.PairDistanceDatasetFull.sequences[].unsqueeze"], "methods", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.MultipleAlignmentDataset.__init__": [[69, 73], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.MultipleAlignmentDataset.__len__": [[74, 76], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.MultipleAlignmentDataset.__getitem__": [[77, 79], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_genome.MSAPairDatasetGeneratorGenome.__init__": [[13, 36], ["print", "util.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads", "torch.from_numpy().long", "torch.from_numpy().float", "print", "torch.from_numpy().long", "util.data_handling.string_generator.string_to_list", "sequences.append", "torch.from_numpy", "torch.from_numpy", "util.data_handling.string_generator.string_to_list", "torch.from_numpy", "numpy.asarray", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.string_to_list", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.string_to_list"], ["    ", "def", "__init__", "(", "self", ",", "strings", ",", "length", ")", ":", "\n", "        ", "self", ".", "datasets", "=", "{", "}", "\n", "\n", "for", "dataset", "in", "[", "'train'", ",", "'val'", "]", ":", "\n", "            ", "print", "(", "\"Generating\"", ",", "dataset", ")", "\n", "\n", "sequences", "=", "[", "string_to_list", "(", "s", ",", "length", "=", "length", ")", "for", "s", "in", "strings", "[", "dataset", "]", "]", "\n", "distances", "=", "cross_distance_matrix_threads", "(", "strings", "[", "dataset", "]", ",", "strings", "[", "dataset", "]", ",", "5", ")", "\n", "\n", "self", ".", "datasets", "[", "dataset", "]", "=", "{", "}", "\n", "self", ".", "datasets", "[", "dataset", "]", "[", "'texts'", "]", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "sequences", ")", ")", ".", "long", "(", ")", "\n", "self", ".", "datasets", "[", "dataset", "]", "[", "'distances'", "]", "=", "torch", ".", "from_numpy", "(", "distances", ")", ".", "float", "(", ")", "\n", "\n", "", "for", "dataset", "in", "[", "'val_msa'", ",", "'test'", "]", ":", "\n", "            ", "sequences", "=", "[", "]", "\n", "print", "(", "\"Generating\"", ",", "dataset", ")", "\n", "\n", "for", "S", "in", "strings", "[", "dataset", "]", ":", "\n", "                ", "sequences_batch", "=", "[", "string_to_list", "(", "s", ",", "length", "=", "length", ")", "for", "s", "in", "S", "]", "\n", "sequences", ".", "append", "(", "sequences_batch", ")", "\n", "\n", "", "self", ".", "datasets", "[", "dataset", "]", "=", "{", "}", "\n", "self", ".", "datasets", "[", "dataset", "]", "[", "'texts'", "]", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "sequences", ")", ")", ".", "long", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_genome.MSAPairDatasetGeneratorGenome.save_as_pickle": [[37, 44], ["os.path.dirname", "os.makedirs", "open", "pickle.dump", "os.path.exists"], "methods", ["None"], ["", "", "def", "save_as_pickle", "(", "self", ",", "filename", ")", ":", "\n", "        ", "directory", "=", "os", ".", "path", ".", "dirname", "(", "filename", ")", "\n", "if", "directory", "!=", "''", "and", "not", "os", ".", "path", ".", "exists", "(", "directory", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "directory", ")", "\n", "\n", "", "with", "open", "(", "filename", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ".", "datasets", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_genomic.EditDistanceGenomicDatasetGenerator.__init__": [[14, 36], ["max", "strings.keys", "print", "util.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads", "torch.from_numpy().long", "torch.from_numpy().float", "max", "util.data_handling.string_generator.string_to_list", "matplotlib.hist", "matplotlib.show", "strings.values", "torch.from_numpy", "torch.from_numpy", "len", "numpy.reshape", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.string_to_list"], ["    ", "def", "__init__", "(", "self", ",", "strings_reference", ",", "strings_query", ",", "n_queries", ",", "plot", "=", "False", ")", ":", "\n", "# compute maximum length and transform sequences into list of integers", "\n", "        ", "length", "=", "max", "(", "len", "(", "s", ")", "for", "s", "in", "strings_reference", "+", "strings_query", ")", "\n", "sequences_references", "=", "[", "string_to_list", "(", "s", ",", "length", "=", "length", ")", "for", "s", "in", "strings_reference", "]", "\n", "sequences_queries", "=", "[", "string_to_list", "(", "s", ",", "length", "=", "length", ")", "for", "s", "in", "strings_query", "]", "\n", "\n", "# compute distances and find reference with minimum distance", "\n", "distances", "=", "cross_distance_matrix_threads", "(", "strings_reference", ",", "strings_query", ",", "5", ")", "\n", "minimum", "=", "np", ".", "min", "(", "distances", ",", "axis", "=", "0", ",", "keepdims", "=", "True", ")", "\n", "\n", "# queries are only valid if there is a unique answer (no exaequo)", "\n", "counts", "=", "np", ".", "sum", "(", "(", "minimum", "+", "0.5", ">", "distances", ")", ".", "astype", "(", "float", ")", ",", "axis", "=", "0", ")", "\n", "valid", "=", "counts", "==", "1", "\n", "labels", "=", "np", ".", "argmin", "(", "distances", ",", "axis", "=", "0", ")", "[", "valid", "]", "[", ":", "n_queries", "]", "\n", "\n", "# print an histogram of the minimum distances", "\n", "if", "plot", ":", "\n", "            ", "plt", ".", "hist", "(", "x", "=", "np", ".", "min", "(", "distances", ",", "axis", "=", "0", ")", "[", "valid", "]", ",", "bins", "=", "'auto'", ",", "color", "=", "'#0504aa'", ",", "alpha", "=", "0.7", ",", "rwidth", "=", "0.85", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "# convert to torch", "\n", "", "self", ".", "sequences_references", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "sequences_references", ")", ")", ".", "long", "(", ")", "\n", "self", ".", "sequences_queries", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "sequences_queries", ")", "[", "valid", "]", "[", ":", "n_queries", "]", ")", ".", "long", "(", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_genomic.EditDistanceGenomicDatasetGenerator.save_as_pickle": [[37, 44], ["os.path.dirname", "os.makedirs", "open", "pickle.dump", "os.path.exists"], "methods", ["None"], ["self", ".", "labels", "=", "torch", ".", "from_numpy", "(", "labels", ")", ".", "float", "(", ")", "\n", "\n", "print", "(", "\"Shapes:\"", ",", "\"References\"", ",", "self", ".", "sequences_references", ".", "shape", ",", "\" Queries\"", ",", "self", ".", "sequences_queries", ".", "shape", ",", "\n", "\" Labels\"", ",", "self", ".", "labels", ".", "shape", ")", "\n", "\n", "", "def", "save_as_pickle", "(", "self", ",", "filename", ")", ":", "\n", "        ", "directory", "=", "os", ".", "path", ".", "dirname", "(", "filename", ")", "\n", "if", "directory", "!=", "''", "and", "not", "os", ".", "path", ".", "exists", "(", "directory", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.EditDistanceDatasetSampled.__init__": [[9, 22], ["util.data_handling.data_loader.index_to_one_hot"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sequences", ".", "shape", "[", "0", "]", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sequences", "[", "index", "]", "\n", "\n", "\n", "", "", "class", "QueryDataset", "(", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "sequences", ",", "labels", ")", ":", "\n", "        ", "self", ".", "sequences", "=", "index_to_one_hot", "(", "sequences", ")", "\n", "self", ".", "labels", "=", "labels", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sequences", ".", "shape", "[", "0", "]", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.EditDistanceDatasetSampled.__len__": [[23, 25], ["None"], "methods", ["None"], ["\n", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sequences", "[", "index", "]", ",", "self", ".", "labels", "[", "index", "]", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.EditDistanceDatasetSampled.__getitem__": [[26, 37], ["torch.Tensor", "torch.all", "random.randint", "torch.cat", "dataset.EditDistanceDatasetSampled.sequences[].unsqueeze", "dataset.EditDistanceDatasetSampled.sequences[].unsqueeze"], "methods", ["None"], ["", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.EditDistanceDatasetComplete.__init__": [[42, 51], ["util.data_handling.data_loader.index_to_one_hot"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.EditDistanceDatasetComplete.__len__": [[52, 54], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.EditDistanceDatasetComplete.__getitem__": [[55, 66], ["torch.cat", "dataset.EditDistanceDatasetComplete.sequences[].unsqueeze", "dataset.EditDistanceDatasetComplete.sequences[].unsqueeze"], "methods", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.EditDistanceDatasetSupervision.__init__": [[69, 80], ["util.data_handling.data_loader.index_to_one_hot", "enumerate"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.EditDistanceDatasetSupervision.__len__": [[81, 83], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset.EditDistanceDatasetSupervision.__getitem__": [[84, 94], ["random.randint", "torch.cat", "dataset.EditDistanceDatasetSupervision.sequences[].unsqueeze", "dataset.EditDistanceDatasetSupervision.sequences[].unsqueeze"], "methods", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.EditDistanceDatasetGenerator.__init__": [[32, 64], ["random.seed", "N_batches.keys", "print", "list", "torch.from_numpy().long", "torch.from_numpy().float", "multiprocessing.Pool", "time.time", "list", "print", "zip", "matplotlib.hist", "matplotlib.show", "tqdm.tqdm.tqdm", "torch.from_numpy", "torch.from_numpy", "pool.imap", "numpy.reshape", "numpy.asarray", "numpy.asarray", "time.time", "numpy.asarray", "range"], "methods", ["None"], ["S", "=", "k_mutations", "(", "S_ref", ",", "np", ".", "random", ".", "randint", "(", "min_changes", ",", "max_changes", ")", ")", "\n", "sequences_queries", ".", "append", "(", "S", ")", "\n", "\n", "# convert to strings for distance routine", "\n", "", "sequences_references_str", "=", "[", "\"\"", ".", "join", "(", "chr", "(", "s", "+", "97", ")", "for", "s", "in", "S", ")", "for", "S", "in", "sequences_references", "]", "\n", "sequences_queries_str", "=", "[", "\"\"", ".", "join", "(", "chr", "(", "s", "+", "97", ")", "for", "s", "in", "S", ")", "for", "S", "in", "sequences_queries", "]", "\n", "\n", "# compute distances and reference with minimum distance", "\n", "distances", "=", "cross_distance_matrix_threads", "(", "sequences_references_str", ",", "sequences_queries_str", ",", "5", ")", "\n", "labels", "=", "np", ".", "argmin", "(", "distances", ",", "axis", "=", "0", ")", "\n", "\n", "# plot histogram of minimum distances", "\n", "if", "plot", ":", "\n", "            ", "plt", ".", "hist", "(", "x", "=", "np", ".", "min", "(", "distances", ",", "axis", "=", "0", ")", ",", "bins", "=", "'auto'", ",", "color", "=", "'#0504aa'", ",", "alpha", "=", "0.7", ",", "rwidth", "=", "0.85", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "# convert to torch", "\n", "", "self", ".", "sequences_references", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "sequences_references", ")", ")", ".", "long", "(", ")", "\n", "self", ".", "sequences_queries", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "sequences_queries", ")", ")", ".", "long", "(", ")", "\n", "self", ".", "labels", "=", "torch", ".", "from_numpy", "(", "labels", ")", ".", "float", "(", ")", "\n", "\n", "", "def", "save_as_pickle", "(", "self", ",", "filename", ")", ":", "\n", "        ", "directory", "=", "os", ".", "path", ".", "dirname", "(", "filename", ")", "\n", "if", "directory", "!=", "''", "and", "not", "os", ".", "path", ".", "exists", "(", "directory", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "directory", ")", "\n", "\n", "", "with", "open", "(", "filename", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "(", "self", ".", "sequences_references", ",", "self", ".", "sequences_queries", ",", "self", ".", "labels", ")", ",", "f", ")", "\n", "\n", "\n", "", "", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--out'", ",", "type", "=", "str", ",", "default", "=", "\"./data/closest_large_1024.pkl\"", ",", "help", "=", "'Output data path'", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.EditDistanceDatasetGenerator.save_as_pickle": [[65, 72], ["os.path.dirname", "os.makedirs", "open", "pickle.dump", "os.path.exists"], "methods", ["None"], ["parser", ".", "add_argument", "(", "'--N_reference'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "help", "=", "'Number of references'", ")", "\n", "parser", ".", "add_argument", "(", "'--N_query'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "help", "=", "'Number of queries'", ")", "\n", "parser", ".", "add_argument", "(", "'--len_sequence'", ",", "type", "=", "int", ",", "default", "=", "1024", ",", "help", "=", "'Length of sequences'", ")", "\n", "parser", ".", "add_argument", "(", "'--min_changes'", ",", "type", "=", "float", ",", "default", "=", "50", ",", "help", "=", "'Minimum number of mutations'", ")", "\n", "parser", ".", "add_argument", "(", "'--max_changes'", ",", "type", "=", "float", ",", "default", "=", "600", ",", "help", "=", "'Maximum number of mutations'", ")", "\n", "parser", ".", "add_argument", "(", "'--initials'", ",", "type", "=", "float", ",", "default", "=", "200", ",", "help", "=", "'Initial independently generated sequences'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'Random seed'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.generate_batch": [[16, 28], ["range", "util.bioinformatics_algorithms.edit_distance.cross_distance_matrix", "string_generator.generate", "util.data_handling.string_generator.k_mutations", "sequences.append", "random.randint", "numpy.random.geometric", "chr"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.k_mutations"], ["seed", "=", "0", ",", "plot", "=", "False", ")", ":", "\n", "        ", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "# generate some sequences independently at random", "\n", "sequences_references", "=", "[", "string_generator", ".", "generate", "(", "len_sequence", ")", "for", "_", "in", "range", "(", "initials", ")", "]", "\n", "sequences_queries", "=", "[", "]", "\n", "\n", "# generate the rest of references from previously generated ones", "\n", "for", "i", "in", "range", "(", "N_reference", "-", "initials", ")", ":", "\n", "            ", "S_ref", "=", "sequences_references", "[", "random", ".", "randint", "(", "0", ",", "len", "(", "sequences_references", ")", "-", "1", ")", "]", "\n", "S", "=", "k_mutations", "(", "S_ref", ",", "np", ".", "random", ".", "randint", "(", "min_changes", ",", "max_changes", ")", ")", "\n", "sequences_references", ".", "append", "(", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.triplet.train.general_arg_parser": [[21, 46], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "general_arg_parser", "(", ")", ":", "\n", "    ", "\"\"\" Parsing of parameters common to all the different models \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset path'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'Disables CUDA training (GPU)'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "42", ",", "help", "=", "'Random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'Number of epochs to train'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.001", ",", "help", "=", "'Initial learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'Weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'Dropout rate (1 - keep probability)'", ")", "\n", "parser", ".", "add_argument", "(", "'--patience'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Patience'", ")", "\n", "parser", ".", "add_argument", "(", "'--print_every'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Print training results every'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'Batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--embedding_size'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'Size of embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--distance'", ",", "type", "=", "str", ",", "default", "=", "'euclidean'", ",", "help", "=", "'Type of distance to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--workers'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'Number of workers'", ")", "\n", "parser", ".", "add_argument", "(", "'--loss'", ",", "type", "=", "str", ",", "default", "=", "\"mse\"", ",", "help", "=", "'Loss function to use (mse, mape or mae)'", ")", "\n", "parser", ".", "add_argument", "(", "'--plot'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'Plot real vs predicted distances'", ")", "\n", "parser", ".", "add_argument", "(", "'--closest_data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset for closest string retrieval tests'", ")", "\n", "parser", ".", "add_argument", "(", "'--hierarchical_data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset for hierarchical clustering'", ")", "\n", "parser", ".", "add_argument", "(", "'--scaling'", ",", "type", "=", "str", ",", "default", "=", "'False'", ",", "help", "=", "'Project to hypersphere (for hyperbolic)'", ")", "\n", "parser", ".", "add_argument", "(", "'--hyp_optimizer'", ",", "type", "=", "str", ",", "default", "=", "'Adam'", ",", "help", "=", "'Optimizer for hyperbolic (Adam or RAdam)'", ")", "\n", "parser", ".", "add_argument", "(", "'--margin'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'Margin loss'", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.triplet.train.execute_train": [[48, 152], ["print", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "train.load_triplet_dataset", "util.data_handling.data_loader.get_dataloaders", "types.SimpleNamespace", "print", "model_class", "closest_string.triplet.models.triplet_encoder.TripletEncoder", "closest_string.triplet.models.triplet_encoder.TripletEncoder.to", "functools.partial", "sum", "print", "time.time", "range", "print", "print", "print", "closest_string.triplet.models.triplet_encoder.TripletEncoder.load_state_dict", "util.data_handling.data_loader.get_dataloaders.keys", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "edit_distance.models.hyperbolics.RAdam", "torch.Adam", "time.time", "train.train", "train.test", "torch.load", "torch.load", "train.test", "print", "print", "closest_string.test.closest_string_testing", "print", "hierarchical_clustering.unsupervised.unsupervised.hierarchical_clustering_testing", "vars", "closest_string.triplet.models.triplet_encoder.TripletEncoder.parameters", "closest_string.triplet.models.triplet_encoder.TripletEncoder.parameters", "p.numel", "print", "sys.stdout.flush", "torch.save", "torch.save", "print", "closest_string.triplet.models.triplet_encoder.TripletEncoder.parameters", "closest_string.triplet.models.triplet_encoder.TripletEncoder.state_dict", "os.remove", "time.time", "time.time"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.triplet.train.load_triplet_dataset", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.get_dataloaders", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.train", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.test", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.test", "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string.test.closest_string_testing", "home.repos.pwc.inspect_result.gcorso_neuroseed.unsupervised.unsupervised.hierarchical_clustering_testing"], ["", "def", "execute_train", "(", "model_class", ",", "model_args", ",", "args", ")", ":", "\n", "# set device", "\n", "    ", "args", ".", "cuda", "=", "not", "args", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "device", "=", "'cuda'", "if", "args", ".", "cuda", "else", "'cpu'", "\n", "print", "(", "'Using device:'", ",", "device", ")", "\n", "\n", "# set the random seed", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "# load data", "\n", "", "datasets", "=", "load_triplet_dataset", "(", "args", ".", "data", ")", "\n", "loaders", "=", "get_dataloaders", "(", "datasets", ",", "batch_size", "=", "args", ".", "batch_size", ",", "workers", "=", "args", ".", "workers", ")", "\n", "\n", "# fix hyperparameters", "\n", "model_args", "=", "SimpleNamespace", "(", "**", "model_args", ")", "\n", "model_args", ".", "device", "=", "device", "\n", "model_args", ".", "len_sequence", "=", "datasets", "[", "'train'", "]", ".", "len_sequence", "\n", "model_args", ".", "embedding_size", "=", "args", ".", "embedding_size", "\n", "model_args", ".", "dropout", "=", "args", ".", "dropout", "\n", "print", "(", "\"Length of sequence\"", ",", "datasets", "[", "'train'", "]", ".", "len_sequence", ")", "\n", "args", ".", "scaling", "=", "True", "if", "args", ".", "scaling", "==", "'True'", "else", "False", "\n", "\n", "# generate model", "\n", "embedding_model", "=", "model_class", "(", "**", "vars", "(", "model_args", ")", ")", "\n", "model", "=", "TripletEncoder", "(", "embedding_model", "=", "embedding_model", ",", "distance", "=", "args", ".", "distance", ",", "scaling", "=", "args", ".", "scaling", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "# select optimizer", "\n", "if", "args", ".", "distance", "==", "'hyperbolic'", "and", "args", ".", "hyp_optimizer", "==", "'RAdam'", ":", "\n", "        ", "optimizer", "=", "RAdam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ",", "weight_decay", "=", "args", ".", "weight_decay", ")", "\n", "\n", "# select loss", "\n", "", "loss", "=", "partial", "(", "triplet_loss", ",", "margin", "=", "args", ".", "margin", ",", "device", "=", "device", ")", "\n", "\n", "# print total number of parameters", "\n", "total_params", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "print", "(", "'Total params'", ",", "total_params", ")", "\n", "\n", "# Train model", "\n", "t_total", "=", "time", ".", "time", "(", ")", "\n", "bad_counter", "=", "0", "\n", "best", "=", "1e10", "\n", "best_epoch", "=", "-", "1", "\n", "start_epoch", "=", "0", "\n", "\n", "for", "epoch", "in", "range", "(", "start_epoch", ",", "args", ".", "epochs", ")", ":", "\n", "        ", "t", "=", "time", ".", "time", "(", ")", "\n", "loss_train", "=", "train", "(", "model", ",", "loaders", "[", "'train'", "]", ",", "optimizer", ",", "loss", ",", "device", ")", "\n", "loss_val", "=", "test", "(", "model", ",", "loaders", "[", "'val'", "]", ",", "loss", ",", "device", ")", "\n", "\n", "# print progress", "\n", "if", "epoch", "%", "args", ".", "print_every", "==", "0", ":", "\n", "            ", "print", "(", "'Epoch: {:04d}'", ".", "format", "(", "epoch", "+", "1", ")", ",", "\n", "'loss_train: {:.6f}'", ".", "format", "(", "loss_train", ")", ",", "\n", "'loss_val: {:.6f}'", ".", "format", "(", "loss_val", ")", ",", "\n", "'time: {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "if", "loss_val", "<", "best", ":", "\n", "# save current model", "\n", "            ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "'{}.pkl'", ".", "format", "(", "epoch", ")", ")", "\n", "# remove previous model", "\n", "if", "best_epoch", ">=", "0", ":", "\n", "                ", "os", ".", "remove", "(", "'{}.pkl'", ".", "format", "(", "best_epoch", ")", ")", "\n", "# update training variables", "\n", "", "best", "=", "loss_val", "\n", "best_epoch", "=", "epoch", "\n", "bad_counter", "=", "0", "\n", "", "else", ":", "\n", "            ", "bad_counter", "+=", "1", "\n", "\n", "", "if", "bad_counter", "==", "args", ".", "patience", ":", "\n", "            ", "print", "(", "'Early stop at epoch {} (no improvement in last {} epochs)'", ".", "format", "(", "epoch", "+", "1", ",", "bad_counter", ")", ")", "\n", "break", "\n", "\n", "", "", "print", "(", "'Optimization Finished!'", ")", "\n", "print", "(", "'Total time elapsed: {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t_total", ")", ")", "\n", "\n", "# Restore best model", "\n", "print", "(", "'Loading {}th epoch'", ".", "format", "(", "best_epoch", "+", "1", ")", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "'{}.pkl'", ".", "format", "(", "best_epoch", ")", ")", ")", "\n", "\n", "# Testing", "\n", "for", "dset", "in", "loaders", ".", "keys", "(", ")", ":", "\n", "        ", "avg_loss", "=", "test", "(", "model", ",", "loaders", "[", "dset", "]", ",", "loss", ",", "device", ")", "\n", "print", "(", "'Final results {}: loss = {:.6f}'", ".", "format", "(", "dset", ",", "avg_loss", ")", ")", "\n", "\n", "\n", "# Nearest neighbour retrieval", "\n", "", "if", "args", ".", "closest_data_path", "!=", "''", ":", "\n", "        ", "print", "(", "\"Closest string retrieval\"", ")", "\n", "closest_string_testing", "(", "encoder_model", "=", "model", ",", "data_path", "=", "args", ".", "closest_data_path", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "device", "=", "device", ",", "distance", "=", "args", ".", "distance", ")", "\n", "\n", "# Hierarchical clustering", "\n", "", "if", "args", ".", "hierarchical_data_path", "!=", "''", ":", "\n", "        ", "print", "(", "\"Hierarchical clustering\"", ")", "\n", "hierarchical_clustering_testing", "(", "encoder_model", "=", "model", ",", "data_path", "=", "args", ".", "hierarchical_data_path", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "device", "=", "device", ",", "distance", "=", "args", ".", "distance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.triplet.train.load_triplet_dataset": [[156, 164], ["sequences.keys", "open", "pickle.load", "closest_string.triplet.dataset.TripletDataset", "sequences[].unsqueeze", "distances[].unsqueeze"], "function", ["None"], ["", "", "def", "load_triplet_dataset", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "sequences", ",", "distances", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "datasets", "=", "{", "}", "\n", "for", "key", "in", "sequences", ".", "keys", "(", ")", ":", "\n", "        ", "datasets", "[", "key", "]", "=", "TripletDataset", "(", "sequences", "[", "key", "]", ".", "unsqueeze", "(", "0", ")", ",", "distances", "[", "key", "]", ".", "unsqueeze", "(", "0", ")", ",", "multiplicity", "=", "10", ")", "\n", "", "return", "datasets", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.triplet.train.train": [[166, 187], ["util.ml_and_math.loss_functions.AverageMeter", "model.train", "sequences.to.to", "optimizer.zero_grad", "model", "loss", "loss.backward", "optimizer.step", "util.ml_and_math.loss_functions.AverageMeter.update", "loss.data.item"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.train", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.loss", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.Arsinh.backward", "home.repos.pwc.inspect_result.gcorso_neuroseed.optim.radam.RAdam.step", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update"], ["", "def", "train", "(", "model", ",", "loader", ",", "optimizer", ",", "loss", ",", "device", ")", ":", "\n", "    ", "avg_loss", "=", "AverageMeter", "(", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "for", "sequences", "in", "loader", ":", "\n", "# move examples to right device", "\n", "        ", "sequences", "=", "sequences", ".", "to", "(", "device", ")", "\n", "\n", "# forward propagation", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "distances", "=", "model", "(", "sequences", ")", "\n", "\n", "# loss and backpropagation", "\n", "loss_train", "=", "loss", "(", "*", "distances", ")", "\n", "loss_train", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# keep track of average loss", "\n", "avg_loss", ".", "update", "(", "loss_train", ".", "data", ".", "item", "(", ")", ",", "sequences", ".", "shape", "[", "0", "]", ")", "\n", "\n", "", "return", "avg_loss", ".", "avg", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.triplet.train.test": [[189, 203], ["util.ml_and_math.loss_functions.AverageMeter", "model.eval", "sequences.to.to", "model", "loss().data.item", "util.ml_and_math.loss_functions.AverageMeter.update", "loss"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.loss"], ["", "def", "test", "(", "model", ",", "loader", ",", "loss", ",", "device", ")", ":", "\n", "    ", "avg_loss", "=", "AverageMeter", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "for", "sequences", "in", "loader", ":", "\n", "# move examples to right device", "\n", "        ", "sequences", "=", "sequences", ".", "to", "(", "device", ")", "\n", "\n", "# forward propagation and loss computation", "\n", "distances", "=", "model", "(", "sequences", ")", "\n", "loss_val", "=", "loss", "(", "*", "distances", ")", ".", "data", ".", "item", "(", ")", "\n", "avg_loss", ".", "update", "(", "loss_val", ",", "sequences", ".", "shape", "[", "0", "]", ")", "\n", "\n", "", "return", "avg_loss", ".", "avg", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.triplet.dataset.TripletDataset.__init__": [[8, 21], ["util.data_handling.data_loader.index_to_one_hot"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot"], ["\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sequences", ".", "shape", "[", "0", "]", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sequences", "[", "index", "]", "\n", "\n", "\n", "", "", "class", "QueryDataset", "(", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "sequences", ",", "labels", ")", ":", "\n", "        ", "self", ".", "sequences", "=", "index_to_one_hot", "(", "sequences", ")", "\n", "self", ".", "labels", "=", "labels", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.triplet.dataset.TripletDataset.__len__": [[22, 24], ["None"], "methods", ["None"], ["        ", "return", "self", ".", "sequences", ".", "shape", "[", "0", "]", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.triplet.dataset.TripletDataset.__getitem__": [[25, 47], ["torch.cat", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.equal", "torch.equal", "torch.equal", "random.randint", "random.randint", "dataset.TripletDataset.sequences[].unsqueeze", "dataset.TripletDataset.sequences[].unsqueeze", "dataset.TripletDataset.sequences[].unsqueeze", "dataset.TripletDataset.sequences[].unsqueeze", "dataset.TripletDataset.sequences[].unsqueeze", "dataset.TripletDataset.sequences[].unsqueeze"], "methods", ["None"], ["        ", "return", "self", ".", "sequences", "[", "index", "]", ",", "self", ".", "labels", "[", "index", "]", "", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.triplet_encoder.TripletEncoder.__init__": [[10, 21], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embedding_model", ",", "distance", "=", "'euclidean'", ",", "scaling", "=", "False", ")", ":", "\n", "        ", "super", "(", "TripletEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embedding_model", "=", "embedding_model", "\n", "self", ".", "distance", "=", "DISTANCE_TORCH", "[", "distance", "]", "\n", "self", ".", "distance_str", "=", "distance", "\n", "\n", "self", ".", "scaling", "=", "None", "\n", "if", "scaling", ":", "\n", "            ", "self", ".", "scaling", "=", "True", "\n", "self", ".", "radius", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "0.5", "]", ")", ",", "requires_grad", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.triplet_encoder.TripletEncoder.normalize_embeddings": [[22, 32], ["torch.normalize", "torch.normalize", "torch.normalize", "triplet_encoder.TripletEncoder.radius.clamp_min().clamp_max", "triplet_encoder.TripletEncoder.radius.clamp_min"], "methods", ["None"], ["", "", "def", "normalize_embeddings", "(", "self", ",", "embeddings", ")", ":", "\n", "        ", "\"\"\" Project embeddings to an hypersphere of a certain radius \"\"\"", "\n", "min_scale", "=", "1e-7", "\n", "\n", "if", "self", ".", "distance_str", "==", "'hyperbolic'", ":", "\n", "            ", "max_scale", "=", "1", "-", "1e-3", "\n", "", "else", ":", "\n", "            ", "max_scale", "=", "1e10", "\n", "\n", "", "return", "F", ".", "normalize", "(", "embeddings", ",", "p", "=", "2", ",", "dim", "=", "1", ")", "*", "self", ".", "radius", ".", "clamp_min", "(", "min_scale", ")", ".", "clamp_max", "(", "max_scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.triplet_encoder.TripletEncoder.encode": [[33, 39], ["triplet_encoder.TripletEncoder.embedding_model", "triplet_encoder.TripletEncoder.normalize_embeddings"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.normalize_embeddings"], ["", "def", "encode", "(", "self", ",", "sequence", ")", ":", "\n", "        ", "\"\"\" Use embedding model and normalization to encode some sequences. \"\"\"", "\n", "enc_sequence", "=", "self", ".", "embedding_model", "(", "sequence", ")", "\n", "if", "self", ".", "scaling", "is", "not", "None", ":", "\n", "            ", "enc_sequence", "=", "self", ".", "normalize_embeddings", "(", "enc_sequence", ")", "\n", "", "return", "enc_sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.triplet_encoder.TripletEncoder.forward": [[40, 54], ["sequence.reshape.reshape.reshape", "triplet_encoder.TripletEncoder.encode", "enc_sequence.reshape.reshape.reshape", "triplet_encoder.TripletEncoder.distance", "triplet_encoder.TripletEncoder.distance"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.encode"], ["", "def", "forward", "(", "self", ",", "sequence", ")", ":", "\n", "# flatten couples", "\n", "        ", "(", "B", ",", "_", ",", "N", ",", "_", ")", "=", "sequence", ".", "shape", "\n", "sequence", "=", "sequence", ".", "reshape", "(", "3", "*", "B", ",", "N", ",", "-", "1", ")", "\n", "\n", "# encode sequences", "\n", "enc_sequence", "=", "self", ".", "encode", "(", "sequence", ")", "\n", "\n", "# compute distances", "\n", "enc_sequence", "=", "enc_sequence", ".", "reshape", "(", "B", ",", "3", ",", "-", "1", ")", "\n", "distance_positive", "=", "self", ".", "distance", "(", "enc_sequence", "[", ":", ",", "0", "]", ",", "enc_sequence", "[", ":", ",", "1", "]", ")", "\n", "distance_negative", "=", "self", ".", "distance", "(", "enc_sequence", "[", ":", ",", "0", "]", ",", "enc_sequence", "[", ":", ",", "2", "]", ")", "\n", "\n", "return", "distance_positive", ",", "distance_negative", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.triplet_encoder.triplet_loss": [[56, 60], ["torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.max", "torch.max", "torch.max", "torch.mean", "torch.mean", "torch.mean", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["None"], ["", "", "def", "triplet_loss", "(", "distance_positive", ",", "distance_negative", ",", "device", ",", "margin", "=", "0.05", ")", ":", "\n", "    ", "zero", "=", "torch", ".", "tensor", "(", "[", "0.0", "]", ")", ".", "to", "(", "device", ")", "\n", "margins", "=", "torch", ".", "max", "(", "distance_positive", "-", "distance_negative", "+", "margin", ",", "zero", ")", "\n", "return", "torch", ".", "mean", "(", "margins", ")", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.mlp.HypHCMLP.__init__": [[8, 16], ["hierarchical_clustering.relaxed.models.model.HypHCModel.__init__", "util.ml_and_math.layers.MLP"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["def", "__init__", "(", "self", ",", "layers", ",", "hidden_size", ",", "dropout", "=", "0.0", ",", "batch_norm", "=", "False", ",", "rank", "=", "2", ",", "temperature", "=", "0.05", ",", "\n", "init_size", "=", "1e-3", ",", "max_scale", "=", "1.", "-", "1e-3", ",", "alphabet_size", "=", "4", ",", "sequence_length", "=", "128", ",", "device", "=", "'cpu'", ")", ":", "\n", "        ", "super", "(", "HypHCMLP", ",", "self", ")", ".", "__init__", "(", "temperature", "=", "temperature", ",", "init_size", "=", "init_size", ",", "max_scale", "=", "max_scale", ")", "\n", "\n", "self", ".", "alphabet_size", "=", "alphabet_size", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "encoder", "=", "MLP", "(", "in_size", "=", "alphabet_size", "*", "sequence_length", ",", "hidden_size", "=", "hidden_size", ",", "out_size", "=", "rank", ",", "\n", "layers", "=", "layers", ",", "mid_activation", "=", "'relu'", ",", "dropout", "=", "dropout", ",", "device", "=", "device", ",", "mid_b_norm", "=", "batch_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.mlp.HypHCMLP.encode": [[17, 22], ["hierarchical_clustering.relaxed.models.model.index_to_one_hot", "sequences.reshape.reshape.reshape", "mlp.HypHCMLP.encoder", "sequences.reshape.reshape.long"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot"], ["", "def", "encode", "(", "self", ",", "triple_ids", "=", "None", ",", "sequences", "=", "None", ")", ":", "\n", "        ", "sequences", "=", "index_to_one_hot", "(", "sequences", ".", "long", "(", ")", ",", "device", "=", "self", ".", "device", ")", "\n", "sequences", "=", "sequences", ".", "reshape", "(", "(", "*", "sequences", ".", "shape", "[", ":", "-", "2", "]", ",", "-", "1", ")", ")", "\n", "e", "=", "self", ".", "encoder", "(", "sequences", ")", "\n", "return", "e", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.embeddings.HypHCEmbeddings.__init__": [[16, 22], ["hierarchical_clustering.relaxed.models.model.HypHCModel.__init__", "torch.Embedding", "torch.Embedding", "util.ml_and_math.poincare.project", "torch.rand", "torch.rand", "torch.rand", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.project"], ["def", "__init__", "(", "self", ",", "n_nodes", "=", "1", ",", "rank", "=", "2", ",", "temperature", "=", "0.05", ",", "init_size", "=", "1e-3", ",", "max_scale", "=", "1.", "-", "1e-3", ")", ":", "\n", "        ", "super", "(", "HypHCEmbeddings", ",", "self", ")", ".", "__init__", "(", "temperature", "=", "temperature", ",", "init_size", "=", "init_size", ",", "max_scale", "=", "max_scale", ")", "\n", "self", ".", "n_nodes", "=", "n_nodes", "\n", "self", ".", "embeddings", "=", "nn", ".", "Embedding", "(", "n_nodes", ",", "rank", ")", "\n", "self", ".", "embeddings", ".", "weight", ".", "data", "=", "project", "(", "\n", "self", ".", "scale", "*", "(", "2", "*", "torch", ".", "rand", "(", "(", "n_nodes", ",", "rank", ")", ")", "-", "1.0", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.embeddings.HypHCEmbeddings.encode": [[24, 27], ["embeddings.HypHCEmbeddings.embeddings"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "triple_ids", ",", "sequences", "=", "None", ")", ":", "\n", "        ", "e", "=", "self", ".", "embeddings", "(", "triple_ids", ")", "\n", "return", "e", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.__init__": [[25, 31], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["def", "__init__", "(", "self", ",", "temperature", "=", "0.05", ",", "init_size", "=", "1e-2", ",", "max_scale", "=", "1.", "-", "1e-3", ")", ":", "\n", "        ", "super", "(", "HypHCModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "temperature", "=", "temperature", "\n", "self", ".", "scale", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "init_size", "]", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "init_size", "=", "init_size", "\n", "self", ".", "max_scale", "=", "max_scale", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.anneal_temperature": [[32, 35], ["None"], "methods", ["None"], ["", "def", "anneal_temperature", "(", "self", ",", "anneal_factor", ")", ":", "\n", "        ", "\"\"\" anneal_factor: scalar for temperature decay \"\"\"", "\n", "self", ".", "temperature", "*=", "anneal_factor", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.normalize_embeddings": [[36, 41], ["torch.normalize", "torch.normalize", "torch.normalize", "model.HypHCModel.scale.clamp_min().clamp_max", "model.HypHCModel.scale.clamp_min"], "methods", ["None"], ["", "def", "normalize_embeddings", "(", "self", ",", "embeddings", ")", ":", "\n", "        ", "\"\"\"Normalize leaves embeddings to have the lie on a diameter.\"\"\"", "\n", "min_scale", "=", "self", ".", "init_size", "\n", "max_scale", "=", "self", ".", "max_scale", "\n", "return", "F", ".", "normalize", "(", "embeddings", ",", "p", "=", "2", ",", "dim", "=", "-", "1", ")", "*", "self", ".", "scale", ".", "clamp_min", "(", "min_scale", ")", ".", "clamp_max", "(", "max_scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.encode": [[42, 44], ["None"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "triple_ids", ",", "sequences", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.loss": [[45, 68], ["model.HypHCModel.encode", "model.HypHCModel.normalize_embeddings", "e.reshape.reshape.reshape", "hierarchical_clustering.relaxed.utils.lca.hyp_lca", "hierarchical_clustering.relaxed.utils.lca.hyp_lca", "hierarchical_clustering.relaxed.utils.lca.hyp_lca", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "triple_ids.reshape", "sequences.reshape", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.encode", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.normalize_embeddings", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca.hyp_lca", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca.hyp_lca", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca.hyp_lca"], ["", "def", "loss", "(", "self", ",", "triple_ids", ",", "sequences", ",", "similarities", ")", ":", "\n", "        ", "\"\"\" Computes the HypHCEmbeddings loss.\n        Args:\n            triple_ids: B x 3 tensor with triple ids\n            sequences:  B x 3 x N tensor with elements indexes\n            similarities: B x 3 tensor with pairwise similarities for triples \n                          [s12, s13, s23]\n        \"\"\"", "\n", "(", "B", ",", "_", ",", "N", ")", "=", "sequences", ".", "shape", "\n", "triple_ids", ",", "sequences", "=", "triple_ids", ".", "reshape", "(", "(", "3", "*", "B", ")", ")", ",", "sequences", ".", "reshape", "(", "(", "3", "*", "B", ",", "N", ")", ")", "\n", "\n", "e", "=", "self", ".", "encode", "(", "triple_ids", ",", "sequences", ")", "\n", "e", "=", "self", ".", "normalize_embeddings", "(", "e", ")", "\n", "e", "=", "e", ".", "reshape", "(", "(", "B", ",", "3", ",", "-", "1", ")", ")", "\n", "\n", "d_12", "=", "hyp_lca", "(", "e", "[", ":", ",", "0", "]", ",", "e", "[", ":", ",", "1", "]", ",", "return_coord", "=", "False", ")", "\n", "d_13", "=", "hyp_lca", "(", "e", "[", ":", ",", "0", "]", ",", "e", "[", ":", ",", "2", "]", ",", "return_coord", "=", "False", ")", "\n", "d_23", "=", "hyp_lca", "(", "e", "[", ":", ",", "1", "]", ",", "e", "[", ":", ",", "2", "]", ",", "return_coord", "=", "False", ")", "\n", "lca_norm", "=", "torch", ".", "cat", "(", "[", "d_12", ",", "d_13", ",", "d_23", "]", ",", "dim", "=", "-", "1", ")", "\n", "weights", "=", "torch", ".", "softmax", "(", "lca_norm", "/", "self", ".", "temperature", ",", "dim", "=", "-", "1", ")", "\n", "w_ord", "=", "torch", ".", "sum", "(", "similarities", "*", "weights", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "total", "=", "torch", ".", "sum", "(", "similarities", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "-", "w_ord", "\n", "return", "torch", ".", "mean", "(", "total", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.decode_tree": [[69, 82], ["model.HypHCModel.normalize_embeddings", "util.ml_and_math.poincare.project().detach().cpu", "networkx.DiGraph", "enumerate", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "hierarchical_clustering.relaxed.utils.linkage.nn_merge_uf_fast_np", "hierarchical_clustering.relaxed.utils.linkage.sl_from_embeddings", "networkx.DiGraph.add_edge", "util.ml_and_math.poincare.project().detach", "y.transpose", "util.ml_and_math.poincare.project"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.normalize_embeddings", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.linkage.nn_merge_uf_fast_np", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.linkage.sl_from_embeddings", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.project"], ["", "def", "decode_tree", "(", "self", ",", "embedded_sequences", ",", "fast_decoding", ")", ":", "\n", "        ", "\"\"\"Build a binary tree (nx graph) from leaves' embeddings. Assume points are normalized to same radius.\"\"\"", "\n", "leaves_embeddings", "=", "self", ".", "normalize_embeddings", "(", "embedded_sequences", ")", "\n", "leaves_embeddings", "=", "project", "(", "leaves_embeddings", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "sim_fn", "=", "lambda", "x", ",", "y", ":", "torch", ".", "matmul", "(", "x", ",", "y", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "if", "fast_decoding", ":", "\n", "            ", "parents", "=", "nn_merge_uf_fast_np", "(", "leaves_embeddings", ",", "S", "=", "sim_fn", ",", "partition_ratio", "=", "1.2", ")", "\n", "", "else", ":", "\n", "            ", "parents", "=", "sl_from_embeddings", "(", "leaves_embeddings", ",", "sim_fn", ")", "\n", "", "tree", "=", "nx", ".", "DiGraph", "(", ")", "\n", "for", "i", ",", "j", "in", "enumerate", "(", "parents", "[", ":", "-", "1", "]", ")", ":", "\n", "            ", "tree", ".", "add_edge", "(", "j", ",", "i", ")", "\n", "", "return", "tree", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.index_to_one_hot": [[16, 20], ["torch.cat", "torch.cat", "torch.cat", "torch.eye", "torch.eye", "torch.eye", "torch.zeros", "torch.zeros", "torch.zeros"], "function", ["None"], ["def", "index_to_one_hot", "(", "x", ",", "alphabet_size", "=", "4", ",", "device", "=", "'cpu'", ")", ":", "\n", "# add one row of zeros because the -1 represents the absence of element and it is encoded with zeros", "\n", "    ", "x", "=", "torch", ".", "cat", "(", "(", "torch", ".", "eye", "(", "alphabet_size", ",", "device", "=", "device", ")", ",", "torch", ".", "zeros", "(", "(", "1", ",", "alphabet_size", ")", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "0", ")", "[", "x", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.linear.HypHCLinear.__init__": [[9, 15], ["hierarchical_clustering.relaxed.models.model.HypHCModel.__init__", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["def", "__init__", "(", "self", ",", "rank", "=", "2", ",", "temperature", "=", "0.05", ",", "init_size", "=", "1e-3", ",", "max_scale", "=", "1.", "-", "1e-3", ",", "alphabet_size", "=", "4", ",", "\n", "sequence_length", "=", "128", ",", "device", "=", "'cpu'", ")", ":", "\n", "        ", "super", "(", "HypHCLinear", ",", "self", ")", ".", "__init__", "(", "temperature", "=", "temperature", ",", "init_size", "=", "init_size", ",", "max_scale", "=", "max_scale", ")", "\n", "self", ".", "alphabet_size", "=", "alphabet_size", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "sequence_length", "*", "alphabet_size", ",", "rank", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.linear.HypHCLinear.encode": [[16, 21], ["hierarchical_clustering.relaxed.models.model.index_to_one_hot", "sequences.reshape.reshape.reshape", "linear.HypHCLinear.linear", "sequences.reshape.reshape.long"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot"], ["", "def", "encode", "(", "self", ",", "triple_ids", "=", "None", ",", "sequences", "=", "None", ")", ":", "\n", "        ", "sequences", "=", "index_to_one_hot", "(", "sequences", ".", "long", "(", ")", ",", "device", "=", "self", ".", "device", ",", "alphabet_size", "=", "self", ".", "alphabet_size", ")", "\n", "sequences", "=", "sequences", ".", "reshape", "(", "(", "*", "sequences", ".", "shape", "[", ":", "-", "2", "]", ",", "-", "1", ")", ")", "\n", "e", "=", "self", ".", "linear", "(", "sequences", ")", "\n", "return", "e", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.cnn.HypHCCNN.__init__": [[11, 36], ["hierarchical_clustering.relaxed.models.model.HypHCModel.__init__", "torch.Linear", "torch.Linear", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "range", "util.ml_and_math.layers.MLP", "cnn.HypHCCNN.conv.add_module", "torch.Conv1d", "torch.Conv1d", "cnn.HypHCCNN.conv.add_module", "cnn.HypHCCNN.conv.add_module", "cnn.HypHCCNN.conv.add_module", "str", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.AvgPool1d", "torch.AvgPool1d", "cnn.HypHCCNN.conv.add_module", "str", "str", "str", "torch.MaxPool1d", "torch.MaxPool1d", "str"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["def", "__init__", "(", "self", ",", "layers", ",", "channels", ",", "kernel_size", ",", "readout_layers", "=", "2", ",", "non_linearity", "=", "True", ",", "pooling", "=", "'avg'", ",", "dropout", "=", "0.0", ",", "\n", "batch_norm", "=", "False", ",", "rank", "=", "2", ",", "temperature", "=", "0.05", ",", "init_size", "=", "1e-3", ",", "max_scale", "=", "1.", "-", "1e-3", ",", "alphabet_size", "=", "4", ",", "\n", "sequence_length", "=", "128", ",", "device", "=", "'cpu'", ")", ":", "\n", "        ", "super", "(", "HypHCCNN", ",", "self", ")", ".", "__init__", "(", "temperature", "=", "temperature", ",", "init_size", "=", "init_size", ",", "max_scale", "=", "max_scale", ")", "\n", "\n", "self", ".", "alphabet_size", "=", "alphabet_size", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "embedding", "=", "nn", ".", "Linear", "(", "alphabet_size", ",", "channels", ")", "\n", "self", ".", "conv", "=", "torch", ".", "nn", ".", "Sequential", "(", ")", "\n", "for", "l", "in", "range", "(", "layers", ")", ":", "\n", "            ", "self", ".", "conv", ".", "add_module", "(", "'conv_'", "+", "str", "(", "l", "+", "1", ")", ",", "nn", ".", "Conv1d", "(", "in_channels", "=", "channels", ",", "out_channels", "=", "channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "padding", "=", "kernel_size", "//", "2", ")", ")", "\n", "if", "batch_norm", ":", "\n", "                ", "self", ".", "conv", ".", "add_module", "(", "'batchnorm_'", "+", "str", "(", "l", "+", "1", ")", ",", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "channels", ")", ")", "\n", "", "if", "non_linearity", ":", "\n", "                ", "self", ".", "conv", ".", "add_module", "(", "'relu_'", "+", "str", "(", "l", "+", "1", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "\n", "", "if", "pooling", "==", "'avg'", ":", "\n", "                ", "self", ".", "conv", ".", "add_module", "(", "pooling", "+", "'_pool_'", "+", "str", "(", "l", "+", "1", ")", ",", "nn", ".", "AvgPool1d", "(", "2", ")", ")", "\n", "", "elif", "pooling", "==", "'max'", ":", "\n", "                ", "self", ".", "conv", ".", "add_module", "(", "pooling", "+", "'pool_'", "+", "str", "(", "l", "+", "1", ")", ",", "nn", ".", "MaxPool1d", "(", "2", ")", ")", "\n", "\n", "", "", "flat_size", "=", "channels", "*", "sequence_length", "if", "pooling", "==", "'none'", "else", "channels", "*", "(", "sequence_length", "//", "2", "**", "layers", ")", "\n", "self", ".", "readout", "=", "MLP", "(", "in_size", "=", "flat_size", ",", "hidden_size", "=", "rank", ",", "out_size", "=", "rank", ",", "\n", "layers", "=", "readout_layers", ",", "mid_activation", "=", "'relu'", ",", "dropout", "=", "dropout", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.cnn.HypHCCNN.encode": [[37, 46], ["hierarchical_clustering.relaxed.models.model.index_to_one_hot", "cnn.HypHCCNN.embedding", "cnn.HypHCCNN.transpose", "cnn.HypHCCNN.conv", "enc_sequences.reshape.reshape.reshape", "cnn.HypHCCNN.readout", "cnn.HypHCCNN.long"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot"], ["", "def", "encode", "(", "self", ",", "triple_ids", "=", "None", ",", "sequences", "=", "None", ")", ":", "\n", "        ", "(", "B", ",", "N", ")", "=", "sequences", ".", "shape", "\n", "sequences", "=", "index_to_one_hot", "(", "sequences", ".", "long", "(", ")", ",", "device", "=", "self", ".", "device", ")", "\n", "sequences", "=", "self", ".", "embedding", "(", "sequences", ")", "\n", "enc_sequences", "=", "sequences", ".", "transpose", "(", "1", ",", "2", ")", "\n", "enc_sequences", "=", "self", ".", "conv", "(", "enc_sequences", ")", "\n", "enc_sequences", "=", "enc_sequences", ".", "reshape", "(", "B", ",", "-", "1", ")", "\n", "embedding", "=", "self", ".", "readout", "(", "enc_sequences", ")", "\n", "return", "embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.cnn.CNN.__init__": [[50, 77], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "range", "util.ml_and_math.layers.MLP", "cnn.CNN.to", "cnn.CNN.conv.add_module", "torch.Conv1d", "torch.Conv1d", "cnn.CNN.conv.add_module", "cnn.CNN.conv.add_module", "cnn.CNN.conv.add_module", "str", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.AvgPool1d", "torch.AvgPool1d", "cnn.CNN.conv.add_module", "str", "str", "str", "torch.MaxPool1d", "torch.MaxPool1d", "str"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["    ", "def", "__init__", "(", "self", ",", "len_text", ",", "embedding_size", ",", "channels", ",", "device", ",", "readout_layers", "=", "2", ",", "layers", "=", "3", ",", "dropout", "=", "0.", ",", "\n", "kernel_size", "=", "3", ",", "alphabet_size", "=", "4", ",", "pooling", "=", "'avg'", ",", "non_linearity", "=", "True", ",", "batch_norm", "=", "False", ")", ":", "\n", "        ", "super", "(", "CNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "pooling", "==", "'none'", "or", "pooling", "==", "'avg'", "or", "pooling", "==", "'max'", ",", "\"Wrong pooling type\"", "\n", "\n", "self", ".", "len_text", "=", "len_text", "\n", "self", ".", "layers", "=", "layers", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "embedding", "=", "nn", ".", "Linear", "(", "alphabet_size", ",", "channels", ")", "\n", "self", ".", "conv", "=", "torch", ".", "nn", ".", "Sequential", "(", ")", "\n", "for", "l", "in", "range", "(", "self", ".", "layers", ")", ":", "\n", "            ", "self", ".", "conv", ".", "add_module", "(", "'conv_'", "+", "str", "(", "l", "+", "1", ")", ",", "nn", ".", "Conv1d", "(", "in_channels", "=", "channels", ",", "out_channels", "=", "channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "padding", "=", "kernel_size", "//", "2", ")", ")", "\n", "if", "batch_norm", ":", "\n", "                ", "self", ".", "conv", ".", "add_module", "(", "'batchnorm_'", "+", "str", "(", "l", "+", "1", ")", ",", "nn", ".", "BatchNorm1d", "(", "num_features", "=", "channels", ")", ")", "\n", "", "if", "non_linearity", ":", "\n", "                ", "self", ".", "conv", ".", "add_module", "(", "'relu_'", "+", "str", "(", "l", "+", "1", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "\n", "", "if", "pooling", "==", "'avg'", ":", "\n", "                ", "self", ".", "conv", ".", "add_module", "(", "pooling", "+", "'_pool_'", "+", "str", "(", "l", "+", "1", ")", ",", "nn", ".", "AvgPool1d", "(", "2", ")", ")", "\n", "", "elif", "pooling", "==", "'max'", ":", "\n", "                ", "self", ".", "conv", ".", "add_module", "(", "pooling", "+", "'pool_'", "+", "str", "(", "l", "+", "1", ")", ",", "nn", ".", "MaxPool1d", "(", "2", ")", ")", "\n", "\n", "", "", "flat_size", "=", "channels", "*", "len_text", "if", "pooling", "==", "'none'", "else", "channels", "*", "(", "len_text", "//", "2", "**", "layers", ")", "\n", "self", ".", "readout", "=", "MLP", "(", "in_size", "=", "flat_size", ",", "hidden_size", "=", "embedding_size", ",", "out_size", "=", "embedding_size", ",", "\n", "layers", "=", "readout_layers", ",", "mid_activation", "=", "'relu'", ",", "dropout", "=", "dropout", ",", "device", "=", "device", ")", "\n", "self", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.cnn.CNN.forward": [[78, 86], ["cnn.CNN.embedding", "cnn.CNN.transpose", "cnn.CNN.conv", "enc_text.reshape.reshape.reshape", "cnn.CNN.readout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "text", ")", ":", "\n", "        ", "(", "B", ",", "N", ",", "_", ")", "=", "text", ".", "shape", "\n", "text", "=", "self", ".", "embedding", "(", "text", ")", "\n", "enc_text", "=", "text", ".", "transpose", "(", "1", ",", "2", ")", "\n", "enc_text", "=", "self", ".", "conv", "(", "enc_text", ")", "\n", "enc_text", "=", "enc_text", ".", "reshape", "(", "B", ",", "-", "1", ")", "\n", "embedding", "=", "self", ".", "readout", "(", "enc_text", ")", "\n", "return", "embedding", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_autoencoder.PairDistanceAutoEncoder.__init__": [[10, 26], ["torch.Module.__init__", "geoopt.PoincareBall", "print"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ",", "distance", "=", "'euclidean'", ",", "std_noise", "=", "0.", ",", "normalization", "=", "-", "1", ",", "device", "=", "'cpu'", ",", "autoregressive", "=", "False", ")", ":", "\n", "        ", "super", "(", "PairDistanceAutoEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "std_noise", "=", "std_noise", "\n", "self", ".", "distance", "=", "DISTANCE_TORCH", "[", "distance", "]", "\n", "self", ".", "autoregressive", "=", "autoregressive", "\n", "\n", "self", ".", "manifold", "=", "None", "\n", "if", "distance", "==", "'hyperbolic'", ":", "\n", "            ", "self", ".", "manifold", "=", "PoincareBall", "(", ")", "\n", "print", "(", "\"Using poincare sampling\"", ")", "\n", "\n", "", "self", ".", "norm", "=", "normalization", "\n", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_autoencoder.PairDistanceAutoEncoder.normalization": [[27, 34], ["torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.norm", "torch.norm", "torch.norm", "torch.norm"], "methods", ["None"], ["", "def", "normalization", "(", "self", ",", "enc", ",", "hyp_max_norm", "=", "1", "-", "1e-2", ")", ":", "\n", "        ", "if", "self", ".", "norm", ">", "0", ":", "\n", "            ", "return", "self", ".", "norm", "*", "enc", "/", "torch", ".", "norm", "(", "enc", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "", "elif", "self", ".", "manifold", "is", "not", "None", ":", "\n", "            ", "scaling", "=", "torch", ".", "clamp", "(", "(", "hyp_max_norm", "/", "torch", ".", "norm", "(", "enc", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", ",", "max", "=", "1", ")", "\n", "return", "scaling", "*", "enc", "\n", "", "return", "enc", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_autoencoder.PairDistanceAutoEncoder.forward": [[35, 58], ["sequence.reshape.reshape.reshape", "pair_autoencoder.PairDistanceAutoEncoder.encoder", "pair_autoencoder.PairDistanceAutoEncoder.reshape", "pair_autoencoder.PairDistanceAutoEncoder.distance", "pair_autoencoder.PairDistanceAutoEncoder.normalization", "pair_autoencoder.PairDistanceAutoEncoder.sample", "pair_autoencoder.PairDistanceAutoEncoder.normalization", "pair_autoencoder.PairDistanceAutoEncoder.reshape", "pair_autoencoder.PairDistanceAutoEncoder.decoder", "pair_autoencoder.PairDistanceAutoEncoder.decoder"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.msa_autoencoder.MultipleAlignmentAutoEncoder.normalization", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_autoencoder.PairDistanceAutoEncoder.sample", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.msa_autoencoder.MultipleAlignmentAutoEncoder.normalization"], ["", "def", "forward", "(", "self", ",", "sequence", ")", ":", "\n", "        ", "(", "B", ",", "_", ",", "N", ")", "=", "sequence", ".", "shape", "\n", "sequence", "=", "sequence", ".", "reshape", "(", "2", "*", "B", ",", "N", ")", "\n", "\n", "# encoder", "\n", "enc_sequence", "=", "self", ".", "encoder", "(", "sequence", ")", "\n", "\n", "# compute distance", "\n", "enc_dist", "=", "enc_sequence", ".", "reshape", "(", "B", ",", "2", ",", "-", "1", ")", "\n", "distance", "=", "self", ".", "distance", "(", "enc_dist", "[", ":", ",", "0", "]", ",", "enc_dist", "[", ":", ",", "1", "]", ")", "\n", "\n", "# normalize if self.norm=True and add noise", "\n", "enc_sequence", "=", "self", ".", "normalization", "(", "enc_sequence", ")", "\n", "enc_sequence", "=", "self", ".", "sample", "(", "enc_sequence", ")", "\n", "enc_sequence", "=", "self", ".", "normalization", "(", "enc_sequence", ")", "\n", "\n", "# decode", "\n", "if", "self", ".", "autoregressive", ":", "\n", "            ", "dec_sequence", "=", "self", ".", "decoder", "(", "enc_sequence", ",", "sequence", ")", "\n", "", "else", ":", "\n", "            ", "dec_sequence", "=", "self", ".", "decoder", "(", "enc_sequence", ")", "\n", "", "dec_sequence", "=", "dec_sequence", ".", "reshape", "(", "B", ",", "2", ",", "N", ",", "-", "1", ")", "\n", "return", "distance", ",", "dec_sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_autoencoder.PairDistanceAutoEncoder.sample": [[59, 73], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "pair_autoencoder.PairDistanceAutoEncoder.manifold.assert_check_vector_on_tangent", "pair_autoencoder.PairDistanceAutoEncoder.manifold.transp", "pair_autoencoder.PairDistanceAutoEncoder.manifold.expmap", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "pair_autoencoder.PairDistanceAutoEncoder.manifold.lambda_x", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.expmap"], ["", "def", "sample", "(", "self", ",", "mean", ")", ":", "\n", "        ", "if", "self", ".", "manifold", "is", "None", ":", "\n", "            ", "return", "mean", "+", "self", ".", "std_noise", "*", "torch", ".", "randn_like", "(", "mean", ")", "\n", "", "else", ":", "\n", "# sampling Wrapped Normal distribution in the Poincar\u00e8 Ball", "\n", "# for more details see https://arxiv.org/pdf/1901.06033.pdf", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "dim", "=", "mean", ".", "shape", "[", "-", "1", "]", "\n", "v", "=", "self", ".", "std_noise", "*", "torch", ".", "randn_like", "(", "mean", ")", "\n", "self", ".", "manifold", ".", "assert_check_vector_on_tangent", "(", "torch", ".", "zeros", "(", "1", ",", "dim", ")", ".", "to", "(", "self", ".", "device", ")", ",", "v", ")", "\n", "v", "=", "v", "/", "self", ".", "manifold", ".", "lambda_x", "(", "torch", ".", "zeros", "(", "1", ",", "dim", ")", ".", "to", "(", "self", ".", "device", ")", ",", "keepdim", "=", "True", ")", "\n", "u", "=", "self", ".", "manifold", ".", "transp", "(", "torch", ".", "zeros", "(", "1", ",", "dim", ")", ".", "to", "(", "self", ".", "device", ")", ",", "mean", ",", "v", ")", "\n", "z", "=", "self", ".", "manifold", ".", "expmap", "(", "mean", ",", "u", ")", "\n", "return", "z", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.msa_autoencoder.MultipleAlignmentAutoEncoder.__init__": [[13, 23], ["torch.Module.__init__", "msa_autoencoder.MultipleAlignmentAutoEncoder.center_choice"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.msa_autoencoder.MultipleAlignmentAutoEncoder.center_choice"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ",", "device", ",", "distance", ",", "center", "=", "'geometric_median'", ",", "normalization", "=", "-", "1", ")", ":", "\n", "        ", "super", "(", "MultipleAlignmentAutoEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "center", "=", "None", "\n", "self", ".", "center_choice", "(", "center", ",", "distance", ")", "\n", "self", ".", "distance", "=", "distance", "\n", "self", ".", "norm", "=", "normalization", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.msa_autoencoder.MultipleAlignmentAutoEncoder.center_choice": [[24, 31], ["functools.partial"], "methods", ["None"], ["", "def", "center_choice", "(", "self", ",", "center", ",", "distance", ")", ":", "\n", "        ", "if", "center", "==", "'geometric_median'", ":", "\n", "            ", "self", ".", "center", "=", "partial", "(", "geometric_median_torch", ",", "distance", "=", "distance", ")", "\n", "", "elif", "center", "==", "'centroid'", ":", "\n", "            ", "self", ".", "center", "=", "centroid", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.msa_autoencoder.MultipleAlignmentAutoEncoder.normalization": [[32, 40], ["print", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm"], "methods", ["None"], ["", "", "def", "normalization", "(", "self", ",", "enc", ",", "hyp_max_norm", "=", "1", "-", "1e-2", ")", ":", "\n", "        ", "if", "self", ".", "norm", ">", "0", ":", "\n", "            ", "print", "(", "self", ".", "norm", ")", "\n", "return", "self", ".", "norm", "*", "enc", "/", "torch", ".", "norm", "(", "enc", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "", "elif", "self", ".", "distance", "==", "'hyperbolic'", ":", "\n", "            ", "scaling", "=", "(", "hyp_max_norm", "/", "torch", ".", "norm", "(", "enc", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", ".", "clamp_max", "(", "1", ")", "\n", "return", "scaling", "*", "enc", "\n", "", "return", "enc", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.msa_autoencoder.MultipleAlignmentAutoEncoder.forward": [[41, 57], ["sequences.reshape.reshape.reshape", "msa_autoencoder.MultipleAlignmentAutoEncoder.encoder", "msa_autoencoder.MultipleAlignmentAutoEncoder.normalization", "msa_autoencoder.MultipleAlignmentAutoEncoder.center", "msa_autoencoder.MultipleAlignmentAutoEncoder.to", "msa_autoencoder.MultipleAlignmentAutoEncoder.normalization", "msa_autoencoder.MultipleAlignmentAutoEncoder.decoder.decode_and_sample", "msa_autoencoder.MultipleAlignmentAutoEncoder.reshape", "msa_autoencoder.MultipleAlignmentAutoEncoder.reshape"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.msa_autoencoder.MultipleAlignmentAutoEncoder.normalization", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.msa_autoencoder.MultipleAlignmentAutoEncoder.normalization", "home.repos.pwc.inspect_result.gcorso_neuroseed.mlp.model.MLPDecoder.decode_and_sample"], ["", "def", "forward", "(", "self", ",", "sequences", ")", ":", "\n", "        ", "(", "B", ",", "K", ",", "N", ")", "=", "sequences", ".", "shape", "\n", "sequences", "=", "sequences", ".", "reshape", "(", "K", "*", "B", ",", "N", ")", "\n", "\n", "# encoder", "\n", "enc_sequences", "=", "self", ".", "encoder", "(", "sequences", ")", "\n", "\n", "# compute center", "\n", "enc_sequences", "=", "self", ".", "normalization", "(", "enc_sequences", ".", "reshape", "(", "B", ",", "K", ",", "-", "1", ")", ")", "\n", "centers", ",", "exp_dist", "=", "self", ".", "center", "(", "enc_sequences", ")", "\n", "centers", "=", "centers", ".", "to", "(", "self", ".", "device", ")", "\n", "centers", "=", "self", ".", "normalization", "(", "centers", ".", "reshape", "(", "B", ",", "-", "1", ")", ")", "\n", "\n", "# decode", "\n", "centers_sequence", "=", "self", ".", "decoder", ".", "decode_and_sample", "(", "centers", ")", "\n", "return", "centers_sequence", ",", "exp_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.msa_autoencoder.centroid": [[59, 61], ["torch.mean", "torch.mean"], "function", ["None"], ["", "", "def", "centroid", "(", "enc_sequences", ")", ":", "\n", "    ", "return", "torch", ".", "mean", "(", "enc_sequences", ",", "dim", "=", "1", ")", ",", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.msa_autoencoder.geometric_median_torch": [[63, 74], ["util.ml_and_math.loss_functions.AverageMeter", "points.detach().cpu().numpy", "util.distance_functions.geometric_median.geometric_median", "medians.append", "util.ml_and_math.loss_functions.AverageMeter.update", "torch.from_numpy().float", "torch.from_numpy().float", "points.detach().cpu", "torch.from_numpy", "torch.from_numpy", "points.detach", "numpy.asarray"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.geometric_median.geometric_median", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update"], ["", "def", "geometric_median_torch", "(", "points", ",", "distance", "=", "'euclidean'", ")", ":", "\n", "    ", "\"\"\" Finds the geometric median via a convex optimization procedure. \"\"\"", "\n", "avg_distance", "=", "AverageMeter", "(", ")", "\n", "\n", "medians", "=", "[", "]", "\n", "for", "Xs", "in", "points", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ":", "\n", "        ", "m", ",", "val", "=", "geometric_median", "(", "Xs", ",", "distance", "=", "distance", ")", "\n", "medians", ".", "append", "(", "m", ")", "\n", "avg_distance", ".", "update", "(", "val", ")", "\n", "\n", "", "return", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "medians", ")", ")", ".", "float", "(", ")", ",", "avg_distance", ".", "avg", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.loss.pair_autoencoder_loss": [[8, 20], ["torch.argmax", "torch.argmax", "torch.mean", "torch.mean", "decoded_sequences.permute.permute", "torch.mse_loss", "util.ml_and_math.loss_functions.MAPE", "torch.cross_entropy", "torch.eq().float", "torch.eq().float", "F.mse_loss.data.item", "util.ml_and_math.loss_functions.MAPE.data.item", "F.cross_entropy.data.item", "torch.mean.data.item", "torch.eq", "torch.eq"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.MAPE"], ["def", "pair_autoencoder_loss", "(", "decoded_sequences", ",", "predicted_distance", ",", "input_sequences", ",", "distance_label", ",", "alpha", ")", ":", "\n", "    ", "pred", "=", "torch", ".", "argmax", "(", "decoded_sequences", ",", "dim", "=", "-", "1", ")", "\n", "reconstruction_accuracy", "=", "torch", ".", "mean", "(", "torch", ".", "eq", "(", "pred", ",", "input_sequences", ")", ".", "float", "(", ")", ")", "\n", "decoded_sequences", "=", "decoded_sequences", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "# (B, 2, N, 4) -> (B, 4, 2, N)", "\n", "\n", "distance_loss", "=", "F", ".", "mse_loss", "(", "predicted_distance", ",", "distance_label", ")", "\n", "distance_mape", "=", "MAPE", "(", "predicted_distance", ",", "distance_label", ")", "\n", "\n", "reconstruction_loss", "=", "F", ".", "cross_entropy", "(", "decoded_sequences", ",", "input_sequences", ")", "\n", "loss", "=", "(", "1", "-", "alpha", ")", "*", "distance_loss", "+", "alpha", "*", "reconstruction_loss", "\n", "return", "loss", ",", "(", "distance_loss", ".", "data", ".", "item", "(", ")", ",", "distance_mape", ".", "data", ".", "item", "(", ")", ",", "\n", "reconstruction_loss", ".", "data", ".", "item", "(", ")", ",", "reconstruction_accuracy", ".", "data", ".", "item", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.loss.multiple_alignment_cost": [[22, 36], ["loss.torch_to_string", "util.ml_and_math.loss_functions.AverageMeter", "range", "loss.torch_to_string", "range", "distance", "util.ml_and_math.loss_functions.AverageMeter.update"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.torch_to_string", "home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.torch_to_string", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update"], ["", "def", "multiple_alignment_cost", "(", "sequences", ",", "centers", ",", "distance", "=", "Levenshtein", ".", "distance", ",", "alphabet_size", "=", "4", ")", ":", "\n", "    ", "(", "B", ",", "K", ",", "N", ")", "=", "sequences", ".", "shape", "\n", "centers", "=", "torch_to_string", "(", "centers", ",", "alphabet_size", ")", "\n", "\n", "mean_distance", "=", "AverageMeter", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "B", ")", ":", "\n", "        ", "strings", "=", "torch_to_string", "(", "sequences", "[", "i", "]", ",", "alphabet_size", ")", "\n", "\n", "for", "j", "in", "range", "(", "K", ")", ":", "\n", "            ", "d", "=", "distance", "(", "strings", "[", "j", "]", ",", "centers", "[", "i", "]", ")", "\n", "mean_distance", ".", "update", "(", "d", ")", "\n", "\n", "", "", "return", "mean_distance", ".", "avg", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.loss.remove_padding": [[38, 42], ["None"], "function", ["None"], ["", "def", "remove_padding", "(", "s", ",", "alphabet_size", "=", "4", ")", ":", "\n", "    ", "while", "s", "[", "-", "1", "]", "==", "alphabet_size", ":", "\n", "        ", "s", "=", "s", "[", ":", "-", "1", "]", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.loss.torch_to_string": [[44, 50], ["S.tolist.tolist", "loss.remove_padding", "str"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.loss.remove_padding"], ["", "def", "torch_to_string", "(", "S", ",", "alphabet_size", "=", "4", ")", ":", "\n", "    ", "S", "=", "S", ".", "tolist", "(", ")", "\n", "S", "=", "[", "remove_padding", "(", "s", ",", "alphabet_size", ")", "for", "s", "in", "S", "]", "\n", "S", "=", "[", "[", "str", "(", "l", ")", "for", "l", "in", "s", "]", "for", "s", "in", "S", "]", "\n", "S", "=", "[", "''", ".", "join", "(", "s", ")", "for", "s", "in", "S", "]", "\n", "return", "S", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.loss.torch_to_string2": [[52, 61], ["sequences.tolist.tolist", "strings.append", "loss.remove_padding", "str"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.loss.remove_padding"], ["", "def", "torch_to_string2", "(", "sequences", ",", "alphabet_size", "=", "4", ")", ":", "\n", "    ", "sequences", "=", "sequences", ".", "tolist", "(", ")", "\n", "strings", "=", "[", "]", "\n", "for", "S", "in", "sequences", ":", "\n", "        ", "S", "=", "[", "remove_padding", "(", "s", ",", "alphabet_size", ")", "for", "s", "in", "S", "]", "\n", "S", "=", "[", "[", "str", "(", "l", ")", "for", "l", "in", "s", "]", "for", "s", "in", "S", "]", "\n", "S", "=", "[", "''", ".", "join", "(", "s", ")", "for", "s", "in", "S", "]", "\n", "strings", ".", "append", "(", "S", ")", "\n", "", "return", "strings", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.__init__": [[10, 21], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embedding_model", ",", "distance", "=", "'euclidean'", ",", "scaling", "=", "False", ")", ":", "\n", "        ", "super", "(", "PairEmbeddingDistance", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embedding_model", "=", "embedding_model", "\n", "self", ".", "distance", "=", "DISTANCE_TORCH", "[", "distance", "]", "\n", "self", ".", "distance_str", "=", "distance", "\n", "\n", "self", ".", "scaling", "=", "None", "\n", "if", "scaling", ":", "\n", "            ", "self", ".", "radius", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "1e-2", "]", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "scaling", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "1.", "]", ")", ",", "requires_grad", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.normalize_embeddings": [[22, 32], ["torch.normalize", "torch.normalize", "torch.normalize", "pair_encoder.PairEmbeddingDistance.radius.clamp_min().clamp_max", "pair_encoder.PairEmbeddingDistance.radius.clamp_min"], "methods", ["None"], ["", "", "def", "normalize_embeddings", "(", "self", ",", "embeddings", ")", ":", "\n", "        ", "\"\"\" Project embeddings to an hypersphere of a certain radius \"\"\"", "\n", "min_scale", "=", "1e-7", "\n", "\n", "if", "self", ".", "distance_str", "==", "'hyperbolic'", ":", "\n", "            ", "max_scale", "=", "1", "-", "1e-3", "\n", "", "else", ":", "\n", "            ", "max_scale", "=", "1e10", "\n", "\n", "", "return", "F", ".", "normalize", "(", "embeddings", ",", "p", "=", "2", ",", "dim", "=", "1", ")", "*", "self", ".", "radius", ".", "clamp_min", "(", "min_scale", ")", ".", "clamp_max", "(", "max_scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.encode": [[33, 39], ["pair_encoder.PairEmbeddingDistance.embedding_model", "pair_encoder.PairEmbeddingDistance.normalize_embeddings"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.normalize_embeddings"], ["", "def", "encode", "(", "self", ",", "sequence", ")", ":", "\n", "        ", "\"\"\" Use embedding model and normalization to encode some sequences. \"\"\"", "\n", "enc_sequence", "=", "self", ".", "embedding_model", "(", "sequence", ")", "\n", "if", "self", ".", "scaling", "is", "not", "None", ":", "\n", "            ", "enc_sequence", "=", "self", ".", "normalize_embeddings", "(", "enc_sequence", ")", "\n", "", "return", "enc_sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.forward": [[40, 56], ["sequence.reshape.reshape.reshape", "pair_encoder.PairEmbeddingDistance.encode", "enc_sequence.reshape.reshape.reshape", "pair_encoder.PairEmbeddingDistance.distance"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.encode"], ["", "def", "forward", "(", "self", ",", "sequence", ")", ":", "\n", "# flatten couples", "\n", "        ", "(", "B", ",", "_", ",", "N", ",", "_", ")", "=", "sequence", ".", "shape", "\n", "sequence", "=", "sequence", ".", "reshape", "(", "2", "*", "B", ",", "N", ",", "-", "1", ")", "\n", "\n", "# encode sequences", "\n", "enc_sequence", "=", "self", ".", "encode", "(", "sequence", ")", "\n", "\n", "# compute distances", "\n", "enc_sequence", "=", "enc_sequence", ".", "reshape", "(", "B", ",", "2", ",", "-", "1", ")", "\n", "distance", "=", "self", ".", "distance", "(", "enc_sequence", "[", ":", ",", "0", "]", ",", "enc_sequence", "[", ":", ",", "1", "]", ")", "\n", "\n", "if", "self", ".", "scaling", "is", "not", "None", ":", "\n", "            ", "distance", "=", "distance", "*", "self", ".", "scaling", "\n", "\n", "", "return", "distance", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.hyperbolics.RAdam.step": [[59, 135], ["closure", "torch.no_grad", "util.ml_and_math.poincare.egrad2rgrad.add_", "util.ml_and_math.poincare.egrad2rgrad", "exp_avg.mul_().add_", "exp_avg_sq.mul_().add_", "util.ml_and_math.poincare.project", "util.ml_and_math.poincare.ptransp", "hyperbolics.copy_or_set_", "exp_avg.set_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "util.ml_and_math.poincare.inner", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "util.ml_and_math.poincare.expmap", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.egrad2rgrad", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.project", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.ptransp", "home.repos.pwc.inspect_result.gcorso_neuroseed.optim.radam.copy_or_set_", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.inner", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.expmap"], ["def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments\n        ---------\n        closure : callable (optional)\n            A closure that reevaluates the model\n            and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "                ", "if", "\"step\"", "not", "in", "group", ":", "\n", "                    ", "group", "[", "\"step\"", "]", "=", "0", "\n", "", "betas", "=", "group", "[", "\"betas\"", "]", "\n", "weight_decay", "=", "group", "[", "\"weight_decay\"", "]", "\n", "eps", "=", "group", "[", "\"eps\"", "]", "\n", "learning_rate", "=", "group", "[", "\"lr\"", "]", "\n", "amsgrad", "=", "group", "[", "\"amsgrad\"", "]", "\n", "for", "point", "in", "group", "[", "\"params\"", "]", ":", "\n", "                    ", "grad", "=", "point", ".", "grad", "\n", "if", "grad", "is", "None", ":", "\n", "                        ", "continue", "\n", "", "if", "grad", ".", "is_sparse", ":", "\n", "                        ", "raise", "RuntimeError", "(", "\n", "\"Riemannian Adam does not support sparse gradients yet (PR is welcome)\"", "\n", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "point", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                        ", "state", "[", "\"step\"", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "\"exp_avg\"", "]", "=", "torch", ".", "zeros_like", "(", "point", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "point", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                            ", "state", "[", "\"max_exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "point", ")", "\n", "# make local variables for easy access", "\n", "", "", "exp_avg", "=", "state", "[", "\"exp_avg\"", "]", "\n", "exp_avg_sq", "=", "state", "[", "\"exp_avg_sq\"", "]", "\n", "# actual step", "\n", "grad", ".", "add_", "(", "point", ",", "alpha", "=", "weight_decay", ")", "\n", "grad", "=", "egrad2rgrad", "(", "point", ",", "grad", ")", "\n", "exp_avg", ".", "mul_", "(", "betas", "[", "0", "]", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1", "-", "betas", "[", "0", "]", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "betas", "[", "1", "]", ")", ".", "add_", "(", "inner", "(", "point", ",", "grad", ")", ",", "alpha", "=", "1", "-", "betas", "[", "1", "]", ")", "\n", "if", "amsgrad", ":", "\n", "                        ", "max_exp_avg_sq", "=", "state", "[", "\"max_exp_avg_sq\"", "]", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", ")", "\n", "", "else", ":", "\n", "                        ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", ")", "\n", "", "group", "[", "\"step\"", "]", "+=", "1", "\n", "bias_correction1", "=", "1", "-", "betas", "[", "0", "]", "**", "group", "[", "\"step\"", "]", "\n", "bias_correction2", "=", "1", "-", "betas", "[", "1", "]", "**", "group", "[", "\"step\"", "]", "\n", "step_size", "=", "(", "\n", "learning_rate", "*", "bias_correction2", "**", "0.5", "/", "bias_correction1", "\n", ")", "\n", "\n", "# copy the state, we need it for retraction", "\n", "# get the direction for ascend", "\n", "direction", "=", "exp_avg", "/", "denom", "\n", "# transport the exponential averaging to the new point", "\n", "new_point", "=", "project", "(", "expmap", "(", "-", "step_size", "*", "direction", ",", "point", ")", ")", "\n", "exp_avg_new", "=", "ptransp", "(", "point", ",", "new_point", ",", "exp_avg", ")", "\n", "# use copy only for user facing point", "\n", "copy_or_set_", "(", "point", ",", "new_point", ")", "\n", "exp_avg", ".", "set_", "(", "exp_avg_new", ")", "\n", "\n", "group", "[", "\"step\"", "]", "+=", "1", "\n", "", "", "", "return", "loss", "", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.models.hyperbolics.copy_or_set_": [[8, 27], ["dest.stride", "source.stride", "dest.copy_", "dest.set_"], "function", ["None"], ["def", "copy_or_set_", "(", "dest", ",", "source", ")", ":", "\n", "    ", "\"\"\"\n    A workaround to respect strides of :code:`dest` when copying :code:`source`\n    (https://github.com/geoopt/geoopt/issues/70)\n    Parameters\n    ----------\n    dest : torch.Tensor\n        Destination tensor where to store new data\n    source : torch.Tensor\n        Source data to put in the new tensor\n    Returns\n    -------\n    dest\n        torch.Tensor, modified inplace\n    \"\"\"", "\n", "if", "dest", ".", "stride", "(", ")", "!=", "source", ".", "stride", "(", ")", ":", "\n", "        ", "return", "dest", ".", "copy_", "(", "source", ")", "\n", "", "else", ":", "\n", "        ", "return", "dest", ".", "set_", "(", "source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.feedforward.model.MLPEncoder.__init__": [[8, 14], ["torch.Module.__init__", "util.ml_and_math.layers.MLP"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["import", "torch", ".", "nn", "as", "nn", "\n", "import", "torch", ".", "nn", ".", "functional", "as", "F", "\n", "\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "lca", "import", "hyp_lca", "\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "linkage", "import", "nn_merge_uf_fast_np", ",", "sl_from_embeddings", "\n", "from", "util", ".", "ml_and_math", ".", "poincare", "import", "project", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.feedforward.model.MLPEncoder.forward": [[15, 23], ["sequence.reshape.reshape.reshape", "model.MLPEncoder.encoder"], "methods", ["None"], ["\n", "def", "index_to_one_hot", "(", "x", ",", "alphabet_size", "=", "4", ",", "device", "=", "'cpu'", ")", ":", "\n", "# add one row of zeros because the -1 represents the absence of element and it is encoded with zeros", "\n", "    ", "x", "=", "torch", ".", "cat", "(", "(", "torch", ".", "eye", "(", "alphabet_size", ",", "device", "=", "device", ")", ",", "torch", ".", "zeros", "(", "(", "1", ",", "alphabet_size", ")", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "0", ")", "[", "x", "]", "\n", "return", "x", "\n", "\n", "\n", "", "class", "HypHCModel", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\" Class interface for the other hyperbolic models for hierarchical clustering \"\"\"", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.Transformer.__init__": [[10, 34], ["torch.Module.__init__", "print", "model.TransformerEncoderModel", "util.ml_and_math.layers.MLP", "model.Transformer.to", "generate_empty_mask().to", "generate_square_previous_mask().to", "model.generate_empty_mask", "generate_local_mask().to", "model.generate_square_previous_mask", "model.generate_local_mask", "int"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__", "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.generate_empty_mask", "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.generate_square_previous_mask", "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.generate_local_mask"], ["\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "lca", "import", "hyp_lca", "\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "linkage", "import", "nn_merge_uf_fast_np", ",", "sl_from_embeddings", "\n", "from", "util", ".", "ml_and_math", ".", "poincare", "import", "project", "\n", "\n", "\n", "def", "index_to_one_hot", "(", "x", ",", "alphabet_size", "=", "4", ",", "device", "=", "'cpu'", ")", ":", "\n", "# add one row of zeros because the -1 represents the absence of element and it is encoded with zeros", "\n", "    ", "x", "=", "torch", ".", "cat", "(", "(", "torch", ".", "eye", "(", "alphabet_size", ",", "device", "=", "device", ")", ",", "torch", ".", "zeros", "(", "(", "1", ",", "alphabet_size", ")", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "0", ")", "[", "x", "]", "\n", "return", "x", "\n", "\n", "\n", "", "class", "HypHCModel", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\" Class interface for the other hyperbolic models for hierarchical clustering \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "temperature", "=", "0.05", ",", "init_size", "=", "1e-2", ",", "max_scale", "=", "1.", "-", "1e-3", ")", ":", "\n", "        ", "super", "(", "HypHCModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "temperature", "=", "temperature", "\n", "self", ".", "scale", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "init_size", "]", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "init_size", "=", "init_size", "\n", "self", ".", "max_scale", "=", "max_scale", "\n", "\n", "", "def", "anneal_temperature", "(", "self", ",", "anneal_factor", ")", ":", "\n", "        ", "\"\"\" anneal_factor: scalar for temperature decay \"\"\"", "\n", "self", ".", "temperature", "*=", "anneal_factor", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.Transformer.forward": [[35, 52], ["torch.pad.reshape().transpose", "model.Transformer.sequence_trans", "enc_sequence.transpose().reshape.transpose().reshape.transpose().reshape", "model.Transformer.readout", "torch.pad", "torch.pad", "torch.pad", "torch.pad.reshape", "enc_sequence.transpose().reshape.transpose().reshape.transpose"], "methods", ["None"], ["\n", "", "def", "normalize_embeddings", "(", "self", ",", "embeddings", ")", ":", "\n", "        ", "\"\"\"Normalize leaves embeddings to have the lie on a diameter.\"\"\"", "\n", "min_scale", "=", "self", ".", "init_size", "\n", "max_scale", "=", "self", ".", "max_scale", "\n", "return", "F", ".", "normalize", "(", "embeddings", ",", "p", "=", "2", ",", "dim", "=", "-", "1", ")", "*", "self", ".", "scale", ".", "clamp_min", "(", "min_scale", ")", ".", "clamp_max", "(", "max_scale", ")", "\n", "\n", "", "def", "encode", "(", "self", ",", "triple_ids", ",", "sequences", ")", ":", "\n", "        ", "pass", "\n", "\n", "", "def", "loss", "(", "self", ",", "triple_ids", ",", "sequences", ",", "similarities", ")", ":", "\n", "        ", "\"\"\" Computes the HypHCEmbeddings loss.\n        Args:\n            triple_ids: B x 3 tensor with triple ids\n            sequences:  B x 3 x N tensor with elements indexes\n            similarities: B x 3 tensor with pairwise similarities for triples \n                          [s12, s13, s23]\n        \"\"\"", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.TransformerEncoderModel.__init__": [[57, 69], ["torch.Module.__init__", "model.PositionalEncoding", "TransformerEncoderLayer", "TransformerEncoder", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "model.TransformerEncoderModel.init_weights", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__", "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.TransformerEncoderModel.init_weights"], ["e", "=", "self", ".", "normalize_embeddings", "(", "e", ")", "\n", "e", "=", "e", ".", "reshape", "(", "(", "B", ",", "3", ",", "-", "1", ")", ")", "\n", "\n", "d_12", "=", "hyp_lca", "(", "e", "[", ":", ",", "0", "]", ",", "e", "[", ":", ",", "1", "]", ",", "return_coord", "=", "False", ")", "\n", "d_13", "=", "hyp_lca", "(", "e", "[", ":", ",", "0", "]", ",", "e", "[", ":", ",", "2", "]", ",", "return_coord", "=", "False", ")", "\n", "d_23", "=", "hyp_lca", "(", "e", "[", ":", ",", "1", "]", ",", "e", "[", ":", ",", "2", "]", ",", "return_coord", "=", "False", ")", "\n", "lca_norm", "=", "torch", ".", "cat", "(", "[", "d_12", ",", "d_13", ",", "d_23", "]", ",", "dim", "=", "-", "1", ")", "\n", "weights", "=", "torch", ".", "softmax", "(", "lca_norm", "/", "self", ".", "temperature", ",", "dim", "=", "-", "1", ")", "\n", "w_ord", "=", "torch", ".", "sum", "(", "similarities", "*", "weights", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "total", "=", "torch", ".", "sum", "(", "similarities", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "-", "w_ord", "\n", "return", "torch", ".", "mean", "(", "total", ")", "\n", "\n", "", "def", "decode_tree", "(", "self", ",", "embedded_sequences", ",", "fast_decoding", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.TransformerEncoderModel.init_weights": [[70, 75], ["model.TransformerEncoderModel.encoder.weight.data.uniform_", "model.TransformerEncoderModel.decoder.bias.data.zero_", "model.TransformerEncoderModel.decoder.weight.data.uniform_"], "methods", ["None"], ["        ", "\"\"\"Build a binary tree (nx graph) from leaves' embeddings. Assume points are normalized to same radius.\"\"\"", "\n", "leaves_embeddings", "=", "self", ".", "normalize_embeddings", "(", "embedded_sequences", ")", "\n", "leaves_embeddings", "=", "project", "(", "leaves_embeddings", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "sim_fn", "=", "lambda", "x", ",", "y", ":", "torch", ".", "matmul", "(", "x", ",", "y", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "if", "fast_decoding", ":", "\n", "            ", "parents", "=", "nn_merge_uf_fast_np", "(", "leaves_embeddings", ",", "S", "=", "sim_fn", ",", "partition_ratio", "=", "1.2", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.TransformerEncoderModel.forward": [[76, 82], ["model.TransformerEncoderModel.pos_encoder", "model.TransformerEncoderModel.transformer_encoder", "model.TransformerEncoderModel.decoder", "model.TransformerEncoderModel.encoder", "math.sqrt"], "methods", ["None"], ["", "else", ":", "\n", "            ", "parents", "=", "sl_from_embeddings", "(", "leaves_embeddings", ",", "sim_fn", ")", "\n", "", "tree", "=", "nx", ".", "DiGraph", "(", ")", "\n", "for", "i", ",", "j", "in", "enumerate", "(", "parents", "[", ":", "-", "1", "]", ")", ":", "\n", "            ", "tree", ".", "add_edge", "(", "j", ",", "i", ")", "\n", "", "return", "tree", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.PositionalEncoding.__init__": [[86, 97], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "pe.unsqueeze().transpose.unsqueeze().transpose.unsqueeze().transpose", "model.PositionalEncoding.register_buffer", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "pe.unsqueeze().transpose.unsqueeze().transpose.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "math.log"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.PositionalEncoding.forward": [[98, 101], ["model.PositionalEncoding.dropout", "x.size"], "methods", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.generate_square_previous_mask": [[103, 107], ["mask.float().masked_fill().masked_fill.float().masked_fill().masked_fill", "torch.triu", "torch.triu", "torch.triu", "float", "torch.ones", "torch.ones", "torch.ones", "mask.float().masked_fill().masked_fill.float().masked_fill", "float", "mask.float().masked_fill().masked_fill.float"], "function", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.generate_local_mask": [[109, 117], ["torch.eye", "torch.eye", "torch.eye", "range", "mask.float().masked_fill().masked_fill.float().masked_fill().masked_fill", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "float", "mask.float().masked_fill().masked_fill.float().masked_fill", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "float", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "mask.float().masked_fill().masked_fill.float"], "function", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.transformer.model.generate_empty_mask": [[119, 122], ["torch.zeros", "torch.zeros", "torch.zeros"], "function", ["None"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.cnn.model.CNN.__init__": [[9, 43], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "range", "print", "util.ml_and_math.layers.MLP", "model.CNN.to", "model.CNN.conv.add_module", "torch.Conv1d", "torch.Conv1d", "model.CNN.conv.add_module", "model.CNN.conv.add_module", "model.CNN.conv.add_module", "str", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.AvgPool1d", "torch.AvgPool1d", "model.CNN.conv.add_module", "str", "str", "str", "torch.MaxPool1d", "torch.MaxPool1d", "str"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["import", "torch", ".", "nn", ".", "functional", "as", "F", "\n", "\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "lca", "import", "hyp_lca", "\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "linkage", "import", "nn_merge_uf_fast_np", ",", "sl_from_embeddings", "\n", "from", "util", ".", "ml_and_math", ".", "poincare", "import", "project", "\n", "\n", "\n", "def", "index_to_one_hot", "(", "x", ",", "alphabet_size", "=", "4", ",", "device", "=", "'cpu'", ")", ":", "\n", "# add one row of zeros because the -1 represents the absence of element and it is encoded with zeros", "\n", "    ", "x", "=", "torch", ".", "cat", "(", "(", "torch", ".", "eye", "(", "alphabet_size", ",", "device", "=", "device", ")", ",", "torch", ".", "zeros", "(", "(", "1", ",", "alphabet_size", ")", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "0", ")", "[", "x", "]", "\n", "return", "x", "\n", "\n", "\n", "", "class", "HypHCModel", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\" Class interface for the other hyperbolic models for hierarchical clustering \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "temperature", "=", "0.05", ",", "init_size", "=", "1e-2", ",", "max_scale", "=", "1.", "-", "1e-3", ")", ":", "\n", "        ", "super", "(", "HypHCModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "temperature", "=", "temperature", "\n", "self", ".", "scale", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "init_size", "]", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "init_size", "=", "init_size", "\n", "self", ".", "max_scale", "=", "max_scale", "\n", "\n", "", "def", "anneal_temperature", "(", "self", ",", "anneal_factor", ")", ":", "\n", "        ", "\"\"\" anneal_factor: scalar for temperature decay \"\"\"", "\n", "self", ".", "temperature", "*=", "anneal_factor", "\n", "\n", "", "def", "normalize_embeddings", "(", "self", ",", "embeddings", ")", ":", "\n", "        ", "\"\"\"Normalize leaves embeddings to have the lie on a diameter.\"\"\"", "\n", "min_scale", "=", "self", ".", "init_size", "\n", "max_scale", "=", "self", ".", "max_scale", "\n", "return", "F", ".", "normalize", "(", "embeddings", ",", "p", "=", "2", ",", "dim", "=", "-", "1", ")", "*", "self", ".", "scale", ".", "clamp_min", "(", "min_scale", ")", ".", "clamp_max", "(", "max_scale", ")", "\n", "\n", "", "def", "encode", "(", "self", ",", "triple_ids", ",", "sequences", ")", ":", "\n", "        ", "pass", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.cnn.model.CNN.forward": [[44, 58], ["model.CNN.embedding", "model.CNN.transpose", "model.CNN.conv", "enc_sequence.reshape.reshape.reshape", "model.CNN.readout"], "methods", ["None"], ["\n", "", "def", "loss", "(", "self", ",", "triple_ids", ",", "sequences", ",", "similarities", ")", ":", "\n", "        ", "\"\"\" Computes the HypHCEmbeddings loss.\n        Args:\n            triple_ids: B x 3 tensor with triple ids\n            sequences:  B x 3 x N tensor with elements indexes\n            similarities: B x 3 tensor with pairwise similarities for triples \n                          [s12, s13, s23]\n        \"\"\"", "\n", "(", "B", ",", "_", ",", "N", ")", "=", "sequences", ".", "shape", "\n", "triple_ids", ",", "sequences", "=", "triple_ids", ".", "reshape", "(", "(", "3", "*", "B", ")", ")", ",", "sequences", ".", "reshape", "(", "(", "3", "*", "B", ",", "N", ")", ")", "\n", "\n", "e", "=", "self", ".", "encode", "(", "triple_ids", ",", "sequences", ")", "\n", "e", "=", "self", ".", "normalize_embeddings", "(", "e", ")", "\n", "e", "=", "e", ".", "reshape", "(", "(", "B", ",", "3", ",", "-", "1", ")", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.relaxed.train.train": [[28, 152], ["print", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed", "print", "hierarchical_clustering.relaxed.datasets.hc_dataset.load_hc_data", "hierarchical_clustering.relaxed.datasets.hc_dataset.HCTripletDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "hierarchical_clustering.relaxed.models.cnn.HypHCCNN.to", "getattr", "getattr.", "print", "range", "print", "print", "torch.cuda.is_available", "hierarchical_clustering.relaxed.utils.training.get_savedir", "logging.info", "os.path.join", "torch.set_default_dtype", "torch.utils.data.TensorDataset", "hierarchical_clustering.relaxed.models.embeddings.HypHCEmbeddings", "hierarchical_clustering.relaxed.models.cnn.HypHCCNN.parameters", "hierarchical_clustering.relaxed.models.cnn.HypHCCNN.train", "enumerate", "print", "hierarchical_clustering.relaxed.models.cnn.HypHCCNN.load_state_dict", "print", "torch.save", "os.path.exists", "os.path.exists", "torch.arange", "torch.from_numpy", "hierarchical_clustering.relaxed.models.linear.HypHCLinear", "hierarchical_clustering.relaxed.models.cnn.HypHCCNN.loss", "Optimizer.zero_grad", "model.loss.backward", "Optimizer.step", "total_loss.item", "hierarchical_clustering.relaxed.models.cnn.HypHCCNN.eval", "train.decode_tree", "hierarchical_clustering.relaxed.utils.metrics.dasgupta_cost", "print", "hierarchical_clustering.relaxed.models.cnn.HypHCCNN.anneal_temperature", "print", "print", "logging.info", "logging.info", "open", "json.dump", "hierarchical_clustering.relaxed.models.mlp.HypHCMLP", "triple_ids.to", "triple_sequences.to", "triple_similarities.to", "hierarchical_clustering.relaxed.models.cnn.HypHCCNN.state_dict", "os.path.join", "hierarchical_clustering.relaxed.models.cnn.HypHCCNN", "print"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.hc_dataset.load_hc_data", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.training.get_savedir", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.train", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.loss", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.Arsinh.backward", "home.repos.pwc.inspect_result.gcorso_neuroseed.optim.radam.RAdam.step", "home.repos.pwc.inspect_result.gcorso_neuroseed.relaxed.train.decode_tree", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.metrics.dasgupta_cost", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.anneal_temperature"], ["parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.001", ",", "help", "=", "'Initial learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'Weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'Dropout rate (1 - keep probability)'", ")", "\n", "parser", ".", "add_argument", "(", "'--patience'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Patience'", ")", "\n", "parser", ".", "add_argument", "(", "'--print_every'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Print training results every'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'Batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--embedding_size'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'Size of embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--distance'", ",", "type", "=", "str", ",", "default", "=", "'euclidean'", ",", "help", "=", "'Type of distance to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--workers'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'Number of workers'", ")", "\n", "parser", ".", "add_argument", "(", "'--loss'", ",", "type", "=", "str", ",", "default", "=", "\"mse\"", ",", "help", "=", "'Loss function to use (mse, mape or mae)'", ")", "\n", "parser", ".", "add_argument", "(", "'--plot'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'Plot real vs predicted distances'", ")", "\n", "parser", ".", "add_argument", "(", "'--closest_data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset for closest string retrieval tests'", ")", "\n", "parser", ".", "add_argument", "(", "'--hierarchical_data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset for hierarchical clustering'", ")", "\n", "parser", ".", "add_argument", "(", "'--scaling'", ",", "type", "=", "str", ",", "default", "=", "'False'", ",", "help", "=", "'Project to hypersphere (for hyperbolic)'", ")", "\n", "parser", ".", "add_argument", "(", "'--hyp_optimizer'", ",", "type", "=", "str", ",", "default", "=", "'Adam'", ",", "help", "=", "'Optimizer for hyperbolic (Adam or RAdam)'", ")", "\n", "parser", ".", "add_argument", "(", "'--margin'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'Margin loss'", ")", "\n", "\n", "return", "parser", "\n", "\n", "\n", "", "def", "execute_train", "(", "model_class", ",", "model_args", ",", "args", ")", ":", "\n", "# set device", "\n", "    ", "args", ".", "cuda", "=", "not", "args", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "device", "=", "'cuda'", "if", "args", ".", "cuda", "else", "'cpu'", "\n", "print", "(", "'Using device:'", ",", "device", ")", "\n", "\n", "# set the random seed", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "# load data", "\n", "", "datasets", "=", "load_triplet_dataset", "(", "args", ".", "data", ")", "\n", "loaders", "=", "get_dataloaders", "(", "datasets", ",", "batch_size", "=", "args", ".", "batch_size", ",", "workers", "=", "args", ".", "workers", ")", "\n", "\n", "# fix hyperparameters", "\n", "model_args", "=", "SimpleNamespace", "(", "**", "model_args", ")", "\n", "model_args", ".", "device", "=", "device", "\n", "model_args", ".", "len_sequence", "=", "datasets", "[", "'train'", "]", ".", "len_sequence", "\n", "model_args", ".", "embedding_size", "=", "args", ".", "embedding_size", "\n", "model_args", ".", "dropout", "=", "args", ".", "dropout", "\n", "print", "(", "\"Length of sequence\"", ",", "datasets", "[", "'train'", "]", ".", "len_sequence", ")", "\n", "args", ".", "scaling", "=", "True", "if", "args", ".", "scaling", "==", "'True'", "else", "False", "\n", "\n", "# generate model", "\n", "embedding_model", "=", "model_class", "(", "**", "vars", "(", "model_args", ")", ")", "\n", "model", "=", "TripletEncoder", "(", "embedding_model", "=", "embedding_model", ",", "distance", "=", "args", ".", "distance", ",", "scaling", "=", "args", ".", "scaling", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "# select optimizer", "\n", "if", "args", ".", "distance", "==", "'hyperbolic'", "and", "args", ".", "hyp_optimizer", "==", "'RAdam'", ":", "\n", "        ", "optimizer", "=", "RAdam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ",", "weight_decay", "=", "args", ".", "weight_decay", ")", "\n", "\n", "# select loss", "\n", "", "loss", "=", "partial", "(", "triplet_loss", ",", "margin", "=", "args", ".", "margin", ",", "device", "=", "device", ")", "\n", "\n", "# print total number of parameters", "\n", "total_params", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "print", "(", "'Total params'", ",", "total_params", ")", "\n", "\n", "# Train model", "\n", "t_total", "=", "time", ".", "time", "(", ")", "\n", "bad_counter", "=", "0", "\n", "best", "=", "1e10", "\n", "best_epoch", "=", "-", "1", "\n", "start_epoch", "=", "0", "\n", "\n", "for", "epoch", "in", "range", "(", "start_epoch", ",", "args", ".", "epochs", ")", ":", "\n", "        ", "t", "=", "time", ".", "time", "(", ")", "\n", "loss_train", "=", "train", "(", "model", ",", "loaders", "[", "'train'", "]", ",", "optimizer", ",", "loss", ",", "device", ")", "\n", "loss_val", "=", "test", "(", "model", ",", "loaders", "[", "'val'", "]", ",", "loss", ",", "device", ")", "\n", "\n", "# print progress", "\n", "if", "epoch", "%", "args", ".", "print_every", "==", "0", ":", "\n", "            ", "print", "(", "'Epoch: {:04d}'", ".", "format", "(", "epoch", "+", "1", ")", ",", "\n", "'loss_train: {:.6f}'", ".", "format", "(", "loss_train", ")", ",", "\n", "'loss_val: {:.6f}'", ".", "format", "(", "loss_val", ")", ",", "\n", "'time: {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "if", "loss_val", "<", "best", ":", "\n", "# save current model", "\n", "            ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "'{}.pkl'", ".", "format", "(", "epoch", ")", ")", "\n", "# remove previous model", "\n", "if", "best_epoch", ">=", "0", ":", "\n", "                ", "os", ".", "remove", "(", "'{}.pkl'", ".", "format", "(", "best_epoch", ")", ")", "\n", "# update training variables", "\n", "", "best", "=", "loss_val", "\n", "best_epoch", "=", "epoch", "\n", "bad_counter", "=", "0", "\n", "", "else", ":", "\n", "            ", "bad_counter", "+=", "1", "\n", "\n", "", "if", "bad_counter", "==", "args", ".", "patience", ":", "\n", "            ", "print", "(", "'Early stop at epoch {} (no improvement in last {} epochs)'", ".", "format", "(", "epoch", "+", "1", ",", "bad_counter", ")", ")", "\n", "break", "\n", "\n", "", "", "print", "(", "'Optimization Finished!'", ")", "\n", "print", "(", "'Total time elapsed: {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t_total", ")", ")", "\n", "\n", "# Restore best model", "\n", "print", "(", "'Loading {}th epoch'", ".", "format", "(", "best_epoch", "+", "1", ")", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "'{}.pkl'", ".", "format", "(", "best_epoch", ")", ")", ")", "\n", "\n", "# Testing", "\n", "for", "dset", "in", "loaders", ".", "keys", "(", ")", ":", "\n", "        ", "avg_loss", "=", "test", "(", "model", ",", "loaders", "[", "dset", "]", ",", "loss", ",", "device", ")", "\n", "print", "(", "'Final results {}: loss = {:.6f}'", ".", "format", "(", "dset", ",", "avg_loss", ")", ")", "\n", "\n", "\n", "# Nearest neighbour retrieval", "\n", "", "if", "args", ".", "closest_data_path", "!=", "''", ":", "\n", "        ", "print", "(", "\"Closest string retrieval\"", ")", "\n", "closest_string_testing", "(", "encoder_model", "=", "model", ",", "data_path", "=", "args", ".", "closest_data_path", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "device", "=", "device", ",", "distance", "=", "args", ".", "distance", ")", "\n", "\n", "# Hierarchical clustering", "\n", "", "if", "args", ".", "hierarchical_data_path", "!=", "''", ":", "\n", "        ", "print", "(", "\"Hierarchical clustering\"", ")", "\n", "hierarchical_clustering_testing", "(", "encoder_model", "=", "model", ",", "data_path", "=", "args", ".", "hierarchical_data_path", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "device", "=", "device", ",", "distance", "=", "args", ".", "distance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.relaxed.train.decode_tree": [[154, 162], ["torch.cat", "model.decode_tree", "model.encode", "torch.cat.append", "ids.to", "seqs.to"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.relaxed.train.decode_tree", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.encode"], ["\n", "\n", "", "", "def", "load_triplet_dataset", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "sequences", ",", "distances", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "datasets", "=", "{", "}", "\n", "for", "key", "in", "sequences", ".", "keys", "(", ")", ":", "\n", "        ", "datasets", "[", "key", "]", "=", "TripletDataset", "(", "sequences", "[", "key", "]", ".", "unsqueeze", "(", "0", ")", ",", "distances", "[", "key", "]", ".", "unsqueeze", "(", "0", ")", ",", "multiplicity", "=", "10", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.relaxed.visualize.decode_tree_and_sequences": [[22, 30], ["torch.cat", "model.encode", "torch.cat.append", "model.decode_tree", "ids.to", "seqs.to"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.encode", "home.repos.pwc.inspect_result.gcorso_neuroseed.relaxed.train.decode_tree"], ["def", "decode_tree_and_sequences", "(", "args", ",", "model", ",", "seq_loader", ",", "device", ")", ":", "\n", "    ", "embedded_sequences", "=", "[", "]", "\n", "for", "ids", ",", "seqs", "in", "seq_loader", ":", "\n", "        ", "e", "=", "model", ".", "encode", "(", "ids", ".", "to", "(", "device", ")", ",", "seqs", ".", "to", "(", "device", ")", ")", "\n", "embedded_sequences", ".", "append", "(", "e", ")", "\n", "\n", "", "embedded_sequences", "=", "torch", ".", "cat", "(", "embedded_sequences", ",", "dim", "=", "0", ")", "\n", "return", "model", ".", "decode_tree", "(", "embedded_sequences", "=", "embedded_sequences", ",", "fast_decoding", "=", "args", ".", "fast_decoding", ")", ",", "embedded_sequences", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.metrics.dasgupta_cost_iterative": [[15, 57], ["len", "list", "list", "len", "tree.nodes", "tree.neighbors", "range", "len", "stack.append", "list", "children[].pop", "tree.neighbors", "len", "sum", "len", "stack.pop", "sum", "similarities[].T[].sum", "enumerate"], "function", ["None"], ["def", "dasgupta_cost_iterative", "(", "tree", ",", "similarities", ")", ":", "\n", "    ", "\"\"\" Non-recursive version of DC. Also works on non-binary trees \"\"\"", "\n", "n", "=", "len", "(", "list", "(", "tree", ".", "nodes", "(", ")", ")", ")", "\n", "root", "=", "n", "-", "1", "\n", "\n", "cost", "=", "[", "0", "]", "*", "n", "\n", "\n", "desc", "=", "[", "None", "]", "*", "n", "# intermediate computation: children of node", "\n", "\n", "children", "=", "[", "list", "(", "tree", ".", "neighbors", "(", "node", ")", ")", "for", "node", "in", "range", "(", "n", ")", "]", "# children remaining to process", "\n", "stack", "=", "[", "root", "]", "\n", "while", "len", "(", "stack", ")", ">", "0", ":", "\n", "        ", "node", "=", "stack", "[", "-", "1", "]", "\n", "if", "len", "(", "children", "[", "node", "]", ")", ">", "0", ":", "\n", "            ", "stack", ".", "append", "(", "children", "[", "node", "]", ".", "pop", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "children_", "=", "list", "(", "tree", ".", "neighbors", "(", "node", ")", ")", "\n", "\n", "if", "len", "(", "children_", ")", "==", "0", ":", "\n", "                ", "desc", "[", "node", "]", "=", "[", "node", "]", "\n", "\n", "", "else", ":", "\n", "# Intermediate computations", "\n", "                ", "desc", "[", "node", "]", "=", "[", "d", "for", "c", "in", "children_", "for", "d", "in", "desc", "[", "c", "]", "]", "\n", "\n", "# Cost at this node", "\n", "# cost_ = similarities[desc[node]].T[desc[node]].sum()", "\n", "# cost_ -= sum([similarities[desc[c]].T[desc[c]].sum() for c in children_])", "\n", "# cost_ = cost_ / 2.0", "\n", "# This is much faster for imbalanced trees", "\n", "cost_", "=", "sum", "(", "[", "similarities", "[", "desc", "[", "c0", "]", "]", ".", "T", "[", "desc", "[", "c1", "]", "]", ".", "sum", "(", ")", "for", "i", ",", "c0", "in", "enumerate", "(", "children_", ")", "for", "c1", "in", "\n", "children_", "[", "i", "+", "1", ":", "]", "]", ")", "\n", "cost_", "*=", "len", "(", "desc", "[", "node", "]", ")", "\n", "\n", "cost", "[", "node", "]", "=", "cost_", "+", "sum", "(", "[", "cost", "[", "c", "]", "for", "c", "in", "children_", "]", ")", "# recursive cost", "\n", "\n", "# Free intermediate computations (otherwise, up to n^2 space for recursive descendants)", "\n", "for", "c", "in", "children_", ":", "\n", "                    ", "desc", "[", "c", "]", "=", "None", "\n", "\n", "", "", "assert", "node", "==", "stack", ".", "pop", "(", ")", "\n", "", "", "return", "2", "*", "cost", "[", "root", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.metrics.dasgupta_cost": [[59, 110], ["len", "len", "hierarchical_clustering.relaxed.utils.tree.descendants_traversal", "hierarchical_clustering.relaxed.utils.tree.descendants_count", "hierarchical_clustering.relaxed.mst.mst.reorder", "list", "numpy.array", "list", "len", "sum", "tree.nodes", "tree.neighbors", "range", "len", "stack.append", "list", "children[].pop", "tree.neighbors", "len", "stack.pop", "numpy.array", "numpy.array", "len", "numpy.add.reduceat", "numpy.add.reduceat"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.utils.tree.descendants_traversal", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.tree.descendants_count"], ["", "def", "dasgupta_cost", "(", "tree", ",", "similarities", ")", ":", "\n", "    ", "\"\"\" Non-recursive version of DC for binary trees.\n\n    Optimized for speed by reordering similarity matrix for locality\n    \"\"\"", "\n", "n", "=", "len", "(", "list", "(", "tree", ".", "nodes", "(", ")", ")", ")", "\n", "root", "=", "n", "-", "1", "\n", "n_leaves", "=", "len", "(", "similarities", ")", "\n", "\n", "leaves", "=", "descendants_traversal", "(", "tree", ")", "\n", "n_desc", ",", "left_desc", "=", "descendants_count", "(", "tree", ")", "\n", "\n", "cost", "=", "[", "0", "]", "*", "n", "# local cost for every node", "\n", "\n", "# reorder similarity matrix for locality", "\n", "# similarities = similarities[leaves].T[leaves] # this is the bottleneck; is there a faster way?", "\n", "similarities", "=", "mst", ".", "reorder", "(", "similarities", ",", "np", ".", "array", "(", "leaves", ")", ",", "n_leaves", ")", "# this is the bottleneck; is there a faster way?", "\n", "\n", "# Recursive computation", "\n", "children", "=", "[", "list", "(", "tree", ".", "neighbors", "(", "node", ")", ")", "for", "node", "in", "range", "(", "n", ")", "]", "# children remaining to process", "\n", "stack", "=", "[", "root", "]", "\n", "while", "len", "(", "stack", ")", ">", "0", ":", "\n", "        ", "node", "=", "stack", "[", "-", "1", "]", "\n", "if", "len", "(", "children", "[", "node", "]", ")", ">", "0", ":", "\n", "            ", "stack", ".", "append", "(", "children", "[", "node", "]", ".", "pop", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "children_", "=", "list", "(", "tree", ".", "neighbors", "(", "node", ")", ")", "\n", "\n", "if", "len", "(", "children_", ")", "<", "2", ":", "\n", "                ", "pass", "\n", "", "elif", "len", "(", "children_", ")", "==", "2", ":", "\n", "                ", "left_c", "=", "children_", "[", "0", "]", "\n", "right_c", "=", "children_", "[", "1", "]", "\n", "\n", "left_range", "=", "[", "left_desc", "[", "left_c", "]", ",", "left_desc", "[", "left_c", "]", "+", "n_desc", "[", "left_c", "]", "]", "\n", "right_range", "=", "[", "left_desc", "[", "right_c", "]", ",", "left_desc", "[", "right_c", "]", "+", "n_desc", "[", "right_c", "]", "]", "\n", "cost_", "=", "np", ".", "add", ".", "reduceat", "(", "\n", "np", ".", "add", ".", "reduceat", "(", "\n", "similarities", "[", "\n", "left_range", "[", "0", "]", ":", "left_range", "[", "1", "]", ",", "\n", "right_range", "[", "0", "]", ":", "right_range", "[", "1", "]", "\n", "]", ",", "[", "0", "]", ",", "axis", "=", "1", "\n", ")", ",", "[", "0", "]", ",", "axis", "=", "0", "\n", ")", "\n", "cost", "[", "node", "]", "=", "cost_", "[", "0", ",", "0", "]", "\n", "\n", "", "else", ":", "\n", "                ", "assert", "False", ",", "\"tree must be binary\"", "\n", "", "assert", "node", "==", "stack", ".", "pop", "(", ")", "\n", "\n", "", "", "return", "2", "*", "sum", "(", "np", ".", "array", "(", "cost", ")", "*", "np", ".", "array", "(", "n_desc", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.mobius_add": [[14, 22], ["numpy.sum", "numpy.sum", "numpy.sum"], "function", ["None"], ["def", "mobius_add", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\"Mobius addition in numpy.\"\"\"", "\n", "xy", "=", "np", ".", "sum", "(", "x", "*", "y", ",", "1", ",", "keepdims", "=", "True", ")", "\n", "x2", "=", "np", ".", "sum", "(", "x", "*", "x", ",", "1", ",", "keepdims", "=", "True", ")", "\n", "y2", "=", "np", ".", "sum", "(", "y", "*", "y", ",", "1", ",", "keepdims", "=", "True", ")", "\n", "num", "=", "(", "1", "+", "2", "*", "xy", "+", "y2", ")", "*", "x", "+", "(", "1", "-", "x2", ")", "*", "y", "\n", "den", "=", "1", "+", "2", "*", "xy", "+", "x2", "*", "y2", "\n", "return", "num", "/", "den", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.mobius_mul": [[24, 28], ["numpy.sqrt", "numpy.sum", "numpy.tanh", "numpy.arctanh"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.tanh", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.arctanh"], ["", "def", "mobius_mul", "(", "x", ",", "t", ")", ":", "\n", "    ", "\"\"\"Mobius multiplication in numpy.\"\"\"", "\n", "normx", "=", "np", ".", "sqrt", "(", "np", ".", "sum", "(", "x", "*", "x", ",", "1", ",", "keepdims", "=", "True", ")", ")", "\n", "return", "np", ".", "tanh", "(", "t", "*", "np", ".", "arctanh", "(", "normx", ")", ")", "*", "x", "/", "normx", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.geodesic_fn": [[30, 38], ["numpy.linspace", "numpy.repeat", "numpy.repeat", "visualization.mobius_add", "visualization.mobius_mul", "visualization.mobius_add", "x.reshape", "len", "y.reshape", "len", "np.linspace.reshape"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.mobius_add", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.mobius_mul", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.mobius_add"], ["", "def", "geodesic_fn", "(", "x", ",", "y", ",", "nb_points", "=", "100", ")", ":", "\n", "    ", "\"\"\"Get coordinates of points on the geodesic between x and y.\"\"\"", "\n", "t", "=", "np", ".", "linspace", "(", "0", ",", "1", ",", "nb_points", ")", "\n", "x_rep", "=", "np", ".", "repeat", "(", "x", ".", "reshape", "(", "(", "1", ",", "-", "1", ")", ")", ",", "len", "(", "t", ")", ",", "0", ")", "\n", "y_rep", "=", "np", ".", "repeat", "(", "y", ".", "reshape", "(", "(", "1", ",", "-", "1", ")", ")", ",", "len", "(", "t", ")", ",", "0", ")", "\n", "t1", "=", "mobius_add", "(", "-", "x_rep", ",", "y_rep", ")", "\n", "t2", "=", "mobius_mul", "(", "t1", ",", "t", ".", "reshape", "(", "(", "-", "1", ",", "1", ")", ")", ")", "\n", "return", "mobius_add", "(", "x_rep", ",", "t2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.plot_geodesic": [[40, 44], ["visualization.geodesic_fn", "ax.plot"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.geodesic_fn"], ["", "def", "plot_geodesic", "(", "x", ",", "y", ",", "ax", ")", ":", "\n", "    ", "\"\"\"Plots geodesic between x and y.\"\"\"", "\n", "points", "=", "geodesic_fn", "(", "x", ",", "y", ")", "\n", "ax", ".", "plot", "(", "points", "[", ":", ",", "0", "]", ",", "points", "[", ":", ",", "1", "]", ",", "color", "=", "'black'", ",", "linewidth", "=", "1.5", ",", "alpha", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.complete_tree": [[46, 73], ["numpy.zeros", "max", "visualization.complete_tree._complete_tree"], "function", ["None"], ["", "def", "complete_tree", "(", "tree", ",", "leaves_embeddings", ")", ":", "\n", "    ", "\"\"\"Get embeddings of internal nodes from leaves' embeddings using LCA construction.\"\"\"", "\n", "\n", "def", "_complete_tree", "(", "embeddings", ",", "node", ")", ":", "\n", "        ", "children", "=", "list", "(", "tree", ".", "neighbors", "(", "node", ")", ")", "\n", "if", "len", "(", "children", ")", "==", "2", ":", "\n", "            ", "left_c", ",", "right_c", "=", "children", "\n", "left_leaf", "=", "is_leaf", "(", "tree", ",", "left_c", ")", "\n", "right_leaf", "=", "is_leaf", "(", "tree", ",", "right_c", ")", "\n", "if", "left_leaf", "and", "right_leaf", ":", "\n", "                ", "pass", "\n", "", "elif", "left_leaf", "and", "not", "right_leaf", ":", "\n", "                ", "embeddings", "=", "_complete_tree", "(", "embeddings", ",", "right_c", ")", "\n", "", "elif", "right_leaf", "and", "not", "left_leaf", ":", "\n", "                ", "embeddings", "=", "_complete_tree", "(", "embeddings", ",", "left_c", ")", "\n", "", "else", ":", "\n", "                ", "embeddings", "=", "_complete_tree", "(", "embeddings", ",", "right_c", ")", "\n", "embeddings", "=", "_complete_tree", "(", "embeddings", ",", "left_c", ")", "\n", "", "embeddings", "[", "node", "]", "=", "hyp_lca_numpy", "(", "embeddings", "[", "left_c", "]", ",", "embeddings", "[", "right_c", "]", ")", "\n", "", "return", "embeddings", "\n", "\n", "", "n", "=", "leaves_embeddings", ".", "shape", "[", "0", "]", "\n", "tree_embeddings", "=", "np", ".", "zeros", "(", "(", "2", "*", "n", "-", "1", ",", "2", ")", ")", "\n", "tree_embeddings", "[", ":", "n", ",", ":", "]", "=", "leaves_embeddings", "\n", "root", "=", "max", "(", "list", "(", "tree", ".", "nodes", "(", ")", ")", ")", "\n", "tree_embeddings", "=", "_complete_tree", "(", "tree_embeddings", ",", "root", ")", "\n", "return", "tree_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.hyp_lca_numpy": [[75, 81], ["torch.from_numpy().view", "torch.from_numpy().view", "hierarchical_clustering.relaxed.utils.lca.hyp_lca", "hierarchical_clustering.relaxed.utils.lca.hyp_lca.view().numpy", "torch.from_numpy", "torch.from_numpy", "hierarchical_clustering.relaxed.utils.lca.hyp_lca.view"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca.hyp_lca"], ["", "def", "hyp_lca_numpy", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\"Computes the hyperbolic LCA in numpy.\"\"\"", "\n", "x", "=", "torch", ".", "from_numpy", "(", "x", ")", ".", "view", "(", "(", "1", ",", "2", ")", ")", "\n", "y", "=", "torch", ".", "from_numpy", "(", "y", ")", ".", "view", "(", "(", "1", ",", "2", ")", ")", "\n", "lca", "=", "hyp_lca", "(", "x", ",", "y", ",", "return_coord", "=", "True", ")", "\n", "return", "lca", ".", "view", "(", "(", "2", ",", ")", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.is_leaf": [[83, 86], ["len", "list", "tree.neighbors"], "function", ["None"], ["", "def", "is_leaf", "(", "tree", ",", "node", ")", ":", "\n", "    ", "\"\"\"check if node is a leaf in tree.\"\"\"", "\n", "return", "len", "(", "list", "(", "tree", ".", "neighbors", "(", "node", ")", ")", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.plot_tree_from_leaves": [[88, 114], ["matplotlib.Circle", "ax.add_artist", "visualization.complete_tree", "visualization.get_colors", "ax.scatter", "tree.edges", "enumerate", "ax.set_xlim", "ax.set_ylim", "ax.axis", "visualization.plot_geodesic", "ax.annotate", "ax.annotate", "dict", "dict"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.complete_tree", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.get_colors", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.plot_geodesic"], ["", "def", "plot_tree_from_leaves", "(", "ax", ",", "tree", ",", "leaves_embeddings", ",", "labels", ",", "color_seed", "=", "1234", ")", ":", "\n", "    ", "\"\"\"Plots a tree on leaves embeddings using the LCA construction.\"\"\"", "\n", "circle", "=", "plt", ".", "Circle", "(", "(", "0", ",", "0", ")", ",", "1.0", ",", "color", "=", "'r'", ",", "alpha", "=", "0.1", ")", "\n", "ax", ".", "add_artist", "(", "circle", ")", "\n", "n", "=", "leaves_embeddings", ".", "shape", "[", "0", "]", "\n", "embeddings", "=", "complete_tree", "(", "tree", ",", "leaves_embeddings", ")", "\n", "colors", "=", "get_colors", "(", "labels", ",", "color_seed", ")", "\n", "ax", ".", "scatter", "(", "embeddings", "[", ":", "n", ",", "0", "]", ",", "embeddings", "[", ":", "n", ",", "1", "]", ",", "c", "=", "colors", ",", "s", "=", "50", ",", "alpha", "=", "0.6", ")", "\n", "\n", "for", "n1", ",", "n2", "in", "tree", ".", "edges", "(", ")", ":", "\n", "        ", "x1", "=", "embeddings", "[", "n1", "]", "\n", "x2", "=", "embeddings", "[", "n2", "]", "\n", "plot_geodesic", "(", "x1", ",", "x2", ",", "ax", ")", "\n", "\n", "", "for", "i", ",", "txt", "in", "enumerate", "(", "labels", ")", ":", "\n", "        ", "if", "embeddings", "[", "i", ",", "0", "]", ">", "0", ":", "\n", "            ", "ax", ".", "annotate", "(", "txt", ",", "(", "embeddings", "[", "i", ",", "0", "]", "*", "1.15", ",", "embeddings", "[", "i", ",", "1", "]", "*", "1.15", ")", ",", "xycoords", "=", "\"data\"", ",", "\n", "va", "=", "\"center\"", ",", "ha", "=", "\"left\"", ",", "bbox", "=", "dict", "(", "boxstyle", "=", "\"round\"", ",", "fc", "=", "\"w\"", ")", ")", "\n", "", "else", ":", "\n", "            ", "ax", ".", "annotate", "(", "txt", ",", "(", "embeddings", "[", "i", ",", "0", "]", "*", "1.15", ",", "embeddings", "[", "i", ",", "1", "]", "*", "1.15", ")", ",", "xycoords", "=", "\"data\"", ",", "\n", "va", "=", "\"center\"", ",", "ha", "=", "\"right\"", ",", "bbox", "=", "dict", "(", "boxstyle", "=", "\"round\"", ",", "fc", "=", "\"w\"", ")", ")", "\n", "\n", "", "", "ax", ".", "set_xlim", "(", "-", "1.05", ",", "1.05", ")", "\n", "ax", ".", "set_ylim", "(", "-", "1.05", ",", "1.05", ")", "\n", "ax", ".", "axis", "(", "\"off\"", ")", "\n", "return", "ax", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.visualization.get_colors": [[116, 126], ["numpy.random.seed", "numpy.unique", "numpy.random.random", "numpy.random.random", "numpy.random.random"], "function", ["None"], ["", "def", "get_colors", "(", "y", ",", "color_seed", "=", "1234", ")", ":", "\n", "    ", "\"\"\"random color assignment for label classes.\"\"\"", "\n", "np", ".", "random", ".", "seed", "(", "color_seed", ")", "\n", "colors", "=", "{", "}", "\n", "for", "k", "in", "np", ".", "unique", "(", "y", ")", ":", "\n", "        ", "r", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "b", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "g", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "colors", "[", "k", "]", "=", "(", "r", ",", "g", ",", "b", ")", "\n", "", "return", "[", "colors", "[", "k", "]", "for", "k", "in", "y", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.training.str2bool": [[13, 23], ["isinstance", "v.lower", "v.lower", "argparse.ArgumentTypeError"], "function", ["None"], ["def", "str2bool", "(", "v", ")", ":", "\n", "    ", "\"\"\"Converts string to boolean.\"\"\"", "\n", "if", "isinstance", "(", "v", ",", "bool", ")", ":", "\n", "        ", "return", "v", "\n", "", "if", "v", ".", "lower", "(", ")", "in", "(", "'yes'", ",", "'true'", ",", "'t'", ",", "'y'", ",", "'1'", ")", ":", "\n", "        ", "return", "True", "\n", "", "elif", "v", ".", "lower", "(", ")", "in", "(", "'no'", ",", "'false'", ",", "'f'", ",", "'n'", ",", "'0'", ")", ":", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "'Boolean value expected.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.training.add_flags_from_config": [[25, 54], ["isinstance", "x.lower", "training.add_flags_from_config", "add_flags_from_config.add_argument", "print", "str", "type", "training.add_flags_from_config.OrNone"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.utils.training.add_flags_from_config"], ["", "", "def", "add_flags_from_config", "(", "parser", ",", "config_dict", ")", ":", "\n", "    ", "\"\"\"Adds a flag (and default value) to an ArgumentParser for each parameter in a config.\"\"\"", "\n", "\n", "def", "OrNone", "(", "default", ")", ":", "\n", "        ", "def", "func", "(", "x", ")", ":", "\n", "# Convert \"none\" to proper None object", "\n", "            ", "if", "x", ".", "lower", "(", ")", "==", "\"none\"", ":", "\n", "                ", "return", "None", "\n", "# If default is None (and x is not None), return x without conversion as str", "\n", "", "elif", "default", "is", "None", ":", "\n", "                ", "return", "str", "(", "x", ")", "\n", "# Otherwise, default has non-None type; convert x to that type", "\n", "", "else", ":", "\n", "                ", "return", "type", "(", "default", ")", "(", "x", ")", "\n", "\n", "", "", "return", "func", "\n", "\n", "", "for", "param", "in", "config_dict", ":", "\n", "        ", "default", "=", "config_dict", "[", "param", "]", "\n", "try", ":", "\n", "            ", "if", "isinstance", "(", "default", ",", "dict", ")", ":", "\n", "                ", "parser", "=", "add_flags_from_config", "(", "parser", ",", "default", ")", "\n", "", "else", ":", "\n", "                ", "parser", ".", "add_argument", "(", "\"--{}\"", ".", "format", "(", "param", ")", ",", "type", "=", "OrNone", "(", "default", ")", ",", "default", "=", "default", ")", "\n", "", "", "except", "argparse", ".", "ArgumentError", ":", "\n", "            ", "print", "(", "\n", "\"Could not add flag for param {} because it was already present.\"", ".", "format", "(", "param", ")", "\n", ")", "\n", "", "", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.training.hash_dict": [[56, 64], ["hashlib.sha256", "sorted", "hashlib.sha256.hexdigest", "list", "values.keys", "hashlib.sha256.update", "str().encode", "str"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.pair_encoder.PairEmbeddingDistance.encode"], ["", "def", "hash_dict", "(", "values", ")", ":", "\n", "    ", "\"\"\"Hash of dict key, value pairs.\"\"\"", "\n", "m", "=", "hashlib", ".", "sha256", "(", ")", "\n", "keys", "=", "sorted", "(", "list", "(", "values", ".", "keys", "(", ")", ")", ")", "\n", "for", "k", "in", "keys", ":", "\n", "        ", "if", "k", "!=", "\"seed\"", ":", "\n", "            ", "m", ".", "update", "(", "str", "(", "values", "[", "k", "]", ")", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "", "", "return", "m", ".", "hexdigest", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.training.get_savedir": [[66, 71], ["None"], "function", ["None"], ["", "def", "get_savedir", "(", "args", ")", ":", "\n", "    ", "\"\"\"Hash of args used for training.\"\"\"", "\n", "#dir_hash = hash_dict(args.__dict__)", "\n", "save_dir", "=", "\"./data\"", "#os.path.join(os.environ[\"SAVEPATH\"], args.dataset, dir_hash)", "\n", "return", "save_dir", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca.isometric_transform": [[13, 18], ["torch.sum", "torch.sum"], "function", ["None"], ["def", "isometric_transform", "(", "a", ",", "x", ")", ":", "\n", "    ", "\"\"\"Reflection (circle inversion of x through orthogonal circle centered at a).\"\"\"", "\n", "r2", "=", "torch", ".", "sum", "(", "a", "**", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "-", "1.", "\n", "u", "=", "x", "-", "a", "\n", "return", "r2", "/", "torch", ".", "sum", "(", "u", "**", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "*", "u", "+", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca.reflection_center": [[20, 23], ["torch.sum"], "function", ["None"], ["", "def", "reflection_center", "(", "mu", ")", ":", "\n", "    ", "\"\"\"Center of inversion circle.\"\"\"", "\n", "return", "mu", "/", "torch", ".", "sum", "(", "mu", "**", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca.euc_reflection": [[25, 35], ["torch.sum", "torch.sum().clamp_min", "torch.sum"], "function", ["None"], ["", "def", "euc_reflection", "(", "x", ",", "a", ")", ":", "\n", "    ", "\"\"\"\n    Euclidean reflection (also hyperbolic) of x\n    Along the geodesic that goes through a and the origin\n    (straight line)\n    \"\"\"", "\n", "xTa", "=", "torch", ".", "sum", "(", "x", "*", "a", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "norm_a_sq", "=", "torch", ".", "sum", "(", "a", "**", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ".", "clamp_min", "(", "MIN_NORM", ")", "\n", "proj", "=", "xTa", "*", "a", "/", "norm_a_sq", "\n", "return", "2", "*", "proj", "-", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca._halve": [[37, 40], ["torch.sqrt", "torch.sum"], "function", ["None"], ["", "def", "_halve", "(", "x", ")", ":", "\n", "    ", "\"\"\" computes the point on the geodesic segment from o to x at half the distance \"\"\"", "\n", "return", "x", "/", "(", "1.", "+", "torch", ".", "sqrt", "(", "1", "-", "torch", ".", "sum", "(", "x", "**", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca.hyp_lca": [[42, 58], ["lca.reflection_center", "lca.isometric_transform", "lca.euc_reflection", "lca.isometric_transform", "lca._halve", "util.ml_and_math.poincare.hyp_dist_o"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca.reflection_center", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca.isometric_transform", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca.euc_reflection", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca.isometric_transform", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.lca._halve", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.hyp_dist_o"], ["", "def", "hyp_lca", "(", "a", ",", "b", ",", "return_coord", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Computes projection of the origin on the geodesic between a and b, at scale c\n\n    More optimized than hyp_lca1\n    \"\"\"", "\n", "r", "=", "reflection_center", "(", "a", ")", "\n", "b_inv", "=", "isometric_transform", "(", "r", ",", "b", ")", "\n", "o_inv", "=", "a", "\n", "o_inv_ref", "=", "euc_reflection", "(", "o_inv", ",", "b_inv", ")", "\n", "o_ref", "=", "isometric_transform", "(", "r", ",", "o_inv_ref", ")", "\n", "proj", "=", "_halve", "(", "o_ref", ")", "\n", "if", "not", "return_coord", ":", "\n", "        ", "return", "hyp_dist_o", "(", "proj", ")", "\n", "", "else", ":", "\n", "        ", "return", "proj", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.linkage.sl_np_mst": [[17, 23], ["hierarchical_clustering.relaxed.mst.mst.mst", "hierarchical_clustering.relaxed.unionfind.unionfind.UnionFind", "unionfind.UnionFind.merge"], "function", ["None"], ["def", "sl_np_mst", "(", "similarities", ")", ":", "\n", "    ", "n", "=", "similarities", ".", "shape", "[", "0", "]", "\n", "ij", ",", "_", "=", "mst", ".", "mst", "(", "similarities", ",", "n", ")", "\n", "uf", "=", "unionfind", ".", "UnionFind", "(", "n", ")", "\n", "uf", ".", "merge", "(", "ij", ")", "\n", "return", "uf", ".", "tree", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.linkage.sl_from_embeddings": [[24, 29], ["S", "linkage.sl_np_mst", "S.numpy"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.utils.linkage.sl_np_mst"], ["", "def", "sl_from_embeddings", "(", "xs", ",", "S", ")", ":", "\n", "#xs0 = xs[None, :, :]", "\n", "#xs1 = xs[:, None, :]", "\n", "    ", "sim_mat", "=", "S", "(", "xs", ",", "xs", ")", "# (n, n)", "\n", "return", "sl_np_mst", "(", "sim_mat", ".", "numpy", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.linkage.nn_merge_uf_fast_np": [[33, 69], ["numpy.meshgrid", "numpy.tril_indices", "numpy.stack", "hierarchical_clustering.relaxed.unionfind.unionfind.UnionFind", "unionfind.UnionFind.merge", "S", "numpy.arange", "numpy.arange", "numpy.argsort", "numpy.argpartition", "int", "ks.append", "numpy.array", "print"], "function", ["None"], ["", "def", "nn_merge_uf_fast_np", "(", "xs", ",", "S", ",", "partition_ratio", "=", "None", ",", "verbose", "=", "False", ")", ":", "\n", "    ", "\"\"\" Uses Cython union find and numpy sorting\n\n    partition_ratio: either None, or real number > 1\n    similarities will be partitioned into buckets of geometrically increasing size\n    \"\"\"", "\n", "n", "=", "xs", ".", "shape", "[", "0", "]", "\n", "# Construct distance matrix (negative similarity; since numpy only has increasing sorting)", "\n", "#xs0 = xs[None, :, :]", "\n", "#xs1 = xs[:, None, :]", "\n", "dist_mat", "=", "-", "S", "(", "xs", ",", "xs", ")", "# (n, n)", "\n", "i", ",", "j", "=", "np", ".", "meshgrid", "(", "np", ".", "arange", "(", "n", ",", "dtype", "=", "int", ")", ",", "np", ".", "arange", "(", "n", ",", "dtype", "=", "int", ")", ")", "\n", "\n", "# Keep only unique pairs (upper triangular indices)", "\n", "idx", "=", "np", ".", "tril_indices", "(", "n", ",", "-", "1", ")", "\n", "ij", "=", "np", ".", "stack", "(", "[", "i", "[", "idx", "]", ",", "j", "[", "idx", "]", "]", ",", "axis", "=", "-", "1", ")", "\n", "dist_mat", "=", "dist_mat", "[", "idx", "]", "\n", "\n", "# Sort pairs", "\n", "if", "partition_ratio", "is", "None", ":", "\n", "        ", "idx", "=", "np", ".", "argsort", "(", "dist_mat", ",", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "        ", "k", ",", "ks", "=", "ij", ".", "shape", "[", "0", "]", ",", "[", "]", "\n", "while", "k", ">", "0", ":", "\n", "            ", "k", "=", "int", "(", "k", "//", "partition_ratio", ")", "\n", "ks", ".", "append", "(", "k", ")", "\n", "", "ks", "=", "np", ".", "array", "(", "ks", ")", "[", ":", ":", "-", "1", "]", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "ks", ")", "\n", "", "idx", "=", "np", ".", "argpartition", "(", "dist_mat", ",", "ks", ",", "axis", "=", "0", ")", "\n", "", "ij", "=", "ij", "[", "idx", "]", "\n", "\n", "# Union find merging", "\n", "uf", "=", "unionfind", ".", "UnionFind", "(", "n", ")", "\n", "uf", ".", "merge", "(", "ij", ")", "\n", "return", "uf", ".", "tree", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.tree.to_nx_tree": [[10, 19], ["scipy.cluster.hierarchy.to_tree", "networkx.DiGraph", "node.get_left", "node.get_right", "nx.DiGraph.add_edge", "nx.DiGraph.add_edge"], "function", ["None"], ["def", "to_nx_tree", "(", "linked", ")", ":", "\n", "    ", "tree", ",", "nodelist", "=", "to_tree", "(", "linked", ",", "rd", "=", "True", ")", "\n", "G", "=", "nx", ".", "DiGraph", "(", ")", "\n", "for", "node", "in", "nodelist", ":", "\n", "        ", "if", "node", ".", "get_left", "(", ")", ":", "\n", "            ", "G", ".", "add_edge", "(", "node", ".", "id", ",", "node", ".", "left", ".", "id", ")", "\n", "", "if", "node", ".", "get_right", "(", ")", ":", "\n", "            ", "G", ".", "add_edge", "(", "node", ".", "id", ",", "node", ".", "right", ".", "id", ")", "\n", "", "", "return", "G", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.tree.descendants_traversal": [[20, 40], ["len", "list", "list", "len", "tree.nodes", "tree.neighbors", "range", "len", "range", "len", "stack.append", "children[].pop", "stack.pop", "traversal.append"], "function", ["None"], ["", "def", "descendants_traversal", "(", "tree", ")", ":", "\n", "    ", "\"\"\"Get all descendants non-recursively, in traversal order.\"\"\"", "\n", "n", "=", "len", "(", "list", "(", "tree", ".", "nodes", "(", ")", ")", ")", "\n", "root", "=", "n", "-", "1", "\n", "\n", "traversal", "=", "[", "]", "\n", "\n", "children", "=", "[", "list", "(", "tree", ".", "neighbors", "(", "node", ")", ")", "for", "node", "in", "range", "(", "n", ")", "]", "# children remaining to process", "\n", "is_leaf", "=", "[", "len", "(", "children", "[", "node", "]", ")", "==", "0", "for", "node", "in", "range", "(", "n", ")", "]", "\n", "stack", "=", "[", "root", "]", "\n", "while", "len", "(", "stack", ")", ">", "0", ":", "\n", "        ", "node", "=", "stack", "[", "-", "1", "]", "\n", "if", "len", "(", "children", "[", "node", "]", ")", ">", "0", ":", "\n", "            ", "stack", ".", "append", "(", "children", "[", "node", "]", ".", "pop", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "node", "==", "stack", ".", "pop", "(", ")", "\n", "if", "is_leaf", "[", "node", "]", ":", "\n", "                ", "traversal", ".", "append", "(", "node", ")", "\n", "\n", "", "", "", "return", "traversal", "[", ":", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.tree.descendants_count": [[42, 70], ["len", "list", "len", "tree.nodes", "list", "range", "len", "stack.append", "list", "tree.neighbors", "children[].pop", "tree.neighbors", "len", "sum", "stack.pop"], "function", ["None"], ["", "def", "descendants_count", "(", "tree", ")", ":", "\n", "    ", "\"\"\"For every node, count its number of descendant leaves, and the number of leaves before it.\"\"\"", "\n", "n", "=", "len", "(", "list", "(", "tree", ".", "nodes", "(", ")", ")", ")", "\n", "root", "=", "n", "-", "1", "\n", "\n", "left", "=", "[", "0", "]", "*", "n", "\n", "desc", "=", "[", "0", "]", "*", "n", "\n", "leaf_idx", "=", "0", "\n", "\n", "children", "=", "[", "list", "(", "tree", ".", "neighbors", "(", "node", ")", ")", "[", ":", ":", "-", "1", "]", "for", "node", "in", "range", "(", "n", ")", "]", "# children remaining to process", "\n", "stack", "=", "[", "root", "]", "\n", "while", "len", "(", "stack", ")", ">", "0", ":", "\n", "        ", "node", "=", "stack", "[", "-", "1", "]", "\n", "if", "len", "(", "children", "[", "node", "]", ")", ">", "0", ":", "\n", "            ", "stack", ".", "append", "(", "children", "[", "node", "]", ".", "pop", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "children_", "=", "list", "(", "tree", ".", "neighbors", "(", "node", ")", ")", "\n", "\n", "if", "len", "(", "children_", ")", "==", "0", ":", "\n", "                ", "desc", "[", "node", "]", "=", "1", "\n", "left", "[", "node", "]", "=", "leaf_idx", "\n", "leaf_idx", "+=", "1", "\n", "", "else", ":", "\n", "                ", "desc", "[", "node", "]", "=", "sum", "(", "[", "desc", "[", "c", "]", "for", "c", "in", "children_", "]", ")", "\n", "left", "[", "node", "]", "=", "left", "[", "children_", "[", "0", "]", "]", "\n", "", "assert", "node", "==", "stack", ".", "pop", "(", ")", "\n", "\n", "", "", "return", "desc", ",", "left", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.optim.radam.RAdam.step": [[62, 138], ["closure", "torch.no_grad", "util.ml_and_math.poincare.egrad2rgrad.add_", "util.ml_and_math.poincare.egrad2rgrad", "exp_avg.mul_().add_", "exp_avg_sq.mul_().add_", "util.ml_and_math.poincare.project", "util.ml_and_math.poincare.ptransp", "radam.copy_or_set_", "exp_avg.set_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "util.ml_and_math.poincare.inner", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "util.ml_and_math.poincare.expmap", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.egrad2rgrad", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.project", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.ptransp", "home.repos.pwc.inspect_result.gcorso_neuroseed.optim.radam.copy_or_set_", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.inner", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.expmap"], ["def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments\n        ---------\n        closure : callable (optional)\n            A closure that reevaluates the model\n            and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "                ", "if", "\"step\"", "not", "in", "group", ":", "\n", "                    ", "group", "[", "\"step\"", "]", "=", "0", "\n", "", "betas", "=", "group", "[", "\"betas\"", "]", "\n", "weight_decay", "=", "group", "[", "\"weight_decay\"", "]", "\n", "eps", "=", "group", "[", "\"eps\"", "]", "\n", "learning_rate", "=", "group", "[", "\"lr\"", "]", "\n", "amsgrad", "=", "group", "[", "\"amsgrad\"", "]", "\n", "for", "point", "in", "group", "[", "\"params\"", "]", ":", "\n", "                    ", "grad", "=", "point", ".", "grad", "\n", "if", "grad", "is", "None", ":", "\n", "                        ", "continue", "\n", "", "if", "grad", ".", "is_sparse", ":", "\n", "                        ", "raise", "RuntimeError", "(", "\n", "\"Riemannian Adam does not support sparse gradients yet (PR is welcome)\"", "\n", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "point", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                        ", "state", "[", "\"step\"", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "\"exp_avg\"", "]", "=", "torch", ".", "zeros_like", "(", "point", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "point", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                            ", "state", "[", "\"max_exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "point", ")", "\n", "# make local variables for easy access", "\n", "", "", "exp_avg", "=", "state", "[", "\"exp_avg\"", "]", "\n", "exp_avg_sq", "=", "state", "[", "\"exp_avg_sq\"", "]", "\n", "# actual step", "\n", "grad", ".", "add_", "(", "point", ",", "alpha", "=", "weight_decay", ")", "\n", "grad", "=", "egrad2rgrad", "(", "point", ",", "grad", ")", "\n", "exp_avg", ".", "mul_", "(", "betas", "[", "0", "]", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1", "-", "betas", "[", "0", "]", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "betas", "[", "1", "]", ")", ".", "add_", "(", "inner", "(", "point", ",", "grad", ")", ",", "alpha", "=", "1", "-", "betas", "[", "1", "]", ")", "\n", "if", "amsgrad", ":", "\n", "                        ", "max_exp_avg_sq", "=", "state", "[", "\"max_exp_avg_sq\"", "]", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", ")", "\n", "", "else", ":", "\n", "                        ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", ")", "\n", "", "group", "[", "\"step\"", "]", "+=", "1", "\n", "bias_correction1", "=", "1", "-", "betas", "[", "0", "]", "**", "group", "[", "\"step\"", "]", "\n", "bias_correction2", "=", "1", "-", "betas", "[", "1", "]", "**", "group", "[", "\"step\"", "]", "\n", "step_size", "=", "(", "\n", "learning_rate", "*", "bias_correction2", "**", "0.5", "/", "bias_correction1", "\n", ")", "\n", "\n", "# copy the state, we need it for retraction", "\n", "# get the direction for ascend", "\n", "direction", "=", "exp_avg", "/", "denom", "\n", "# transport the exponential averaging to the new point", "\n", "new_point", "=", "project", "(", "expmap", "(", "-", "step_size", "*", "direction", ",", "point", ")", ")", "\n", "exp_avg_new", "=", "ptransp", "(", "point", ",", "new_point", ",", "exp_avg", ")", "\n", "# use copy only for user facing point", "\n", "copy_or_set_", "(", "point", ",", "new_point", ")", "\n", "exp_avg", ".", "set_", "(", "exp_avg_new", ")", "\n", "\n", "group", "[", "\"step\"", "]", "+=", "1", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.optim.radam.copy_or_set_": [[11, 30], ["dest.stride", "source.stride", "dest.copy_", "dest.set_"], "function", ["None"], ["def", "copy_or_set_", "(", "dest", ",", "source", ")", ":", "\n", "    ", "\"\"\"\n    A workaround to respect strides of :code:`dest` when copying :code:`source`\n    (https://github.com/geoopt/geoopt/issues/70)\n    Parameters\n    ----------\n    dest : torch.Tensor\n        Destination tensor where to store new data\n    source : torch.Tensor\n        Source data to put in the new tensor\n    Returns\n    -------\n    dest\n        torch.Tensor, modified inplace\n    \"\"\"", "\n", "if", "dest", ".", "stride", "(", ")", "!=", "source", ".", "stride", "(", ")", ":", "\n", "        ", "return", "dest", ".", "copy_", "(", "source", ")", "\n", "", "else", ":", "\n", "        ", "return", "dest", ".", "set_", "(", "source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.triples.samples_triples": [[10, 35], ["int", "numpy.arange", "numpy.array", "np.array.T.reshape", "numpy.concatenate", "numpy.meshgrid", "print", "numpy.random.choice", "print", "int", "numpy.random.choice", "numpy.repeat().reshape", "numpy.concatenate", "numpy.arange", "numpy.arange", "numpy.random.randint", "numpy.repeat", "numpy.expand_dims"], "function", ["None"], ["def", "samples_triples", "(", "n_nodes", ",", "num_samples", ")", ":", "\n", "    ", "num_samples", "=", "int", "(", "num_samples", ")", "\n", "all_nodes", "=", "np", ".", "arange", "(", "n_nodes", ")", "\n", "mesh", "=", "np", ".", "array", "(", "np", ".", "meshgrid", "(", "all_nodes", ",", "all_nodes", ")", ")", "\n", "pairs", "=", "mesh", ".", "T", ".", "reshape", "(", "-", "1", ",", "2", ")", "\n", "pairs", "=", "pairs", "[", "pairs", "[", ":", ",", "0", "]", "<", "pairs", "[", ":", ",", "1", "]", "]", "\n", "n_pairs", "=", "pairs", ".", "shape", "[", "0", "]", "\n", "if", "num_samples", "<", "n_pairs", ":", "\n", "        ", "print", "(", "\"Generating all pairs subset\"", ")", "\n", "subset", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "n_pairs", ")", ",", "num_samples", ",", "replace", "=", "False", ")", "\n", "pairs", "=", "pairs", "[", "subset", "]", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Generating all pairs superset\"", ")", "\n", "k_base", "=", "int", "(", "num_samples", "/", "n_pairs", ")", "\n", "k_rem", "=", "num_samples", "-", "(", "k_base", "*", "n_pairs", ")", "\n", "subset", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "n_pairs", ")", ",", "k_rem", ",", "replace", "=", "False", ")", "\n", "pairs_rem", "=", "pairs", "[", "subset", "]", "\n", "pairs_base", "=", "np", ".", "repeat", "(", "np", ".", "expand_dims", "(", "pairs", ",", "0", ")", ",", "k_base", ",", "axis", "=", "0", ")", ".", "reshape", "(", "(", "-", "1", ",", "2", ")", ")", "\n", "pairs", "=", "np", ".", "concatenate", "(", "[", "pairs_base", ",", "pairs_rem", "]", ",", "axis", "=", "0", ")", "\n", "", "num_samples", "=", "pairs", ".", "shape", "[", "0", "]", "\n", "triples", "=", "np", ".", "concatenate", "(", "\n", "[", "pairs", ",", "np", ".", "random", ".", "randint", "(", "n_nodes", ",", "size", "=", "(", "num_samples", ",", "1", ")", ")", "]", ",", "\n", "axis", "=", "1", "\n", ")", "\n", "return", "triples", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.triples.generate_all_triples": [[37, 44], ["tqdm.tqdm", "numpy.array", "numpy.arange", "numpy.arange", "numpy.arange"], "function", ["None"], ["", "def", "generate_all_triples", "(", "n_nodes", ")", ":", "\n", "    ", "triples", "=", "[", "]", "\n", "for", "n1", "in", "tqdm", "(", "np", ".", "arange", "(", "n_nodes", ")", ")", ":", "\n", "        ", "for", "n2", "in", "np", ".", "arange", "(", "n1", "+", "1", ",", "n_nodes", ")", ":", "\n", "            ", "for", "n3", "in", "np", ".", "arange", "(", "n2", "+", "1", ",", "n_nodes", ")", ":", "\n", "                ", "triples", "+=", "[", "(", "n1", ",", "n2", ",", "n3", ")", "]", "\n", "", "", "", "return", "np", ".", "array", "(", "triples", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.hc_dataset.HCTripletDataset.__init__": [[24, 31], ["hc_dataset.HCTripletDataset.generate_triples"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.hc_dataset.HCTripletDataset.generate_triples"], ["def", "__init__", "(", "self", ",", "sequences", ",", "similarities", ",", "num_samples", ")", ":", "\n", "        ", "\"\"\"Creates Hierarchical Clustering dataset with triples. \"\"\"", "\n", "self", ".", "sequences", "=", "sequences", "\n", "self", ".", "similarities", "=", "similarities", "\n", "self", ".", "n_nodes", "=", "self", ".", "similarities", ".", "shape", "[", "0", "]", "\n", "self", ".", "triples", "=", "self", ".", "generate_triples", "(", "num_samples", ")", "\n", "self", ".", "len_sequences", "=", "sequences", ".", "shape", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.hc_dataset.HCTripletDataset.__len__": [[32, 34], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "triples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.hc_dataset.HCTripletDataset.__getitem__": [[35, 46], ["numpy.array", "numpy.array", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "triple", "=", "self", ".", "triples", "[", "idx", "]", "\n", "seq1", "=", "self", ".", "sequences", "[", "triple", "[", "0", "]", "]", "\n", "seq2", "=", "self", ".", "sequences", "[", "triple", "[", "1", "]", "]", "\n", "seq3", "=", "self", ".", "sequences", "[", "triple", "[", "2", "]", "]", "\n", "sequences", "=", "np", ".", "array", "(", "[", "seq1", ",", "seq2", ",", "seq3", "]", ")", "\n", "s12", "=", "self", ".", "similarities", "[", "triple", "[", "0", "]", ",", "triple", "[", "1", "]", "]", "\n", "s13", "=", "self", ".", "similarities", "[", "triple", "[", "0", "]", ",", "triple", "[", "2", "]", "]", "\n", "s23", "=", "self", ".", "similarities", "[", "triple", "[", "1", "]", ",", "triple", "[", "2", "]", "]", "\n", "similarities", "=", "np", ".", "array", "(", "[", "s12", ",", "s13", ",", "s23", "]", ")", "\n", "return", "torch", ".", "from_numpy", "(", "triple", ")", ",", "torch", ".", "from_numpy", "(", "sequences", ")", ",", "torch", ".", "from_numpy", "(", "similarities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.hc_dataset.HCTripletDataset.generate_triples": [[47, 55], ["logging.info", "logging.info", "hierarchical_clustering.relaxed.datasets.triples.samples_triples.astype", "hierarchical_clustering.relaxed.datasets.triples.generate_all_triples", "hierarchical_clustering.relaxed.datasets.triples.samples_triples"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.triples.generate_all_triples", "home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.triples.samples_triples"], ["", "def", "generate_triples", "(", "self", ",", "num_samples", ")", ":", "\n", "        ", "logging", ".", "info", "(", "\"Generating triples.\"", ")", "\n", "if", "num_samples", "<", "0", ":", "\n", "            ", "triples", "=", "generate_all_triples", "(", "self", ".", "n_nodes", ")", "\n", "", "else", ":", "\n", "            ", "triples", "=", "samples_triples", "(", "self", ".", "n_nodes", ",", "num_samples", "=", "num_samples", ")", "\n", "", "logging", ".", "info", "(", "\"Total of {} triples\"", ".", "format", "(", "triples", ".", "shape", "[", "0", "]", ")", ")", "\n", "return", "triples", ".", "astype", "(", "\"int64\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.hc_dataset.load_hc_data": [[15, 19], ["open", "pickle.load"], "function", ["None"], ["def", "load_hc_data", "(", "dataset", ")", ":", "\n", "    ", "with", "open", "(", "dataset", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "x", ",", "similarities", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "x", ",", "similarities", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.unsupervised.unsupervised.execute_test": [[19, 41], ["print", "numpy.random.seed", "torch.manual_seed", "torch.load", "model_class", "print", "model_class.load_state_dict", "model_class.eval", "unsupervised.hierarchical_clustering_testing", "torch.cuda.is_available", "torch.cuda.manual_seed", "vars"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.unsupervised.unsupervised.hierarchical_clustering_testing"], ["def", "execute_test", "(", "args", ")", ":", "\n", "# set device", "\n", "    ", "args", ".", "cuda", "=", "not", "args", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "device", "=", "'cuda'", "if", "args", ".", "cuda", "else", "'cpu'", "\n", "print", "(", "'Using device:'", ",", "device", ")", "\n", "\n", "# set the random seed", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "# model", "\n", "", "model_class", ",", "model_args", ",", "state_dict", ",", "distance", "=", "torch", ".", "load", "(", "args", ".", "encoder_path", ")", "\n", "encoder_model", "=", "model_class", "(", "**", "vars", "(", "model_args", ")", ")", "\n", "\n", "# Restore best model", "\n", "print", "(", "'Loading model '", "+", "args", ".", "encoder_path", ")", "\n", "encoder_model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "encoder_model", ".", "eval", "(", ")", "\n", "\n", "hierarchical_clustering_testing", "(", "encoder_model", ",", "args", ".", "data", ",", "args", ".", "batch_size", ",", "device", ",", "distance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.unsupervised.unsupervised.hierarchical_clustering_testing": [[43, 68], ["hierarchical_clustering.relaxed.datasets.hc_dataset.load_hc_data", "torch.from_numpy().long", "print", "util.data_handling.data_loader.index_to_one_hot", "torch.utils.data.DataLoader", "closest_string.test.embed_strings", "numpy.diag_indices", "print", "hierarchical_clustering.relaxed.utils.tree.to_nx_tree", "hierarchical_clustering.relaxed.utils.metrics.dasgupta_cost", "torch.from_numpy", "scipy.cluster.hierarchy.linkage", "scipy.spatial.distance.squareform"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.hc_dataset.load_hc_data", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot", "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string.test.embed_strings", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.tree.to_nx_tree", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.metrics.dasgupta_cost"], ["", "def", "hierarchical_clustering_testing", "(", "encoder_model", ",", "data_path", ",", "batch_size", ",", "device", ",", "distance", ")", ":", "\n", "# load data", "\n", "    ", "strings", ",", "similarities", "=", "load_hc_data", "(", "data_path", ")", "\n", "strings", "=", "torch", ".", "from_numpy", "(", "strings", ")", ".", "long", "(", ")", "\n", "print", "(", "\"Hierarchical\"", ",", "strings", ".", "shape", ")", "\n", "strings", "=", "index_to_one_hot", "(", "strings", ")", "\n", "strings_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "strings", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "\n", "# embed sequences and compute distance matrix", "\n", "embedded_strings", "=", "embed_strings", "(", "strings_loader", ",", "encoder_model", ",", "device", ")", "\n", "estimate_distances", "=", "DISTANCE_MATRIX", "[", "distance", "]", "(", "embedded_strings", ",", "embedded_strings", ",", "encoder_model", ".", "scaling", ")", "\n", "\n", "# fix the problems caused by floating point arithmetic: it must be symmetric and with diagonal 0", "\n", "estimate_distances", "=", "(", "estimate_distances", "+", "estimate_distances", ".", "T", ")", "/", "2", "\n", "ind", "=", "np", ".", "diag_indices", "(", "estimate_distances", ".", "shape", "[", "0", "]", ")", "\n", "estimate_distances", "[", "ind", "[", "0", "]", ",", "ind", "[", "1", "]", "]", "=", "0.0", "\n", "\n", "# run agglomerative clustering algorithms", "\n", "metrics", "=", "{", "}", "\n", "for", "method", "in", "[", "\"single\"", ",", "\"complete\"", ",", "\"average\"", ",", "\"ward\"", "]", ":", "\n", "        ", "metrics", "[", "method", "]", "=", "{", "}", "\n", "baseline_tree", "=", "to_nx_tree", "(", "linkage", "(", "squareform", "(", "estimate_distances", ")", ",", "method", ")", ")", "\n", "dc", "=", "dasgupta_cost", "(", "baseline_tree", ",", "similarities", ")", "\n", "metrics", "[", "method", "]", "[", "\"DC\"", "]", "=", "dc", "\n", "", "print", "(", "metrics", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.Artanh.forward": [[14, 21], ["x.double.double.clamp", "ctx.save_for_backward", "x.double.double.double", "torch.log_().sub_().mul_().to", "torch.log_().sub_().mul_", "torch.log_().sub_", "torch.log_", "torch.log_"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "clamp", "(", "-", "1", "+", "1e-5", ",", "1", "-", "1e-5", ")", "\n", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "dtype", "=", "x", ".", "dtype", "\n", "x", "=", "x", ".", "double", "(", ")", "\n", "return", "(", "torch", ".", "log_", "(", "1", "+", "x", ")", ".", "sub_", "(", "torch", ".", "log_", "(", "1", "-", "x", ")", ")", ")", ".", "mul_", "(", "0.5", ")", ".", "to", "(", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.Artanh.backward": [[22, 27], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "input", ",", "=", "ctx", ".", "saved_tensors", "\n", "grad", "=", "grad_output", "/", "(", "1", "-", "input", "**", "2", ")", "\n", "return", "grad", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.Arcosh.forward": [[40, 46], ["x.clamp.clamp.clamp", "ctx.save_for_backward", "x.clamp.clamp.double", "torch.sqrt_", "x.clamp.double.pow"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "clamp", "(", "min", "=", "1", "+", "1e-7", ")", "\n", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "z", "=", "x", ".", "double", "(", ")", "\n", "return", "(", "z", "+", "torch", ".", "sqrt_", "(", "z", ".", "pow", "(", "2", ")", "-", "1", ")", ")", ".", "clamp_min_", "(", "1e-15", ")", ".", "log_", "(", ")", ".", "to", "(", "x", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.Arcosh.backward": [[47, 51], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "input", ",", "=", "ctx", ".", "saved_tensors", "\n", "return", "grad_output", "/", "(", "input", "**", "2", "-", "1", ")", "**", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.Arsinh.forward": [[64, 69], ["ctx.save_for_backward", "x.double", "torch.sqrt_", "x.double.pow"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "z", "=", "x", ".", "double", "(", ")", "\n", "return", "(", "z", "+", "torch", ".", "sqrt_", "(", "1", "+", "z", ".", "pow", "(", "2", ")", ")", ")", ".", "clamp_min_", "(", "1e-15", ")", ".", "log_", "(", ")", ".", "to", "(", "x", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.Arsinh.backward": [[70, 74], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "input", ",", "=", "ctx", ".", "saved_tensors", "\n", "return", "grad_output", "/", "(", "1", "+", "input", "**", "2", ")", "**", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.arctanh": [[29, 31], ["Artanh.apply"], "function", ["None"], ["", "", "def", "arctanh", "(", "x", ")", ":", "\n", "    ", "return", "Artanh", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.tanh": [[33, 35], ["x.clamp().tanh", "x.clamp"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.tanh"], ["", "def", "tanh", "(", "x", ")", ":", "\n", "    ", "return", "x", ".", "clamp", "(", "-", "15", ",", "15", ")", ".", "tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.arcosh": [[53, 55], ["Arcosh.apply"], "function", ["None"], ["", "", "def", "arcosh", "(", "x", ")", ":", "\n", "    ", "return", "Arcosh", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.cosh": [[57, 59], ["x.clamp().cosh", "x.clamp"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.cosh"], ["", "def", "cosh", "(", "x", ",", "clamp", "=", "15", ")", ":", "\n", "    ", "return", "x", ".", "clamp", "(", "-", "clamp", ",", "clamp", ")", ".", "cosh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.arsinh": [[76, 78], ["Arsinh.apply"], "function", ["None"], ["", "", "def", "arsinh", "(", "x", ")", ":", "\n", "    ", "return", "Arsinh", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.sinh": [[80, 82], ["x.clamp().sinh", "x.clamp"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.sinh"], ["", "def", "sinh", "(", "x", ",", "clamp", "=", "15", ")", ":", "\n", "    ", "return", "x", ".", "clamp", "(", "-", "clamp", ",", "clamp", ")", ".", "sinh", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.layers.FCLayer.__init__": [[30, 51], ["torch.Module.__init__", "locals", "torch.Linear().to", "torch.Linear().to", "torch.Linear().to", "layers.get_activation", "layers.FCLayer.reset_parameters", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.BatchNorm1d().to", "torch.BatchNorm1d().to", "torch.BatchNorm1d().to", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.layers.get_activation", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.layers.FCLayer.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "in_size", ",", "out_size", ",", "activation", "=", "'relu'", ",", "dropout", "=", "0.", ",", "b_norm", "=", "False", ",", "bias", "=", "True", ",", "init_fn", "=", "None", ",", "\n", "device", "=", "'cpu'", ")", ":", "\n", "        ", "super", "(", "FCLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "__params", "=", "locals", "(", ")", "\n", "del", "self", ".", "__params", "[", "'__class__'", "]", "\n", "del", "self", ".", "__params", "[", "'self'", "]", "\n", "self", ".", "in_size", "=", "in_size", "\n", "self", ".", "out_size", "=", "out_size", "\n", "self", ".", "bias", "=", "bias", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "in_size", ",", "out_size", ",", "bias", "=", "bias", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "dropout", "=", "None", "\n", "self", ".", "b_norm", "=", "None", "\n", "if", "dropout", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "", "if", "b_norm", ":", "\n", "            ", "self", ".", "b_norm", "=", "nn", ".", "BatchNorm1d", "(", "out_size", ")", ".", "to", "(", "device", ")", "\n", "", "self", ".", "activation", "=", "get_activation", "(", "activation", ")", "\n", "self", ".", "init_fn", "=", "nn", ".", "init", ".", "xavier_uniform_", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.layers.FCLayer.reset_parameters": [[52, 58], ["init_fn", "layers.FCLayer.linear.bias.data.zero_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ",", "init_fn", "=", "None", ")", ":", "\n", "        ", "init_fn", "=", "init_fn", "or", "self", ".", "init_fn", "\n", "if", "init_fn", "is", "not", "None", ":", "\n", "            ", "init_fn", "(", "self", ".", "linear", ".", "weight", ",", "1", "/", "self", ".", "in_size", ")", "\n", "", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "linear", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.layers.FCLayer.forward": [[59, 71], ["layers.FCLayer.linear", "layers.FCLayer.activation", "layers.FCLayer.dropout", "layers.FCLayer.b_norm().transpose", "layers.FCLayer.b_norm", "layers.FCLayer.b_norm", "layers.FCLayer.transpose"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "h", "=", "self", ".", "linear", "(", "x", ")", "\n", "if", "self", ".", "activation", "is", "not", "None", ":", "\n", "            ", "h", "=", "self", ".", "activation", "(", "h", ")", "\n", "", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "h", "=", "self", ".", "dropout", "(", "h", ")", "\n", "", "if", "self", ".", "b_norm", "is", "not", "None", ":", "\n", "            ", "if", "h", ".", "shape", "[", "1", "]", "!=", "self", ".", "out_size", ":", "\n", "                ", "h", "=", "self", ".", "b_norm", "(", "h", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "", "else", ":", "\n", "                ", "h", "=", "self", ".", "b_norm", "(", "h", ")", "\n", "", "", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.layers.FCLayer.__repr__": [[72, 76], ["str", "str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "' ('", "+", "str", "(", "self", ".", "in_size", ")", "+", "' -> '", "+", "str", "(", "self", ".", "out_size", ")", "+", "')'", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.layers.MLP.__init__": [[83, 103], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "layers.MLP.fully_connected.append", "layers.MLP.fully_connected.append", "range", "layers.MLP.fully_connected.append", "layers.FCLayer", "layers.FCLayer", "layers.MLP.fully_connected.append", "layers.FCLayer", "layers.FCLayer"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["def", "__init__", "(", "self", ",", "in_size", ",", "hidden_size", ",", "out_size", ",", "layers", ",", "mid_activation", "=", "'relu'", ",", "last_activation", "=", "'none'", ",", "\n", "dropout", "=", "0.", ",", "mid_b_norm", "=", "False", ",", "last_b_norm", "=", "False", ",", "device", "=", "'cuda'", ")", ":", "\n", "        ", "super", "(", "MLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "in_size", "=", "in_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "out_size", "=", "out_size", "\n", "\n", "self", ".", "fully_connected", "=", "nn", ".", "ModuleList", "(", ")", "\n", "if", "layers", "<=", "1", ":", "\n", "            ", "self", ".", "fully_connected", ".", "append", "(", "FCLayer", "(", "in_size", ",", "out_size", ",", "activation", "=", "last_activation", ",", "b_norm", "=", "last_b_norm", ",", "\n", "device", "=", "device", ",", "dropout", "=", "dropout", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "fully_connected", ".", "append", "(", "FCLayer", "(", "in_size", ",", "hidden_size", ",", "activation", "=", "mid_activation", ",", "b_norm", "=", "mid_b_norm", ",", "\n", "device", "=", "device", ",", "dropout", "=", "dropout", ")", ")", "\n", "for", "_", "in", "range", "(", "layers", "-", "2", ")", ":", "\n", "                ", "self", ".", "fully_connected", ".", "append", "(", "FCLayer", "(", "hidden_size", ",", "hidden_size", ",", "activation", "=", "mid_activation", ",", "\n", "b_norm", "=", "mid_b_norm", ",", "device", "=", "device", ",", "dropout", "=", "dropout", ")", ")", "\n", "", "self", ".", "fully_connected", ".", "append", "(", "FCLayer", "(", "hidden_size", ",", "out_size", ",", "activation", "=", "last_activation", ",", "b_norm", "=", "last_b_norm", ",", "\n", "device", "=", "device", ",", "dropout", "=", "dropout", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.layers.MLP.forward": [[104, 108], ["fc"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "fc", "in", "self", ".", "fully_connected", ":", "\n", "            ", "x", "=", "fc", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.layers.MLP.__repr__": [[109, 113], ["str", "str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "' ('", "+", "str", "(", "self", ".", "in_size", ")", "+", "' -> '", "+", "str", "(", "self", ".", "out_size", ")", "+", "')'", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.layers.GatedCNNLayer.__init__": [[117, 123], ["torch.Module.__init__", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "padding", "=", "True", ")", ":", "\n", "        ", "super", "(", "GatedCNNLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "kernel", "=", "kernel_size", "\n", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "in_channels", "=", "in_channels", ",", "out_channels", "=", "out_channels", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ")", "\n", "self", ".", "conv_gate", "=", "nn", ".", "Conv1d", "(", "in_channels", "=", "in_channels", ",", "out_channels", "=", "out_channels", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.layers.GatedCNNLayer.forward": [[124, 131], ["layers.GatedCNNLayer.conv", "layers.GatedCNNLayer.conv_gate", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x (B, Cin, N)", "\n", "\n", "        ", "A", "=", "self", ".", "conv", "(", "x", ")", "\n", "B", "=", "self", ".", "conv_gate", "(", "x", ")", "\n", "x", "=", "A", "*", "torch", ".", "sigmoid", "(", "B", ")", "\n", "return", "x", "", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.layers.get_activation": [[14, 26], ["callable", "isinstance", "activation.lower", "len", "vars", "activation.lower", "x.lower"], "function", ["None"], ["def", "get_activation", "(", "activation", ")", ":", "\n", "    ", "\"\"\" returns the activation function represented by the input string \"\"\"", "\n", "if", "activation", "and", "callable", "(", "activation", ")", ":", "\n", "# activation is already a function", "\n", "        ", "return", "activation", "\n", "# search in SUPPORTED_ACTIVATION_MAP a torch.nn.modules.activation", "\n", "", "activation", "=", "[", "x", "for", "x", "in", "SUPPORTED_ACTIVATION_MAP", "if", "activation", ".", "lower", "(", ")", "==", "x", ".", "lower", "(", ")", "]", "\n", "assert", "len", "(", "activation", ")", "==", "1", "and", "isinstance", "(", "activation", "[", "0", "]", ",", "str", ")", ",", "'Unhandled activation function'", "\n", "activation", "=", "activation", "[", "0", "]", "\n", "if", "activation", ".", "lower", "(", ")", "==", "'none'", ":", "\n", "        ", "return", "None", "\n", "", "return", "vars", "(", "torch", ".", "nn", ".", "modules", ".", "activation", ")", "[", "activation", "]", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.egrad2rgrad": [[16, 21], ["poincare.lambda_", "lambda_.pow"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.lambda_"], ["def", "egrad2rgrad", "(", "p", ",", "dp", ")", ":", "\n", "    ", "\"\"\"Converts Euclidean gradient to Hyperbolic gradient.\"\"\"", "\n", "lambda_p", "=", "lambda_", "(", "p", ")", "\n", "dp", "/=", "lambda_p", ".", "pow", "(", "2", ")", "\n", "return", "dp", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.lambda_": [[23, 27], ["torch.sum", "x.data.pow"], "function", ["None"], ["", "def", "lambda_", "(", "x", ")", ":", "\n", "    ", "\"\"\"Computes the conformal factor.\"\"\"", "\n", "x_sqnorm", "=", "torch", ".", "sum", "(", "x", ".", "data", ".", "pow", "(", "2", ")", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "return", "2", "/", "(", "1.", "-", "x_sqnorm", ")", ".", "clamp_min", "(", "MIN_NORM", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.inner": [[29, 35], ["poincare.lambda_"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.lambda_"], ["", "def", "inner", "(", "x", ",", "u", ",", "v", "=", "None", ")", ":", "\n", "    ", "\"\"\"Computes inner product for two tangent vectors.\"\"\"", "\n", "if", "v", "is", "None", ":", "\n", "        ", "v", "=", "u", "\n", "", "lx", "=", "lambda_", "(", "x", ")", "\n", "return", "lx", "**", "2", "*", "(", "u", "*", "v", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.gyration": [[37, 48], ["u.pow().sum", "v.pow().sum", "u.pow", "v.pow", "d.clamp_min"], "function", ["None"], ["", "def", "gyration", "(", "u", ",", "v", ",", "w", ")", ":", "\n", "    ", "\"\"\"Gyration.\"\"\"", "\n", "u2", "=", "u", ".", "pow", "(", "2", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "v2", "=", "v", ".", "pow", "(", "2", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "uv", "=", "(", "u", "*", "v", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "uw", "=", "(", "u", "*", "w", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "vw", "=", "(", "v", "*", "w", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "a", "=", "-", "uw", "*", "v2", "+", "vw", "+", "2", "*", "uv", "*", "vw", "\n", "b", "=", "-", "vw", "*", "u2", "-", "uw", "\n", "d", "=", "1", "+", "2", "*", "uv", "+", "u2", "*", "v2", "\n", "return", "w", "+", "2", "*", "(", "a", "*", "u", "+", "b", "*", "v", ")", "/", "d", ".", "clamp_min", "(", "MIN_NORM", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.ptransp": [[50, 55], ["poincare.lambda_", "poincare.lambda_", "poincare.gyration"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.lambda_", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.lambda_", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.gyration"], ["", "def", "ptransp", "(", "x", ",", "y", ",", "u", ")", ":", "\n", "    ", "\"\"\"Parallel transport.\"\"\"", "\n", "lx", "=", "lambda_", "(", "x", ")", "\n", "ly", "=", "lambda_", "(", "y", ")", "\n", "return", "gyration", "(", "y", ",", "-", "x", ",", "u", ")", "*", "lx", "/", "ly", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.expmap": [[57, 62], ["u.norm().clamp_min", "poincare.mobius_add", "u.norm", "util.ml_and_math.math.tanh", "poincare.lambda_"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.mobius_add", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.tanh", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.lambda_"], ["", "def", "expmap", "(", "u", ",", "p", ")", ":", "\n", "    ", "u_norm", "=", "u", ".", "norm", "(", "dim", "=", "-", "1", ",", "p", "=", "2", ",", "keepdim", "=", "True", ")", ".", "clamp_min", "(", "MIN_NORM", ")", "\n", "second_term", "=", "tanh", "(", "lambda_", "(", "p", ")", "*", "u_norm", "/", "2", ")", "*", "u", "/", "u_norm", "\n", "gamma_1", "=", "mobius_add", "(", "p", ",", "second_term", ")", "\n", "return", "gamma_1", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.project": [[64, 72], ["x.norm().clamp_min", "torch.where", "x.norm"], "function", ["None"], ["", "def", "project", "(", "x", ")", ":", "\n", "    ", "\"\"\"Projects points on the manifold.\"\"\"", "\n", "norm", "=", "x", ".", "norm", "(", "dim", "=", "-", "1", ",", "p", "=", "2", ",", "keepdim", "=", "True", ")", ".", "clamp_min", "(", "MIN_NORM", ")", "\n", "eps", "=", "BALL_EPS", "[", "x", ".", "dtype", "]", "\n", "maxnorm", "=", "(", "1", "-", "eps", ")", "\n", "cond", "=", "norm", ">", "maxnorm", "\n", "projected", "=", "x", "/", "norm", "*", "maxnorm", "\n", "return", "torch", ".", "where", "(", "cond", ",", "projected", ",", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.mobius_add": [[74, 82], ["torch.sum", "torch.sum", "torch.sum", "denom.clamp_min"], "function", ["None"], ["", "def", "mobius_add", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\"Mobius addition.\"\"\"", "\n", "x2", "=", "torch", ".", "sum", "(", "x", "*", "x", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "y2", "=", "torch", ".", "sum", "(", "y", "*", "y", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "xy", "=", "torch", ".", "sum", "(", "x", "*", "y", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "num", "=", "(", "1", "+", "2", "*", "xy", "+", "y2", ")", "*", "x", "+", "(", "1", "-", "x2", ")", "*", "y", "\n", "denom", "=", "1", "+", "2", "*", "xy", "+", "x2", "*", "y2", "\n", "return", "num", "/", "denom", ".", "clamp_min", "(", "MIN_NORM", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.mobius_mul": [[84, 88], ["x.norm().clamp_min", "x.norm", "util.ml_and_math.math.tanh", "util.ml_and_math.math.arctanh"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.tanh", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.arctanh"], ["", "def", "mobius_mul", "(", "x", ",", "t", ")", ":", "\n", "    ", "\"\"\"Mobius scalar multiplication.\"\"\"", "\n", "normx", "=", "x", ".", "norm", "(", "dim", "=", "-", "1", ",", "p", "=", "2", ",", "keepdim", "=", "True", ")", ".", "clamp_min", "(", "MIN_NORM", ")", "\n", "return", "tanh", "(", "t", "*", "arctanh", "(", "normx", ")", ")", "*", "x", "/", "normx", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.get_midpoint_o": [[90, 95], ["poincare.mobius_mul"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.mobius_mul"], ["", "def", "get_midpoint_o", "(", "x", ")", ":", "\n", "    ", "\"\"\"\n    Computes hyperbolic midpoint between x and the origin.\n    \"\"\"", "\n", "return", "mobius_mul", "(", "x", ",", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.poincare.hyp_dist_o": [[97, 103], ["x.norm", "util.ml_and_math.math.arctanh"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.arctanh"], ["", "def", "hyp_dist_o", "(", "x", ")", ":", "\n", "    ", "\"\"\"\n    Computes hyperbolic distance between x and the origin.\n    \"\"\"", "\n", "x_norm", "=", "x", ".", "norm", "(", "dim", "=", "-", "1", ",", "p", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "return", "2", "*", "arctanh", "(", "x_norm", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.__init__": [[4, 9], ["tuple"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "len_tuple", "=", "0", ")", ":", "\n", "        ", "self", ".", "len_tuple", "=", "len_tuple", "\n", "self", ".", "avg", "=", "0", "if", "len_tuple", "==", "0", "else", "tuple", "(", "[", "0", "]", "*", "len_tuple", ")", "\n", "self", ".", "sum", "=", "0", "if", "len_tuple", "==", "0", "else", "[", "0", "]", "*", "len_tuple", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update": [[10, 18], ["tuple", "torch.is_tensor", "val.data.item", "range", "torch.is_tensor", "val[].data.item"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "count", "+=", "n", "\n", "if", "self", ".", "len_tuple", "==", "0", ":", "\n", "            ", "self", ".", "sum", "+=", "(", "val", ".", "data", ".", "item", "(", ")", "if", "torch", ".", "is_tensor", "(", "val", ")", "else", "val", ")", "*", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "", "else", ":", "\n", "            ", "self", ".", "sum", "=", "[", "self", ".", "sum", "[", "i", "]", "+", "(", "val", "[", "i", "]", ".", "data", ".", "item", "(", ")", "if", "torch", ".", "is_tensor", "(", "val", ")", "else", "val", "[", "i", "]", ")", "*", "n", "for", "i", "in", "range", "(", "self", ".", "len_tuple", ")", "]", "\n", "self", ".", "avg", "=", "tuple", "(", "s", "/", "self", ".", "count", "for", "s", "in", "self", ".", "sum", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.accuracy": [[20, 24], ["torch.argmax", "torch.mean", "torch.mean.item", "torch.eq().float", "torch.eq"], "function", ["None"], ["", "", "", "def", "accuracy", "(", "output", ",", "target", ")", ":", "\n", "    ", "estimate", "=", "torch", ".", "argmax", "(", "output", ",", "dim", "=", "1", ")", "\n", "acc", "=", "torch", ".", "mean", "(", "torch", ".", "eq", "(", "estimate", ",", "target", ")", ".", "float", "(", ")", ")", "\n", "return", "acc", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.MAPE": [[26, 28], ["torch.mean", "torch.abs"], "function", ["None"], ["", "def", "MAPE", "(", "pred", ",", "label", ",", "eps", "=", "1e-6", ")", ":", "\n", "    ", "return", "torch", ".", "mean", "(", "torch", ".", "abs", "(", "pred", "-", "label", ")", "/", "(", "label", "+", "eps", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_matrix.euclidean_matrix": [[9, 12], ["torch.cdist", "torch.cdist"], "function", ["None"], ["def", "euclidean_matrix", "(", "enc_reference", ",", "enc_query", ",", "scaling", "=", "None", ")", ":", "\n", "    ", "distances", "=", "torch", ".", "cdist", "(", "enc_reference", ",", "enc_query", ")", "\n", "return", "distances", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_matrix.square_matrix": [[14, 17], ["distance_matrix.euclidean_matrix"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_matrix.euclidean_matrix"], ["", "def", "square_matrix", "(", "enc_reference", ",", "enc_query", ",", "scaling", "=", "None", ")", ":", "\n", "    ", "d", "=", "euclidean_matrix", "(", "enc_reference", ",", "enc_query", ")", "\n", "return", "d", "*", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_matrix.manhattan_matrix": [[19, 22], ["torch.cdist", "torch.cdist"], "function", ["None"], ["", "def", "manhattan_matrix", "(", "enc_reference", ",", "enc_query", ",", "scaling", "=", "None", ")", ":", "\n", "    ", "distances", "=", "torch", ".", "cdist", "(", "enc_reference", ",", "enc_query", ",", "p", "=", "1", ")", "\n", "return", "distances", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_matrix.cosine_matrix": [[24, 31], ["torch.zeros", "torch.zeros", "range", "torch.cosine_similarity", "enc_query[].repeat"], "function", ["None"], ["", "def", "cosine_matrix", "(", "enc_reference", ",", "enc_query", ",", "scaling", "=", "None", ")", ":", "\n", "    ", "(", "N", ",", "D", ")", "=", "enc_reference", ".", "shape", "\n", "(", "M", ",", "D", ")", "=", "enc_query", ".", "shape", "\n", "cosine_sim", "=", "torch", ".", "zeros", "(", "(", "N", ",", "M", ")", ",", "device", "=", "enc_reference", ".", "device", ")", "\n", "for", "j", "in", "range", "(", "M", ")", ":", "\n", "        ", "cosine_sim", "[", ":", ",", "j", "]", "=", "F", ".", "cosine_similarity", "(", "enc_reference", ",", "enc_query", "[", "j", ":", "j", "+", "1", "]", ".", "repeat", "(", "N", ",", "1", ")", ")", "\n", "", "return", "1", "-", "cosine_sim", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_matrix.hyperbolic_matrix": [[33, 43], ["torch.zeros", "torch.zeros", "range", "util.distance_functions.distance_functions.hyperbolic_distance", "enc_query[].repeat", "torch.zeros.detach().cpu", "scaling.detach().cpu", "torch.zeros.detach", "scaling.detach"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_functions.hyperbolic_distance"], ["", "def", "hyperbolic_matrix", "(", "enc_reference", ",", "enc_query", ",", "scaling", "=", "None", ")", ":", "\n", "    ", "(", "N", ",", "D", ")", "=", "enc_reference", ".", "shape", "\n", "(", "M", ",", "D", ")", "=", "enc_query", ".", "shape", "\n", "d", "=", "torch", ".", "zeros", "(", "(", "N", ",", "M", ")", ",", "device", "=", "enc_reference", ".", "device", ")", "\n", "for", "j", "in", "range", "(", "M", ")", ":", "\n", "        ", "d", "[", ":", ",", "j", "]", "=", "hyperbolic_distance", "(", "enc_reference", ",", "enc_query", "[", "j", ":", "j", "+", "1", "]", ".", "repeat", "(", "N", ",", "1", ")", ")", "\n", "\n", "", "if", "scaling", "is", "not", "None", ":", "\n", "        ", "d", "=", "d", ".", "detach", "(", ")", ".", "cpu", "(", ")", "*", "scaling", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_matrix.hyperbolic_matrix_numpy": [[45, 52], ["numpy.sum", "numpy.sum", "numpy.arccosh", "scipy.spatial.distance.cdist", "numpy.maximum", "numpy.maximum", "numpy.expand_dims", "numpy.expand_dims"], "function", ["None"], ["", "def", "hyperbolic_matrix_numpy", "(", "u", ",", "v", ",", "eps", "=", "1e-9", ")", ":", "\n", "    ", "m", "=", "scipy", ".", "spatial", ".", "distance", ".", "cdist", "(", "u", ",", "v", ")", "**", "2", "\n", "u_sqr", "=", "np", ".", "sum", "(", "u", "**", "2", ",", "axis", "=", "1", ")", "\n", "v_sqr", "=", "np", ".", "sum", "(", "v", "**", "2", ",", "axis", "=", "1", ")", "\n", "divisor", "=", "np", ".", "maximum", "(", "1.", "-", "np", ".", "expand_dims", "(", "u_sqr", ",", "axis", "=", "1", ")", ",", "eps", ")", "*", "np", ".", "maximum", "(", "1.", "-", "np", ".", "expand_dims", "(", "v_sqr", ",", "axis", "=", "0", ")", ",", "eps", ")", "\n", "D_ij", "=", "np", ".", "arccosh", "(", "1", "+", "2", "*", "m", "/", "divisor", ")", "\n", "return", "D_ij", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.geometric_median.mean_distances": [[17, 19], ["scipy.spatial.distance.cdist().mean", "scipy.spatial.distance.cdist"], "function", ["None"], ["def", "mean_distances", "(", "center", ",", "points", ",", "distance", "=", "'euclidean'", ")", ":", "\n", "    ", "return", "cdist", "(", "[", "center", "]", ",", "points", ",", "metric", "=", "distance_convertion", "[", "distance", "]", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.geometric_median.geometric_median": [[21, 30], ["points.mean", "scipy.optimize.minimize", "scipy.optimize.minimize", "functools.partial", "functools.partial"], "function", ["None"], ["", "def", "geometric_median", "(", "points", ",", "distance", "=", "'euclidean'", ")", ":", "\n", "    ", "centroid", "=", "points", ".", "mean", "(", "axis", "=", "0", ")", "# start from centroid", "\n", "\n", "if", "distance", "==", "'hyperbolic'", ":", "\n", "#constraint = NonlinearConstraint(fun=np.norm, lb=0, ub=1-1e-6)", "\n", "        ", "optimize_result", "=", "minimize", "(", "partial", "(", "mean_distances", ",", "points", "=", "points", ",", "distance", "=", "distance", ")", ",", "centroid", ")", "\n", "", "else", ":", "\n", "        ", "optimize_result", "=", "minimize", "(", "partial", "(", "mean_distances", ",", "points", "=", "points", ",", "distance", "=", "distance", ")", ",", "centroid", ",", "method", "=", "'COBYLA'", ")", "\n", "", "return", "optimize_result", ".", "x", ",", "optimize_result", ".", "fun", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_functions.square_distance": [[6, 10], ["torch.sum", "torch.sum"], "function", ["None"], ["def", "square_distance", "(", "t1_emb", ",", "t2_emb", ")", ":", "\n", "    ", "D", "=", "t1_emb", "-", "t2_emb", "\n", "d", "=", "torch", ".", "sum", "(", "D", "*", "D", ",", "dim", "=", "-", "1", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_functions.euclidean_distance": [[12, 16], ["torch.norm", "torch.norm"], "function", ["None"], ["", "def", "euclidean_distance", "(", "t1_emb", ",", "t2_emb", ")", ":", "\n", "    ", "D", "=", "t1_emb", "-", "t2_emb", "\n", "d", "=", "torch", ".", "norm", "(", "D", ",", "dim", "=", "-", "1", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_functions.cosine_distance": [[18, 20], ["torch.functional.cosine_similarity"], "function", ["None"], ["", "def", "cosine_distance", "(", "t1_emb", ",", "t2_emb", ")", ":", "\n", "    ", "return", "1", "-", "nn", ".", "functional", ".", "cosine_similarity", "(", "t1_emb", ",", "t2_emb", ",", "dim", "=", "-", "1", ",", "eps", "=", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_functions.manhattan_distance": [[22, 26], ["torch.sum", "torch.sum", "torch.abs", "torch.abs"], "function", ["None"], ["", "def", "manhattan_distance", "(", "t1_emb", ",", "t2_emb", ")", ":", "\n", "    ", "D", "=", "t1_emb", "-", "t2_emb", "\n", "d", "=", "torch", ".", "sum", "(", "torch", ".", "abs", "(", "D", ")", ",", "dim", "=", "-", "1", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_functions.hyperbolic_distance": [[28, 35], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sqrt", "torch.sqrt", "torch.log", "torch.log"], "function", ["None"], ["", "def", "hyperbolic_distance", "(", "u", ",", "v", ",", "epsilon", "=", "1e-7", ")", ":", "# changed from epsilon=1e-7 to reduce error", "\n", "    ", "sqdist", "=", "torch", ".", "sum", "(", "(", "u", "-", "v", ")", "**", "2", ",", "dim", "=", "-", "1", ")", "\n", "squnorm", "=", "torch", ".", "sum", "(", "u", "**", "2", ",", "dim", "=", "-", "1", ")", "\n", "sqvnorm", "=", "torch", ".", "sum", "(", "v", "**", "2", ",", "dim", "=", "-", "1", ")", "\n", "x", "=", "1", "+", "2", "*", "sqdist", "/", "(", "(", "1", "-", "squnorm", ")", "*", "(", "1", "-", "sqvnorm", ")", ")", "+", "epsilon", "\n", "z", "=", "torch", ".", "sqrt", "(", "x", "**", "2", "-", "1", ")", "\n", "return", "torch", ".", "log", "(", "x", "+", "z", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.distance_functions.hyperbolic_distance_numpy": [[37, 44], ["numpy.sum", "numpy.sum", "numpy.sum", "numpy.sqrt", "numpy.log"], "function", ["None"], ["", "def", "hyperbolic_distance_numpy", "(", "u", ",", "v", ",", "epsilon", "=", "1e-9", ")", ":", "\n", "    ", "sqdist", "=", "np", ".", "sum", "(", "(", "u", "-", "v", ")", "**", "2", ",", "axis", "=", "-", "1", ")", "\n", "squnorm", "=", "np", ".", "sum", "(", "u", "**", "2", ",", "axis", "=", "-", "1", ")", "\n", "sqvnorm", "=", "np", ".", "sum", "(", "v", "**", "2", ",", "axis", "=", "-", "1", ")", "\n", "x", "=", "1", "+", "2", "*", "sqdist", "/", "(", "(", "1", "-", "squnorm", ")", "*", "(", "1", "-", "sqvnorm", ")", ")", "+", "epsilon", "\n", "z", "=", "np", ".", "sqrt", "(", "x", "**", "2", "-", "1", ")", "\n", "return", "np", ".", "log", "(", "x", "+", "z", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.test_distance_torch.TestDistanceTorchFunctions.random_vector_unit_ball": [[10, 15], ["torch.rand", "torch.norm", "norm.clamp_min"], "methods", ["None"], ["    ", "def", "random_vector_unit_ball", "(", "self", ",", "size", ",", "epsilon", "=", "1e-5", ")", ":", "\n", "        ", "vec", "=", "torch", ".", "rand", "(", "size", ")", "\n", "norm", "=", "torch", ".", "norm", "(", "vec", ")", "+", "epsilon", "\n", "vec", "=", "vec", "/", "norm", ".", "clamp_min", "(", "1", ")", "\n", "return", "vec", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.test_distance_torch.TestDistanceTorchFunctions.test_shape_random_points": [[16, 26], ["torch.manual_seed", "test_distance_torch.TestDistanceTorchFunctions.random_vector_unit_ball", "test_distance_torch.TestDistanceTorchFunctions.random_vector_unit_ball", "util.distance_functions.distance_functions.DISTANCE_TORCH.keys"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.test_distance_torch.TestDistanceTorchFunctions.random_vector_unit_ball", "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.test_distance_torch.TestDistanceTorchFunctions.random_vector_unit_ball"], ["", "def", "test_shape_random_points", "(", "self", ")", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "0", ")", "\n", "\n", "vec_1", "=", "self", ".", "random_vector_unit_ball", "(", "SIZE", ")", "\n", "vec_2", "=", "self", ".", "random_vector_unit_ball", "(", "SIZE", ")", "\n", "\n", "for", "d", "in", "DISTANCE_TORCH", ".", "keys", "(", ")", ":", "\n", "            ", "dist", "=", "DISTANCE_TORCH", "[", "d", "]", "(", "vec_1", ",", "vec_2", ")", "\n", "\n", "assert", "dist", ".", "shape", "==", "SIZE", "[", ":", "-", "1", "]", ",", "\"Wrong shape for \"", "+", "d", "+", "\" distance\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.test_distance_torch.TestDistanceTorchFunctions.test_identity_random_points": [[27, 37], ["torch.manual_seed", "test_distance_torch.TestDistanceTorchFunctions.random_vector_unit_ball", "util.distance_functions.distance_functions.DISTANCE_TORCH.keys", "torch.all", "torch.isclose", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.test_distance_torch.TestDistanceTorchFunctions.random_vector_unit_ball"], ["", "", "def", "test_identity_random_points", "(", "self", ")", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "1", ")", "\n", "\n", "vec_1", "=", "self", ".", "random_vector_unit_ball", "(", "SIZE", ")", "\n", "\n", "for", "d", "in", "DISTANCE_TORCH", ".", "keys", "(", ")", ":", "\n", "            ", "dist", "=", "DISTANCE_TORCH", "[", "d", "]", "(", "vec_1", ",", "vec_1", ")", "\n", "\n", "assert", "torch", ".", "all", "(", "torch", ".", "isclose", "(", "dist", ",", "torch", ".", "tensor", "(", "0.0", ")", ",", "atol", "=", "1e-6", ")", ")", ",", "d", "+", "' distance of vector with itself different from 0'", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.test_distance_torch.TestDistanceTorchFunctions.test_reflection_random_points": [[38, 50], ["torch.manual_seed", "test_distance_torch.TestDistanceTorchFunctions.random_vector_unit_ball", "test_distance_torch.TestDistanceTorchFunctions.random_vector_unit_ball", "util.distance_functions.distance_functions.DISTANCE_TORCH.keys", "torch.all", "torch.isclose"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.test_distance_torch.TestDistanceTorchFunctions.random_vector_unit_ball", "home.repos.pwc.inspect_result.gcorso_neuroseed.distance_functions.test_distance_torch.TestDistanceTorchFunctions.random_vector_unit_ball"], ["", "", "def", "test_reflection_random_points", "(", "self", ")", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "2", ")", "\n", "\n", "vec_1", "=", "self", ".", "random_vector_unit_ball", "(", "SIZE", ")", "\n", "vec_2", "=", "self", ".", "random_vector_unit_ball", "(", "SIZE", ")", "\n", "\n", "for", "d", "in", "DISTANCE_TORCH", ".", "keys", "(", ")", ":", "\n", "            ", "dist1", "=", "DISTANCE_TORCH", "[", "d", "]", "(", "vec_1", ",", "vec_2", ")", "\n", "dist2", "=", "DISTANCE_TORCH", "[", "d", "]", "(", "vec_2", ",", "vec_1", ")", "\n", "\n", "assert", "torch", ".", "all", "(", "torch", ".", "isclose", "(", "dist1", ",", "dist2", ",", "atol", "=", "1e-6", ")", ")", ",", "d", "+", "' distance of vectors is not symmetric (commutative)'", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.load_dataset": [[7, 10], ["open", "pickle.load"], "function", ["None"], ["def", "load_dataset", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "return", "pickle", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.get_dataloaders": [[12, 18], ["datasets.keys", "torch.utils.data.DataLoader"], "function", ["None"], ["", "", "def", "get_dataloaders", "(", "datasets", ",", "batch_size", ",", "workers", ")", ":", "\n", "    ", "loaders", "=", "{", "}", "\n", "for", "key", "in", "datasets", ".", "keys", "(", ")", ":", "\n", "        ", "loaders", "[", "key", "]", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "datasets", "[", "key", "]", ",", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "num_workers", "=", "workers", ")", "\n", "", "return", "loaders", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot": [[20, 24], ["torch.cat", "torch.eye", "torch.zeros"], "function", ["None"], ["", "def", "index_to_one_hot", "(", "x", ",", "alphabet_size", "=", "4", ",", "device", "=", "'cpu'", ")", ":", "\n", "# add one row of zeros because the -1 represents the absence of element and it is encoded with zeros", "\n", "    ", "x", "=", "torch", ".", "cat", "(", "(", "torch", ".", "eye", "(", "alphabet_size", ",", "device", "=", "device", ")", ",", "torch", ".", "zeros", "(", "(", "1", ",", "alphabet_size", ")", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "0", ")", "[", "x", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.dataset_to_one_hot": [[26, 29], ["dic.keys", "data_loader.index_to_one_hot"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot"], ["", "def", "dataset_to_one_hot", "(", "dic", ",", "alphabet_size", "=", "4", ")", ":", "\n", "    ", "for", "dset", "in", "dic", ".", "keys", "(", ")", ":", "\n", "        ", "dic", "[", "dset", "]", "=", "index_to_one_hot", "(", "dic", "[", "dset", "]", ",", "alphabet_size", "=", "alphabet_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.__init__": [[23, 26], ["random.seed"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "alphabet_size", "=", "4", ",", "seed", "=", "None", ")", ":", "\n", "        ", "if", "seed", "is", "not", "None", ":", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "alphabet_size", "=", "alphabet_size", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate": [[27, 30], ["random.randint", "range"], "methods", ["None"], ["", "def", "generate", "(", "self", ",", "length", ")", ":", "\n", "        ", "string", "=", "[", "random", ".", "randint", "(", "0", ",", "self", ".", "alphabet_size", "-", "1", ")", "for", "_", "in", "range", "(", "length", ")", "]", "\n", "return", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.string_to_list": [[16, 20], ["range", "len", "len"], "function", ["None"], ["def", "string_to_list", "(", "s", ",", "length", "=", "None", ",", "alphabet", "=", "DNA_ALPHABET", ")", ":", "\n", "    ", "L", "=", "[", "alphabet", "[", "s", "[", "i", "]", "]", "if", "i", "<", "len", "(", "s", ")", "else", "-", "1", "\n", "for", "i", "in", "range", "(", "length", "if", "length", "is", "not", "None", "else", "len", "(", "s", ")", ")", "]", "\n", "return", "L", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.k_mutations": [[32, 50], ["S.copy", "random.randint", "random.randint", "random.randint", "random.randint", "S.copy.pop", "random.randint", "S.copy.insert", "len", "len", "random.randint", "len"], "function", ["None"], ["", "", "def", "k_mutations", "(", "S", ",", "k", ",", "alphabet_size", "=", "4", ")", ":", "\n", "    ", "S1", "=", "S", ".", "copy", "(", ")", "# shallow-cloning", "\n", "while", "k", ">", "0", ":", "\n", "        ", "change", "=", "random", ".", "randint", "(", "0", ",", "1", ")", "\n", "\n", "if", "change", "==", "0", ":", "# substitution", "\n", "            ", "idx", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "S1", ")", "-", "1", ")", "\n", "S1", "[", "idx", "]", "=", "random", ".", "randint", "(", "0", ",", "alphabet_size", "-", "1", ")", "\n", "k", "-=", "1", "\n", "\n", "", "elif", "change", "==", "1", ":", "# remove and insert", "\n", "            ", "idx", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "S1", ")", "-", "1", ")", "\n", "S1", ".", "pop", "(", "idx", ")", "\n", "idx", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "S1", ")", ")", "\n", "S1", ".", "insert", "(", "idx", ",", "random", ".", "randint", "(", "0", ",", "alphabet_size", "-", "1", ")", ")", "\n", "k", "-=", "2", "\n", "\n", "", "", "return", "S1", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.inexact_matching.smith_waterman_edit_distance": [[4, 28], ["numpy.zeros", "range", "range", "numpy.min", "range", "range", "len", "len", "len", "start_indices.append", "len", "len", "len", "min", "min", "len"], "function", ["None"], ["def", "smith_waterman_edit_distance", "(", "pattern", ",", "text", ")", ":", "\n", "# in order to provide the start indices reverse the strings", "\n", "    ", "text", "=", "text", "[", ":", ":", "-", "1", "]", "\n", "pattern", "=", "pattern", "[", ":", ":", "-", "1", "]", "\n", "\n", "s", "=", "np", ".", "zeros", "(", "(", "len", "(", "text", ")", "+", "1", ",", "len", "(", "pattern", ")", "+", "1", ")", ")", "\n", "\n", "for", "j", "in", "range", "(", "1", ",", "len", "(", "pattern", ")", "+", "1", ")", ":", "\n", "        ", "s", "[", "0", ",", "j", "]", "=", "j", "\n", "\n", "", "for", "i", "in", "range", "(", "1", ",", "len", "(", "text", ")", "+", "1", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "1", ",", "len", "(", "pattern", ")", "+", "1", ")", ":", "\n", "            ", "s", "[", "i", ",", "j", "]", "=", "min", "(", "s", "[", "i", ",", "j", "-", "1", "]", ",", "s", "[", "i", "-", "1", ",", "j", "]", ",", "s", "[", "i", "-", "1", ",", "j", "-", "1", "]", ")", "+", "1", "\n", "if", "text", "[", "i", "-", "1", "]", "==", "pattern", "[", "j", "-", "1", "]", ":", "\n", "                ", "s", "[", "i", ",", "j", "]", "=", "min", "(", "s", "[", "i", ",", "j", "]", ",", "s", "[", "i", "-", "1", ",", "j", "-", "1", "]", ")", "\n", "\n", "", "", "", "min_cost", "=", "np", ".", "min", "(", "s", "[", ":", ",", "-", "1", "]", ")", "\n", "start_indices", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "text", ")", ")", ":", "\n", "        ", "if", "s", "[", "len", "(", "pattern", ")", "-", "i", ",", "-", "1", "]", "==", "min_cost", ":", "\n", "            ", "start_indices", ".", "append", "(", "i", ")", "\n", "\n", "", "", "return", "start_indices", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.dp_edit_distance": [[10, 25], ["numpy.zeros", "range", "range", "range", "range", "len", "len", "len", "len", "len", "len", "min", "min", "len", "len"], "function", ["None"], ["def", "dp_edit_distance", "(", "s1", ",", "s2", ")", ":", "\n", "# Not used because too slow, instead use Levenshtein.distance from a library written in C", "\n", "    ", "s", "=", "np", ".", "zeros", "(", "(", "len", "(", "s1", ")", "+", "1", ",", "len", "(", "s2", ")", "+", "1", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "s1", ")", "+", "1", ")", ":", "\n", "        ", "s", "[", "i", ",", "0", "]", "=", "i", "\n", "", "for", "j", "in", "range", "(", "1", ",", "len", "(", "s2", ")", "+", "1", ")", ":", "\n", "        ", "s", "[", "0", ",", "j", "]", "=", "j", "\n", "\n", "", "for", "i", "in", "range", "(", "1", ",", "len", "(", "s1", ")", "+", "1", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "1", ",", "len", "(", "s2", ")", "+", "1", ")", ":", "\n", "            ", "s", "[", "i", ",", "j", "]", "=", "min", "(", "s", "[", "i", ",", "j", "-", "1", "]", ",", "s", "[", "i", "-", "1", ",", "j", "]", ",", "s", "[", "i", "-", "1", ",", "j", "-", "1", "]", ")", "+", "1", "\n", "if", "s1", "[", "i", "-", "1", "]", "==", "s2", "[", "j", "-", "1", "]", ":", "\n", "                ", "s", "[", "i", ",", "j", "]", "=", "min", "(", "s", "[", "i", ",", "j", "]", ",", "s", "[", "i", "-", "1", ",", "j", "-", "1", "]", ")", "\n", "", "", "", "return", "s", "[", "len", "(", "s1", ")", ",", "len", "(", "s2", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_row": [[27, 30], ["distance"], "function", ["None"], ["", "def", "cross_distance_row", "(", "args", ",", "distance", "=", "Levenshtein", ".", "distance", ")", ":", "\n", "    ", "a", ",", "B", "=", "args", "\n", "return", "[", "distance", "(", "a", ",", "b", ")", "for", "b", "in", "B", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix_threads": [[32, 43], ["multiprocessing.Pool", "time.time", "list", "print", "numpy.array", "tqdm.tqdm", "pool.imap", "functools.partial", "zip", "len", "time.time", "len", "len"], "function", ["None"], ["", "def", "cross_distance_matrix_threads", "(", "A", ",", "B", ",", "n_thread", ",", "distance", "=", "Levenshtein", ".", "distance", ")", ":", "\n", "    ", "with", "Pool", "(", "n_thread", ")", "as", "pool", ":", "\n", "        ", "start", "=", "time", ".", "time", "(", ")", "\n", "distance_matrix", "=", "list", "(", "\n", "tqdm", "(", "\n", "pool", ".", "imap", "(", "partial", "(", "cross_distance_row", ",", "distance", "=", "distance", ")", ",", "zip", "(", "A", ",", "[", "B", "for", "_", "in", "A", "]", ")", ")", ",", "\n", "total", "=", "len", "(", "A", ")", ",", "\n", "desc", "=", "\"Edit distance {}x{}\"", ".", "format", "(", "len", "(", "A", ")", ",", "len", "(", "B", ")", ")", ",", "\n", ")", ")", "\n", "print", "(", "\"Time to compute the matrix: {}\"", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start", ")", ")", "\n", "return", "np", ".", "array", "(", "distance_matrix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix": [[45, 47], ["numpy.array", "numpy.array", "distance"], "function", ["None"], ["", "", "def", "cross_distance_matrix", "(", "A", ",", "B", ",", "distance", "=", "Levenshtein", ".", "distance", ")", ":", "\n", "    ", "return", "np", ".", "array", "(", "np", ".", "array", "(", "[", "[", "distance", "(", "a", ",", "b", ")", "for", "b", "in", "B", "]", "for", "a", "in", "A", "]", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.exact_matching.brute_force": [[1, 12], ["range", "range", "len", "matches.append", "len", "len"], "function", ["None"], ["def", "brute_force", "(", "pattern", ",", "text", ")", ":", "\n", "    ", "matches", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "text", ")", "-", "len", "(", "pattern", ")", "+", "1", ")", ":", "\n", "        ", "different", "=", "False", "\n", "for", "j", "in", "range", "(", "len", "(", "pattern", ")", ")", ":", "\n", "            ", "if", "pattern", "[", "j", "]", "!=", "text", "[", "i", "+", "j", "]", ":", "\n", "                ", "different", "=", "True", "\n", "break", "\n", "", "", "if", "not", "different", ":", "\n", "            ", "matches", ".", "append", "(", "i", ")", "\n", "", "", "return", "matches", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.exact_matching.z_preprocessing": [[14, 30], ["len", "range", "max", "min"], "function", ["None"], ["", "def", "z_preprocessing", "(", "S", ")", ":", "\n", "    ", "\"\"\"\n    Z algorithm for the fundamental preprocessing of a string:\n    Given a string S and a position i > 1, let Z_i(S) be the length of the longest\n    substring of S that starts at i and matches a prefix of S.\n    \"\"\"", "\n", "n", "=", "len", "(", "S", ")", "\n", "z", "=", "[", "0", "]", "*", "n", "\n", "x", ",", "y", "=", "0", ",", "0", "\n", "for", "i", "in", "range", "(", "1", ",", "n", ")", ":", "\n", "        ", "z", "[", "i", "]", "=", "max", "(", "0", ",", "min", "(", "z", "[", "i", "-", "x", "]", ",", "y", "-", "i", "+", "1", ")", ")", "\n", "while", "i", "+", "z", "[", "i", "]", "<", "n", "and", "S", "[", "z", "[", "i", "]", "]", "==", "S", "[", "i", "+", "z", "[", "i", "]", "]", ":", "\n", "            ", "x", "=", "i", "\n", "y", "=", "i", "+", "z", "[", "i", "]", "\n", "z", "[", "i", "]", "+=", "1", "\n", "", "", "return", "z", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.exact_matching.z_matching": [[32, 39], ["exact_matching.z_preprocessing", "range", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.exact_matching.z_preprocessing"], ["", "def", "z_matching", "(", "pattern", ",", "text", ")", ":", "\n", "    ", "assert", "not", "(", "'$'", "in", "pattern", "or", "'$'", "in", "text", ")", ",", "'$ is the special character reserved for separation'", "\n", "\n", "combined", "=", "pattern", "+", "[", "'$'", "]", "+", "text", "\n", "z", "=", "z_preprocessing", "(", "combined", ")", "\n", "matches", "=", "[", "idx", "-", "len", "(", "pattern", ")", "-", "1", "for", "idx", "in", "range", "(", "len", "(", "z", ")", ")", "if", "z", "[", "idx", "]", "==", "len", "(", "pattern", ")", "]", "\n", "return", "matches", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.exact_matching.boyer_moore_preprocessing": [[41, 51], ["range", "len"], "function", ["None"], ["", "def", "boyer_moore_preprocessing", "(", "pattern", ",", "alphabet_size", "=", "4", ")", ":", "\n", "    ", "\"\"\"\n    Bad character rule used by Boyer-Moore algorithm:\n    For each character x in the alphabet, let R(x) be the position of right-most occurrence of character x in P.\n    R(x) is defined to be zero if x does not occur in P.\n    \"\"\"", "\n", "R", "=", "[", "0", "]", "*", "alphabet_size", "\n", "for", "i", "in", "range", "(", "len", "(", "pattern", ")", ")", ":", "\n", "        ", "R", "[", "pattern", "[", "i", "]", "]", "=", "i", "\n", "", "return", "R", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.exact_matching.boyer_moore_matching": [[53, 69], ["exact_matching.boyer_moore_preprocessing", "range", "matches.append", "len", "len", "len", "max"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.exact_matching.boyer_moore_preprocessing"], ["", "def", "boyer_moore_matching", "(", "pattern", ",", "text", ")", ":", "\n", "    ", "R", "=", "boyer_moore_preprocessing", "(", "pattern", ")", "\n", "matches", "=", "[", "]", "\n", "k", "=", "0", "\n", "while", "k", "<", "len", "(", "text", ")", "-", "len", "(", "pattern", ")", "+", "1", ":", "\n", "        ", "match", "=", "True", "\n", "for", "i", "in", "range", "(", "len", "(", "pattern", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "if", "text", "[", "k", "+", "i", "]", "!=", "pattern", "[", "i", "]", ":", "\n", "                ", "k", "+=", "max", "(", "1", ",", "i", "-", "R", "[", "text", "[", "k", "+", "i", "]", "]", ")", "\n", "match", "=", "False", "\n", "break", "\n", "", "", "if", "match", ":", "\n", "            ", "matches", ".", "append", "(", "k", ")", "\n", "k", "+=", "1", "\n", "\n", "", "", "return", "matches", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.exact_matching.kmp_preprocessing": [[71, 94], ["len", "len"], "function", ["None"], ["", "def", "kmp_preprocessing", "(", "pattern", ")", ":", "\n", "    ", "\"\"\"\n    Knuth-Morris-Pratt algorithm shift preprocessing:\n    For each position i in pattern P, define T_i(P) to be the length of the longest\n    proper suffix of P[l..i] that matches a prefix of P.\n    \"\"\"", "\n", "T", "=", "[", "0", "]", "*", "(", "len", "(", "pattern", ")", "+", "1", ")", "\n", "pos", "=", "1", "\n", "cnd", "=", "0", "\n", "\n", "T", "[", "0", "]", "=", "-", "1", "\n", "while", "pos", "<", "len", "(", "pattern", ")", ":", "\n", "        ", "if", "pattern", "[", "pos", "]", "==", "pattern", "[", "cnd", "]", ":", "\n", "            ", "T", "[", "pos", "]", "=", "T", "[", "cnd", "]", "\n", "", "else", ":", "\n", "            ", "T", "[", "pos", "]", "=", "cnd", "\n", "cnd", "=", "T", "[", "cnd", "]", "\n", "while", "cnd", ">=", "0", "and", "pattern", "[", "pos", "]", "!=", "pattern", "[", "cnd", "]", ":", "\n", "                ", "cnd", "=", "T", "[", "cnd", "]", "\n", "", "", "pos", "+=", "1", "\n", "cnd", "+=", "1", "\n", "", "T", "[", "pos", "]", "=", "cnd", "\n", "return", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.exact_matching.kmp_matching": [[96, 116], ["exact_matching.kmp_preprocessing", "len", "len", "matches.append"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.exact_matching.kmp_preprocessing"], ["", "def", "kmp_matching", "(", "pattern", ",", "text", ")", ":", "\n", "    ", "T", "=", "kmp_preprocessing", "(", "pattern", ")", "\n", "\n", "matches", "=", "[", "]", "\n", "j", "=", "0", "\n", "k", "=", "0", "\n", "\n", "while", "j", "<", "len", "(", "text", ")", ":", "\n", "        ", "if", "pattern", "[", "k", "]", "==", "text", "[", "j", "]", ":", "\n", "            ", "j", "+=", "1", "\n", "k", "+=", "1", "\n", "if", "k", "==", "len", "(", "pattern", ")", ":", "\n", "                ", "matches", ".", "append", "(", "j", "-", "k", ")", "\n", "k", "=", "T", "[", "k", "]", "\n", "", "", "else", ":", "\n", "            ", "k", "=", "T", "[", "k", "]", "\n", "if", "k", "<", "0", ":", "\n", "                ", "j", "+=", "1", "\n", "k", "+=", "1", "\n", "", "", "", "return", "matches", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.test_edit_distance.TestEditDistanceConsensus.test_100_independent": [[13, 27], ["range", "util.data_handling.string_generator.IndependentGenerator", "util.data_handling.string_generator.IndependentGenerator.generate", "util.data_handling.string_generator.IndependentGenerator.generate", "util.bioinformatics_algorithms.edit_distance.dp_edit_distance", "Levenshtein.distance", "chr", "chr"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate", "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.dp_edit_distance"], ["    ", "def", "test_100_independent", "(", "self", ")", ":", "\n", "        ", "for", "seed", "in", "range", "(", "TEST_CASES", ")", ":", "\n", "            ", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "4", ",", "seed", "=", "seed", ")", "\n", "sequence1", "=", "generator", ".", "generate", "(", "length", "=", "100", ")", "\n", "sequence2", "=", "generator", ".", "generate", "(", "length", "=", "100", ")", "\n", "\n", "distance1", "=", "dp_edit_distance", "(", "sequence1", ",", "sequence2", ")", "\n", "\n", "string1", "=", "\"\"", ".", "join", "(", "chr", "(", "s", "+", "97", ")", "for", "s", "in", "sequence1", ")", "\n", "string2", "=", "\"\"", ".", "join", "(", "chr", "(", "s", "+", "97", ")", "for", "s", "in", "sequence2", ")", "\n", "distance2", "=", "Levenshtein", ".", "distance", "(", "string1", ",", "string2", ")", "\n", "\n", "assert", "distance1", "==", "distance2", ",", "'Mismatch between dp_edit_distance and Levenshtein.distance'", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.test_edit_distance.TestEditDistanceConsensus.test_random_len_independent": [[28, 44], ["range", "util.data_handling.string_generator.IndependentGenerator", "random.seed", "util.data_handling.string_generator.IndependentGenerator.generate", "util.data_handling.string_generator.IndependentGenerator.generate", "util.bioinformatics_algorithms.edit_distance.dp_edit_distance", "Levenshtein.distance", "random.randint", "random.randint", "chr", "chr"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate", "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.dp_edit_distance"], ["", "", "def", "test_random_len_independent", "(", "self", ")", ":", "\n", "        ", "for", "seed", "in", "range", "(", "TEST_CASES", ")", ":", "\n", "            ", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "4", ",", "seed", "=", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "sequence1", "=", "generator", ".", "generate", "(", "length", "=", "random", ".", "randint", "(", "10", ",", "100", ")", ")", "\n", "sequence2", "=", "generator", ".", "generate", "(", "length", "=", "random", ".", "randint", "(", "10", ",", "100", ")", ")", "\n", "\n", "distance1", "=", "dp_edit_distance", "(", "sequence1", ",", "sequence2", ")", "\n", "\n", "string1", "=", "\"\"", ".", "join", "(", "chr", "(", "s", "+", "97", ")", "for", "s", "in", "sequence1", ")", "\n", "string2", "=", "\"\"", ".", "join", "(", "chr", "(", "s", "+", "97", ")", "for", "s", "in", "sequence2", ")", "\n", "distance2", "=", "Levenshtein", ".", "distance", "(", "string1", ",", "string2", ")", "\n", "\n", "assert", "distance1", "==", "distance2", ",", "'Mismatch between dp_edit_distance and Levenshtein.distance'", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.test_edit_distance.TestEditDistanceConsensus.test_equal_independent": [[45, 58], ["range", "util.data_handling.string_generator.IndependentGenerator", "random.seed", "util.data_handling.string_generator.IndependentGenerator.generate", "util.bioinformatics_algorithms.edit_distance.dp_edit_distance", "Levenshtein.distance", "random.randint", "chr"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate", "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.dp_edit_distance"], ["", "", "def", "test_equal_independent", "(", "self", ")", ":", "\n", "        ", "for", "seed", "in", "range", "(", "TEST_CASES", ")", ":", "\n", "            ", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "4", ",", "seed", "=", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "sequence1", "=", "generator", ".", "generate", "(", "length", "=", "random", ".", "randint", "(", "10", ",", "100", ")", ")", "\n", "distance1", "=", "dp_edit_distance", "(", "sequence1", ",", "sequence1", ")", "\n", "\n", "string1", "=", "\"\"", ".", "join", "(", "chr", "(", "s", "+", "97", ")", "for", "s", "in", "sequence1", ")", "\n", "distance2", "=", "Levenshtein", ".", "distance", "(", "string1", ",", "string1", ")", "\n", "\n", "assert", "distance1", "==", "0", ",", "'dp_edit_distance on equal sequences different from 0'", "\n", "assert", "distance2", "==", "0", ",", "'Levenshtein.distance on equal sequences different from 0'", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.test_exact_matching.TestExactMatchingConsensus.test_10independent_5substr": [[17, 34], ["range", "util.data_handling.string_generator.IndependentGenerator", "util.data_handling.string_generator.IndependentGenerator.generate", "random.seed", "random.randint", "EXACT_MATCHING_ALGORITHMS.keys", "EXACT_MATCHING_ALGORITHMS.keys", "sorted", "str"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate"], ["    ", "def", "test_10independent_5substr", "(", "self", ")", ":", "\n", "        ", "for", "seed", "in", "range", "(", "TEST_CASES", ")", ":", "\n", "            ", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "4", ",", "seed", "=", "seed", ")", "\n", "sequence", "=", "generator", ".", "generate", "(", "length", "=", "10", ")", "\n", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "start", "=", "random", ".", "randint", "(", "0", ",", "4", ")", "\n", "pattern", "=", "sequence", "[", "start", ":", "start", "+", "5", "]", "\n", "\n", "matches", "=", "{", "}", "\n", "for", "algorithm", "in", "EXACT_MATCHING_ALGORITHMS", ".", "keys", "(", ")", ":", "\n", "                ", "matches", "[", "algorithm", "]", "=", "sorted", "(", "EXACT_MATCHING_ALGORITHMS", "[", "algorithm", "]", "(", "pattern", ",", "sequence", ")", ")", "\n", "\n", "", "for", "algorithm", "in", "EXACT_MATCHING_ALGORITHMS", ".", "keys", "(", ")", ":", "\n", "                ", "if", "algorithm", "!=", "'brute_force'", ":", "\n", "                    ", "assert", "matches", "[", "algorithm", "]", "==", "matches", "[", "'brute_force'", "]", ",", "'Mismatch between brute force and '", "+", "algorithm", "+", "\": \"", "+", "str", "(", "matches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.test_exact_matching.TestExactMatchingConsensus.test_100independent_10substr": [[35, 52], ["range", "util.data_handling.string_generator.IndependentGenerator", "util.data_handling.string_generator.IndependentGenerator.generate", "random.seed", "random.randint", "EXACT_MATCHING_ALGORITHMS.keys", "EXACT_MATCHING_ALGORITHMS.keys", "sorted", "str"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate"], ["", "", "", "", "def", "test_100independent_10substr", "(", "self", ")", ":", "\n", "        ", "for", "seed", "in", "range", "(", "TEST_CASES", ")", ":", "\n", "            ", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "4", ",", "seed", "=", "seed", ")", "\n", "sequence", "=", "generator", ".", "generate", "(", "length", "=", "100", ")", "\n", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "start", "=", "random", ".", "randint", "(", "0", ",", "89", ")", "\n", "pattern", "=", "sequence", "[", "start", ":", "start", "+", "10", "]", "\n", "\n", "matches", "=", "{", "}", "\n", "for", "algorithm", "in", "EXACT_MATCHING_ALGORITHMS", ".", "keys", "(", ")", ":", "\n", "                ", "matches", "[", "algorithm", "]", "=", "sorted", "(", "EXACT_MATCHING_ALGORITHMS", "[", "algorithm", "]", "(", "pattern", ",", "sequence", ")", ")", "\n", "\n", "", "for", "algorithm", "in", "EXACT_MATCHING_ALGORITHMS", ".", "keys", "(", ")", ":", "\n", "                ", "if", "algorithm", "!=", "'brute_force'", ":", "\n", "                    ", "assert", "matches", "[", "algorithm", "]", "==", "matches", "[", "'brute_force'", "]", ",", "'Mismatch between brute force and '", "+", "algorithm", "+", "\": \"", "+", "str", "(", "matches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.test_exact_matching.TestExactMatchingConsensus.test_100independent_5independent": [[53, 68], ["range", "util.data_handling.string_generator.IndependentGenerator", "util.data_handling.string_generator.IndependentGenerator.generate", "util.data_handling.string_generator.IndependentGenerator.generate", "random.seed", "EXACT_MATCHING_ALGORITHMS.keys", "EXACT_MATCHING_ALGORITHMS.keys", "sorted", "str"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.string_generator.IndependentGenerator.generate"], ["", "", "", "", "def", "test_100independent_5independent", "(", "self", ")", ":", "\n", "        ", "for", "seed", "in", "range", "(", "TEST_CASES", ")", ":", "\n", "            ", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "4", ",", "seed", "=", "seed", ")", "\n", "sequence", "=", "generator", ".", "generate", "(", "length", "=", "100", ")", "\n", "pattern", "=", "generator", ".", "generate", "(", "length", "=", "5", ")", "\n", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "matches", "=", "{", "}", "\n", "for", "algorithm", "in", "EXACT_MATCHING_ALGORITHMS", ".", "keys", "(", ")", ":", "\n", "                ", "matches", "[", "algorithm", "]", "=", "sorted", "(", "EXACT_MATCHING_ALGORITHMS", "[", "algorithm", "]", "(", "pattern", ",", "sequence", ")", ")", "\n", "\n", "", "for", "algorithm", "in", "EXACT_MATCHING_ALGORITHMS", ".", "keys", "(", ")", ":", "\n", "                ", "if", "algorithm", "!=", "'brute_force'", ":", "\n", "                    ", "assert", "matches", "[", "algorithm", "]", "==", "matches", "[", "'brute_force'", "]", ",", "'Mismatch between brute force and '", "+", "algorithm", "+", "\": \"", "+", "str", "(", "matches", ")", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_steiner_dataset_generation.TestMSADatasetGenerationGenomic.__init__": [[10, 21], ["unittest.TestCase.__init__", "random.seed", "multiple_alignment.steiner_string.task.dataset_generator_genome.MSAPairDatasetGeneratorGenome", "tests.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "tests.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "random.randint", "range"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna"], ["    ", "def", "__init__", "(", "self", ",", "methodName", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "methodName", ")", "\n", "random", ".", "seed", "(", "0", ")", "\n", "strings", "=", "[", "generate_random_dna", "(", "50", ")", "]", "+", "[", "generate_random_dna", "(", "random", ".", "randint", "(", "10", ",", "50", ")", ")", "for", "_", "in", "range", "(", "39", ")", "]", "\n", "sequences", "=", "{", "\n", "'train'", ":", "strings", "[", ":", "10", "]", ",", "\n", "'val'", ":", "strings", "[", "10", ":", "15", "]", ",", "\n", "'val_msa'", ":", "[", "strings", "[", "15", ":", "20", "]", ",", "strings", "[", "20", ":", "25", "]", "]", ",", "\n", "'test'", ":", "[", "strings", "[", "25", ":", "30", "]", ",", "strings", "[", "30", ":", "35", "]", ",", "strings", "[", "35", ":", "]", "]", "\n", "}", "\n", "self", ".", "dataset", "=", "MSAPairDatasetGeneratorGenome", "(", "strings", "=", "sequences", ",", "length", "=", "50", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_steiner_dataset_generation.TestMSADatasetGenerationGenomic.test_sequences_shapes": [[22, 27], ["None"], "methods", ["None"], ["", "def", "test_sequences_shapes", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "dataset", ".", "datasets", "[", "'train'", "]", "[", "'texts'", "]", ".", "shape", "==", "(", "10", ",", "50", ")", ",", "\"Sequences train shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "datasets", "[", "'val'", "]", "[", "'texts'", "]", ".", "shape", "==", "(", "5", ",", "50", ")", ",", "\"Sequences val shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "datasets", "[", "'val_msa'", "]", "[", "'texts'", "]", ".", "shape", "==", "(", "2", ",", "5", ",", "50", ")", ",", "\"Sequences val msa shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "datasets", "[", "'test'", "]", "[", "'texts'", "]", ".", "shape", "==", "(", "3", ",", "5", ",", "50", ")", ",", "\"Sequences test shape is not correct\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_steiner_dataset_generation.TestMSADatasetGenerationGenomic.test_distances_shapes": [[28, 31], ["None"], "methods", ["None"], ["", "def", "test_distances_shapes", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "dataset", ".", "datasets", "[", "'train'", "]", "[", "'distances'", "]", ".", "shape", "==", "(", "10", ",", "10", ")", ",", "\"Distances train shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "datasets", "[", "'val'", "]", "[", "'distances'", "]", ".", "shape", "==", "(", "5", ",", "5", ")", ",", "\"Distances val shape is not correct\"", "", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_guide_testing.TestMSAGuideTree.test_tree_output": [[51, 69], ["test_msa_guide_testing.generate_dataset_and_parser", "copy.copy", "edit_distance.train.execute_train", "os.path.exists", "test_msa_guide_testing.remove_files", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files"], ["    ", "def", "test_tree_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "edit_dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'cosine'", "\n", "\n", "# run method storing output", "\n", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "\n", "# check output tree", "\n", "assert", "path", ".", "exists", "(", "'njtree.dnd'", ")", ",", "\"Tree file missing\"", "\n", "\n", "# remove files", "\n", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_guide_testing.TestMSAGuideTree.test_sequence_output": [[70, 88], ["test_msa_guide_testing.generate_dataset_and_parser", "copy.copy", "edit_distance.train.execute_train", "os.path.exists", "test_msa_guide_testing.remove_files", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files"], ["", "def", "test_sequence_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "edit_dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'cosine'", "\n", "\n", "# run method storing output", "\n", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "\n", "# check output tree", "\n", "assert", "path", ".", "exists", "(", "'sequences.fasta'", ")", ",", "\"Sequences file missing\"", "\n", "\n", "# remove files", "\n", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_guide_testing.generate_dataset_and_parser": [[21, 37], ["util.data_handling.string_generator.IndependentGenerator", "edit_distance.task.dataset_generator_genomic.EditDistanceGenomicDatasetGenerator", "edit_distance.task.dataset_generator_genomic.EditDistanceGenomicDatasetGenerator.save_as_pickle", "edit_distance.train.general_arg_parser", "edit_distance.train.general_arg_parser.parse_args", "random.choice", "tests.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "tests.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "range", "random.randint", "range"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.EditDistanceDatasetGenerator.save_as_pickle", "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.general_arg_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna"], ["def", "generate_dataset_and_parser", "(", ")", ":", "\n", "    ", "folder_name", "=", "''", ".", "join", "(", "random", ".", "choice", "(", "string", ".", "ascii_lowercase", ")", "for", "_", "in", "range", "(", "10", ")", ")", "\n", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "ALPHABET_SIZE", ",", "seed", "=", "0", ")", "\n", "edit_dataset_name", "=", "folder_name", "+", "'/test_ed.pkl'", "\n", "strings", "=", "[", "generate_random_dna", "(", "50", ")", "]", "+", "[", "generate_random_dna", "(", "random", ".", "randint", "(", "10", ",", "50", ")", ")", "for", "_", "in", "range", "(", "19", ")", "]", "\n", "strings_dict", "=", "{", "'train'", ":", "strings", "[", ":", "10", "]", ",", "'val'", ":", "strings", "[", "10", ":", "15", "]", ",", "'test'", ":", "strings", "[", "15", ":", "]", "}", "\n", "edit_dataset", "=", "EditDistanceGenomicDatasetGenerator", "(", "strings", "=", "strings_dict", ")", "\n", "edit_dataset", ".", "save_as_pickle", "(", "edit_dataset_name", ")", "\n", "\n", "parser", "=", "general_arg_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "data", "=", "edit_dataset_name", "\n", "args", ".", "epochs", "=", "2", "\n", "args", ".", "print_every", "=", "1", "\n", "args", ".", "construct_msa_tree", "=", "'True'", "\n", "return", "folder_name", ",", "edit_dataset_name", ",", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_guide_testing.remove_files": [[39, 47], ["os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "os.remove", "os.rmdir", "os.remove", "os.remove", "os.remove", "os.remove", "os.remove"], "function", ["None"], ["", "def", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ")", ":", "\n", "    ", "if", "path", ".", "exists", "(", "'MLPEncoder.pkl'", ")", ":", "os", ".", "remove", "(", "'MLPEncoder.pkl'", ")", "\n", "if", "path", ".", "exists", "(", "'0.pkl'", ")", ":", "os", ".", "remove", "(", "'0.pkl'", ")", "\n", "if", "path", ".", "exists", "(", "'1.pkl'", ")", ":", "os", ".", "remove", "(", "'1.pkl'", ")", "\n", "if", "path", ".", "exists", "(", "'njtree.dnd'", ")", ":", "os", ".", "remove", "(", "'njtree.dnd'", ")", "\n", "if", "path", ".", "exists", "(", "'sequences.fasta'", ")", ":", "os", ".", "remove", "(", "'sequences.fasta'", ")", "\n", "os", ".", "remove", "(", "edit_dataset_name", ")", "\n", "os", ".", "rmdir", "(", "folder_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_steiner_training.TestMSASteinerTraining.test_mlp_model_output": [[54, 71], ["test_msa_steiner_training.generate_dataset_and_parser", "io.StringIO", "io.StringIO.getvalue", "test_msa_steiner_training.remove_files", "contextlib.redirect_stdout", "multiple_alignment.steiner_string.train.execute_train", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["    ", "def", "test_mlp_model_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "encoder_class", "=", "MLPEncoder", ",", "decoder_class", "=", "MLPDecoder", ",", "\n", "encoder_args", "=", "dict", "(", "hidden_size", "=", "5", ",", "\n", "layers", "=", "2", ")", ",", "\n", "decoder_args", "=", "dict", "(", "hidden_size", "=", "5", ",", "\n", "layers", "=", "2", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "\"Cost multiple\"", "in", "out", "and", "\"Final multiple val\"", "in", "out", "and", "\"Final loss_val\"", "in", "out", "and", "\"Final loss_train\"", "in", "out", ",", "'Wrong output format'", "\n", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_steiner_training.TestMSASteinerTraining.test_cnn_model_output": [[72, 95], ["test_msa_steiner_training.generate_dataset_and_parser", "io.StringIO", "io.StringIO.getvalue", "test_msa_steiner_training.remove_files", "contextlib.redirect_stdout", "multiple_alignment.steiner_string.train.execute_train", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_cnn_model_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "encoder_class", "=", "CNNEncoder", ",", "decoder_class", "=", "CNNDecoder", ",", "\n", "encoder_args", "=", "dict", "(", "readout_layers", "=", "1", ",", "\n", "channels", "=", "4", ",", "\n", "layers", "=", "2", ",", "\n", "kernel_size", "=", "3", ",", "\n", "non_linearity", "=", "True", ")", ",", "\n", "decoder_args", "=", "dict", "(", "readout_layers", "=", "1", ",", "\n", "channels", "=", "4", ",", "\n", "layers", "=", "2", ",", "\n", "kernel_size", "=", "3", ",", "\n", "non_linearity", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "\"Cost multiple\"", "in", "out", "and", "\"Final multiple val\"", "in", "out", "and", "\"Final loss_val\"", "in", "out", "and", "\"Final loss_train\"", "in", "out", ",", "'Wrong output format'", "\n", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_steiner_training.TestMSASteinerTraining.test_gru_model_output": [[96, 114], ["test_msa_steiner_training.generate_dataset_and_parser", "io.StringIO", "io.StringIO.getvalue", "test_msa_steiner_training.remove_files", "contextlib.redirect_stdout", "multiple_alignment.steiner_string.train.execute_train", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_gru_model_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "encoder_class", "=", "GRUEncoder", ",", "decoder_class", "=", "GRUDecoder", ",", "\n", "encoder_args", "=", "dict", "(", "recurrent_layers", "=", "1", ",", "\n", "hidden_size", "=", "5", ")", ",", "\n", "decoder_args", "=", "dict", "(", "recurrent_layers", "=", "1", ",", "\n", "hidden_size", "=", "5", ",", "\n", "reverse", "=", "'True'", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "\"Cost multiple\"", "in", "out", "and", "\"Final multiple val\"", "in", "out", "and", "\"Final loss_val\"", "in", "out", "and", "\"Final loss_train\"", "in", "out", ",", "'Wrong output format'", "\n", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_steiner_training.TestMSASteinerTraining.test_square_distance_output": [[115, 135], ["test_msa_steiner_training.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_msa_steiner_training.remove_files", "contextlib.redirect_stdout", "multiple_alignment.steiner_string.train.execute_train", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_square_distance_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'square'", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "encoder_class", "=", "MLPEncoder", ",", "decoder_class", "=", "MLPDecoder", ",", "\n", "encoder_args", "=", "dict", "(", "hidden_size", "=", "5", ",", "\n", "layers", "=", "2", ")", ",", "\n", "decoder_args", "=", "dict", "(", "hidden_size", "=", "5", ",", "\n", "layers", "=", "2", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "\"Cost multiple\"", "in", "out", "and", "\"Final multiple val\"", "in", "out", "and", "\"Final loss_val\"", "in", "out", "and", "\"Final loss_train\"", "in", "out", ",", "'Wrong output format'", "\n", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_steiner_training.TestMSASteinerTraining.test_cosine_distance_output": [[136, 156], ["test_msa_steiner_training.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_msa_steiner_training.remove_files", "contextlib.redirect_stdout", "multiple_alignment.steiner_string.train.execute_train", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_cosine_distance_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'cosine'", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "encoder_class", "=", "MLPEncoder", ",", "decoder_class", "=", "MLPDecoder", ",", "\n", "encoder_args", "=", "dict", "(", "hidden_size", "=", "5", ",", "\n", "layers", "=", "2", ")", ",", "\n", "decoder_args", "=", "dict", "(", "hidden_size", "=", "5", ",", "\n", "layers", "=", "2", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "\"Cost multiple\"", "in", "out", "and", "\"Final multiple val\"", "in", "out", "and", "\"Final loss_val\"", "in", "out", "and", "\"Final loss_train\"", "in", "out", ",", "'Wrong output format'", "\n", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_steiner_training.TestMSASteinerTraining.test_manhattan_distance_output": [[157, 177], ["test_msa_steiner_training.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_msa_steiner_training.remove_files", "contextlib.redirect_stdout", "multiple_alignment.steiner_string.train.execute_train", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_manhattan_distance_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'manhattan'", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "encoder_class", "=", "MLPEncoder", ",", "decoder_class", "=", "MLPDecoder", ",", "\n", "encoder_args", "=", "dict", "(", "hidden_size", "=", "5", ",", "\n", "layers", "=", "2", ")", ",", "\n", "decoder_args", "=", "dict", "(", "hidden_size", "=", "5", ",", "\n", "layers", "=", "2", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "\"Cost multiple\"", "in", "out", "and", "\"Final multiple val\"", "in", "out", "and", "\"Final loss_val\"", "in", "out", "and", "\"Final loss_train\"", "in", "out", ",", "'Wrong output format'", "\n", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_steiner_training.TestMSASteinerTraining.test_hyperbolic_distance_output": [[178, 199], ["test_msa_steiner_training.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_msa_steiner_training.remove_files", "contextlib.redirect_stdout", "multiple_alignment.steiner_string.train.execute_train", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_hyperbolic_distance_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'hyperbolic'", "\n", "args", ".", "scaling", "=", "True", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "encoder_class", "=", "MLPEncoder", ",", "decoder_class", "=", "MLPDecoder", ",", "\n", "encoder_args", "=", "dict", "(", "hidden_size", "=", "5", ",", "\n", "layers", "=", "2", ")", ",", "\n", "decoder_args", "=", "dict", "(", "hidden_size", "=", "5", ",", "\n", "layers", "=", "2", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "\"Cost multiple\"", "in", "out", "and", "\"Final multiple val\"", "in", "out", "and", "\"Final loss_val\"", "in", "out", "and", "\"Final loss_train\"", "in", "out", ",", "'Wrong output format'", "\n", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_steiner_training.generate_dataset_and_parser": [[23, 43], ["multiple_alignment.steiner_string.task.dataset_generator_genome.MSAPairDatasetGeneratorGenome", "multiple_alignment.steiner_string.task.dataset_generator_genome.MSAPairDatasetGeneratorGenome.save_as_pickle", "multiple_alignment.steiner_string.parser.general_arg_parser", "multiple_alignment.steiner_string.parser.general_arg_parser.parse_args", "random.choice", "tests.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "tests.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "range", "random.randint", "range"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.EditDistanceDatasetGenerator.save_as_pickle", "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.general_arg_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna"], ["def", "generate_dataset_and_parser", "(", ")", ":", "\n", "    ", "folder_name", "=", "''", ".", "join", "(", "random", ".", "choice", "(", "string", ".", "ascii_lowercase", ")", "for", "_", "in", "range", "(", "10", ")", ")", "\n", "dataset_name", "=", "folder_name", "+", "'/test_msa_model.pkl'", "\n", "strings", "=", "[", "generate_random_dna", "(", "50", ")", "]", "+", "[", "generate_random_dna", "(", "random", ".", "randint", "(", "10", ",", "50", ")", ")", "for", "_", "in", "range", "(", "39", ")", "]", "\n", "sequences", "=", "{", "\n", "'train'", ":", "strings", "[", ":", "10", "]", ",", "\n", "'val'", ":", "strings", "[", "10", ":", "15", "]", ",", "\n", "'val_msa'", ":", "[", "strings", "[", "15", ":", "20", "]", ",", "strings", "[", "20", ":", "25", "]", "]", ",", "\n", "'test'", ":", "[", "strings", "[", "25", ":", "30", "]", ",", "strings", "[", "30", ":", "35", "]", ",", "strings", "[", "35", ":", "]", "]", "\n", "}", "\n", "dataset", "=", "MSAPairDatasetGeneratorGenome", "(", "strings", "=", "sequences", ",", "length", "=", "50", ")", "\n", "dataset", ".", "save_as_pickle", "(", "dataset_name", ")", "\n", "\n", "parser", "=", "general_arg_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "data", "=", "dataset_name", "\n", "args", ".", "epochs", "=", "2", "\n", "args", ".", "print_every", "=", "1", "\n", "args", ".", "distance", "=", "\"euclidean\"", "\n", "return", "folder_name", ",", "dataset_name", ",", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.multiple_sequence_alignment_tests.test_msa_steiner_training.remove_files": [[45, 50], ["os.path.exists", "os.path.exists", "os.remove", "os.rmdir", "os.remove", "os.remove"], "function", ["None"], ["", "def", "remove_files", "(", "folder_name", ",", "dataset_name", ")", ":", "\n", "    ", "if", "path", ".", "exists", "(", "'0.pkl'", ")", ":", "os", ".", "remove", "(", "'0.pkl'", ")", "\n", "if", "path", ".", "exists", "(", "'1.pkl'", ")", ":", "os", ".", "remove", "(", "'1.pkl'", ")", "\n", "os", ".", "remove", "(", "dataset_name", ")", "\n", "os", ".", "rmdir", "(", "folder_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_dataset_generation_synthetic.TestHCDatasetGenerationSynthetic.__init__": [[15, 21], ["unittest.TestCase.__init__", "util.data_handling.string_generator.IndependentGenerator", "hierarchical_clustering.task.dataset_generator_synthetic.HierarchicalClusteringDatasetGenerator"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["    ", "def", "__init__", "(", "self", ",", "methodName", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "methodName", ")", "\n", "self", ".", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "ALPHABET_SIZE", ",", "seed", "=", "0", ")", "\n", "self", ".", "dataset", "=", "HierarchicalClusteringDatasetGenerator", "(", "N_reference", "=", "10", ",", "N_leaves", "=", "15", ",", "len_sequence", "=", "20", ",", "\n", "min_changes", "=", "3", ",", "max_changes", "=", "10", ",", "\n", "string_generator", "=", "self", ".", "generator", ",", "seed", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_dataset_generation_synthetic.TestHCDatasetGenerationSynthetic.test_shapes": [[22, 25], ["None"], "methods", ["None"], ["", "def", "test_shapes", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "dataset", ".", "sequences_leaves", ".", "shape", "==", "(", "15", ",", "20", ")", ",", "\"Sequences shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "similarities", ".", "shape", "==", "(", "15", ",", "15", ")", ",", "\"Similarities shape is not correct\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_dataset_generation_synthetic.TestHCDatasetGenerationSynthetic.test_range_labels": [[26, 29], ["numpy.all", "numpy.all"], "methods", ["None"], ["", "def", "test_range_labels", "(", "self", ")", ":", "\n", "        ", "assert", "np", ".", "all", "(", "self", ".", "dataset", ".", "similarities", "<=", "1", ")", ",", "\"Similarities out of size references\"", "\n", "assert", "np", ".", "all", "(", "self", ".", "dataset", ".", "similarities", ">=", "0", ")", ",", "\"Negative similarities\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_dataset_generation_synthetic.TestHCDatasetGenerationSynthetic.test_range_elements": [[30, 32], ["numpy.all"], "methods", ["None"], ["", "def", "test_range_elements", "(", "self", ")", ":", "\n", "        ", "assert", "np", ".", "all", "(", "self", ".", "dataset", ".", "sequences_leaves", "<", "ALPHABET_SIZE", ")", ",", "\"Sequences elements out of size\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_dataset_generation_genomic.TestHCDatasetGenerationSynthetic.__init__": [[15, 20], ["unittest.TestCase.__init__", "random.seed", "hierarchical_clustering.task.dataset_generator_genomic.HierarchicalClusteringGenomicDatasetGenerator", "test_hc_dataset_generation_genomic.generate_random_dna", "test_hc_dataset_generation_genomic.generate_random_dna", "random.randint", "range"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna"], ["    ", "def", "__init__", "(", "self", ",", "methodName", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "methodName", ")", "\n", "random", ".", "seed", "(", "0", ")", "\n", "strings", "=", "[", "generate_random_dna", "(", "50", ")", "]", "+", "[", "generate_random_dna", "(", "random", ".", "randint", "(", "10", ",", "50", ")", ")", "for", "_", "in", "range", "(", "19", ")", "]", "\n", "self", ".", "dataset", "=", "HierarchicalClusteringGenomicDatasetGenerator", "(", "strings", "=", "strings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_dataset_generation_genomic.TestHCDatasetGenerationSynthetic.test_shapes": [[21, 24], ["None"], "methods", ["None"], ["", "def", "test_shapes", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "dataset", ".", "sequences_leaves", ".", "shape", "==", "(", "20", ",", "50", ")", ",", "\"Sequences shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "similarities", ".", "shape", "==", "(", "20", ",", "20", ")", ",", "\"Similarities shape is not correct\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_dataset_generation_genomic.TestHCDatasetGenerationSynthetic.test_range_labels": [[25, 28], ["numpy.all", "numpy.all"], "methods", ["None"], ["", "def", "test_range_labels", "(", "self", ")", ":", "\n", "        ", "assert", "np", ".", "all", "(", "self", ".", "dataset", ".", "similarities", "<=", "1", ")", ",", "\"Similarities out of size references\"", "\n", "assert", "np", ".", "all", "(", "self", ".", "dataset", ".", "similarities", ">=", "0", ")", ",", "\"Negative similarities\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_dataset_generation_genomic.generate_random_dna": [[9, 11], ["random.choice", "range"], "function", ["None"], ["def", "generate_random_dna", "(", "length", ")", ":", "\n", "    ", "return", "''", ".", "join", "(", "random", ".", "choice", "(", "[", "'A'", ",", "'C'", ",", "'G'", ",", "'T'", "]", ")", "for", "_", "in", "range", "(", "length", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_unsupervised_testing.TestHCDatasetGenerationSynthetic.test_cosine_distance_stdout": [[58, 80], ["test_hc_unsupervised_testing.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_hc_unsupervised_testing.remove_files", "contextlib.redirect_stdout", "edit_distance.train.execute_train", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["    ", "def", "test_cosine_distance_stdout", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "edit_dataset_name", ",", "hc_dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'cosine'", "\n", "\n", "# run method storing output", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "\"'single': {'DC'\"", "in", "out", "and", "\"'complete': {'DC'\"", "in", "out", "and", "\"'average': {'DC'\"", "in", "out", ",", "'Wrong output format for cosine distance'", "\n", "\n", "# remove files", "\n", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ",", "hc_dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_unsupervised_testing.TestHCDatasetGenerationSynthetic.test_euclidean_distance_stdout": [[81, 103], ["test_hc_unsupervised_testing.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_hc_unsupervised_testing.remove_files", "contextlib.redirect_stdout", "edit_distance.train.execute_train", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_euclidean_distance_stdout", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "edit_dataset_name", ",", "hc_dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'euclidean'", "\n", "\n", "# run method storing output", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "\"'single': {'DC'\"", "in", "out", "and", "\"'complete': {'DC'\"", "in", "out", "and", "\"'average': {'DC'\"", "in", "out", ",", "'Wrong output format for euclidean distance'", "\n", "\n", "# remove files", "\n", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ",", "hc_dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_unsupervised_testing.TestHCDatasetGenerationSynthetic.test_square_distance_stdout": [[104, 126], ["test_hc_unsupervised_testing.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_hc_unsupervised_testing.remove_files", "contextlib.redirect_stdout", "edit_distance.train.execute_train", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_square_distance_stdout", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "edit_dataset_name", ",", "hc_dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'square'", "\n", "\n", "# run method storing output", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "\"'single': {'DC'\"", "in", "out", "and", "\"'complete': {'DC'\"", "in", "out", "and", "\"'average': {'DC'\"", "in", "out", ",", "'Wrong output format for square distance'", "\n", "\n", "# remove files", "\n", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ",", "hc_dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_unsupervised_testing.TestHCDatasetGenerationSynthetic.test_manhattan_distance_stdout": [[127, 149], ["test_hc_unsupervised_testing.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_hc_unsupervised_testing.remove_files", "contextlib.redirect_stdout", "edit_distance.train.execute_train", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_manhattan_distance_stdout", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "edit_dataset_name", ",", "hc_dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'manhattan'", "\n", "\n", "# run method storing output", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "\"'single': {'DC'\"", "in", "out", "and", "\"'complete': {'DC'\"", "in", "out", "and", "\"'average': {'DC'\"", "in", "out", ",", "'Wrong output format for manhattan distance'", "\n", "\n", "# remove files", "\n", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ",", "hc_dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_unsupervised_testing.TestHCDatasetGenerationSynthetic.test_hyperbolic_distance_stdout": [[150, 172], ["test_hc_unsupervised_testing.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_hc_unsupervised_testing.remove_files", "contextlib.redirect_stdout", "edit_distance.train.execute_train", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_hyperbolic_distance_stdout", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "edit_dataset_name", ",", "hc_dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'hyperbolic'", "\n", "\n", "# run method storing output", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "\"'single': {'DC'\"", "in", "out", "and", "\"'complete': {'DC'\"", "in", "out", "and", "\"'average': {'DC'\"", "in", "out", ",", "'Wrong output format for hyperbolic distance'", "\n", "\n", "# remove files", "\n", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ",", "hc_dataset_name", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_unsupervised_testing.generate_dataset_and_parser": [[20, 45], ["util.data_handling.string_generator.IndependentGenerator", "edit_distance.task.dataset_generator_synthetic.EditDistanceDatasetGenerator", "edit_distance.task.dataset_generator_synthetic.EditDistanceDatasetGenerator.save_as_pickle", "hierarchical_clustering.task.dataset_generator_synthetic.HierarchicalClusteringDatasetGenerator", "hierarchical_clustering.task.dataset_generator_synthetic.HierarchicalClusteringDatasetGenerator.save_as_pickle", "edit_distance.train.general_arg_parser", "edit_distance.train.general_arg_parser.parse_args", "random.choice", "range"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.EditDistanceDatasetGenerator.save_as_pickle", "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.EditDistanceDatasetGenerator.save_as_pickle", "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.general_arg_parser"], ["def", "generate_dataset_and_parser", "(", ")", ":", "\n", "    ", "folder_name", "=", "''", ".", "join", "(", "random", ".", "choice", "(", "string", ".", "ascii_lowercase", ")", "for", "_", "in", "range", "(", "10", ")", ")", "\n", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "ALPHABET_SIZE", ",", "seed", "=", "0", ")", "\n", "edit_dataset_name", "=", "folder_name", "+", "'/test_ed.pkl'", "\n", "edit_dataset", "=", "EditDistanceDatasetGenerator", "(", "\n", "N_batches", "=", "{", "\"train\"", ":", "2", ",", "\"val\"", ":", "2", ",", "\"test\"", ":", "2", "}", ",", "\n", "batch_size", "=", "{", "\"train\"", ":", "5", ",", "\"val\"", ":", "3", ",", "\"test\"", ":", "3", "}", ",", "\n", "len_sequence", "=", "{", "\"train\"", ":", "10", ",", "\"val\"", ":", "10", ",", "\"test\"", ":", "10", "}", ",", "\n", "max_changes", "=", "{", "\"train\"", ":", "2", ",", "\"val\"", ":", "2", ",", "\"test\"", ":", "2", "}", ",", "\n", "string_generator", "=", "generator", ",", "seed", "=", "0", ")", "\n", "edit_dataset", ".", "save_as_pickle", "(", "edit_dataset_name", ")", "\n", "\n", "hc_dataset_name", "=", "folder_name", "+", "'/test_hc.pkl'", "\n", "hc_dataset", "=", "HierarchicalClusteringDatasetGenerator", "(", "N_reference", "=", "3", ",", "N_leaves", "=", "4", ",", "len_sequence", "=", "10", ",", "\n", "min_changes", "=", "2", ",", "max_changes", "=", "4", ",", "\n", "string_generator", "=", "generator", ",", "seed", "=", "0", ")", "\n", "hc_dataset", ".", "save_as_pickle", "(", "hc_dataset_name", ")", "\n", "\n", "parser", "=", "general_arg_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "data", "=", "edit_dataset_name", "\n", "args", ".", "epochs", "=", "2", "\n", "args", ".", "print_every", "=", "1", "\n", "args", ".", "hierarchical_data_path", "=", "hc_dataset_name", "\n", "return", "folder_name", ",", "edit_dataset_name", ",", "hc_dataset_name", ",", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.hierarchical_clustering_tests.test_hc_unsupervised_testing.remove_files": [[47, 54], ["os.path.exists", "os.path.exists", "os.path.exists", "os.remove", "os.remove", "os.rmdir", "os.remove", "os.remove", "os.remove"], "function", ["None"], ["", "def", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ",", "hc_dataset_name", ")", ":", "\n", "    ", "if", "path", ".", "exists", "(", "'MLPEncoder.pkl'", ")", ":", "os", ".", "remove", "(", "'MLPEncoder.pkl'", ")", "\n", "if", "path", ".", "exists", "(", "'0.pkl'", ")", ":", "os", ".", "remove", "(", "'0.pkl'", ")", "\n", "if", "path", ".", "exists", "(", "'1.pkl'", ")", ":", "os", ".", "remove", "(", "'1.pkl'", ")", "\n", "os", ".", "remove", "(", "edit_dataset_name", ")", "\n", "os", ".", "remove", "(", "hc_dataset_name", ")", "\n", "os", ".", "rmdir", "(", "folder_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_dataset_generation_genomic.TestClosestDatasetGenerationSynthetic.__init__": [[14, 20], ["unittest.TestCase.__init__", "random.seed", "closest_string.task.dataset_generator_genomic.ClosestStringGenomicDatasetGenerator", "test_closest_dataset_generation_genomic.generate_random_dna", "test_closest_dataset_generation_genomic.generate_random_dna", "test_closest_dataset_generation_genomic.generate_random_dna", "random.randint", "range", "random.randint", "range"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna"], ["    ", "def", "__init__", "(", "self", ",", "methodName", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "methodName", ")", "\n", "random", ".", "seed", "(", "0", ")", "\n", "references", "=", "[", "generate_random_dna", "(", "50", ")", "]", "+", "[", "generate_random_dna", "(", "random", ".", "randint", "(", "10", ",", "50", ")", ")", "for", "_", "in", "range", "(", "9", ")", "]", "\n", "queries", "=", "[", "generate_random_dna", "(", "random", ".", "randint", "(", "10", ",", "50", ")", ")", "for", "_", "in", "range", "(", "25", ")", "]", "\n", "self", ".", "dataset", "=", "ClosestStringGenomicDatasetGenerator", "(", "strings_reference", "=", "references", ",", "strings_query", "=", "queries", ",", "n_queries", "=", "15", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_dataset_generation_genomic.TestClosestDatasetGenerationSynthetic.test_shapes": [[21, 25], ["None"], "methods", ["None"], ["", "def", "test_shapes", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "dataset", ".", "sequences_references", ".", "shape", "==", "(", "10", ",", "50", ")", ",", "\"Sequences references shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "sequences_queries", ".", "shape", "==", "(", "15", ",", "50", ")", ",", "\"Sequences queries shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "labels", ".", "shape", "==", "(", "15", ",", ")", ",", "\"Labels shape is not correct\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_dataset_generation_genomic.TestClosestDatasetGenerationSynthetic.test_range_labels": [[26, 29], ["torch.all", "torch.all"], "methods", ["None"], ["", "def", "test_range_labels", "(", "self", ")", ":", "\n", "        ", "assert", "torch", ".", "all", "(", "self", ".", "dataset", ".", "labels", "<", "10", ")", ",", "\"Labels out of size references\"", "\n", "assert", "torch", ".", "all", "(", "self", ".", "dataset", ".", "labels", ">=", "0", ")", ",", "\"Negative labels\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_dataset_generation_genomic.generate_random_dna": [[8, 10], ["random.choice", "range"], "function", ["None"], ["def", "generate_random_dna", "(", "length", ")", ":", "\n", "    ", "return", "''", ".", "join", "(", "random", ".", "choice", "(", "[", "'A'", ",", "'C'", ",", "'G'", ",", "'T'", "]", ")", "for", "_", "in", "range", "(", "length", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_testing.TestClosestDatasetGenerationSynthetic.test_cosine_distance_stdout": [[60, 81], ["test_closest_testing.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_closest_testing.remove_files", "contextlib.redirect_stdout", "edit_distance.train.execute_train", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["    ", "def", "test_cosine_distance_stdout", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "edit_dataset_name", ",", "closest_dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'cosine'", "\n", "\n", "# run method storing output", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "'Top1:'", "in", "out", "and", "'Top5:'", "in", "out", "and", "'Top10:'", "in", "out", ",", "'Wrong output format for cosine distance'", "\n", "\n", "# remove files", "\n", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ",", "closest_dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_testing.TestClosestDatasetGenerationSynthetic.test_euclidean_distance_stdout": [[82, 103], ["test_closest_testing.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_closest_testing.remove_files", "contextlib.redirect_stdout", "edit_distance.train.execute_train", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_euclidean_distance_stdout", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "edit_dataset_name", ",", "closest_dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'euclidean'", "\n", "\n", "# run method storing output", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "'Top1:'", "in", "out", "and", "'Top5:'", "in", "out", "and", "'Top10:'", "in", "out", ",", "'Wrong output format for euclidean distance'", "\n", "\n", "# remove files", "\n", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ",", "closest_dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_testing.TestClosestDatasetGenerationSynthetic.test_square_distance_stdout": [[105, 126], ["test_closest_testing.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_closest_testing.remove_files", "contextlib.redirect_stdout", "edit_distance.train.execute_train", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_square_distance_stdout", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "edit_dataset_name", ",", "closest_dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'square'", "\n", "\n", "# run method storing output", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "'Top1:'", "in", "out", "and", "'Top5:'", "in", "out", "and", "'Top10:'", "in", "out", ",", "'Wrong output format for square distance'", "\n", "\n", "# remove files", "\n", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ",", "closest_dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_testing.TestClosestDatasetGenerationSynthetic.test_manhattan_distance_stdout": [[127, 148], ["test_closest_testing.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_closest_testing.remove_files", "contextlib.redirect_stdout", "edit_distance.train.execute_train", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_manhattan_distance_stdout", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "edit_dataset_name", ",", "closest_dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'manhattan'", "\n", "\n", "# run method storing output", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "'Top1:'", "in", "out", "and", "'Top5:'", "in", "out", "and", "'Top10:'", "in", "out", ",", "'Wrong output format for manhattan distance'", "\n", "\n", "# remove files", "\n", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ",", "closest_dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_testing.TestClosestDatasetGenerationSynthetic.test_hyperbolic_distance_stdout": [[149, 170], ["test_closest_testing.generate_dataset_and_parser", "copy.copy", "io.StringIO", "io.StringIO.getvalue", "test_closest_testing.remove_files", "contextlib.redirect_stdout", "edit_distance.train.execute_train", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train"], ["", "def", "test_hyperbolic_distance_stdout", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "edit_dataset_name", ",", "closest_dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'hyperbolic'", "\n", "\n", "# run method storing output", "\n", "f", "=", "io", ".", "StringIO", "(", ")", "\n", "with", "redirect_stdout", "(", "f", ")", ":", "\n", "            ", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "", "out", "=", "f", ".", "getvalue", "(", ")", "\n", "\n", "# check correct output", "\n", "assert", "'Top1:'", "in", "out", "and", "'Top5:'", "in", "out", "and", "'Top10:'", "in", "out", ",", "'Wrong output format for hyperbolic distance'", "\n", "\n", "# remove files", "\n", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ",", "closest_dataset_name", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_testing.generate_dataset_and_parser": [[23, 47], ["util.data_handling.string_generator.IndependentGenerator", "edit_distance.task.dataset_generator_synthetic.EditDistanceDatasetGenerator", "edit_distance.task.dataset_generator_synthetic.EditDistanceDatasetGenerator.save_as_pickle", "closest_string.task.dataset_generator_synthetic.ClosestStringDatasetGenerator", "closest_string.task.dataset_generator_synthetic.ClosestStringDatasetGenerator.save_as_pickle", "edit_distance.train.general_arg_parser", "edit_distance.train.general_arg_parser.parse_args", "random.choice", "range"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.EditDistanceDatasetGenerator.save_as_pickle", "home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.EditDistanceDatasetGenerator.save_as_pickle", "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.general_arg_parser"], ["def", "generate_dataset_and_parser", "(", ")", ":", "\n", "    ", "folder_name", "=", "''", ".", "join", "(", "random", ".", "choice", "(", "string", ".", "ascii_lowercase", ")", "for", "_", "in", "range", "(", "10", ")", ")", "\n", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "ALPHABET_SIZE", ",", "seed", "=", "0", ")", "\n", "edit_dataset_name", "=", "folder_name", "+", "'/test_ed.pkl'", "\n", "edit_dataset", "=", "EditDistanceDatasetGenerator", "(", "\n", "N_batches", "=", "{", "\"train\"", ":", "2", ",", "\"val\"", ":", "2", ",", "\"test\"", ":", "2", "}", ",", "\n", "batch_size", "=", "{", "\"train\"", ":", "5", ",", "\"val\"", ":", "3", ",", "\"test\"", ":", "3", "}", ",", "\n", "len_sequence", "=", "{", "\"train\"", ":", "10", ",", "\"val\"", ":", "10", ",", "\"test\"", ":", "10", "}", ",", "\n", "max_changes", "=", "{", "\"train\"", ":", "2", ",", "\"val\"", ":", "2", ",", "\"test\"", ":", "2", "}", ",", "\n", "string_generator", "=", "generator", ",", "seed", "=", "0", ")", "\n", "edit_dataset", ".", "save_as_pickle", "(", "edit_dataset_name", ")", "\n", "\n", "closest_dataset_name", "=", "folder_name", "+", "'/test_closest.pkl'", "\n", "closest_dataset", "=", "ClosestStringDatasetGenerator", "(", "N_reference", "=", "3", ",", "N_query", "=", "4", ",", "len_sequence", "=", "10", ",", "min_changes", "=", "2", ",", "\n", "max_changes", "=", "4", ",", "initials", "=", "3", ",", "string_generator", "=", "generator", ",", "seed", "=", "0", ")", "\n", "closest_dataset", ".", "save_as_pickle", "(", "closest_dataset_name", ")", "\n", "\n", "parser", "=", "general_arg_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "data", "=", "edit_dataset_name", "\n", "args", ".", "epochs", "=", "2", "\n", "args", ".", "print_every", "=", "1", "\n", "args", ".", "closest_data_path", "=", "closest_dataset_name", "\n", "return", "folder_name", ",", "edit_dataset_name", ",", "closest_dataset_name", ",", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_testing.remove_files": [[49, 56], ["os.path.exists", "os.path.exists", "os.path.exists", "os.remove", "os.remove", "os.rmdir", "os.remove", "os.remove", "os.remove"], "function", ["None"], ["", "def", "remove_files", "(", "folder_name", ",", "edit_dataset_name", ",", "closest_dataset_name", ")", ":", "\n", "    ", "if", "path", ".", "exists", "(", "'MLPEncoder.pkl'", ")", ":", "os", ".", "remove", "(", "'MLPEncoder.pkl'", ")", "\n", "if", "path", ".", "exists", "(", "'0.pkl'", ")", ":", "os", ".", "remove", "(", "'0.pkl'", ")", "\n", "if", "path", ".", "exists", "(", "'1.pkl'", ")", ":", "os", ".", "remove", "(", "'1.pkl'", ")", "\n", "os", ".", "remove", "(", "edit_dataset_name", ")", "\n", "os", ".", "remove", "(", "closest_dataset_name", ")", "\n", "os", ".", "rmdir", "(", "folder_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_dataset_generation_synthetic.TestClosestDatasetGenerationSynthetic.__init__": [[13, 18], ["unittest.TestCase.__init__", "util.data_handling.string_generator.IndependentGenerator", "closest_string.task.dataset_generator_synthetic.ClosestStringDatasetGenerator"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["    ", "def", "__init__", "(", "self", ",", "methodName", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "methodName", ")", "\n", "self", ".", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "ALPHABET_SIZE", ",", "seed", "=", "0", ")", "\n", "self", ".", "dataset", "=", "ClosestStringDatasetGenerator", "(", "N_reference", "=", "10", ",", "N_query", "=", "15", ",", "len_sequence", "=", "20", ",", "min_changes", "=", "3", ",", "\n", "max_changes", "=", "10", ",", "initials", "=", "3", ",", "string_generator", "=", "self", ".", "generator", ",", "seed", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_dataset_generation_synthetic.TestClosestDatasetGenerationSynthetic.test_shapes": [[19, 23], ["None"], "methods", ["None"], ["", "def", "test_shapes", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "dataset", ".", "sequences_references", ".", "shape", "==", "(", "10", ",", "20", ")", ",", "\"Sequences references shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "sequences_queries", ".", "shape", "==", "(", "15", ",", "20", ")", ",", "\"Sequences queries shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "labels", ".", "shape", "==", "(", "15", ",", ")", ",", "\"Labels shape is not correct\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_dataset_generation_synthetic.TestClosestDatasetGenerationSynthetic.test_range_labels": [[24, 27], ["torch.all", "torch.all"], "methods", ["None"], ["", "def", "test_range_labels", "(", "self", ")", ":", "\n", "        ", "assert", "torch", ".", "all", "(", "self", ".", "dataset", ".", "labels", "<", "10", ")", ",", "\"Labels out of size references\"", "\n", "assert", "torch", ".", "all", "(", "self", ".", "dataset", ".", "labels", ">=", "0", ")", ",", "\"Negative labels\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string_tests.test_closest_dataset_generation_synthetic.TestClosestDatasetGenerationSynthetic.test_range_elements": [[28, 31], ["torch.all", "torch.all"], "methods", ["None"], ["", "def", "test_range_elements", "(", "self", ")", ":", "\n", "        ", "assert", "torch", ".", "all", "(", "self", ".", "dataset", ".", "sequences_references", "<", "ALPHABET_SIZE", ")", ",", "\"Sequences references elements out of size\"", "\n", "assert", "torch", ".", "all", "(", "self", ".", "dataset", ".", "sequences_queries", "<", "ALPHABET_SIZE", ")", ",", "\"Sequences queries elements out of size\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.TestEDDatasetGenerationSynthetic.__init__": [[13, 19], ["unittest.TestCase.__init__", "random.seed", "edit_distance.task.dataset_generator_genomic.EditDistanceGenomicDatasetGenerator", "test_ed_dataset_generation_genomic.generate_random_dna", "test_ed_dataset_generation_genomic.generate_random_dna", "random.randint", "range"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna"], ["    ", "def", "__init__", "(", "self", ",", "methodName", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "methodName", ")", "\n", "random", ".", "seed", "(", "0", ")", "\n", "strings", "=", "[", "generate_random_dna", "(", "50", ")", "]", "+", "[", "generate_random_dna", "(", "random", ".", "randint", "(", "10", ",", "50", ")", ")", "for", "_", "in", "range", "(", "39", ")", "]", "\n", "strings_dict", "=", "{", "'train'", ":", "strings", "[", ":", "20", "]", ",", "'val'", ":", "strings", "[", "20", ":", "30", "]", ",", "'test'", ":", "strings", "[", "30", ":", "]", "}", "\n", "self", ".", "dataset", "=", "EditDistanceGenomicDatasetGenerator", "(", "strings", "=", "strings_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.TestEDDatasetGenerationSynthetic.test_shape_sequences": [[20, 24], ["None"], "methods", ["None"], ["", "def", "test_shape_sequences", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "dataset", ".", "sequences", "[", "'train'", "]", ".", "shape", "==", "(", "20", ",", "50", ")", ",", "\"Sequences train shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "sequences", "[", "'val'", "]", ".", "shape", "==", "(", "10", ",", "50", ")", ",", "\"Sequences val shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "sequences", "[", "'test'", "]", ".", "shape", "==", "(", "10", ",", "50", ")", ",", "\"Sequences test shape is not correct\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.TestEDDatasetGenerationSynthetic.test_shape_distances": [[25, 29], ["None"], "methods", ["None"], ["", "def", "test_shape_distances", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "dataset", ".", "distances", "[", "'train'", "]", ".", "shape", "==", "(", "20", ",", "20", ")", ",", "\"Distances train shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "distances", "[", "'val'", "]", ".", "shape", "==", "(", "10", ",", "10", ")", ",", "\"Distances val shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "distances", "[", "'test'", "]", ".", "shape", "==", "(", "10", ",", "10", ")", ",", "\"Distances test shape is not correct\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.TestEDDatasetGenerationSynthetic.test_range_elements": [[30, 34], ["torch.all", "torch.all", "torch.all"], "methods", ["None"], ["", "def", "test_range_elements", "(", "self", ")", ":", "\n", "        ", "assert", "torch", ".", "all", "(", "self", ".", "dataset", ".", "sequences", "[", "'train'", "]", "<", "4", ")", ",", "\"Sequences train elements out of size\"", "\n", "assert", "torch", ".", "all", "(", "self", ".", "dataset", ".", "sequences", "[", "'val'", "]", "<", "4", ")", ",", "\"Sequences val elements out of size\"", "\n", "assert", "torch", ".", "all", "(", "self", ".", "dataset", ".", "sequences", "[", "'test'", "]", "<", "4", ")", ",", "\"Sequences test elements out of size\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_genomic.generate_random_dna": [[7, 9], ["random.choice", "range"], "function", ["None"], ["def", "generate_random_dna", "(", "length", ")", ":", "\n", "    ", "return", "''", ".", "join", "(", "random", ".", "choice", "(", "[", "'A'", ",", "'C'", ",", "'G'", ",", "'T'", "]", ")", "for", "_", "in", "range", "(", "length", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_synthetic.TestEDDatasetGenerationSynthetic.__init__": [[12, 21], ["unittest.TestCase.__init__", "util.data_handling.string_generator.IndependentGenerator", "edit_distance.task.dataset_generator_synthetic.EditDistanceDatasetGenerator"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["    ", "def", "__init__", "(", "self", ",", "methodName", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "methodName", ")", "\n", "self", ".", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "ALPHABET_SIZE", ",", "seed", "=", "0", ")", "\n", "self", ".", "dataset", "=", "EditDistanceDatasetGenerator", "(", "\n", "N_batches", "=", "{", "\"train\"", ":", "4", ",", "\"val\"", ":", "2", ",", "\"test\"", ":", "3", "}", ",", "\n", "batch_size", "=", "{", "\"train\"", ":", "5", ",", "\"val\"", ":", "3", ",", "\"test\"", ":", "4", "}", ",", "\n", "len_sequence", "=", "{", "\"train\"", ":", "10", ",", "\"val\"", ":", "10", ",", "\"test\"", ":", "10", "}", ",", "\n", "max_changes", "=", "{", "\"train\"", ":", "4", ",", "\"val\"", ":", "4", ",", "\"test\"", ":", "4", "}", ",", "\n", "string_generator", "=", "self", ".", "generator", ",", "seed", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_synthetic.TestEDDatasetGenerationSynthetic.test_shape_sequences": [[22, 26], ["None"], "methods", ["None"], ["", "def", "test_shape_sequences", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "dataset", ".", "sequences", "[", "'train'", "]", ".", "shape", "==", "(", "4", ",", "5", ",", "10", ")", ",", "\"Sequences train shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "sequences", "[", "'val'", "]", ".", "shape", "==", "(", "2", ",", "3", ",", "10", ")", ",", "\"Sequences val shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "sequences", "[", "'test'", "]", ".", "shape", "==", "(", "3", ",", "4", ",", "10", ")", ",", "\"Sequences test shape is not correct\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_synthetic.TestEDDatasetGenerationSynthetic.test_shape_distances": [[27, 31], ["None"], "methods", ["None"], ["", "def", "test_shape_distances", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "dataset", ".", "distances", "[", "'train'", "]", ".", "shape", "==", "(", "4", ",", "5", ",", "5", ")", ",", "\"Distances train shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "distances", "[", "'val'", "]", ".", "shape", "==", "(", "2", ",", "3", ",", "3", ")", ",", "\"Distances val shape is not correct\"", "\n", "assert", "self", ".", "dataset", ".", "distances", "[", "'test'", "]", ".", "shape", "==", "(", "3", ",", "4", ",", "4", ")", ",", "\"Distances test shape is not correct\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_dataset_generation_synthetic.TestEDDatasetGenerationSynthetic.test_range_elements": [[32, 36], ["torch.all", "torch.all", "torch.all"], "methods", ["None"], ["", "def", "test_range_elements", "(", "self", ")", ":", "\n", "        ", "assert", "torch", ".", "all", "(", "self", ".", "dataset", ".", "sequences", "[", "'train'", "]", "<", "ALPHABET_SIZE", ")", ",", "\"Sequences train elements out of size\"", "\n", "assert", "torch", ".", "all", "(", "self", ".", "dataset", ".", "sequences", "[", "'val'", "]", "<", "ALPHABET_SIZE", ")", ",", "\"Sequences val elements out of size\"", "\n", "assert", "torch", ".", "all", "(", "self", ".", "dataset", ".", "sequences", "[", "'test'", "]", "<", "ALPHABET_SIZE", ")", ",", "\"Sequences test elements out of size\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.TestEDTraining.test_mlp_model_output": [[51, 64], ["test_ed_training.generate_dataset_and_parser", "edit_distance.train.execute_train", "os.path.exists", "os.remove", "test_ed_training.remove_files", "os.path.exists", "os.path.exists", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files"], ["    ", "def", "test_mlp_model_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "\n", "assert", "path", ".", "exists", "(", "'MLPEncoder.pkl'", ")", "\n", "assert", "path", ".", "exists", "(", "'0.pkl'", ")", "or", "path", ".", "exists", "(", "'1.pkl'", ")", "\n", "\n", "os", ".", "remove", "(", "'MLPEncoder.pkl'", ")", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.TestEDTraining.test_cnn_model_output": [[65, 83], ["test_ed_training.generate_dataset_and_parser", "edit_distance.train.execute_train", "os.path.exists", "os.remove", "test_ed_training.remove_files", "os.path.exists", "os.path.exists", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files"], ["", "def", "test_cnn_model_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "execute_train", "(", "model_class", "=", "CNN", ",", "\n", "model_args", "=", "dict", "(", "readout_layers", "=", "1", ",", "\n", "channels", "=", "4", ",", "\n", "layers", "=", "2", ",", "\n", "kernel_size", "=", "3", ",", "\n", "pooling", "=", "'avg'", ",", "\n", "non_linearity", "=", "True", ",", "\n", "batch_norm", "=", "True", ",", "\n", "stride", "=", "1", ")", ",", "\n", "args", "=", "args", ")", "\n", "\n", "assert", "path", ".", "exists", "(", "'CNN.pkl'", ")", "\n", "assert", "path", ".", "exists", "(", "'0.pkl'", ")", "or", "path", ".", "exists", "(", "'1.pkl'", ")", "\n", "\n", "os", ".", "remove", "(", "'CNN.pkl'", ")", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.TestEDTraining.test_gru_model_output": [[84, 97], ["test_ed_training.generate_dataset_and_parser", "edit_distance.train.execute_train", "os.path.exists", "os.remove", "test_ed_training.remove_files", "os.path.exists", "os.path.exists", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files"], ["", "def", "test_gru_model_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "execute_train", "(", "model_class", "=", "GRU", ",", "\n", "model_args", "=", "dict", "(", "recurrent_layers", "=", "1", ",", "\n", "readout_layers", "=", "1", ",", "\n", "hidden_size", "=", "5", ")", ",", "\n", "args", "=", "args", ")", "\n", "\n", "assert", "path", ".", "exists", "(", "'GRU.pkl'", ")", "\n", "assert", "path", ".", "exists", "(", "'0.pkl'", ")", "or", "path", ".", "exists", "(", "'1.pkl'", ")", "\n", "\n", "os", ".", "remove", "(", "'GRU.pkl'", ")", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.TestEDTraining.test_transformer_model_output": [[98, 116], ["test_ed_training.generate_dataset_and_parser", "edit_distance.train.execute_train", "os.path.exists", "os.remove", "test_ed_training.remove_files", "os.path.exists", "os.path.exists", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files"], ["", "def", "test_transformer_model_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "execute_train", "(", "model_class", "=", "Transformer", ",", "\n", "model_args", "=", "dict", "(", "segment_size", "=", "2", ",", "\n", "trans_layers", "=", "1", ",", "\n", "readout_layers", "=", "1", ",", "\n", "hidden_size", "=", "4", ",", "\n", "mask", "=", "'empty'", ",", "\n", "heads", "=", "2", ",", "\n", "layer_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "\n", "assert", "path", ".", "exists", "(", "'Transformer.pkl'", ")", "\n", "assert", "path", ".", "exists", "(", "'0.pkl'", ")", "or", "path", ".", "exists", "(", "'1.pkl'", ")", "\n", "\n", "os", ".", "remove", "(", "'Transformer.pkl'", ")", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.TestEDTraining.test_square_distance_output": [[117, 133], ["test_ed_training.generate_dataset_and_parser", "copy.copy", "edit_distance.train.execute_train", "os.path.exists", "os.remove", "test_ed_training.remove_files", "os.path.exists", "os.path.exists", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files"], ["", "def", "test_square_distance_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'square'", "\n", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "\n", "assert", "path", ".", "exists", "(", "'MLPEncoder.pkl'", ")", "\n", "assert", "path", ".", "exists", "(", "'0.pkl'", ")", "or", "path", ".", "exists", "(", "'1.pkl'", ")", "\n", "\n", "os", ".", "remove", "(", "'MLPEncoder.pkl'", ")", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.TestEDTraining.test_cosine_distance_output": [[134, 150], ["test_ed_training.generate_dataset_and_parser", "copy.copy", "edit_distance.train.execute_train", "os.path.exists", "os.remove", "test_ed_training.remove_files", "os.path.exists", "os.path.exists", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files"], ["", "def", "test_cosine_distance_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'cosine'", "\n", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "\n", "assert", "path", ".", "exists", "(", "'MLPEncoder.pkl'", ")", "\n", "assert", "path", ".", "exists", "(", "'0.pkl'", ")", "or", "path", ".", "exists", "(", "'1.pkl'", ")", "\n", "\n", "os", ".", "remove", "(", "'MLPEncoder.pkl'", ")", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.TestEDTraining.test_manhattan_distance_output": [[151, 167], ["test_ed_training.generate_dataset_and_parser", "copy.copy", "edit_distance.train.execute_train", "os.path.exists", "os.remove", "test_ed_training.remove_files", "os.path.exists", "os.path.exists", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files"], ["", "def", "test_manhattan_distance_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'manhattan'", "\n", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "\n", "assert", "path", ".", "exists", "(", "'MLPEncoder.pkl'", ")", "\n", "assert", "path", ".", "exists", "(", "'0.pkl'", ")", "or", "path", ".", "exists", "(", "'1.pkl'", ")", "\n", "\n", "os", ".", "remove", "(", "'MLPEncoder.pkl'", ")", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.TestEDTraining.test_hyperbolic_distance_output": [[168, 185], ["test_ed_training.generate_dataset_and_parser", "copy.copy", "edit_distance.train.execute_train", "os.path.exists", "os.remove", "test_ed_training.remove_files", "os.path.exists", "os.path.exists", "dict"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files"], ["", "def", "test_hyperbolic_distance_output", "(", "self", ")", ":", "\n", "        ", "folder_name", ",", "dataset_name", ",", "args", "=", "generate_dataset_and_parser", "(", ")", "\n", "\n", "args", "=", "copy", ".", "copy", "(", "args", ")", "\n", "args", ".", "distance", "=", "'hyperbolic'", "\n", "args", ".", "scaling", "=", "True", "\n", "execute_train", "(", "model_class", "=", "MLPEncoder", ",", "\n", "model_args", "=", "dict", "(", "layers", "=", "2", ",", "\n", "hidden_size", "=", "5", ",", "\n", "batch_norm", "=", "True", ")", ",", "\n", "args", "=", "args", ")", "\n", "\n", "assert", "path", ".", "exists", "(", "'MLPEncoder.pkl'", ")", "\n", "assert", "path", ".", "exists", "(", "'0.pkl'", ")", "or", "path", ".", "exists", "(", "'1.pkl'", ")", "\n", "\n", "os", ".", "remove", "(", "'MLPEncoder.pkl'", ")", "\n", "remove_files", "(", "folder_name", ",", "dataset_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.generate_dataset_and_parser": [[21, 40], ["util.data_handling.string_generator.IndependentGenerator", "edit_distance.task.dataset_generator_synthetic.EditDistanceDatasetGenerator", "edit_distance.task.dataset_generator_synthetic.EditDistanceDatasetGenerator.save_as_pickle", "edit_distance.train.general_arg_parser", "edit_distance.train.general_arg_parser.parse_args", "random.choice", "range"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.task.dataset_generator_synthetic.EditDistanceDatasetGenerator.save_as_pickle", "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.general_arg_parser"], ["def", "generate_dataset_and_parser", "(", ")", ":", "\n", "    ", "folder_name", "=", "''", ".", "join", "(", "random", ".", "choice", "(", "string", ".", "ascii_lowercase", ")", "for", "_", "in", "range", "(", "10", ")", ")", "\n", "generator", "=", "IndependentGenerator", "(", "alphabet_size", "=", "ALPHABET_SIZE", ",", "seed", "=", "0", ")", "\n", "dataset_name", "=", "folder_name", "+", "'/test_ed_model.pkl'", "\n", "dataset", "=", "EditDistanceDatasetGenerator", "(", "\n", "N_batches", "=", "{", "\"train\"", ":", "2", ",", "\"val\"", ":", "2", ",", "\"test\"", ":", "2", "}", ",", "\n", "batch_size", "=", "{", "\"train\"", ":", "5", ",", "\"val\"", ":", "3", ",", "\"test\"", ":", "3", "}", ",", "\n", "len_sequence", "=", "{", "\"train\"", ":", "10", ",", "\"val\"", ":", "10", ",", "\"test\"", ":", "10", "}", ",", "\n", "max_changes", "=", "{", "\"train\"", ":", "2", ",", "\"val\"", ":", "2", ",", "\"test\"", ":", "2", "}", ",", "\n", "string_generator", "=", "generator", ",", "seed", "=", "0", ")", "\n", "dataset", ".", "save_as_pickle", "(", "dataset_name", ")", "\n", "\n", "parser", "=", "general_arg_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "data", "=", "dataset_name", "\n", "args", ".", "epochs", "=", "2", "\n", "args", ".", "print_every", "=", "1", "\n", "args", ".", "distance", "=", "\"euclidean\"", "\n", "return", "folder_name", ",", "dataset_name", ",", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance_tests.test_ed_training.remove_files": [[42, 47], ["os.path.exists", "os.path.exists", "os.remove", "os.rmdir", "os.remove", "os.remove"], "function", ["None"], ["", "def", "remove_files", "(", "folder_name", ",", "dataset_name", ")", ":", "\n", "    ", "if", "path", ".", "exists", "(", "'0.pkl'", ")", ":", "os", ".", "remove", "(", "'0.pkl'", ")", "\n", "if", "path", ".", "exists", "(", "'1.pkl'", ")", ":", "os", ".", "remove", "(", "'1.pkl'", ")", "\n", "os", ".", "remove", "(", "dataset_name", ")", "\n", "os", ".", "rmdir", "(", "folder_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.baselines.multiple_alignment_cost_baseline": [[11, 40], ["util.ml_and_math.loss_functions.AverageMeter", "util.ml_and_math.loss_functions.AverageMeter", "util.ml_and_math.loss_functions.AverageMeter", "util.ml_and_math.loss_functions.AverageMeter", "range", "multiple_alignment.steiner_string.models.loss.torch_to_string", "util.bioinformatics_algorithms.edit_distance.cross_distance_matrix", "util.ml_and_math.loss_functions.AverageMeter.update", "util.ml_and_math.loss_functions.AverageMeter.update", "Levenshtein.median", "util.bioinformatics_algorithms.edit_distance.cross_distance_matrix", "util.ml_and_math.loss_functions.AverageMeter.update", "Levenshtein.quickmedian", "util.bioinformatics_algorithms.edit_distance.cross_distance_matrix", "util.ml_and_math.loss_functions.AverageMeter.update", "numpy.mean", "numpy.min", "numpy.mean", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.torch_to_string", "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update", "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update", "home.repos.pwc.inspect_result.gcorso_neuroseed.bioinformatics_algorithms.edit_distance.cross_distance_matrix", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update"], ["from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "tree", "import", "to_nx_tree", "\n", "\n", "\n", "dataset", "=", "\"\"", "\n", "x", ",", "similarities", "=", "load_hc_data", "(", "dataset", ")", "\n", "metrics", "=", "{", "}", "\n", "for", "method", "in", "[", "\"single\"", ",", "\"complete\"", ",", "\"average\"", ",", "\"ward\"", "]", ":", "\n", "    ", "metrics", "[", "method", "]", "=", "{", "}", "\n", "baseline_tree", "=", "to_nx_tree", "(", "linkage", "(", "squareform", "(", "1", "-", "similarities", ")", ",", "method", ")", ")", "\n", "dc", "=", "dasgupta_cost", "(", "baseline_tree", ",", "similarities", ")", "\n", "metrics", "[", "method", "]", "[", "\"DC\"", "]", "=", "dc", "\n", "", "print", "(", "metrics", ")", "", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.baselines.run_baselines": [[42, 59], ["print", "multiple_alignment.steiner_string.task.dataset.MultipleAlignmentDataset", "baselines.multiple_alignment_cost_baseline", "print", "print", "multiple_alignment.steiner_string.task.dataset.MultipleAlignmentDataset", "baselines.multiple_alignment_cost_baseline", "print", "open", "pickle.load"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.baselines.multiple_alignment_cost_baseline", "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.baselines.multiple_alignment_cost_baseline"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.parser.general_arg_parser": [[4, 30], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "general_arg_parser", "(", ")", ":", "\n", "    ", "\"\"\" Parsing of parameters common to all the different models \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Data'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'Disables CUDA training (GPU)'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "42", ",", "help", "=", "'Random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "help", "=", "'Number of epochs to train'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.0003", ",", "help", "=", "'Initial learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "1e-6", ",", "help", "=", "'Weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'Dropout rate (1 - keep probability)'", ")", "\n", "parser", ".", "add_argument", "(", "'--patience'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Patience'", ")", "\n", "parser", ".", "add_argument", "(", "'--print_every'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Print training results every'", ")", "\n", "parser", ".", "add_argument", "(", "'--eval_every'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Print training results every'", ")", "\n", "parser", ".", "add_argument", "(", "'--expid'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'Experiment ID'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'Batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--embedding_size'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Size of embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--distance'", ",", "type", "=", "str", ",", "default", "=", "'hyperbolic'", ",", "help", "=", "'Type of distance to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--center'", ",", "type", "=", "str", ",", "default", "=", "'geometric_median'", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--workers'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'Number of workers'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--std_noise'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--validation_multiple'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate_multiple'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--normalization'", ",", "type", "=", "float", ",", "default", "=", "-", "1", ",", "help", "=", "''", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.train.execute_train": [[19, 129], ["print", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "train.load_msa_distance_dataset", "util.data_handling.data_loader.get_dataloaders", "types.SimpleNamespace", "types.SimpleNamespace", "encoder_class", "decoder_class", "multiple_alignment.steiner_string.models.pair_autoencoder.PairDistanceAutoEncoder", "multiple_alignment.steiner_string.models.msa_autoencoder.MultipleAlignmentAutoEncoder", "torch.Adam", "functools.partial", "sum", "print", "time.time", "range", "print", "print", "print", "multiple_alignment.steiner_string.models.pair_autoencoder.PairDistanceAutoEncoder.load_state_dict", "print", "print", "print", "train.test_multiple", "print", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "multiple_alignment.steiner_string.models.pair_autoencoder.PairDistanceAutoEncoder.parameters", "time.time", "train.train", "train.test_pair", "torch.load", "torch.load", "vars", "vars", "p.numel", "print", "sys.stdout.flush", "multiple_alignment.steiner_string.models.pair_autoencoder.PairDistanceAutoEncoder.parameters", "train.test_multiple", "train.test_multiple", "print", "torch.save", "torch.save", "print", "time.time", "train.test_multiple", "multiple_alignment.steiner_string.models.pair_autoencoder.PairDistanceAutoEncoder.state_dict", "os.remove", "time.time", "time.time"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.train.load_msa_distance_dataset", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.get_dataloaders", "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.train.test_multiple", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.train", "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.train.test_pair", "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.train.test_multiple", "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.train.test_multiple", "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.train.test_multiple"], ["\n", "\n", "def", "general_arg_parser", "(", ")", ":", "\n", "    ", "\"\"\" Parsing of parameters common to all the different models \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset path'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'Disables CUDA training (GPU)'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "42", ",", "help", "=", "'Random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'Number of epochs to train'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.001", ",", "help", "=", "'Initial learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'Weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'Dropout rate (1 - keep probability)'", ")", "\n", "parser", ".", "add_argument", "(", "'--patience'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Patience'", ")", "\n", "parser", ".", "add_argument", "(", "'--print_every'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Print training results every'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'Batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--embedding_size'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'Size of embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--distance'", ",", "type", "=", "str", ",", "default", "=", "'euclidean'", ",", "help", "=", "'Type of distance to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--workers'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'Number of workers'", ")", "\n", "parser", ".", "add_argument", "(", "'--loss'", ",", "type", "=", "str", ",", "default", "=", "\"mse\"", ",", "help", "=", "'Loss function to use (mse, mape or mae)'", ")", "\n", "parser", ".", "add_argument", "(", "'--plot'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'Plot real vs predicted distances'", ")", "\n", "parser", ".", "add_argument", "(", "'--closest_data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset for closest string retrieval tests'", ")", "\n", "parser", ".", "add_argument", "(", "'--hierarchical_data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset for hierarchical clustering'", ")", "\n", "parser", ".", "add_argument", "(", "'--scaling'", ",", "type", "=", "str", ",", "default", "=", "'False'", ",", "help", "=", "'Project to hypersphere (for hyperbolic)'", ")", "\n", "parser", ".", "add_argument", "(", "'--hyp_optimizer'", ",", "type", "=", "str", ",", "default", "=", "'Adam'", ",", "help", "=", "'Optimizer for hyperbolic (Adam or RAdam)'", ")", "\n", "parser", ".", "add_argument", "(", "'--margin'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'Margin loss'", ")", "\n", "\n", "return", "parser", "\n", "\n", "\n", "", "def", "execute_train", "(", "model_class", ",", "model_args", ",", "args", ")", ":", "\n", "# set device", "\n", "    ", "args", ".", "cuda", "=", "not", "args", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "device", "=", "'cuda'", "if", "args", ".", "cuda", "else", "'cpu'", "\n", "print", "(", "'Using device:'", ",", "device", ")", "\n", "\n", "# set the random seed", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "# load data", "\n", "", "datasets", "=", "load_triplet_dataset", "(", "args", ".", "data", ")", "\n", "loaders", "=", "get_dataloaders", "(", "datasets", ",", "batch_size", "=", "args", ".", "batch_size", ",", "workers", "=", "args", ".", "workers", ")", "\n", "\n", "# fix hyperparameters", "\n", "model_args", "=", "SimpleNamespace", "(", "**", "model_args", ")", "\n", "model_args", ".", "device", "=", "device", "\n", "model_args", ".", "len_sequence", "=", "datasets", "[", "'train'", "]", ".", "len_sequence", "\n", "model_args", ".", "embedding_size", "=", "args", ".", "embedding_size", "\n", "model_args", ".", "dropout", "=", "args", ".", "dropout", "\n", "print", "(", "\"Length of sequence\"", ",", "datasets", "[", "'train'", "]", ".", "len_sequence", ")", "\n", "args", ".", "scaling", "=", "True", "if", "args", ".", "scaling", "==", "'True'", "else", "False", "\n", "\n", "# generate model", "\n", "embedding_model", "=", "model_class", "(", "**", "vars", "(", "model_args", ")", ")", "\n", "model", "=", "TripletEncoder", "(", "embedding_model", "=", "embedding_model", ",", "distance", "=", "args", ".", "distance", ",", "scaling", "=", "args", ".", "scaling", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "# select optimizer", "\n", "if", "args", ".", "distance", "==", "'hyperbolic'", "and", "args", ".", "hyp_optimizer", "==", "'RAdam'", ":", "\n", "        ", "optimizer", "=", "RAdam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ",", "weight_decay", "=", "args", ".", "weight_decay", ")", "\n", "\n", "# select loss", "\n", "", "loss", "=", "partial", "(", "triplet_loss", ",", "margin", "=", "args", ".", "margin", ",", "device", "=", "device", ")", "\n", "\n", "# print total number of parameters", "\n", "total_params", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "print", "(", "'Total params'", ",", "total_params", ")", "\n", "\n", "# Train model", "\n", "t_total", "=", "time", ".", "time", "(", ")", "\n", "bad_counter", "=", "0", "\n", "best", "=", "1e10", "\n", "best_epoch", "=", "-", "1", "\n", "start_epoch", "=", "0", "\n", "\n", "for", "epoch", "in", "range", "(", "start_epoch", ",", "args", ".", "epochs", ")", ":", "\n", "        ", "t", "=", "time", ".", "time", "(", ")", "\n", "loss_train", "=", "train", "(", "model", ",", "loaders", "[", "'train'", "]", ",", "optimizer", ",", "loss", ",", "device", ")", "\n", "loss_val", "=", "test", "(", "model", ",", "loaders", "[", "'val'", "]", ",", "loss", ",", "device", ")", "\n", "\n", "# print progress", "\n", "if", "epoch", "%", "args", ".", "print_every", "==", "0", ":", "\n", "            ", "print", "(", "'Epoch: {:04d}'", ".", "format", "(", "epoch", "+", "1", ")", ",", "\n", "'loss_train: {:.6f}'", ".", "format", "(", "loss_train", ")", ",", "\n", "'loss_val: {:.6f}'", ".", "format", "(", "loss_val", ")", ",", "\n", "'time: {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "if", "loss_val", "<", "best", ":", "\n", "# save current model", "\n", "            ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "'{}.pkl'", ".", "format", "(", "epoch", ")", ")", "\n", "# remove previous model", "\n", "if", "best_epoch", ">=", "0", ":", "\n", "                ", "os", ".", "remove", "(", "'{}.pkl'", ".", "format", "(", "best_epoch", ")", ")", "\n", "# update training variables", "\n", "", "best", "=", "loss_val", "\n", "best_epoch", "=", "epoch", "\n", "bad_counter", "=", "0", "\n", "", "else", ":", "\n", "            ", "bad_counter", "+=", "1", "\n", "\n", "", "if", "bad_counter", "==", "args", ".", "patience", ":", "\n", "            ", "print", "(", "'Early stop at epoch {} (no improvement in last {} epochs)'", ".", "format", "(", "epoch", "+", "1", ",", "bad_counter", ")", ")", "\n", "break", "\n", "\n", "", "", "print", "(", "'Optimization Finished!'", ")", "\n", "print", "(", "'Total time elapsed: {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t_total", ")", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.train.load_msa_distance_dataset": [[131, 143], ["multiple_alignment.steiner_string.task.dataset.PairDistanceDatasetSampled", "multiple_alignment.steiner_string.task.dataset.PairDistanceDatasetSampled", "multiple_alignment.steiner_string.task.dataset.MultipleAlignmentDataset", "multiple_alignment.steiner_string.task.dataset.MultipleAlignmentDataset", "open", "pickle.load", "[].unsqueeze", "[].unsqueeze", "[].unsqueeze", "[].unsqueeze"], "function", ["None"], ["# Restore best model", "\n", "print", "(", "'Loading {}th epoch'", ".", "format", "(", "best_epoch", "+", "1", ")", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "'{}.pkl'", ".", "format", "(", "best_epoch", ")", ")", ")", "\n", "\n", "# Testing", "\n", "for", "dset", "in", "loaders", ".", "keys", "(", ")", ":", "\n", "        ", "avg_loss", "=", "test", "(", "model", ",", "loaders", "[", "dset", "]", ",", "loss", ",", "device", ")", "\n", "print", "(", "'Final results {}: loss = {:.6f}'", ".", "format", "(", "dset", ",", "avg_loss", ")", ")", "\n", "\n", "\n", "# Nearest neighbour retrieval", "\n", "", "if", "args", ".", "closest_data_path", "!=", "''", ":", "\n", "        ", "print", "(", "\"Closest string retrieval\"", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.train.train": [[145, 161], ["util.ml_and_math.loss_functions.AverageMeter", "util.ml_and_math.loss_functions.AverageMeter", "pair_autoencoder.train", "optimizer.zero_grad", "pair_autoencoder", "loss", "loss_train.backward", "optimizer.step", "util.ml_and_math.loss_functions.AverageMeter.update", "util.ml_and_math.loss_functions.AverageMeter.update", "sequences.to", "distances.to"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.train", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.loss", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.Arsinh.backward", "home.repos.pwc.inspect_result.gcorso_neuroseed.optim.radam.RAdam.step", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update"], ["batch_size", "=", "args", ".", "batch_size", ",", "device", "=", "device", ",", "distance", "=", "args", ".", "distance", ")", "\n", "\n", "# Hierarchical clustering", "\n", "", "if", "args", ".", "hierarchical_data_path", "!=", "''", ":", "\n", "        ", "print", "(", "\"Hierarchical clustering\"", ")", "\n", "hierarchical_clustering_testing", "(", "encoder_model", "=", "model", ",", "data_path", "=", "args", ".", "hierarchical_data_path", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "device", "=", "device", ",", "distance", "=", "args", ".", "distance", ")", "\n", "\n", "#torch.save((model_class, model_args, model.embedding_model.state_dict(), args.distance), '{}.pkl'.format(model_class.__name__))", "\n", "\n", "\n", "", "", "def", "load_triplet_dataset", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "sequences", ",", "distances", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "datasets", "=", "{", "}", "\n", "for", "key", "in", "sequences", ".", "keys", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.train.test_pair": [[163, 176], ["util.ml_and_math.loss_functions.AverageMeter", "util.ml_and_math.loss_functions.AverageMeter", "pair_autoencoder.eval", "pair_autoencoder", "loss", "util.ml_and_math.loss_functions.AverageMeter.update", "util.ml_and_math.loss_functions.AverageMeter.update", "sequences.to", "distances.to"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.loss", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update"], ["", "return", "datasets", "\n", "\n", "\n", "", "def", "train", "(", "model", ",", "loader", ",", "optimizer", ",", "loss", ",", "device", ")", ":", "\n", "    ", "avg_loss", "=", "AverageMeter", "(", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "for", "sequences", "in", "loader", ":", "\n", "# move examples to right device", "\n", "        ", "sequences", "=", "sequences", ".", "to", "(", "device", ")", "\n", "\n", "# forward propagation", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "distances", "=", "model", "(", "sequences", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.steiner_string.train.test_multiple": [[178, 191], ["util.ml_and_math.loss_functions.AverageMeter", "util.ml_and_math.loss_functions.AverageMeter", "multiple_autoencoder.eval", "sequences.to.to", "multiple_autoencoder", "multiple_alignment.steiner_string.models.loss.multiple_alignment_cost", "util.ml_and_math.loss_functions.AverageMeter.update", "util.ml_and_math.loss_functions.AverageMeter.update"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.loss.multiple_alignment_cost", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update"], ["# loss and backpropagation", "\n", "loss_train", "=", "loss", "(", "*", "distances", ")", "\n", "loss_train", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# keep track of average loss", "\n", "avg_loss", ".", "update", "(", "loss_train", ".", "data", ".", "item", "(", ")", ",", "sequences", ".", "shape", "[", "0", "]", ")", "\n", "\n", "", "return", "avg_loss", ".", "avg", "\n", "\n", "\n", "", "def", "test", "(", "model", ",", "loader", ",", "loss", ",", "device", ")", ":", "\n", "    ", "avg_loss", "=", "AverageMeter", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.convolutional.model.CNNEncoder.__init__": [[8, 24], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "range", "util.ml_and_math.layers.MLP", "model.CNNEncoder.to", "model.CNNEncoder.conv.add_module", "torch.Conv1d", "torch.Conv1d", "model.CNNEncoder.conv.add_module", "str", "torch.ReLU", "torch.ReLU", "str"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["import", "torch", ".", "nn", "as", "nn", "\n", "import", "torch", ".", "nn", ".", "functional", "as", "F", "\n", "\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "lca", "import", "hyp_lca", "\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "linkage", "import", "nn_merge_uf_fast_np", ",", "sl_from_embeddings", "\n", "from", "util", ".", "ml_and_math", ".", "poincare", "import", "project", "\n", "\n", "\n", "def", "index_to_one_hot", "(", "x", ",", "alphabet_size", "=", "4", ",", "device", "=", "'cpu'", ")", ":", "\n", "# add one row of zeros because the -1 represents the absence of element and it is encoded with zeros", "\n", "    ", "x", "=", "torch", ".", "cat", "(", "(", "torch", ".", "eye", "(", "alphabet_size", ",", "device", "=", "device", ")", ",", "torch", ".", "zeros", "(", "(", "1", ",", "alphabet_size", ")", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "0", ")", "[", "x", "]", "\n", "return", "x", "\n", "\n", "\n", "", "class", "HypHCModel", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\" Class interface for the other hyperbolic models for hierarchical clustering \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.convolutional.model.CNNEncoder.forward": [[25, 33], ["model.CNNEncoder.embedding", "model.CNNEncoder.transpose", "model.CNNEncoder.conv", "enc_sequence.reshape.reshape.reshape", "model.CNNEncoder.readout"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "temperature", "=", "0.05", ",", "init_size", "=", "1e-2", ",", "max_scale", "=", "1.", "-", "1e-3", ")", ":", "\n", "        ", "super", "(", "HypHCModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "temperature", "=", "temperature", "\n", "self", ".", "scale", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "init_size", "]", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "init_size", "=", "init_size", "\n", "self", ".", "max_scale", "=", "max_scale", "\n", "\n", "", "def", "anneal_temperature", "(", "self", ",", "anneal_factor", ")", ":", "\n", "        ", "\"\"\" anneal_factor: scalar for temperature decay \"\"\"", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.convolutional.model.CNNDecoder.__init__": [[37, 54], ["torch.Module.__init__", "util.ml_and_math.layers.MLP", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "range", "torch.Linear", "torch.Linear", "model.CNNDecoder.to", "model.CNNDecoder.de_conv.add_module", "torch.ConvTranspose1d", "torch.ConvTranspose1d", "model.CNNDecoder.de_conv.add_module", "str", "torch.ReLU", "torch.ReLU", "str"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["        ", "\"\"\"Normalize leaves embeddings to have the lie on a diameter.\"\"\"", "\n", "min_scale", "=", "self", ".", "init_size", "\n", "max_scale", "=", "self", ".", "max_scale", "\n", "return", "F", ".", "normalize", "(", "embeddings", ",", "p", "=", "2", ",", "dim", "=", "-", "1", ")", "*", "self", ".", "scale", ".", "clamp_min", "(", "min_scale", ")", ".", "clamp_max", "(", "max_scale", ")", "\n", "\n", "", "def", "encode", "(", "self", ",", "triple_ids", ",", "sequences", ")", ":", "\n", "        ", "pass", "\n", "\n", "", "def", "loss", "(", "self", ",", "triple_ids", ",", "sequences", ",", "similarities", ")", ":", "\n", "        ", "\"\"\" Computes the HypHCEmbeddings loss.\n        Args:\n            triple_ids: B x 3 tensor with triple ids\n            sequences:  B x 3 x N tensor with elements indexes\n            similarities: B x 3 tensor with pairwise similarities for triples \n                          [s12, s13, s23]\n        \"\"\"", "\n", "(", "B", ",", "_", ",", "N", ")", "=", "sequences", ".", "shape", "\n", "triple_ids", ",", "sequences", "=", "triple_ids", ".", "reshape", "(", "(", "3", "*", "B", ")", ")", ",", "sequences", ".", "reshape", "(", "(", "3", "*", "B", ",", "N", ")", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.convolutional.model.CNNDecoder.forward": [[55, 64], ["model.CNNDecoder.de_readout", "enc_sequence.transpose.transpose.reshape", "model.CNNDecoder.de_conv", "enc_sequence.transpose.transpose.transpose", "model.CNNDecoder.de_embedding"], "methods", ["None"], ["\n", "e", "=", "self", ".", "encode", "(", "triple_ids", ",", "sequences", ")", "\n", "e", "=", "self", ".", "normalize_embeddings", "(", "e", ")", "\n", "e", "=", "e", ".", "reshape", "(", "(", "B", ",", "3", ",", "-", "1", ")", ")", "\n", "\n", "d_12", "=", "hyp_lca", "(", "e", "[", ":", ",", "0", "]", ",", "e", "[", ":", ",", "1", "]", ",", "return_coord", "=", "False", ")", "\n", "d_13", "=", "hyp_lca", "(", "e", "[", ":", ",", "0", "]", ",", "e", "[", ":", ",", "2", "]", ",", "return_coord", "=", "False", ")", "\n", "d_23", "=", "hyp_lca", "(", "e", "[", ":", ",", "1", "]", ",", "e", "[", ":", ",", "2", "]", ",", "return_coord", "=", "False", ")", "\n", "lca_norm", "=", "torch", ".", "cat", "(", "[", "d_12", ",", "d_13", ",", "d_23", "]", ",", "dim", "=", "-", "1", ")", "\n", "weights", "=", "torch", ".", "softmax", "(", "lca_norm", "/", "self", ".", "temperature", ",", "dim", "=", "-", "1", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.convolutional.model.CNNDecoder.decode_and_sample": [[65, 69], ["model.CNNDecoder.forward", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.forward"], ["w_ord", "=", "torch", ".", "sum", "(", "similarities", "*", "weights", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "total", "=", "torch", ".", "sum", "(", "similarities", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "-", "w_ord", "\n", "return", "torch", ".", "mean", "(", "total", ")", "\n", "\n", "", "def", "decode_tree", "(", "self", ",", "embedded_sequences", ",", "fast_decoding", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.mlp.model.MLPEncoder.__init__": [[10, 17], ["torch.Module.__init__", "util.ml_and_math.layers.MLP", "model.MLPEncoder.to"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "lca", "import", "hyp_lca", "\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "linkage", "import", "nn_merge_uf_fast_np", ",", "sl_from_embeddings", "\n", "from", "util", ".", "ml_and_math", ".", "poincare", "import", "project", "\n", "\n", "\n", "def", "index_to_one_hot", "(", "x", ",", "alphabet_size", "=", "4", ",", "device", "=", "'cpu'", ")", ":", "\n", "# add one row of zeros because the -1 represents the absence of element and it is encoded with zeros", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.mlp.model.MLPEncoder.forward": [[18, 23], ["util.data_handling.data_loader.index_to_one_hot", "model.MLPEncoder.mlp", "util.data_handling.data_loader.index_to_one_hot.reshape"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.index_to_one_hot"], ["    ", "x", "=", "torch", ".", "cat", "(", "(", "torch", ".", "eye", "(", "alphabet_size", ",", "device", "=", "device", ")", ",", "torch", ".", "zeros", "(", "(", "1", ",", "alphabet_size", ")", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "0", ")", "[", "x", "]", "\n", "return", "x", "\n", "\n", "\n", "", "class", "HypHCModel", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\" Class interface for the other hyperbolic models for hierarchical clustering \"\"\"", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.mlp.model.MLPDecoder.__init__": [[27, 34], ["torch.Module.__init__", "util.ml_and_math.layers.MLP", "model.MLPDecoder.to"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["self", ".", "temperature", "=", "temperature", "\n", "self", ".", "scale", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "init_size", "]", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "init_size", "=", "init_size", "\n", "self", ".", "max_scale", "=", "max_scale", "\n", "\n", "", "def", "anneal_temperature", "(", "self", ",", "anneal_factor", ")", ":", "\n", "        ", "\"\"\" anneal_factor: scalar for temperature decay \"\"\"", "\n", "self", ".", "temperature", "*=", "anneal_factor", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.mlp.model.MLPDecoder.forward": [[35, 41], ["model.MLPDecoder.mlp", "dec_sequence.reshape.reshape.reshape"], "methods", ["None"], ["\n", "", "def", "normalize_embeddings", "(", "self", ",", "embeddings", ")", ":", "\n", "        ", "\"\"\"Normalize leaves embeddings to have the lie on a diameter.\"\"\"", "\n", "min_scale", "=", "self", ".", "init_size", "\n", "max_scale", "=", "self", ".", "max_scale", "\n", "return", "F", ".", "normalize", "(", "embeddings", ",", "p", "=", "2", ",", "dim", "=", "-", "1", ")", "*", "self", ".", "scale", ".", "clamp_min", "(", "min_scale", ")", ".", "clamp_max", "(", "max_scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.mlp.model.MLPDecoder.decode_and_sample": [[42, 46], ["model.MLPDecoder.forward", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.forward"], ["", "def", "encode", "(", "self", ",", "triple_ids", ",", "sequences", ")", ":", "\n", "        ", "pass", "\n", "\n", "", "def", "loss", "(", "self", ",", "triple_ids", ",", "sequences", ",", "similarities", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.torch_to_string": [[21, 27], ["S.tolist.tolist", "multiple_alignment.steiner_string.models.loss.remove_padding"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.models.loss.remove_padding"], ["def", "torch_to_string", "(", "S", ",", "alphabet_size", "=", "4", ")", ":", "\n", "    ", "S", "=", "S", ".", "tolist", "(", ")", "\n", "S", "=", "[", "remove_padding", "(", "s", ",", "alphabet_size", ")", "for", "s", "in", "S", "]", "\n", "S", "=", "[", "[", "DNA_ALPHABET", "[", "l", "]", "for", "l", "in", "s", "]", "for", "s", "in", "S", "]", "\n", "S", "=", "[", "''", ".", "join", "(", "s", ")", "for", "s", "in", "S", "]", "\n", "return", "S", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.save_fasta": [[29, 36], ["guide_tree.torch_to_string", "open", "range", "str", "range", "len", "f.write", "f.write", "len"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.torch_to_string"], ["", "def", "save_fasta", "(", "sequences", ")", ":", "\n", "    ", "strings", "=", "torch_to_string", "(", "sequences", ")", "\n", "names", "=", "[", "'S'", "+", "str", "(", "i", ")", "for", "i", "in", "range", "(", "len", "(", "strings", ")", ")", "]", "\n", "with", "open", "(", "'sequences.fasta'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "strings", ")", ")", ":", "\n", "            ", "f", ".", "write", "(", "'> '", "+", "names", "[", "i", "]", "+", "'\\n'", ")", "\n", "f", ".", "write", "(", "strings", "[", "i", "]", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.remove_inner_nodes_tree": [[38, 44], ["re.sub", "open", "[].decode", "open", "f.write", "f.readline"], "function", ["None"], ["", "", "", "def", "remove_inner_nodes_tree", "(", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "t", "=", "f", ".", "readline", "(", ")", "[", ":", "-", "1", "]", ".", "decode", "(", "'UTF-8'", ")", "\n", "", "t", "=", "re", ".", "sub", "(", "'Inner[0-9]+'", ",", "''", ",", "t", ")", "\n", "with", "open", "(", "filename", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.build_guide_trees": [[46, 69], ["Bio.Phylo.TreeConstruction._DistanceMatrix", "print", "Bio.Phylo.TreeConstruction.DistanceTreeConstructor", "time.time", "Bio.Phylo.TreeConstruction.DistanceTreeConstructor.nj", "print", "Bio.Phylo.write", "guide_tree.remove_inner_nodes_tree", "distance_matrix[].tolist", "range", "str", "range", "len", "len", "time.time"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.remove_inner_nodes_tree"], ["", "", "def", "build_guide_trees", "(", "distance_matrix", ")", ":", "\n", "# build distance matrix biopython object", "\n", "    ", "matrix", "=", "[", "distance_matrix", "[", "i", ",", ":", "i", "+", "1", "]", ".", "tolist", "(", ")", "for", "i", "in", "range", "(", "len", "(", "distance_matrix", ")", ")", "]", "\n", "names", "=", "[", "'S'", "+", "str", "(", "i", ")", "for", "i", "in", "range", "(", "len", "(", "distance_matrix", ")", ")", "]", "\n", "dm", "=", "_DistanceMatrix", "(", "names", ",", "matrix", ")", "\n", "print", "(", "'Constructed matrix'", ")", "\n", "constructor", "=", "DistanceTreeConstructor", "(", ")", "\n", "\n", "# construct neighbour joining tree", "\n", "t", "=", "time", ".", "time", "(", ")", "\n", "tree", "=", "constructor", ".", "nj", "(", "dm", ")", "\n", "print", "(", "'Constructed nj tree in {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t", ")", ")", "\n", "Phylo", ".", "write", "(", "tree", ",", "\"njtree.dnd\"", ",", "\"newick\"", ")", "\n", "remove_inner_nodes_tree", "(", "\"njtree.dnd\"", ")", "\n", "\n", "\"\"\"\n    # construct upgma tree\n    t = time.time()\n    tree = constructor.upgma(dm)\n    print('Constructed upgma tree in {:.4f}s'.format(time.time() - t))\n    Phylo.write(tree, \"upgmatree.dnd\", \"newick\")\n    remove_inner_nodes_tree(\"upgmatree.dnd\")\n    \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.ground_truth_guide_trees": [[70, 75], ["torch.max", "guide_tree.save_fasta", "guide_tree.build_guide_trees"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.save_fasta", "home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.build_guide_trees"], ["", "def", "ground_truth_guide_trees", "(", "dataset", ")", ":", "\n", "    ", "_", ",", "sequences", "=", "torch", ".", "max", "(", "dataset", ".", "sequences", ",", "dim", "=", "-", "1", ")", "\n", "distances", "=", "dataset", ".", "distances", "\n", "save_fasta", "(", "sequences", ")", "\n", "build_guide_trees", "(", "distances", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.approximate_guide_trees": [[77, 102], ["sys.setrecursionlimit", "torch.utils.data.DataLoader", "time.time", "closest_string.test.embed_strings", "print", "numpy.diag_indices", "torch.max", "guide_tree.save_fasta", "guide_tree.build_guide_trees", "time.time"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string.test.embed_strings", "home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.save_fasta", "home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.build_guide_trees"], ["", "def", "approximate_guide_trees", "(", "dataset", ",", "encoder_model", ",", "distance", ",", "batch_size", ",", "device", ")", ":", "\n", "    ", "sys", ".", "setrecursionlimit", "(", "2400", ")", "\n", "\n", "# combine datasets", "\n", "# print(dataset['train'].sequences.shape, dataset['val'].sequences.shape, dataset['test'].sequences.shape)", "\n", "# strings = torch.cat([dataset['train'].sequences.squeeze(), dataset['val'].sequences.squeeze(), dataset['test'].sequences.squeeze()], dim=0)", "\n", "# print(strings.shape)", "\n", "\n", "# embed sequences and compute distance matrix", "\n", "strings_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", ".", "sequences", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "\n", "t", "=", "time", ".", "time", "(", ")", "\n", "embedded_strings", "=", "embed_strings", "(", "strings_loader", ",", "encoder_model", ",", "device", ")", "\n", "estimate_distances", "=", "DISTANCE_MATRIX", "[", "distance", "]", "(", "embedded_strings", ",", "embedded_strings", ",", "encoder_model", ".", "scaling", ")", "\n", "print", "(", "'Constructed approximate distance matrix in {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t", ")", ")", "\n", "\n", "# fix the problems caused by floating point arithmetic: it must be symmetric and with diagonal 0", "\n", "estimate_distances", "=", "(", "estimate_distances", "+", "estimate_distances", ".", "T", ")", "/", "2", "\n", "ind", "=", "np", ".", "diag_indices", "(", "estimate_distances", ".", "shape", "[", "0", "]", ")", "\n", "estimate_distances", "[", "ind", "[", "0", "]", ",", "ind", "[", "1", "]", "]", "=", "0.0", "\n", "\n", "# build trees and save files", "\n", "_", ",", "sequences", "=", "torch", ".", "max", "(", "dataset", ".", "sequences", ",", "dim", "=", "-", "1", ")", "\n", "save_fasta", "(", "sequences", ")", "\n", "build_guide_trees", "(", "estimate_distances", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.general_arg_parser": [[23, 48], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset path'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'Disables CUDA training (GPU)'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "42", ",", "help", "=", "'Random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'Number of epochs to train'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.001", ",", "help", "=", "'Initial learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'Weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'Dropout rate (1 - keep probability)'", ")", "\n", "parser", ".", "add_argument", "(", "'--patience'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Patience'", ")", "\n", "parser", ".", "add_argument", "(", "'--print_every'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Print training results every'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'Batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--embedding_size'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'Size of embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--distance'", ",", "type", "=", "str", ",", "default", "=", "'euclidean'", ",", "help", "=", "'Type of distance to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--workers'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'Number of workers'", ")", "\n", "parser", ".", "add_argument", "(", "'--loss'", ",", "type", "=", "str", ",", "default", "=", "\"mse\"", ",", "help", "=", "'Loss function to use (mse, mape or mae)'", ")", "\n", "parser", ".", "add_argument", "(", "'--plot'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'Plot real vs predicted distances'", ")", "\n", "parser", ".", "add_argument", "(", "'--closest_data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset for closest string retrieval tests'", ")", "\n", "parser", ".", "add_argument", "(", "'--hierarchical_data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset for hierarchical clustering'", ")", "\n", "parser", ".", "add_argument", "(", "'--scaling'", ",", "type", "=", "str", ",", "default", "=", "'False'", ",", "help", "=", "'Project to hypersphere (for hyperbolic)'", ")", "\n", "parser", ".", "add_argument", "(", "'--hyp_optimizer'", ",", "type", "=", "str", ",", "default", "=", "'Adam'", ",", "help", "=", "'Optimizer for hyperbolic (Adam or RAdam)'", ")", "\n", "parser", ".", "add_argument", "(", "'--margin'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'Margin loss'", ")", "\n", "\n", "return", "parser", "\n", "\n", "\n", "", "def", "execute_train", "(", "model_class", ",", "model_args", ",", "args", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.execute_train": [[50, 184], ["print", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "train.load_edit_distance_dataset", "util.data_handling.data_loader.get_dataloaders", "types.SimpleNamespace", "print", "model_class", "edit_distance.models.pair_encoder.PairEmbeddingDistance", "edit_distance.models.pair_encoder.PairEmbeddingDistance.to", "sum", "print", "time.time", "range", "print", "print", "print", "edit_distance.models.pair_encoder.PairEmbeddingDistance.load_state_dict", "util.data_handling.data_loader.get_dataloaders.keys", "torch.save", "torch.save", "torch.save", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "edit_distance.models.hyperbolics.RAdam", "torch.Adam", "torch.MSELoss", "time.time", "train.train", "train.test", "torch.load", "torch.load", "torch.load", "print", "print", "closest_string.test.closest_string_testing", "print", "hierarchical_clustering.unsupervised.unsupervised.hierarchical_clustering_testing", "print", "multiple_alignment.guide_tree.guide_tree.approximate_guide_trees", "print", "train.load_edit_distance_dataset", "util.data_handling.data_loader.get_dataloaders", "util.data_handling.data_loader.get_dataloaders.keys", "vars", "edit_distance.models.pair_encoder.PairEmbeddingDistance.parameters", "edit_distance.models.pair_encoder.PairEmbeddingDistance.parameters", "torch.L1Loss", "p.numel", "print", "sys.stdout.flush", "torch.save", "torch.save", "torch.save", "print", "train.test_and_plot", "train.test", "print", "edit_distance.models.pair_encoder.PairEmbeddingDistance.embedding_model.state_dict", "edit_distance.models.pair_encoder.PairEmbeddingDistance.parameters", "edit_distance.models.pair_encoder.PairEmbeddingDistance.state_dict", "os.remove", "time.time", "max", "train.test_and_plot", "train.test", "time.time"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.load_edit_distance_dataset", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.get_dataloaders", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.train", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.test", "home.repos.pwc.inspect_result.gcorso_neuroseed.closest_string.test.closest_string_testing", "home.repos.pwc.inspect_result.gcorso_neuroseed.unsupervised.unsupervised.hierarchical_clustering_testing", "home.repos.pwc.inspect_result.gcorso_neuroseed.guide_tree.guide_tree.approximate_guide_trees", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.load_edit_distance_dataset", "home.repos.pwc.inspect_result.gcorso_neuroseed.data_handling.data_loader.get_dataloaders", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.test_and_plot", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.test", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.test_and_plot", "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.test"], ["    ", "args", ".", "cuda", "=", "not", "args", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "device", "=", "'cuda'", "if", "args", ".", "cuda", "else", "'cpu'", "\n", "print", "(", "'Using device:'", ",", "device", ")", "\n", "\n", "# set the random seed", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "# load data", "\n", "", "datasets", "=", "load_triplet_dataset", "(", "args", ".", "data", ")", "\n", "loaders", "=", "get_dataloaders", "(", "datasets", ",", "batch_size", "=", "args", ".", "batch_size", ",", "workers", "=", "args", ".", "workers", ")", "\n", "\n", "# fix hyperparameters", "\n", "model_args", "=", "SimpleNamespace", "(", "**", "model_args", ")", "\n", "model_args", ".", "device", "=", "device", "\n", "model_args", ".", "len_sequence", "=", "datasets", "[", "'train'", "]", ".", "len_sequence", "\n", "model_args", ".", "embedding_size", "=", "args", ".", "embedding_size", "\n", "model_args", ".", "dropout", "=", "args", ".", "dropout", "\n", "print", "(", "\"Length of sequence\"", ",", "datasets", "[", "'train'", "]", ".", "len_sequence", ")", "\n", "args", ".", "scaling", "=", "True", "if", "args", ".", "scaling", "==", "'True'", "else", "False", "\n", "\n", "# generate model", "\n", "embedding_model", "=", "model_class", "(", "**", "vars", "(", "model_args", ")", ")", "\n", "model", "=", "TripletEncoder", "(", "embedding_model", "=", "embedding_model", ",", "distance", "=", "args", ".", "distance", ",", "scaling", "=", "args", ".", "scaling", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "# select optimizer", "\n", "if", "args", ".", "distance", "==", "'hyperbolic'", "and", "args", ".", "hyp_optimizer", "==", "'RAdam'", ":", "\n", "        ", "optimizer", "=", "RAdam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ",", "weight_decay", "=", "args", ".", "weight_decay", ")", "\n", "\n", "# select loss", "\n", "", "loss", "=", "partial", "(", "triplet_loss", ",", "margin", "=", "args", ".", "margin", ",", "device", "=", "device", ")", "\n", "\n", "# print total number of parameters", "\n", "total_params", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "print", "(", "'Total params'", ",", "total_params", ")", "\n", "\n", "# Train model", "\n", "t_total", "=", "time", ".", "time", "(", ")", "\n", "bad_counter", "=", "0", "\n", "best", "=", "1e10", "\n", "best_epoch", "=", "-", "1", "\n", "start_epoch", "=", "0", "\n", "\n", "for", "epoch", "in", "range", "(", "start_epoch", ",", "args", ".", "epochs", ")", ":", "\n", "        ", "t", "=", "time", ".", "time", "(", ")", "\n", "loss_train", "=", "train", "(", "model", ",", "loaders", "[", "'train'", "]", ",", "optimizer", ",", "loss", ",", "device", ")", "\n", "loss_val", "=", "test", "(", "model", ",", "loaders", "[", "'val'", "]", ",", "loss", ",", "device", ")", "\n", "\n", "# print progress", "\n", "if", "epoch", "%", "args", ".", "print_every", "==", "0", ":", "\n", "            ", "print", "(", "'Epoch: {:04d}'", ".", "format", "(", "epoch", "+", "1", ")", ",", "\n", "'loss_train: {:.6f}'", ".", "format", "(", "loss_train", ")", ",", "\n", "'loss_val: {:.6f}'", ".", "format", "(", "loss_val", ")", ",", "\n", "'time: {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "if", "loss_val", "<", "best", ":", "\n", "# save current model", "\n", "            ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "'{}.pkl'", ".", "format", "(", "epoch", ")", ")", "\n", "# remove previous model", "\n", "if", "best_epoch", ">=", "0", ":", "\n", "                ", "os", ".", "remove", "(", "'{}.pkl'", ".", "format", "(", "best_epoch", ")", ")", "\n", "# update training variables", "\n", "", "best", "=", "loss_val", "\n", "best_epoch", "=", "epoch", "\n", "bad_counter", "=", "0", "\n", "", "else", ":", "\n", "            ", "bad_counter", "+=", "1", "\n", "\n", "", "if", "bad_counter", "==", "args", ".", "patience", ":", "\n", "            ", "print", "(", "'Early stop at epoch {} (no improvement in last {} epochs)'", ".", "format", "(", "epoch", "+", "1", ",", "bad_counter", ")", ")", "\n", "break", "\n", "\n", "", "", "print", "(", "'Optimization Finished!'", ")", "\n", "print", "(", "'Total time elapsed: {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t_total", ")", ")", "\n", "\n", "# Restore best model", "\n", "print", "(", "'Loading {}th epoch'", ".", "format", "(", "best_epoch", "+", "1", ")", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "'{}.pkl'", ".", "format", "(", "best_epoch", ")", ")", ")", "\n", "\n", "# Testing", "\n", "for", "dset", "in", "loaders", ".", "keys", "(", ")", ":", "\n", "        ", "avg_loss", "=", "test", "(", "model", ",", "loaders", "[", "dset", "]", ",", "loss", ",", "device", ")", "\n", "print", "(", "'Final results {}: loss = {:.6f}'", ".", "format", "(", "dset", ",", "avg_loss", ")", ")", "\n", "\n", "\n", "# Nearest neighbour retrieval", "\n", "", "if", "args", ".", "closest_data_path", "!=", "''", ":", "\n", "        ", "print", "(", "\"Closest string retrieval\"", ")", "\n", "closest_string_testing", "(", "encoder_model", "=", "model", ",", "data_path", "=", "args", ".", "closest_data_path", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "device", "=", "device", ",", "distance", "=", "args", ".", "distance", ")", "\n", "\n", "# Hierarchical clustering", "\n", "", "if", "args", ".", "hierarchical_data_path", "!=", "''", ":", "\n", "        ", "print", "(", "\"Hierarchical clustering\"", ")", "\n", "hierarchical_clustering_testing", "(", "encoder_model", "=", "model", ",", "data_path", "=", "args", ".", "hierarchical_data_path", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "device", "=", "device", ",", "distance", "=", "args", ".", "distance", ")", "\n", "\n", "#torch.save((model_class, model_args, model.embedding_model.state_dict(), args.distance), '{}.pkl'.format(model_class.__name__))", "\n", "\n", "\n", "", "", "def", "load_triplet_dataset", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "sequences", ",", "distances", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "datasets", "=", "{", "}", "\n", "for", "key", "in", "sequences", ".", "keys", "(", ")", ":", "\n", "        ", "datasets", "[", "key", "]", "=", "TripletDataset", "(", "sequences", "[", "key", "]", ".", "unsqueeze", "(", "0", ")", ",", "distances", "[", "key", "]", ".", "unsqueeze", "(", "0", ")", ",", "multiplicity", "=", "10", ")", "\n", "", "return", "datasets", "\n", "\n", "\n", "", "def", "train", "(", "model", ",", "loader", ",", "optimizer", ",", "loss", ",", "device", ")", ":", "\n", "    ", "avg_loss", "=", "AverageMeter", "(", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "for", "sequences", "in", "loader", ":", "\n", "# move examples to right device", "\n", "        ", "sequences", "=", "sequences", ".", "to", "(", "device", ")", "\n", "\n", "# forward propagation", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "distances", "=", "model", "(", "sequences", ")", "\n", "\n", "# loss and backpropagation", "\n", "loss_train", "=", "loss", "(", "*", "distances", ")", "\n", "loss_train", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# keep track of average loss", "\n", "avg_loss", ".", "update", "(", "loss_train", ".", "data", ".", "item", "(", ")", ",", "sequences", ".", "shape", "[", "0", "]", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.load_edit_distance_dataset": [[186, 201], ["sequences.keys", "open", "pickle.load", "len", "edit_distance.task.dataset.EditDistanceDatasetSampled", "edit_distance.task.dataset.EditDistanceDatasetSampled", "edit_distance.task.dataset.EditDistanceDatasetComplete", "sequences[].unsqueeze", "distances[].unsqueeze"], "function", ["None"], ["", "return", "avg_loss", ".", "avg", "\n", "\n", "\n", "", "def", "test", "(", "model", ",", "loader", ",", "loss", ",", "device", ")", ":", "\n", "    ", "avg_loss", "=", "AverageMeter", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "for", "sequences", "in", "loader", ":", "\n", "# move examples to right device", "\n", "        ", "sequences", "=", "sequences", ".", "to", "(", "device", ")", "\n", "\n", "# forward propagation and loss computation", "\n", "distances", "=", "model", "(", "sequences", ")", "\n", "loss_val", "=", "loss", "(", "*", "distances", ")", ".", "data", ".", "item", "(", ")", "\n", "avg_loss", ".", "update", "(", "loss_val", ",", "sequences", ".", "shape", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.train": [[203, 224], ["util.ml_and_math.loss_functions.AverageMeter", "model.train", "optimizer.zero_grad", "model", "loss", "loss.backward", "optimizer.step", "util.ml_and_math.loss_functions.AverageMeter.update", "sequences.to", "labels.to", "loss.data.item"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.train", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.loss", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.math.Arsinh.backward", "home.repos.pwc.inspect_result.gcorso_neuroseed.optim.radam.RAdam.step", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update"], ["", ""]], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.test": [[226, 241], ["util.ml_and_math.loss_functions.AverageMeter", "model.eval", "model", "loss().data.item", "util.ml_and_math.loss_functions.MAPE().data.item", "util.ml_and_math.loss_functions.AverageMeter.update", "sequences.to", "labels.to", "loss", "util.ml_and_math.loss_functions.MAPE"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.loss", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.MAPE"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.edit_distance.train.test_and_plot": [[243, 272], ["util.ml_and_math.loss_functions.AverageMeter", "model.eval", "numpy.concatenate", "numpy.concatenate", "pickle.dump", "model", "loss().data.item", "util.ml_and_math.loss_functions.MAPE().data.item", "util.ml_and_math.loss_functions.AverageMeter.update", "output_list.append", "labels_list.append", "open", "sequences.to", "np.concatenate.to", "model.cpu().detach().numpy", "np.concatenate.cpu().detach().numpy", "loss", "util.ml_and_math.loss_functions.MAPE", "model.cpu().detach", "np.concatenate.cpu().detach", "model.cpu", "np.concatenate.cpu"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update", "home.repos.pwc.inspect_result.gcorso_neuroseed.models.model.HypHCModel.loss", "home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.MAPE"], []], "home.repos.pwc.inspect_result.gcorso_neuroseed.kmer.model.Kmer.__init__": [[10, 15], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "lca", "import", "hyp_lca", "\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "linkage", "import", "nn_merge_uf_fast_np", ",", "sl_from_embeddings", "\n", "from", "util", ".", "ml_and_math", ".", "poincare", "import", "project", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.kmer.model.Kmer.forward": [[16, 29], ["torch.argmax().detach().numpy", "torch.argmax().detach().numpy", "torch.argmax().detach().numpy", "torch.argmax().detach().numpy", "numpy.apply_along_axis", "numpy.zeros", "range", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "functools.partial", "len", "numpy.bincount", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "torch.argmax().detach", "range", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "len", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["None"], ["def", "index_to_one_hot", "(", "x", ",", "alphabet_size", "=", "4", ",", "device", "=", "'cpu'", ")", ":", "\n", "# add one row of zeros because the -1 represents the absence of element and it is encoded with zeros", "\n", "    ", "x", "=", "torch", ".", "cat", "(", "(", "torch", ".", "eye", "(", "alphabet_size", ",", "device", "=", "device", ")", ",", "torch", ".", "zeros", "(", "(", "1", ",", "alphabet_size", ")", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "0", ")", "[", "x", "]", "\n", "return", "x", "\n", "\n", "\n", "", "class", "HypHCModel", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\" Class interface for the other hyperbolic models for hierarchical clustering \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "temperature", "=", "0.05", ",", "init_size", "=", "1e-2", ",", "max_scale", "=", "1.", "-", "1e-3", ")", ":", "\n", "        ", "super", "(", "HypHCModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "temperature", "=", "temperature", "\n", "self", ".", "scale", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "init_size", "]", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "init_size", "=", "init_size", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__": [[8, 18], ["torch.Module.__init__", "torch.GRU", "util.ml_and_math.layers.MLP", "model.GRU.to"], "methods", ["home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.__init__"], ["import", "torch", ".", "nn", "as", "nn", "\n", "import", "torch", ".", "nn", ".", "functional", "as", "F", "\n", "\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "lca", "import", "hyp_lca", "\n", "from", "hierarchical_clustering", ".", "relaxed", ".", "utils", ".", "linkage", "import", "nn_merge_uf_fast_np", ",", "sl_from_embeddings", "\n", "from", "util", ".", "ml_and_math", ".", "poincare", "import", "project", "\n", "\n", "\n", "def", "index_to_one_hot", "(", "x", ",", "alphabet_size", "=", "4", ",", "device", "=", "'cpu'", ")", ":", "\n", "# add one row of zeros because the -1 represents the absence of element and it is encoded with zeros", "\n", "    ", "x", "=", "torch", ".", "cat", "(", "(", "torch", ".", "eye", "(", "alphabet_size", ",", "device", "=", "device", ")", ",", "torch", ".", "zeros", "(", "(", "1", ",", "alphabet_size", ")", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "0", ")", "[", "x", "]", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.recurrent.model.GRU.forward": [[19, 31], ["sequence.transpose.transpose.transpose", "model.GRU.sequence_encoder", "model.GRU.readout"], "methods", ["None"], ["return", "x", "\n", "\n", "\n", "", "class", "HypHCModel", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\" Class interface for the other hyperbolic models for hierarchical clustering \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "temperature", "=", "0.05", ",", "init_size", "=", "1e-2", ",", "max_scale", "=", "1.", "-", "1e-3", ")", ":", "\n", "        ", "super", "(", "HypHCModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "temperature", "=", "temperature", "\n", "self", ".", "scale", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "init_size", "]", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "init_size", "=", "init_size", "\n", "self", ".", "max_scale", "=", "max_scale", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.general_arg_parser": [[16, 25], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["from", "hierarchical_clustering", ".", "unsupervised", ".", "unsupervised", "import", "hierarchical_clustering_testing", "\n", "from", "util", ".", "data_handling", ".", "data_loader", "import", "get_dataloaders", "\n", "from", "util", ".", "ml_and_math", ".", "loss_functions", "import", "AverageMeter", "\n", "\n", "\n", "def", "general_arg_parser", "(", ")", ":", "\n", "    ", "\"\"\" Parsing of parameters common to all the different models \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset path'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'Disables CUDA training (GPU)'", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.edit_distance_train": [[27, 63], ["range", "len", "encoder_method", "scipy.spatial.distance.cdist", "numpy.sum", "numpy.sum", "numpy.concatenate().flatten", "matplotlib.plot", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "p.append", "np.concatenate().flatten.append", "numpy.concatenate().flatten", "numpy.concatenate", "numpy.concatenate"], "function", ["None"], ["parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'Number of epochs to train'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.001", ",", "help", "=", "'Initial learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'Weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'Dropout rate (1 - keep probability)'", ")", "\n", "parser", ".", "add_argument", "(", "'--patience'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'Patience'", ")", "\n", "parser", ".", "add_argument", "(", "'--print_every'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Print training results every'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'Batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--embedding_size'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'Size of embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--distance'", ",", "type", "=", "str", ",", "default", "=", "'euclidean'", ",", "help", "=", "'Type of distance to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--workers'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'Number of workers'", ")", "\n", "parser", ".", "add_argument", "(", "'--loss'", ",", "type", "=", "str", ",", "default", "=", "\"mse\"", ",", "help", "=", "'Loss function to use (mse, mape or mae)'", ")", "\n", "parser", ".", "add_argument", "(", "'--plot'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'Plot real vs predicted distances'", ")", "\n", "parser", ".", "add_argument", "(", "'--closest_data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset for closest string retrieval tests'", ")", "\n", "parser", ".", "add_argument", "(", "'--hierarchical_data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Dataset for hierarchical clustering'", ")", "\n", "parser", ".", "add_argument", "(", "'--scaling'", ",", "type", "=", "str", ",", "default", "=", "'False'", ",", "help", "=", "'Project to hypersphere (for hyperbolic)'", ")", "\n", "parser", ".", "add_argument", "(", "'--hyp_optimizer'", ",", "type", "=", "str", ",", "default", "=", "'Adam'", ",", "help", "=", "'Optimizer for hyperbolic (Adam or RAdam)'", ")", "\n", "parser", ".", "add_argument", "(", "'--margin'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'Margin loss'", ")", "\n", "\n", "return", "parser", "\n", "\n", "\n", "", "def", "execute_train", "(", "model_class", ",", "model_args", ",", "args", ")", ":", "\n", "# set device", "\n", "    ", "args", ".", "cuda", "=", "not", "args", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "device", "=", "'cuda'", "if", "args", ".", "cuda", "else", "'cpu'", "\n", "print", "(", "'Using device:'", ",", "device", ")", "\n", "\n", "# set the random seed", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "# load data", "\n", "", "datasets", "=", "load_triplet_dataset", "(", "args", ".", "data", ")", "\n", "loaders", "=", "get_dataloaders", "(", "datasets", ",", "batch_size", "=", "args", ".", "batch_size", ",", "workers", "=", "args", ".", "workers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.edit_distance_test": [[65, 99], ["util.ml_and_math.loss_functions.AverageMeter", "range", "len", "encoder_method", "numpy.sum", "numpy.abs", "util.ml_and_math.loss_functions.AverageMeter.update", "numpy.concatenate().flatten", "numpy.concatenate().flatten", "matplotlib.plot", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "scipy.spatial.distance.cdist", "numpy.sum", "numpy.sum", "np.concatenate().flatten.append", "np.concatenate().flatten.append", "numpy.where", "numpy.concatenate", "numpy.concatenate"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.ml_and_math.loss_functions.AverageMeter.update"], ["model_args", "=", "SimpleNamespace", "(", "**", "model_args", ")", "\n", "model_args", ".", "device", "=", "device", "\n", "model_args", ".", "len_sequence", "=", "datasets", "[", "'train'", "]", ".", "len_sequence", "\n", "model_args", ".", "embedding_size", "=", "args", ".", "embedding_size", "\n", "model_args", ".", "dropout", "=", "args", ".", "dropout", "\n", "print", "(", "\"Length of sequence\"", ",", "datasets", "[", "'train'", "]", ".", "len_sequence", ")", "\n", "args", ".", "scaling", "=", "True", "if", "args", ".", "scaling", "==", "'True'", "else", "False", "\n", "\n", "# generate model", "\n", "embedding_model", "=", "model_class", "(", "**", "vars", "(", "model_args", ")", ")", "\n", "model", "=", "TripletEncoder", "(", "embedding_model", "=", "embedding_model", ",", "distance", "=", "args", ".", "distance", ",", "scaling", "=", "args", ".", "scaling", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "# select optimizer", "\n", "if", "args", ".", "distance", "==", "'hyperbolic'", "and", "args", ".", "hyp_optimizer", "==", "'RAdam'", ":", "\n", "        ", "optimizer", "=", "RAdam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ",", "weight_decay", "=", "args", ".", "weight_decay", ")", "\n", "\n", "# select loss", "\n", "", "loss", "=", "partial", "(", "triplet_loss", ",", "margin", "=", "args", ".", "margin", ",", "device", "=", "device", ")", "\n", "\n", "# print total number of parameters", "\n", "total_params", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "print", "(", "'Total params'", ",", "total_params", ")", "\n", "\n", "# Train model", "\n", "t_total", "=", "time", ".", "time", "(", ")", "\n", "bad_counter", "=", "0", "\n", "best", "=", "1e10", "\n", "best_epoch", "=", "-", "1", "\n", "start_epoch", "=", "0", "\n", "\n", "for", "epoch", "in", "range", "(", "start_epoch", ",", "args", ".", "epochs", ")", ":", "\n", "        ", "t", "=", "time", ".", "time", "(", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.correct_distance": [[101, 110], ["None"], "function", ["None"], ["loss_val", "=", "test", "(", "model", ",", "loaders", "[", "'val'", "]", ",", "loss", ",", "device", ")", "\n", "\n", "# print progress", "\n", "if", "epoch", "%", "args", ".", "print_every", "==", "0", ":", "\n", "            ", "print", "(", "'Epoch: {:04d}'", ".", "format", "(", "epoch", "+", "1", ")", ",", "\n", "'loss_train: {:.6f}'", ".", "format", "(", "loss_train", ")", ",", "\n", "'loss_val: {:.6f}'", ".", "format", "(", "loss_val", ")", ",", "\n", "'time: {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.test_method": [[112, 149], ["train.correct_distance", "texts.keys", "encoder_methods.keys", "open", "pickle.load", "train.edit_distance_train", "train.edit_distance_test", "train.edit_distance_test", "print", "texts[].numpy", "texts[].unsqueeze().numpy", "distance_labels[].numpy", "distance_labels[].unsqueeze().numpy", "train.closest_string_test", "train.hierarchical_clustering_test", "len", "len", "texts[].unsqueeze", "distance_labels[].unsqueeze"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.correct_distance", "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.edit_distance_train", "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.edit_distance_test", "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.edit_distance_test", "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.closest_string_test", "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.hierarchical_clustering_test"], ["# save current model", "\n", "            ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "'{}.pkl'", ".", "format", "(", "epoch", ")", ")", "\n", "# remove previous model", "\n", "if", "best_epoch", ">=", "0", ":", "\n", "                ", "os", ".", "remove", "(", "'{}.pkl'", ".", "format", "(", "best_epoch", ")", ")", "\n", "# update training variables", "\n", "", "best", "=", "loss_val", "\n", "best_epoch", "=", "epoch", "\n", "bad_counter", "=", "0", "\n", "", "else", ":", "\n", "            ", "bad_counter", "+=", "1", "\n", "\n", "", "if", "bad_counter", "==", "args", ".", "patience", ":", "\n", "            ", "print", "(", "'Early stop at epoch {} (no improvement in last {} epochs)'", ".", "format", "(", "epoch", "+", "1", ",", "bad_counter", ")", ")", "\n", "break", "\n", "\n", "", "", "print", "(", "'Optimization Finished!'", ")", "\n", "print", "(", "'Total time elapsed: {:.4f}s'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "t_total", ")", ")", "\n", "\n", "# Restore best model", "\n", "print", "(", "'Loading {}th epoch'", ".", "format", "(", "best_epoch", "+", "1", ")", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "'{}.pkl'", ".", "format", "(", "best_epoch", ")", ")", ")", "\n", "\n", "# Testing", "\n", "for", "dset", "in", "loaders", ".", "keys", "(", ")", ":", "\n", "        ", "avg_loss", "=", "test", "(", "model", ",", "loaders", "[", "dset", "]", ",", "loss", ",", "device", ")", "\n", "print", "(", "'Final results {}: loss = {:.6f}'", ".", "format", "(", "dset", ",", "avg_loss", ")", ")", "\n", "\n", "\n", "# Nearest neighbour retrieval", "\n", "", "if", "args", ".", "closest_data_path", "!=", "''", ":", "\n", "        ", "print", "(", "\"Closest string retrieval\"", ")", "\n", "closest_string_testing", "(", "encoder_model", "=", "model", ",", "data_path", "=", "args", ".", "closest_data_path", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "device", "=", "device", ",", "distance", "=", "args", ".", "distance", ")", "\n", "\n", "# Hierarchical clustering", "\n", "", "if", "args", ".", "hierarchical_data_path", "!=", "''", ":", "\n", "        ", "print", "(", "\"Hierarchical clustering\"", ")", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.closest_string_test": [[151, 170], ["encoder_method", "scipy.spatial.distance.cdist", "numpy.sum", "print", "print", "open", "pickle.load", "texts_references.numpy", "texts_queries.numpy", "labels.long().numpy", "numpy.mean", "numpy.concatenate", "len", "len", "label_distances.reshape", "range", "labels.long", "numpy.arange"], "function", ["None"], ["batch_size", "=", "args", ".", "batch_size", ",", "device", "=", "device", ",", "distance", "=", "args", ".", "distance", ")", "\n", "\n", "#torch.save((model_class, model_args, model.embedding_model.state_dict(), args.distance), '{}.pkl'.format(model_class.__name__))", "\n", "\n", "\n", "", "", "def", "load_triplet_dataset", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "sequences", ",", "distances", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "datasets", "=", "{", "}", "\n", "for", "key", "in", "sequences", ".", "keys", "(", ")", ":", "\n", "        ", "datasets", "[", "key", "]", "=", "TripletDataset", "(", "sequences", "[", "key", "]", ".", "unsqueeze", "(", "0", ")", ",", "distances", "[", "key", "]", ".", "unsqueeze", "(", "0", ")", ",", "multiplicity", "=", "10", ")", "\n", "", "return", "datasets", "\n", "\n", "\n", "", "def", "train", "(", "model", ",", "loader", ",", "optimizer", ",", "loss", ",", "device", ")", ":", "\n", "    ", "avg_loss", "=", "AverageMeter", "(", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "for", "sequences", "in", "loader", ":", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.train.hierarchical_clustering_test": [[172, 193], ["hierarchical_clustering.relaxed.datasets.loading.load_hc_data", "encoder_method", "numpy.diag_indices", "print", "scipy.spatial.distance.cdist", "hierarchical_clustering.relaxed.utils.tree.to_nx_tree", "hierarchical_clustering.relaxed.utils.metrics.dasgupta_cost", "scipy.cluster.hierarchy.linkage", "scipy.spatial.distance.squareform"], "function", ["home.repos.pwc.inspect_result.gcorso_neuroseed.datasets.hc_dataset.load_hc_data", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.tree.to_nx_tree", "home.repos.pwc.inspect_result.gcorso_neuroseed.utils.metrics.dasgupta_cost"], ["        ", "sequences", "=", "sequences", ".", "to", "(", "device", ")", "\n", "\n", "# forward propagation", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "distances", "=", "model", "(", "sequences", ")", "\n", "\n", "# loss and backpropagation", "\n", "loss_train", "=", "loss", "(", "*", "distances", ")", "\n", "loss_train", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# keep track of average loss", "\n", "avg_loss", ".", "update", "(", "loss_train", ".", "data", ".", "item", "(", ")", ",", "sequences", ".", "shape", "[", "0", "]", ")", "\n", "\n", "", "return", "avg_loss", ".", "avg", "\n", "\n", "\n", "", "def", "test", "(", "model", ",", "loader", ",", "loss", ",", "device", ")", ":", "\n", "    ", "avg_loss", "=", "AverageMeter", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "for", "sequences", "in", "loader", ":", "\n"]], "home.repos.pwc.inspect_result.gcorso_neuroseed.baselines.kmer.kmer": [[8, 26], ["numpy.apply_along_axis", "range", "functools.partial", "numpy.unique", "indices.reshape", "len", "numpy.zeros", "numpy.zeros", "len", "numpy.bincount", "range", "len"], "function", ["None"], ["def", "kmer", "(", "S", ",", "k", ",", "alphabet_size", "=", "4", ",", "compress_zeros", "=", "False", ")", ":", "\n", "    ", "kernel", "=", "[", "alphabet_size", "**", "p", "for", "p", "in", "range", "(", "k", ")", "]", "\n", "kmers", "=", "np", ".", "apply_along_axis", "(", "partial", "(", "np", ".", "convolve", ",", "v", "=", "kernel", ",", "mode", "=", "'valid'", ")", ",", "1", ",", "S", ")", "\n", "\n", "if", "compress_zeros", ":", "\n", "# if compress_zeros = True subsequences not contained in any of the sequences are ignored", "\n", "        ", "x", ",", "indices", "=", "np", ".", "unique", "(", "kmers", ",", "return_inverse", "=", "True", ")", "\n", "kmers", "=", "indices", ".", "reshape", "(", "kmers", ".", "shape", ")", "\n", "dimensions", "=", "len", "(", "x", ")", "\n", "vectors", "=", "np", ".", "zeros", "(", "(", "S", ".", "shape", "[", "0", "]", ",", "dimensions", ")", ")", "\n", "", "else", ":", "\n", "        ", "vectors", "=", "np", ".", "zeros", "(", "(", "S", ".", "shape", "[", "0", "]", ",", "alphabet_size", "**", "k", ")", ")", "\n", "\n", "", "for", "d", "in", "range", "(", "len", "(", "S", ")", ")", ":", "\n", "        ", "bbins", "=", "np", ".", "bincount", "(", "kmers", "[", "d", "]", ")", "\n", "vectors", "[", "d", "]", "[", ":", "len", "(", "bbins", ")", "]", "+=", "bbins", "\n", "\n", "", "return", "vectors", "\n", "\n"]]}