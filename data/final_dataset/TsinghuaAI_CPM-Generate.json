{"home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.get_batch": [[17, 29], ["tokens.view().contiguous.view().contiguous", "generate_samples.get_masks_and_position_ids", "tokens.view().contiguous.view"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.get_masks_and_position_ids"], ["def", "get_batch", "(", "context_tokens", ",", "args", ")", ":", "\n", "    ", "tokens", "=", "context_tokens", "\n", "tokens", "=", "tokens", ".", "view", "(", "args", ".", "batch_size", ",", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Get the masks and postition ids.", "\n", "attention_mask", ",", "loss_mask", ",", "position_ids", "=", "get_masks_and_position_ids", "(", "\n", "tokens", ",", "\n", "args", ".", "eod_token", ",", "\n", "args", ".", "reset_position_ids", ",", "\n", "args", ".", "reset_attention_mask", ")", "\n", "\n", "return", "tokens", ",", "attention_mask", ",", "position_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.top_k_logits": [[30, 56], ["float", "logits.view().contiguous.view().contiguous", "torch.sort", "torch.sort", "torch.cumsum", "torch.cumsum", "sorted_indices_to_remove[].clone", "logits.view().contiguous.view().contiguous", "torch.softmax", "logits.view().contiguous.view", "logits.view().contiguous.view", "torch.topk", "torch.topk", "logits.view().contiguous.size"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.cumsum", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.cumsum", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "def", "top_k_logits", "(", "logits", ",", "top_k", "=", "0", ",", "top_p", "=", "0.0", ",", "filter_value", "=", "-", "float", "(", "'Inf'", ")", ")", ":", "\n", "# This function has been mostly taken from huggingface conversational ai code at", "\n", "# https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313", "\n", "\n", "    ", "if", "top_k", ">", "0", ":", "\n", "# Remove all tokens with a probability less than the last token of the top-k", "\n", "        ", "indices_to_remove", "=", "logits", "<", "torch", ".", "topk", "(", "logits", ",", "top_k", ")", "[", "0", "]", "[", "...", ",", "-", "1", ",", "None", "]", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "\n", "", "if", "top_p", ">", "0.0", ":", "\n", "#convert to 1D", "\n", "        ", "logits", "=", "logits", ".", "view", "(", "logits", ".", "size", "(", ")", "[", "1", "]", ")", ".", "contiguous", "(", ")", "\n", "sorted_logits", ",", "sorted_indices", "=", "torch", ".", "sort", "(", "logits", ",", "descending", "=", "True", ")", "\n", "cumulative_probs", "=", "torch", ".", "cumsum", "(", "F", ".", "softmax", "(", "sorted_logits", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Remove tokens with cumulative probability above the threshold", "\n", "sorted_indices_to_remove", "=", "cumulative_probs", ">", "top_p", "\n", "# Shift the indices to the right to keep also the first token above the threshold", "\n", "sorted_indices_to_remove", "[", "...", ",", "1", ":", "]", "=", "sorted_indices_to_remove", "[", "...", ",", ":", "-", "1", "]", ".", "clone", "(", ")", "\n", "sorted_indices_to_remove", "[", "...", ",", "0", "]", "=", "0", "\n", "indices_to_remove", "=", "sorted_indices", "[", "sorted_indices_to_remove", "]", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "#going back to 2D", "\n", "logits", "=", "logits", ".", "view", "(", "1", ",", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.prepare_tokenizer": [[59, 82], ["make_tokenizer", "print", "make_tokenizer.get_command", "mpu.get_model_parallel_world_size"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.make_tokenizer", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.get_command", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size"], ["", "def", "prepare_tokenizer", "(", "args", ")", ":", "\n", "\n", "    ", "tokenizer_args", "=", "{", "\n", "'tokenizer_type'", ":", "args", ".", "tokenizer_type", ",", "\n", "'corpus'", ":", "None", ",", "\n", "'model_path'", ":", "args", ".", "tokenizer_path", ",", "\n", "'vocab_size'", ":", "args", ".", "vocab_size", ",", "\n", "'model_type'", ":", "args", ".", "tokenizer_model_type", ",", "\n", "'cache_dir'", ":", "args", ".", "cache_dir", "}", "\n", "tokenizer", "=", "make_tokenizer", "(", "**", "tokenizer_args", ")", "\n", "\n", "args", ".", "tokenizer_num_tokens", "=", "tokenizer", ".", "num_tokens", "\n", "args", ".", "tokenizer_num_type_tokens", "=", "tokenizer", ".", "num_type_tokens", "\n", "args", ".", "eod_token", "=", "tokenizer", ".", "get_command", "(", "'eos'", ")", ".", "Id", "\n", "\n", "after", "=", "tokenizer", ".", "num_tokens", "\n", "while", "after", "%", "mpu", ".", "get_model_parallel_world_size", "(", ")", "!=", "0", ":", "\n", "        ", "after", "+=", "1", "\n", "\n", "", "args", ".", "vocab_size", "=", "after", "\n", "print", "(", "\"prepare tokenizer done\"", ",", "flush", "=", "True", ")", "\n", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.load_ocnli_data": [[83, 184], ["arguments.get_args", "os.path.join", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "mpu.get_data_parallel_world_size", "mpu.get_data_parallel_rank", "data.samplers.DistributedBatchSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "open", "tokenizer.encode", "len", "range", "torch.tensor.append", "len", "tokens.extend", "torch.tensor.append", "tokenizer.encode", "len", "range", "torch.tensor.append", "len", "tokens.extend", "torch.tensor.append", "tokenizer.encode", "len", "range", "torch.tensor.append", "len", "tokens.extend", "torch.tensor.append", "data.samplers.RandomSampler", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "objs.append", "tokenizer.encode", "tokenizer.encode", "tokenizer.encode", "torch.tensor.append", "json.loads", "len", "len", "len", "torch.tensor.append", "torch.tensor.append", "line.strip"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.get_args", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "load_ocnli_data", "(", "data_path", ",", "data_type", ",", "tokenizer", ")", ":", "\n", "    ", "args", "=", "get_args", "(", ")", "\n", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "data_type", "+", "'.json'", ")", "\n", "objs", "=", "[", "]", "\n", "with", "open", "(", "filename", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "objs", ".", "append", "(", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", ")", "\n", "\n", "", "", "pad_id", "=", "tokenizer", ".", "encoder", "[", "'<pad>'", "]", "\n", "args", ".", "eod_token", "=", "tokenizer", ".", "encoder", "[", "'<eod>'", "]", "\n", "\n", "all_tokens_1", "=", "[", "]", "\n", "all_masks_1", "=", "[", "]", "\n", "all_tokens_2", "=", "[", "]", "\n", "all_masks_2", "=", "[", "]", "\n", "all_tokens_3", "=", "[", "]", "\n", "all_masks_3", "=", "[", "]", "\n", "all_labels", "=", "[", "]", "\n", "for", "obj", "in", "objs", ":", "\n", "\n", "        ", "if", "obj", "[", "'label'", "]", "==", "'-'", ":", "\n", "            ", "continue", "\n", "\n", "", "prompt", "=", "\"{}\uff1f\u5bf9\uff0c\"", ".", "format", "(", "obj", "[", "'sentence1'", "]", ")", "\n", "prompt_tokens", "=", "tokenizer", ".", "encode", "(", "prompt", ")", "\n", "prompt_len", "=", "len", "(", "prompt_tokens", ")", "\n", "tokens", "=", "prompt_tokens", "+", "tokenizer", ".", "encode", "(", "obj", "[", "'sentence2'", "]", ")", "\n", "second_mask", "=", "[", "0", "]", "*", "(", "args", ".", "seq_length", "-", "1", ")", "\n", "for", "idx", "in", "range", "(", "prompt_len", "-", "1", ",", "len", "(", "tokens", ")", "-", "1", ")", ":", "\n", "            ", "second_mask", "[", "idx", "]", "=", "1", "\n", "", "all_masks_1", ".", "append", "(", "second_mask", ")", "\n", "token_length", "=", "len", "(", "tokens", ")", "\n", "assert", "token_length", "<", "args", ".", "seq_length", "\n", "tokens", ".", "extend", "(", "[", "pad_id", "]", "*", "(", "args", ".", "seq_length", "-", "token_length", ")", ")", "\n", "all_tokens_1", ".", "append", "(", "tokens", ")", "\n", "\n", "prompt", "=", "\"{}\uff1f\u9519\uff0c\"", ".", "format", "(", "obj", "[", "'sentence1'", "]", ")", "\n", "prompt_tokens", "=", "tokenizer", ".", "encode", "(", "prompt", ")", "\n", "prompt_len", "=", "len", "(", "prompt_tokens", ")", "\n", "tokens", "=", "prompt_tokens", "+", "tokenizer", ".", "encode", "(", "obj", "[", "'sentence2'", "]", ")", "\n", "second_mask", "=", "[", "0", "]", "*", "(", "args", ".", "seq_length", "-", "1", ")", "\n", "for", "idx", "in", "range", "(", "prompt_len", "-", "1", ",", "len", "(", "tokens", ")", "-", "1", ")", ":", "\n", "            ", "second_mask", "[", "idx", "]", "=", "1", "\n", "", "all_masks_2", ".", "append", "(", "second_mask", ")", "\n", "token_length", "=", "len", "(", "tokens", ")", "\n", "assert", "token_length", "<", "args", ".", "seq_length", "\n", "tokens", ".", "extend", "(", "[", "pad_id", "]", "*", "(", "args", ".", "seq_length", "-", "token_length", ")", ")", "\n", "all_tokens_2", ".", "append", "(", "tokens", ")", "\n", "\n", "prompt", "=", "\"{}\uff1f\u4e5f\u8bb8\uff0c\"", ".", "format", "(", "obj", "[", "'sentence1'", "]", ")", "\n", "prompt_tokens", "=", "tokenizer", ".", "encode", "(", "prompt", ")", "\n", "prompt_len", "=", "len", "(", "prompt_tokens", ")", "\n", "tokens", "=", "prompt_tokens", "+", "tokenizer", ".", "encode", "(", "obj", "[", "'sentence2'", "]", ")", "\n", "second_mask", "=", "[", "0", "]", "*", "(", "args", ".", "seq_length", "-", "1", ")", "\n", "for", "idx", "in", "range", "(", "prompt_len", "-", "1", ",", "len", "(", "tokens", ")", "-", "1", ")", ":", "\n", "            ", "second_mask", "[", "idx", "]", "=", "1", "\n", "", "all_masks_3", ".", "append", "(", "second_mask", ")", "\n", "token_length", "=", "len", "(", "tokens", ")", "\n", "assert", "token_length", "<", "args", ".", "seq_length", "\n", "tokens", ".", "extend", "(", "[", "pad_id", "]", "*", "(", "args", ".", "seq_length", "-", "token_length", ")", ")", "\n", "all_tokens_3", ".", "append", "(", "tokens", ")", "\n", "\n", "if", "obj", "[", "'label'", "]", "==", "'entailment'", ":", "\n", "            ", "all_labels", ".", "append", "(", "[", "0", "]", ")", "\n", "", "elif", "obj", "[", "'label'", "]", "==", "'contradiction'", ":", "\n", "            ", "all_labels", ".", "append", "(", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "all_labels", ".", "append", "(", "[", "2", "]", ")", "\n", "\n", "", "", "all_tokens_1", "=", "torch", ".", "tensor", "(", "all_tokens_1", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_masks_1", "=", "torch", ".", "tensor", "(", "all_masks_1", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "all_tokens_2", "=", "torch", ".", "tensor", "(", "all_tokens_2", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_masks_2", "=", "torch", ".", "tensor", "(", "all_masks_2", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "all_tokens_3", "=", "torch", ".", "tensor", "(", "all_tokens_3", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_masks_3", "=", "torch", ".", "tensor", "(", "all_masks_3", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "all_labels", "=", "torch", ".", "tensor", "(", "all_labels", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "dataset", "=", "TensorDataset", "(", "all_tokens_1", ",", "all_masks_1", ",", "all_tokens_2", ",", "all_masks_2", ",", "all_tokens_3", ",", "all_masks_3", ",", "all_labels", ")", "\n", "\n", "# Data parallel arguments.", "\n", "world_size", "=", "mpu", ".", "get_data_parallel_world_size", "(", ")", "\n", "rank", "=", "mpu", ".", "get_data_parallel_rank", "(", ")", "\n", "global_batch_size", "=", "args", ".", "batch_size", "*", "world_size", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", "\n", "# Use a random sampler with distributed batch sampler.", "\n", "if", "data_type", "==", "'train'", ":", "\n", "        ", "sampler", "=", "RandomSampler", "(", "dataset", ")", "\n", "", "else", ":", "\n", "        ", "sampler", "=", "torch", ".", "utils", ".", "data", ".", "SequentialSampler", "(", "dataset", ")", "\n", "", "batch_sampler", "=", "DistributedBatchSampler", "(", "sampler", "=", "sampler", ",", "\n", "batch_size", "=", "global_batch_size", ",", "\n", "drop_last", "=", "True", ",", "\n", "rank", "=", "rank", ",", "\n", "world_size", "=", "world_size", ")", "\n", "\n", "# Torch dataloader.", "\n", "return", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", ",", "\n", "batch_sampler", "=", "batch_sampler", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "pin_memory", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.load_iflytek_data": [[185, 261], ["arguments.get_args", "os.path.join", "enumerate", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "mpu.get_data_parallel_world_size", "mpu.get_data_parallel_rank", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "data.samplers.DistributedBatchSampler", "open", "open", "enumerate", "all_labels.append", "enumerate", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "objs.append", "os.path.join", "json.loads", "labels.append", "tokenizer.encode", "random.sample", "random.sample.append", "random.sample.index", "tokenizer.encode", "len", "range", "torch.tensor.append", "len", "tokens.extend", "torch.tensor.append", "json.loads", "line.strip", "random.sample", "line.strip", "len"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.get_args", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "load_iflytek_data", "(", "data_path", ",", "data_type", ",", "tokenizer", ",", "few_shot", "=", "False", ")", ":", "\n", "    ", "args", "=", "get_args", "(", ")", "\n", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "data_type", "+", "'.json'", ")", "\n", "objs", "=", "[", "]", "\n", "with", "open", "(", "filename", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "objs", ".", "append", "(", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", ")", "\n", "\n", "", "", "pad_id", "=", "tokenizer", ".", "encoder", "[", "'<pad>'", "]", "\n", "args", ".", "eod_token", "=", "tokenizer", ".", "encoder", "[", "'<eod>'", "]", "\n", "\n", "labels", "=", "[", "]", "\n", "label_map", "=", "{", "}", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'labels.json'", ")", ")", "as", "fin", ":", "\n", "        ", "for", "i", ",", "line", "in", "enumerate", "(", "fin", ")", ":", "\n", "            ", "obj", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "labels", ".", "append", "(", "obj", "[", "'label_des'", "]", ")", "\n", "label_map", "[", "obj", "[", "'label_des'", "]", "]", "=", "i", "\n", "\n", "", "", "all_tokens", "=", "[", "]", "\n", "all_masks", "=", "[", "]", "\n", "all_labels", "=", "[", "]", "\n", "for", "_", ",", "obj", "in", "enumerate", "(", "objs", ")", ":", "\n", "        ", "sentence", "=", "obj", "[", "'sentence'", "]", "\n", "tokenized_sentence", "=", "tokenizer", ".", "encode", "(", "sentence", ")", "[", ":", "args", ".", "seq_length", "-", "20", "]", "\n", "\n", "if", "few_shot", ":", "\n", "            ", "cur_labels", "=", "random", ".", "sample", "(", "labels", ",", "3", ")", "\n", "while", "obj", "[", "'label_des'", "]", "in", "cur_labels", ":", "\n", "                ", "cur_labels", "=", "random", ".", "sample", "(", "labels", ",", "3", ")", "\n", "", "cur_labels", ".", "append", "(", "obj", "[", "'label_des'", "]", ")", "\n", "cur_label", "=", "cur_labels", ".", "index", "(", "obj", "[", "'label_des'", "]", ")", "\n", "assert", "cur_label", "!=", "-", "1", "\n", "", "else", ":", "\n", "            ", "cur_labels", "=", "labels", "\n", "cur_label", "=", "label_map", "[", "obj", "[", "'label_des'", "]", "]", "\n", "\n", "", "all_labels", ".", "append", "(", "cur_label", ")", "\n", "\n", "for", "_", ",", "label", "in", "enumerate", "(", "cur_labels", ")", ":", "\n", "            ", "prompt", "=", "\"\u8fd9\u662f\u5173\u4e8e{}\u7684\u5e94\u7528\u7a0b\u5e8f\uff1a\"", ".", "format", "(", "label", ")", "\n", "prompt_tokens", "=", "tokenizer", ".", "encode", "(", "prompt", ")", "\n", "prompt_len", "=", "len", "(", "prompt_tokens", ")", "\n", "tokens", "=", "prompt_tokens", "+", "tokenized_sentence", "\n", "second_mask", "=", "[", "0", "]", "*", "(", "args", ".", "seq_length", "-", "1", ")", "\n", "for", "idx", "in", "range", "(", "prompt_len", "-", "1", ",", "len", "(", "tokens", ")", "-", "1", ")", ":", "\n", "                ", "second_mask", "[", "idx", "]", "=", "1", "\n", "", "all_masks", ".", "append", "(", "second_mask", ")", "\n", "token_length", "=", "len", "(", "tokens", ")", "\n", "assert", "token_length", "<", "args", ".", "seq_length", "\n", "tokens", ".", "extend", "(", "[", "pad_id", "]", "*", "(", "args", ".", "seq_length", "-", "token_length", ")", ")", "\n", "all_tokens", ".", "append", "(", "tokens", ")", "\n", "\n", "", "", "all_tokens", "=", "torch", ".", "tensor", "(", "all_tokens", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_masks", "=", "torch", ".", "tensor", "(", "all_masks", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "dataset", "=", "TensorDataset", "(", "all_tokens", ",", "all_masks", ")", "\n", "\n", "# Data parallel arguments.", "\n", "world_size", "=", "mpu", ".", "get_data_parallel_world_size", "(", ")", "\n", "rank", "=", "mpu", ".", "get_data_parallel_rank", "(", ")", "\n", "global_batch_size", "=", "args", ".", "batch_size", "*", "world_size", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", "\n", "sampler", "=", "torch", ".", "utils", ".", "data", ".", "SequentialSampler", "(", "dataset", ")", "\n", "batch_sampler", "=", "DistributedBatchSampler", "(", "sampler", "=", "sampler", ",", "\n", "batch_size", "=", "global_batch_size", ",", "\n", "drop_last", "=", "True", ",", "\n", "rank", "=", "rank", ",", "\n", "world_size", "=", "world_size", ")", "\n", "\n", "# Torch dataloader.", "\n", "return", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", ",", "\n", "batch_sampler", "=", "batch_sampler", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "pin_memory", "=", "True", ")", ",", "all_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.load_tnews_data": [[262, 341], ["arguments.get_args", "os.path.join", "enumerate", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "mpu.get_data_parallel_world_size", "mpu.get_data_parallel_rank", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "data.samplers.DistributedBatchSampler", "open", "open", "enumerate", "all_labels.append", "enumerate", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "objs.append", "os.path.join", "json.loads", "labels.append", "tokenizer.encode", "random.sample", "random.sample.append", "random.sample.index", "tokenizer.encode", "len", "range", "torch.tensor.append", "len", "tokens.extend", "torch.tensor.append", "json.loads", "line.strip", "random.sample", "line.strip", "len"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.get_args", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "load_tnews_data", "(", "data_path", ",", "data_type", ",", "tokenizer", ",", "few_shot", "=", "False", ")", ":", "\n", "    ", "args", "=", "get_args", "(", ")", "\n", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "data_type", "+", "'.json'", ")", "\n", "objs", "=", "[", "]", "\n", "with", "open", "(", "filename", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "objs", ".", "append", "(", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", ")", "\n", "\n", "", "", "pad_id", "=", "tokenizer", ".", "encoder", "[", "'<pad>'", "]", "\n", "args", ".", "eod_token", "=", "tokenizer", ".", "encoder", "[", "'<eod>'", "]", "\n", "\n", "labels", "=", "[", "]", "\n", "label_map", "=", "{", "}", "\n", "label_reverse", "=", "{", "}", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'labels.json'", ")", ")", "as", "fin", ":", "\n", "        ", "for", "i", ",", "line", "in", "enumerate", "(", "fin", ")", ":", "\n", "            ", "obj", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "labels", ".", "append", "(", "obj", "[", "'label_desc'", "]", ")", "\n", "label_map", "[", "obj", "[", "'label_desc'", "]", "]", "=", "i", "\n", "label_reverse", "[", "obj", "[", "'label'", "]", "]", "=", "obj", "[", "'label_desc'", "]", "\n", "\n", "", "", "all_tokens", "=", "[", "]", "\n", "all_masks", "=", "[", "]", "\n", "all_labels", "=", "[", "]", "\n", "for", "_", ",", "obj", "in", "enumerate", "(", "objs", ")", ":", "\n", "        ", "sentence", "=", "obj", "[", "'sentence'", "]", "\n", "tokenized_sentence", "=", "tokenizer", ".", "encode", "(", "sentence", ")", "[", ":", "args", ".", "seq_length", "-", "20", "]", "\n", "obj", "[", "'label_desc'", "]", "=", "label_reverse", "[", "obj", "[", "'label'", "]", "]", "\n", "\n", "if", "few_shot", ":", "\n", "            ", "cur_labels", "=", "random", ".", "sample", "(", "labels", ",", "3", ")", "\n", "while", "obj", "[", "'label_desc'", "]", "in", "cur_labels", ":", "\n", "                ", "cur_labels", "=", "random", ".", "sample", "(", "labels", ",", "3", ")", "\n", "", "cur_labels", ".", "append", "(", "obj", "[", "'label_desc'", "]", ")", "\n", "cur_label", "=", "cur_labels", ".", "index", "(", "obj", "[", "'label_desc'", "]", ")", "\n", "assert", "cur_label", "!=", "-", "1", "\n", "", "else", ":", "\n", "            ", "cur_labels", "=", "labels", "\n", "cur_label", "=", "label_map", "[", "obj", "[", "'label_desc'", "]", "]", "\n", "\n", "", "all_labels", ".", "append", "(", "cur_label", ")", "\n", "\n", "for", "_", ",", "label", "in", "enumerate", "(", "cur_labels", ")", ":", "\n", "            ", "prompt", "=", "\"\u8fd9\u662f\u5173\u4e8e{}\u7684\u6587\u7ae0\uff1a\"", ".", "format", "(", "label", ")", "\n", "prompt_tokens", "=", "tokenizer", ".", "encode", "(", "prompt", ")", "\n", "prompt_len", "=", "len", "(", "prompt_tokens", ")", "\n", "tokens", "=", "prompt_tokens", "+", "tokenized_sentence", "\n", "second_mask", "=", "[", "0", "]", "*", "(", "args", ".", "seq_length", "-", "1", ")", "\n", "for", "idx", "in", "range", "(", "prompt_len", "-", "1", ",", "len", "(", "tokens", ")", "-", "1", ")", ":", "\n", "                ", "second_mask", "[", "idx", "]", "=", "1", "\n", "", "all_masks", ".", "append", "(", "second_mask", ")", "\n", "token_length", "=", "len", "(", "tokens", ")", "\n", "assert", "token_length", "<", "args", ".", "seq_length", "\n", "tokens", ".", "extend", "(", "[", "pad_id", "]", "*", "(", "args", ".", "seq_length", "-", "token_length", ")", ")", "\n", "all_tokens", ".", "append", "(", "tokens", ")", "\n", "\n", "", "", "all_tokens", "=", "torch", ".", "tensor", "(", "all_tokens", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_masks", "=", "torch", ".", "tensor", "(", "all_masks", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "dataset", "=", "TensorDataset", "(", "all_tokens", ",", "all_masks", ")", "\n", "\n", "# Data parallel arguments.", "\n", "world_size", "=", "mpu", ".", "get_data_parallel_world_size", "(", ")", "\n", "rank", "=", "mpu", ".", "get_data_parallel_rank", "(", ")", "\n", "global_batch_size", "=", "args", ".", "batch_size", "*", "world_size", "\n", "num_workers", "=", "args", ".", "num_workers", "\n", "\n", "sampler", "=", "torch", ".", "utils", ".", "data", ".", "SequentialSampler", "(", "dataset", ")", "\n", "batch_sampler", "=", "DistributedBatchSampler", "(", "sampler", "=", "sampler", ",", "\n", "batch_size", "=", "global_batch_size", ",", "\n", "drop_last", "=", "True", ",", "\n", "rank", "=", "rank", ",", "\n", "world_size", "=", "world_size", ")", "\n", "\n", "# Torch dataloader.", "\n", "return", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", ",", "\n", "batch_sampler", "=", "batch_sampler", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "pin_memory", "=", "True", ")", ",", "all_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.evaluate_ocnli": [[342, 400], ["model.eval", "torch.no_grad", "torch.no_grad", "tqdm.tqdm", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "generate_samples.get_batch", "model", "mpu.vocab_parallel_cross_entropy", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.stack().view().cpu().detach().numpy", "torch.stack().view().cpu().detach().numpy", "generate_samples.get_batch", "model", "mpu.vocab_parallel_cross_entropy", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.stack().view().cpu().detach().numpy", "torch.stack().view().cpu().detach().numpy", "generate_samples.get_batch", "model", "mpu.vocab_parallel_cross_entropy", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.stack().view().cpu().detach().numpy", "torch.stack().view().cpu().detach().numpy", "torch.distributed.all_gather", "torch.distributed.all_gather", "x.to", "output[].contiguous().float", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.zeros_like", "torch.zeros_like", "mpu.get_data_parallel_group", "output[].contiguous().float", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.zeros_like", "torch.zeros_like", "mpu.get_data_parallel_group", "output[].contiguous().float", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.zeros_like", "torch.zeros_like", "mpu.get_data_parallel_group", "torch.zeros_like", "torch.zeros_like", "mpu.get_data_parallel_group", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.stack", "torch.stack", "labels.view().cpu().detach().numpy.view().cpu().detach().numpy", "sum", "len", "range", "torch.stack().view().cpu().detach", "torch.stack().view().cpu().detach", "range", "torch.stack().view().cpu().detach", "torch.stack().view().cpu().detach", "range", "torch.stack().view().cpu().detach", "torch.stack().view().cpu().detach", "range", "numpy.argmin", "output[].contiguous", "mpu.get_data_parallel_world_size", "output[].contiguous", "mpu.get_data_parallel_world_size", "output[].contiguous", "mpu.get_data_parallel_world_size", "mpu.get_data_parallel_world_size", "labels.view().cpu().detach().numpy.view().cpu().detach", "numpy.array", "zip", "zip", "torch.stack().view().cpu", "torch.stack().view().cpu", "torch.stack().view().cpu", "torch.stack().view().cpu", "torch.stack().view().cpu", "torch.stack().view().cpu", "labels.view().cpu().detach().numpy.view().cpu", "torch.stack().view", "torch.stack().view", "torch.stack().view", "torch.stack().view", "torch.stack().view", "torch.stack().view", "labels.view().cpu().detach().numpy.view", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.get_batch", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.cross_entropy.vocab_parallel_cross_entropy", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.get_batch", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.cross_entropy.vocab_parallel_cross_entropy", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.get_batch", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.cross_entropy.vocab_parallel_cross_entropy", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_world_size"], ["", "def", "evaluate_ocnli", "(", "model", ",", "dev_dataloader", ",", "device", ",", "args", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "correct", "=", "0", "\n", "total", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch", "in", "tqdm", ".", "tqdm", "(", "dev_dataloader", ")", ":", "\n", "            ", "tokens_1", ",", "masks_1", ",", "tokens_2", ",", "masks_2", ",", "tokens_3", ",", "masks_3", ",", "labels", "=", "[", "x", ".", "to", "(", "device", ")", "for", "x", "in", "batch", "]", "\n", "\n", "tokens", ",", "attention_mask", ",", "position_ids", "=", "get_batch", "(", "tokens_1", ",", "args", ")", "\n", "output", ",", "_", "=", "model", "(", "tokens", ",", "position_ids", ",", "attention_mask", ")", "\n", "\n", "losses", "=", "mpu", ".", "vocab_parallel_cross_entropy", "(", "output", "[", ":", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", ".", "float", "(", ")", ",", "tokens", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "output_1", "=", "torch", ".", "sum", "(", "losses", "*", "masks_1", ",", "1", ")", "/", "torch", ".", "sum", "(", "masks_1", ",", "-", "1", ")", "\n", "\n", "tensor_list", "=", "[", "torch", ".", "zeros_like", "(", "output_1", ")", "for", "_", "in", "range", "(", "mpu", ".", "get_data_parallel_world_size", "(", ")", ")", "]", "\n", "torch", ".", "distributed", ".", "all_gather", "(", "tensor_list", ",", "output_1", ",", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "output_1", "=", "torch", ".", "stack", "(", "tensor_list", ",", "0", ")", ".", "view", "(", "-", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# --------------", "\n", "tokens", ",", "attention_mask", ",", "position_ids", "=", "get_batch", "(", "tokens_2", ",", "args", ")", "\n", "output", ",", "_", "=", "model", "(", "tokens", ",", "position_ids", ",", "attention_mask", ")", "\n", "losses", "=", "mpu", ".", "vocab_parallel_cross_entropy", "(", "output", "[", ":", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", ".", "float", "(", ")", ",", "tokens", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "output_2", "=", "torch", ".", "sum", "(", "losses", "*", "masks_2", ",", "1", ")", "/", "torch", ".", "sum", "(", "masks_2", ",", "-", "1", ")", "\n", "\n", "tensor_list", "=", "[", "torch", ".", "zeros_like", "(", "output_2", ")", "for", "_", "in", "range", "(", "mpu", ".", "get_data_parallel_world_size", "(", ")", ")", "]", "\n", "torch", ".", "distributed", ".", "all_gather", "(", "tensor_list", ",", "output_2", ",", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "output_2", "=", "torch", ".", "stack", "(", "tensor_list", ",", "0", ")", ".", "view", "(", "-", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# ---------------", "\n", "\n", "tokens", ",", "attention_mask", ",", "position_ids", "=", "get_batch", "(", "tokens_3", ",", "args", ")", "\n", "output", ",", "_", "=", "model", "(", "tokens", ",", "position_ids", ",", "attention_mask", ")", "\n", "losses", "=", "mpu", ".", "vocab_parallel_cross_entropy", "(", "output", "[", ":", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", ".", "float", "(", ")", ",", "tokens", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "output_3", "=", "torch", ".", "sum", "(", "losses", "*", "masks_3", ",", "1", ")", "/", "torch", ".", "sum", "(", "masks_3", ",", "-", "1", ")", "\n", "\n", "tensor_list", "=", "[", "torch", ".", "zeros_like", "(", "output_3", ")", "for", "_", "in", "range", "(", "mpu", ".", "get_data_parallel_world_size", "(", ")", ")", "]", "\n", "torch", ".", "distributed", ".", "all_gather", "(", "tensor_list", ",", "output_3", ",", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "output_3", "=", "torch", ".", "stack", "(", "tensor_list", ",", "0", ")", ".", "view", "(", "-", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "\n", "# --------------", "\n", "\n", "tensor_list_labels", "=", "[", "torch", ".", "zeros_like", "(", "labels", ")", "for", "_", "in", "range", "(", "mpu", ".", "get_data_parallel_world_size", "(", ")", ")", "]", "\n", "torch", ".", "distributed", ".", "all_gather", "(", "tensor_list_labels", ",", "labels", ",", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "                ", "labels", "=", "torch", ".", "stack", "(", "tensor_list_labels", ",", "0", ")", "\n", "labels", "=", "labels", ".", "view", "(", "-", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "res", "=", "[", "np", ".", "argmin", "(", "np", ".", "array", "(", "x", ")", ")", "for", "x", "in", "zip", "(", "output_1", ",", "output_2", ",", "output_3", ")", "]", "\n", "res", "=", "[", "x", "==", "y", "for", "x", ",", "y", "in", "zip", "(", "res", ",", "labels", ")", "]", "\n", "correct", "+=", "sum", "(", "res", ")", "\n", "total", "+=", "len", "(", "res", ")", "\n", "\n", "", "", "", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "\"EVAL\"", ",", "correct", ",", "total", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.evaluate": [[401, 436], ["model.eval", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.no_grad", "torch.no_grad", "tqdm.tqdm", "torch.distributed.get_rank", "torch.distributed.get_rank", "range", "print", "generate_samples.get_batch", "model", "mpu.vocab_parallel_cross_entropy", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.stack().view().cpu().detach().numpy", "torch.stack().view().cpu().detach().numpy", "max", "len", "numpy.argmin", "x.to", "output[].contiguous().float", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.zeros_like", "torch.zeros_like", "mpu.get_data_parallel_group", "torch.distributed.get_rank", "torch.distributed.get_rank", "range", "torch.stack().view().cpu().detach", "torch.stack().view().cpu().detach", "res.append", "output[].contiguous", "mpu.get_data_parallel_world_size", "torch.stack().view().cpu", "torch.stack().view().cpu", "torch.stack().view", "torch.stack().view", "torch.stack", "torch.stack"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.get_batch", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.cross_entropy.vocab_parallel_cross_entropy", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_world_size"], ["", "", "def", "evaluate", "(", "model", ",", "dev_dataloader", ",", "all_labels", ",", "device", ",", "args", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "res", "=", "[", "]", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch", "in", "tqdm", ".", "tqdm", "(", "dev_dataloader", ")", ":", "\n", "            ", "tokens", ",", "masks", "=", "[", "x", ".", "to", "(", "device", ")", "for", "x", "in", "batch", "]", "\n", "\n", "tokens", ",", "attention_mask", ",", "position_ids", "=", "get_batch", "(", "tokens", ",", "args", ")", "\n", "output", ",", "_", "=", "model", "(", "tokens", ",", "position_ids", ",", "attention_mask", ")", "\n", "losses", "=", "mpu", ".", "vocab_parallel_cross_entropy", "(", "output", "[", ":", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", ".", "float", "(", ")", ",", "tokens", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "output", "=", "torch", ".", "sum", "(", "losses", "*", "masks", ",", "1", ")", "/", "torch", ".", "sum", "(", "masks", ",", "-", "1", ")", "\n", "\n", "tensor_list", "=", "[", "torch", ".", "zeros_like", "(", "output", ")", "for", "_", "in", "range", "(", "mpu", ".", "get_data_parallel_world_size", "(", ")", ")", "]", "\n", "torch", ".", "distributed", ".", "all_gather", "(", "tensor_list", ",", "output", ",", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "output", "=", "torch", ".", "stack", "(", "tensor_list", ",", "0", ")", ".", "view", "(", "-", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "                ", "for", "v", "in", "output", ":", "\n", "                    ", "res", ".", "append", "(", "v", ")", "\n", "\n", "", "", "", "", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "cnt", "=", "0", "\n", "label_size", "=", "max", "(", "all_labels", ")", "+", "1", "\n", "num_inst", "=", "len", "(", "res", ")", "//", "label_size", "\n", "for", "x", "in", "range", "(", "num_inst", ")", ":", "\n", "            ", "label", "=", "all_labels", "[", "x", "]", "\n", "cur_res", "=", "res", "[", "x", "*", "label_size", ":", "(", "x", "+", "1", ")", "*", "label_size", "]", "\n", "pos", "=", "np", ".", "argmin", "(", "cur_res", ")", "\n", "if", "pos", "==", "label", ":", "\n", "                ", "cnt", "+=", "1", "\n", "", "", "print", "(", "\"EVAL\"", ",", "cnt", ",", "num_inst", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.main": [[437, 476], ["arguments.get_args", "generate_samples.initialize_distributed", "generate_samples.set_random_seed", "data_utils.tokenization_gpt2.GPT2Tokenizer", "torch.cuda.current_device", "torch.cuda.current_device", "generate_samples.setup_model", "os.path.join", "os.path.join", "zero-shot-cls.load_ocnli_data", "zero-shot-cls.evaluate_ocnli", "zero-shot-cls.load_iflytek_data", "zero-shot-cls.evaluate", "zero-shot-cls.load_tnews_data", "zero-shot-cls.evaluate", "print"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.get_args", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.initialize_distributed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.setup_model", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.load_ocnli_data", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.evaluate_ocnli", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.load_iflytek_data", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.evaluate", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.load_tnews_data", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.zero-shot-cls.evaluate"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "\"\"\"Main training program.\"\"\"", "\n", "\n", "# Disable CuDNN.", "\n", "torch", ".", "backends", ".", "cudnn", ".", "enabled", "=", "False", "\n", "\n", "# Arguments.", "\n", "args", "=", "get_args", "(", ")", "\n", "\n", "# Pytorch distributed.", "\n", "initialize_distributed", "(", "args", ")", "\n", "\n", "# Random seeds for reproducability.", "\n", "set_random_seed", "(", "args", ".", "seed", ")", "\n", "\n", "#get the tokenizer", "\n", "tokenizer", "=", "GPT2Tokenizer", "(", "os", ".", "path", ".", "join", "(", "args", ".", "tokenizer_path", ",", "'vocab.json'", ")", ",", "os", ".", "path", ".", "join", "(", "args", ".", "tokenizer_path", ",", "'chinese_vocab.model'", ")", ")", "\n", "\n", "# load data", "\n", "assert", "args", ".", "eval_data_path", "is", "not", "None", "\n", "\n", "device", "=", "torch", ".", "cuda", ".", "current_device", "(", ")", "\n", "args", ".", "eod_token", "=", "tokenizer", ".", "encoder", "[", "'<eod>'", "]", "\n", "\n", "# Model", "\n", "args", ".", "parallel_output", "=", "True", "\n", "model", "=", "setup_model", "(", "args", ")", "\n", "\n", "if", "args", ".", "task", "==", "\"ocnli\"", ":", "\n", "        ", "dev_dataloader", "=", "load_ocnli_data", "(", "args", ".", "eval_data_path", ",", "'dev'", ",", "tokenizer", ")", "\n", "evaluate_ocnli", "(", "model", ",", "dev_dataloader", ",", "device", ",", "args", ")", "\n", "", "elif", "args", ".", "task", "==", "\"iflytek\"", ":", "\n", "        ", "dev_dataloader", ",", "all_labels", "=", "load_iflytek_data", "(", "args", ".", "eval_data_path", ",", "'dev'", ",", "tokenizer", ")", "\n", "evaluate", "(", "model", ",", "dev_dataloader", ",", "all_labels", ",", "device", ",", "args", ")", "\n", "", "elif", "args", ".", "task", "==", "\"tnews\"", ":", "\n", "        ", "dev_dataloader", ",", "all_labels", "=", "load_tnews_data", "(", "args", ".", "eval_data_path", ",", "'dev'", ",", "tokenizer", ")", "\n", "evaluate", "(", "model", ",", "dev_dataloader", ",", "all_labels", ",", "device", ",", "args", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Unknown task!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.add_model_config_args": [[23, 68], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["def", "add_model_config_args", "(", "parser", ")", ":", "\n", "    ", "\"\"\"Model arguments\"\"\"", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'model'", ",", "'model configuration'", ")", "\n", "\n", "group", ".", "add_argument", "(", "'--pretrained-bert'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use a pretrained bert-large-uncased model instead'", "\n", "'of initializing from scratch. See '", "\n", "'--tokenizer-model-type to specify which pretrained '", "\n", "'BERT model to use'", ")", "\n", "group", ".", "add_argument", "(", "'--attention-dropout'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "group", ".", "add_argument", "(", "'--num-attention-heads'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "\n", "help", "=", "'num of transformer attention heads'", ")", "\n", "group", ".", "add_argument", "(", "'--hidden-size'", ",", "type", "=", "int", ",", "default", "=", "1024", ",", "\n", "help", "=", "'tansformer hidden size'", ")", "\n", "group", ".", "add_argument", "(", "'--intermediate-size'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'transformer embedding dimension for FFN'", "\n", "'set to 4*`--hidden-size` if it is None'", ")", "\n", "group", ".", "add_argument", "(", "'--num-layers'", ",", "type", "=", "int", ",", "default", "=", "24", ",", "\n", "help", "=", "'num decoder layers'", ")", "\n", "group", ".", "add_argument", "(", "'--layernorm-epsilon'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "\n", "help", "=", "'layer norm epsilon'", ")", "\n", "group", ".", "add_argument", "(", "'--hidden-dropout'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'dropout probability for hidden state transformer'", ")", "\n", "group", ".", "add_argument", "(", "'--max-position-embeddings'", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "'maximum number of position embeddings to use'", ")", "\n", "group", ".", "add_argument", "(", "'--vocab-size'", ",", "type", "=", "int", ",", "default", "=", "30522", ",", "\n", "help", "=", "'vocab size to use for non-character-level '", "\n", "'tokenization. This value will only be used when '", "\n", "'creating a tokenizer'", ")", "\n", "group", ".", "add_argument", "(", "'--deep-init'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'initialize bert model similar to gpt2 model.'", "\n", "'scales initialization of projection layers by a '", "\n", "'factor of 1/sqrt(2N). Necessary to train bert '", "\n", "'models larger than BERT-Large.'", ")", "\n", "group", ".", "add_argument", "(", "'--make-vocab-size-divisible-by'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "'Pad the vocab size to be divisible by this value.'", "\n", "'This is added for computational efficieny reasons.'", ")", "\n", "group", ".", "add_argument", "(", "'--cpu-optimizer'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Run optimizer on CPU'", ")", "\n", "group", ".", "add_argument", "(", "'--cpu_torch_adam'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use Torch Adam as optimizer on CPU.'", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.add_fp16_config_args": [[70, 97], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_fp16_config_args", "(", "parser", ")", ":", "\n", "    ", "\"\"\"Mixed precision arguments.\"\"\"", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'fp16'", ",", "'fp16 configurations'", ")", "\n", "\n", "group", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Run model in fp16 mode'", ")", "\n", "group", ".", "add_argument", "(", "'--fp32-embedding'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'embedding in fp32'", ")", "\n", "group", ".", "add_argument", "(", "'--fp32-layernorm'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'layer norm in fp32'", ")", "\n", "group", ".", "add_argument", "(", "'--fp32-tokentypes'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'embedding token types in fp32'", ")", "\n", "group", ".", "add_argument", "(", "'--fp32-allreduce'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'all-reduce in fp32'", ")", "\n", "group", ".", "add_argument", "(", "'--hysteresis'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "'hysteresis for dynamic loss scaling'", ")", "\n", "group", ".", "add_argument", "(", "'--loss-scale'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "\n", "help", "=", "'Static loss scaling, positive power of 2 '", "\n", "'values can improve fp16 convergence. If None, dynamic'", "\n", "'loss scaling is used.'", ")", "\n", "group", ".", "add_argument", "(", "'--loss-scale-window'", ",", "type", "=", "float", ",", "default", "=", "1000", ",", "\n", "help", "=", "'Window over which to raise/lower dynamic scale'", ")", "\n", "group", ".", "add_argument", "(", "'--min-scale'", ",", "type", "=", "float", ",", "default", "=", "1", ",", "\n", "help", "=", "'Minimum loss scale for dynamic loss scale'", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.add_training_args": [[99, 175], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_training_args", "(", "parser", ")", ":", "\n", "    ", "\"\"\"Training arguments.\"\"\"", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'train'", ",", "'training configurations'", ")", "\n", "\n", "group", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "'Data Loader batch size'", ")", "\n", "group", ".", "add_argument", "(", "'--weight-decay'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "\n", "help", "=", "'weight decay coefficient for L2 regularization'", ")", "\n", "group", ".", "add_argument", "(", "'--checkpoint-activations'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'checkpoint activation to allow for training '", "\n", "'with larger models and sequences'", ")", "\n", "group", ".", "add_argument", "(", "'--checkpoint-num-layers'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'chunk size (number of layers) for checkpointing'", ")", "\n", "group", ".", "add_argument", "(", "'--clip-grad'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'gradient clipping'", ")", "\n", "group", ".", "add_argument", "(", "'--train-iters'", ",", "type", "=", "int", ",", "default", "=", "1000000", ",", "\n", "help", "=", "'total number of iterations to train over all training runs'", ")", "\n", "group", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'report interval'", ")", "\n", "group", ".", "add_argument", "(", "'--exit-interval'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'Exit the program after this many new iterations.'", ")", "\n", "\n", "group", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1234", ",", "\n", "help", "=", "'random seed'", ")", "\n", "# Batch prodecuer arguments", "\n", "group", ".", "add_argument", "(", "'--reset-position-ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Reset posistion ids after end-of-document token.'", ")", "\n", "group", ".", "add_argument", "(", "'--reset-attention-mask'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Reset self attention maske after '", "\n", "'end-of-document token.'", ")", "\n", "\n", "# Learning rate.", "\n", "group", ".", "add_argument", "(", "'--lr-decay-iters'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'number of iterations to decay LR over,'", "\n", "' If None defaults to `--train-iters`*`--epochs`'", ")", "\n", "group", ".", "add_argument", "(", "'--lr-decay-style'", ",", "type", "=", "str", ",", "default", "=", "'linear'", ",", "\n", "choices", "=", "[", "'constant'", ",", "'linear'", ",", "'cosine'", ",", "'exponential'", "]", ",", "\n", "help", "=", "'learning rate decay function'", ")", "\n", "group", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "1.0e-4", ",", "\n", "help", "=", "'initial learning rate'", ")", "\n", "group", ".", "add_argument", "(", "'--warmup'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "\n", "help", "=", "'percentage of data to warmup on (.01 = 1% of all '", "\n", "'training iters). Default 0.01'", ")", "\n", "# model checkpointing", "\n", "group", ".", "add_argument", "(", "'--save'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Output directory to save checkpoints to.'", ")", "\n", "group", ".", "add_argument", "(", "'--save-interval'", ",", "type", "=", "int", ",", "default", "=", "5000", ",", "\n", "help", "=", "'number of iterations between saves'", ")", "\n", "group", ".", "add_argument", "(", "'--no-save-optim'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Do not save current optimizer.'", ")", "\n", "group", ".", "add_argument", "(", "'--no-save-rng'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Do not save current rng state.'", ")", "\n", "group", ".", "add_argument", "(", "'--load'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Path to a directory containing a model checkpoint.'", ")", "\n", "group", ".", "add_argument", "(", "'--no-load-optim'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Do not load optimizer when loading checkpoint.'", ")", "\n", "group", ".", "add_argument", "(", "'--no-load-rng'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Do not load rng state when loading checkpoint.'", ")", "\n", "group", ".", "add_argument", "(", "'--finetune'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Load model for finetuning. Do not load optimizer '", "\n", "'or rng state from checkpoint and set iteration to 0. '", "\n", "'Assumed when loading a release checkpoint.'", ")", "\n", "group", ".", "add_argument", "(", "'--resume-dataloader'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Resume the dataloader when resuming training. '", "\n", "'Does not apply to tfrecords dataloader, try resuming'", "\n", "'with a different seed in this case.'", ")", "\n", "# distributed training args", "\n", "group", ".", "add_argument", "(", "'--distributed-backend'", ",", "default", "=", "'nccl'", ",", "\n", "help", "=", "'which backend to use for distributed '", "\n", "'training. One of [gloo, nccl]'", ")", "\n", "\n", "group", ".", "add_argument", "(", "'--local_rank'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'local rank passed from distributed launcher'", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.add_evaluation_args": [[177, 211], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_evaluation_args", "(", "parser", ")", ":", "\n", "    ", "\"\"\"Evaluation arguments.\"\"\"", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'validation'", ",", "'validation configurations'", ")", "\n", "\n", "group", ".", "add_argument", "(", "'--eval-batch-size'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'Data Loader batch size for evaluation datasets.'", "\n", "'Defaults to `--batch-size`'", ")", "\n", "group", ".", "add_argument", "(", "'--eval-iters'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'number of iterations to run for evaluation'", "\n", "'validation/test for'", ")", "\n", "group", ".", "add_argument", "(", "'--eval-interval'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "\n", "help", "=", "'interval between running evaluation on validation set'", ")", "\n", "group", ".", "add_argument", "(", "'--eval-seq-length'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'Maximum sequence length to process for '", "\n", "'evaluation. Defaults to `--seq-length`'", ")", "\n", "group", ".", "add_argument", "(", "'--eval-max-preds-per-seq'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'Maximum number of predictions to use for '", "\n", "'evaluation. Defaults to '", "\n", "'math.ceil(`--eval-seq-length`*.15/10)*10'", ")", "\n", "group", ".", "add_argument", "(", "'--overlapping-eval'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'sliding window for overlapping eval '", ")", "\n", "group", ".", "add_argument", "(", "'--cloze-eval'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Evaluation dataset from `--valid-data` is a cloze task'", ")", "\n", "group", ".", "add_argument", "(", "'--eval-hf'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'perform evaluation with huggingface openai model.'", "\n", "'use `--load` to specify weights path to be loaded'", ")", "\n", "group", ".", "add_argument", "(", "'--load-openai'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load openai weights into our model. Use `--load` '", "\n", "'to specify weights path to be loaded'", ")", "\n", "group", ".", "add_argument", "(", "'--eval-data-path'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "group", ".", "add_argument", "(", "'--task'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.add_text_generate_args": [[212, 222], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_text_generate_args", "(", "parser", ")", ":", "\n", "    ", "\"\"\"Text generate arguments.\"\"\"", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Text generation'", ",", "'configurations'", ")", "\n", "group", ".", "add_argument", "(", "\"--temperature\"", ",", "type", "=", "float", ",", "default", "=", "1.0", ")", "\n", "group", ".", "add_argument", "(", "\"--top_p\"", ",", "type", "=", "float", ",", "default", "=", "0.0", ")", "\n", "group", ".", "add_argument", "(", "\"--top_k\"", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "group", ".", "add_argument", "(", "\"--out-seq-length\"", ",", "type", "=", "int", ",", "default", "=", "256", ")", "\n", "group", ".", "add_argument", "(", "\"--input-text\"", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.add_data_args": [[224, 313], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_data_args", "(", "parser", ")", ":", "\n", "    ", "\"\"\"Train/valid/test data arguments.\"\"\"", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'data'", ",", "'data configurations'", ")", "\n", "group", ".", "add_argument", "(", "'--data-impl'", ",", "type", "=", "str", ",", "default", "=", "'infer'", ",", "\n", "choices", "=", "[", "'lazy'", ",", "'cached'", ",", "'mmap'", ",", "'infer'", "]", ",", "\n", "help", "=", "'Implementation of indexed datasets.'", ")", "\n", "group", ".", "add_argument", "(", "'--mmap-warmup'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Warm up mmap files.'", ")", "\n", "group", ".", "add_argument", "(", "'--model-parallel-size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'size of the model parallel.'", ")", "\n", "group", ".", "add_argument", "(", "'--shuffle'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Shuffle data. Shuffling is deterministic '", "\n", "'based on seed and current epoch.'", ")", "\n", "#group.add_argument('--train-data', nargs='+', default=None,", "\n", "#                   help='Whitespace separated filenames or corpora names '", "\n", "#                   'for training.')", "\n", "group", ".", "add_argument", "(", "'--data-path'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Path to combined dataset to split.'", ")", "\n", "\n", "group", ".", "add_argument", "(", "'--use-npy-data-loader'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use the numpy data loader. If set, then'", "\n", "'train-data-path, val-data-path, and test-data-path'", "\n", "'should also be provided.'", ")", "\n", "group", ".", "add_argument", "(", "'--train-data-path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'path to the training data'", ")", "\n", "group", ".", "add_argument", "(", "'--val-data-path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'path to the validation data'", ")", "\n", "group", ".", "add_argument", "(", "'--test-data-path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'path to the test data'", ")", "\n", "group", ".", "add_argument", "(", "'--input-data-sizes-file'", ",", "type", "=", "str", ",", "default", "=", "'sizes.txt'", ",", "\n", "help", "=", "'the filename containing all the shards sizes'", ")", "\n", "\n", "group", ".", "add_argument", "(", "'--delim'", ",", "default", "=", "','", ",", "\n", "help", "=", "'delimiter used to parse csv data files'", ")", "\n", "group", ".", "add_argument", "(", "'--text-key'", ",", "default", "=", "'sentence'", ",", "\n", "help", "=", "'key to use to extract text from json/csv'", ")", "\n", "group", ".", "add_argument", "(", "'--eval-text-key'", ",", "default", "=", "None", ",", "\n", "help", "=", "'key to use to extract text from '", "\n", "'json/csv evaluation datasets'", ")", "\n", "group", ".", "add_argument", "(", "'--valid-data'", ",", "nargs", "=", "'*'", ",", "default", "=", "None", ",", "\n", "help", "=", "\"\"\"Filename for validation data.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'--split'", ",", "default", "=", "'1000,1,1'", ",", "\n", "help", "=", "'comma-separated list of proportions for training,'", "\n", "' validation, and test split'", ")", "\n", "group", ".", "add_argument", "(", "'--test-data'", ",", "nargs", "=", "'*'", ",", "default", "=", "None", ",", "\n", "help", "=", "\"\"\"Filename for testing\"\"\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'--lazy-loader'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'whether to lazy read the data set'", ")", "\n", "group", ".", "add_argument", "(", "'--loose-json'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use loose json (one json-formatted string per '", "\n", "'newline), instead of tight json (data file is one '", "\n", "'json string)'", ")", "\n", "group", ".", "add_argument", "(", "'--presplit-sentences'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Dataset content consists of documents where '", "\n", "'each document consists of newline separated sentences'", ")", "\n", "group", ".", "add_argument", "(", "'--num-workers'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "\"\"\"Number of workers to use for dataloading\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'--tokenizer-model-type'", ",", "type", "=", "str", ",", "\n", "default", "=", "'bert-large-uncased'", ",", "\n", "help", "=", "\"Model type to use for sentencepiece tokenization \\\n                       (one of ['bpe', 'char', 'unigram', 'word']) or \\\n                       bert vocab to use for BertWordPieceTokenizer (one of \\\n                       ['bert-large-uncased', 'bert-large-cased', etc.])\"", ")", "\n", "group", ".", "add_argument", "(", "'--tokenizer-path'", ",", "type", "=", "str", ",", "default", "=", "'tokenizer.model'", ",", "\n", "help", "=", "'path used to save/load sentencepiece tokenization '", "\n", "'models'", ")", "\n", "group", ".", "add_argument", "(", "'--tokenizer-type'", ",", "type", "=", "str", ",", "\n", "default", "=", "'BertWordPieceTokenizer'", ",", "\n", "choices", "=", "[", "'CharacterLevelTokenizer'", ",", "\n", "'SentencePieceTokenizer'", ",", "\n", "'BertWordPieceTokenizer'", ",", "\n", "'GPT2BPETokenizer'", "]", ",", "\n", "help", "=", "'what type of tokenizer to use'", ")", "\n", "group", ".", "add_argument", "(", "\"--cache-dir\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Where to store pre-trained BERT downloads\"", ")", "\n", "group", ".", "add_argument", "(", "'--use-tfrecords'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load `--train-data`, `--valid-data`, '", "\n", "'`--test-data` from BERT tf records instead of '", "\n", "'normal data pipeline'", ")", "\n", "group", ".", "add_argument", "(", "'--seq-length'", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "\"Maximum sequence length to process\"", ")", "\n", "group", ".", "add_argument", "(", "'--max-preds-per-seq'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'Maximum number of predictions to use per sequence.'", "\n", "'Defaults to math.ceil(`--seq-length`*.15/10)*10.'", "\n", "'MUST BE SPECIFIED IF `--use-tfrecords` is True.'", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.get_args": [[314, 368], ["argparse.ArgumentParser", "arguments.add_model_config_args", "arguments.add_fp16_config_args", "arguments.add_training_args", "arguments.add_evaluation_args", "arguments.add_text_generate_args", "arguments.add_data_args", "add_data_args.parse_args", "torch.cuda.is_available", "int", "int", "os.getenv", "min", "print", "os.getenv", "os.getenv", "int", "int", "int", "int", "print", "os.getenv", "os.getenv", "os.getenv", "os.getenv", "print"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.add_model_config_args", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.add_fp16_config_args", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.add_training_args", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.add_evaluation_args", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.add_text_generate_args", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.add_data_args"], ["", "def", "get_args", "(", ")", ":", "\n", "    ", "\"\"\"Parse all the args.\"\"\"", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'PyTorch BERT Model'", ")", "\n", "parser", "=", "add_model_config_args", "(", "parser", ")", "\n", "parser", "=", "add_fp16_config_args", "(", "parser", ")", "\n", "parser", "=", "add_training_args", "(", "parser", ")", "\n", "parser", "=", "add_evaluation_args", "(", "parser", ")", "\n", "parser", "=", "add_text_generate_args", "(", "parser", ")", "\n", "parser", "=", "add_data_args", "(", "parser", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "not", "args", ".", "data_path", "and", "not", "args", ".", "train_data_path", ":", "\n", "        ", "print", "(", "'WARNING: No training data specified'", ")", "\n", "\n", "", "args", ".", "cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "\n", "args", ".", "rank", "=", "int", "(", "os", ".", "getenv", "(", "'RANK'", ",", "'0'", ")", ")", "\n", "args", ".", "world_size", "=", "int", "(", "os", ".", "getenv", "(", "\"WORLD_SIZE\"", ",", "'1'", ")", ")", "\n", "\n", "if", "os", ".", "getenv", "(", "'OMPI_COMM_WORLD_LOCAL_RANK'", ")", ":", "\n", "# We are using (OpenMPI) mpirun for launching distributed data parallel processes", "\n", "        ", "local_rank", "=", "int", "(", "os", ".", "getenv", "(", "'OMPI_COMM_WORLD_LOCAL_RANK'", ")", ")", "\n", "local_size", "=", "int", "(", "os", ".", "getenv", "(", "'OMPI_COMM_WORLD_LOCAL_SIZE'", ")", ")", "\n", "\n", "# Possibly running with Slurm", "\n", "num_nodes", "=", "int", "(", "os", ".", "getenv", "(", "'SLURM_JOB_NUM_NODES'", ",", "'1'", ")", ")", "\n", "nodeid", "=", "int", "(", "os", ".", "getenv", "(", "'SLURM_NODEID'", ",", "'0'", ")", ")", "\n", "\n", "args", ".", "local_rank", "=", "local_rank", "\n", "args", ".", "rank", "=", "nodeid", "*", "local_size", "+", "local_rank", "\n", "args", ".", "world_size", "=", "num_nodes", "*", "local_size", "\n", "\n", "", "args", ".", "model_parallel_size", "=", "min", "(", "args", ".", "model_parallel_size", ",", "args", ".", "world_size", ")", "\n", "if", "args", ".", "rank", "==", "0", ":", "\n", "        ", "print", "(", "'using world size: {} and model-parallel size: {} '", ".", "format", "(", "\n", "args", ".", "world_size", ",", "args", ".", "model_parallel_size", ")", ")", "\n", "\n", "", "args", ".", "dynamic_loss_scale", "=", "False", "\n", "if", "args", ".", "loss_scale", "is", "None", ":", "\n", "        ", "args", ".", "dynamic_loss_scale", "=", "True", "\n", "if", "args", ".", "rank", "==", "0", ":", "\n", "            ", "print", "(", "' > using dynamic loss scaling'", ")", "\n", "\n", "# The args fp32_* or fp16_* meant to be active when the", "\n", "# args fp16 is set. So the default behaviour should all", "\n", "# be false.", "\n", "", "", "if", "not", "args", ".", "fp16", ":", "\n", "        ", "args", ".", "fp32_embedding", "=", "False", "\n", "args", ".", "fp32_tokentypes", "=", "False", "\n", "args", ".", "fp32_layernorm", "=", "False", "\n", "\n", "", "return", "args", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.get_masks_and_position_ids": [[39, 90], ["data.size", "torch.tril().view", "torch.tril().view", "torch.ones", "torch.ones", "torch.arange", "torch.arange", "position_ids.clone.unsqueeze().expand_as", "data.size", "position_ids.clone.clone", "range", "torch.tril", "torch.tril", "position_ids.clone.unsqueeze", "range", "torch.ones", "torch.ones", "eod_index.clone.clone", "eod_index.clone.size"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["def", "get_masks_and_position_ids", "(", "data", ",", "\n", "eod_token", ",", "\n", "reset_position_ids", ",", "\n", "reset_attention_mask", ")", ":", "\n", "# Extract batch size and sequence length.", "\n", "    ", "batch_size", ",", "seq_length", "=", "data", ".", "size", "(", ")", "\n", "\n", "# Attention mask (lower triangular).", "\n", "if", "reset_attention_mask", ":", "\n", "        ", "att_mask_batch", "=", "batch_size", "\n", "", "else", ":", "\n", "        ", "att_mask_batch", "=", "1", "\n", "", "attention_mask", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "att_mask_batch", ",", "seq_length", ",", "seq_length", ")", ",", "device", "=", "data", ".", "device", ")", ")", ".", "view", "(", "\n", "att_mask_batch", ",", "1", ",", "seq_length", ",", "seq_length", ")", "\n", "\n", "# Loss mask.", "\n", "loss_mask", "=", "torch", ".", "ones", "(", "data", ".", "size", "(", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "data", ".", "device", ")", "\n", "loss_mask", "[", "data", "==", "eod_token", "]", "=", "0.0", "\n", "\n", "# Position ids.", "\n", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "data", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "data", ")", "\n", "# We need to clone as the ids will be modifed based on batch index.", "\n", "if", "reset_position_ids", ":", "\n", "        ", "position_ids", "=", "position_ids", ".", "clone", "(", ")", "\n", "\n", "", "if", "reset_position_ids", "or", "reset_attention_mask", ":", "\n", "# Loop through the batches:", "\n", "        ", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "\n", "# Find indecies where EOD token is.", "\n", "            ", "eod_index", "=", "position_ids", "[", "b", ",", "data", "[", "b", "]", "==", "eod_token", "]", "\n", "# Detach indecies from positions if going to modify positions.", "\n", "if", "reset_position_ids", ":", "\n", "                ", "eod_index", "=", "eod_index", ".", "clone", "(", ")", "\n", "\n", "# Loop through EOD indecies:", "\n", "", "prev_index", "=", "0", "\n", "for", "j", "in", "range", "(", "eod_index", ".", "size", "(", ")", "[", "0", "]", ")", ":", "\n", "                ", "i", "=", "eod_index", "[", "j", "]", "\n", "# Mask attention loss.", "\n", "if", "reset_attention_mask", ":", "\n", "                    ", "attention_mask", "[", "b", ",", "0", ",", "(", "i", "+", "1", ")", ":", ",", ":", "(", "i", "+", "1", ")", "]", "=", "0", "\n", "# Reset positions.", "\n", "", "if", "reset_position_ids", ":", "\n", "                    ", "position_ids", "[", "b", ",", "(", "i", "+", "1", ")", ":", "]", "-=", "(", "i", "+", "1", "-", "prev_index", ")", "\n", "prev_index", "=", "i", "+", "1", "\n", "\n", "", "", "", "", "return", "attention_mask", ",", "loss_mask", ",", "position_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.initialize_distributed": [[92, 113], ["torch.cuda.set_device", "torch.cuda.set_device", "os.getenv", "os.getenv", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "mpu.initialize_model_parallel", "torch.cuda.device_count", "torch.cuda.device_count"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel"], ["", "def", "initialize_distributed", "(", "args", ")", ":", "\n", "    ", "\"\"\"Initialize torch.distributed.\"\"\"", "\n", "\n", "# Manually set the device ids.", "\n", "device", "=", "args", ".", "rank", "%", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "if", "args", ".", "local_rank", "is", "not", "None", ":", "\n", "        ", "device", "=", "args", ".", "local_rank", "\n", "", "torch", ".", "cuda", ".", "set_device", "(", "device", ")", "\n", "# Call the init process", "\n", "init_method", "=", "'tcp://'", "\n", "master_ip", "=", "os", ".", "getenv", "(", "'MASTER_ADDR'", ",", "'localhost'", ")", "\n", "master_port", "=", "os", ".", "getenv", "(", "'MASTER_PORT'", ",", "'6000'", ")", "\n", "#init_method += master_ip + ':' + master_port", "\n", "init_method", "+=", "master_ip", "+", "':'", "+", "'12580'", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "\n", "backend", "=", "args", ".", "distributed_backend", ",", "\n", "world_size", "=", "args", ".", "world_size", ",", "rank", "=", "args", ".", "rank", ",", "\n", "init_method", "=", "init_method", ")", "\n", "\n", "# Set the model-parallel / data-parallel communicators.", "\n", "mpu", ".", "initialize_model_parallel", "(", "args", ".", "model_parallel_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.set_random_seed": [[114, 122], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "mpu.model_parallel_cuda_manual_seed"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.model_parallel_cuda_manual_seed"], ["", "def", "set_random_seed", "(", "seed", ")", ":", "\n", "    ", "\"\"\"Set random seed for reproducability.\"\"\"", "\n", "\n", "if", "seed", "is", "not", "None", "and", "seed", ">", "0", ":", "\n", "        ", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "mpu", ".", "model_parallel_cuda_manual_seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.get_batch": [[123, 136], ["tokens.to.view().contiguous", "tokens.to.to", "generate_samples.get_masks_and_position_ids", "tokens.to.view"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.get_masks_and_position_ids"], ["", "", "def", "get_batch", "(", "context_tokens", ",", "device", ",", "args", ")", ":", "\n", "    ", "tokens", "=", "context_tokens", "\n", "tokens", "=", "tokens", ".", "view", "(", "args", ".", "batch_size", ",", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "tokens", "=", "tokens", ".", "to", "(", "device", ")", "\n", "\n", "# Get the masks and postition ids.", "\n", "attention_mask", ",", "loss_mask", ",", "position_ids", "=", "get_masks_and_position_ids", "(", "\n", "tokens", ",", "\n", "args", ".", "eod_token", ",", "\n", "args", ".", "reset_position_ids", ",", "\n", "args", ".", "reset_attention_mask", ")", "\n", "\n", "return", "tokens", ",", "attention_mask", ",", "position_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.top_k_logits": [[137, 163], ["float", "logits.view().contiguous.view().contiguous", "torch.sort", "torch.sort", "torch.cumsum", "torch.cumsum", "sorted_indices_to_remove[].clone", "logits.view().contiguous.view().contiguous", "torch.softmax", "logits.view().contiguous.view", "logits.view().contiguous.view", "torch.topk", "torch.topk", "logits.view().contiguous.size"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.cumsum", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.cumsum", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "def", "top_k_logits", "(", "logits", ",", "top_k", "=", "0", ",", "top_p", "=", "0.0", ",", "filter_value", "=", "-", "float", "(", "'Inf'", ")", ")", ":", "\n", "# This function has been mostly taken from huggingface conversational ai code at", "\n", "# https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313", "\n", "\n", "    ", "if", "top_k", ">", "0", ":", "\n", "# Remove all tokens with a probability less than the last token of the top-k", "\n", "        ", "indices_to_remove", "=", "logits", "<", "torch", ".", "topk", "(", "logits", ",", "top_k", ")", "[", "0", "]", "[", "...", ",", "-", "1", ",", "None", "]", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "\n", "", "if", "top_p", ">", "0.0", ":", "\n", "#convert to 1D", "\n", "        ", "logits", "=", "logits", ".", "view", "(", "logits", ".", "size", "(", ")", "[", "1", "]", ")", ".", "contiguous", "(", ")", "\n", "sorted_logits", ",", "sorted_indices", "=", "torch", ".", "sort", "(", "logits", ",", "descending", "=", "True", ")", "\n", "cumulative_probs", "=", "torch", ".", "cumsum", "(", "F", ".", "softmax", "(", "sorted_logits", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Remove tokens with cumulative probability above the threshold", "\n", "sorted_indices_to_remove", "=", "cumulative_probs", ">", "top_p", "\n", "# Shift the indices to the right to keep also the first token above the threshold", "\n", "sorted_indices_to_remove", "[", "...", ",", "1", ":", "]", "=", "sorted_indices_to_remove", "[", "...", ",", ":", "-", "1", "]", ".", "clone", "(", ")", "\n", "sorted_indices_to_remove", "[", "...", ",", "0", "]", "=", "0", "\n", "indices_to_remove", "=", "sorted_indices", "[", "sorted_indices_to_remove", "]", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "#going back to 2D", "\n", "logits", "=", "logits", ".", "view", "(", "1", ",", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.generate_samples": [[165, 276], ["model.eval", "torch.no_grad", "torch.no_grad", "torch.distributed.barrier", "torch.distributed.barrier", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.distributed.broadcast", "torch.distributed.broadcast", "terminate_runs_tensor[].item", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.distributed.broadcast", "torch.distributed.broadcast", "torch.distributed.broadcast", "torch.distributed.broadcast", "context_length_tensor[].item", "generate_samples.get_batch", "time.time", "torch.distributed.barrier", "torch.distributed.barrier", "mpu.get_model_parallel_rank", "tokenizer.encode", "len", "mpu.get_model_parallel_src_rank", "tokenizer.encode.extend", "mpu.get_model_parallel_src_rank", "mpu.get_model_parallel_src_rank", "generate_samples.top_k_logits", "torch.softmax", "torch.multinomial", "torch.multinomial", "torch.distributed.broadcast", "torch.distributed.broadcast", "tokens.view().contiguous", "tokenizer.decode", "tokenizer.decode.find", "mpu.get_model_parallel_rank", "os.system", "print", "print", "tokens.view().contiguous", "tokenizer.decode", "print", "mpu.get_model_parallel_group", "open().read().strip", "input", "tokenizer.encode", "len", "mpu.get_model_parallel_group", "mpu.get_model_parallel_group", "mpu.get_model_parallel_group", "model", "model", "mpu.get_model_parallel_src_rank", "tokens.view().contiguous.tolist", "os.system", "print", "print", "print", "tokens.view().contiguous.tolist", "mpu.get_model_parallel_group", "print", "input", "print", "x.half", "mpu.get_model_parallel_group", "tokens.view", "mpu.get_model_parallel_rank", "tokens.view", "len", "tokenizer.decode.find", "open().read", "len", "tokenizer.decode.find", "time.time", "time.time", "open"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.get_batch", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_src_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_src_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_src_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.top_k_logits", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.decode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.decode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_src_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank"], ["", "def", "generate_samples", "(", "model", ",", "tokenizer", ",", "args", ",", "device", ")", ":", "\n", "\n", "    ", "context_count", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "torch", ".", "distributed", ".", "barrier", "(", "group", "=", "mpu", ".", "get_model_parallel_group", "(", ")", ")", "\n", "terminate_runs", "=", "0", "\n", "\n", "if", "mpu", ".", "get_model_parallel_rank", "(", ")", "==", "0", ":", "\n", "                ", "if", "args", ".", "input_text", ":", "\n", "                    ", "raw_text", "=", "open", "(", "args", ".", "input_text", ")", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "", "else", ":", "\n", "                    ", "raw_text", "=", "input", "(", "\"\\nContext prompt (stop to exit) >>> \"", ")", "\n", "while", "not", "raw_text", ":", "\n", "                        ", "print", "(", "'Prompt should not be empty!'", ")", "\n", "raw_text", "=", "input", "(", "\"\\nContext prompt (stop to exit) >>> \"", ")", "\n", "\n", "", "", "if", "\"stop\"", "in", "raw_text", ":", "\n", "                    ", "terminate_runs", "=", "1", "\n", "", "else", ":", "\n", "#context_tokens = tokenizer.EncodeAsIds(raw_text).tokenization", "\n", "                    ", "context_tokens", "=", "tokenizer", ".", "encode", "(", "raw_text", ")", "\n", "context_length", "=", "len", "(", "context_tokens", ")", "\n", "\n", "if", "context_length", ">=", "args", ".", "seq_length", "//", "2", ":", "\n", "                        ", "print", "(", "\"\\nContext length\"", ",", "context_length", ",", "\"\\nPlease give smaller context (half of the sequence length)!\"", ")", "\n", "continue", "\n", "", "", "", "else", ":", "\n", "#context_tokens = tokenizer.EncodeAsIds(\"EMPTY TEXT\").tokenization", "\n", "                ", "context_tokens", "=", "tokenizer", ".", "encode", "(", "\"\u7a7a\u6587\u672c\"", ")", "\n", "context_length", "=", "len", "(", "context_tokens", ")", "\n", "\n", "", "terminate_runs_tensor", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "[", "terminate_runs", "]", ")", "\n", "torch", ".", "distributed", ".", "broadcast", "(", "terminate_runs_tensor", ",", "mpu", ".", "get_model_parallel_src_rank", "(", ")", ",", "group", "=", "mpu", ".", "get_model_parallel_group", "(", ")", ")", "\n", "terminate_runs", "=", "terminate_runs_tensor", "[", "0", "]", ".", "item", "(", ")", "\n", "\n", "if", "terminate_runs", "==", "1", ":", "\n", "                ", "return", "\n", "\n", "", "pad_id", "=", "tokenizer", ".", "encoder", "[", "'<pad>'", "]", "\n", "args", ".", "eod_token", "=", "tokenizer", ".", "encoder", "[", "'<eod>'", "]", "\n", "if", "context_length", "<", "args", ".", "seq_length", ":", "\n", "                ", "context_tokens", ".", "extend", "(", "[", "pad_id", "]", "*", "(", "args", ".", "seq_length", "-", "context_length", ")", ")", "\n", "\n", "", "context_tokens_tensor", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "context_tokens", ")", "\n", "context_length_tensor", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "[", "context_length", "]", ")", "\n", "\n", "torch", ".", "distributed", ".", "broadcast", "(", "context_length_tensor", ",", "mpu", ".", "get_model_parallel_src_rank", "(", ")", ",", "group", "=", "mpu", ".", "get_model_parallel_group", "(", ")", ")", "\n", "torch", ".", "distributed", ".", "broadcast", "(", "context_tokens_tensor", ",", "mpu", ".", "get_model_parallel_src_rank", "(", ")", ",", "group", "=", "mpu", ".", "get_model_parallel_group", "(", ")", ")", "\n", "\n", "context_length", "=", "context_length_tensor", "[", "0", "]", ".", "item", "(", ")", "\n", "tokens", ",", "attention_mask", ",", "position_ids", "=", "get_batch", "(", "context_tokens_tensor", ",", "device", ",", "args", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "counter", "=", "0", "\n", "org_context_length", "=", "context_length", "\n", "\n", "past_key_values", "=", "None", "\n", "while", "counter", "<", "(", "org_context_length", "+", "args", ".", "out_seq_length", ")", ":", "\n", "                ", "if", "counter", "==", "0", ":", "\n", "                    ", "logits", ",", "past_key_values", "=", "model", "(", "tokens", "[", ":", ",", ":", "context_length", "]", ",", "position_ids", "[", ":", ",", ":", "context_length", "]", ",", "attention_mask", "[", ":", ",", ":", ",", ":", "context_length", ",", ":", "context_length", "]", ",", "past_key_values", "=", "past_key_values", ",", "use_cache", "=", "True", ")", "\n", "logits", "=", "logits", "[", ":", ",", "context_length", "-", "1", ",", ":", "]", "\n", "", "else", ":", "\n", "                    ", "logits", ",", "past_key_values", "=", "model", "(", "tokens", "[", ":", ",", "context_length", "-", "1", ":", "context_length", "]", ",", "position_ids", "[", ":", ",", "context_length", "-", "1", ":", "context_length", "]", ",", "attention_mask", "[", ":", ",", ":", ",", "context_length", "-", "1", ",", ":", "context_length", "]", ",", "past_key_values", "=", "past_key_values", ",", "use_cache", "=", "True", ")", "\n", "logits", "=", "logits", "[", ":", ",", "0", ",", ":", "]", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "past_key_values", "=", "[", "x", ".", "half", "(", ")", "for", "x", "in", "past_key_values", "]", "\n", "", "else", ":", "\n", "                    ", "past_key_values", "=", "[", "x", "for", "x", "in", "past_key_values", "]", "\n", "", "logits", "=", "top_k_logits", "(", "logits", ",", "top_k", "=", "args", ".", "top_k", ",", "top_p", "=", "args", ".", "top_p", ")", "\n", "log_probs", "=", "F", ".", "softmax", "(", "logits", "/", "args", ".", "temperature", ",", "dim", "=", "-", "1", ")", "\n", "prev", "=", "torch", ".", "multinomial", "(", "log_probs", ",", "num_samples", "=", "1", ")", "\n", "tokens", "[", "0", ",", "context_length", "]", "=", "prev", "[", "0", "]", "\n", "torch", ".", "distributed", ".", "broadcast", "(", "tokens", ",", "mpu", ".", "get_model_parallel_src_rank", "(", ")", ",", "group", "=", "mpu", ".", "get_model_parallel_group", "(", ")", ")", "\n", "context_length", "+=", "1", "\n", "counter", "+=", "1", "\n", "\n", "output_tokens_list", "=", "tokens", ".", "view", "(", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "decode_tokens", "=", "tokenizer", ".", "decode", "(", "output_tokens_list", ".", "tolist", "(", ")", ")", "\n", "token_end", "=", "decode_tokens", ".", "find", "(", "\"<eod>\"", ")", "\n", "\n", "\n", "if", "mpu", ".", "get_model_parallel_rank", "(", ")", "==", "0", "and", "(", "counter", "%", "16", "==", "0", "or", "token_end", "!=", "-", "1", ")", ":", "\n", "                   ", "os", ".", "system", "(", "'clear'", ")", "\n", "print", "(", "\"\\nTaken time {:.2f}\\n\"", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"\\nContext:\"", ",", "raw_text", ",", "flush", "=", "True", ")", "\n", "trim_decode_tokens", "=", "decode_tokens", "[", "len", "(", "raw_text", ")", ":", "decode_tokens", ".", "find", "(", "\"<eod>\"", ")", "]", "\n", "print", "(", "\"\\nCPM:\"", ",", "trim_decode_tokens", ",", "flush", "=", "True", ")", "\n", "", "if", "token_end", "!=", "-", "1", ":", "\n", "#print(token_end)", "\n", "                   ", "break", "\n", "\n", "", "", "if", "mpu", ".", "get_model_parallel_rank", "(", ")", "==", "0", ":", "\n", "                ", "os", ".", "system", "(", "'clear'", ")", "\n", "print", "(", "\"\\nTaken time {:.2f}\\n\"", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"\\nContext:\"", ",", "raw_text", ",", "flush", "=", "True", ")", "\n", "output_tokens_list", "=", "tokens", ".", "view", "(", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "decode_tokens", "=", "tokenizer", ".", "decode", "(", "output_tokens_list", ".", "tolist", "(", ")", ")", "\n", "trim_decode_tokens", "=", "decode_tokens", "[", "len", "(", "raw_text", ")", ":", "decode_tokens", ".", "find", "(", "\"<eod>\"", ")", "]", "\n", "print", "(", "\"\\nCPM:\"", ",", "trim_decode_tokens", ",", "flush", "=", "True", ")", "\n", "#print(token_end)", "\n", "", "raw_text", "=", "None", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", "group", "=", "mpu", ".", "get_model_parallel_group", "(", ")", ")", "\n", "context_count", "+=", "1", "\n", "\n", "if", "args", ".", "input_text", ":", "\n", "                ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.prepare_tokenizer": [[277, 300], ["make_tokenizer", "print", "make_tokenizer.get_command", "mpu.get_model_parallel_world_size"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.make_tokenizer", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.get_command", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size"], ["", "", "", "", "def", "prepare_tokenizer", "(", "args", ")", ":", "\n", "\n", "    ", "tokenizer_args", "=", "{", "\n", "'tokenizer_type'", ":", "args", ".", "tokenizer_type", ",", "\n", "'corpus'", ":", "None", ",", "\n", "'model_path'", ":", "args", ".", "tokenizer_path", ",", "\n", "'vocab_size'", ":", "args", ".", "vocab_size", ",", "\n", "'model_type'", ":", "args", ".", "tokenizer_model_type", ",", "\n", "'cache_dir'", ":", "args", ".", "cache_dir", "}", "\n", "tokenizer", "=", "make_tokenizer", "(", "**", "tokenizer_args", ")", "\n", "\n", "args", ".", "tokenizer_num_tokens", "=", "tokenizer", ".", "num_tokens", "\n", "args", ".", "tokenizer_num_type_tokens", "=", "tokenizer", ".", "num_type_tokens", "\n", "args", ".", "eod_token", "=", "tokenizer", ".", "get_command", "(", "'eos'", ")", ".", "Id", "\n", "\n", "after", "=", "tokenizer", ".", "num_tokens", "\n", "while", "after", "%", "mpu", ".", "get_model_parallel_world_size", "(", ")", "!=", "0", ":", "\n", "        ", "after", "+=", "1", "\n", "\n", "", "args", ".", "vocab_size", "=", "after", "\n", "print", "(", "\"prepare tokenizer done\"", ",", "flush", "=", "True", ")", "\n", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.get_model": [[301, 338], ["utils.print_rank_0", "model.GPT2Model", "model.DistributedDataParallel.cuda", "mpu.get_data_parallel_rank", "print", "torch.cuda.current_device", "torch.cuda.current_device", "fp16.FP16_Module", "torch.cuda.current_device", "torch.cuda.current_device", "model.DistributedDataParallel", "model.DistributedDataParallel", "mpu.get_model_parallel_rank", "sum", "mpu.get_data_parallel_group", "p.nelement", "model.DistributedDataParallel.parameters"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group"], ["", "def", "get_model", "(", "args", ")", ":", "\n", "    ", "\"\"\"Build the model.\"\"\"", "\n", "\n", "print_rank_0", "(", "'building CPM model ...'", ")", "\n", "model", "=", "GPT2Model", "(", "num_layers", "=", "args", ".", "num_layers", ",", "\n", "vocab_size", "=", "args", ".", "vocab_size", ",", "\n", "hidden_size", "=", "args", ".", "hidden_size", ",", "\n", "num_attention_heads", "=", "args", ".", "num_attention_heads", ",", "\n", "embedding_dropout_prob", "=", "args", ".", "hidden_dropout", ",", "\n", "attention_dropout_prob", "=", "args", ".", "attention_dropout", ",", "\n", "output_dropout_prob", "=", "args", ".", "hidden_dropout", ",", "\n", "max_sequence_length", "=", "args", ".", "max_position_embeddings", ",", "\n", "checkpoint_activations", "=", "args", ".", "checkpoint_activations", ",", "\n", "checkpoint_num_layers", "=", "args", ".", "checkpoint_num_layers", ",", "\n", "parallel_output", "=", "args", ".", "parallel_output", ")", "\n", "\n", "if", "mpu", ".", "get_data_parallel_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "' > number of parameters on model parallel rank {}: {}'", ".", "format", "(", "\n", "mpu", ".", "get_model_parallel_rank", "(", ")", ",", "\n", "sum", "(", "[", "p", ".", "nelement", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", ")", ")", ",", "flush", "=", "True", ")", "\n", "\n", "# GPU allocation.", "\n", "", "model", ".", "cuda", "(", "torch", ".", "cuda", ".", "current_device", "(", ")", ")", "\n", "\n", "# Fp16 conversion.", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "model", "=", "FP16_Module", "(", "model", ")", "\n", "\n", "# Wrap model for distributed training.", "\n", "", "if", "USE_TORCH_DDP", ":", "\n", "        ", "i", "=", "torch", ".", "cuda", ".", "current_device", "(", ")", "\n", "model", "=", "DDP", "(", "model", ",", "device_ids", "=", "[", "i", "]", ",", "output_device", "=", "i", ",", "\n", "process_group", "=", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "DDP", "(", "model", ")", "\n", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.setup_model": [[340, 348], ["generate_samples.get_model", "utils.load_checkpoint_model"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.get_model", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_checkpoint_model"], ["", "def", "setup_model", "(", "args", ")", ":", "\n", "    ", "\"\"\"Setup model.\"\"\"", "\n", "\n", "model", "=", "get_model", "(", "args", ")", "\n", "\n", "args", ".", "iteration", "=", "load_checkpoint_model", "(", "model", ",", "args", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.main": [[349, 381], ["print", "utils.Timers", "arguments.get_args", "generate_samples.initialize_distributed", "generate_samples.set_random_seed", "data_utils.tokenization_gpt2.GPT2Tokenizer", "generate_samples.setup_model", "generate_samples.generate_samples", "os.path.join", "os.path.join", "torch.cuda.current_device", "torch.cuda.current_device"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.arguments.get_args", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.initialize_distributed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.setup_model", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.generate_samples.generate_samples"], ["", "def", "main", "(", ")", ":", "\n", "    ", "\"\"\"Main training program.\"\"\"", "\n", "\n", "print", "(", "'Generate Samples'", ")", "\n", "\n", "# Disable CuDNN.", "\n", "torch", ".", "backends", ".", "cudnn", ".", "enabled", "=", "False", "\n", "\n", "# Timer.", "\n", "timers", "=", "Timers", "(", ")", "\n", "\n", "# Arguments.", "\n", "args", "=", "get_args", "(", ")", "\n", "\n", "# Pytorch distributed.", "\n", "initialize_distributed", "(", "args", ")", "\n", "\n", "# Random seeds for reproducability.", "\n", "set_random_seed", "(", "args", ".", "seed", ")", "\n", "\n", "#get the tokenizer", "\n", "tokenizer", "=", "GPT2Tokenizer", "(", "os", ".", "path", ".", "join", "(", "args", ".", "tokenizer_path", ",", "'vocab.json'", ")", ",", "os", ".", "path", ".", "join", "(", "args", ".", "tokenizer_path", ",", "'chinese_vocab.model'", ")", ")", "\n", "\n", "# Model", "\n", "args", ".", "parallel_output", "=", "False", "\n", "model", "=", "setup_model", "(", "args", ")", "\n", "\n", "#setting default batch size to 1", "\n", "args", ".", "batch_size", "=", "1", "\n", "\n", "#generate samples", "\n", "generate_samples", "(", "model", ",", "tokenizer", ",", "args", ",", "torch", ".", "cuda", ".", "current_device", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.__init__": [[26, 29], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "defaults", "=", "{", "}", ")", ":", "\n", "        ", "super", "(", "DataConfig", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "defaults", "=", "defaults", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply": [[30, 35], ["configure_data.DataConfig.apply_defaults", "configure_data.make_loaders", "torch.distributed.get_rank", "print"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply_defaults", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.make_loaders"], ["", "def", "apply", "(", "self", ",", "args", ")", ":", "\n", "        ", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "            ", "print", "(", "'configuring data'", ")", "\n", "", "self", ".", "apply_defaults", "(", "args", ")", "\n", "return", "make_loaders", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.set_defaults": [[36, 39], ["kwargs.items"], "methods", ["None"], ["", "def", "set_defaults", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "defaults", "[", "k", "]", "=", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply_defaults": [[40, 45], ["configure_data.DataConfig.defaults.items", "k.replace.replace.replace", "hasattr", "setattr"], "methods", ["None"], ["", "", "def", "apply_defaults", "(", "self", ",", "args", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "self", ".", "defaults", ".", "items", "(", ")", ":", "\n", "            ", "k", "=", "k", ".", "replace", "(", "'-'", ",", "'_'", ")", "\n", "if", "not", "hasattr", "(", "args", ",", "k", ")", ":", "\n", "                ", "setattr", "(", "args", ",", "k", ",", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.make_data_loader": [[47, 77], ["torch.distributed.get_world_size", "torch.distributed.get_rank", "torch.utils.data.DataLoader", "data_utils.samplers.RandomSampler", "torch.utils.data.SequentialSampler", "data_utils.samplers.DistributedBatchSampler", "torch.utils.data.BatchSampler", "mpu.get_data_parallel_group", "mpu.get_data_parallel_group"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group"], ["", "", "", "", "def", "make_data_loader", "(", "dataset", ",", "batch_size", ",", "args", ")", ":", "\n", "\n", "    ", "shuffle", "=", "args", ".", "shuffle", "\n", "if", "shuffle", ":", "\n", "        ", "sampler", "=", "data_utils", ".", "samplers", ".", "RandomSampler", "(", "dataset", ",", "replacement", "=", "True", ",", "num_samples", "=", "batch_size", "*", "args", ".", "train_iters", ")", "\n", "", "else", ":", "\n", "        ", "sampler", "=", "torch", ".", "utils", ".", "data", ".", "SequentialSampler", "(", "dataset", ")", "\n", "", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", "\n", "group", "=", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", "group", "=", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "distributed", "=", "world_size", ">", "1", "\n", "drop_last", "=", "distributed", "\n", "\n", "if", "distributed", ":", "\n", "        ", "batch_sampler", "=", "data_utils", ".", "samplers", ".", "DistributedBatchSampler", "(", "sampler", ",", "\n", "batch_size", ",", "\n", "drop_last", ",", "\n", "rank", ",", "\n", "world_size", ")", "\n", "", "else", ":", "\n", "        ", "batch_sampler", "=", "torch", ".", "utils", ".", "data", ".", "BatchSampler", "(", "sampler", ",", "\n", "batch_size", ",", "\n", "drop_last", ")", "\n", "\n", "", "data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", ",", "\n", "batch_sampler", "=", "batch_sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "pin_memory", "=", "True", ")", "\n", "\n", "return", "data_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.make_tfrecord_loaders": [[79, 114], ["data_utils.tf_dl.TFRecordDataLoader", "data_utils.make_tokenizer", "max", "data_utils.tf_dl.TFRecordDataLoader", "data_utils.tf_dl.TFRecordDataLoader"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.make_tokenizer"], ["", "def", "make_tfrecord_loaders", "(", "args", ")", ":", "\n", "    ", "\"\"\"Load train/val/test dataset from shuffled TFRecords\"\"\"", "\n", "\n", "import", "data_utils", ".", "tf_dl", "\n", "data_set_args", "=", "{", "'batch_size'", ":", "args", ".", "batch_size", ",", "\n", "'max_seq_len'", ":", "args", ".", "seq_length", ",", "\n", "'max_preds_per_seq'", ":", "args", ".", "max_preds_per_seq", ",", "\n", "'train'", ":", "True", ",", "\n", "'num_workers'", ":", "max", "(", "args", ".", "num_workers", ",", "1", ")", ",", "\n", "'seed'", ":", "args", ".", "seed", "+", "args", ".", "rank", "+", "1", ",", "\n", "'threaded_dl'", ":", "args", ".", "num_workers", ">", "0", "\n", "}", "\n", "train", "=", "data_utils", ".", "tf_dl", ".", "TFRecordDataLoader", "(", "args", ".", "train_data", ",", "\n", "**", "data_set_args", ")", "\n", "data_set_args", "[", "'train'", "]", "=", "False", "\n", "if", "args", ".", "eval_seq_length", "is", "not", "None", ":", "\n", "        ", "data_set_args", "[", "'max_seq_len'", "]", "=", "args", ".", "eval_seq_length", "\n", "", "if", "args", ".", "eval_max_preds_per_seq", "is", "not", "None", ":", "\n", "        ", "data_set_args", "[", "'max_preds_per_seq'", "]", "=", "args", ".", "eval_max_preds_per_seq", "\n", "", "valid", "=", "None", "\n", "if", "args", ".", "valid_data", "is", "not", "None", ":", "\n", "        ", "valid", "=", "data_utils", ".", "tf_dl", ".", "TFRecordDataLoader", "(", "args", ".", "valid_data", ",", "\n", "**", "data_set_args", ")", "\n", "", "test", "=", "None", "\n", "if", "args", ".", "test_data", "is", "not", "None", ":", "\n", "        ", "test", "=", "data_utils", ".", "tf_dl", ".", "TFRecordDataLoader", "(", "args", ".", "test_data", ",", "\n", "**", "data_set_args", ")", "\n", "", "tokenizer", "=", "data_utils", ".", "make_tokenizer", "(", "args", ".", "tokenizer_type", ",", "\n", "train", ",", "\n", "args", ".", "tokenizer_path", ",", "\n", "args", ".", "vocab_size", ",", "\n", "args", ".", "tokenizer_model_type", ",", "\n", "cache_dir", "=", "args", ".", "cache_dir", ")", "\n", "\n", "return", "(", "train", ",", "valid", ",", "test", ")", ",", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.make_loaders": [[116, 203], ["torch.distributed.get_world_size", "configure_data.get_split", "copy.copy", "configure_data.make_tfrecord_loaders", "data_utils.make_dataset", "data_utils.should_split", "data_utils.make_dataset", "data_utils.make_dataset", "configure_data.make_data_loader", "configure_data.make_data_loader", "configure_data.make_data_loader", "mpu.get_data_parallel_group"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.get_split", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.make_tfrecord_loaders", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.make_dataset", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.__init__.should_split", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.make_dataset", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.make_dataset", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.make_data_loader", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.make_data_loader", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.make_data_loader", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group"], ["", "def", "make_loaders", "(", "args", ")", ":", "\n", "    ", "\"\"\"makes training/val/test\"\"\"", "\n", "\n", "if", "args", ".", "use_tfrecords", ":", "\n", "        ", "return", "make_tfrecord_loaders", "(", "args", ")", "\n", "", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", "\n", "group", "=", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "batch_size", "=", "args", ".", "batch_size", "*", "world_size", "\n", "eval_batch_size", "=", "batch_size", "\n", "if", "args", ".", "eval_batch_size", "is", "not", "None", ":", "\n", "        ", "eval_batch_size", "=", "args", ".", "eval_batch_size", "*", "world_size", "\n", "", "seq_length", "=", "args", ".", "seq_length", "\n", "if", "seq_length", "<", "0", ":", "\n", "        ", "seq_length", "=", "seq_length", "*", "world_size", "\n", "", "eval_seq_length", "=", "args", ".", "eval_seq_length", "\n", "if", "eval_seq_length", "is", "not", "None", "and", "eval_seq_length", "<", "0", ":", "\n", "        ", "eval_seq_length", "=", "eval_seq_length", "*", "world_size", "\n", "", "split", "=", "get_split", "(", "args", ")", "\n", "data_set_args", "=", "{", "\n", "'path'", ":", "args", ".", "train_data", ",", "\n", "'seq_length'", ":", "seq_length", ",", "\n", "'lazy'", ":", "args", ".", "lazy_loader", ",", "\n", "'delim'", ":", "args", ".", "delim", ",", "\n", "'text_key'", ":", "args", ".", "text_key", ",", "\n", "'label_key'", ":", "'label'", ",", "\n", "'non_binary_cols'", ":", "None", ",", "\n", "'ds_type'", ":", "args", ".", "data_set_type", ",", "\n", "'split'", ":", "split", ",", "\n", "'loose'", ":", "args", ".", "loose_json", ",", "\n", "'tokenizer_type'", ":", "args", ".", "tokenizer_type", ",", "\n", "'tokenizer_model_path'", ":", "args", ".", "tokenizer_path", ",", "\n", "'vocab_size'", ":", "args", ".", "vocab_size", ",", "\n", "'model_type'", ":", "args", ".", "tokenizer_model_type", ",", "\n", "'cache_dir'", ":", "args", ".", "cache_dir", ",", "\n", "'max_preds_per_seq'", ":", "args", ".", "max_preds_per_seq", ",", "\n", "'presplit_sentences'", ":", "args", ".", "presplit_sentences", "}", "\n", "\n", "eval_set_args", "=", "copy", ".", "copy", "(", "data_set_args", ")", "\n", "eval_set_args", "[", "'split'", "]", "=", "[", "1.", "]", "\n", "# if optional eval args were set then replace their", "\n", "# equivalent values in the arg dict", "\n", "if", "eval_seq_length", ":", "\n", "        ", "eval_set_args", "[", "'seq_length'", "]", "=", "eval_seq_length", "\n", "", "if", "args", ".", "eval_max_preds_per_seq", ":", "\n", "        ", "eval_set_args", "[", "'max_preds_per_seq'", "]", "=", "args", ".", "eval_max_preds_per_seq", "\n", "", "if", "args", ".", "eval_text_key", "is", "not", "None", ":", "\n", "        ", "eval_set_args", "[", "'text_key'", "]", "=", "args", ".", "eval_text_key", "\n", "\n", "# make datasets splits and tokenizer", "\n", "", "train", "=", "None", "\n", "valid", "=", "None", "\n", "test", "=", "None", "\n", "\n", "if", "args", ".", "train_data", "is", "not", "None", ":", "\n", "        ", "train", ",", "tokenizer", "=", "data_utils", ".", "make_dataset", "(", "**", "data_set_args", ")", "\n", "if", "data_utils", ".", "should_split", "(", "split", ")", ":", "\n", "            ", "train", ",", "valid", ",", "test", "=", "train", "\n", "", "eval_set_args", "[", "'tokenizer'", "]", "=", "tokenizer", "\n", "\n", "# make training and val dataset if necessary", "\n", "", "if", "valid", "is", "None", "and", "args", ".", "valid_data", "is", "not", "None", ":", "\n", "        ", "eval_set_args", "[", "'path'", "]", "=", "args", ".", "valid_data", "\n", "valid", ",", "tokenizer", "=", "data_utils", ".", "make_dataset", "(", "**", "eval_set_args", ")", "\n", "eval_set_args", "[", "'tokenizer'", "]", "=", "tokenizer", "\n", "", "if", "test", "is", "None", "and", "args", ".", "test_data", "is", "not", "None", ":", "\n", "        ", "eval_set_args", "[", "'path'", "]", "=", "args", ".", "test_data", "\n", "test", ",", "tokenizer", "=", "data_utils", ".", "make_dataset", "(", "**", "eval_set_args", ")", "\n", "\n", "# wrap datasets with data loader", "\n", "", "if", "train", "is", "not", "None", "and", "args", ".", "batch_size", ">", "0", ":", "\n", "        ", "train", "=", "make_data_loader", "(", "train", ",", "batch_size", ",", "args", ")", "\n", "args", ".", "do_train", "=", "True", "\n", "", "else", ":", "\n", "        ", "args", ".", "do_train", "=", "False", "\n", "", "eval_batch_size", "=", "eval_batch_size", "if", "eval_batch_size", "!=", "0", "else", "batch_size", "\n", "if", "valid", "is", "not", "None", ":", "\n", "        ", "valid", "=", "make_data_loader", "(", "valid", ",", "eval_batch_size", ",", "args", ")", "\n", "args", ".", "do_valid", "=", "True", "\n", "", "else", ":", "\n", "        ", "args", ".", "do_valid", "=", "False", "\n", "", "if", "test", "is", "not", "None", ":", "\n", "        ", "test", "=", "make_data_loader", "(", "test", ",", "eval_batch_size", ",", "args", ")", "\n", "args", ".", "do_test", "=", "True", "\n", "", "else", ":", "\n", "        ", "args", ".", "do_test", "=", "False", "\n", "\n", "", "return", "(", "train", ",", "valid", ",", "test", ")", ",", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.get_split": [[204, 227], ["sum", "sum", "args.split.find", "splits.append", "len", "splits.append", "float", "args.split.find", "args.split.split", "float", "float", "args.split.split"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "get_split", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Get dataset splits from comma separated string list\n    \"\"\"", "\n", "splits", "=", "[", "]", "\n", "if", "args", ".", "split", ".", "find", "(", "','", ")", "!=", "-", "1", ":", "\n", "        ", "splits", "=", "[", "float", "(", "s", ")", "for", "s", "in", "args", ".", "split", ".", "split", "(", "','", ")", "]", "\n", "", "elif", "args", ".", "split", ".", "find", "(", "'/'", ")", "!=", "-", "1", ":", "\n", "        ", "splits", "=", "[", "float", "(", "s", ")", "for", "s", "in", "args", ".", "split", ".", "split", "(", "'/'", ")", "]", "\n", "", "else", ":", "\n", "        ", "splits", "=", "[", "float", "(", "args", ".", "split", ")", "]", "\n", "", "split_total", "=", "sum", "(", "splits", ")", "\n", "if", "split_total", "<", "1.", ":", "\n", "        ", "splits", ".", "append", "(", "1", "-", "split_total", ")", "\n", "", "while", "len", "(", "splits", ")", "<", "3", ":", "\n", "        ", "splits", ".", "append", "(", "0.", ")", "\n", "", "splits", "=", "splits", "[", ":", "3", "]", "\n", "if", "args", ".", "valid_data", "is", "not", "None", ":", "\n", "        ", "splits", "[", "1", "]", "=", "0.", "\n", "", "if", "args", ".", "test_data", "is", "not", "None", ":", "\n", "        ", "splits", "[", "2", "]", "=", "0.", "\n", "", "final_sum", "=", "sum", "(", "splits", ")", "\n", "return", "[", "s", "/", "final_sum", "for", "s", "in", "splits", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.configure_data": [[228, 247], ["configure_data.DataConfig"], "function", ["None"], ["", "def", "configure_data", "(", ")", ":", "\n", "\n", "    ", "\"\"\"add cmdline flags for configuring datasets\"\"\"", "\n", "# These are options that are used by data_utils, but are either", "\n", "# deprecated or not meant to be exposed to the command line user.", "\n", "# These options are intneded to be set in code by specific scripts.", "\n", "defaults", "=", "{", "\n", "'world_size'", ":", "1", ",", "\n", "'rank'", ":", "-", "1", ",", "\n", "'persist_state'", ":", "0", ",", "\n", "'lazy'", ":", "False", ",", "\n", "'transpose'", ":", "False", ",", "\n", "'data_set_type'", ":", "'supervised'", ",", "\n", "'seq_length'", ":", "256", ",", "\n", "'eval_seq_length'", ":", "256", ",", "\n", "'samples_per_shard'", ":", "100", "\n", "}", "\n", "\n", "return", "DataConfig", "(", "defaults", "=", "defaults", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.Timers.__init__": [[114, 116], ["None"], "methods", ["None"], ["", "", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "timers", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.Timers.__call__": [[117, 121], ["utils.Timers.Timer"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "name", ")", ":", "\n", "        ", "if", "name", "not", "in", "self", ".", "timers", ":", "\n", "            ", "self", ".", "timers", "[", "name", "]", "=", "self", ".", "Timer", "(", "name", ")", "\n", "", "return", "self", ".", "timers", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.Timers.log": [[122, 131], ["utils.print_rank_0", "utils.Timers.timers[].elapsed"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0"], ["", "def", "log", "(", "self", ",", "names", ",", "normalizer", "=", "1.0", ",", "reset", "=", "True", ")", ":", "\n", "        ", "\"\"\"Log a group of timers.\"\"\"", "\n", "assert", "normalizer", ">", "0.0", "\n", "string", "=", "'time (ms)'", "\n", "for", "name", "in", "names", ":", "\n", "            ", "elapsed_time", "=", "self", ".", "timers", "[", "name", "]", ".", "elapsed", "(", "\n", "reset", "=", "reset", ")", "*", "1000.0", "/", "normalizer", "\n", "string", "+=", "' | {}: {:.2f}'", ".", "format", "(", "name", ",", "elapsed_time", ")", "\n", "", "print_rank_0", "(", "string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0": [[30, 36], ["torch.distributed.is_initialized", "print", "torch.distributed.get_rank", "print"], "function", ["None"], ["def", "print_rank_0", "(", "message", ")", ":", "\n", "    ", "if", "torch", ".", "distributed", ".", "is_initialized", "(", ")", ":", "\n", "        ", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "            ", "print", "(", "message", ",", "flush", "=", "True", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "message", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_args": [[38, 45], ["print", "vars", "print", "len", "getattr"], "function", ["None"], ["", "", "def", "print_args", "(", "args", ")", ":", "\n", "    ", "\"\"\"Print arguments.\"\"\"", "\n", "\n", "print", "(", "'arguments:'", ",", "flush", "=", "True", ")", "\n", "for", "arg", "in", "vars", "(", "args", ")", ":", "\n", "        ", "dots", "=", "'.'", "*", "(", "29", "-", "len", "(", "arg", ")", ")", "\n", "print", "(", "'  {} {} {}'", ".", "format", "(", "arg", ",", "dots", ",", "getattr", "(", "args", ",", "arg", ")", ")", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_params_min_max_norm": [[47, 65], ["torch.distributed.get_rank", "isinstance", "print", "param.data.min", "param.data.max", "param.data.norm", "int"], "function", ["None"], ["", "", "def", "print_params_min_max_norm", "(", "optimizer", ",", "iteration", ")", ":", "\n", "    ", "\"\"\"Print min, max, and norm of all parameters.\"\"\"", "\n", "index", "=", "0", "\n", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "string", "=", "'iteration, rank, index, model-parallel,min, max, norm\\n'", "\n", "optimizer_", "=", "optimizer", "\n", "if", "isinstance", "(", "optimizer", ",", "FP16_Optimizer", ")", ":", "\n", "        ", "optimizer_", "=", "optimizer", ".", "optimizer", "\n", "", "for", "param_group", "in", "optimizer_", ".", "param_groups", ":", "\n", "        ", "for", "param", "in", "param_group", "[", "'params'", "]", ":", "\n", "            ", "index", "+=", "1", "\n", "min_", "=", "param", ".", "data", ".", "min", "(", ")", "\n", "max_", "=", "param", ".", "data", ".", "max", "(", ")", "\n", "norm", "=", "param", ".", "data", ".", "norm", "(", ")", "\n", "string", "+=", "'{:7d}, {:4d}, {:4d}, {:2d}, '", ".", "format", "(", "\n", "iteration", ",", "rank", ",", "index", ",", "int", "(", "param", ".", "model_parallel", ")", ")", "\n", "string", "+=", "'{:.6E}, {:.6E}, {:.6E}\\n'", ".", "format", "(", "min_", ",", "max_", ",", "norm", ")", "\n", "", "", "print", "(", "string", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.report_memory": [[133, 146], ["utils.print_rank_0", "torch.cuda.memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.memory_cached", "torch.cuda.max_memory_cached"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0"], ["", "", "def", "report_memory", "(", "name", ")", ":", "\n", "    ", "\"\"\"Simple GPU memory report.\"\"\"", "\n", "\n", "mega_bytes", "=", "1024.0", "*", "1024.0", "\n", "string", "=", "name", "+", "' memory (MB)'", "\n", "string", "+=", "' | allocated: {}'", ".", "format", "(", "\n", "torch", ".", "cuda", ".", "memory_allocated", "(", ")", "/", "mega_bytes", ")", "\n", "string", "+=", "' | max allocated: {}'", ".", "format", "(", "\n", "torch", ".", "cuda", ".", "max_memory_allocated", "(", ")", "/", "mega_bytes", ")", "\n", "string", "+=", "' | cached: {}'", ".", "format", "(", "torch", ".", "cuda", ".", "memory_cached", "(", ")", "/", "mega_bytes", ")", "\n", "string", "+=", "' | max cached: {}'", ".", "format", "(", "\n", "torch", ".", "cuda", ".", "max_memory_cached", "(", ")", "/", "mega_bytes", ")", "\n", "print_rank_0", "(", "string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.get_checkpoint_name": [[148, 158], ["os.path.join", "mpu.get_data_parallel_rank", "mpu.get_model_parallel_rank"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank"], ["", "def", "get_checkpoint_name", "(", "checkpoints_path", ",", "iteration", ",", "release", "=", "False", ",", "zero", "=", "False", ")", ":", "\n", "    ", "if", "release", ":", "\n", "        ", "d", "=", "'release'", "\n", "", "else", ":", "\n", "        ", "d", "=", "'{:d}'", ".", "format", "(", "iteration", ")", "\n", "", "if", "zero", ":", "\n", "        ", "dp_rank", "=", "mpu", ".", "get_data_parallel_rank", "(", ")", "\n", "d", "+=", "'_zero_dp_rank_{}'", ".", "format", "(", "dp_rank", ")", "\n", "", "return", "os", ".", "path", ".", "join", "(", "checkpoints_path", ",", "d", ",", "\n", "'mp_rank_{:02d}_model_states.pt'", ".", "format", "(", "mpu", ".", "get_model_parallel_rank", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.ensure_directory_exists": [[160, 164], ["os.path.dirname", "os.path.exists", "os.makedirs"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "def", "ensure_directory_exists", "(", "filename", ")", ":", "\n", "    ", "dirname", "=", "os", ".", "path", ".", "dirname", "(", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "dirname", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "dirname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.get_checkpoint_tracker_filename": [[166, 168], ["os.path.join"], "function", ["None"], ["", "", "def", "get_checkpoint_tracker_filename", "(", "checkpoints_path", ")", ":", "\n", "    ", "return", "os", ".", "path", ".", "join", "(", "checkpoints_path", ",", "'latest_checkpointed_iteration.txt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.save_zero_checkpoint": [[170, 177], ["utils.get_checkpoint_name", "utils.ensure_directory_exists", "torch.save", "print", "optimizer.state_dict"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.get_checkpoint_name", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.ensure_directory_exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.state_dict"], ["", "def", "save_zero_checkpoint", "(", "args", ",", "iteration", ",", "optimizer", ")", ":", "\n", "    ", "zero_sd", "=", "{", "'iteration'", ":", "iteration", ",", "\n", "'optimizer_state_dict'", ":", "optimizer", ".", "state_dict", "(", ")", "}", "\n", "zero_checkpoint_name", "=", "get_checkpoint_name", "(", "args", ".", "save", ",", "iteration", ",", "zero", "=", "True", ")", "\n", "ensure_directory_exists", "(", "zero_checkpoint_name", ")", "\n", "torch", ".", "save", "(", "zero_sd", ",", "zero_checkpoint_name", ")", "\n", "print", "(", "'  successfully saved {}'", ".", "format", "(", "zero_checkpoint_name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.save_checkpoint": [[178, 223], ["isinstance", "torch.distributed.barrier", "torch.distributed.barrier", "mpu.get_data_parallel_rank", "utils.get_checkpoint_name", "print", "model.state_dict", "utils.ensure_directory_exists", "torch.save", "print", "torch.distributed.get_rank", "utils.get_checkpoint_tracker_filename", "random.getstate", "numpy.random.get_state", "torch.get_rng_state", "torch.cuda.get_rng_state", "mpu.get_cuda_rng_tracker().get_states", "open", "f.write", "torch.distributed.get_rank", "optimizer.state_dict", "lr_scheduler.state_dict", "str", "mpu.get_cuda_rng_tracker"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.get_checkpoint_name", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.state_dict", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.ensure_directory_exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.get_checkpoint_tracker_filename", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.get_states", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.state_dict", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.state_dict", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker"], ["", "def", "save_checkpoint", "(", "iteration", ",", "model", ",", "optimizer", ",", "\n", "lr_scheduler", ",", "args", ")", ":", "\n", "    ", "\"\"\"Save a model checkpoint.\"\"\"", "\n", "# Only rank zer0 of the data parallel writes to the disk.", "\n", "if", "isinstance", "(", "model", ",", "torchDDP", ")", ":", "\n", "        ", "model", "=", "model", ".", "module", "\n", "\n", "", "if", "mpu", ".", "get_data_parallel_rank", "(", ")", "==", "0", ":", "\n", "        ", "checkpoint_name", "=", "get_checkpoint_name", "(", "args", ".", "save", ",", "iteration", ")", "\n", "print", "(", "'global rank {} is saving checkpoint at iteration {:7d} to {}'", ".", "\n", "format", "(", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "iteration", ",", "checkpoint_name", ")", ")", "\n", "\n", "sd", "=", "{", "}", "\n", "sd", "[", "'iteration'", "]", "=", "iteration", "\n", "sd", "[", "'model'", "]", "=", "model", ".", "state_dict", "(", ")", "\n", "\n", "# Optimizer stuff.", "\n", "if", "not", "args", ".", "no_save_optim", ":", "\n", "            ", "if", "optimizer", "is", "not", "None", ":", "\n", "                ", "sd", "[", "'optimizer'", "]", "=", "optimizer", ".", "state_dict", "(", ")", "\n", "", "if", "lr_scheduler", "is", "not", "None", ":", "\n", "                ", "sd", "[", "'lr_scheduler'", "]", "=", "lr_scheduler", ".", "state_dict", "(", ")", "\n", "\n", "# rng states.", "\n", "", "", "if", "not", "args", ".", "no_save_rng", ":", "\n", "            ", "sd", "[", "'random_rng_state'", "]", "=", "random", ".", "getstate", "(", ")", "\n", "sd", "[", "'np_rng_state'", "]", "=", "np", ".", "random", ".", "get_state", "(", ")", "\n", "sd", "[", "'torch_rng_state'", "]", "=", "torch", ".", "get_rng_state", "(", ")", "\n", "sd", "[", "'cuda_rng_state'", "]", "=", "torch", ".", "cuda", ".", "get_rng_state", "(", ")", "\n", "sd", "[", "'rng_tracker_states'", "]", "=", "mpu", ".", "get_cuda_rng_tracker", "(", ")", ".", "get_states", "(", ")", "\n", "\n", "\n", "", "ensure_directory_exists", "(", "checkpoint_name", ")", "\n", "torch", ".", "save", "(", "sd", ",", "checkpoint_name", ")", "\n", "print", "(", "'  successfully saved {}'", ".", "format", "(", "checkpoint_name", ")", ")", "\n", "\n", "# Wait so everyone is done (necessary)", "\n", "", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "# And update the latest iteration", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "tracker_filename", "=", "get_checkpoint_tracker_filename", "(", "args", ".", "save", ")", "\n", "with", "open", "(", "tracker_filename", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "iteration", ")", ")", "\n", "# Wait so everyone is done (not necessary)", "\n", "", "", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.save_ds_checkpoint": [[224, 238], ["model.save_checkpoint", "random.getstate", "numpy.random.get_state", "torch.get_rng_state", "torch.cuda.get_rng_state", "mpu.get_cuda_rng_tracker().get_states", "mpu.get_cuda_rng_tracker"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.save_checkpoint", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.get_states", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker"], ["", "def", "save_ds_checkpoint", "(", "iteration", ",", "model", ",", "args", ")", ":", "\n", "    ", "\"\"\"Save a model checkpoint.\"\"\"", "\n", "\n", "sd", "=", "{", "}", "\n", "sd", "[", "'iteration'", "]", "=", "iteration", "\n", "# rng states.", "\n", "if", "not", "args", ".", "no_save_rng", ":", "\n", "        ", "sd", "[", "'random_rng_state'", "]", "=", "random", ".", "getstate", "(", ")", "\n", "sd", "[", "'np_rng_state'", "]", "=", "np", ".", "random", ".", "get_state", "(", ")", "\n", "sd", "[", "'torch_rng_state'", "]", "=", "torch", ".", "get_rng_state", "(", ")", "\n", "sd", "[", "'cuda_rng_state'", "]", "=", "torch", ".", "cuda", ".", "get_rng_state", "(", ")", "\n", "sd", "[", "'rng_tracker_states'", "]", "=", "mpu", ".", "get_cuda_rng_tracker", "(", ")", ".", "get_states", "(", ")", "\n", "\n", "", "model", ".", "save_checkpoint", "(", "args", ".", "save", ",", "iteration", ",", "client_state", "=", "sd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.get_checkpoint_iteration": [[240, 266], ["utils.get_checkpoint_tracker_filename", "os.path.isfile", "utils.print_rank_0", "utils.print_rank_0", "open", "f.read().strip", "int", "f.read", "utils.print_rank_0", "exit"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.get_checkpoint_tracker_filename", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0"], ["", "def", "get_checkpoint_iteration", "(", "args", ")", ":", "\n", "# Read the tracker file and set the iteration.", "\n", "    ", "tracker_filename", "=", "get_checkpoint_tracker_filename", "(", "args", ".", "load", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "tracker_filename", ")", ":", "\n", "        ", "print_rank_0", "(", "'WARNING: could not find the metadata file {} '", ".", "format", "(", "\n", "tracker_filename", ")", ")", "\n", "print_rank_0", "(", "'    will not load any checkpoints and will start from '", "\n", "'random'", ")", "\n", "return", "0", ",", "False", ",", "False", "\n", "", "iteration", "=", "0", "\n", "release", "=", "False", "\n", "with", "open", "(", "tracker_filename", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "metastring", "=", "f", ".", "read", "(", ")", ".", "strip", "(", ")", "\n", "try", ":", "\n", "            ", "iteration", "=", "int", "(", "metastring", ")", "\n", "", "except", "ValueError", ":", "\n", "            ", "release", "=", "metastring", "==", "'release'", "\n", "if", "not", "release", ":", "\n", "                ", "print_rank_0", "(", "'ERROR: Invalid metadata file {}. Exiting'", ".", "format", "(", "\n", "tracker_filename", ")", ")", "\n", "exit", "(", ")", "\n", "\n", "", "", "", "assert", "iteration", ">", "0", "or", "release", ",", "'error parsing metadata file {}'", ".", "format", "(", "\n", "tracker_filename", ")", "\n", "\n", "return", "iteration", ",", "release", ",", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_checkpoint_model": [[267, 301], ["utils.get_checkpoint_iteration", "utils.get_checkpoint_name", "torch.load", "isinstance", "torch.distributed.barrier", "mpu.get_data_parallel_rank", "print", "model.load_state_dict", "mpu.get_data_parallel_rank", "print", "utils.print_rank_0", "exit", "torch.distributed.get_rank"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.get_checkpoint_iteration", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.get_checkpoint_name", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.load_state_dict", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0"], ["", "def", "load_checkpoint_model", "(", "model", ",", "args", ")", ":", "\n", "    ", "\"\"\"Load a model checkpoint.\"\"\"", "\n", "\n", "iteration", ",", "release", ",", "success", "=", "get_checkpoint_iteration", "(", "args", ")", "\n", "\n", "if", "not", "success", ":", "\n", "        ", "return", "0", "\n", "\n", "# Checkpoint.", "\n", "", "checkpoint_name", "=", "get_checkpoint_name", "(", "args", ".", "load", ",", "iteration", ",", "release", ")", "\n", "\n", "if", "mpu", ".", "get_data_parallel_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'global rank {} is loading checkpoint {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "checkpoint_name", ")", ")", "\n", "\n", "# Load the checkpoint.", "\n", "", "sd", "=", "torch", ".", "load", "(", "checkpoint_name", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "if", "isinstance", "(", "model", ",", "torchDDP", ")", ":", "\n", "        ", "model", "=", "model", ".", "module", "\n", "\n", "# Model.", "\n", "", "try", ":", "\n", "        ", "model", ".", "load_state_dict", "(", "sd", "[", "'module'", "]", ")", "\n", "", "except", "KeyError", ":", "\n", "        ", "print_rank_0", "(", "'A metadata file exists but unable to load model '", "\n", "'from checkpoint {}, exiting'", ".", "format", "(", "checkpoint_name", ")", ")", "\n", "exit", "(", ")", "\n", "\n", "", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "mpu", ".", "get_data_parallel_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'  successfully loaded {}'", ".", "format", "(", "checkpoint_name", ")", ")", "\n", "\n", "", "return", "iteration", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_weights": [[302, 320], ["src.named_parameters", "str", "load.copy_", "type", "data.t().contiguous.t().contiguous", "data.t().contiguous.t"], "function", ["None"], ["", "def", "load_weights", "(", "src", ",", "dst", ",", "dst2src", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Loads weights from src to dst via in place copy.\n    src is a huggingface gpt2model, while dst is one of our models.\n    dst2src=True loads parameters from our models into huggingface's.\n    ^dst2src is still untested\n    \"\"\"", "\n", "conv_layer", "=", "'Conv1D'", "in", "str", "(", "type", "(", "src", ")", ")", "\n", "for", "n", ",", "p", "in", "src", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "dst2src", ":", "\n", "            ", "data", "=", "dst", ".", "_parameters", "[", "n", "]", ".", "data", "\n", "load", "=", "p", ".", "data", "\n", "", "else", ":", "\n", "            ", "data", "=", "p", ".", "data", "\n", "load", "=", "dst", ".", "_parameters", "[", "n", "]", ".", "data", "\n", "", "if", "conv_layer", "and", "'weight'", "in", "n", ":", "\n", "            ", "data", "=", "data", ".", "t", "(", ")", ".", "contiguous", "(", ")", "\n", "", "load", ".", "copy_", "(", "data", ")", "\n", "#        dst._parameters[n].data.copy_(data)", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_mlp": [[322, 325], ["utils.load_weights", "utils.load_weights"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_weights", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_weights"], ["", "", "def", "load_mlp", "(", "our", ",", "oai", ",", "dst2src", "=", "False", ")", ":", "\n", "    ", "load_weights", "(", "oai", ".", "c_fc", ",", "our", ".", "dense_h_to_4h", ",", "dst2src", ")", "\n", "load_weights", "(", "oai", ".", "c_proj", ",", "our", ".", "dense_4h_to_h", ",", "dst2src", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_attention": [[326, 329], ["utils.load_weights", "utils.load_weights"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_weights", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_weights"], ["", "def", "load_attention", "(", "our", ",", "oai", ",", "dst2src", "=", "False", ")", ":", "\n", "    ", "load_weights", "(", "oai", ".", "c_attn", ",", "our", ".", "query_key_value", ",", "dst2src", ")", "\n", "load_weights", "(", "oai", ".", "c_proj", ",", "our", ".", "dense", ",", "dst2src", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_transformer_layer": [[330, 335], ["utils.load_weights", "utils.load_weights", "utils.load_mlp", "utils.load_attention"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_weights", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_weights", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_mlp", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_attention"], ["", "def", "load_transformer_layer", "(", "our", ",", "oai", ",", "dst2src", "=", "False", ")", ":", "\n", "    ", "load_weights", "(", "oai", ".", "ln_1", ",", "our", ".", "input_layernorm", ",", "dst2src", ")", "\n", "load_weights", "(", "oai", ".", "ln_2", ",", "our", ".", "post_attention_layernorm", ",", "dst2src", ")", "\n", "load_mlp", "(", "our", ".", "mlp", ",", "oai", ".", "mlp", ",", "dst2src", ")", "\n", "load_attention", "(", "our", ".", "attention", ",", "oai", ".", "attn", ",", "dst2src", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.move_weights": [[336, 352], ["utils.load_weights", "utils.load_weights", "utils.load_weights", "zip", "utils.load_transformer_layer"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_weights", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_weights", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_weights", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.load_transformer_layer"], ["", "def", "move_weights", "(", "our", ",", "oai", ",", "dst2src", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Loads weights from `oai` to `our` via in place copy.\n    `oai` is a huggingface gpt2model, while `our` is one of our models.\n    dst2src=True loads parameters from our models into huggingface's.\n    ^dst2src=True is still untested\n    \"\"\"", "\n", "#    while isinstance(our, (torchDDP, model.distributed.DistributedDataParallel, FP16_Module)):", "\n", "#        our=our.module", "\n", "transformer_model", "=", "oai", ".", "transformer", "\n", "load_weights", "(", "transformer_model", ".", "ln_f", ",", "our", ".", "transformer", ".", "final_layernorm", ",", "dst2src", ")", "\n", "load_weights", "(", "transformer_model", ".", "wte", ",", "our", ".", "word_embeddings", ",", "dst2src", ")", "\n", "load_weights", "(", "transformer_model", ".", "wpe", ",", "our", ".", "position_embeddings", ",", "dst2src", ")", "\n", "\n", "for", "our_layer", ",", "oai_layer", "in", "zip", "(", "our", ".", "transformer", ".", "layers", ",", "oai", ".", "transformer", ".", "h", ")", ":", "\n", "        ", "load_transformer_layer", "(", "our_layer", ",", "oai_layer", ",", "dst2src", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.tofp16.__init__": [[32, 34], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "tofp16", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.tofp16.forward": [[35, 37], ["input.half"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "input", ".", "half", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.FP16Model.__init__": [[93, 96], ["torch.Module.__init__", "fp16util.convert_network"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.convert_network"], ["def", "__init__", "(", "self", ",", "network", ")", ":", "\n", "        ", "super", "(", "FP16Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "network", "=", "convert_network", "(", "network", ",", "dtype", "=", "torch", ".", "half", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.FP16Model.forward": [[97, 100], ["tuple", "fp16util.FP16Model.network", "t.half"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "*", "inputs", ")", ":", "\n", "        ", "inputs", "=", "tuple", "(", "t", ".", "half", "(", ")", "for", "t", "in", "inputs", ")", "\n", "return", "self", ".", "network", "(", "*", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.BN_convert_float": [[39, 50], ["module.children", "isinstance", "module.float", "fp16util.BN_convert_float"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.BN_convert_float"], ["", "", "def", "BN_convert_float", "(", "module", ")", ":", "\n", "    ", "\"\"\"\n    Utility function for network_to_half().\n\n    Retained for legacy purposes.\n    \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "_BatchNorm", ")", "and", "module", ".", "affine", "is", "True", ":", "\n", "        ", "module", ".", "float", "(", ")", "\n", "", "for", "child", "in", "module", ".", "children", "(", ")", ":", "\n", "        ", "BN_convert_float", "(", "child", ")", "\n", "", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.network_to_half": [[52, 59], ["torch.Sequential", "fp16util.tofp16", "fp16util.BN_convert_float", "network.half"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.BN_convert_float"], ["", "def", "network_to_half", "(", "network", ")", ":", "\n", "    ", "\"\"\"\n    Convert model to half precision in a batchnorm-safe way.\n\n    Retained for legacy purposes. It is recommended to use FP16Model.\n    \"\"\"", "\n", "return", "nn", ".", "Sequential", "(", "tofp16", "(", ")", ",", "BN_convert_float", "(", "network", ".", "half", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.convert_module": [[61, 75], ["module.parameters", "module.buffers", "buf.data.to", "param.data.to", "param._grad.data.to"], "function", ["None"], ["", "def", "convert_module", "(", "module", ",", "dtype", ")", ":", "\n", "    ", "\"\"\"\n    Converts a module's immediate parameters and buffers to dtype.\n    \"\"\"", "\n", "for", "param", "in", "module", ".", "parameters", "(", "recurse", "=", "False", ")", ":", "\n", "        ", "if", "param", "is", "not", "None", ":", "\n", "            ", "if", "param", ".", "data", ".", "dtype", ".", "is_floating_point", ":", "\n", "                ", "param", ".", "data", "=", "param", ".", "data", ".", "to", "(", "dtype", "=", "dtype", ")", "\n", "", "if", "param", ".", "_grad", "is", "not", "None", "and", "param", ".", "_grad", ".", "data", ".", "dtype", ".", "is_floating_point", ":", "\n", "                ", "param", ".", "_grad", ".", "data", "=", "param", ".", "_grad", ".", "data", ".", "to", "(", "dtype", "=", "dtype", ")", "\n", "\n", "", "", "", "for", "buf", "in", "module", ".", "buffers", "(", "recurse", "=", "False", ")", ":", "\n", "        ", "if", "buf", "is", "not", "None", "and", "buf", ".", "data", ".", "dtype", ".", "is_floating_point", ":", "\n", "            ", "buf", ".", "data", "=", "buf", ".", "data", ".", "to", "(", "dtype", "=", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.convert_network": [[77, 86], ["network.modules", "fp16util.convert_module", "isinstance"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.convert_module"], ["", "", "", "def", "convert_network", "(", "network", ",", "dtype", ")", ":", "\n", "    ", "\"\"\"\n    Converts a network's parameters and buffers to dtype.\n    \"\"\"", "\n", "for", "module", "in", "network", ".", "modules", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "_BatchNorm", ")", "and", "module", ".", "affine", "is", "True", ":", "\n", "            ", "continue", "\n", "", "convert_module", "(", "module", ",", "dtype", ")", "\n", "", "return", "network", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.backwards_debug_hook": [[102, 104], ["RuntimeError"], "function", ["None"], ["", "", "def", "backwards_debug_hook", "(", "grad", ")", ":", "\n", "    ", "raise", "RuntimeError", "(", "\"master_params recieved a gradient in the backward pass!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.prep_param_lists": [[105, 149], ["torch.nn.Parameter", "torch.nn.Parameter", "model.parameters", "torch._utils._flatten_dense_tensors().float", "_flatten_dense_tensors().float.new", "param.clone().float().detach", "print", "torch._utils._flatten_dense_tensors", "_flatten_dense_tensors().float.size", "param.clone().float", "param.clone"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "def", "prep_param_lists", "(", "model", ",", "flat_master", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Creates a list of FP32 master parameters for a given model, as in\n    `Training Neural Networks with Mixed Precision:  Real Examples`_.\n\n    Args:\n        model (torch.nn.Module): Existing Pytorch model\n        flat_master (bool, optional, default=False):  Flatten the master parameters into a single tensor, as a performance optimization.\n    Returns:\n        A tuple (``model_params``, ``master_params``). ``model_params`` is a list of the model's parameters for later use with :func:`model_grads_to_master_grads` and :func:`master_params_to_model_params`.  ``master_params`` is a list of FP32 master gradients.  If ``flat_master=True``, ``master_params`` will be a list with one element.\n\n    Example::\n\n        model_params, master_params = prep_param_lists(model)\n\n    .. warning::\n        Currently, if ``flat_master=True``, all the model's parameters must be the same type.  If the model has parameters of different types, use ``flat_master=False``, or use :class:`FP16_Optimizer`.\n\n    .. _`Training Neural Networks with Mixed Precision:  Real Examples`:\n        http://on-demand.gputechconf.com/gtc/2018/video/S81012/\n    \"\"\"", "\n", "model_params", "=", "[", "param", "for", "param", "in", "model", ".", "parameters", "(", ")", "if", "param", ".", "requires_grad", "]", "\n", "\n", "if", "flat_master", ":", "\n", "# Give the user some more useful error messages", "\n", "        ", "try", ":", "\n", "# flatten_dense_tensors returns a contiguous flat array.", "\n", "# http://pytorch.org/docs/master/_modules/torch/_utils.html", "\n", "            ", "master_params", "=", "_flatten_dense_tensors", "(", "[", "param", ".", "data", "for", "param", "in", "model_params", "]", ")", ".", "float", "(", ")", "\n", "", "except", ":", "\n", "            ", "print", "(", "\"Error in prep_param_lists:  model may contain a mixture of parameters \"", "\n", "\"of different types.  Use flat_master=False, or use F16_Optimizer.\"", ")", "\n", "raise", "\n", "", "master_params", "=", "torch", ".", "nn", ".", "Parameter", "(", "master_params", ")", "\n", "master_params", ".", "requires_grad", "=", "True", "\n", "# master_params.register_hook(backwards_debug_hook)", "\n", "if", "master_params", ".", "grad", "is", "None", ":", "\n", "            ", "master_params", ".", "grad", "=", "master_params", ".", "new", "(", "*", "master_params", ".", "size", "(", ")", ")", "\n", "", "return", "model_params", ",", "[", "master_params", "]", "\n", "", "else", ":", "\n", "        ", "master_params", "=", "[", "param", ".", "clone", "(", ")", ".", "float", "(", ")", ".", "detach", "(", ")", "for", "param", "in", "model_params", "]", "\n", "for", "param", "in", "master_params", ":", "\n", "            ", "param", ".", "requires_grad", "=", "True", "\n", "", "return", "model_params", ",", "master_params", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.model_grads_to_master_grads": [[151, 171], ["master_params[].grad.data.copy_", "zip", "torch._utils._flatten_dense_tensors", "master.grad.data.copy_", "torch.autograd.Variable", "master.data.new", "master.data.size"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "", "def", "model_grads_to_master_grads", "(", "model_params", ",", "master_params", ",", "flat_master", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Copy model gradients to master gradients.  \n\n    Args:\n        model_params:  List of model parameters created by :func:`prep_param_lists`.\n        master_params:  List of FP32 master parameters created by :func:`prep_param_lists`.  If ``master_params`` was created with ``flat_master=True``, ``flat_master=True`` should also be supplied to :func:`model_grads_to_master_grads`.\n    \"\"\"", "\n", "if", "flat_master", ":", "\n", "# The flattening may incur one more deep copy than is necessary.", "\n", "        ", "master_params", "[", "0", "]", ".", "grad", ".", "data", ".", "copy_", "(", "\n", "_flatten_dense_tensors", "(", "[", "p", ".", "grad", ".", "data", "for", "p", "in", "model_params", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "for", "model", ",", "master", "in", "zip", "(", "model_params", ",", "master_params", ")", ":", "\n", "            ", "if", "model", ".", "grad", "is", "not", "None", ":", "\n", "                ", "if", "master", ".", "grad", "is", "None", ":", "\n", "                    ", "master", ".", "grad", "=", "Variable", "(", "master", ".", "data", ".", "new", "(", "*", "master", ".", "data", ".", "size", "(", ")", ")", ")", "\n", "", "master", ".", "grad", ".", "data", ".", "copy_", "(", "model", ".", "grad", ".", "data", ")", "\n", "", "else", ":", "\n", "                ", "master", ".", "grad", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.master_params_to_model_params": [[173, 188], ["zip", "zip", "torch._utils._unflatten_dense_tensors", "model.data.copy_", "model.data.copy_"], "function", ["None"], ["", "", "", "", "def", "master_params_to_model_params", "(", "model_params", ",", "master_params", ",", "flat_master", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Copy master parameters to model parameters.\n\n    Args:\n        model_params:  List of model parameters created by :func:`prep_param_lists`.\n        master_params:  List of FP32 master parameters created by :func:`prep_param_lists`.  If ``master_params`` was created with ``flat_master=True``, ``flat_master=True`` should also be supplied to :func:`master_params_to_model_params`.\n    \"\"\"", "\n", "if", "flat_master", ":", "\n", "        ", "for", "model", ",", "master", "in", "zip", "(", "model_params", ",", "\n", "_unflatten_dense_tensors", "(", "master_params", "[", "0", "]", ".", "data", ",", "model_params", ")", ")", ":", "\n", "            ", "model", ".", "data", ".", "copy_", "(", "master", ")", "\n", "", "", "else", ":", "\n", "        ", "for", "model", ",", "master", "in", "zip", "(", "model_params", ",", "master_params", ")", ":", "\n", "            ", "model", ".", "data", ".", "copy_", "(", "master", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.to_python_float": [[191, 196], ["hasattr", "t.item"], "function", ["None"], ["", "", "", "def", "to_python_float", "(", "t", ")", ":", "\n", "    ", "if", "hasattr", "(", "t", ",", "'item'", ")", ":", "\n", "        ", "return", "t", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "        ", "return", "t", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.LossScaler.__init__": [[38, 40], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "scale", "=", "1", ")", ":", "\n", "        ", "self", ".", "cur_scale", "=", "scale", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.LossScaler.has_overflow": [[42, 44], ["None"], "methods", ["None"], ["", "def", "has_overflow", "(", "self", ",", "params", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.LossScaler._has_inf_or_nan": [[46, 48], ["None"], "methods", ["None"], ["", "def", "_has_inf_or_nan", "(", "x", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.LossScaler.update_scale": [[49, 51], ["None"], "methods", ["None"], ["", "def", "update_scale", "(", "self", ",", "overflow", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.LossScaler.loss_scale": [[52, 55], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "loss_scale", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cur_scale", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.LossScaler.scale_gradient": [[56, 58], ["tuple"], "methods", ["None"], ["", "def", "scale_gradient", "(", "self", ",", "module", ",", "grad_in", ",", "grad_out", ")", ":", "\n", "        ", "return", "tuple", "(", "self", ".", "loss_scale", "*", "g", "for", "g", "in", "grad_in", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.LossScaler.backward": [[59, 62], ["scaled_loss.backward"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward"], ["", "def", "backward", "(", "self", ",", "loss", ",", "retain_graph", "=", "False", ")", ":", "\n", "        ", "scaled_loss", "=", "loss", "*", "self", ".", "loss_scale", "\n", "scaled_loss", ".", "backward", "(", "retain_graph", "=", "retain_graph", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.DynamicLossScaler.__init__": [[89, 105], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "init_scale", "=", "2", "**", "32", ",", "\n", "scale_factor", "=", "2.", ",", "\n", "scale_window", "=", "1000", ",", "\n", "min_scale", "=", "1", ",", "\n", "delayed_shift", "=", "1", ",", "\n", "consecutive_hysteresis", "=", "False", ")", ":", "\n", "        ", "self", ".", "cur_scale", "=", "init_scale", "\n", "self", ".", "cur_iter", "=", "0", "\n", "self", ".", "last_overflow_iter", "=", "-", "1", "\n", "self", ".", "scale_factor", "=", "scale_factor", "\n", "self", ".", "scale_window", "=", "scale_window", "\n", "self", ".", "min_scale", "=", "min_scale", "\n", "self", ".", "delayed_shift", "=", "delayed_shift", "\n", "self", ".", "cur_hysteresis", "=", "delayed_shift", "\n", "self", ".", "consecutive_hysteresis", "=", "consecutive_hysteresis", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.DynamicLossScaler.has_overflow_serial": [[107, 113], ["loss_scaler.DynamicLossScaler._has_inf_or_nan"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.DynamicLossScaler._has_inf_or_nan"], ["", "def", "has_overflow_serial", "(", "self", ",", "params", ")", ":", "\n", "        ", "for", "p", "in", "params", ":", "\n", "            ", "if", "p", ".", "grad", "is", "not", "None", "and", "DynamicLossScaler", ".", "_has_inf_or_nan", "(", "p", ".", "grad", ".", "data", ")", ":", "\n", "                ", "return", "True", "\n", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.DynamicLossScaler.has_overflow": [[114, 124], ["loss_scaler.DynamicLossScaler.has_overflow_serial", "torch.cuda.ByteTensor", "torch.distributed.all_reduce", "overflow_gpu[].item", "bool", "mpu.get_model_parallel_group"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.DynamicLossScaler.has_overflow_serial", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group"], ["", "def", "has_overflow", "(", "self", ",", "params", ")", ":", "\n", "        ", "overflow", "=", "self", ".", "has_overflow_serial", "(", "params", ")", "\n", "# Since each model parallel GPU carries only part of the model,", "\n", "# make sure overflow flag is synced across all the model parallel GPUs", "\n", "overflow_gpu", "=", "torch", ".", "cuda", ".", "ByteTensor", "(", "[", "overflow", "]", ")", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "overflow_gpu", ",", "\n", "op", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "MAX", ",", "\n", "group", "=", "mpu", ".", "get_model_parallel_group", "(", ")", ")", "\n", "overflow", "=", "overflow_gpu", "[", "0", "]", ".", "item", "(", ")", "\n", "return", "bool", "(", "overflow", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.DynamicLossScaler._has_inf_or_nan": [[127, 146], ["float", "x.float().sum", "float", "x.float", "float"], "methods", ["None"], ["", "def", "_has_inf_or_nan", "(", "x", ")", ":", "\n", "        ", "try", ":", "\n", "# if x is half, the .float() incurs an additional deep copy, but it's necessary if ", "\n", "# Pytorch's .sum() creates a one-element tensor of the same type as x ", "\n", "# (which is true for some recent version of pytorch).", "\n", "            ", "cpu_sum", "=", "float", "(", "x", ".", "float", "(", ")", ".", "sum", "(", ")", ")", "\n", "# More efficient version that can be used if .sum() returns a Python scalar", "\n", "# cpu_sum = float(x.sum())", "\n", "", "except", "RuntimeError", "as", "instance", ":", "\n", "# We want to check if inst is actually an overflow exception.", "\n", "# RuntimeError could come from a different error.", "\n", "# If so, we still want the exception to propagate.", "\n", "            ", "if", "\"value cannot be converted\"", "not", "in", "instance", ".", "args", "[", "0", "]", ":", "\n", "                ", "raise", "\n", "", "return", "True", "\n", "", "else", ":", "\n", "            ", "if", "cpu_sum", "==", "float", "(", "'inf'", ")", "or", "cpu_sum", "==", "-", "float", "(", "'inf'", ")", "or", "cpu_sum", "!=", "cpu_sum", ":", "\n", "                ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.DynamicLossScaler.update_scale": [[148, 173], ["hasattr", "hasattr", "hasattr", "hasattr", "max"], "methods", ["None"], ["", "", "def", "update_scale", "(", "self", ",", "overflow", ")", ":", "\n", "\n", "        ", "if", "not", "hasattr", "(", "self", ",", "'min_scale'", ")", ":", "\n", "            ", "self", ".", "min_scale", "=", "1", "\n", "", "if", "not", "hasattr", "(", "self", ",", "'delayed_shift'", ")", ":", "\n", "            ", "self", ".", "delayed_shift", "=", "1", "\n", "", "if", "not", "hasattr", "(", "self", ",", "'cur_hysteresis'", ")", ":", "\n", "            ", "self", ".", "cur_hysteresis", "=", "1", "\n", "", "if", "not", "hasattr", "(", "self", ",", "'consecutive_hysteresis'", ")", ":", "\n", "            ", "self", ".", "consecutive_hysteresis", "=", "True", "\n", "", "if", "overflow", ":", "\n", "# self.cur_scale /= self.scale_factor", "\n", "            ", "if", "self", ".", "delayed_shift", "==", "1", "or", "self", ".", "cur_hysteresis", "==", "1", ":", "\n", "                ", "self", ".", "cur_scale", "=", "max", "(", "self", ".", "cur_scale", "/", "self", ".", "scale_factor", ",", "self", ".", "min_scale", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "cur_hysteresis", "-=", "1", "\n", "", "self", ".", "last_overflow_iter", "=", "self", ".", "cur_iter", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "consecutive_hysteresis", ":", "\n", "                ", "self", ".", "cur_hysteresis", "=", "self", ".", "delayed_shift", "\n", "", "if", "(", "self", ".", "cur_iter", "-", "self", ".", "last_overflow_iter", ")", "%", "self", ".", "scale_window", "==", "0", ":", "\n", "                ", "if", "not", "self", ".", "consecutive_hysteresis", ":", "\n", "                    ", "self", ".", "cur_hysteresis", "=", "self", ".", "delayed_shift", "\n", "", "self", ".", "cur_scale", "*=", "self", ".", "scale_factor", "\n", "", "", "self", ".", "cur_iter", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.DynamicLossScaler.loss_scale": [[174, 177], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "loss_scale", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cur_scale", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.DynamicLossScaler.scale_gradient": [[178, 180], ["tuple"], "methods", ["None"], ["", "def", "scale_gradient", "(", "self", ",", "module", ",", "grad_in", ",", "grad_out", ")", ":", "\n", "        ", "return", "tuple", "(", "self", ".", "loss_scale", "*", "g", "for", "g", "in", "grad_in", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.DynamicLossScaler.backward": [[181, 184], ["scaled_loss.backward"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward"], ["", "def", "backward", "(", "self", ",", "loss", ",", "retain_graph", "=", "False", ")", ":", "\n", "        ", "scaled_loss", "=", "loss", "*", "self", ".", "loss_scale", "\n", "scaled_loss", ".", "backward", "(", "retain_graph", "=", "retain_graph", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.to_python_float": [[20, 25], ["hasattr", "t.item"], "function", ["None"], ["def", "to_python_float", "(", "t", ")", ":", "\n", "    ", "if", "hasattr", "(", "t", ",", "'item'", ")", ":", "\n", "        ", "return", "t", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "        ", "return", "t", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Module.__init__": [[60, 63], ["torch.nn.Module.__init__", "fp16.FP16_Module.add_module", "module.half"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "module", ")", ":", "\n", "        ", "super", "(", "FP16_Module", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_module", "(", "'module'", ",", "module", ".", "half", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Module.forward": [[64, 66], ["fp16.fp16_to_fp32", "fp16.FP16_Module.module", "fp16.fp32_to_fp16"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.fp16_to_fp32", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.fp32_to_fp16"], ["", "def", "forward", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "fp16_to_fp32", "(", "self", ".", "module", "(", "*", "(", "fp32_to_fp16", "(", "inputs", ")", ")", ",", "**", "kwargs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Module.state_dict": [[67, 69], ["fp16.FP16_Module.module.state_dict"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.state_dict"], ["", "def", "state_dict", "(", "self", ",", "destination", "=", "None", ",", "prefix", "=", "''", ",", "keep_vars", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "module", ".", "state_dict", "(", "destination", ",", "prefix", ",", "keep_vars", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Module.load_state_dict": [[70, 72], ["fp16.FP16_Module.module.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "strict", "=", "True", ")", ":", "\n", "        ", "self", ".", "module", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.__init__": [[168, 241], ["enumerate", "fp16.FP16_Optimizer.optimizer.load_state_dict", "SystemError", "fp16.FP16_Optimizer.maybe_print", "enumerate", "fp16.FP16_Optimizer.fp16_groups.append", "fp16.FP16_Optimizer.fp32_from_fp16_groups.append", "fp16.FP16_Optimizer.fp32_from_fp32_groups.append", "fp16.FP16_Optimizer.optimizer.state_dict", "loss_scaler.LossScaler", "loss_scaler.DynamicLossScaler", "loss_scaler.DynamicLossScaler", "param.type", "fp16.FP16_Optimizer.maybe_print", "fp16_params_this_group.append", "param.detach().clone().float", "fp32_from_fp16_params_this_group.append", "fp16.FP16_Optimizer.optimizer.state.pop", "param.type", "fp16.FP16_Optimizer.maybe_print", "fp32_params_this_group.append", "TypeError", "param.size", "param.detach().clone", "param.size", "param.type", "param.detach"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.load_state_dict", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.maybe_print", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.state_dict", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.maybe_print", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.maybe_print", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["def", "__init__", "(", "self", ",", "\n", "init_optimizer", ",", "\n", "static_loss_scale", "=", "1.0", ",", "\n", "dynamic_loss_scale", "=", "False", ",", "\n", "dynamic_loss_args", "=", "None", ",", "\n", "verbose", "=", "False", ")", ":", "\n", "        ", "if", "not", "torch", ".", "cuda", ".", "is_available", ":", "\n", "            ", "raise", "SystemError", "(", "\"Cannot use fp16 without CUDA.\"", ")", "\n", "\n", "", "self", ".", "verbose", "=", "verbose", "\n", "\n", "self", ".", "optimizer", "=", "init_optimizer", "\n", "# init_state_dict sets up an alternative way to cast per-param state tensors.", "\n", "# Stashing here in case https://github.com/pytorch/pytorch/issues/7733 makes it necessary.", "\n", "# init_state_dict = init_optimizer.state_dict()", "\n", "\n", "self", ".", "fp16_groups", "=", "[", "]", "\n", "self", ".", "fp32_from_fp16_groups", "=", "[", "]", "\n", "self", ".", "fp32_from_fp32_groups", "=", "[", "]", "\n", "for", "i", ",", "param_group", "in", "enumerate", "(", "self", ".", "optimizer", ".", "param_groups", ")", ":", "\n", "            ", "self", ".", "maybe_print", "(", "\"FP16_Optimizer processing param group {}:\"", ".", "format", "(", "i", ")", ")", "\n", "fp16_params_this_group", "=", "[", "]", "\n", "fp32_params_this_group", "=", "[", "]", "\n", "fp32_from_fp16_params_this_group", "=", "[", "]", "\n", "for", "i", ",", "param", "in", "enumerate", "(", "param_group", "[", "'params'", "]", ")", ":", "\n", "                ", "if", "param", ".", "requires_grad", ":", "\n", "                    ", "if", "param", ".", "type", "(", ")", "==", "'torch.cuda.HalfTensor'", ":", "\n", "                        ", "self", ".", "maybe_print", "(", "\"FP16_Optimizer received torch.cuda.HalfTensor with {}\"", "\n", ".", "format", "(", "param", ".", "size", "(", ")", ")", ")", "\n", "fp16_params_this_group", ".", "append", "(", "param", ")", "\n", "master_param", "=", "param", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "float", "(", ")", "\n", "master_param", ".", "requires_grad", "=", "True", "\n", "# Copythe model parallel flag.", "\n", "master_param", ".", "model_parallel", "=", "param", ".", "model_parallel", "\n", "param_group", "[", "'params'", "]", "[", "i", "]", "=", "master_param", "\n", "fp32_from_fp16_params_this_group", ".", "append", "(", "master_param", ")", "\n", "# Reset existing state dict key to the new master param.", "\n", "# We still need to recast per-param state tensors, if any, to FP32.", "\n", "if", "param", "in", "self", ".", "optimizer", ".", "state", ":", "\n", "                           ", "self", ".", "optimizer", ".", "state", "[", "master_param", "]", "=", "self", ".", "optimizer", ".", "state", ".", "pop", "(", "param", ")", "\n", "", "", "elif", "param", ".", "type", "(", ")", "==", "'torch.cuda.FloatTensor'", ":", "\n", "                        ", "self", ".", "maybe_print", "(", "\"FP16_Optimizer received torch.cuda.FloatTensor with {}\"", "\n", ".", "format", "(", "param", ".", "size", "(", ")", ")", ")", "\n", "fp32_params_this_group", ".", "append", "(", "param", ")", "\n", "param_group", "[", "'params'", "]", "[", "i", "]", "=", "param", "\n", "", "else", ":", "\n", "                        ", "raise", "TypeError", "(", "\"Wrapped parameters must be either \"", "\n", "\"torch.cuda.FloatTensor or torch.cuda.HalfTensor. \"", "\n", "\"Received {}\"", ".", "format", "(", "param", ".", "type", "(", ")", ")", ")", "\n", "\n", "", "", "", "self", ".", "fp16_groups", ".", "append", "(", "fp16_params_this_group", ")", "\n", "self", ".", "fp32_from_fp16_groups", ".", "append", "(", "fp32_from_fp16_params_this_group", ")", "\n", "self", ".", "fp32_from_fp32_groups", ".", "append", "(", "fp32_params_this_group", ")", "\n", "\n", "# Leverage state_dict() and load_state_dict() to recast preexisting per-param state tensors", "\n", "", "self", ".", "optimizer", ".", "load_state_dict", "(", "self", ".", "optimizer", ".", "state_dict", "(", ")", ")", "\n", "# alternative way to cast per-param state tensors:", "\n", "# self.optimizer.load_state_dict(init_state_dict)", "\n", "\n", "if", "dynamic_loss_scale", ":", "\n", "            ", "self", ".", "dynamic_loss_scale", "=", "True", "\n", "if", "dynamic_loss_args", "is", "not", "None", ":", "\n", "                ", "self", ".", "loss_scaler", "=", "DynamicLossScaler", "(", "**", "dynamic_loss_args", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "loss_scaler", "=", "DynamicLossScaler", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "dynamic_loss_scale", "=", "False", "\n", "self", ".", "loss_scaler", "=", "LossScaler", "(", "static_loss_scale", ")", "\n", "\n", "", "self", ".", "overflow", "=", "False", "\n", "self", ".", "first_closure_call_this_step", "=", "True", "\n", "\n", "self", ".", "clip_grad_norm", "=", "clip_grad_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.maybe_print": [[242, 245], ["print"], "methods", ["None"], ["", "def", "maybe_print", "(", "self", ",", "msg", ")", ":", "\n", "        ", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.__getstate__": [[246, 248], ["RuntimeError"], "methods", ["None"], ["", "", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"FP16_Optimizer should be serialized using state_dict().\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.__setstate__": [[249, 251], ["RuntimeError"], "methods", ["None"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"FP16_Optimizer should be deserialized using load_state_dict().\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.zero_grad": [[252, 277], ["p.grad.detach_", "p.grad.zero_", "param.grad.detach_", "param.grad.zero_"], "methods", ["None"], ["", "def", "zero_grad", "(", "self", ",", "set_grads_to_None", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Zero fp32 and fp16 parameter grads.\n        \"\"\"", "\n", "# In principle, only the .grad attributes of the model params need to be zeroed,", "\n", "# because gradients are copied into the FP32 master params.  However, we zero", "\n", "# all gradients owned by the optimizer, just to be safe:", "\n", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "             ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                 ", "if", "set_grads_to_None", ":", "\n", "                     ", "p", ".", "grad", "=", "None", "\n", "", "else", ":", "\n", "                     ", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                         ", "p", ".", "grad", ".", "detach_", "(", ")", "\n", "p", ".", "grad", ".", "zero_", "(", ")", "\n", "\n", "# Zero fp16 gradients owned by the model:", "\n", "", "", "", "", "for", "fp16_group", "in", "self", ".", "fp16_groups", ":", "\n", "            ", "for", "param", "in", "fp16_group", ":", "\n", "                ", "if", "set_grads_to_None", ":", "\n", "                    ", "param", ".", "grad", "=", "None", "\n", "", "else", ":", "\n", "                    ", "if", "param", ".", "grad", "is", "not", "None", ":", "\n", "                        ", "param", ".", "grad", ".", "detach_", "(", ")", "# as in torch.optim.optimizer.zero_grad()", "\n", "param", ".", "grad", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._check_overflow": [[278, 287], ["fp16.FP16_Optimizer.loss_scaler.has_overflow", "params.append", "params.append"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.DynamicLossScaler.has_overflow", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "", "", "", "", "def", "_check_overflow", "(", "self", ")", ":", "\n", "        ", "params", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "fp16_groups", ":", "\n", "            ", "for", "param", "in", "group", ":", "\n", "                ", "params", ".", "append", "(", "param", ")", "\n", "", "", "for", "group", "in", "self", ".", "fp32_from_fp32_groups", ":", "\n", "            ", "for", "param", "in", "group", ":", "\n", "                ", "params", ".", "append", "(", "param", ")", "\n", "", "", "self", ".", "overflow", "=", "self", ".", "loss_scaler", ".", "has_overflow", "(", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._update_scale": [[288, 290], ["fp16.FP16_Optimizer.loss_scaler.update_scale"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.loss_scaler.DynamicLossScaler.update_scale"], ["", "def", "_update_scale", "(", "self", ",", "has_overflow", "=", "False", ")", ":", "\n", "        ", "self", ".", "loss_scaler", ".", "update_scale", "(", "has_overflow", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._master_params_to_model_params": [[291, 294], ["zip", "fp16util.master_params_to_model_params"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.master_params_to_model_params"], ["", "def", "_master_params_to_model_params", "(", "self", ")", ":", "\n", "        ", "for", "fp16_group", ",", "fp32_from_fp16_group", "in", "zip", "(", "self", ".", "fp16_groups", ",", "self", ".", "fp32_from_fp16_groups", ")", ":", "\n", "            ", "master_params_to_model_params", "(", "fp16_group", ",", "fp32_from_fp16_group", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._model_params_to_master_params": [[295, 298], ["zip", "fp16util.master_params_to_model_params"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.master_params_to_model_params"], ["", "", "def", "_model_params_to_master_params", "(", "self", ")", ":", "\n", "        ", "for", "fp16_group", ",", "fp32_from_fp16_group", "in", "zip", "(", "self", ".", "fp16_groups", ",", "self", ".", "fp32_from_fp16_groups", ")", ":", "\n", "            ", "master_params_to_model_params", "(", "fp32_from_fp16_group", ",", "fp16_group", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._model_grads_to_master_grads": [[301, 304], ["zip", "fp16util.model_grads_to_master_grads"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16util.model_grads_to_master_grads"], ["", "", "def", "_model_grads_to_master_grads", "(", "self", ")", ":", "\n", "        ", "for", "fp16_group", ",", "fp32_from_fp16_group", "in", "zip", "(", "self", ".", "fp16_groups", ",", "self", ".", "fp32_from_fp16_groups", ")", ":", "\n", "            ", "model_grads_to_master_grads", "(", "fp16_group", ",", "fp32_from_fp16_group", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._downscale_master": [[305, 311], ["param.grad.data.mul_"], "methods", ["None"], ["", "", "def", "_downscale_master", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "loss_scale", "!=", "1.0", ":", "\n", "            ", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "                ", "for", "param", "in", "group", "[", "'params'", "]", ":", "\n", "                    ", "if", "param", ".", "grad", "is", "not", "None", ":", "\n", "                        ", "param", ".", "grad", ".", "data", ".", "mul_", "(", "1.", "/", "self", ".", "loss_scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.clip_master_grads": [[312, 335], ["fp16.FP16_Optimizer.clip_grad_norm", "fp32_params.append"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.grads.clip_grad_norm", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "", "", "", "", "def", "clip_master_grads", "(", "self", ",", "max_norm", ",", "norm_type", "=", "2", ")", ":", "\n", "        ", "\"\"\"\n        Clips fp32 master gradients via ``torch.nn.utils.clip_grad_norm``.\n\n        Args:\n            max_norm (float or int): max norm of the gradients\n            norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n                infinity norm.\n\n        Returns:\n            Total norm of the current fp32 gradients (viewed as a single vector).\n\n        .. warning::\n            Returns -1 if the most recently computed fp16 gradients overflowed (that is, if ``self.overflow`` is ``True``).\n        \"\"\"", "\n", "if", "not", "self", ".", "overflow", ":", "\n", "            ", "fp32_params", "=", "[", "]", "\n", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "                ", "for", "param", "in", "param_group", "[", "'params'", "]", ":", "\n", "                    ", "fp32_params", ".", "append", "(", "param", ")", "\n", "", "", "return", "self", ".", "clip_grad_norm", "(", "fp32_params", ",", "max_norm", ",", "norm_type", ")", "\n", "", "else", ":", "\n", "            ", "return", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.state_dict": [[336, 356], ["fp16.FP16_Optimizer.optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.state_dict"], ["", "", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\n        of the contained Pytorch optimizer.\n        Example::\n\n            checkpoint = {}\n            checkpoint['model'] = model.state_dict()\n            checkpoint['optimizer'] = optimizer.state_dict()\n            torch.save(checkpoint, \"saved.pth\")\n        \"\"\"", "\n", "state_dict", "=", "{", "}", "\n", "state_dict", "[", "'loss_scaler'", "]", "=", "self", ".", "loss_scaler", "\n", "state_dict", "[", "'dynamic_loss_scale'", "]", "=", "self", ".", "dynamic_loss_scale", "\n", "state_dict", "[", "'overflow'", "]", "=", "self", ".", "overflow", "\n", "state_dict", "[", "'first_closure_call_this_step'", "]", "=", "self", ".", "first_closure_call_this_step", "\n", "state_dict", "[", "'optimizer_state_dict'", "]", "=", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "state_dict", "[", "'fp32_from_fp16'", "]", "=", "self", ".", "fp32_from_fp16_groups", "\n", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.load_state_dict": [[357, 398], ["fp16.FP16_Optimizer.optimizer.load_state_dict", "zip", "zip", "current.data.copy_"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"\n        Loads a state_dict created by an earlier call to state_dict(). \n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``, \n        whose parameters in turn came from ``model``, it is expected that the user \n        will call ``model.load_state_dict()`` before\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\n\n        Example::\n\n            model = torch.nn.Linear(D_in, D_out).cuda().half()\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n            ...\n            checkpoint = torch.load(\"saved.pth\")\n            model.load_state_dict(checkpoint['model'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n        \"\"\"", "\n", "# I think it should actually be ok to reload the optimizer before the model.", "\n", "self", ".", "loss_scaler", "=", "state_dict", "[", "'loss_scaler'", "]", "\n", "self", ".", "dynamic_loss_scale", "=", "state_dict", "[", "'dynamic_loss_scale'", "]", "\n", "self", ".", "overflow", "=", "state_dict", "[", "'overflow'", "]", "\n", "self", ".", "first_closure_call_this_step", "=", "state_dict", "[", "'first_closure_call_this_step'", "]", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "state_dict", "[", "'optimizer_state_dict'", "]", ")", "\n", "# At this point, the optimizer's references to the model's fp32 parameters are up to date.", "\n", "# The optimizer's hyperparameters and internal buffers are also up to date.  ", "\n", "# However, the fp32 master copies of the model's fp16 params stored by the optimizer are still", "\n", "# out of date.  There are two options.  ", "\n", "# 1:  Refresh the master params from the model's fp16 params.  ", "\n", "# This requires less storage but incurs precision loss.", "\n", "# 2:  Save and restore the fp32 master copies separately.", "\n", "# We choose option 2.", "\n", "# ", "\n", "# Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device ", "\n", "# of their associated parameters, because it's possible those buffers might not exist yet in ", "\n", "# the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been ", "\n", "# constructed in the same way as the one whose state_dict we are loading, the same master params", "\n", "# are guaranteed to exist, so we can just copy_() from the saved master params.", "\n", "for", "current_group", ",", "saved_group", "in", "zip", "(", "self", ".", "fp32_from_fp16_groups", ",", "state_dict", "[", "'fp32_from_fp16'", "]", ")", ":", "\n", "            ", "for", "current", ",", "saved", "in", "zip", "(", "current_group", ",", "saved_group", ")", ":", "\n", "                ", "current", ".", "data", ".", "copy_", "(", "saved", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.step": [[399, 454], ["fp16.FP16_Optimizer._update_scale", "fp16.FP16_Optimizer._master_params_to_model_params", "fp16.FP16_Optimizer.maybe_print", "fp16.FP16_Optimizer._step_with_closure", "fp16.FP16_Optimizer.optimizer.step"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._update_scale", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._master_params_to_model_params", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.maybe_print", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._step_with_closure", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.step"], ["", "", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "# could add clip option.", "\n", "        ", "\"\"\"\n        If no closure is supplied, :attr:`step` should be called after \n        ``fp16_optimizer_obj.backward(loss)``.\n        :attr:`step` updates the fp32 master copy of parameters using the optimizer supplied to\n        :class:`FP16_Optimizer`'s constructor, then copies the updated fp32 params into the fp16 params\n        originally referenced by :class:`FP16_Optimizer`'s constructor, so the user may immediately run\n        another forward pass using their model.\n\n        If a closure is supplied, :attr:`step` may be called without a prior call to \n        :attr:`backward(loss)`.\n        This control flow is identical to `ordinary Pytorch optimizer use`_ with closures.\n        However, the user should take care that any ``loss.backward()`` call within the closure\n        has been replaced by ``fp16_optimizer_obj.backward(loss)``.\n\n        Args:\n           closure (optional):  Closure that will be supplied to the underlying optimizer originally passed to :class:`FP16_Optimizer`'s constructor.  closure should call :attr:`zero_grad()` on the :class:`FP16_Optimizer` object, compute the loss, call :attr:`backward(loss)`, and return the loss.\n\n        Example with closure::\n\n            # optimizer is assumed to be an FP16_Optimizer object, previously constructed from an \n            # existing pytorch optimizer.\n            for input, target in dataset:\n                def closure():\n                    optimizer.zero_grad()\n                    output = model(input)\n                    loss = loss_fn(output, target)\n                    # loss.backward() becomes:\n                    optimizer.backward(loss)\n                    return loss\n                optimizer.step(closure)\n\n        .. warning::\n            Currently, calling :attr:`step` with a closure is not compatible with dynamic loss scaling.\n\n        .. _`ordinary Pytorch optimizer use`:\n            http://pytorch.org/docs/master/optim.html#optimizer-step-closure\n        \"\"\"", "\n", "\n", "scale", "=", "self", ".", "loss_scaler", ".", "loss_scale", "\n", "self", ".", "_update_scale", "(", "self", ".", "overflow", ")", "\n", "\n", "if", "self", ".", "overflow", ":", "\n", "            ", "self", ".", "maybe_print", "(", "\"OVERFLOW! Skipping step. Attempted loss scale: {}, reducing to {}\"", "\n", ".", "format", "(", "scale", ",", "self", ".", "loss_scale", ")", ")", "\n", "return", "\n", "\n", "", "if", "closure", "is", "not", "None", ":", "\n", "            ", "retval", "=", "self", ".", "_step_with_closure", "(", "closure", ")", "\n", "", "else", ":", "\n", "            ", "retval", "=", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "self", ".", "_master_params_to_model_params", "(", ")", "\n", "\n", "return", "retval", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._step_with_closure": [[455, 493], ["fp16.FP16_Optimizer.optimizer.step", "closure", "fp16.FP16_Optimizer._master_params_to_model_params", "fp16.FP16_Optimizer._update_scale", "fp16.FP16_Optimizer.maybe_print", "closure"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.step", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._master_params_to_model_params", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._update_scale", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.maybe_print"], ["", "def", "_step_with_closure", "(", "self", ",", "closure", ")", ":", "\n", "        ", "def", "wrapped_closure", "(", ")", ":", "\n", "# helpful for debugging", "\n", "# print(\"Calling wrapped_closure, first_closure_call_this_step = {}\"", "\n", "#       .format(self.first_closure_call_this_step))", "\n", "            ", "if", "self", ".", "first_closure_call_this_step", ":", "\n", "# We expect that the fp16 params are initially fresh on entering self.step(),", "\n", "# so _master_params_to_model_params() is unnecessary the first time wrapped_closure()", "\n", "# is called within self.optimizer.step().", "\n", "                ", "self", ".", "first_closure_call_this_step", "=", "False", "\n", "", "else", ":", "\n", "# If self.optimizer.step() internally calls wrapped_closure more than once,", "\n", "# it may update the fp32 params after each call.  However, self.optimizer ", "\n", "# doesn't know about the fp16 params at all.  If the fp32 params get updated,", "\n", "# we can't rely on self.optimizer to refresh the fp16 params.  We need", "\n", "# to handle that manually:", "\n", "                ", "self", ".", "_master_params_to_model_params", "(", ")", "\n", "# Our API expects the user to give us ownership of the backward() call by", "\n", "# replacing all calls to loss.backward() with optimizer.backward(loss).", "\n", "# This requirement holds whether or not the call to backward() is made within a closure.", "\n", "# If the user is properly calling optimizer.backward(loss) within \"closure,\" ", "\n", "# calling closure() here will give the fp32 master params fresh gradients", "\n", "# for the optimizer to play with, so all wrapped_closure needs to do is call ", "\n", "# closure() and return the loss.", "\n", "", "temp_loss", "=", "closure", "(", ")", "\n", "while", "(", "self", ".", "overflow", ")", ":", "\n", "                ", "scale", "=", "self", ".", "loss_scaler", ".", "loss_scale", "\n", "self", ".", "_update_scale", "(", "self", ".", "overflow", ")", "\n", "self", ".", "maybe_print", "(", "\"OVERFLOW within closure! Skipping step. Attempted loss scale: {}, \"", "\n", "\"reducing to {}\"", ".", "format", "(", "scale", ",", "self", ".", "loss_scale", ")", ")", "\n", "temp_loss", "=", "closure", "(", ")", "\n", "", "return", "temp_loss", "\n", "\n", "", "retval", "=", "self", ".", "optimizer", ".", "step", "(", "wrapped_closure", ")", "\n", "\n", "self", ".", "first_closure_call_this_step", "=", "True", "\n", "\n", "return", "retval", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.backward": [[494, 555], ["fp16.FP16_Optimizer.loss_scaler.backward", "loss.float", "fp16.FP16_Optimizer.update_master_grads"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.update_master_grads"], ["", "def", "backward", "(", "self", ",", "loss", ",", "update_master_grads", "=", "True", ",", "retain_graph", "=", "False", ")", ":", "\n", "        ", "\"\"\" \n        :attr:`backward` performs the following conceptual steps:\n\n        1. fp32_loss = loss.float() (see first Note below)\n        2. scaled_loss = fp32_loss*loss_scale\n        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model's leaves (which may be fp16, fp32, or a mixture, depending how your model was defined).\n        4. fp16 grads are then copied to the master params' ``.grad`` attributes (see second Note), which are guaranteed to be fp32.\n        5. Finally, master grads are divided by loss_scale.\n\n        In this way, after :attr:`backward`, the master params have fresh gradients,\n        and :attr:`step` may be called.\n\n        .. note::\n            :attr:`backward` internally converts the loss to fp32 before applying the loss scale.\n            This provides some additional safety against overflow if the user has supplied an \n            fp16 loss value.  \n            However, for maximum overflow safety, the user should\n            compute the loss criterion (MSE, cross entropy, etc) in fp32 before supplying it to \n            :attr:`backward`.\n\n        .. warning::\n            The gradients found in a model's leaves after the call to \n            :attr:`backward` should not be regarded as valid in general, \n            because it's possible \n            they have been scaled (and in the case of dynamic loss scaling, \n            the scale factor may change over time).  \n            If the user wants to inspect gradients after a call to :attr:`backward`,  \n            only the master gradients should be regarded as valid.  These can be retrieved via\n            :attr:`inspect_master_grad_data()`.\n\n        Args:\n            loss:  The loss output by the user's model.  loss may be either float or half (but see first Note above).\n            update_master_grads (bool, optional, default=True):  Option to copy fp16 grads to fp32 grads on this call.  By setting this to False, the user can delay the copy, which is useful to eliminate redundant fp16->fp32 grad copies if :attr:`backward` is being called on multiple losses in one iteration.  If set to False, the user becomes responsible for calling :attr:`update_master_grads` before calling :attr:`step`.\n            retain_graph (bool, optional, default=False):  Forwards the usual ``retain_graph=True`` option to the internal call to ``loss.backward``.  If ``retain_graph`` is being used to accumulate gradient values from multiple backward passes before calling ``optimizer.step``, passing ``update_master_grads=False`` is also recommended (see Example below).\n\n        Example::\n\n            # Ordinary operation:\n            optimizer.backward(loss)\n\n            # Naive operation with multiple losses (technically valid, but less efficient):\n            # fp32 grads will be correct after the second call,  but \n            # the first call incurs an unnecessary fp16->fp32 grad copy.\n            optimizer.backward(loss1)\n            optimizer.backward(loss2)\n\n            # More efficient way to handle multiple losses:\n            # The fp16->fp32 grad copy is delayed until fp16 grads from all \n            # losses have been accumulated.\n            optimizer.backward(loss1, update_master_grads=False)\n            optimizer.backward(loss2, update_master_grads=False)\n            optimizer.update_master_grads()\n        \"\"\"", "\n", "# To consider:  try multiple backward passes using retain_grad=True to find ", "\n", "# a loss scale that works.  After you find a loss scale that works, do a final dummy", "\n", "# backward pass with retain_graph=False to tear down the graph.  Doing this would avoid ", "\n", "# discarding the iteration,  but probably wouldn't improve overall efficiency.  ", "\n", "self", ".", "loss_scaler", ".", "backward", "(", "loss", ".", "float", "(", ")", ",", "retain_graph", "=", "retain_graph", ")", "\n", "if", "update_master_grads", ":", "\n", "            ", "self", ".", "update_master_grads", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.update_master_grads": [[556, 568], ["fp16.FP16_Optimizer._model_grads_to_master_grads", "fp16.FP16_Optimizer._downscale_master", "fp16.FP16_Optimizer._check_overflow"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._model_grads_to_master_grads", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._downscale_master", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._check_overflow"], ["", "", "def", "update_master_grads", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Copy the ``.grad`` attribute from stored references to fp16 parameters to \n        the ``.grad`` attribute of the fp32 master parameters that are directly \n        updated by the optimizer.  :attr:`update_master_grads` only needs to be called if\n        ``fp16_optimizer_obj.backward`` was called with ``update_master_grads=False``.\n        \"\"\"", "\n", "if", "self", ".", "dynamic_loss_scale", ":", "\n", "            ", "self", ".", "_check_overflow", "(", ")", "\n", "if", "self", ".", "overflow", ":", "return", "\n", "", "self", ".", "_model_grads_to_master_grads", "(", ")", "\n", "self", ".", "_downscale_master", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer.inspect_master_grad_data": [[569, 601], ["print", "master_grads_data.append", "master_grads_this_group.append", "master_grads_this_group.append"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "inspect_master_grad_data", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        When running with :class:`FP16_Optimizer`, \n        ``.grad`` attributes of a model's fp16 leaves should not be\n        regarded as truthful, because they might be scaled.  \n        After a call to :attr:`fp16_optimizer_obj.backward(loss)`, if no overflow was encountered,\n        the fp32 master params' ``.grad``\n        attributes will contain valid gradients properly divided by the loss scale.  However, \n        because :class:`FP16_Optimizer` flattens some parameters, accessing them may be \n        nonintuitive.  :attr:`inspect_master_grad_data`\n        allows those gradients to be viewed with shapes corresponding to their associated model leaves.\n\n        Returns:\n            List of lists (one list for each parameter group).  The list for each parameter group\n            is a list of the ``.grad.data`` attributes of the fp32 master params belonging to that group.                 \n        \"\"\"", "\n", "if", "self", ".", "overflow", ":", "\n", "            ", "print", "(", "\"Warning:  calling FP16_Optimizer.inspect_master_grad_data while in an overflow state.  \"", "\n", "\"Gradients are currently invalid (may be inf, nan, or stale).  Returning None.\"", ")", "\n", "return", "None", "\n", "", "else", ":", "\n", "# The optimizer owns only references to master params.", "\n", "            ", "master_grads_data", "=", "[", "]", "\n", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "                ", "master_grads_this_group", "=", "[", "]", "\n", "for", "param", "in", "param_group", "[", "'params'", "]", ":", "\n", "                    ", "if", "param", ".", "grad", "is", "not", "None", ":", "\n", "                        ", "master_grads_this_group", ".", "append", "(", "param", ".", "grad", ".", "data", ")", "\n", "", "else", ":", "\n", "                        ", "master_grads_this_group", ".", "append", "(", "None", ")", "\n", "", "", "master_grads_data", ".", "append", "(", "master_grads_this_group", ")", "\n", "", "return", "master_grads_data", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._get_loss_scale": [[604, 606], ["None"], "methods", ["None"], ["", "", "def", "_get_loss_scale", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "loss_scaler", ".", "loss_scale", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._set_loss_scale": [[607, 609], ["None"], "methods", ["None"], ["", "def", "_set_loss_scale", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "loss_scaler", ".", "cur_scale", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._get_state": [[613, 615], ["None"], "methods", ["None"], ["def", "_get_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "optimizer", ".", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._set_state": [[616, 618], ["None"], "methods", ["None"], ["", "def", "_set_state", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "optimizer", ".", "state", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._get_param_groups": [[623, 625], ["None"], "methods", ["None"], ["def", "_get_param_groups", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "optimizer", ".", "param_groups", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.FP16_Optimizer._set_param_groups": [[626, 628], ["None"], "methods", ["None"], ["", "def", "_set_param_groups", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "optimizer", ".", "param_groups", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.conversion_helper": [[28, 36], ["isinstance", "isinstance", "conversion", "fp16.conversion_helper", "tuple"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.conversion_helper"], ["def", "conversion_helper", "(", "val", ",", "conversion", ")", ":", "\n", "    ", "\"\"\"Apply conversion to val. Recursively apply conversion if `val` is a nested tuple/list structure.\"\"\"", "\n", "if", "not", "isinstance", "(", "val", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "return", "conversion", "(", "val", ")", "\n", "", "rtn", "=", "[", "conversion_helper", "(", "v", ",", "conversion", ")", "for", "v", "in", "val", "]", "\n", "if", "isinstance", "(", "val", ",", "tuple", ")", ":", "\n", "        ", "rtn", "=", "tuple", "(", "rtn", ")", "\n", "", "return", "rtn", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.fp32_to_fp16": [[37, 47], ["fp16.conversion_helper", "isinstance", "isinstance", "val.half.half"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.conversion_helper"], ["", "def", "fp32_to_fp16", "(", "val", ")", ":", "\n", "    ", "\"\"\"Convert fp32 `val` to fp16\"\"\"", "\n", "def", "half_conversion", "(", "val", ")", ":", "\n", "        ", "val_typecheck", "=", "val", "\n", "if", "isinstance", "(", "val_typecheck", ",", "(", "Parameter", ",", "Variable", ")", ")", ":", "\n", "            ", "val_typecheck", "=", "val", ".", "data", "\n", "", "if", "isinstance", "(", "val_typecheck", ",", "FLOAT_TYPES", ")", ":", "\n", "            ", "val", "=", "val", ".", "half", "(", ")", "\n", "", "return", "val", "\n", "", "return", "conversion_helper", "(", "val", ",", "half_conversion", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.fp16_to_fp32": [[48, 58], ["fp16.conversion_helper", "isinstance", "isinstance", "val.float.float"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.fp16.fp16.conversion_helper"], ["", "def", "fp16_to_fp32", "(", "val", ")", ":", "\n", "    ", "\"\"\"Convert fp16 `val` to fp32\"\"\"", "\n", "def", "float_conversion", "(", "val", ")", ":", "\n", "        ", "val_typecheck", "=", "val", "\n", "if", "isinstance", "(", "val_typecheck", ",", "(", "Parameter", ",", "Variable", ")", ")", ":", "\n", "            ", "val_typecheck", "=", "val", ".", "data", "\n", "", "if", "isinstance", "(", "val_typecheck", ",", "HALF_TYPES", ")", ":", "\n", "            ", "val", "=", "val", ".", "float", "(", ")", "\n", "", "return", "val", "\n", "", "return", "conversion_helper", "(", "val", ",", "float_conversion", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.samplers.RandomSampler.__init__": [[36, 52], ["ValueError", "ValueError", "isinstance", "ValueError", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "data_source", ",", "replacement", "=", "False", ",", "num_samples", "=", "None", ")", ":", "\n", "        ", "self", ".", "data_source", "=", "data_source", "\n", "self", ".", "replacement", "=", "replacement", "\n", "self", ".", "_num_samples", "=", "num_samples", "\n", "self", ".", "epoch", "=", "-", "1", "\n", "\n", "if", "self", ".", "_num_samples", "is", "not", "None", "and", "replacement", "is", "False", ":", "\n", "            ", "raise", "ValueError", "(", "\"With replacement=False, num_samples should not be specified, \"", "\n", "\"since a random permute will be performed.\"", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "self", ".", "num_samples", ",", "int", ")", "or", "self", ".", "num_samples", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"num_samples should be a positive integer \"", "\n", "\"value, but got num_samples={}\"", ".", "format", "(", "self", ".", "num_samples", ")", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "replacement", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"replacement should be a boolean value, but got \"", "\n", "\"replacement={}\"", ".", "format", "(", "self", ".", "replacement", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.samplers.RandomSampler.num_samples": [[53, 59], ["len"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "num_samples", "(", "self", ")", ":", "\n", "# dataset size might change at runtime", "\n", "        ", "if", "self", ".", "_num_samples", "is", "None", ":", "\n", "            ", "return", "len", "(", "self", ".", "data_source", ")", "\n", "", "return", "self", ".", "_num_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.samplers.RandomSampler.__iter__": [[60, 68], ["len", "torch.Generator", "iter", "torch.Generator.manual_seed", "iter", "torch.randperm().tolist", "torch.randint().tolist", "torch.randperm", "torch.randint"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "n", "=", "len", "(", "self", ".", "data_source", ")", "\n", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "if", "self", ".", "epoch", ">=", "0", ":", "\n", "            ", "g", ".", "manual_seed", "(", "self", ".", "epoch", ")", "\n", "", "if", "self", ".", "replacement", ":", "\n", "            ", "return", "iter", "(", "torch", ".", "randint", "(", "high", "=", "n", ",", "size", "=", "(", "self", ".", "num_samples", ",", ")", ",", "dtype", "=", "torch", ".", "int64", ",", "generator", "=", "g", ")", ".", "tolist", "(", ")", ")", "\n", "", "return", "iter", "(", "torch", ".", "randperm", "(", "n", ",", "generator", "=", "g", ")", ".", "tolist", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.samplers.RandomSampler.__len__": [[69, 71], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.samplers.RandomSampler.set_epoch": [[72, 74], ["None"], "methods", ["None"], ["", "def", "set_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "self", ".", "epoch", "=", "epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.samplers.DistributedBatchSampler.__init__": [[81, 92], ["super().__init__", "torch.distributed.get_rank"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["def", "__init__", "(", "self", ",", "sampler", ",", "batch_size", ",", "drop_last", ",", "rank", "=", "-", "1", ",", "world_size", "=", "2", ",", "wrap_last", "=", "False", ")", ":", "\n", "        ", "super", "(", "DistributedBatchSampler", ",", "self", ")", ".", "__init__", "(", "sampler", ",", "batch_size", ",", "drop_last", ")", "\n", "if", "rank", "==", "-", "1", ":", "\n", "            ", "assert", "False", ",", "'should not be here'", "\n", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "", "self", ".", "rank", "=", "rank", "\n", "self", ".", "world_size", "=", "world_size", "\n", "self", ".", "sampler", ".", "wrap_around", "=", "0", "\n", "self", ".", "wrap_around", "=", "0", "\n", "self", ".", "wrap_last", "=", "wrap_last", "\n", "self", ".", "start_iter", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.samplers.DistributedBatchSampler.__iter__": [[93, 124], ["samplers.DistributedBatchSampler.data_iterator", "len", "batch.append", "len", "samplers.DistributedBatchSampler._batch", "numpy.array", "len", "isinstance", "samplers.DistributedBatchSampler._batch", "list", "enumerate", "samplers.DistributedBatchSampler.data_iterator", "batch.append", "len", "len"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.DistributedBatchSampler.data_iterator", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.DistributedBatchSampler._batch", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.DistributedBatchSampler._batch", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.DistributedBatchSampler.data_iterator", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "batch", "=", "[", "]", "\n", "last_batch", "=", "None", "\n", "i", "=", "0", "\n", "for", "idx", "in", "self", ".", "data_iterator", "(", "self", ".", "sampler", ",", "wrap_around", "=", "False", ")", ":", "\n", "            ", "batch", ".", "append", "(", "idx", ")", "\n", "if", "len", "(", "batch", ")", "==", "self", ".", "batch_size", ":", "\n", "                ", "tbatch", "=", "self", ".", "_batch", "(", "batch", ")", "\n", "if", "i", ">=", "self", ".", "start_iter", ":", "\n", "                    ", "yield", "tbatch", "\n", "self", ".", "start_iter", "=", "0", "\n", "", "i", "+=", "1", "\n", "last_batch", "=", "np", ".", "array", "(", "list", "(", "tbatch", ")", ")", "\n", "batch", "=", "[", "]", "\n", "", "", "batch_len", "=", "len", "(", "batch", ")", "\n", "if", "batch_len", ">", "0", "and", "not", "self", ".", "drop_last", ":", "\n", "            ", "if", "self", ".", "wrap_last", ":", "\n", "                ", "self", ".", "sampler", ".", "wrap_around", "-=", "(", "self", ".", "batch_size", ")", "\n", "self", ".", "wrap_around", "+=", "(", "len", "(", "batch", ")", ")", "\n", "self", ".", "wrap_around", "%=", "self", ".", "batch_size", "\n", "if", "isinstance", "(", "self", ".", "sampler", ",", "TransposedSampler", ")", ":", "\n", "                    ", "for", "i", ",", "idx", "in", "enumerate", "(", "self", ".", "data_iterator", "(", "self", ".", "sampler", ",", "wrap_around", "=", "True", ")", ")", ":", "\n", "                        ", "if", "i", "==", "0", ":", "\n", "                            ", "continue", "\n", "", "batch", ".", "append", "(", "idx", ")", "\n", "new_batch_len", "=", "len", "(", "batch", ")", "\n", "if", "len", "(", "batch", ")", "==", "self", ".", "batch_size", ":", "\n", "                            ", "break", "\n", "", "", "", "", "yield", "self", ".", "_batch", "(", "batch", ")", "\n", "", "if", "self", ".", "wrap_last", ":", "\n", "            ", "self", ".", "sampler", ".", "wrap_around", "+=", "self", ".", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.samplers.DistributedBatchSampler.data_iterator": [[125, 134], ["enumerate"], "methods", ["None"], ["", "", "def", "data_iterator", "(", "self", ",", "_iter", ",", "wrap_around", "=", "False", ")", ":", "\n", "        ", "\"\"\"iterates through data and handles wrap around\"\"\"", "\n", "for", "i", ",", "idx", "in", "enumerate", "(", "_iter", ")", ":", "\n", "            ", "if", "i", "<", "self", ".", "wrap_around", "%", "self", ".", "batch_size", ":", "\n", "                ", "continue", "\n", "", "if", "wrap_around", ":", "\n", "                ", "self", ".", "wrap_around", "+=", "1", "\n", "self", ".", "wrap_around", "%=", "self", ".", "batch_size", "\n", "", "yield", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.samplers.DistributedBatchSampler._batch": [[135, 140], ["None"], "methods", ["None"], ["", "", "def", "_batch", "(", "self", ",", "batch", ")", ":", "\n", "        ", "\"\"\"extracts samples only pertaining to this worker's batch\"\"\"", "\n", "start", "=", "self", ".", "rank", "*", "self", ".", "batch_size", "//", "self", ".", "world_size", "\n", "end", "=", "(", "self", ".", "rank", "+", "1", ")", "*", "self", ".", "batch_size", "//", "self", ".", "world_size", "\n", "return", "batch", "[", "start", ":", "end", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.ProcessorTokenizer.__init__": [[81, 84], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tokenizer", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "process_fn", "=", "process_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.ProcessorTokenizer.__call__": [[85, 91], ["lazy_loader.ProcessorTokenizer.tokenizer", "lazy_loader.ProcessorTokenizer.process_fn"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.tokenizer"], ["", "def", "__call__", "(", "self", ",", "string", ")", ":", "\n", "        ", "if", "self", ".", "tokenizer", "is", "not", "None", ":", "\n", "            ", "string", "=", "self", ".", "tokenizer", "(", "string", ",", "process_fn", "=", "self", ".", "process_fn", ")", "\n", "", "elif", "self", ".", "process_fn", "is", "not", "None", ":", "\n", "            ", "string", "=", "self", ".", "process_fn", "(", "string", ")", "\n", "", "return", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.lazy_array_loader.__init__": [[110, 128], ["lazy_loader.get_lazy_path", "os.path.join", "open", "os.path.join", "pickle.load", "list", "list", "torch.multiprocessing.Lock", "mmap.mmap", "open", "itertools.accumulate", "lazy_loader.lazy_array_loader.file.fileno"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.get_lazy_path"], ["def", "__init__", "(", "self", ",", "path", ",", "data_type", "=", "'data'", ",", "mem_map", "=", "False", ",", "map_fn", "=", "None", ")", ":", "\n", "        ", "lazypath", "=", "get_lazy_path", "(", "path", ")", "\n", "datapath", "=", "os", ".", "path", ".", "join", "(", "lazypath", ",", "data_type", ")", "\n", "#get file where array entries are concatenated into one big string", "\n", "self", ".", "_file", "=", "open", "(", "datapath", ",", "'rb'", ")", "\n", "self", ".", "file", "=", "self", ".", "_file", "\n", "#memory map file if necessary", "\n", "self", ".", "mem_map", "=", "mem_map", "\n", "if", "self", ".", "mem_map", ":", "\n", "            ", "self", ".", "file", "=", "mmap", ".", "mmap", "(", "self", ".", "file", ".", "fileno", "(", ")", ",", "0", ",", "prot", "=", "mmap", ".", "PROT_READ", ")", "\n", "", "lenpath", "=", "os", ".", "path", ".", "join", "(", "lazypath", ",", "data_type", "+", "'.len.pkl'", ")", "\n", "self", ".", "lens", "=", "pkl", ".", "load", "(", "open", "(", "lenpath", ",", "'rb'", ")", ")", "\n", "self", ".", "ends", "=", "list", "(", "accumulate", "(", "self", ".", "lens", ")", ")", "\n", "self", ".", "dumb_ends", "=", "list", "(", "self", ".", "ends", ")", "\n", "self", ".", "read_lock", "=", "Lock", "(", ")", "\n", "self", ".", "process_fn", "=", "map_fn", "\n", "self", ".", "map_fn", "=", "map_fn", "\n", "self", ".", "_tokenizer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.lazy_array_loader.SetTokenizer": [[129, 140], ["lazy_loader.ProcessorTokenizer", "hasattr"], "methods", ["None"], ["", "def", "SetTokenizer", "(", "self", ",", "tokenizer", ")", ":", "\n", "        ", "\"\"\"\n        logic to set and remove (set to None) tokenizer.\n        combines preprocessing/tokenization into one callable.\n        \"\"\"", "\n", "if", "tokenizer", "is", "None", ":", "\n", "            ", "if", "not", "hasattr", "(", "self", ",", "'_tokenizer'", ")", ":", "\n", "                ", "self", ".", "_tokenizer", "=", "tokenizer", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "_tokenizer", "=", "tokenizer", "\n", "", "self", ".", "map_fn", "=", "ProcessorTokenizer", "(", "tokenizer", ",", "self", ".", "process_fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.lazy_array_loader.GetTokenizer": [[141, 143], ["None"], "methods", ["None"], ["", "def", "GetTokenizer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.lazy_array_loader.__getitem__": [[144, 170], ["isinstance", "lazy_loader.lazy_array_loader.file_read", "lazy_loader.lazy_array_loader.file_read", "lazy_loader.split_strings", "lazy_loader.lazy_array_loader.map_fn", "lazy_loader.lazy_array_loader.map_fn"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.lazy_array_loader.file_read", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.lazy_array_loader.file_read", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.split_strings"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"\n        read file and splice strings based on string ending array `self.ends`\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "index", ",", "slice", ")", ":", "\n", "            ", "if", "index", "==", "0", ":", "\n", "                ", "start", "=", "0", "\n", "", "else", ":", "\n", "                ", "start", "=", "self", ".", "ends", "[", "index", "-", "1", "]", "\n", "", "end", "=", "self", ".", "ends", "[", "index", "]", "\n", "rtn", "=", "self", ".", "file_read", "(", "start", ",", "end", ")", "\n", "if", "self", ".", "map_fn", "is", "not", "None", ":", "\n", "                ", "return", "self", ".", "map_fn", "(", "rtn", ")", "\n", "", "", "else", ":", "\n", "# if slice, fetch strings with 1 diskread and then splice in memory", "\n", "            ", "chr_lens", "=", "self", ".", "ends", "[", "index", "]", "\n", "if", "index", ".", "start", "==", "0", "or", "index", ".", "start", "is", "None", ":", "\n", "                ", "start", "=", "0", "\n", "", "else", ":", "\n", "                ", "start", "=", "self", ".", "ends", "[", "index", ".", "start", "-", "1", "]", "\n", "", "stop", "=", "chr_lens", "[", "-", "1", "]", "\n", "strings", "=", "self", ".", "file_read", "(", "start", ",", "stop", ")", "\n", "rtn", "=", "split_strings", "(", "strings", ",", "start", ",", "chr_lens", ")", "\n", "if", "self", ".", "map_fn", "is", "not", "None", ":", "\n", "                ", "return", "self", ".", "map_fn", "(", "[", "s", "for", "s", "in", "rtn", "]", ")", "\n", "", "", "return", "rtn", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.lazy_array_loader.__len__": [[171, 173], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "ends", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.lazy_array_loader.file_read": [[174, 195], ["lazy_loader.lazy_array_loader.read_lock.acquire", "lazy_loader.lazy_array_loader.file.seek", "lazy_loader.lazy_array_loader.read_lock.release", "rtn.decode.decode.decode", "lazy_loader.lazy_array_loader.file.read", "lazy_loader.lazy_array_loader.file.read", "rtn.decode.decode.decode"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.decode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.decode"], ["", "def", "file_read", "(", "self", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "\n", "        ", "\"\"\"read specified portion of file\"\"\"", "\n", "\n", "# atomic reads to avoid race conditions with multiprocess dataloader", "\n", "self", ".", "read_lock", ".", "acquire", "(", ")", "\n", "# seek to start of file read", "\n", "self", ".", "file", ".", "seek", "(", "start", ")", "\n", "# read to end of file if no end point provided", "\n", "if", "end", "is", "None", ":", "\n", "            ", "rtn", "=", "self", ".", "file", ".", "read", "(", ")", "\n", "#else read amount needed to reach end point", "\n", "", "else", ":", "\n", "            ", "rtn", "=", "self", ".", "file", ".", "read", "(", "end", "-", "start", ")", "\n", "", "self", ".", "read_lock", ".", "release", "(", ")", "\n", "#TODO: @raulp figure out mem map byte string bug", "\n", "#if mem map'd need to decode byte string to string", "\n", "rtn", "=", "rtn", ".", "decode", "(", "'utf-8'", ",", "'ignore'", ")", "\n", "# rtn = str(rtn)", "\n", "if", "self", ".", "mem_map", ":", "\n", "            ", "rtn", "=", "rtn", ".", "decode", "(", "'unicode_escape'", ")", "\n", "", "return", "rtn", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.get_lazy_path": [[25, 30], ["os.path.splitext"], "function", ["None"], ["def", "get_lazy_path", "(", "path", ")", ":", "\n", "    ", "\"\"\"\n    Gets directory path where lazy files are stored.\n    \"\"\"", "\n", "return", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "0", "]", "+", "'.lazy'", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.exists_lazy": [[31, 43], ["os.listdir", "os.path.exists", "lazy_loader.get_lazy_path", "lazy_loader.get_lazy_path"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.get_lazy_path", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.get_lazy_path"], ["", "def", "exists_lazy", "(", "path", ",", "data_type", "=", "'data'", ")", ":", "\n", "    ", "\"\"\"\n    Check if we've already made a lazy version of this file for the `data_type` field.\n    \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "get_lazy_path", "(", "path", ")", ")", ":", "\n", "        ", "return", "False", "\n", "", "contents", "=", "os", ".", "listdir", "(", "get_lazy_path", "(", "path", ")", ")", "\n", "if", "data_type", "not", "in", "contents", ":", "\n", "        ", "return", "False", "\n", "", "if", "data_type", "+", "'.len.pkl'", "not", "in", "contents", ":", "\n", "        ", "return", "False", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.make_lazy": [[44, 69], ["lazy_loader.get_lazy_path", "os.path.join", "os.path.join", "os.path.exists", "os.makedirs", "pickle.dump", "torch.distributed.is_initialized", "torch.distributed.get_rank", "open", "open", "os.path.exists", "time.sleep", "isinstance", "s.encode", "f.write", "len", "str_lens.append"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.get_lazy_path", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "make_lazy", "(", "path", ",", "strs", ",", "data_type", "=", "'data'", ")", ":", "\n", "    ", "\"\"\"\n    Make lazy version of `data_type` field of the file. Byte offsets\n    corresponding to data indices are stored in a `.len.pkl` data file.\n    \"\"\"", "\n", "lazypath", "=", "get_lazy_path", "(", "path", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "lazypath", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "lazypath", ")", "\n", "", "datapath", "=", "os", ".", "path", ".", "join", "(", "lazypath", ",", "data_type", ")", "\n", "lenpath", "=", "os", ".", "path", ".", "join", "(", "lazypath", ",", "data_type", "+", "'.len.pkl'", ")", "\n", "if", "not", "torch", ".", "distributed", ".", "is_initialized", "(", ")", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "with", "open", "(", "datapath", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "str_lens", "=", "[", "]", "\n", "str_cnt", "=", "0", "\n", "for", "s", "in", "strs", ":", "\n", "                ", "if", "isinstance", "(", "s", ",", "dict", ")", ":", "\n", "                    ", "s", "=", "s", "[", "'text'", "]", "\n", "", "encoded", "=", "s", ".", "encode", "(", "'utf-8'", ")", "\n", "f", ".", "write", "(", "encoded", ")", "\n", "str_cnt", "=", "len", "(", "encoded", ")", "\n", "str_lens", ".", "append", "(", "str_cnt", ")", "\n", "", "", "pkl", ".", "dump", "(", "str_lens", ",", "open", "(", "lenpath", ",", "'wb'", ")", ")", "\n", "", "else", ":", "\n", "        ", "while", "not", "os", ".", "path", ".", "exists", "(", "lenpath", ")", ":", "\n", "            ", "time", ".", "sleep", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.lazy_loader.split_strings": [[70, 75], ["zip"], "function", ["None"], ["", "", "", "def", "split_strings", "(", "strings", ",", "start", ",", "chr_lens", ")", ":", "\n", "    ", "\"\"\"\n    Split strings based on string lengths and given start.\n    \"\"\"", "\n", "return", "[", "strings", "[", "i", "-", "start", ":", "j", "-", "start", "]", "for", "i", ",", "j", "in", "zip", "(", "[", "start", "]", "+", "chr_lens", "[", ":", "-", "1", "]", ",", "chr_lens", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.__init__": [[40, 49], ["json.load", "sentencepiece.SentencePieceProcessor", "str.maketrans", "int", "io.open", "tokenization_gpt2.GPT2Tokenizer.encoder.items"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab_file", ",", "model_file", ",", "max_len", "=", "None", ")", ":", "\n", "        ", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "self", ".", "encoder", "=", "json", ".", "load", "(", "open", "(", "vocab_file", ")", ")", "\n", "self", ".", "decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "encoder", ".", "items", "(", ")", "}", "\n", "\n", "self", ".", "sp", "=", "spm", ".", "SentencePieceProcessor", "(", "model_file", "=", "model_file", ")", "\n", "self", ".", "translator", "=", "str", ".", "maketrans", "(", "\" \\n\"", ",", "\"\\u2582\\u2583\"", ")", "\n", "\n", "self", ".", "eod_id", "=", "self", ".", "encoder", "[", "'<eod>'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.vocab_size": [[50, 53], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "encoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.__len__": [[54, 56], ["len", "len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "encoder", ")", "+", "len", "(", "self", ".", "special_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.eod": [[57, 60], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "eod", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "eod_id", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.tokenize": [[61, 66], ["tokenization_gpt2.GPT2Tokenizer.sp.encode", "x.translate", "jieba.cut"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\" Tokenize a string. \"\"\"", "\n", "seg_list", "=", "[", "x", ".", "translate", "(", "self", ".", "translator", ")", "for", "x", "in", "jieba", ".", "cut", "(", "text", ",", "cut_all", "=", "False", ")", "]", "\n", "new_seg", "=", "\" \"", ".", "join", "(", "seg_list", ")", "\n", "return", "self", ".", "sp", ".", "encode", "(", "new_seg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode": [[67, 70], ["tokenization_gpt2.GPT2Tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.WordpieceTokenizer.tokenize"], ["", "def", "encode", "(", "self", ",", "text", ")", ":", "\n", "        ", "res", "=", "self", ".", "tokenize", "(", "text", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.decode": [[71, 75], ["tokenization_gpt2.GPT2Tokenizer.sp.decode", "text.replace().replace().replace.replace().replace().replace.replace().replace().replace", "text.replace().replace().replace.replace().replace().replace.replace().replace", "text.replace().replace().replace.replace().replace().replace.replace"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.decode"], ["", "def", "decode", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "text", "=", "self", ".", "sp", ".", "decode", "(", "tokens", ")", "\n", "text", "=", "text", ".", "replace", "(", "' '", ",", "''", ")", ".", "replace", "(", "'\\u2582'", ",", "' '", ")", ".", "replace", "(", "'\\u2583'", ",", "'\\n'", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.url_to_filename": [[43, 59], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode"], ["def", "url_to_filename", "(", "url", ",", "etag", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.filename_to_url": [[61, 85], ["os.path.join", "isinstance", "str", "os.path.exists", "EnvironmentError", "os.path.exists", "EnvironmentError", "io.open", "json.load"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "def", "filename_to_url", "(", "filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.cached_path": [[87, 115], ["urlparse", "isinstance", "str", "isinstance", "str", "file_utils.get_from_cache", "os.path.exists", "EnvironmentError", "ValueError"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.get_from_cache", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "def", "cached_path", "(", "url_or_filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.split_s3_path": [[117, 128], ["urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ")", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.s3_request": [[130, 147], ["functools.wraps", "func", "int", "EnvironmentError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.s3_etag": [[149, 156], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ")", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.s3_get": [[158, 164], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.http_get": [[166, 176], ["requests.get", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "tqdm.tqdm.close", "int", "tqdm.tqdm.update", "temp_file.write", "len"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write"], ["", "def", "http_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.get_from_cache": [[178, 236], ["url.startswith", "file_utils.url_to_filename", "os.path.join", "isinstance", "str", "os.path.exists", "os.makedirs", "file_utils.s3_etag", "requests.head", "requests.head.headers.get", "os.path.exists", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "file_utils.s3_get", "file_utils.http_get", "io.open", "shutil.copyfileobj", "io.open", "json.dump"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.url_to_filename", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.s3_etag", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.s3_get", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.http_get"], ["", "def", "get_from_cache", "(", "url", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "cache_dir", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.read_set_from_file": [[238, 248], ["set", "io.open", "set.add", "line.rstrip"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.add"], ["", "def", "read_set_from_file", "(", "filename", ")", ":", "\n", "    ", "'''\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    '''", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.get_file_extension": [[250, 254], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ",", "dot", "=", "True", ",", "lower", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.corpora.wikipedia.__init__": [[27, 35], ["os.path.exists", "datasets.json_dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "os", ".", "path", ".", "exists", "(", "wikipedia", ".", "PATH", ")", ",", "wikipedia", ".", "assert_str", "\n", "if", "not", "kwargs", ":", "\n", "            ", "kwargs", "=", "{", "}", "\n", "", "kwargs", "[", "'text_key'", "]", "=", "'text'", "\n", "kwargs", "[", "'loose_json'", "]", "=", "True", "\n", "super", "(", "wikipedia", ",", "self", ")", ".", "__init__", "(", "wikipedia", ".", "PATH", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.corpora.webtext.__init__": [[45, 53], ["os.path.exists", "datasets.json_dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "os", ".", "path", ".", "exists", "(", "webtext", ".", "PATH", ")", ",", "webtext", ".", "assert_str", "\n", "if", "not", "kwargs", ":", "\n", "            ", "kwargs", "=", "{", "}", "\n", "", "kwargs", "[", "'text_key'", "]", "=", "'text'", "\n", "kwargs", "[", "'loose_json'", "]", "=", "True", "\n", "super", "(", "webtext", ",", "self", ")", ".", "__init__", "(", "webtext", ".", "PATH", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.__init__": [[57, 68], ["tokenization.Tokenization.parse_command_tokens"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.parse_command_tokens"], ["def", "__init__", "(", "self", ",", "tokenization", ",", "text", "=", "None", ",", "original_text", "=", "None", ",", "command_tokens", "=", "None", ",", "asIds", "=", "True", ")", ":", "\n", "        ", "self", ".", "tokenization", "=", "tokenization", "\n", "self", ".", "text", "=", "text", "\n", "if", "self", ".", "text", "is", "None", ":", "\n", "            ", "self", ".", "text", "=", "self", ".", "tokenization", "\n", "", "self", ".", "original_text", "=", "original_text", "\n", "if", "self", ".", "original_text", "is", "None", ":", "\n", "            ", "self", ".", "original_text", "=", "self", ".", "text", "\n", "", "self", ".", "command_tokens", "=", "command_tokens", "\n", "self", ".", "asIds", "=", "asIds", "\n", "self", ".", "parse_command_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.set_command_tokens": [[69, 72], ["tokenization.Tokenization.parse_command_tokens"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.parse_command_tokens"], ["", "def", "set_command_tokens", "(", "self", ",", "command_tokens", ")", ":", "\n", "        ", "self", ".", "command_tokens", "=", "command_tokens", "\n", "return", "self", ".", "parse_command_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.parse_command_tokens": [[73, 81], ["setattr", "setattr"], "methods", ["None"], ["", "def", "parse_command_tokens", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "command_tokens", "is", "None", ":", "\n", "            ", "return", "\n", "", "for", "command_token", "in", "self", ".", "command_tokens", ":", "\n", "            ", "if", "self", ".", "asIds", ":", "\n", "                ", "setattr", "(", "self", ",", "command_token", ".", "name", ",", "command_token", ".", "Id", ")", "\n", "", "else", ":", "\n", "                ", "setattr", "(", "self", ",", "command_token", ".", "name", ",", "command_token", ".", "token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.__getitem__": [[82, 84], ["None"], "methods", ["None"], ["", "", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "tokenization", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.__len__": [[85, 87], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "tokenization", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.insert": [[88, 101], ["isinstance", "tokenization.Tokenization.tokenization.insert", "isinstance", "len"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.insert"], ["", "def", "insert", "(", "self", ",", "idx", ",", "other", ")", ":", "\n", "        ", "if", "isinstance", "(", "other", ",", "(", "CommandToken", ",", "TypeToken", ")", ")", ":", "\n", "            ", "self", ".", "tokenization", ".", "insert", "(", "idx", ",", "other", ".", "Id", ")", "\n", "if", "idx", "==", "0", ":", "\n", "                ", "self", ".", "text", "=", "other", ".", "token", "+", "self", ".", "text", "\n", "self", ".", "original_text", "=", "other", ".", "token", "+", "self", ".", "original_text", "\n", "", "elif", "idx", "==", "len", "(", "self", ".", "tokenization", ")", "-", "1", ":", "\n", "                ", "self", ".", "text", "+=", "other", ".", "token", "\n", "self", ".", "original_text", "+=", "other", ".", "token", "\n", "", "", "elif", "isinstance", "(", "other", ",", "Tokenization", ")", ":", "\n", "            ", "self", ".", "tokenization", "=", "self", ".", "tokenization", "[", ":", "idx", "]", "+", "other", ".", "tokenization", "+", "self", ".", "tokenization", "[", "idx", ":", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "tokenization", "=", "self", ".", "tokenization", "[", ":", "idx", "]", "+", "other", ".", "tokenization", "+", "self", ".", "tokenization", "[", "idx", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append": [[102, 114], ["isinstance", "tokenization.Tokenization.tokenization.append", "isinstance", "tokenization.Tokenization.tokenization.extend", "tokenization.Tokenization.tokenization.append"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "", "def", "append", "(", "self", ",", "other", ")", ":", "\n", "        ", "if", "isinstance", "(", "other", ",", "(", "CommandToken", ",", "TypeToken", ")", ")", ":", "\n", "            ", "self", ".", "tokenization", ".", "append", "(", "other", ".", "Id", ")", "\n", "self", ".", "text", "+=", "other", ".", "token", "\n", "self", ".", "original_text", "+=", "other", ".", "token", "\n", "", "elif", "isinstance", "(", "other", ",", "Tokenization", ")", ":", "\n", "            ", "self", ".", "tokenization", ".", "extend", "(", "other", ".", "tokenization", ")", "\n", "self", ".", "text", "+=", "other", ".", "text", "\n", "self", ".", "original_text", "+=", "other", ".", "original_text", "\n", "", "else", ":", "\n", "            ", "self", ".", "tokenization", ".", "append", "(", "other", ")", "\n", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend": [[115, 131], ["isinstance", "tokenization.Tokenization.tokenization.append", "isinstance", "isinstance", "tokenization.Tokenization.tokenization.extend", "isinstance", "tokenization.Tokenization.tokenization.extend", "tokenization.Tokenization.tokenization.extend"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend"], ["", "def", "extend", "(", "self", ",", "other", ")", ":", "\n", "        ", "if", "isinstance", "(", "other", ",", "(", "CommandToken", ",", "TypeToken", ")", ")", ":", "\n", "            ", "self", ".", "tokenization", ".", "append", "(", "other", ".", "Id", ")", "\n", "self", ".", "text", "+=", "other", ".", "token", "\n", "self", ".", "original_text", "+=", "other", ".", "token", "\n", "", "elif", "isinstance", "(", "other", ",", "list", ")", "and", "isinstance", "(", "other", "[", "0", "]", ",", "(", "CommandToken", ",", "TypeToken", ")", ")", ":", "\n", "            ", "self", ".", "tokenization", ".", "extend", "(", "[", "o", ".", "Id", "for", "o", "in", "other", "]", ")", "\n", "self", ".", "text", "+=", "[", "o", ".", "token", "for", "o", "in", "other", "]", "\n", "self", ".", "original_text", "+=", "[", "o", ".", "token", "for", "o", "in", "other", "]", "\n", "", "elif", "isinstance", "(", "other", ",", "Tokenization", ")", ":", "\n", "            ", "self", ".", "tokenization", ".", "extend", "(", "other", ".", "tokenization", ")", "\n", "self", ".", "text", "+=", "other", ".", "text", "\n", "self", ".", "original_text", "+=", "other", ".", "original_text", "\n", "", "else", ":", "\n", "            ", "self", ".", "tokenization", ".", "extend", "(", "other", ")", "\n", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CommandToken.__init__": [[141, 145], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "name", ",", "token", ",", "Id", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "token", "=", "token", "\n", "self", ".", "Id", "=", "Id", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CommandToken.__str__": [[146, 148], ["str", "COMMAND_TUPLE"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "COMMAND_TUPLE", "(", "self", ".", "name", ",", "self", ".", "token", ",", "self", ".", "Id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TypeToken.__init__": [[169, 173], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "name", ",", "token", ",", "Id", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "token", "=", "token", "\n", "self", ".", "Id", "=", "Id", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TypeToken.__str__": [[174, 176], ["str", "TYPE_TUPLE"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "TYPE_TUPLE", "(", "self", ".", "name", ",", "self", ".", "token", ",", "self", ".", "Id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.__init__": [[202, 243], ["tokenization.Tokenizer._vocab.update", "list", "list", "list", "hasattr", "len", "hasattr", "len", "hasattr", "hasattr", "len", "list", "list", "tokenization.Tokenizer.command_token_map.keys", "tokenization.Tokenizer.type_token_map.keys", "tokenization.Tokenizer.command_token_map.keys", "tokenization.Tokenizer.command_id_map.items", "tokenization.Tokenizer.text_tokenizer.vocab.items", "tokenization.Tokenizer.command_id_map.items", "tokenization.Tokenizer.type_id_map.items", "tokenization.Tokenizer.text_tokenizer.vocab.items"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "text_tokenizer", ",", "command_tokens", "=", "None", ",", "type_tokens", "=", "None", ")", ":", "\n", "# set text tokenizer", "\n", "        ", "self", ".", "text_tokenizer", "=", "text_tokenizer", "\n", "if", "not", "hasattr", "(", "self", ",", "'num_text_tokens'", ")", ":", "\n", "            ", "self", ".", "num_text_tokens", "=", "len", "(", "self", ".", "text_tokenizer", ")", "\n", "\n", "# set command tokens", "\n", "", "if", "command_tokens", "is", "None", ":", "\n", "            ", "command_tokens", "=", "DEFAULT_COMMAND_TOKENS", "\n", "", "self", ".", "_command_tokens", "=", "command_tokens", "\n", "self", ".", "command_name_map", "=", "{", "tok", ".", "name", ":", "tok", "for", "tok", "in", "self", ".", "_command_tokens", "}", "\n", "self", ".", "command_token_map", "=", "{", "tok", ".", "token", ":", "tok", "for", "tok", "in", "self", ".", "_command_tokens", "}", "\n", "self", ".", "command_id_map", "=", "{", "tok", ".", "Id", ":", "tok", "for", "tok", "in", "self", ".", "_command_tokens", "}", "\n", "if", "not", "hasattr", "(", "self", ",", "'num_command_tokens'", ")", ":", "\n", "            ", "self", ".", "num_command_tokens", "=", "len", "(", "self", ".", "_command_tokens", ")", "\n", "", "if", "not", "hasattr", "(", "self", ",", "'num_tokens'", ")", ":", "\n", "            ", "self", ".", "num_tokens", "=", "self", ".", "num_command_tokens", "+", "self", ".", "num_text_tokens", "\n", "\n", "# set type tokens", "\n", "", "if", "type_tokens", "is", "None", ":", "\n", "            ", "type_tokens", "=", "DEFAULT_TYPE_TOKENS", "\n", "", "self", ".", "type_tokens", "=", "type_tokens", "\n", "self", ".", "type_name_map", "=", "{", "tok", ".", "name", ":", "tok", "for", "tok", "in", "self", ".", "type_tokens", "}", "\n", "self", ".", "type_token_map", "=", "{", "tok", ".", "token", ":", "tok", "for", "tok", "in", "self", ".", "type_tokens", "}", "\n", "self", ".", "type_id_map", "=", "{", "tok", ".", "Id", ":", "tok", "for", "tok", "in", "self", ".", "type_tokens", "}", "\n", "if", "not", "hasattr", "(", "self", ",", "'num_type_tokens'", ")", ":", "\n", "            ", "self", ".", "num_type_tokens", "=", "len", "(", "self", ".", "type_tokens", ")", "\n", "\n", "# parse tokens and vocabs from tokenizer", "\n", "", "self", ".", "_tokens", "=", "list", "(", "self", ".", "command_token_map", ".", "keys", "(", ")", ")", "+", "list", "(", "self", ".", "text_tokenizer", ".", "tokens", ")", "\n", "self", ".", "_vocab", "=", "{", "t", ":", "Id", "for", "Id", ",", "t", "in", "self", ".", "command_id_map", ".", "items", "(", ")", "}", "\n", "self", ".", "_vocab", ".", "update", "(", "{", "t", ":", "Id", "+", "self", ".", "num_command_tokens", "for", "t", ",", "Id", "in", "self", ".", "text_tokenizer", ".", "vocab", ".", "items", "(", ")", "}", ")", "\n", "\n", "self", ".", "_text_tokens", "=", "list", "(", "self", ".", "text_tokenizer", ".", "tokens", ")", "\n", "self", ".", "_text_token_vocab", "=", "{", "t", ":", "Id", "+", "self", ".", "num_command_tokens", "for", "t", ",", "Id", "in", "self", ".", "text_tokenizer", ".", "vocab", ".", "items", "(", ")", "}", "\n", "\n", "self", ".", "_command_token_tokens", "=", "list", "(", "self", ".", "command_token_map", ".", "keys", "(", ")", ")", "\n", "self", ".", "_command_token_vocab", "=", "{", "t", ":", "Id", "for", "Id", ",", "t", "in", "self", ".", "command_id_map", ".", "items", "(", ")", "}", "\n", "\n", "self", ".", "_token_types", "=", "list", "(", "self", ".", "type_token_map", ".", "keys", "(", ")", ")", "\n", "self", ".", "_token_type_vocab", "=", "{", "t", ":", "Id", "for", "Id", ",", "t", "in", "self", ".", "type_id_map", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.__call__": [[245, 248], ["tokenization.Tokenizer.EncodeAsIds"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsIds"], ["", "def", "__call__", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "\"\"\"run preprocessing and encode text as Ids\"\"\"", "\n", "return", "self", ".", "EncodeAsIds", "(", "text", ",", "process_fn", "=", "process_fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.__len__": [[249, 252], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"total number of tokens\"\"\"", "\n", "return", "self", ".", "num_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.get_command": [[253, 256], ["None"], "methods", ["None"], ["", "def", "get_command", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"get command token corresponding to `name`\"\"\"", "\n", "return", "self", ".", "command_name_map", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.get_type": [[257, 260], ["None"], "methods", ["None"], ["", "def", "get_type", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"get type token corresponding to `name`\"\"\"", "\n", "return", "self", ".", "type_name_map", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.tokens": [[261, 265], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "tokens", "(", "self", ")", ":", "\n", "        ", "\"\"\"list (or iterable) of all tokens for tokenizer\"\"\"", "\n", "return", "self", ".", "_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.vocab": [[266, 270], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab", "(", "self", ")", ":", "\n", "        ", "\"\"\"dictionary mapping tokens to ids for tokenizer\"\"\"", "\n", "return", "self", ".", "_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.token_types": [[271, 275], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "token_types", "(", "self", ")", ":", "\n", "        ", "\"\"\"list (or iterable) of all token types for tokenizer\"\"\"", "\n", "return", "self", ".", "_token_types", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.token_type_vocab": [[276, 280], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "token_type_vocab", "(", "self", ")", ":", "\n", "        ", "\"\"\"dictionary mapping token types to ids for tokenizer\"\"\"", "\n", "return", "self", ".", "_token_type_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.command_tokens": [[281, 285], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "command_tokens", "(", "self", ")", ":", "\n", "        ", "\"\"\"list (or iterable) of all command tokens for tokenizer\"\"\"", "\n", "return", "self", ".", "_command_token_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.command_token_vocab": [[286, 290], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "command_token_vocab", "(", "self", ")", ":", "\n", "        ", "\"\"\"dictionary mapping command tokens to ids for tokenizer\"\"\"", "\n", "return", "self", ".", "_command_token_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.text_tokens": [[291, 295], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "text_tokens", "(", "self", ")", ":", "\n", "        ", "\"\"\"list (or iterable) of text tokens for text tokenizer\"\"\"", "\n", "return", "self", ".", "_text_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.text_token_vocab": [[296, 300], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "text_token_vocab", "(", "self", ")", ":", "\n", "        ", "\"\"\"dictionary mapping text tokens to ids for text tokenizer\"\"\"", "\n", "return", "self", ".", "_text_token_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.EncodeAsIds": [[301, 309], ["tokenization.Tokenizer.Tokenizer.text_tokenizer.EncodeAsIds", "tokenization.Tokenizer.Tokenizer.set_command_tokens"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsIds", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.set_command_tokens"], ["", "def", "EncodeAsIds", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        encode text using text tokenizer and shift Id values for command tokens\n        \"\"\"", "\n", "tokenization", "=", "self", ".", "text_tokenizer", ".", "EncodeAsIds", "(", "text", ",", "process_fn", "=", "process_fn", ")", "\n", "tokenization", ".", "tokenization", "=", "[", "t", "+", "self", ".", "num_command_tokens", "for", "t", "in", "tokenization", ".", "tokenization", "]", "\n", "tokenization", ".", "set_command_tokens", "(", "self", ".", "_command_tokens", ")", "\n", "return", "tokenization", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.EncodeAsTokens": [[310, 317], ["tokenization.Tokenizer.Tokenizer.text_tokenizer.EncodeAsTokens", "tokenization.Tokenizer.Tokenizer.set_command_tokens"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsTokens", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.set_command_tokens"], ["", "def", "EncodeAsTokens", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        encode text as tokens using text tokenizer\n        \"\"\"", "\n", "tokenization", "=", "self", ".", "text_tokenizer", ".", "EncodeAsTokens", "(", "text", ",", "process_fn", "=", "process_fn", ")", "\n", "tokenization", ".", "set_command_tokens", "(", "self", ".", "_command_tokens", ")", "\n", "return", "tokenization", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.IdToToken": [[318, 327], ["isinstance", "tokenization.Tokenizer.text_tokenizer.IdToToken"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.IdToToken"], ["", "def", "IdToToken", "(", "self", ",", "Id", ",", "type_token", "=", "False", ")", ":", "\n", "        ", "\"\"\"convert Id to token accounting for command and type tokens\"\"\"", "\n", "if", "isinstance", "(", "Id", ",", "(", "TypeToken", ",", "CommandToken", ")", ")", ":", "\n", "            ", "return", "Id", ".", "token", "\n", "", "if", "type_token", ":", "\n", "            ", "return", "self", ".", "type_id_map", "[", "Id", "]", ".", "token", "\n", "", "if", "Id", "<", "self", ".", "num_command_tokens", ":", "\n", "            ", "return", "self", ".", "command_id_map", "[", "Id", "]", ".", "token", "\n", "", "return", "self", ".", "text_tokenizer", ".", "IdToToken", "(", "Id", "-", "self", ".", "num_command_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.TokenToId": [[328, 337], ["isinstance", "tokenization.Tokenizer.text_tokenizer.TokenToId"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.TokenToId"], ["", "def", "TokenToId", "(", "self", ",", "token", ",", "type_token", "=", "False", ")", ":", "\n", "        ", "\"\"\"convert token to Id accounting for command and type tokens\"\"\"", "\n", "if", "isinstance", "(", "token", ",", "(", "TypeToken", ",", "CommandToken", ")", ")", ":", "\n", "            ", "return", "token", ".", "Id", "\n", "", "if", "type_token", ":", "\n", "            ", "return", "self", ".", "type_token_map", "[", "token", "]", ".", "Id", "\n", "", "if", "token", "in", "self", ".", "command_token_map", ":", "\n", "            ", "return", "self", ".", "command_token_map", "[", "token", "]", ".", "Id", "\n", "", "return", "self", ".", "text_tokenizer", ".", "TokenToId", "(", "token", ")", "+", "self", ".", "num_command_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.DecodeIds": [[338, 363], ["isinstance", "isinstance", "rtn_strs.append", "rtn_strs.append", "rtn_strs.append", "tokenization.Tokenizer.text_tokenizer.DecodeIds", "tokenization.Tokenizer.text_tokenizer.DecodeIds", "rtn_strs.append", "rtn_strs.append", "current_str.append", "isinstance", "tokenization.Tokenizer.text_tokenizer.DecodeIds"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.DecodeIds", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.DecodeIds", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.DecodeIds"], ["", "def", "DecodeIds", "(", "self", ",", "Ids", ",", "type_token", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        convert Ids to tokens accounting for command and type tokens, tokens\n        are joined and returned as a string.\n        \"\"\"", "\n", "if", "type_token", ":", "\n", "            ", "return", "' '", ".", "join", "(", "Id", ".", "token", "if", "isinstance", "(", "Id", ",", "TypeToken", ")", "else", "self", ".", "type_id_map", "[", "Id", "]", ".", "token", "for", "Id", "in", "Ids", ")", "\n", "", "rtn_strs", "=", "[", "]", "\n", "current_str", "=", "[", "]", "\n", "if", "isinstance", "(", "Ids", ",", "Tokenization", ")", ":", "\n", "            ", "Ids", "=", "Ids", ".", "tokenization", "\n", "", "for", "Id", "in", "Ids", ":", "\n", "            ", "if", "isinstance", "(", "Id", ",", "CommandToken", ")", ":", "\n", "                ", "rtn_strs", ".", "append", "(", "self", ".", "text_tokenizer", ".", "DecodeIds", "(", "current_str", ")", ")", "\n", "current_str", "=", "[", "]", "\n", "rtn_strs", ".", "append", "(", "t", ".", "token", ")", "\n", "", "elif", "Id", "<", "self", ".", "num_command_tokens", ":", "\n", "                ", "rtn_strs", ".", "append", "(", "self", ".", "text_tokenizer", ".", "DecodeIds", "(", "current_str", ")", ")", "\n", "current_str", "=", "[", "]", "\n", "rtn_strs", ".", "append", "(", "self", ".", "command_id_map", "[", "Id", "]", ".", "token", ")", "\n", "", "else", ":", "\n", "                ", "current_str", ".", "append", "(", "Id", "-", "self", ".", "num_command_tokens", ")", "\n", "", "", "if", "current_str", "!=", "[", "]", ":", "\n", "            ", "rtn_strs", ".", "append", "(", "self", ".", "text_tokenizer", ".", "DecodeIds", "(", "current_str", ")", ")", "\n", "", "return", "' '", ".", "join", "(", "rtn_strs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.DecodeTokens": [[364, 388], ["isinstance", "isinstance", "rtn_strs.append", "rtn_strs.append", "rtn_strs.append", "tokenization.Tokenizer.text_tokenizer.DecodeTokens", "tokenization.Tokenizer.text_tokenizer.DecodeTokens", "rtn_strs.append", "rtn_strs.append", "current_str.append", "isinstance", "tokenization.Tokenizer.text_tokenizer.DecodeTokens"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.DecodeTokens", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.DecodeTokens", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.DecodeTokens"], ["", "def", "DecodeTokens", "(", "self", ",", "Tokens", ",", "type_token", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        convert tokens to a string accounting for command and type tokens.\n        \"\"\"", "\n", "if", "type_token", ":", "\n", "            ", "return", "' '", ".", "join", "(", "t", ".", "token", "if", "isinstance", "(", "t", ",", "TypeToken", ")", "else", "t", "for", "t", "in", "Tokens", ")", "\n", "", "rtn_strs", "=", "[", "]", "\n", "current_str", "=", "[", "]", "\n", "if", "isinstance", "(", "Tokens", ",", "Tokenization", ")", ":", "\n", "            ", "Tokens", "=", "Tokens", ".", "tokenization", "\n", "", "for", "t", "in", "Tokens", ":", "\n", "            ", "if", "isinstance", "(", "t", ",", "CommandToken", ")", ":", "\n", "                ", "rtn_strs", ".", "append", "(", "self", ".", "text_tokenizer", ".", "DecodeTokens", "(", "current_str", ")", ")", "\n", "current_str", "=", "[", "]", "\n", "rtn_strs", ".", "append", "(", "t", ".", "token", ")", "\n", "", "elif", "t", "in", "self", ".", "command_token_map", ":", "\n", "                ", "rtn_strs", ".", "append", "(", "self", ".", "text_tokenizer", ".", "DecodeTokens", "(", "current_str", ")", ")", "\n", "current_str", "=", "[", "]", "\n", "rtn_strs", ".", "append", "(", "t", ")", "\n", "", "else", ":", "\n", "                ", "current_str", ".", "append", "(", "t", ")", "\n", "", "", "if", "current_str", "!=", "[", "]", ":", "\n", "            ", "rtn_strs", ".", "append", "(", "self", ".", "text_tokenizer", ".", "DecodeTokens", "(", "current_str", ")", ")", "\n", "", "return", "' '", ".", "join", "(", "rtn_strs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.__init__": [[393, 398], ["hasattr", "hasattr"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "if", "not", "hasattr", "(", "self", ",", "'num_text_tokens'", ")", ":", "\n", "            ", "self", ".", "num_text_tokens", "=", "0", "\n", "", "if", "not", "hasattr", "(", "self", ",", "'num_tokens'", ")", ":", "\n", "            ", "self", ".", "num_tokens", "=", "self", ".", "num_text_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.__call__": [[399, 401], ["tokenization.TextTokenizer.EncodeAsIds"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsIds"], ["", "", "def", "__call__", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "EncodeAsIds", "(", "text", ",", "process_fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.__len__": [[402, 404], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_text_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.tokens": [[405, 409], ["NotImplementedError"], "methods", ["None"], ["", "@", "property", "\n", "def", "tokens", "(", "self", ")", ":", "\n", "        ", "\"\"\"list (or iterable) of text tokens for text tokenizer\"\"\"", "\n", "raise", "NotImplementedError", "(", "'TextTokenizer tokens property not implemented'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.vocab": [[410, 414], ["NotImplementedError"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab", "(", "self", ")", ":", "\n", "        ", "\"\"\"dictionary mapping tokens to ids\"\"\"", "\n", "raise", "NotImplementedError", "(", "'TextTokenizer vocab property not implemented'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.exists": [[415, 419], ["NotImplementedError"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "exists", "(", "model_path", ")", ":", "\n", "        ", "\"\"\"check if the filepath for a text tokenizer exists\"\"\"", "\n", "raise", "NotImplementedError", "(", "'TextTokenizer exists method not implemented'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.Train": [[420, 423], ["NotImplementedError"], "methods", ["None"], ["", "def", "Train", "(", "self", ",", "corpus", ")", ":", "\n", "        ", "\"\"\"train a tokenizer on a data corpus and save model for future use\"\"\"", "\n", "raise", "NotImplementedError", "(", "'TextTokenizer Train not implemented'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.EncodeAsIds": [[424, 430], ["NotImplementedError"], "methods", ["None"], ["", "def", "EncodeAsIds", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Preprocess text and encode as ids. Return a tokenization object with\n        original text, processed text, and id tokenization.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "'TextTokenizer EncodeAsIds not implemented'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.EncodeAsTokens": [[431, 437], ["NotImplementedError"], "methods", ["None"], ["", "def", "EncodeAsTokens", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Preprocess text and encode as tokens. Return a tokenization object with\n        original text, processed text, and token tokenization.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "'TextTokenizer EncodeAsTokens not implemented'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.IdToToken": [[438, 441], ["NotImplementedError"], "methods", ["None"], ["", "def", "IdToToken", "(", "self", ",", "Id", ")", ":", "\n", "        ", "\"\"\"Convert an Id to Token. Reverse lookup of self.vocab\"\"\"", "\n", "raise", "NotImplementedError", "(", "'TextTokenizer IdToToken not implemented'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.TokenToId": [[442, 445], ["NotImplementedError"], "methods", ["None"], ["", "def", "TokenToId", "(", "self", ",", "token", ")", ":", "\n", "        ", "\"\"\"Convert a Token to Id. Lookup of self.vocab\"\"\"", "\n", "raise", "NotImplementedError", "(", "'TextTokenizer TokenToId not implemented'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.DecodeIds": [[446, 449], ["NotImplementedError"], "methods", ["None"], ["", "def", "DecodeIds", "(", "self", ",", "Ids", ")", ":", "\n", "        ", "\"\"\"Convert a list or tokenization object of Ids to a text string\"\"\"", "\n", "raise", "NotImplementedError", "(", "'TextTokenizer DecodeIds not implemented'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.TextTokenizer.DecodeTokens": [[450, 453], ["NotImplementedError"], "methods", ["None"], ["", "def", "DecodeTokens", "(", "self", ",", "Tokens", ")", ":", "\n", "        ", "\"\"\"Convert a list or tokenization object of tokens to a text string\"\"\"", "\n", "raise", "NotImplementedError", "(", "'TextTokenizer DecodeTokens not implemented'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CharacterLevelTokenizer.__init__": [[459, 464], ["tokenization.TextTokenizer.__init__", "tokenization.CharacterLevelTokenizer.IdToToken", "range", "enumerate"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.IdToToken"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "num_text_tokens", "=", "256", "\n", "super", "(", "CharacterLevelTokenizer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_tokens", "=", "[", "self", ".", "IdToToken", "(", "Id", ")", "for", "Id", "in", "range", "(", "self", ".", "num_text_tokens", ")", "]", "\n", "self", ".", "_vocab", "=", "{", "t", ":", "i", "for", "i", ",", "t", "in", "enumerate", "(", "self", ".", "_tokens", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CharacterLevelTokenizer.__len__": [[465, 467], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "256", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CharacterLevelTokenizer.exists": [[468, 471], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "exists", "(", "model_path", ")", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CharacterLevelTokenizer.Train": [[472, 474], ["None"], "methods", ["None"], ["", "def", "Train", "(", "self", ",", "corpus", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CharacterLevelTokenizer.tokens": [[475, 478], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "tokens", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CharacterLevelTokenizer.vocab": [[479, 482], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CharacterLevelTokenizer.EncodeAsIds": [[483, 491], ["tokenization.Tokenization", "process_fn", "str", "tokenization.CharacterLevelTokenizer.TokenToId"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.TokenToId"], ["", "def", "EncodeAsIds", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "\"\"\"convert text to ascii 256 Ids\"\"\"", "\n", "processed_text", "=", "text", "\n", "if", "process_fn", "is", "not", "None", ":", "\n", "            ", "processed_text", "=", "process_fn", "(", "processed_text", ")", "\n", "processed_text", "=", "str", "(", "processed_text", ")", "\n", "", "tokens", "=", "[", "self", ".", "TokenToId", "(", "c", ")", "for", "c", "in", "processed_text", "]", "\n", "return", "Tokenization", "(", "tokens", ",", "processed_text", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CharacterLevelTokenizer.EncodeAsTokens": [[492, 500], ["str", "tokenization.Tokenization", "process_fn"], "methods", ["None"], ["", "def", "EncodeAsTokens", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "\"\"\"convert text to ascii 256 characters\"\"\"", "\n", "processed_text", "=", "text", "\n", "if", "process_fn", "is", "not", "None", ":", "\n", "            ", "processed_text", "=", "process_fn", "(", "processed_text", ")", "\n", "", "processed_text", "=", "str", "(", "processed_text", ")", "\n", "tokens", "=", "[", "c", "for", "c", "in", "processed_text", "]", "\n", "return", "Tokenization", "(", "tokens", ",", "processed_text", ",", "text", ",", "asIds", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CharacterLevelTokenizer.IdToToken": [[501, 504], ["chr"], "methods", ["None"], ["", "def", "IdToToken", "(", "self", ",", "Id", ")", ":", "\n", "        ", "\"\"\"ascii index to character\"\"\"", "\n", "return", "chr", "(", "Id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CharacterLevelTokenizer.TokenToId": [[505, 508], ["ord"], "methods", ["None"], ["", "def", "TokenToId", "(", "self", ",", "token", ")", ":", "\n", "        ", "\"\"\"ascii character to index\"\"\"", "\n", "return", "ord", "(", "token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CharacterLevelTokenizer.DecodeIds": [[509, 514], ["isinstance", "tokenization.CharacterLevelTokenizer.IdToToken"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.IdToToken"], ["", "def", "DecodeIds", "(", "self", ",", "Ids", ")", ":", "\n", "        ", "\"\"\"converts ascii ids to tokens before joining them into text\"\"\"", "\n", "if", "isinstance", "(", "Ids", ",", "Tokenization", ")", ":", "\n", "            ", "Ids", "=", "Ids", ".", "tokenization", "\n", "", "return", "''", ".", "join", "(", "[", "self", ".", "IdToToken", "(", "tok", ")", "for", "tok", "in", "Ids", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.CharacterLevelTokenizer.DecodeTokens": [[515, 520], ["isinstance"], "methods", ["None"], ["", "def", "DecodeTokens", "(", "self", ",", "Tokens", ")", ":", "\n", "        ", "\"\"\"just concatenates ascii tokens into text\"\"\"", "\n", "if", "isinstance", "(", "Tokens", ",", "Tokenization", ")", ":", "\n", "            ", "Tokens", "=", "Tokens", ".", "tokenization", "\n", "", "return", "''", ".", "join", "(", "Tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.__init__": [[578, 591], ["model_type.lower", "tokenization.SentencePieceTokenizer.load_spm_model", "tokenization.TextTokenizer.__init__", "tokenization.SentencePieceTokenizer.exists", "tokenization.SentencePieceTokenizer.Train"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.load_spm_model", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.Train"], ["def", "__init__", "(", "self", ",", "model_type", "=", "'bpe'", ",", "vocab_size", "=", "None", ",", "corpus", "=", "None", ",", "model_path", "=", "None", ",", "character_coverage", "=", "1.0", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "character_coverage", "=", "character_coverage", "\n", "self", ".", "model_type", "=", "model_type", ".", "lower", "(", ")", "\n", "self", ".", "spm_model", "=", "model_path", "\n", "self", ".", "num_text_tokens", "=", "vocab_size", "\n", "make_train", "=", "not", "SentencePieceTokenizer", ".", "exists", "(", "self", ".", "spm_model", ")", "\n", "if", "make_train", ":", "\n", "            ", "assert", "corpus", "is", "not", "None", "and", "self", ".", "num_text_tokens", "is", "not", "None", "\n", "self", ".", "Train", "(", "corpus", ",", "self", ".", "num_text_tokens", ")", "\n", "", "self", ".", "_tokens", "=", "[", "]", "\n", "self", ".", "_vocab", "=", "{", "}", "\n", "self", ".", "load_spm_model", "(", ")", "\n", "super", "(", "SentencePieceTokenizer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.__len__": [[592, 594], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_text_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.tokens": [[595, 598], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "tokens", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.vocab": [[599, 602], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.exists": [[603, 613], ["os.path.exists", "model_path.endswith", "os.path.exists"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "@", "staticmethod", "\n", "def", "exists", "(", "model_path", ")", ":", "\n", "        ", "if", "model_path", "is", "None", ":", "\n", "            ", "return", "False", "\n", "# check if path exists", "\n", "", "dne", "=", "not", "os", ".", "path", ".", "exists", "(", "model_path", ")", "\n", "# check if path.model exists", "\n", "if", "dne", "and", "not", "model_path", ".", "endswith", "(", "'.model'", ")", ":", "\n", "            ", "dne", "=", "not", "os", ".", "path", ".", "exists", "(", "model_path", "+", "'.model'", ")", "\n", "", "return", "not", "dne", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.load_spm_model": [[614, 623], ["sentencepiece.SentencePieceProcessor", "tokenization.SentencePieceTokenizer.sp.Load", "len", "tokenization.SentencePieceTokenizer.IdToToken", "os.path.exists", "tokenization.SentencePieceTokenizer.spm_model.endswith", "range", "enumerate"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.IdToToken", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "def", "load_spm_model", "(", "self", ")", ":", "\n", "        ", "\"\"\"load sentencepiece model and parse vocab\"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "spm_model", ")", "and", "not", "self", ".", "spm_model", ".", "endswith", "(", "'.model'", ")", ":", "\n", "            ", "self", ".", "spm_model", "=", "self", ".", "spm_model", "+", "'.model'", "\n", "", "self", ".", "sp", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "self", ".", "sp", ".", "Load", "(", "self", ".", "spm_model", ")", "\n", "self", ".", "vocab_size", "=", "self", ".", "num_text_tokens", "=", "len", "(", "self", ".", "sp", ")", "\n", "self", ".", "_tokens", "=", "[", "self", ".", "IdToToken", "(", "t", ")", "for", "t", "in", "range", "(", "self", ".", "vocab_size", ")", "]", "\n", "self", ".", "_vocab", "=", "{", "t", ":", "i", "for", "i", ",", "t", "in", "enumerate", "(", "self", ".", "_tokens", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.Train": [[624, 650], ["str", "use_model_path.endswith", "tokenization.get_corpus_freq", "min", "print", "print", "train_string.format.format.format", "print", "sentencepiece.SentencePieceTrainer.Train", "os.remove", "print", "random.randint", "int", "use_model_path.rfind"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.get_corpus_freq", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.Train"], ["", "def", "Train", "(", "self", ",", "corpus", ",", "num_text_tokens", ")", ":", "\n", "        ", "\"\"\"train sentencepiece model on corpus using word frequencies\"\"\"", "\n", "self", ".", "num_text_tokens", "=", "num_text_tokens", "\n", "use_model_path", "=", "self", ".", "spm_model", "\n", "random_hash", "=", "str", "(", "random", ".", "randint", "(", "0", ",", "2147483647", ")", ")", "\n", "if", "use_model_path", "is", "None", ":", "\n", "            ", "use_model_path", "=", "random_hash", "\n", "", "if", "use_model_path", ".", "endswith", "(", "'.model'", ")", ":", "\n", "            ", "use_model_path", "=", "use_model_path", "[", ":", "use_model_path", ".", "rfind", "(", "'.model'", ")", "]", "\n", "", "input_path", "=", "use_model_path", "+", "'.tsv.'", "+", "random_hash", "\n", "line_count", ",", "maxlenline", "=", "get_corpus_freq", "(", "corpus", ",", "input_path", ")", "\n", "line_count", "=", "min", "(", "line_count", ",", "MAX_SENTENCEPIECE_SENTENCES", ")", "\n", "print", "(", "'line count used as input_sentence_size '", ",", "line_count", ",", "flush", "=", "True", ")", "\n", "print", "(", "'training sentencepiece model'", ",", "flush", "=", "True", ")", "\n", "train_string", "=", "'--input={file_path} --model_prefix={model_prefix} --vocab_size={vocab_size}'", "+", "' --model_type={model_type} --character_coverage={character_coverage} '", "+", "'--input_sentence_size={input_sentence_size} '", "+", "'--input_format=tsv'", "\n", "train_string", "=", "train_string", ".", "format", "(", "file_path", "=", "input_path", ",", "model_prefix", "=", "use_model_path", ",", "vocab_size", "=", "num_text_tokens", ",", "\n", "model_type", "=", "self", ".", "model_type", ",", "character_coverage", "=", "self", ".", "character_coverage", ",", "\n", "input_sentence_size", "=", "int", "(", "line_count", ")", ")", "#, #)#,", "\n", "print", "(", "\"calling spm.SentencePieceTrainer.Train(%s)\"", "%", "(", "train_string", ")", ",", "flush", "=", "True", ")", "\n", "spm", ".", "SentencePieceTrainer", ".", "Train", "(", "train_string", ")", "\n", "os", ".", "remove", "(", "input_path", ")", "\n", "self", ".", "spm_model", "=", "use_model_path", "+", "'.model'", "\n", "print", "(", "'sentencepiece model written to '", "+", "self", ".", "spm_model", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.EncodeAsIds": [[651, 658], ["tokenization.SentencePieceTokenizer.sp.EncodeAsIds", "tokenization.Tokenization", "process_fn"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsIds"], ["", "def", "EncodeAsIds", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "\"\"\"convert text to sentencepiece Ids\"\"\"", "\n", "processed_text", "=", "text", "\n", "if", "process_fn", "is", "not", "None", ":", "\n", "            ", "processed_text", "=", "process_fn", "(", "processed_text", ")", "\n", "", "tokens", "=", "self", ".", "sp", ".", "EncodeAsIds", "(", "processed_text", ")", "\n", "return", "Tokenization", "(", "tokens", ",", "processed_text", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.EncodeAsTokens": [[659, 666], ["tokenization.SentencePieceTokenizer.sp.EncodeAsTokens", "tokenization.Tokenization", "process_fn"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsTokens"], ["", "def", "EncodeAsTokens", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "\"\"\"convert text to sentencepiece tokens\"\"\"", "\n", "processed_text", "=", "text", "\n", "if", "process_fn", "is", "not", "None", ":", "\n", "            ", "processed_text", "=", "process_fn", "(", "processed_text", ")", "\n", "", "tokens", "=", "self", ".", "sp", ".", "EncodeAsTokens", "(", "processed_text", ")", "\n", "return", "Tokenization", "(", "tokens", ",", "processed_text", ",", "text", ",", "asIds", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.IdToToken": [[667, 670], ["tokenization.SentencePieceTokenizer.sp.IdToPiece"], "methods", ["None"], ["", "def", "IdToToken", "(", "self", ",", "Id", ")", ":", "\n", "        ", "\"\"\"convert Id to sentencpiece token\"\"\"", "\n", "return", "self", ".", "sp", ".", "IdToPiece", "(", "Id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.TokenToId": [[671, 674], ["tokenization.SentencePieceTokenizer.sp.PieceToId"], "methods", ["None"], ["", "def", "TokenToId", "(", "self", ",", "token", ")", ":", "\n", "        ", "\"\"\"convert sentencpiece token to Id\"\"\"", "\n", "return", "self", ".", "sp", ".", "PieceToId", "(", "token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.DecodeIds": [[675, 680], ["isinstance", "tokenization.SentencePieceTokenizer.sp.DecodeIds"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.DecodeIds"], ["", "def", "DecodeIds", "(", "self", ",", "Ids", ")", ":", "\n", "        ", "\"\"\"converts ids to a text string\"\"\"", "\n", "if", "isinstance", "(", "Ids", ",", "Tokenization", ")", ":", "\n", "            ", "Ids", "=", "Ids", ".", "tokenization", "\n", "", "return", "self", ".", "sp", ".", "DecodeIds", "(", "Ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.SentencePieceTokenizer.DecodeTokens": [[681, 686], ["isinstance", "tokenization.SentencePieceTokenizer.sp.DecodeTokens"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.DecodeTokens"], ["", "def", "DecodeTokens", "(", "self", ",", "Tokens", ")", ":", "\n", "        ", "\"\"\"converts sentencepiece tokens to a text string\"\"\"", "\n", "if", "isinstance", "(", "Tokens", ",", "Tokenization", ")", ":", "\n", "            ", "Tokens", "=", "Tokens", ".", "tokenization", "\n", "", "return", "self", ".", "sp", ".", "DecodeTokens", "(", "Tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.BertWordPieceTokenizer.__init__": [[692, 744], ["wordpiece.BertTokenizer.from_pretrained", "int", "len", "list", "list", "list", "list", "torch.distributed.get_rank", "print", "torch.distributed.get_rank", "print", "tokenization.CommandToken", "tokenization.CommandToken", "tokenization.CommandToken", "tokenization.CommandToken", "tokenization.CommandToken", "tokenization.TypeToken", "tokenization.TypeToken", "tokenization.BertWordPieceTokenizer.text_tokenizer.vocab.keys", "tokenization.BertWordPieceTokenizer.command_token_map.keys", "tokenization.BertWordPieceTokenizer.type_token_map.keys", "tokenization.BertWordPieceTokenizer.text_tokenizer.vocab.items", "tokenization.BertWordPieceTokenizer.text_tokenizer.vocab.items", "tokenization.BertWordPieceTokenizer.command_id_map.items", "tokenization.BertWordPieceTokenizer.type_id_map.items"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.PreTrainedBertModel.from_pretrained"], ["def", "__init__", "(", "self", ",", "tokenizer_model_type", "=", "None", ",", "cache_dir", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "# default to bert-large-uncased tokenizer", "\n", "        ", "if", "tokenizer_model_type", "not", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "tokenizer_model_type", "=", "'bert-large-uncased'", "\n", "", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "            ", "print", "(", "'loading BertWordPieceTokenizer ('", ",", "tokenizer_model_type", ",", "') from cache_dir '", ",", "cache_dir", ")", "\n", "", "do_lower_case", "=", "not", "(", "'-cased'", "in", "tokenizer_model_type", "or", "'chinese'", "in", "tokenizer_model_type", ")", "\n", "self", ".", "text_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "tokenizer_model_type", ",", "do_lower_case", "=", "do_lower_case", ",", "cache_dir", "=", "cache_dir", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "            ", "print", "(", "'loaded'", ",", "tokenizer_model_type", ")", "\n", "# disable max len warnings by increasing max len", "\n", "", "self", ".", "text_tokenizer", ".", "max_len", "=", "int", "(", "1e12", ")", "\n", "\n", "# set command tokens from wordpiece tokenizer values", "\n", "self", ".", "num_command_tokens", "=", "5", "\n", "self", ".", "num_tokens", "=", "len", "(", "self", ".", "text_tokenizer", ".", "vocab", ")", "\n", "self", ".", "num_text_tokens", "=", "self", ".", "num_tokens", "-", "5", "\n", "self", ".", "num_type_tokens", "=", "2", "\n", "\n", "self", ".", "_command_tokens", "=", "[", "\n", "CommandToken", "(", "'pad'", ",", "'[PAD]'", ",", "self", ".", "text_tokenizer", ".", "vocab", "[", "'[PAD]'", "]", ")", ",", "\n", "CommandToken", "(", "'ENC'", ",", "'[CLS]'", ",", "self", ".", "text_tokenizer", ".", "vocab", "[", "'[CLS]'", "]", ")", ",", "\n", "CommandToken", "(", "'MASK'", ",", "'[MASK]'", ",", "self", ".", "text_tokenizer", ".", "vocab", "[", "'[MASK]'", "]", ")", ",", "\n", "CommandToken", "(", "'unk'", ",", "'[UNK]'", ",", "self", ".", "text_tokenizer", ".", "vocab", "[", "'[UNK]'", "]", ")", ",", "\n", "CommandToken", "(", "'sep'", ",", "'[SEP]'", ",", "self", ".", "text_tokenizer", ".", "vocab", "[", "'[SEP]'", "]", ")", ",", "\n", "]", "\n", "self", ".", "command_name_map", "=", "{", "tok", ".", "name", ":", "tok", "for", "tok", "in", "self", ".", "_command_tokens", "}", "\n", "self", ".", "command_token_map", "=", "{", "tok", ".", "token", ":", "tok", "for", "tok", "in", "self", ".", "_command_tokens", "}", "\n", "self", ".", "command_id_map", "=", "{", "tok", ".", "Id", ":", "tok", "for", "tok", "in", "self", ".", "_command_tokens", "}", "\n", "\n", "# set type tokens", "\n", "self", ".", "type_tokens", "=", "[", "\n", "TypeToken", "(", "'str0'", ",", "'<str0>'", ",", "0", ")", ",", "\n", "TypeToken", "(", "'str1'", ",", "'<str1>'", ",", "1", ")", ",", "\n", "]", "\n", "self", ".", "type_name_map", "=", "{", "tok", ".", "name", ":", "tok", "for", "tok", "in", "self", ".", "type_tokens", "}", "\n", "self", ".", "type_token_map", "=", "{", "tok", ".", "token", ":", "tok", "for", "tok", "in", "self", ".", "type_tokens", "}", "\n", "self", ".", "type_id_map", "=", "{", "tok", ".", "Id", ":", "tok", "for", "tok", "in", "self", ".", "type_tokens", "}", "\n", "\n", "# parse tokens and vocabs from tokenizer", "\n", "\n", "self", ".", "_tokens", "=", "list", "(", "self", ".", "text_tokenizer", ".", "vocab", ".", "keys", "(", ")", ")", "\n", "self", ".", "_vocab", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "text_tokenizer", ".", "vocab", ".", "items", "(", ")", "}", "\n", "\n", "self", ".", "_text_tokens", "=", "list", "(", "self", ".", "_tokens", ")", "\n", "self", ".", "_text_token_vocab", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "text_tokenizer", ".", "vocab", ".", "items", "(", ")", "}", "\n", "\n", "self", ".", "_command_token_tokens", "=", "list", "(", "self", ".", "command_token_map", ".", "keys", "(", ")", ")", "\n", "self", ".", "_command_token_vocab", "=", "{", "t", ":", "Id", "for", "Id", ",", "t", "in", "self", ".", "command_id_map", ".", "items", "(", ")", "}", "\n", "\n", "self", ".", "_token_types", "=", "list", "(", "self", ".", "type_token_map", ".", "keys", "(", ")", ")", "\n", "self", ".", "_token_type_vocab", "=", "{", "t", ":", "Id", "for", "Id", ",", "t", "in", "self", ".", "type_id_map", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.BertWordPieceTokenizer.EncodeAsIds": [[745, 753], ["tokenization.BertWordPieceTokenizer.text_tokenizer.tokenize", "tokenization.BertWordPieceTokenizer.text_tokenizer.convert_tokens_to_ids", "tokenization.Tokenization", "process_fn"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BertTokenizer.convert_tokens_to_ids"], ["", "def", "EncodeAsIds", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "\"\"\"convert text to wordpiece Ids\"\"\"", "\n", "processed_text", "=", "text", "\n", "if", "process_fn", "is", "not", "None", ":", "\n", "            ", "processed_text", "=", "process_fn", "(", "processed_text", ")", "\n", "", "tokens", "=", "self", ".", "text_tokenizer", ".", "tokenize", "(", "processed_text", ")", "\n", "Ids", "=", "self", ".", "text_tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "return", "Tokenization", "(", "Ids", ",", "processed_text", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.BertWordPieceTokenizer.EncodeAsTokens": [[754, 761], ["tokenization.BertWordPieceTokenizer.text_tokenizer.tokenize", "tokenization.Tokenization", "process_fn"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.WordpieceTokenizer.tokenize"], ["", "def", "EncodeAsTokens", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "\"\"\"convert wordpiece token to Id\"\"\"", "\n", "processed_text", "=", "text", "\n", "if", "process_fn", "is", "not", "None", ":", "\n", "            ", "processed_text", "=", "process_fn", "(", "processed_text", ")", "\n", "", "tokens", "=", "self", ".", "text_tokenizer", ".", "tokenize", "(", "processed_text", ")", "\n", "return", "Tokenization", "(", "tokens", ",", "processed_text", ",", "text", ",", "asIds", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.BertWordPieceTokenizer.IdToToken": [[762, 769], ["isinstance"], "methods", ["None"], ["", "def", "IdToToken", "(", "self", ",", "Id", ",", "type_token", "=", "False", ")", ":", "\n", "        ", "\"\"\"convert Id to sentencpiece token\"\"\"", "\n", "if", "isinstance", "(", "Id", ",", "(", "TypeToken", ",", "CommandToken", ")", ")", ":", "\n", "            ", "return", "Id", ".", "token", "\n", "", "if", "type_token", ":", "\n", "            ", "return", "self", ".", "type_id_map", "[", "Id", "]", ".", "token", "\n", "", "return", "self", ".", "text_tokenizer", ".", "ids_to_tokens", "[", "Id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.BertWordPieceTokenizer.TokenToId": [[770, 777], ["isinstance"], "methods", ["None"], ["", "def", "TokenToId", "(", "self", ",", "token", ",", "type_token", "=", "False", ")", ":", "\n", "        ", "\"\"\"convert sentencpiece token to Id\"\"\"", "\n", "if", "isinstance", "(", "token", ",", "(", "TypeToken", ",", "CommandToken", ")", ")", ":", "\n", "            ", "return", "token", ".", "Id", "\n", "", "if", "type_token", ":", "\n", "            ", "return", "self", ".", "type_token_map", "[", "token", "]", ".", "Id", "\n", "", "return", "self", ".", "text_tokenizer", ".", "vocab", "[", "token", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.BertWordPieceTokenizer.DecodeIds": [[778, 789], ["isinstance", "tokenization.BertWordPieceTokenizer.text_tokenizer.convert_ids_to_tokens", "tokenization.BertWordPieceTokenizer.append", "isinstance"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BertTokenizer.convert_ids_to_tokens", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "DecodeIds", "(", "self", ",", "Ids", ",", "type_token", "=", "False", ")", ":", "\n", "        ", "\"\"\"converts ids to wordpiece tokens and joins them as a text string\"\"\"", "\n", "if", "type_token", ":", "\n", "            ", "return", "' '", ".", "join", "(", "Id", ".", "token", "if", "isinstance", "(", "Id", ",", "TypeToken", ")", "else", "self", ".", "type_id_map", "[", "Id", "]", ".", "token", "for", "Id", "in", "Ids", ")", "\n", "", "if", "isinstance", "(", "Ids", ",", "Tokenization", ")", ":", "\n", "            ", "Ids", "=", "Ids", ".", "tokenization", "\n", "", "Tokens", "=", "[", "]", "\n", "for", "Id", "in", "Ids", ":", "\n", "            ", "Tokens", ".", "append", "(", "self", ".", "text_tokenizer", ".", "ids_to_tokens", "[", "Id", "]", "if", "Id", "!=", "-", "1", "else", "'-1'", ")", "\n", "", "Tokens", "=", "self", ".", "text_tokenizer", ".", "convert_ids_to_tokens", "(", "Ids", ")", "\n", "return", "' '", ".", "join", "(", "Tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.BertWordPieceTokenizer.DecodeTokens": [[790, 797], ["isinstance", "isinstance"], "methods", ["None"], ["", "def", "DecodeTokens", "(", "self", ",", "Tokens", ",", "type_token", "=", "False", ")", ":", "\n", "        ", "\"\"\"converts wordpiece tokens to a text string\"\"\"", "\n", "if", "type_token", ":", "\n", "            ", "return", "' '", ".", "join", "(", "t", ".", "token", "if", "isinstance", "(", "t", ",", "TypeToken", ")", "else", "t", "for", "t", "in", "Tokens", ")", "\n", "", "if", "isinstance", "(", "Tokens", ",", "Tokenization", ")", ":", "\n", "            ", "Tokens", "=", "Tokens", ".", "tokenization", "\n", "", "return", "' '", ".", "join", "(", "Tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.__init__": [[800, 838], ["tokenization_gpt2.GPT2Tokenizer.from_pretrained", "int", "len", "list", "list", "list", "list", "tokenization.CommandToken", "tokenization.CommandToken", "tokenization.TypeToken", "tokenization.TypeToken", "tokenization.GPT2BPETokenizer.text_tokenizer.encoder.keys", "tokenization.GPT2BPETokenizer.command_token_map.keys", "tokenization.GPT2BPETokenizer.type_token_map.keys", "tokenization.GPT2BPETokenizer.text_tokenizer.encoder.items", "tokenization.GPT2BPETokenizer.text_tokenizer.encoder.items", "tokenization.GPT2BPETokenizer.command_id_map.items", "tokenization.GPT2BPETokenizer.type_id_map.items"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.PreTrainedBertModel.from_pretrained"], ["    ", "def", "__init__", "(", "self", ",", "cache_dir", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "text_tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "'gpt2'", ",", "\n", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "#disable max len warnings by increasing max len", "\n", "self", ".", "text_tokenizer", ".", "max_len", "=", "int", "(", "1e12", ")", "\n", "self", ".", "num_command_tokens", "=", "2", "\n", "self", ".", "num_tokens", "=", "len", "(", "self", ".", "text_tokenizer", ".", "encoder", ")", "\n", "self", ".", "num_text_tokens", "=", "self", ".", "num_tokens", "-", "1", "\n", "self", ".", "num_type_tokens", "=", "2", "\n", "\n", "self", ".", "_command_tokens", "=", "[", "\n", "CommandToken", "(", "'pad'", ",", "'<|endoftext|>'", ",", "self", ".", "text_tokenizer", ".", "encoder", "[", "'<|endoftext|>'", "]", ")", ",", "\n", "CommandToken", "(", "'eos'", ",", "'<|endoftext|>'", ",", "self", ".", "text_tokenizer", ".", "encoder", "[", "'<|endoftext|>'", "]", ")", ",", "\n", "]", "\n", "self", ".", "command_name_map", "=", "{", "tok", ".", "name", ":", "tok", "for", "tok", "in", "self", ".", "_command_tokens", "}", "\n", "self", ".", "command_token_map", "=", "{", "tok", ".", "token", ":", "tok", "for", "tok", "in", "self", ".", "_command_tokens", "}", "\n", "self", ".", "command_id_map", "=", "{", "tok", ".", "Id", ":", "tok", "for", "tok", "in", "self", ".", "_command_tokens", "}", "\n", "\n", "self", ".", "type_tokens", "=", "[", "\n", "TypeToken", "(", "'str0'", ",", "'<str0>'", ",", "0", ")", ",", "\n", "TypeToken", "(", "'str1'", ",", "'<str1>'", ",", "1", ")", ",", "\n", "]", "\n", "self", ".", "type_name_map", "=", "{", "tok", ".", "name", ":", "tok", "for", "tok", "in", "self", ".", "type_tokens", "}", "\n", "self", ".", "type_token_map", "=", "{", "tok", ".", "token", ":", "tok", "for", "tok", "in", "self", ".", "type_tokens", "}", "\n", "self", ".", "type_id_map", "=", "{", "tok", ".", "Id", ":", "tok", "for", "tok", "in", "self", ".", "type_tokens", "}", "\n", "\n", "self", ".", "_tokens", "=", "list", "(", "self", ".", "text_tokenizer", ".", "encoder", ".", "keys", "(", ")", ")", "\n", "self", ".", "_vocab", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "text_tokenizer", ".", "encoder", ".", "items", "(", ")", "}", "\n", "\n", "self", ".", "_text_tokens", "=", "list", "(", "self", ".", "_tokens", ")", "\n", "self", ".", "_text_token_vocab", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "text_tokenizer", ".", "encoder", ".", "items", "(", ")", "}", "\n", "\n", "self", ".", "_command_token_tokens", "=", "list", "(", "self", ".", "command_token_map", ".", "keys", "(", ")", ")", "\n", "self", ".", "_command_token_vocab", "=", "{", "t", ":", "Id", "for", "Id", ",", "t", "in", "self", ".", "command_id_map", ".", "items", "(", ")", "}", "\n", "\n", "self", ".", "_token_types", "=", "list", "(", "self", ".", "type_token_map", ".", "keys", "(", ")", ")", "\n", "self", ".", "_token_type_vocab", "=", "{", "t", ":", "Id", "for", "Id", ",", "t", "in", "self", ".", "type_id_map", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsIds": [[839, 848], ["Tokenization.GPT2BPETokenizer.text_tokenizer.encode", "Tokenization.Tokenization", "Tokenization.Tokenization.set_command_tokens", "process_fn"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.set_command_tokens"], ["", "def", "EncodeAsIds", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "processed_text", "=", "text", "\n", "if", "process_fn", "is", "not", "None", ":", "\n", "            ", "processed_text", "=", "process_fn", "(", "processed_text", ")", "\n", "", "Ids", "=", "self", ".", "text_tokenizer", ".", "encode", "(", "processed_text", ")", "\n", "#return Tokenization(Ids, processed_text, text)", "\n", "tokenization", "=", "Tokenization", "(", "Ids", ",", "processed_text", ",", "text", ")", "\n", "tokenization", ".", "set_command_tokens", "(", "self", ".", "_command_tokens", ")", "\n", "return", "tokenization", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsTokens": [[850, 861], ["regex.findall", "Tokenization.Tokenization", "Tokenization.Tokenization.set_command_tokens", "process_fn", "tokens.extend", "token.encode", "Tokenization.GPT2BPETokenizer.text_tokenizer.bpe().split", "Tokenization.GPT2BPETokenizer.text_tokenizer.bpe"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.set_command_tokens", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.encode"], ["", "def", "EncodeAsTokens", "(", "self", ",", "text", ",", "process_fn", "=", "None", ")", ":", "\n", "        ", "processed_text", "=", "text", "\n", "if", "process_fn", "is", "not", "None", ":", "\n", "            ", "processed_text", "=", "process_fn", "(", "processed_text", ")", "\n", "", "tokens", "=", "[", "]", "\n", "for", "token", "in", "re", ".", "findall", "(", "self", ".", "text_tokenizer", ".", "pat", ",", "processed_text", ")", ":", "\n", "            ", "token", "=", "''", ".", "join", "(", "self", ".", "text_tokenizer", ".", "bye_encoder", "[", "b", "]", "for", "b", "in", "token", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "tokens", ".", "extend", "(", "bpe_token", "for", "bpe_token", "in", "self", ".", "text_tokenizer", ".", "bpe", "(", "token", ")", ".", "split", "(", "' '", ")", ")", "\n", "", "tokenization", "=", "Tokenization", "(", "tokens", ",", "processed_text", ",", "text", ",", "asIds", "=", "False", ")", "\n", "tokenization", ".", "set_command_tokens", "(", "self", ".", "_command_tokens", ")", "\n", "return", "tokenization", "\n", "#return Tokenization(tokens, processed_text, text, asIds=False)", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.IdToToken": [[863, 869], ["isinstance"], "methods", ["None"], ["", "def", "IdToToken", "(", "self", ",", "Id", ",", "type_token", "=", "False", ")", ":", "\n", "        ", "if", "isinstance", "(", "Id", ",", "(", "TypeToken", ",", "CommandToken", ")", ")", ":", "\n", "            ", "return", "Id", ".", "token", "\n", "", "if", "type_token", ":", "\n", "            ", "return", "self", ".", "type_id_map", "[", "Id", "]", ".", "token", "\n", "", "return", "self", ".", "text_tokenizer", ".", "decoder", "[", "Id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.TokenToId": [[870, 876], ["isinstance"], "methods", ["None"], ["", "def", "TokenToId", "(", "self", ",", "token", ",", "type_token", "=", "False", ")", ":", "\n", "        ", "if", "isinstance", "(", "token", ",", "(", "TypeToken", ",", "CommandToken", ")", ")", ":", "\n", "            ", "return", "token", ".", "Id", "\n", "", "if", "type_token", ":", "\n", "            ", "return", "self", ".", "type_token_map", "[", "token", "]", ".", "Id", "\n", "", "return", "self", ".", "text_tokenizer", ".", "encoder", "[", "token", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.DecodeIds": [[877, 883], ["isinstance", "tokenization.GPT2BPETokenizer.text_tokenizer.decode", "isinstance"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.decode"], ["", "def", "DecodeIds", "(", "self", ",", "Ids", ",", "type_token", "=", "False", ")", ":", "\n", "        ", "if", "type_token", ":", "\n", "            ", "return", "' '", ".", "join", "(", "Id", ".", "token", "if", "isinstance", "(", "Id", ",", "TypeToken", ")", "else", "self", ".", "type_id_map", "[", "Id", "]", ".", "token", "for", "Id", "in", "Ids", ")", "\n", "", "if", "isinstance", "(", "Ids", ",", "Tokenization", ")", ":", "\n", "            ", "Ids", "=", "Ids", ".", "tokenization", "\n", "", "return", "self", ".", "text_tokenizer", ".", "decode", "(", "Ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.DecodeTokens": [[884, 890], ["isinstance", "tokenization.GPT2BPETokenizer.text_tokenizer.decode", "tokenization.GPT2BPETokenizer.TokenToId", "isinstance"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization_gpt2.GPT2Tokenizer.decode", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.TokenToId"], ["", "def", "DecodeTokens", "(", "self", ",", "Tokens", ",", "type_token", "=", "False", ")", ":", "\n", "        ", "if", "type_token", ":", "\n", "            ", "return", "' '", ".", "join", "(", "t", ".", "token", "if", "isinstance", "(", "t", ",", "TypeToken", ")", "else", "t", "for", "t", "in", "Tokens", ")", "\n", "", "if", "isinstance", "(", "Tokens", ",", "Tokenization", ")", ":", "\n", "            ", "Tokens", "=", "Tokens", ".", "tokenization", "\n", "", "return", "self", ".", "text_tokenizer", ".", "decode", "(", "[", "self", ".", "TokenToId", "(", "tok", ")", "for", "tok", "in", "Tokens", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.make_tokenizer": [[31, 45], ["isinstance", "eval.", "tokenization.Tokenizer", "eval", "tokenization.BertWordPieceTokenizer", "tokenization.GPT2BPETokenizer"], "function", ["None"], ["def", "make_tokenizer", "(", "tokenizer_type", ",", "corpus", ",", "model_path", "=", "None", ",", "vocab_size", "=", "None", ",", "model_type", "=", "'bpe'", ",", "pad_token", "=", "0", ",", "character_coverage", "=", "1.0", ",", "command_tokens", "=", "None", ",", "type_tokens", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    Helper function to instantiate a tokenizer given common combinations of options.\n    \"\"\"", "\n", "tokenizer_class", "=", "tokenizer_type", "\n", "if", "isinstance", "(", "tokenizer_class", ",", "str", ")", ":", "\n", "        ", "tokenizer_class", "=", "eval", "(", "tokenizer_class", ")", "\n", "", "if", "tokenizer_class", "is", "BertWordPieceTokenizer", ":", "\n", "        ", "return", "BertWordPieceTokenizer", "(", "model_type", ",", "**", "kwargs", ")", "\n", "", "elif", "tokenizer_class", "is", "GPT2BPETokenizer", ":", "\n", "        ", "return", "GPT2BPETokenizer", "(", "**", "kwargs", ")", "\n", "", "text_tokenizer", "=", "tokenizer_class", "(", "corpus", "=", "corpus", ",", "vocab_size", "=", "vocab_size", ",", "model_path", "=", "model_path", ",", "model_type", "=", "model_type", ",", "\n", "pad_token", "=", "pad_token", ",", "character_coverage", "=", "character_coverage", ")", "\n", "return", "Tokenizer", "(", "text_tokenizer", ",", "command_tokens", ",", "type_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.prep_command_tokens": [[137, 139], ["tokenization.CommandToken", "token_format.format"], "function", ["None"], ["def", "prep_command_tokens", "(", "tokenlist", ",", "token_format", "=", "token_format", ")", ":", "\n", "    ", "return", "[", "CommandToken", "(", "tok", "[", "0", "]", ",", "token_format", ".", "format", "(", "tok", "[", "0", "]", ")", ",", "tok", "[", "1", "]", ")", "for", "tok", "in", "tokenlist", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.prep_type_tokens": [[165, 167], ["tokenization.TypeToken", "token_format.format"], "function", ["None"], ["def", "prep_type_tokens", "(", "tokenlist", ",", "token_format", "=", "token_format", ")", ":", "\n", "    ", "return", "[", "TypeToken", "(", "tok", "[", "0", "]", ",", "token_format", ".", "format", "(", "tok", "[", "0", "]", ")", ",", "tok", "[", "1", "]", ")", "for", "tok", "in", "tokenlist", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.get_corpus_freq": [[524, 575], ["nltk.download", "print", "print", "print", "sorted", "print", "isinstance", "entry.strip().split", "freqs.items", "open", "csv.writer", "freqs_sorted.items", "nltk.tokenize.sent_tokenize", "len", "str", "str", "str", "csv.writer.writerow", "entry.strip", "max", "sentence.split", "len", "len", "len", "str", "str"], "function", ["None"], ["def", "get_corpus_freq", "(", "dataset", ",", "filepath", ",", "filetype", "=", "'tsv'", ")", ":", "\n", "    ", "\"\"\"\n    Take corpus, split it into sentences, and extract word frequencies.\n    Write frequencies to `filepath` as a tsv. Only write the first\n    MAX_SENTENCEPIECE_SENTENCES most common words to the file.\n    \"\"\"", "\n", "nltk", ".", "download", "(", "'punkt'", ",", "download_dir", "=", "\"./nltk\"", ")", "\n", "if", "filetype", "==", "'tsv'", ":", "\n", "        ", "delimiter", "=", "'\\t'", "\n", "", "else", ":", "\n", "        ", "delimiter", "=", "','", "\n", "\n", "", "print", "(", "\"compute corpus frequency\\n\"", ",", "flush", "=", "True", ")", "\n", "\n", "total_sentence_count", "=", "0", "\n", "maxlen", "=", "0", "\n", "freqs", "=", "{", "}", "\n", "for", "entry", "in", "dataset", ":", "\n", "        ", "if", "isinstance", "(", "entry", ",", "dict", ")", ":", "\n", "            ", "entry", "=", "entry", "[", "'text'", "]", "\n", "", "lines", "=", "entry", ".", "strip", "(", ")", ".", "split", "(", "'\\n'", ")", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "sentences", "=", "nltk_tokenize", ".", "sent_tokenize", "(", "line", ")", "\n", "total_sentence_count", "+=", "len", "(", "sentences", ")", "\n", "for", "sentence", "in", "sentences", ":", "\n", "                ", "maxlen", "=", "max", "(", "len", "(", "line", ")", ",", "maxlen", ")", "\n", "for", "word", "in", "sentence", ".", "split", "(", ")", ":", "\n", "                    ", "if", "word", "not", "in", "freqs", ":", "\n", "                        ", "freqs", "[", "word", "]", "=", "0", "\n", "", "freqs", "[", "word", "]", "+=", "1", "\n", "\n", "", "", "", "", "print", "(", "\"length of freqs before truncating \"", "+", "str", "(", "len", "(", "freqs", ")", ")", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"file path for freq \"", "+", "str", "(", "filepath", ")", ",", "flush", "=", "True", ")", "\n", "\n", "freqs_sorted", "=", "{", "}", "\n", "counter", "=", "0", "\n", "for", "word", ",", "count", "in", "sorted", "(", "freqs", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", ":", "\n", "        ", "if", "counter", ">=", "MAX_SENTENCEPIECE_SENTENCES", ":", "\n", "            ", "break", "\n", "", "counter", "+=", "1", "\n", "freqs_sorted", "[", "word", "]", "=", "count", "\n", "\n", "\n", "", "print", "(", "\"length of freqs after trancating \"", "+", "str", "(", "len", "(", "freqs_sorted", ")", ")", ",", "flush", "=", "True", ")", "\n", "\n", "with", "open", "(", "filepath", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "writer", "=", "csv", ".", "writer", "(", "f", ",", "delimiter", "=", "delimiter", ")", "\n", "for", "k", ",", "v", "in", "freqs_sorted", ".", "items", "(", ")", ":", "\n", "            ", "writer", ".", "writerow", "(", "[", "str", "(", "k", ")", ",", "str", "(", "v", ")", "]", ")", "\n", "\n", "", "", "return", "total_sentence_count", ",", "maxlen", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tf_dl.TFRecordDataLoader.__init__": [[26, 64], ["tensorflow.set_random_seed", "isinstance", "tf_dl.Record2Example", "tf_dl.TFRecordDataLoader.dataset.apply", "tensorflow.data.Dataset.from_tensor_slices", "tf_dl.TFRecordDataLoader.dataset.repeat", "tf_dl.TFRecordDataLoader.dataset.shuffle", "tf_dl.TFRecordDataLoader.dataset.apply", "tf_dl.TFRecordDataLoader.dataset.shuffle", "tensorflow.data.TFRecordDataset", "tf_dl.TFRecordDataLoader.dataset.repeat", "tensorflow.contrib.data.map_and_batch", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.constant", "tensorflow.contrib.data.parallel_interleave", "len", "min", "len"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["    ", "def", "__init__", "(", "self", ",", "records", ",", "batch_size", ",", "max_seq_len", ",", "max_preds_per_seq", ",", "train", ",", "num_workers", "=", "2", ",", "seed", "=", "1", ",", "threaded_dl", "=", "False", ")", ":", "\n", "        ", "assert", "max_preds_per_seq", "is", "not", "None", ",", "\"--max-preds-per-seq MUST BE SPECIFIED when using tfrecords\"", "\n", "tf", ".", "set_random_seed", "(", "seed", ")", "\n", "if", "isinstance", "(", "records", ",", "str", ")", ":", "\n", "            ", "records", "=", "[", "records", "]", "\n", "\n", "", "self", ".", "record_converter", "=", "Record2Example", "(", "{", "\"input_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "max_seq_len", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_mask\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "max_seq_len", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"segment_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "max_seq_len", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"masked_lm_positions\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "max_preds_per_seq", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"masked_lm_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "max_preds_per_seq", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"masked_lm_weights\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "max_preds_per_seq", "]", ",", "tf", ".", "float32", ")", ",", "\n", "\"next_sentence_labels\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "1", "]", ",", "tf", ".", "int64", ")", "}", ")", "\n", "\n", "#Instantiate dataset according to original BERT implementation", "\n", "if", "train", ":", "\n", "            ", "self", ".", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "tf", ".", "constant", "(", "records", ")", ")", "\n", "self", ".", "dataset", "=", "self", ".", "dataset", ".", "repeat", "(", ")", "\n", "self", ".", "dataset", "=", "self", ".", "dataset", ".", "shuffle", "(", "buffer_size", "=", "len", "(", "records", ")", ")", "\n", "\n", "# use sloppy tfrecord dataset", "\n", "self", ".", "dataset", "=", "self", ".", "dataset", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "parallel_interleave", "(", "\n", "tf", ".", "data", ".", "TFRecordDataset", ",", "\n", "sloppy", "=", "train", ",", "\n", "cycle_length", "=", "min", "(", "num_workers", ",", "len", "(", "records", ")", ")", ")", ")", "\n", "self", ".", "dataset", "=", "self", ".", "dataset", ".", "shuffle", "(", "buffer_size", "=", "100", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dataset", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "records", ")", "\n", "self", ".", "dataset", "=", "self", ".", "dataset", ".", "repeat", "(", ")", "\n", "\n", "# Instantiate dataloader (do not drop remainder for eval)", "\n", "", "loader_args", "=", "{", "'batch_size'", ":", "batch_size", ",", "\n", "'num_parallel_batches'", ":", "num_workers", ",", "\n", "'drop_remainder'", ":", "train", "}", "\n", "self", ".", "dataloader", "=", "self", ".", "dataset", ".", "apply", "(", "tf", ".", "contrib", ".", "data", ".", "map_and_batch", "(", "self", ".", "record_converter", ",", "**", "loader_args", ")", ")", "\n", "self", ".", "threaded_dl", "=", "threaded_dl", "\n", "self", ".", "num_workers", "=", "num_workers", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tf_dl.TFRecordDataLoader.__iter__": [[65, 74], ["iter", "iter", "tf_dl.MultiprocessLoader", "tf_dl.convert_tf_example_to_torch_tensors"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tf_dl.convert_tf_example_to_torch_tensors"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threaded_dl", ":", "\n", "            ", "data_iter", "=", "iter", "(", "MultiprocessLoader", "(", "self", ".", "dataloader", ",", "self", ".", "num_workers", ")", ")", "\n", "for", "item", "in", "data_iter", ":", "\n", "                ", "yield", "item", "\n", "", "", "else", ":", "\n", "            ", "data_iter", "=", "iter", "(", "self", ".", "dataloader", ")", "\n", "for", "item", "in", "data_iter", ":", "\n", "                ", "yield", "convert_tf_example_to_torch_tensors", "(", "item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tf_dl.Record2Example.__init__": [[76, 78], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "feature_map", ")", ":", "\n", "        ", "self", ".", "feature_map", "=", "feature_map", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tf_dl.Record2Example.__call__": [[79, 86], ["tensorflow.parse_single_example", "list", "tensorflow.parse_single_example.items", "tensorflow.to_int32"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "record", ")", ":", "\n", "        ", "\"\"\"Decodes a BERT TF record to a TF example.\"\"\"", "\n", "example", "=", "tf", ".", "parse_single_example", "(", "record", ",", "self", ".", "feature_map", ")", "\n", "for", "k", ",", "v", "in", "list", "(", "example", ".", "items", "(", ")", ")", ":", "\n", "            ", "if", "v", ".", "dtype", "==", "tf", ".", "int64", ":", "\n", "                ", "example", "[", "k", "]", "=", "tf", ".", "to_int32", "(", "v", ")", "\n", "", "", "return", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tf_dl.MultiprocessLoader.__init__": [[101, 104], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dataloader", ",", "num_workers", "=", "2", ")", ":", "\n", "        ", "self", ".", "dl", "=", "dataloader", "\n", "self", ".", "queue_size", "=", "2", "*", "num_workers", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tf_dl.MultiprocessLoader.__iter__": [[105, 116], ["queue.Queue", "threading.Thread", "threading.Thread.start", "threading.Thread.is_alive", "print", "queue.Queue.get", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "output_queue", "=", "queue", ".", "Queue", "(", "self", ".", "queue_size", ")", "\n", "output_thread", "=", "threading", ".", "Thread", "(", "target", "=", "_multiproc_iter", ",", "\n", "args", "=", "(", "self", ".", "dl", ",", "output_queue", ")", ")", "\n", "output_thread", ".", "daemon", "=", "True", "\n", "output_thread", ".", "start", "(", ")", "\n", "\n", "while", "output_thread", ".", "is_alive", "(", ")", ":", "\n", "            ", "yield", "output_queue", ".", "get", "(", "block", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "RuntimeError", "(", "'TF record data loader thread exited unexpectedly'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tf_dl.convert_tf_example_to_torch_tensors": [[87, 99], ["numpy.zeros_like", "enumerate", "v.numpy", "numpy.ones_like", "item[].astype", "enumerate", "torch.from_numpy", "example.items", "output.items"], "function", ["None"], ["", "", "def", "convert_tf_example_to_torch_tensors", "(", "example", ")", ":", "\n", "    ", "item", "=", "{", "k", ":", "(", "v", ".", "numpy", "(", ")", ")", "for", "k", ",", "v", "in", "example", ".", "items", "(", ")", "}", "\n", "mask", "=", "np", ".", "zeros_like", "(", "item", "[", "'input_ids'", "]", ")", "\n", "mask_labels", "=", "np", ".", "ones_like", "(", "item", "[", "'input_ids'", "]", ")", "*", "-", "1", "\n", "for", "b", ",", "row", "in", "enumerate", "(", "item", "[", "'masked_lm_positions'", "]", ".", "astype", "(", "int", ")", ")", ":", "\n", "        ", "for", "i", ",", "idx", "in", "enumerate", "(", "row", ")", ":", "\n", "            ", "if", "item", "[", "'masked_lm_weights'", "]", "[", "b", ",", "i", "]", "!=", "0", ":", "\n", "                ", "mask", "[", "b", ",", "idx", "]", "=", "1", "\n", "mask_labels", "[", "b", ",", "idx", "]", "=", "item", "[", "'masked_lm_ids'", "]", "[", "b", ",", "i", "]", "\n", "", "", "", "output", "=", "{", "'text'", ":", "item", "[", "'input_ids'", "]", ",", "'types'", ":", "item", "[", "'segment_ids'", "]", ",", "'is_random'", ":", "item", "[", "'next_sentence_labels'", "]", ",", "\n", "'pad_mask'", ":", "1", "-", "item", "[", "'input_mask'", "]", ",", "'mask'", ":", "mask", ",", "'mask_labels'", ":", "mask_labels", "}", "\n", "return", "{", "k", ":", "torch", ".", "from_numpy", "(", "v", ")", "for", "k", ",", "v", "in", "output", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tf_dl._multiproc_iter": [[117, 122], ["iter", "tf_dl.convert_tf_example_to_torch_tensors", "output_queue.put"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tf_dl.convert_tf_example_to_torch_tensors"], ["", "", "", "def", "_multiproc_iter", "(", "dl", ",", "output_queue", ")", ":", "\n", "    ", "data_iter", "=", "iter", "(", "dl", ")", "\n", "for", "item", "in", "data_iter", ":", "\n", "        ", "tensors", "=", "convert_tf_example_to_torch_tensors", "(", "item", ")", "\n", "output_queue", ".", "put", "(", "tensors", ",", "block", "=", "True", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BertTokenizer.__init__": [[77, 106], ["wordpiece.load_vocab", "collections.OrderedDict", "wordpiece.WordpieceTokenizer", "os.path.isfile", "ValueError", "wordpiece.BasicTokenizer", "int", "wordpiece.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ",", "max_len", "=", "None", ",", "do_basic_tokenize", "=", "True", ",", "\n", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BertTokenizer.\n\n        Args:\n          vocab_file: Path to a one-wordpiece-per-line vocabulary file\n          do_lower_case: Whether to lower case the input\n                         Only has an effect when do_wordpiece_only=False\n          do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n          max_len: An artificial maximum length to truncate tokenized sequences to;\n                         Effective maximum length is always the minimum of this\n                         value (if specified) and the underlying BERT model's\n                         sequence length.\n          never_split: List of tokens which will never be split during tokenization.\n                         Only has an effect when do_wordpiece_only=False\n        \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"", "\n", "\"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "ids_to_tokens", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "(", "ids", ",", "tok", ")", "for", "tok", ",", "ids", "in", "self", ".", "vocab", ".", "items", "(", ")", "]", ")", "\n", "self", ".", "do_basic_tokenize", "=", "do_basic_tokenize", "\n", "if", "do_basic_tokenize", ":", "\n", "          ", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ",", "\n", "never_split", "=", "never_split", ")", "\n", "", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BertTokenizer.tokenize": [[107, 116], ["wordpiece.BertTokenizer.basic_tokenizer.tokenize", "wordpiece.BertTokenizer.wordpiece_tokenizer.tokenize", "wordpiece.BertTokenizer.wordpiece_tokenizer.tokenize", "wordpiece.BertTokenizer.append"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "if", "self", ".", "do_basic_tokenize", ":", "\n", "          ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "              ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                  ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "", "else", ":", "\n", "          ", "split_tokens", "=", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "text", ")", "\n", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BertTokenizer.convert_tokens_to_ids": [[117, 129], ["ids.append", "len", "logger.warning", "len"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "vocab", "[", "token", "]", ")", "\n", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this BERT model ({} > {}). Running this\"", "\n", "\" sequence through BERT will result in indexing errors\"", ".", "format", "(", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BertTokenizer.convert_ids_to_tokens": [[130, 136], ["tokens.append"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", ".", "append", "(", "self", ".", "ids_to_tokens", "[", "i", "]", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BertTokenizer.from_pretrained": [[137, 174], ["os.path.isdir", "cls", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.file_utils.cached_path", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "pretrained_model_name_or_path", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "vocab_file", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_file", ",", "VOCAB_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "vocab_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BasicTokenizer.__init__": [[179, 189], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "do_lower_case", "=", "True", ",", "\n", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "never_split", "=", "never_split", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BasicTokenizer.tokenize": [[190, 210], ["wordpiece.BasicTokenizer._clean_text", "wordpiece.BasicTokenizer._tokenize_chinese_chars", "wordpiece.whitespace_tokenize", "wordpiece.whitespace_tokenize", "split_tokens.extend", "wordpiece.BasicTokenizer.lower", "wordpiece.BasicTokenizer._run_strip_accents", "wordpiece.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.whitespace_tokenize", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.whitespace_tokenize", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "never_split", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BasicTokenizer._run_strip_accents": [[211, 221], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BasicTokenizer._run_split_on_punc": [[222, 243], ["list", "len", "wordpiece._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece._is_punctuation", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "text", "in", "self", ".", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BasicTokenizer._tokenize_chinese_chars": [[244, 256], ["ord", "wordpiece.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BasicTokenizer._is_chinese_char", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BasicTokenizer._is_chinese_char": [[257, 278], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.BasicTokenizer._clean_text": [[279, 291], ["ord", "wordpiece._is_whitespace", "wordpiece._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece._is_whitespace", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece._is_control", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.WordpieceTokenizer.__init__": [[296, 300], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.WordpieceTokenizer.tokenize": [[301, 351], ["wordpiece.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.whitespace_tokenize", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.load_vocab": [[50, 63], ["collections.OrderedDict", "io.open", "reader.readline", "token.strip.strip"], "function", ["None"], ["def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece.whitespace_tokenize": [[65, 72], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece._is_whitespace": [[353, 363], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece._is_control": [[365, 375], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.wordpiece._is_punctuation": [[377, 391], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.__init__.should_split": [[29, 39], ["max", "sum"], "function", ["None"], ["from", ".", "fp16", "import", "*", "\n", "from", ".", "loss_scaler", "import", "*", "\n", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.__init__.get_ext": [[40, 43], ["os.path.splitext"], "function", ["None"], []], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.__init__.get_dataset": [[44, 56], ["__init__.supported_corpus", "__init__.get_ext", "datasets.json_dataset", "datasets.csv_dataset", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.__init__.supported_corpus", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.__init__.get_ext"], []], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.__init__.supported_corpus": [[57, 60], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.__init__.make_dataset": [[61, 122], ["isinstance", "isinstance", "datasets.GPT2Dataset.SetTokenizer", "__init__.should_split", "eval", "__init__.make_dataset.get_dataset_from_path"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.SetTokenizer", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.__init__.should_split"], []], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.cumsum": [[47, 55], ["len", "r.append"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["@", "staticmethod", "\n", "def", "cumsum", "(", "sequence", ")", ":", "\n", "        ", "r", ",", "s", "=", "[", "]", ",", "0", "\n", "for", "e", "in", "sequence", ":", "\n", "            ", "l", "=", "len", "(", "e", ")", "\n", "r", ".", "append", "(", "l", "+", "s", ")", "\n", "s", "+=", "l", "\n", "", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.__init__": [[56, 65], ["torch.utils.data.Dataset.__init__", "list", "datasets.ConcatDataset.cumsum", "len", "sum", "len", "isinstance"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.cumsum"], ["", "def", "__init__", "(", "self", ",", "datasets", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "ConcatDataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "len", "(", "datasets", ")", ">", "0", ",", "'datasets should not be an empty iterable'", "\n", "self", ".", "datasets", "=", "list", "(", "datasets", ")", "\n", "self", ".", "is_lazy", "=", "sum", "(", "[", "isinstance", "(", "ds", ",", "lazy_array_loader", ")", "for", "ds", "in", "self", ".", "datasets", "]", ")", "==", "len", "(", "self", ".", "datasets", ")", "\n", "self", ".", "cumulative_sizes", "=", "self", ".", "cumsum", "(", "self", ".", "datasets", ")", "\n", "self", ".", "_X", "=", "None", "\n", "self", ".", "_Y", "=", "None", "\n", "self", ".", "_lens", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.SetTokenizer": [[66, 69], ["ds.SetTokenizer"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.SetTokenizer"], ["", "def", "SetTokenizer", "(", "self", ",", "tokenizer", ")", ":", "\n", "        ", "for", "ds", "in", "self", ".", "datasets", ":", "\n", "            ", "ds", ".", "SetTokenizer", "(", "tokenizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.GetTokenizer": [[70, 72], ["datasets.ConcatDataset.datasets[].GetTokenizer"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.GetTokenizer"], ["", "", "def", "GetTokenizer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "datasets", "[", "0", "]", ".", "GetTokenizer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.__len__": [[73, 75], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cumulative_sizes", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.__getitem__": [[76, 83], ["bisect.bisect_right"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "dataset_idx", "=", "bisect_right", "(", "self", ".", "cumulative_sizes", ",", "idx", ")", "\n", "if", "dataset_idx", "==", "0", ":", "\n", "            ", "sample_idx", "=", "idx", "\n", "", "else", ":", "\n", "            ", "sample_idx", "=", "idx", "-", "self", ".", "cumulative_sizes", "[", "dataset_idx", "-", "1", "]", "\n", "", "return", "self", ".", "datasets", "[", "dataset_idx", "]", "[", "sample_idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.lens": [[84, 95], ["datasets.ConcatDataset._lens.extend", "datasets.ConcatDataset._lens.extend", "isinstance", "len", "len"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend"], ["", "@", "property", "\n", "def", "lens", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_lens", "is", "None", ":", "\n", "            ", "self", ".", "_lens", "=", "[", "]", "\n", "if", "self", ".", "is_lazy", ":", "\n", "                ", "for", "data", "in", "self", ".", "datasets", ":", "\n", "                    ", "self", ".", "_lens", ".", "extend", "(", "data", ".", "lens", ")", "\n", "", "", "else", ":", "\n", "                ", "for", "data", "in", "self", ".", "datasets", ":", "\n", "                    ", "self", ".", "_lens", ".", "extend", "(", "[", "len", "(", "d", "[", "'text'", "]", ")", "if", "isinstance", "(", "d", ",", "dict", ")", "else", "len", "(", "d", ")", "for", "d", "in", "data", "]", ")", "\n", "", "", "", "return", "self", ".", "_lens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.X": [[96, 103], ["datasets.ConcatDataset._X.extend"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend"], ["", "@", "property", "\n", "def", "X", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_X", "is", "None", ":", "\n", "            ", "self", ".", "_X", "=", "[", "]", "\n", "for", "data", "in", "self", ".", "datasets", ":", "\n", "                ", "self", ".", "_X", ".", "extend", "(", "data", ".", "X", ")", "\n", "", "", "return", "self", ".", "_X", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.Y": [[104, 112], ["numpy.array", "datasets.ConcatDataset._Y.extend", "list"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend"], ["", "@", "property", "\n", "def", "Y", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_Y", "is", "None", ":", "\n", "            ", "self", ".", "_Y", "=", "[", "]", "\n", "for", "data", "in", "self", ".", "datasets", ":", "\n", "                ", "self", ".", "_Y", ".", "extend", "(", "list", "(", "data", ".", "Y", ")", ")", "\n", "", "self", ".", "_Y", "=", "np", ".", "array", "(", "self", ".", "_Y", ")", "\n", "", "return", "self", ".", "_Y", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.ConcatDataset.cummulative_sizes": [[113, 118], ["warnings.warn"], "methods", ["None"], ["", "@", "property", "\n", "def", "cummulative_sizes", "(", "self", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "\"cummulative_sizes attribute is renamed to \"", "\n", "\"cumulative_sizes\"", ",", "DeprecationWarning", ",", "stacklevel", "=", "2", ")", "\n", "return", "self", ".", "cumulative_sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.SplitDataset.__init__": [[129, 137], ["list", "isinstance", "hasattr", "operator.itemgetter", "list"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "ds", ",", "split_inds", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "split_inds", "=", "list", "(", "split_inds", ")", "\n", "self", ".", "wrapped_data", "=", "ds", "\n", "self", ".", "is_lazy", "=", "isinstance", "(", "ds", ",", "lazy_array_loader", ")", "or", "(", "hasattr", "(", "ds", ",", "'is_lazy'", ")", "and", "ds", ".", "is_lazy", ")", "\n", "if", "self", ".", "is_lazy", ":", "\n", "            ", "self", ".", "lens", "=", "itemgetter", "(", "*", "self", ".", "split_inds", ")", "(", "list", "(", "self", ".", "wrapped_data", ".", "lens", ")", ")", "\n", "", "self", ".", "_X", "=", "None", "\n", "self", ".", "_Y", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.SplitDataset.__len__": [[138, 140], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "split_inds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.SplitDataset.__getitem__": [[141, 143], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "wrapped_data", "[", "self", ".", "split_inds", "[", "index", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.SplitDataset.SetTokenizer": [[144, 146], ["datasets.SplitDataset.wrapped_data.SetTokenizer"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.SetTokenizer"], ["", "def", "SetTokenizer", "(", "self", ",", "tokenizer", ")", ":", "\n", "        ", "self", ".", "wrapped_data", ".", "SetTokenizer", "(", "tokenizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.SplitDataset.GetTokenizer": [[147, 149], ["datasets.SplitDataset.wrapped_data.GetTokenizer"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.GetTokenizer"], ["", "def", "GetTokenizer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "wrapped_data", ".", "GetTokenizer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.SplitDataset.X": [[150, 155], ["operator.itemgetter"], "methods", ["None"], ["", "@", "property", "\n", "def", "X", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_X", "is", "None", ":", "\n", "            ", "self", ".", "_X", "=", "itemgetter", "(", "*", "self", ".", "split_inds", ")", "(", "self", ".", "wrapped_data", ".", "X", ")", "\n", "", "return", "self", ".", "_X", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.SplitDataset.Y": [[156, 161], ["numpy.array", "operator.itemgetter"], "methods", ["None"], ["", "@", "property", "\n", "def", "Y", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_Y", "is", "None", ":", "\n", "            ", "self", ".", "_Y", "=", "np", ".", "array", "(", "itemgetter", "(", "*", "self", ".", "split_inds", ")", "(", "self", ".", "wrapped_data", ".", "Y", ")", ")", "\n", "", "return", "self", ".", "_Y", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.SplitDataset.__iter__": [[162, 165], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "idx", "in", "self", ".", "split_inds", ":", "\n", "            ", "yield", "self", ".", "wrapped_data", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.csv_dataset.__init__": [[217, 255], ["datasets.csv_dataset.SetTokenizer", "pandas.read_csv.dropna", "data[].values.tolist", "isinstance", "pandas.read_csv", "binarize_labels", "pandas.read_csv", "numpy.ones", "len"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.SetTokenizer"], ["def", "__init__", "(", "self", ",", "path", ",", "tokenizer", "=", "None", ",", "preprocess_fn", "=", "None", ",", "delim", "=", "','", ",", "\n", "binarize_sent", "=", "False", ",", "drop_unlabeled", "=", "False", ",", "text_key", "=", "'sentence'", ",", "label_key", "=", "'label'", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "is_lazy", "=", "False", "\n", "self", ".", "preprocess_fn", "=", "preprocess_fn", "\n", "self", ".", "SetTokenizer", "(", "tokenizer", ")", "\n", "self", ".", "path", "=", "path", "\n", "self", ".", "delim", "=", "delim", "\n", "self", ".", "text_key", "=", "text_key", "\n", "self", ".", "label_key", "=", "label_key", "\n", "self", ".", "drop_unlabeled", "=", "drop_unlabeled", "\n", "\n", "if", "'.tsv'", "in", "self", ".", "path", ":", "\n", "            ", "self", ".", "delim", "=", "'\\t'", "\n", "\n", "\n", "", "self", ".", "X", "=", "[", "]", "\n", "self", ".", "Y", "=", "[", "]", "\n", "try", ":", "\n", "            ", "cols", "=", "[", "text_key", "]", "\n", "if", "isinstance", "(", "label_key", ",", "list", ")", ":", "\n", "                ", "cols", "+=", "label_key", "\n", "", "else", ":", "\n", "                ", "cols", "+=", "[", "label_key", "]", "\n", "", "data", "=", "pd", ".", "read_csv", "(", "self", ".", "path", ",", "sep", "=", "self", ".", "delim", ",", "usecols", "=", "cols", ",", "encoding", "=", "'latin-1'", ")", "\n", "", "except", ":", "\n", "            ", "data", "=", "pd", ".", "read_csv", "(", "self", ".", "path", ",", "sep", "=", "self", ".", "delim", ",", "usecols", "=", "[", "text_key", "]", ",", "encoding", "=", "'latin-1'", ")", "\n", "\n", "", "data", "=", "data", ".", "dropna", "(", "axis", "=", "0", ")", "\n", "\n", "self", ".", "X", "=", "data", "[", "text_key", "]", ".", "values", ".", "tolist", "(", ")", "\n", "try", ":", "\n", "            ", "self", ".", "Y", "=", "data", "[", "label_key", "]", ".", "values", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "Y", "=", "np", ".", "ones", "(", "len", "(", "self", ".", "X", ")", ")", "*", "-", "1", "\n", "\n", "", "if", "binarize_sent", ":", "\n", "            ", "self", ".", "Y", "=", "binarize_labels", "(", "self", ".", "Y", ",", "hard", "=", "binarize_sent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.csv_dataset.SetTokenizer": [[256, 264], ["hasattr"], "methods", ["None"], ["", "", "def", "SetTokenizer", "(", "self", ",", "tokenizer", ")", ":", "\n", "        ", "if", "tokenizer", "is", "None", ":", "\n", "            ", "self", ".", "using_tokenizer", "=", "False", "\n", "if", "not", "hasattr", "(", "self", ",", "'_tokenizer'", ")", ":", "\n", "                ", "self", ".", "_tokenizer", "=", "tokenizer", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "using_tokenizer", "=", "True", "\n", "self", ".", "_tokenizer", "=", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.csv_dataset.GetTokenizer": [[265, 267], ["None"], "methods", ["None"], ["", "", "def", "GetTokenizer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.csv_dataset.tokenizer": [[268, 273], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "tokenizer", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "using_tokenizer", ":", "\n", "            ", "return", "self", ".", "_tokenizer", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.csv_dataset.__len__": [[274, 276], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "X", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.csv_dataset.__getitem__": [[277, 291], ["isinstance", "datasets.csv_dataset.tokenizer.EncodeAsIds", "len", "datasets.csv_dataset.preprocess_fn", "datasets.csv_dataset.tokenizer.EncodeAsIds", "datasets.csv_dataset.preprocess_fn"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsIds", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsIds"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"process+tokenize string and return string,label,and stringlen\"\"\"", "\n", "x", "=", "self", ".", "X", "[", "index", "]", "\n", "if", "self", ".", "tokenizer", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "tokenizer", ".", "EncodeAsIds", "(", "x", ",", "self", ".", "preprocess_fn", ")", "\n", "", "elif", "self", ".", "preprocess_fn", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "preprocess_fn", "(", "x", ")", "\n", "", "y", "=", "self", ".", "Y", "[", "index", "]", "\n", "if", "isinstance", "(", "y", ",", "str", ")", ":", "\n", "            ", "if", "self", ".", "tokenizer", "is", "not", "None", ":", "\n", "                ", "y", "=", "self", ".", "tokenizer", ".", "EncodeAsIds", "(", "y", ",", "self", ".", "preprocess_fn", ")", "\n", "", "elif", "self", ".", "preprocess_fn", "is", "not", "None", ":", "\n", "                ", "y", "=", "self", ".", "preprocess_fn", "(", "y", ")", "\n", "", "", "return", "{", "'text'", ":", "x", ",", "'length'", ":", "len", "(", "x", ")", ",", "'label'", ":", "y", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.csv_dataset.write": [[292, 314], ["print", "open", "csv.writer", "enumerate", "csv.writer.writerow", "zip", "csv.writer.writerow", "csv.writer.writerow", "csv.writer.writerow", "tuple", "tuple", "next"], "methods", ["None"], ["", "def", "write", "(", "self", ",", "writer_gen", "=", "None", ",", "path", "=", "None", ",", "skip_header", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        given a generator of metrics for each of the data points X_i,\n            write the metrics, text, and labels to a csv file\n        \"\"\"", "\n", "if", "path", "is", "None", ":", "\n", "            ", "path", "=", "self", ".", "path", "+", "'.results'", "\n", "", "print", "(", "'generating csv at '", "+", "path", ")", "\n", "with", "open", "(", "path", ",", "'w'", ")", "as", "csvfile", ":", "\n", "            ", "c", "=", "csv", ".", "writer", "(", "csvfile", ",", "delimiter", "=", "self", ".", "delim", ")", "\n", "if", "writer_gen", "is", "not", "None", ":", "\n", "#if first item of generator is a header of what the metrics mean then write header to csv file", "\n", "                ", "if", "not", "skip_header", ":", "\n", "                    ", "header", "=", "(", "self", ".", "label_key", ",", ")", "+", "tuple", "(", "next", "(", "writer_gen", ")", ")", "+", "(", "self", ".", "text_key", ",", ")", "\n", "c", ".", "writerow", "(", "header", ")", "\n", "", "for", "i", ",", "row", "in", "enumerate", "(", "writer_gen", ")", ":", "\n", "                    ", "row", "=", "(", "self", ".", "Y", "[", "i", "]", ",", ")", "+", "tuple", "(", "row", ")", "+", "(", "self", ".", "X", "[", "i", "]", ",", ")", "\n", "c", ".", "writerow", "(", "row", ")", "\n", "", "", "else", ":", "\n", "                ", "c", ".", "writerow", "(", "[", "self", ".", "label_key", ",", "self", ".", "text_key", "]", ")", "\n", "for", "row", "in", "zip", "(", "self", ".", "Y", ",", "self", ".", "X", ")", ":", "\n", "                    ", "c", ".", "writerow", "(", "row", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.__init__": [[330, 349], ["datasets.json_dataset.SetTokenizer", "datasets.json_dataset.load_json_stream", "datasets.json_dataset.X.append", "datasets.json_dataset.Y.append", "binarize_labels"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.SetTokenizer", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.load_json_stream", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["def", "__init__", "(", "self", ",", "path", ",", "tokenizer", "=", "None", ",", "preprocess_fn", "=", "None", ",", "binarize_sent", "=", "False", ",", "\n", "text_key", "=", "'sentence'", ",", "label_key", "=", "'label'", ",", "loose_json", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "is_lazy", "=", "False", "\n", "self", ".", "preprocess_fn", "=", "preprocess_fn", "\n", "self", ".", "path", "=", "path", "\n", "self", ".", "SetTokenizer", "(", "tokenizer", ")", "\n", "self", ".", "X", "=", "[", "]", "\n", "self", ".", "Y", "=", "[", "]", "\n", "self", ".", "text_key", "=", "text_key", "\n", "self", ".", "label_key", "=", "label_key", "\n", "self", ".", "loose_json", "=", "loose_json", "\n", "\n", "for", "j", "in", "self", ".", "load_json_stream", "(", "self", ".", "path", ")", ":", "\n", "            ", "s", "=", "j", "[", "text_key", "]", "\n", "self", ".", "X", ".", "append", "(", "s", ")", "\n", "self", ".", "Y", ".", "append", "(", "j", "[", "label_key", "]", ")", "\n", "\n", "", "if", "binarize_sent", ":", "\n", "            ", "self", ".", "Y", "=", "binarize_labels", "(", "self", ".", "Y", ",", "hard", "=", "binarize_sent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.SetTokenizer": [[350, 358], ["hasattr"], "methods", ["None"], ["", "", "def", "SetTokenizer", "(", "self", ",", "tokenizer", ")", ":", "\n", "        ", "if", "tokenizer", "is", "None", ":", "\n", "            ", "self", ".", "using_tokenizer", "=", "False", "\n", "if", "not", "hasattr", "(", "self", ",", "'_tokenizer'", ")", ":", "\n", "                ", "self", ".", "_tokenizer", "=", "tokenizer", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "using_tokenizer", "=", "True", "\n", "self", ".", "_tokenizer", "=", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.GetTokenizer": [[359, 361], ["None"], "methods", ["None"], ["", "", "def", "GetTokenizer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.tokenizer": [[362, 367], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "tokenizer", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "using_tokenizer", ":", "\n", "            ", "return", "self", ".", "_tokenizer", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.__getitem__": [[368, 382], ["isinstance", "datasets.json_dataset.tokenizer.EncodeAsIds", "len", "datasets.json_dataset.preprocess_fn", "datasets.json_dataset.tokenizer.EncodeAsIds", "datasets.json_dataset.preprocess_fn"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsIds", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsIds"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"gets the index'th string from the dataset\"\"\"", "\n", "x", "=", "self", ".", "X", "[", "index", "]", "\n", "if", "self", ".", "tokenizer", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "tokenizer", ".", "EncodeAsIds", "(", "x", ",", "self", ".", "preprocess_fn", ")", "\n", "", "elif", "self", ".", "preprocess_fn", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "preprocess_fn", "(", "x", ")", "\n", "", "y", "=", "self", ".", "Y", "[", "index", "]", "\n", "if", "isinstance", "(", "y", ",", "str", ")", ":", "\n", "            ", "if", "self", ".", "tokenizer", "is", "not", "None", ":", "\n", "                ", "y", "=", "self", ".", "tokenizer", ".", "EncodeAsIds", "(", "y", ",", "self", ".", "preprocess_fn", ")", "\n", "", "elif", "self", ".", "preprocess_fn", "is", "not", "None", ":", "\n", "                ", "y", "=", "self", ".", "preprocess_fn", "(", "y", ")", "\n", "", "", "return", "{", "'text'", ":", "x", ",", "'length'", ":", "len", "(", "x", ")", ",", "'label'", ":", "y", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.__len__": [[383, 385], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "X", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write": [[386, 426], ["datasets.json_dataset.save_json_stream", "enumerate", "datasets.json_dataset.write.out_stream"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.save_json_stream"], ["", "def", "write", "(", "self", ",", "writer_gen", "=", "None", ",", "path", "=", "None", ",", "skip_header", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        given a generator of metrics for each of the data points X_i,\n            write the metrics, text, and labels to a json file\n        \"\"\"", "\n", "if", "path", "is", "None", ":", "\n", "            ", "path", "=", "self", ".", "path", "+", "'.results'", "\n", "\n", "", "jsons", "=", "[", "]", "\n", "\n", "if", "writer_gen", "is", "not", "None", ":", "\n", "#if first item of generator is a header of what the metrics mean then write header to csv file", "\n", "            ", "def", "gen_helper", "(", ")", ":", "\n", "                ", "keys", "=", "{", "}", "\n", "keys", "[", "0", "]", "=", "self", ".", "label_key", "\n", "if", "not", "skip_header", ":", "\n", "                    ", "for", "idx", ",", "k", "in", "enumerate", "(", "tuple", "(", "next", "(", "writer_gen", ")", ")", ")", ":", "\n", "                        ", "keys", "[", "idx", "+", "1", "]", "=", "k", "\n", "", "", "for", "i", ",", "row", "in", "enumerate", "(", "writer_gen", ")", ":", "\n", "                    ", "if", "i", "==", "0", "and", "skip_header", ":", "\n", "                        ", "for", "idx", ",", "_", "in", "enumerate", "(", "row", ")", ":", "\n", "                            ", "keys", "[", "idx", "+", "1", "]", "=", "'metric_%d'", "%", "(", "idx", ",", ")", "\n", "", "", "j", "=", "{", "}", "\n", "for", "idx", ",", "v", "in", "enumerate", "(", "(", "self", ".", "Y", "[", "i", "]", ",", ")", "+", "tuple", "(", "row", ")", ")", ":", "\n", "                        ", "k", "=", "keys", "[", "idx", "]", "\n", "j", "[", "k", "]", "=", "v", "\n", "", "yield", "j", "\n", "", "", "", "else", ":", "\n", "            ", "def", "gen_helper", "(", ")", ":", "\n", "                ", "for", "y", "in", "self", ".", "Y", ":", "\n", "                    ", "j", "=", "{", "}", "\n", "j", "[", "self", ".", "label_key", "]", "=", "y", "\n", "yield", "j", "\n", "\n", "", "", "", "def", "out_stream", "(", ")", ":", "\n", "            ", "for", "i", ",", "j", "in", "enumerate", "(", "gen_helper", "(", ")", ")", ":", "\n", "                ", "j", "[", "self", ".", "text_key", "]", "=", "self", ".", "X", "[", "i", "]", "\n", "yield", "j", "\n", "\n", "", "", "self", ".", "save_json_stream", "(", "path", ",", "out_stream", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.save_json_stream": [[427, 439], ["json.dump", "open", "enumerate", "open", "json.dumps", "f.write"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write"], ["", "def", "save_json_stream", "(", "self", ",", "save_path", ",", "json_stream", ")", ":", "\n", "        ", "if", "self", ".", "loose_json", ":", "\n", "            ", "with", "open", "(", "save_path", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "for", "i", ",", "j", "in", "enumerate", "(", "json_stream", ")", ":", "\n", "                    ", "write_string", "=", "''", "\n", "if", "i", "!=", "0", ":", "\n", "                        ", "write_string", "=", "'\\n'", "\n", "", "write_string", "+=", "json", ".", "dumps", "(", "j", ")", "\n", "f", ".", "write", "(", "write_string", ")", "\n", "", "", "", "else", ":", "\n", "            ", "jsons", "=", "[", "j", "for", "j", "in", "json_stream", "]", "\n", "json", ".", "dump", "(", "jsons", ",", "open", "(", "save_path", ",", "'w'", ")", ",", "separators", "=", "(", "','", ",", "':'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.load_json_stream": [[440, 455], ["json.load", "iter", "datasets.json_dataset.write.gen_helper"], "methods", ["None"], ["", "", "def", "load_json_stream", "(", "self", ",", "load_path", ")", ":", "\n", "        ", "if", "not", "self", ".", "loose_json", ":", "\n", "            ", "jsons", "=", "json", ".", "load", "(", "open", "(", "load_path", ",", "'r'", ")", ")", "\n", "generator", "=", "iter", "(", "jsons", ")", "\n", "", "else", ":", "\n", "            ", "def", "gen_helper", "(", ")", ":", "\n", "                ", "with", "open", "(", "load_path", ",", "'r'", ")", "as", "f", ":", "\n", "                    ", "for", "row", "in", "f", ":", "\n", "                        ", "yield", "json", ".", "loads", "(", "row", ")", "\n", "", "", "", "generator", "=", "gen_helper", "(", ")", "\n", "\n", "", "for", "j", "in", "generator", ":", "\n", "            ", "if", "self", ".", "label_key", "not", "in", "j", ":", "\n", "                ", "j", "[", "self", ".", "label_key", "]", "=", "-", "1", "\n", "", "yield", "j", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.GPT2Dataset.__init__": [[458, 478], ["len", "datasets.GPT2Dataset.ds.GetTokenizer", "datasets.GPT2Dataset.ds.SetTokenizer", "datasets.GPT2Dataset.init_weighting"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.GetTokenizer", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.SetTokenizer", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.GPT2Dataset.init_weighting"], ["    ", "def", "__init__", "(", "self", ",", "ds", ",", "\n", "max_seq_len", "=", "1024", ",", "\n", "num_samples", "=", "None", ",", "\n", "weighted", "=", "True", ",", "\n", "sample_across_doc", "=", "True", ",", "\n", "random_across_doc_sampling", "=", "True", ",", "\n", "sentence_start", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "ds", "=", "ds", "\n", "self", ".", "ds_len", "=", "len", "(", "self", ".", "ds", ")", "\n", "self", ".", "num_samples", "=", "num_samples", "\n", "if", "num_samples", "is", "None", ":", "\n", "            ", "self", ".", "num_samples", "=", "1000", "*", "self", ".", "ds_len", "\n", "", "self", ".", "max_seq_len", "=", "max_seq_len", "\n", "self", ".", "tokenizer", "=", "self", ".", "ds", ".", "GetTokenizer", "(", ")", "\n", "self", ".", "ds", ".", "SetTokenizer", "(", "None", ")", "\n", "self", ".", "weighted", "=", "weighted", "\n", "self", ".", "sample_across_doc", "=", "sample_across_doc", "\n", "self", ".", "random_across_doc_sampling", "=", "random_across_doc_sampling", "\n", "self", ".", "sentence_start", "=", "sentence_start", "\n", "self", ".", "init_weighting", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.GPT2Dataset.init_weighting": [[479, 490], ["numpy.sum", "list", "hasattr", "numpy.array", "numpy.array", "itertools.accumulate", "isinstance", "len", "len"], "methods", ["None"], ["", "def", "init_weighting", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "weighted", ":", "\n", "            ", "if", "hasattr", "(", "self", ".", "ds", ",", "'is_lazy'", ")", "and", "self", ".", "ds", ".", "is_lazy", ":", "\n", "                ", "lens", "=", "np", ".", "array", "(", "self", ".", "ds", ".", "lens", ")", "\n", "", "else", ":", "\n", "                ", "lens", "=", "np", ".", "array", "(", "[", "len", "(", "d", "[", "'text'", "]", ")", "if", "isinstance", "(", "d", ",", "dict", ")", "\n", "else", "len", "(", "d", ")", "for", "d", "in", "self", ".", "ds", "]", ")", "\n", "", "self", ".", "total_len", "=", "np", ".", "sum", "(", "lens", ")", "\n", "self", ".", "weighting", "=", "list", "(", "accumulate", "(", "lens", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "weighting", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.GPT2Dataset.get_weighted_samples": [[491, 497], ["np_rng.randint", "bisect.bisect_right", "np_rng.randint"], "methods", ["None"], ["", "", "def", "get_weighted_samples", "(", "self", ",", "np_rng", ")", ":", "\n", "        ", "if", "self", ".", "weighting", "is", "not", "None", ":", "\n", "            ", "idx", "=", "np_rng", ".", "randint", "(", "self", ".", "total_len", ")", "\n", "return", "bisect_right", "(", "self", ".", "weighting", ",", "idx", ")", "\n", "", "else", ":", "\n", "            ", "return", "np_rng", ".", "randint", "(", "self", ".", "ds_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.GPT2Dataset.__len__": [[498, 500], ["None"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.GPT2Dataset.__getitem__": [[501, 540], ["random.Random", "numpy.random.RandomState", "datasets.GPT2Dataset.get_weighted_samples", "datasets.GPT2Dataset.getidx", "len", "datasets.GPT2Dataset.pad_seq", "numpy.random.RandomState.randint", "numpy.array", "list", "len", "datasets.GPT2Dataset.getidx", "numpy.random.RandomState.randint", "list.pop", "datasets.GPT2Dataset.contains_sentence_end", "len", "datasets.GPT2Dataset.get_weighted_samples", "range", "len"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.get_weighted_samples", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.GPT2Dataset.getidx", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.pad_seq", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.GPT2Dataset.getidx", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.GPT2Dataset.contains_sentence_end", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.get_weighted_samples"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "# init rng", "\n", "        ", "rng", "=", "random", ".", "Random", "(", "idx", ")", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "seed", "=", "[", "rng", ".", "randint", "(", "0", ",", "2", "**", "32", "-", "1", ")", "for", "_", "in", "range", "(", "16", ")", "]", ")", "\n", "\n", "# get possibly weighted random index from dataset", "\n", "data_idx", "=", "self", ".", "get_weighted_samples", "(", "rng", ")", "\n", "#        data_idx = rng.choice(self.ds_len, p=self.weighting)", "\n", "tokens", "=", "self", ".", "getidx", "(", "data_idx", ")", "\n", "\n", "# truncate or pad tokens", "\n", "num_tokens", "=", "len", "(", "tokens", ")", "\n", "tokens_to_strip", "=", "num_tokens", "-", "self", ".", "max_seq_len", "-", "1", "\n", "if", "tokens_to_strip", ">", "0", ":", "\n", "            ", "strip_left_tokens", "=", "rng", ".", "randint", "(", "tokens_to_strip", "+", "1", ")", "\n", "tokens", "=", "tokens", "[", "strip_left_tokens", ":", "]", "\n", "if", "self", ".", "sentence_start", ":", "\n", "                ", "token_copy", "=", "list", "(", "tokens", ")", "\n", "not_done", "=", "True", "\n", "while", "(", "len", "(", "token_copy", ")", ">", "0", ")", "and", "not_done", ":", "\n", "                    ", "tok", "=", "token_copy", ".", "pop", "(", "0", ")", "\n", "if", "self", ".", "contains_sentence_end", "(", "tok", ")", ":", "\n", "                        ", "tokens", "=", "token_copy", "\n", "not_done", "=", "False", "\n", "", "", "", "strip_right_rokens", "=", "len", "(", "tokens", ")", "-", "self", ".", "max_seq_len", "-", "1", "\n", "if", "strip_right_rokens", ">", "0", ":", "\n", "                ", "tokens", "=", "tokens", "[", ":", "-", "strip_right_rokens", "]", "\n", "\n", "", "", "if", "self", ".", "sample_across_doc", ":", "\n", "            ", "while", "(", "len", "(", "tokens", ")", "<", "(", "self", ".", "max_seq_len", "+", "1", ")", ")", ":", "\n", "                ", "if", "self", ".", "random_across_doc_sampling", ":", "\n", "                    ", "data_idx", "=", "self", ".", "get_weighted_samples", "(", "rng", ")", "\n", "", "else", ":", "\n", "                    ", "data_idx", "=", "(", "data_idx", "+", "1", ")", "%", "self", ".", "ds_len", "\n", "", "tokens", "+=", "self", ".", "getidx", "(", "data_idx", ")", "\n", "", "tokens", "=", "tokens", "[", ":", "(", "self", ".", "max_seq_len", "+", "1", ")", "]", "\n", "\n", "", "tokens", "=", "self", ".", "pad_seq", "(", "tokens", ")", "\n", "return", "{", "'text'", ":", "np", ".", "array", "(", "tokens", ")", ",", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.GPT2Dataset.getidx": [[541, 550], ["isinstance", "datasets.GPT2Dataset.tokenizer.EncodeAsIds", "datasets.GPT2Dataset.append", "datasets.GPT2Dataset.tokenizer.get_command"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsIds", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.get_command"], ["", "def", "getidx", "(", "self", ",", "data_idx", ")", ":", "\n", "        ", "data", "=", "self", ".", "ds", "[", "data_idx", "]", "\n", "if", "isinstance", "(", "data", ",", "dict", ")", ":", "\n", "            ", "data", "=", "data", "[", "'text'", "]", "\n", "# tokenize", "\n", "", "tokenization", "=", "self", ".", "tokenizer", ".", "EncodeAsIds", "(", "data", ")", "\n", "tokenization", ".", "append", "(", "self", ".", "tokenizer", ".", "get_command", "(", "'eos'", ")", ")", "\n", "tokens", "=", "tokenization", ".", "tokenization", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.GPT2Dataset.pad_seq": [[551, 556], ["max", "len", "datasets.GPT2Dataset.tokenizer.get_command"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.get_command"], ["", "def", "pad_seq", "(", "self", ",", "seq", ")", ":", "\n", "        ", "total_tokens", "=", "self", ".", "max_seq_len", "+", "1", "\n", "num_pad_tokens", "=", "max", "(", "0", ",", "total_tokens", "-", "len", "(", "seq", ")", ")", "\n", "seq", "+=", "[", "self", ".", "tokenizer", ".", "get_command", "(", "'pad'", ")", ".", "Id", "]", "*", "(", "num_pad_tokens", ")", "\n", "return", "seq", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.GPT2Dataset.contains_sentence_end": [[557, 566], ["datasets.GPT2Dataset.tokenizer.IdToToken"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.IdToToken"], ["", "def", "contains_sentence_end", "(", "self", ",", "tok", ")", ":", "\n", "        ", "tok", "=", "self", ".", "tokenizer", ".", "IdToToken", "(", "tok", ")", "\n", "if", "'.'", "in", "tok", ":", "\n", "            ", "return", "True", "\n", "", "if", "'?'", "in", "tok", ":", "\n", "            ", "return", "True", "\n", "", "if", "'!'", "in", "tok", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.__init__": [[579, 599], ["len", "datasets.bert_sentencepair_dataset.ds.GetTokenizer", "list", "datasets.bert_sentencepair_dataset.ds.SetTokenizer", "datasets.bert_sentencepair_dataset.get_weighting", "datasets.bert_sentencepair_dataset.tokenizer.text_token_vocab.values", "nltk.download", "math.ceil"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.GetTokenizer", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.SetTokenizer", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.get_weighting"], ["def", "__init__", "(", "self", ",", "ds", ",", "max_seq_len", "=", "512", ",", "mask_lm_prob", "=", ".15", ",", "max_preds_per_seq", "=", "None", ",", "short_seq_prob", "=", ".01", ",", "dataset_size", "=", "None", ",", "presplit_sentences", "=", "False", ",", "weighted", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "ds", "=", "ds", "\n", "self", ".", "ds_len", "=", "len", "(", "self", ".", "ds", ")", "\n", "self", ".", "tokenizer", "=", "self", ".", "ds", ".", "GetTokenizer", "(", ")", "\n", "self", ".", "vocab_words", "=", "list", "(", "self", ".", "tokenizer", ".", "text_token_vocab", ".", "values", "(", ")", ")", "\n", "self", ".", "ds", ".", "SetTokenizer", "(", "None", ")", "\n", "self", ".", "max_seq_len", "=", "max_seq_len", "\n", "self", ".", "mask_lm_prob", "=", "mask_lm_prob", "\n", "if", "max_preds_per_seq", "is", "None", ":", "\n", "            ", "max_preds_per_seq", "=", "math", ".", "ceil", "(", "max_seq_len", "*", "mask_lm_prob", "/", "10", ")", "*", "10", "\n", "", "self", ".", "max_preds_per_seq", "=", "max_preds_per_seq", "\n", "self", ".", "short_seq_prob", "=", "short_seq_prob", "\n", "self", ".", "dataset_size", "=", "dataset_size", "\n", "if", "self", ".", "dataset_size", "is", "None", ":", "\n", "            ", "self", ".", "dataset_size", "=", "self", ".", "ds_len", "*", "(", "self", ".", "ds_len", "-", "1", ")", "\n", "", "self", ".", "presplit_sentences", "=", "presplit_sentences", "\n", "if", "not", "self", ".", "presplit_sentences", ":", "\n", "            ", "nltk", ".", "download", "(", "'punkt'", ",", "download_dir", "=", "\"./nltk\"", ")", "\n", "", "self", ".", "weighted", "=", "weighted", "\n", "self", ".", "get_weighting", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.get_weighting": [[600, 610], ["numpy.sum", "list", "hasattr", "numpy.array", "numpy.array", "itertools.accumulate", "isinstance", "len", "len"], "methods", ["None"], ["", "def", "get_weighting", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "weighted", ":", "\n", "            ", "if", "hasattr", "(", "self", ".", "ds", ",", "'is_lazy'", ")", "and", "self", ".", "ds", ".", "is_lazy", ":", "\n", "                ", "lens", "=", "np", ".", "array", "(", "self", ".", "ds", ".", "lens", ")", "\n", "", "else", ":", "\n", "                ", "lens", "=", "np", ".", "array", "(", "[", "len", "(", "d", "[", "'text'", "]", ")", "if", "isinstance", "(", "d", ",", "dict", ")", "else", "len", "(", "d", ")", "for", "d", "in", "self", ".", "ds", "]", ")", "\n", "", "self", ".", "total_len", "=", "np", ".", "sum", "(", "lens", ")", "\n", "self", ".", "weighting", "=", "list", "(", "accumulate", "(", "lens", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "weighting", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.get_weighted_samples": [[611, 617], ["np_rng.randint", "bisect.bisect_right", "np_rng.randint"], "methods", ["None"], ["", "", "def", "get_weighted_samples", "(", "self", ",", "np_rng", ")", ":", "\n", "        ", "if", "self", ".", "weighting", "is", "not", "None", ":", "\n", "            ", "idx", "=", "np_rng", ".", "randint", "(", "self", ".", "total_len", ")", "\n", "return", "bisect_right", "(", "self", ".", "weighting", ",", "idx", ")", "\n", "", "else", ":", "\n", "            ", "return", "np_rng", ".", "randint", "(", "self", ".", "ds_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.__len__": [[618, 620], ["None"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dataset_size", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.__getitem__": [[621, 647], ["random.Random", "numpy.random.RandomState", "datasets.bert_sentencepair_dataset.truncate_seq_pair", "datasets.bert_sentencepair_dataset.create_masked_lm_predictions", "random.Random.random", "random.Random.randint", "datasets.bert_sentencepair_dataset.create_random_sentencepair", "len", "len", "numpy.array", "numpy.array", "int", "numpy.array", "numpy.array", "numpy.array", "random.Random.randint", "range"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.truncate_seq_pair", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.create_masked_lm_predictions", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.create_random_sentencepair"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "# get rng state corresponding to index (allows deterministic random pair)", "\n", "        ", "rng", "=", "random", ".", "Random", "(", "idx", ")", "\n", "np_rng", "=", "np", ".", "random", ".", "RandomState", "(", "seed", "=", "[", "rng", ".", "randint", "(", "0", ",", "2", "**", "32", "-", "1", ")", "for", "_", "in", "range", "(", "16", ")", "]", ")", "\n", "# get seq length", "\n", "target_seq_length", "=", "self", ".", "max_seq_len", "\n", "short_seq", "=", "False", "\n", "if", "rng", ".", "random", "(", ")", "<", "self", ".", "short_seq_prob", ":", "\n", "            ", "target_seq_length", "=", "rng", ".", "randint", "(", "2", ",", "target_seq_length", ")", "\n", "short_seq", "=", "True", "\n", "\n", "# get sentence pair and label", "\n", "", "is_random_next", "=", "None", "\n", "lena", "=", "0", "\n", "lenb", "=", "0", "\n", "while", "(", "is_random_next", "is", "None", ")", "or", "(", "lena", "<", "1", ")", "or", "(", "lenb", "<", "1", ")", ":", "\n", "            ", "tokensa", ",", "tokensb", ",", "is_random_next", "=", "self", ".", "create_random_sentencepair", "(", "target_seq_length", ",", "rng", ",", "np_rng", ")", "\n", "lena", "=", "len", "(", "tokensa", "[", "0", "]", ")", "\n", "lenb", "=", "len", "(", "tokensb", "[", "0", "]", ")", "\n", "\n", "# truncate sentence pair to max_seq_len", "\n", "", "tokensa", ",", "tokensb", "=", "self", ".", "truncate_seq_pair", "(", "tokensa", ",", "tokensb", ",", "self", ".", "max_seq_len", ",", "rng", ")", "\n", "# join sentence pair, mask, and pad", "\n", "tokens", ",", "mask", ",", "mask_labels", ",", "pad_mask", "=", "self", ".", "create_masked_lm_predictions", "(", "tokensa", ",", "tokensb", ",", "self", ".", "mask_lm_prob", ",", "self", ".", "max_preds_per_seq", ",", "self", ".", "vocab_words", ",", "rng", ")", "\n", "sample", "=", "{", "'text'", ":", "np", ".", "array", "(", "tokens", "[", "0", "]", ")", ",", "'types'", ":", "np", ".", "array", "(", "tokens", "[", "1", "]", ")", ",", "'is_random'", ":", "int", "(", "is_random_next", ")", ",", "'mask'", ":", "np", ".", "array", "(", "mask", ")", ",", "'mask_labels'", ":", "np", ".", "array", "(", "mask_labels", ")", ",", "'pad_mask'", ":", "np", ".", "array", "(", "pad_mask", ")", "}", "\n", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.sentence_split": [[648, 658], ["document.split", "rtn.extend", "nltk.tokenize.sent_tokenize"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend"], ["", "def", "sentence_split", "(", "self", ",", "document", ")", ":", "\n", "        ", "\"\"\"split document into sentences\"\"\"", "\n", "lines", "=", "document", ".", "split", "(", "'\\n'", ")", "\n", "if", "self", ".", "presplit_sentences", ":", "\n", "            ", "return", "[", "line", "for", "line", "in", "lines", "if", "line", "]", "\n", "", "rtn", "=", "[", "]", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "if", "line", "!=", "''", ":", "\n", "                ", "rtn", ".", "extend", "(", "tokenize", ".", "sent_tokenize", "(", "line", ")", ")", "\n", "", "", "return", "rtn", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.sentence_tokenize": [[659, 665], ["datasets.bert_sentencepair_dataset.tokenizer.EncodeAsIds", "str", "len", "datasets.bert_sentencepair_dataset.tokenizer.get_type"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.GPT2BPETokenizer.EncodeAsIds", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.get_type"], ["", "def", "sentence_tokenize", "(", "self", ",", "sent", ",", "sentence_num", "=", "0", ",", "beginning", "=", "False", ",", "ending", "=", "False", ")", ":", "\n", "        ", "\"\"\"tokenize sentence and get token types\"\"\"", "\n", "tokens", "=", "self", ".", "tokenizer", ".", "EncodeAsIds", "(", "sent", ")", ".", "tokenization", "\n", "str_type", "=", "'str'", "+", "str", "(", "sentence_num", ")", "\n", "token_types", "=", "[", "self", ".", "tokenizer", ".", "get_type", "(", "str_type", ")", ".", "Id", "]", "*", "len", "(", "tokens", ")", "\n", "return", "tokens", ",", "token_types", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.get_doc": [[666, 672], ["isinstance"], "methods", ["None"], ["", "def", "get_doc", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"gets text of document corresponding to idx\"\"\"", "\n", "rtn", "=", "self", ".", "ds", "[", "idx", "]", "\n", "if", "isinstance", "(", "rtn", ",", "dict", ")", ":", "\n", "            ", "rtn", "=", "rtn", "[", "'text'", "]", "\n", "", "return", "rtn", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.create_random_sentencepair": [[673, 753], ["rng.randint", "range", "datasets.bert_sentencepair_dataset.sentence_split", "len", "datasets.bert_sentencepair_dataset.sentence_tokenize", "curr_strs.append", "curr_str_types.append", "len", "len", "rng.randint", "tokens_a.extend", "token_types_a.extend", "range", "datasets.bert_sentencepair_dataset.get_weighted_samples", "rng.randint", "datasets.bert_sentencepair_dataset.get_doc", "len", "len", "len", "rng.random", "len", "rng.randint", "len", "tokens_b.extend", "token_types_b.extend", "len", "rng.randint", "int", "datasets.bert_sentencepair_dataset.sentence_split", "len", "datasets.bert_sentencepair_dataset.sentence_tokenize", "len", "tokens_b.extend", "token_types_b.extend", "len", "datasets.bert_sentencepair_dataset.get_doc", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.sentence_split", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.sentence_tokenize", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.get_weighted_samples", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.get_doc", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.sentence_split", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.sentence_tokenize", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.get_doc"], ["", "def", "create_random_sentencepair", "(", "self", ",", "target_seq_length", ",", "rng", ",", "np_rng", ")", ":", "\n", "        ", "\"\"\"\n        fetches a random sentencepair corresponding to rng state similar to\n        https://github.com/google-research/bert/blob/master/create_pretraining_data.py#L248-L294\n        \"\"\"", "\n", "is_random_next", "=", "None", "\n", "\n", "curr_strs", "=", "[", "]", "\n", "curr_str_types", "=", "[", "]", "\n", "curr_len", "=", "0", "\n", "\n", "while", "curr_len", "<", "1", ":", "\n", "            ", "curr_len", "=", "0", "\n", "doc_a", "=", "None", "\n", "while", "doc_a", "is", "None", ":", "\n", "                ", "if", "self", ".", "weighted", ":", "\n", "# doc_a_idx = np_rng.choice(self.ds_len, p=self.weighting)", "\n", "                    ", "doc_a_idx", "=", "self", ".", "get_weighted_samples", "(", "np_rng", ")", "\n", "", "else", ":", "\n", "                    ", "doc_a_idx", "=", "rng", ".", "randint", "(", "0", ",", "self", ".", "ds_len", "-", "1", ")", "\n", "", "doc_a", "=", "self", ".", "sentence_split", "(", "self", ".", "get_doc", "(", "doc_a_idx", ")", ")", "\n", "if", "not", "doc_a", ":", "\n", "                    ", "doc_a", "=", "None", "\n", "\n", "", "", "random_start_a", "=", "rng", ".", "randint", "(", "0", ",", "len", "(", "doc_a", ")", "-", "1", ")", "\n", "while", "random_start_a", "<", "len", "(", "doc_a", ")", ":", "\n", "                ", "sentence", "=", "doc_a", "[", "random_start_a", "]", "\n", "sentence", ",", "sentence_types", "=", "self", ".", "sentence_tokenize", "(", "sentence", ",", "0", ",", "random_start_a", "==", "0", ",", "random_start_a", "==", "len", "(", "doc_a", ")", ")", "\n", "curr_strs", ".", "append", "(", "sentence", ")", "\n", "curr_str_types", ".", "append", "(", "sentence_types", ")", "\n", "curr_len", "+=", "len", "(", "sentence", ")", "\n", "if", "random_start_a", "==", "len", "(", "doc_a", ")", "-", "1", "or", "curr_len", ">=", "target_seq_length", ":", "\n", "                    ", "break", "\n", "", "random_start_a", "=", "(", "random_start_a", "+", "1", ")", "\n", "\n", "", "", "if", "curr_strs", ":", "\n", "            ", "num_a", "=", "1", "\n", "if", "len", "(", "curr_strs", ")", ">=", "2", ":", "\n", "                ", "num_a", "=", "rng", ".", "randint", "(", "0", ",", "len", "(", "curr_strs", ")", ")", "\n", "\n", "", "tokens_a", "=", "[", "]", "\n", "token_types_a", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "num_a", ")", ":", "\n", "                ", "tokens_a", ".", "extend", "(", "curr_strs", "[", "j", "]", ")", "\n", "token_types_a", ".", "extend", "(", "curr_str_types", "[", "j", "]", ")", "\n", "\n", "", "tokens_b", "=", "[", "]", "\n", "token_types_b", "=", "[", "]", "\n", "is_random_next", "=", "False", "\n", "if", "len", "(", "curr_strs", ")", "==", "1", "or", "rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                ", "is_random_next", "=", "True", "\n", "target_b_length", "=", "target_seq_length", "-", "len", "(", "tokens_a", ")", "\n", "b_len", "=", "0", "\n", "while", "b_len", "<", "1", ":", "\n", "                    ", "doc_b", "=", "None", "\n", "while", "doc_b", "is", "None", ":", "\n", "                        ", "doc_b_idx", "=", "rng", ".", "randint", "(", "0", ",", "self", ".", "ds_len", "-", "2", ")", "\n", "doc_b_idx", "+=", "int", "(", "doc_b_idx", ">=", "doc_a_idx", ")", "\n", "\n", "doc_b", "=", "self", ".", "sentence_split", "(", "self", ".", "get_doc", "(", "doc_b_idx", ")", ")", "\n", "if", "not", "doc_b", ":", "\n", "                            ", "doc_b", "=", "None", "\n", "\n", "", "", "random_start_b", "=", "rng", ".", "randint", "(", "0", ",", "len", "(", "doc_b", ")", "-", "1", ")", "\n", "while", "random_start_b", "<", "len", "(", "doc_b", ")", ":", "\n", "                        ", "sentence_b", "=", "doc_b", "[", "random_start_b", "]", "\n", "new_b_tokens", ",", "new_b_types", "=", "self", ".", "sentence_tokenize", "(", "sentence_b", ",", "1", ",", "random_start_b", "==", "0", ",", "random_start_b", "==", "len", "(", "doc_b", ")", ")", "\n", "b_len", "+=", "len", "(", "new_b_tokens", ")", "\n", "tokens_b", ".", "extend", "(", "new_b_tokens", ")", "\n", "token_types_b", ".", "extend", "(", "new_b_types", ")", "\n", "if", "len", "(", "tokens_b", ")", ">=", "target_b_length", ":", "\n", "                            ", "break", "\n", "", "random_start_b", "=", "(", "random_start_b", "+", "1", ")", "\n", "", "", "", "else", ":", "\n", "                ", "is_random_next", "=", "False", "\n", "for", "j", "in", "range", "(", "num_a", ",", "len", "(", "curr_strs", ")", ")", ":", "\n", "                    ", "tokens_b", ".", "extend", "(", "curr_strs", "[", "j", "]", ")", "\n", "token_types_b", ".", "extend", "(", "curr_str_types", "[", "j", "]", ")", "\n", "\n", "", "", "", "return", "(", "tokens_a", ",", "token_types_a", ")", ",", "(", "tokens_b", ",", "token_types_b", ")", ",", "is_random_next", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.truncate_seq_pair": [[754, 784], ["len", "len", "len", "len", "len", "rng.random", "trunc_tokens.pop", "trunc_types.pop", "trunc_tokens.pop", "trunc_types.pop"], "methods", ["None"], ["", "def", "truncate_seq_pair", "(", "self", ",", "a", ",", "b", ",", "max_seq_len", ",", "rng", ")", ":", "\n", "        ", "\"\"\"\n        Truncate sequence pair according to original BERT implementation:\n        https://github.com/google-research/bert/blob/master/create_pretraining_data.py#L391\n        \"\"\"", "\n", "tokens_a", ",", "token_types_a", "=", "a", "\n", "tokens_b", ",", "token_types_b", "=", "b", "\n", "max_num_tokens", "=", "max_seq_len", "-", "3", "\n", "while", "True", ":", "\n", "            ", "len_a", "=", "len", "(", "tokens_a", ")", "\n", "len_b", "=", "len", "(", "tokens_b", ")", "\n", "total_length", "=", "len_a", "+", "len_b", "\n", "if", "total_length", "<=", "max_num_tokens", ":", "\n", "                ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "                ", "trunc_tokens", "=", "tokens_a", "\n", "trunc_types", "=", "token_types_a", "\n", "", "else", ":", "\n", "                ", "trunc_tokens", "=", "tokens_b", "\n", "trunc_types", "=", "token_types_b", "\n", "\n", "", "assert", "len", "(", "trunc_tokens", ")", ">=", "1", "\n", "\n", "if", "rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                ", "trunc_tokens", ".", "pop", "(", "0", ")", "\n", "trunc_types", ".", "pop", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "trunc_tokens", ".", "pop", "(", ")", "\n", "trunc_types", ".", "pop", "(", ")", "\n", "", "", "return", "(", "tokens_a", ",", "token_types_a", ")", ",", "(", "tokens_b", ",", "token_types_b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.mask_token": [[785, 802], ["rng.random", "datasets.bert_sentencepair_dataset.tokenizer.get_command", "rng.random", "rng.choice"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.get_command"], ["", "def", "mask_token", "(", "self", ",", "idx", ",", "tokens", ",", "types", ",", "vocab_words", ",", "rng", ")", ":", "\n", "        ", "\"\"\"\n        helper function to mask `idx` token from `tokens` according to\n        section 3.3.1 of https://arxiv.org/pdf/1810.04805.pdf\n        \"\"\"", "\n", "label", "=", "tokens", "[", "idx", "]", "\n", "if", "rng", ".", "random", "(", ")", "<", "0.8", ":", "\n", "            ", "new_label", "=", "self", ".", "tokenizer", ".", "get_command", "(", "'MASK'", ")", ".", "Id", "\n", "", "else", ":", "\n", "            ", "if", "rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                ", "new_label", "=", "label", "\n", "", "else", ":", "\n", "                ", "new_label", "=", "rng", ".", "choice", "(", "vocab_words", ")", "\n", "\n", "", "", "tokens", "[", "idx", "]", "=", "new_label", "\n", "\n", "return", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.pad_seq": [[803, 809], ["max", "len", "len", "datasets.bert_sentencepair_dataset.tokenizer.get_command"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.get_command"], ["", "def", "pad_seq", "(", "self", ",", "seq", ")", ":", "\n", "        ", "\"\"\"helper function to pad sequence pair\"\"\"", "\n", "num_pad", "=", "max", "(", "0", ",", "self", ".", "max_seq_len", "-", "len", "(", "seq", ")", ")", "\n", "pad_mask", "=", "[", "0", "]", "*", "len", "(", "seq", ")", "+", "[", "1", "]", "*", "num_pad", "\n", "seq", "+=", "[", "self", ".", "tokenizer", ".", "get_command", "(", "'pad'", ")", ".", "Id", "]", "*", "num_pad", "\n", "return", "seq", ",", "pad_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.create_masked_lm_predictions": [[810, 841], ["len", "len", "rng.shuffle", "datasets.bert_sentencepair_dataset.pad_seq", "datasets.bert_sentencepair_dataset.pad_seq", "min", "sorted", "list", "list", "max", "len", "len", "datasets.bert_sentencepair_dataset.mask_token", "int", "datasets.bert_sentencepair_dataset.tokenizer.get_command", "range", "range", "round", "datasets.bert_sentencepair_dataset.tokenizer.get_command", "len", "datasets.bert_sentencepair_dataset.tokenizer.get_command"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.pad_seq", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.pad_seq", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.bert_sentencepair_dataset.mask_token", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.get_command", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.get_command", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenizer.get_command"], ["", "def", "create_masked_lm_predictions", "(", "self", ",", "a", ",", "b", ",", "mask_lm_prob", ",", "max_preds_per_seq", ",", "vocab_words", ",", "rng", ")", ":", "\n", "        ", "\"\"\"\n        Mask sequence pair for BERT training according to:\n        https://github.com/google-research/bert/blob/master/create_pretraining_data.py#L338\n        \"\"\"", "\n", "tokens_a", ",", "token_types_a", "=", "a", "\n", "tokens_b", ",", "token_types_b", "=", "b", "\n", "tokens", "=", "[", "self", ".", "tokenizer", ".", "get_command", "(", "'ENC'", ")", ".", "Id", "]", "+", "tokens_a", "+", "[", "self", ".", "tokenizer", ".", "get_command", "(", "'sep'", ")", ".", "Id", "]", "+", "tokens_b", "+", "[", "self", ".", "tokenizer", ".", "get_command", "(", "'sep'", ")", ".", "Id", "]", "\n", "token_types", "=", "[", "token_types_a", "[", "0", "]", "]", "+", "token_types_a", "+", "[", "token_types_a", "[", "0", "]", "]", "+", "token_types_b", "+", "[", "token_types_b", "[", "0", "]", "]", "\n", "\n", "len_a", "=", "len", "(", "tokens_a", ")", "\n", "len_b", "=", "len", "(", "tokens_b", ")", "\n", "\n", "cand_indices", "=", "[", "idx", "+", "1", "for", "idx", "in", "range", "(", "len_a", ")", "]", "+", "[", "idx", "+", "2", "+", "len_a", "for", "idx", "in", "range", "(", "len_b", ")", "]", "\n", "\n", "rng", ".", "shuffle", "(", "cand_indices", ")", "\n", "\n", "output_tokens", ",", "pad_mask", "=", "self", ".", "pad_seq", "(", "list", "(", "tokens", ")", ")", "\n", "output_types", ",", "_", "=", "self", ".", "pad_seq", "(", "list", "(", "token_types", ")", ")", "\n", "\n", "num_to_predict", "=", "min", "(", "max_preds_per_seq", ",", "max", "(", "1", ",", "int", "(", "round", "(", "len", "(", "tokens", ")", "*", "mask_lm_prob", ")", ")", ")", ")", "\n", "\n", "mask", "=", "[", "0", "]", "*", "len", "(", "output_tokens", ")", "\n", "mask_labels", "=", "[", "-", "1", "]", "*", "len", "(", "output_tokens", ")", "\n", "\n", "for", "idx", "in", "sorted", "(", "cand_indices", "[", ":", "num_to_predict", "]", ")", ":", "\n", "            ", "mask", "[", "idx", "]", "=", "1", "\n", "label", "=", "self", ".", "mask_token", "(", "idx", ",", "output_tokens", ",", "output_types", ",", "vocab_words", ",", "rng", ")", "\n", "mask_labels", "[", "idx", "]", "=", "label", "\n", "\n", "", "return", "(", "output_tokens", ",", "output_types", ")", ",", "mask", ",", "mask_labels", ",", "pad_mask", "\n", "", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.split_ds": [[166, 198], ["sum", "numpy.array", "len", "numpy.arange", "enumerate", "Exception", "numpy.random.shuffle", "len", "int", "datasets.SplitDataset", "int", "max"], "function", ["None"], ["", "", "", "def", "split_ds", "(", "ds", ",", "split", "=", "[", ".8", ",", ".2", ",", ".0", "]", ",", "shuffle", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Split a dataset into subsets given proportions of how\n    much to allocate per split. If a split is 0% returns None for that split.\n    Purpose: Useful for creating train/val/test splits\n    Arguments:\n        ds (Dataset or array-like): Data to be split.\n        split (1D array-like): proportions to split `ds`. `sum(splits) != 0`\n        shuffle (boolean): Randomly split dataset. Default: True\n    \"\"\"", "\n", "split_sum", "=", "sum", "(", "split", ")", "\n", "if", "split_sum", "==", "0", ":", "\n", "        ", "raise", "Exception", "(", "'Split cannot sum to 0.'", ")", "\n", "", "split", "=", "np", ".", "array", "(", "split", ")", "\n", "split", "/=", "split_sum", "\n", "ds_len", "=", "len", "(", "ds", ")", "\n", "inds", "=", "np", ".", "arange", "(", "ds_len", ")", "\n", "if", "shuffle", ":", "\n", "        ", "np", ".", "random", ".", "shuffle", "(", "inds", ")", "\n", "", "start_idx", "=", "0", "\n", "residual_idx", "=", "0", "\n", "rtn_ds", "=", "[", "None", "]", "*", "len", "(", "split", ")", "\n", "for", "i", ",", "f", "in", "enumerate", "(", "split", ")", ":", "\n", "        ", "if", "f", "!=", "0", ":", "\n", "            ", "proportion", "=", "ds_len", "*", "split", "[", "i", "]", "\n", "residual_idx", "+=", "proportion", "%", "1", "\n", "split_", "=", "int", "(", "int", "(", "proportion", ")", "+", "residual_idx", ")", "\n", "split_inds", "=", "inds", "[", "start_idx", ":", "start_idx", "+", "max", "(", "split_", ",", "1", ")", "]", "\n", "rtn_ds", "[", "i", "]", "=", "SplitDataset", "(", "ds", ",", "split_inds", ")", "\n", "start_idx", "+=", "split_", "\n", "residual_idx", "%=", "1", "\n", "", "", "return", "rtn_ds", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertConfig.__init__": [[148, 213], ["isinstance", "json.loads.items", "isinstance", "open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "deep_init", "=", "False", ",", "\n", "fp32_layernorm", "=", "False", ",", "\n", "fp32_embedding", "=", "False", ",", "\n", "fp32_tokentypes", "=", "False", ",", "\n", "layernorm_epsilon", "=", "1e-12", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "self", ".", "deep_init", "=", "deep_init", "\n", "self", ".", "fp32_layernorm", "=", "fp32_layernorm", "\n", "self", ".", "fp32_embedding", "=", "fp32_embedding", "\n", "self", ".", "layernorm_epsilon", "=", "layernorm_epsilon", "\n", "self", ".", "fp32_tokentypes", "=", "fp32_tokentypes", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained model config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertConfig.from_dict": [[215, 222], ["modeling.BertConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertConfig.from_json_file": [[223, 229], ["cls.from_dict", "open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertConfig.__repr__": [[230, 232], ["str", "modeling.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertConfig.to_dict": [[233, 237], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertConfig.to_json_string": [[238, 241], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertEmbeddings.__init__": [[264, 281], ["torch.nn.Module.__init__", "mpu.VocabParallelEmbedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "modeling.normal_init_method"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.normal_init_method"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "#self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)", "\n", "self", ".", "word_embeddings", "=", "mpu", ".", "VocabParallelEmbedding", "(", "\n", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "\n", "init_method", "=", "normal_init_method", "(", "mean", "=", "0.0", ",", "\n", "std", "=", "config", ".", "initializer_range", ")", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "fp32_layernorm", "=", "config", ".", "fp32_layernorm", "\n", "self", ".", "fp32_embedding", "=", "config", ".", "fp32_embedding", "\n", "self", ".", "fp32_tokentypes", "=", "config", ".", "fp32_tokentypes", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layernorm_epsilon", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertEmbeddings.forward": [[282, 321], ["input_ids.size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "modeling.BertEmbeddings.word_embeddings", "modeling.BertEmbeddings.position_embeddings", "modeling.BertEmbeddings.token_type_embeddings", "modeling.BertEmbeddings.dropout", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "embeddings.type.type.type", "modeling.BertEmbeddings.LayerNorm", "embeddings.type.type.type", "modeling.BertEmbeddings.LayerNorm", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze", "embeddings.type.type.half", "embeddings.type.type.float", "modeling.BertEmbeddings.float", "embeddings.type.type.half", "embeddings.type.type.float", "embeddings.type.type.half", "embeddings.type.type.type", "modeling.BertEmbeddings.float", "modeling.BertEmbeddings.float", "embeddings.type.type.half", "embeddings.type.type.type"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "if", "not", "self", ".", "fp32_tokentypes", ":", "\n", "\n", "            ", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "if", "self", ".", "fp32_embedding", "and", "not", "self", ".", "fp32_layernorm", ":", "\n", "                ", "embeddings", "=", "embeddings", ".", "half", "(", ")", "\n", "", "previous_type", "=", "embeddings", ".", "type", "(", ")", "\n", "if", "self", ".", "fp32_layernorm", ":", "\n", "                ", "embeddings", "=", "embeddings", ".", "float", "(", ")", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "if", "self", ".", "fp32_layernorm", ":", "\n", "                ", "if", "self", ".", "fp32_embedding", ":", "\n", "                    ", "embeddings", "=", "embeddings", ".", "half", "(", ")", "\n", "", "else", ":", "\n", "                    ", "embeddings", "=", "embeddings", ".", "type", "(", "previous_type", ")", "\n", "", "", "", "else", ":", "\n", "            ", "embeddings", "=", "words_embeddings", ".", "float", "(", ")", "+", "position_embeddings", ".", "float", "(", ")", "+", "token_type_embeddings", ".", "float", "(", ")", "\n", "if", "self", ".", "fp32_tokentypes", "and", "not", "self", ".", "fp32_layernorm", ":", "\n", "                ", "embeddings", "=", "embeddings", ".", "half", "(", ")", "\n", "", "previous_type", "=", "embeddings", ".", "type", "(", ")", "\n", "if", "self", ".", "fp32_layernorm", ":", "\n", "                ", "embeddings", "=", "embeddings", ".", "float", "(", ")", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "if", "self", ".", "fp32_layernorm", ":", "\n", "                ", "if", "self", ".", "fp32_tokentypes", ":", "\n", "                    ", "embeddings", "=", "embeddings", ".", "half", "(", ")", "\n", "", "else", ":", "\n", "                    ", "embeddings", "=", "embeddings", ".", "type", "(", "previous_type", ")", "\n", "", "", "", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertSelfAttention.__init__": [[324, 339], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertSelfAttention.transpose_for_scores": [[340, 344], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertSelfAttention.forward": [[345, 374], ["modeling.BertSelfAttention.query", "modeling.BertSelfAttention.key", "modeling.BertSelfAttention.value", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "modeling.BertSelfAttention.dropout", "modeling.BertSelfAttention.type", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling.BertSelfAttention.transpose", "math.sqrt", "torch.nn.Softmax", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "previous_type", "=", "attention_probs", ".", "type", "(", ")", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertSelfOutput.__init__": [[377, 396], ["torch.nn.Module.__init__", "mpu.RowParallelLinear", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "hasattr", "modeling.scaled_init_method", "modeling.normal_init_method"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.scaled_init_method", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.normal_init_method"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "'deep_init'", ")", "and", "config", ".", "deep_init", ":", "\n", "            ", "init_method", "=", "scaled_init_method", "(", "mean", "=", "0.0", ",", "\n", "std", "=", "config", ".", "initializer_range", ",", "\n", "num_layers", "=", "config", ".", "num_hidden_layers", ")", "\n", "", "else", ":", "\n", "            ", "init_method", "=", "normal_init_method", "(", "mean", "=", "0.0", ",", "\n", "std", "=", "config", ".", "initializer_range", ")", "\n", "", "self", ".", "dense", "=", "mpu", ".", "RowParallelLinear", "(", "\n", "input_size", "=", "config", ".", "hidden_size", ",", "\n", "output_size", "=", "config", ".", "hidden_size", ",", "\n", "bias", "=", "True", ",", "\n", "input_is_parallel", "=", "True", ",", "\n", "stride", "=", "1", ",", "\n", "init_method", "=", "init_method", ")", "\n", "self", ".", "fp32_layernorm", "=", "config", ".", "fp32_layernorm", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layernorm_epsilon", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertSelfOutput.forward": [[397, 408], ["modeling.BertSelfOutput.dense", "modeling.BertSelfOutput.dropout", "ln_input.float.float.type", "modeling.BertSelfOutput.LayerNorm", "ln_input.float.float.float", "hidden_states.type.type.type"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "ln_input", "=", "hidden_states", "+", "input_tensor", "\n", "previous_type", "=", "ln_input", ".", "type", "(", ")", "\n", "if", "self", ".", "fp32_layernorm", ":", "\n", "            ", "ln_input", "=", "ln_input", ".", "float", "(", ")", "\n", "", "hidden_states", "=", "self", ".", "LayerNorm", "(", "ln_input", ")", "\n", "if", "self", ".", "fp32_layernorm", ":", "\n", "            ", "hidden_states", "=", "hidden_states", ".", "type", "(", "previous_type", ")", "\n", "", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertAttention.__init__": [[411, 421], ["torch.nn.Module.__init__", "mpu.BertParallelSelfAttention", "modeling.BertSelfOutput", "modeling.normal_init_method"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.normal_init_method"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "mpu", ".", "BertParallelSelfAttention", "(", "\n", "hidden_size", "=", "config", ".", "hidden_size", ",", "\n", "num_attention_heads", "=", "config", ".", "num_attention_heads", ",", "\n", "dropout_prob", "=", "config", ".", "attention_probs_dropout_prob", ",", "\n", "output_parallel", "=", "True", ",", "\n", "init_method", "=", "normal_init_method", "(", "mean", "=", "0.0", ",", "\n", "std", "=", "config", ".", "initializer_range", ")", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertAttention.forward": [[422, 426], ["modeling.BertAttention.self", "modeling.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "attention_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertIntermediate.__init__": [[429, 441], ["torch.nn.Module.__init__", "mpu.ColumnParallelLinear", "isinstance", "modeling.normal_init_method"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.normal_init_method"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "mpu", ".", "ColumnParallelLinear", "(", "\n", "input_size", "=", "config", ".", "hidden_size", ",", "\n", "output_size", "=", "config", ".", "intermediate_size", ",", "\n", "bias", "=", "True", ",", "\n", "gather_output", "=", "False", ",", "\n", "stride", "=", "1", ",", "\n", "init_method", "=", "normal_init_method", "(", "mean", "=", "0.0", ",", "\n", "std", "=", "config", ".", "initializer_range", ")", ")", "\n", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertIntermediate.forward": [[442, 446], ["modeling.BertIntermediate.dense", "modeling.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertOutput.__init__": [[449, 468], ["torch.nn.Module.__init__", "mpu.RowParallelLinear", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "hasattr", "modeling.scaled_init_method", "modeling.normal_init_method"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.scaled_init_method", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.normal_init_method"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "'deep_init'", ")", "and", "config", ".", "deep_init", ":", "\n", "            ", "init_method", "=", "scaled_init_method", "(", "mean", "=", "0.0", ",", "\n", "std", "=", "config", ".", "initializer_range", ",", "\n", "num_layers", "=", "config", ".", "num_hidden_layers", ")", "\n", "", "else", ":", "\n", "            ", "init_method", "=", "normal_init_method", "(", "mean", "=", "0.0", ",", "\n", "std", "=", "config", ".", "initializer_range", ")", "\n", "", "self", ".", "dense", "=", "mpu", ".", "RowParallelLinear", "(", "\n", "input_size", "=", "config", ".", "intermediate_size", ",", "\n", "output_size", "=", "config", ".", "hidden_size", ",", "\n", "bias", "=", "True", ",", "\n", "input_is_parallel", "=", "True", ",", "\n", "stride", "=", "1", ",", "\n", "init_method", "=", "init_method", ")", "\n", "self", ".", "fp32_layernorm", "=", "config", ".", "fp32_layernorm", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layernorm_epsilon", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertOutput.forward": [[469, 480], ["modeling.BertOutput.dense", "modeling.BertOutput.dropout", "ln_input.float.float.type", "modeling.BertOutput.LayerNorm", "ln_input.float.float.float", "hidden_states.type.type.type"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "ln_input", "=", "hidden_states", "+", "input_tensor", "\n", "previous_type", "=", "ln_input", ".", "type", "(", ")", "\n", "if", "self", ".", "fp32_layernorm", ":", "\n", "            ", "ln_input", "=", "ln_input", ".", "float", "(", ")", "\n", "", "hidden_states", "=", "self", ".", "LayerNorm", "(", "ln_input", ")", "\n", "if", "self", ".", "fp32_layernorm", ":", "\n", "            ", "hidden_states", "=", "hidden_states", ".", "type", "(", "previous_type", ")", "\n", "", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertLayer.__init__": [[483, 488], ["torch.nn.Module.__init__", "modeling.BertAttention", "modeling.BertIntermediate", "modeling.BertOutput"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertLayer.forward": [[489, 494], ["modeling.BertLayer.attention", "modeling.BertLayer.intermediate", "modeling.BertLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertEncoder.__init__": [[497, 502], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "modeling.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "#layer = BertLayer(config)", "\n", "#self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertEncoder.forward": [[512, 541], ["len", "enumerate", "all_encoder_layers.append", "mpu.checkpoint", "layer_module", "layer", "modeling.BertEncoder.forward.custom"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.checkpoint"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "True", ",", "checkpoint_activations", "=", "False", ")", ":", "\n", "        ", "all_encoder_layers", "=", "[", "]", "\n", "def", "custom", "(", "start", ",", "end", ")", ":", "\n", "            ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                ", "layers", "=", "self", ".", "layer", "[", "start", ":", "end", "]", "\n", "x_", "=", "inputs", "[", "0", "]", "\n", "for", "layer", "in", "layers", ":", "\n", "                    ", "x_", "=", "layer", "(", "x_", ",", "inputs", "[", "1", "]", ")", "\n", "", "return", "x_", "\n", "", "return", "custom_forward", "\n", "\n", "", "if", "checkpoint_activations", ":", "\n", "            ", "l", "=", "0", "\n", "num_layers", "=", "len", "(", "self", ".", "layer", ")", "\n", "chunk_length", "=", "1", "#math.ceil(math.sqrt(num_layers))", "\n", "while", "l", "<", "num_layers", ":", "\n", "                ", "hidden_states", "=", "mpu", ".", "checkpoint", "(", "custom", "(", "l", ",", "l", "+", "chunk_length", ")", ",", "hidden_states", ",", "attention_mask", "*", "1", ")", "\n", "l", "+=", "chunk_length", "\n", "# decoder layers", "\n", "", "", "else", ":", "\n", "            ", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "                ", "hidden_states", "=", "layer_module", "(", "hidden_states", ",", "attention_mask", ")", "\n", "\n", "if", "output_all_encoded_layers", ":", "\n", "                    ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "\n", "", "", "", "if", "not", "output_all_encoded_layers", "or", "checkpoint_activations", ":", "\n", "            ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertPooler.__init__": [[544, 548], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertPooler.forward": [[549, 556], ["modeling.BertPooler.dense", "modeling.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertPredictionHeadTransform.__init__": [[559, 566], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "isinstance"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPredictionHeadTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layernorm_epsilon", ")", "\n", "self", ".", "fp32_layernorm", "=", "config", ".", "fp32_layernorm", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertPredictionHeadTransform.forward": [[567, 577], ["modeling.BertPredictionHeadTransform.dense", "modeling.BertPredictionHeadTransform.transform_act_fn", "hidden_states.type.type.type", "modeling.BertPredictionHeadTransform.LayerNorm", "hidden_states.type.type.float", "hidden_states.type.type.type"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "previous_type", "=", "hidden_states", ".", "type", "(", ")", "\n", "if", "self", ".", "fp32_layernorm", ":", "\n", "            ", "hidden_states", "=", "hidden_states", ".", "float", "(", ")", "\n", "", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "if", "self", ".", "fp32_layernorm", ":", "\n", "            ", "hidden_states", "=", "hidden_states", ".", "type", "(", "previous_type", ")", "\n", "", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertLMPredictionHead.__init__": [[580, 601], ["torch.nn.Module.__init__", "modeling.BertPredictionHeadTransform", "torch.nn.Parameter", "torch.nn.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "bert_model_embedding_weights.size", "tensor.half"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertLMPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "#self.decoder = nn.Linear(bert_model_embedding_weights.size(1),", "\n", "#                         bert_model_embedding_weights.size(0),", "\n", "#                         bias=False)", "\n", "self", ".", "decoder_weight", "=", "bert_model_embedding_weights", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ")", ")", "\n", "self", ".", "bias", ".", "model_parallel", "=", "True", "\n", "self", ".", "fp32_embedding", "=", "config", ".", "fp32_embedding", "\n", "self", ".", "fp32_layernorm", "=", "config", ".", "fp32_layernorm", "\n", "def", "convert_to_type", "(", "tensor", ")", ":", "\n", "            ", "if", "self", ".", "fp32_embedding", ":", "\n", "                ", "return", "tensor", ".", "half", "(", ")", "\n", "", "else", ":", "\n", "                ", "return", "tensor", "\n", "", "", "self", ".", "type_converter", "=", "convert_to_type", "\n", "self", ".", "converted", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertLMPredictionHead.forward": [[602, 616], ["modeling.BertLMPredictionHead.transform", "mpu.copy_to_model_parallel_region", "torch.linear", "torch.linear", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.transform.half", "modeling.BertLMPredictionHead.transform.LayerNorm.float"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.copy_to_model_parallel_region"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "if", "not", "self", ".", "converted", ":", "\n", "            ", "self", ".", "converted", "=", "True", "\n", "if", "self", ".", "fp32_embedding", ":", "\n", "                ", "self", ".", "transform", ".", "half", "(", ")", "\n", "if", "self", ".", "fp32_layernorm", ":", "\n", "                    ", "self", ".", "transform", ".", "LayerNorm", ".", "float", "(", ")", "\n", "", "", "", "hidden_states", "=", "self", ".", "transform", "(", "self", ".", "type_converter", "(", "hidden_states", ")", ")", "\n", "# hidden_states = self.decoder(hidden_states) + self.bias", "\n", "hidden_states", "=", "mpu", ".", "copy_to_model_parallel_region", "(", "hidden_states", ")", "\n", "hidden_states", "=", "F", ".", "linear", "(", "self", ".", "type_converter", "(", "hidden_states", ")", ",", "\n", "self", ".", "type_converter", "(", "self", ".", "decoder_weight", ")", ",", "\n", "self", ".", "type_converter", "(", "self", ".", "bias", ")", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertOnlyMLMHead.__init__": [[619, 622], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertOnlyMLMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertOnlyMLMHead.forward": [[623, 626], ["modeling.BertOnlyMLMHead.predictions"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertOnlyNSPHead.__init__": [[629, 632], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOnlyNSPHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertOnlyNSPHead.forward": [[633, 636], ["modeling.BertOnlyNSPHead.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pooled_output", ")", ":", "\n", "        ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertPreTrainingHeads.__init__": [[639, 643], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingHeads", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertPreTrainingHeads.forward": [[644, 652], ["modeling.BertPreTrainingHeads.predictions", "modeling.BertPreTrainingHeads.seq_relationship.parameters", "modeling.BertPreTrainingHeads.seq_relationship", "pooled_output.type_as.type_as.type_as"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ",", "pooled_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "for", "p", "in", "self", ".", "seq_relationship", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "p", "is", "None", ":", "\n", "                ", "continue", "\n", "", "pooled_output", "=", "pooled_output", ".", "type_as", "(", "p", ")", "\n", "", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.PreTrainedBertModel.__init__": [[658, 668], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "PreTrainedBertModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "BertConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"", "\n", "\"To create a model from a Google pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.PreTrainedBertModel.init_bert_weights": [[669, 681], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_bert_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.PreTrainedBertModel.from_pretrained": [[682, 795], ["os.path.isdir", "os.path.join", "modeling.BertConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "torch.load.keys", "zip", "getattr", "torch.load.copy", "torch.load.copy", "modeling.PreTrainedBertModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertConfig.from_json_file"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "\n", "fp32_layernorm", "=", "False", ",", "fp32_embedding", "=", "False", ",", "layernorm_epsilon", "=", "1e-12", ",", "\n", "fp32_tokentypes", "=", "False", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "pretrained_model_name", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "tempdir", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", ":", "\n", "            ", "serialization_dir", "=", "resolved_archive_file", "\n", "", "else", ":", "\n", "# Extract archive to temp dir", "\n", "            ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "logger", ".", "info", "(", "\"extracting archive file {} to temp dir {}\"", ".", "format", "(", "\n", "resolved_archive_file", ",", "tempdir", ")", ")", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "'r:gz'", ")", "as", "archive", ":", "\n", "                ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "serialization_dir", "=", "tempdir", "\n", "# Load config", "\n", "", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", "\n", "config", "=", "BertConfig", ".", "from_json_file", "(", "config_file", ")", "\n", "config", ".", "fp32_layernorm", "=", "fp32_layernorm", "\n", "config", ".", "fp32_embedding", "=", "fp32_embedding", "\n", "config", ".", "layernorm_epsilon", "=", "layernorm_epsilon", "\n", "config", ".", "fp32_tokentypes", "=", "fp32_tokentypes", "\n", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "# Instantiate model.", "\n", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", ":", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "WEIGHTS_NAME", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "weights_path", ")", "\n", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "'gamma'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'gamma'", ",", "'weight'", ")", "\n", "", "if", "'beta'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'beta'", ",", "'bias'", ")", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "", "", "", "load", "(", "model", ",", "prefix", "=", "''", "if", "hasattr", "(", "model", ",", "'bert'", ")", "else", "'bert.'", ")", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights of {} not initialized from pretrained model: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights from pretrained model not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", ")", "\n", "", "if", "tempdir", ":", "\n", "# Clean up temp dir", "\n", "            ", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertModel.__init__": [[841, 847], ["modeling.PreTrainedBertModel.__init__", "modeling.BertEmbeddings", "modeling.BertEncoder", "modeling.BertPooler", "modeling.BertModel.apply"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertModel.forward": [[848, 884], ["torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "modeling.BertModel.embeddings", "modeling.BertModel.encoder", "modeling.BertModel.pooler.parameters", "modeling.BertModel.pooler", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "sequence_output.type_as.type_as.type_as", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "next", "modeling.BertModel.encoder.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "output_all_encoded_layers", "=", "True", ",", "checkpoint_activations", "=", "False", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "encoder", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "token_type_ids", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "\n", "checkpoint_activations", "=", "checkpoint_activations", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "for", "p", "in", "self", ".", "pooler", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "p", "is", "None", ":", "\n", "                ", "continue", "\n", "", "sequence_output", "=", "sequence_output", ".", "type_as", "(", "p", ")", "\n", "break", "\n", "", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", "or", "checkpoint_activations", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForPreTraining.__init__": [[936, 941], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertPreTrainingHeads", "modeling.BertForPreTraining.apply"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForPreTraining", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForPreTraining.forward": [[942, 956], ["modeling.BertForPreTraining.bert", "modeling.BertForPreTraining.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores.view().float", "masked_lm_labels.view", "seq_relationship_score.view().float", "next_sentence_label.view", "prediction_scores.view", "seq_relationship_score.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "next_sentence_label", "=", "None", ",", "checkpoint_activations", "=", "False", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "checkpoint_activations", "=", "checkpoint_activations", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "sequence_output", ",", "pooled_output", ")", "\n", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", "and", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ".", "float", "(", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ".", "float", "(", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "total_loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForMaskedLM.__init__": [[1000, 1005], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertOnlyMLMHead", "modeling.BertForMaskedLM.apply"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForMaskedLM", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyMLMHead", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForMaskedLM.forward": [[1006, 1017], ["modeling.BertForMaskedLM.bert", "modeling.BertForMaskedLM.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling.BertForMaskedLM.view", "masked_lm_labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "checkpoint_activations", "=", "False", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "checkpoint_activations", "=", "checkpoint_activations", ")", "\n", "prediction_scores", "=", "self", ".", "cls", "(", "sequence_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "masked_lm_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForNextSentencePrediction.__init__": [[1062, 1067], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertOnlyNSPHead", "modeling.BertForNextSentencePrediction.apply"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForNextSentencePrediction", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyNSPHead", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForNextSentencePrediction.forward": [[1068, 1079], ["modeling.BertForNextSentencePrediction.bert", "modeling.BertForNextSentencePrediction.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling.BertForNextSentencePrediction.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "next_sentence_label", "=", "None", ",", "checkpoint_activations", "=", "False", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "checkpoint_activations", "=", "checkpoint_activations", ")", "\n", "seq_relationship_score", "=", "self", ".", "cls", "(", "pooled_output", ")", "\n", "\n", "if", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "next_sentence_loss", "\n", "", "else", ":", "\n", "            ", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForSequenceClassification.__init__": [[1126, 1133], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertForSequenceClassification.apply"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForSequenceClassification.forward": [[1134, 1145], ["modeling.BertForSequenceClassification.bert", "modeling.BertForSequenceClassification.dropout", "modeling.BertForSequenceClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling.BertForSequenceClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "checkpoint_activations", "=", "False", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "checkpoint_activations", "=", "checkpoint_activations", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForMultipleChoice.__init__": [[1191, 1198], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertForMultipleChoice.apply"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["def", "__init__", "(", "self", ",", "config", ",", "num_choices", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForMultipleChoice", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_choices", "=", "num_choices", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForMultipleChoice.forward": [[1199, 1214], ["input_ids.view", "token_type_ids.view", "attention_mask.view", "modeling.BertForMultipleChoice.bert", "modeling.BertForMultipleChoice.dropout", "modeling.BertForMultipleChoice.classifier", "modeling.BertForMultipleChoice.view", "input_ids.size", "token_type_ids.size", "attention_mask.size", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss."], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "checkpoint_activations", "=", "False", ")", ":", "\n", "        ", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "\n", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "flat_input_ids", ",", "flat_token_type_ids", ",", "flat_attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "checkpoint_activations", "=", "checkpoint_activations", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_choices", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "reshaped_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForTokenClassification.__init__": [[1261, 1276], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "mpu.RowParallelLinear", "modeling.BertForTokenClassification.apply", "modeling.normal_init_method"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.normal_init_method"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForTokenClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "#self.classifier = nn.Linear(config.hidden_size, num_labels)", "\n", "self", ".", "classifier", "=", "mpu", ".", "RowParallelLinear", "(", "\n", "input_size", "=", "config", ".", "hidden_size", ",", "\n", "output_size", "=", "num_labels", ",", "\n", "bias", "=", "True", ",", "\n", "input_is_parallel", "=", "True", ",", "\n", "stride", "=", "1", ",", "\n", "init_method", "=", "normal_init_method", "(", "mean", "=", "0.0", ",", "\n", "std", "=", "config", ".", "initializer_range", ")", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForTokenClassification.forward": [[1277, 1289], ["modeling.BertForTokenClassification.bert", "modeling.BertForTokenClassification.classifier", "mpu.get_cuda_rng_tracker().fork", "modeling.BertForTokenClassification.dropout", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling.BertForTokenClassification.view", "labels.view", "mpu.get_cuda_rng_tracker"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.fork", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "checkpoint_activations", "=", "False", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "checkpoint_activations", "=", "checkpoint_activations", ")", "\n", "with", "mpu", ".", "get_cuda_rng_tracker", "(", ")", ".", "fork", "(", ")", ":", "\n", "            ", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForQuestionAnswering.__init__": [[1338, 1353], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "mpu.RowParallelLinear", "modeling.BertForQuestionAnswering.apply", "modeling.normal_init_method"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.normal_init_method"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "# TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version", "\n", "# self.dropout = nn.Dropout(config.hidden_dropout_prob)", "\n", "#self.qa_outputs = nn.Linear(config.hidden_size, 2)", "\n", "self", ".", "qa_outputs", "=", "mpu", ".", "RowParallelLinear", "(", "\n", "input_size", "=", "config", ".", "hidden_size", ",", "\n", "output_size", "=", "2", ",", "\n", "bias", "=", "True", ",", "\n", "input_is_parallel", "=", "True", ",", "\n", "stride", "=", "1", ",", "\n", "init_method", "=", "normal_init_method", "(", "mean", "=", "0.0", ",", "\n", "std", "=", "config", ".", "initializer_range", ")", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.BertForQuestionAnswering.forward": [[1354, 1379], ["modeling.BertForQuestionAnswering.bert", "modeling.BertForQuestionAnswering.qa_outputs", "modeling.BertForQuestionAnswering.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "start_logits.squeeze.squeeze.size", "start_positions.squeeze.squeeze.clamp_", "end_positions.squeeze.squeeze.clamp_", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "len", "start_positions.squeeze.squeeze.squeeze", "len", "end_positions.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.size", "end_positions.squeeze.squeeze.size"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ",", "checkpoint_activations", "=", "False", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "checkpoint_activations", "=", "checkpoint_activations", ")", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension", "\n", "            ", "if", "len", "(", "start_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len", "(", "end_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our model inputs, we ignore these terms", "\n", "", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.normal_init_method": [[42, 46], ["torch.nn.init.normal_", "torch.nn.init.normal_"], "function", ["None"], ["def", "normal_init_method", "(", "mean", ",", "std", ")", ":", "\n", "    ", "def", "init_", "(", "tensor", ")", ":", "\n", "        ", "return", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "tensor", ",", "mean", "=", "mean", ",", "std", "=", "std", ")", "\n", "", "return", "init_", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.scaled_init_method": [[47, 54], ["math.sqrt", "torch.nn.init.normal_", "torch.nn.init.normal_"], "function", ["None"], ["", "def", "scaled_init_method", "(", "mean", ",", "std", ",", "num_layers", ")", ":", "\n", "    ", "\"\"\"Init method based on N(0, sigma/sqrt(2*num_layers).\"\"\"", "\n", "std", "=", "std", "/", "math", ".", "sqrt", "(", "2.0", "*", "num_layers", ")", "\n", "def", "init_", "(", "tensor", ")", ":", "\n", "        ", "return", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "tensor", ",", "mean", "=", "mean", ",", "std", "=", "std", ")", "\n", "\n", "", "return", "init_", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.load_tf_weights_in_bert": [[70, 129], ["os.path.abspath", "print", "tf.train.list_variables", "zip", "print", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "any", "print", "torch.from_numpy", "torch.from_numpy", "print", "print", "re.fullmatch", "getattr", "re.split", "getattr", "len", "int", "np.transpose", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["def", "load_tf_weights_in_bert", "(", "model", ",", "tf_checkpoint_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "re", "\n", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "print", "(", "\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "tf_checkpoint_path", ")", "\n", "print", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "\n", "# Load weights from TF model", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "print", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ")", "\n", "\n", "", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", ".", "split", "(", "'/'", ")", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained model", "\n", "if", "any", "(", "n", "in", "[", "\"adam_v\"", ",", "\"adam_m\"", "]", "for", "n", "in", "name", ")", ":", "\n", "            ", "print", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r'[A-Za-z]+_\\d+'", ",", "m_name", ")", ":", "\n", "                ", "l", "=", "re", ".", "split", "(", "r'_(\\d+)'", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "l", "=", "[", "m_name", "]", "\n", "", "if", "l", "[", "0", "]", "==", "'kernel'", "or", "l", "[", "0", "]", "==", "'gamma'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_bias'", "or", "l", "[", "0", "]", "==", "'beta'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'bias'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_weights'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "else", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "\n", "", "if", "len", "(", "l", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "l", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "if", "m_name", "[", "-", "11", ":", "]", "==", "'_embeddings'", ":", "\n", "            ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "m_name", "==", "'kernel'", ":", "\n", "            ", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "print", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.gelu": [[131, 137], ["torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.swish": [[139, 141], ["torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.distributed.DistributedDataParallel.__init__": [[26, 75], ["torch.nn.modules.Module.__init__", "mpu.get_data_parallel_group", "mpu.get_model_parallel_rank", "distributed.DistributedDataParallel.module.parameters", "list", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "distributed.DistributedDataParallel.module.parameters", "torch.broadcast", "torch.broadcast", "distributed.DistributedDataParallel.module.named_parameters", "torch.autograd.Variable._execution_engine.queue_callback", "torch.autograd.Variable._execution_engine.queue_callback", "torch._utils._flatten_dense_tensors", "torch._utils._flatten_dense_tensors", "torch.all_reduce", "torch.all_reduce", "torch.cuda.synchronize", "torch.cuda.synchronize", "torch.cuda.synchronize", "torch.cuda.synchronize", "zip", "param.data.type", "buckets[].append", "print", "coalesced.float.float.float", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size", "torch._utils._unflatten_dense_tensors", "torch._utils._unflatten_dense_tensors", "buf.copy_"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["    ", "def", "__init__", "(", "self", ",", "module", ")", ":", "\n", "        ", "super", "(", "DistributedDataParallel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "warn_on_half", "=", "True", "if", "dist", ".", "_backend", "==", "dist", ".", "dist_backend", ".", "GLOO", "else", "False", "\n", "\n", "self", ".", "module", "=", "module", "\n", "self", ".", "data_parallel_group", "=", "mpu", ".", "get_data_parallel_group", "(", ")", "\n", "src_rank", "=", "mpu", ".", "get_model_parallel_rank", "(", ")", "\n", "for", "p", "in", "self", ".", "module", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "torch", ".", "is_tensor", "(", "p", ")", ":", "\n", "                ", "dist", ".", "broadcast", "(", "p", ",", "src_rank", ",", "group", "=", "self", ".", "data_parallel_group", ")", "\n", "\n", "", "", "def", "allreduce_params", "(", "reduce_after", "=", "True", ",", "no_scale", "=", "False", ",", "fp32_allreduce", "=", "False", ")", ":", "\n", "            ", "if", "(", "self", ".", "needs_reduction", ")", ":", "\n", "                ", "self", ".", "needs_reduction", "=", "False", "\n", "buckets", "=", "{", "}", "\n", "for", "name", ",", "param", "in", "self", ".", "module", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "if", "param", ".", "requires_grad", "and", "param", ".", "grad", "is", "not", "None", ":", "\n", "                        ", "tp", "=", "(", "param", ".", "data", ".", "type", "(", ")", ")", "\n", "if", "tp", "not", "in", "buckets", ":", "\n", "                            ", "buckets", "[", "tp", "]", "=", "[", "]", "\n", "", "buckets", "[", "tp", "]", ".", "append", "(", "param", ")", "\n", "", "", "if", "self", ".", "warn_on_half", ":", "\n", "                    ", "if", "torch", ".", "cuda", ".", "HalfTensor", "in", "buckets", ":", "\n", "                        ", "print", "(", "\"WARNING: gloo dist backend for half parameters may be extremely slow.\"", "+", "\n", "\" It is recommended to use the NCCL backend in this case.\"", ")", "\n", "self", ".", "warn_on_half", "=", "False", "\n", "", "", "for", "tp", "in", "buckets", ":", "\n", "                    ", "bucket", "=", "buckets", "[", "tp", "]", "\n", "grads", "=", "[", "param", ".", "grad", ".", "data", "for", "param", "in", "bucket", "]", "\n", "coalesced", "=", "_flatten_dense_tensors", "(", "grads", ")", "\n", "if", "fp32_allreduce", ":", "\n", "                        ", "coalesced", "=", "coalesced", ".", "float", "(", ")", "\n", "", "if", "not", "no_scale", "and", "not", "reduce_after", ":", "\n", "                        ", "coalesced", "/=", "dist", ".", "get_world_size", "(", "group", "=", "self", ".", "data_parallel_group", ")", "\n", "", "dist", ".", "all_reduce", "(", "coalesced", ",", "group", "=", "self", ".", "data_parallel_group", ")", "\n", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "if", "not", "no_scale", "and", "reduce_after", ":", "\n", "                        ", "coalesced", "/=", "dist", ".", "get_world_size", "(", "group", "=", "self", ".", "data_parallel_group", ")", "\n", "", "for", "buf", ",", "synced", "in", "zip", "(", "grads", ",", "_unflatten_dense_tensors", "(", "coalesced", ",", "grads", ")", ")", ":", "\n", "                        ", "buf", ".", "copy_", "(", "synced", ")", "\n", "", "", "", "", "self", ".", "hook_handles", "=", "[", "]", "\n", "self", ".", "hooks", "=", "[", "]", "\n", "for", "param", "in", "list", "(", "self", ".", "module", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "def", "allreduce_hook", "(", "*", "unused", ")", ":", "\n", "                ", "Variable", ".", "_execution_engine", ".", "queue_callback", "(", "allreduce_params", ")", "\n", "#    handle = param.register_hook(allreduce_hook)", "\n", "#self.hooks.append(allreduce_hook)", "\n", "#self.hook_handles.append(handle)", "\n", "", "", "self", ".", "allreduce_params", "=", "allreduce_params", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.distributed.DistributedDataParallel.forward": [[76, 79], ["distributed.DistributedDataParallel.module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "needs_reduction", "=", "True", "\n", "return", "self", ".", "module", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.distributed.DistributedDataParallel.state_dict": [[80, 88], ["distributed.DistributedDataParallel.module.state_dict"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.state_dict"], ["", "def", "state_dict", "(", "self", ",", "destination", "=", "None", ",", "prefix", "=", "''", ",", "keep_vars", "=", "False", ")", ":", "\n", "#[h.remove() for h in self.hook_handles]", "\n", "        ", "sd", "=", "self", ".", "module", ".", "state_dict", "(", "destination", ",", "prefix", ",", "keep_vars", ")", "\n", "# for handle, hook in zip(self.hook_handles, self.hooks):", "\n", "#     d = handle.hooks_dict_ref()", "\n", "#     d[handle.id] = hook", "\n", "\n", "return", "sd", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.distributed.DistributedDataParallel.load_state_dict": [[89, 91], ["distributed.DistributedDataParallel.module.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "strict", "=", "True", ")", ":", "\n", "        ", "self", ".", "module", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.gpt2_modeling.GPT2Model.__init__": [[42, 82], ["super().__init__", "gpt2_modeling.init_method_normal", "mpu.VocabParallelEmbedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "init_method_normal.", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "mpu.GPT2ParallelTransformer"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.gpt2_modeling.init_method_normal"], ["def", "__init__", "(", "self", ",", "\n", "num_layers", ",", "\n", "vocab_size", ",", "\n", "hidden_size", ",", "\n", "num_attention_heads", ",", "\n", "embedding_dropout_prob", ",", "\n", "attention_dropout_prob", ",", "\n", "output_dropout_prob", ",", "\n", "max_sequence_length", ",", "\n", "checkpoint_activations", ",", "\n", "checkpoint_num_layers", "=", "1", ",", "\n", "parallel_output", "=", "True", ")", ":", "\n", "\n", "        ", "super", "(", "GPT2Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "parallel_output", "=", "parallel_output", "\n", "\n", "init_method", "=", "init_method_normal", "(", "std", "=", "0.02", ")", "\n", "\n", "# Word embeddings (parallel).", "\n", "self", ".", "word_embeddings", "=", "mpu", ".", "VocabParallelEmbedding", "(", "\n", "vocab_size", ",", "hidden_size", ",", "init_method", "=", "init_method", ")", "\n", "\n", "# Position embedding (serial).", "\n", "self", ".", "position_embeddings", "=", "torch", ".", "nn", ".", "Embedding", "(", "max_sequence_length", ",", "\n", "hidden_size", ")", "\n", "# Initialize the position embeddings.", "\n", "init_method", "(", "self", ".", "position_embeddings", ".", "weight", ")", "\n", "\n", "# Embeddings dropout", "\n", "self", ".", "embedding_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "embedding_dropout_prob", ")", "\n", "\n", "# Transformer", "\n", "self", ".", "transformer", "=", "mpu", ".", "GPT2ParallelTransformer", "(", "num_layers", ",", "\n", "hidden_size", ",", "\n", "num_attention_heads", ",", "\n", "attention_dropout_prob", ",", "\n", "output_dropout_prob", ",", "\n", "checkpoint_activations", ",", "\n", "checkpoint_num_layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.gpt2_modeling.GPT2Model.forward": [[83, 106], ["gpt2_modeling.GPT2Model.word_embeddings", "gpt2_modeling.GPT2Model.position_embeddings", "gpt2_modeling.GPT2Model.embedding_dropout", "gpt2_modeling.GPT2Model.transformer", "mpu.copy_to_model_parallel_region", "torch.linear", "torch.linear", "mpu.gather_from_model_parallel_region"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.copy_to_model_parallel_region", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.gather_from_model_parallel_region"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "position_ids", ",", "attention_mask", ",", "past_key_values", "=", "None", ",", "use_cache", "=", "False", ")", ":", "\n", "\n", "# Embeddings.", "\n", "        ", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "\n", "\n", "# Dropout.", "\n", "embeddings", "=", "self", ".", "embedding_dropout", "(", "embeddings", ")", "\n", "\n", "# Transformer.", "\n", "transformer_output", ",", "presents", "=", "self", ".", "transformer", "(", "embeddings", ",", "attention_mask", ",", "past_key_values", "=", "past_key_values", ",", "use_cache", "=", "use_cache", ")", "\n", "\n", "# Parallel logits.", "\n", "transformer_output_parallel", "=", "mpu", ".", "copy_to_model_parallel_region", "(", "\n", "transformer_output", ")", "\n", "logits_parallel", "=", "F", ".", "linear", "(", "transformer_output_parallel", ",", "\n", "self", ".", "word_embeddings", ".", "weight", ")", "\n", "\n", "if", "self", ".", "parallel_output", ":", "\n", "            ", "return", "logits_parallel", ",", "presents", "\n", "\n", "", "return", "mpu", ".", "gather_from_model_parallel_region", "(", "logits_parallel", ")", ",", "presents", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.gpt2_modeling.init_method_normal": [[24, 33], ["torch.nn.init.normal_", "torch.nn.init.normal_"], "function", ["None"], ["def", "init_method_normal", "(", "std", "=", "0.02", ")", ":", "\n", "    ", "\"\"\"Init method based on normal distribution.\n\n    This is only used for embeddings. The transformer has its\n    own initializer.\n    \"\"\"", "\n", "def", "init_", "(", "tensor", ")", ":", "\n", "        ", "return", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "tensor", ",", "mean", "=", "0.0", ",", "std", "=", "std", ")", "\n", "", "return", "init_", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.gpt2_modeling.gpt2_get_params_for_weight_decay_optimization": [[108, 126], ["module.modules", "isinstance", "no_weight_decay_params[].extend", "weight_decay_params[].extend", "no_weight_decay_params[].extend", "list", "list", "list", "module_._parameters.values", "module_._parameters.items", "module_._parameters.items"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend"], ["", "", "def", "gpt2_get_params_for_weight_decay_optimization", "(", "module", ")", ":", "\n", "\n", "    ", "weight_decay_params", "=", "{", "'params'", ":", "[", "]", "}", "\n", "no_weight_decay_params", "=", "{", "'params'", ":", "[", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "for", "module_", "in", "module", ".", "modules", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "module_", ",", "(", "mpu", ".", "LayerNorm", ",", "torch", ".", "nn", ".", "LayerNorm", ")", ")", ":", "\n", "            ", "no_weight_decay_params", "[", "'params'", "]", ".", "extend", "(", "\n", "[", "p", "for", "p", "in", "list", "(", "module_", ".", "_parameters", ".", "values", "(", ")", ")", "\n", "if", "p", "is", "not", "None", "]", ")", "\n", "", "else", ":", "\n", "            ", "weight_decay_params", "[", "'params'", "]", ".", "extend", "(", "\n", "[", "p", "for", "n", ",", "p", "in", "list", "(", "module_", ".", "_parameters", ".", "items", "(", ")", ")", "\n", "if", "p", "is", "not", "None", "and", "n", "!=", "'bias'", "]", ")", "\n", "no_weight_decay_params", "[", "'params'", "]", ".", "extend", "(", "\n", "[", "p", "for", "n", ",", "p", "in", "list", "(", "module_", ".", "_parameters", ".", "items", "(", ")", ")", "\n", "if", "p", "is", "not", "None", "and", "n", "==", "'bias'", "]", ")", "\n", "\n", "", "", "return", "weight_decay_params", ",", "no_weight_decay_params", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.__init__": [[47, 77], ["super().__init__", "modeling.BertForPreTraining.from_pretrained", "modeling.BertConfig", "modeling.BertForPreTraining"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.modeling.PreTrainedBertModel.from_pretrained"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "args", ".", "pretrained_bert", ":", "\n", "            ", "self", ".", "model", "=", "BertForPreTraining", ".", "from_pretrained", "(", "\n", "args", ".", "tokenizer_model_type", ",", "\n", "cache_dir", "=", "args", ".", "cache_dir", ",", "\n", "fp32_layernorm", "=", "args", ".", "fp32_layernorm", ",", "\n", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "\n", "layernorm_epsilon", "=", "args", ".", "layernorm_epsilon", ")", "\n", "", "else", ":", "\n", "            ", "if", "args", ".", "intermediate_size", "is", "None", ":", "\n", "                ", "intermediate_size", "=", "4", "*", "args", ".", "hidden_size", "\n", "", "else", ":", "\n", "                ", "intermediate_size", "=", "args", ".", "intermediate_size", "\n", "", "self", ".", "config", "=", "BertConfig", "(", "\n", "args", ".", "tokenizer_num_tokens", ",", "\n", "hidden_size", "=", "args", ".", "hidden_size", ",", "\n", "num_hidden_layers", "=", "args", ".", "num_layers", ",", "\n", "num_attention_heads", "=", "args", ".", "num_attention_heads", ",", "\n", "intermediate_size", "=", "intermediate_size", ",", "\n", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout", ",", "\n", "attention_probs_dropout_prob", "=", "args", ".", "attention_dropout", ",", "\n", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "\n", "type_vocab_size", "=", "args", ".", "tokenizer_num_type_tokens", ",", "\n", "fp32_layernorm", "=", "args", ".", "fp32_layernorm", ",", "\n", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "\n", "fp32_tokentypes", "=", "args", ".", "fp32_tokentypes", ",", "\n", "layernorm_epsilon", "=", "args", ".", "layernorm_epsilon", ",", "\n", "deep_init", "=", "args", ".", "deep_init", ")", "\n", "self", ".", "model", "=", "BertForPreTraining", "(", "self", ".", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.forward": [[78, 83], ["model.BertModel.model"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input_tokens", ",", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "checkpoint_activations", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "model", "(", "\n", "input_tokens", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "checkpoint_activations", "=", "checkpoint_activations", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.state_dict": [[84, 87], ["model.BertModel.model.state_dict"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.state_dict"], ["", "def", "state_dict", "(", "self", ",", "destination", "=", "None", ",", "prefix", "=", "''", ",", "keep_vars", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "model", ".", "state_dict", "(", "destination", "=", "destination", ",", "prefix", "=", "prefix", ",", "\n", "keep_vars", "=", "keep_vars", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.load_state_dict": [[88, 90], ["model.BertModel.model.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.BertModel.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "strict", "=", "True", ")", ":", "\n", "        ", "return", "self", ".", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.model.model.get_params_for_weight_decay_optimization": [[25, 43], ["module.modules", "isinstance", "no_weight_decay_params[].extend", "weight_decay_params[].extend", "no_weight_decay_params[].extend", "list", "list", "list", "module_._parameters.values", "module_._parameters.items", "module_._parameters.items"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend"], ["def", "get_params_for_weight_decay_optimization", "(", "module", ")", ":", "\n", "\n", "    ", "weight_decay_params", "=", "{", "'params'", ":", "[", "]", "}", "\n", "no_weight_decay_params", "=", "{", "'params'", ":", "[", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "for", "module_", "in", "module", ".", "modules", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "module_", ",", "(", "BertLayerNorm", ",", "torch", ".", "nn", ".", "LayerNorm", ")", ")", ":", "\n", "            ", "no_weight_decay_params", "[", "'params'", "]", ".", "extend", "(", "\n", "[", "p", "for", "p", "in", "list", "(", "module_", ".", "_parameters", ".", "values", "(", ")", ")", "\n", "if", "p", "is", "not", "None", "]", ")", "\n", "", "else", ":", "\n", "            ", "weight_decay_params", "[", "'params'", "]", ".", "extend", "(", "\n", "[", "p", "for", "n", ",", "p", "in", "list", "(", "module_", ".", "_parameters", ".", "items", "(", ")", ")", "\n", "if", "p", "is", "not", "None", "and", "n", "!=", "'bias'", "]", ")", "\n", "no_weight_decay_params", "[", "'params'", "]", ".", "extend", "(", "\n", "[", "p", "for", "n", ",", "p", "in", "list", "(", "module_", ".", "_parameters", ".", "items", "(", ")", ")", "\n", "if", "p", "is", "not", "None", "and", "n", "==", "'bias'", "]", ")", "\n", "\n", "", "", "return", "weight_decay_params", ",", "no_weight_decay_params", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.grads.clip_grad_norm": [[28, 75], ["isinstance", "list", "float", "float", "filter", "max", "torch.cuda.FloatTensor", "torch.distributed.all_reduce", "total_norm_cuda[].item", "torch.cuda.FloatTensor", "torch.distributed.all_reduce", "total_norm_cuda[].item", "p.grad.data.mul_", "p.grad.data.abs().max", "float", "initialize.get_model_parallel_group", "p.grad.data.norm", "float", "initialize.get_model_parallel_group", "initialize.get_model_parallel_rank", "p.grad.data.norm.item", "p.grad.data.abs"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank"], ["def", "clip_grad_norm", "(", "parameters", ",", "max_norm", ",", "norm_type", "=", "2", ")", ":", "\n", "    ", "\"\"\"Clips gradient norm of an iterable of parameters.\n\n    This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and\n    added functionality to handle model parallel parameters. Note that\n    the gradients are modified in place.\n\n    Arguments:\n        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n            single Tensor that will have gradients normalized\n        max_norm (float or int): max norm of the gradients\n        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n            infinity norm.\n\n    Returns:\n        Total norm of the parameters (viewed as a single vector).\n    \"\"\"", "\n", "if", "isinstance", "(", "parameters", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "parameters", "=", "[", "parameters", "]", "\n", "", "parameters", "=", "list", "(", "filter", "(", "lambda", "p", ":", "p", ".", "grad", "is", "not", "None", ",", "parameters", ")", ")", "\n", "max_norm", "=", "float", "(", "max_norm", ")", "\n", "norm_type", "=", "float", "(", "norm_type", ")", "\n", "if", "norm_type", "==", "inf", ":", "\n", "        ", "total_norm", "=", "max", "(", "p", ".", "grad", ".", "data", ".", "abs", "(", ")", ".", "max", "(", ")", "for", "p", "in", "parameters", ")", "\n", "total_norm_cuda", "=", "torch", ".", "cuda", ".", "FloatTensor", "(", "[", "float", "(", "total_norm", ")", "]", ")", "\n", "# Take max across all GPUs.", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "total_norm_cuda", ",", "\n", "op", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "MAX", ",", "\n", "group", "=", "get_model_parallel_group", "(", ")", ")", "\n", "total_norm", "=", "total_norm_cuda", "[", "0", "]", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "        ", "total_norm", "=", "0", "\n", "for", "p", "in", "parameters", ":", "\n", "            ", "if", "p", ".", "model_parallel", "or", "(", "get_model_parallel_rank", "(", ")", "==", "0", ")", ":", "\n", "                ", "param_norm", "=", "p", ".", "grad", ".", "data", ".", "norm", "(", "norm_type", ")", "\n", "total_norm", "+=", "param_norm", ".", "item", "(", ")", "**", "norm_type", "\n", "# Sum across all model parallel GPUs.", "\n", "", "", "total_norm_cuda", "=", "torch", ".", "cuda", ".", "FloatTensor", "(", "[", "float", "(", "total_norm", ")", "]", ")", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "total_norm_cuda", ",", "\n", "op", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", ",", "\n", "group", "=", "get_model_parallel_group", "(", ")", ")", "\n", "total_norm", "=", "total_norm_cuda", "[", "0", "]", ".", "item", "(", ")", "**", "(", "1.", "/", "norm_type", ")", "\n", "", "clip_coef", "=", "max_norm", "/", "(", "total_norm", "+", "1e-6", ")", "\n", "if", "clip_coef", "<", "1", ":", "\n", "        ", "for", "p", "in", "parameters", ":", "\n", "            ", "p", ".", "grad", ".", "data", ".", "mul_", "(", "clip_coef", ")", "\n", "", "", "return", "total_norm", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._CopyToModelParallelRegion.forward": [[82, 85], ["None"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input_", ")", ":", "\n", "        ", "return", "input_", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._CopyToModelParallelRegion.backward": [[86, 89], ["mappings._reduce"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._reduce"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "return", "_reduce", "(", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._ReduceFromModelParallelRegion.forward": [[94, 97], ["mappings._reduce"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._reduce"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input_", ")", ":", "\n", "        ", "return", "_reduce", "(", "input_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._ReduceFromModelParallelRegion.backward": [[98, 101], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "return", "grad_output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._ScatterToModelParallelRegion.forward": [[106, 109], ["mappings._split"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._split"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input_", ")", ":", "\n", "        ", "return", "_split", "(", "input_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._ScatterToModelParallelRegion.backward": [[110, 113], ["mappings._gather"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._gather"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "return", "_gather", "(", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._GatherFromModelParallelRegion.forward": [[118, 121], ["mappings._gather"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._gather"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input_", ")", ":", "\n", "        ", "return", "_gather", "(", "input_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._GatherFromModelParallelRegion.backward": [[122, 125], ["mappings._split"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._split"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "return", "_split", "(", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._reduce": [[22, 34], ["initialize.get_model_parallel_group", "torch.distributed.all_reduce", "torch.distributed.get_world_size"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group"], ["def", "_reduce", "(", "input_", ")", ":", "\n", "    ", "\"\"\"All-reduce the the input tensor across model parallel group.\"\"\"", "\n", "group", "=", "get_model_parallel_group", "(", ")", "\n", "\n", "# Bypass the function if we are using only 1 GPU.", "\n", "if", "torch", ".", "distributed", ".", "get_world_size", "(", "group", "=", "group", ")", "==", "1", ":", "\n", "        ", "return", "input_", "\n", "\n", "# All-reduce.", "\n", "", "torch", ".", "distributed", ".", "all_reduce", "(", "input_", ",", "group", "=", "group", ")", "\n", "\n", "return", "input_", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._split": [[36, 54], ["initialize.get_model_parallel_group", "torch.distributed.get_world_size", "utils.split_tensor_along_last_dim", "torch.distributed.get_rank", "input_list[].contiguous", "torch.distributed.get_world_size"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.split_tensor_along_last_dim"], ["", "def", "_split", "(", "input_", ")", ":", "\n", "    ", "\"\"\"Split the tensor along its last dimension and keep the\n    corresponding slice.\"\"\"", "\n", "group", "=", "get_model_parallel_group", "(", ")", "\n", "\n", "# Bypass the function if we are using only 1 GPU.", "\n", "if", "torch", ".", "distributed", ".", "get_world_size", "(", "group", "=", "group", ")", "==", "1", ":", "\n", "        ", "return", "input_", "\n", "\n", "# Split along last dimension.", "\n", "", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", "group", "=", "group", ")", "\n", "input_list", "=", "split_tensor_along_last_dim", "(", "input_", ",", "world_size", ")", "\n", "\n", "# Note: torch.split does not create contiguous tensors by default.", "\n", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", "group", "=", "group", ")", "\n", "output", "=", "input_list", "[", "rank", "]", ".", "contiguous", "(", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings._gather": [[56, 77], ["initialize.get_model_parallel_group", "torch.distributed.get_rank", "torch.distributed.get_world_size", "torch.distributed.all_gather", "torch.cat().contiguous", "torch.distributed.get_world_size", "input_.dim", "torch.empty_like", "range", "torch.cat"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group"], ["", "def", "_gather", "(", "input_", ")", ":", "\n", "    ", "\"\"\"Gather tensors and concatinate along the last dimension.\"\"\"", "\n", "group", "=", "get_model_parallel_group", "(", ")", "\n", "\n", "# Bypass the function if we are using only 1 GPU.", "\n", "if", "torch", ".", "distributed", ".", "get_world_size", "(", "group", "=", "group", ")", "==", "1", ":", "\n", "        ", "return", "input_", "\n", "\n", "# Size and dimension.", "\n", "", "last_dim", "=", "input_", ".", "dim", "(", ")", "-", "1", "\n", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", "group", "=", "group", ")", "\n", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", "group", "=", "group", ")", "\n", "\n", "tensor_list", "=", "[", "torch", ".", "empty_like", "(", "input_", ")", "for", "_", "in", "range", "(", "world_size", ")", "]", "\n", "tensor_list", "[", "rank", "]", "=", "input_", "\n", "torch", ".", "distributed", ".", "all_gather", "(", "tensor_list", ",", "input_", ",", "group", "=", "group", ")", "\n", "\n", "# Note: torch.cat already creates a contiguous tensor.", "\n", "output", "=", "torch", ".", "cat", "(", "tensor_list", ",", "dim", "=", "last_dim", ")", ".", "contiguous", "(", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.copy_to_model_parallel_region": [[131, 133], ["_CopyToModelParallelRegion.apply"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["", "", "def", "copy_to_model_parallel_region", "(", "input_", ")", ":", "\n", "    ", "return", "_CopyToModelParallelRegion", ".", "apply", "(", "input_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.reduce_from_model_parallel_region": [[134, 136], ["_ReduceFromModelParallelRegion.apply"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["", "def", "reduce_from_model_parallel_region", "(", "input_", ")", ":", "\n", "    ", "return", "_ReduceFromModelParallelRegion", ".", "apply", "(", "input_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.scatter_to_model_parallel_region": [[137, 139], ["_ScatterToModelParallelRegion.apply"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["", "def", "scatter_to_model_parallel_region", "(", "input_", ")", ":", "\n", "    ", "return", "_ScatterToModelParallelRegion", ".", "apply", "(", "input_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.gather_from_model_parallel_region": [[140, 142], ["_GatherFromModelParallelRegion.apply"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["", "def", "gather_from_model_parallel_region", "(", "input_", ")", ":", "\n", "    ", "return", "_GatherFromModelParallelRegion", ".", "apply", "(", "input_", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.cross_entropy._VocabParallelCrossEntropy.forward": [[27, 82], ["vocab_parallel_logits.clone", "torch.distributed.all_reduce", "vocab_parallel_logits.clone.sub_", "vocab_parallel_logits.clone.exp", "vocab_parallel_logits.clone.exp.sum", "torch.distributed.all_reduce", "initialize.get_model_parallel_rank", "initialize.get_model_parallel_world_size", "get_vocab_range", "vocab_parallel_logits.clone.view", "masked_target.view", "torch.arange", "predicted_logits_1d.view_as", "torch.distributed.all_reduce", "vocab_parallel_logits.clone.exp.div_", "ctx.save_for_backward", "torch.max", "logits_max.unsqueeze", "vocab_parallel_logits.size", "target.clone", "torch.log", "logits.exp.sum.unsqueeze", "initialize.get_model_parallel_group", "initialize.get_model_parallel_group", "initialize.get_model_parallel_group", "vocab_parallel_logits.clone.view.size"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.Timers.log", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "vocab_parallel_logits", ",", "target", ")", ":", "\n", "\n", "# Copy so the input remains unchanged.", "\n", "        ", "logits", "=", "vocab_parallel_logits", ".", "clone", "(", ")", "\n", "# Maximum value along vocab dimension across all GPUs.", "\n", "logits_max", "=", "torch", ".", "max", "(", "logits", ",", "dim", "=", "-", "1", ")", "[", "0", "]", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "logits_max", ",", "\n", "op", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "MAX", ",", "\n", "group", "=", "get_model_parallel_group", "(", ")", ")", "\n", "# Subtract the maximum value.", "\n", "logits", ".", "sub_", "(", "logits_max", ".", "unsqueeze", "(", "dim", "=", "-", "1", ")", ")", "\n", "# Sum of exponential of logits along vocab dimension across all GPUs.", "\n", "exp_logits", "=", "logits", ".", "exp", "(", ")", "\n", "sum_exp_logits", "=", "exp_logits", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "sum_exp_logits", ",", "\n", "op", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", ",", "\n", "group", "=", "get_model_parallel_group", "(", ")", ")", "\n", "\n", "# Get the partition's vocab indecies", "\n", "get_vocab_range", "=", "VocabUtility", ".", "vocab_range_from_per_partition_vocab_size", "\n", "partition_vocab_size", "=", "vocab_parallel_logits", ".", "size", "(", ")", "[", "-", "1", "]", "\n", "rank", "=", "get_model_parallel_rank", "(", ")", "\n", "world_size", "=", "get_model_parallel_world_size", "(", ")", "\n", "vocab_start_index", ",", "vocab_end_index", "=", "get_vocab_range", "(", "\n", "partition_vocab_size", ",", "rank", ",", "world_size", ")", "\n", "\n", "# Create a mask of valid vocab ids (1 means it needs to be masked).", "\n", "target_mask", "=", "(", "target", "<", "vocab_start_index", ")", "|", "(", "target", ">=", "vocab_end_index", ")", "\n", "masked_target", "=", "target", ".", "clone", "(", ")", "-", "vocab_start_index", "\n", "masked_target", "[", "target_mask", "]", "=", "0", "\n", "\n", "# Get predicted-logits = logits[target].", "\n", "# For Simplicity, we convert logits to a 2-D tensor with size", "\n", "# [*, partition-vocab-size] and target to a 1-D tensor of size [*].", "\n", "logits_2d", "=", "logits", ".", "view", "(", "-", "1", ",", "partition_vocab_size", ")", "\n", "masked_target_1d", "=", "masked_target", ".", "view", "(", "-", "1", ")", "\n", "arange_1d", "=", "torch", ".", "arange", "(", "start", "=", "0", ",", "end", "=", "logits_2d", ".", "size", "(", ")", "[", "0", "]", ",", "\n", "device", "=", "logits_2d", ".", "device", ")", "\n", "predicted_logits_1d", "=", "logits_2d", "[", "arange_1d", ",", "masked_target_1d", "]", "\n", "predicted_logits", "=", "predicted_logits_1d", ".", "view_as", "(", "target", ")", "\n", "predicted_logits", "[", "target_mask", "]", "=", "0.0", "\n", "# All reduce is needed to get the chunks from other GPUs.", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "predicted_logits", ",", "\n", "op", "=", "torch", ".", "distributed", ".", "ReduceOp", ".", "SUM", ",", "\n", "group", "=", "get_model_parallel_group", "(", ")", ")", "\n", "\n", "# Loss = log(sum(exp(logits))) - predicted-logit.", "\n", "loss", "=", "torch", ".", "log", "(", "sum_exp_logits", ")", "-", "predicted_logits", "\n", "\n", "# Store softmax, target-mask and masked-target for backward pass.", "\n", "exp_logits", ".", "div_", "(", "sum_exp_logits", ".", "unsqueeze", "(", "dim", "=", "-", "1", ")", ")", "\n", "ctx", ".", "save_for_backward", "(", "exp_logits", ",", "target_mask", ",", "masked_target_1d", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.cross_entropy._VocabParallelCrossEntropy.backward": [[83, 105], ["grad_input.view", "torch.arange", "grad_input.mul_", "softmax.size", "target_mask.view().float", "grad_output.unsqueeze", "grad_input.view.size", "target_mask.view"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "\n", "# Retreive tensors from the forward path.", "\n", "        ", "softmax", ",", "target_mask", ",", "masked_target_1d", "=", "ctx", ".", "saved_tensors", "\n", "\n", "# All the inputs have softmax as thier gradient.", "\n", "grad_input", "=", "softmax", "\n", "# For simplicity, work with the 2D gradient.", "\n", "partition_vocab_size", "=", "softmax", ".", "size", "(", ")", "[", "-", "1", "]", "\n", "grad_2d", "=", "grad_input", ".", "view", "(", "-", "1", ",", "partition_vocab_size", ")", "\n", "\n", "# Add the gradient from matching classes.", "\n", "arange_1d", "=", "torch", ".", "arange", "(", "start", "=", "0", ",", "end", "=", "grad_2d", ".", "size", "(", ")", "[", "0", "]", ",", "\n", "device", "=", "grad_2d", ".", "device", ")", "\n", "grad_2d", "[", "arange_1d", ",", "masked_target_1d", "]", "-=", "(", "\n", "1.0", "-", "target_mask", ".", "view", "(", "-", "1", ")", ".", "float", "(", ")", ")", "\n", "\n", "# Finally elementwise multiplication with the output gradients.", "\n", "grad_input", ".", "mul_", "(", "grad_output", ".", "unsqueeze", "(", "dim", "=", "-", "1", ")", ")", "\n", "\n", "return", "grad_input", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.cross_entropy.vocab_parallel_cross_entropy": [[107, 110], ["_VocabParallelCrossEntropy.apply"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["", "", "def", "vocab_parallel_cross_entropy", "(", "vocab_parallel_logits", ",", "target", ")", ":", "\n", "    ", "\"\"\"Helper function for the cross entropy.\"\"\"", "\n", "return", "_VocabParallelCrossEntropy", ".", "apply", "(", "vocab_parallel_logits", ",", "target", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel": [[30, 79], ["torch.distributed.is_initialized", "torch.distributed.get_world_size", "min", "utils.ensure_divisibility", "torch.distributed.get_rank", "range", "range", "torch.distributed.get_rank", "print", "range", "torch.distributed.new_group", "range", "torch.distributed.new_group"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.ensure_divisibility"], ["def", "initialize_model_parallel", "(", "model_parallel_size_", ")", ":", "\n", "    ", "\"\"\"\n    Initialize model data parallel groups.\n\n    Arguments:\n        model_parallel_size: number of GPUs used to parallelize model.\n\n    Let's say we have a total of 8 GPUs denoted by g0 ... g7 and we\n    use 2 GPUs to parallelize the model. The present function will\n    create 4 model parallel groups and 2 data parallel grous as:\n        4 model parallel groups:\n            [g0, g1], [g2, g3], [g4, g5], [g6, g7]\n        2 data parallel groups:\n            [g0, g2, g4, g6], [g1, g3, g5, g7]\n    Note that for efficiency, the caller should make sure adjacent ranks\n    are on the same DGX box. For example if we are using 2 DGX-1 boxes\n    with a total of 16 GPUs, rank 0 to 7 belong to the first box and\n    ranks 8 to 15 belong to the second box.\n    \"\"\"", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> initializing model parallel with size {}'", ".", "format", "(", "\n", "model_parallel_size_", ")", ")", "\n", "# Get world size and rank. Ensure some consistencies.", "\n", "", "assert", "torch", ".", "distributed", ".", "is_initialized", "(", ")", "\n", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "model_parallel_size", "=", "min", "(", "model_parallel_size_", ",", "world_size", ")", "\n", "ensure_divisibility", "(", "world_size", ",", "model_parallel_size", ")", "\n", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "\n", "# Build the data parallel groups.", "\n", "global", "_DATA_PARALLEL_GROUP", "\n", "assert", "_DATA_PARALLEL_GROUP", "is", "None", ",", "'data parallel group is already initialized'", "\n", "for", "i", "in", "range", "(", "model_parallel_size", ")", ":", "\n", "        ", "ranks", "=", "range", "(", "i", ",", "world_size", ",", "model_parallel_size", ")", "\n", "group", "=", "torch", ".", "distributed", ".", "new_group", "(", "ranks", ")", "\n", "if", "i", "==", "(", "rank", "%", "model_parallel_size", ")", ":", "\n", "            ", "_DATA_PARALLEL_GROUP", "=", "group", "\n", "\n", "# Build the model parallel groups.", "\n", "", "", "global", "_MODEL_PARALLEL_GROUP", "\n", "assert", "_MODEL_PARALLEL_GROUP", "is", "None", ",", "'model parallel group is already initialized'", "\n", "for", "i", "in", "range", "(", "world_size", "//", "model_parallel_size", ")", ":", "\n", "        ", "ranks", "=", "range", "(", "i", "*", "model_parallel_size", ",", "\n", "(", "i", "+", "1", ")", "*", "model_parallel_size", ")", "\n", "group", "=", "torch", ".", "distributed", ".", "new_group", "(", "ranks", ")", "\n", "if", "i", "==", "(", "rank", "//", "model_parallel_size", ")", ":", "\n", "            ", "_MODEL_PARALLEL_GROUP", "=", "group", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.model_parallel_is_initialized": [[81, 86], ["None"], "function", ["None"], ["", "", "", "def", "model_parallel_is_initialized", "(", ")", ":", "\n", "    ", "\"\"\"Check if model and data parallel groups are initialized.\"\"\"", "\n", "if", "_MODEL_PARALLEL_GROUP", "is", "None", "or", "_DATA_PARALLEL_GROUP", "is", "None", ":", "\n", "        ", "return", "False", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group": [[88, 93], ["None"], "function", ["None"], ["", "def", "get_model_parallel_group", "(", ")", ":", "\n", "    ", "\"\"\"Get the model parallel group the caller rank belongs to.\"\"\"", "\n", "assert", "_MODEL_PARALLEL_GROUP", "is", "not", "None", ",", "'model parallel group is not initialized'", "\n", "return", "_MODEL_PARALLEL_GROUP", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group": [[95, 100], ["None"], "function", ["None"], ["", "def", "get_data_parallel_group", "(", ")", ":", "\n", "    ", "\"\"\"Get the data parallel group the caller rank belongs to.\"\"\"", "\n", "assert", "_DATA_PARALLEL_GROUP", "is", "not", "None", ",", "'data parallel group is not initialized'", "\n", "return", "_DATA_PARALLEL_GROUP", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size": [[102, 105], ["torch.distributed.get_world_size", "initialize.get_model_parallel_group"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group"], ["", "def", "get_model_parallel_world_size", "(", ")", ":", "\n", "    ", "\"\"\"Return world size for the model parallel group.\"\"\"", "\n", "return", "torch", ".", "distributed", ".", "get_world_size", "(", "group", "=", "get_model_parallel_group", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank": [[107, 110], ["torch.distributed.get_rank", "initialize.get_model_parallel_group"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group"], ["", "def", "get_model_parallel_rank", "(", ")", ":", "\n", "    ", "\"\"\"Return my rank for the model parallel group.\"\"\"", "\n", "return", "torch", ".", "distributed", ".", "get_rank", "(", "group", "=", "get_model_parallel_group", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_src_rank": [[112, 118], ["torch.distributed.get_rank", "initialize.get_model_parallel_world_size"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size"], ["", "def", "get_model_parallel_src_rank", "(", ")", ":", "\n", "    ", "\"\"\"Calculate the global rank corresponding to a local rank zeor\n    in the model parallel group.\"\"\"", "\n", "global_rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "local_world_size", "=", "get_model_parallel_world_size", "(", ")", "\n", "return", "(", "global_rank", "//", "local_world_size", ")", "*", "local_world_size", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_world_size": [[120, 123], ["torch.distributed.get_world_size", "initialize.get_data_parallel_group"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group"], ["", "def", "get_data_parallel_world_size", "(", ")", ":", "\n", "    ", "\"\"\"Return world size for the data parallel group.\"\"\"", "\n", "return", "torch", ".", "distributed", ".", "get_world_size", "(", "group", "=", "get_data_parallel_group", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_rank": [[125, 128], ["torch.distributed.get_rank", "initialize.get_data_parallel_group"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group"], ["", "def", "get_data_parallel_rank", "(", ")", ":", "\n", "    ", "\"\"\"Return my rank for the data parallel group.\"\"\"", "\n", "return", "torch", ".", "distributed", ".", "get_rank", "(", "group", "=", "get_data_parallel_group", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel": [[130, 136], ["None"], "function", ["None"], ["", "def", "destroy_model_parallel", "(", ")", ":", "\n", "    ", "\"\"\"Set the groups to none.\"\"\"", "\n", "global", "_MODEL_PARALLEL_GROUP", "\n", "_MODEL_PARALLEL_GROUP", "=", "None", "\n", "global", "_DATA_PARALLEL_GROUP", "\n", "_DATA_PARALLEL_GROUP", "=", "None", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.data._check_data_types": [[26, 31], ["None"], "function", ["None"], ["def", "_check_data_types", "(", "keys", ",", "data", ",", "target_dtype", ")", ":", "\n", "    ", "\"\"\"Check that all the keys have the same target data type.\"\"\"", "\n", "for", "key", "in", "keys", ":", "\n", "        ", "assert", "data", "[", "key", "]", ".", "dtype", "==", "target_dtype", ",", "'{} has data type {} which '", "'is different than {}'", ".", "format", "(", "key", ",", "data", "[", "key", "]", ".", "dtype", ",", "target_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.data._build_key_size_numel_dictionaries": [[33, 74], ["torch.cuda.LongTensor", "torch.distributed.broadcast", "torch.cuda.LongTensor.cpu", "initialize.get_model_parallel_rank", "initialize.get_model_parallel_src_rank", "range", "data[].size", "enumerate", "initialize.get_model_parallel_group", "data[].size.append", "data[].dim"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_src_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "", "def", "_build_key_size_numel_dictionaries", "(", "keys", ",", "data", ")", ":", "\n", "    ", "\"\"\"Build the size on rank 0 and broadcast.\"\"\"", "\n", "max_dim", "=", "_MAX_DATA_DIM", "\n", "sizes", "=", "[", "0", "for", "_", "in", "range", "(", "max_dim", ")", "for", "_", "in", "keys", "]", "\n", "\n", "# Pack the sizes on rank zero.", "\n", "if", "get_model_parallel_rank", "(", ")", "==", "0", ":", "\n", "        ", "offset", "=", "0", "\n", "for", "key", "in", "keys", ":", "\n", "            ", "assert", "data", "[", "key", "]", ".", "dim", "(", ")", "<", "max_dim", ",", "'you should increase MAX_DATA_DIM'", "\n", "size", "=", "data", "[", "key", "]", ".", "size", "(", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "size", ")", ":", "\n", "                ", "sizes", "[", "i", "+", "offset", "]", "=", "s", "\n", "", "offset", "+=", "max_dim", "\n", "\n", "# Move to GPU and broadcast.", "\n", "", "", "sizes_cuda", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "sizes", ")", "\n", "torch", ".", "distributed", ".", "broadcast", "(", "sizes_cuda", ",", "get_model_parallel_src_rank", "(", ")", ",", "\n", "group", "=", "get_model_parallel_group", "(", ")", ")", "\n", "\n", "# Move back to cpu and unpack.", "\n", "sizes_cpu", "=", "sizes_cuda", ".", "cpu", "(", ")", "\n", "key_size", "=", "{", "}", "\n", "key_numel", "=", "{", "}", "\n", "total_numel", "=", "0", "\n", "offset", "=", "0", "\n", "for", "key", "in", "keys", ":", "\n", "        ", "i", "=", "0", "\n", "size", "=", "[", "]", "\n", "numel", "=", "1", "\n", "while", "sizes_cpu", "[", "offset", "+", "i", "]", ">", "0", ":", "\n", "            ", "this_size", "=", "sizes_cpu", "[", "offset", "+", "i", "]", "\n", "size", ".", "append", "(", "this_size", ")", "\n", "numel", "*=", "this_size", "\n", "i", "+=", "1", "\n", "", "key_size", "[", "key", "]", "=", "size", "\n", "key_numel", "[", "key", "]", "=", "numel", "\n", "total_numel", "+=", "numel", "\n", "offset", "+=", "max_dim", "\n", "\n", "", "return", "key_size", ",", "key_numel", ",", "total_numel", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.data.broadcast_data": [[76, 117], ["data._build_key_size_numel_dictionaries", "torch.distributed.broadcast", "initialize.get_model_parallel_rank", "data._check_data_types", "torch.cat().cuda", "torch.empty", "initialize.get_model_parallel_src_rank", "torch.empty.narrow().view", "initialize.get_model_parallel_group", "torch.cat", "torch.cuda.current_device", "torch.empty.narrow", "data[].contiguous().view", "data[].contiguous"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.data._build_key_size_numel_dictionaries", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.data._check_data_types", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_src_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group"], ["", "def", "broadcast_data", "(", "keys", ",", "data", ",", "datatype", ")", ":", "\n", "    ", "\"\"\"Broadcast data from rank zero of each model parallel group to the\n    members of the same model parallel group.\n\n    Arguments:\n        keys: list of keys in the data disctionary to be broadcasted\n        data: data dictionary of string keys and cpu tensor values.\n        datatype: torch data type of all tensors in data associated\n                  with keys.\n    \"\"\"", "\n", "# Build (key, size) and (key, number of elements) dictionaries along", "\n", "# with the total number of elements on all ranks.", "\n", "key_size", ",", "key_numel", ",", "total_numel", "=", "_build_key_size_numel_dictionaries", "(", "keys", ",", "\n", "data", ")", "\n", "\n", "# Pack on rank zero.", "\n", "if", "get_model_parallel_rank", "(", ")", "==", "0", ":", "\n", "# Check that all keys have the same data type.", "\n", "        ", "_check_data_types", "(", "keys", ",", "data", ",", "datatype", ")", "\n", "# Flatten the data associated with the keys", "\n", "flatten_data", "=", "torch", ".", "cat", "(", "\n", "[", "data", "[", "key", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "for", "key", "in", "keys", "]", ",", "dim", "=", "0", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "flatten_data", "=", "torch", ".", "empty", "(", "total_numel", ",", "\n", "device", "=", "torch", ".", "cuda", ".", "current_device", "(", ")", ",", "\n", "dtype", "=", "datatype", ")", "\n", "\n", "# Boradcast", "\n", "", "torch", ".", "distributed", ".", "broadcast", "(", "flatten_data", ",", "get_model_parallel_src_rank", "(", ")", ",", "\n", "group", "=", "get_model_parallel_group", "(", ")", ")", "\n", "\n", "# Unpack", "\n", "output", "=", "{", "}", "\n", "offset", "=", "0", "\n", "for", "key", "in", "keys", ":", "\n", "        ", "size", "=", "key_size", "[", "key", "]", "\n", "numel", "=", "key_numel", "[", "key", "]", "\n", "output", "[", "key", "]", "=", "flatten_data", ".", "narrow", "(", "0", ",", "offset", ",", "numel", ")", ".", "view", "(", "size", ")", "\n", "offset", "+=", "numel", "\n", "\n", "", "return", "output", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers.VocabParallelEmbedding.__init__": [[87, 116], ["super().__init__", "utils.VocabUtility.vocab_range_from_global_vocab_size", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "layers._initialize_affine_weight", "initialize.get_model_parallel_rank", "initialize.get_model_parallel_world_size", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.VocabUtility.vocab_range_from_global_vocab_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers._initialize_affine_weight", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size"], ["def", "__init__", "(", "self", ",", "num_embeddings", ",", "embedding_dim", ",", "\n", "init_method", "=", "init", ".", "xavier_normal_", ")", ":", "\n", "        ", "super", "(", "VocabParallelEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# Keep the input dimensions.", "\n", "self", ".", "num_embeddings", "=", "num_embeddings", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "# Set the detauls for compatibility.", "\n", "self", ".", "padding_idx", "=", "None", "\n", "self", ".", "max_norm", "=", "None", "\n", "self", ".", "norm_type", "=", "2.", "\n", "self", ".", "scale_grad_by_freq", "=", "False", "\n", "self", ".", "sparse", "=", "False", "\n", "self", ".", "_weight", "=", "None", "\n", "# Divide the weight matrix along the vocaburaly dimension.", "\n", "self", ".", "vocab_start_index", ",", "self", ".", "vocab_end_index", "=", "VocabUtility", ".", "vocab_range_from_global_vocab_size", "(", "\n", "self", ".", "num_embeddings", ",", "get_model_parallel_rank", "(", ")", ",", "\n", "get_model_parallel_world_size", "(", ")", ")", "\n", "self", ".", "num_embeddings_per_partition", "=", "self", ".", "vocab_end_index", "-", "self", ".", "vocab_start_index", "\n", "\n", "# Allocate weights.", "\n", "self", ".", "weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "num_embeddings_per_partition", ",", "\n", "self", ".", "embedding_dim", ")", ")", "\n", "self", ".", "weight", ".", "model_parallel", "=", "True", "\n", "# And initialize.", "\n", "_initialize_affine_weight", "(", "\n", "self", ".", "weight", ",", "self", ".", "num_embeddings", ",", "self", ".", "embedding_dim", ",", "\n", "self", ".", "num_embeddings_per_partition", ",", "0", ",", "init_method", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers.VocabParallelEmbedding.forward": [[117, 134], ["torch.embedding", "torch.embedding", "torch.embedding", "mappings.reduce_from_model_parallel_region", "input_.clone"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.reduce_from_model_parallel_region"], ["", "def", "forward", "(", "self", ",", "input_", ")", ":", "\n", "# Build the mask.", "\n", "        ", "input_mask", "=", "(", "input_", "<", "self", ".", "vocab_start_index", ")", "|", "(", "input_", ">=", "self", ".", "vocab_end_index", ")", "\n", "# Mask the input.", "\n", "masked_input", "=", "input_", ".", "clone", "(", ")", "-", "self", ".", "vocab_start_index", "\n", "masked_input", "[", "input_mask", "]", "=", "0", "\n", "# Get the embeddings.", "\n", "output_parallel", "=", "F", ".", "embedding", "(", "masked_input", ",", "self", ".", "weight", ",", "\n", "self", ".", "padding_idx", ",", "self", ".", "max_norm", ",", "\n", "self", ".", "norm_type", ",", "self", ".", "scale_grad_by_freq", ",", "\n", "self", ".", "sparse", ")", "\n", "# Mask the output embedding.", "\n", "output_parallel", "[", "input_mask", ",", ":", "]", "=", "0.0", "\n", "# Reduce across all the model parallel GPUs.", "\n", "output", "=", "reduce_from_model_parallel_region", "(", "output_parallel", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers.ParallelEmbedding.__init__": [[146, 174], ["super().__init__", "initialize.get_model_parallel_world_size", "utils.divide", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "layers._initialize_affine_weight", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers._initialize_affine_weight"], ["def", "__init__", "(", "self", ",", "num_embeddings", ",", "embedding_dim", ",", "\n", "init_method", "=", "init", ".", "xavier_normal_", ",", "\n", "keep_master_weight_for_test", "=", "False", ")", ":", "\n", "        ", "super", "(", "ParallelEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# Keep the input dimensions.", "\n", "self", ".", "num_embeddings", "=", "num_embeddings", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "# Set some detauls for compatibility.", "\n", "self", ".", "padding_idx", "=", "None", "\n", "self", ".", "max_norm", "=", "None", "\n", "self", ".", "norm_type", "=", "2.", "\n", "self", ".", "scale_grad_by_freq", "=", "False", "\n", "self", ".", "sparse", "=", "False", "\n", "self", ".", "_weight", "=", "None", "\n", "# Divide the weight matrix along the embedding dimension.", "\n", "world_size", "=", "get_model_parallel_world_size", "(", ")", "\n", "self", ".", "embedding_dim_per_partition", "=", "divide", "(", "self", ".", "embedding_dim", ",", "\n", "world_size", ")", "\n", "\n", "# Allocate weights.", "\n", "self", ".", "weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "num_embeddings", ",", "\n", "self", ".", "embedding_dim_per_partition", ")", ")", "\n", "self", ".", "weight", ".", "model_parallel", "=", "True", "\n", "# And initialize.", "\n", "_initialize_affine_weight", "(", "\n", "self", ".", "weight", ",", "self", ".", "num_embeddings", ",", "self", ".", "embedding_dim", ",", "\n", "self", ".", "embedding_dim_per_partition", ",", "1", ",", "init_method", ",", "\n", "stride", "=", "1", ",", "return_master_weight", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers.ParallelEmbedding.forward": [[175, 183], ["mappings.copy_to_model_parallel_region", "torch.embedding", "torch.embedding", "torch.embedding", "mappings.gather_from_model_parallel_region"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.copy_to_model_parallel_region", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.gather_from_model_parallel_region"], ["", "def", "forward", "(", "self", ",", "input_", ")", ":", "\n", "        ", "input_parallel", "=", "copy_to_model_parallel_region", "(", "input_", ")", "\n", "output_parallel", "=", "F", ".", "embedding", "(", "input_parallel", ",", "self", ".", "weight", ",", "\n", "self", ".", "padding_idx", ",", "self", ".", "max_norm", ",", "\n", "self", ".", "norm_type", ",", "self", ".", "scale_grad_by_freq", ",", "\n", "self", ".", "sparse", ")", "\n", "output", "=", "gather_from_model_parallel_region", "(", "output_parallel", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers.ColumnParallelLinear.__init__": [[205, 238], ["super().__init__", "initialize.get_model_parallel_world_size", "utils.divide", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "layers._initialize_affine_weight", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "layers.ColumnParallelLinear.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "layers.ColumnParallelLinear.bias.zero_"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers._initialize_affine_weight"], ["def", "__init__", "(", "self", ",", "input_size", ",", "output_size", ",", "bias", "=", "True", ",", "gather_output", "=", "True", ",", "\n", "init_method", "=", "init", ".", "xavier_normal_", ",", "stride", "=", "1", ",", "\n", "keep_master_weight_for_test", "=", "False", ")", ":", "\n", "        ", "super", "(", "ColumnParallelLinear", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Keep input parameters", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "output_size", "=", "output_size", "\n", "self", ".", "gather_output", "=", "gather_output", "\n", "# Divide the weight matrix along the last dimension.", "\n", "world_size", "=", "get_model_parallel_world_size", "(", ")", "\n", "self", ".", "output_size_per_partition", "=", "divide", "(", "output_size", ",", "world_size", ")", "\n", "\n", "# Parameters.", "\n", "# Note: torch.nn.functional.linear performs XA^T + b and as a result", "\n", "# we allocate the transpose.", "\n", "self", ".", "weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "output_size_per_partition", ",", "\n", "self", ".", "input_size", ")", ")", "\n", "self", ".", "weight", ".", "model_parallel", "=", "True", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "output_size_per_partition", ")", ")", "\n", "self", ".", "bias", ".", "model_parallel", "=", "True", "\n", "# Always initialize bias to zero.", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "self", ".", "bias", ".", "zero_", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "\n", "# Initialize weight.", "\n", "", "self", ".", "master_weight", "=", "_initialize_affine_weight", "(", "\n", "self", ".", "weight", ",", "self", ".", "output_size", ",", "self", ".", "input_size", ",", "\n", "self", ".", "output_size_per_partition", ",", "0", ",", "init_method", ",", "\n", "stride", "=", "stride", ",", "return_master_weight", "=", "keep_master_weight_for_test", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers.ColumnParallelLinear.forward": [[239, 250], ["mappings.copy_to_model_parallel_region", "torch.linear", "torch.linear", "torch.linear", "mappings.gather_from_model_parallel_region"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.copy_to_model_parallel_region", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.gather_from_model_parallel_region"], ["", "def", "forward", "(", "self", ",", "input_", ")", ":", "\n", "# Set up backprop all-reduce.", "\n", "        ", "input_parallel", "=", "copy_to_model_parallel_region", "(", "input_", ")", "\n", "# Matrix multiply.", "\n", "output_parallel", "=", "F", ".", "linear", "(", "input_parallel", ",", "self", ".", "weight", ",", "self", ".", "bias", ")", "\n", "if", "self", ".", "gather_output", ":", "\n", "# All-gather across the partitions.", "\n", "            ", "output", "=", "gather_from_model_parallel_region", "(", "output_parallel", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "output_parallel", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers.RowParallelLinear.__init__": [[278, 311], ["super().__init__", "initialize.get_model_parallel_world_size", "utils.divide", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "layers._initialize_affine_weight", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "layers.RowParallelLinear.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "layers.RowParallelLinear.bias.zero_"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers._initialize_affine_weight"], ["def", "__init__", "(", "self", ",", "input_size", ",", "output_size", ",", "bias", "=", "True", ",", "\n", "input_is_parallel", "=", "False", ",", "\n", "init_method", "=", "init", ".", "xavier_normal_", ",", "stride", "=", "1", ",", "\n", "keep_master_weight_for_test", "=", "False", ")", ":", "\n", "        ", "super", "(", "RowParallelLinear", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Keep input parameters", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "output_size", "=", "output_size", "\n", "self", ".", "input_is_parallel", "=", "input_is_parallel", "\n", "# Divide the weight matrix along the last dimension.", "\n", "world_size", "=", "get_model_parallel_world_size", "(", ")", "\n", "self", ".", "input_size_per_partition", "=", "divide", "(", "input_size", ",", "world_size", ")", "\n", "\n", "# Parameters.", "\n", "# Note: torch.nn.functional.linear performs XA^T + b and as a result", "\n", "# we allocate the transpose.", "\n", "self", ".", "weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "output_size", ",", "\n", "self", ".", "input_size_per_partition", ")", ")", "\n", "self", ".", "weight", ".", "model_parallel", "=", "True", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "output_size", ")", ")", "\n", "# Always initialize bias to zero.", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "self", ".", "bias", ".", "zero_", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "\n", "# Initialize weight.", "\n", "", "self", ".", "master_weight", "=", "_initialize_affine_weight", "(", "\n", "self", ".", "weight", ",", "self", ".", "output_size", ",", "self", ".", "input_size", ",", "\n", "self", ".", "input_size_per_partition", ",", "1", ",", "init_method", ",", "\n", "stride", "=", "stride", ",", "return_master_weight", "=", "keep_master_weight_for_test", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers.RowParallelLinear.forward": [[312, 327], ["torch.linear", "torch.linear", "torch.linear", "mappings.reduce_from_model_parallel_region", "mappings.scatter_to_model_parallel_region"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.reduce_from_model_parallel_region", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.scatter_to_model_parallel_region"], ["", "def", "forward", "(", "self", ",", "input_", ")", ":", "\n", "# Set up backprop all-reduce.", "\n", "        ", "if", "self", ".", "input_is_parallel", ":", "\n", "            ", "input_parallel", "=", "input_", "\n", "", "else", ":", "\n", "            ", "input_parallel", "=", "scatter_to_model_parallel_region", "(", "input_", ")", "\n", "# Matrix multiply.", "\n", "", "output_parallel", "=", "F", ".", "linear", "(", "input_parallel", ",", "self", ".", "weight", ")", "\n", "# All-reduce across all the partitions.", "\n", "output_", "=", "reduce_from_model_parallel_region", "(", "output_parallel", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "output", "=", "output_", "+", "self", ".", "bias", "\n", "", "else", ":", "\n", "            ", "output", "=", "output_", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers._initialize_affine_weight": [[42, 75], ["initialize.get_model_parallel_world_size", "torch.empty", "torch.empty", "torch.empty", "init_method", "utils.divide", "torch.split", "torch.split", "torch.split", "initialize.get_model_parallel_rank", "init_method", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.cat", "torch.cat", "torch.cat"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank"], ["def", "_initialize_affine_weight", "(", "weight", ",", "output_size", ",", "input_size", ",", "\n", "per_partition_size", ",", "partition_dim", ",", "init_method", ",", "\n", "stride", "=", "1", ",", "return_master_weight", "=", "False", ")", ":", "\n", "    ", "\"\"\"Initialize affine weight for model parallel.\n\n    Build the master weight on all processes and scatter\n    the relevant chunk.\"\"\"", "\n", "# If we only use 1 process for model parallelism, bypass scatter.", "\n", "world_size", "=", "get_model_parallel_world_size", "(", ")", "\n", "if", "world_size", "==", "1", ":", "\n", "        ", "init_method", "(", "weight", ")", "\n", "if", "return_master_weight", ":", "\n", "            ", "return", "weight", "\n", "", "return", "None", "\n", "\n", "# Initialize master weight", "\n", "", "master_weight", "=", "torch", ".", "empty", "(", "output_size", ",", "input_size", ",", "\n", "dtype", "=", "weight", ".", "dtype", ",", "\n", "requires_grad", "=", "False", ")", "\n", "init_method", "(", "master_weight", ")", "\n", "\n", "# Split and copy", "\n", "per_partition_per_stride_size", "=", "divide", "(", "per_partition_size", ",", "stride", ")", "\n", "weight_list", "=", "torch", ".", "split", "(", "master_weight", ",", "per_partition_per_stride_size", ",", "\n", "dim", "=", "partition_dim", ")", "\n", "rank", "=", "get_model_parallel_rank", "(", ")", "\n", "my_weight_list", "=", "weight_list", "[", "rank", ":", ":", "world_size", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "torch", ".", "cat", "(", "my_weight_list", ",", "dim", "=", "partition_dim", ",", "out", "=", "weight", ")", "\n", "", "if", "return_master_weight", ":", "\n", "        ", "return", "master_weight", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.__init__": [[127, 132], ["set"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "# Map from a string name to the cuda rng state.", "\n", "        ", "self", ".", "states_", "=", "{", "}", "\n", "# Seeds are just for book keeping and ensure no seed is set twice.", "\n", "self", ".", "seeds_", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.reset": [[133, 137], ["set"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Set to the initial state (no tracker).\"\"\"", "\n", "self", ".", "states_", "=", "{", "}", "\n", "self", ".", "seeds_", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.get_states": [[138, 145], ["None"], "methods", ["None"], ["", "def", "get_states", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get rng states. Copy the dictionary so we have direct\n        pointers to the states, not just a pointer to the dictionary.\"\"\"", "\n", "states", "=", "{", "}", "\n", "for", "name", "in", "self", ".", "states_", ":", "\n", "            ", "states", "[", "name", "]", "=", "self", ".", "states_", "[", "name", "]", "\n", "", "return", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.set_states": [[146, 150], ["None"], "methods", ["None"], ["", "def", "set_states", "(", "self", ",", "states", ")", ":", "\n", "        ", "\"\"\"Set the rng states. For efficiency purposes, we do not check\n        the size of seed for compatibility.\"\"\"", "\n", "self", ".", "states_", "=", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.add": [[151, 167], ["random.CudaRNGStatesTracker.seeds_.add", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "random._set_cuda_rng_state", "Exception", "Exception"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.add", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random._set_cuda_rng_state"], ["", "def", "add", "(", "self", ",", "name", ",", "seed", ")", ":", "\n", "        ", "\"\"\"Track the rng state.\"\"\"", "\n", "# Check seed is not already used.", "\n", "if", "seed", "in", "self", ".", "seeds_", ":", "\n", "            ", "raise", "Exception", "(", "'seed {} already exists'", ".", "format", "(", "seed", ")", ")", "\n", "", "self", ".", "seeds_", ".", "add", "(", "seed", ")", "\n", "# Check that state is not already defined.", "\n", "if", "name", "in", "self", ".", "states_", ":", "\n", "            ", "raise", "Exception", "(", "'cuda rng state {} already exists'", ".", "format", "(", "name", ")", ")", "\n", "# Get the current rng state.", "\n", "", "orig_rng_state", "=", "torch", ".", "cuda", ".", "get_rng_state", "(", ")", "\n", "# Set the new state and store it.", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "seed", ")", "\n", "self", ".", "states_", "[", "name", "]", "=", "torch", ".", "cuda", ".", "get_rng_state", "(", ")", "\n", "# Reset rng state to what it was.", "\n", "_set_cuda_rng_state", "(", "orig_rng_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.fork": [[168, 187], ["torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "random._set_cuda_rng_state", "Exception", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "random._set_cuda_rng_state"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random._set_cuda_rng_state", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random._set_cuda_rng_state"], ["", "@", "contextlib", ".", "contextmanager", "\n", "def", "fork", "(", "self", ",", "name", "=", "_MODEL_PARALLEL_RNG_TRACKER_NAME", ")", ":", "\n", "        ", "\"\"\"Fork the cuda rng state, perform operations, and exit with\n        the original state.\"\"\"", "\n", "# Check if we have added the state", "\n", "if", "name", "not", "in", "self", ".", "states_", ":", "\n", "            ", "raise", "Exception", "(", "'cuda rng state {} is not added'", ".", "format", "(", "name", ")", ")", "\n", "# Store current rng state.", "\n", "", "orig_cuda_rng_state", "=", "torch", ".", "cuda", ".", "get_rng_state", "(", ")", "\n", "# Set rng state to the desired one", "\n", "_set_cuda_rng_state", "(", "self", ".", "states_", "[", "name", "]", ")", "\n", "# Do the stuff we wanted to do.", "\n", "try", ":", "\n", "            ", "yield", "\n", "", "finally", ":", "\n", "# Update the current rng state for later use.", "\n", "            ", "self", ".", "states_", "[", "name", "]", "=", "torch", ".", "cuda", ".", "get_rng_state", "(", ")", "\n", "# And set the state to the original state we started with.", "\n", "_set_cuda_rng_state", "(", "orig_cuda_rng_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.forward": [[280, 330], ["torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "get_cuda_rng_tracker().get_states", "initialize.get_model_parallel_rank", "initialize.get_model_parallel_world_size", "initialize.get_model_parallel_group", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.Stream", "torch.cuda.Stream", "torch.cuda.Stream", "torch.cuda.Stream", "torch.cuda.Stream", "torch.cuda.Stream", "torch.cuda.Stream", "torch.cuda.Stream", "torch.cuda.Stream", "inputs.append", "item.to", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "run_function", "zip", "ctx.save_for_backward", "ctx.save_for_backward", "torch.get_rank", "torch.get_rank", "torch.get_rank", "print", "item.detach().contiguous().view().narrow().clone", "random.get_cuda_rng_tracker", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "new_args.append", "new_args.append", "arg.size", "item.detach().contiguous().view().narrow", "random.get_partition_start", "random.get_partition_size", "item.detach().contiguous().view", "item.detach().contiguous", "item.detach"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.get_states", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_partition_start", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_partition_size"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "run_function", ",", "*", "args", ")", ":", "\n", "        ", "ctx", ".", "run_function", "=", "run_function", "\n", "global", "mp_rank", ",", "mp_size", ",", "mp_group", "\n", "if", "mp_rank", "is", "None", ":", "\n", "            ", "mp_rank", "=", "get_model_parallel_rank", "(", ")", "\n", "mp_size", "=", "get_model_parallel_world_size", "(", ")", "\n", "mp_group", "=", "get_model_parallel_group", "(", ")", "\n", "\n", "\n", "", "global", "cuda_device", ",", "transport_stream", ",", "PARTITION_ACTIVATIONS", "\n", "if", "cuda_device", "is", "None", ":", "\n", "            ", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "                ", "print", "(", "f\"Partition Activations {PARTITION_ACTIVATIONS} and Correctness Check {PA_CORRECTNESS_TEST}\"", ")", "\n", "\n", "", "cuda_device", "=", "torch", ".", "cuda", ".", "current_device", "(", ")", "\n", "#The transport stream is used to overlap the allgather communication for the activations", "\n", "#with the computation in the backward pass", "\n", "transport_stream", "=", "torch", ".", "cuda", ".", "Stream", "(", "device", "=", "cuda_device", ")", "\n", "\n", "", "if", "PARTITION_ACTIVATIONS", ":", "\n", "            ", "inputs", "=", "[", "item", ".", "detach", "(", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ".", "narrow", "(", "0", ",", "get_partition_start", "(", "item", ")", ",", "get_partition_size", "(", "item", ")", ")", ".", "clone", "(", ")", "for", "item", "in", "args", "[", ":", "-", "1", "]", "]", "\n", "inputs", ".", "append", "(", "args", "[", "-", "1", "]", ")", "\n", "\n", "#just in case something funky is happening such as reuse of inputs", "\n", "", "inputs_cuda", "=", "[", "item", ".", "to", "(", "cuda_device", ")", "for", "item", "in", "args", "]", "\n", "\n", "# Copy the rng states.", "\n", "ctx", ".", "fwd_cpu_rng_state", "=", "torch", ".", "get_rng_state", "(", ")", "\n", "ctx", ".", "fwd_cuda_rng_state", "=", "torch", ".", "cuda", ".", "get_rng_state", "(", ")", "\n", "ctx", ".", "fwd_cuda_rng_state_tracker", "=", "get_cuda_rng_tracker", "(", ")", ".", "get_states", "(", ")", "\n", "\n", "#ctx.save_for_backward(*args)", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "outputs", "=", "run_function", "(", "*", "inputs_cuda", ")", "\n", "\n", "", "del", "inputs_cuda", "\n", "\n", "if", "PARTITION_ACTIVATIONS", ":", "\n", "            ", "new_args", "=", "[", "]", "\n", "for", "arg", ",", "inp", "in", "zip", "(", "args", ",", "inputs", ")", ":", "\n", "                ", "size", "=", "torch", ".", "tensor", "(", "arg", ".", "size", "(", ")", ")", "\n", "arg", ".", "data", "=", "inp", ".", "data", "\n", "new_args", ".", "append", "(", "arg", ")", "\n", "new_args", ".", "append", "(", "size", ")", "\n", "", "ctx", ".", "save_for_backward", "(", "*", "new_args", ")", "\n", "", "else", ":", "\n", "            ", "ctx", ".", "save_for_backward", "(", "*", "args", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward": [[331, 373], ["torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state", "get_cuda_rng_tracker().get_states", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "random._set_cuda_rng_state", "get_cuda_rng_tracker().set_states", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "torch.set_rng_state", "random._set_cuda_rng_state", "get_cuda_rng_tracker().set_states", "isinstance", "torch.autograd.backward", "torch.autograd.backward", "torch.autograd.backward", "torch.autograd.backward", "torch.autograd.backward", "torch.autograd.backward", "torch.autograd.backward", "torch.autograd.backward", "torch.autograd.backward", "torch.autograd._is_checkpoint_valid", "torch.autograd._is_checkpoint_valid", "torch.autograd._is_checkpoint_valid", "torch.autograd._is_checkpoint_valid", "torch.autograd._is_checkpoint_valid", "torch.autograd._is_checkpoint_valid", "torch.autograd._is_checkpoint_valid", "torch.autograd._is_checkpoint_valid", "torch.autograd._is_checkpoint_valid", "RuntimeError", "random.detach_variable", "torch.cuda.current_stream", "torch.cuda.current_stream", "torch.cuda.current_stream", "torch.cuda.current_stream", "torch.cuda.current_stream", "torch.cuda.current_stream", "torch.cuda.current_stream", "torch.cuda.current_stream", "torch.cuda.current_stream", "torch.cuda.current_stream.wait_stream", "torch.cuda.current_stream.wait_stream", "torch.cuda.current_stream.wait_stream", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "ctx.run_function", "tuple", "torch.cuda.stream", "torch.cuda.stream", "torch.cuda.stream", "torch.cuda.stream", "torch.cuda.stream", "torch.cuda.stream", "torch.cuda.stream", "torch.cuda.stream", "torch.cuda.stream", "random.get_full_inputs", "random.detach_variable", "random.get_cuda_rng_tracker", "random.get_cuda_rng_tracker", "random.get_cuda_rng_tracker"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.get_states", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random._set_cuda_rng_state", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.set_states", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random._set_cuda_rng_state", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.set_states", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.detach_variable", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_full_inputs", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.detach_variable", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "*", "args", ")", ":", "\n", "        ", "if", "not", "torch", ".", "autograd", ".", "_is_checkpoint_valid", "(", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Checkpointing is not compatible with .grad(), \"", "\n", "\"please use .backward() if possible\"", ")", "\n", "\n", "", "global", "cuda_device", ",", "transport_stream", ",", "PARTITION_ACTIVATIONS", "\n", "\n", "if", "PARTITION_ACTIVATIONS", ":", "\n", "            ", "with", "torch", ".", "cuda", ".", "stream", "(", "transport_stream", ")", ":", "\n", "                ", "inputs", "=", "get_full_inputs", "(", "ctx", ".", "saved_tensors", ")", "\n", "detached_inputs", "=", "detach_variable", "(", "inputs", ")", "\n", "", "", "else", ":", "\n", "            ", "inputs", "=", "ctx", ".", "saved_tensors", "\n", "detached_inputs", "=", "detach_variable", "(", "inputs", ")", "\n", "\n", "# Store the current states.", "\n", "", "bwd_cpu_rng_state", "=", "torch", ".", "get_rng_state", "(", ")", "\n", "bwd_cuda_rng_state", "=", "torch", ".", "cuda", ".", "get_rng_state", "(", ")", "\n", "bwd_cuda_rng_state_tracker", "=", "get_cuda_rng_tracker", "(", ")", ".", "get_states", "(", ")", "\n", "\n", "# Set the states to what it used to be before the forward pass.", "\n", "torch", ".", "set_rng_state", "(", "ctx", ".", "fwd_cpu_rng_state", ")", "\n", "_set_cuda_rng_state", "(", "ctx", ".", "fwd_cuda_rng_state", ")", "\n", "get_cuda_rng_tracker", "(", ")", ".", "set_states", "(", "ctx", ".", "fwd_cuda_rng_state_tracker", ")", "\n", "\n", "if", "PARTITION_ACTIVATIONS", ":", "\n", "            ", "current_stream", "=", "torch", ".", "cuda", ".", "current_stream", "(", ")", "\n", "current_stream", ".", "wait_stream", "(", "transport_stream", ")", "\n", "\n", "", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "            ", "outputs", "=", "ctx", ".", "run_function", "(", "*", "detached_inputs", ")", "\n", "\n", "# Set the states back to what it was at the start of this function.", "\n", "", "torch", ".", "set_rng_state", "(", "bwd_cpu_rng_state", ")", "\n", "_set_cuda_rng_state", "(", "bwd_cuda_rng_state", ")", "\n", "get_cuda_rng_tracker", "(", ")", ".", "set_states", "(", "bwd_cuda_rng_state_tracker", ")", "\n", "\n", "if", "isinstance", "(", "outputs", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "outputs", "=", "(", "outputs", ",", ")", "\n", "", "torch", ".", "autograd", ".", "backward", "(", "outputs", ",", "args", ")", "\n", "return", "(", "None", ",", ")", "+", "tuple", "(", "inp", ".", "grad", "for", "inp", "in", "detached_inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.see_memory_usage": [[35, 46], ["torch.barrier", "torch.get_rank", "print", "print", "print", "print", "print", "print", "torch.cuda.memory_allocated", "torch.cuda.memory_allocated", "torch.cuda.memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.memory_cached", "torch.cuda.memory_cached", "torch.cuda.memory_cached", "torch.cuda.max_memory_cached", "torch.cuda.max_memory_cached", "torch.cuda.max_memory_cached"], "function", ["None"], ["def", "see_memory_usage", "(", "message", ",", "force", "=", "False", ")", ":", "\n", "    ", "if", "not", "force", ":", "\n", "        ", "return", "\n", "", "dist", ".", "barrier", "(", ")", "\n", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "message", ")", "\n", "print", "(", "\"Memory Allocated \"", ",", "torch", ".", "cuda", ".", "memory_allocated", "(", ")", "/", "(", "1024", "*", "1024", "*", "1024", ")", ",", "\"GigaBytes\"", ")", "\n", "print", "(", "\"Max Memory Allocated \"", ",", "torch", ".", "cuda", ".", "max_memory_allocated", "(", ")", "/", "(", "1024", "*", "1024", "*", "1024", ")", ",", "\"GigaBytes\"", ")", "\n", "print", "(", "\"Cache Allocated \"", ",", "torch", ".", "cuda", ".", "memory_cached", "(", ")", "/", "(", "1024", "*", "1024", "*", "1024", ")", ",", "\"GigaBytes\"", ")", "\n", "print", "(", "\"Max cache Allocated \"", ",", "torch", ".", "cuda", ".", "max_memory_cached", "(", ")", "/", "(", "1024", "*", "1024", "*", "1024", ")", ",", "\"GigaBytes\"", ")", "\n", "print", "(", "\" \"", ")", "\n", "#input(\"Press Any Key To Continue ..\")", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.detach_variable": [[62, 84], ["isinstance", "tuple", "RuntimeError", "inp.to.detach", "out.append", "isinstance", "out.append", "inp.to", "type"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["def", "detach_variable", "(", "inputs", ",", "device", "=", "None", ")", ":", "\n", "    ", "if", "isinstance", "(", "inputs", ",", "tuple", ")", ":", "\n", "        ", "out", "=", "[", "]", "\n", "for", "inp", "in", "inputs", ":", "\n", "            ", "if", "not", "isinstance", "(", "inp", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "out", ".", "append", "(", "inp", ")", "\n", "continue", "\n", "\n", "", "requires_grad", "=", "inp", ".", "requires_grad", "\n", "\n", "if", "device", "is", "not", "None", ":", "\n", "                ", "x", "=", "inp", ".", "to", "(", "device", "=", "device", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "inp", "\n", "\n", "", "x", "=", "x", ".", "detach", "(", ")", "\n", "x", ".", "requires_grad", "=", "requires_grad", "\n", "out", ".", "append", "(", "x", ")", "\n", "", "return", "tuple", "(", "out", ")", "\n", "", "else", ":", "\n", "        ", "raise", "RuntimeError", "(", "\n", "\"Only tuple of tensors is supported. Got Unsupported input type: \"", ",", "type", "(", "inputs", ")", ".", "__name__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random._set_cuda_rng_state": [[85, 116], ["torch.cuda._lazy_call", "hasattr", "callable", "torch.device", "torch.device", "torch.device", "isinstance", "default_generator.set_state", "torch.cuda.device", "torch._C._cuda_setRNGState", "torch.device", "torch.device", "torch.device", "isinstance", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "torch.device", "torch.device", "torch.device"], "function", ["None"], ["", "", "def", "_set_cuda_rng_state", "(", "new_state", ",", "device", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\"Sets the random number generator state of the current GPU.\n\n    Argumentss:\n        new_state (torch.ByteTensor): The desired state\n    This function is adapted from PyTorch repo (torch.cuda.set_rng_state)\n    with a single change: the input state is not cloned. Cloning caused\n    major performance issues for +4 GPU cases.\n    \"\"\"", "\n", "if", "hasattr", "(", "_C", ",", "'_cuda_setRNGState'", ")", "and", "callable", "(", "_C", ".", "_cuda_setRNGState", ")", ":", "\n", "# older PyTorch", "\n", "        ", "def", "cb", "(", ")", ":", "\n", "            ", "with", "device_ctx_manager", "(", "device", ")", ":", "\n", "                ", "_C", ".", "_cuda_setRNGState", "(", "new_state", ")", "\n", "", "", "", "else", ":", "\n", "# newer PyTorch", "\n", "        ", "if", "device", "==", "-", "1", ":", "\n", "            ", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "", "elif", "isinstance", "(", "device", ",", "str", ")", ":", "\n", "            ", "device", "=", "torch", ".", "device", "(", "device", ")", "\n", "", "elif", "isinstance", "(", "device", ",", "int", ")", ":", "\n", "            ", "device", "=", "torch", ".", "device", "(", "'cuda'", ",", "device", ")", "\n", "\n", "", "def", "cb", "(", ")", ":", "\n", "            ", "idx", "=", "device", ".", "index", "\n", "if", "idx", "is", "None", ":", "\n", "                ", "idx", "=", "torch", ".", "cuda", ".", "current_device", "(", ")", "\n", "", "default_generator", "=", "torch", ".", "cuda", ".", "default_generators", "[", "idx", "]", "\n", "default_generator", ".", "set_state", "(", "new_state", ")", "\n", "\n", "", "", "_lazy_call", "(", "cb", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker": [[193, 196], ["None"], "function", ["None"], ["def", "get_cuda_rng_tracker", "(", ")", ":", "\n", "    ", "\"\"\"Get cuda rng tracker.\"\"\"", "\n", "return", "_CUDA_RNG_STATE_TRACKER", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.model_parallel_cuda_manual_seed": [[198, 234], ["_CUDA_RNG_STATE_TRACKER.reset", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "_CUDA_RNG_STATE_TRACKER.add", "initialize.get_model_parallel_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "initialize.get_model_parallel_rank", "initialize.get_data_parallel_rank"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.reset", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.add", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_rank"], ["", "def", "model_parallel_cuda_manual_seed", "(", "seed", ")", ":", "\n", "    ", "\"\"\"Initialize model parallel cuda seed.\n\n    This function should be called after the model parallel is\n    initialized. Also, no torch.cuda.manual_seed should be called\n    after this function. Basically, this is replacement for that\n    function.\n    Two set of RNG states are tracked:\n        default state: This is for data parallelism and is the same among a\n                       set of model parallel GPUs but different across\n                       different model paralle groups. This is used for\n                       example for dropout in the non-model-parallel regions.\n        model-parallel state: This state is different among a set of model\n                              parallel GPUs, but the same across data parallel\n                              groups. This is used for example for dropout in\n                              model parallel regions.\n    \"\"\"", "\n", "# 2718 is just for fun and any POSITIVE value will work.", "\n", "offset", "=", "seed", "+", "2718", "\n", "model_parallel_seed", "=", "offset", "+", "get_model_parallel_rank", "(", ")", "\n", "# Data parallel gets the original sedd.", "\n", "data_parallel_seed", "=", "seed", "\n", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> initializing model parallel cuda seeds on global rank {}, '", "\n", "'model parallel rank {}, and data parallel rank {} with '", "\n", "'model parallel seed: {} and data parallel seed: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "get_model_parallel_rank", "(", ")", ",", "\n", "get_data_parallel_rank", "(", ")", ",", "model_parallel_seed", ",", "\n", "data_parallel_seed", ")", ",", "flush", "=", "True", ")", "\n", "", "_CUDA_RNG_STATE_TRACKER", ".", "reset", "(", ")", "\n", "# Set the default state.", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "data_parallel_seed", ")", "\n", "# and model parallel state.", "\n", "_CUDA_RNG_STATE_TRACKER", ".", "add", "(", "_MODEL_PARALLEL_RNG_TRACKER_NAME", ",", "\n", "model_parallel_seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_partition_start": [[236, 241], ["random.get_partition_size", "int"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_partition_size"], ["", "def", "get_partition_start", "(", "item", ")", ":", "\n", "    ", "global", "mp_rank", ",", "mp_size", ",", "mp_group", "\n", "partition_size", "=", "get_partition_size", "(", "item", ")", "\n", "start", "=", "partition_size", "*", "mp_rank", "\n", "return", "int", "(", "start", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_partition_size": [[242, 247], ["item.numel", "int"], "function", ["None"], ["", "def", "get_partition_size", "(", "item", ")", ":", "\n", "    ", "global", "mp_rank", ",", "mp_size", ",", "mp_group", "\n", "size", "=", "item", ".", "numel", "(", ")", "\n", "partition_size", "=", "size", "/", "mp_size", "\n", "return", "int", "(", "partition_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_full_inputs": [[248, 270], ["range", "inputs.append", "tuple", "item.numel", "torch.zeros", "torch.zeros", "torch.zeros", "range", "torch.all_gather", "torch.zeros.view", "inputs.append", "int", "torch.zeros.narrow", "partitions.append", "list", "flat_tensor.narrow.copy_", "size.numpy", "len"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "get_full_inputs", "(", "tensors", ")", ":", "\n", "    ", "inputs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "int", "(", "len", "(", "tensors", ")", "/", "2", ")", "-", "1", ")", ":", "\n", "        ", "item", "=", "tensors", "[", "2", "*", "i", "]", "\n", "size", "=", "tensors", "[", "2", "*", "i", "+", "1", "]", "\n", "partition_size", "=", "item", ".", "numel", "(", ")", "\n", "tensor_size", "=", "partition_size", "*", "mp_size", "\n", "flat_tensor", "=", "torch", ".", "zeros", "(", "[", "tensor_size", "]", ",", "dtype", "=", "item", ".", "dtype", ",", "device", "=", "item", ".", "device", ")", "\n", "partitions", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "mp_size", ")", ":", "\n", "            ", "part_i", "=", "flat_tensor", ".", "narrow", "(", "0", ",", "partition_size", "*", "i", ",", "partition_size", ")", "\n", "if", "i", "==", "mp_rank", ":", "\n", "                ", "part_i", ".", "copy_", "(", "item", ")", "\n", "", "partitions", ".", "append", "(", "part_i", ")", "\n", "", "dist", ".", "all_gather", "(", "partitions", ",", "partitions", "[", "mp_rank", "]", ",", "group", "=", "mp_group", ")", "\n", "input_tensor", "=", "flat_tensor", ".", "view", "(", "list", "(", "size", ".", "numpy", "(", ")", ")", ")", "\n", "item", ".", "data", "=", "input_tensor", ".", "data", "\n", "\n", "inputs", ".", "append", "(", "item", ")", "\n", "", "inputs", ".", "append", "(", "tensors", "[", "-", "2", "]", ")", "\n", "\n", "return", "tuple", "(", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.checkpoint": [[375, 379], ["CheckpointFunction.apply"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.configure_data.DataConfig.apply"], ["", "", "def", "checkpoint", "(", "function", ",", "*", "args", ")", ":", "\n", "    ", "\"\"\"Checkpoint a model or part of the model.\n    This has been directly copied from torch.utils.checkpoint.\"\"\"", "\n", "return", "CheckpointFunction", ".", "apply", "(", "function", ",", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.partition_activations_in_checkpoint": [[380, 385], ["torch.get_rank", "print"], "function", ["None"], ["", "def", "partition_activations_in_checkpoint", "(", "partition_activation", ")", ":", "\n", "    ", "global", "PARTITION_ACTIVATIONS", "\n", "PARTITION_ACTIVATIONS", "=", "partition_activation", "\n", "if", "dist", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "f\"**************Partition Activations {PARTITION_ACTIVATIONS}************\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.GPT2ParallelSelfAttention.__init__": [[64, 99], ["super().__init__", "initialize.get_model_parallel_world_size", "utils.divide", "utils.divide", "utils.divide", "layers.ColumnParallelLinear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "layers.RowParallelLinear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "deepspeed.checkpointing.is_configured"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide"], ["def", "__init__", "(", "self", ",", "hidden_size", ",", "num_attention_heads", ",", "\n", "attention_dropout_prob", ",", "output_dropout_prob", ",", "\n", "init_method", ",", "output_layer_init_method", "=", "None", ")", ":", "\n", "        ", "super", "(", "GPT2ParallelSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# Set output layer initialization if not provided.", "\n", "if", "output_layer_init_method", "is", "None", ":", "\n", "            ", "output_layer_init_method", "=", "init_method", "\n", "# Per attention head and per partition values.", "\n", "", "world_size", "=", "get_model_parallel_world_size", "(", ")", "\n", "self", ".", "hidden_size_per_partition", "=", "divide", "(", "hidden_size", ",", "world_size", ")", "\n", "self", ".", "hidden_size_per_attention_head", "=", "divide", "(", "hidden_size", ",", "\n", "num_attention_heads", ")", "\n", "self", ".", "num_attention_heads_per_partition", "=", "divide", "(", "num_attention_heads", ",", "\n", "world_size", ")", "\n", "# Strided linear layer.", "\n", "self", ".", "query_key_value", "=", "ColumnParallelLinear", "(", "hidden_size", ",", "3", "*", "hidden_size", ",", "\n", "stride", "=", "3", ",", "\n", "gather_output", "=", "False", ",", "\n", "init_method", "=", "init_method", ")", "\n", "# Dropout. Note that for a single iteration, this layer will generate", "\n", "# different outputs on different number of parallel partitions but", "\n", "# on average it should not be partition dependent.", "\n", "self", ".", "attention_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "attention_dropout_prob", ")", "\n", "\n", "# Output.", "\n", "self", ".", "dense", "=", "RowParallelLinear", "(", "hidden_size", ",", "\n", "hidden_size", ",", "\n", "input_is_parallel", "=", "True", ",", "\n", "init_method", "=", "output_layer_init_method", ")", "\n", "self", ".", "output_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "output_dropout_prob", ")", "\n", "\n", "if", "deepspeed", ".", "checkpointing", ".", "is_configured", "(", ")", ":", "\n", "            ", "global", "get_cuda_rng_tracker", ",", "checkpoint", "\n", "get_cuda_rng_tracker", "=", "deepspeed", ".", "checkpointing", ".", "get_cuda_rng_tracker", "\n", "checkpoint", "=", "deepspeed", ".", "checkpointing", ".", "checkpoint", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.GPT2ParallelSelfAttention._transpose_for_scores": [[101, 110], ["tensor.view.view.view", "tensor.view.view.permute", "tensor.view.view.size"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "", "def", "_transpose_for_scores", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "\"\"\"Transpose a 3D tensor [b, s, np*hn] into a 4D tensor with\n        size [b, np, s, hn].\n        \"\"\"", "\n", "new_tensor_shape", "=", "tensor", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads_per_partition", ",", "\n", "self", ".", "hidden_size_per_attention_head", ")", "\n", "tensor", "=", "tensor", ".", "view", "(", "*", "new_tensor_shape", ")", "\n", "return", "tensor", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.GPT2ParallelSelfAttention.forward": [[111, 175], ["transformer.GPT2ParallelSelfAttention.query_key_value", "utils.split_tensor_along_last_dim", "transformer.GPT2ParallelSelfAttention._transpose_for_scores", "transformer.GPT2ParallelSelfAttention._transpose_for_scores", "transformer.GPT2ParallelSelfAttention._transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "transformer.GPT2ParallelSelfAttention.dense", "transformer.GPT2ParallelSelfAttention.output_dropout", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat.transpose", "torch.cat.transpose", "math.sqrt", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "random.get_cuda_rng_tracker().fork", "transformer.GPT2ParallelSelfAttention.attention_dropout", "torch.cat().half", "torch.cat().half", "torch.cat().half", "torch.cat().half", "torch.cat().half", "torch.cat().half", "torch.cat().half", "torch.cat().half", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "context_layer.view.view.permute", "context_layer.view.view.size", "random.get_cuda_rng_tracker", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.split_tensor_along_last_dim", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelSelfAttention._transpose_for_scores", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelSelfAttention._transpose_for_scores", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelSelfAttention._transpose_for_scores", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.fork", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "ltor_mask", ",", "layer_past", "=", "None", ",", "use_cache", "=", "False", ")", ":", "\n", "# hidden_states: [b, s, h]", "\n", "# ltor_mask: [1, 1, s, s]", "\n", "\n", "# Attention heads. [b, s, hp]", "\n", "        ", "mixed_x_layer", "=", "self", ".", "query_key_value", "(", "hidden_states", ")", "\n", "(", "mixed_query_layer", ",", "\n", "mixed_key_layer", ",", "\n", "mixed_value_layer", ")", "=", "split_tensor_along_last_dim", "(", "mixed_x_layer", ",", "3", ")", "\n", "\n", "\n", "# Reshape and transpose [b, np, s, hn]", "\n", "query_layer", "=", "self", ".", "_transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "_transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "_transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "if", "layer_past", "is", "not", "None", ":", "\n", "            ", "past_key", ",", "past_value", "=", "layer_past", "[", "0", "]", ",", "layer_past", "[", "1", "]", "\n", "if", "key_layer", ".", "dtype", "==", "torch", ".", "half", ":", "\n", "                ", "key_layer", "=", "torch", ".", "cat", "(", "(", "past_key", ",", "key_layer", ")", ",", "dim", "=", "-", "2", ")", ".", "half", "(", ")", "\n", "value_layer", "=", "torch", ".", "cat", "(", "(", "past_value", ",", "value_layer", ")", ",", "dim", "=", "-", "2", ")", ".", "half", "(", ")", "\n", "", "else", ":", "\n", "                ", "key_layer", "=", "torch", ".", "cat", "(", "(", "past_key", ",", "key_layer", ")", ",", "dim", "=", "-", "2", ")", "\n", "value_layer", "=", "torch", ".", "cat", "(", "(", "past_value", ",", "value_layer", ")", ",", "dim", "=", "-", "2", ")", "\n", "\n", "\n", "", "", "if", "use_cache", ":", "\n", "            ", "present", "=", "torch", ".", "stack", "(", "(", "key_layer", ",", "value_layer", ")", ")", "\n", "", "else", ":", "\n", "            ", "present", "=", "None", "\n", "\n", "# Raw attention scores. [b, np, s, s]", "\n", "", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "\n", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "\n", "self", ".", "hidden_size_per_attention_head", ")", "\n", "# Apply the left to right attention mask.", "\n", "attention_scores", "=", "torch", ".", "mul", "(", "attention_scores", ",", "ltor_mask", ")", "-", "10000.0", "*", "(", "1.0", "-", "ltor_mask", ")", "\n", "\n", "# Attention probabilities. [b, np, s, s]", "\n", "attention_probs", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "with", "get_cuda_rng_tracker", "(", ")", ".", "fork", "(", ")", ":", "\n", "            ", "attention_probs", "=", "self", ".", "attention_dropout", "(", "attention_probs", ")", "\n", "\n", "# Context layer.", "\n", "# [b, np, s, hn]", "\n", "", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "# [b, s, np, hn]", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "hidden_size_per_partition", ",", ")", "\n", "# [b, s, hp]", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "# Output. [b, s, h]", "\n", "output", "=", "self", ".", "dense", "(", "context_layer", ")", "\n", "output", "=", "self", ".", "output_dropout", "(", "output", ")", "\n", "\n", "output", "=", "[", "output", ",", "present", "]", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.GPT2ParallelMLP.__init__": [[206, 223], ["super().__init__", "layers.ColumnParallelLinear", "layers.RowParallelLinear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["def", "__init__", "(", "self", ",", "hidden_size", ",", "output_dropout_prob", ",", "init_method", ",", "\n", "output_layer_init_method", "=", "None", ")", ":", "\n", "        ", "super", "(", "GPT2ParallelMLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# Set output layer initialization if not provided.", "\n", "if", "output_layer_init_method", "is", "None", ":", "\n", "            ", "output_layer_init_method", "=", "init_method", "\n", "# Project to 4h.", "\n", "", "self", ".", "dense_h_to_4h", "=", "ColumnParallelLinear", "(", "hidden_size", ",", "4", "*", "hidden_size", ",", "\n", "gather_output", "=", "False", ",", "\n", "init_method", "=", "init_method", ")", "\n", "# Project back to h.", "\n", "self", ".", "dense_4h_to_h", "=", "RowParallelLinear", "(", "\n", "4", "*", "hidden_size", ",", "\n", "hidden_size", ",", "\n", "input_is_parallel", "=", "True", ",", "\n", "init_method", "=", "output_layer_init_method", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "output_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.GPT2ParallelMLP.forward": [[224, 233], ["transformer.GPT2ParallelMLP.dense_h_to_4h", "transformer.gelu", "transformer.GPT2ParallelMLP.dense_4h_to_h", "transformer.GPT2ParallelMLP.dropout"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.gelu"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# [b, s, 4hp]", "\n", "        ", "intermediate_parallel", "=", "self", ".", "dense_h_to_4h", "(", "hidden_states", ")", "\n", "intermediate_parallel", "=", "gelu", "(", "intermediate_parallel", ")", "\n", "\n", "# [b, s, h]", "\n", "output", "=", "self", ".", "dense_4h_to_h", "(", "intermediate_parallel", ")", "\n", "output", "=", "self", ".", "dropout", "(", "output", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.GPT2ParallelTransformerLayer.__init__": [[263, 298], ["super().__init__", "apex.normalization.fused_layer_norm.FusedLayerNorm", "transformer.GPT2ParallelSelfAttention", "apex.normalization.fused_layer_norm.FusedLayerNorm", "transformer.GPT2ParallelMLP"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "hidden_size", ",", "\n", "num_attention_heads", ",", "\n", "attention_dropout_prob", ",", "\n", "output_dropout_prob", ",", "\n", "layernorm_epsilon", ",", "\n", "init_method", ",", "\n", "output_layer_init_method", "=", "None", ")", ":", "\n", "        ", "super", "(", "GPT2ParallelTransformerLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# Set output layer initialization if not provided.", "\n", "if", "output_layer_init_method", "is", "None", ":", "\n", "            ", "output_layer_init_method", "=", "init_method", "\n", "\n", "# Layernorm on the input data.", "\n", "", "self", ".", "input_layernorm", "=", "LayerNorm", "(", "hidden_size", ",", "eps", "=", "layernorm_epsilon", ")", "\n", "\n", "# Self attention.", "\n", "self", ".", "attention", "=", "GPT2ParallelSelfAttention", "(", "\n", "hidden_size", ",", "\n", "num_attention_heads", ",", "\n", "attention_dropout_prob", ",", "\n", "output_dropout_prob", ",", "\n", "init_method", ",", "\n", "output_layer_init_method", "=", "output_layer_init_method", ")", "\n", "\n", "# Layernorm on the input data.", "\n", "self", ".", "post_attention_layernorm", "=", "LayerNorm", "(", "hidden_size", ",", "\n", "eps", "=", "layernorm_epsilon", ")", "\n", "\n", "# MLP", "\n", "self", ".", "mlp", "=", "GPT2ParallelMLP", "(", "\n", "hidden_size", ",", "\n", "output_dropout_prob", ",", "\n", "init_method", ",", "\n", "output_layer_init_method", "=", "output_layer_init_method", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.GPT2ParallelTransformerLayer.forward": [[299, 319], ["transformer.GPT2ParallelTransformerLayer.input_layernorm", "transformer.GPT2ParallelTransformerLayer.attention", "transformer.GPT2ParallelTransformerLayer.post_attention_layernorm", "transformer.GPT2ParallelTransformerLayer.mlp"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "ltor_mask", ",", "layer_past", "=", "None", ",", "use_cache", "=", "False", ")", ":", "\n", "# hidden_states: [b, s, h]", "\n", "# ltor_mask: [1, 1, s, s]", "\n", "\n", "# Layer norm at the begining of the transformer layer.", "\n", "        ", "layernorm_output", "=", "self", ".", "input_layernorm", "(", "hidden_states", ")", "\n", "# Self attention.", "\n", "attention_output", ",", "present", "=", "self", ".", "attention", "(", "layernorm_output", ",", "ltor_mask", ",", "layer_past", "=", "layer_past", ",", "use_cache", "=", "use_cache", ")", "\n", "# Residual connection.", "\n", "layernorm_input", "=", "hidden_states", "+", "attention_output", "\n", "# Layer norm post the self attention.", "\n", "layernorm_output", "=", "self", ".", "post_attention_layernorm", "(", "layernorm_input", ")", "\n", "# MLP.", "\n", "mlp_output", "=", "self", ".", "mlp", "(", "layernorm_output", ")", "\n", "# Second residual connection.", "\n", "output", "=", "layernorm_input", "+", "mlp_output", "\n", "\n", "output", "=", "[", "output", ",", "present", "]", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.GPT2ParallelTransformer.__init__": [[372, 413], ["super().__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "apex.normalization.fused_layer_norm.FusedLayerNorm", "deepspeed.checkpointing.is_configured", "transformer.scaled_init_method", "transformer.GPT2ParallelTransformerLayer", "transformer.unscaled_init_method", "transformer.GPT2ParallelTransformer.__init__.get_layer"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.scaled_init_method", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.unscaled_init_method"], ["def", "__init__", "(", "self", ",", "\n", "num_layers", ",", "\n", "hidden_size", ",", "\n", "num_attention_heads", ",", "\n", "attention_dropout_prob", ",", "\n", "output_dropout_prob", ",", "\n", "checkpoint_activations", ",", "\n", "checkpoint_num_layers", "=", "1", ",", "\n", "layernorm_epsilon", "=", "1.0e-5", ",", "\n", "init_method_std", "=", "0.02", ",", "\n", "use_scaled_init_for_output_weights", "=", "True", ")", ":", "\n", "        ", "super", "(", "GPT2ParallelTransformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# Store activation checkpoiting flag.", "\n", "self", ".", "checkpoint_activations", "=", "checkpoint_activations", "\n", "self", ".", "checkpoint_num_layers", "=", "checkpoint_num_layers", "\n", "\n", "output_layer_init_method", "=", "None", "\n", "if", "use_scaled_init_for_output_weights", ":", "\n", "            ", "output_layer_init_method", "=", "scaled_init_method", "(", "init_method_std", ",", "\n", "num_layers", ")", "\n", "", "def", "get_layer", "(", ")", ":", "\n", "            ", "return", "GPT2ParallelTransformerLayer", "(", "\n", "hidden_size", ",", "\n", "num_attention_heads", ",", "\n", "attention_dropout_prob", ",", "\n", "output_dropout_prob", ",", "\n", "layernorm_epsilon", ",", "\n", "unscaled_init_method", "(", "init_method_std", ")", ",", "\n", "output_layer_init_method", "=", "output_layer_init_method", ")", "\n", "\n", "# Transformer layers.", "\n", "", "self", ".", "layers", "=", "torch", ".", "nn", ".", "ModuleList", "(", "\n", "[", "get_layer", "(", ")", "for", "_", "in", "range", "(", "num_layers", ")", "]", ")", "\n", "\n", "# Final layer norm before output.", "\n", "self", ".", "final_layernorm", "=", "LayerNorm", "(", "hidden_size", ",", "eps", "=", "layernorm_epsilon", ")", "\n", "\n", "if", "deepspeed", ".", "checkpointing", ".", "is_configured", "(", ")", ":", "\n", "            ", "global", "get_cuda_rng_tracker", ",", "checkpoint", "\n", "get_cuda_rng_tracker", "=", "deepspeed", ".", "checkpointing", ".", "get_cuda_rng_tracker", "\n", "checkpoint", "=", "deepspeed", ".", "checkpointing", ".", "checkpoint", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.GPT2ParallelTransformer.forward": [[415, 457], ["transformer.GPT2ParallelTransformer.final_layernorm", "[].size", "len", "enumerate", "len", "random.checkpoint", "zip", "layer", "layer", "transformer.GPT2ParallelTransformer.forward.custom"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.checkpoint"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "past_key_values", "=", "None", ",", "use_cache", "=", "False", ")", ":", "\n", "\n", "        ", "def", "custom", "(", "start", ",", "end", ")", ":", "\n", "            ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                ", "layers_", "=", "self", ".", "layers", "[", "start", ":", "end", "]", "\n", "x_", "=", "inputs", "[", "0", "]", "\n", "for", "layer", "in", "layers_", ":", "\n", "                    ", "x_", "=", "layer", "(", "x_", ",", "inputs", "[", "1", "]", ")", "\n", "", "return", "x_", "\n", "", "return", "custom_forward", "\n", "\n", "", "if", "past_key_values", "is", "None", ":", "\n", "            ", "past_length", "=", "0", "\n", "past_key_values", "=", "[", "None", "]", "*", "len", "(", "self", ".", "layers", ")", "\n", "", "else", ":", "\n", "            ", "past_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "size", "(", "-", "2", ")", "\n", "\n", "", "presents", "=", "[", "]", "\n", "all_hidden_states", "=", "[", "]", "\n", "\n", "if", "self", ".", "checkpoint_activations", ":", "\n", "            ", "l", "=", "0", "\n", "num_layers", "=", "len", "(", "self", ".", "layers", ")", "\n", "chunk_length", "=", "self", ".", "checkpoint_num_layers", "\n", "while", "l", "<", "num_layers", ":", "\n", "                ", "hidden_states", "=", "checkpoint", "(", "custom", "(", "l", ",", "l", "+", "chunk_length", ")", ",", "\n", "hidden_states", ",", "attention_mask", ")", "\n", "l", "+=", "chunk_length", "\n", "", "", "else", ":", "\n", "            ", "for", "i", ",", "(", "layer", ",", "layer_past", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "layers", ",", "past_key_values", ")", ")", ":", "\n", "#all_hidden_states += [hidden_states,]", "\n", "#all_hidden_states += [hidden_states.view(hidden_states.size()),]", "\n", "                ", "hidden_states", ",", "present", "=", "layer", "(", "hidden_states", ",", "attention_mask", ",", "layer_past", "=", "layer_past", ",", "use_cache", "=", "use_cache", ")", "\n", "if", "use_cache", ":", "\n", "                    ", "presents", "+=", "[", "present", ",", "]", "\n", "\n", "# Final layer norm.", "\n", "", "", "", "output", "=", "self", ".", "final_layernorm", "(", "hidden_states", ")", "\n", "\n", "output", "=", "[", "output", ",", "presents", "]", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelSelfAttention.__init__": [[484, 514], ["super().__init__", "initialize.get_model_parallel_world_size", "utils.divide", "utils.divide", "utils.divide", "layers.ColumnParallelLinear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "deepspeed.checkpointing.is_configured"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide"], ["def", "__init__", "(", "self", ",", "hidden_size", ",", "num_attention_heads", ",", "\n", "dropout_prob", ",", "output_parallel", "=", "False", ",", "\n", "init_method", "=", "init", ".", "xavier_normal_", ")", ":", "\n", "        ", "super", "(", "BertParallelSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# Input configuration.", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "dropout_prob", "=", "dropout_prob", "\n", "self", ".", "output_parallel", "=", "output_parallel", "\n", "# Per attention head and per partition values.", "\n", "world_size", "=", "get_model_parallel_world_size", "(", ")", "\n", "self", ".", "hidden_size_per_partition", "=", "divide", "(", "hidden_size", ",", "world_size", ")", "\n", "self", ".", "hidden_size_per_attention_head", "=", "divide", "(", "hidden_size", ",", "\n", "num_attention_heads", ")", "\n", "self", ".", "num_attention_heads_per_partition", "=", "divide", "(", "num_attention_heads", ",", "\n", "world_size", ")", "\n", "# Strided linear layer.", "\n", "self", ".", "query_key_value", "=", "ColumnParallelLinear", "(", "hidden_size", ",", "3", "*", "hidden_size", ",", "\n", "stride", "=", "3", ",", "\n", "gather_output", "=", "False", ",", "\n", "init_method", "=", "init_method", ")", "\n", "# Dropout. Note that for a single iteration, this layer will generate", "\n", "# different outputs on different number of parallel partitions but", "\n", "# on average it should not be partition dependent.", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout_prob", ")", "\n", "\n", "if", "deepspeed", ".", "checkpointing", ".", "is_configured", "(", ")", ":", "\n", "            ", "global", "get_cuda_rng_tracker", ",", "checkpoint", "\n", "get_cuda_rng_tracker", "=", "deepspeed", ".", "checkpointing", ".", "get_cuda_rng_tracker", "\n", "checkpoint", "=", "deepspeed", ".", "checkpointing", ".", "checkpoint", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelSelfAttention._transpose_for_scores": [[516, 525], ["tensor.view.view.view", "tensor.view.view.permute", "tensor.view.view.size"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "", "def", "_transpose_for_scores", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "\"\"\"Transpose a 3D tensor [b, s, np*hn] into a 4D tensor with\n        size [b, np, s, hn].\n        \"\"\"", "\n", "new_tensor_shape", "=", "tensor", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads_per_partition", ",", "\n", "self", ".", "hidden_size_per_attention_head", ")", "\n", "tensor", "=", "tensor", ".", "view", "(", "*", "new_tensor_shape", ")", "\n", "return", "tensor", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelSelfAttention.forward": [[526, 570], ["transformer.BertParallelSelfAttention.query_key_value", "utils.split_tensor_along_last_dim", "transformer.BertParallelSelfAttention._transpose_for_scores", "transformer.BertParallelSelfAttention._transpose_for_scores", "transformer.BertParallelSelfAttention._transpose_for_scores", "math.sqrt", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "math.sqrt", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "random.get_cuda_rng_tracker().fork", "transformer.BertParallelSelfAttention.dropout", "mappings.gather_from_model_parallel_region", "transformer.BertParallelSelfAttention.transpose", "context_layer.view.view.permute", "context_layer.view.view.size", "random.get_cuda_rng_tracker"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.split_tensor_along_last_dim", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelSelfAttention._transpose_for_scores", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelSelfAttention._transpose_for_scores", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelSelfAttention._transpose_for_scores", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.fork", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.gather_from_model_parallel_region", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "\n", "# Attention heads. [b, s, hp]", "\n", "        ", "mixed_x_layer", "=", "self", ".", "query_key_value", "(", "hidden_states", ")", "\n", "(", "mixed_query_layer", ",", "\n", "mixed_key_layer", ",", "\n", "mixed_value_layer", ")", "=", "split_tensor_along_last_dim", "(", "mixed_x_layer", ",", "3", ")", "\n", "\n", "# Reshape and transpose [b, np, s, hn]", "\n", "query_layer", "=", "self", ".", "_transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "_transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "_transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Raw attention scores. [b, np, s, s]", "\n", "norm_factor", "=", "math", ".", "sqrt", "(", "math", ".", "sqrt", "(", "self", ".", "hidden_size_per_attention_head", ")", ")", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", "/", "norm_factor", ",", "\n", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "/", "norm_factor", ")", "\n", "# Apply the attention mask.", "\n", "attention_scores", "+=", "attention_mask", "\n", "\n", "# Attention probabilities. [b, np, s, s]", "\n", "attention_probs", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "with", "get_cuda_rng_tracker", "(", ")", ".", "fork", "(", ")", ":", "\n", "            ", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "# Context layer.", "\n", "# [b, np, s, hn]", "\n", "", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "# [b, s, np, hn]", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "hidden_size_per_partition", ",", ")", "\n", "# [b, s, hp]", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "# Output. [b, s, h]", "\n", "if", "self", ".", "output_parallel", ":", "\n", "            ", "output", "=", "context_layer", "\n", "", "else", ":", "\n", "            ", "output", "=", "gather_from_model_parallel_region", "(", "context_layer", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelTransformerOutput.__init__": [[575, 586], ["super().__init__", "layers.RowParallelLinear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "apex.normalization.fused_layer_norm.FusedLayerNorm"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "output_size", ",", "dropout_prob", ",", "\n", "layernorm_epsilon", "=", "1.0e-12", ",", "input_is_parallel", "=", "False", ",", "\n", "init_method", "=", "init", ".", "xavier_normal_", ")", ":", "\n", "        ", "super", "(", "BertParallelTransformerOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# Components.", "\n", "self", ".", "dense", "=", "RowParallelLinear", "(", "input_size", ",", "\n", "output_size", ",", "\n", "input_is_parallel", "=", "input_is_parallel", ",", "\n", "init_method", "=", "init_method", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout_prob", ")", "\n", "self", ".", "layernorm", "=", "LayerNorm", "(", "output_size", ",", "eps", "=", "layernorm_epsilon", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelTransformerOutput.forward": [[587, 593], ["transformer.BertParallelTransformerOutput.dense", "transformer.BertParallelTransformerOutput.dropout", "transformer.BertParallelTransformerOutput.layernorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "layernorm_input", "=", "hidden_states", "+", "input_tensor", "\n", "hidden_states", "=", "self", ".", "layernorm", "(", "layernorm_input", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelTransformerLayer.__init__": [[626, 660], ["super().__init__", "transformer.BertParallelSelfAttention", "transformer.BertParallelTransformerOutput", "layers.ColumnParallelLinear", "transformer.BertParallelTransformerOutput"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "hidden_size", ",", "\n", "intermediate_size", ",", "\n", "num_attention_heads", ",", "\n", "attention_dropout_prob", ",", "\n", "output_dropout_prob", ",", "\n", "intermediate_activation_fn", ",", "\n", "layernorm_epsilon", ",", "\n", "init_method", "=", "init", ".", "xavier_normal_", ")", ":", "\n", "        ", "super", "(", "BertParallelTransformerLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Self attention.", "\n", "self", ".", "attention", "=", "BertParallelSelfAttention", "(", "hidden_size", ",", "\n", "num_attention_heads", ",", "\n", "attention_dropout_prob", ",", "\n", "output_parallel", "=", "True", ",", "\n", "init_method", "=", "init_method", ")", "\n", "# Self attention output.", "\n", "self", ".", "self_output", "=", "BertParallelTransformerOutput", "(", "\n", "hidden_size", ",", "hidden_size", ",", "output_dropout_prob", ",", "\n", "layernorm_epsilon", "=", "layernorm_epsilon", ",", "\n", "input_is_parallel", "=", "True", ",", "\n", "init_method", "=", "init_method", ")", "\n", "# Intermediate.", "\n", "self", ".", "intermediate", "=", "ColumnParallelLinear", "(", "hidden_size", ",", "intermediate_size", ",", "\n", "gather_output", "=", "False", ",", "\n", "init_method", "=", "init_method", ")", "\n", "self", ".", "intermediate_activation_fn", "=", "intermediate_activation_fn", "\n", "# Output.", "\n", "self", ".", "output", "=", "BertParallelTransformerOutput", "(", "\n", "intermediate_size", ",", "hidden_size", ",", "output_dropout_prob", ",", "\n", "layernorm_epsilon", "=", "layernorm_epsilon", ",", "\n", "input_is_parallel", "=", "True", ",", "\n", "init_method", "=", "init_method", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.BertParallelTransformerLayer.forward": [[661, 677], ["transformer.BertParallelTransformerLayer.attention", "transformer.BertParallelTransformerLayer.self_output", "transformer.BertParallelTransformerLayer.intermediate", "transformer.BertParallelTransformerLayer.intermediate_activation_fn", "transformer.BertParallelTransformerLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "# [b, s, hp]", "\n", "        ", "attention_output_parallel", "=", "self", ".", "attention", "(", "hidden_states", ",", "\n", "attention_mask", ")", "\n", "# [b, s, h]", "\n", "attention_self_output", "=", "self", ".", "self_output", "(", "attention_output_parallel", ",", "\n", "hidden_states", ")", "\n", "# [b, s, ip]", "\n", "intermediate_output_parallel", "=", "self", ".", "intermediate", "(", "attention_self_output", ")", "\n", "intermediate_output_parallel", "=", "self", ".", "intermediate_activation_fn", "(", "\n", "intermediate_output_parallel", ")", "\n", "# [b, s, h]", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output_parallel", ",", "\n", "attention_self_output", ")", "\n", "\n", "return", "layer_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.gelu_impl": [[177, 182], ["torch.tanh", "torch.tanh"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "gelu_impl", "(", "x", ")", ":", "\n", "     ", "\"\"\"OpenAI's gelu implementation.\"\"\"", "\n", "return", "0.5", "*", "x", "*", "(", "1.0", "+", "torch", ".", "tanh", "(", "0.7978845608028654", "*", "x", "*", "\n", "(", "1.0", "+", "0.044715", "*", "x", "*", "x", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.gelu": [[183, 185], ["transformer.gelu_impl"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.gelu_impl"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "return", "gelu_impl", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.unscaled_init_method": [[321, 327], ["torch.nn.init.normal_", "torch.nn.init.normal_"], "function", ["None"], ["", "", "def", "unscaled_init_method", "(", "sigma", ")", ":", "\n", "    ", "\"\"\"Init method based on N(0, sigma).\"\"\"", "\n", "def", "init_", "(", "tensor", ")", ":", "\n", "        ", "return", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "tensor", ",", "mean", "=", "0.0", ",", "std", "=", "sigma", ")", "\n", "\n", "", "return", "init_", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.transformer.scaled_init_method": [[329, 336], ["math.sqrt", "torch.nn.init.normal_", "torch.nn.init.normal_"], "function", ["None"], ["", "def", "scaled_init_method", "(", "sigma", ",", "num_layers", ")", ":", "\n", "    ", "\"\"\"Init method based on N(0, sigma/sqrt(2*num_layers).\"\"\"", "\n", "std", "=", "sigma", "/", "math", ".", "sqrt", "(", "2.0", "*", "num_layers", ")", "\n", "def", "init_", "(", "tensor", ")", ":", "\n", "        ", "return", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "tensor", ",", "mean", "=", "0.0", ",", "std", "=", "std", ")", "\n", "\n", "", "return", "init_", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.VocabUtility.vocab_range_from_per_partition_vocab_size": [[59, 65], ["None"], "methods", ["None"], ["max_", "=", "param", ".", "data", ".", "max", "(", ")", "\n", "norm", "=", "param", ".", "data", ".", "norm", "(", ")", "\n", "string", "+=", "'{:7d}, {:4d}, {:4d}, {:2d}, '", ".", "format", "(", "\n", "iteration", ",", "rank", ",", "index", ",", "int", "(", "param", ".", "model_parallel", ")", ")", "\n", "string", "+=", "'{:.6E}, {:.6E}, {:.6E}\\n'", ".", "format", "(", "min_", ",", "max_", ",", "norm", ")", "\n", "", "", "print", "(", "string", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.VocabUtility.vocab_range_from_global_vocab_size": [[66, 71], ["utils.divide", "utils.VocabUtility.vocab_range_from_per_partition_vocab_size"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.VocabUtility.vocab_range_from_per_partition_vocab_size"], ["\n", "", "class", "Timers", ":", "\n", "    ", "\"\"\"Group of timers.\"\"\"", "\n", "\n", "class", "Timer", ":", "\n", "        ", "\"\"\"Timer.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.ensure_divisibility": [[20, 24], ["None"], "function", ["None"], ["import", "time", "\n", "import", "numpy", "as", "np", "\n", "import", "torch", "\n", "\n", "from", "torch", ".", "nn", ".", "parallel", ".", "distributed", "import", "DistributedDataParallel", "as", "torchDDP", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide": [[26, 31], ["utils.ensure_divisibility"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.ensure_divisibility"], ["import", "mpu", "\n", "import", "model", "\n", "\n", "\n", "def", "print_rank_0", "(", "message", ")", ":", "\n", "    ", "if", "torch", ".", "distributed", ".", "is_initialized", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.split_tensor_along_last_dim": [[33, 52], ["utils.divide", "torch.split", "tensor.dim", "tuple", "tensor.size", "chunk.contiguous"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.utils.divide", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["            ", "print", "(", "message", ",", "flush", "=", "True", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "message", ",", "flush", "=", "True", ")", "\n", "\n", "\n", "", "", "def", "print_args", "(", "args", ")", ":", "\n", "    ", "\"\"\"Print arguments.\"\"\"", "\n", "\n", "print", "(", "'arguments:'", ",", "flush", "=", "True", ")", "\n", "for", "arg", "in", "vars", "(", "args", ")", ":", "\n", "        ", "dots", "=", "'.'", "*", "(", "29", "-", "len", "(", "arg", ")", ")", "\n", "print", "(", "'  {} {} {}'", ".", "format", "(", "arg", ",", "dots", ",", "getattr", "(", "args", ",", "arg", ")", ")", ",", "flush", "=", "True", ")", "\n", "\n", "\n", "", "", "def", "print_params_min_max_norm", "(", "optimizer", ",", "iteration", ")", ":", "\n", "    ", "\"\"\"Print min, max, and norm of all parameters.\"\"\"", "\n", "index", "=", "0", "\n", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "string", "=", "'iteration, rank, index, model-parallel,min, max, norm\\n'", "\n", "optimizer_", "=", "optimizer", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.IdentityLayer.__init__": [[26, 29], ["super().__init__", "torch.nn.Parameter", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", ",", "scale", "=", "1.0", ")", ":", "\n", "        ", "super", "(", "IdentityLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "scale", "*", "torch", ".", "randn", "(", "size", ")", ")", "\n", "", "def", "forward", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.IdentityLayer.forward": [[29, 31], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed": [[33, 39], ["random.seed", "numpy.random.seed", "torch.manual_seed", "mpu.model_parallel_cuda_manual_seed"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.model_parallel_cuda_manual_seed"], ["", "", "def", "set_random_seed", "(", "seed", ")", ":", "\n", "    ", "\"\"\"Set random seed for reproducability.\"\"\"", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "numpy", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "mpu", ".", "model_parallel_cuda_manual_seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.initialize_distributed": [[41, 73], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "int", "int", "print", "torch.cuda.set_device", "os.getenv", "os.getenv", "torch.distributed.init_process_group", "os.getenv", "os.getenv", "torch.cuda.device_count"], "function", ["None"], ["", "def", "initialize_distributed", "(", "backend", "=", "'nccl'", ")", ":", "\n", "    ", "\"\"\"Initialize torch.distributed.\"\"\"", "\n", "# Get local rank in case it is provided.", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--local_rank'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'local rank passed from distributed launcher'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "local_rank", "=", "args", ".", "local_rank", "\n", "\n", "# Get rank and world size.", "\n", "rank", "=", "int", "(", "os", ".", "getenv", "(", "'RANK'", ",", "'0'", ")", ")", "\n", "world_size", "=", "int", "(", "os", ".", "getenv", "(", "\"WORLD_SIZE\"", ",", "'1'", ")", ")", "\n", "\n", "print", "(", "'> initializing torch.distributed with local rank: {}, '", "\n", "'rank: {}, world size: {}'", ".", "format", "(", "local_rank", ",", "rank", ",", "world_size", ")", ")", "\n", "\n", "# Set the device id.", "\n", "device", "=", "rank", "%", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "if", "local_rank", "is", "not", "None", ":", "\n", "        ", "device", "=", "local_rank", "\n", "", "torch", ".", "cuda", ".", "set_device", "(", "device", ")", "\n", "\n", "# Call the init process.", "\n", "init_method", "=", "'tcp://'", "\n", "master_ip", "=", "os", ".", "getenv", "(", "'MASTER_ADDR'", ",", "'localhost'", ")", "\n", "master_port", "=", "os", ".", "getenv", "(", "'MASTER_PORT'", ",", "'6000'", ")", "\n", "init_method", "+=", "master_ip", "+", "':'", "+", "master_port", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "\n", "backend", "=", "backend", ",", "\n", "world_size", "=", "world_size", ",", "\n", "rank", "=", "rank", ",", "\n", "init_method", "=", "init_method", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.print_separator": [[75, 83], ["torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.get_rank", "print", "len"], "function", ["None"], ["", "def", "print_separator", "(", "message", ")", ":", "\n", "    ", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "filler_len", "=", "(", "78", "-", "len", "(", "message", ")", ")", "//", "2", "\n", "filler", "=", "'-'", "*", "filler_len", "\n", "string", "=", "'\\n'", "+", "filler", "+", "' {} '", ".", "format", "(", "message", ")", "+", "filler", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "string", ",", "flush", "=", "True", ")", "\n", "", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_data.test_boradcast_data": [[29, 79], ["mpu.initialize_model_parallel", "torch.manual_seed", "mpu.get_model_parallel_world_size", "list", "torch.FloatTensor().random_", "data[].clone", "mpu.data._check_data_types", "mpu.data._build_key_size_numel_dictionaries", "mpu.data.broadcast_data", "mpu.destroy_model_parallel", "torch.distributed.barrier", "torch.distributed.get_rank", "print", "key_size_t.keys", "torch.LongTensor().random_", "data[].clone", "mpu.get_model_parallel_rank", "functools.reduce", "data_t[].cuda", "torch.distributed.get_rank", "print", "mpu.get_data_parallel_rank", "torch.FloatTensor", "data_b[].sub().abs().max", "torch.LongTensor", "data_b[].sub().abs", "data_b[].sub"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.data._check_data_types", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.data._build_key_size_numel_dictionaries", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.data.broadcast_data", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_rank"], ["def", "test_boradcast_data", "(", "model_parallel_size", ")", ":", "\n", "\n", "    ", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing boradcast_data with model parallel size {} ...'", ".", "\n", "format", "(", "model_parallel_size", ")", ")", "\n", "\n", "", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size", ")", "\n", "torch", ".", "manual_seed", "(", "1234", "+", "mpu", ".", "get_data_parallel_rank", "(", ")", ")", "\n", "model_parallel_size", "=", "mpu", ".", "get_model_parallel_world_size", "(", ")", "\n", "\n", "key_size_t", "=", "{", "'key1'", ":", "[", "7", ",", "11", "]", ",", "\n", "'key2'", ":", "[", "8", ",", "2", ",", "1", "]", ",", "\n", "'key3'", ":", "[", "13", "]", ",", "\n", "'key4'", ":", "[", "5", ",", "1", ",", "2", "]", ",", "\n", "'key5'", ":", "[", "5", ",", "12", "]", "}", "\n", "keys", "=", "list", "(", "key_size_t", ".", "keys", "(", ")", ")", "\n", "\n", "data", "=", "{", "}", "\n", "data_t", "=", "{", "}", "\n", "for", "key", "in", "key_size_t", ":", "\n", "        ", "data", "[", "key", "]", "=", "torch", ".", "LongTensor", "(", "size", "=", "key_size_t", "[", "key", "]", ")", ".", "random_", "(", "0", ",", "1000", ")", "\n", "data_t", "[", "key", "]", "=", "data", "[", "key", "]", ".", "clone", "(", ")", "\n", "", "data", "[", "'keyX'", "]", "=", "torch", ".", "FloatTensor", "(", "size", "=", "(", "5", ",", ")", ")", ".", "random_", "(", "0", ",", "1000", ")", "\n", "data_t", "[", "'keyX'", "]", "=", "data", "[", "'keyX'", "]", ".", "clone", "(", ")", "\n", "if", "mpu", ".", "get_model_parallel_rank", "(", ")", "!=", "0", ":", "\n", "        ", "data", "=", "None", "\n", "\n", "", "data_utils", ".", "_check_data_types", "(", "keys", ",", "data_t", ",", "torch", ".", "int64", ")", "\n", "key_size", ",", "key_numel", ",", "total_numel", "=", "data_utils", ".", "_build_key_size_numel_dictionaries", "(", "keys", ",", "data", ")", "\n", "for", "key", "in", "keys", ":", "\n", "        ", "assert", "key_size", "[", "key", "]", "==", "key_size_t", "[", "key", "]", "\n", "", "total_numel_t", "=", "0", "\n", "for", "key", "in", "keys", ":", "\n", "        ", "target_size", "=", "functools", ".", "reduce", "(", "operator", ".", "mul", ",", "key_size_t", "[", "key", "]", ",", "1", ")", "\n", "assert", "key_numel", "[", "key", "]", "==", "target_size", "\n", "total_numel_t", "+=", "target_size", "\n", "", "assert", "total_numel", "==", "total_numel_t", "\n", "\n", "data_b", "=", "data_utils", ".", "broadcast_data", "(", "keys", ",", "data", ",", "torch", ".", "int64", ")", "\n", "for", "key", "in", "keys", ":", "\n", "        ", "tensor", "=", "data_t", "[", "key", "]", ".", "cuda", "(", ")", "\n", "assert", "data_b", "[", "key", "]", ".", "sub", "(", "tensor", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "==", "0", "\n", "\n", "# Reset groups", "\n", "", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'>> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_cross_entropy.torch_cross_entropy": [[31, 44], ["commons.set_random_seed", "commons.IdentityLayer().cuda", "IdentityLayer().cuda.", "torch.cuda.LongTensor().random_", "torch.cuda.LongTensor().random_", "torch.cross_entropy().view_as().mean", "F.cross_entropy().view_as().mean.backward", "commons.IdentityLayer", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cross_entropy().view_as", "torch.cross_entropy", "identity.view", "torch.cuda.LongTensor().random_.view", "identity.size"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["def", "torch_cross_entropy", "(", "batch_size", ",", "seq_length", ",", "vocab_size", ",", "\n", "logits_scale", ",", "seed", ")", ":", "\n", "    ", "set_random_seed", "(", "seed", ")", "\n", "identity", "=", "IdentityLayer", "(", "(", "batch_size", ",", "seq_length", ",", "vocab_size", ")", ",", "\n", "scale", "=", "logits_scale", ")", ".", "cuda", "(", ")", "\n", "logits", "=", "identity", "(", ")", "\n", "target", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "\n", "size", "=", "(", "batch_size", ",", "seq_length", ")", ")", ".", "random_", "(", "0", ",", "vocab_size", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "size", "(", ")", "[", "-", "1", "]", ")", ",", "\n", "target", ".", "view", "(", "-", "1", ")", ",", "\n", "reduction", "=", "'none'", ")", ".", "view_as", "(", "target", ")", ".", "mean", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "return", "loss", ",", "identity", ".", "weight", ".", "grad", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_cross_entropy.mpu_cross_entropy": [[46, 58], ["commons.set_random_seed", "commons.IdentityLayer().cuda", "IdentityLayer().cuda.", "mpu.scatter_to_model_parallel_region", "torch.cuda.LongTensor().random_", "torch.cuda.LongTensor().random_", "mpu.cross_entropy.vocab_parallel_cross_entropy().mean", "vocab_parallel_cross_entropy().mean.backward", "commons.IdentityLayer", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "mpu.cross_entropy.vocab_parallel_cross_entropy"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.mappings.scatter_to_model_parallel_region", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.cross_entropy.vocab_parallel_cross_entropy"], ["", "def", "mpu_cross_entropy", "(", "batch_size", ",", "seq_length", ",", "vocab_size", ",", "\n", "logits_scale", ",", "seed", ")", ":", "\n", "    ", "set_random_seed", "(", "seed", ")", "\n", "identity", "=", "IdentityLayer", "(", "(", "batch_size", ",", "seq_length", ",", "vocab_size", ")", ",", "\n", "scale", "=", "logits_scale", ")", ".", "cuda", "(", ")", "\n", "logits", "=", "identity", "(", ")", "\n", "logits_parallel", "=", "mpu", ".", "scatter_to_model_parallel_region", "(", "logits", ")", "\n", "target", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "\n", "size", "=", "(", "batch_size", ",", "seq_length", ")", ")", ".", "random_", "(", "0", ",", "vocab_size", ")", "\n", "loss", "=", "vocab_parallel_cross_entropy", "(", "logits_parallel", ",", "target", ")", ".", "mean", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "return", "loss", ",", "identity", ".", "weight", ".", "grad", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_cross_entropy.test_cross_entropy": [[60, 99], ["mpu.initialize_model_parallel", "mpu.get_model_parallel_world_size", "test_cross_entropy.torch_cross_entropy", "test_cross_entropy.mpu_cross_entropy", "loss_torch.sub_().abs().max", "print", "grad_torch.sub_().abs().max", "print", "mpu.destroy_model_parallel", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "loss_torch.sub_().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "grad_torch.sub_().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "loss_torch.sub_", "grad_torch.sub_"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_cross_entropy.torch_cross_entropy", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_cross_entropy.mpu_cross_entropy", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel"], ["", "def", "test_cross_entropy", "(", "model_parallel_size", ")", ":", "\n", "\n", "    ", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing cross entropy with model parallel size {} ...'", ".", "\n", "format", "(", "model_parallel_size", ")", ")", "\n", "\n", "", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size", ")", "\n", "model_parallel_size", "=", "mpu", ".", "get_model_parallel_world_size", "(", ")", "\n", "\n", "batch_size", "=", "13", "\n", "seq_length", "=", "17", "\n", "vocab_size_per_partition", "=", "11", "\n", "logits_scale", "=", "1000.0", "\n", "vocab_size", "=", "vocab_size_per_partition", "*", "model_parallel_size", "\n", "seed", "=", "1234", "\n", "\n", "loss_torch", ",", "grad_torch", "=", "torch_cross_entropy", "(", "batch_size", ",", "seq_length", ",", "\n", "vocab_size", ",", "logits_scale", ",", "\n", "seed", ")", "\n", "loss_mpu", ",", "grad_mpu", "=", "mpu_cross_entropy", "(", "batch_size", ",", "seq_length", ",", "\n", "vocab_size", ",", "logits_scale", ",", "\n", "seed", ")", "\n", "\n", "error", "=", "loss_torch", ".", "sub_", "(", "loss_mpu", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "print", "(", "'   max error in loss on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-6", "\n", "\n", "error", "=", "grad_torch", ".", "sub_", "(", "grad_mpu", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "print", "(", "'   max error in grad on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-6", "\n", "\n", "# Reset groups", "\n", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'>> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_random.test_set_cuda_rng_state": [[26, 86], ["mpu.initialize_model_parallel", "mpu.get_model_parallel_world_size", "torch.cuda.manual_seed", "torch.cuda.FloatTensor", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state.clone", "range", "torch.cuda.FloatTensor.clone", "torch.cuda.get_rng_state", "torch.cuda.get_rng_state.sub().max", "print", "mpu.random._set_cuda_rng_state", "range", "mpu.random._set_cuda_rng_state", "range", "torch.cuda.FloatTensor.clone", "tensor.clone.sub().abs().max", "print", "torch.cuda.get_rng_state.sub().max", "print", "mpu.destroy_model_parallel", "torch.distributed.barrier", "torch.distributed.get_rank", "print", "torch.randn", "torch.cuda.get_rng_state.sub().max", "torch.cuda.get_rng_state().sub().max", "torch.randn", "torch.randn", "torch.distributed.get_rank", "print", "torch.cuda.get_rng_state.sub", "torch.distributed.get_rank", "tensor.clone.sub().abs", "torch.distributed.get_rank", "torch.cuda.get_rng_state.sub", "torch.distributed.get_rank", "torch.cuda.get_rng_state.sub", "torch.cuda.get_rng_state().sub", "tensor.clone.sub", "torch.cuda.get_rng_state"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random._set_cuda_rng_state", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random._set_cuda_rng_state", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel"], ["def", "test_set_cuda_rng_state", "(", "model_parallel_size", ")", ":", "\n", "\n", "    ", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing set_rng_state with size {} ...'", ".", "\n", "format", "(", "model_parallel_size", ")", ")", "\n", "\n", "", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size", ")", "\n", "model_parallel_size", "=", "mpu", ".", "get_model_parallel_world_size", "(", ")", "\n", "\n", "size", "=", "123", "\n", "seed", "=", "1234", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "1234", ")", "\n", "tensor", "=", "torch", ".", "cuda", ".", "FloatTensor", "(", "size", ")", "\n", "\n", "# Get the state", "\n", "rng_state", "=", "torch", ".", "cuda", ".", "get_rng_state", "(", ")", "\n", "rng_state_copy", "=", "rng_state", ".", "clone", "(", ")", "\n", "\n", "# Do some stuff.", "\n", "for", "_", "in", "range", "(", "5", ")", ":", "\n", "        ", "torch", ".", "randn", "(", "size", ",", "out", "=", "tensor", ")", "\n", "", "result_1", "=", "tensor", ".", "clone", "(", ")", "\n", "\n", "assert", "rng_state", ".", "sub", "(", "rng_state_copy", ")", ".", "max", "(", ")", "==", "0", "\n", "assert", "torch", ".", "cuda", ".", "get_rng_state", "(", ")", ".", "sub", "(", "rng_state_copy", ")", ".", "max", "(", ")", ">", "0", "\n", "\n", "# State should be different.", "\n", "new_rng_state", "=", "torch", ".", "cuda", ".", "get_rng_state", "(", ")", "\n", "max_diff", "=", "new_rng_state", ".", "sub", "(", "rng_state", ")", ".", "max", "(", ")", "\n", "print", "(", "'   max diff in rng state (should be non-zero) on global rank {}: {}'", ".", "\n", "format", "(", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "max_diff", ")", ")", "\n", "assert", "max_diff", ">", "0", "\n", "\n", "# Reset the rng state and do the same stuff.", "\n", "mpu", ".", "random", ".", "_set_cuda_rng_state", "(", "rng_state", ")", "\n", "for", "_", "in", "range", "(", "5", ")", ":", "\n", "        ", "torch", ".", "randn", "(", "size", ",", "out", "=", "tensor", ")", "\n", "", "mpu", ".", "random", ".", "_set_cuda_rng_state", "(", "rng_state", ")", "\n", "for", "_", "in", "range", "(", "5", ")", ":", "\n", "        ", "torch", ".", "randn", "(", "size", ",", "out", "=", "tensor", ")", "\n", "", "result_2", "=", "tensor", ".", "clone", "(", ")", "\n", "\n", "# Results should be the same", "\n", "error", "=", "result_2", ".", "sub", "(", "result_1", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "print", "(", "'   max error in generated tensors (should be zero) on '", "\n", "'global rank {}: {}'", ".", "format", "(", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-6", "\n", "\n", "# Input state should have remained intact.", "\n", "error", "=", "rng_state", ".", "sub", "(", "rng_state_copy", ")", ".", "max", "(", ")", "\n", "print", "(", "'   max error in rng state (should be zero) on global rank {}: {}'", ".", "\n", "format", "(", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "==", "0", "\n", "\n", "# Reset groups", "\n", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'>> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_random.test_cuda_rng_tracker": [[88, 157], ["mpu.initialize_model_parallel", "mpu.get_model_parallel_world_size", "torch.cuda.FloatTensor", "torch.cuda.manual_seed", "torch.randn", "torch.cuda.FloatTensor.clone", "torch.randn", "torch.cuda.FloatTensor.clone", "torch.cuda.manual_seed", "torch.randn", "torch.cuda.FloatTensor.clone", "torch.randn", "torch.cuda.FloatTensor.clone", "torch.cuda.manual_seed", "mpu.get_cuda_rng_tracker().add", "torch.randn", "torch.cuda.FloatTensor.clone", "torch.randn", "torch.cuda.FloatTensor.clone", "tensor.clone.sub().abs().max", "min", "print", "max", "max", "max", "print", "mpu.get_cuda_rng_tracker().reset", "mpu.destroy_model_parallel", "torch.distributed.barrier", "torch.distributed.get_rank", "print", "mpu.get_cuda_rng_tracker().fork", "torch.randn", "torch.cuda.FloatTensor.clone", "mpu.get_cuda_rng_tracker().fork", "torch.randn", "torch.cuda.FloatTensor.clone", "tensor.clone.sub().abs().max", "tensor.clone.sub().abs().max", "tensor.clone.sub().abs().max", "tensor.clone.sub().abs().max", "tensor.clone.sub().abs().max", "torch.distributed.get_rank", "print", "mpu.get_cuda_rng_tracker", "tensor.clone.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "mpu.get_cuda_rng_tracker", "mpu.get_cuda_rng_tracker", "mpu.get_cuda_rng_tracker", "tensor.clone.sub().abs", "tensor.clone.sub().abs", "tensor.clone.sub().abs", "tensor.clone.sub().abs", "tensor.clone.sub().abs", "tensor.clone.sub", "tensor.clone.sub", "tensor.clone.sub", "tensor.clone.sub", "tensor.clone.sub", "tensor.clone.sub"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.add", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.reset", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.fork", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.fork", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker"], ["", "", "def", "test_cuda_rng_tracker", "(", "model_parallel_size", ")", ":", "\n", "\n", "    ", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing cuda rng tracker with size {} ...'", ".", "\n", "format", "(", "model_parallel_size", ")", ")", "\n", "\n", "", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size", ")", "\n", "model_parallel_size", "=", "mpu", ".", "get_model_parallel_world_size", "(", ")", "\n", "\n", "seed_1", "=", "1234", "\n", "seed_2", "=", "4321", "\n", "size", "=", "[", "12", ",", "21", "]", "\n", "tensor", "=", "torch", ".", "cuda", ".", "FloatTensor", "(", "size", ")", "\n", "\n", "# Set to seed_1 and generate two tensors.", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "seed_1", ")", "\n", "torch", ".", "randn", "(", "size", ",", "out", "=", "tensor", ")", "\n", "target_11", "=", "tensor", ".", "clone", "(", ")", "\n", "torch", ".", "randn", "(", "size", ",", "out", "=", "tensor", ")", "\n", "target_12", "=", "tensor", ".", "clone", "(", ")", "\n", "\n", "# Set to seed_2 and generate two tensors.", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "seed_2", ")", "\n", "torch", ".", "randn", "(", "size", ",", "out", "=", "tensor", ")", "\n", "target_21", "=", "tensor", ".", "clone", "(", ")", "\n", "torch", ".", "randn", "(", "size", ",", "out", "=", "tensor", ")", "\n", "target_22", "=", "tensor", ".", "clone", "(", ")", "\n", "\n", "# Now if we interleave seed_1 and seed_2,", "\n", "# we should still get the same tensors", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "seed_1", ")", "\n", "mpu", ".", "get_cuda_rng_tracker", "(", ")", ".", "add", "(", "'test'", ",", "seed_2", ")", "\n", "\n", "torch", ".", "randn", "(", "size", ",", "out", "=", "tensor", ")", "\n", "result_11", "=", "tensor", ".", "clone", "(", ")", "\n", "\n", "with", "mpu", ".", "get_cuda_rng_tracker", "(", ")", ".", "fork", "(", "'test'", ")", ":", "\n", "        ", "torch", ".", "randn", "(", "size", ",", "out", "=", "tensor", ")", "\n", "result_21", "=", "tensor", ".", "clone", "(", ")", "\n", "\n", "", "torch", ".", "randn", "(", "size", ",", "out", "=", "tensor", ")", "\n", "result_12", "=", "tensor", ".", "clone", "(", ")", "\n", "\n", "with", "mpu", ".", "get_cuda_rng_tracker", "(", ")", ".", "fork", "(", "'test'", ")", ":", "\n", "        ", "torch", ".", "randn", "(", "size", ",", "out", "=", "tensor", ")", "\n", "result_22", "=", "tensor", ".", "clone", "(", ")", "\n", "\n", "", "diff", "=", "result_11", ".", "sub", "(", "result_21", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "diff", "=", "min", "(", "diff", ",", "result_12", ".", "sub", "(", "result_22", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ")", "\n", "print", "(", "'   max diff in generated tensors (should be non-zero) on '", "\n", "'global rank {}: {}'", ".", "format", "(", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "diff", ")", ")", "\n", "assert", "diff", ">", "1.0e-6", "\n", "error", "=", "max", "(", "result_11", ".", "sub", "(", "target_11", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ",", "\n", "result_12", ".", "sub", "(", "target_12", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ")", "\n", "error", "=", "max", "(", "error", ",", "result_21", ".", "sub", "(", "target_21", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ")", "\n", "error", "=", "max", "(", "error", ",", "result_22", ".", "sub", "(", "target_22", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ")", "\n", "print", "(", "'   max error in generated tensors (should be zero) on '", "\n", "'global rank {}: {}'", ".", "format", "(", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-6", "\n", "\n", "# Reset the tracker", "\n", "mpu", ".", "get_cuda_rng_tracker", "(", ")", ".", "reset", "(", ")", "\n", "\n", "# Reset groups", "\n", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'>> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_random.test_model_parallel_cuda_manual_seed": [[159, 183], ["mpu.initialize_model_parallel", "mpu.get_model_parallel_world_size", "mpu.model_parallel_cuda_manual_seed", "mpu.get_cuda_rng_tracker().reset", "mpu.destroy_model_parallel", "torch.distributed.barrier", "torch.distributed.get_rank", "print", "torch.cuda.initial_seed", "mpu.get_cuda_rng_tracker().fork", "torch.distributed.get_rank", "print", "torch.cuda.initial_seed", "mpu.get_cuda_rng_tracker", "mpu.get_cuda_rng_tracker", "mpu.get_model_parallel_rank"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.model_parallel_cuda_manual_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.reset", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.fork", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.get_cuda_rng_tracker", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank"], ["", "", "def", "test_model_parallel_cuda_manual_seed", "(", "model_parallel_size", ")", ":", "\n", "\n", "    ", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing model parallel cuda manual seed with size {} ...'", ".", "\n", "format", "(", "model_parallel_size", ")", ")", "\n", "\n", "", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size", ")", "\n", "model_parallel_size", "=", "mpu", ".", "get_model_parallel_world_size", "(", ")", "\n", "\n", "mpu", ".", "model_parallel_cuda_manual_seed", "(", "12345", ")", "\n", "assert", "torch", ".", "cuda", ".", "initial_seed", "(", ")", "==", "12345", "\n", "with", "mpu", ".", "get_cuda_rng_tracker", "(", ")", ".", "fork", "(", ")", ":", "\n", "        ", "assert", "torch", ".", "cuda", ".", "initial_seed", "(", ")", "==", "(", "12345", "+", "2718", "+", "\n", "mpu", ".", "get_model_parallel_rank", "(", ")", ")", "\n", "\n", "# Reset the tracker", "\n", "", "mpu", ".", "get_cuda_rng_tracker", "(", ")", ".", "reset", "(", ")", "\n", "\n", "# Reset groups", "\n", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'>> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_initialize.test_initialize_model_parallel": [[26, 63], ["min", "mpu.initialize_model_parallel", "mpu.model_parallel_is_initialized", "test_initialize.test_initialize_model_parallel.check"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.model_parallel_is_initialized"], ["def", "test_initialize_model_parallel", "(", "model_parallel_size", ")", ":", "\n", "\n", "    ", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing initialize_model_parallel with size {} ...'", ".", "format", "(", "\n", "model_parallel_size", ")", ")", "\n", "", "model_parallel_size_", "=", "min", "(", "model_parallel_size", ",", "\n", "torch", ".", "distributed", ".", "get_world_size", "(", ")", ")", "\n", "assert", "not", "mpu", ".", "model_parallel_is_initialized", "(", ")", "\n", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size_", ")", "\n", "assert", "mpu", ".", "model_parallel_is_initialized", "(", ")", "\n", "\n", "# Checks.", "\n", "def", "check", "(", "group", ",", "world_size", ",", "rank", ")", ":", "\n", "        ", "assert", "world_size", "==", "torch", ".", "distributed", ".", "get_world_size", "(", "group", "=", "group", ")", "\n", "assert", "rank", "==", "torch", ".", "distributed", ".", "get_rank", "(", "group", "=", "group", ")", "\n", "\n", "# Model parallel.", "\n", "", "world_size", "=", "model_parallel_size_", "\n", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "%", "model_parallel_size_", "\n", "assert", "world_size", "==", "mpu", ".", "get_model_parallel_world_size", "(", ")", "\n", "assert", "rank", "==", "mpu", ".", "get_model_parallel_rank", "(", ")", "\n", "check", "(", "mpu", ".", "get_model_parallel_group", "(", ")", ",", "world_size", ",", "rank", ")", "\n", "\n", "\n", "# Data parallel.", "\n", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "//", "model_parallel_size_", "\n", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "//", "model_parallel_size", "\n", "assert", "world_size", "==", "mpu", ".", "get_data_parallel_world_size", "(", ")", "\n", "assert", "rank", "==", "mpu", ".", "get_data_parallel_rank", "(", ")", "\n", "check", "(", "mpu", ".", "get_data_parallel_group", "(", ")", ",", "world_size", ",", "rank", ")", "\n", "\n", "# Reset groups", "\n", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'>> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_initialize.test_get_model_parallel_src_rank": [[65, 86], ["min", "mpu.initialize_model_parallel", "mpu.model_parallel_is_initialized", "mpu.destroy_model_parallel", "torch.distributed.barrier", "torch.distributed.get_rank", "print", "torch.distributed.get_world_size", "mpu.model_parallel_is_initialized", "torch.distributed.get_rank", "mpu.get_model_parallel_rank", "mpu.get_model_parallel_src_rank", "torch.distributed.get_rank", "print"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.model_parallel_is_initialized", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.model_parallel_is_initialized", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_src_rank"], ["", "", "def", "test_get_model_parallel_src_rank", "(", "model_parallel_size_", ")", ":", "\n", "\n", "    ", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing get_model_parallel_src_rank with size {} ...'", ".", "format", "(", "\n", "model_parallel_size_", ")", ")", "\n", "", "model_parallel_size", "=", "min", "(", "model_parallel_size_", ",", "\n", "torch", ".", "distributed", ".", "get_world_size", "(", ")", ")", "\n", "assert", "not", "mpu", ".", "model_parallel_is_initialized", "(", ")", "\n", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size", ")", "\n", "assert", "mpu", ".", "model_parallel_is_initialized", "(", ")", "\n", "\n", "# Checks", "\n", "src_rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "-", "mpu", ".", "get_model_parallel_rank", "(", ")", "\n", "assert", "mpu", ".", "get_model_parallel_src_rank", "(", ")", "==", "src_rank", "\n", "\n", "# Reset groups", "\n", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'>> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.IdentityLayer2D.__init__": [[179, 183], ["super().__init__", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "m", ",", "n", ")", ":", "\n", "        ", "super", "(", "IdentityLayer2D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "m", ",", "n", ")", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "weight", ")", "\n", "", "def", "forward", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.IdentityLayer2D.forward": [[183, 185], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.IdentityLayer3D.__init__": [[320, 324], ["super().__init__", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "m", ",", "n", ",", "k", ")", ":", "\n", "        ", "super", "(", "IdentityLayer3D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "m", ",", "n", ",", "k", ")", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "weight", ")", "\n", "", "def", "forward", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.IdentityLayer3D.forward": [[324, 326], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.test_parallel_embedding": [[31, 107], ["mpu.initialize_model_parallel", "mpu.get_model_parallel_world_size", "commons.set_random_seed", "torch.LongTensor().random_().cuda", "torch.LongTensor().random_().cuda", "torch.randn().cuda", "torch.randn().cuda", "commons.set_random_seed", "torch.nn.Embedding().cuda", "torch.nn.Embedding().cuda", "torch.nn.Embedding().cuda.", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum.backward", "commons.set_random_seed", "mpu.layers.ParallelEmbedding().cuda", "layers.ParallelEmbedding().cuda.", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum.backward", "commons.set_random_seed", "mpu.layers.VocabParallelEmbedding().cuda", "layers.VocabParallelEmbedding().cuda.", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum.backward", "torch.distributed.barrier", "torch.distributed.barrier", "torch.mul().sum.sub().abs", "print", "torch.distributed.barrier", "torch.distributed.barrier", "torch.mul().sum.sub().abs", "print", "layers.ParallelEmbedding().cuda.weight.grad.sub().abs().max", "print", "layers.VocabParallelEmbedding().cuda.weight.grad.sub().abs().max", "print", "mpu.destroy_model_parallel", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "torch.split", "torch.split", "torch.split", "torch.split", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "torch.LongTensor().random_", "torch.LongTensor().random_", "torch.randn", "torch.randn", "torch.nn.Embedding", "torch.nn.Embedding", "torch.mul", "torch.mul", "mpu.layers.ParallelEmbedding", "torch.mul", "torch.mul", "mpu.layers.VocabParallelEmbedding", "torch.mul", "torch.mul", "torch.mul().sum.sub", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.mul().sum.sub", "torch.distributed.get_rank", "torch.distributed.get_rank", "mpu.get_model_parallel_rank", "layers.ParallelEmbedding().cuda.weight.grad.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "mpu.get_model_parallel_rank", "layers.VocabParallelEmbedding().cuda.weight.grad.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.LongTensor", "torch.LongTensor", "layers.ParallelEmbedding().cuda.weight.grad.sub", "layers.VocabParallelEmbedding().cuda.weight.grad.sub"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank"], ["def", "test_parallel_embedding", "(", "model_parallel_size", ")", ":", "\n", "\n", "    ", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing parallel embedding with model parallel size {} ...'", ".", "\n", "format", "(", "model_parallel_size", ")", ")", "\n", "\n", "", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size", ")", "\n", "model_parallel_size", "=", "mpu", ".", "get_model_parallel_world_size", "(", ")", "\n", "\n", "batch_size", "=", "17", "\n", "seq_length", "=", "23", "\n", "vocab_size", "=", "48", "\n", "hidden_size", "=", "16", "\n", "seed", "=", "1236", "\n", "\n", "set_random_seed", "(", "123", ")", "\n", "input_data", "=", "torch", ".", "LongTensor", "(", "\n", "size", "=", "(", "batch_size", ",", "seq_length", ")", ")", ".", "random_", "(", "0", ",", "vocab_size", ")", ".", "cuda", "(", ")", "\n", "loss_weight", "=", "torch", ".", "randn", "(", "[", "batch_size", ",", "seq_length", ",", "hidden_size", "]", ")", ".", "cuda", "(", ")", "\n", "\n", "set_random_seed", "(", "seed", ")", "\n", "embedding_original", "=", "torch", ".", "nn", ".", "Embedding", "(", "vocab_size", ",", "hidden_size", ")", ".", "cuda", "(", ")", "\n", "\n", "output", "=", "embedding_original", "(", "input_data", ")", "\n", "loss_original", "=", "torch", ".", "mul", "(", "output", ",", "loss_weight", ")", ".", "sum", "(", ")", "\n", "loss_original", ".", "backward", "(", ")", "\n", "\n", "set_random_seed", "(", "seed", ")", "\n", "embedding_parallel", "=", "layers", ".", "ParallelEmbedding", "(", "\n", "vocab_size", ",", "hidden_size", ",", "init_method", "=", "init", ".", "normal_", ")", ".", "cuda", "(", ")", "\n", "output", "=", "embedding_parallel", "(", "input_data", ")", "\n", "loss_parallel", "=", "torch", ".", "mul", "(", "output", ",", "loss_weight", ")", ".", "sum", "(", ")", "\n", "loss_parallel", ".", "backward", "(", ")", "\n", "\n", "set_random_seed", "(", "seed", ")", "\n", "embedding_vocab_parallel", "=", "layers", ".", "VocabParallelEmbedding", "(", "\n", "vocab_size", ",", "hidden_size", ",", "init_method", "=", "init", ".", "normal_", ")", ".", "cuda", "(", ")", "\n", "output", "=", "embedding_vocab_parallel", "(", "input_data", ")", "\n", "loss_vocab_parallel", "=", "torch", ".", "mul", "(", "output", ",", "loss_weight", ")", ".", "sum", "(", ")", "\n", "loss_vocab_parallel", ".", "backward", "(", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "error", "=", "loss_parallel", ".", "sub", "(", "loss_original", ")", ".", "abs", "(", ")", "\n", "print", "(", "'   error in loss (parallel) on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-12", ",", "'error: {}'", ".", "format", "(", "error", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "error", "=", "loss_vocab_parallel", ".", "sub", "(", "loss_original", ")", ".", "abs", "(", ")", "\n", "print", "(", "'   error in loss (vocab parallel) on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-12", ",", "'error: {}'", ".", "format", "(", "error", ")", "\n", "\n", "weight_grad_orig", "=", "torch", ".", "split", "(", "embedding_original", ".", "weight", ".", "grad", ",", "\n", "hidden_size", "//", "model_parallel_size", ",", "\n", "1", ")", "[", "mpu", ".", "get_model_parallel_rank", "(", ")", "]", "\n", "error", "=", "embedding_parallel", ".", "weight", ".", "grad", ".", "sub", "(", "weight_grad_orig", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "print", "(", "'   error in grad (parallel) on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-12", ",", "'error: {}'", ".", "format", "(", "error", ")", "\n", "\n", "weight_grad_orig", "=", "torch", ".", "split", "(", "embedding_original", ".", "weight", ".", "grad", ",", "\n", "vocab_size", "//", "model_parallel_size", ",", "\n", "0", ")", "[", "mpu", ".", "get_model_parallel_rank", "(", ")", "]", "\n", "error", "=", "embedding_vocab_parallel", ".", "weight", ".", "grad", ".", "sub", "(", "\n", "weight_grad_orig", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "print", "(", "'   error in grad (vocab parallel) on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-12", ",", "'error: {}'", ".", "format", "(", "error", ")", "\n", "\n", "# Reset groups", "\n", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'>> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.test_initialize_affine_weight": [[109, 176], ["mpu.initialize_model_parallel", "mpu.get_model_parallel_world_size", "torch.empty", "torch.empty", "commons.set_random_seed", "mpu.layers._initialize_affine_weight", "commons.set_random_seed", "torch.empty", "torch.empty", "torch.nn.init.normal_", "torch.nn.init.normal_", "mpu.get_model_parallel_rank", "[].contiguous().clone", "torch.empty.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "torch.empty", "torch.empty", "commons.set_random_seed", "mpu.layers._initialize_affine_weight", "commons.set_random_seed", "torch.empty", "torch.empty", "torch.nn.init.normal_", "torch.nn.init.normal_", "mpu.get_model_parallel_rank", "[].contiguous().clone", "torch.empty.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "mpu.destroy_model_parallel", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "[].contiguous", "torch.empty.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "[].contiguous", "torch.empty.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.empty.sub", "torch.empty.sub", "torch.split", "torch.split", "torch.split", "torch.split"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers._initialize_affine_weight", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.layers._initialize_affine_weight", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel"], ["", "", "def", "test_initialize_affine_weight", "(", "model_parallel_size", ")", ":", "\n", "\n", "    ", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing initialize_affine_weight with model parallel '", "\n", "'size: {}'", ".", "format", "(", "model_parallel_size", ")", ")", "\n", "", "model_parallel_size", "=", "mpu", ".", "get_model_parallel_world_size", "(", ")", "\n", "\n", "seed", "=", "12345", "\n", "input_size_coeff", "=", "13", "\n", "input_size", "=", "input_size_coeff", "*", "model_parallel_size", "\n", "output_size_coeff", "=", "17", "\n", "output_size", "=", "output_size_coeff", "*", "model_parallel_size", "\n", "\n", "# ---------------", "\n", "# Column parallel", "\n", "# ---------------", "\n", "weight", "=", "torch", ".", "empty", "(", "output_size_coeff", ",", "input_size", ")", "\n", "set_random_seed", "(", "seed", ")", "\n", "layers", ".", "_initialize_affine_weight", "(", "weight", ",", "output_size", ",", "input_size", ",", "\n", "\n", "output_size_coeff", ",", "0", ",", "\n", "torch", ".", "nn", ".", "init", ".", "normal_", ")", "\n", "# Target.", "\n", "set_random_seed", "(", "seed", ")", "\n", "master_weight", "=", "torch", ".", "empty", "(", "output_size", ",", "input_size", ")", "\n", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "master_weight", ")", "\n", "rank", "=", "mpu", ".", "get_model_parallel_rank", "(", ")", "\n", "my_weight", "=", "torch", ".", "split", "(", "master_weight", ",", "output_size_coeff", ",", "\n", "dim", "=", "0", ")", "[", "rank", "]", ".", "contiguous", "(", ")", ".", "clone", "(", ")", "\n", "\n", "# Compare.", "\n", "error", "=", "weight", ".", "sub", "(", "my_weight", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   column parallel max error (should be zero) on global rank '", "\n", "'{}: {}'", ".", "format", "(", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-6", "\n", "\n", "# ------------", "\n", "# Row parallel", "\n", "# ------------", "\n", "weight", "=", "torch", ".", "empty", "(", "output_size", ",", "input_size_coeff", ")", "\n", "set_random_seed", "(", "seed", ")", "\n", "mpu", ".", "layers", ".", "_initialize_affine_weight", "(", "weight", ",", "output_size", ",", "input_size", ",", "\n", "input_size_coeff", ",", "1", ",", "\n", "torch", ".", "nn", ".", "init", ".", "normal_", ")", "\n", "# Target.", "\n", "set_random_seed", "(", "seed", ")", "\n", "master_weight", "=", "torch", ".", "empty", "(", "output_size", ",", "input_size", ")", "\n", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "master_weight", ")", "\n", "rank", "=", "mpu", ".", "get_model_parallel_rank", "(", ")", "\n", "my_weight", "=", "torch", ".", "split", "(", "master_weight", ",", "input_size_coeff", ",", "\n", "dim", "=", "1", ")", "[", "rank", "]", ".", "contiguous", "(", ")", ".", "clone", "(", ")", "\n", "\n", "# Compare.", "\n", "error", "=", "weight", ".", "sub", "(", "my_weight", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   row parallel max error (should be zero) on global rank '", "\n", "'{}: {}'", ".", "format", "(", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-6", "\n", "\n", "# Reset groups", "\n", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "' >> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.test_column_parallel_linear": [[187, 252], ["mpu.initialize_model_parallel", "mpu.get_model_parallel_world_size", "commons.set_random_seed", "IdentityLayer2D().cuda", "mpu.ColumnParallelLinear().cuda", "torch.randn().cuda", "torch.randn().cuda", "IdentityLayer2D().cuda.", "mpu.ColumnParallelLinear().cuda.", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum.backward", "mpu.ColumnParallelLinear().cuda.master_weight.cuda", "torch.matmul", "torch.matmul", "torch.matmul().view", "torch.matmul().view", "torch.matmul", "torch.matmul", "mpu.get_model_parallel_rank", "[].contiguous().clone", "[].contiguous().clone.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "[].contiguous().clone", "[].contiguous().clone.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "torch.matmul.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "mpu.destroy_model_parallel", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "dLdY.t", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "test_layers.IdentityLayer2D", "mpu.ColumnParallelLinear", "torch.randn", "torch.randn", "torch.mul", "torch.mul", "torch.matmul", "torch.matmul", "[].contiguous", "[].contiguous().clone.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "[].contiguous", "[].contiguous().clone.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.matmul.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.ones().cuda().t", "torch.ones().cuda().t", "[].contiguous().clone.sub", "[].contiguous().clone.sub", "torch.matmul.sub", "torch.ones().cuda", "torch.ones().cuda", "torch.split", "torch.split", "torch.split", "torch.split", "torch.ones", "torch.ones"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel"], ["", "", "def", "test_column_parallel_linear", "(", "model_parallel_size", ")", ":", "\n", "\n", "    ", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing ColumnParallelLinear with model parallel '", "\n", "'size: {}'", ".", "format", "(", "model_parallel_size", ")", ")", "\n", "", "model_parallel_size", "=", "mpu", ".", "get_model_parallel_world_size", "(", ")", "\n", "\n", "seed", "=", "12345", "\n", "set_random_seed", "(", "seed", ")", "\n", "input_size_coeff", "=", "13", "\n", "input_size", "=", "input_size_coeff", "*", "model_parallel_size", "\n", "output_size_coeff", "=", "17", "\n", "output_size", "=", "output_size_coeff", "*", "model_parallel_size", "\n", "batch_size", "=", "7", "\n", "\n", "# Network", "\n", "identity_layer", "=", "IdentityLayer2D", "(", "batch_size", ",", "input_size", ")", ".", "cuda", "(", ")", "\n", "linear_layer", "=", "mpu", ".", "ColumnParallelLinear", "(", "\n", "input_size", ",", "output_size", ",", "keep_master_weight_for_test", "=", "True", ")", ".", "cuda", "(", ")", "\n", "loss_weight", "=", "torch", ".", "randn", "(", "[", "batch_size", ",", "output_size", "]", ")", ".", "cuda", "(", ")", "\n", "# Forward", "\n", "input_", "=", "identity_layer", "(", ")", "\n", "output", "=", "linear_layer", "(", "input_", ")", "\n", "loss", "=", "torch", ".", "mul", "(", "output", ",", "loss_weight", ")", ".", "sum", "(", ")", "\n", "# Backward", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# Values.", "\n", "dLdY", "=", "loss_weight", "\n", "X", "=", "identity_layer", ".", "weight", "\n", "A", "=", "linear_layer", ".", "master_weight", ".", "cuda", "(", ")", "\n", "dLdA", "=", "torch", ".", "matmul", "(", "dLdY", ".", "t", "(", ")", ",", "X", ")", "\n", "dLdb", "=", "torch", ".", "matmul", "(", "torch", ".", "ones", "(", "batch_size", ",", "1", ")", ".", "cuda", "(", ")", ".", "t", "(", ")", ",", "dLdY", ")", ".", "view", "(", "-", "1", ")", "\n", "dLdX", "=", "torch", ".", "matmul", "(", "dLdY", ",", "A", ")", "\n", "\n", "rank", "=", "mpu", ".", "get_model_parallel_rank", "(", ")", "\n", "my_dLdA", "=", "torch", ".", "split", "(", "dLdA", ",", "output_size_coeff", ",", "\n", "dim", "=", "0", ")", "[", "rank", "]", ".", "contiguous", "(", ")", ".", "clone", "(", ")", "\n", "error", "=", "my_dLdA", ".", "sub", "(", "linear_layer", ".", "weight", ".", "grad", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   error in dLdA on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-6", "\n", "\n", "my_dLdb", "=", "torch", ".", "split", "(", "dLdb", ",", "output_size_coeff", ",", "\n", "dim", "=", "0", ")", "[", "rank", "]", ".", "contiguous", "(", ")", ".", "clone", "(", ")", "\n", "error", "=", "my_dLdb", ".", "sub", "(", "linear_layer", ".", "bias", ".", "grad", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   error in dLdb on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-6", "\n", "\n", "error", "=", "dLdX", ".", "sub", "(", "identity_layer", ".", "weight", ".", "grad", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   error in dLdX on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-6", "\n", "\n", "# Reset groups", "\n", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "' >> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.test_row_parallel_linear": [[254, 317], ["mpu.initialize_model_parallel", "mpu.get_model_parallel_world_size", "commons.set_random_seed", "IdentityLayer2D().cuda", "mpu.RowParallelLinear().cuda", "torch.randn().cuda", "torch.randn().cuda", "IdentityLayer2D().cuda.", "mpu.RowParallelLinear().cuda.", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum.backward", "mpu.RowParallelLinear().cuda.master_weight.cuda", "torch.matmul", "torch.matmul", "torch.matmul().view", "torch.matmul().view", "torch.matmul", "torch.matmul", "mpu.get_model_parallel_rank", "[].contiguous().clone", "[].contiguous().clone.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "torch.matmul().view.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "torch.matmul.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "mpu.destroy_model_parallel", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "dLdY.t", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "test_layers.IdentityLayer2D", "mpu.RowParallelLinear", "torch.randn", "torch.randn", "torch.mul", "torch.mul", "torch.matmul", "torch.matmul", "[].contiguous", "[].contiguous().clone.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.matmul().view.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.matmul.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.ones().cuda().t", "torch.ones().cuda().t", "[].contiguous().clone.sub", "torch.matmul().view.sub", "torch.matmul.sub", "torch.ones().cuda", "torch.ones().cuda", "torch.split", "torch.split", "torch.ones", "torch.ones"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel"], ["", "", "def", "test_row_parallel_linear", "(", "model_parallel_size", ")", ":", "\n", "\n", "    ", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing RowParallelLinear with model parallel '", "\n", "'size: {}'", ".", "format", "(", "model_parallel_size", ")", ")", "\n", "", "model_parallel_size", "=", "mpu", ".", "get_model_parallel_world_size", "(", ")", "\n", "\n", "seed", "=", "12345", "\n", "set_random_seed", "(", "seed", ")", "\n", "input_size_coeff", "=", "13", "\n", "input_size", "=", "input_size_coeff", "*", "model_parallel_size", "\n", "output_size_coeff", "=", "17", "\n", "output_size", "=", "output_size_coeff", "*", "model_parallel_size", "\n", "batch_size", "=", "7", "\n", "\n", "# Network", "\n", "identity_layer", "=", "IdentityLayer2D", "(", "batch_size", ",", "input_size", ")", ".", "cuda", "(", ")", "\n", "linear_layer", "=", "mpu", ".", "RowParallelLinear", "(", "\n", "input_size", ",", "output_size", ",", "keep_master_weight_for_test", "=", "True", ")", ".", "cuda", "(", ")", "\n", "loss_weight", "=", "torch", ".", "randn", "(", "[", "batch_size", ",", "output_size", "]", ")", ".", "cuda", "(", ")", "\n", "# Forward", "\n", "input_", "=", "identity_layer", "(", ")", "\n", "output", "=", "linear_layer", "(", "input_", ")", "\n", "loss", "=", "torch", ".", "mul", "(", "output", ",", "loss_weight", ")", ".", "sum", "(", ")", "\n", "# Backward", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# Values.", "\n", "dLdY", "=", "loss_weight", "\n", "X", "=", "identity_layer", ".", "weight", "\n", "A", "=", "linear_layer", ".", "master_weight", ".", "cuda", "(", ")", "\n", "dLdA", "=", "torch", ".", "matmul", "(", "dLdY", ".", "t", "(", ")", ",", "X", ")", "\n", "dLdb", "=", "torch", ".", "matmul", "(", "torch", ".", "ones", "(", "batch_size", ",", "1", ")", ".", "cuda", "(", ")", ".", "t", "(", ")", ",", "dLdY", ")", ".", "view", "(", "-", "1", ")", "\n", "dLdX", "=", "torch", ".", "matmul", "(", "dLdY", ",", "A", ")", "\n", "\n", "rank", "=", "mpu", ".", "get_model_parallel_rank", "(", ")", "\n", "my_dLdA", "=", "torch", ".", "split", "(", "dLdA", ",", "input_size_coeff", ",", "\n", "dim", "=", "1", ")", "[", "rank", "]", ".", "contiguous", "(", ")", ".", "clone", "(", ")", "\n", "error", "=", "my_dLdA", ".", "sub", "(", "linear_layer", ".", "weight", ".", "grad", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   error in dLdA on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-6", "\n", "\n", "error", "=", "dLdb", ".", "sub", "(", "linear_layer", ".", "bias", ".", "grad", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   error in dLdb on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-6", "\n", "\n", "error", "=", "dLdX", ".", "sub", "(", "identity_layer", ".", "weight", ".", "grad", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   error in dLdX on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "1.0e-6", "\n", "\n", "# Reset groups", "\n", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "' >> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.parallel_self_attention": [[328, 359], ["mpu.initialize_model_parallel", "mpu.get_model_parallel_world_size", "commons.set_random_seed", "IdentityLayer3D().cuda", "mpu.BertParallelSelfAttention().cuda", "torch.randn().cuda", "torch.randn().cuda", "torch.randn().cuda", "torch.randn().cuda", "IdentityLayer3D().cuda.", "mpu.BertParallelSelfAttention().cuda.", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum.backward", "mpu.get_model_parallel_rank", "mpu.destroy_model_parallel", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "test_layers.IdentityLayer3D", "mpu.BertParallelSelfAttention", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.mul", "torch.mul"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel"], ["", "", "def", "parallel_self_attention", "(", "model_parallel_size", ",", "num_att_heads_per_partition", ",", "\n", "hidden_size_per_att_head", ",", "dropout_prob", ",", "batch_size", ",", "\n", "sequence_length", ")", ":", "\n", "    ", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size", ")", "\n", "model_parallel_size", "=", "mpu", ".", "get_model_parallel_world_size", "(", ")", "\n", "\n", "seed", "=", "12345", "\n", "set_random_seed", "(", "seed", ")", "\n", "\n", "num_att_heads", "=", "num_att_heads_per_partition", "*", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "hidden_size", "=", "hidden_size_per_att_head", "*", "num_att_heads", "\n", "\n", "# Network", "\n", "identity_layer", "=", "IdentityLayer3D", "(", "batch_size", ",", "sequence_length", ",", "\n", "hidden_size", ")", ".", "cuda", "(", ")", "\n", "attention_layer", "=", "mpu", ".", "BertParallelSelfAttention", "(", "hidden_size", ",", "num_att_heads", ",", "\n", "dropout_prob", ")", ".", "cuda", "(", ")", "\n", "loss_weight", "=", "torch", ".", "randn", "(", "[", "batch_size", ",", "sequence_length", ",", "hidden_size", "]", ")", ".", "cuda", "(", ")", "\n", "attention_mask", "=", "torch", ".", "randn", "(", "[", "batch_size", ",", "1", ",", "1", ",", "sequence_length", "]", ")", ".", "cuda", "(", ")", "\n", "# Forward", "\n", "input_", "=", "identity_layer", "(", ")", "\n", "output", "=", "attention_layer", "(", "input_", ",", "attention_mask", ")", "\n", "loss", "=", "torch", ".", "mul", "(", "output", ",", "loss_weight", ")", ".", "sum", "(", ")", "\n", "# Backward", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "rank", "=", "mpu", ".", "get_model_parallel_rank", "(", ")", "\n", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "return", "rank", ",", "hidden_size", ",", "model_parallel_size", ",", "loss", ",", "attention_layer", ",", "identity_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.test_parallel_self_attention": [[361, 411], ["test_layers..", "test_layers..", "loss_1.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "torch.cat", "torch.cat", "torch.cat.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "identity_layer_1.weight.grad.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "torch.split", "torch.split", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "loss_1.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.cat.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "identity_layer_1.weight.grad.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "loss_1.sub", "torch.cat.sub", "identity_layer_1.weight.grad.sub"], "function", ["None"], ["", "def", "test_parallel_self_attention", "(", "model_parallel_size", ")", ":", "\n", "\n", "    ", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing ParallelSelfAttention with model parallel '", "\n", "'size: {}'", ".", "format", "(", "model_parallel_size", ")", ")", "\n", "\n", "", "num_att_heads_per_partition", "=", "3", "\n", "hidden_size_per_att_head", "=", "7", "\n", "dropout_prob", "=", "0.0", "# has to be zero", "\n", "batch_size", "=", "5", "\n", "sequence_length", "=", "13", "\n", "\n", "rank_1", ",", "hideen_size_1", ",", "model_parallel_size_1", ",", "loss_1", ",", "attention_layer_1", ",", "identity_layer_1", "=", "parallel_self_attention", "(", "\n", "1", ",", "num_att_heads_per_partition", ",", "\n", "hidden_size_per_att_head", ",", "dropout_prob", ",", "batch_size", ",", "sequence_length", ")", "\n", "\n", "rank", ",", "hidden_size", ",", "model_parallel_size", ",", "loss", ",", "attention_layer", ",", "identity_layer", "=", "parallel_self_attention", "(", "\n", "model_parallel_size", ",", "num_att_heads_per_partition", ",", "\n", "hidden_size_per_att_head", ",", "dropout_prob", ",", "batch_size", ",", "sequence_length", ")", "\n", "assert", "hideen_size_1", "==", "hidden_size", "\n", "\n", "error", "=", "loss_1", ".", "sub", "(", "loss", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   loss error on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "5.0e-6", "\n", "\n", "my_lin_grad_list", "=", "torch", ".", "split", "(", "\n", "attention_layer_1", ".", "query_key_value", ".", "weight", ".", "grad", ",", "\n", "hidden_size", "//", "model_parallel_size", ",", "0", ")", "[", "rank", ":", ":", "model_parallel_size", "]", "\n", "my_lin_grad", "=", "torch", ".", "cat", "(", "my_lin_grad_list", ",", "dim", "=", "0", ")", "\n", "error", "=", "my_lin_grad", ".", "sub", "(", "\n", "attention_layer", ".", "query_key_value", ".", "weight", ".", "grad", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   weight gradient error on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "5.0e-6", "\n", "\n", "error", "=", "identity_layer_1", ".", "weight", ".", "grad", ".", "sub", "(", "\n", "identity_layer", ".", "weight", ".", "grad", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   input gradient error on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "5.0e-6", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "' >> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.parallel_transformer": [[412, 446], ["mpu.initialize_model_parallel", "mpu.get_model_parallel_world_size", "commons.set_random_seed", "IdentityLayer3D().cuda", "mpu.BertParallelTransformerLayer().cuda", "torch.randn().cuda", "torch.randn().cuda", "torch.randn().cuda", "torch.randn().cuda", "IdentityLayer3D().cuda.", "mpu.BertParallelTransformerLayer().cuda.", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum.backward", "mpu.get_model_parallel_rank", "mpu.destroy_model_parallel", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "test_layers.IdentityLayer3D", "mpu.BertParallelTransformerLayer", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.mul", "torch.mul"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.initialize_model_parallel", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_world_size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.commons.set_random_seed", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CheckpointFunction.backward", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_model_parallel_rank", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.destroy_model_parallel"], ["", "", "def", "parallel_transformer", "(", "model_parallel_size", ",", "num_att_heads_per_partition", ",", "\n", "hidden_size_per_att_head", ",", "batch_size", ",", "sequence_length", ")", ":", "\n", "\n", "    ", "mpu", ".", "initialize_model_parallel", "(", "model_parallel_size", ")", "\n", "model_parallel_size", "=", "mpu", ".", "get_model_parallel_world_size", "(", ")", "\n", "\n", "seed", "=", "12345", "\n", "set_random_seed", "(", "seed", ")", "\n", "\n", "num_att_heads", "=", "num_att_heads_per_partition", "*", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "hidden_size", "=", "hidden_size_per_att_head", "*", "num_att_heads", "\n", "intermediate_size", "=", "4", "*", "hidden_size", "\n", "\n", "# Network", "\n", "identity_layer", "=", "IdentityLayer3D", "(", "batch_size", ",", "sequence_length", ",", "\n", "hidden_size", ")", ".", "cuda", "(", ")", "\n", "transformer_layer", "=", "mpu", ".", "BertParallelTransformerLayer", "(", "\n", "hidden_size", ",", "intermediate_size", ",", "num_att_heads", ",", "0.0", ",", "0.0", ",", "\n", "torch", ".", "nn", ".", "functional", ".", "relu", ",", "1.0e-5", ")", ".", "cuda", "(", ")", "\n", "\n", "loss_weight", "=", "torch", ".", "randn", "(", "[", "batch_size", ",", "sequence_length", ",", "hidden_size", "]", ")", ".", "cuda", "(", ")", "\n", "attention_mask", "=", "torch", ".", "randn", "(", "[", "batch_size", ",", "1", ",", "1", ",", "sequence_length", "]", ")", ".", "cuda", "(", ")", "\n", "# Forward", "\n", "input_", "=", "identity_layer", "(", ")", "\n", "output", "=", "transformer_layer", "(", "input_", ",", "attention_mask", ")", "\n", "loss", "=", "torch", ".", "mul", "(", "output", ",", "loss_weight", ")", ".", "sum", "(", ")", "\n", "# Backward", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "rank", "=", "mpu", ".", "get_model_parallel_rank", "(", ")", "\n", "mpu", ".", "destroy_model_parallel", "(", ")", "\n", "return", "rank", ",", "hidden_size", ",", "model_parallel_size", ",", "loss", ",", "transformer_layer", ",", "identity_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.test_parallel_transformer_layer": [[448, 485], ["test_layers.parallel_transformer", "test_layers.parallel_transformer", "loss_1.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "identity_layer_1.weight.grad.sub().abs().max", "torch.distributed.barrier", "torch.distributed.barrier", "print", "torch.distributed.barrier", "torch.distributed.barrier", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "torch.distributed.get_rank", "torch.distributed.get_rank", "print", "loss_1.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "identity_layer_1.weight.grad.sub().abs", "torch.distributed.get_rank", "torch.distributed.get_rank", "loss_1.sub", "identity_layer_1.weight.grad.sub"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.parallel_transformer", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.tests.test_layers.parallel_transformer"], ["", "def", "test_parallel_transformer_layer", "(", "model_parallel_size", ")", ":", "\n", "\n", "    ", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "'> testing ParallelTransformerLayer with model parallel '", "\n", "'size: {}'", ".", "format", "(", "model_parallel_size", ")", ")", "\n", "\n", "", "num_att_heads_per_partition", "=", "3", "\n", "hidden_size_per_att_head", "=", "7", "\n", "batch_size", "=", "5", "\n", "sequence_length", "=", "13", "\n", "\n", "rank_1", ",", "hidden_size_1", ",", "model_parallel_size_1", ",", "loss_1", ",", "transformer_layer_1", ",", "identity_layer_1", "=", "parallel_transformer", "(", "\n", "1", ",", "num_att_heads_per_partition", ",", "\n", "hidden_size_per_att_head", ",", "batch_size", ",", "sequence_length", ")", "\n", "\n", "rank", ",", "hidden_size", ",", "model_parallel_size", ",", "loss", ",", "transformer_layer", ",", "identity_layer", "=", "parallel_transformer", "(", "\n", "model_parallel_size", ",", "num_att_heads_per_partition", ",", "\n", "hidden_size_per_att_head", ",", "batch_size", ",", "sequence_length", ")", "\n", "\n", "error", "=", "loss_1", ".", "sub", "(", "loss", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   loss error on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "5.0e-5", ",", "'error: {}'", ".", "format", "(", "error", ")", "\n", "\n", "error", "=", "identity_layer_1", ".", "weight", ".", "grad", ".", "sub", "(", "\n", "identity_layer", ".", "weight", ".", "grad", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "print", "(", "'   input gradient error on global rank {}: {}'", ".", "format", "(", "\n", "torch", ".", "distributed", ".", "get_rank", "(", ")", ",", "error", ")", ")", "\n", "assert", "error", "<", "5.0e-5", ",", "'error: {}'", ".", "format", "(", "error", ")", "\n", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "' >> passed the test :-)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.compile_helper": [[25, 31], ["os.path.abspath", "os.path.dirname"], "function", ["None"], ["def", "compile_helper", "(", ")", ":", "\n", "    ", "\"\"\"Compile helper function ar runtime. Make sure this\n    is invoked on a single process.\"\"\"", "\n", "import", "os", "\n", "import", "subprocess", "\n", "path", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ")", "\n", "#ret = subprocess.run(['make', '-C', path])", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.build_training_sample": [[38, 98], ["dataset_utils.get_a_and_b_segments", "dataset_utils.truncate_segments", "dataset_utils.create_tokens_and_tokentypes", "dataset_utils.create_masked_lm_predictions", "dataset_utils.pad_and_convert_to_numpy", "len", "len", "len", "int", "int"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.get_a_and_b_segments", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.truncate_segments", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.create_tokens_and_tokentypes", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.create_masked_lm_predictions", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.pad_and_convert_to_numpy"], ["", "def", "build_training_sample", "(", "sample", ",", "\n", "target_seq_length", ",", "max_seq_length", ",", "\n", "vocab_id_list", ",", "vocab_id_to_token_dict", ",", "\n", "cls_id", ",", "sep_id", ",", "mask_id", ",", "pad_id", ",", "\n", "masked_lm_prob", ",", "np_rng", ")", ":", "\n", "    ", "\"\"\"Biuld training sample.\n\n    Arguments:\n        sample: A list of sentences in which each sentence is a list token ids.\n        target_seq_length: Desired sequence length.\n        max_seq_length: Maximum length of the sequence. All values are padded to\n            this length.\n        vocab_id_list: List of vocabulary ids. Used to pick a random id.\n        vocab_id_to_token_dict: A dictionary from vocab ids to text tokens.\n        cls_id: Start of example id.\n        sep_id: Separator id.\n        mask_id: Mask token id.\n        pad_id: Padding token id.\n        masked_lm_prob: Probability to mask tokens.\n        np_rng: Random number genenrator. Note that this rng state should be\n              numpy and not python since python randint is inclusive for\n              the opper bound whereas the numpy one is exclusive.\n    \"\"\"", "\n", "\n", "# We assume that we have at least two sentences in the sample", "\n", "assert", "len", "(", "sample", ")", ">", "1", "\n", "assert", "target_seq_length", "<=", "max_seq_length", "\n", "\n", "# Divide sample into two segments (A and B).", "\n", "tokens_a", ",", "tokens_b", ",", "is_next_random", "=", "get_a_and_b_segments", "(", "sample", ",", "np_rng", ")", "\n", "\n", "# Truncate to `target_sequence_length`.", "\n", "max_num_tokens", "=", "target_seq_length", "\n", "truncated", "=", "truncate_segments", "(", "tokens_a", ",", "tokens_b", ",", "len", "(", "tokens_a", ")", ",", "\n", "len", "(", "tokens_b", ")", ",", "max_num_tokens", ",", "np_rng", ")", "\n", "\n", "# Build tokens and toketypes.", "\n", "tokens", ",", "tokentypes", "=", "create_tokens_and_tokentypes", "(", "tokens_a", ",", "tokens_b", ",", "\n", "cls_id", ",", "sep_id", ")", "\n", "\n", "# Masking.", "\n", "max_predictions_per_seq", "=", "masked_lm_prob", "*", "max_num_tokens", "\n", "(", "tokens", ",", "masked_positions", ",", "masked_labels", ",", "_", ")", "=", "create_masked_lm_predictions", "(", "\n", "tokens", ",", "vocab_id_list", ",", "vocab_id_to_token_dict", ",", "masked_lm_prob", ",", "\n", "cls_id", ",", "sep_id", ",", "mask_id", ",", "max_predictions_per_seq", ",", "np_rng", ")", "\n", "\n", "# Padding.", "\n", "tokens_np", ",", "tokentypes_np", ",", "labels_np", ",", "padding_mask_np", ",", "loss_mask_np", "=", "pad_and_convert_to_numpy", "(", "tokens", ",", "tokentypes", ",", "masked_positions", ",", "\n", "masked_labels", ",", "pad_id", ",", "max_seq_length", ")", "\n", "\n", "train_sample", "=", "{", "\n", "'text'", ":", "tokens_np", ",", "\n", "'types'", ":", "tokentypes_np", ",", "\n", "'labels'", ":", "labels_np", ",", "\n", "'is_random'", ":", "int", "(", "is_next_random", ")", ",", "\n", "'loss_mask'", ":", "loss_mask_np", ",", "\n", "'padding_mask'", ":", "padding_mask_np", ",", "\n", "'truncated'", ":", "int", "(", "truncated", ")", "}", "\n", "return", "train_sample", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.get_a_and_b_segments": [[100, 130], ["len", "range", "range", "np_rng.randint", "tokens_a.extend", "tokens_b.extend", "np_rng.random"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend"], ["", "def", "get_a_and_b_segments", "(", "sample", ",", "np_rng", ")", ":", "\n", "    ", "\"\"\"Divide sample into a and b segments.\"\"\"", "\n", "\n", "# Number of sentences in the sample.", "\n", "n_sentences", "=", "len", "(", "sample", ")", "\n", "# Make sure we always have two sentences.", "\n", "assert", "n_sentences", ">", "1", ",", "'make sure each sample has at least two sentences.'", "\n", "\n", "# First part:", "\n", "# `a_end` is how many sentences go into the `A`.", "\n", "a_end", "=", "1", "\n", "if", "n_sentences", ">=", "3", ":", "\n", "# Note that randin in numpy is exclusive.", "\n", "        ", "a_end", "=", "np_rng", ".", "randint", "(", "1", ",", "n_sentences", ")", "\n", "", "tokens_a", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "a_end", ")", ":", "\n", "        ", "tokens_a", ".", "extend", "(", "sample", "[", "j", "]", ")", "\n", "\n", "# Second part:", "\n", "", "tokens_b", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "a_end", ",", "n_sentences", ")", ":", "\n", "        ", "tokens_b", ".", "extend", "(", "sample", "[", "j", "]", ")", "\n", "\n", "# Random next:", "\n", "", "is_next_random", "=", "False", "\n", "if", "np_rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "        ", "is_next_random", "=", "True", "\n", "tokens_a", ",", "tokens_b", "=", "tokens_b", ",", "tokens_a", "\n", "\n", "", "return", "tokens_a", ",", "tokens_b", ",", "is_next_random", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.truncate_segments": [[132, 151], ["np_rng.random", "tokens.pop"], "function", ["None"], ["", "def", "truncate_segments", "(", "tokens_a", ",", "tokens_b", ",", "len_a", ",", "len_b", ",", "max_num_tokens", ",", "np_rng", ")", ":", "\n", "    ", "\"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"", "\n", "#print(len_a, len_b, max_num_tokens)", "\n", "assert", "len_a", ">", "0", "\n", "assert", "len_b", ">", "0", "\n", "if", "len_a", "+", "len_b", "<=", "max_num_tokens", ":", "\n", "        ", "return", "False", "\n", "", "while", "len_a", "+", "len_b", ">", "max_num_tokens", ":", "\n", "        ", "if", "len_a", ">", "len_b", ":", "\n", "            ", "len_a", "-=", "1", "\n", "tokens", "=", "tokens_a", "\n", "", "else", ":", "\n", "            ", "len_b", "-=", "1", "\n", "tokens", "=", "tokens_b", "\n", "", "if", "np_rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "            ", "del", "tokens", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "tokens", ".", "pop", "(", ")", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.create_tokens_and_tokentypes": [[153, 177], ["tokens.append", "tokentypes.append", "tokens.append", "tokentypes.append", "tokens.append", "tokentypes.append", "tokens.append", "tokentypes.append", "tokens.append", "tokentypes.append"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "create_tokens_and_tokentypes", "(", "tokens_a", ",", "tokens_b", ",", "cls_id", ",", "sep_id", ")", ":", "\n", "    ", "\"\"\"Merge segments A and B, add [CLS] and [SEP] and build tokentypes.\"\"\"", "\n", "\n", "tokens", "=", "[", "]", "\n", "tokentypes", "=", "[", "]", "\n", "# [CLS].", "\n", "tokens", ".", "append", "(", "cls_id", ")", "\n", "tokentypes", ".", "append", "(", "0", ")", "\n", "# Segment A.", "\n", "for", "token", "in", "tokens_a", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "tokentypes", ".", "append", "(", "0", ")", "\n", "# [SEP].", "\n", "", "tokens", ".", "append", "(", "sep_id", ")", "\n", "tokentypes", ".", "append", "(", "0", ")", "\n", "# Segment B.", "\n", "for", "token", "in", "tokens_b", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "tokentypes", ".", "append", "(", "1", ")", "\n", "# [SEP].", "\n", "", "tokens", ".", "append", "(", "sep_id", ")", "\n", "tokentypes", ".", "append", "(", "1", ")", "\n", "\n", "return", "tokens", ",", "tokentypes", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.is_start_piece": [[183, 190], ["piece.startswith"], "function", ["None"], ["def", "is_start_piece", "(", "piece", ")", ":", "\n", "    ", "\"\"\"Check if the current word piece is the starting piece (BERT).\"\"\"", "\n", "# When a word has been split into", "\n", "# WordPieces, the first token does not have any marker and any subsequence", "\n", "# tokens are prefixed with ##. So whenever we see the ## token, we", "\n", "# append it to the previous set of word indexes.", "\n", "return", "not", "piece", ".", "startswith", "(", "\"##\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.create_masked_lm_predictions": [[192, 374], ["enumerate", "list", "min", "numpy.arange", "pvals.sum", "range", "np_rng.shuffle", "set", "np_rng.shuffle", "set", "sorted", "len", "max", "numpy.arange", "len", "ngram_indexes.append", "np_rng.choice", "sum", "len", "sorted", "list", "np_rng.shuffle", "list", "zip", "masked_lm_positions.append", "masked_lm_labels.append", "cand_indexes[].append", "cand_indexes.append", "dataset_utils.is_start_piece", "int", "ngram_index.append", "len", "sum", "set.add", "sorted.append", "numpy.random.choice", "sum", "len", "sorted.append", "len", "dataset_utils.is_start_piece", "round", "len", "len", "len", "len", "np_rng.random", "MaskedLmInstance", "len", "sum", "sorted.add", "MaskedLmInstance", "len", "pvals[].sum", "np_rng.random", "len", "len", "len", "len", "len", "len", "pvals[].sum", "len", "np_rng.randint", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.is_start_piece", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.add", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.is_start_piece", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.random.CudaRNGStatesTracker.add"], ["", "def", "create_masked_lm_predictions", "(", "tokens", ",", "\n", "vocab_id_list", ",", "vocab_id_to_token_dict", ",", "\n", "masked_lm_prob", ",", "\n", "cls_id", ",", "sep_id", ",", "mask_id", ",", "\n", "max_predictions_per_seq", ",", "\n", "np_rng", ",", "\n", "max_ngrams", "=", "3", ",", "\n", "do_whole_word_mask", "=", "True", ",", "\n", "favor_longer_ngram", "=", "False", ",", "\n", "do_permutation", "=", "False", ")", ":", "\n", "    ", "\"\"\"Creates the predictions for the masked LM objective.\n    Note: Tokens here are vocab ids and not text tokens.\"\"\"", "\n", "\n", "cand_indexes", "=", "[", "]", "\n", "# Note(mingdachen): We create a list for recording if the piece is", "\n", "# the starting piece of current token, where 1 means true, so that", "\n", "# on-the-fly whole word masking is possible.", "\n", "token_boundary", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "if", "token", "==", "cls_id", "or", "token", "==", "sep_id", ":", "\n", "            ", "token_boundary", "[", "i", "]", "=", "1", "\n", "continue", "\n", "# Whole Word Masking means that if we mask all of the wordpieces", "\n", "# corresponding to an original word.", "\n", "#", "\n", "# Note that Whole Word Masking does *not* change the training code", "\n", "# at all -- we still predict each WordPiece independently, softmaxed", "\n", "# over the entire vocabulary.", "\n", "", "if", "(", "do_whole_word_mask", "and", "len", "(", "cand_indexes", ")", ">=", "1", "and", "\n", "not", "is_start_piece", "(", "vocab_id_to_token_dict", "[", "token", "]", ")", ")", ":", "\n", "            ", "cand_indexes", "[", "-", "1", "]", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "            ", "cand_indexes", ".", "append", "(", "[", "i", "]", ")", "\n", "if", "is_start_piece", "(", "vocab_id_to_token_dict", "[", "token", "]", ")", ":", "\n", "                ", "token_boundary", "[", "i", "]", "=", "1", "\n", "\n", "", "", "", "output_tokens", "=", "list", "(", "tokens", ")", "\n", "\n", "masked_lm_positions", "=", "[", "]", "\n", "masked_lm_labels", "=", "[", "]", "\n", "\n", "if", "masked_lm_prob", "==", "0", ":", "\n", "        ", "return", "(", "output_tokens", ",", "masked_lm_positions", ",", "\n", "masked_lm_labels", ",", "token_boundary", ")", "\n", "\n", "", "num_to_predict", "=", "min", "(", "max_predictions_per_seq", ",", "\n", "max", "(", "1", ",", "int", "(", "round", "(", "len", "(", "tokens", ")", "*", "masked_lm_prob", ")", ")", ")", ")", "\n", "\n", "# Note(mingdachen):", "\n", "# By default, we set the probilities to favor shorter ngram sequences.", "\n", "ngrams", "=", "np", ".", "arange", "(", "1", ",", "max_ngrams", "+", "1", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "pvals", "=", "1.", "/", "np", ".", "arange", "(", "1", ",", "max_ngrams", "+", "1", ")", "\n", "pvals", "/=", "pvals", ".", "sum", "(", "keepdims", "=", "True", ")", "\n", "\n", "if", "favor_longer_ngram", ":", "\n", "        ", "pvals", "=", "pvals", "[", ":", ":", "-", "1", "]", "\n", "\n", "", "ngram_indexes", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "len", "(", "cand_indexes", ")", ")", ":", "\n", "        ", "ngram_index", "=", "[", "]", "\n", "for", "n", "in", "ngrams", ":", "\n", "            ", "ngram_index", ".", "append", "(", "cand_indexes", "[", "idx", ":", "idx", "+", "n", "]", ")", "\n", "", "ngram_indexes", ".", "append", "(", "ngram_index", ")", "\n", "\n", "", "np_rng", ".", "shuffle", "(", "ngram_indexes", ")", "\n", "\n", "masked_lms", "=", "[", "]", "\n", "covered_indexes", "=", "set", "(", ")", "\n", "for", "cand_index_set", "in", "ngram_indexes", ":", "\n", "        ", "if", "len", "(", "masked_lms", ")", ">=", "num_to_predict", ":", "\n", "            ", "break", "\n", "", "if", "not", "cand_index_set", ":", "\n", "            ", "continue", "\n", "# Note(mingdachen):", "\n", "# Skip current piece if they are covered in lm masking or previous ngrams.", "\n", "", "for", "index_set", "in", "cand_index_set", "[", "0", "]", ":", "\n", "            ", "for", "index", "in", "index_set", ":", "\n", "                ", "if", "index", "in", "covered_indexes", ":", "\n", "                    ", "continue", "\n", "\n", "", "", "", "n", "=", "np_rng", ".", "choice", "(", "ngrams", "[", ":", "len", "(", "cand_index_set", ")", "]", ",", "\n", "p", "=", "pvals", "[", ":", "len", "(", "cand_index_set", ")", "]", "/", "\n", "pvals", "[", ":", "len", "(", "cand_index_set", ")", "]", ".", "sum", "(", "keepdims", "=", "True", ")", ")", "\n", "index_set", "=", "sum", "(", "cand_index_set", "[", "n", "-", "1", "]", ",", "[", "]", ")", "\n", "n", "-=", "1", "\n", "# Note(mingdachen):", "\n", "# Repeatedly looking for a candidate that does not exceed the", "\n", "# maximum number of predictions by trying shorter ngrams.", "\n", "while", "len", "(", "masked_lms", ")", "+", "len", "(", "index_set", ")", ">", "num_to_predict", ":", "\n", "            ", "if", "n", "==", "0", ":", "\n", "                ", "break", "\n", "", "index_set", "=", "sum", "(", "cand_index_set", "[", "n", "-", "1", "]", ",", "[", "]", ")", "\n", "n", "-=", "1", "\n", "# If adding a whole-word mask would exceed the maximum number of", "\n", "# predictions, then just skip this candidate.", "\n", "", "if", "len", "(", "masked_lms", ")", "+", "len", "(", "index_set", ")", ">", "num_to_predict", ":", "\n", "            ", "continue", "\n", "", "is_any_index_covered", "=", "False", "\n", "for", "index", "in", "index_set", ":", "\n", "            ", "if", "index", "in", "covered_indexes", ":", "\n", "                ", "is_any_index_covered", "=", "True", "\n", "break", "\n", "", "", "if", "is_any_index_covered", ":", "\n", "            ", "continue", "\n", "", "for", "index", "in", "index_set", ":", "\n", "            ", "covered_indexes", ".", "add", "(", "index", ")", "\n", "\n", "masked_token", "=", "None", "\n", "# 80% of the time, replace with [MASK]", "\n", "if", "np_rng", ".", "random", "(", ")", "<", "0.8", ":", "\n", "                ", "masked_token", "=", "mask_id", "\n", "", "else", ":", "\n", "# 10% of the time, keep original", "\n", "                ", "if", "np_rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                    ", "masked_token", "=", "tokens", "[", "index", "]", "\n", "# 10% of the time, replace with random word", "\n", "", "else", ":", "\n", "                    ", "masked_token", "=", "vocab_id_list", "[", "np_rng", ".", "randint", "(", "0", ",", "len", "(", "vocab_id_list", ")", ")", "]", "\n", "\n", "", "", "output_tokens", "[", "index", "]", "=", "masked_token", "\n", "\n", "masked_lms", ".", "append", "(", "MaskedLmInstance", "(", "index", "=", "index", ",", "label", "=", "tokens", "[", "index", "]", ")", ")", "\n", "", "", "assert", "len", "(", "masked_lms", ")", "<=", "num_to_predict", "\n", "\n", "np_rng", ".", "shuffle", "(", "ngram_indexes", ")", "\n", "\n", "select_indexes", "=", "set", "(", ")", "\n", "if", "do_permutation", ":", "\n", "        ", "for", "cand_index_set", "in", "ngram_indexes", ":", "\n", "            ", "if", "len", "(", "select_indexes", ")", ">=", "num_to_predict", ":", "\n", "                ", "break", "\n", "", "if", "not", "cand_index_set", ":", "\n", "                ", "continue", "\n", "# Note(mingdachen):", "\n", "# Skip current piece if they are covered in lm masking or previous ngrams.", "\n", "", "for", "index_set", "in", "cand_index_set", "[", "0", "]", ":", "\n", "                ", "for", "index", "in", "index_set", ":", "\n", "                    ", "if", "index", "in", "covered_indexes", "or", "index", "in", "select_indexes", ":", "\n", "                        ", "continue", "\n", "\n", "", "", "", "n", "=", "np", ".", "random", ".", "choice", "(", "ngrams", "[", ":", "len", "(", "cand_index_set", ")", "]", ",", "\n", "p", "=", "pvals", "[", ":", "len", "(", "cand_index_set", ")", "]", "/", "\n", "pvals", "[", ":", "len", "(", "cand_index_set", ")", "]", ".", "sum", "(", "keepdims", "=", "True", ")", ")", "\n", "index_set", "=", "sum", "(", "cand_index_set", "[", "n", "-", "1", "]", ",", "[", "]", ")", "\n", "n", "-=", "1", "\n", "\n", "while", "len", "(", "select_indexes", ")", "+", "len", "(", "index_set", ")", ">", "num_to_predict", ":", "\n", "                ", "if", "n", "==", "0", ":", "\n", "                    ", "break", "\n", "", "index_set", "=", "sum", "(", "cand_index_set", "[", "n", "-", "1", "]", ",", "[", "]", ")", "\n", "n", "-=", "1", "\n", "# If adding a whole-word mask would exceed the maximum number of", "\n", "# predictions, then just skip this candidate.", "\n", "", "if", "len", "(", "select_indexes", ")", "+", "len", "(", "index_set", ")", ">", "num_to_predict", ":", "\n", "                ", "continue", "\n", "", "is_any_index_covered", "=", "False", "\n", "for", "index", "in", "index_set", ":", "\n", "                ", "if", "index", "in", "covered_indexes", "or", "index", "in", "select_indexes", ":", "\n", "                    ", "is_any_index_covered", "=", "True", "\n", "break", "\n", "", "", "if", "is_any_index_covered", ":", "\n", "                ", "continue", "\n", "", "for", "index", "in", "index_set", ":", "\n", "                ", "select_indexes", ".", "add", "(", "index", ")", "\n", "", "", "assert", "len", "(", "select_indexes", ")", "<=", "num_to_predict", "\n", "\n", "select_indexes", "=", "sorted", "(", "select_indexes", ")", "\n", "permute_indexes", "=", "list", "(", "select_indexes", ")", "\n", "np_rng", ".", "shuffle", "(", "permute_indexes", ")", "\n", "orig_token", "=", "list", "(", "output_tokens", ")", "\n", "\n", "for", "src_i", ",", "tgt_i", "in", "zip", "(", "select_indexes", ",", "permute_indexes", ")", ":", "\n", "            ", "output_tokens", "[", "src_i", "]", "=", "orig_token", "[", "tgt_i", "]", "\n", "masked_lms", ".", "append", "(", "MaskedLmInstance", "(", "index", "=", "src_i", ",", "label", "=", "orig_token", "[", "src_i", "]", ")", ")", "\n", "\n", "", "", "masked_lms", "=", "sorted", "(", "masked_lms", ",", "key", "=", "lambda", "x", ":", "x", ".", "index", ")", "\n", "\n", "for", "p", "in", "masked_lms", ":", "\n", "        ", "masked_lm_positions", ".", "append", "(", "p", ".", "index", ")", "\n", "masked_lm_labels", ".", "append", "(", "p", ".", "label", ")", "\n", "", "return", "(", "output_tokens", ",", "masked_lm_positions", ",", "masked_lm_labels", ",", "token_boundary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.pad_and_convert_to_numpy": [[376, 407], ["len", "numpy.array", "numpy.array", "numpy.array", "range", "numpy.array", "numpy.array", "len", "len", "len", "len"], "function", ["None"], ["", "def", "pad_and_convert_to_numpy", "(", "tokens", ",", "tokentypes", ",", "masked_positions", ",", "\n", "masked_labels", ",", "pad_id", ",", "max_seq_length", ")", ":", "\n", "    ", "\"\"\"Pad sequences and convert them to numpy.\"\"\"", "\n", "\n", "# Some checks.", "\n", "num_tokens", "=", "len", "(", "tokens", ")", "\n", "padding_length", "=", "max_seq_length", "-", "num_tokens", "\n", "assert", "padding_length", ">=", "0", "\n", "assert", "len", "(", "tokentypes", ")", "==", "num_tokens", "\n", "assert", "len", "(", "masked_positions", ")", "==", "len", "(", "masked_labels", ")", "\n", "\n", "# Tokens and token types.", "\n", "filler", "=", "[", "pad_id", "]", "*", "padding_length", "\n", "tokens_np", "=", "np", ".", "array", "(", "tokens", "+", "filler", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "tokentypes_np", "=", "np", ".", "array", "(", "tokentypes", "+", "filler", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "# Padding mask.", "\n", "padding_mask_np", "=", "np", ".", "array", "(", "[", "1", "]", "*", "num_tokens", "+", "[", "0", "]", "*", "padding_length", ",", "\n", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "# Lables and loss mask.", "\n", "labels", "=", "[", "-", "1", "]", "*", "max_seq_length", "\n", "loss_mask", "=", "[", "0", "]", "*", "max_seq_length", "\n", "for", "i", "in", "range", "(", "len", "(", "masked_positions", ")", ")", ":", "\n", "        ", "assert", "masked_positions", "[", "i", "]", "<", "num_tokens", "\n", "labels", "[", "masked_positions", "[", "i", "]", "]", "=", "masked_labels", "[", "i", "]", "\n", "loss_mask", "[", "masked_positions", "[", "i", "]", "]", "=", "1", "\n", "", "labels_np", "=", "np", ".", "array", "(", "labels", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "loss_mask_np", "=", "np", ".", "array", "(", "loss_mask", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "return", "tokens_np", ",", "tokentypes_np", ",", "labels_np", ",", "padding_mask_np", ",", "loss_mask_np", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.RandomSampler.__init__": [[35, 53], ["ValueError", "ValueError", "isinstance", "ValueError", "isinstance"], "methods", ["None"], ["\n", "def", "__init__", "(", "self", ",", "data_source", ",", "replacement", "=", "False", ",", "num_samples", "=", "None", ")", ":", "\n", "        ", "self", ".", "data_source", "=", "data_source", "\n", "self", ".", "replacement", "=", "replacement", "\n", "self", ".", "_num_samples", "=", "num_samples", "\n", "self", ".", "epoch", "=", "-", "1", "\n", "\n", "if", "self", ".", "_num_samples", "is", "not", "None", "and", "replacement", "is", "False", ":", "\n", "            ", "raise", "ValueError", "(", "\"With replacement=False, num_samples should not be specified, \"", "\n", "\"since a random permute will be performed.\"", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "self", ".", "num_samples", ",", "int", ")", "or", "self", ".", "num_samples", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"num_samples should be a positive integer \"", "\n", "\"value, but got num_samples={}\"", ".", "format", "(", "self", ".", "num_samples", ")", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "replacement", ",", "bool", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"replacement should be a boolean value, but got \"", "\n", "\"replacement={}\"", ".", "format", "(", "self", ".", "replacement", ")", ")", "\n", "\n", "", "", "@", "property", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.RandomSampler.num_samples": [[54, 60], ["len"], "methods", ["None"], ["def", "num_samples", "(", "self", ")", ":", "\n", "# dataset size might change at runtime", "\n", "        ", "if", "self", ".", "_num_samples", "is", "None", ":", "\n", "            ", "return", "len", "(", "self", ".", "data_source", ")", "\n", "", "return", "self", ".", "_num_samples", "\n", "\n", "", "def", "__iter__", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.RandomSampler.__iter__": [[61, 70], ["len", "torch.Generator", "iter", "torch.Generator.manual_seed", "iter", "torch.randperm().tolist", "torch.randint().tolist", "torch.randperm", "torch.randint"], "methods", ["None"], ["        ", "n", "=", "len", "(", "self", ".", "data_source", ")", "\n", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "if", "self", ".", "epoch", ">=", "0", ":", "\n", "            ", "g", ".", "manual_seed", "(", "self", ".", "epoch", ")", "\n", "", "if", "self", ".", "replacement", ":", "\n", "            ", "return", "iter", "(", "torch", ".", "randint", "(", "high", "=", "n", ",", "size", "=", "(", "self", ".", "num_samples", ",", ")", ",", "dtype", "=", "torch", ".", "int64", ",", "generator", "=", "g", ")", ".", "tolist", "(", ")", ")", "\n", "", "return", "iter", "(", "torch", ".", "randperm", "(", "n", ",", "generator", "=", "g", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_samples", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.RandomSampler.__len__": [[71, 73], ["None"], "methods", ["None"], ["\n", "", "def", "set_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "self", ".", "epoch", "=", "epoch", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.RandomSampler.set_epoch": [[74, 76], ["None"], "methods", ["None"], ["\n", "", "", "class", "DistributedBatchSampler", "(", "data", ".", "sampler", ".", "BatchSampler", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.DistributedBatchSampler.__init__": [[95, 109], ["super().__init__", "torch.distributed.get_rank"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["last_batch", "=", "None", "\n", "i", "=", "0", "\n", "for", "idx", "in", "self", ".", "data_iterator", "(", "self", ".", "sampler", ",", "wrap_around", "=", "False", ")", ":", "\n", "            ", "batch", ".", "append", "(", "idx", ")", "\n", "if", "len", "(", "batch", ")", "==", "self", ".", "batch_size", ":", "\n", "                ", "tbatch", "=", "self", ".", "_batch", "(", "batch", ")", "\n", "if", "i", ">=", "self", ".", "start_iter", ":", "\n", "                    ", "yield", "tbatch", "\n", "self", ".", "start_iter", "=", "0", "\n", "", "i", "+=", "1", "\n", "last_batch", "=", "np", ".", "array", "(", "list", "(", "tbatch", ")", ")", "\n", "batch", "=", "[", "]", "\n", "", "", "batch_len", "=", "len", "(", "batch", ")", "\n", "if", "batch_len", ">", "0", "and", "not", "self", ".", "drop_last", ":", "\n", "            ", "if", "self", ".", "wrap_last", ":", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.DistributedBatchSampler.__iter__": [[110, 131], ["samplers.DistributedBatchSampler.data_iterator", "len", "batch.append", "len", "samplers.DistributedBatchSampler._batch", "len", "samplers.DistributedBatchSampler._batch"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.DistributedBatchSampler.data_iterator", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.DistributedBatchSampler._batch", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.DistributedBatchSampler._batch"], ["                ", "self", ".", "sampler", ".", "wrap_around", "-=", "(", "self", ".", "batch_size", ")", "\n", "self", ".", "wrap_around", "+=", "(", "len", "(", "batch", ")", ")", "\n", "self", ".", "wrap_around", "%=", "self", ".", "batch_size", "\n", "if", "isinstance", "(", "self", ".", "sampler", ",", "TransposedSampler", ")", ":", "\n", "                    ", "for", "i", ",", "idx", "in", "enumerate", "(", "self", ".", "data_iterator", "(", "self", ".", "sampler", ",", "wrap_around", "=", "True", ")", ")", ":", "\n", "                        ", "if", "i", "==", "0", ":", "\n", "                            ", "continue", "\n", "", "batch", ".", "append", "(", "idx", ")", "\n", "new_batch_len", "=", "len", "(", "batch", ")", "\n", "if", "len", "(", "batch", ")", "==", "self", ".", "batch_size", ":", "\n", "                            ", "break", "\n", "", "", "", "", "yield", "self", ".", "_batch", "(", "batch", ")", "\n", "", "if", "self", ".", "wrap_last", ":", "\n", "            ", "self", ".", "sampler", ".", "wrap_around", "+=", "self", ".", "batch_size", "\n", "\n", "", "", "def", "data_iterator", "(", "self", ",", "_iter", ",", "wrap_around", "=", "False", ")", ":", "\n", "        ", "\"\"\"iterates through data and handles wrap around\"\"\"", "\n", "for", "i", ",", "idx", "in", "enumerate", "(", "_iter", ")", ":", "\n", "            ", "if", "i", "<", "self", ".", "wrap_around", "%", "self", ".", "batch_size", ":", "\n", "                ", "continue", "\n", "", "if", "wrap_around", ":", "\n", "                ", "self", ".", "wrap_around", "+=", "1", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.DistributedBatchSampler.data_iterator": [[132, 141], ["enumerate"], "methods", ["None"], ["self", ".", "wrap_around", "%=", "self", ".", "batch_size", "\n", "", "yield", "idx", "\n", "\n", "", "", "def", "_batch", "(", "self", ",", "batch", ")", ":", "\n", "        ", "\"\"\"extracts samples only pertaining to this worker's batch\"\"\"", "\n", "start", "=", "self", ".", "rank", "*", "self", ".", "batch_size", "//", "self", ".", "world_size", "\n", "end", "=", "(", "self", ".", "rank", "+", "1", ")", "*", "self", ".", "batch_size", "//", "self", ".", "world_size", "\n", "return", "batch", "[", "start", ":", "end", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.samplers.DistributedBatchSampler._batch": [[142, 149], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.__init__": [[131, 136], ["super().__init__", "indexed_dataset.IndexedDataset.read_index"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.read_index"], ["def", "__init__", "(", "self", ",", "path", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "path", "=", "path", "\n", "self", ".", "data_file", "=", "None", "\n", "self", ".", "read_index", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.read_index": [[137, 154], ["open", "f.read", "f.read", "struct.unpack", "struct.unpack", "struct.unpack", "indexed_dataset.read_longs", "indexed_dataset.read_longs", "indexed_dataset.read_longs", "indexed_dataset.read_longs", "indexed_dataset.index_file_path", "struct.unpack", "f.read", "f.read", "f.read"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.index_file_path"], ["", "def", "read_index", "(", "self", ",", "path", ")", ":", "\n", "        ", "with", "open", "(", "index_file_path", "(", "path", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "magic", "=", "f", ".", "read", "(", "8", ")", "\n", "assert", "magic", "==", "self", ".", "_HDR_MAGIC", ",", "(", "\n", "'Index file doesn\\'t match expected format. '", "\n", "'Make sure that --dataset-impl is configured properly.'", "\n", ")", "\n", "version", "=", "f", ".", "read", "(", "8", ")", "\n", "assert", "struct", ".", "unpack", "(", "'<Q'", ",", "version", ")", "==", "(", "1", ",", ")", "\n", "code", ",", "self", ".", "element_size", "=", "struct", ".", "unpack", "(", "'<QQ'", ",", "f", ".", "read", "(", "16", ")", ")", "\n", "self", ".", "dtype", "=", "dtypes", "[", "code", "]", "\n", "self", ".", "_len", ",", "self", ".", "s", "=", "struct", ".", "unpack", "(", "'<QQ'", ",", "f", ".", "read", "(", "16", ")", ")", "\n", "self", ".", "doc_count", "=", "struct", ".", "unpack", "(", "'<Q'", ",", "f", ".", "read", "(", "8", ")", ")", "\n", "self", ".", "dim_offsets", "=", "read_longs", "(", "f", ",", "self", ".", "_len", "+", "1", ")", "\n", "self", ".", "data_offsets", "=", "read_longs", "(", "f", ",", "self", ".", "_len", "+", "1", ")", "\n", "self", ".", "sizes", "=", "read_longs", "(", "f", ",", "self", ".", "s", ")", "\n", "self", ".", "doc_idx", "=", "read_longs", "(", "f", ",", "self", ".", "doc_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.read_data": [[155, 157], ["open", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.data_file_path"], ["", "", "def", "read_data", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "data_file", "=", "open", "(", "data_file_path", "(", "path", ")", ",", "'rb'", ",", "buffering", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.check_index": [[158, 161], ["IndexError"], "methods", ["None"], ["", "def", "check_index", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "i", "<", "0", "or", "i", ">=", "self", ".", "_len", ":", "\n", "            ", "raise", "IndexError", "(", "'index out of range'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.__del__": [[162, 165], ["indexed_dataset.IndexedDataset.data_file.close"], "methods", ["None"], ["", "", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "data_file", ":", "\n", "            ", "self", ".", "data_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.__getitem__": [[167, 190], ["isinstance", "indexed_dataset.IndexedDataset.read_data", "indexed_dataset.IndexedDataset.check_index", "numpy.empty", "indexed_dataset.IndexedDataset.data_file.seek", "indexed_dataset.IndexedDataset.data_file.readinto", "isinstance", "idx.indices", "sum", "numpy.empty", "indexed_dataset.IndexedDataset.data_file.seek", "indexed_dataset.IndexedDataset.data_file.readinto", "list", "numpy.split", "len", "ValueError", "itertools.accumulate"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.read_data", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.check_index"], ["", "", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "if", "not", "self", ".", "data_file", ":", "\n", "            ", "self", ".", "read_data", "(", "self", ".", "path", ")", "\n", "", "if", "isinstance", "(", "idx", ",", "int", ")", ":", "\n", "            ", "i", "=", "idx", "\n", "self", ".", "check_index", "(", "i", ")", "\n", "tensor_size", "=", "self", ".", "sizes", "[", "self", ".", "dim_offsets", "[", "i", "]", ":", "self", ".", "dim_offsets", "[", "i", "+", "1", "]", "]", "\n", "a", "=", "np", ".", "empty", "(", "tensor_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "data_file", ".", "seek", "(", "self", ".", "data_offsets", "[", "i", "]", "*", "self", ".", "element_size", ")", "\n", "self", ".", "data_file", ".", "readinto", "(", "a", ")", "\n", "return", "a", "\n", "", "elif", "isinstance", "(", "idx", ",", "slice", ")", ":", "\n", "            ", "start", ",", "stop", ",", "step", "=", "idx", ".", "indices", "(", "len", "(", "self", ")", ")", "\n", "if", "step", "!=", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\"Slices into indexed_dataset must be contiguous\"", ")", "\n", "", "sizes", "=", "self", ".", "sizes", "[", "self", ".", "dim_offsets", "[", "start", "]", ":", "self", ".", "dim_offsets", "[", "stop", "]", "]", "\n", "size", "=", "sum", "(", "sizes", ")", "\n", "a", "=", "np", ".", "empty", "(", "size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "data_file", ".", "seek", "(", "self", ".", "data_offsets", "[", "start", "]", "*", "self", ".", "element_size", ")", "\n", "self", ".", "data_file", ".", "readinto", "(", "a", ")", "\n", "offsets", "=", "list", "(", "accumulate", "(", "sizes", ")", ")", "\n", "sents", "=", "np", ".", "split", "(", "a", ",", "offsets", "[", ":", "-", "1", "]", ")", "\n", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.__len__": [[191, 193], ["None"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_len", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.num_tokens": [[194, 196], ["None"], "methods", ["None"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size": [[197, 199], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.exists": [[200, 204], ["os.path.exists", "os.path.exists", "indexed_dataset.index_file_path", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.index_file_path", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.data_file_path"], ["", "@", "staticmethod", "\n", "def", "exists", "(", "path", ")", ":", "\n", "        ", "return", "(", "\n", "os", ".", "path", ".", "exists", "(", "index_file_path", "(", "path", ")", ")", "and", "os", ".", "path", ".", "exists", "(", "data_file_path", "(", "path", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.supports_prefetch": [[206, 209], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "False", "# avoid prefetching to save memory", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedCachedDataset.__init__": [[213, 217], ["indexed_dataset.IndexedDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "path", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "path", ")", "\n", "self", ".", "cache", "=", "None", "\n", "self", ".", "cache_index", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedCachedDataset.supports_prefetch": [[218, 221], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedCachedDataset.prefetch": [[222, 245], ["all", "sorted", "numpy.empty", "indexed_dataset.IndexedCachedDataset.cache_index.clear", "indexed_dataset.IndexedCachedDataset.read_data", "set", "indexed_dataset.IndexedCachedDataset.data_file.seek", "indexed_dataset.IndexedCachedDataset.data_file.readinto", "indexed_dataset.IndexedCachedDataset.data_file.close"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.read_data"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "if", "all", "(", "i", "in", "self", ".", "cache_index", "for", "i", "in", "indices", ")", ":", "\n", "            ", "return", "\n", "", "if", "not", "self", ".", "data_file", ":", "\n", "            ", "self", ".", "read_data", "(", "self", ".", "path", ")", "\n", "", "indices", "=", "sorted", "(", "set", "(", "indices", ")", ")", "\n", "total_size", "=", "0", "\n", "for", "i", "in", "indices", ":", "\n", "            ", "total_size", "+=", "self", ".", "data_offsets", "[", "i", "+", "1", "]", "-", "self", ".", "data_offsets", "[", "i", "]", "\n", "", "self", ".", "cache", "=", "np", ".", "empty", "(", "total_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "ptx", "=", "0", "\n", "self", ".", "cache_index", ".", "clear", "(", ")", "\n", "for", "i", "in", "indices", ":", "\n", "            ", "self", ".", "cache_index", "[", "i", "]", "=", "ptx", "\n", "size", "=", "self", ".", "data_offsets", "[", "i", "+", "1", "]", "-", "self", ".", "data_offsets", "[", "i", "]", "\n", "a", "=", "self", ".", "cache", "[", "ptx", ":", "ptx", "+", "size", "]", "\n", "self", ".", "data_file", ".", "seek", "(", "self", ".", "data_offsets", "[", "i", "]", "*", "self", ".", "element_size", ")", "\n", "self", ".", "data_file", ".", "readinto", "(", "a", ")", "\n", "ptx", "+=", "size", "\n", "", "if", "self", ".", "data_file", ":", "\n", "# close and delete data file after prefetch so we can pickle", "\n", "            ", "self", ".", "data_file", ".", "close", "(", ")", "\n", "self", ".", "data_file", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedCachedDataset.__getitem__": [[247, 262], ["isinstance", "indexed_dataset.IndexedCachedDataset.check_index", "numpy.empty", "numpy.copyto", "isinstance", "range", "sents.append", "idx.indices", "len"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.check_index", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "if", "isinstance", "(", "idx", ",", "int", ")", ":", "\n", "            ", "i", "=", "idx", "\n", "self", ".", "check_index", "(", "i", ")", "\n", "tensor_size", "=", "self", ".", "sizes", "[", "self", ".", "dim_offsets", "[", "i", "]", ":", "self", ".", "dim_offsets", "[", "i", "+", "1", "]", "]", "\n", "a", "=", "np", ".", "empty", "(", "tensor_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "ptx", "=", "self", ".", "cache_index", "[", "i", "]", "\n", "np", ".", "copyto", "(", "a", ",", "self", ".", "cache", "[", "ptx", ":", "ptx", "+", "a", ".", "size", "]", ")", "\n", "return", "a", "\n", "", "elif", "isinstance", "(", "idx", ",", "slice", ")", ":", "\n", "# Hack just to make this work, can optimizer later if necessary", "\n", "            ", "sents", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "*", "idx", ".", "indices", "(", "len", "(", "self", ")", ")", ")", ":", "\n", "                ", "sents", ".", "append", "(", "self", "[", "i", "]", ")", "\n", "", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDatasetBuilder.__init__": [[275, 283], ["open"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "out_file", ",", "dtype", "=", "np", ".", "int32", ")", ":", "\n", "        ", "self", ".", "out_file", "=", "open", "(", "out_file", ",", "'wb'", ")", "\n", "self", ".", "dtype", "=", "dtype", "\n", "self", ".", "data_offsets", "=", "[", "0", "]", "\n", "self", ".", "dim_offsets", "=", "[", "0", "]", "\n", "self", ".", "sizes", "=", "[", "]", "\n", "self", ".", "element_size", "=", "self", ".", "element_sizes", "[", "self", ".", "dtype", "]", "\n", "self", ".", "doc_idx", "=", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDatasetBuilder.add_item": [[284, 290], ["indexed_dataset.IndexedDatasetBuilder.out_file.write", "indexed_dataset.IndexedDatasetBuilder.data_offsets.append", "tensor.size", "indexed_dataset.IndexedDatasetBuilder.dim_offsets.append", "numpy.array", "indexed_dataset.IndexedDatasetBuilder.sizes.append", "tensor.numpy", "len", "tensor.size"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDataset.size"], ["", "def", "add_item", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "bytes", "=", "self", ".", "out_file", ".", "write", "(", "np", ".", "array", "(", "tensor", ".", "numpy", "(", ")", ",", "dtype", "=", "self", ".", "dtype", ")", ")", "\n", "self", ".", "data_offsets", ".", "append", "(", "self", ".", "data_offsets", "[", "-", "1", "]", "+", "bytes", "/", "self", ".", "element_size", ")", "\n", "for", "s", "in", "tensor", ".", "size", "(", ")", ":", "\n", "            ", "self", ".", "sizes", ".", "append", "(", "s", ")", "\n", "", "self", ".", "dim_offsets", ".", "append", "(", "self", ".", "dim_offsets", "[", "-", "1", "]", "+", "len", "(", "tensor", ".", "size", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDatasetBuilder.end_document": [[291, 293], ["indexed_dataset.IndexedDatasetBuilder.doc_idx.append", "len"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "end_document", "(", "self", ")", ":", "\n", "        ", "self", ".", "doc_idx", ".", "append", "(", "len", "(", "self", ".", "sizes", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDatasetBuilder.merge_file_": [[294, 313], ["indexed_dataset.IndexedDataset", "indexed_dataset.IndexedDatasetBuilder.sizes.extend", "indexed_dataset.IndexedDatasetBuilder.data_offsets.append", "indexed_dataset.IndexedDatasetBuilder.dim_offsets.append", "open", "indexed_dataset.data_file_path", "f.read", "indexed_dataset.IndexedDatasetBuilder.out_file.write"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.extend", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.data_file_path", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write"], ["", "def", "merge_file_", "(", "self", ",", "another_file", ")", ":", "\n", "        ", "index", "=", "IndexedDataset", "(", "another_file", ")", "\n", "assert", "index", ".", "dtype", "==", "self", ".", "dtype", "\n", "\n", "begin", "=", "self", ".", "data_offsets", "[", "-", "1", "]", "\n", "for", "offset", "in", "index", ".", "data_offsets", "[", "1", ":", "]", ":", "\n", "            ", "self", ".", "data_offsets", ".", "append", "(", "begin", "+", "offset", ")", "\n", "", "self", ".", "sizes", ".", "extend", "(", "index", ".", "sizes", ")", "\n", "begin", "=", "self", ".", "dim_offsets", "[", "-", "1", "]", "\n", "for", "dim_offset", "in", "index", ".", "dim_offsets", "[", "1", ":", "]", ":", "\n", "            ", "self", ".", "dim_offsets", ".", "append", "(", "begin", "+", "dim_offset", ")", "\n", "\n", "", "with", "open", "(", "data_file_path", "(", "another_file", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "while", "True", ":", "\n", "                ", "data", "=", "f", ".", "read", "(", "1024", ")", "\n", "if", "data", ":", "\n", "                    ", "self", ".", "out_file", ".", "write", "(", "data", ")", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedDatasetBuilder.finalize": [[314, 327], ["indexed_dataset.IndexedDatasetBuilder.out_file.close", "open", "open.write", "open.write", "open.write", "open.write", "open.write", "indexed_dataset.write_longs", "indexed_dataset.write_longs", "indexed_dataset.write_longs", "indexed_dataset.write_longs", "open.close", "struct.pack", "struct.pack", "struct.pack", "struct.pack", "indexed_dataset.code", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.code"], ["", "", "", "", "def", "finalize", "(", "self", ",", "index_file", ")", ":", "\n", "        ", "self", ".", "out_file", ".", "close", "(", ")", "\n", "index", "=", "open", "(", "index_file", ",", "'wb'", ")", "\n", "index", ".", "write", "(", "b'TNTIDX\\x00\\x00'", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<Q'", ",", "1", ")", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<QQ'", ",", "code", "(", "self", ".", "dtype", ")", ",", "self", ".", "element_size", ")", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<QQ'", ",", "len", "(", "self", ".", "data_offsets", ")", "-", "1", ",", "len", "(", "self", ".", "sizes", ")", ")", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<Q'", ",", "len", "(", "self", ".", "doc_idx", ")", ")", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "dim_offsets", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "data_offsets", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "sizes", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "doc_idx", ")", "\n", "index", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.__init__": [[445, 453], ["super().__init__", "indexed_dataset.MMapIndexedDataset._do_init"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset._do_init"], ["", "", "def", "__init__", "(", "self", ",", "path", ",", "skip_warmup", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_path", "=", "None", "\n", "self", ".", "_index", "=", "None", "\n", "self", ".", "_bin_buffer", "=", "None", "\n", "\n", "self", ".", "_do_init", "(", "path", ",", "skip_warmup", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.__getstate__": [[454, 456], ["None"], "methods", ["None"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_path", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.__setstate__": [[457, 459], ["indexed_dataset.MMapIndexedDataset._do_init"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset._do_init"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "self", ".", "_do_init", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset._do_init": [[460, 471], ["indexed_dataset.MMapIndexedDataset.Index", "utils.print_rank_0", "numpy.memmap", "utils.print_rank_0", "memoryview", "indexed_dataset.index_file_path", "utils.print_rank_0", "indexed_dataset._warmup_mmap_file", "indexed_dataset.data_file_path", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.index_file_path", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset._warmup_mmap_file", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.data_file_path", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.data_file_path"], ["", "def", "_do_init", "(", "self", ",", "path", ",", "skip_warmup", ")", ":", "\n", "        ", "self", ".", "_path", "=", "path", "\n", "self", ".", "_index", "=", "self", ".", "Index", "(", "index_file_path", "(", "self", ".", "_path", ")", ",", "skip_warmup", ")", "\n", "\n", "if", "not", "skip_warmup", ":", "\n", "            ", "print_rank_0", "(", "\"    warming up data mmap file...\"", ")", "\n", "_warmup_mmap_file", "(", "data_file_path", "(", "self", ".", "_path", ")", ")", "\n", "", "print_rank_0", "(", "\"    creating numpy buffer of mmap...\"", ")", "\n", "self", ".", "_bin_buffer_mmap", "=", "np", ".", "memmap", "(", "data_file_path", "(", "self", ".", "_path", ")", ",", "mode", "=", "'r'", ",", "order", "=", "'C'", ")", "\n", "print_rank_0", "(", "\"    creating memory view of numpy buffer...\"", ")", "\n", "self", ".", "_bin_buffer", "=", "memoryview", "(", "self", ".", "_bin_buffer_mmap", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.__del__": [[472, 476], ["indexed_dataset.MMapIndexedDataset._bin_buffer_mmap._mmap.close"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_bin_buffer_mmap", ".", "_mmap", ".", "close", "(", ")", "\n", "del", "self", ".", "_bin_buffer_mmap", "\n", "del", "self", ".", "_index", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.__len__": [[477, 479], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.__getitem__": [[481, 499], ["isinstance", "numpy.frombuffer", "isinstance", "idx.indices", "list", "sum", "numpy.frombuffer", "numpy.split", "len", "ValueError", "itertools.accumulate"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "if", "isinstance", "(", "idx", ",", "int", ")", ":", "\n", "            ", "ptr", ",", "size", "=", "self", ".", "_index", "[", "idx", "]", "\n", "np_array", "=", "np", ".", "frombuffer", "(", "self", ".", "_bin_buffer", ",", "dtype", "=", "self", ".", "_index", ".", "dtype", ",", "\n", "count", "=", "size", ",", "offset", "=", "ptr", ")", "\n", "return", "np_array", "\n", "", "elif", "isinstance", "(", "idx", ",", "slice", ")", ":", "\n", "            ", "start", ",", "stop", ",", "step", "=", "idx", ".", "indices", "(", "len", "(", "self", ")", ")", "\n", "if", "step", "!=", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\"Slices into indexed_dataset must be contiguous\"", ")", "\n", "", "ptr", "=", "self", ".", "_index", ".", "_pointers", "[", "start", "]", "\n", "sizes", "=", "self", ".", "_index", ".", "_sizes", "[", "idx", "]", "\n", "offsets", "=", "list", "(", "accumulate", "(", "sizes", ")", ")", "\n", "total_size", "=", "sum", "(", "sizes", ")", "\n", "np_array", "=", "np", ".", "frombuffer", "(", "self", ".", "_bin_buffer", ",", "dtype", "=", "self", ".", "_index", ".", "dtype", ",", "\n", "count", "=", "total_size", ",", "offset", "=", "ptr", ")", "\n", "sents", "=", "np", ".", "split", "(", "np_array", ",", "offsets", "[", ":", "-", "1", "]", ")", "\n", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get": [[500, 513], ["numpy.frombuffer", "numpy.dtype"], "methods", ["None"], ["", "", "def", "get", "(", "self", ",", "idx", ",", "offset", "=", "0", ",", "length", "=", "None", ")", ":", "\n", "        ", "\"\"\" Retrieves a single item from the dataset with the option to only\n        return a portion of the item.\n\n        get(idx) is the same as [idx] but get() does not support slicing.\n        \"\"\"", "\n", "ptr", ",", "size", "=", "self", ".", "_index", "[", "idx", "]", "\n", "if", "length", "is", "None", ":", "\n", "            ", "length", "=", "size", "-", "offset", "\n", "", "ptr", "+=", "offset", "*", "np", ".", "dtype", "(", "self", ".", "_index", ".", "dtype", ")", ".", "itemsize", "\n", "np_array", "=", "np", ".", "frombuffer", "(", "self", ".", "_bin_buffer", ",", "dtype", "=", "self", ".", "_index", ".", "dtype", ",", "\n", "count", "=", "length", ",", "offset", "=", "ptr", ")", "\n", "return", "np_array", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.sizes": [[514, 517], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "sizes", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_index", ".", "sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.doc_idx": [[518, 521], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "doc_idx", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_index", ".", "doc_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get_doc_idx": [[522, 524], ["None"], "methods", ["None"], ["", "def", "get_doc_idx", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_index", ".", "_doc_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.set_doc_idx": [[525, 527], ["None"], "methods", ["None"], ["", "def", "set_doc_idx", "(", "self", ",", "doc_idx_", ")", ":", "\n", "        ", "self", ".", "_index", ".", "_doc_idx", "=", "doc_idx_", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.supports_prefetch": [[528, 531], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists": [[532, 536], ["os.path.exists", "os.path.exists", "indexed_dataset.index_file_path", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.index_file_path", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.data_file_path"], ["", "@", "staticmethod", "\n", "def", "exists", "(", "path", ")", ":", "\n", "        ", "return", "(", "\n", "os", ".", "path", ".", "exists", "(", "index_file_path", "(", "path", ")", ")", "and", "os", ".", "path", ".", "exists", "(", "data_file_path", "(", "path", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDatasetBuilder.__init__": [[540, 545], ["open"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "out_file", ",", "dtype", "=", "np", ".", "int64", ")", ":", "\n", "        ", "self", ".", "_data_file", "=", "open", "(", "out_file", ",", "'wb'", ")", "\n", "self", ".", "_dtype", "=", "dtype", "\n", "self", ".", "_sizes", "=", "[", "]", "\n", "self", ".", "_doc_idx", "=", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDatasetBuilder.add_item": [[546, 550], ["numpy.array", "indexed_dataset.MMapIndexedDatasetBuilder._data_file.write", "indexed_dataset.MMapIndexedDatasetBuilder._sizes.append", "tensor.numpy", "numpy.array.tobytes"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "add_item", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "np_array", "=", "np", ".", "array", "(", "tensor", ".", "numpy", "(", ")", ",", "dtype", "=", "self", ".", "_dtype", ")", "\n", "self", ".", "_data_file", ".", "write", "(", "np_array", ".", "tobytes", "(", "order", "=", "'C'", ")", ")", "\n", "self", ".", "_sizes", ".", "append", "(", "np_array", ".", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDatasetBuilder.end_document": [[551, 553], ["indexed_dataset.MMapIndexedDatasetBuilder._doc_idx.append", "len"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "end_document", "(", "self", ")", ":", "\n", "        ", "self", ".", "_doc_idx", ".", "append", "(", "len", "(", "self", ".", "_sizes", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDatasetBuilder.merge_file_": [[554, 565], ["MMapIndexedDataset.Index", "indexed_dataset.index_file_path", "indexed_dataset.MMapIndexedDatasetBuilder._sizes.append", "open", "shutil.copyfileobj", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.index_file_path", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.data_file_path"], ["", "def", "merge_file_", "(", "self", ",", "another_file", ")", ":", "\n", "# Concatenate index", "\n", "        ", "index", "=", "MMapIndexedDataset", ".", "Index", "(", "index_file_path", "(", "another_file", ")", ")", "\n", "assert", "index", ".", "dtype", "==", "self", ".", "_dtype", "\n", "\n", "for", "size", "in", "index", ".", "sizes", ":", "\n", "            ", "self", ".", "_sizes", ".", "append", "(", "size", ")", "\n", "\n", "# Concatenate data", "\n", "", "with", "open", "(", "data_file_path", "(", "another_file", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "shutil", ".", "copyfileobj", "(", "f", ",", "self", ".", "_data_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize": [[566, 571], ["indexed_dataset.MMapIndexedDatasetBuilder._data_file.close", "MMapIndexedDataset.Index.writer", "index.write"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write"], ["", "", "def", "finalize", "(", "self", ",", "index_file", ")", ":", "\n", "        ", "self", ".", "_data_file", ".", "close", "(", ")", "\n", "\n", "with", "MMapIndexedDataset", ".", "Index", ".", "writer", "(", "index_file", ",", "self", ".", "_dtype", ")", "as", "index", ":", "\n", "            ", "index", ".", "write", "(", "self", ".", "_sizes", ",", "self", ".", "_doc_idx", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.__best_fitting_dtype": [[24, 29], ["None"], "function", ["None"], ["def", "__best_fitting_dtype", "(", "vocab_size", "=", "None", ")", ":", "\n", "    ", "if", "vocab_size", "is", "not", "None", "and", "vocab_size", "<", "65500", ":", "\n", "        ", "return", "np", ".", "uint16", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "int32", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.get_available_dataset_impl": [[31, 33], ["None"], "function", ["None"], ["", "", "def", "get_available_dataset_impl", "(", ")", ":", "\n", "    ", "return", "[", "'lazy'", ",", "'cached'", ",", "'mmap'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.infer_dataset_impl": [[35, 49], ["indexed_dataset.IndexedDataset.exists", "print", "print", "open", "f.read", "indexed_dataset.index_file_path"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.index_file_path"], ["", "def", "infer_dataset_impl", "(", "path", ")", ":", "\n", "    ", "if", "IndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "        ", "with", "open", "(", "index_file_path", "(", "path", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "magic", "=", "f", ".", "read", "(", "8", ")", "\n", "if", "magic", "==", "IndexedDataset", ".", "_HDR_MAGIC", ":", "\n", "                ", "return", "'cached'", "\n", "", "elif", "magic", "==", "MMapIndexedDataset", ".", "Index", ".", "_HDR_MAGIC", "[", ":", "8", "]", ":", "\n", "                ", "return", "'mmap'", "\n", "", "else", ":", "\n", "                ", "return", "None", "\n", "", "", "", "else", ":", "\n", "        ", "print", "(", "f\"Dataset does not exist: {path}\"", ")", "\n", "print", "(", "\"Path should be a basename that both .idx and .bin can be appended to get full filenames.\"", ")", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.make_builder": [[51, 56], ["indexed_dataset.MMapIndexedDatasetBuilder", "indexed_dataset.IndexedDatasetBuilder", "indexed_dataset.__best_fitting_dtype"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.__best_fitting_dtype"], ["", "", "def", "make_builder", "(", "out_file", ",", "impl", ",", "vocab_size", "=", "None", ")", ":", "\n", "    ", "if", "impl", "==", "'mmap'", ":", "\n", "        ", "return", "MMapIndexedDatasetBuilder", "(", "out_file", ",", "dtype", "=", "__best_fitting_dtype", "(", "vocab_size", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "IndexedDatasetBuilder", "(", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.make_dataset": [[58, 73], ["print", "indexed_dataset.IndexedDataset.exists", "print", "print", "indexed_dataset.infer_dataset_impl", "indexed_dataset.IndexedDataset.exists", "indexed_dataset.IndexedDataset", "indexed_dataset.IndexedDataset.exists", "indexed_dataset.IndexedCachedDataset", "indexed_dataset.MMapIndexedDataset.exists", "indexed_dataset.MMapIndexedDataset"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.infer_dataset_impl", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "", "def", "make_dataset", "(", "path", ",", "impl", ",", "skip_warmup", "=", "False", ")", ":", "\n", "    ", "if", "not", "IndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "        ", "print", "(", "f\"Dataset does not exist: {path}\"", ")", "\n", "print", "(", "\"Path should be a basename that both .idx and .bin can be appended to get full filenames.\"", ")", "\n", "return", "None", "\n", "", "if", "impl", "==", "'infer'", ":", "\n", "        ", "impl", "=", "infer_dataset_impl", "(", "path", ")", "\n", "", "if", "impl", "==", "'lazy'", "and", "IndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "        ", "return", "IndexedDataset", "(", "path", ")", "\n", "", "elif", "impl", "==", "'cached'", "and", "IndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "        ", "return", "IndexedCachedDataset", "(", "path", ")", "\n", "", "elif", "impl", "==", "'mmap'", "and", "MMapIndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "        ", "return", "MMapIndexedDataset", "(", "path", ",", "skip_warmup", ")", "\n", "", "print", "(", "f\"Unknown dataset implementation: {impl}\"", ")", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.dataset_exists": [[75, 80], ["indexed_dataset.MMapIndexedDataset.exists", "indexed_dataset.IndexedDataset.exists"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "def", "dataset_exists", "(", "path", ",", "impl", ")", ":", "\n", "    ", "if", "impl", "==", "'mmap'", ":", "\n", "        ", "return", "MMapIndexedDataset", ".", "exists", "(", "path", ")", "\n", "", "else", ":", "\n", "        ", "return", "IndexedDataset", ".", "exists", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.read_longs": [[82, 86], ["numpy.empty", "f.readinto"], "function", ["None"], ["", "", "def", "read_longs", "(", "f", ",", "n", ")", ":", "\n", "    ", "a", "=", "np", ".", "empty", "(", "n", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "f", ".", "readinto", "(", "a", ")", "\n", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.write_longs": [[88, 90], ["f.write", "numpy.array"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.datasets.json_dataset.write"], ["", "def", "write_longs", "(", "f", ",", "a", ")", ":", "\n", "    ", "f", ".", "write", "(", "np", ".", "array", "(", "a", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.code": [[104, 109], ["dtypes.keys", "ValueError"], "function", ["None"], ["def", "code", "(", "dtype", ")", ":", "\n", "    ", "for", "k", "in", "dtypes", ".", "keys", "(", ")", ":", "\n", "        ", "if", "dtypes", "[", "k", "]", "==", "dtype", ":", "\n", "            ", "return", "k", "\n", "", "", "raise", "ValueError", "(", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.index_file_path": [[111, 113], ["None"], "function", ["None"], ["", "def", "index_file_path", "(", "prefix_path", ")", ":", "\n", "    ", "return", "prefix_path", "+", "'.idx'", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.data_file_path": [[115, 117], ["None"], "function", ["None"], ["", "def", "data_file_path", "(", "prefix_path", ")", ":", "\n", "    ", "return", "prefix_path", "+", "'.bin'", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.create_doc_idx": [[119, 125], ["enumerate", "doc_idx.append"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "create_doc_idx", "(", "sizes", ")", ":", "\n", "    ", "doc_idx", "=", "[", "0", "]", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "sizes", ")", ":", "\n", "        ", "if", "s", "==", "0", ":", "\n", "            ", "doc_idx", ".", "append", "(", "i", "+", "1", ")", "\n", "", "", "return", "doc_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset._warmup_mmap_file": [[329, 333], ["open", "stream.read"], "function", ["None"], ["", "", "def", "_warmup_mmap_file", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'rb'", ")", "as", "stream", ":", "\n", "        ", "while", "stream", ".", "read", "(", "100", "*", "1024", "*", "1024", ")", ":", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.bert_dataset.BertDataset.__init__": [[104, 135], ["bert_dataset.get_samples_mapping_", "get_tokenizer", "list", "get_tokenizer.inv_vocab.keys"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.bert_dataset.get_samples_mapping_"], ["    ", "def", "__init__", "(", "self", ",", "name", ",", "indexed_dataset", ",", "data_prefix", ",", "\n", "num_epochs", ",", "max_num_samples", ",", "masked_lm_prob", ",", "\n", "max_seq_length", ",", "short_seq_prob", ",", "seed", ")", ":", "\n", "\n", "# Params to store.", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "seed", "=", "seed", "\n", "self", ".", "masked_lm_prob", "=", "masked_lm_prob", "\n", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "\n", "# Dataset.", "\n", "self", ".", "indexed_dataset", "=", "indexed_dataset", "\n", "\n", "# Build the samples mapping.", "\n", "self", ".", "samples_mapping", "=", "get_samples_mapping_", "(", "self", ".", "indexed_dataset", ",", "\n", "data_prefix", ",", "\n", "num_epochs", ",", "\n", "max_num_samples", ",", "\n", "self", ".", "max_seq_length", ",", "\n", "short_seq_prob", ",", "\n", "self", ".", "seed", ",", "\n", "self", ".", "name", ")", "\n", "\n", "# Vocab stuff.", "\n", "tokenizer", "=", "get_tokenizer", "(", ")", "\n", "self", ".", "vocab_id_list", "=", "list", "(", "tokenizer", ".", "inv_vocab", ".", "keys", "(", ")", ")", "\n", "self", ".", "vocab_id_to_token_dict", "=", "tokenizer", ".", "inv_vocab", "\n", "self", ".", "cls_id", "=", "tokenizer", ".", "cls", "\n", "self", ".", "sep_id", "=", "tokenizer", ".", "sep", "\n", "self", ".", "mask_id", "=", "tokenizer", ".", "mask", "\n", "self", ".", "pad_id", "=", "tokenizer", ".", "pad", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.bert_dataset.BertDataset.__len__": [[136, 138], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "samples_mapping", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.bert_dataset.BertDataset.__getitem__": [[139, 155], ["range", "numpy.random.RandomState", "data.dataset_utils.build_training_sample", "sample.append"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.build_training_sample", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "\n", "        ", "start_index", ",", "end_index", ",", "seq_length", "=", "self", ".", "samples_mapping", "[", "idx", "]", "\n", "sample", "=", "[", "]", "\n", "for", "index", "in", "range", "(", "start_index", ",", "end_index", ")", ":", "\n", "            ", "sample", ".", "append", "(", "self", ".", "indexed_dataset", "[", "index", "]", ")", "\n", "# Note that this rng state should be numpy and not python since", "\n", "# python randint is inclusive whereas the numpy one is exclusive.", "\n", "", "np_rng", "=", "np", ".", "random", ".", "RandomState", "(", "seed", "=", "(", "self", ".", "seed", "+", "idx", ")", ")", "\n", "return", "build_training_sample", "(", "sample", ",", "seq_length", ",", "\n", "self", ".", "max_seq_length", ",", "# needed for padding", "\n", "self", ".", "vocab_id_list", ",", "\n", "self", ".", "vocab_id_to_token_dict", ",", "\n", "self", ".", "cls_id", ",", "self", ".", "sep_id", ",", "\n", "self", ".", "mask_id", ",", "self", ".", "pad_id", ",", "\n", "self", ".", "masked_lm_prob", ",", "np_rng", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.bert_dataset.build_train_valid_test_datasets": [[32, 100], ["bert_dataset.get_indexed_dataset_", "bert_dataset.get_train_valid_test_split_", "utils.print_rank_0", "bert_dataset.build_train_valid_test_datasets.print_split_stats"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.get_indexed_dataset_", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.bert_dataset.get_train_valid_test_split_", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0"], ["def", "build_train_valid_test_datasets", "(", "data_prefix", ",", "data_impl", ",", "splits_string", ",", "\n", "train_valid_test_num_samples", ",", "\n", "max_seq_length", ",", "masked_lm_prob", ",", "\n", "short_seq_prob", ",", "seed", ",", "skip_warmup", ")", ":", "\n", "\n", "# Indexed dataset.", "\n", "    ", "indexed_dataset", "=", "get_indexed_dataset_", "(", "data_prefix", ",", "\n", "data_impl", ",", "\n", "skip_warmup", ")", "\n", "\n", "# Get start and end indices of train/valid/train into doc-idx", "\n", "# Note that doc-idx is desinged to be num-docs + 1 so we can", "\n", "# easily iterate over it.", "\n", "total_num_of_documents", "=", "indexed_dataset", ".", "doc_idx", ".", "shape", "[", "0", "]", "-", "1", "\n", "splits", "=", "get_train_valid_test_split_", "(", "splits_string", ",", "total_num_of_documents", ")", "\n", "\n", "# Print stats about the splits.", "\n", "print_rank_0", "(", "' > dataset split:'", ")", "\n", "\n", "def", "print_split_stats", "(", "name", ",", "index", ")", ":", "\n", "        ", "print_rank_0", "(", "'    {}:'", ".", "format", "(", "name", ")", ")", "\n", "print_rank_0", "(", "'     document indices in [{}, {}) total of {} '", "\n", "'documents'", ".", "format", "(", "splits", "[", "index", "]", ",", "splits", "[", "index", "+", "1", "]", ",", "\n", "splits", "[", "index", "+", "1", "]", "-", "splits", "[", "index", "]", ")", ")", "\n", "start_index", "=", "indexed_dataset", ".", "doc_idx", "[", "splits", "[", "index", "]", "]", "\n", "end_index", "=", "indexed_dataset", ".", "doc_idx", "[", "splits", "[", "index", "+", "1", "]", "]", "\n", "print_rank_0", "(", "'     sentence indices in [{}, {}) total of {} '", "\n", "'sentences'", ".", "format", "(", "start_index", ",", "end_index", ",", "\n", "end_index", "-", "start_index", ")", ")", "\n", "", "print_split_stats", "(", "'train'", ",", "0", ")", "\n", "print_split_stats", "(", "'validation'", ",", "1", ")", "\n", "print_split_stats", "(", "'test'", ",", "2", ")", "\n", "\n", "def", "build_dataset", "(", "index", ",", "name", ")", ":", "\n", "        ", "dataset", "=", "None", "\n", "if", "splits", "[", "index", "+", "1", "]", ">", "splits", "[", "index", "]", ":", "\n", "# Get the pointer to the original doc-idx so we can set it later.", "\n", "            ", "doc_idx_ptr", "=", "indexed_dataset", ".", "get_doc_idx", "(", ")", "\n", "# Slice the doc-idx", "\n", "start_index", "=", "splits", "[", "index", "]", "\n", "# Add +1 so we can index into the dataset to get the upper bound.", "\n", "end_index", "=", "splits", "[", "index", "+", "1", "]", "+", "1", "\n", "# New doc_idx view.", "\n", "indexed_dataset", ".", "set_doc_idx", "(", "doc_idx_ptr", "[", "start_index", ":", "end_index", "]", ")", "\n", "# Build the dataset accordingly.", "\n", "dataset", "=", "BertDataset", "(", "\n", "name", "=", "name", ",", "\n", "indexed_dataset", "=", "indexed_dataset", ",", "\n", "data_prefix", "=", "data_prefix", ",", "\n", "num_epochs", "=", "None", ",", "\n", "max_num_samples", "=", "train_valid_test_num_samples", "[", "index", "]", ",", "\n", "masked_lm_prob", "=", "masked_lm_prob", ",", "\n", "max_seq_length", "=", "max_seq_length", ",", "\n", "short_seq_prob", "=", "short_seq_prob", ",", "\n", "seed", "=", "seed", ")", "\n", "# Set the original pointer so dataset remains the main dataset.", "\n", "indexed_dataset", ".", "set_doc_idx", "(", "doc_idx_ptr", ")", "\n", "# Checks.", "\n", "assert", "indexed_dataset", ".", "doc_idx", "[", "0", "]", "==", "0", "\n", "assert", "indexed_dataset", ".", "doc_idx", ".", "shape", "[", "0", "]", "==", "(", "total_num_of_documents", "+", "1", ")", "\n", "", "return", "dataset", "\n", "\n", "", "train_dataset", "=", "build_dataset", "(", "0", ",", "'train'", ")", "\n", "valid_dataset", "=", "build_dataset", "(", "1", ",", "'valid'", ")", "\n", "test_dataset", "=", "build_dataset", "(", "2", ",", "'test'", ")", "\n", "\n", "return", "(", "train_dataset", ",", "valid_dataset", ",", "test_dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.bert_dataset.get_indexed_dataset_": [[157, 176], ["utils.print_rank_0", "time.time", "data.indexed_dataset.make_dataset", "utils.print_rank_0", "utils.print_rank_0", "utils.print_rank_0", "utils.print_rank_0", "time.time"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.make_dataset", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0"], ["", "", "def", "get_indexed_dataset_", "(", "data_prefix", ",", "data_impl", ",", "skip_warmup", ")", ":", "\n", "\n", "    ", "print_rank_0", "(", "' > building dataset index ...'", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "indexed_dataset", "=", "make_indexed_dataset", "(", "data_prefix", ",", "\n", "data_impl", ",", "\n", "skip_warmup", ")", "\n", "assert", "indexed_dataset", ".", "sizes", ".", "shape", "[", "0", "]", "==", "indexed_dataset", ".", "doc_idx", "[", "-", "1", "]", "\n", "print_rank_0", "(", "' > finished creating indexed dataset in {:4f} '", "\n", "'seconds'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "\n", "print_rank_0", "(", "' > indexed dataset stats:'", ")", "\n", "print_rank_0", "(", "'    number of documents: {}'", ".", "format", "(", "\n", "indexed_dataset", ".", "doc_idx", ".", "shape", "[", "0", "]", "-", "1", ")", ")", "\n", "print_rank_0", "(", "'    number of sentences: {}'", ".", "format", "(", "\n", "indexed_dataset", ".", "sizes", ".", "shape", "[", "0", "]", ")", ")", "\n", "\n", "return", "indexed_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.bert_dataset.get_train_valid_test_split_": [[178, 204], ["sum", "enumerate", "range", "splits_string.find", "len", "splits.append", "splits_index.append", "len", "len", "float", "splits_string.find", "splits_string.split", "float", "float", "int", "splits_string.split", "round", "float"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append"], ["", "def", "get_train_valid_test_split_", "(", "splits_string", ",", "size", ")", ":", "\n", "    ", "\"\"\" Get dataset splits from comma or '/' separated string list.\"\"\"", "\n", "\n", "splits", "=", "[", "]", "\n", "if", "splits_string", ".", "find", "(", "','", ")", "!=", "-", "1", ":", "\n", "        ", "splits", "=", "[", "float", "(", "s", ")", "for", "s", "in", "splits_string", ".", "split", "(", "','", ")", "]", "\n", "", "elif", "splits_string", ".", "find", "(", "'/'", ")", "!=", "-", "1", ":", "\n", "        ", "splits", "=", "[", "float", "(", "s", ")", "for", "s", "in", "splits_string", ".", "split", "(", "'/'", ")", "]", "\n", "", "else", ":", "\n", "        ", "splits", "=", "[", "float", "(", "splits_string", ")", "]", "\n", "", "while", "len", "(", "splits", ")", "<", "3", ":", "\n", "        ", "splits", ".", "append", "(", "0.", ")", "\n", "", "splits", "=", "splits", "[", ":", "3", "]", "\n", "splits_sum", "=", "sum", "(", "splits", ")", "\n", "assert", "splits_sum", ">", "0.0", "\n", "splits", "=", "[", "split", "/", "splits_sum", "for", "split", "in", "splits", "]", "\n", "splits_index", "=", "[", "0", "]", "\n", "for", "index", ",", "split", "in", "enumerate", "(", "splits", ")", ":", "\n", "        ", "splits_index", ".", "append", "(", "splits_index", "[", "index", "]", "+", "\n", "int", "(", "round", "(", "split", "*", "float", "(", "size", ")", ")", ")", ")", "\n", "", "diff", "=", "splits_index", "[", "-", "1", "]", "-", "size", "\n", "for", "index", "in", "range", "(", "1", ",", "len", "(", "splits_index", ")", ")", ":", "\n", "        ", "splits_index", "[", "index", "]", "-=", "diff", "\n", "", "assert", "len", "(", "splits_index", ")", "==", "4", "\n", "assert", "splits_index", "[", "-", "1", "]", "==", "size", "\n", "return", "splits_index", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.bert_dataset.get_samples_mapping_": [[206, 289], ["torch.cuda.LongTensor", "torch.distributed.all_reduce", "utils.print_rank_0", "time.time", "numpy.load", "utils.print_rank_0", "utils.print_rank_0", "print", "time.time", "utils.print_rank_0", "compile_helper", "helpers.build_mapping", "utils.print_rank_0", "numpy.save", "utils.print_rank_0", "utils.print_rank_0", "counts[].item", "torch.distributed.get_world_size", "ValueError", "torch.distributed.get_rank", "os.path.isfile", "torch.distributed.get_rank", "mpu.get_data_parallel_group", "numpy.iinfo", "numpy.iinfo", "numpy.iinfo", "numpy.iinfo", "mpu.get_data_parallel_group", "time.time", "time.time"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.compile_helper", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group"], ["", "def", "get_samples_mapping_", "(", "indexed_dataset", ",", "\n", "data_prefix", ",", "\n", "num_epochs", ",", "\n", "max_num_samples", ",", "\n", "max_seq_length", ",", "\n", "short_seq_prob", ",", "\n", "seed", ",", "\n", "name", ")", ":", "\n", "    ", "if", "not", "num_epochs", ":", "\n", "        ", "if", "not", "max_num_samples", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need to specify either max_num_samples \"", "\n", "\"or num_epochs\"", ")", "\n", "", "num_epochs", "=", "np", ".", "iinfo", "(", "np", ".", "int32", ")", ".", "max", "-", "1", "\n", "", "if", "not", "max_num_samples", ":", "\n", "        ", "max_num_samples", "=", "np", ".", "iinfo", "(", "np", ".", "int64", ")", ".", "max", "-", "1", "\n", "\n", "# Filename of the index mapping", "\n", "", "indexmap_filename", "=", "data_prefix", "\n", "indexmap_filename", "+=", "'_{}_indexmap'", ".", "format", "(", "name", ")", "\n", "if", "num_epochs", "!=", "(", "np", ".", "iinfo", "(", "np", ".", "int32", ")", ".", "max", "-", "1", ")", ":", "\n", "        ", "indexmap_filename", "+=", "'_{}ep'", ".", "format", "(", "num_epochs", ")", "\n", "", "if", "max_num_samples", "!=", "(", "np", ".", "iinfo", "(", "np", ".", "int64", ")", ".", "max", "-", "1", ")", ":", "\n", "        ", "indexmap_filename", "+=", "'_{}mns'", ".", "format", "(", "max_num_samples", ")", "\n", "", "indexmap_filename", "+=", "'_{}msl'", ".", "format", "(", "max_seq_length", ")", "\n", "indexmap_filename", "+=", "'_{:0.2f}ssp'", ".", "format", "(", "short_seq_prob", ")", "\n", "indexmap_filename", "+=", "'_{}s'", ".", "format", "(", "seed", ")", "\n", "indexmap_filename", "+=", "'.npy'", "\n", "\n", "# Build the indexed mapping if not exist.", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", "and", "not", "os", ".", "path", ".", "isfile", "(", "indexmap_filename", ")", ":", "\n", "        ", "print", "(", "' > WARNING: could not find index map file {}, building '", "\n", "'the indices on rank 0 ...'", ".", "format", "(", "indexmap_filename", ")", ")", "\n", "\n", "# Make sure the types match the helpers input types.", "\n", "assert", "indexed_dataset", ".", "doc_idx", ".", "dtype", "==", "np", ".", "int64", "\n", "assert", "indexed_dataset", ".", "sizes", ".", "dtype", "==", "np", ".", "int32", "\n", "\n", "# Build samples mapping", "\n", "verbose", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "print_rank_0", "(", "' > building sapmles index mapping for {} ...'", ".", "format", "(", "\n", "name", ")", ")", "\n", "# First compile and then import.", "\n", "from", "data", ".", "dataset_utils", "import", "compile_helper", "\n", "compile_helper", "(", ")", "\n", "from", "data", "import", "helpers", "\n", "samples_mapping", "=", "helpers", ".", "build_mapping", "(", "\n", "indexed_dataset", ".", "doc_idx", ",", "\n", "indexed_dataset", ".", "sizes", ",", "\n", "num_epochs", ",", "\n", "max_num_samples", ",", "\n", "max_seq_length", "-", "3", ",", "# account for added tokens", "\n", "short_seq_prob", ",", "\n", "seed", ",", "\n", "verbose", ")", "\n", "print_rank_0", "(", "' > done building sapmles index maping'", ")", "\n", "np", ".", "save", "(", "indexmap_filename", ",", "samples_mapping", ",", "allow_pickle", "=", "True", ")", "\n", "print_rank_0", "(", "' > saved the index mapping in {}'", ".", "format", "(", "\n", "indexmap_filename", ")", ")", "\n", "# Make sure all the ranks have built the mapping", "\n", "print_rank_0", "(", "' > elasped time to build and save samples mapping '", "\n", "'(seconds): {:4f}'", ".", "format", "(", "\n", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "# This should be a barrier but nccl barrier assumes", "\n", "# device_index=rank which is not the case for model", "\n", "# parallel case", "\n", "", "counts", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "[", "1", "]", ")", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "counts", ",", "group", "=", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "assert", "counts", "[", "0", "]", ".", "item", "(", ")", "==", "torch", ".", "distributed", ".", "get_world_size", "(", "\n", "group", "=", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "\n", "# Load indexed dataset.", "\n", "print_rank_0", "(", "' > loading indexed mapping from {}'", ".", "format", "(", "\n", "indexmap_filename", ")", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "samples_mapping", "=", "np", ".", "load", "(", "indexmap_filename", ",", "allow_pickle", "=", "True", ",", "mmap_mode", "=", "'r'", ")", "\n", "print_rank_0", "(", "'    loaded indexed file in {:3.3f} seconds'", ".", "format", "(", "\n", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "print_rank_0", "(", "'    total number of samples: {}'", ".", "format", "(", "\n", "samples_mapping", ".", "shape", "[", "0", "]", ")", ")", "\n", "\n", "return", "samples_mapping", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__init__": [[91, 105], ["gpt2_dataset._build_index_mappings", "numpy.min", "numpy.max"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset._build_index_mappings"], ["    ", "def", "__init__", "(", "self", ",", "name", ",", "data_prefix", ",", "documents", ",", "indexed_dataset", ",", "\n", "num_samples", ",", "seq_length", ",", "seed", ")", ":", "\n", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "indexed_dataset", "=", "indexed_dataset", "\n", "\n", "# Checks", "\n", "assert", "np", ".", "min", "(", "documents", ")", ">=", "0", "\n", "assert", "np", ".", "max", "(", "documents", ")", "<", "indexed_dataset", ".", "sizes", ".", "shape", "[", "0", "]", "\n", "\n", "# Build index mappings.", "\n", "self", ".", "doc_idx", ",", "self", ".", "sample_idx", ",", "self", ".", "shuffle_idx", "=", "_build_index_mappings", "(", "\n", "self", ".", "name", ",", "data_prefix", ",", "documents", ",", "self", ".", "indexed_dataset", ".", "sizes", ",", "\n", "num_samples", ",", "seq_length", ",", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__len__": [[106, 110], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "# -1 is due to data structure used to retieve the index:", "\n", "#    sample i --> [sample_idx[i], sample_idx[i+1])", "\n", "        ", "return", "self", ".", "sample_idx", ".", "shape", "[", "0", "]", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.GPT2Dataset.__getitem__": [[111, 138], ["gpt2_dataset.GPT2Dataset.indexed_dataset.get", "range", "sample_list.append", "numpy.concatenate", "numpy.array", "gpt2_dataset.GPT2Dataset.indexed_dataset.get", "sample_list.append", "gpt2_dataset.GPT2Dataset.indexed_dataset.get", "gpt2_dataset.GPT2Dataset.indexed_dataset.get"], "methods", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data_utils.tokenization.Tokenization.append", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "# Get the shuffled index.", "\n", "        ", "idx", "=", "self", ".", "shuffle_idx", "[", "idx", "]", "\n", "# Start and end documents and offsets.", "\n", "doc_index_f", "=", "self", ".", "sample_idx", "[", "idx", "]", "[", "0", "]", "\n", "doc_index_l", "=", "self", ".", "sample_idx", "[", "idx", "+", "1", "]", "[", "0", "]", "\n", "offset_f", "=", "self", ".", "sample_idx", "[", "idx", "]", "[", "1", "]", "\n", "offset_l", "=", "self", ".", "sample_idx", "[", "idx", "+", "1", "]", "[", "1", "]", "\n", "# If we are within the same document, just extract the chunk.", "\n", "if", "doc_index_f", "==", "doc_index_l", ":", "\n", "            ", "sample", "=", "self", ".", "indexed_dataset", ".", "get", "(", "self", ".", "doc_idx", "[", "doc_index_f", "]", ",", "\n", "offset", "=", "offset_f", ",", "\n", "length", "=", "offset_l", "-", "offset_f", "+", "1", ")", "\n", "", "else", ":", "\n", "# Otherwise, get the rest of the initial document.", "\n", "            ", "sample_list", "=", "[", "self", ".", "indexed_dataset", ".", "get", "(", "self", ".", "doc_idx", "[", "doc_index_f", "]", ",", "\n", "offset", "=", "offset_f", ")", "]", "\n", "# Loop over all in between documents and add the entire document.", "\n", "for", "i", "in", "range", "(", "doc_index_f", "+", "1", ",", "doc_index_l", ")", ":", "\n", "                ", "sample_list", ".", "append", "(", "self", ".", "indexed_dataset", ".", "get", "(", "self", ".", "doc_idx", "[", "i", "]", ")", ")", "\n", "# And finally add the relevant portion of last document.", "\n", "", "sample_list", ".", "append", "(", "self", ".", "indexed_dataset", ".", "get", "(", "\n", "self", ".", "doc_idx", "[", "doc_index_l", "]", ",", "\n", "length", "=", "offset_l", "+", "1", ")", ")", "\n", "sample", "=", "np", ".", "concatenate", "(", "sample_list", ")", "\n", "\n", "", "return", "{", "'text'", ":", "np", ".", "array", "(", "sample", ",", "dtype", "=", "np", ".", "int64", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.build_train_valid_test_datasets": [[30, 71], ["gpt2_dataset.get_indexed_dataset_", "data.bert_dataset.get_train_valid_test_split_", "utils.print_rank_0", "gpt2_dataset.build_train_valid_test_datasets.print_split_stats"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.get_indexed_dataset_", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.bert_dataset.get_train_valid_test_split_", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0"], ["def", "build_train_valid_test_datasets", "(", "data_prefix", ",", "data_impl", ",", "splits_string", ",", "\n", "train_valid_test_num_samples", ",", "\n", "seq_length", ",", "seed", ",", "skip_warmup", ")", ":", "\n", "    ", "\"\"\"Build train, valid, and test datasets.\"\"\"", "\n", "\n", "# Indexed dataset.", "\n", "indexed_dataset", "=", "get_indexed_dataset_", "(", "data_prefix", ",", "\n", "data_impl", ",", "\n", "skip_warmup", ")", "\n", "\n", "total_num_of_documents", "=", "indexed_dataset", ".", "sizes", ".", "shape", "[", "0", "]", "\n", "splits", "=", "get_train_valid_test_split_", "(", "splits_string", ",", "total_num_of_documents", ")", "\n", "\n", "# Print stats about the splits.", "\n", "print_rank_0", "(", "' > dataset split:'", ")", "\n", "\n", "def", "print_split_stats", "(", "name", ",", "index", ")", ":", "\n", "        ", "print_rank_0", "(", "'    {}:'", ".", "format", "(", "name", ")", ")", "\n", "print_rank_0", "(", "'     document indices in [{}, {}) total of {} '", "\n", "'documents'", ".", "format", "(", "splits", "[", "index", "]", ",", "splits", "[", "index", "+", "1", "]", ",", "\n", "splits", "[", "index", "+", "1", "]", "-", "splits", "[", "index", "]", ")", ")", "\n", "", "print_split_stats", "(", "'train'", ",", "0", ")", "\n", "print_split_stats", "(", "'validation'", ",", "1", ")", "\n", "print_split_stats", "(", "'test'", ",", "2", ")", "\n", "\n", "def", "build_dataset", "(", "index", ",", "name", ")", ":", "\n", "        ", "dataset", "=", "None", "\n", "if", "splits", "[", "index", "+", "1", "]", ">", "splits", "[", "index", "]", ":", "\n", "            ", "documents", "=", "np", ".", "arange", "(", "start", "=", "splits", "[", "index", "]", ",", "stop", "=", "splits", "[", "index", "+", "1", "]", ",", "\n", "step", "=", "1", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "dataset", "=", "GPT2Dataset", "(", "name", ",", "data_prefix", ",", "\n", "documents", ",", "indexed_dataset", ",", "\n", "train_valid_test_num_samples", "[", "index", "]", ",", "\n", "seq_length", ",", "seed", ")", "\n", "", "return", "dataset", "\n", "\n", "", "train_dataset", "=", "build_dataset", "(", "0", ",", "'train'", ")", "\n", "valid_dataset", "=", "build_dataset", "(", "1", ",", "'valid'", ")", "\n", "test_dataset", "=", "build_dataset", "(", "2", ",", "'test'", ")", "\n", "\n", "return", "(", "train_dataset", ",", "valid_dataset", ",", "test_dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset.get_indexed_dataset_": [[73, 87], ["utils.print_rank_0", "time.time", "data.indexed_dataset.make_dataset", "utils.print_rank_0", "utils.print_rank_0", "time.time"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.make_dataset", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0"], ["", "def", "get_indexed_dataset_", "(", "data_prefix", ",", "data_impl", ",", "skip_warmup", ")", ":", "\n", "    ", "\"\"\"Build indexed dataset.\"\"\"", "\n", "print_rank_0", "(", "' > building dataset index ...'", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "indexed_dataset", "=", "make_indexed_dataset", "(", "data_prefix", ",", "\n", "data_impl", ",", "\n", "skip_warmup", ")", "\n", "print_rank_0", "(", "' > finished creating indexed dataset in {:4f} '", "\n", "'seconds'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "print_rank_0", "(", "'    number of documents: {}'", ".", "format", "(", "\n", "indexed_dataset", ".", "sizes", ".", "shape", "[", "0", "]", ")", ")", "\n", "\n", "return", "indexed_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset._build_index_mappings": [[140, 229], ["gpt2_dataset._num_tokens", "gpt2_dataset._num_epochs", "numpy.random.RandomState", "torch.cuda.LongTensor", "torch.distributed.all_reduce", "time.time", "utils.print_rank_0", "numpy.load", "utils.print_rank_0", "numpy.load", "utils.print_rank_0", "numpy.load", "utils.print_rank_0", "utils.print_rank_0", "utils.print_rank_0", "torch.distributed.get_rank", "counts[].item", "torch.distributed.get_world_size", "utils.print_rank_0", "time.time", "gpt2_dataset._build_doc_idx", "numpy.save", "utils.print_rank_0", "time.time", "compile_helper", "helpers.build_sample_idx", "numpy.save", "utils.print_rank_0", "time.time", "gpt2_dataset._build_shuffle_idx", "numpy.save", "utils.print_rank_0", "mpu.get_data_parallel_group", "os.path.isfile", "os.path.isfile", "os.path.isfile", "mpu.get_data_parallel_group", "time.time", "time.time", "time.time", "time.time"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset._num_tokens", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset._num_epochs", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset._build_doc_idx", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.dataset_utils.compile_helper", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset._build_shuffle_idx", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.None.utils.print_rank_0", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.mpu.initialize.get_data_parallel_group"], ["", "", "def", "_build_index_mappings", "(", "name", ",", "data_prefix", ",", "documents", ",", "sizes", ",", "\n", "num_samples", ",", "seq_length", ",", "seed", ")", ":", "\n", "    ", "\"\"\"Build doc-idx, sample-idx, and shuffle-idx.\n    doc-idx: is an array (ordered) of documents to be used in training.\n    sample-idx: is the start document index and document offset for each\n       training sample.\n    shuffle-idx: maps the sample index into a random index into sample-idx.\n    \"\"\"", "\n", "# Number of tokens in each epoch and number of required epochs.", "\n", "tokens_per_epoch", "=", "_num_tokens", "(", "documents", ",", "sizes", ")", "\n", "num_epochs", "=", "_num_epochs", "(", "tokens_per_epoch", ",", "seq_length", ",", "num_samples", ")", "\n", "# rng state", "\n", "np_rng", "=", "np", ".", "random", ".", "RandomState", "(", "seed", "=", "seed", ")", "\n", "\n", "# Filename of the index mappings.", "\n", "_filename", "=", "data_prefix", "\n", "_filename", "+=", "'_{}_indexmap'", ".", "format", "(", "name", ")", "\n", "_filename", "+=", "'_{}ns'", ".", "format", "(", "num_samples", ")", "\n", "_filename", "+=", "'_{}sl'", ".", "format", "(", "seq_length", ")", "\n", "_filename", "+=", "'_{}s'", ".", "format", "(", "seed", ")", "\n", "doc_idx_filename", "=", "_filename", "+", "'_doc_idx.npy'", "\n", "sample_idx_filename", "=", "_filename", "+", "'_sample_idx.npy'", "\n", "shuffle_idx_filename", "=", "_filename", "+", "'_shuffle_idx.npy'", "\n", "\n", "# Build the indexed mapping if not exist.", "\n", "if", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "if", "(", "not", "os", ".", "path", ".", "isfile", "(", "doc_idx_filename", ")", ")", "or", "(", "not", "os", ".", "path", ".", "isfile", "(", "sample_idx_filename", ")", ")", "or", "(", "not", "os", ".", "path", ".", "isfile", "(", "shuffle_idx_filename", ")", ")", ":", "\n", "\n", "            ", "print_rank_0", "(", "' > WARNING: could not find index map files, building '", "\n", "'the indices on rank 0 ...'", ")", "\n", "# doc-idx.", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "doc_idx", "=", "_build_doc_idx", "(", "documents", ",", "num_epochs", ",", "np_rng", ")", "\n", "np", ".", "save", "(", "doc_idx_filename", ",", "doc_idx", ",", "allow_pickle", "=", "True", ")", "\n", "print_rank_0", "(", "' > elasped time to build and save doc-idx mapping '", "\n", "'(seconds): {:4f}'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "# sample-idx.", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "# Use C++ implementation for speed.", "\n", "# First compile and then import.", "\n", "from", "data", ".", "dataset_utils", "import", "compile_helper", "\n", "compile_helper", "(", ")", "\n", "from", "data", "import", "helpers", "\n", "assert", "doc_idx", ".", "dtype", "==", "np", ".", "int32", "\n", "assert", "sizes", ".", "dtype", "==", "np", ".", "int32", "\n", "sample_idx", "=", "helpers", ".", "build_sample_idx", "(", "sizes", ",", "doc_idx", ",", "seq_length", ",", "\n", "num_epochs", ",", "tokens_per_epoch", ")", "\n", "# sample_idx = _build_sample_idx(sizes, doc_idx, seq_length,", "\n", "#                               num_epochs, tokens_per_epoch)", "\n", "np", ".", "save", "(", "sample_idx_filename", ",", "sample_idx", ",", "allow_pickle", "=", "True", ")", "\n", "print_rank_0", "(", "' > elasped time to build and save sample-idx mapping '", "\n", "'(seconds): {:4f}'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "# shuffle-idx.", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "# -1 is due to data structure used to retieve the index:", "\n", "#    sample i --> [sample_idx[i], sample_idx[i+1])", "\n", "shuffle_idx", "=", "_build_shuffle_idx", "(", "sample_idx", ".", "shape", "[", "0", "]", "-", "1", ",", "np_rng", ")", "\n", "np", ".", "save", "(", "shuffle_idx_filename", ",", "shuffle_idx", ",", "allow_pickle", "=", "True", ")", "\n", "print_rank_0", "(", "' > elasped time to build and save shuffle-idx mapping'", "\n", "' (seconds): {:4f}'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "\n", "# This should be a barrier but nccl barrier assumes", "\n", "# device_index=rank which is not the case for model", "\n", "# parallel case", "\n", "", "", "counts", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "[", "1", "]", ")", "\n", "torch", ".", "distributed", ".", "all_reduce", "(", "counts", ",", "group", "=", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "assert", "counts", "[", "0", "]", ".", "item", "(", ")", "==", "torch", ".", "distributed", ".", "get_world_size", "(", "\n", "group", "=", "mpu", ".", "get_data_parallel_group", "(", ")", ")", "\n", "\n", "# Load mappings.", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "print_rank_0", "(", "' > loading doc-idx mapping from {}'", ".", "format", "(", "\n", "doc_idx_filename", ")", ")", "\n", "doc_idx", "=", "np", ".", "load", "(", "doc_idx_filename", ",", "allow_pickle", "=", "True", ",", "mmap_mode", "=", "'r'", ")", "\n", "print_rank_0", "(", "' > loading sample-idx mapping from {}'", ".", "format", "(", "\n", "sample_idx_filename", ")", ")", "\n", "sample_idx", "=", "np", ".", "load", "(", "sample_idx_filename", ",", "allow_pickle", "=", "True", ",", "mmap_mode", "=", "'r'", ")", "\n", "print_rank_0", "(", "' > loading shuffle-idx mapping from {}'", ".", "format", "(", "\n", "shuffle_idx_filename", ")", ")", "\n", "shuffle_idx", "=", "np", ".", "load", "(", "shuffle_idx_filename", ",", "allow_pickle", "=", "True", ",", "mmap_mode", "=", "'r'", ")", "\n", "print_rank_0", "(", "'    loaded indexed file in {:3.3f} seconds'", ".", "format", "(", "\n", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "print_rank_0", "(", "'    total number of samples: {}'", ".", "format", "(", "\n", "sample_idx", ".", "shape", "[", "0", "]", ")", ")", "\n", "print_rank_0", "(", "'    total number of epochs: {}'", ".", "format", "(", "num_epochs", ")", ")", "\n", "\n", "return", "doc_idx", ",", "sample_idx", ",", "shuffle_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset._num_tokens": [[231, 234], ["numpy.sum"], "function", ["None"], ["", "def", "_num_tokens", "(", "documents", ",", "sizes", ")", ":", "\n", "    ", "\"\"\"Total number of tokens in the dataset.\"\"\"", "\n", "return", "np", ".", "sum", "(", "sizes", "[", "documents", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset._num_epochs": [[236, 249], ["None"], "function", ["None"], ["", "def", "_num_epochs", "(", "tokens_per_epoch", ",", "seq_length", ",", "num_samples", ")", ":", "\n", "    ", "\"\"\"Based on number of samples and sequence lenght, calculate how many\n    epochs will be needed.\"\"\"", "\n", "num_epochs", "=", "0", "\n", "total_tokens", "=", "0", "\n", "while", "True", ":", "\n", "        ", "num_epochs", "+=", "1", "\n", "total_tokens", "+=", "tokens_per_epoch", "\n", "# -1 is because we need to retrieve seq_length + 1 token each time", "\n", "# but the last token will overlap with the first token of the next", "\n", "# sample except for the last sample.", "\n", "if", "(", "(", "total_tokens", "-", "1", ")", "//", "seq_length", ")", ">=", "num_samples", ":", "\n", "            ", "return", "num_epochs", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset._build_doc_idx": [[251, 260], ["doc_idx.astype.reshape", "doc_idx.astype.astype", "np_rng.shuffle", "len"], "function", ["None"], ["", "", "", "def", "_build_doc_idx", "(", "documents", ",", "num_epochs", ",", "np_rng", ")", ":", "\n", "    ", "\"\"\"Build an array with length = number-of-epochs * number-of-dcuments.\n    Each index is mapped to a corresponding document.\"\"\"", "\n", "doc_idx", "=", "np", ".", "mgrid", "[", "0", ":", "num_epochs", ",", "0", ":", "len", "(", "documents", ")", "]", "[", "1", "]", "\n", "doc_idx", "[", ":", "]", "=", "documents", "\n", "doc_idx", "=", "doc_idx", ".", "reshape", "(", "-", "1", ")", "\n", "doc_idx", "=", "doc_idx", ".", "astype", "(", "np", ".", "int32", ")", "\n", "np_rng", ".", "shuffle", "(", "doc_idx", ")", "\n", "return", "doc_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset._build_sample_idx": [[262, 309], ["numpy.zeros"], "function", ["None"], ["", "def", "_build_sample_idx", "(", "sizes", ",", "doc_idx", ",", "seq_length", ",", "\n", "num_epochs", ",", "tokens_per_epoch", ")", ":", "\n", "    ", "\"\"\"Sample index mapping is a 2D array with sizes\n    [number-of-samples + 1, 2] where [..., 0] contains\n    the index into `doc_idx` and [..., 1] is the\n    starting offset in that document.\"\"\"", "\n", "\n", "# Total number of samples. For -1 see comments in `_num_epochs`.", "\n", "num_samples", "=", "(", "num_epochs", "*", "tokens_per_epoch", "-", "1", ")", "//", "seq_length", "\n", "sample_idx", "=", "np", ".", "zeros", "(", "[", "num_samples", "+", "1", ",", "2", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "# Index into sample_idx.", "\n", "sample_index", "=", "0", "\n", "# Index into doc_idx.", "\n", "doc_idx_index", "=", "0", "\n", "# Begining offset for each document.", "\n", "doc_offset", "=", "0", "\n", "# Start with first document and no offset.", "\n", "sample_idx", "[", "sample_index", "]", "[", "0", "]", "=", "doc_idx_index", "\n", "sample_idx", "[", "sample_index", "]", "[", "1", "]", "=", "doc_offset", "\n", "sample_index", "+=", "1", "\n", "while", "sample_index", "<=", "num_samples", ":", "\n", "# Start with a fresh sequence.", "\n", "        ", "remaining_seq_length", "=", "seq_length", "+", "1", "\n", "while", "remaining_seq_length", "!=", "0", ":", "\n", "# Get the document length.", "\n", "            ", "doc_id", "=", "doc_idx", "[", "doc_idx_index", "]", "\n", "doc_length", "=", "sizes", "[", "doc_id", "]", "-", "doc_offset", "\n", "# And add it to the current sequence.", "\n", "remaining_seq_length", "-=", "doc_length", "\n", "# If we have more than a full sequence, adjust offset and set", "\n", "# remaining length to zero so we return from the while loop.", "\n", "# Note that -1 here is for the same reason we have -1 in", "\n", "# `_num_epochs` calculations.", "\n", "if", "remaining_seq_length", "<=", "0", ":", "\n", "                ", "doc_offset", "+=", "(", "remaining_seq_length", "+", "doc_length", "-", "1", ")", "\n", "remaining_seq_length", "=", "0", "\n", "", "else", ":", "\n", "# Otherwise, start from the begining of the next document.", "\n", "                ", "doc_idx_index", "+=", "1", "\n", "doc_offset", "=", "0", "\n", "# Record the sequence.", "\n", "", "", "sample_idx", "[", "sample_index", "]", "[", "0", "]", "=", "doc_idx_index", "\n", "sample_idx", "[", "sample_index", "]", "[", "1", "]", "=", "doc_offset", "\n", "sample_index", "+=", "1", "\n", "\n", "", "return", "sample_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.gpt2_dataset._build_shuffle_idx": [[311, 319], ["numpy.arange", "np_rng.shuffle", "numpy.iinfo"], "function", ["None"], ["", "def", "_build_shuffle_idx", "(", "size", ",", "np_rng", ")", ":", "\n", "    ", "\"\"\"Build the range [0, size) and shuffle.\"\"\"", "\n", "dtype_", "=", "np", ".", "uint32", "\n", "if", "size", ">=", "(", "np", ".", "iinfo", "(", "np", ".", "uint32", ")", ".", "max", "-", "1", ")", ":", "\n", "        ", "dtype_", "=", "np", ".", "int64", "\n", "", "shuffle_idx", "=", "np", ".", "arange", "(", "start", "=", "0", ",", "stop", "=", "size", ",", "step", "=", "1", ",", "dtype", "=", "dtype_", ")", "\n", "np_rng", ".", "shuffle", "(", "shuffle_idx", ")", "\n", "return", "shuffle_idx", "\n", "", ""]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.test.test_indexed_dataset.test_indexed_dataset": [[17, 41], ["megatron.data.indexed_dataset.make_dataset", "megatron.tokenizer.build_tokenizer", "print", "print", "print", "range", "len", "len", "indexed_dataset.make_dataset.prefetch", "print", "print", "range", "len", "len", "s.data.tolist", "megatron.tokenizer.build_tokenizer.detokenize", "print", "print", "len", "len"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.make_dataset", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.IndexedCachedDataset.prefetch"], ["def", "test_indexed_dataset", "(", "args", ")", ":", "\n", "    ", "ds", "=", "indexed_dataset", ".", "make_dataset", "(", "args", ".", "data", ",", "args", ".", "dataset_impl", ")", "\n", "tokenizer", "=", "build_tokenizer", "(", "args", ")", "\n", "print", "(", "len", "(", "ds", ".", "doc_idx", ")", ")", "\n", "print", "(", "len", "(", "ds", ")", ")", "\n", "print", "(", "ds", ".", "doc_idx", "[", "-", "1", "]", ")", "\n", "if", "ds", ".", "supports_prefetch", ":", "\n", "# just prefetch the whole thing in test (so assume it is small)", "\n", "        ", "ds", ".", "prefetch", "(", "range", "(", "len", "(", "ds", ")", ")", ")", "\n", "", "if", "args", ".", "count", ">", "len", "(", "ds", ".", "doc_idx", ")", "-", "1", ":", "\n", "        ", "args", ".", "count", "=", "len", "(", "ds", ".", "doc_idx", ")", "-", "1", "\n", "\n", "", "for", "i", "in", "range", "(", "args", ".", "count", ")", ":", "\n", "        ", "start", "=", "ds", ".", "doc_idx", "[", "i", "]", "\n", "end", "=", "ds", ".", "doc_idx", "[", "i", "+", "1", "]", "\n", "ids", "=", "ds", "[", "start", ":", "end", "]", "\n", "print", "(", "f\"Document {i}:\"", ")", "\n", "print", "(", "\"--------------\"", ")", "\n", "for", "s", "in", "ids", ":", "\n", "            ", "assert", "len", "(", "s", ")", ">", "0", "\n", "l", "=", "s", ".", "data", ".", "tolist", "(", ")", "\n", "text", "=", "tokenizer", ".", "detokenize", "(", "l", ")", "\n", "print", "(", "text", ")", "\n", "print", "(", "\"---\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.test.test_indexed_dataset.test_indexed_dataset_get": [[43, 62], ["megatron.data.indexed_dataset.make_dataset", "megatron.tokenizer.build_tokenizer", "print", "indexed_dataset.make_dataset.get", "print", "print", "indexed_dataset.make_dataset.get", "print", "indexed_dataset.make_dataset.get", "print", "indexed_dataset.make_dataset.get", "print"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.make_dataset", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.MMapIndexedDataset.get"], ["", "", "", "def", "test_indexed_dataset_get", "(", "args", ")", ":", "\n", "    ", "ds", "=", "indexed_dataset", ".", "make_dataset", "(", "args", ".", "data", ",", "args", ".", "dataset_impl", ")", "\n", "tokenizer", "=", "build_tokenizer", "(", "args", ")", "\n", "size", "=", "ds", ".", "sizes", "[", "0", "]", "\n", "print", "(", "f\"size: {size}\"", ")", "\n", "full", "=", "ds", ".", "get", "(", "0", ")", "\n", "print", "(", "full", ")", "\n", "# print(tokenizer.detokenize(full.data.tolist()))", "\n", "print", "(", "\"---\"", ")", "\n", "end", "=", "ds", ".", "get", "(", "0", ",", "offset", "=", "size", "-", "10", ")", "\n", "print", "(", "end", ")", "\n", "# print(tokenizer.detokenize(end.data.tolist()))", "\n", "\n", "start", "=", "ds", ".", "get", "(", "0", ",", "length", "=", "10", ")", "\n", "print", "(", "start", ")", "\n", "# print(tokenizer.detokenize(start.data.tolist()))", "\n", "\n", "part", "=", "ds", ".", "get", "(", "0", ",", "offset", "=", "2", ",", "length", "=", "8", ")", "\n", "print", "(", "part", ")", "\n", "# print(tokenizer.detokenize(part.data.tolist()))", "\n"]], "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.test.test_indexed_dataset.main": [[82, 122], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "test_indexed_dataset.test_indexed_dataset_get", "megatron.data.indexed_dataset.infer_dataset_impl"], "function", ["home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.test.test_indexed_dataset.test_indexed_dataset_get", "home.repos.pwc.inspect_result.TsinghuaAI_CPM-Generate.data.indexed_dataset.infer_dataset_impl"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data'", ",", "type", "=", "str", ",", "help", "=", "'prefix to data files'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset-impl'", ",", "type", "=", "str", ",", "default", "=", "'infer'", ",", "\n", "choices", "=", "[", "'lazy'", ",", "'cached'", ",", "'mmap'", ",", "'infer'", "]", ")", "\n", "parser", ".", "add_argument", "(", "'--count'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'Number of samples/documents to print'", ")", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "title", "=", "'tokenizer'", ")", "\n", "group", ".", "add_argument", "(", "'--tokenizer-type'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "choices", "=", "[", "'BertWordPieceLowerCase'", ",", "\n", "'GPT2BPETokenizer'", "]", ",", "\n", "help", "=", "'What type of tokenizer to use.'", ")", "\n", "group", ".", "add_argument", "(", "'--vocab-file'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Path to the vocab file'", ")", "\n", "group", ".", "add_argument", "(", "'--merge-file'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Path to the BPE merge file (if necessary).'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "5", ",", "\n", "help", "=", "'Number of epochs to plan for'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-num-samples'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'Maximum number of samples to plan for'", ")", "\n", "parser", ".", "add_argument", "(", "'--masked-lm-prob'", ",", "type", "=", "float", ",", "default", "=", "0.15", ",", "\n", "help", "=", "'probability of masking tokens'", ")", "\n", "parser", ".", "add_argument", "(", "'--seq-length'", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "'maximum sequence length'", ")", "\n", "parser", ".", "add_argument", "(", "'--short-seq-prob'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'probability of creating a short sequence'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1234", ",", "\n", "help", "=", "'random seed'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "rank", "=", "0", "\n", "args", ".", "make_vocab_size_divisible_by", "=", "128", "\n", "args", ".", "model_parallel_size", "=", "1", "\n", "\n", "if", "args", ".", "dataset_impl", "==", "\"infer\"", ":", "\n", "        ", "args", ".", "dataset_impl", "=", "indexed_dataset", ".", "infer_dataset_impl", "(", "args", ".", "data", ")", "\n", "\n", "#    test_albert_dataset(args)", "\n", "", "test_indexed_dataset_get", "(", "args", ")", "\n", "\n"]]}