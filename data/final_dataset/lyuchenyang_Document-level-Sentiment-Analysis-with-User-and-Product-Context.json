{"home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.modeling.SecondPretrainedBert.__init__": [[15, 21], ["transformers.BertPreTrainedModel.__init__", "transformers.BertForSequenceClassification", "torch.Embedding", "torch.Embedding", "torch.Embedding"], "methods", ["home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.Lang.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_embeddings", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "bert", "=", "BertForSequenceClassification", "(", "config", ")", "\n", "self", ".", "embedding_matrix", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.modeling.SecondPretrainedBert.forward": [[22, 36], ["modeling.SecondPretrainedBert.bert", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.SecondPretrainedBert.embedding_matrix", "modeling.SecondPretrainedBert.embedding_matrix.weight.index_copy", "user_product.view", "all_cls.detach().clone", "all_cls.detach().clone", "modeling.SecondPretrainedBert.view().detach", "all_cls.detach", "all_cls.detach", "modeling.SecondPretrainedBert.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "user_product", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "bert", "(", "**", "inputs", ")", "\n", "\n", "loss", ",", "last_hidden_states", "=", "outputs", "[", "0", "]", ",", "outputs", "[", "2", "]", "[", "self", ".", "config", ".", "num_hidden_layers", "-", "1", "]", "\n", "\n", "all_cls", "=", "last_hidden_states", "[", ":", ",", "0", ",", ":", "]", "\n", "\n", "_all_cls", "=", "torch", ".", "cat", "(", "[", "all_cls", ".", "detach", "(", ")", ".", "clone", "(", ")", ",", "all_cls", ".", "detach", "(", ")", ".", "clone", "(", ")", "]", ",", "dim", "=", "0", ")", "\n", "up_embeddings", "=", "self", ".", "embedding_matrix", "(", "user_product", ")", "\n", "\n", "self", ".", "embedding_matrix", ".", "weight", ".", "index_copy", "(", "0", ",", "user_product", ".", "view", "(", "-", "1", ")", ",", "\n", "up_embeddings", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "detach", "(", ")", "+", "_all_cls", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.modeling.IncrementalContextBert.__init__": [[40, 82], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.CELU", "torch.CELU", "torch.CELU", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "modeling.IncrementalContextBert.init_weights", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.Lang.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_embeddings", ",", "up_vocab", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "\n", "if", "config", ".", "do_shrink", ":", "\n", "            ", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "config", ".", "inner_size", ")", "\n", "self", ".", "to_hidden_size", "=", "nn", ".", "Linear", "(", "config", ".", "inner_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "to_inner_size", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "inner_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "\n", "", "self", ".", "multi_head_attention", "=", "torch", ".", "nn", ".", "MultiheadAttention", "(", "config", ".", "hidden_size", ",", "config", ".", "attention_heads", ")", "\n", "\n", "# Linear layers used to transform cls token, user and product embeddings", "\n", "self", ".", "linear_t", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "hidden_size", ")", "\n", "self", ".", "linear_u", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "hidden_size", ")", "\n", "self", ".", "linear_p", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "linear_update", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "linear_f", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "hidden_size", ")", "\n", "self", ".", "linear_g", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "hidden_size", ")", "\n", "\n", "# Activation functions", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "gelu", "=", "gelu", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "celu", "=", "nn", ".", "CELU", "(", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "# Classification layer", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "num_labels", ")", "\n", "\n", "# An empirical initializad number, still needed to be explored", "\n", "self", ".", "alpha", "=", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "-", "10", ",", "dtype", "=", "torch", ".", "float", ")", ",", "requires_grad", "=", "True", ")", "\n", "\n", "self", ".", "up_vocab", "=", "up_vocab", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.modeling.IncrementalContextBert.forward": [[83, 119], ["modeling.IncrementalContextBert.bert", "modeling.IncrementalContextBert.embedding", "modeling.IncrementalContextBert.multi_head_attention", "modeling.IncrementalContextBert.sigmoid", "modeling.IncrementalContextBert.sigmoid", "modeling.IncrementalContextBert.sigmoid", "modeling.IncrementalContextBert.classifier", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.IncrementalContextBert.embedding", "outputs[].transpose", "modeling.IncrementalContextBert.to_hidden_size", "modeling.IncrementalContextBert.transpose", "modeling.IncrementalContextBert.linear_t", "modeling.IncrementalContextBert.sigmoid", "modeling.IncrementalContextBert.sigmoid", "modeling.IncrementalContextBert.to_inner_size", "user_product.view().detach", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "modeling.IncrementalContextBert.embedding.weight.index_copy", "modeling.IncrementalContextBert.linear_u", "modeling.IncrementalContextBert.linear_p", "modeling.IncrementalContextBert.sigmoid", "user_product.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "user_product", ",", "up_indices", "=", "None", ",", "up_embeddings", "=", "None", ")", ":", "\n", "        ", "if", "up_indices", "is", "not", "None", "and", "up_embeddings", "is", "not", "None", ":", "\n", "            ", "p_up_embeddings", "=", "self", ".", "embedding", "(", "up_indices", ")", "\n", "update_embeddings", "=", "p_up_embeddings", "+", "self", ".", "sigmoid", "(", "self", ".", "alpha", ")", "*", "up_embeddings", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "self", ".", "embedding", ".", "weight", ".", "index_copy", "(", "0", ",", "up_indices", ",", "update_embeddings", ")", "\n", "\n", "", "", "outputs", "=", "self", ".", "bert", "(", "**", "inputs", ")", "\n", "\n", "last_hidden_states", ",", "cls_hidden_states", "=", "outputs", "[", "0", "]", ".", "transpose", "(", "0", ",", "1", ")", ",", "outputs", "[", "1", "]", "\n", "\n", "up_embeddings", "=", "self", ".", "embedding", "(", "user_product", ")", "\n", "\n", "if", "self", ".", "config", ".", "do_shrink", ":", "\n", "            ", "up_embeddings", "=", "self", ".", "to_hidden_size", "(", "up_embeddings", ")", "\n", "\n", "", "att_up", "=", "self", ".", "multi_head_attention", "(", "up_embeddings", ".", "transpose", "(", "0", ",", "1", ")", ",", "last_hidden_states", ",", "last_hidden_states", ")", "\n", "att_u", ",", "att_p", "=", "att_up", "[", "0", "]", "[", "0", ",", ":", ",", ":", "]", ",", "att_up", "[", "0", "]", "[", "1", ",", ":", ",", ":", "]", "\n", "\n", "z_cls", "=", "self", ".", "sigmoid", "(", "self", ".", "linear_t", "(", "cls_hidden_states", ")", ")", "\n", "z_att_u", ",", "z_att_p", "=", "self", ".", "sigmoid", "(", "self", ".", "linear_u", "(", "att_u", ")", ")", ",", "self", ".", "sigmoid", "(", "self", ".", "linear_p", "(", "att_p", ")", ")", "\n", "\n", "z_u", "=", "self", ".", "sigmoid", "(", "z_cls", "+", "z_att_u", ")", "\n", "z_p", "=", "self", ".", "sigmoid", "(", "z_cls", "+", "z_att_p", ")", "\n", "\n", "cls_input", "=", "cls_hidden_states", "+", "z_u", "*", "att_u", "+", "z_p", "*", "att_p", "\n", "\n", "logits", "=", "self", ".", "classifier", "(", "cls_input", ")", "\n", "# logits = self.softmax(logits)", "\n", "\n", "new_up_embeddings", "=", "torch", ".", "cat", "(", "[", "z_att_u", ",", "z_att_p", "]", ",", "dim", "=", "0", ")", "\n", "\n", "if", "self", ".", "config", ".", "do_shrink", ":", "\n", "            ", "new_up_embeddings", "=", "self", ".", "to_inner_size", "(", "new_up_embeddings", ")", "\n", "\n", "", "return", "logits", ",", "user_product", ".", "view", "(", "-", "1", ")", ".", "detach", "(", ")", ",", "new_up_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.modeling.IncrementalContextRoberta.__init__": [[126, 168], ["transformers.BertPreTrainedModel.__init__", "transformers.RobertaModel", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.CELU", "torch.CELU", "torch.CELU", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "modeling.IncrementalContextRoberta.init_weights", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.Lang.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_embeddings", ",", "up_vocab", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "roberta", "=", "RobertaModel", "(", "config", ")", "\n", "\n", "if", "config", ".", "do_shrink", ":", "\n", "            ", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "config", ".", "inner_size", ")", "\n", "self", ".", "to_hidden_size", "=", "nn", ".", "Linear", "(", "config", ".", "inner_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "to_inner_size", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "inner_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "\n", "", "self", ".", "multi_head_attention", "=", "torch", ".", "nn", ".", "MultiheadAttention", "(", "config", ".", "hidden_size", ",", "config", ".", "attention_heads", ")", "\n", "\n", "# Linear layers used to transform cls token, user and product embeddings", "\n", "self", ".", "linear_t", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "hidden_size", ")", "\n", "self", ".", "linear_u", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "hidden_size", ")", "\n", "self", ".", "linear_p", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "linear_update", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "linear_f", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "hidden_size", ")", "\n", "self", ".", "linear_g", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "hidden_size", ")", "\n", "\n", "# Activation functions", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "gelu", "=", "gelu", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "celu", "=", "nn", ".", "CELU", "(", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "# Classification layer", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "in_features", "=", "config", ".", "hidden_size", ",", "out_features", "=", "config", ".", "num_labels", ")", "\n", "\n", "# An empirical initializad number, still needed to be explored", "\n", "self", ".", "alpha", "=", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "-", "10", ",", "dtype", "=", "torch", ".", "float", ")", ",", "requires_grad", "=", "True", ")", "\n", "\n", "self", ".", "up_vocab", "=", "up_vocab", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.modeling.IncrementalContextRoberta.forward": [[169, 205], ["modeling.IncrementalContextRoberta.roberta", "modeling.IncrementalContextRoberta.embedding", "modeling.IncrementalContextRoberta.multi_head_attention", "modeling.IncrementalContextRoberta.sigmoid", "modeling.IncrementalContextRoberta.sigmoid", "modeling.IncrementalContextRoberta.sigmoid", "modeling.IncrementalContextRoberta.classifier", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.IncrementalContextRoberta.embedding", "outputs[].transpose", "modeling.IncrementalContextRoberta.to_hidden_size", "modeling.IncrementalContextRoberta.transpose", "modeling.IncrementalContextRoberta.linear_t", "modeling.IncrementalContextRoberta.sigmoid", "modeling.IncrementalContextRoberta.sigmoid", "modeling.IncrementalContextRoberta.to_inner_size", "user_product.view().detach", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "modeling.IncrementalContextRoberta.embedding.weight.index_copy", "modeling.IncrementalContextRoberta.linear_u", "modeling.IncrementalContextRoberta.linear_p", "modeling.IncrementalContextRoberta.sigmoid", "user_product.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "user_product", ",", "up_indices", "=", "None", ",", "up_embeddings", "=", "None", ")", ":", "\n", "        ", "if", "up_indices", "is", "not", "None", "and", "up_embeddings", "is", "not", "None", ":", "\n", "            ", "p_up_embeddings", "=", "self", ".", "embedding", "(", "up_indices", ")", "\n", "update_embeddings", "=", "p_up_embeddings", "+", "self", ".", "sigmoid", "(", "self", ".", "alpha", ")", "*", "up_embeddings", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "self", ".", "embedding", ".", "weight", ".", "index_copy", "(", "0", ",", "up_indices", ",", "update_embeddings", ")", "\n", "\n", "", "", "outputs", "=", "self", ".", "roberta", "(", "**", "inputs", ")", "\n", "\n", "last_hidden_states", ",", "cls_hidden_states", "=", "outputs", "[", "0", "]", ".", "transpose", "(", "0", ",", "1", ")", ",", "outputs", "[", "1", "]", "\n", "\n", "up_embeddings", "=", "self", ".", "embedding", "(", "user_product", ")", "\n", "\n", "if", "self", ".", "config", ".", "do_shrink", ":", "\n", "            ", "up_embeddings", "=", "self", ".", "to_hidden_size", "(", "up_embeddings", ")", "\n", "\n", "", "att_up", "=", "self", ".", "multi_head_attention", "(", "up_embeddings", ".", "transpose", "(", "0", ",", "1", ")", ",", "last_hidden_states", ",", "last_hidden_states", ")", "\n", "att_u", ",", "att_p", "=", "att_up", "[", "0", "]", "[", "0", ",", ":", ",", ":", "]", ",", "att_up", "[", "0", "]", "[", "1", ",", ":", ",", ":", "]", "\n", "\n", "z_cls", "=", "self", ".", "sigmoid", "(", "self", ".", "linear_t", "(", "cls_hidden_states", ")", ")", "\n", "z_att_u", ",", "z_att_p", "=", "self", ".", "sigmoid", "(", "self", ".", "linear_u", "(", "att_u", ")", ")", ",", "self", ".", "sigmoid", "(", "self", ".", "linear_p", "(", "att_p", ")", ")", "\n", "\n", "z_u", "=", "self", ".", "sigmoid", "(", "z_cls", "+", "z_att_u", ")", "\n", "z_p", "=", "self", ".", "sigmoid", "(", "z_cls", "+", "z_att_p", ")", "\n", "\n", "cls_input", "=", "cls_hidden_states", "+", "z_u", "*", "att_u", "+", "z_p", "*", "att_p", "\n", "\n", "logits", "=", "self", ".", "classifier", "(", "cls_input", ")", "\n", "# logits = self.softmax(logits)", "\n", "\n", "new_up_embeddings", "=", "torch", ".", "cat", "(", "[", "z_att_u", ",", "z_att_p", "]", ",", "dim", "=", "0", ")", "\n", "\n", "if", "self", ".", "config", ".", "do_shrink", ":", "\n", "            ", "new_up_embeddings", "=", "self", ".", "to_inner_size", "(", "new_up_embeddings", ")", "\n", "\n", "", "return", "logits", ",", "user_product", ".", "view", "(", "-", "1", ")", ".", "detach", "(", ")", ",", "new_up_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.modeling.FocalLoss.__init__": [[208, 216], ["torch.Module.__init__", "isinstance", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.Lang.__init__"], ["    ", "def", "__init__", "(", "self", ",", "gamma", "=", "0", ",", "alpha", "=", "None", ",", "size_average", "=", "True", ")", ":", "\n", "        ", "super", "(", "FocalLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "alpha", "=", "alpha", "\n", "if", "isinstance", "(", "alpha", ",", "list", ")", ":", "\n", "            ", "self", ".", "alpha", "=", "torch", ".", "tensor", "(", "alpha", ")", "\n", "", "self", ".", "size_average", "=", "size_average", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.modeling.FocalLoss.forward": [[217, 234], ["torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "logpt.view.view.gather", "logpt.view.view.view", "target.view", "modeling.FocalLoss.alpha.gather", "loss.mean", "loss.sum", "modeling.FocalLoss.alpha.type", "input.data.type", "modeling.FocalLoss.alpha.type_as", "target.data.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "target", ")", ":", "\n", "        ", "logpt", "=", "F", ".", "log_softmax", "(", "input", ",", "dim", "=", "1", ")", "\n", "logpt", "=", "logpt", ".", "gather", "(", "1", ",", "target", ".", "view", "(", "-", "1", ",", "1", ")", ")", "\n", "logpt", "=", "logpt", ".", "view", "(", "-", "1", ")", "\n", "pt", "=", "logpt", "\n", "\n", "if", "self", ".", "alpha", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "alpha", ".", "type", "(", ")", "!=", "input", ".", "data", ".", "type", "(", ")", ":", "\n", "                ", "self", ".", "alpha", "=", "self", ".", "alpha", ".", "type_as", "(", "input", ".", "data", ")", "\n", "", "at", "=", "self", ".", "alpha", ".", "gather", "(", "0", ",", "target", ".", "data", ".", "view", "(", "-", "1", ")", ")", "\n", "logpt", "=", "logpt", "*", "at", "\n", "\n", "", "loss", "=", "-", "1", "*", "(", "1", "-", "pt", ")", "**", "self", ".", "gamma", "*", "logpt", "\n", "if", "self", ".", "size_average", ":", "\n", "            ", "return", "loss", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "loss", ".", "sum", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.set_seed": [[61, 67], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "function", ["None"], ["def", "set_seed", "(", "args", ")", ":", "\n", "    ", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.second_train": [[69, 87], ["torch.utils.data.RandomSampler", "torch.utils.data.DataLoader", "tqdm.tqdm", "model.eval", "tuple", "torch.no_grad", "torch.no_grad", "torch.cat", "torch.cat", "model", "t.to", "batch[].view", "batch[].view"], "function", ["None"], ["", "", "def", "second_train", "(", "args", ",", "train_dataset", ",", "model", ")", ":", "\n", "    ", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "train_dataset", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Second Train Iterating\"", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "batch", "=", "tuple", "(", "t", ".", "to", "(", "args", ".", "device", ")", "for", "t", "in", "batch", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "inputs", "=", "{", "\"input_ids\"", ":", "batch", "[", "0", "]", ",", "\"attention_mask\"", ":", "batch", "[", "1", "]", ",", "\"labels\"", ":", "batch", "[", "3", "]", "}", "\n", "if", "args", ".", "model_type", "!=", "\"roberta\"", ":", "\n", "                ", "inputs", "[", "\"token_type_ids\"", "]", "=", "(", "\n", "batch", "[", "2", "]", "if", "args", ".", "model_type", "in", "[", "\"bert\"", ",", "\"xlnet\"", "]", "else", "None", "\n", ")", "\n", "\n", "", "user", ",", "product", "=", "batch", "[", "4", "]", ".", "view", "(", "-", "1", ",", "1", ")", ",", "batch", "[", "5", "]", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "user_product", "=", "torch", ".", "cat", "(", "[", "user", ",", "product", "]", ",", "dim", "=", "1", ")", "\n", "_", "=", "model", "(", "inputs", ",", "user_product", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.train": [[89, 242], ["SummaryWriter", "len", "torch.utils.data.RandomSampler", "torch.utils.data.DataLoader", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "model.zero_grad", "tqdm.trange", "run_document_level_sa.set_seed", "torch.nn.CrossEntropyLoss", "SummaryWriter.close", "len", "len", "int", "modeling.FocalLoss", "tqdm.tqdm", "enumerate", "int", "model.train", "tuple", "torch.cat", "torch.cat", "modeling.FocalLoss.", "loss.mean.backward", "loss.mean.item", "batch[].view", "batch[].view", "model", "model", "logits.view", "batch[].view", "loss.mean.mean", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "model.zero_grad", "t.to", "model.parameters", "logs.items", "print", "model.named_parameters", "model.named_parameters", "any", "model.named_parameters", "model.named_parameters", "any", "run_document_level_sa.evaluate", "evaluate.items", "transformers.get_linear_schedule_with_warmup.get_lr", "SummaryWriter.add_scalar", "json.dumps", "any", "any"], "function", ["home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.set_seed", "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.train", "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.evaluate"], ["", "", "", "def", "train", "(", "args", ",", "train_dataset", ",", "model", ",", "tokenizer", ",", "freeze", "=", "True", ",", "dev_set", "=", "None", ",", "eval_set", "=", "None", ",", "global_s", "=", "0", ")", ":", "\n", "    ", "\"\"\" Training the model \"\"\"", "\n", "tb_writer", "=", "SummaryWriter", "(", ")", "\n", "\n", "num_labels", "=", "len", "(", "args", ".", "label_list", ")", "\n", "\n", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "train_dataset", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "\n", "t_total", "=", "len", "(", "train_dataloader", ")", "*", "args", ".", "num_train_epochs", "\n", "\n", "if", "args", ".", "is_second_training", "and", "freeze", ":", "\n", "        ", "model", ".", "embedding", ".", "weight", ".", "require_grad", "=", "False", "\n", "\n", "# Prepare optimizer for training", "\n", "", "if", "args", ".", "is_incremental", "and", "args", ".", "model_type", "==", "\"roberta\"", ":", "\n", "        ", "large_lr", "=", "[", "\"embedding.weight\"", "]", "\n", "optimizer_group_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "large_lr", ")", "]", ",", "\n", "\"lr\"", ":", "args", ".", "learning_rate", ",", "\n", "}", ",", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "large_lr", ")", "]", ",", "\n", "\"lr\"", ":", "args", ".", "amply", "*", "args", ".", "learning_rate", ",", "\n", "}", "\n", "]", "\n", "", "else", ":", "\n", "        ", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_group_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "args", ".", "weight_decay", "\n", "}", ",", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "0.0", "\n", "}", "\n", "]", "\n", "\n", "", "optimizer", "=", "AdamW", "(", "optimizer_group_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "eps", "=", "args", ".", "adam_epsilon", ")", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "int", "(", "args", ".", "warmup_steps", "*", "t_total", ")", ",", "num_training_steps", "=", "t_total", ")", "\n", "\n", "# Train", "\n", "# Train!", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_dataset", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num Epochs = %d\"", ",", "args", ".", "num_train_epochs", ")", "\n", "logger", ".", "info", "(", "\"  Instantaneous batch size per GPU = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Gradient Accumulation steps = %d\"", ",", "args", ".", "gradient_accumulation_steps", ")", "\n", "logger", ".", "info", "(", "\"  Total optimization steps = %d\"", ",", "t_total", ")", "\n", "\n", "global_step", "=", "global_s", "\n", "epochs_trained", "=", "0", "\n", "steps_trained_in_current_epoch", "=", "0", "\n", "\n", "# Check if continuing training from a checkpoint", "\n", "# if os.path.exists(args.model_name_or_path):", "\n", "#     global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])", "\n", "#     epochs_trained = global_step // len(train_dataloader // args.gradient_accumulation_steps)", "\n", "#     steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)", "\n", "#", "\n", "#     logger.info(\"  Continuing training from epoch %d\", epochs_trained)", "\n", "#     logger.info(\"  Continuing training from global step %d\", global_step)", "\n", "#     logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)", "\n", "\n", "tr_loss", ",", "logging_loss", "=", "0.0", ",", "0.0", "\n", "model", ".", "zero_grad", "(", ")", "\n", "train_iterator", "=", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", "\n", "set_seed", "(", "args", ")", "# Added here for reproductibility (even between python 2 and 3)", "\n", "up_indices", ",", "new_embeddings", "=", "None", ",", "None", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "if", "args", ".", "is_focal_loss", ":", "\n", "        ", "loss_fct", "=", "FocalLoss", "(", "gamma", "=", "3", ",", "alpha", "=", "args", ".", "alpha", ")", "\n", "\n", "", "for", "_", "in", "train_iterator", ":", "\n", "        ", "epoch_iterator", "=", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ")", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "epoch_iterator", ")", ":", "\n", "\n", "# Skip past any already trained steps", "\n", "            ", "if", "steps_trained_in_current_epoch", ">", "0", ":", "\n", "                ", "steps_trained_in_current_epoch", "-=", "1", "\n", "\n", "", "model", ".", "train", "(", ")", "\n", "batch", "=", "tuple", "(", "t", ".", "to", "(", "args", ".", "device", ")", "for", "t", "in", "batch", ")", "\n", "\n", "user", ",", "product", "=", "batch", "[", "4", "]", ".", "view", "(", "-", "1", ",", "1", ")", ",", "batch", "[", "5", "]", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "user_product", "=", "torch", ".", "cat", "(", "[", "user", ",", "product", "]", ",", "dim", "=", "1", ")", "\n", "\n", "inputs", "=", "{", "'input_ids'", ":", "batch", "[", "0", "]", ",", "\n", "'attention_mask'", ":", "batch", "[", "1", "]", "}", "\n", "if", "args", ".", "model_type", "!=", "'distilbert'", ":", "\n", "                ", "inputs", "[", "'token_type_ids'", "]", "=", "batch", "[", "2", "]", "if", "args", ".", "model_type", "in", "[", "'bert'", ",", "'xlnet'", "]", "else", "None", "# XLM, DistilBERT and RoBERTa don't use segment_ids", "\n", "\n", "", "if", "args", ".", "is_incremental", ":", "\n", "                ", "logits", ",", "up_indices", ",", "new_embeddings", "=", "model", "(", "inputs", ",", "user_product", ",", "up_indices", ",", "new_embeddings", ")", "\n", "", "else", ":", "\n", "                ", "inputs", "[", "'labels'", "]", "=", "batch", "[", "3", "]", "\n", "outputs", "=", "model", "(", "**", "inputs", ")", "\n", "logits", "=", "outputs", "[", "1", "]", "# model outputs are always tuple in transformers (see doc)", "\n", "\n", "", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "num_labels", ")", ",", "batch", "[", "3", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu parallel training", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "\n", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "\n", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "# Update learning rate schedule", "\n", "model", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "if", "args", ".", "logging_steps", ">", "0", "and", "global_step", "%", "args", ".", "logging_steps", "==", "0", ":", "\n", "                    ", "logs", "=", "{", "}", "\n", "if", "global_step", "%", "(", "5", "*", "args", ".", "logging_steps", "==", "0", ")", "and", "dev_set", "is", "not", "None", ":", "\n", "                        ", "results", "=", "evaluate", "(", "args", ",", "model", ",", "dev_set", ",", "tokenizer", ")", "\n", "for", "key", ",", "value", "in", "results", ".", "items", "(", ")", ":", "\n", "                            ", "eval_key", "=", "'eval_{}'", ".", "format", "(", "key", ")", "\n", "logs", "[", "eval_key", "]", "=", "value", "\n", "\n", "", "", "loss_scalar", "=", "(", "tr_loss", "-", "logging_loss", ")", "/", "args", ".", "logging_steps", "\n", "learning_rate_scalar", "=", "scheduler", ".", "get_lr", "(", ")", "[", "0", "]", "\n", "logs", "[", "'learning_rate'", "]", "=", "learning_rate_scalar", "\n", "logs", "[", "'loss'", "]", "=", "loss_scalar", "\n", "logging_loss", "=", "tr_loss", "\n", "\n", "for", "key", ",", "value", "in", "logs", ".", "items", "(", ")", ":", "\n", "                        ", "tb_writer", ".", "add_scalar", "(", "key", ",", "value", ",", "global_step", ")", "\n", "", "print", "(", "json", ".", "dumps", "(", "{", "**", "logs", ",", "**", "{", "'step'", ":", "global_step", "}", "}", ")", ")", "\n", "\n", "# if args.save_steps > 0 and global_step % args.save_steps == 0:", "\n", "#     # Save model checkpoint", "\n", "#     output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))", "\n", "#     if not os.path.exists(output_dir):", "\n", "#         os.makedirs(output_dir)", "\n", "#     model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training", "\n", "#     model_to_save.save_pretrained(output_dir)", "\n", "#     torch.save(args, os.path.join(output_dir, 'training_args.bin'))", "\n", "#     logger.info(\"Saving model checkpoint to %s\", output_dir)", "\n", "\n", "\n", "", "", "", "", "tb_writer", ".", "close", "(", ")", "\n", "global_step", "=", "1", "if", "global_step", "==", "0", "else", "global_step", "\n", "\n", "return", "global_step", ",", "tr_loss", "/", "global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.evaluate": [[244, 313], ["zip", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "logger.info", "logger.info", "logger.info", "tqdm.tqdm", "results.update", "utils.eval_to_file", "print", "print", "print", "print", "print", "print", "os.path.exists", "os.makedirs", "len", "model.eval", "tuple", "torch.cat", "torch.cat", "numpy.argmax", "sklearn.accuracy_score", "sklearn.precision_score", "sklearn.recall_score", "sklearn.f1_score", "sklearn.mean_squared_error", "sklearn.mean_squared_error", "batch[].view", "batch[].view", "torch.no_grad", "torch.no_grad", "logits.detach().cpu().numpy", "batch[].detach().cpu().numpy", "numpy.append", "numpy.append", "numpy.squeeze", "t.to", "model", "model", "logits.detach().cpu().numpy", "batch[].detach().cpu().numpy", "logits.detach().cpu", "batch[].detach().cpu", "logits.detach().cpu", "batch[].detach().cpu", "logits.detach", "batch[].detach", "logits.detach", "batch[].detach"], "function", ["home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.eval_to_file"], ["", "def", "evaluate", "(", "args", ",", "model", ",", "eval_dataset", ",", "tokenizer", ",", "prefix", "=", "\"\"", ")", ":", "\n", "# Loop to handle MNLI double evaluation (matched, mis-matched)", "\n", "    ", "eval_task_names", "=", "(", "args", ".", "task_name", ",", ")", "\n", "eval_outputs_dirs", "=", "(", "args", ".", "output_dir", ",", ")", "\n", "\n", "results", "=", "{", "}", "\n", "for", "eval_task", ",", "eval_output_dir", "in", "zip", "(", "eval_task_names", ",", "eval_outputs_dirs", ")", ":", "\n", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "eval_output_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "eval_output_dir", ")", "\n", "\n", "", "args", ".", "eval_batch_size", "=", "args", ".", "train_batch_size", "\n", "# Note that DistributedSampler samples randomly", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_dataset", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_dataset", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "\n", "# Eval!", "\n", "logger", ".", "info", "(", "\"***** Running evaluation {} *****\"", ".", "format", "(", "prefix", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_dataset", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "eval_loss", "=", "0.0", "\n", "nb_eval_steps", "=", "0", "\n", "preds", "=", "None", "\n", "out_label_ids", "=", "None", "\n", "up_indices", ",", "new_embeddings", "=", "None", ",", "None", "\n", "for", "batch", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "batch", "=", "tuple", "(", "t", ".", "to", "(", "args", ".", "device", ")", "for", "t", "in", "batch", ")", "\n", "\n", "user", ",", "product", "=", "batch", "[", "4", "]", ".", "view", "(", "-", "1", ",", "1", ")", ",", "batch", "[", "5", "]", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "user_product", "=", "torch", ".", "cat", "(", "[", "user", ",", "product", "]", ",", "dim", "=", "1", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "inputs", "=", "{", "'input_ids'", ":", "batch", "[", "0", "]", ",", "'attention_mask'", ":", "batch", "[", "1", "]", "}", "\n", "if", "args", ".", "model_type", "!=", "'roberta'", ":", "\n", "                    ", "inputs", "[", "'token_type_ids'", "]", "=", "batch", "[", "2", "]", "if", "args", ".", "model_type", "in", "[", "'bert'", ",", "\n", "'xlnet'", "]", "else", "None", "# XLM, DistilBERT and RoBERTa don't use segment_ids", "\n", "\n", "", "if", "args", ".", "is_incremental", ":", "\n", "                    ", "logits", ",", "up_indices", ",", "new_embeddings", "=", "model", "(", "inputs", ",", "user_product", ")", "\n", "", "else", ":", "\n", "                    ", "inputs", "[", "'labels'", "]", "=", "batch", "[", "3", "]", "\n", "outputs", "=", "model", "(", "**", "inputs", ")", "\n", "logits", "=", "outputs", "[", "1", "]", "# model outputs are always tuple in transformers (see doc)", "\n", "", "", "nb_eval_steps", "+=", "1", "\n", "if", "preds", "is", "None", ":", "\n", "                ", "preds", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "out_label_ids", "=", "batch", "[", "3", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                ", "preds", "=", "np", ".", "append", "(", "preds", ",", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "0", ")", "\n", "out_label_ids", "=", "np", ".", "append", "(", "out_label_ids", ",", "batch", "[", "3", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "0", ")", "\n", "\n", "", "", "if", "args", ".", "output_mode", "==", "\"classification\"", ":", "\n", "            ", "preds", "=", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "1", ")", "\n", "", "elif", "args", ".", "output_mode", "==", "\"regression\"", ":", "\n", "            ", "preds", "=", "np", ".", "squeeze", "(", "preds", ")", "\n", "", "result", "=", "{", "'acc'", ":", "(", "preds", "==", "out_label_ids", ")", ".", "mean", "(", ")", "}", "\n", "results", ".", "update", "(", "result", ")", "\n", "\n", "eval_to_file", "(", "args", ".", "eval_out_file", ",", "preds", ",", "out_label_ids", ")", "\n", "print", "(", "\"accuracy: \"", ",", "metric", ".", "accuracy_score", "(", "out_label_ids", ",", "preds", ")", ")", "\n", "print", "(", "\"precision: \"", ",", "metric", ".", "precision_score", "(", "out_label_ids", ",", "preds", ",", "average", "=", "'macro'", ")", ")", "\n", "print", "(", "\"recall: \"", ",", "metric", ".", "recall_score", "(", "out_label_ids", ",", "preds", ",", "average", "=", "'macro'", ")", ")", "\n", "print", "(", "\"F1: \"", ",", "metric", ".", "f1_score", "(", "out_label_ids", ",", "preds", ",", "average", "=", "'macro'", ")", ")", "\n", "print", "(", "\"Mean Squared Error: \"", ",", "metric", ".", "mean_squared_error", "(", "out_label_ids", ",", "preds", ")", ")", "\n", "print", "(", "\"Root Mean Squared Error: \"", ",", "metric", ".", "mean_squared_error", "(", "out_label_ids", ",", "preds", ",", "squared", "=", "False", ")", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.main": [[315, 538], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_known_args", "torch.device", "torch.device", "tokenizer_class.from_pretrained", "len", "utils.load_data", "run_document_level_sa.set_seed", "args.model_type.lower", "config_class.from_pretrained", "model_class.from_pretrained.to", "logger.info", "ValueError", "model_class.from_pretrained", "model_class.from_pretrained", "run_document_level_sa.train", "logger.info", "SecondPretrainingBert.from_pretrained", "SecondPretrainingBert.from_pretrained.to", "run_document_level_sa.second_train", "modeling.IncrementalContextBert.from_pretrained", "model_class.from_pretrained.to", "run_document_level_sa.train", "logger.info", "logger.info", "model_to_save.save_pretrained", "tokenizer_class.from_pretrained.save_pretrained", "torch.save", "torch.save", "tokenizer_class.from_pretrained", "logger.info", "str", "str", "str", "os.path.exists", "os.makedirs", "hasattr", "os.path.join", "list", "logging.getLogger().setLevel", "model_class.from_pretrained.to", "run_document_level_sa.evaluate", "run_document_level_sa.evaluate", "dict", "results.update", "model_class.from_pretrained", "model_class.from_pretrained", "str", "str", "os.path.dirname", "logging.getLogger", "len", "checkpoint.split", "checkpoint.find", "checkpoint.split", "sorted", "dict.items", "str", "str", "glob.glob", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.load_data", "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.set_seed", "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.train", "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.second_train", "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.train", "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.evaluate", "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.run_document_level_sa.evaluate"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--task_name\"", ",", "type", "=", "str", ",", "default", "=", "\"yelp-2013\"", ",", "\n", "help", "=", "\"the name of the training task (the dataset name)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_size\"", ",", "type", "=", "str", ",", "default", "=", "\"base\"", ",", "\n", "help", "=", "\"the size of pre-trained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_type\"", ",", "type", "=", "str", ",", "default", "=", "\"bert\"", ",", "\n", "help", "=", "\"the type of pre-trained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--epochs\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"the numebr of training epochs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--incremental\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"use incremental mode\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--second_train\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"use second train mode\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"whether to train the model or not\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"whether to evaluate the model or not\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "\"the weight decay rate\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "type", "=", "float", ",", "default", "=", "3e-5", ",", "\n", "help", "=", "\"the learning rate used to train the model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "\n", "help", "=", "\"the warm_up step rate\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "\"the maximum sequence length used to load dataset\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"the random seed used in model initialization and dataloader\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "\"the batch size used in training and evaluation\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--device\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"the device id used for training and evaluation\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_shrink\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"whether to shrink the embedding size in order to reduce the amount of parameters\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--inner_size\"", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "\"the inner embedding size when do_shrink is true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_heads\"", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "\"the attention heads used in multi head attention function\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--is_focal_loss\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"whether to use focal loss function or not\"", ")", "\n", "\n", "arguments", ",", "_", "=", "parser", ".", "parse_known_args", "(", ")", "\n", "\n", "args", ".", "task_name", "=", "arguments", ".", "task_name", "\n", "args", ".", "model_size", "=", "arguments", ".", "model_size", "\n", "args", ".", "num_train_epochs", "=", "arguments", ".", "epochs", "\n", "args", ".", "is_incremental", "=", "arguments", ".", "incremental", "\n", "args", ".", "is_second_training", "=", "arguments", ".", "second_train", "\n", "args", ".", "do_train", "=", "arguments", ".", "do_train", "\n", "args", ".", "do_eval", "=", "arguments", ".", "do_eval", "\n", "args", ".", "weight_decay", "=", "arguments", ".", "weight_decay", "\n", "args", ".", "learning_rate", "=", "arguments", ".", "learning_rate", "\n", "args", ".", "warmup_steps", "=", "arguments", ".", "warmup_steps", "\n", "args", ".", "max_seq_length", "=", "arguments", ".", "max_seq_length", "\n", "args", ".", "seed", "=", "arguments", ".", "seed", "\n", "args", ".", "train_batch_size", "=", "arguments", ".", "batch_size", "\n", "args", ".", "device", "=", "torch", ".", "device", "(", "\"cuda:\"", "+", "str", "(", "arguments", ".", "device", ")", ")", "\n", "args", ".", "model_type", "=", "arguments", ".", "model_type", "\n", "args", ".", "is_focal_loss", "=", "arguments", ".", "is_focal_loss", "\n", "args", ".", "do_shrink", "=", "arguments", ".", "do_shrink", "\n", "\n", "if", "args", ".", "task_name", "==", "'imdb'", ":", "\n", "        ", "args", ".", "label_list", "=", "label_list_imdb", "\n", "", "else", ":", "\n", "        ", "args", ".", "label_list", "=", "label_list", "\n", "\n", "", "if", "args", ".", "is_incremental", "and", "args", ".", "is_second_training", ":", "\n", "        ", "raise", "ValueError", "(", "\"Incremental and second training modes can't be applied at the same time!\"", ")", "\n", "\n", "", "if", "args", ".", "model_type", "==", "'bert'", ":", "\n", "        ", "args", ".", "model_name_or_path", "=", "\"bert-\"", "+", "args", ".", "model_size", "+", "\"-uncased\"", "\n", "", "if", "args", ".", "model_type", "==", "'roberta'", ":", "\n", "        ", "args", ".", "model_name_or_path", "=", "\"roberta-\"", "+", "args", ".", "model_size", "\n", "\n", "", "if", "args", ".", "is_incremental", ":", "\n", "        ", "model_type", "=", "\"incremental\"", "\n", "", "else", ":", "\n", "        ", "model_type", "=", "'vanilla'", "\n", "\n", "", "if", "args", ".", "is_second_training", ":", "\n", "        ", "model_type", "=", "'second_train'", "\n", "\n", "", "if", "args", ".", "do_shrink", ":", "\n", "        ", "model_type", "+=", "\"_shrink\"", "\n", "\n", "", "output_dir", "=", "\"trained_model/\"", "+", "args", ".", "model_name_or_path", "+", "\"_\"", "+", "args", ".", "task_name", "+", "'_'", "+", "model_type", "+", "\"_epochs_\"", "+", "str", "(", "args", ".", "num_train_epochs", ")", "+", "\"_lr_\"", "+", "str", "(", "args", ".", "learning_rate", ")", "+", "\"_weight-decay_\"", "+", "str", "(", "args", ".", "weight_decay", ")", "+", "\"_warmup_\"", "+", "str", "(", "args", ".", "warmup_steps", ")", "+", "\"_mql_\"", "+", "str", "(", "args", ".", "max_seq_length", ")", "+", "'_shrink_'", "+", "str", "(", "arguments", ".", "inner_size", ")", "+", "\"_seed_\"", "+", "str", "(", "args", ".", "seed", ")", "+", "\"/\"", "\n", "eval_dir", "=", "\"eval_results/\"", "+", "args", ".", "model_name_or_path", "+", "\"_\"", "+", "args", ".", "task_name", "+", "'_'", "+", "model_type", "+", "\"_epochs_\"", "+", "str", "(", "args", ".", "num_train_epochs", ")", "+", "\"_lr_\"", "+", "str", "(", "args", ".", "learning_rate", ")", "+", "\"_weight-decay_\"", "+", "str", "(", "args", ".", "weight_decay", ")", "+", "\"_warmup_\"", "+", "str", "(", "args", ".", "warmup_steps", ")", "+", "\"_mql_\"", "+", "str", "(", "args", ".", "max_seq_length", ")", "+", "'_shrink_'", "+", "str", "(", "arguments", ".", "inner_size", ")", "+", "\"_seed_\"", "+", "str", "(", "args", ".", "seed", ")", "+", "\".log\"", "\n", "\n", "args", ".", "output_dir", "=", "output_dir", "\n", "args", ".", "eval_out_file", "=", "eval_dir", "\n", "\n", "config_class", ",", "tokenizer_class", ",", "model_class", "=", "MODEL_CLASSES", "[", "args", ".", "model_type", "]", "\n", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "\n", "args", ".", "model_name_or_path", ",", "\n", "do_lower_case", "=", "True", ",", "\n", ")", "\n", "\n", "num_labels", "=", "len", "(", "args", ".", "label_list", ")", "\n", "\n", "data_dirs", "=", "[", "\"data/document-level-sa-dataset/\"", "+", "args", ".", "task_name", "+", "\"-seg-20-20.train.ss\"", ",", "\n", "\"data/document-level-sa-dataset/\"", "+", "args", ".", "task_name", "+", "\"-seg-20-20.dev.ss\"", ",", "\n", "\"data/document-level-sa-dataset/\"", "+", "args", ".", "task_name", "+", "\"-seg-20-20.test.ss\"", ",", "\n", "]", "\n", "train_dataset", ",", "dev_dataset", ",", "test_dataset", ",", "up_vocab", "=", "load_data", "(", "args", ",", "data_dirs", ",", "tokenizer", ")", "\n", "\n", "# long_dev_dataset, long_test_dataset = load_dev_and_eval_data(args, data_dirs[1:], tokenizer, up_vocab)", "\n", "\n", "set_seed", "(", "args", ")", "\n", "args", ".", "model_type", "=", "args", ".", "model_type", ".", "lower", "(", ")", "\n", "config", "=", "config_class", ".", "from_pretrained", "(", "\n", "args", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", ")", "\n", "config", ".", "output_attention", "=", "True", "\n", "config", ".", "output_hidden_states", "=", "True", "\n", "config", ".", "inner_size", "=", "arguments", ".", "inner_size", "\n", "config", ".", "do_shrink", "=", "arguments", ".", "do_shrink", "\n", "config", ".", "attention_heads", "=", "arguments", ".", "attention_heads", "\n", "if", "args", ".", "is_incremental", ":", "\n", "        ", "model_class", "=", "IncrementalContextBert", "if", "args", ".", "model_type", "==", "\"bert\"", "else", "IncrementalContextRoberta", "\n", "model", "=", "model_class", ".", "from_pretrained", "(", "\n", "args", ".", "model_name_or_path", ",", "\n", "num_embeddings", "=", "up_vocab", ".", "word_count", ",", "\n", "up_vocab", "=", "up_vocab", ",", "\n", "config", "=", "config", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "model_class", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ",", "config", "=", "config", ")", "\n", "\n", "", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "args", ")", "\n", "\n", "# Training", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "global_step", ",", "tr_loss", "=", "train", "(", "args", ",", "train_dataset", ",", "model", ",", "tokenizer", ",", "freeze", "=", "False", ",", "dev_set", "=", "dev_dataset", ",", "eval_set", "=", "test_dataset", ")", "\n", "logger", ".", "info", "(", "\" global_step = %s, average loss = %s\"", ",", "global_step", ",", "tr_loss", ")", "\n", "\n", "", "if", "args", ".", "is_second_training", ":", "\n", "        ", "st_model", "=", "SecondPretrainingBert", ".", "from_pretrained", "(", "\n", "args", ".", "model_name_or_path", ",", "\n", "num_embeddings", "=", "up_vocab", ".", "word_count", ",", "\n", "config", "=", "config", "\n", ")", "\n", "st_model", ".", "bert", "=", "model", "\n", "st_model", ".", "to", "(", "args", ".", "device", ")", "\n", "second_train", "(", "args", ",", "train_dataset", ",", "st_model", ")", "\n", "\n", "embedding_matrix", "=", "st_model", ".", "embedding_matrix", "\n", "\n", "new_model", "=", "IncrementalContextBert", ".", "from_pretrained", "(", "\n", "args", ".", "model_name_or_path", ",", "\n", "num_embeddings", "=", "up_vocab", ".", "word_count", ",", "\n", "up_vocab", "=", "up_vocab", ",", "\n", "config", "=", "config", ",", "\n", ")", "\n", "\n", "new_model", ".", "embedding", "=", "embedding_matrix", "\n", "\n", "args", ".", "is_incremental", "=", "True", "\n", "model", "=", "new_model", "\n", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "global_step", ",", "tr_loss", "=", "train", "(", "args", ",", "train_dataset", ",", "model", ",", "tokenizer", ")", "\n", "logger", ".", "info", "(", "\" global_step = %s, average loss = %s\"", ",", "global_step", ",", "tr_loss", ")", "\n", "\n", "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()", "\n", "", "if", "args", ".", "do_train", ":", "\n", "# Create output directory if needed", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Saving model checkpoint to %s\"", ",", "args", ".", "output_dir", ")", "\n", "# Save a trained model, configuration and tokenizer using `save_pretrained()`.", "\n", "# They can then be reloaded using `from_pretrained()`", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\n", "'module'", ")", "else", "model", "# Take care of distributed/parallel training", "\n", "model_to_save", ".", "save_pretrained", "(", "args", ".", "output_dir", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "args", ".", "output_dir", ")", "\n", "\n", "# Good practice: save your training arguments together with the trained model", "\n", "torch", ".", "save", "(", "args", ",", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'training_args.bin'", ")", ")", "\n", "\n", "# Load a trained model and vocabulary that you have fine-tuned", "\n", "# model = model_class.from_pretrained(args.output_dir)", "\n", "# tokenizer = tokenizer_class.from_pretrained(args.output_dir)", "\n", "# model.to(args.device)", "\n", "\n", "# Evaluation", "\n", "", "results", "=", "{", "}", "\n", "if", "args", ".", "do_eval", ":", "\n", "        ", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "args", ".", "output_dir", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "checkpoints", "=", "[", "args", ".", "output_dir", "]", "\n", "if", "args", ".", "eval_all_checkpoints", ":", "\n", "            ", "checkpoints", "=", "list", "(", "\n", "os", ".", "path", ".", "dirname", "(", "c", ")", "for", "c", "in", "sorted", "(", "glob", ".", "glob", "(", "args", ".", "output_dir", "+", "'/**/'", "+", "WEIGHTS_NAME", ",", "recursive", "=", "True", ")", ")", ")", "\n", "logging", ".", "getLogger", "(", "\"transformers.modeling_utils\"", ")", ".", "setLevel", "(", "logging", ".", "WARN", ")", "# Reduce logging", "\n", "", "logger", ".", "info", "(", "\"Evaluate the following checkpoints: %s\"", ",", "checkpoints", ")", "\n", "for", "checkpoint", "in", "checkpoints", ":", "\n", "            ", "global_step", "=", "checkpoint", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", "if", "len", "(", "checkpoints", ")", ">", "1", "else", "\"\"", "\n", "prefix", "=", "checkpoint", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "if", "checkpoint", ".", "find", "(", "'checkpoint'", ")", "!=", "-", "1", "else", "\"\"", "\n", "\n", "if", "args", ".", "is_incremental", "or", "args", ".", "is_second_training", ":", "\n", "                ", "model", "=", "model_class", ".", "from_pretrained", "(", "\n", "checkpoint", ",", "\n", "num_embeddings", "=", "up_vocab", ".", "word_count", ",", "\n", "up_vocab", "=", "up_vocab", ",", "\n", "config", "=", "config", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "model", "=", "model_class", ".", "from_pretrained", "(", "checkpoint", ")", "\n", "", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "result", "=", "evaluate", "(", "args", ",", "model", ",", "dev_dataset", ",", "tokenizer", ",", "prefix", "=", "prefix", ")", "\n", "result", "=", "evaluate", "(", "args", ",", "model", ",", "test_dataset", ",", "tokenizer", ",", "prefix", "=", "prefix", ")", "\n", "result", "=", "dict", "(", "(", "k", "+", "'_{}'", ".", "format", "(", "global_step", ")", ",", "v", ")", "for", "k", ",", "v", "in", "result", ".", "items", "(", ")", ")", "\n", "results", ".", "update", "(", "result", ")", "\n", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.pargs.Arguments.__init__": [[7, 29], ["torch.device", "torch.cuda.device_count", "torch.cuda.is_available"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "name", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "max_seq_length", "=", "256", "\n", "self", ".", "output_mode", "=", "\"classification\"", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "self", ".", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "self", ".", "seed", "=", "1", "\n", "self", ".", "train_batch_size", "=", "32", "\n", "self", ".", "eval_batch_size", "=", "1", "\n", "self", ".", "num_train_epochs", "=", "1", "\n", "self", ".", "adam_epsilon", "=", "1e-8", "\n", "self", ".", "model_name_or_path", "=", "\"bert-base-uncased\"", "\n", "self", ".", "max_grad_norm", "=", "1.0", "\n", "self", ".", "logging_steps", "=", "400", "\n", "self", ".", "save_steps", "=", "200000", "\n", "self", ".", "eval_all_checkpoints", "=", "False", "\n", "self", ".", "do_lower_case", "=", "False", "\n", "self", ".", "is_incremental", "=", "False", "\n", "self", ".", "gradient_accumulation_steps", "=", "1", "\n", "self", ".", "task_name", "=", "'document-level-sa'", "\n", "self", ".", "label_list", "=", "[", "]", "\n", "self", ".", "amply", "=", "10", "", "", "", ""]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.Lang.__init__": [[80, 86], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "name", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "word2index", "=", "{", "'<unk>'", ":", "0", "}", "\n", "self", ".", "word2count", "=", "{", "}", "\n", "self", ".", "index2word", "=", "{", "}", "\n", "self", ".", "word_count", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.Lang.addWord": [[87, 95], ["None"], "methods", ["None"], ["", "def", "addWord", "(", "self", ",", "word", ")", ":", "\n", "        ", "if", "word", "not", "in", "self", ".", "word2index", ":", "\n", "            ", "self", ".", "word2index", "[", "word", "]", "=", "self", ".", "word_count", "\n", "self", ".", "index2word", "[", "self", ".", "word_count", "]", "=", "word", "\n", "self", ".", "word_count", "+=", "1", "\n", "self", ".", "word2count", "[", "word", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "self", ".", "word2count", "[", "word", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.Lang.addSentence": [[96, 99], ["sentence.split", "utils.Lang.addWord"], "methods", ["home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.Lang.addWord"], ["", "", "def", "addSentence", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "for", "word", "in", "sentence", ".", "split", "(", ")", ":", "\n", "            ", "self", ".", "addWord", "(", "word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.convert_examples_to_features": [[11, 77], ["enumerate", "print", "len", "tokenizer.encode", "features.append", "float", "enumerate", "logger.info", "len", "tokenizer.encode_plus", "len", "len", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "InputFeatures", "float", "KeyError", "len", "str", "str", "str"], "function", ["None"], ["def", "convert_examples_to_features", "(", "examples", ",", "tokenizer", ",", "max_length", "=", "512", ",", "label_list", "=", "None", ",", "output_mode", "=", "None", ",", "pad_on_left", "=", "False", ",", "pad_token", "=", "0", ",", "pad_token_segment_id", "=", "0", ",", "mask_padding_with_zero", "=", "True", ")", ":", "\n", "\n", "    ", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "\n", "features", "=", "[", "]", "\n", "over_len", "=", "0", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "len_examples", "=", "len", "(", "examples", ")", "\n", "if", "ex_index", "%", "10000", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Writing example %d/%d\"", "%", "(", "ex_index", ",", "len_examples", ")", ")", "\n", "\n", "", "encoded_text", "=", "tokenizer", ".", "encode", "(", "example", ".", "text_a", ")", "\n", "if", "len", "(", "encoded_text", ")", ">", "max_length", ":", "\n", "            ", "over_len", "+=", "1", "\n", "input_ids", "=", "encoded_text", "[", ":", "129", "]", "+", "encoded_text", "[", "-", "383", ":", "]", "\n", "token_type_ids", "=", "[", "0", "]", "*", "max_length", "\n", "", "else", ":", "\n", "            ", "inputs", "=", "tokenizer", ".", "encode_plus", "(", "example", ".", "text_a", ",", "example", ".", "text_b", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "max_length", ",", ")", "\n", "input_ids", ",", "token_type_ids", "=", "inputs", "[", "\"input_ids\"", "]", ",", "inputs", "[", "\"token_type_ids\"", "]", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "", "attention_mask", "=", "[", "1", "if", "mask_padding_with_zero", "else", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding_length", "=", "max_length", "-", "len", "(", "input_ids", ")", "\n", "if", "pad_on_left", ":", "\n", "            ", "input_ids", "=", "(", "[", "pad_token", "]", "*", "padding_length", ")", "+", "input_ids", "\n", "attention_mask", "=", "(", "[", "0", "if", "mask_padding_with_zero", "else", "1", "]", "*", "padding_length", ")", "+", "attention_mask", "\n", "token_type_ids", "=", "(", "[", "pad_token_segment_id", "]", "*", "padding_length", ")", "+", "token_type_ids", "\n", "", "else", ":", "\n", "            ", "input_ids", "=", "input_ids", "+", "(", "[", "pad_token", "]", "*", "padding_length", ")", "\n", "attention_mask", "=", "attention_mask", "+", "(", "[", "0", "if", "mask_padding_with_zero", "else", "1", "]", "*", "padding_length", ")", "\n", "token_type_ids", "=", "token_type_ids", "+", "(", "[", "pad_token_segment_id", "]", "*", "padding_length", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_length", ",", "\"Error with input length {} vs {}\"", ".", "format", "(", "len", "(", "input_ids", ")", ",", "max_length", ")", "\n", "assert", "len", "(", "attention_mask", ")", "==", "max_length", ",", "\"Error with input length {} vs {}\"", ".", "format", "(", "\n", "len", "(", "attention_mask", ")", ",", "max_length", "\n", ")", "\n", "assert", "len", "(", "token_type_ids", ")", "==", "max_length", ",", "\"Error with input length {} vs {}\"", ".", "format", "(", "\n", "len", "(", "token_type_ids", ")", ",", "max_length", "\n", ")", "\n", "\n", "if", "output_mode", "==", "\"classification\"", ":", "\n", "            ", "label", "=", "label_map", "[", "example", ".", "label", "]", "\n", "", "elif", "output_mode", "==", "\"regression\"", ":", "\n", "            ", "label", "=", "float", "(", "example", ".", "label", ")", "\n", "", "else", ":", "\n", "            ", "raise", "KeyError", "(", "output_mode", ")", "\n", "\n", "", "if", "ex_index", "<", "5", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"attention_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "attention_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"token_type_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "token_type_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "\n", "input_ids", "=", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "token_type_ids", "=", "token_type_ids", ",", "label", "=", "label", "\n", ")", "\n", ")", "\n", "\n", "", "print", "(", "float", "(", "over_len", "/", "len", "(", "examples", ")", ")", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.eval_to_file": [[101, 115], ["sklearn.accuracy_score", "sklearn.precision_score", "sklearn.recall_score", "sklearn.f1_score", "sklearn.mean_squared_error", "sklearn.mean_squared_error", "open", "json.dump", "str", "str", "str", "str", "str", "str"], "function", ["None"], ["", "", "", "def", "eval_to_file", "(", "out_file_name", ",", "preds", ",", "true_labels", ")", ":", "\n", "    ", "accuracy", "=", "metric", ".", "accuracy_score", "(", "true_labels", ",", "preds", ")", "\n", "precision", "=", "metric", ".", "precision_score", "(", "true_labels", ",", "preds", ",", "average", "=", "'macro'", ")", "\n", "recall", "=", "metric", ".", "recall_score", "(", "true_labels", ",", "preds", ",", "average", "=", "'macro'", ")", "\n", "f1", "=", "metric", ".", "f1_score", "(", "true_labels", ",", "preds", ",", "average", "=", "'macro'", ")", "\n", "MSE", "=", "metric", ".", "mean_squared_error", "(", "true_labels", ",", "preds", ")", "\n", "RMSE", "=", "metric", ".", "mean_squared_error", "(", "true_labels", ",", "preds", ",", "squared", "=", "False", ")", "\n", "\n", "model_statistics", "=", "{", "}", "\n", "with", "open", "(", "out_file_name", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "line", "=", "\"accuracy: \"", "+", "str", "(", "accuracy", ")", "+", "\"\\n\"", "+", "\"precision: \"", "+", "str", "(", "precision", ")", "+", "\"\\n\"", "+", "\"recall: \"", "+", "str", "(", "recall", ")", "+", "\"\\n\"", "+", "\"F1: \"", "+", "str", "(", "f1", ")", "+", "\"\\n\"", "+", "\"MSE: \"", "+", "str", "(", "MSE", ")", "+", "\"\\n\"", "+", "\"RMSE: \"", "+", "str", "(", "RMSE", ")", "+", "\"\\n\"", "\n", "model_statistics", "[", "'statistics'", "]", "=", "line", "\n", "\n", "json", ".", "dump", "(", "model_statistics", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.remove_chars": [[117, 121], ["text.replace.replace"], "function", ["None"], ["", "", "def", "remove_chars", "(", "text", ",", "target", ")", ":", "\n", "    ", "for", "t", "in", "target", ":", "\n", "        ", "text", "=", "text", ".", "replace", "(", "t", ",", "\"\"", ")", "\n", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.load_data": [[123, 179], ["utils.Lang", "tqdm.tqdm", "datasets.append", "os.path.isfile", "torch.utils.data.TensorDataset", "datasets.append", "pickle.load", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "pickle.dump", "open", "open", "f.readlines", "tqdm.tqdm", "enumerate", "utils.convert_examples_to_features", "torch.tensor", "torch.tensor", "open", "line.split", "utils.remove_chars", "utils.Lang.addWord", "utils.Lang.addWord", "user_ids.append", "product_ids.append", "examples.append", "transformers.InputExample", "tokenizer.convert_tokens_to_ids"], "function", ["home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.convert_examples_to_features", "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.remove_chars", "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.Lang.addWord", "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.Lang.addWord"], ["", "def", "load_data", "(", "args", ",", "data_dirs", ",", "tokenizer", ")", ":", "\n", "    ", "up_vocab", "=", "Lang", "(", "'user_product'", ")", "\n", "datasets", "=", "[", "]", "\n", "target", "=", "[", "'<sssss>'", "]", "\n", "for", "dir", "in", "tqdm", "(", "data_dirs", ",", "desc", "=", "\"Loading dataset\"", ")", ":", "\n", "        ", "cache_dir", "=", "dir", "+", "\".cache_\"", "+", "args", ".", "model_type", "\n", "if", "os", ".", "path", ".", "isfile", "(", "cache_dir", ")", ":", "\n", "            ", "all_input_ids", ",", "all_attention_mask", ",", "all_token_type_ids", ",", "all_labels", ",", "all_user_ids", ",", "all_product_ids", ",", "up_vocab", "=", "pickle", ".", "load", "(", "open", "(", "cache_dir", ",", "'rb'", ")", ")", "\n", "", "else", ":", "\n", "            ", "examples", "=", "[", "]", "\n", "user_ids", ",", "product_ids", "=", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "dir", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "lines", "=", "tqdm", "(", "lines", ",", "desc", "=", "\"lines\"", ")", "\n", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "lines", ")", ":", "\n", "                    ", "guid", "=", "\"%s-%s\"", "%", "(", "\"document-sa\"", ",", "i", ")", "\n", "a", ",", "b", ",", "c", ",", "d", "=", "line", ".", "split", "(", "'\\t\\t'", ")", "\n", "text_a", "=", "remove_chars", "(", "d", ",", "target", ")", "\n", "\n", "label", "=", "c", "\n", "up_vocab", ".", "addWord", "(", "a", ")", "\n", "up_vocab", ".", "addWord", "(", "b", ")", "\n", "\n", "user_ids", ".", "append", "(", "up_vocab", ".", "word2index", "[", "a", "]", ")", "\n", "product_ids", ".", "append", "(", "up_vocab", ".", "word2index", "[", "b", "]", ")", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", "\n", ")", "\n", "", "features", "=", "convert_examples_to_features", "(", "\n", "examples", ",", "\n", "tokenizer", ",", "\n", "label_list", "=", "args", ".", "label_list", ",", "\n", "max_length", "=", "args", ".", "max_seq_length", ",", "\n", "output_mode", "=", "args", ".", "output_mode", ",", "\n", "pad_on_left", "=", "False", ",", "\n", "pad_token", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "tokenizer", ".", "pad_token", "]", ")", "[", "0", "]", ",", "\n", "pad_token_segment_id", "=", "0", ",", "\n", ")", "\n", "", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_attention_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "attention_mask", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_token_type_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "token_type_ids", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "args", ".", "output_mode", "==", "\"classification\"", ":", "\n", "                ", "all_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "if", "args", ".", "output_mode", "==", "\"regression\"", ":", "\n", "                ", "all_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label", "for", "f", "in", "features", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "", "all_user_ids", "=", "torch", ".", "tensor", "(", "user_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_product_ids", "=", "torch", ".", "tensor", "(", "product_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "pickle", ".", "dump", "(", "[", "all_input_ids", ",", "all_attention_mask", ",", "all_token_type_ids", ",", "all_labels", ",", "all_user_ids", ",", "all_product_ids", ",", "up_vocab", "]", ",", "open", "(", "cache_dir", ",", "\"wb\"", ")", ",", "protocol", "=", "4", ")", "\n", "\n", "", "dataset", "=", "TensorDataset", "(", "all_input_ids", ",", "all_attention_mask", ",", "all_token_type_ids", ",", "all_labels", ",", "all_user_ids", ",", "all_product_ids", ")", "\n", "\n", "datasets", ".", "append", "(", "dataset", ")", "\n", "", "datasets", ".", "append", "(", "up_vocab", ")", "\n", "return", "datasets", "\n", "\n"]], "home.repos.pwc.inspect_result.lyuchenyang_Document-level-Sentiment-Analysis-with-User-and-Product-Context.None.utils.get_label_distribution": [[181, 197], ["open", "f.readlines", "sum", "line.split", "int", "range", "len"], "function", ["None"], ["", "def", "get_label_distribution", "(", "datadir", ",", "reverse", "=", "True", ")", ":", "\n", "    ", "sta", "=", "{", "}", "\n", "with", "open", "(", "datadir", ",", "\" r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "a", ",", "b", ",", "c", ",", "d", "=", "line", ".", "split", "(", "'\\t\\t'", ")", "\n", "label", "=", "int", "(", "c", ")", "\n", "if", "label", "not", "in", "sta", ":", "\n", "                ", "sta", "[", "label", "]", "=", "0", "\n", "", "sta", "[", "label", "]", "+=", "1", "\n", "", "total", "=", "sum", "(", "sta", "[", "k", "]", "for", "k", "in", "sta", ")", "\n", "distri", "=", "[", "sta", "[", "e", "+", "1", "]", "/", "total", "for", "e", "in", "range", "(", "len", "(", "sta", ")", ")", "]", "\n", "if", "reverse", ":", "\n", "            ", "distri", "=", "[", "1", "/", "e", "for", "e", "in", "distri", "]", "\n", "", "return", "distri", "\n", "", "", ""]]}