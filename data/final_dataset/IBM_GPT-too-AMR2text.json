{"home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.models.GPT2ConditionalLMHeadModel.__init__": [[6, 8], ["pytorch_transformers.GPT2LMHeadModel.__init__"], "methods", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "GPT2ConditionalLMHeadModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.models.GPT2ConditionalLMHeadModel.forward": [[9, 31], ["models.GPT2ConditionalLMHeadModel.transformer", "models.GPT2ConditionalLMHeadModel.lm_head", "lm_logits[].contiguous", "labels[].contiguous", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "lm_logits[].contiguous.view", "labels[].contiguous.view", "lm_logits[].contiguous.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "position_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "\n", "labels", "=", "None", ",", "past", "=", "None", ",", "head_mask", "=", "None", ",", "reduction", "=", "'mean'", ")", ":", "\n", "        ", "transformer_outputs", "=", "self", ".", "transformer", "(", "\n", "input_ids", ",", "position_ids", "=", "position_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "past", "=", "past", ",", "head_mask", "=", "head_mask", ")", "\n", "hidden_states", "=", "transformer_outputs", "[", "0", "]", "\n", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "hidden_states", ")", "\n", "\n", "outputs", "=", "(", "lm_logits", ",", ")", "+", "transformer_outputs", "[", "1", ":", "]", "\n", "if", "labels", "is", "not", "None", ":", "\n", "# Shift so that tokens < n predict n", "\n", "            ", "shift_logits", "=", "lm_logits", "[", "...", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "shift_labels", "=", "labels", "[", "...", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", "# Flatten the tokens", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ",", "reduction", "=", "reduction", ")", "\n", "loss", "=", "loss_fct", "(", "shift_logits", ".", "view", "(", "-", "1", ",", "shift_logits", ".", "size", "(", "-", "1", ")", ")", ",", "\n", "shift_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "# (loss), lm_logits, presents, (all hidden_states), (attentions)", "\n", "", "return", "outputs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.sampling_batch.main": [[26, 239], ["logger.info", "pytorch_transformers.GPT2Tokenizer.from_pretrained", "GPT2Tokenizer.from_pretrained.add_tokens", "torch.nn.DataParallel.resize_token_embeddings", "torch.nn.DataParallel.to", "torch.nn.DataParallel.eval", "torch.LogSoftmax", "GPT2Tokenizer.from_pretrained.convert_tokens_to_ids", "logger.info", "logger.info", "open", "list", "tqdm.tqdm", "pytorch_transformers.GPT2LMHeadModel.from_pretrained", "models.GPT2ConditionalLMHeadModel.from_pretrained", "len", "logger.info", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "logger.info", "utils.get_data_loaders", "utils.get_data_loaders", "open().readlines", "os.path.join", "utils.trim_batch", "tuple", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "range", "enumerate", "args.output_data.lower", "os.path.join", "os.path.join", "torch.cat.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat.append", "tokenizer.decode.tolist", "GPT2Tokenizer.from_pretrained.decode", "print", "open.flush", "list.append", "print", "print", "open", "input_tensor.to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.argmax", "torch.argmax", "torch.argmax", "torch.where.unsqueeze().unsqueeze", "torch.cat.append", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "tokenizer.decode.index", "tokenizer.decode.replace", "tokenizer.decode.replace().lower", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.nn.DataParallel.", "top_k_top_p.top_k_top_p_filtering", "torch.softmax", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.where", "torch.where", "torch.where", "tokenizer.decode.split", "torch.where.unsqueeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.nn.DataParallel.", "range", "torch.topk", "torch.topk", "torch.topk", "Exception", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "tokenizer.decode.replace", "torch.no_grad", "torch.no_grad", "torch.no_grad", "nn.LogSoftmax.", "torch.nn.DataParallel.", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "input_ids[].expand", "torch.cat", "torch.cat", "torch.cat", "input_ids[].expand"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_data_loaders", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_data_loaders", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.trim_batch", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.zeros", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.zeros", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.zeros"], ["def", "main", "(", "args", ")", ":", "\n", "\n", "    ", "debug", "=", "False", "\n", "# Load a pre-defined tokenizer (GPT-2), create config and model", "\n", "logger", ".", "info", "(", "\"Prepare tokenizer, pretrained model and optimizer - add \\\n                special tokens for fine-tuning\"", ")", "\n", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "args", ".", "model_path", ",", "\n", "cache_dir", "=", "args", ".", "dataset_cache", ")", "\n", "tokenizer", ".", "add_tokens", "(", "SPECIAL_TOKENS", ")", "\n", "tokenizer", ".", "sep_token", "=", "'<sep>'", "\n", "\n", "if", "'amr'", "in", "args", ".", "dataset_type", ":", "\n", "        ", "qgen", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "args", ".", "model_path", ",", "\n", "cache_dir", "=", "args", ".", "dataset_cache", ")", "\n", "", "else", ":", "\n", "        ", "qgen", "=", "GPT2ConditionalLMHeadModel", ".", "from_pretrained", "(", "args", ".", "model_path", ",", "cache_dir", "=", "args", ".", "dataset_cache", ")", "\n", "", "qgen", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "qgen", ".", "to", "(", "args", ".", "device", ")", "\n", "qgen", ".", "eval", "(", ")", "\n", "\n", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "0", ")", "\n", "\n", "bos", ",", "eos", ",", "ctx", ",", "ans", ",", "que", ",", "pad", ",", "gen", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "SPECIAL_TOKENS", ")", "\n", "\n", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "        ", "logger", ".", "info", "(", "\"Setting model to DataParallel.\"", ")", "\n", "qgen", "=", "torch", ".", "nn", ".", "DataParallel", "(", "qgen", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Prepare datasets\"", ")", "\n", "if", "\"amr\"", "in", "args", ".", "dataset_type", ":", "\n", "        ", "logger", ".", "info", "(", "\"Decoding with AMR dev set\"", ")", "\n", "dataloader", "=", "get_data_loaders", "(", "args", ",", "tokenizer", ",", "qgen", ",", "\n", "dataset_name", "=", "args", ".", "output_data", ",", "\n", "shuffle", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "dataloader", "=", "get_data_loaders", "(", "args", ",", "tokenizer", ",", "qgen", ",", "shuffle", "=", "False", ")", "\n", "\n", "", "if", "'amr'", "in", "args", ".", "dataset_type", ":", "\n", "        ", "if", "args", ".", "output_data", ".", "lower", "(", ")", "==", "\"test\"", ":", "\n", "            ", "ref", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dataset_path", ",", "\"test.tok.text\"", ")", "\n", "", "else", ":", "\n", "            ", "ref", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dataset_path", ",", "\"dev.tok.text\"", ")", "\n", "", "ref", "=", "open", "(", "ref", ")", ".", "readlines", "(", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Decode: \"", "+", "args", ".", "decoder", ")", "\n", "\n", "# Output file name", "\n", "f", "=", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoint", ",", "'output.txt'", ")", ",", "'w'", ")", "\n", "text_outputs", "=", "list", "(", ")", "\n", "\n", "# beam search variables", "\n", "beam_size", "=", "1", "if", "args", ".", "beam_size", "is", "None", "else", "args", ".", "beam_size", "\n", "output_size", "=", "1", "if", "args", ".", "output_size", "is", "None", "else", "args", ".", "output_size", "\n", "beam_candidates", "=", "args", ".", "beam_candidates", "\n", "\n", "# General variables", "\n", "max_length", "=", "args", ".", "max_input_length", "\n", "\n", "instance", "=", "0", "\n", "for", "batch", "in", "tqdm", "(", "dataloader", ")", ":", "\n", "\n", "        ", "batch", "=", "trim_batch", "(", "batch", ",", "pad", ")", "\n", "_", ",", "_", ",", "_", ",", "_", ",", "input_ids", ",", "_", ",", "token_type_ids", ",", "attention_mask", "=", "tuple", "(", "input_tensor", ".", "to", "(", "args", ".", "device", ")", "for", "input_tensor", "in", "batch", ")", "\n", "\n", "past", "=", "None", "\n", "\n", "o", "=", "0", "\n", "all_probs", "=", "torch", ".", "zeros", "(", "beam_size", ",", "1", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "original_input_len", "=", "input_ids", ".", "shape", "[", "1", "]", "\n", "start", "=", "True", "\n", "\n", "# general variables", "\n", "questions", "=", "[", "]", "\n", "\n", "for", "idx", "in", "range", "(", "max_length", ")", ":", "\n", "###################", "\n", "# Greedy decoding", "\n", "###################", "\n", "            ", "if", "args", ".", "decoder", "==", "\"greedy\"", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "logits", ",", "past", "=", "qgen", "(", "input_ids", "=", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "past", "=", "past", ")", "\n", "", "outputs", "=", "torch", ".", "argmax", "(", "logits", "[", "0", ",", "-", "1", ",", ":", "]", ")", "\n", "outputs", "=", "outputs", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "###################", "\n", "# Nucleous Sampling", "\n", "###################", "\n", "", "elif", "args", ".", "decoder", "==", "\"sampling\"", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "logits", ",", "past", "=", "qgen", "(", "input_ids", "=", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "past", "=", "past", ")", "\n", "# bs x seq_len x V", "\n", "", "logits", "=", "logits", "[", ":", ",", "-", "1", ",", ":", "]", "/", "args", ".", "temperature", "\n", "logits", "=", "top_k_top_p_filtering", "(", "logits", ",", "top_k", "=", "args", ".", "top_k", ",", "\n", "top_p", "=", "args", ".", "top_p", ")", "\n", "# bs x V", "\n", "probs", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "# bs x 1", "\n", "outputs", "=", "torch", ".", "multinomial", "(", "probs", ",", "num_samples", "=", "1", ")", "\n", "outputs", "=", "torch", ".", "where", "(", "input_ids", "[", ":", ",", "-", "1", ":", "]", "==", "\n", "eos", ",", "input_ids", "[", ":", ",", "-", "1", ":", "]", ",", "outputs", ")", "\n", "\n", "###################", "\n", "# BEAM Search", "\n", "###################", "\n", "", "elif", "args", ".", "decoder", "==", "\"beam\"", ":", "\n", "# Beam search", "\n", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "logits", "=", "qgen", "(", "input_ids", ")", "[", "0", "]", "\n", "\n", "", "out_paths", "=", "None", "\n", "probs", "=", "None", "\n", "\n", "for", "k", "in", "range", "(", "logits", ".", "shape", "[", "0", "]", ")", ":", "\n", "                    ", "log_p", "=", "logsoftmax", "(", "logits", "[", "k", ",", "-", "1", ",", ":", "]", ")", "\n", "p", "=", "log_p", "+", "all_probs", "[", "k", "]", "\n", "\n", "if", "start", ":", "\n", "                        ", "predicted_top_k", "=", "torch", ".", "topk", "(", "p", ",", "beam_size", ")", "\n", "start", "=", "False", "\n", "", "else", ":", "\n", "                        ", "predicted_top_k", "=", "torch", ".", "topk", "(", "p", ",", "beam_candidates", ")", "\n", "\n", "", "p_top_k_tokens", "=", "predicted_top_k", ".", "indices", "[", ":", ",", "None", "]", "\n", "p_top_k_probs", "=", "predicted_top_k", ".", "values", "[", ":", ",", "None", "]", "\n", "\n", "# Store paths", "\n", "if", "out_paths", "is", "None", ":", "\n", "                        ", "out_paths", "=", "torch", ".", "cat", "(", "(", "input_ids", "[", "k", "]", ".", "expand", "(", "\n", "p_top_k_tokens", ".", "shape", "[", "0", "]", ",", "\n", "input_ids", ".", "shape", "[", "1", "]", ")", ",", "p_top_k_tokens", ")", ",", "1", ")", "\n", "\n", "", "else", ":", "\n", "                        ", "out_paths", "=", "torch", ".", "cat", "(", "(", "out_paths", ",", "torch", ".", "cat", "(", "(", "\n", "input_ids", "[", "k", "]", ".", "expand", "(", "p_top_k_tokens", ".", "shape", "[", "0", "]", ",", "\n", "input_ids", ".", "shape", "[", "1", "]", ")", ",", "\n", "p_top_k_tokens", ")", ",", "1", ")", ")", ",", "0", ")", "\n", "", "if", "probs", "is", "None", ":", "\n", "                        ", "probs", "=", "p_top_k_probs", "\n", "", "else", ":", "\n", "                        ", "probs", "=", "torch", ".", "cat", "(", "(", "probs", ",", "p_top_k_probs", ")", ",", "0", ")", "\n", "\n", "", "", "global_top_k", "=", "torch", ".", "topk", "(", "probs", ",", "k", "=", "beam_size", ",", "dim", "=", "0", ")", "\n", "input_ids", "=", "out_paths", "[", "global_top_k", ".", "indices", "[", ":", ",", "0", "]", ",", ":", "]", "\n", "all_probs", "=", "global_top_k", ".", "values", "\n", "o", "+=", "1", "\n", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "'Not valid decoder '", "+", "args", ".", "decoder", ")", "\n", "\n", "#######################", "\n", "# Termination condition", "\n", "#######################", "\n", "", "if", "not", "args", ".", "decoder", "==", "'beam'", ":", "\n", "# correctly shape inputs for next round", "\n", "                ", "input_ids", "=", "outputs", "\n", "token_type_ids", "=", "token_type_ids", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "# if all the outputs are special tokens", "\n", "questions", ".", "append", "(", "outputs", ")", "\n", "if", "(", "outputs", "==", "eos", ")", ".", "all", "(", ")", ":", "\n", "                    ", "break", "\n", "", "", "else", ":", "\n", "                ", "outputs", "=", "input_ids", "[", ":", ",", "original_input_len", ":", "]", "\n", "\n", "if", "(", "(", "outputs", "==", "eos", ")", ".", "sum", "(", "dim", "=", "1", ")", ">", "0", ")", ".", "all", "(", ")", ":", "\n", "                    ", "break", "\n", "\n", "################", "\n", "# Output to file", "\n", "################", "\n", "", "", "", "if", "args", ".", "decoder", "!=", "'beam'", ":", "\n", "# append an extra <eos> in case max length is reached", "\n", "            ", "questions", ".", "append", "(", "torch", ".", "zeros_like", "(", "outputs", ")", ".", "fill_", "(", "eos", ")", ")", "\n", "questions", "=", "torch", ".", "cat", "(", "questions", ",", "dim", "=", "1", ")", "\n", "\n", "", "else", ":", "\n", "            ", "questions", ".", "append", "(", "outputs", ")", "\n", "questions", "=", "questions", "[", "0", "]", "\n", "\n", "", "for", "i", ",", "question", "in", "enumerate", "(", "questions", ")", ":", "\n", "            ", "question", "=", "question", ".", "tolist", "(", ")", "\n", "\n", "if", "eos", "in", "question", ":", "\n", "                ", "idx", "=", "question", ".", "index", "(", "eos", ")", "\n", "", "else", ":", "\n", "                ", "idx", "=", "-", "1", "\n", "\n", "", "question", "=", "tokenizer", ".", "decode", "(", "question", "[", ":", "idx", "]", ")", "\n", "if", "'<generate>'", "in", "question", ":", "\n", "                ", "question", "=", "question", ".", "split", "(", "'<generate>'", ")", "[", "1", "]", "\n", "\n", "# Print outputs to file and save in text_outputs", "\n", "", "print", "(", "question", ".", "replace", "(", "'\\n'", ",", "' '", ")", ",", "file", "=", "f", ")", "\n", "f", ".", "flush", "(", ")", "\n", "text_outputs", ".", "append", "(", "question", ".", "replace", "(", "'\\n'", ",", "' '", ")", ".", "lower", "(", ")", ")", "\n", "\n", "# Limit number of outputs to output_size", "\n", "if", "i", ">=", "output_size", "-", "1", ":", "\n", "                ", "break", "\n", "\n", "", "", "if", "'amr'", "in", "args", ".", "dataset_type", "and", "debug", ":", "\n", "            ", "print", "(", "\"GOLD: \"", ",", "ref", "[", "instance", "]", ")", "\n", "print", "(", "\"final: \"", ",", "text_outputs", "[", "instance", "]", ")", "\n", "\n", "", "instance", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.sampling_batch.run": [[241, 292], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "vars().items", "os.path.join", "os.makedirs", "shutil.copyfile", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "logging.basicConfig", "logger.info", "logger.info", "random.seed", "torch.random.manual_seed", "torch.random.manual_seed", "torch.random.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "sampling_batch.main", "open", "yaml.safe_load", "utils.dotdict", "os.path.join", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "pprint.pformat", "vars"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.train.main"], ["", "", "def", "run", "(", ")", ":", "\n", "    ", "parser", "=", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"-c\"", ",", "\"--config_path\"", ",", "default", "=", "'config/config.yaml'", ",", "\n", "help", "=", "\"The default config file.\"", ")", "\n", "# obligatory arguments", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dataset_path\"", ",", "\n", "help", "=", "\"Input data folder\"", ",", "\n", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dataset_cache\"", ",", "\n", "help", "=", "\"Cache for input data folder\"", ",", "\n", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-mq\"", ",", "\"--model_path\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "'Pretrained model path to local checkpoint'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-e\"", ",", "\"--exp_name\"", ",", "type", "=", "str", ",", "default", "=", "'qgen'", ",", "\n", "help", "=", "'The name of experiment'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# Read config from yaml file.", "\n", "config_file", "=", "args", ".", "config_path", "\n", "with", "open", "(", "config_file", ")", "as", "reader", ":", "\n", "        ", "config", "=", "yaml", ".", "safe_load", "(", "reader", ")", "\n", "config", "=", "dotdict", "(", "config", ")", "\n", "\n", "# overload with command line arguments", "\n", "", "for", "k", ",", "v", "in", "vars", "(", "args", ")", ".", "items", "(", ")", ":", "\n", "        ", "config", "[", "k", "]", "=", "v", "\n", "\n", "", "config", ".", "checkpoint", "=", "os", ".", "path", ".", "join", "(", "config", ".", "model_path", ",", "\n", "\"sampling\"", ",", "config", ".", "exp_name", ")", "\n", "os", ".", "makedirs", "(", "config", ".", "checkpoint", ",", "exist_ok", "=", "True", ")", "\n", "copyfile", "(", "config", ".", "config_path", ",", "os", ".", "path", ".", "join", "(", "config", ".", "checkpoint", ",", "\n", "\"config.yaml\"", ")", ")", "\n", "\n", "config", ".", "device", "=", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", "\n", "config", ".", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "config", ".", "n_gpu", "=", "1", "\n", "\n", "# logging is set to INFO", "\n", "logging", ".", "basicConfig", "(", "level", "=", "logging", ".", "INFO", ")", "\n", "logger", ".", "info", "(", "\"Arguments: %s\"", ",", "pformat", "(", "config", ")", ")", "\n", "logger", ".", "info", "(", "\"device: {}, n_gpu {}\"", ".", "format", "(", "config", ".", "device", ",", "config", ".", "n_gpu", ")", ")", "\n", "\n", "random", ".", "seed", "(", "config", ".", "seed", ")", "\n", "torch", ".", "random", ".", "manual_seed", "(", "config", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "config", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "config", ".", "seed", ")", "\n", "main", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.train.main": [[29, 137], ["logger.info", "pytorch_transformers.GPT2Tokenizer.from_pretrained", "GPT2Tokenizer.from_pretrained.add_tokens", "GPT2Tokenizer.from_pretrained.add_tokens", "logger.info", "torch.nn.DataParallel.resize_token_embeddings", "logger.info", "torch.nn.DataParallel.to", "logger.info", "pytorch_transformers.AdamW", "GPT2Tokenizer.from_pretrained.convert_tokens_to_ids", "logger.info", "utils.get_data_loaders", "ignite.engine.Engine", "ignite.metrics.RunningAverage().attach", "ignite.contrib.handlers.ProgressBar().attach", "ignite.contrib.handlers.PiecewiseLinear", "ignite.engine.Engine.add_event_handler", "ignite.handlers.ModelCheckpoint", "ignite.engine.Engine.add_event_handler", "torch.save", "getattr().config.to_json_file", "GPT2Tokenizer.from_pretrained.save_vocabulary", "ignite.engine.Engine.run", "pytorch_transformers.GPT2LMHeadModel.from_pretrained", "models.GPT2ConditionalLMHeadModel.from_pretrained", "len", "torch.nn.DataParallel.parameters", "logger.info", "torch.nn.DataParallel", "utils.trim_batch", "torch.nn.DataParallel.train", "torch.tensor", "utils.apply_loss.item", "dict", "os.path.join", "os.path.join", "tuple", "utils.apply_loss", "ignite.metrics.RunningAverage", "ignite.contrib.handlers.ProgressBar", "getattr", "torch.nn.DataParallel.", "getattr", "input_tensor.to", "len"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_data_loaders", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.sampling_batch.run", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.trim_batch", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.apply_loss"], ["def", "main", "(", "args", ")", ":", "\n", "# Load a pre-defined tokenizer (GPT-2), create config and model", "\n", "    ", "logger", ".", "info", "(", "\"Prepare tokenizer, pretrained model and optimizer - \\\n                add special tokens for fine-tuning\"", ")", "\n", "\n", "gpt_tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "qgen_model_path", ",", "cache_dir", "=", "args", ".", "dataset_cache", ")", "\n", "gpt_tokenizer", ".", "sep_token", "=", "'<sep>'", "\n", "\n", "gpt_tokenizer", ".", "add_tokens", "(", "SPECIAL_TOKENS", ")", "\n", "gpt_tokenizer", ".", "add_tokens", "(", "AMR_SPECIAL_TOKENS", ")", "\n", "if", "'amr'", "in", "args", ".", "dataset_type", ":", "\n", "        ", "qgen", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "\n", "args", ".", "qgen_model_path", ",", "cache_dir", "=", "args", ".", "dataset_cache", ")", "\n", "", "else", ":", "\n", "        ", "qgen", "=", "GPT2ConditionalLMHeadModel", ".", "from_pretrained", "(", "\n", "args", ".", "qgen_model_path", ",", "cache_dir", "=", "args", ".", "dataset_cache", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Adjust model size to new tokens\"", ")", "\n", "qgen", ".", "resize_token_embeddings", "(", "len", "(", "gpt_tokenizer", ")", ")", "\n", "logger", ".", "info", "(", "\"Set model to GPU usage\"", ")", "\n", "qgen", ".", "to", "(", "args", ".", "device", ")", "\n", "logger", ".", "info", "(", "\"Set up optimizer\"", ")", "\n", "qgen_optimizer", "=", "AdamW", "(", "\n", "qgen", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "eps", "=", "args", ".", "adam_epsilon", ")", "\n", "\n", "bos", ",", "eos", ",", "ctx", ",", "ans", ",", "que", ",", "pad", ",", "gen", "=", "gpt_tokenizer", ".", "convert_tokens_to_ids", "(", "SPECIAL_TOKENS", ")", "\n", "\n", "# if args.n_gpu > 1:", "\n", "if", "False", ":", "\n", "        ", "logger", ".", "info", "(", "\"More then 1 GPU for training\"", ")", "\n", "qgen", "=", "torch", ".", "nn", ".", "DataParallel", "(", "qgen", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Prepare datasets\"", ")", "\n", "if", "args", ".", "use_silver_data", ":", "\n", "        ", "data_type", "=", "'Silver'", "\n", "", "else", ":", "\n", "        ", "data_type", "=", "'Train'", "\n", "\n", "", "dataloader", "=", "get_data_loaders", "(", "\n", "args", ",", "gpt_tokenizer", ",", "qgen", ",", "dataset_name", "=", "data_type", ")", "\n", "\n", "# Define training function", "\n", "def", "update", "(", "engine", ",", "batch", ")", ":", "\n", "\n", "# remove extra pad from batches", "\n", "        ", "batch", "=", "trim_batch", "(", "batch", ",", "pad", ")", "\n", "\n", "qgen", ".", "train", "(", ")", "\n", "\n", "loss", "=", "torch", ".", "tensor", "(", "[", "0.0", "]", ")", "\n", "###################################", "\n", "# MLE training with teacher forcing", "\n", "###################################", "\n", "if", "'sl'", "in", "args", ".", "learning", ":", "\n", "            ", "input_ids", ",", "lm_labels", ",", "token_type_ids", ",", "attention_mask", ",", "_", ",", "_", ",", "_", ",", "_", "=", "tuple", "(", "input_tensor", ".", "to", "(", "args", ".", "device", ")", "for", "input_tensor", "in", "batch", ")", "\n", "loss_ce", "=", "qgen", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "labels", "=", "lm_labels", ",", "\n", "token_type_ids", "=", "token_type_ids", ")", "[", "0", "]", "\n", "loss", "=", "apply_loss", "(", "\n", "engine", ".", "state", ".", "iteration", ",", "\n", "qgen_optimizer", ",", "\n", "loss_ce", ",", "\n", "args", ")", "\n", "", "return", "loss", ".", "item", "(", ")", "\n", "", "trainer", "=", "Engine", "(", "update", ")", "\n", "\n", "# Add progressbar with loss", "\n", "RunningAverage", "(", "output_transform", "=", "lambda", "x", ":", "x", ")", ".", "attach", "(", "trainer", ",", "\"loss\"", ")", "\n", "ProgressBar", "(", "persist", "=", "True", ")", ".", "attach", "(", "trainer", ",", "metric_names", "=", "[", "'loss'", "]", ")", "\n", "\n", "# Linearly decrease the learning rate from lr to zero", "\n", "scheduler", "=", "PiecewiseLinear", "(", "\n", "qgen_optimizer", ",", "\"lr\"", ",", "[", "\n", "(", "0", ",", "args", ".", "learning_rate", ")", ",", "(", "args", ".", "n_epochs", "*", "len", "(", "dataloader", ")", ",", "0.0", ")", "]", ")", "\n", "trainer", ".", "add_event_handler", "(", "Events", ".", "ITERATION_STARTED", ",", "scheduler", ")", "\n", "\n", "# Save checkpoints", "\n", "checkpoint_handler", "=", "ModelCheckpoint", "(", "\n", "args", ".", "checkpoint", ",", "\n", "'checkpoint'", ",", "\n", "save_interval", "=", "1", ",", "\n", "n_saved", "=", "20", ",", "\n", "require_empty", "=", "False", ")", "\n", "\n", "# \"getattr\" take care of distributed encapsulation", "\n", "trainer", ".", "add_event_handler", "(", "\n", "Events", ".", "EPOCH_COMPLETED", ",", "checkpoint_handler", ",", "{", "\n", "'mymodel'", ":", "getattr", "(", "\n", "qgen", ",", "'module'", ",", "qgen", ")", "}", ")", "\n", "\n", "# save training config", "\n", "torch", ".", "save", "(", "dict", "(", "args", ")", ",", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoint", ",", "'training_args.bin'", ")", ")", "\n", "getattr", "(", "\n", "qgen", ",", "\n", "'module'", ",", "\n", "qgen", ")", ".", "config", ".", "to_json_file", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "checkpoint", ",", "\n", "CONFIG_NAME", ")", ")", "\n", "gpt_tokenizer", ".", "save_vocabulary", "(", "args", ".", "checkpoint", ")", "\n", "\n", "trainer", ".", "run", "(", "dataloader", ",", "max_epochs", "=", "args", ".", "n_epochs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.edit_config.argument_parsing": [[6, 23], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parsing", "(", ")", ":", "\n", "\n", "# Argument hanlding", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Get config values'", "\n", ")", "\n", "# jbinfo args", "\n", "parser", ".", "add_argument", "(", "\n", "'-i'", ",", "'--in-config'", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "'Path fo config'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-g'", ",", "'--get'", ",", "\n", "help", "=", "'Get value of field'", "\n", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.tokenize_and_encode": [[17, 26], ["list", "tqdm.tqdm", "tokenizer.convert_tokens_to_ids", "tokenizer.convert_tokens_to_ids", "list.append", "tokenizer.tokenize", "tokenizer.tokenize"], "function", ["None"], ["def", "tokenize_and_encode", "(", "dataset", ",", "tokenizer", ")", ":", "\n", "    ", "encoded_dataset", "=", "list", "(", ")", "\n", "for", "data_inst", "in", "tqdm", "(", "dataset", ")", ":", "\n", "        ", "tok_amr", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "\n", "tokenizer", ".", "tokenize", "(", "data_inst", "[", "0", "]", ")", ")", "\n", "tok_txt", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "\n", "tokenizer", ".", "tokenize", "(", "data_inst", "[", "1", "]", ")", ")", "\n", "encoded_dataset", ".", "append", "(", "(", "tok_amr", ",", "tok_txt", ")", ")", "\n", "", "return", "encoded_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.split_list": [[30, 39], ["list", "L.index", "list.append"], "function", ["None"], ["", "def", "split_list", "(", "L", ",", "S", ")", ":", "\n", "    ", "\"\"\"Split list given a list of breaking elements\"\"\"", "\n", "output", "=", "list", "(", ")", "\n", "for", "s", "in", "S", ":", "\n", "        ", "if", "s", "in", "L", ":", "\n", "            ", "idx", "=", "L", ".", "index", "(", "s", ")", "\n", "output", ".", "append", "(", "L", "[", ":", "idx", "]", ")", "\n", "L", "=", "L", "[", "idx", "+", "1", ":", "]", "\n", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.pre_process_amr_leftpad": [[41, 80], ["tokenizer.convert_tokens_to_ids", "int", "list", "len", "list", "len", "len", "itertools.chain", "itertools.chain", "len", "len", "list", "len", "len", "len", "len", "itertools.chain", "len", "len"], "function", ["None"], ["", "def", "pre_process_amr_leftpad", "(", "\n", "amr_graph", ",", "text", ",", "tokenizer", ",", "max_input_length", ",", "cap_length", ",", "\n", "with_text", "=", "True", ",", "with_masking", "=", "True", ",", "split_sent", "=", "False", ")", ":", "\n", "    ", "bos", ",", "eos", ",", "ctx", ",", "ans", ",", "que", ",", "pad", ",", "gen", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "SPECIAL_TOKENS", ")", "\n", "\n", "padded", "=", "[", "]", "\n", "\n", "amr", "=", "[", "bos", "]", "+", "amr_graph", "+", "[", "gen", "]", "\n", "text", "=", "(", "text", "+", "[", "eos", "]", "if", "with_text", "else", "[", "]", ")", "\n", "\n", "max_len", "=", "int", "(", "(", "max_input_length", "-", "3", ")", "/", "2", ")", "\n", "\n", "if", "len", "(", "amr", ")", ">", "max_len", ":", "\n", "        ", "amr", "=", "amr", "[", ":", "max_len", "]", "\n", "amr", "[", "-", "1", "]", "=", "gen", "\n", "", "if", "len", "(", "text", ")", ">", "max_len", ":", "\n", "        ", "text", "=", "text", "[", ":", "max_len", "]", "\n", "\n", "", "combined", "=", "list", "(", "chain", "(", "amr", ",", "text", ")", ")", "\n", "len_combined", "=", "len", "(", "combined", ")", "\n", "\n", "if", "len_combined", "<", "max_input_length", ":", "\n", "        ", "len_reamining", "=", "max_input_length", "-", "len_combined", "\n", "padded", "=", "[", "pad", "]", "*", "len_reamining", "\n", "\n", "", "instance", "=", "{", "}", "\n", "instance", "[", "\"input_ids\"", "]", "=", "list", "(", "chain", "(", "padded", ",", "amr", ",", "text", ")", ")", "\n", "instance", "[", "\"token_type_ids\"", "]", "=", "[", "pad", "]", "*", "len", "(", "padded", ")", "+", "[", "ctx", "]", "*", "len", "(", "amr", ")", "+", "[", "ans", "]", "*", "len", "(", "text", ")", "\n", "instance", "[", "\"attention_mask\"", "]", "=", "[", "0", "]", "*", "len", "(", "padded", ")", "+", "[", "1", "]", "*", "(", "max_input_length", "-", "len", "(", "padded", ")", ")", "\n", "\n", "if", "with_masking", ":", "\n", "        ", "instance", "[", "\"labels\"", "]", "=", "[", "-", "1", "]", "*", "(", "len", "(", "padded", ")", "+", "len", "(", "amr", ")", ")", "+", "text", "\n", "", "else", ":", "\n", "        ", "instance", "[", "\"labels\"", "]", "=", "[", "-", "1", "]", "*", "len", "(", "padded", ")", "+", "list", "(", "chain", "(", "amr", ",", "text", ")", ")", "\n", "\n", "", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.pre_process_amr": [[82, 123], ["tokenizer.convert_tokens_to_ids", "numpy.full", "numpy.full", "numpy.full", "numpy.full", "len", "len", "len", "len", "len", "len", "len", "len"], "function", ["None"], ["", "def", "pre_process_amr", "(", "amr_graph", ",", "text", ",", "tokenizer", ",", "input_len", ",", "cap_length", ",", "\n", "with_text", ",", "with_masking", "=", "False", ")", ":", "\n", "\n", "    ", "instance", "=", "{", "}", "\n", "\n", "bos", ",", "eos", ",", "ctx", ",", "ans", ",", "que", ",", "pad", ",", "gen", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "SPECIAL_TOKENS", ")", "\n", "\n", "if", "not", "with_text", ":", "\n", "        ", "text", "=", "[", "]", "\n", "end", "=", "[", "]", "\n", "", "else", ":", "\n", "        ", "end", "=", "[", "eos", "]", "\n", "\n", "", "input_ids", "=", "np", ".", "full", "(", "(", "input_len", ")", ",", "pad", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "token_type_ids", "=", "np", ".", "full", "(", "(", "input_len", ")", ",", "pad", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "labels", "=", "np", ".", "full", "(", "(", "input_len", ")", ",", "-", "1", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "att_mask", "=", "np", ".", "full", "(", "(", "input_len", ")", ",", "0", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "amr_and_text", "=", "[", "\n", "bos", "]", "+", "amr_graph", "[", ":", "cap_length", "]", "+", "[", "gen", "]", "+", "text", "[", ":", "cap_length", "]", "+", "end", "\n", "type_list", "=", "[", "bos", "]", "+", "[", "ctx", "]", "*", "len", "(", "amr_graph", "[", ":", "cap_length", "]", ")", "+", "[", "gen", "]", "+", "[", "ans", "]", "*", "len", "(", "text", "[", ":", "cap_length", "]", ")", "+", "end", "\n", "att_mask_list", "=", "[", "1", "]", "*", "len", "(", "amr_and_text", ")", "\n", "if", "with_masking", ":", "\n", "        ", "label_list", "=", "[", "-", "1", "]", "+", "[", "-", "1", "]", "*", "len", "(", "amr_graph", "[", ":", "cap_length", "]", "\n", ")", "+", "[", "-", "1", "]", "+", "text", "[", ":", "cap_length", "]", "+", "end", "\n", "", "else", ":", "\n", "        ", "label_list", "=", "amr_and_text", "\n", "\n", "", "input_ids", "[", ":", "len", "(", "amr_and_text", ")", "]", "=", "amr_and_text", "\n", "token_type_ids", "[", ":", "len", "(", "amr_and_text", ")", "]", "=", "type_list", "\n", "labels", "[", ":", "len", "(", "amr_and_text", ")", "]", "=", "label_list", "\n", "att_mask", "[", ":", "len", "(", "amr_and_text", ")", "]", "=", "att_mask_list", "\n", "\n", "instance", "[", "'input_ids'", "]", "=", "input_ids", "\n", "instance", "[", "'token_type_ids'", "]", "=", "token_type_ids", "\n", "instance", "[", "'labels'", "]", "=", "labels", "\n", "instance", "[", "'attention_mask'", "]", "=", "att_mask", "\n", "\n", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.pre_process_amr_datasets_decode": [[125, 148], ["len", "numpy.zeros", "enumerate", "tensor_datasets.append", "torch.tensor", "int", "len"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.zeros"], ["", "def", "pre_process_amr_datasets_decode", "(", "\n", "encoded_datasets", ",", "input_len", ",", "cap_length", ",", "start_token", ",", "delimiter_token", ",", "\n", "eos_token", ",", "pad_token", ")", ":", "\n", "    ", "\"\"\" Pre-process datasets containing lists of tuples(story,\n        1st continuation, 2nd continuation, label)\n\n        To Transformer inputs of shape (n_batch, n_alternative, length)\n        comprising for each batch, continuation:\n        input_ids[batch, alternative, :] = [start_token] + story[:cap_length]\n        + [delimiter_token] + cont1[:cap_length] + [clf_token]\n    \"\"\"", "\n", "tensor_datasets", "=", "[", "]", "\n", "for", "dataset", "in", "encoded_datasets", ":", "\n", "        ", "n_batch", "=", "len", "(", "dataset", ")", "\n", "input_ids", "=", "np", ".", "zeros", "(", "(", "n_batch", ",", "int", "(", "input_len", "/", "2", ")", "+", "1", ")", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "for", "i", ",", "(", "amr_graph", ",", "text", ")", ",", "in", "enumerate", "(", "dataset", ")", ":", "\n", "            ", "amr_and_text", "=", "[", "\n", "start_token", "]", "+", "amr_graph", "[", ":", "cap_length", "]", "+", "[", "delimiter_token", "]", "\n", "input_ids", "[", "i", ",", ":", "len", "(", "amr_and_text", ")", "]", "=", "amr_and_text", "\n", "\n", "", "tensor_datasets", ".", "append", "(", "torch", ".", "tensor", "(", "input_ids", ")", ")", "\n", "\n", "", "return", "tensor_datasets", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.load_amr": [[150, 168], ["os.path.join", "os.path.join", "os.path.join", "dataamr.AMRData", "dataamr.AMRData.load_data"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.load_data"], ["", "def", "load_amr", "(", "args", ")", ":", "\n", "\n", "    ", "dataset_path", "=", "args", ".", "dataset_path", "\n", "train_file", "=", "os", ".", "path", ".", "join", "(", "dataset_path", ",", "\"train.amr\"", ")", "\n", "dev_file", "=", "os", ".", "path", ".", "join", "(", "dataset_path", ",", "\"dev.amr\"", ")", "\n", "test_file", "=", "os", ".", "path", ".", "join", "(", "dataset_path", ",", "\"test.amr\"", ")", "\n", "silver_train_file", "=", "\"silver.amr\"", "\n", "\n", "amr", "=", "AMRData", "(", "\n", "train_file", ",", "\n", "dev_file", ",", "\n", "test_file", ",", "\n", "silver_train_file", ",", "\n", "use_silver_data", "=", "args", ".", "use_silver_data", ",", "\n", "small", "=", "args", ".", "small", ")", "\n", "amr", ".", "load_data", "(", ")", "\n", "\n", "return", "amr", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.update_model": [[170, 173], ["tokenizer.add_tokens", "model.resize_token_embeddings", "len"], "function", ["None"], ["", "def", "update_model", "(", "tokenizer", ",", "model", ",", "amr", ")", ":", "\n", "    ", "tokenizer", ".", "add_tokens", "(", "amr", ".", "edges", ")", "\n", "model", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.read_amr": [[175, 279], ["print", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "str", "str", "str", "str", "len", "len", "len", "len", "str", "zip", "zip", "logger.info", "sys.exit", "len", "zip", "zip", "zip", "zip", "zip", "zip", "zip", "zip", "zip", "zip", "zip", "zip", "zip", "zip", "zip", "zip"], "function", ["None"], ["", "def", "read_amr", "(", "tokenizer", ",", "amr", ",", "args", ")", ":", "\n", "\n", "    ", "print", "(", "\"Reading AMR dataset\"", ")", "\n", "\n", "input_format", "=", "args", ".", "input_format", "\n", "small", "=", "args", ".", "small", "\n", "use_silver_data", "=", "args", ".", "use_silver_data", "\n", "\n", "dataset_train_silver", "=", "None", "\n", "logger", ".", "info", "(", "\"Number of new tokens added to the vocabulary \"", "+", "\n", "str", "(", "len", "(", "amr", ".", "edges", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"train size \"", "+", "str", "(", "len", "(", "amr", ".", "X_train", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"dev size \"", "+", "str", "(", "len", "(", "amr", ".", "X_dev", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"train size \"", "+", "str", "(", "len", "(", "amr", ".", "X_test", ")", ")", ")", "\n", "if", "args", ".", "use_silver_data", ":", "\n", "        ", "logger", ".", "info", "(", "\"silver size \"", "+", "str", "(", "len", "(", "amr", ".", "X_silver_train", ")", ")", ")", "\n", "", "logger", ".", "info", "(", "\"Encoding dataset...\"", ")", "\n", "logger", ".", "info", "(", "\" * Prepare\"", ")", "\n", "\n", "if", "args", ".", "tokenized_input", ":", "\n", "        ", "Y_train", "=", "amr", ".", "Y_train_tok", "\n", "Y_dev", "=", "amr", ".", "Y_dev_tok", "\n", "Y_test", "=", "amr", ".", "Y_test_tok", "\n", "", "else", ":", "\n", "        ", "Y_train", "=", "amr", ".", "Y_train", "\n", "Y_dev", "=", "amr", ".", "Y_dev", "\n", "Y_test", "=", "amr", ".", "Y_test", "\n", "Y_silver_train", "=", "amr", ".", "Y_silver_train", "\n", "\n", "# Using the correct input for the experiment", "\n", "", "if", "input_format", "==", "\"linearized_with_attributes\"", "or", "input_format", "==", "\"linearized_simple\"", ":", "\n", "        ", "if", "small", ":", "\n", "# Small only for debugging", "\n", "            ", "dataset_train", "=", "(", "\n", "[", "(", "\" \"", ".", "join", "(", "x", ")", ",", "\n", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "\n", "amr", ".", "X_train_simple", "[", ":", "50", "]", ",", "\n", "Y_train", "[", ":", "50", "]", ")", "]", ")", "\n", "if", "use_silver_data", ":", "\n", "                ", "dataset_train_silver", "=", "(", "[", "(", "\" \"", ".", "join", "(", "x", ")", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "\n", "amr", ".", "X_silver_train_simple", "[", ":", "50", "]", ",", "Y_silver_train", "[", ":", "50", "]", ")", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "dataset_train", "=", "(", "[", "(", "\" \"", ".", "join", "(", "x", ")", ",", "y", ")", "\n", "for", "x", ",", "y", "in", "zip", "(", "amr", ".", "X_train_simple", ",", "Y_train", ")", "]", ")", "\n", "if", "use_silver_data", ":", "\n", "                ", "dataset_train_silver", "=", "(", "[", "(", "\" \"", ".", "join", "(", "x", ")", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "\n", "amr", ".", "X_silver_train_simple", ",", "Y_silver_train", ")", "]", ")", "\n", "", "", "dataset_dev", "=", "(", "[", "(", "\" \"", ".", "join", "(", "x", ")", ",", "y", ")", "\n", "for", "x", ",", "y", "in", "zip", "(", "amr", ".", "X_dev_simple", ",", "Y_dev", ")", "]", ")", "\n", "dataset_test", "=", "(", "[", "(", "\" \"", ".", "join", "(", "x", ")", ",", "y", ")", "\n", "for", "x", ",", "y", "in", "zip", "(", "amr", ".", "X_test_simple", ",", "Y_test", ")", "]", ")", "\n", "", "elif", "input_format", "==", "\"only_nodes\"", ":", "\n", "        ", "if", "small", ":", "\n", "# Small only for debugging", "\n", "            ", "dataset_train", "=", "(", "[", "(", "\" \"", ".", "join", "(", "x", ")", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "\n", "amr", ".", "X_train_simple_only_nodes", "[", ":", "50", "]", ",", "Y_train", "[", ":", "50", "]", ")", "]", ")", "\n", "if", "use_silver_data", ":", "\n", "                ", "dataset_train_silver", "=", "(", "[", "(", "\" \"", ".", "join", "(", "x", ")", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "\n", "amr", ".", "X_silver_train_simple_only_nodes", "[", ":", "50", "]", ",", "\n", "Y_silver_train", "[", ":", "50", "]", ")", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "dataset_train", "=", "(", "[", "(", "\" \"", ".", "join", "(", "x", ")", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "\n", "amr", ".", "X_train_simple_only_nodes", ",", "Y_train", ")", "]", ")", "\n", "if", "use_silver_data", ":", "\n", "                ", "dataset_train_silver", "=", "(", "[", "(", "\" \"", ".", "join", "(", "x", ")", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "\n", "amr", ".", "X_silver_train_simple_only_nodes", ",", "Y_silver_train", ")", "]", ")", "\n", "\n", "", "", "dataset_dev", "=", "(", "[", "(", "\" \"", ".", "join", "(", "x", ")", ",", "y", ")", "\n", "for", "x", ",", "y", "in", "zip", "(", "amr", ".", "X_dev_simple_only_nodes", ",", "Y_dev", ")", "]", ")", "\n", "dataset_test", "=", "(", "[", "(", "\" \"", ".", "join", "(", "x", ")", ",", "y", ")", "for", "x", ",", "y", "in", "\n", "zip", "(", "amr", ".", "X_test_simple_only_nodes", ",", "Y_test", ")", "]", ")", "\n", "\n", "", "elif", "input_format", "==", "\"original\"", ":", "\n", "        ", "if", "small", ":", "\n", "# Small only for debugging", "\n", "            ", "dataset_train", "=", "(", "\n", "[", "(", "x", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "\n", "amr", ".", "X_train_raw", "[", ":", "50", "]", ",", "\n", "Y_train", "[", ":", "50", "]", ")", "]", ")", "\n", "if", "use_silver_data", ":", "\n", "                ", "dataset_train_silver", "=", "(", "\n", "[", "(", "x", ",", "y", ")", "for", "x", ",", "\n", "y", "\n", "in", "zip", "(", "\n", "amr", ".", "X_silver_train_raw", "[", ":", "50", "]", ",", "\n", "Y_silver_train", "[", ":", "50", "]", ")", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "dataset_train", "=", "(", "[", "(", "x", ",", "y", ")", "\n", "for", "x", ",", "y", "in", "zip", "(", "amr", ".", "X_train_raw", ",", "Y_train", ")", "]", ")", "\n", "if", "use_silver_data", ":", "\n", "\n", "                ", "dataset_train_silver", "=", "(", "[", "(", "x", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "\n", "amr", ".", "X_silver_train_raw", ",", "Y_silver_train", ")", "]", ")", "\n", "\n", "", "", "dataset_dev", "=", "(", "[", "(", "x", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "amr", ".", "X_dev_raw", ",", "Y_dev", ")", "]", ")", "\n", "dataset_test", "=", "(", "[", "(", "x", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "amr", ".", "X_test_raw", ",", "Y_test", ")", "]", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "input_format", "+", "\" is not a valid input format\"", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "", "return", "dataset_train", ",", "dataset_dev", ",", "dataset_test", ",", "dataset_train_silver", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.tokenize_amr": [[281, 315], ["logger.info", "amr_utils.tokenize_and_encode", "amr_utils.tokenize_and_encode", "amr_utils.tokenize_and_encode", "len", "list", "print", "logger.info", "amr_utils.tokenize_and_encode", "logger.info", "round", "len", "list.append"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.tokenize_and_encode", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.tokenize_and_encode", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.tokenize_and_encode", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.tokenize_and_encode"], ["", "def", "tokenize_amr", "(", "tokenizer", ",", "args", ",", "dataset_train", ",", "dataset_dev", ",", "\n", "dataset_test", ",", "dataset_train_silver", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\" * Tokenize train set\"", ")", "\n", "\n", "encoded_dataset_train_silver", "=", "None", "\n", "encoded_dataset_train", "=", "tokenize_and_encode", "(", "dataset_train", ",", "tokenizer", ")", "\n", "if", "args", ".", "use_silver_data", ":", "\n", "        ", "logger", ".", "info", "(", "\"Encoding silver dataset\"", ")", "\n", "encoded_dataset_train_silver", "=", "tokenize_and_encode", "(", "\n", "dataset_train_silver", ",", "tokenizer", ")", "\n", "", "encoded_dataset_dev", "=", "tokenize_and_encode", "(", "dataset_dev", ",", "tokenizer", ")", "\n", "encoded_dataset_test", "=", "tokenize_and_encode", "(", "dataset_test", ",", "tokenizer", ")", "\n", "\n", "total_tokens", "=", "len", "(", "encoded_dataset_train", ")", "\n", "token_count", "=", "0", "\n", "tmp_encoded_dataset_train", "=", "list", "(", ")", "\n", "\n", "# Remove training examples with bigger size than max_size", "\n", "for", "x_inst", ",", "y_inst", "in", "encoded_dataset_train", ":", "\n", "        ", "if", "len", "(", "x_inst", ")", ">", "args", ".", "max_length", ":", "\n", "            ", "token_count", "+=", "1", "\n", "", "else", ":", "\n", "            ", "tmp_encoded_dataset_train", ".", "append", "(", "(", "x_inst", ",", "y_inst", ")", ")", "\n", "\n", "", "", "if", "args", ".", "exclude_large", ":", "\n", "        ", "logger", ".", "info", "(", "\" * [exclude_large] \\\nRemoving the training instances bigger than max_size\"", ")", "\n", "encoded_dataset_train", "=", "tmp_encoded_dataset_train", "\n", "\n", "", "print", "(", "\"Training:\"", ",", "round", "(", "(", "token_count", "/", "total_tokens", ")", "*", "100", ",", "2", ")", ",", "\n", "\"% :\"", ",", "token_count", ",", "\"instances from\"", ",", "total_tokens", ")", "\n", "\n", "return", "encoded_dataset_train", ",", "encoded_dataset_dev", ",", "encoded_dataset_test", ",", "encoded_dataset_train_silver", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.preproc_amr": [[317, 373], ["collections.defaultdict", "logger.info", "enumerate", "logger.info", "list", "tokenizer.convert_tokens_to_ids", "enumerate", "amr_utils.pre_process_amr_leftpad", "pre_process_amr_leftpad.items", "tensor_datasets.append", "datasets[].append", "torch.tensor", "len", "list.append", "amr_utils.split_list", "list.append", "list.append"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.pre_process_amr_leftpad", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.split_list"], ["", "def", "preproc_amr", "(", "args", ",", "tokenizer", ",", "encoded_dataset", ",", "with_text", "=", "True", ",", ")", ":", "\n", "\n", "    ", "datasets", "=", "defaultdict", "(", "list", ")", "\n", "\n", "# Split amr graphs if flag activated and the graph is large", "\n", "if", "args", ".", "split_sent", ":", "\n", "        ", "logger", ".", "info", "(", "\" * Splitting amr graph for big graphs\"", ")", "\n", "tmp_encoded_dataset", "=", "list", "(", ")", "\n", "special", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "AMR_SPECIAL_TOKENS", ")", "\n", "multi_sent", "=", "special", "[", "0", "]", "\n", "join", "=", "special", "[", "1", "]", "\n", "sents", "=", "special", "[", "2", ":", "]", "\n", "\n", "for", "idx", ",", "(", "amr_graph", ",", "text", ")", "in", "enumerate", "(", "encoded_dataset", ")", ":", "\n", "            ", "if", "len", "(", "amr_graph", ")", ">", "100", ":", "\n", "\n", "                ", "if", "multi_sent", "in", "amr_graph", ":", "\n", "                    ", "amr_graph", "=", "amr_graph", "[", "4", ":", "]", "\n", "amr_graph", "=", "amr_graph", "[", ":", "-", "1", "]", "\n", "amr_split", "=", "split_list", "(", "amr_graph", ",", "sents", ")", "\n", "\n", "for", "amr_sub", "in", "amr_split", ":", "\n", "                        ", "if", "amr_sub", ":", "\n", "                            ", "tmp_encoded_dataset", ".", "append", "(", "[", "[", "join", "]", "+", "amr_sub", ",", "text", "]", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "tmp_encoded_dataset", ".", "append", "(", "[", "amr_graph", ",", "text", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "tmp_encoded_dataset", ".", "append", "(", "[", "amr_graph", ",", "text", "]", ")", "\n", "\n", "", "", "encoded_dataset", "=", "tmp_encoded_dataset", "\n", "\n", "", "logger", ".", "info", "(", "\" * Prepare input vectors\"", ")", "\n", "for", "idx", ",", "(", "amr_graph", ",", "text", ")", "in", "enumerate", "(", "encoded_dataset", ")", ":", "\n", "        ", "if", "idx", ">", "args", ".", "max_num_examples", ":", "\n", "            ", "break", "\n", "\n", "", "instance_que", "=", "pre_process_amr_leftpad", "(", "\n", "amr_graph", ",", "\n", "text", ",", "\n", "tokenizer", ",", "\n", "args", ".", "max_input_length", ",", "\n", "args", ".", "max_length", ",", "\n", "with_text", "=", "with_text", ",", "\n", "with_masking", "=", "args", ".", "with_masking", ",", "\n", "split_sent", "=", "args", ".", "split_sent", ")", "\n", "for", "input_name", ",", "input_array", "in", "instance_que", ".", "items", "(", ")", ":", "\n", "            ", "datasets", "[", "input_name", "]", ".", "append", "(", "input_array", ")", "\n", "\n", "", "", "tensor_datasets", "=", "[", "]", "\n", "datasets_padded", "=", "datasets", "\n", "for", "input_name", "in", "MODEL_INPUTS", ":", "\n", "        ", "padded", "=", "datasets_padded", "[", "input_name", "]", "\n", "\n", "tensor_datasets", ".", "append", "(", "torch", ".", "tensor", "(", "padded", ")", ")", "\n", "\n", "", "return", "tensor_datasets", "\n", "", ""]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.__init__": [[84, 165], ["list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list", "list"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "train_file", ",", "dev_file", ",", "test_file", ",", "silver", ",", "\n", "input_format", "=", "\"raw\"", ",", "use_silver_data", "=", "False", ",", "small", "=", "False", ")", ":", "\n", "\n", "# Include atributes of each node to the linearized version of the graph", "\n", "        ", "self", ".", "use_silver_data", "=", "use_silver_data", "\n", "self", ".", "input_format", "=", "input_format", "\n", "self", ".", "small", "=", "small", "\n", "\n", "self", ".", "train_file", "=", "train_file", "\n", "self", ".", "X_train", "=", "list", "(", ")", "\n", "self", ".", "Y_train", "=", "list", "(", ")", "\n", "self", ".", "Y_train_tok", "=", "list", "(", ")", "\n", "self", ".", "X_train_simple", "=", "list", "(", ")", "\n", "self", ".", "X_train_simple_attributes", "=", "list", "(", ")", "\n", "self", ".", "X_train_simple_only_nodes", "=", "list", "(", ")", "\n", "self", ".", "X_train_concepts", "=", "list", "(", ")", "\n", "self", ".", "X_train_ints", "=", "list", "(", ")", "\n", "self", ".", "X_train_raw", "=", "list", "(", ")", "\n", "self", ".", "Y_train_ints", "=", "list", "(", ")", "\n", "self", ".", "amr_train", "=", "None", "\n", "\n", "self", ".", "silver_train_file", "=", "silver", "\n", "self", ".", "X_silver_train", "=", "list", "(", ")", "\n", "self", ".", "Y_silver_train", "=", "list", "(", ")", "\n", "self", ".", "Y_silver_train_tok", "=", "list", "(", ")", "\n", "self", ".", "X_silver_train_simple", "=", "list", "(", ")", "\n", "self", ".", "X_silver_train_simple_attributes", "=", "list", "(", ")", "\n", "self", ".", "X_silver_train_simple_only_nodes", "=", "list", "(", ")", "\n", "self", ".", "X_silver_train_concepts", "=", "list", "(", ")", "\n", "self", ".", "X_silver_train_ints", "=", "list", "(", ")", "\n", "self", ".", "X_silver_train_raw", "=", "list", "(", ")", "\n", "self", ".", "Y_silver_train_ints", "=", "list", "(", ")", "\n", "self", ".", "amr_silver_train", "=", "None", "\n", "\n", "self", ".", "dev_file", "=", "dev_file", "\n", "self", ".", "X_dev", "=", "list", "(", ")", "\n", "self", ".", "Y_dev", "=", "list", "(", ")", "\n", "self", ".", "Y_dev_tok", "=", "list", "(", ")", "\n", "self", ".", "X_dev_simple", "=", "list", "(", ")", "\n", "self", ".", "X_dev_simple_attributes", "=", "list", "(", ")", "\n", "self", ".", "X_dev_simple_only_nodes", "=", "list", "(", ")", "\n", "self", ".", "X_dev_concepts", "=", "list", "(", ")", "\n", "self", ".", "X_dev_ints", "=", "list", "(", ")", "\n", "self", ".", "X_dev_raw", "=", "list", "(", ")", "\n", "self", ".", "Y_dev_ints", "=", "list", "(", ")", "\n", "\n", "self", ".", "test_file", "=", "test_file", "\n", "self", ".", "X_test", "=", "list", "(", ")", "\n", "self", ".", "Y_test", "=", "list", "(", ")", "\n", "self", ".", "Y_test_tok", "=", "list", "(", ")", "\n", "self", ".", "X_test_simple", "=", "list", "(", ")", "\n", "self", ".", "X_test_simple_attributes", "=", "list", "(", ")", "\n", "self", ".", "X_test_simple_only_nodes", "=", "list", "(", ")", "\n", "self", ".", "X_test_ints", "=", "list", "(", ")", "\n", "self", ".", "X_test_raw", "=", "list", "(", ")", "\n", "self", ".", "Y_test_ints", "=", "list", "(", ")", "\n", "\n", "self", ".", "edges", "=", "list", "(", ")", "\n", "self", ".", "edges_w_attributes", "=", "list", "(", ")", "\n", "\n", "self", ".", "lin_to_int", "=", "{", "\n", "PAD", ":", "PAD_IDX", ",", "\n", "BOS", ":", "BOS_IDX", ",", "\n", "EOS", ":", "EOS_IDX", ",", "\n", "OOV", ":", "OOV_IDX", "}", "\n", "self", ".", "int_to_lin", "=", "{", "\n", "PAD_IDX", ":", "PAD", ",", "\n", "BOS_IDX", ":", "BOS", ",", "\n", "EOS_IDX", ":", "EOS", ",", "\n", "OOV_IDX", ":", "OOV", "}", "\n", "\n", "self", ".", "word_to_int", "=", "{", "\n", "PAD", ":", "PAD_IDX", ",", "\n", "BOS", ":", "BOS_IDX", ",", "\n", "EOS", ":", "EOS_IDX", ",", "\n", "OOV", ":", "OOV_IDX", "}", "\n", "self", ".", "int_to_word", "=", "{", "\n", "PAD_IDX", ":", "PAD", ",", "\n", "BOS_IDX", ":", "BOS", ",", "\n", "EOS_IDX", ":", "EOS", ",", "\n", "OOV_IDX", ":", "OOV", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.get_list": [[166, 191], ["amr.graph.get_list_node", "list", "n1.__repr__", "len", "type", "str", "str"], "methods", ["None"], ["", "def", "get_list", "(", "self", ",", "amr", ")", ":", "\n", "        ", "if", "self", ".", "input_format", "==", "\"linearized_simple\"", ":", "\n", "            ", "with_attributes", "=", "False", "\n", "", "else", ":", "\n", "            ", "with_attributes", "=", "True", "\n", "\n", "", "dfs_list", "=", "amr", ".", "graph", ".", "get_list_node", "(", ")", "\n", "out_list", "=", "list", "(", ")", "\n", "for", "n1", ",", "t", ",", "n2", "in", "dfs_list", ":", "\n", "            ", "try", ":", "\n", "                ", "out_list", "+=", "[", "\":\"", "+", "t", ",", "n1", ".", "__repr__", "(", ")", "]", "\n", "", "except", "BaseException", ":", "\n", "                ", "return", "None", "\n", "# If the nodes has attributes, itter through it and add it to the", "\n", "# list", "\n", "", "if", "with_attributes", ":", "\n", "                ", "if", "len", "(", "n1", ".", "attributes", ")", ">", "1", ":", "\n", "                    ", "for", "attr", "in", "n1", ".", "attributes", "[", "1", ":", "]", ":", "\n", "                        ", "if", "type", "(", "attr", "[", "1", "]", ")", "!=", "str", "(", ")", ":", "\n", "                            ", "attr_tmp", "=", "str", "(", "attr", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                            ", "attr_tmp", "=", "attr", "[", "1", "]", "\n", "# Attach to final list", "\n", "", "out_list", "+=", "[", "\":\"", "+", "attr", "[", "0", "]", ",", "attr_tmp", "]", "\n", "", "", "", "", "return", "out_list", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.simplify": [[193, 205], ["step.replace.replace.startswith", "step.replace.replace.replace", "step.replace.replace.replace", "step.replace.replace.replace", "step.replace.replace.split", "step.replace.replace.split"], "methods", ["None"], ["", "def", "simplify", "(", "self", ",", "step", ")", ":", "\n", "        ", "if", "step", ".", "startswith", "(", "\":\"", ")", ":", "\n", "            ", "return", "step", ",", "True", "\n", "", "step", "=", "step", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "\n", "step", "=", "step", ".", "replace", "(", "'\"'", ",", "\"\"", ")", "\n", "step", "=", "step", ".", "replace", "(", "\"_\"", ",", "\" \"", ")", "\n", "if", "\"/\"", "in", "step", ":", "\n", "            ", "step", "=", "step", ".", "split", "(", "\"/\"", ")", "[", "1", "]", "\n", "\n", "", "if", "step", "!=", "'-'", ":", "\n", "            ", "step", "=", "step", ".", "split", "(", "\"-\"", ")", "[", "0", "]", "\n", "", "return", "step", ",", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.load_data": [[207, 369], ["logger.info", "stog.data.dataset_readers.amr_parsing.io.AMRIO.read", "tqdm.tqdm.tqdm", "stog.data.dataset_readers.amr_parsing.io.AMRIO.read", "tqdm.tqdm.tqdm", "stog.data.dataset_readers.amr_parsing.io.AMRIO.read", "tqdm.tqdm.tqdm", "enumerate", "str().splitlines", "dataamr.AMRData.X_train_raw.append", "dataamr.AMRData.get_list", "dataamr.AMRData.X_train.append", "dataamr.AMRData.Y_train.append", "dataamr.AMRData.Y_train_tok.append", "list", "list", "dataamr.AMRData.X_train_simple.append", "dataamr.AMRData.X_train_simple_only_nodes.append", "amr.sentence.split", "print", "stog.data.dataset_readers.amr_parsing.io.AMRIO.read", "enumerate", "print", "print", "enumerate", "str().splitlines", "dataamr.AMRData.X_dev_raw.append", "dataamr.AMRData.get_list", "dataamr.AMRData.X_dev.append", "dataamr.AMRData.Y_dev.append", "dataamr.AMRData.Y_dev_tok.append", "list", "list", "dataamr.AMRData.X_dev_simple.append", "dataamr.AMRData.X_dev_simple_only_nodes.append", "enumerate", "str().splitlines", "dataamr.AMRData.X_test_raw.append", "dataamr.AMRData.get_list", "dataamr.AMRData.X_test.append", "dataamr.AMRData.Y_test.append", "dataamr.AMRData.Y_test_tok.append", "list", "list", "dataamr.AMRData.X_test_simple.append", "dataamr.AMRData.X_test_simple_only_nodes.append", "amr_line.strip", "raw_amr.append", "dataamr.AMRData.simplify", "list.append", "dataamr.AMRData.get_list", "str().splitlines", "dataamr.AMRData.X_silver_train_raw.append", "dataamr.AMRData.X_silver_train.append", "dataamr.AMRData.Y_silver_train.append", "dataamr.AMRData.Y_silver_train_tok.append", "list", "list", "dataamr.AMRData.X_silver_train_simple.append", "dataamr.AMRData.X_silver_train_simple_only_nodes.append", "amr.sentence.split", "len", "amr_line.strip", "raw_amr.append", "dataamr.AMRData.simplify", "list.append", "amr_line.strip", "raw_amr.append", "dataamr.AMRData.simplify", "list.append", "str", "dataamr.AMRData.lin_to_int.keys", "len", "step.startswith", "list.append", "dataamr.AMRData.edges.append", "dataamr.AMRData.word_to_int.keys", "len", "amr_line.strip", "raw_amr.append", "dataamr.AMRData.simplify", "list.append", "str", "step.startswith", "list.append", "dataamr.AMRData.edges.append", "str", "step.startswith", "list.append", "dataamr.AMRData.edges.append", "str", "dataamr.AMRData.lin_to_int.keys", "len", "step.startswith", "list.append", "dataamr.AMRData.edges.append", "dataamr.AMRData.word_to_int.keys", "len", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.get_list", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.get_list", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.get_list", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.simplify", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.get_list", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.simplify", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.simplify", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.simplify"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Parsing and linearizing the AMR dataset\"", ")", "\n", "\n", "train_amr", "=", "AMRIO", ".", "read", "(", "self", ".", "train_file", ")", "\n", "\n", "for", "i", ",", "amr", "in", "tqdm", "(", "enumerate", "(", "train_amr", ")", ",", "desc", "=", "'Train AMR'", ")", ":", "\n", "# Raw version", "\n", "            ", "if", "self", ".", "small", "and", "i", ">", "50", ":", "\n", "                ", "break", "\n", "\n", "", "raw_amr", "=", "[", "]", "\n", "for", "amr_line", "in", "str", "(", "amr", ".", "graph", ")", ".", "splitlines", "(", ")", ":", "\n", "                ", "striped_amr", "=", "amr_line", ".", "strip", "(", ")", "\n", "raw_amr", ".", "append", "(", "striped_amr", ")", "\n", "", "self", ".", "X_train_raw", ".", "append", "(", "\" \"", ".", "join", "(", "raw_amr", ")", ")", "\n", "\n", "linearized_amr", "=", "self", ".", "get_list", "(", "amr", ")", "\n", "\n", "self", ".", "X_train", ".", "append", "(", "linearized_amr", "[", "1", ":", "]", ")", "\n", "self", ".", "Y_train", ".", "append", "(", "amr", ".", "sentence", ")", "\n", "self", ".", "Y_train_tok", ".", "append", "(", "amr", ".", "tokens", ")", "\n", "\n", "# Vocabulary Create dictionaries and simplify list", "\n", "simpl", "=", "list", "(", ")", "\n", "simpl_only_nodes", "=", "list", "(", ")", "\n", "for", "step", "in", "linearized_amr", ":", "\n", "                ", "if", "step", "not", "in", "self", ".", "lin_to_int", ".", "keys", "(", ")", ":", "\n", "                    ", "self", ".", "lin_to_int", "[", "step", "]", "=", "len", "(", "self", ".", "lin_to_int", ")", "\n", "self", ".", "int_to_lin", "[", "len", "(", "self", ".", "int_to_lin", ")", "]", "=", "step", "\n", "# simplyfied AMR version", "\n", "", "step", ",", "edge", "=", "self", ".", "simplify", "(", "step", ")", "\n", "simpl", ".", "append", "(", "step", ")", "\n", "if", "not", "step", ".", "startswith", "(", "\":\"", ")", ":", "\n", "                    ", "simpl_only_nodes", ".", "append", "(", "step", ")", "\n", "# Identify edges and save them", "\n", "", "if", "edge", "and", "step", "not", "in", "self", ".", "edges", ":", "\n", "                    ", "self", ".", "edges", ".", "append", "(", "step", ")", "\n", "\n", "", "", "self", ".", "X_train_simple", ".", "append", "(", "simpl", ")", "\n", "self", ".", "X_train_simple_only_nodes", ".", "append", "(", "simpl_only_nodes", ")", "\n", "\n", "sent", "=", "amr", ".", "sentence", ".", "split", "(", ")", "\n", "for", "word", "in", "sent", ":", "\n", "                ", "if", "word", "not", "in", "self", ".", "word_to_int", ".", "keys", "(", ")", ":", "\n", "                    ", "self", ".", "word_to_int", "[", "word", "]", "=", "len", "(", "self", ".", "word_to_int", ")", "\n", "self", ".", "int_to_word", "[", "len", "(", "self", ".", "int_to_word", ")", "]", "=", "word", "\n", "\n", "", "", "", "if", "self", ".", "use_silver_data", ":", "\n", "            ", "print", "(", "\"Processing silver data from\"", ",", "self", ".", "silver_train_file", ")", "\n", "ii", "=", "0", "\n", "\n", "silver_train_amr", "=", "AMRIO", ".", "read", "(", "self", ".", "silver_train_file", ")", "\n", "for", "i", ",", "amr", "in", "enumerate", "(", "silver_train_amr", ")", ":", "\n", "                ", "if", "self", ".", "small", "and", "i", ">", "50", ":", "\n", "                    ", "break", "\n", "\n", "# Raw version", "\n", "", "raw_amr", "=", "[", "]", "\n", "ii", "+=", "1", "\n", "linearized_amr", "=", "self", ".", "get_list", "(", "amr", ")", "\n", "if", "linearized_amr", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "for", "amr_line", "in", "str", "(", "amr", ".", "graph", ")", ".", "splitlines", "(", ")", ":", "\n", "                    ", "striped_amr", "=", "amr_line", ".", "strip", "(", ")", "\n", "raw_amr", ".", "append", "(", "striped_amr", ")", "\n", "", "self", ".", "X_silver_train_raw", ".", "append", "(", "\" \"", ".", "join", "(", "raw_amr", ")", ")", "\n", "\n", "self", ".", "X_silver_train", ".", "append", "(", "linearized_amr", "[", "1", ":", "]", ")", "\n", "self", ".", "Y_silver_train", ".", "append", "(", "amr", ".", "sentence", ")", "\n", "self", ".", "Y_silver_train_tok", ".", "append", "(", "amr", ".", "tokens", ")", "\n", "\n", "# Vocabulary Create dictionaries and simplify list", "\n", "simpl", "=", "list", "(", ")", "\n", "simpl_only_nodes", "=", "list", "(", ")", "\n", "for", "step", "in", "linearized_amr", ":", "\n", "                    ", "if", "step", "not", "in", "self", ".", "lin_to_int", ".", "keys", "(", ")", ":", "\n", "                        ", "self", ".", "lin_to_int", "[", "step", "]", "=", "len", "(", "self", ".", "lin_to_int", ")", "\n", "self", ".", "int_to_lin", "[", "len", "(", "self", ".", "int_to_lin", ")", "]", "=", "step", "\n", "# simplyfied AMR version", "\n", "", "step", ",", "edge", "=", "self", ".", "simplify", "(", "step", ")", "\n", "simpl", ".", "append", "(", "step", ")", "\n", "if", "not", "step", ".", "startswith", "(", "\":\"", ")", ":", "\n", "                        ", "simpl_only_nodes", ".", "append", "(", "step", ")", "\n", "# Identify edges and save them", "\n", "", "if", "edge", "and", "step", "not", "in", "self", ".", "edges", ":", "\n", "                        ", "self", ".", "edges", ".", "append", "(", "step", ")", "\n", "\n", "", "", "self", ".", "X_silver_train_simple", ".", "append", "(", "simpl", ")", "\n", "self", ".", "X_silver_train_simple_only_nodes", ".", "append", "(", "simpl_only_nodes", ")", "\n", "\n", "sent", "=", "amr", ".", "sentence", ".", "split", "(", ")", "\n", "for", "word", "in", "sent", ":", "\n", "                    ", "if", "word", "not", "in", "self", ".", "word_to_int", ".", "keys", "(", ")", ":", "\n", "                        ", "self", ".", "word_to_int", "[", "word", "]", "=", "len", "(", "self", ".", "word_to_int", ")", "\n", "self", ".", "int_to_word", "[", "len", "(", "self", ".", "int_to_word", ")", "]", "=", "word", "\n", "", "", "", "print", "(", "\"Silver data with size:\"", ",", "len", "(", "self", ".", "X_silver_train_raw", ")", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"No silver data performed\"", ")", "\n", "\n", "", "dev_amr", "=", "AMRIO", ".", "read", "(", "self", ".", "dev_file", ")", "\n", "for", "i", ",", "amr", "in", "tqdm", "(", "enumerate", "(", "dev_amr", ")", ",", "desc", "=", "'Dev AMR'", ")", ":", "\n", "            ", "if", "self", ".", "small", "and", "i", ">", "50", ":", "\n", "                ", "break", "\n", "\n", "# Raw input", "\n", "", "raw_amr", "=", "[", "]", "\n", "for", "amr_line", "in", "str", "(", "amr", ".", "graph", ")", ".", "splitlines", "(", ")", ":", "\n", "                ", "striped_amr", "=", "amr_line", ".", "strip", "(", ")", "\n", "raw_amr", ".", "append", "(", "striped_amr", ")", "\n", "", "self", ".", "X_dev_raw", ".", "append", "(", "\" \"", ".", "join", "(", "raw_amr", ")", ")", "\n", "\n", "linearized_amr", "=", "self", ".", "get_list", "(", "amr", ")", "\n", "self", ".", "X_dev", ".", "append", "(", "linearized_amr", "[", "1", ":", "]", ")", "\n", "self", ".", "Y_dev", ".", "append", "(", "amr", ".", "sentence", ")", "\n", "self", ".", "Y_dev_tok", ".", "append", "(", "amr", ".", "tokens", ")", "\n", "\n", "# simplyfied AMR version", "\n", "simpl", "=", "list", "(", ")", "\n", "simpl_only_nodes", "=", "list", "(", ")", "\n", "for", "step", "in", "linearized_amr", ":", "\n", "                ", "step", ",", "edge", "=", "self", ".", "simplify", "(", "step", ")", "\n", "simpl", ".", "append", "(", "step", ")", "\n", "if", "not", "step", ".", "startswith", "(", "\":\"", ")", ":", "\n", "                    ", "simpl_only_nodes", ".", "append", "(", "step", ")", "\n", "", "if", "edge", "and", "step", "not", "in", "self", ".", "edges", ":", "\n", "                    ", "self", ".", "edges", ".", "append", "(", "step", ")", "\n", "", "", "self", ".", "X_dev_simple", ".", "append", "(", "simpl", ")", "\n", "self", ".", "X_dev_simple_only_nodes", ".", "append", "(", "simpl_only_nodes", ")", "\n", "\n", "", "test_amr", "=", "AMRIO", ".", "read", "(", "self", ".", "test_file", ")", "\n", "self", ".", "amr_test", "=", "test_amr", "\n", "for", "i", ",", "amr", "in", "tqdm", "(", "enumerate", "(", "test_amr", ")", ",", "desc", "=", "'Test AMR'", ")", ":", "\n", "            ", "if", "self", ".", "small", "and", "i", ">", "50", ":", "\n", "                ", "break", "\n", "\n", "# Raw version", "\n", "", "raw_amr", "=", "[", "]", "\n", "for", "amr_line", "in", "str", "(", "amr", ".", "graph", ")", ".", "splitlines", "(", ")", ":", "\n", "                ", "striped_amr", "=", "amr_line", ".", "strip", "(", ")", "\n", "raw_amr", ".", "append", "(", "striped_amr", ")", "\n", "", "self", ".", "X_test_raw", ".", "append", "(", "\" \"", ".", "join", "(", "raw_amr", ")", ")", "\n", "\n", "linearized_amr", "=", "self", ".", "get_list", "(", "amr", ")", "\n", "self", ".", "X_test", ".", "append", "(", "linearized_amr", "[", "1", ":", "]", ")", "\n", "self", ".", "Y_test", ".", "append", "(", "amr", ".", "sentence", ")", "\n", "self", ".", "Y_test_tok", ".", "append", "(", "amr", ".", "tokens", ")", "\n", "\n", "# simplyfied AMR version", "\n", "simpl", "=", "list", "(", ")", "\n", "simpl_only_nodes", "=", "list", "(", ")", "\n", "for", "step", "in", "linearized_amr", ":", "\n", "\n", "                ", "step", ",", "edge", "=", "self", ".", "simplify", "(", "step", ")", "\n", "simpl", ".", "append", "(", "step", ")", "\n", "if", "not", "step", ".", "startswith", "(", "\":\"", ")", ":", "\n", "                    ", "simpl_only_nodes", ".", "append", "(", "step", ")", "\n", "\n", "", "if", "edge", "and", "step", "not", "in", "self", ".", "edges", ":", "\n", "                    ", "self", ".", "edges", ".", "append", "(", "step", ")", "\n", "", "", "self", ".", "X_test_simple", ".", "append", "(", "simpl", ")", "\n", "self", ".", "X_test_simple_only_nodes", ".", "append", "(", "simpl_only_nodes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.output_data": [[370, 436], ["print", "open", "open", "open", "open", "open", "open", "open", "open", "open", "open", "open", "open", "print", "zip", "print", "zip", "print", "zip", "open.close", "open.close", "open.close", "open.close", "open.close", "open.close", "open.close", "open.close", "open.close", "open.close", "open.close", "open.close", "len", "len", "len", "len", "print", "print", "print", "print", "len", "len", "len", "len", "print", "print", "print", "print", "len", "len", "len", "len", "print", "print", "print", "print"], "methods", ["None"], ["", "", "def", "output_data", "(", "self", ",", "output_src_file", ",", "output_trg_file", ")", ":", "\n", "        ", "print", "(", "\"Write linearized AMRs to file\"", ")", "\n", "F_train_src", "=", "open", "(", "output_src_file", "+", "\".train\"", ",", "\"w\"", ")", "\n", "F_train_raw_src", "=", "open", "(", "output_src_file", "+", "\".amr.train\"", ",", "\"w\"", ")", "\n", "F_train_trg", "=", "open", "(", "output_trg_file", "+", "\".train\"", ",", "\"w\"", ")", "\n", "F_train_tok_trg", "=", "open", "(", "output_trg_file", "+", "\".tok.train\"", ",", "\"w\"", ")", "\n", "F_dev_src", "=", "open", "(", "output_src_file", "+", "\".dev\"", ",", "\"w\"", ")", "\n", "F_dev_raw_src", "=", "open", "(", "output_src_file", "+", "\".amr.dev\"", ",", "\"w\"", ")", "\n", "F_dev_trg", "=", "open", "(", "output_trg_file", "+", "\".dev\"", ",", "\"w\"", ")", "\n", "F_dev_tok_trg", "=", "open", "(", "output_trg_file", "+", "\".tok.dev\"", ",", "\"w\"", ")", "\n", "F_test_src", "=", "open", "(", "output_src_file", "+", "\".test\"", ",", "\"w\"", ")", "\n", "F_test_raw_src", "=", "open", "(", "output_src_file", "+", "\".amr.test\"", ",", "\"w\"", ")", "\n", "F_test_trg", "=", "open", "(", "output_trg_file", "+", "\".test\"", ",", "\"w\"", ")", "\n", "F_test_tok_trg", "=", "open", "(", "output_trg_file", "+", "\".tok.test\"", ",", "\"w\"", ")", "\n", "\n", "print", "(", "\n", "\"TRAIN: src lin:\"", ",", "len", "(", "\n", "self", ".", "X_train", ")", ",", "\"src amr\"", ",", "len", "(", "\n", "self", ".", "X_train_raw", ")", ",", "\"trg text\"", ",", "len", "(", "\n", "self", ".", "Y_train_tok", ")", ",", "\"trg tok\"", ",", "len", "(", "\n", "self", ".", "Y_train_tok", ")", ")", "\n", "for", "x", ",", "x_raw", ",", "y", ",", "y_tok", "in", "zip", "(", "\n", "self", ".", "X_train", ",", "self", ".", "X_train_raw", ",", "self", ".", "Y_train", ",", "self", ".", "Y_train_tok", ")", ":", "\n", "            ", "print", "(", "\" \"", ".", "join", "(", "x", ")", ",", "file", "=", "F_train_src", ")", "\n", "print", "(", "y_tok", ",", "file", "=", "F_train_trg", ")", "\n", "print", "(", "x_raw", ",", "file", "=", "F_train_raw_src", ")", "\n", "print", "(", "y_tok", ",", "file", "=", "F_train_tok_trg", ")", "\n", "\n", "", "print", "(", "\n", "\"dev: src lin:\"", ",", "len", "(", "\n", "self", ".", "X_dev", ")", ",", "\"src amr\"", ",", "len", "(", "\n", "self", ".", "X_dev_raw", ")", ",", "\"trg text\"", ",", "len", "(", "\n", "self", ".", "Y_dev", ")", ",", "\"trg tok\"", ",", "len", "(", "\n", "self", ".", "Y_dev_tok", ")", ")", "\n", "for", "x", ",", "x_raw", ",", "y", ",", "y_tok", "in", "zip", "(", "\n", "self", ".", "X_dev", ",", "self", ".", "X_dev_raw", ",", "self", ".", "Y_dev", ",", "self", ".", "Y_dev_tok", ")", ":", "\n", "            ", "print", "(", "\" \"", ".", "join", "(", "x", ")", ",", "file", "=", "F_dev_src", ")", "\n", "print", "(", "y_tok", ",", "file", "=", "F_dev_trg", ")", "\n", "print", "(", "x_raw", ",", "file", "=", "F_dev_raw_src", ")", "\n", "print", "(", "y_tok", ",", "file", "=", "F_dev_tok_trg", ")", "\n", "\n", "", "print", "(", "\n", "\"test: src lin:\"", ",", "len", "(", "\n", "self", ".", "X_test", ")", ",", "\"src amr\"", ",", "len", "(", "\n", "self", ".", "X_test_raw", ")", ",", "\"trg text\"", ",", "len", "(", "\n", "self", ".", "Y_test", ")", ",", "\"trg tok\"", ",", "len", "(", "\n", "self", ".", "Y_test_tok", ")", ")", "\n", "for", "x", ",", "x_raw", ",", "y", ",", "y_tok", "in", "zip", "(", "\n", "self", ".", "X_test", ",", "self", ".", "X_test_raw", ",", "self", ".", "Y_test", ",", "self", ".", "Y_test_tok", ")", ":", "\n", "            ", "print", "(", "\" \"", ".", "join", "(", "x", ")", ",", "file", "=", "F_test_src", ")", "\n", "print", "(", "y_tok", ",", "file", "=", "F_test_trg", ")", "\n", "print", "(", "x_raw", ",", "file", "=", "F_test_raw_src", ")", "\n", "print", "(", "y_tok", ",", "file", "=", "F_test_tok_trg", ")", "\n", "\n", "", "F_train_src", ".", "close", "(", ")", "\n", "F_train_trg", ".", "close", "(", ")", "\n", "F_train_raw_src", ".", "close", "(", ")", "\n", "F_train_tok_trg", ".", "close", "(", ")", "\n", "F_dev_src", ".", "close", "(", ")", "\n", "F_dev_trg", ".", "close", "(", ")", "\n", "F_dev_raw_src", ".", "close", "(", ")", "\n", "F_dev_tok_trg", ".", "close", "(", ")", "\n", "F_test_src", ".", "close", "(", ")", "\n", "F_test_trg", ".", "close", "(", ")", "\n", "F_test_raw_src", ".", "close", "(", ")", "\n", "F_test_tok_trg", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.AMRData.to_ints": [[437, 487], ["print", "tqdm.tqdm.tqdm", "zip", "zip", "zip", "tqdm.tqdm.tqdm.close", "dataamr.AMRData.X_train_ints.append", "dataamr.AMRData.Y_train_ints.append", "tqdm.tqdm.tqdm.update", "list", "list", "dataamr.AMRData.Y_dev_ints.append", "dataamr.AMRData.X_dev_ints.append", "tqdm.tqdm.tqdm.update", "list", "list", "dataamr.AMRData.Y_test_ints.append", "dataamr.AMRData.X_test_ints.append", "tqdm.tqdm.tqdm.update", "len", "dataamr.AMRData.lin_to_int.keys", "list.append", "list.append", "dataamr.AMRData.word_to_int.keys", "list.append", "list.append", "dataamr.AMRData.lin_to_int.keys", "list.append", "list.append", "dataamr.AMRData.word_to_int.keys", "list.append", "list.append", "len", "len", "y.split"], "methods", ["None"], ["", "def", "to_ints", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"Transform to ints\"", ")", "\n", "pbar", "=", "tqdm", "(", "total", "=", "len", "(", "self", ".", "X_train", ")", "+", "len", "(", "self", ".", "X_dev", ")", "+", "len", "(", "self", ".", "X_test", ")", ")", "\n", "\n", "for", "x", ",", "y", "in", "zip", "(", "self", ".", "X_train", ",", "self", ".", "Y_train", ")", ":", "\n", "            ", "self", ".", "X_train_ints", ".", "append", "(", "[", "self", ".", "lin_to_int", "[", "x_i", "]", "\n", "for", "x_i", "in", "x", "]", "+", "[", "EOS_IDX", "]", ")", "\n", "self", ".", "Y_train_ints", ".", "append", "(", "[", "self", ".", "word_to_int", "[", "y_i", "]", "\n", "for", "y_i", "in", "y", ".", "split", "(", ")", "]", "+", "[", "EOS_IDX", "]", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "", "for", "x", ",", "y", "in", "zip", "(", "self", ".", "X_dev", ",", "self", ".", "Y_dev", ")", ":", "\n", "            ", "x_in", "=", "list", "(", ")", "\n", "y_in", "=", "list", "(", ")", "\n", "for", "x_i", "in", "x", ":", "\n", "                ", "if", "x_i", "not", "in", "self", ".", "lin_to_int", ".", "keys", "(", ")", ":", "\n", "                    ", "x_in", ".", "append", "(", "self", ".", "lin_to_int", "[", "OOV", "]", ")", "\n", "", "else", ":", "\n", "                    ", "x_in", ".", "append", "(", "self", ".", "lin_to_int", "[", "x_i", "]", ")", "\n", "", "", "for", "y_i", "in", "y", ":", "\n", "                ", "if", "y_i", "not", "in", "self", ".", "word_to_int", ".", "keys", "(", ")", ":", "\n", "                    ", "y_in", ".", "append", "(", "self", ".", "word_to_int", "[", "OOV", "]", ")", "\n", "", "else", ":", "\n", "                    ", "y_in", ".", "append", "(", "self", ".", "word_to_int", "[", "y_i", "]", ")", "\n", "\n", "", "", "self", ".", "Y_dev_ints", ".", "append", "(", "y_in", "+", "[", "EOS_IDX", "]", ")", "\n", "self", ".", "X_dev_ints", ".", "append", "(", "x_in", "+", "[", "EOS_IDX", "]", ")", "\n", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "", "for", "x", ",", "y", "in", "zip", "(", "self", ".", "X_test", ",", "self", ".", "Y_test", ")", ":", "\n", "            ", "x_in", "=", "list", "(", ")", "\n", "y_in", "=", "list", "(", ")", "\n", "for", "x_i", "in", "x", ":", "\n", "                ", "if", "x_i", "not", "in", "self", ".", "lin_to_int", ".", "keys", "(", ")", ":", "\n", "                    ", "x_in", ".", "append", "(", "self", ".", "lin_to_int", "[", "OOV", "]", ")", "\n", "", "else", ":", "\n", "                    ", "x_in", ".", "append", "(", "self", ".", "lin_to_int", "[", "x_i", "]", ")", "\n", "", "", "for", "y_i", "in", "y", ":", "\n", "                ", "if", "y_i", "not", "in", "self", ".", "word_to_int", ".", "keys", "(", ")", ":", "\n", "                    ", "y_in", ".", "append", "(", "self", ".", "word_to_int", "[", "OOV", "]", ")", "\n", "", "else", ":", "\n", "                    ", "y_in", ".", "append", "(", "self", ".", "word_to_int", "[", "y_i", "]", ")", "\n", "\n", "", "", "self", ".", "Y_test_ints", ".", "append", "(", "y_in", "+", "[", "EOS_IDX", "]", ")", "\n", "self", ".", "X_test_ints", ".", "append", "(", "x_in", "+", "[", "EOS_IDX", "]", ")", "\n", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "", "pbar", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.dataamr.batch_data": [[11, 81], ["zip", "zip", "zip", "print", "print", "print", "print", "print", "print", "src_batch.append", "trg_batch.append", "src_batch.append", "trg_batch.append", "src_batch.append", "trg_batch.append", "len", "len", "len", "len", "len", "data_train.append", "len", "len", "len", "len", "len", "data_dev.append", "len", "len", "len", "len", "len", "data_test.append", "seq.extend", "seq.extend", "seq.extend", "seq.extend", "seq.extend", "seq.extend", "len", "len", "len", "extra.utils.LongTensor", "extra.utils.LongTensor", "extra.utils.LongTensor", "extra.utils.LongTensor", "extra.utils.LongTensor", "extra.utils.LongTensor", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.LongTensor", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.LongTensor", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.LongTensor", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.LongTensor", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.LongTensor", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.LongTensor"], ["def", "batch_data", "(", "amr_data", ",", "batch_size", "=", "20", ")", ":", "\n", "    ", "data_train", "=", "[", "]", "\n", "src_batch", "=", "[", "]", "\n", "trg_batch", "=", "[", "]", "\n", "src_batch_len", "=", "0", "\n", "trg_batch_len", "=", "0", "\n", "for", "src", ",", "trg", "in", "zip", "(", "amr_data", ".", "X_train_ints", ",", "amr_data", ".", "Y_train_ints", ")", ":", "\n", "        ", "if", "len", "(", "src", ")", ">", "src_batch_len", ":", "\n", "            ", "src_batch_len", "=", "len", "(", "src", ")", "\n", "", "if", "len", "(", "trg", ")", ">", "trg_batch_len", ":", "\n", "            ", "trg_batch_len", "=", "len", "(", "trg", ")", "\n", "", "src_batch", ".", "append", "(", "src", ")", "\n", "trg_batch", ".", "append", "(", "trg", ")", "\n", "if", "len", "(", "src_batch", ")", "==", "batch_size", ":", "\n", "            ", "for", "seq", "in", "src_batch", ":", "\n", "                ", "seq", ".", "extend", "(", "[", "PAD_IDX", "]", "*", "(", "src_batch_len", "-", "len", "(", "seq", ")", ")", ")", "\n", "", "for", "seq", "in", "trg_batch", ":", "\n", "                ", "seq", ".", "extend", "(", "[", "PAD_IDX", "]", "*", "(", "trg_batch_len", "-", "len", "(", "seq", ")", ")", ")", "\n", "", "data_train", ".", "append", "(", "(", "LongTensor", "(", "src_batch", ")", ",", "LongTensor", "(", "trg_batch", ")", ")", ")", "\n", "src_batch", "=", "[", "]", "\n", "trg_batch", "=", "[", "]", "\n", "src_batch_len", "=", "0", "\n", "trg_batch_len", "=", "0", "\n", "\n", "", "", "data_dev", "=", "[", "]", "\n", "for", "src", ",", "trg", "in", "zip", "(", "amr_data", ".", "X_dev_ints", ",", "amr_data", ".", "Y_dev_ints", ")", ":", "\n", "        ", "if", "len", "(", "src", ")", ">", "src_batch_len", ":", "\n", "            ", "src_batch_len", "=", "len", "(", "src", ")", "\n", "", "if", "len", "(", "trg", ")", ">", "trg_batch_len", ":", "\n", "            ", "trg_batch_len", "=", "len", "(", "trg", ")", "\n", "", "src_batch", ".", "append", "(", "src", ")", "\n", "trg_batch", ".", "append", "(", "trg", ")", "\n", "if", "len", "(", "src_batch", ")", "==", "batch_size", ":", "\n", "            ", "for", "seq", "in", "src_batch", ":", "\n", "                ", "seq", ".", "extend", "(", "[", "PAD_IDX", "]", "*", "(", "src_batch_len", "-", "len", "(", "seq", ")", ")", ")", "\n", "", "for", "seq", "in", "trg_batch", ":", "\n", "                ", "seq", ".", "extend", "(", "[", "PAD_IDX", "]", "*", "(", "trg_batch_len", "-", "len", "(", "seq", ")", ")", ")", "\n", "", "data_dev", ".", "append", "(", "(", "LongTensor", "(", "src_batch", ")", ",", "LongTensor", "(", "trg_batch", ")", ")", ")", "\n", "src_batch", "=", "[", "]", "\n", "trg_batch", "=", "[", "]", "\n", "src_batch_len", "=", "0", "\n", "trg_batch_len", "=", "0", "\n", "\n", "", "", "data_test", "=", "[", "]", "\n", "for", "src", ",", "trg", "in", "zip", "(", "amr_data", ".", "X_test_ints", ",", "amr_data", ".", "Y_test_ints", ")", ":", "\n", "        ", "if", "len", "(", "src", ")", ">", "src_batch_len", ":", "\n", "            ", "src_batch_len", "=", "len", "(", "src", ")", "\n", "", "if", "len", "(", "trg", ")", ">", "trg_batch_len", ":", "\n", "            ", "trg_batch_len", "=", "len", "(", "trg", ")", "\n", "", "src_batch", ".", "append", "(", "src", ")", "\n", "trg_batch", ".", "append", "(", "trg", ")", "\n", "if", "len", "(", "src_batch", ")", "==", "batch_size", ":", "\n", "            ", "for", "seq", "in", "src_batch", ":", "\n", "                ", "seq", ".", "extend", "(", "[", "PAD_IDX", "]", "*", "(", "src_batch_len", "-", "len", "(", "seq", ")", ")", ")", "\n", "", "for", "seq", "in", "trg_batch", ":", "\n", "                ", "seq", ".", "extend", "(", "[", "PAD_IDX", "]", "*", "(", "trg_batch_len", "-", "len", "(", "seq", ")", ")", ")", "\n", "", "data_test", ".", "append", "(", "(", "LongTensor", "(", "src_batch", ")", ",", "LongTensor", "(", "trg_batch", ")", ")", ")", "\n", "src_batch", "=", "[", "]", "\n", "trg_batch", "=", "[", "]", "\n", "src_batch_len", "=", "0", "\n", "trg_batch_len", "=", "0", "\n", "\n", "", "", "print", "(", "\"Training data size: %d\"", "%", "(", "len", "(", "data_train", ")", "*", "batch_size", ")", ")", "\n", "print", "(", "\"Training batch size: %d\"", "%", "batch_size", ")", "\n", "print", "(", "\"Dev data size: %d\"", "%", "(", "len", "(", "data_dev", ")", "*", "batch_size", ")", ")", "\n", "print", "(", "\"Dev batch size: %d\"", "%", "batch_size", ")", "\n", "print", "(", "\"Test data size: %d\"", "%", "(", "len", "(", "data_test", ")", "*", "batch_size", ")", ")", "\n", "print", "(", "\"Test batch size: %d\"", "%", "batch_size", ")", "\n", "\n", "return", "data_train", ",", "data_dev", ",", "data_test", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.find_sub_list": [[28, 35], ["len", "enumerate"], "function", ["None"], ["", "def", "find_sub_list", "(", "sl", ",", "l", ")", ":", "\n", "    ", "\"find starting and ending indices of sublist in list\"", "\n", "sll", "=", "len", "(", "sl", ")", "\n", "for", "ind", "in", "(", "i", "for", "i", ",", "e", "in", "enumerate", "(", "l", ")", "if", "e", "==", "sl", "[", "0", "]", ")", ":", "\n", "        ", "if", "l", "[", "ind", ":", "ind", "+", "sll", "]", "==", "sl", ":", "\n", "            ", "return", "ind", ",", "ind", "+", "sll", "-", "1", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_best_indexes": [[37, 48], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "get_best_indexes", "(", "logits", ",", "n_best_size", "=", "1", ")", ":", "\n", "    ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "        ", "if", "i", ">=", "n_best_size", ":", "\n", "            ", "break", "\n", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.normalize_answer": [[50, 66], ["utils.normalize_answer.white_space_fix"], "function", ["None"], ["", "def", "normalize_answer", "(", "s", ")", ":", "\n", "    ", "\"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"", "\n", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re", ".", "sub", "(", "r'\\b(a|an|the)\\b'", ",", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.convert_input_to_text": [[68, 83], ["tokenizer.convert_tokens_to_ids", "input_ids[].tolist", "input_ids[].tolist", "input_ids[].tolist", "tokenizer.decode"], "function", ["None"], ["", "def", "convert_input_to_text", "(", "input_ids", ",", "tokenizer", ",", "decode", "=", "True", ")", ":", "\n", "    ", "bos", ",", "eos", ",", "ctx", ",", "ans", ",", "que", ",", "pad", ",", "_", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "SPECIAL_TOKENS", ")", "\n", "a_idx", "=", "(", "input_ids", "==", "ans", ")", ".", "nonzero", "(", ")", ".", "item", "(", ")", "\n", "c_idx", "=", "(", "input_ids", "==", "ctx", ")", ".", "nonzero", "(", ")", ".", "item", "(", ")", "\n", "q_idx", "=", "(", "input_ids", "==", "que", ")", ".", "nonzero", "(", ")", ".", "item", "(", ")", "\n", "eos_idx", "=", "(", "input_ids", "==", "eos", ")", ".", "nonzero", "(", ")", ".", "item", "(", ")", "\n", "c", "=", "input_ids", "[", "c_idx", "+", "1", ":", "a_idx", "]", ".", "tolist", "(", ")", "\n", "a", "=", "input_ids", "[", "a_idx", "+", "1", ":", "q_idx", "]", ".", "tolist", "(", ")", "\n", "q", "=", "input_ids", "[", "q_idx", "+", "1", ":", "eos_idx", "]", ".", "tolist", "(", ")", "\n", "\n", "triplet", "=", "[", "c", ",", "a", ",", "q", "]", "\n", "if", "decode", ":", "\n", "        ", "return", "[", "tokenizer", ".", "decode", "(", "element", ")", "for", "element", "in", "triplet", "]", "\n", "", "return", "triplet", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.convert_question_to_text": [[85, 92], ["tokenizer.convert_tokens_to_ids", "[].item", "question[].tolist", "tokenizer.decode"], "function", ["None"], ["", "def", "convert_question_to_text", "(", "question", ",", "tokenizer", ",", "decode", "=", "True", ")", ":", "\n", "    ", "eos", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "\"<eos>\"", ")", "\n", "eos_idx", "=", "(", "question", "==", "eos", ")", ".", "nonzero", "(", ")", "[", "0", "]", ".", "item", "(", ")", "\n", "q", "=", "question", "[", ":", "eos_idx", "]", ".", "tolist", "(", ")", "\n", "if", "decode", ":", "\n", "        ", "return", "tokenizer", ".", "decode", "(", "q", ")", "\n", "", "return", "q", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.trim_pad": [[94, 99], ["[].min"], "function", ["None"], ["", "def", "trim_pad", "(", "input_ids", ",", "lm_labels", ",", "token_type_ids", ",", "attention_mask", ",", "pad", ")", ":", "\n", "    ", "min_idx", "=", "(", "input_ids", "!=", "pad", ")", ".", "nonzero", "(", ")", "[", ":", ",", "1", "]", ".", "min", "(", ")", "\n", "\n", "return", "[", "input_ids", "[", ":", ",", "min_idx", ":", "]", ",", "lm_labels", "[", ":", ",", "min_idx", ":", "]", ",", "\n", "token_type_ids", "[", ":", ",", "min_idx", ":", "]", ",", "attention_mask", "[", ":", ",", "min_idx", ":", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.trim_pad_max": [[101, 106], ["[].max"], "function", ["None"], ["", "def", "trim_pad_max", "(", "input_ids", ",", "lm_labels", ",", "token_type_ids", ",", "attention_mask", ",", "pad", ")", ":", "\n", "    ", "min_idx", "=", "(", "input_ids", "!=", "pad", ")", ".", "nonzero", "(", ")", "[", ":", ",", "1", "]", ".", "max", "(", ")", "\n", "\n", "return", "[", "input_ids", "[", ":", ",", "min_idx", ":", "]", ",", "lm_labels", "[", ":", ",", "min_idx", ":", "]", ",", "\n", "token_type_ids", "[", ":", ",", "min_idx", ":", "]", ",", "attention_mask", "[", ":", ",", "min_idx", ":", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.trim_batch": [[108, 115], ["utils.trim_pad", "utils.trim_pad"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.trim_pad", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.trim_pad"], ["", "def", "trim_batch", "(", "batch", ",", "pad", ")", ":", "\n", "    ", "input_ids", ",", "lm_labels", ",", "token_type_ids", ",", "attention_mask", ",", "partial_input_ids", ",", "partial_lm_labels", ",", "partial_token_type_ids", ",", "partial_attention_mask", "=", "batch", "\n", "return", "trim_pad", "(", "input_ids", ",", "lm_labels", ",", "token_type_ids", ",", "attention_mask", ",", "pad", ")", "+", "trim_pad", "(", "partial_input_ids", ",", "partial_lm_labels", ",", "\n", "partial_token_type_ids", ",", "partial_attention_mask", ",", "pad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.apply_loss": [[117, 129], ["loss.mean.backward", "loss.mean.mean", "torch.nn.utils.clip_grad_norm_", "optimizer.step", "optimizer.zero_grad"], "function", ["None"], ["", "def", "apply_loss", "(", "idx", ",", "optimizer", ",", "loss", ",", "args", ",", "retain_graph", "=", "False", ")", ":", "\n", "    ", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "        ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "", "loss", "/=", "args", ".", "gradient_accumulation_steps", "\n", "loss", ".", "backward", "(", "retain_graph", "=", "retain_graph", ")", "\n", "if", "args", ".", "max_norm", "is", "not", "None", ":", "\n", "        ", "params", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'params'", "]", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "params", ",", "args", ".", "max_norm", ")", "\n", "", "if", "idx", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "        ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.pad_dataset": [[131, 140], ["float", "logger.info", "min", "max", "len", "len"], "function", ["None"], ["", "def", "pad_dataset", "(", "dataset", ",", "padding", "=", "0", ",", "max_input_length", "=", "float", "(", "'inf'", ")", ")", ":", "\n", "    ", "\"\"\" Pad the dataset. This could be optimized by defining a\n    Dataset class and padd only batches but this is simpler. \"\"\"", "\n", "logger", ".", "info", "(", "\"Pad inputs and convert to Tensor\"", ")", "\n", "max_l", "=", "min", "(", "max", "(", "len", "(", "x", ")", "for", "x", "in", "dataset", "[", "\"input_ids\"", "]", ")", ",", "max_input_length", ")", "\n", "for", "name", "in", "PADDED_INPUTS", ":", "\n", "        ", "dataset", "[", "name", "]", "=", "[", "[", "padding", "if", "name", "!=", "\"labels\"", "else", "-", "1", "]", "\n", "*", "(", "max_l", "-", "len", "(", "x", ")", ")", "+", "x", "[", ":", "max_l", "]", "for", "x", "in", "dataset", "[", "name", "]", "]", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.build_que_input_from_segments": [[142, 173], ["tokenizer.convert_tokens_to_ids", "list", "len", "list", "itertools.chain", "itertools.chain", "len", "len", "len", "len", "len", "len", "len", "len", "len"], "function", ["None"], ["", "def", "build_que_input_from_segments", "(", "context", ",", "answer", ",", "question", ",", "tokenizer", ",", "\n", "max_input_length", "=", "1000", ",", "with_eos", "=", "True", ",", "\n", "with_labels", "=", "True", ")", ":", "\n", "    ", "\"\"\" Build a sequence of input from 3 segments:\n        context, answer, question \"\"\"", "\n", "bos", ",", "eos", ",", "ctx", ",", "ans", ",", "que", ",", "pad", ",", "gen", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "SPECIAL_TOKENS", ")", "\n", "\n", "padded", "=", "[", "]", "\n", "context", "=", "[", "bos", ",", "ctx", "]", "+", "context", "\n", "answer", "=", "[", "ans", "]", "+", "answer", "\n", "question", "=", "[", "que", "]", "+", "question", "+", "(", "[", "eos", "]", "if", "with_eos", "else", "[", "]", ")", "\n", "\n", "combined", "=", "list", "(", "chain", "(", "context", ",", "answer", ",", "question", ")", ")", "\n", "len_combined", "=", "len", "(", "combined", ")", "\n", "\n", "if", "len_combined", ">", "max_input_length", ":", "\n", "        ", "len_context", "=", "max_input_length", "-", "len", "(", "answer", ")", "-", "len", "(", "question", ")", "\n", "context", "=", "context", "[", ":", "len_context", "]", "\n", "", "elif", "len_combined", "<", "max_input_length", ":", "\n", "        ", "len_reamining", "=", "max_input_length", "-", "len_combined", "\n", "padded", "=", "[", "pad", "]", "*", "len_reamining", "\n", "\n", "", "instance", "=", "{", "}", "\n", "instance", "[", "\"input_ids\"", "]", "=", "list", "(", "chain", "(", "padded", ",", "context", ",", "answer", ",", "question", ")", ")", "\n", "instance", "[", "\"token_type_ids\"", "]", "=", "[", "pad", "]", "*", "len", "(", "padded", ")", "+", "[", "ctx", "]", "*", "len", "(", "context", ")", "+", "[", "ans", "]", "*", "len", "(", "answer", ")", "+", "[", "que", "]", "*", "len", "(", "question", ")", "\n", "if", "with_labels", ":", "\n", "        ", "instance", "[", "\"labels\"", "]", "=", "[", "-", "1", "]", "*", "(", "len", "(", "padded", ")", "+", "len", "(", "context", ")", "+", "len", "(", "answer", ")", "\n", "+", "1", ")", "+", "question", "[", "1", ":", "]", "\n", "", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.build_ans_input_from_segments_bert": [[175, 209], ["tokenizer.convert_tokens_to_ids", "list", "len", "list", "utils.find_sub_list", "itertools.chain", "itertools.chain", "len", "len", "len", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.find_sub_list"], ["", "def", "build_ans_input_from_segments_bert", "(", "context", ",", "answer", ",", "question", ",", "tokenizer", ",", "\n", "max_input_length", ")", ":", "\n", "    ", "\"\"\" Build a sequence of input from 3 segments:\n        context, question, answer \"\"\"", "\n", "cls", ",", "sep", ",", "pad", ",", "unused0", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "SPECIAL_TOKENS_BERT", ")", "\n", "\n", "padded", "=", "[", "]", "\n", "question", "=", "[", "cls", "]", "+", "question", "+", "[", "sep", "]", "\n", "context", "=", "context", "+", "[", "sep", "]", "\n", "\n", "combined", "=", "list", "(", "chain", "(", "context", ",", "question", ")", ")", "\n", "len_combined", "=", "len", "(", "combined", ")", "\n", "\n", "if", "len_combined", ">=", "max_input_length", ":", "\n", "        ", "len_context", "=", "max_input_length", "-", "len", "(", "question", ")", "\n", "context", "=", "context", "[", ":", "len_context", "]", "\n", "", "else", ":", "\n", "        ", "remaining_len", "=", "max_input_length", "-", "len", "(", "question", ")", "-", "len", "(", "context", ")", "\n", "padded", "=", "[", "pad", "]", "*", "remaining_len", "\n", "\n", "", "instance", "=", "{", "}", "\n", "instance", "[", "\"input_ids\"", "]", "=", "list", "(", "chain", "(", "question", ",", "context", ",", "padded", ")", ")", "\n", "instance", "[", "\"token_type_ids\"", "]", "=", "[", "pad", "]", "*", "len", "(", "question", ")", "+", "[", "unused0", "]", "*", "len", "(", "context", ")", "+", "[", "pad", "]", "*", "len", "(", "padded", ")", "\n", "instance", "[", "\"attention_mask\"", "]", "=", "[", "unused0", "]", "*", "len", "(", "question", ")", "+", "[", "unused0", "]", "*", "len", "(", "context", ")", "+", "[", "pad", "]", "*", "len", "(", "padded", ")", "\n", "\n", "pair", "=", "find_sub_list", "(", "answer", ",", "instance", "[", "\"input_ids\"", "]", ")", "\n", "# TODO: Find why pair fails?", "\n", "if", "not", "pair", ":", "\n", "        ", "return", "None", "\n", "", "instance", "[", "\"start_positions\"", "]", ",", "instance", "[", "\"end_positions\"", "]", "=", "pair", "\n", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.build_ans_input_from_segments": [[211, 229], ["tokenizer.convert_tokens_to_ids", "list", "itertools.chain", "len", "len", "len", "len", "len"], "function", ["None"], ["", "def", "build_ans_input_from_segments", "(", "context", ",", "answer", ",", "question", ",", "tokenizer", ",", "\n", "with_eos", "=", "True", ")", ":", "\n", "    ", "\"\"\" Build a sequence of input from 3 segments:\n        context, question, answer \"\"\"", "\n", "bos", ",", "eos", ",", "ctx", ",", "ans", ",", "que", ",", "_", ",", "_", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "SPECIAL_TOKENS", ")", "\n", "\n", "context", "=", "[", "bos", ",", "ctx", "]", "+", "context", "\n", "question", "=", "[", "que", "]", "+", "question", "\n", "answer", "=", "[", "ans", "]", "+", "answer", "+", "(", "[", "eos", "]", "if", "with_eos", "else", "[", "]", ")", "\n", "\n", "instance", "=", "{", "}", "\n", "instance", "[", "\"input_ids\"", "]", "=", "list", "(", "chain", "(", "context", ",", "question", ",", "answer", ")", ")", "\n", "instance", "[", "\"token_type_ids\"", "]", "=", "[", "ctx", "]", "*", "len", "(", "context", ")", "+", "[", "que", "]", "*", "len", "(", "question", ")", "+", "[", "ans", "]", "*", "len", "(", "answer", ")", "\n", "instance", "[", "\"labels\"", "]", "=", "[", "-", "1", "]", "*", "len", "(", "context", ")", "+", "[", "-", "1", "]", "*", "len", "(", "question", ")", "+", "[", "-", "1", "]", "+", "answer", "[", "1", ":", "]", "\n", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.read_cardie": [[236, 263], ["enumerate", "open", "f.readlines", "open", "f.readlines", "open", "f.readlines", "zip", "context.strip.strip", "question.strip.strip", "contexts.append", "answers.append", "questions.append", "answer.strip().split", "answer.strip"], "function", ["None"], ["", "def", "read_cardie", "(", "dataset_path", ")", ":", "\n", "    ", "dataset_path_context", "=", "dataset_path", "+", "'.context'", "\n", "dataset_path_answers", "=", "dataset_path", "+", "'.answers'", "\n", "dataset_path_questions", "=", "dataset_path", "+", "'.questions'", "\n", "\n", "contexts", ",", "questions", ",", "answers", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "dataset_path_context", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ",", "\n", "errors", "=", "'ignore'", ")", "as", "f", ":", "\n", "        ", "contexts_dataset", "=", "f", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "dataset_path_answers", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ",", "\n", "errors", "=", "'ignore'", ")", "as", "f", ":", "\n", "        ", "answers_dataset", "=", "f", ".", "readlines", "(", ")", "\n", "", "with", "open", "(", "dataset_path_questions", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ",", "\n", "errors", "=", "'ignore'", ")", "as", "f", ":", "\n", "        ", "questions_dataset", "=", "f", ".", "readlines", "(", ")", "\n", "", "for", "idx", ",", "(", "context", ",", "answer", ",", "question", ")", "in", "enumerate", "(", "zip", "(", "contexts_dataset", ",", "\n", "answers_dataset", ",", "\n", "questions_dataset", ")", ")", ":", "\n", "        ", "context", "=", "context", ".", "strip", "(", ")", "\n", "answer", "=", "answer", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "[", "0", "]", "\n", "question", "=", "question", ".", "strip", "(", ")", "\n", "contexts", ".", "append", "(", "context", ")", "\n", "answers", ".", "append", "(", "answer", ")", "\n", "questions", ".", "append", "(", "question", ")", "\n", "if", "idx", ">=", "86633", "*", "4", ":", "\n", "            ", "break", "\n", "", "", "return", "[", "contexts", ",", "answers", ",", "questions", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.read_drop": [[265, 278], ["DropReader", "enumerate", "DropReader.read", "contexts.append", "questions.append", "answers.append", "logger.info"], "function", ["None"], ["", "def", "read_drop", "(", "dataset_path", ")", ":", "\n", "    ", "from", "allennlp", ".", "data", ".", "dataset_readers", "import", "DropReader", "\n", "reader", "=", "DropReader", "(", "instance_format", "=", "\"bert\"", ")", "\n", "contexts", ",", "questions", ",", "answers", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "idx", ",", "instance", "in", "enumerate", "(", "reader", ".", "read", "(", "dataset_path", ")", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "answers", ".", "append", "(", "instance", "[", "'metadata'", "]", "[", "'answer_texts'", "]", "[", "0", "]", ")", "\n", "", "except", ":", "\n", "            ", "logger", ".", "info", "(", "f\"Skipping {idx} while reading drop.\"", ")", "\n", "continue", "\n", "", "contexts", ".", "append", "(", "instance", "[", "'metadata'", "]", "[", "'original_passage'", "]", ")", "\n", "questions", ".", "append", "(", "instance", "[", "'metadata'", "]", "[", "'original_question'", "]", ")", "\n", "", "return", "[", "contexts", ",", "answers", ",", "questions", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.read_natural_questions": [[280, 306], ["MRQAReader", "enumerate", "MRQAReader.read", "len", "re.sub", "contexts.append", "questions.append", "logger.info", "answers.append", "logger.info", "instance[].split"], "function", ["None"], ["", "def", "read_natural_questions", "(", "dataset_path", ")", ":", "\n", "    ", "from", "mrqa_reader", "import", "MRQAReader", "\n", "reader", "=", "MRQAReader", "(", ")", "\n", "contexts", ",", "questions", ",", "answers", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "idx", ",", "instance", "in", "enumerate", "(", "reader", ".", "read", "(", "dataset_path", ")", ")", ":", "\n", "        ", "instance", "=", "instance", "[", "'metadata'", "]", "\n", "has_answer", "=", "instance", "[", "\"has_answer\"", "]", "\n", "if", "not", "has_answer", ":", "\n", "            ", "logger", ".", "info", "(", "f\"Skipping {idx} without answer.\"", ")", "\n", "continue", "\n", "\n", "", "answer", "=", "instance", "[", "\"answer_texts_list\"", "]", "\n", "if", "len", "(", "answer", ")", ":", "\n", "            ", "answers", ".", "append", "(", "answer", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "f\"Skipping {idx} while reading natural questions.\"", ")", "\n", "continue", "\n", "\n", "", "context", "=", "instance", "[", "\"original_passage\"", "]", ".", "split", "(", "\"[SEP]\"", ")", "[", "1", "]", "\n", "context", "=", "re", ".", "sub", "(", "r'<.*?>'", ",", "''", ",", "context", ")", "\n", "contexts", ".", "append", "(", "context", ")", "\n", "\n", "question", "=", "\" \"", ".", "join", "(", "instance", "[", "\"question_tokens\"", "]", ")", "+", "\"?\"", "\n", "questions", ".", "append", "(", "question", ")", "\n", "\n", "", "return", "[", "contexts", ",", "answers", ",", "questions", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.read_squad": [[308, 334], ["MRQAReader", "enumerate", "MRQAReader.read", "len", "contexts.append", "questions.append", "logger.info", "answers.append", "logger.info", "instance[].split"], "function", ["None"], ["", "def", "read_squad", "(", "dataset_path", ")", ":", "\n", "    ", "from", "mrqa_reader", "import", "MRQAReader", "\n", "reader", "=", "MRQAReader", "(", ")", "\n", "contexts", ",", "questions", ",", "answers", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "idx", ",", "instance", "in", "enumerate", "(", "reader", ".", "read", "(", "dataset_path", ")", ")", ":", "\n", "        ", "instance", "=", "instance", "[", "'metadata'", "]", "\n", "answer", "=", "instance", "[", "\"answer_texts_list\"", "]", "\n", "has_answer", "=", "instance", "[", "\"has_answer\"", "]", "\n", "if", "not", "has_answer", ":", "\n", "            ", "logger", ".", "info", "(", "f\"Using None for question {idx}.\"", ")", "\n", "answer", "=", "[", "'None'", "]", "\n", "\n", "", "if", "len", "(", "answer", ")", ":", "\n", "            ", "answers", ".", "append", "(", "answer", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "f\"Skipping {idx} while reading natural questions.\"", ")", "\n", "continue", "\n", "\n", "", "context", "=", "instance", "[", "\"original_passage\"", "]", ".", "split", "(", "\"[SEP]\"", ")", "[", "1", "]", "\n", "# context = re.sub(r'<.*?>', '', context)", "\n", "contexts", ".", "append", "(", "context", ")", "\n", "\n", "question", "=", "\" \"", ".", "join", "(", "instance", "[", "\"question_tokens\"", "]", ")", "\n", "questions", ".", "append", "(", "question", ")", "\n", "\n", "", "return", "[", "contexts", ",", "answers", ",", "questions", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.read_squad2": [[336, 349], ["SquadReader", "enumerate", "SquadReader.read", "contexts.append", "questions.append", "answers.append", "answers.append"], "function", ["None"], ["", "def", "read_squad2", "(", "dataset_path", ")", ":", "\n", "    ", "from", "allennlp", ".", "data", ".", "dataset_readers", "import", "SquadReader", "\n", "reader", "=", "SquadReader", "(", ")", "\n", "contexts", ",", "questions", ",", "answers", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "idx", ",", "instance", "in", "enumerate", "(", "reader", ".", "read", "(", "dataset_path", ")", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "answers", ".", "append", "(", "instance", "[", "'metadata'", "]", "[", "'answer_texts'", "]", "[", "0", "]", ")", "\n", "", "except", ":", "\n", "# using None for non-answerable questions", "\n", "            ", "answers", ".", "append", "(", "'None'", ")", "\n", "", "contexts", ".", "append", "(", "\" \"", ".", "join", "(", "instance", "[", "'metadata'", "]", "[", "'passage_tokens'", "]", ")", ")", "\n", "questions", ".", "append", "(", "\" \"", ".", "join", "(", "instance", "[", "'metadata'", "]", "[", "'question_tokens'", "]", ")", ")", "\n", "", "return", "[", "contexts", ",", "answers", ",", "questions", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_dataset": [[351, 395], ["os.path.join", "logger.info", "os.path.basename", "os.path.isfile", "logger.info", "torch.load", "logger.info", "logger.info", "utils.get_dataset.tokenize"], "function", ["None"], ["", "def", "get_dataset", "(", "tokenizer", ",", "dataset_path", ",", "dataset_cache_dir", "=", "None", ",", "\n", "dataset_type", "=", "None", ",", "re_tokenize", "=", "False", ")", ":", "\n", "    ", "\"\"\" Get dataset from path \"\"\"", "\n", "# dataset_path = dataset_path or PERSONACHAT_URL", "\n", "dataset_cache", "=", "os", ".", "path", ".", "join", "(", "dataset_cache_dir", ",", "os", ".", "path", ".", "basename", "(", "dataset_path", ")", ")", "\n", "logger", ".", "info", "(", "\"Check dataset cache at %s\"", ",", "dataset_cache", ")", "\n", "if", "dataset_cache", "and", "os", ".", "path", ".", "isfile", "(", "dataset_cache", ")", "and", "not", "re_tokenize", ":", "\n", "        ", "logger", ".", "info", "(", "\"Load tokenized dataset from cache at %s\"", ",", "dataset_cache", ")", "\n", "dataset", "=", "torch", ".", "load", "(", "dataset_cache", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Get dataset from %s\"", ",", "dataset_path", ")", "\n", "\n", "if", "'squad'", "in", "dataset_type", ":", "\n", "            ", "dataset", "=", "read_squad2", "(", "dataset_path", ")", "\n", "", "elif", "'dusquad'", "in", "dataset_type", ":", "\n", "            ", "dataset", "=", "read_squad2", "(", "dataset_path", ")", "\n", "", "elif", "'squad2'", "in", "dataset_type", ":", "\n", "            ", "dataset", "=", "read_squad2", "(", "dataset_path", ")", "\n", "", "elif", "'drop'", "in", "dataset_type", ":", "\n", "            ", "dataset", "=", "read_drop", "(", "dataset_path", ")", "\n", "", "elif", "'natural_questions'", "in", "dataset_type", ":", "\n", "            ", "dataset", "=", "read_natural_questions", "(", "dataset_path", ")", "\n", "", "elif", "'cardie'", "in", "dataset_type", ":", "\n", "            ", "dataset", "=", "read_cardie", "(", "dataset_path", ")", "\n", "", "else", ":", "\n", "            ", "NotImplementedError", "\n", "\n", "", "logger", ".", "info", "(", "\"Tokenize and encode the dataset\"", ")", "\n", "\n", "def", "tokenize", "(", "obj", ")", ":", "\n", "            ", "if", "isinstance", "(", "obj", ",", "str", ")", ":", "\n", "                ", "return", "tokenizer", ".", "encode", "(", "obj", ")", "\n", "", "if", "isinstance", "(", "obj", ",", "dict", ")", ":", "\n", "                ", "return", "dict", "(", "(", "n", ",", "tokenize", "(", "o", ")", ")", "for", "n", ",", "o", "in", "obj", ".", "items", "(", ")", ")", "\n", "", "return", "list", "(", "tokenize", "(", "o", ")", "for", "o", "in", "obj", ")", "\n", "", "dataset", "=", "tokenize", "(", "dataset", ")", "\n", "if", "dataset_cache", ":", "\n", "            ", "os", ".", "makedirs", "(", "dataset_cache_dir", ",", "exist_ok", "=", "True", ")", "\n", "logger", ".", "info", "(", "\n", "f'Saving tokenized dataset to cache at {dataset_cache}'", ")", "\n", "torch", ".", "save", "(", "dataset", ",", "dataset_cache", ")", "\n", "\n", "", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_datasets": [[397, 462], ["collections.defaultdict", "os.path.join", "utils.get_dataset", "dataset.append", "os.path.join", "utils.get_dataset", "dataset.append", "os.path.join", "utils.get_dataset", "dataset.append", "os.path.join", "utils.get_dataset", "dataset.append", "os.path.join", "utils.get_dataset", "dataset.append", "os.path.join", "utils.get_dataset", "dataset.append", "enumerate", "tensor_datasets.append", "zip", "utils.build_que_input_from_segments", "build_que_input_from_segments.items", "torch.tensor", "datasets[].append"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_dataset", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_dataset", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_dataset", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_dataset", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_dataset", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_dataset", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.build_que_input_from_segments"], ["", "def", "get_datasets", "(", "args", ",", "tokenizer", ",", "with_question", "=", "False", ")", ":", "\n", "# Download and tokenize training dataset", "\n", "    ", "dataset", "=", "[", "]", "\n", "if", "'drop'", "in", "args", ".", "dataset_type", ":", "\n", "        ", "dataset_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dataset_path", ",", "\n", "f'drop_dataset_{args.traintype}.json'", ")", "\n", "datasubset", "=", "get_dataset", "(", "tokenizer", ",", "dataset_path", ",", "args", ".", "dataset_cache", ",", "\n", "args", ".", "dataset_type", ",", "args", ".", "re_tokenize", ")", "\n", "dataset", ".", "append", "(", "datasubset", ")", "\n", "", "if", "'natural_questions'", "in", "args", ".", "dataset_type", ":", "\n", "        ", "dataset_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dataset_path", ",", "\n", "f'NaturalQuestionsShort.{args.traintype}.jsonl.gz'", ")", "\n", "datasubset", "=", "get_dataset", "(", "tokenizer", ",", "dataset_path", ",", "args", ".", "dataset_cache", ",", "\n", "args", ".", "dataset_type", ",", "args", ".", "re_tokenize", ")", "\n", "dataset", ".", "append", "(", "datasubset", ")", "\n", "", "if", "'squad'", "in", "args", ".", "dataset_type", ":", "\n", "        ", "dataset_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dataset_path", ",", "\n", "f'SQuAD.{args.traintype}.json'", ")", "\n", "datasubset", "=", "get_dataset", "(", "tokenizer", ",", "dataset_path", ",", "args", ".", "dataset_cache", ",", "\n", "args", ".", "dataset_type", ",", "args", ".", "re_tokenize", ")", "\n", "dataset", ".", "append", "(", "datasubset", ")", "\n", "", "if", "'dusquad'", "in", "args", ".", "dataset_type", ":", "\n", "        ", "dataset_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dataset_path", ",", "\n", "f'{args.traintype}-v1.1.json'", ")", "\n", "datasubset", "=", "get_dataset", "(", "tokenizer", ",", "dataset_path", ",", "args", ".", "dataset_cache", ",", "\n", "args", ".", "dataset_type", ",", "args", ".", "re_tokenize", ")", "\n", "dataset", ".", "append", "(", "datasubset", ")", "\n", "", "if", "'squad2'", "in", "args", ".", "dataset_type", ":", "\n", "        ", "dataset_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dataset_path", ",", "\n", "f'SQuAD2.{args.traintype}.json'", ")", "\n", "datasubset", "=", "get_dataset", "(", "tokenizer", ",", "dataset_path", ",", "args", ".", "dataset_cache", ",", "\n", "args", ".", "dataset_type", ",", "args", ".", "re_tokenize", ")", "\n", "dataset", ".", "append", "(", "datasubset", ")", "\n", "", "if", "'cardie'", "in", "args", ".", "dataset_type", ":", "\n", "        ", "dataset_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dataset_path", ",", "\n", "f'du-cardie.ca.{args.traintype}'", ")", "\n", "datasubset", "=", "get_dataset", "(", "tokenizer", ",", "dataset_path", ",", "args", ".", "dataset_cache", ",", "\n", "args", ".", "dataset_type", ",", "args", ".", "re_tokenize", ")", "\n", "dataset", ".", "append", "(", "datasubset", ")", "\n", "\n", "", "datasets", "=", "defaultdict", "(", "list", ")", "\n", "for", "datasubset", "in", "dataset", ":", "\n", "        ", "for", "idx", ",", "(", "context", ",", "answer", ",", "question", ")", "in", "enumerate", "(", "zip", "(", "*", "datasubset", ")", ")", ":", "\n", "            ", "if", "idx", ">", "args", ".", "max_num_examples", ":", "\n", "                ", "break", "\n", "\n", "", "if", "with_question", ":", "\n", "                ", "with_eos", "=", "True", "\n", "", "else", ":", "\n", "# for autoregressive question: []", "\n", "# no <eos> string at the end", "\n", "                ", "question", "=", "[", "]", "\n", "with_eos", "=", "False", "\n", "", "instance_que", "=", "build_que_input_from_segments", "(", "\n", "context", ",", "answer", ",", "question", ",", "tokenizer", ",", "\n", "max_input_length", "=", "args", ".", "max_input_length", ",", "with_eos", "=", "with_eos", ")", "\n", "for", "input_name", ",", "input_array", "in", "instance_que", ".", "items", "(", ")", ":", "\n", "                ", "datasets", "[", "input_name", "]", ".", "append", "(", "input_array", ")", "\n", "\n", "", "", "", "tensor_datasets", "=", "[", "]", "\n", "datasets_padded", "=", "datasets", "\n", "for", "input_name", "in", "MODEL_INPUTS", ":", "\n", "        ", "tensor_datasets", ".", "append", "(", "torch", ".", "tensor", "(", "datasets_padded", "[", "input_name", "]", ")", ")", "\n", "", "return", "tensor_datasets", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_data_loaders": [[464, 546], ["logger.info", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "logger.info", "amr_utils.load_amr", "amr_utils.update_model", "logger.info", "logger.info", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "logger.info", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "logger.info", "os.path.isfile", "logger.info", "logger.info", "torch.load", "torch.load", "torch.load", "logger.info", "amr_utils.read_amr", "amr_utils.tokenize_amr", "dataset_name.lower", "amr_utils.preproc_amr", "amr_utils.preproc_amr", "utils.get_datasets", "utils.get_datasets", "torch.load", "os.makedirs", "logger.info", "torch.save", "torch.save", "torch.save", "dataset_name.lower", "torch.save", "dataset_name.lower", "dataset_name.lower", "print", "print", "len"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.load_amr", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.update_model", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.read_amr", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.tokenize_amr", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.preproc_amr", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.amr_utils.preproc_amr", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_datasets", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.None.utils.get_datasets"], ["", "def", "get_data_loaders", "(", "args", ",", "tokenizer", ",", "model", ",", "dataset_name", "=", "\"Train\"", ",", "\n", "shuffle", "=", "True", ")", ":", "\n", "    ", "if", "'amr'", "in", "args", ".", "dataset_type", ":", "\n", "        ", "logger", ".", "info", "(", "\"The data set is AMR\"", ")", "\n", "\n", "file_name_train", "=", "f'amr_{args.input_format}_train'", "\n", "file_name_dev", "=", "f'amr_{args.input_format}_dev'", "\n", "file_name_test", "=", "f'amr_{args.input_format}_test'", "\n", "file_name_silver", "=", "'amr_silver_data.txt'", "\n", "\n", "dataset_cache_silver", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dataset_cache", ",", "\n", "file_name_silver", ")", "\n", "dataset_cache", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dataset_cache", ",", "file_name_train", ")", "\n", "dataset_cache_dev", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dataset_cache", ",", "file_name_dev", ")", "\n", "dataset_cache_test", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dataset_cache", ",", "file_name_test", ")", "\n", "\n", "logger", ".", "info", "(", "\"Loding AMR\"", ")", "\n", "amr", "=", "load_amr", "(", "args", ")", "\n", "update_model", "(", "tokenizer", ",", "model", ",", "amr", ")", "\n", "\n", "if", "dataset_cache", "and", "os", ".", "path", ".", "isfile", "(", "dataset_cache", ")", "and", "not", "args", ".", "re_tokenize", ":", "\n", "            ", "logger", ".", "info", "(", "\"Loding tokenized AMR from cache\"", ")", "\n", "logger", ".", "info", "(", "\n", "f'Load tokenized dataset from cache at {dataset_cache}'", "\n", ")", "\n", "encoded_dataset_train", "=", "torch", ".", "load", "(", "dataset_cache", ")", "\n", "encoded_dataset_dev", "=", "torch", ".", "load", "(", "dataset_cache_dev", ")", "\n", "encoded_dataset_test", "=", "torch", ".", "load", "(", "dataset_cache_test", ")", "\n", "if", "args", ".", "use_silver_data", ":", "\n", "                ", "encoded_dataset_silver", "=", "torch", ".", "load", "(", "dataset_cache_silver", ")", "\n", "", "else", ":", "\n", "                ", "encoded_dataset_silver", "=", "None", "\n", "", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"Tokenize AMR\"", ")", "\n", "datasets", "=", "read_amr", "(", "tokenizer", ",", "amr", ",", "args", ")", "\n", "encoded_dataset_train", ",", "encoded_dataset_dev", ",", "encoded_dataset_test", ",", "encoded_dataset_silver", "=", "tokenize_amr", "(", "tokenizer", ",", "args", ",", "*", "datasets", ")", "\n", "if", "dataset_cache", ":", "\n", "                ", "os", ".", "makedirs", "(", "args", ".", "dataset_cache", ",", "exist_ok", "=", "True", ")", "\n", "logger", ".", "info", "(", "\n", "f'Saving tokenized dataset to cache at {dataset_cache}'", ")", "\n", "torch", ".", "save", "(", "encoded_dataset_train", ",", "dataset_cache", ")", "\n", "torch", ".", "save", "(", "encoded_dataset_dev", ",", "dataset_cache_dev", ")", "\n", "torch", ".", "save", "(", "encoded_dataset_test", ",", "dataset_cache_test", ")", "\n", "if", "args", ".", "use_silver_data", ":", "\n", "                    ", "torch", ".", "save", "(", "encoded_dataset_silver", ",", "dataset_cache_silver", ")", "\n", "\n", "", "", "", "logger", ".", "info", "(", "\"Using \"", "+", "dataset_name", ")", "\n", "if", "dataset_name", ".", "lower", "(", ")", "==", "\"train\"", ":", "\n", "            ", "encoded_dataset", "=", "encoded_dataset_train", "\n", "", "elif", "dataset_name", ".", "lower", "(", ")", "==", "\"dev\"", ":", "\n", "            ", "encoded_dataset", "=", "encoded_dataset_dev", "\n", "", "elif", "dataset_name", ".", "lower", "(", ")", "==", "\"test\"", ":", "\n", "            ", "encoded_dataset", "=", "encoded_dataset_test", "\n", "", "elif", "dataset_name", ".", "lower", "(", ")", "==", "\"silver\"", ":", "\n", "            ", "print", "(", "\"Encoding data set we are going to use is Silver\"", ")", "\n", "print", "(", "len", "(", "encoded_dataset_silver", ")", ")", "\n", "encoded_dataset", "=", "encoded_dataset_silver", "\n", "\n", "", "tensor_datasets", "=", "preproc_amr", "(", "args", ",", "tokenizer", ",", "encoded_dataset", ",", "with_text", "=", "True", ")", "+", "preproc_amr", "(", "args", ",", "tokenizer", ",", "encoded_dataset", ",", "with_text", "=", "False", ")", "\n", "\n", "logger", ".", "info", "(", "\"Build train and validation dataloaders\"", ")", "\n", "dataset", "=", "TensorDataset", "(", "*", "tensor_datasets", ")", "\n", "train_loader", "=", "DataLoader", "(", "dataset", ",", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "shuffle", ",", "pin_memory", "=", "True", ")", "\n", "\n", "", "else", ":", "\n", "        ", "tensor_datasets", "=", "get_datasets", "(", "args", ",", "tokenizer", ",", "with_question", "=", "True", ")", "+", "get_datasets", "(", "args", ",", "tokenizer", ",", "with_question", "=", "False", ")", "\n", "\n", "logger", ".", "info", "(", "\"Build train and validation dataloaders\"", ")", "\n", "train_dataset", "=", "TensorDataset", "(", "*", "tensor_datasets", ")", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "shuffle", ",", "pin_memory", "=", "True", ")", "\n", "\n", "logger", ".", "info", "(", "\"Train dataset (Batch, Seq length): {}\"", "\n", ".", "format", "(", "train_dataset", ".", "tensors", "[", "0", "]", ".", "shape", ")", ")", "\n", "", "return", "train_loader", "\n", "", ""]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.split_beams.argument_parsing": [[5, 36], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parsing", "(", ")", ":", "\n", "\n", "# Argument hanlding", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Splits output of beam decoding into individual files'", "\n", ")", "\n", "# jbinfo args", "\n", "parser", ".", "add_argument", "(", "\n", "'--in-text'", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "'One sentence per file beam sentences contiguous in beam'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--out-text'", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "'Sentences for one beam index. Needs %K in name to identify index'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--beam-size'", ",", "\n", "type", "=", "int", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "'size of the beam'", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "assert", "'%K'", "in", "args", ".", "out_text", ",", "\"--out-text name must contain %K to be replaced by beam index\"", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.preproces.argument_parser": [[7, 34], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parser", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Preprocess AMR data'", ")", "\n", "# Multiple input parameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-amr\"", ",", "\n", "help", "=", "\"input AMR file\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-amr\"", ",", "\n", "help", "=", "\"output (post-processed) AMR file\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-tokens\"", ",", "\n", "help", "=", "\"tokens from AMR\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--stog-fix\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Reformat AMR token to be parseable by publict stog\"", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.preproces.fix_tokens_file": [[36, 68], ["re.compile", "open", "re.compile.match", "line.rstrip", "tokens.append", "json.dumps", "new_amr.append", "new_amr.append", "token_line.match.groups"], "function", ["None"], ["", "def", "fix_tokens_file", "(", "file_path", ")", ":", "\n", "    ", "\"\"\"\n    Replace each\n\n    # ::tok sentence\n\n    by json parsable version\n\n    # ::token json-parseable-sentence\n\n    so that\n\n    sentence == json.loads(json-parseable-sentence)\n    \"\"\"", "\n", "\n", "token_line", "=", "re", ".", "compile", "(", "'^# ::tok (.*)'", ")", "\n", "\n", "# read and modifiy token lines", "\n", "new_amr", "=", "[", "]", "\n", "tokens", "=", "[", "]", "\n", "with", "open", "(", "file_path", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ":", "\n", "            ", "fetch", "=", "token_line", ".", "match", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "if", "fetch", ":", "\n", "                ", "sentence", "=", "fetch", ".", "groups", "(", ")", "[", "0", "]", "\n", "tokens", ".", "append", "(", "sentence", ")", "\n", "json_str", "=", "json", ".", "dumps", "(", "sentence", ")", "\n", "new_amr", ".", "append", "(", "f'# ::tokens {json_str}\\n'", ")", "\n", "", "else", ":", "\n", "                ", "new_amr", ".", "append", "(", "line", ")", "\n", "\n", "", "", "", "return", "new_amr", ",", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.rank_model.argument_parsing": [[10, 34], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parsing", "(", ")", ":", "\n", "\n", "# Argument hanlding", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Organize model results'", "\n", ")", "\n", "# jbinfo args", "\n", "parser", ".", "add_argument", "(", "\n", "'--checkpoints'", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "'DATA/models/'", ",", "\n", "help", "=", "'Folder containing checkpoints, config.sh etc'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--link-best'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'do not link or relink best smatch model'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--no-print'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'do not print'", "\n", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.rank_model.read_results": [[36, 44], ["open", "line.startswith", "float", "line.split"], "function", ["None"], ["", "def", "read_results", "(", "result_files", ")", ":", "\n", "    ", "scores", "=", "{", "score", ":", "None", "for", "score", "in", "SCORES", "}", "\n", "with", "open", "(", "result_files", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ":", "\n", "            ", "for", "score", "in", "SCORES", ":", "\n", "                ", "if", "line", ".", "startswith", "(", "score", ")", ":", "\n", "                    ", "scores", "[", "score", "]", "=", "float", "(", "line", ".", "split", "(", ")", "[", "1", "]", ")", "\n", "", "", "", "", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.rank_model.collect_results": [[46, 110], ["re.compile", "os.walk", "enumerate", "re.compile.match", "os.listdir", "print", "print", "re.compile.match().groups", "epoch_folders.append", "model_folders.append", "results_re.match", "items.append", "empty_folders.append", "print", "results_regex.match().groups", "epochs.append", "rank_model.read_results", "re.compile.match", "int", "os.path.join", "int", "sorted", "results_regex.match"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.rank_model.read_results"], ["", "def", "collect_results", "(", "args", ",", "results_regex", ",", "main_score", ")", ":", "\n", "\n", "# regex of decoding output", "\n", "    ", "result_path_re", "=", "re", ".", "compile", "(", "\n", "f'{args.checkpoints}/?([^/]+)/sampling/([^/]+)'", "\n", ")", "\n", "\n", "# Find folders containing results for all epochs for each given model", "\n", "epoch_folders", "=", "[", "]", "\n", "model_folders", "=", "[", "]", "\n", "for", "path_pieces", "in", "os", ".", "walk", "(", "args", ".", "checkpoints", ")", ":", "\n", "        ", "root", "=", "path_pieces", "[", "0", "]", "\n", "if", "result_path_re", ".", "match", "(", "root", ")", ":", "\n", "            ", "model_tag", ",", "test_tag", "=", "result_path_re", ".", "match", "(", "root", ")", ".", "groups", "(", ")", "\n", "epoch_folders", ".", "append", "(", "\n", "f'{args.checkpoints}/{model_tag}/sampling/{test_tag}'", "\n", ")", "\n", "model_folders", ".", "append", "(", "f'{args.checkpoints}/{model_tag}'", ")", "\n", "\n", "# loop over each model, loop over results for each epoch, extract results", "\n", "# and keep the best BLEU from all epochs", "\n", "", "", "items", "=", "[", "]", "\n", "empty_folders", "=", "[", "]", "\n", "for", "index", ",", "epoch_folder", "in", "enumerate", "(", "epoch_folders", ")", ":", "\n", "\n", "# loop each file, I it is an epoch result read it and keep best results", "\n", "        ", "best_results", "=", "{", "score", ":", "None", "for", "score", "in", "SCORES", "}", "\n", "best_epoch", "=", "None", "\n", "epochs", "=", "[", "]", "\n", "for", "basename", "in", "os", ".", "listdir", "(", "epoch_folder", ")", ":", "\n", "            ", "if", "results_re", ".", "match", "(", "basename", ")", ":", "\n", "                ", "name", ",", "epoch", "=", "results_regex", ".", "match", "(", "basename", ")", ".", "groups", "(", ")", "\n", "epochs", ".", "append", "(", "int", "(", "epoch", ")", ")", "\n", "#if 'beam' in epoch_folder:", "\n", "#    import ipdb; ipdb.set_trace(context=30)", "\n", "results", "=", "read_results", "(", "os", ".", "path", ".", "join", "(", "epoch_folder", ",", "basename", ")", ")", "\n", "if", "(", "\n", "best_results", "[", "main_score", "]", "is", "None", "or", "\n", "results", "[", "main_score", "]", ">", "best_results", "[", "main_score", "]", "\n", ")", ":", "\n", "                    ", "best_results", "=", "results", "\n", "best_epoch", "=", "int", "(", "epoch", ")", "\n", "\n", "", "", "", "if", "best_results", "[", "main_score", "]", "is", "not", "None", ":", "\n", "# Store data", "\n", "            ", "items", ".", "append", "(", "{", "\n", "'model_folder'", ":", "model_folders", "[", "index", "]", ",", "\n", "'results_folder'", ":", "epoch_folder", ",", "\n", "f'best_{main_score}_epoch'", ":", "best_epoch", ",", "\n", "'epochs'", ":", "sorted", "(", "epochs", ")", "\n", "}", ")", "\n", "# Add all scores", "\n", "for", "score", "in", "SCORES", ":", "\n", "                ", "items", "[", "-", "1", "]", "[", "score", "]", "=", "best_results", "[", "score", "]", "\n", "", "", "else", ":", "\n", "            ", "empty_folders", ".", "append", "(", "epoch_folder", ")", "\n", "\n", "", "", "if", "empty_folders", ":", "\n", "        ", "print", "(", "'Empty folders'", ")", "\n", "for", "f", "in", "empty_folders", ":", "\n", "            ", "print", "(", "f", ")", "\n", "", "print", "(", "\"\"", ")", "\n", "\n", "", "return", "items", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.rank_model.print_table": [[112, 131], ["sorted", "rank_model.ptable", "row.append", "row.append", "rows.append", "len", "max", "row.append", "row.append", "item[].split"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.rank_model.ptable"], ["", "def", "print_table", "(", "args", ",", "items", ",", "results_re_pattern", ",", "main_score", ")", ":", "\n", "    ", "widths", "=", "[", "]", "\n", "rows", "=", "[", "[", "'experiment name'", ",", "'best epoch'", "]", "+", "SCORES", "]", "\n", "centering", "=", "[", "'<'", ",", "'^'", "]", "+", "[", "'^'", "]", "*", "len", "(", "SCORES", ")", "\n", "for", "item", "in", "sorted", "(", "items", ",", "key", "=", "lambda", "x", ":", "x", "[", "main_score", "]", ")", ":", "\n", "        ", "row", "=", "[", "]", "\n", "row", ".", "append", "(", "item", "[", "'results_folder'", "]", ".", "split", "(", "args", ".", "checkpoints", ")", "[", "1", "]", "[", "1", ":", "]", ")", "\n", "row", ".", "append", "(", "\n", "'{}/{}'", ".", "format", "(", "item", "[", "f'best_{main_score}_epoch'", "]", ",", "\n", "max", "(", "item", "[", "'epochs'", "]", ")", ")", "\n", ")", "\n", "for", "score", "in", "SCORES", ":", "\n", "            ", "if", "item", "[", "score", "]", "is", "None", ":", "\n", "                ", "row", ".", "append", "(", "' '", ")", "\n", "", "else", ":", "\n", "                ", "row", ".", "append", "(", "'{:0.1f}'", ".", "format", "(", "item", "[", "score", "]", ")", ")", "\n", "", "", "rows", ".", "append", "(", "row", ")", "\n", "\n", "", "ptable", "(", "rows", ",", "centering", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.rank_model.ptable": [[133, 158], ["len", "re.compile", "enumerate", "print", "print", "max", "enumerate", "table_str.append", "row_sep.join", "range", "row_str.append", "col_sep.join", "len", "len", "len", "re.compile.sub", "re.compile.sub"], "function", ["None"], ["", "def", "ptable", "(", "rows", ",", "centering", ")", ":", "\n", "\n", "    ", "num_columns", "=", "len", "(", "rows", "[", "0", "]", ")", "\n", "# bash scape chars (used for formatting, have length 0 on display)", "\n", "BASH_SCAPE", "=", "re", ".", "compile", "(", "'\\\\x1b\\[\\d+m|\\\\x1b\\[0m'", ")", "\n", "column_widths", "=", "[", "max", "(", "[", "len", "(", "BASH_SCAPE", ".", "sub", "(", "''", ",", "row", "[", "i", "]", ")", ")", "for", "row", "in", "rows", "]", ")", "for", "i", "in", "range", "(", "num_columns", ")", "]", "\n", "\n", "table_str", "=", "[", "]", "\n", "col_sep", "=", "' '", "\n", "for", "i", ",", "row", "in", "enumerate", "(", "rows", ")", ":", "\n", "        ", "row_str", "=", "[", "]", "\n", "for", "j", ",", "cell", "in", "enumerate", "(", "row", ")", ":", "\n", "# need to discount for bash scape chars", "\n", "            ", "delta", "=", "len", "(", "cell", ")", "-", "len", "(", "BASH_SCAPE", ".", "sub", "(", "''", ",", "cell", ")", ")", "\n", "if", "i", "==", "0", ":", "\n", "# Header has all cells centered", "\n", "                ", "align", "=", "'^'", "\n", "", "else", ":", "\n", "                ", "align", "=", "centering", "[", "j", "]", "\n", "", "row_str", ".", "append", "(", "'{:{align}{width}} '", ".", "format", "(", "cell", ",", "align", "=", "align", ",", "width", "=", "column_widths", "[", "j", "]", "+", "delta", ")", ")", "\n", "", "table_str", ".", "append", "(", "col_sep", ".", "join", "(", "row_str", ")", ")", "\n", "\n", "", "row_sep", "=", "'\\n'", "\n", "print", "(", "row_sep", ".", "join", "(", "table_str", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.postprocess.argument_parser": [[7, 36], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parser", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Preprocess AMR data'", ")", "\n", "# Multiple input parameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-tokens\"", ",", "\n", "help", "=", "\"input tokens\"", ",", "\n", "required", "=", "True", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-amr\"", ",", "\n", "help", "=", "\"input AMR file\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-tokens\"", ",", "\n", "help", "=", "\"tokens from AMR\"", ",", "\n", "required", "=", "True", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--remove-repetitions\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Remove repetitions\"", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.postprocess.tokenize_sentence": [[38, 45], ["re.sub", "re.sub", "print", "print", "re.sub.replace"], "function", ["None"], ["", "def", "tokenize_sentence", "(", "text", ",", "debug", "=", "False", ")", ":", "\n", "#debug = True", "\n", "    ", "if", "debug", ":", "print", "(", "text", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"('ll|n't|'m|'s|'d|'re)\"", ",", "r\" \\1\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"(\\s+)\"", ",", "r\" \"", ",", "text", ")", "\n", "if", "debug", ":", "print", "(", "text", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.postprocess.remove_rep": [[47, 112], ["text.split", "reversed", "len", "len", "len", "len", "len", "len", "len", "ipdb.set_trace", "len", "s_one_gram[].start", "re.finditer", "re.finditer", "re.finditer", "len", "len", "len", "len", "s_one_gram[].start", "re.escape", "re.escape", "re.escape", "s_one_gram[].start", "len", "len", "len", "len", "s_one_gram[].start", "s_one_gram[].start", "len", "len", "s_one_gram[].start"], "function", ["None"], ["", "def", "remove_rep", "(", "text", ")", ":", "\n", "    ", "pattern", "=", "\"\"", "\n", "stext", "=", "text", ".", "split", "(", "\" \"", ")", "\n", "if", "len", "(", "stext", ")", "<", "10", ":", "\n", "        ", "return", "text", "\n", "", "one_gram", "=", "\"\"", "\n", "two_gram", "=", "\"\"", "\n", "three_gram", "=", "\"\"", "\n", "cut", "=", "False", "\n", "cutting_point", "=", "0", "\n", "last", "=", "0", "\n", "current", "=", "\"\"", "\n", "for", "w", "in", "reversed", "(", "stext", ")", ":", "\n", "        ", "current", "+=", "w", "+", "\" \"", "+", "w", "\n", "three_gram", "=", "w", "+", "\" \"", "+", "two_gram", "\n", "two_gram", "=", "w", "+", "\" \"", "+", "one_gram", "\n", "one_gram", "=", "w", "\n", "try", ":", "\n", "            ", "s_one_gram", "=", "[", "m", "for", "m", "in", "re", ".", "finditer", "(", "re", ".", "escape", "(", "one_gram", ")", ",", "text", ")", "]", "\n", "s_two_gram", "=", "[", "m", "for", "m", "in", "re", ".", "finditer", "(", "re", ".", "escape", "(", "two_gram", ")", ",", "text", ")", "]", "\n", "s_three_gram", "=", "[", "m", "for", "m", "in", "re", ".", "finditer", "(", "re", ".", "escape", "(", "three_gram", ")", ",", "text", ")", "]", "\n", "", "except", ":", "\n", "            ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", "context", "=", "5", ")", "\n", "", "prop_one_gram", "=", "len", "(", "s_one_gram", ")", "/", "len", "(", "stext", ")", "\n", "prop_two_gram", "=", "len", "(", "s_two_gram", ")", "/", "len", "(", "stext", ")", "\n", "prop_three_gram", "=", "len", "(", "s_three_gram", ")", "/", "len", "(", "stext", ")", "\n", "\n", "if", "prop_three_gram", ">", "0.03", "and", "len", "(", "s_three_gram", ")", ">", "4", ":", "\n", "#print(s_three_gram)", "\n", "#print(s_three_gram[1].group(), s_one_gram[1].start(), last, len(text)-len(current))", "\n", "            ", "if", "last", ">", "len", "(", "text", ")", "-", "len", "(", "current", ")", ":", "\n", "#print(text)", "\n", "#print(prop_three_gram, \"-\"*80)", "\n", "#print(\"-\"*80)", "\n", "#print(\"THREE suggested cut:\", text[:s_one_gram[1].start()])", "\n", "#input()", "\n", "                ", "return", "text", "[", ":", "s_one_gram", "[", "1", "]", ".", "start", "(", ")", "]", "\n", "", "if", "not", "cut", ":", "\n", "                ", "cut", "=", "True", "\n", "last", "=", "s_one_gram", "[", "-", "2", "]", ".", "start", "(", ")", "\n", "", "", "elif", "prop_two_gram", ">", "0.04", "and", "len", "(", "s_two_gram", ")", ">", "6", "and", "len", "(", "s_two_gram", ")", ">", "3", ":", "\n", "#print(s_two_gram)", "\n", "#print(s_two_gram[1].group(), s_one_gram[1].start(), last, len(text)-len(current))", "\n", "            ", "if", "last", ">", "len", "(", "text", ")", "-", "len", "(", "current", ")", ":", "\n", "#print(text)", "\n", "#print(prop_two_gram, \"-\"*80)", "\n", "#print(\"TWO suggested cut:\", text[:s_one_gram[1].start()])", "\n", "#input()", "\n", "                ", "return", "text", "[", ":", "s_one_gram", "[", "1", "]", ".", "start", "(", ")", "]", "\n", "", "if", "not", "cut", ":", "\n", "                ", "cut", "=", "True", "\n", "last", "=", "s_one_gram", "[", "-", "2", "]", ".", "start", "(", ")", "\n", "", "", "elif", "prop_one_gram", ">", "0.2", "and", "len", "(", "s_one_gram", ")", ">", "10", "and", "len", "(", "w", ")", ">", "2", "and", "w", "not", "in", "string", ".", "punctuation", ":", "\n", "#print(s_one_gram[1].group(), s_one_gram[1].start(), last, len(text)-len(current))", "\n", "#print(cut, last)", "\n", "            ", "if", "last", ">", "len", "(", "text", ")", "-", "len", "(", "current", ")", ":", "\n", "#print(text)", "\n", "#print(prop_one_gram, \"-\"*80)", "\n", "#print(\"ONE suggested cut:\", text[:s_one_gram[1].start()])", "\n", "#input()", "\n", "                ", "return", "text", "[", ":", "s_one_gram", "[", "1", "]", ".", "start", "(", ")", "]", "\n", "", "if", "not", "cut", ":", "\n", "                ", "cut", "=", "True", "\n", "last", "=", "s_one_gram", "[", "-", "2", "]", ".", "start", "(", ")", "\n", "", "", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.postprocess.preproc_doc": [[114, 123], ["new_text.append", "tokenize_sentence.lower", "postprocess.tokenize_sentence", "postprocess.remove_rep"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.eval_sacrebleu.tokenize_sentence", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.remove_rep.remove_rep"], ["", "def", "preproc_doc", "(", "text", ",", "lower_case", "=", "True", ",", "tokenize", "=", "True", ")", ":", "\n", "    ", "new_text", "=", "[", "]", "\n", "for", "line", "in", "text", ":", "\n", "        ", "if", "lower_case", ":", "\n", "            ", "line", "=", "line", ".", "lower", "(", ")", "\n", "", "if", "tokenize", ":", "\n", "            ", "line", "=", "tokenize_sentence", "(", "line", ")", "\n", "", "new_text", ".", "append", "(", "remove_rep", "(", "line", ")", ")", "\n", "", "return", "new_text", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.remove_rep.tokenize_sentence": [[5, 12], ["re.sub", "re.sub", "print", "print", "re.sub.replace"], "function", ["None"], ["def", "tokenize_sentence", "(", "text", ",", "debug", "=", "False", ")", ":", "\n", "#debug = True", "\n", "    ", "if", "debug", ":", "print", "(", "text", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"('ll|n't|'m|'s|'d|'re)\"", ",", "r\" \\1\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"(\\s+)\"", ",", "r\" \"", ",", "text", ")", "\n", "if", "debug", ":", "print", "(", "text", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.remove_rep.remove_rep": [[14, 79], ["text.split", "reversed", "len", "len", "len", "len", "len", "len", "len", "ipdb.set_trace", "len", "s_one_gram[].start", "re.finditer", "re.finditer", "re.finditer", "len", "len", "len", "len", "s_one_gram[].start", "re.escape", "re.escape", "re.escape", "s_one_gram[].start", "len", "len", "len", "len", "s_one_gram[].start", "s_one_gram[].start", "len", "len", "s_one_gram[].start"], "function", ["None"], ["", "def", "remove_rep", "(", "text", ")", ":", "\n", "    ", "pattern", "=", "\"\"", "\n", "stext", "=", "text", ".", "split", "(", "\" \"", ")", "\n", "if", "len", "(", "stext", ")", "<", "10", ":", "\n", "        ", "return", "text", "\n", "", "one_gram", "=", "\"\"", "\n", "two_gram", "=", "\"\"", "\n", "three_gram", "=", "\"\"", "\n", "cut", "=", "False", "\n", "cutting_point", "=", "0", "\n", "last", "=", "0", "\n", "current", "=", "\"\"", "\n", "for", "w", "in", "reversed", "(", "stext", ")", ":", "\n", "        ", "current", "+=", "w", "+", "\" \"", "+", "w", "\n", "three_gram", "=", "w", "+", "\" \"", "+", "two_gram", "\n", "two_gram", "=", "w", "+", "\" \"", "+", "one_gram", "\n", "one_gram", "=", "w", "\n", "try", ":", "\n", "            ", "s_one_gram", "=", "[", "m", "for", "m", "in", "re", ".", "finditer", "(", "re", ".", "escape", "(", "one_gram", ")", ",", "text", ")", "]", "\n", "s_two_gram", "=", "[", "m", "for", "m", "in", "re", ".", "finditer", "(", "re", ".", "escape", "(", "two_gram", ")", ",", "text", ")", "]", "\n", "s_three_gram", "=", "[", "m", "for", "m", "in", "re", ".", "finditer", "(", "re", ".", "escape", "(", "three_gram", ")", ",", "text", ")", "]", "\n", "", "except", ":", "\n", "            ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", "context", "=", "5", ")", "\n", "", "prop_one_gram", "=", "len", "(", "s_one_gram", ")", "/", "len", "(", "stext", ")", "\n", "prop_two_gram", "=", "len", "(", "s_two_gram", ")", "/", "len", "(", "stext", ")", "\n", "prop_three_gram", "=", "len", "(", "s_three_gram", ")", "/", "len", "(", "stext", ")", "\n", "\n", "if", "prop_three_gram", ">", "0.03", "and", "len", "(", "s_three_gram", ")", ">", "4", ":", "\n", "#print(s_three_gram)", "\n", "#print(s_three_gram[1].group(), s_one_gram[1].start(), last, len(text)-len(current))", "\n", "            ", "if", "last", ">", "len", "(", "text", ")", "-", "len", "(", "current", ")", ":", "\n", "#print(text)", "\n", "#print(prop_three_gram, \"-\"*80)", "\n", "#print(\"-\"*80)", "\n", "#print(\"THREE suggested cut:\", text[:s_one_gram[1].start()])", "\n", "#input()", "\n", "                ", "return", "text", "[", ":", "s_one_gram", "[", "1", "]", ".", "start", "(", ")", "]", "\n", "", "if", "not", "cut", ":", "\n", "                ", "cut", "=", "True", "\n", "last", "=", "s_one_gram", "[", "-", "2", "]", ".", "start", "(", ")", "\n", "", "", "elif", "prop_two_gram", ">", "0.04", "and", "len", "(", "s_two_gram", ")", ">", "6", "and", "len", "(", "s_two_gram", ")", ">", "3", ":", "\n", "#print(s_two_gram)", "\n", "#print(s_two_gram[1].group(), s_one_gram[1].start(), last, len(text)-len(current))", "\n", "            ", "if", "last", ">", "len", "(", "text", ")", "-", "len", "(", "current", ")", ":", "\n", "#print(text)", "\n", "#print(prop_two_gram, \"-\"*80)", "\n", "#print(\"TWO suggested cut:\", text[:s_one_gram[1].start()])", "\n", "#input()", "\n", "                ", "return", "text", "[", ":", "s_one_gram", "[", "1", "]", ".", "start", "(", ")", "]", "\n", "", "if", "not", "cut", ":", "\n", "                ", "cut", "=", "True", "\n", "last", "=", "s_one_gram", "[", "-", "2", "]", ".", "start", "(", ")", "\n", "", "", "elif", "prop_one_gram", ">", "0.2", "and", "len", "(", "s_one_gram", ")", ">", "10", "and", "len", "(", "w", ")", ">", "2", "and", "w", "not", "in", "string", ".", "punctuation", ":", "\n", "#print(s_one_gram[1].group(), s_one_gram[1].start(), last, len(text)-len(current))", "\n", "#print(cut, last)", "\n", "            ", "if", "last", ">", "len", "(", "text", ")", "-", "len", "(", "current", ")", ":", "\n", "#print(text)", "\n", "#print(prop_one_gram, \"-\"*80)", "\n", "#print(\"ONE suggested cut:\", text[:s_one_gram[1].start()])", "\n", "#input()", "\n", "                ", "return", "text", "[", ":", "s_one_gram", "[", "1", "]", ".", "start", "(", ")", "]", "\n", "", "if", "not", "cut", ":", "\n", "                ", "cut", "=", "True", "\n", "last", "=", "s_one_gram", "[", "-", "2", "]", ".", "start", "(", ")", "\n", "", "", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.remove_rep.preproc_doc": [[80, 91], ["open().read().splitlines", "list", "list.append", "open().read", "tokenize_sentence.lower", "remove_rep.tokenize_sentence", "remove_rep.remove_rep", "open"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.eval_sacrebleu.tokenize_sentence", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.amr2txt.remove_rep.remove_rep"], ["", "def", "preproc_doc", "(", "file_name", ",", "lower_case", "=", "True", ",", "tokenize", "=", "True", ")", ":", "\n", "    ", "text", "=", "open", "(", "file_name", ")", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "output", "=", "list", "(", ")", "\n", "for", "line", "in", "text", ":", "\n", "        ", "if", "lower_case", ":", "\n", "            ", "line", "=", "line", ".", "lower", "(", ")", "\n", "", "if", "tokenize", ":", "\n", "            ", "line", "=", "tokenize_sentence", "(", "line", ")", "\n", "", "output", ".", "append", "(", "remove_rep", "(", "line", ")", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.tokenize.tokenize_sentence": [[4, 9], ["re.sub", "re.sub", "re.sub.strip"], "function", ["None"], ["def", "tokenize_sentence", "(", "text", ",", "debug", "=", "False", ")", ":", "\n", "    ", "text", "=", "re", ".", "sub", "(", "r\"('ll|n't|'m|'s|'d|'re)\"", ",", "r\" \\1\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"(\\s+)\"", ",", "r\" \"", ",", "text", ")", "\n", "\n", "return", "text", ".", "strip", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.eval_sacrebleu.argument_parser": [[8, 32], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parser", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Preprocess AMR data'", ")", "\n", "# Multiple input parameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-tokens\"", ",", "\n", "help", "=", "\"input tokens\"", ",", "\n", "required", "=", "True", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-reference-tokens\"", ",", "\n", "help", "=", "\"refrence tokens to compute metric\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-results\"", ",", "\n", "help", "=", "\"tokens from AMR\"", ",", "\n", "required", "=", "True", ",", "\n", "type", "=", "str", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.eval_sacrebleu.tokenize_sentence": [[34, 38], ["re.sub", "re.sub"], "function", ["None"], ["", "def", "tokenize_sentence", "(", "text", ",", "debug", "=", "False", ")", ":", "\n", "    ", "text", "=", "re", ".", "sub", "(", "r\"('ll|n't|'m|'s|'d|'re)\"", ",", "r\" \\1\"", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"(\\s+)\"", ",", "r\" \"", ",", "text", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.eval_sacrebleu.raw_corpus_bleu": [[40, 46], ["sacrebleu.corpus_bleu"], "function", ["None"], ["", "def", "raw_corpus_bleu", "(", "hypothesis", ":", "Iterable", "[", "str", "]", ",", "reference", ":", "Iterable", "[", "str", "]", ",", "\n", "offset", ":", "Optional", "[", "float", "]", "=", "0.01", ")", "->", "float", ":", "\n", "    ", "bleu", "=", "sacrebleu", ".", "corpus_bleu", "(", "hypothesis", ",", "reference", ",", "smooth_value", "=", "offset", ",", "\n", "force", "=", "True", ",", "use_effective_order", "=", "False", ",", "\n", "lowercase", "=", "True", ")", "\n", "return", "bleu", ".", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.eval_sacrebleu.raw_corpus_chrf": [[48, 54], ["sacrebleu.corpus_chrf"], "function", ["None"], ["", "def", "raw_corpus_chrf", "(", "hypotheses", ":", "Iterable", "[", "str", "]", ",", "\n", "references", ":", "Iterable", "[", "str", "]", ")", "->", "float", ":", "\n", "    ", "return", "sacrebleu", ".", "corpus_chrf", "(", "hypotheses", ",", "references", ",", "\n", "order", "=", "sacrebleu", ".", "CHRF_ORDER", ",", "\n", "beta", "=", "sacrebleu", ".", "CHRF_BETA", ",", "\n", "remove_whitespace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.eval_sacrebleu.read_tokens": [[55, 59], ["open", "fid.readlines"], "function", ["None"], ["", "def", "read_tokens", "(", "in_tokens_file", ")", ":", "\n", "    ", "with", "open", "(", "in_tokens_file", ")", "as", "fid", ":", "\n", "        ", "lines", "=", "fid", ".", "readlines", "(", ")", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.Tensor": [[6, 12], ["torch.tensor", "torch.tensor", "torch.device"], "function", ["None"], ["from", "collections", "import", "defaultdict", "\n", "\n", "import", "torch", "\n", "from", "torch", ".", "utils", ".", "data", "import", "DataLoader", ",", "TensorDataset", "\n", "import", "numpy", "as", "np", "\n", "from", "constants", "import", "SPECIAL_TOKENS", ",", "MODEL_INPUTS", ",", "PADDED_INPUTS", "\n", "from", "amr_utils", "import", "read_amr", ",", "load_amr", ",", "update_model", ",", "tokenize_amr", ","]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.LongTensor": [[14, 20], ["torch.tensor", "torch.tensor", "torch.device"], "function", ["None"], ["\n", "from", "constants", "import", "SPECIAL_TOKENS_BERT", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__file__", ")", "\n", "eps", "=", "np", ".", "finfo", "(", "np", ".", "float32", ")", ".", "eps", ".", "item", "(", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.zeros": [[22, 28], ["torch.zeros", "torch.zeros", "torch.device"], "function", ["home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.zeros", "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.zeros"], ["    ", "\"\"\"dot.notation access to dictionary attributes\"\"\"", "\n", "__getattr__", "=", "dict", ".", "get", "\n", "__setattr__", "=", "dict", ".", "__setitem__", "\n", "__delattr__", "=", "dict", ".", "__delitem__", "\n", "\n", "\n", "", "def", "find_sub_list", "(", "sl", ",", "l", ")", ":", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.maskset": [[30, 33], ["x.data.eq", "x.size", "x.data.eq.sum"], "function", ["None"], ["sll", "=", "len", "(", "sl", ")", "\n", "for", "ind", "in", "(", "i", "for", "i", ",", "e", "in", "enumerate", "(", "l", ")", "if", "e", "==", "sl", "[", "0", "]", ")", ":", "\n", "        ", "if", "l", "[", "ind", ":", "ind", "+", "sll", "]", "==", "sl", ":", "\n", "            ", "return", "ind", ",", "ind", "+", "sll", "-", "1", "\n"]], "home.repos.pwc.inspect_result.IBM_GPT-too-AMR2text.extra.utils.pause_or_exit": [[35, 39], ["input", "input.lower", "sys.exit"], "function", ["None"], ["\n", "\n", "", "def", "get_best_indexes", "(", "logits", ",", "n_best_size", "=", "1", ")", ":", "\n", "    ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "\n"]]}