{"home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.training_step.data_training": [[3, 27], ["numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "function", ["None"], ["def", "data_training", "(", "replay_memory", ")", ":", "\n", "    ", "\"\"\"\n    Function to extract and prepare the information from the replay buffer for the model training\n    \n    Arguments:\n    ----------\n    replay_memory: \n        Dataset with finite buffer \n\n    Returns:\n    --------\n    states: array\n        array containing all the states visited in the last steps, according to the buffer lenght.\n    actions: array\n        array containing the action taken at each state\n    rewards: array\n        reward received for each tuple state-action\n    next_states: array\n        next states resulted from each tuple state-action\n    dones: bool\n        whether or not the episode is over (it is a superfluous information since the lenght is set in main.py).\n    \"\"\"", "\n", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", "=", "np", ".", "array", "(", "replay_memory", ")", "[", ":", ",", "0", "]", ",", "np", ".", "array", "(", "replay_memory", ")", "[", ":", ",", "1", "]", ",", "np", ".", "array", "(", "replay_memory", ")", "[", ":", ",", "2", "]", ",", "np", ".", "array", "(", "replay_memory", ")", "[", ":", ",", "3", "]", ",", "np", ".", "array", "(", "replay_memory", ")", "[", ":", ",", "4", "]", "\n", "return", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.training_step.training_step": [[28, 67], ["training_step.data_training", "numpy.concatenate", "states.reshape.reshape", "numpy.concatenate().reshape", "numpy.concatenate", "numpy.concatenate", "next_states.reshape.reshape", "model.train", "model.fit", "numpy.concatenate"], "function", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.training_step.data_training", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.train", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.fit"], ["", "def", "training_step", "(", "replay_memory", ",", "model", ",", "ensemble", ",", "obs_space", ",", "action_space", ")", ":", "\n", "    ", "\"\"\"\n    Function to train the model.\n\n    The relevant information are extracted from the replay buffer, states and actions are the inputs to predict the next states. \n    The model is not trained to predict the next states directly, but rather the difference from the next state and the current state: in this way the performance has been proved to be more stable (Deisenroth, 2017).\n\n    Arguments:\n    ----------\n    replay_memory: \n        dataset with finite buffer, it is passed to data_training\n    model:\n        model to be trained\n    ensemble: bool\n        boolean for code compatibility\n    action_space:\n        observation space of the environment to reshape the array\n    obs_space:\n        action space of the environment to reshape the array\n\n    Returns:\n    --------\n    model: \n        model after training\n    \"\"\"", "\n", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", "=", "data_training", "(", "replay_memory", ")", "\n", "batch_size", "=", "states", ".", "shape", "[", "0", "]", "\n", "states", "=", "np", ".", "concatenate", "(", "states", ")", "\n", "states", "=", "states", ".", "reshape", "(", "batch_size", ",", "obs_space", ")", "\n", "actions", "=", "np", ".", "concatenate", "(", "actions", ")", ".", "reshape", "(", "batch_size", ",", "action_space", ")", "\n", "data", "=", "np", ".", "concatenate", "(", "(", "states", ",", "actions", ")", ",", "axis", "=", "1", ")", "\n", "next_states", "=", "np", ".", "concatenate", "(", "next_states", ")", "\n", "next_states", "=", "next_states", ".", "reshape", "(", "batch_size", ",", "obs_space", ")", "\n", "delta_states", "=", "next_states", "-", "states", "\n", "if", "ensemble", ":", "\n", "        ", "model", ".", "train", "(", "data", ",", "delta_states", ")", "\n", "", "else", ":", "\n", "        ", "model", ".", "fit", "(", "data", ",", "delta_states", ")", "\n", "", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.main.main": [[10, 119], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "vars", "MB_trainer.MB_Trainer", "MB_trainer.MB_Trainer", "MB_trainer.MB_Trainer", "MB_trainer.MB_Trainer", "MB_trainer.MB_Trainer", "multiprocessing.Process", "multiprocessing.Process", "multiprocessing.Process", "multiprocessing.Process", "multiprocessing.Process", "multiprocessing.Process.start", "multiprocessing.Process.start", "multiprocessing.Process.start", "multiprocessing.Process.start", "multiprocessing.Process.start", "MB_trainer.MB_Trainer", "MB_trainer.MB_Trainer", "MB_trainer.MB_Trainer", "MB_trainer.MB_Trainer", "MB_trainer.MB_Trainer", "multiprocessing.Process", "multiprocessing.Process", "multiprocessing.Process", "multiprocessing.Process", "multiprocessing.Process", "multiprocessing.Process.start", "multiprocessing.Process.start", "multiprocessing.Process.start", "multiprocessing.Process.start", "multiprocessing.Process.start"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "    ", "\"\"\"\n        Function to set the parameters for the RL algorithm. \n        \n        The function takes all the arguments and converts them to a dictionary params, which is passed to the class MB_trainer that initializes all the parameters, creates the environment and the model to run, and starts the RL algorithm through run_training_loop().\n        \n        Arguments:\n        ----------\n        env_name: str\n            Name of the environment to create\n        ep_len: int\n            lenght of each episode (steps for episode). No need to be specified since it is initialized depending on the env_name\n        n_iter: int\n            number of episode to run for each seed\n        model: str\n            name of the model to run, it is in ['deterministicNN', 'deterministic_ensemble', 'probabilisticNN', 'probabilistic_ensemble', 'dropoutNN', 'dropout_ensemble', 'gaussian_process']\n        ensemble_size: int\n            number of NNs in the ensemble\n        mpc_horizon: int\n            planning horizon of each trajectory in the random shooting algorithm \n        mpc_num_action_sequences: int\n            number of different trajectories in the random shooting algorithm\n        learning_rate: float\n            learning rate of the NN\n        n_layers: int\n            number of hidden layers of the model\n        n_hidden: int\n            number of units/neurons of each hidden layer\n        activation_in: str\n            activation function of hidden layers: for 'deterministicNN' and 'deterministic_ensemble' choose 'relu' or 'elu', \n                                                  for 'probabilisticNN' and 'probabilistic_ensemble' choose 'relu', 'tanh' or 'sigmoid',\n                                                  for 'dropoutNN' and 'dropout_ensemble' choose 'relu'\n        epochs: int\n            epochs of training\n        replay_buffer: int\n            max lenght of the dataset (replay_memory) the model is trained on\n        seed: int\n            number of the seed to start\n        nb_seeds: int\n             number of different seeds to run\n        \"\"\"", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--env_name'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--ep_len'", ",", "type", "=", "int", ",", "default", "=", "200", ")", "\n", "parser", ".", "add_argument", "(", "'--n_iter'", ",", "'-n'", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--model1'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--model2'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--ensemble_size'", ",", "'-e'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--mpc_horizon'", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "parser", ".", "add_argument", "(", "'--mpc_num_action_sequences'", ",", "type", "=", "int", ",", "default", "=", "500", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "'-lr'", ",", "type", "=", "float", ",", "default", "=", "0.001", ")", "\n", "parser", ".", "add_argument", "(", "'--n_layers'", ",", "'-l'", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "'--n_hidden'", ",", "'-s'", ",", "type", "=", "int", ",", "default", "=", "40", ")", "\n", "parser", ".", "add_argument", "(", "'--activation_in'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "30", ")", "\n", "parser", ".", "add_argument", "(", "'--replay_buffer'", ",", "type", "=", "int", ",", "default", "=", "2000", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "#parser.add_argument('--nb_seeds', type=int, default=1) ", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# convert to dictionary", "\n", "params", "=", "vars", "(", "args", ")", "\n", "\n", "\n", "###################", "\n", "### RUN TRAINING", "\n", "###################", "\n", "\n", "trainer1", "=", "MB_Trainer", "(", "params", ")", "\n", "trainer2", "=", "MB_Trainer", "(", "params", ")", "\n", "trainer3", "=", "MB_Trainer", "(", "params", ")", "\n", "trainer4", "=", "MB_Trainer", "(", "params", ")", "\n", "trainer5", "=", "MB_Trainer", "(", "params", ")", "\n", "\n", "t1", "=", "Process", "(", "target", "=", "trainer1", ".", "run_training_loop", ",", "args", "=", "(", "42", ",", "params", "[", "'model1'", "]", ")", ")", "\n", "t2", "=", "Process", "(", "target", "=", "trainer2", ".", "run_training_loop", ",", "args", "=", "(", "43", ",", "params", "[", "'model1'", "]", ")", ")", "\n", "t3", "=", "Process", "(", "target", "=", "trainer3", ".", "run_training_loop", ",", "args", "=", "(", "44", ",", "params", "[", "'model1'", "]", ")", ")", "\n", "t4", "=", "Process", "(", "target", "=", "trainer4", ".", "run_training_loop", ",", "args", "=", "(", "45", ",", "params", "[", "'model1'", "]", ")", ")", "\n", "t5", "=", "Process", "(", "target", "=", "trainer5", ".", "run_training_loop", ",", "args", "=", "(", "46", ",", "params", "[", "'model1'", "]", ")", ")", "\n", "\n", "t1", ".", "start", "(", ")", "\n", "t2", ".", "start", "(", ")", "\n", "t3", ".", "start", "(", ")", "\n", "t4", ".", "start", "(", ")", "\n", "t5", ".", "start", "(", ")", "\n", "\n", "if", "params", "[", "'model2'", "]", "is", "not", "None", ":", "\n", "        ", "trainer6", "=", "MB_Trainer", "(", "params", ")", "\n", "trainer7", "=", "MB_Trainer", "(", "params", ")", "\n", "trainer8", "=", "MB_Trainer", "(", "params", ")", "\n", "trainer9", "=", "MB_Trainer", "(", "params", ")", "\n", "trainer10", "=", "MB_Trainer", "(", "params", ")", "\n", "\n", "t6", "=", "Process", "(", "target", "=", "trainer6", ".", "run_training_loop", ",", "args", "=", "(", "42", ",", "params", "[", "'model2'", "]", ")", ")", "\n", "t7", "=", "Process", "(", "target", "=", "trainer7", ".", "run_training_loop", ",", "args", "=", "(", "43", ",", "params", "[", "'model2'", "]", ")", ")", "\n", "t8", "=", "Process", "(", "target", "=", "trainer8", ".", "run_training_loop", ",", "args", "=", "(", "44", ",", "params", "[", "'model2'", "]", ")", ")", "\n", "t9", "=", "Process", "(", "target", "=", "trainer9", ".", "run_training_loop", ",", "args", "=", "(", "45", ",", "params", "[", "'model2'", "]", ")", ")", "\n", "t10", "=", "Process", "(", "target", "=", "trainer10", ".", "run_training_loop", ",", "args", "=", "(", "46", ",", "params", "[", "'model2'", "]", ")", ")", "\n", "\n", "t6", ".", "start", "(", ")", "\n", "t7", ".", "start", "(", ")", "\n", "t8", ".", "start", "(", ")", "\n", "t9", ".", "start", "(", ")", "\n", "t10", ".", "start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.play_one_step.play_one_step": [[3, 59], ["play_one_step.epsilon_greedy_policy", "action.reshape.reshape", "env.step", "next_state.reshape.reshape", "cost_fn", "replay_memory.append", "state.reshape", "action.reshape.reshape"], "function", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.play_one_step.epsilon_greedy_policy"], ["def", "play_one_step", "(", "replay_memory", ",", "state", ",", "epsilon", ",", "episode", ",", "horizon", ",", "num_paths", ",", "obs_space", ",", "action_space", ",", "model", ",", "epistemic_uncertainty", ",", "ensemble", ",", "env", ",", "cost_fn", ")", ":", "\n", "    ", "\"\"\"\n    Function to compute one step of the loop.\n\n    The action of the current step is chosen by using the epsilon_greedy_policy function. \n    The agent takes the action in the environment and observes the next state, the reward, if done and further info. \n    The reward is computed by using our own cost functions. Then all the information of the current step is collected to the dataset.\n\n    Arguments:\n    ----------\n    replay_memory: list\n        Dataset of finite buffer, used to append all the information\n    state:\n        current state, it is passed to epsilon_greedy_policy\n    epsilon: float\n        current epsilon, it is passed to epsilon_greedy_policy to decide whether to explore or exploit\n    episode: int\n        current episode, it is passed to epsilon_greedy_policy\n    horizon: int\n        horizon of random shooting, it is passed to epsilon_greedy_policy\n    num_paths: int\n        number of trajectories of random shooting, it is passed to epsilon_greedy_policy\n    obs_space: int\n        observation space to reshape the observation array\n    action_space: int\n        action space to reshape the action array\n    model:\n        model to be exploited in the optimization algorithm (random shooting + MPC), it is passed to epsilon_greedy_policy\n    epistemic_uncertainty: bool\n        boolean to decide how to explore the environment (random exploration or Information Gain exploration), it is passed to epsilon_greedy_policy\n    ensemble: bool\n        boolean for compatibility of different functions, it is passed to epsilon_greedy_policy\n    env:\n        environment the agent interacts with, it is passed to epsilon_greedy_policy\n    cost_fn:\n        cost function of the environment, it is passed to epsilon_greedy_policy\n    Returns:\n    --------\n    replay_memory: list\n        The updated dataset\n    next_state:\n        the new state after the agent takes the action\n    reward:\n        the reward the agent receives for taking that action\n    done: bool\n        whether the episode is over or not\n    info:\n        further information\n    \"\"\"", "\n", "action", "=", "epsilon_greedy_policy", "(", "state", ",", "epsilon", ",", "episode", ",", "horizon", ",", "num_paths", ",", "model", ",", "epistemic_uncertainty", ",", "ensemble", ",", "env", ",", "cost_fn", ")", "\n", "action", "=", "action", ".", "reshape", "(", "action_space", ",", "1", ")", "\n", "next_state", ",", "_", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "next_state", "=", "next_state", ".", "reshape", "(", "obs_space", ",", ")", "\n", "reward", "=", "cost_fn", "(", "state", ".", "reshape", "(", "1", ",", "obs_space", ")", ",", "action", ".", "reshape", "(", "1", ",", "action_space", ")", ",", "env", ")", "\n", "replay_memory", ".", "append", "(", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", ")", "\n", "return", "replay_memory", ",", "next_state", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.play_one_step.epsilon_greedy_policy": [[60, 106], ["numpy.random.rand", "play_one_step.get_action", "play_one_step.IG_exploration", "env.action_space.sample"], "function", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.play_one_step.get_action", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.play_one_step.IG_exploration"], ["", "def", "epsilon_greedy_policy", "(", "state", ",", "epsilon", ",", "episode", ",", "horizon", ",", "num_paths", ",", "model", ",", "epistemic_uncertainty", ",", "ensemble", ",", "env", ",", "cost_fn", ")", ":", "\n", "    ", "\"\"\"\n    Policy of the agent to select the action, deciding between exploration and exploitation.\n\n    A random float in [0,1] is generated, if the number is less than epsilon (i.e. with probability epsilon) the agent explores, otherwise exploits the model through MPC and random shooting (get_action). \n    If the model explores, then if the model can quantify its uncertainty and it is trained -> explores maximizing the Information Gain, otherwise explores randomly.\n\n    Arguments:\n    ----------\n    state:\n        current state, it is used both for exploration and exploitation\n    epsilon: float\n        current epsilon, probability of exploration\n    episode: int\n        current episode, to know whether the model is trained\n    horizon: int\n        horizon of random shooting, it is passed to get_action\n    num_paths: int\n        number of trajectories of random shooting, it is passed to get_action\n    model:\n        model to be used both for exploration and exploitation\n    epistemic_uncertainty: bool\n        boolean to decide how to explore the environment, then it is passed to get_action for code compatibility of the different models\n    ensemble: bool\n        boolean for compatibility of different functions, it is passed to get_action and IG_exploration\n    env:\n        environment the agent interacts with\n    cost_fn:\n        cost function of the environment, it is passed to get_action\n\n    Returns:\n    --------\n    IG_exploration(): \n        action from IG exploration\n    env.action_space.sample()[:, np.newaxis]:\n        action from random exploration\n    get_action():\n        action from MPC + RS\n    \"\"\"", "\n", "if", "np", ".", "random", ".", "rand", "(", ")", "<", "epsilon", ":", "\n", "        ", "if", "epistemic_uncertainty", "and", "episode", ">", "0", ":", "\n", "            ", "return", "IG_exploration", "(", "state", ",", "model", ",", "ensemble", ",", "env", ")", "# IG exploration", "\n", "", "else", ":", "\n", "            ", "return", "env", ".", "action_space", ".", "sample", "(", ")", "[", ":", ",", "np", ".", "newaxis", "]", "# epsilon greedy exploration", "\n", "", "", "else", ":", "\n", "        ", "return", "get_action", "(", "state", ",", "horizon", ",", "num_paths", ",", "model", ",", "epistemic_uncertainty", ",", "ensemble", ",", "env", ",", "cost_fn", ")", "# MPC + random shooting", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.play_one_step.get_action": [[107, 157], ["range", "play_one_step.trajectory_cost_fn", "numpy.argmax", "obs.append", "obs_list.append", "act_list.append", "numpy.concatenate", "numpy.array", "numpy.array", "range", "actions.append", "model.predict_ensemble", "numpy.array", "env.action_space.sample", "range", "numpy.array", "numpy.array", "model.predict", "model.predict"], "function", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.play_one_step.trajectory_cost_fn", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.predict_ensemble", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.predict", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.predict"], ["", "", "def", "get_action", "(", "state", ",", "horizon", ",", "num_paths", ",", "model", ",", "epistemic_uncertainty", ",", "ensemble", ",", "env", ",", "cost_fn", ")", ":", "\n", "    ", "\"\"\"\n    Optimization algorithms to exploit the model, it combines Model Predictive Control (MPC) and random shooting.\n\n    Several trajectories, with finite horizon, are computed in parallel. \n    For each trajectory, a random action is sampled and the model predicts the next state, given this action and the current state, until the horizon lenght. \n    All the trajectories are evaluated with the trajectory_cost_fn function, then the first action [0] of the best trajectory [j] is selected.\n\n    Arguments:\n    ----------\n    state:\n        current state, it is used both for exploration and exploitation\n    horizon:\n        horizon of random shooting\n    num_paths:\n        number of trajectories of random shooting\n    model:\n        model to predict the next states for each trajectory\n    epistemic_uncertainty:\n        boolean for code compatibility of the different models\n    ensemble:\n        boolean for code compatibility of the different models\n    env:\n        environment the agent interacts with, used to sample the actions\n    cost_fn:\n        cost function of the environment, it is passed to trajectory_cost_fn\n\n    Returns:\n    --------\n    act_list[0][j]: \n        First action of the best trajectory\n    \"\"\"", "\n", "obs", ",", "obs_list", ",", "act_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "[", "obs", ".", "append", "(", "state", ")", "for", "_", "in", "range", "(", "num_paths", ")", "]", "\n", "for", "_", "in", "range", "(", "horizon", ")", ":", "\n", "        ", "obs_list", ".", "append", "(", "obs", ")", "\n", "actions", "=", "[", "]", "\n", "[", "actions", ".", "append", "(", "env", ".", "action_space", ".", "sample", "(", ")", ")", "for", "_", "in", "range", "(", "num_paths", ")", "]", "\n", "act_list", ".", "append", "(", "actions", ")", "\n", "data", "=", "np", ".", "concatenate", "(", "(", "np", ".", "array", "(", "obs", ")", ",", "np", ".", "array", "(", "actions", ")", ")", ",", "axis", "=", "1", ")", "\n", "if", "ensemble", ":", "\n", "            ", "delta_obs", ",", "delta_obs_total", ",", "delta_obs_std", "=", "model", ".", "predict_ensemble", "(", "data", ")", "# for dropout_ens, delta_obs_std is logvar but we don't need it", "\n", "", "elif", "epistemic_uncertainty", ":", "\n", "            ", "delta_obs", ",", "delta_obs_total", ",", "delta_obs_var", "=", "model", ".", "predict", "(", "data", ")", "# for GP delta_obs_total is nan but we don't need it", "\n", "", "else", ":", "\n", "            ", "delta_obs", "=", "model", ".", "predict", "(", "data", ")", "\n", "", "obs", "=", "delta_obs", "+", "np", ".", "array", "(", "obs", ")", "\n", "", "trajectory_cost_list", "=", "trajectory_cost_fn", "(", "cost_fn", ",", "np", ".", "array", "(", "obs_list", ")", ",", "np", ".", "array", "(", "act_list", ")", ",", "env", ")", "\n", "j", "=", "np", ".", "argmax", "(", "trajectory_cost_list", ")", "\n", "return", "act_list", "[", "0", "]", "[", "j", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.play_one_step.trajectory_cost_fn": [[158, 183], ["range", "len", "cost_fn"], "function", ["None"], ["", "def", "trajectory_cost_fn", "(", "cost_fn", ",", "states", ",", "actions", ",", "env", ")", ":", "\n", "    ", "\"\"\"\n    Function to compute the total rewards for all the trajectories\n\n    It takes two lists as inputs: the list of all the states for all the trajectories, and the list of all the actions for all the trajectories. \n    It computes in parallel the total rewards of the trajectories, over all the steps i of the horizon planning.\n\n    Arguments:\n    ----------\n    cost_fn: \n        Cost function of the environment, it computes the cost of the current state and action, then returns -cost since the environment returns rewards, not costs\n    states: list\n        list of all the states of all the trajectories\n    actions: list\n        list of all the actions of all the trajectories\n\n    Returns:\n    --------\n    trajectory_cost: array\n        array which contains the total rewards for each trajectory, It has num_path shape.\n    \"\"\"", "\n", "trajectory_cost", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "actions", ")", ")", ":", "\n", "        ", "trajectory_cost", "+=", "cost_fn", "(", "states", "[", "i", "]", ",", "actions", "[", "i", "]", ",", "env", ")", "\n", "", "return", "trajectory_cost", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.play_one_step.IG_exploration": [[184, 224], ["numpy.concatenate", "numpy.argmax", "actions.append", "obs.append", "model.predict_ensemble", "model.uncertainty().sum", "model.predict", "numpy.sum", "env.action_space.sample", "range", "range", "numpy.array", "numpy.array", "model.uncertainty", "model.uncertainty"], "function", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.predict_ensemble", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.predict", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.uncertainty", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.uncertainty"], ["", "def", "IG_exploration", "(", "state", ",", "model", ",", "ensemble", ",", "env", ")", ":", "\n", "    ", "\"\"\"\n    Exploration policy based on Information Gain.\n\n    This function defines the exploration for the models which can quantify the epistemic uncertainty. \n    A certain number of actions  is sampled, then the model predicts the next state, given the current state, for each different action. \n    In addition, the model then quantify the epistemic uncertainty for each prediction. \n    Since the epistemic uncertainty is calculated as a variance of each dimension of each state, can be directly sum up over the observation space of each prediction, in order to have a comparable value among the different states. \n    Finally, it is selected the action with the highest uncertainty: in this way, the agent will see the true next state for that action, the tuple will be appended to the dataset the model will be trained on, so it will no longer be uncertain about it. \n    This policy allows to visit areas of the environment where the model is uncertain only, without exploring areas it is very certain about.\n\n    Arguments:\n    ----------\n    state: \n        the current state to compute the next states for each action\n    model:\n        the model to predict the next states\n    ensemble: bool\n        boolen for code compatibility\n    env:\n        environment, it is used to sample the actions\n\n    Returns:\n    --------\n    actions[j]: array 1D\n        action with the highest epistemic uncertainty\n    \"\"\"", "\n", "actions", "=", "[", "]", "\n", "obs", "=", "[", "]", "\n", "[", "actions", ".", "append", "(", "env", ".", "action_space", ".", "sample", "(", ")", ")", "for", "_", "in", "range", "(", "50", ")", "]", "\n", "[", "obs", ".", "append", "(", "state", ")", "for", "_", "in", "range", "(", "50", ")", "]", "\n", "data", "=", "np", ".", "concatenate", "(", "(", "np", ".", "array", "(", "obs", ")", ",", "np", ".", "array", "(", "actions", ")", ")", ",", "axis", "=", "1", ")", "\n", "if", "ensemble", ":", "\n", "        ", "delta_obs", ",", "delta_obs_total", ",", "delta_obs_std", "=", "model", ".", "predict_ensemble", "(", "data", ")", "# for dropout_ens is logvar but we don't need it", "\n", "epistemic_uncertainty", "=", "model", ".", "uncertainty", "(", ")", ".", "sum", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "delta_obs", ",", "delta_obs_total", ",", "delta_obs_var", "=", "model", ".", "predict", "(", "data", ")", "# for GP delta_obs_total is nan but we don't need it", "\n", "epistemic_uncertainty", "=", "np", ".", "sum", "(", "model", ".", "uncertainty", "(", ")", ",", "1", ")", "\n", "", "j", "=", "np", ".", "argmax", "(", "epistemic_uncertainty", ")", "\n", "return", "actions", "[", "j", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.cost_functions.pendulum_cost_fn": [[6, 16], ["numpy.clip", "numpy.arccos", "action.reshape.reshape", "cost_functions.pendulum_cost_fn.angle_normalize"], "function", ["None"], ["def", "pendulum_cost_fn", "(", "state", ",", "action", ",", "env", ")", ":", "\n", "    ", "def", "angle_normalize", "(", "x", ")", ":", "\n", "        ", "return", "(", "(", "(", "x", "+", "np", ".", "pi", ")", "%", "(", "2", "*", "np", ".", "pi", ")", ")", "-", "np", ".", "pi", ")", "\n", "", "cos_theta", "=", "state", "[", ":", ",", "0", "]", "\n", "thdot", "=", "state", "[", ":", ",", "2", "]", "\n", "cos_theta", "=", "np", ".", "clip", "(", "cos_theta", ",", "-", "1", ",", "1", ")", "\n", "th", "=", "np", ".", "arccos", "(", "cos_theta", ")", "\n", "action", "=", "action", ".", "reshape", "(", "th", ".", "shape", ")", "\n", "costs", "=", "angle_normalize", "(", "th", ")", "**", "2", "+", ".1", "*", "thdot", "**", "2", "+", ".001", "*", "(", "action", "**", "2", ")", "\n", "return", "-", "costs", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.cost_functions.half_cheetah_cost_fn": [[18, 25], ["numpy.square().sum", "numpy.square"], "function", ["None"], ["", "def", "half_cheetah_cost_fn", "(", "state", ",", "action", ",", "env", ")", ":", "\n", "    ", "assert", "state", ".", "shape", "[", "1", "]", "==", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "assert", "action", ".", "shape", "[", "1", "]", "==", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "reward_ctrl", "=", "-", "0.1", "*", "np", ".", "square", "(", "action", ")", ".", "sum", "(", "1", ")", "\n", "reward_run", "=", "state", "[", ":", ",", "8", "]", "\n", "reward", "=", "reward_run", "+", "reward_ctrl", "\n", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.cost_functions.inverted_pendulum_cost_fn": [[26, 42], ["None"], "function", ["None"], ["", "def", "inverted_pendulum_cost_fn", "(", "state", ",", "action", ",", "env", ")", ":", "\n", "    ", "assert", "state", ".", "shape", "[", "1", "]", "==", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "assert", "action", ".", "shape", "[", "1", "]", "==", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "#data_dict = {'start_state': state, 'action': action}", "\n", "xpos_ob_pos", "=", "0", "\n", "ypos_ob_pos", "=", "1", "\n", "ypos_target", "=", "0.0", "\n", "xpos_coeff", "=", "0.0", "\n", "# xpos penalty", "\n", "xpos", "=", "state", "[", ":", ",", "xpos_ob_pos", "]", "\n", "xpos_reward", "=", "-", "(", "xpos", "**", "2", ")", "*", "xpos_coeff", "\n", "\n", "# ypos penalty", "\n", "ypos", "=", "state", "[", ":", ",", "ypos_ob_pos", "]", "\n", "ypos_reward", "=", "-", "(", "ypos", "-", "ypos_target", ")", "**", "2", "\n", "return", "xpos_reward", "+", "ypos_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.cost_functions.reacher_cost_fn": [[43, 50], ["numpy.linalg.norm", "numpy.square().sum", "numpy.square"], "function", ["None"], ["", "def", "reacher_cost_fn", "(", "state", ",", "action", ",", "env", ")", ":", "\n", "    ", "assert", "state", ".", "shape", "[", "1", "]", "==", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "assert", "action", ".", "shape", "[", "1", "]", "==", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "distance", "=", "state", "[", ":", ",", "[", "2", ",", "3", "]", "]", "\n", "reward_dist", "=", "-", "np", ".", "linalg", ".", "norm", "(", "distance", ",", "axis", "=", "1", ")", "\n", "reward_ctrl", "=", "-", "np", ".", "square", "(", "action", ")", ".", "sum", "(", "1", ")", "\n", "return", "reward_dist", "+", "reward_ctrl", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.cost_functions.double_inverted_pendulum_cost_fn": [[51, 64], ["numpy.clip", "numpy.arcsin", "numpy.clip", "numpy.arcsin"], "function", ["None"], ["", "def", "double_inverted_pendulum_cost_fn", "(", "state", ",", "action", ",", "env", ")", ":", "\n", "    ", "assert", "state", ".", "shape", "[", "1", "]", "==", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "assert", "action", ".", "shape", "[", "1", "]", "==", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "theta_sin", "=", "state", "[", ":", ",", "1", "]", "\n", "theta_sin", "=", "np", ".", "clip", "(", "theta_sin", ",", "-", "1", ",", "1", ")", "\n", "theta", "=", "np", ".", "arcsin", "(", "theta_sin", ")", "\n", "gamma_sin", "=", "state", "[", ":", ",", "2", "]", "\n", "gamma_sin", "=", "np", ".", "clip", "(", "gamma_sin", ",", "-", "1", ",", "1", ")", "\n", "gamma", "=", "np", ".", "arcsin", "(", "gamma_sin", ")", "\n", "theta_dot", "=", "state", "[", ":", ",", "6", "]", "\n", "gamma_dot", "=", "state", "[", ":", ",", "7", "]", "\n", "vel_penalty", "=", "1e-3", "*", "theta_dot", "**", "2", "+", "5e-3", "*", "gamma_dot", "**", "2", "\n", "return", "-", "(", "theta", "**", "2", ")", "-", "(", "gamma", "**", "2", ")", "-", "vel_penalty", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.cost_functions.hopper_cost_fn": [[65, 74], ["numpy.square().sum", "numpy.square"], "function", ["None"], ["", "def", "hopper_cost_fn", "(", "state", ",", "action", ",", "env", ")", ":", "\n", "    ", "assert", "state", ".", "shape", "[", "1", "]", "==", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "assert", "state", ".", "shape", "[", "1", "]", "==", "11", "\n", "assert", "action", ".", "shape", "[", "1", "]", "==", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "reward_ctrl", "=", "-", "0.1", "*", "np", ".", "square", "(", "action", ")", ".", "sum", "(", "1", ")", "\n", "reward_run", "=", "state", "[", ":", ",", "5", "]", "\n", "reward_height", "=", "-", "3", "*", "(", "state", "[", ":", ",", "0", "]", ")", "**", "2", "\n", "reward", "=", "reward_run", "+", "reward_ctrl", "+", "reward_height", "+", "1", "\n", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.cost_functions.walker2D_cost_fn": [[75, 83], ["numpy.square().sum", "numpy.square"], "function", ["None"], ["", "def", "walker2D_cost_fn", "(", "state", ",", "action", ",", "env", ")", ":", "\n", "    ", "assert", "state", ".", "shape", "[", "1", "]", "==", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "assert", "action", ".", "shape", "[", "1", "]", "==", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "reward_ctrl", "=", "-", "0.1", "*", "np", ".", "square", "(", "action", ")", ".", "sum", "(", "1", ")", "\n", "reward_run", "=", "state", "[", ":", ",", "8", "]", "\n", "reward_height", "=", "-", "3", "*", "(", "state", "[", ":", ",", "0", "]", ")", "**", "2", "\n", "reward", "=", "reward_run", "+", "reward_ctrl", "+", "reward_height", "+", "1", "\n", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.cost_functions.ant_cost_fn": [[84, 92], ["numpy.square().sum", "numpy.square"], "function", ["None"], ["", "def", "ant_cost_fn", "(", "state", ",", "action", ",", "env", ")", ":", "\n", "    ", "assert", "state", ".", "shape", "[", "1", "]", "==", "27", "\n", "assert", "action", ".", "shape", "[", "1", "]", "==", "8", "\n", "reward_ctrl", "=", "-", "0.1", "*", "np", ".", "square", "(", "action", ")", ".", "sum", "(", "1", ")", "\n", "reward_run", "=", "state", "[", ":", ",", "13", "]", "\n", "reward_height", "=", "-", "3", "*", "(", "state", "[", ":", ",", "0", "]", "-", "0.75", ")", "**", "2", "\n", "reward", "=", "reward_run", "+", "reward_ctrl", "+", "reward_height", "+", "1", "\n", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.MB_trainer.MB_Trainer.__init__": [[27, 109], ["gym.make"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        MB_trainer is the class where all the params are initialized and the RL alrgorithm is executed.\n        \n        Here the environment is created using self.env_name, then for each environment the episode length and the cost function are set.\n        \n        Arguments:\n        ----------\n        params['env_name']: str\n            name of the environment to be created, it is in list ['Pendulum-v0', ..., 'AntMuJoCoEnv-v0']\n        params['seed']: int\n            seed number for the models\n        params['model']: str \n            model to run, it is in list ['deterministicNN', 'deterministic_ensemble', 'probabilisticNN', 'probabilistic_ensemble', 'dropoutNN', 'dropout_ensemble', 'gaussian_process']\n        params['n_layers']: int\n            number of hidden layers of the model\n        params['n_hidden']: int\n            number of units/neurons of each hidden layer\n        params['activation_in']: str\n            activation function of hidden layers\n        params['epochs']: int\n            epochs of training\n        params['learning_rate']: float\n            learning rate of the NN\n        params['ensemble_size']: int\n            number of NNs in the ensemble (usually 5)\n        params['replay_buffer']: int\n            max lenght of the dataset (replay_memory), usually 2000\n        Raises: \n        -------\n        RuntimeError:\n            If params['env_name'] is not in the expected list\n        \n        \"\"\"", "\n", "self", ".", "params", "=", "params", "\n", "self", ".", "env_name", "=", "params", "[", "'env_name'", "]", "\n", "#self.seed = params['seed']                 # it is set for each process in main.py", "\n", "self", ".", "nb_layers", "=", "params", "[", "'n_layers'", "]", "\n", "self", ".", "n_hidden", "=", "params", "[", "'n_hidden'", "]", "\n", "self", ".", "activation_in", "=", "params", "[", "'activation_in'", "]", "\n", "self", ".", "epochs", "=", "params", "[", "'epochs'", "]", "\n", "self", ".", "lr", "=", "params", "[", "'learning_rate'", "]", "\n", "self", ".", "n_ensemble", "=", "params", "[", "'ensemble_size'", "]", "\n", "self", ".", "replay_buffer", "=", "params", "[", "'replay_buffer'", "]", "\n", "\n", "if", "self", ".", "env_name", "==", "'Pendulum-v0'", ":", "\n", "            ", "params", "[", "'ep_len'", "]", "=", "200", "\n", "cost_fn", "=", "pendulum_cost_fn", "\n", "", "elif", "self", ".", "env_name", "==", "'HalfCheetahMuJoCoEnv-v0'", ":", "\n", "            ", "params", "[", "'ep_len'", "]", "=", "500", "\n", "cost_fn", "=", "half_cheetah_cost_fn", "\n", "", "elif", "self", ".", "env_name", "==", "'InvertedPendulumMuJoCoEnv-v0'", ":", "\n", "            ", "params", "[", "'ep_len'", "]", "=", "100", "\n", "cost_fn", "=", "inverted_pendulum_cost_fn", "\n", "", "elif", "self", ".", "env_name", "==", "'ReacherPyBulletEnv-v0'", ":", "\n", "            ", "params", "[", "'ep_len'", "]", "=", "50", "\n", "cost_fn", "=", "reacher_cost_fn", "\n", "", "elif", "self", ".", "env_name", "==", "'InvertedDoublePendulumMuJoCoEnv-v0'", ":", "\n", "            ", "params", "[", "'ep_len'", "]", "=", "100", "\n", "cost_fn", "=", "double_inverted_pendulum_cost_fn", "\n", "", "elif", "self", ".", "env_name", "==", "'HopperMuJoCoEnv-v0'", ":", "\n", "            ", "params", "[", "'ep_len'", "]", "=", "500", "\n", "cost_fn", "=", "hopper_cost_fn", "\n", "", "elif", "self", ".", "env_name", "==", "'Walker2DMuJoCoEnv-v0'", ":", "\n", "            ", "params", "[", "'ep_len'", "]", "=", "500", "\n", "cost_fn", "=", "walker2D_cost_fn", "\n", "", "elif", "self", ".", "env_name", "==", "'AntMuJoCoEnv-v0'", ":", "\n", "            ", "params", "[", "'ep_len'", "]", "=", "500", "\n", "cost_fn", "=", "ant_cost_fn", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "\n", "\n", "", "env", "=", "gym", ".", "make", "(", "self", ".", "env_name", ")", "\n", "self", ".", "env", "=", "env", "\n", "self", ".", "ep_len", "=", "params", "[", "'ep_len'", "]", "\n", "self", ".", "cost_fn", "=", "cost_fn", "\n", "D", "=", "self", ".", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "#output shape model", "\n", "A", "=", "self", ".", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "Q", "=", "D", "+", "A", "# input shape model", "\n", "self", ".", "D", "=", "D", "\n", "self", ".", "A", "=", "A", "\n", "self", ".", "Q", "=", "Q", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.MB_trainer.MB_Trainer.run_training_loop": [[110, 276], ["tf.config.experimental.list_physical_devices", "keras.backend.clear_session", "numpy.random.seed", "tf.random.set_seed", "collections.deque", "MB_trainer.MB_Trainer.env.seed", "print", "range", "MB_trainer.MB_Trainer.env.close", "NN", "MB_trainer.MB_Trainer.env.reset().reshape", "range", "seed_rewards.append", "print", "training_step.training_step.training_step", "open", "pickle.dump", "tf.config.experimental.list_logical_devices", "print", "ens_NNs", "max", "play_one_step.play_one_step.play_one_step", "state.reshape.reshape.reshape", "time.strftime", "tf.config.experimental.set_memory_growth", "len", "len", "print", "PNN", "MB_trainer.MB_Trainer.env.reset", "ens_PNNs", "str", "BNN", "ens_BNN", "gpflow.config.set_default_float", "GP"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.training_step.training_step", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.None.play_one_step.play_one_step"], ["", "def", "run_training_loop", "(", "self", ",", "seed", ",", "model_name", ")", ":", "\n", "\n", "        ", "\"\"\"\n        RL algorithm, the agent interacts with the environment, learns the dynamics function and collects rewards\n        \n        The function begins with importing all the keras/tensorflow related functions and initializing the model through self.model_name and the chosen hyperparameters. \n        Then two booleans (epistemic_uncertainty and ensemble) are defined to control the exploration and the compatibility of the RL algorithm for each different model. \n        This is made because this function will be run in parallel for different seeds, and because of a conflict between keras and multiprocessing, everything related to keras/tensorflow is to be defined within each process.\n        Afterwards, the RL algorithm starts. Each seed iteration presents many episode iterations (between 50 and 500) and the length of each episode depends on the enviroment. \n        Core of this function is play_one_step that computes one step and it collects new information wich is appended to the dataset. \n        At each step, the agent chooses to expore with probability epsilon or to exploit the model (1 - epsilon). \n        Epsilon starts at 1 in the first episode (only exploration), then it remains constant at 0.01. \n        At the end of each episode, the model is trained on the whole dataset, which keeps the last 2000 (can be set with params['replay_buffer']) data only.\n        \n        Arguments:\n        ----------\n        self.env: \n            environment\n        model_name: str \n            model to run, it is in list ['deterministicNN', 'deterministic_ensemble', 'probabilisticNN', 'probabilistic_ensemble', 'dropoutNN', 'dropout_ensemble', 'gaussian_process']\n        self.nb_layers: int\n            number of hidden layers of the model\n        self.n_hidden: int\n            number of units/neurons of each hidden layer\n        self.activation_in: str\n            activation function of hidden layers\n        self.epochs: int\n            epochs of training\n        self.lr: float\n            learning rate of the NN\n        self.n_ensemble: int\n            number of NNs in the ensemble (usually 5)\n        self.replay_buffer: int\n            max lenght of the dataset (replay_memory), usually 2000\n        seed: int\n            random seed for each parallel process\n            \n        Raises: \n        -------\n        RuntimeError:\n            If model_name is not in the expected list\n        \n        \"\"\"", "\n", "\n", "##########################", "\n", "### Model Initialization", "\n", "##########################", "\n", "\n", "import", "tensorflow", "as", "tf", "\n", "\n", "gpus", "=", "tf", ".", "config", ".", "experimental", ".", "list_physical_devices", "(", "'GPU'", ")", "\n", "if", "gpus", ":", "\n", "          ", "try", ":", "\n", "# Currently, memory growth needs to be the same across GPUs", "\n", "            ", "for", "gpu", "in", "gpus", ":", "\n", "              ", "tf", ".", "config", ".", "experimental", ".", "set_memory_growth", "(", "gpu", ",", "True", ")", "\n", "", "logical_gpus", "=", "tf", ".", "config", ".", "experimental", ".", "list_logical_devices", "(", "'GPU'", ")", "\n", "print", "(", "len", "(", "gpus", ")", ",", "\"Physical GPUs,\"", ",", "len", "(", "logical_gpus", ")", ",", "\"Logical GPUs\"", ")", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "# Memory growth must be set before GPUs have been initialized", "\n", "            ", "print", "(", "e", ")", "\n", "\n", "#from tensorflow import keras # it is much slower than import keras alone", "\n", "", "", "import", "keras", "\n", "import", "gpflow", "\n", "from", "gpflow", ".", "ci_utils", "import", "ci_niter", "\n", "\n", "#import tensorflow.keras.backend as K", "\n", "#from tensorflow.keras import initializers", "\n", "#from tensorflow.keras.models import Model, Sequential", "\n", "#from tensorflow.keras.layers import Input, Dense, Lambda, Wrapper, concatenate, InputSpec", "\n", "\n", "from", "models", ".", "deterministicNN", "import", "NN", ",", "ens_NNs", "\n", "from", "models", ".", "PNN", "import", "PNN", ",", "ens_PNNs", "\n", "from", "models", ".", "ConcreteDropout", "import", "ConcreteDropout", ",", "BNN", ",", "ens_BNN", "\n", "from", "models", ".", "GP", "import", "GP", "\n", "\n", "keras", ".", "backend", ".", "clear_session", "(", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "tf", ".", "random", ".", "set_seed", "(", "seed", ")", "\n", "\n", "if", "model_name", "==", "'deterministicNN'", ":", "\n", "            ", "model", "=", "NN", "(", "env", "=", "self", ".", "env", ",", "nb_layers", "=", "self", ".", "nb_layers", ",", "n_hidden", "=", "self", ".", "n_hidden", ",", "activation_in", "=", "self", ".", "activation_in", ",", "\n", "kernel_initializer", "=", "\"he_normal\"", ",", "epochs", "=", "self", ".", "epochs", ",", "l_rate", "=", "self", ".", "lr", ",", "env_name", "=", "self", ".", "env_name", ")", "\n", "epistemic_uncertainty", "=", "False", "\n", "ensemble", "=", "False", "\n", "", "elif", "model_name", "==", "'deterministic_ensemble'", ":", "\n", "            ", "model", "=", "ens_NNs", "(", "env", "=", "self", ".", "env", ",", "nb_layers", "=", "self", ".", "nb_layers", ",", "n_hidden", "=", "self", ".", "n_hidden", ",", "activation_in", "=", "self", ".", "activation_in", ",", "\n", "kernel_initializer", "=", "\"he_normal\"", ",", "epochs", "=", "self", ".", "epochs", ",", "l_rate", "=", "self", ".", "lr", ",", "n_ensemble", "=", "self", ".", "n_ensemble", ",", "\n", "env_name", "=", "self", ".", "env_name", ")", "\n", "epistemic_uncertainty", "=", "True", "\n", "ensemble", "=", "True", "\n", "", "elif", "model_name", "==", "'probabilisticNN'", ":", "\n", "            ", "model", "=", "PNN", "(", "env", "=", "self", ".", "env", ",", "reg", "=", "'anc'", ",", "n_hidden", "=", "self", ".", "n_hidden", ",", "activation_in", "=", "'relu'", ",", "data_noise", "=", "0.001", ",", "\n", "n_data", "=", "self", ".", "replay_buffer", ",", "\n", "epochs", "=", "self", ".", "epochs", ",", "l_rate", "=", "self", ".", "lr", ",", "env_name", "=", "self", ".", "env_name", ")", "\n", "epistemic_uncertainty", "=", "False", "\n", "ensemble", "=", "False", "\n", "", "elif", "model_name", "==", "'probabilistic_ensemble'", ":", "\n", "            ", "model", "=", "ens_PNNs", "(", "env", "=", "self", ".", "env", ",", "reg", "=", "'anc'", ",", "n_hidden", "=", "self", ".", "n_hidden", ",", "activation_in", "=", "'relu'", ",", "data_noise", "=", "0.001", ",", "\n", "n_data", "=", "self", ".", "replay_buffer", ",", "\n", "epochs", "=", "self", ".", "epochs", ",", "l_rate", "=", "self", ".", "lr", ",", "n_ensemble", "=", "self", ".", "n_ensemble", ",", "env_name", "=", "self", ".", "env_name", ")", "\n", "epistemic_uncertainty", "=", "True", "\n", "ensemble", "=", "True", "\n", "", "elif", "model_name", "==", "'dropoutNN'", ":", "\n", "            ", "model", "=", "BNN", "(", "env", "=", "self", ".", "env", ",", "nb_units", "=", "self", ".", "n_hidden", ",", "nb_layers", "=", "self", ".", "nb_layers", ",", "activation", "=", "'relu'", ",", "\n", "n_data", "=", "self", ".", "replay_buffer", ",", "\n", "dropout", "=", "0.1", ",", "T", "=", "20", ",", "tau", "=", "1.0", ",", "lengthscale", "=", "1e-4", ",", "train_flag", "=", "True", ",", "env_name", "=", "self", ".", "env_name", ")", "\n", "epistemic_uncertainty", "=", "True", "\n", "ensemble", "=", "False", "\n", "", "elif", "model_name", "==", "'dropout_ensemble'", ":", "\n", "            ", "model", "=", "ens_BNN", "(", "env", "=", "self", ".", "env", ",", "nb_units", "=", "self", ".", "n_hidden", ",", "nb_layers", "=", "self", ".", "nb_layers", ",", "activation", "=", "'relu'", ",", "\n", "n_data", "=", "self", ".", "replay_buffer", ",", "\n", "dropout", "=", "0.1", ",", "T", "=", "20", ",", "tau", "=", "1.0", ",", "lengthscale", "=", "1e-4", ",", "n_ensemble", "=", "self", ".", "n_ensemble", ",", "train_flag", "=", "True", ",", "\n", "env_name", "=", "self", ".", "env_name", ")", "\n", "epistemic_uncertainty", "=", "True", "\n", "ensemble", "=", "True", "\n", "", "elif", "model_name", "==", "'gaussian_process'", ":", "\n", "            ", "gpflow", ".", "config", ".", "set_default_float", "(", "np", ".", "float64", ")", "\n", "model", "=", "GP", "(", "env", "=", "self", ".", "env", ",", "gp_model", "=", "'GPR'", ")", "\n", "epistemic_uncertainty", "=", "True", "\n", "ensemble", "=", "False", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "\n", "\n", "", "self", ".", "model", "=", "model", "\n", "self", ".", "epistemic_uncertainty", "=", "epistemic_uncertainty", "\n", "self", ".", "ensemble", "=", "ensemble", "\n", "\n", "#####################", "\n", "### RL TRAINING LOOP", "\n", "#####################", "\n", "\n", "replay_memory", "=", "deque", "(", "maxlen", "=", "self", ".", "params", "[", "'replay_buffer'", "]", ")", "\n", "self", ".", "env", ".", "seed", "(", "seed", ")", "\n", "seed_rewards", "=", "[", "]", "# total rewards for each different seed", "\n", "print", "(", "'Starting training loop for seed'", ",", "seed", ")", "\n", "for", "episode", "in", "range", "(", "self", ".", "params", "[", "'n_iter'", "]", ")", ":", "\n", "            ", "episode_rewards", "=", "0", "\n", "state", "=", "self", ".", "env", ".", "reset", "(", ")", ".", "reshape", "(", "self", ".", "D", ",", ")", "\n", "for", "step", "in", "range", "(", "self", ".", "ep_len", ")", ":", "\n", "                ", "epsilon", "=", "max", "(", "1", "-", "episode", "/", "1", ",", "0.01", ")", "\n", "replay_memory", ",", "state", ",", "reward", ",", "done", ",", "info", "=", "play_one_step", "(", "replay_memory", ",", "state", ",", "epsilon", ",", "episode", ",", "\n", "horizon", "=", "self", ".", "params", "[", "'mpc_horizon'", "]", ",", "\n", "num_paths", "=", "self", ".", "params", "[", "'mpc_num_action_sequences'", "]", ",", "\n", "obs_space", "=", "self", ".", "D", ",", "\n", "action_space", "=", "self", ".", "A", ",", "\n", "model", "=", "self", ".", "model", ",", "\n", "epistemic_uncertainty", "=", "self", ".", "epistemic_uncertainty", ",", "\n", "ensemble", "=", "self", ".", "ensemble", ",", "\n", "env", "=", "self", ".", "env", ",", "\n", "cost_fn", "=", "self", ".", "cost_fn", ")", "\n", "state", "=", "state", ".", "reshape", "(", "self", ".", "D", ",", ")", "\n", "episode_rewards", "+=", "reward", "\n", "#if done:", "\n", "#    break", "\n", "", "seed_rewards", ".", "append", "(", "episode_rewards", ")", "\n", "print", "(", "\"\\rEpisode: {}, Steps: {}, rewards: {}, model: {}, seed: {}, eps: {:.3f}\"", ".", "format", "(", "episode", ",", "step", "+", "1", ",", "episode_rewards", ",", "\n", "model_name", ",", "seed", ",", "epsilon", ")", ",", "end", "=", "\"\"", ")", "\n", "self", ".", "model", "=", "training_step", "(", "replay_memory", ",", "model", "=", "self", ".", "model", ",", "\n", "ensemble", "=", "self", ".", "ensemble", ",", "obs_space", "=", "self", ".", "D", ",", "action_space", "=", "self", ".", "A", ")", "\n", "\n", "", "file", "=", "'rewards_'", "+", "model_name", "+", "'_'", "+", "self", ".", "env_name", "+", "'_'", "+", "'seed'", "+", "str", "(", "seed", ")", "+", "'_'", "+", "time", ".", "strftime", "(", "\"%d-%m-%Y\"", ")", "+", "'.pickle'", "\n", "with", "open", "(", "file", ",", "\"wb\"", ")", "as", "fp", ":", "\n", "            ", "pickle", ".", "dump", "(", "seed_rewards", ",", "fp", ")", "\n", "", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.PNN.PNN.__init__": [[20, 137], ["numpy.random.normal", "numpy.random.normal", "numpy.random.normal", "numpy.random.normal", "numpy.random.normal", "numpy.random.normal", "numpy.random.normal", "numpy.random.normal", "numpy.random.normal", "numpy.random.normal", "keras.models.Sequential", "keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.models.Sequential.compile", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "keras.optimizers.Adam", "keras.optimizers.Adam", "keras.optimizers.Adam", "keras.optimizers.Adam", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.initializers.Constant", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.square", "keras.square", "keras.square", "keras.square", "keras.square", "keras.square", "keras.square", "keras.square", "keras.square", "keras.square", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.sum", "keras.square", "keras.square", "keras.square", "keras.square", "keras.square", "keras.square", "keras.square", "keras.square", "keras.square", "keras.square"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "env", ",", "reg", "=", "'anc'", ",", "n_hidden", "=", "40", ",", "activation_in", "=", "'relu'", ",", "data_noise", "=", "0.001", ",", "n_data", "=", "32", ",", "epochs", "=", "30", ",", "\n", "l_rate", "=", "0.001", ",", "ens_num", "=", "0", ",", "env_name", "=", "'Pendulum-v0'", ")", ":", "\n", "        ", "\"\"\"\n        :env: environment\n        :param reg: type of regularisation to use - anc (anchoring) reg (regularised) free (unconstrained)\n        :param n_hidden: units per layer\n        :param activation_in: activation hidden layers - relu tanh sigmoid\n        :param data_noise: estimated data_noise\n        :param n_data: number of data \n        :param epochs: epochs\n        :param l_rate: lr\n        \"\"\"", "\n", "self", ".", "env", "=", "env", "\n", "self", ".", "reg", "=", "reg", "\n", "self", ".", "n_hidden", "=", "n_hidden", "\n", "self", ".", "activation_in", "=", "activation_in", "\n", "self", ".", "data_noise", "=", "data_noise", "\n", "self", ".", "n_data", "=", "n_data", "\n", "self", ".", "epochs", "=", "epochs", "\n", "self", ".", "l_rate", "=", "l_rate", "\n", "self", ".", "ens_num", "=", "ens_num", "\n", "self", ".", "env_name", "=", "env_name", "\n", "\n", "D", "=", "self", ".", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "A", "=", "self", ".", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "Q", "=", "D", "+", "A", "# input shape", "\n", "\n", "W1_var", "=", "20", "/", "Q", "# 1st layer weights and biases", "\n", "W_mid_var", "=", "1", "/", "self", ".", "n_hidden", "# 2st layer weights and biases", "\n", "W_last_var", "=", "1", "/", "self", ".", "n_hidden", "# 3st layer weights and biases", "\n", "\n", "# get initialisations, and regularisation values", "\n", "W1_lambda", "=", "self", ".", "data_noise", "/", "(", "D", "*", "W1_var", ")", "\n", "W1_anc", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0", ",", "scale", "=", "np", ".", "sqrt", "(", "W1_var", ")", ",", "size", "=", "[", "Q", ",", "self", ".", "n_hidden", "]", ")", "\n", "W1_init", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0", ",", "scale", "=", "np", ".", "sqrt", "(", "W1_var", ")", ",", "size", "=", "[", "Q", ",", "self", ".", "n_hidden", "]", ")", "\n", "\n", "b1_var", "=", "W1_var", "\n", "b1_lambda", "=", "self", ".", "data_noise", "/", "(", "D", "*", "b1_var", ")", "\n", "b1_anc", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0", ",", "scale", "=", "np", ".", "sqrt", "(", "b1_var", ")", ",", "size", "=", "[", "self", ".", "n_hidden", "]", ")", "\n", "b1_init", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0", ",", "scale", "=", "np", ".", "sqrt", "(", "b1_var", ")", ",", "size", "=", "[", "self", ".", "n_hidden", "]", ")", "\n", "\n", "W_mid_lambda", "=", "self", ".", "data_noise", "/", "(", "D", "*", "W_mid_var", ")", "\n", "W_mid_anc", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0", ",", "scale", "=", "np", ".", "sqrt", "(", "W_mid_var", ")", ",", "size", "=", "[", "self", ".", "n_hidden", ",", "self", ".", "n_hidden", "]", ")", "\n", "W_mid_init", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0", ",", "scale", "=", "np", ".", "sqrt", "(", "W_mid_var", ")", ",", "size", "=", "[", "self", ".", "n_hidden", ",", "self", ".", "n_hidden", "]", ")", "\n", "\n", "b_mid_var", "=", "W_mid_var", "\n", "b_mid_lambda", "=", "self", ".", "data_noise", "/", "(", "D", "*", "b_mid_var", ")", "\n", "b_mid_anc", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0", ",", "scale", "=", "np", ".", "sqrt", "(", "b_mid_var", ")", ",", "size", "=", "[", "self", ".", "n_hidden", "]", ")", "\n", "b_mid_init", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0", ",", "scale", "=", "np", ".", "sqrt", "(", "b_mid_var", ")", ",", "size", "=", "[", "self", ".", "n_hidden", "]", ")", "\n", "\n", "W_last_lambda", "=", "self", ".", "data_noise", "/", "(", "D", "*", "W_last_var", ")", "\n", "W_last_anc", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0", ",", "scale", "=", "np", ".", "sqrt", "(", "W_last_var", ")", ",", "size", "=", "[", "self", ".", "n_hidden", ",", "D", "]", ")", "\n", "W_last_init", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0", ",", "scale", "=", "np", ".", "sqrt", "(", "W_last_var", ")", ",", "size", "=", "[", "self", ".", "n_hidden", ",", "D", "]", ")", "\n", "\n", "# create custom regularised", "\n", "def", "custom_reg_W1", "(", "weight_matrix", ")", ":", "\n", "            ", "if", "self", ".", "reg", "==", "'reg'", ":", "\n", "                ", "return", "K", ".", "sum", "(", "K", ".", "square", "(", "weight_matrix", ")", ")", "*", "W1_lambda", "/", "self", ".", "n_data", "\n", "", "elif", "self", ".", "reg", "==", "'free'", ":", "\n", "                ", "return", "0.", "\n", "", "elif", "self", ".", "reg", "==", "'anc'", ":", "\n", "                ", "return", "K", ".", "sum", "(", "K", ".", "square", "(", "weight_matrix", "-", "W1_anc", ")", ")", "*", "W1_lambda", "/", "self", ".", "n_data", "\n", "\n", "", "", "def", "custom_reg_b1", "(", "weight_matrix", ")", ":", "\n", "            ", "if", "self", ".", "reg", "==", "'reg'", ":", "\n", "                ", "return", "K", ".", "sum", "(", "K", ".", "square", "(", "weight_matrix", ")", ")", "*", "b1_lambda", "/", "self", ".", "n_data", "\n", "", "elif", "self", ".", "reg", "==", "'free'", ":", "\n", "                ", "return", "0.", "\n", "", "elif", "self", ".", "reg", "==", "'anc'", ":", "\n", "                ", "return", "K", ".", "sum", "(", "K", ".", "square", "(", "weight_matrix", "-", "b1_anc", ")", ")", "*", "b1_lambda", "/", "self", ".", "n_data", "\n", "\n", "", "", "def", "custom_reg_W_mid", "(", "weight_matrix", ")", ":", "\n", "            ", "if", "self", ".", "reg", "==", "'reg'", ":", "\n", "                ", "return", "K", ".", "sum", "(", "K", ".", "square", "(", "weight_matrix", ")", ")", "*", "W_mid_lambda", "/", "self", ".", "n_data", "\n", "", "elif", "self", ".", "reg", "==", "'free'", ":", "\n", "                ", "return", "0.", "\n", "", "elif", "self", ".", "reg", "==", "'anc'", ":", "\n", "                ", "return", "K", ".", "sum", "(", "K", ".", "square", "(", "weight_matrix", "-", "W_mid_anc", ")", ")", "*", "W_mid_lambda", "/", "self", ".", "n_data", "\n", "\n", "", "", "def", "custom_reg_b_mid", "(", "weight_matrix", ")", ":", "\n", "            ", "if", "self", ".", "reg", "==", "'reg'", ":", "\n", "                ", "return", "K", ".", "sum", "(", "K", ".", "square", "(", "weight_matrix", ")", ")", "*", "b_mid_lambda", "/", "self", ".", "n_data", "\n", "", "elif", "self", ".", "reg", "==", "'free'", ":", "\n", "                ", "return", "0.", "\n", "", "elif", "self", ".", "reg", "==", "'anc'", ":", "\n", "                ", "return", "K", ".", "sum", "(", "K", ".", "square", "(", "weight_matrix", "-", "b_mid_anc", ")", ")", "*", "b_mid_lambda", "/", "self", ".", "n_data", "\n", "\n", "", "", "def", "custom_reg_W_last", "(", "weight_matrix", ")", ":", "\n", "            ", "if", "self", ".", "reg", "==", "'reg'", ":", "\n", "                ", "return", "K", ".", "sum", "(", "K", ".", "square", "(", "weight_matrix", ")", ")", "*", "W_last_lambda", "/", "self", ".", "n_data", "\n", "", "elif", "self", ".", "reg", "==", "'free'", ":", "\n", "                ", "return", "0.", "\n", "", "elif", "self", ".", "reg", "==", "'anc'", ":", "\n", "                ", "return", "K", ".", "sum", "(", "K", ".", "square", "(", "weight_matrix", "-", "W_last_anc", ")", ")", "*", "W_last_lambda", "/", "self", ".", "n_data", "\n", "\n", "", "", "model", "=", "Sequential", "(", ")", "\n", "model", ".", "add", "(", "Dense", "(", "self", ".", "n_hidden", ",", "activation", "=", "self", ".", "activation_in", ",", "input_shape", "=", "(", "Q", ",", ")", ",", "\n", "kernel_initializer", "=", "keras", ".", "initializers", ".", "Constant", "(", "value", "=", "W1_init", ")", ",", "\n", "bias_initializer", "=", "keras", ".", "initializers", ".", "Constant", "(", "value", "=", "b1_init", ")", ",", "\n", "kernel_regularizer", "=", "custom_reg_W1", ",", "\n", "bias_regularizer", "=", "custom_reg_b1", ")", ")", "\n", "\n", "model", ".", "add", "(", "Dense", "(", "self", ".", "n_hidden", ",", "activation", "=", "self", ".", "activation_in", ",", "\n", "kernel_initializer", "=", "keras", ".", "initializers", ".", "Constant", "(", "value", "=", "W_mid_init", ")", ",", "\n", "bias_initializer", "=", "keras", ".", "initializers", ".", "Constant", "(", "value", "=", "b_mid_init", ")", ",", "\n", "kernel_regularizer", "=", "custom_reg_W_mid", ",", "\n", "bias_regularizer", "=", "custom_reg_b_mid", ")", ")", "\n", "\n", "model", ".", "add", "(", "Dense", "(", "D", ",", "activation", "=", "'linear'", ",", "use_bias", "=", "False", ",", "\n", "kernel_initializer", "=", "keras", ".", "initializers", ".", "Constant", "(", "value", "=", "W_last_init", ")", ",", "\n", "kernel_regularizer", "=", "custom_reg_W_last", ")", ")", "\n", "\n", "model", ".", "compile", "(", "loss", "=", "'mean_squared_error'", ",", "\n", "optimizer", "=", "keras", ".", "optimizers", ".", "Adam", "(", "lr", "=", "l_rate", ")", ")", "\n", "\n", "self", ".", "model", "=", "model", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.PNN.PNN.fit": [[138, 152], ["keras.callbacks.ModelCheckpoint", "keras.callbacks.ModelCheckpoint", "keras.callbacks.ModelCheckpoint", "keras.callbacks.ModelCheckpoint", "keras.callbacks.EarlyStopping", "keras.callbacks.EarlyStopping", "keras.callbacks.EarlyStopping", "keras.callbacks.EarlyStopping", "PNN.PNN.model.fit", "str", "str"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.fit"], ["", "def", "fit", "(", "self", ",", "X", ",", "Y", ",", "batch_size", "=", "32", ",", "epochs", "=", "30", ",", "verbose", "=", "2", ",", "validation_split", "=", "0.1", ")", ":", "\n", "        ", "\"\"\"\n        Trains model\n        :param epochs: defines how many times each training point is revisited during training time\n        \"\"\"", "\n", "# save checkpoints", "\n", "weights_file_std", "=", "'./folder_models/PNN_'", "+", "str", "(", "self", ".", "ens_num", ")", "+", "'_'", "+", "str", "(", "self", ".", "env_name", ")", "+", "'_check_point_weights.h5'", "\n", "model_checkpoint", "=", "keras", ".", "callbacks", ".", "ModelCheckpoint", "(", "weights_file_std", ",", "monitor", "=", "'val_loss'", ",", "save_best_only", "=", "True", ",", "\n", "save_weights_only", "=", "True", ",", "mode", "=", "'auto'", ",", "verbose", "=", "0", ")", "\n", "\n", "Early_Stop", "=", "keras", ".", "callbacks", ".", "EarlyStopping", "(", "patience", "=", "10", ",", "restore_best_weights", "=", "True", ")", "\n", "self", ".", "model", ".", "fit", "(", "X", ",", "Y", ",", "epochs", "=", "epochs", ",", "\n", "batch_size", "=", "batch_size", ",", "verbose", "=", "verbose", ",", "\n", "validation_split", "=", "validation_split", ",", "callbacks", "=", "[", "Early_Stop", ",", "\n", "#model_checkpoint", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.PNN.PNN.predict": [[155, 158], ["PNN.PNN.model.predict"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.predict"], ["", "def", "predict", "(", "self", ",", "x_test", ",", "verbose", "=", "0", ")", ":", "\n", "        ", "y_pred", "=", "self", ".", "model", ".", "predict", "(", "x_test", ",", "verbose", "=", "verbose", ")", "\n", "return", "y_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.PNN.PNN.summary": [[159, 161], ["PNN.PNN.model.summary"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.summary"], ["", "def", "summary", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "summary", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.PNN.PNN.get_weights": [[162, 166], ["PNN.PNN.model.get_weights"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.get_weights"], ["", "def", "get_weights", "(", "self", ")", ":", "\n", "        ", "weights", "=", "self", ".", "model", ".", "get_weights", "(", ")", "\n", "self", ".", "weights", "=", "weights", "\n", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.PNN.PNN.set_weights": [[167, 169], ["PNN.PNN.model.set_weights"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.set_weights"], ["", "def", "set_weights", "(", "self", ",", "weights", ")", ":", "\n", "        ", "self", ".", "model", ".", "set_weights", "(", "weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.PNN.ens_PNNs.__init__": [[175, 209], ["range", "print", "NNs.append", "NNs[].summary", "PNN.PNN"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.summary"], ["def", "__init__", "(", "self", ",", "env", ",", "reg", "=", "'anc'", ",", "n_hidden", "=", "40", ",", "activation_in", "=", "'relu'", ",", "data_noise", "=", "0.001", ",", "n_data", "=", "32", ",", "epochs", "=", "30", ",", "\n", "l_rate", "=", "0.001", ",", "n_ensemble", "=", "5", ",", "ens_num", "=", "0", ",", "env_name", "=", "'Pendulum-v0'", ")", ":", "\n", "        ", "\"\"\"\n        :env: environment\n        :param reg: type of regularisation to use - anc (anchoring) reg (regularised) free (unconstrained)\n        :param n_hidden: units per layer\n        :param activation_in: activation hidden layers - relu tanh sigmoid\n        :param data_noise: estimated data_noise\n        :param n_data: number of data \n        :param epochs: epochs\n        :param l_rate: lr\n        :param n_ensemble: number of NNs\n        \"\"\"", "\n", "\n", "self", ".", "env", "=", "env", "\n", "self", ".", "reg", "=", "reg", "\n", "self", ".", "n_hidden", "=", "n_hidden", "\n", "self", ".", "activation_in", "=", "activation_in", "\n", "self", ".", "data_noise", "=", "data_noise", "\n", "self", ".", "n_data", "=", "n_data", "\n", "self", ".", "epochs", "=", "epochs", "\n", "self", ".", "l_rate", "=", "l_rate", "\n", "self", ".", "n_ensemble", "=", "n_ensemble", "\n", "self", ".", "ens_num", "=", "ens_num", "\n", "self", ".", "env_name", "=", "env_name", "\n", "\n", "NNs", "=", "[", "]", "\n", "for", "m", "in", "range", "(", "self", ".", "n_ensemble", ")", ":", "\n", "            ", "NNs", ".", "append", "(", "PNN", "(", "env", "=", "self", ".", "env", ",", "reg", "=", "self", ".", "reg", ",", "n_hidden", "=", "self", ".", "n_hidden", ",", "activation_in", "=", "self", ".", "activation_in", ",", "\n", "data_noise", "=", "self", ".", "data_noise", ",", "n_data", "=", "self", ".", "n_data", ",", "epochs", "=", "self", ".", "epochs", ",", "l_rate", "=", "self", ".", "l_rate", ",", "ens_num", "=", "m", ",", "\n", "env_name", "=", "self", ".", "env_name", ")", ")", "\n", "", "print", "(", "NNs", "[", "-", "1", "]", ".", "summary", "(", ")", ")", "\n", "self", ".", "NNs", "=", "NNs", "\n", "return", "#NNs ", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.PNN.ens_PNNs.train": [[210, 222], ["range", "len", "print", "PNN.ens_PNNs.NNs[].fit", "str", "str"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.fit"], ["", "def", "train", "(", "self", ",", "X_train", ",", "y_train", ",", "batch_size", "=", "32", ",", "validation_split", "=", "0.1", ")", ":", "\n", "\n", "        ", "NNs_hist_train", "=", "[", "]", "\n", "for", "m", "in", "range", "(", "len", "(", "self", ".", "NNs", ")", ")", ":", "\n", "            ", "print", "(", "'-- training: '", "+", "str", "(", "m", "+", "1", ")", "+", "' of '", "+", "str", "(", "self", ".", "n_ensemble", ")", "+", "' NNs --'", ")", "\n", "hist", "=", "self", ".", "NNs", "[", "m", "]", ".", "fit", "(", "X_train", ",", "y_train", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "epochs", "=", "self", ".", "epochs", ",", "\n", "verbose", "=", "0", ",", "\n", "validation_split", "=", "validation_split", ")", "\n", "#NNs_hist_train.append(hist.history['loss'])", "\n", "", "return", "NNs_hist_train", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.PNN.ens_PNNs.predict_ensemble": [[223, 240], ["range", "numpy.array", "numpy.mean", "numpy.std", "numpy.sqrt", "len", "numpy.array.append", "PNN.ens_PNNs.NNs[].predict", "numpy.square"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.predict"], ["", "def", "predict_ensemble", "(", "self", ",", "x_test", ")", ":", "\n", "        ", "''' fn to predict given a list of NNs (an ensemble)'''", "\n", "y_preds", "=", "[", "]", "\n", "for", "m", "in", "range", "(", "len", "(", "self", ".", "NNs", ")", ")", ":", "\n", "            ", "y_preds", ".", "append", "(", "self", ".", "NNs", "[", "m", "]", ".", "predict", "(", "x_test", ",", "verbose", "=", "0", ")", ")", "\n", "", "y_preds", "=", "np", ".", "array", "(", "y_preds", ")", "# predictions for all the NNs", "\n", "y_preds_mu", "=", "np", ".", "mean", "(", "y_preds", ",", "axis", "=", "0", ")", "# mean prediction", "\n", "y_preds_std", "=", "np", ".", "std", "(", "y_preds", ",", "axis", "=", "0", ",", "ddof", "=", "1", ")", "# epistemic uncertainty ", "\n", "\n", "# add on data noise", "\n", "y_preds_std", "=", "np", ".", "sqrt", "(", "np", ".", "square", "(", "y_preds_std", ")", "+", "self", ".", "data_noise", ")", "\n", "\n", "self", ".", "y_preds_mu", "=", "y_preds_mu", "\n", "self", ".", "y_preds_std", "=", "y_preds_std", "\n", "self", ".", "y_preds", "=", "y_preds", "\n", "\n", "return", "y_preds_mu", ",", "y_preds", ",", "y_preds_std", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.PNN.ens_PNNs.uncertainty": [[242, 246], ["numpy.var"], "methods", ["None"], ["", "def", "uncertainty", "(", "self", ")", ":", "\n", "        ", "y_preds_mu", ",", "y_preds", ",", "y_preds_std", "=", "self", ".", "y_preds_mu", ",", "self", ".", "y_preds", ",", "self", ".", "y_preds_std", "\n", "epistemic_uncertainty", "=", "np", ".", "var", "(", "y_preds", ",", "0", ")", "#.mean(0)", "\n", "return", "epistemic_uncertainty", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.PNN.ens_PNNs.get_weights": [[247, 253], ["range", "len", "weights.append", "PNN.ens_PNNs.NNs[].get_weights"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.get_weights"], ["", "def", "get_weights", "(", "self", ")", ":", "\n", "        ", "weights", "=", "[", "]", "\n", "for", "n", "in", "range", "(", "len", "(", "self", ".", "NNs", ")", ")", ":", "\n", "            ", "weights", ".", "append", "(", "self", ".", "NNs", "[", "n", "]", ".", "get_weights", "(", ")", ")", "\n", "", "self", ".", "weights", "=", "weights", "\n", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.PNN.ens_PNNs.set_weights": [[254, 257], ["range", "len", "PNN.ens_PNNs.NNs[].set_weights"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.set_weights"], ["", "def", "set_weights", "(", "self", ",", "weights", ")", ":", "\n", "        ", "for", "n", "in", "range", "(", "len", "(", "self", ".", "NNs", ")", ")", ":", "\n", "            ", "self", ".", "NNs", "[", "n", "]", ".", "set_weights", "(", "weights", "[", "n", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.deterministicNN.NN.__init__": [[12, 46], ["keras.models.Sequential", "keras.models.Sequential.add", "range", "keras.models.Sequential.add", "keras.optimizers.Adam", "keras.models.Sequential.compile", "keras.layers.InputLayer", "keras.models.Sequential.add", "keras.layers.Dense", "keras.layers.Dense"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "env", ",", "nb_layers", "=", "3", ",", "n_hidden", "=", "32", ",", "activation_in", "=", "'relu'", ",", "\n", "kernel_initializer", "=", "\"he_normal\"", ",", "epochs", "=", "30", ",", "l_rate", "=", "0.001", ",", "ens_num", "=", "0", ",", "env_name", "=", "'Pendulum-v0'", ")", ":", "\n", "        ", "\"\"\"\n        :env: environment\n        :param n_hidden: units per layer\n        :param nb_layers: number of hidden layers\n        :param activation_in: activation hidden layers\n        :param kernel_initializer: kernel initializer\n        :param data_noise: estimated data_noise\n        :param epochs: epochs\n        :param l_rate: lr\n        \"\"\"", "\n", "self", ".", "env", "=", "env", "\n", "self", ".", "nb_layers", "=", "nb_layers", "\n", "self", ".", "n_hidden", "=", "n_hidden", "\n", "self", ".", "activation_in", "=", "activation_in", "\n", "self", ".", "kernel_initializer", "=", "kernel_initializer", "\n", "self", ".", "epochs", "=", "epochs", "\n", "self", ".", "l_rate", "=", "l_rate", "\n", "self", ".", "ens_num", "=", "ens_num", "\n", "self", ".", "env_name", "=", "env_name", "\n", "\n", "D", "=", "self", ".", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "#output shape", "\n", "A", "=", "self", ".", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "Q", "=", "D", "+", "A", "# input shape", "\n", "\n", "model", "=", "keras", ".", "models", ".", "Sequential", "(", ")", "\n", "model", ".", "add", "(", "keras", ".", "layers", ".", "InputLayer", "(", "input_shape", "=", "[", "Q", "]", ")", ")", "\n", "for", "layer", "in", "range", "(", "self", ".", "nb_layers", ")", ":", "\n", "            ", "model", ".", "add", "(", "keras", ".", "layers", ".", "Dense", "(", "self", ".", "n_hidden", ",", "activation", "=", "self", ".", "activation_in", ",", "kernel_initializer", "=", "self", ".", "kernel_initializer", ")", ")", "\n", "", "model", ".", "add", "(", "keras", ".", "layers", ".", "Dense", "(", "D", ")", ")", "\n", "optimizer", "=", "keras", ".", "optimizers", ".", "Adam", "(", "lr", "=", "self", ".", "l_rate", ")", "\n", "model", ".", "compile", "(", "loss", "=", "\"mse\"", ",", "optimizer", "=", "optimizer", ")", "\n", "self", ".", "model", "=", "model", "\n", "#return #model", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.deterministicNN.NN.fit": [[48, 62], ["keras.callbacks.ModelCheckpoint", "keras.callbacks.EarlyStopping", "deterministicNN.NN.model.fit", "str", "str"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.fit"], ["", "def", "fit", "(", "self", ",", "X", ",", "Y", ",", "batch_size", "=", "32", ",", "epochs", "=", "30", ",", "verbose", "=", "2", ",", "validation_split", "=", "0.1", ")", ":", "\n", "        ", "\"\"\"\n        Trains model\n        :param epochs: defines how many times each training point is revisited during training time\n        \"\"\"", "\n", "# save checkpoints", "\n", "weights_file_std", "=", "'./folder_models/NN_'", "+", "str", "(", "self", ".", "ens_num", ")", "+", "'_'", "+", "str", "(", "self", ".", "env_name", ")", "+", "'_check_point_weights.h5'", "\n", "model_checkpoint", "=", "keras", ".", "callbacks", ".", "ModelCheckpoint", "(", "weights_file_std", ",", "monitor", "=", "'val_loss'", ",", "save_best_only", "=", "True", ",", "\n", "save_weights_only", "=", "True", ",", "mode", "=", "'auto'", ",", "verbose", "=", "0", ")", "\n", "\n", "Early_Stop", "=", "keras", ".", "callbacks", ".", "EarlyStopping", "(", "patience", "=", "10", ",", "restore_best_weights", "=", "True", ")", "\n", "self", ".", "model", ".", "fit", "(", "X", ",", "Y", ",", "epochs", "=", "epochs", ",", "\n", "batch_size", "=", "batch_size", ",", "verbose", "=", "verbose", ",", "\n", "validation_split", "=", "validation_split", ",", "callbacks", "=", "[", "Early_Stop", "#, model_checkpoint", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.deterministicNN.NN.predict": [[64, 67], ["deterministicNN.NN.model.predict"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.predict"], ["", "def", "predict", "(", "self", ",", "x_test", ",", "verbose", "=", "0", ")", ":", "\n", "        ", "y_pred", "=", "self", ".", "model", ".", "predict", "(", "x_test", ",", "verbose", "=", "verbose", ")", "\n", "return", "y_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.deterministicNN.NN.summary": [[68, 70], ["deterministicNN.NN.model.summary"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.summary"], ["", "def", "summary", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "summary", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.deterministicNN.NN.get_weights": [[71, 75], ["deterministicNN.NN.model.get_weights"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.get_weights"], ["", "def", "get_weights", "(", "self", ")", ":", "\n", "        ", "weights", "=", "self", ".", "model", ".", "get_weights", "(", ")", "\n", "self", ".", "weights", "=", "weights", "\n", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.deterministicNN.NN.set_weights": [[76, 78], ["deterministicNN.NN.model.set_weights"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.set_weights"], ["", "def", "set_weights", "(", "self", ",", "weights", ")", ":", "\n", "        ", "self", ".", "model", ".", "set_weights", "(", "weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.deterministicNN.ens_NNs.__init__": [[83, 115], ["range", "print", "NNs.append", "NNs[].summary", "deterministicNN.NN"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.summary"], ["def", "__init__", "(", "self", ",", "env", ",", "nb_layers", "=", "3", ",", "n_hidden", "=", "32", ",", "activation_in", "=", "'relu'", ",", "kernel_initializer", "=", "\"he_normal\"", ",", "\n", "epochs", "=", "30", ",", "l_rate", "=", "0.001", ",", "n_ensemble", "=", "5", ",", "ens_num", "=", "0", ",", "env_name", "=", "'Pendulum-v0'", ")", ":", "\n", "        ", "\"\"\"\n        :env: environment\n        :param n_hidden: units per layer\n        :param nb_layers: number of hidden layers\n        :param activation_in: activation hidden layers\n        :param kernel_initializer: kernel initializer\n        :param data_noise: estimated data_noise\n        :param epochs: epochs\n        :param l_rate: lr\n        :param n_ensemble: number of NNs\n        \"\"\"", "\n", "\n", "self", ".", "env", "=", "env", "\n", "self", ".", "nb_layers", "=", "nb_layers", "\n", "self", ".", "n_hidden", "=", "n_hidden", "\n", "self", ".", "activation_in", "=", "activation_in", "\n", "self", ".", "kernel_initializer", "=", "kernel_initializer", "\n", "self", ".", "epochs", "=", "epochs", "\n", "self", ".", "l_rate", "=", "l_rate", "\n", "self", ".", "n_ensemble", "=", "n_ensemble", "\n", "self", ".", "ens_num", "=", "ens_num", "\n", "self", ".", "env_name", "=", "env_name", "\n", "\n", "NNs", "=", "[", "]", "\n", "for", "m", "in", "range", "(", "self", ".", "n_ensemble", ")", ":", "\n", "            ", "NNs", ".", "append", "(", "NN", "(", "env", "=", "self", ".", "env", ",", "nb_layers", "=", "self", ".", "nb_layers", ",", "n_hidden", "=", "self", ".", "n_hidden", ",", "activation_in", "=", "self", ".", "activation_in", ",", "\n", "kernel_initializer", "=", "self", ".", "kernel_initializer", ",", "epochs", "=", "self", ".", "epochs", ",", "l_rate", "=", "self", ".", "l_rate", ",", "ens_num", "=", "m", ",", "\n", "env_name", "=", "self", ".", "env_name", ")", ")", "\n", "", "print", "(", "NNs", "[", "-", "1", "]", ".", "summary", "(", ")", ")", "\n", "self", ".", "NNs", "=", "NNs", "\n", "#return NNs ", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.deterministicNN.ens_NNs.train": [[117, 130], ["range", "len", "print", "deterministicNN.ens_NNs.NNs[].fit", "str", "str"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.fit"], ["", "def", "train", "(", "self", ",", "X_train", ",", "y_train", ",", "batch_size", "=", "32", ",", "validation_split", "=", "0.1", ")", ":", "\n", "\n", "\n", "        ", "NNs_hist_train", "=", "[", "]", "\n", "for", "m", "in", "range", "(", "len", "(", "self", ".", "NNs", ")", ")", ":", "\n", "            ", "print", "(", "'-- training: '", "+", "str", "(", "m", "+", "1", ")", "+", "' of '", "+", "str", "(", "self", ".", "n_ensemble", ")", "+", "' NNs --'", ")", "\n", "hist", "=", "self", ".", "NNs", "[", "m", "]", ".", "fit", "(", "X_train", ",", "y_train", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "epochs", "=", "self", ".", "epochs", ",", "\n", "verbose", "=", "0", ",", "\n", "validation_split", "=", "validation_split", ")", "\n", "#NNs_hist_train.append(hist.history['loss'])", "\n", "", "return", "NNs_hist_train", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.deterministicNN.ens_NNs.predict_ensemble": [[131, 145], ["range", "numpy.array", "numpy.mean", "numpy.std", "len", "numpy.array.append", "deterministicNN.ens_NNs.NNs[].predict"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.predict"], ["", "def", "predict_ensemble", "(", "self", ",", "x_test", ")", ":", "\n", "        ", "''' fn to predict given a list of NNs (an ensemble)'''", "\n", "y_preds", "=", "[", "]", "\n", "for", "m", "in", "range", "(", "len", "(", "self", ".", "NNs", ")", ")", ":", "\n", "            ", "y_preds", ".", "append", "(", "self", ".", "NNs", "[", "m", "]", ".", "predict", "(", "x_test", ",", "verbose", "=", "0", ")", ")", "\n", "", "y_preds", "=", "np", ".", "array", "(", "y_preds", ")", "\n", "y_preds_mu", "=", "np", ".", "mean", "(", "y_preds", ",", "axis", "=", "0", ")", "\n", "y_preds_std", "=", "np", ".", "std", "(", "y_preds", ",", "axis", "=", "0", ",", "ddof", "=", "1", ")", "\n", "\n", "self", ".", "y_preds_mu", "=", "y_preds_mu", "\n", "self", ".", "y_preds_std", "=", "y_preds_std", "\n", "self", ".", "y_preds", "=", "y_preds", "\n", "\n", "return", "y_preds_mu", ",", "y_preds", ",", "y_preds_std", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.deterministicNN.ens_NNs.uncertainty": [[147, 151], ["numpy.var"], "methods", ["None"], ["", "def", "uncertainty", "(", "self", ")", ":", "\n", "        ", "y_preds_mu", ",", "y_preds", ",", "y_preds_std", "=", "self", ".", "y_preds_mu", ",", "self", ".", "y_preds", ",", "self", ".", "y_preds_std", "\n", "epistemic_uncertainty", "=", "np", ".", "var", "(", "y_preds", ",", "0", ")", "#.mean(0)", "\n", "return", "epistemic_uncertainty", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.deterministicNN.ens_NNs.get_weights": [[152, 158], ["range", "len", "weights.append", "deterministicNN.ens_NNs.NNs[].get_weights"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.get_weights"], ["", "def", "get_weights", "(", "self", ")", ":", "\n", "        ", "weights", "=", "[", "]", "\n", "for", "n", "in", "range", "(", "len", "(", "self", ".", "NNs", ")", ")", ":", "\n", "            ", "weights", ".", "append", "(", "self", ".", "NNs", "[", "n", "]", ".", "get_weights", "(", ")", ")", "\n", "", "self", ".", "weights", "=", "weights", "\n", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.deterministicNN.ens_NNs.set_weights": [[159, 162], ["range", "len", "deterministicNN.ens_NNs.NNs[].set_weights"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.set_weights"], ["", "def", "set_weights", "(", "self", ",", "weights", ")", ":", "\n", "        ", "for", "n", "in", "range", "(", "len", "(", "self", ".", "NNs", ")", ")", ":", "\n", "            ", "self", ".", "NNs", "[", "n", "]", ".", "set_weights", "(", "weights", "[", "n", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.__init__": [[57, 69], ["keras.layers.wrappers.Wrapper.__init__", "numpy.log", "numpy.log", "numpy.log", "numpy.log"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.__init__"], ["def", "__init__", "(", "self", ",", "layer", ",", "weight_regularizer", "=", "1e-6", ",", "dropout_regularizer", "=", "1e-5", ",", "\n", "init_min", "=", "0.1", ",", "init_max", "=", "0.1", ",", "is_mc_dropout", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "'kernel_regularizer'", "not", "in", "kwargs", "\n", "super", "(", "ConcreteDropout", ",", "self", ")", ".", "__init__", "(", "layer", ",", "**", "kwargs", ")", "\n", "self", ".", "weight_regularizer", "=", "weight_regularizer", "\n", "self", ".", "dropout_regularizer", "=", "dropout_regularizer", "\n", "self", ".", "is_mc_dropout", "=", "is_mc_dropout", "\n", "self", ".", "supports_masking", "=", "True", "\n", "self", ".", "p_logit", "=", "None", "\n", "self", ".", "p", "=", "None", "\n", "self", ".", "init_min", "=", "np", ".", "log", "(", "init_min", ")", "-", "np", ".", "log", "(", "1.", "-", "init_min", ")", "\n", "self", ".", "init_max", "=", "np", ".", "log", "(", "init_max", ")", "-", "np", ".", "log", "(", "1.", "-", "init_max", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.build": [[70, 94], ["keras.engine.InputSpec", "super().build", "ConcreteDropout.ConcreteDropout.layer.add_weight", "keras.backend.sigmoid", "numpy.prod", "keras.backend.sum", "ConcreteDropout.ConcreteDropout.layer.add_loss", "ConcreteDropout.ConcreteDropout.layer.build", "len", "keras.backend.log", "keras.backend.log", "keras.initializers.RandomUniform", "keras.backend.sum", "keras.backend.square"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.build", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.build"], ["", "def", "build", "(", "self", ",", "input_shape", "=", "None", ")", ":", "\n", "        ", "self", ".", "input_spec", "=", "InputSpec", "(", "shape", "=", "input_shape", ")", "\n", "if", "not", "self", ".", "layer", ".", "built", ":", "\n", "            ", "self", ".", "layer", ".", "build", "(", "input_shape", ")", "\n", "self", ".", "layer", ".", "built", "=", "True", "\n", "", "super", "(", "ConcreteDropout", ",", "self", ")", ".", "build", "(", ")", "\n", "\n", "# initialise p", "\n", "self", ".", "p_logit", "=", "self", ".", "layer", ".", "add_weight", "(", "name", "=", "'p_logit'", ",", "\n", "shape", "=", "(", "1", ",", ")", ",", "\n", "initializer", "=", "initializers", ".", "RandomUniform", "(", "self", ".", "init_min", ",", "self", ".", "init_max", ")", ",", "\n", "trainable", "=", "True", ")", "\n", "self", ".", "p", "=", "K", ".", "sigmoid", "(", "self", ".", "p_logit", "[", "0", "]", ")", "\n", "\n", "# initialise regulariser / prior KL term", "\n", "assert", "len", "(", "input_shape", ")", "==", "2", ",", "'this wrapper only supports Dense layers'", "\n", "input_dim", "=", "np", ".", "prod", "(", "input_shape", "[", "-", "1", "]", ")", "# we drop only last dim", "\n", "weight", "=", "self", ".", "layer", ".", "kernel", "\n", "kernel_regularizer", "=", "self", ".", "weight_regularizer", "*", "K", ".", "sum", "(", "K", ".", "square", "(", "weight", ")", ")", "/", "(", "1.", "-", "self", ".", "p", ")", "\n", "dropout_regularizer", "=", "self", ".", "p", "*", "K", ".", "log", "(", "self", ".", "p", ")", "\n", "dropout_regularizer", "+=", "(", "1.", "-", "self", ".", "p", ")", "*", "K", ".", "log", "(", "1.", "-", "self", ".", "p", ")", "\n", "dropout_regularizer", "*=", "self", ".", "dropout_regularizer", "*", "input_dim", "\n", "regularizer", "=", "K", ".", "sum", "(", "kernel_regularizer", "+", "dropout_regularizer", ")", "\n", "self", ".", "layer", ".", "add_loss", "(", "regularizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.compute_output_shape": [[95, 97], ["ConcreteDropout.ConcreteDropout.layer.compute_output_shape"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.compute_output_shape"], ["", "def", "compute_output_shape", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "return", "self", ".", "layer", ".", "compute_output_shape", "(", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.concrete_dropout": [[98, 121], ["keras.backend.cast_to_floatx", "keras.backend.random_uniform", "keras.backend.sigmoid", "keras.backend.epsilon", "keras.backend.log", "keras.backend.shape", "keras.backend.log", "keras.backend.log", "keras.backend.log"], "methods", ["None"], ["", "def", "concrete_dropout", "(", "self", ",", "x", ")", ":", "\n", "        ", "'''\n        Concrete dropout - used at training time (gradients can be propagated)\n        :param x: input\n        :return:  approx. dropped out input\n        '''", "\n", "eps", "=", "K", ".", "cast_to_floatx", "(", "K", ".", "epsilon", "(", ")", ")", "\n", "temp", "=", "0.1", "\n", "\n", "unif_noise", "=", "K", ".", "random_uniform", "(", "shape", "=", "K", ".", "shape", "(", "x", ")", ")", "\n", "drop_prob", "=", "(", "\n", "K", ".", "log", "(", "self", ".", "p", "+", "eps", ")", "\n", "-", "K", ".", "log", "(", "1.", "-", "self", ".", "p", "+", "eps", ")", "\n", "+", "K", ".", "log", "(", "unif_noise", "+", "eps", ")", "\n", "-", "K", ".", "log", "(", "1.", "-", "unif_noise", "+", "eps", ")", "\n", ")", "\n", "drop_prob", "=", "K", ".", "sigmoid", "(", "drop_prob", "/", "temp", ")", "\n", "random_tensor", "=", "1.", "-", "drop_prob", "\n", "\n", "retain_prob", "=", "1.", "-", "self", ".", "p", "\n", "x", "*=", "random_tensor", "\n", "x", "/=", "retain_prob", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.call": [[122, 131], ["ConcreteDropout.ConcreteDropout.layer.call", "keras.backend.in_train_phase", "ConcreteDropout.ConcreteDropout.concrete_dropout", "ConcreteDropout.ConcreteDropout.layer.call", "ConcreteDropout.ConcreteDropout.layer.call", "ConcreteDropout.ConcreteDropout.concrete_dropout"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.call", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.concrete_dropout", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.call", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.call", "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ConcreteDropout.concrete_dropout"], ["", "def", "call", "(", "self", ",", "inputs", ",", "training", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "is_mc_dropout", ":", "\n", "            ", "return", "self", ".", "layer", ".", "call", "(", "self", ".", "concrete_dropout", "(", "inputs", ")", ")", "\n", "", "else", ":", "\n", "            ", "def", "relaxed_dropped_inputs", "(", ")", ":", "\n", "                ", "return", "self", ".", "layer", ".", "call", "(", "self", ".", "concrete_dropout", "(", "inputs", ")", ")", "\n", "", "return", "K", ".", "in_train_phase", "(", "relaxed_dropped_inputs", ",", "\n", "self", ".", "layer", ".", "call", "(", "inputs", ")", ",", "\n", "training", "=", "training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.__init__": [[139, 195], ["keras.layers.Input", "range", "keras.layers.concatenate", "keras.models.Model", "ConcreteDropout.BNN.model.compile", "keras.backend.backend", "keras.backend.clear_session", "ConcreteDropout.ConcreteDropout", "ConcreteDropout.ConcreteDropout", "keras.backend.exp", "keras.backend.sum", "ConcreteDropout.ConcreteDropout", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "env", ",", "nb_units", "=", "32", ",", "nb_layers", "=", "3", ",", "activation", "=", "'relu'", ",", "n_data", "=", "32", ",", "\n", "\n", "dropout", "=", "0.1", ",", "T", "=", "10", ",", "tau", "=", "1.0", ",", "lengthscale", "=", "1e-4", ",", "ens_num", "=", "0", ",", "train_flag", "=", "True", ",", "env_name", "=", "'Pendulum-v0'", ")", ":", "\n", "        ", "\"\"\"\n        :env: environment\n        :param nb_units: units per layer\n        :param nb_layers: number of hidden layers\n        :param activation: layers activation\n        :param n_data: number of data\n        :param dropout: probability of perceptron being dropped out\n        :param T: number of samples during test time\n        :param tau: precision of prior\n        :param lengthscale: lengthscale\n        \"\"\"", "\n", "self", ".", "env", "=", "env", "\n", "D", "=", "self", ".", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "\n", "self", ".", "ens_num", "=", "ens_num", "\n", "self", ".", "env_name", "=", "env_name", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "T", "=", "T", "\n", "self", ".", "tau", "=", "tau", "\n", "self", ".", "lengthscale", "=", "lengthscale", "\n", "self", ".", "n_data", "=", "n_data", "\n", "\n", "self", ".", "weight_decay", "=", "(", "(", "1", "-", "self", ".", "dropout", ")", "*", "self", ".", "lengthscale", "**", "2", ")", "/", "(", "self", ".", "n_data", "*", "self", ".", "tau", ")", "\n", "self", ".", "nb_units", "=", "nb_units", "\n", "self", ".", "nb_layers", "=", "nb_layers", "\n", "self", ".", "activation", "=", "activation", "\n", "self", ".", "train_flag", "=", "train_flag", "\n", "\n", "if", "K", ".", "backend", "(", ")", "==", "'tensorflow'", ":", "\n", "            ", "K", ".", "clear_session", "(", ")", "\n", "", "N", "=", "self", ".", "n_data", "\n", "wd", "=", "self", ".", "lengthscale", "**", "2.", "/", "N", "\n", "dd", "=", "2.", "/", "N", "\n", "A", "=", "self", ".", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "Q", "=", "D", "+", "A", "\n", "inp", "=", "Input", "(", "shape", "=", "(", "Q", ",", ")", ")", "\n", "x", "=", "inp", "\n", "for", "_", "in", "range", "(", "nb_layers", ")", ":", "\n", "            ", "x", "=", "ConcreteDropout", "(", "Dense", "(", "self", ".", "nb_units", ",", "activation", "=", "self", ".", "activation", ")", ",", "weight_regularizer", "=", "wd", ",", "dropout_regularizer", "=", "dd", ")", "(", "x", ")", "\n", "", "mean", "=", "ConcreteDropout", "(", "Dense", "(", "D", ")", ",", "weight_regularizer", "=", "wd", ",", "dropout_regularizer", "=", "dd", ")", "(", "x", ")", "\n", "log_var", "=", "ConcreteDropout", "(", "Dense", "(", "D", ")", ",", "weight_regularizer", "=", "wd", ",", "dropout_regularizer", "=", "dd", ")", "(", "x", ")", "\n", "out", "=", "concatenate", "(", "[", "mean", ",", "log_var", "]", ")", "\n", "self", ".", "model", "=", "Model", "(", "inp", ",", "out", ")", "\n", "\n", "def", "heteroscedastic_loss_Gal", "(", "true", ",", "pred", ")", ":", "\n", "            ", "mean", "=", "pred", "[", ":", ",", ":", "D", "]", "\n", "log_var", "=", "pred", "[", ":", ",", "D", ":", "]", "\n", "precision", "=", "K", ".", "exp", "(", "-", "log_var", ")", "\n", "return", "K", ".", "sum", "(", "precision", "*", "(", "true", "-", "mean", ")", "**", "2.", "+", "log_var", ",", "-", "1", ")", "\n", "\n", "\n", "\n", "", "self", ".", "model", ".", "compile", "(", "optimizer", "=", "'adam'", ",", "loss", "=", "heteroscedastic_loss_Gal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.fit": [[197, 212], ["keras.callbacks.ModelCheckpoint", "keras.callbacks.EarlyStopping", "ConcreteDropout.BNN.model.fit", "str", "str"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.fit"], ["", "def", "fit", "(", "self", ",", "X", ",", "Y", ",", "batch_size", "=", "32", ",", "epochs", "=", "30", ",", "validation_split", "=", "0.1", ",", "verbose", "=", "2", ")", ":", "\n", "        ", "\"\"\"\n        Trains model\n        :param epochs: defines how many times each training point is revisited during training time\n        \"\"\"", "\n", "# save checkpoints", "\n", "weights_file_std", "=", "'./folder_models/BNN_'", "+", "str", "(", "self", ".", "ens_num", ")", "+", "'_'", "+", "str", "(", "self", ".", "env_name", ")", "+", "'_check_point_weights.h5'", "\n", "model_checkpoint", "=", "keras", ".", "callbacks", ".", "ModelCheckpoint", "(", "weights_file_std", ",", "monitor", "=", "'val_loss'", ",", "save_best_only", "=", "True", ",", "\n", "save_weights_only", "=", "True", ",", "mode", "=", "'auto'", ",", "verbose", "=", "0", ")", "\n", "\n", "Early_Stop", "=", "keras", ".", "callbacks", ".", "EarlyStopping", "(", "patience", "=", "10", ",", "restore_best_weights", "=", "True", ")", "\n", "#if self.train_flag:", "\n", "self", ".", "historyBNN", "=", "self", ".", "model", ".", "fit", "(", "X", ",", "Y", ",", "epochs", "=", "epochs", ",", "\n", "batch_size", "=", "batch_size", ",", "verbose", "=", "verbose", ",", "\n", "validation_split", "=", "validation_split", ",", "callbacks", "=", "[", "Early_Stop", "#, model_checkpoint", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.predict": [[216, 226], ["numpy.array", "numpy.mean", "ConcreteDropout.BNN.model.predict", "range"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.predict"], ["", "def", "predict", "(", "self", ",", "X_test", ")", ":", "\n", "        ", "D", "=", "self", ".", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "Yt_hat", "=", "np", ".", "array", "(", "[", "self", ".", "model", ".", "predict", "(", "X_test", ",", "verbose", "=", "0", ")", "for", "_", "in", "range", "(", "self", ".", "T", ")", "]", ")", "\n", "mean", "=", "Yt_hat", "[", ":", ",", ":", ",", ":", "D", "]", "\n", "logvar", "=", "Yt_hat", "[", ":", ",", ":", ",", "D", ":", "]", "\n", "MC_pred", "=", "np", ".", "mean", "(", "mean", ",", "0", ")", "\n", "self", ".", "MC_pred", "=", "MC_pred", "\n", "self", ".", "mean", "=", "mean", "\n", "self", ".", "logvar", "=", "logvar", "\n", "return", "MC_pred", ",", "mean", ",", "logvar", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.uncertainty": [[227, 233], ["numpy.var", "numpy.mean", "numpy.exp"], "methods", ["None"], ["", "def", "uncertainty", "(", "self", ")", ":", "\n", "        ", "MC_pred", ",", "means", ",", "logvars", "=", "self", ".", "MC_pred", ",", "self", ".", "mean", ",", "self", ".", "logvar", "\n", "epistemic_uncertainty", "=", "np", ".", "var", "(", "means", ",", "0", ")", "#.mean(0)", "\n", "logvar", "=", "np", ".", "mean", "(", "logvars", ",", "0", ")", "\n", "aleatoric_uncertainty", "=", "np", ".", "exp", "(", "logvar", ")", "#.mean(0)", "\n", "return", "epistemic_uncertainty", "#, aleatoric_uncertainty", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.summary": [[234, 236], ["ConcreteDropout.BNN.model.summary"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.summary"], ["", "def", "summary", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "summary", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.get_weights": [[237, 241], ["ConcreteDropout.BNN.model.get_weights"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.get_weights"], ["", "def", "get_weights", "(", "self", ")", ":", "\n", "        ", "weights", "=", "self", ".", "model", ".", "get_weights", "(", ")", "\n", "self", ".", "weights", "=", "weights", "\n", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.set_weights": [[242, 244], ["ConcreteDropout.BNN.model.set_weights"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.set_weights"], ["", "def", "set_weights", "(", "self", ",", "weights", ")", ":", "\n", "        ", "self", ".", "model", ".", "set_weights", "(", "weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.__init__": [[252, 293], ["range", "print", "NNs.append", "NNs[].summary", "ConcreteDropout.BNN"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.BNN.summary"], ["def", "__init__", "(", "self", ",", "env", ",", "nb_units", "=", "32", ",", "nb_layers", "=", "3", ",", "activation", "=", "'relu'", ",", "n_data", "=", "32", ",", "dropout", "=", "0.1", ",", "T", "=", "10", ",", "\n", "tau", "=", "1.0", ",", "lengthscale", "=", "1e-4", ",", "ens_num", "=", "0", ",", "n_ensemble", "=", "5", ",", "train_flag", "=", "True", ",", "env_name", "=", "'Pendulum-v0'", ")", ":", "\n", "        ", "\"\"\"\n        :env: environment\n        :param nb_units: units per layer\n        :param nb_layers: number of hidden layers\n        :param activation: layers activation\n        :param n_data: number of data\n        :param dropout: probability of perceptron being dropped out\n        :param T: number of samples during test time\n        :param tau: precision of prior\n        :param lengthscale: lengthscale\n        :param n_ensemble: number of BNNs\n        \"\"\"", "\n", "self", ".", "env", "=", "env", "\n", "#D = self.env.observation_space.shape[0]", "\n", "self", ".", "ens_num", "=", "ens_num", "\n", "self", ".", "n_ensemble", "=", "n_ensemble", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "T", "=", "T", "\n", "self", ".", "tau", "=", "tau", "\n", "self", ".", "lengthscale", "=", "lengthscale", "\n", "self", ".", "n_data", "=", "n_data", "\n", "\n", "self", ".", "weight_decay", "=", "(", "(", "1", "-", "self", ".", "dropout", ")", "*", "self", ".", "lengthscale", "**", "2", ")", "/", "(", "self", ".", "n_data", "*", "self", ".", "tau", ")", "\n", "self", ".", "nb_units", "=", "nb_units", "\n", "self", ".", "nb_layers", "=", "nb_layers", "\n", "self", ".", "activation", "=", "activation", "\n", "self", ".", "train_flag", "=", "train_flag", "\n", "self", ".", "env_name", "=", "env_name", "\n", "\n", "NNs", "=", "[", "]", "\n", "for", "m", "in", "range", "(", "self", ".", "n_ensemble", ")", ":", "\n", "#np.random.seed(seed=m)", "\n", "#random.seed(m)", "\n", "            ", "NNs", ".", "append", "(", "BNN", "(", "env", "=", "self", ".", "env", ",", "nb_units", "=", "self", ".", "nb_units", ",", "nb_layers", "=", "self", ".", "nb_layers", ",", "activation", "=", "self", ".", "activation", ",", "\n", "n_data", "=", "self", ".", "n_data", ",", "dropout", "=", "self", ".", "dropout", ",", "T", "=", "self", ".", "T", ",", "tau", "=", "self", ".", "tau", ",", "lengthscale", "=", "self", ".", "lengthscale", ",", "\n", "ens_num", "=", "m", ",", "train_flag", "=", "self", ".", "train_flag", ",", "env_name", "=", "self", ".", "env_name", ")", ")", "\n", "", "print", "(", "NNs", "[", "-", "1", "]", ".", "summary", "(", ")", ")", "\n", "self", ".", "NNs", "=", "NNs", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.train": [[295, 309], ["range", "len", "print", "ConcreteDropout.ens_BNN.NNs[].fit", "str", "str"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.fit"], ["", "def", "train", "(", "self", ",", "X_train", ",", "y_train", ",", "batch_size", "=", "32", ",", "epochs", "=", "30", ",", "validation_split", "=", "0.1", ")", ":", "\n", "\n", "        ", "NNs_hist_train", "=", "[", "]", "\n", "for", "m", "in", "range", "(", "len", "(", "self", ".", "NNs", ")", ")", ":", "\n", "#np.random.seed(seed=m)", "\n", "#random.seed(m)", "\n", "            ", "print", "(", "'-- training: '", "+", "str", "(", "m", "+", "1", ")", "+", "' of '", "+", "str", "(", "self", ".", "n_ensemble", ")", "+", "' NNs --'", ")", "\n", "hist", "=", "self", ".", "NNs", "[", "m", "]", ".", "fit", "(", "X_train", ",", "y_train", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "epochs", "=", "epochs", ",", "\n", "verbose", "=", "0", ",", "\n", "validation_split", "=", "validation_split", ")", "\n", "#NNs_hist_train.append(hist.history['loss'])", "\n", "", "return", "NNs_hist_train", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.predict_ensemble": [[310, 335], ["range", "numpy.array", "numpy.array", "numpy.array", "numpy.mean", "numpy.mean", "numpy.mean", "len", "ConcreteDropout.ens_BNN.NNs[].predict", "numpy.array.append", "numpy.array.append", "numpy.array.append"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.predict"], ["", "def", "predict_ensemble", "(", "self", ",", "x_test", ")", ":", "\n", "        ", "''' fn to predict given a list of NNs (an ensemble)'''", "\n", "\n", "MC_preds", "=", "[", "]", "\n", "means_preds", "=", "[", "]", "\n", "logvar_preds", "=", "[", "]", "\n", "for", "m", "in", "range", "(", "len", "(", "self", ".", "NNs", ")", ")", ":", "\n", "#np.random.seed(seed=m)", "\n", "#random.seed(m)", "\n", "            ", "MC_pred", ",", "mean", ",", "logvar", "=", "self", ".", "NNs", "[", "m", "]", ".", "predict", "(", "x_test", ")", "\n", "MC_preds", ".", "append", "(", "MC_pred", ")", "\n", "means_preds", ".", "append", "(", "mean", ")", "\n", "logvar_preds", ".", "append", "(", "logvar", ")", "\n", "", "MC_preds", "=", "np", ".", "array", "(", "MC_preds", ")", "# mean predictions for all the NNs", "\n", "means_preds", "=", "np", ".", "array", "(", "means_preds", ")", "# mean predictions for all the NNs and all the samples", "\n", "logvar_preds", "=", "np", ".", "array", "(", "logvar_preds", ")", "# var predictions for all the NNs and all the samples", "\n", "MC_pred", "=", "np", ".", "mean", "(", "MC_preds", ",", "axis", "=", "0", ")", "# mean prediction averaged over the NNs", "\n", "means", "=", "np", ".", "mean", "(", "means_preds", ",", "0", ")", "# mean predictions for all the samples (averaged over the NNs)", "\n", "logvars", "=", "np", ".", "mean", "(", "logvar_preds", ",", "0", ")", "# var predictions for all the samples (averaged over the NNs)", "\n", "\n", "self", ".", "MC_pred", "=", "MC_pred", "\n", "self", ".", "means", "=", "means", "\n", "self", ".", "logvars", "=", "logvars", "\n", "\n", "return", "MC_pred", ",", "means", ",", "logvars", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.uncertainty": [[337, 343], ["numpy.var", "numpy.mean", "numpy.exp"], "methods", ["None"], ["", "def", "uncertainty", "(", "self", ")", ":", "\n", "        ", "MC_pred", ",", "means", ",", "logvars", "=", "self", ".", "MC_pred", ",", "self", ".", "means", ",", "self", ".", "logvars", "\n", "epistemic_uncertainty", "=", "np", ".", "var", "(", "means", ",", "0", ")", "#.mean(0)", "\n", "logvar", "=", "np", ".", "mean", "(", "logvars", ",", "0", ")", "\n", "aleatoric_uncertainty", "=", "np", ".", "exp", "(", "logvar", ")", "#.mean(0)", "\n", "return", "epistemic_uncertainty", "#, aleatoric_uncertainty", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.get_weights": [[344, 350], ["range", "len", "weights.append", "ConcreteDropout.ens_BNN.NNs[].get_weights"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.get_weights"], ["", "def", "get_weights", "(", "self", ")", ":", "\n", "        ", "weights", "=", "[", "]", "\n", "for", "n", "in", "range", "(", "len", "(", "self", ".", "NNs", ")", ")", ":", "\n", "            ", "weights", ".", "append", "(", "self", ".", "NNs", "[", "n", "]", ".", "get_weights", "(", ")", ")", "\n", "", "self", ".", "weights", "=", "weights", "\n", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.set_weights": [[351, 354], ["range", "len", "ConcreteDropout.ens_BNN.NNs[].set_weights"], "methods", ["home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.ConcreteDropout.ens_BNN.set_weights"], ["", "def", "set_weights", "(", "self", ",", "weights", ")", ":", "\n", "        ", "for", "n", "in", "range", "(", "len", "(", "self", ".", "NNs", ")", ")", ":", "\n", "            ", "self", ".", "NNs", "[", "n", "]", ".", "set_weights", "(", "weights", "[", "n", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.__init__": [[8, 41], ["numpy.random.uniform", "gpflow.kernels.Matern52", "gpflow.kernels.SharedIndependent", "GP.GP.env.reset().reshape", "GP.GP.env.action_space.sample().reshape", "numpy.concatenate", "gpflow.inducing_variables.SharedIndependentInducingVariables", "gpflow.models.SVGP", "gpflow.inducing_variables.InducingPoints", "gpflow.likelihoods.Gaussian", "gpflow.kernels.SquaredExponential", "gpflow.kernels.Linear", "GP.GP.env.reset", "GP.GP.env.action_space.sample"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "env", ",", "gp_model", ",", "kernel", "=", "'Matern52'", ",", "mean_function", "=", "None", ",", "batch_size", "=", "32", ")", ":", "\n", "        ", "\"\"\"\n        :env: environment\n        :gp_model: two GPs are defined, namely 'GPR' and 'SVGP'. The former is the one applied in the evaluation.\n        :param kernel: kernel function for the GP\n        \"\"\"", "\n", "self", ".", "env", "=", "env", "\n", "self", ".", "gp_model", "=", "gp_model", "\n", "self", ".", "kernel", "=", "kernel", "\n", "self", ".", "mean_function", "=", "mean_function", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "\n", "D", "=", "self", ".", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "A", "=", "self", ".", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "Q", "=", "D", "+", "A", "# input shape", "\n", "lengthscales", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ",", "Q", ")", "\n", "if", "self", ".", "gp_model", "==", "'GPR'", "and", "self", ".", "kernel", "==", "'Matern52'", ":", "\n", "            ", "k", "=", "gpflow", ".", "kernels", ".", "Matern52", "(", "lengthscales", "=", "lengthscales", ")", "\n", "", "elif", "self", ".", "gp_model", "==", "'SVGP'", ":", "\n", "            ", "k", "=", "gpflow", ".", "kernels", ".", "SharedIndependent", "(", "gpflow", ".", "kernels", ".", "SquaredExponential", "(", ")", "+", "gpflow", ".", "kernels", ".", "Linear", "(", ")", ",", "output_dim", "=", "D", ")", "\n", "state", "=", "self", ".", "env", ".", "reset", "(", ")", ".", "reshape", "(", "1", ",", "D", ")", "\n", "action", "=", "self", ".", "env", ".", "action_space", ".", "sample", "(", ")", ".", "reshape", "(", "1", ",", "A", ")", "\n", "Z", "=", "np", ".", "concatenate", "(", "(", "state", ",", "action", ")", ",", "axis", "=", "1", ")", "\n", "iv", "=", "gpflow", ".", "inducing_variables", ".", "SharedIndependentInducingVariables", "(", "gpflow", ".", "inducing_variables", ".", "InducingPoints", "(", "Z", ")", ")", "\n", "model", "=", "gpflow", ".", "models", ".", "SVGP", "(", "k", ",", "gpflow", ".", "likelihoods", ".", "Gaussian", "(", ")", ",", "inducing_variable", "=", "iv", ",", "num_latent_gps", "=", "D", ")", "\n", "self", ".", "model", "=", "model", "\n", "\n", "", "self", ".", "D", "=", "D", "\n", "self", ".", "Q", "=", "Q", "\n", "self", ".", "lengthscales", "=", "lengthscales", "\n", "self", ".", "k", "=", "k", "\n", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.fit": [[42, 74], ["print", "gpflow.optimizers.Scipy", "X.mean", "X.std", "Y.mean", "Y.std", "gpflow.models.GPR", "gpflow.ci_utils.ci_niter", "str", "gpflow.optimizers.Scipy.minimize", "numpy.array", "numpy.array", "GP.GP.model.training_loss_closure"], "methods", ["None"], ["", "def", "fit", "(", "self", ",", "X", ",", "Y", ",", "variance", "=", "0.001", ",", "optimize", "=", "True", ",", "maxiter", "=", "100", ")", ":", "\n", "\n", "        ", "print", "(", "'-- fitting gaussian process on '", "+", "str", "(", "X", ".", "shape", "[", "0", "]", ")", "+", "' data --'", ")", "\n", "\n", "opt", "=", "gpflow", ".", "optimizers", ".", "Scipy", "(", ")", "\n", "mean_X", "=", "X", ".", "mean", "(", ")", "\n", "std_X", "=", "X", ".", "std", "(", ")", "\n", "mean_Y", "=", "Y", ".", "mean", "(", ")", "\n", "std_Y", "=", "Y", ".", "std", "(", ")", "\n", "X", "=", "(", "X", "-", "mean_X", ")", "/", "std_X", "\n", "Y", "=", "(", "Y", "-", "mean_Y", ")", "/", "std_Y", "\n", "self", ".", "mean_X", "=", "mean_X", "\n", "self", ".", "std_X", "=", "std_X", "\n", "self", ".", "mean_Y", "=", "mean_Y", "\n", "self", ".", "std_Y", "=", "std_Y", "\n", "if", "self", ".", "gp_model", "==", "'GPR'", ":", "\n", "            ", "model", "=", "gpflow", ".", "models", ".", "GPR", "(", "data", "=", "(", "np", ".", "array", "(", "X", ",", "dtype", "=", "float", ")", ",", "np", ".", "array", "(", "Y", ",", "dtype", "=", "float", ")", ")", ",", "kernel", "=", "self", ".", "k", ",", "\n", "mean_function", "=", "self", ".", "mean_function", ",", "noise_variance", "=", "variance", ")", "\n", "#model.likelihood.variance.assign(variance)", "\n", "#model.likelihood.variance.fixed = True", "\n", "#if optimize:", "\n", "#    opt_logs = opt.minimize(model.training_loss, model.trainable_variables, options=dict(maxiter=maxiter))", "\n", "self", ".", "model", "=", "model", "\n", "", "elif", "self", ".", "gp_model", "==", "'SVGP'", ":", "\n", "            ", "data", "=", "X", ",", "Y", "\n", "MAXITER", "=", "ci_niter", "(", "2000", ")", "\n", "#self.model.likelihood.variance.assign(variance)", "\n", "if", "optimize", ":", "\n", "                ", "opt", ".", "minimize", "(", "self", ".", "model", ".", "training_loss_closure", "(", "data", ")", ",", "variables", "=", "self", ".", "model", ".", "trainable_variables", ",", "\n", "method", "=", "\"l-bfgs-b\"", ",", "options", "=", "{", "\"maxiter\"", ":", "MAXITER", "}", ",", ")", "\n", "\n", "", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.predict": [[75, 85], ["GP.GP.model.predict_f", "numpy.array"], "methods", ["None"], ["", "def", "predict", "(", "self", ",", "x_test", ")", ":", "\n", "\n", "        ", "x_test", "=", "(", "x_test", "-", "self", ".", "mean_X", ")", "/", "self", ".", "std_X", "\n", "mean", ",", "var", "=", "self", ".", "model", ".", "predict_f", "(", "np", ".", "array", "(", "x_test", ",", "dtype", "=", "float", ")", ")", "\n", "mean", "=", "mean", "*", "self", ".", "std_Y", "+", "self", ".", "mean_Y", "\n", "var", "=", "var", "*", "self", ".", "std_Y", "\n", "self", ".", "mean", "=", "mean", "\n", "self", ".", "var", "=", "var", "\n", "means_tot", "=", "np", ".", "nan", "# just for compatibility with other models", "\n", "return", "mean", ",", "means_tot", ",", "var", "\n", "\n"]], "home.repos.pwc.inspect_result.giarcieri_assessing-the-influence-of-models-on-the-performance-of-reinforcement-learning-algorithms.models.GP.GP.uncertainty": [[86, 89], ["None"], "methods", ["None"], ["", "def", "uncertainty", "(", "self", ")", ":", "\n", "        ", "epistemic_uncertainty", "=", "self", ".", "var", "\n", "return", "epistemic_uncertainty", "\n", "\n"]]}