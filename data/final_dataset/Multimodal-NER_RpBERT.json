{"home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.mner.BertInputFeatures.__init__": [[22, 37], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "unique_id", ",", "\n", "tokens", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "input_type_ids", ",", "\n", "token_subtoken_count", ",", "\n", ")", ":", "\n", "        ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "input_type_ids", "=", "input_type_ids", "\n", "self", ".", "token_subtoken_count", "=", "token_subtoken_count", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.mner.MNER.__init__": [[40, 60], ["super().__init__", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "embeddings", ",", "pretrain_model", ",", "pretrained_weight", "=", "None", ",", "num_of_tags", "=", "10", ")", ":", "\n", "        ", "super", "(", "MNER", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "params", "=", "params", "\n", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "self", ".", "layer_indexes", "=", "[", "-", "1", "]", "\n", "self", ".", "pooling_operation", "=", "\"first\"", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "self", ".", "name", "=", "'bert-base-uncased'", "\n", "\n", "self", ".", "input_embeddding_size", "=", "768", "+", "embeddings", ".", "embedding_length", "\n", "lstm_input_size", "=", "self", ".", "input_embeddding_size", "\n", "if", "self", ".", "params", ".", "pretrain_load", "==", "1", ":", "\n", "            ", "self", ".", "pretrain_model", "=", "pretrain_model", "\n", "", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "params", ".", "dropout", ")", "\n", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "input_size", "=", "lstm_input_size", ",", "hidden_size", "=", "params", ".", "hidden_dimension", "//", "2", ",", "\n", "num_layers", "=", "params", ".", "n_layers", ",", "bidirectional", "=", "True", ")", "\n", "\n", "self", ".", "projection", "=", "nn", ".", "Linear", "(", "in_features", "=", "params", ".", "hidden_dimension", ",", "out_features", "=", "num_of_tags", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.mner.MNER._convert_sentences_to_features": [[61, 113], ["enumerate", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "mner.MNER.tokenizer.convert_tokens_to_ids", "features.append", "mner.MNER.tokenizer.tokenize", "bert_tokenization.extend", "len", "len", "tokens.append", "input_type_ids.append", "len", "len", "mner.MNER.append", "input_mask.append", "input_type_ids.append", "mner.BertInputFeatures"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize"], ["", "def", "_convert_sentences_to_features", "(", "\n", "self", ",", "sentences", ",", "max_sequence_length", ":", "int", "\n", ")", "->", "[", "BertInputFeatures", "]", ":", "\n", "\n", "        ", "max_sequence_length", "=", "max_sequence_length", "+", "2", "\n", "\n", "features", ":", "List", "[", "BertInputFeatures", "]", "=", "[", "]", "\n", "for", "(", "sentence_index", ",", "sentence", ")", "in", "enumerate", "(", "sentences", ")", ":", "\n", "\n", "            ", "bert_tokenization", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "token_subtoken_count", ":", "Dict", "[", "int", ",", "int", "]", "=", "{", "}", "\n", "\n", "for", "token", "in", "sentence", ":", "\n", "                ", "subtokens", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ".", "text", ")", "\n", "bert_tokenization", ".", "extend", "(", "subtokens", ")", "\n", "token_subtoken_count", "[", "token", ".", "idx", "]", "=", "len", "(", "subtokens", ")", "\n", "\n", "", "if", "len", "(", "bert_tokenization", ")", ">", "max_sequence_length", "-", "2", ":", "\n", "                ", "bert_tokenization", "=", "bert_tokenization", "[", "0", ":", "(", "max_sequence_length", "-", "2", ")", "]", "\n", "\n", "", "tokens", "=", "[", "]", "\n", "input_type_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "bert_tokenization", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "input_ids", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_sequence_length", ":", "\n", "                ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "BertInputFeatures", "(", "\n", "unique_id", "=", "sentence_index", ",", "\n", "tokens", "=", "tokens", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "input_type_ids", "=", "input_type_ids", ",", "\n", "token_subtoken_count", "=", "token_subtoken_count", ",", "\n", ")", "\n", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.mner.MNER._add_embeddings_internal": [[114, 175], ["len", "mner.MNER._convert_sentences_to_features", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "mner.MNER.pretrain_model.mybert", "enumerate", "max", "mner.MNER.pretrain_model", "enumerate", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "subtoken_embeddings.append", "mner.MNER.tokenizer.tokenize", "token.set_embedding", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "token.set_embedding", "sentence.to_tokenized_string", "embedding.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BertEmbeddings._convert_sentences_to_features", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ",", "img_obj", ",", "relation", "=", "None", ")", ":", "\n", "# sentences = [sentence for sentence in x_flair]", "\n", "        ", "longest_sentence_in_batch", ":", "int", "=", "len", "(", "\n", "max", "(", "\n", "[", "\n", "self", ".", "tokenizer", ".", "tokenize", "(", "sentence", ".", "to_tokenized_string", "(", ")", ")", "\n", "for", "sentence", "in", "sentences", "\n", "]", ",", "\n", "key", "=", "len", ",", "\n", ")", "\n", ")", "\n", "\n", "features", "=", "self", ".", "_convert_sentences_to_features", "(", "\n", "sentences", ",", "longest_sentence_in_batch", "\n", ")", "\n", "all_input_ids", "=", "torch", ".", "LongTensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "features", "]", ")", ".", "to", "(", "\n", "flair", ".", "device", "\n", ")", "\n", "all_input_masks", "=", "torch", ".", "LongTensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "features", "]", ")", ".", "to", "(", "\n", "flair", ".", "device", "\n", ")", "\n", "\n", "if", "relation", "is", "None", ":", "\n", "            ", "pair_out", "=", "self", ".", "pretrain_model", "(", "all_input_ids", ",", "img_obj", ",", "None", ",", "None", ")", "\n", "return", "pair_out", "\n", "\n", "", "all_encoder_layers", ",", "attention_probs", "=", "self", ".", "pretrain_model", ".", "mybert", "(", "img_obj", ",", "all_input_ids", ",", "relation", ")", "\n", "\n", "for", "sentence_index", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "\n", "            ", "feature", "=", "features", "[", "sentence_index", "]", "\n", "\n", "# get aggregated embeddings for each BERT-subtoken in sentence", "\n", "subtoken_embeddings", "=", "[", "]", "\n", "for", "token_index", ",", "_", "in", "enumerate", "(", "feature", ".", "tokens", ")", ":", "\n", "                ", "subtoken_embeddings", ".", "append", "(", "all_encoder_layers", "[", "sentence_index", "]", "[", "token_index", "]", ")", "\n", "\n", "# get the current sentence object", "\n", "", "token_idx", "=", "0", "\n", "for", "token", "in", "sentence", ":", "\n", "# add concatenated embedding to sentence", "\n", "                ", "token_idx", "+=", "1", "\n", "\n", "if", "self", ".", "pooling_operation", "==", "\"first\"", ":", "\n", "# use first subword embedding if pooling operation is 'first'", "\n", "                    ", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "subtoken_embeddings", "[", "token_idx", "]", ")", "\n", "", "else", ":", "\n", "# otherwise, do a mean over all subwords in token", "\n", "                    ", "embeddings", "=", "subtoken_embeddings", "[", "\n", "token_idx", ":", "token_idx", "\n", "+", "feature", ".", "token_subtoken_count", "[", "token", ".", "idx", "]", "\n", "]", "\n", "embeddings", "=", "[", "\n", "embedding", ".", "unsqueeze", "(", "0", ")", "for", "embedding", "in", "embeddings", "\n", "]", "\n", "mean", "=", "torch", ".", "mean", "(", "torch", ".", "cat", "(", "embeddings", ",", "dim", "=", "0", ")", ",", "dim", "=", "0", ")", "\n", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "mean", ")", "\n", "\n", "", "token_idx", "+=", "feature", ".", "token_subtoken_count", "[", "token", ".", "idx", "]", "-", "1", "\n", "\n", "", "", "return", "attention_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.mner.MNER.forward": [[176, 223], ["mner.MNER.embeddings.embed", "mner.MNER._add_embeddings_internal", "max", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "list", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "mner.MNER.dropout", "embeds.permute.permute.permute", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "mner.MNER.lstm", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "mner.MNER.permute", "mner.MNER.dropout", "mner.MNER.projection", "mner.MNER._add_embeddings_internal", "len", "sentence_lens.numpy", "mner.MNER.permute", "len", "pre_allocated_zero_tensor[].cuda", "list.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "token.get_each_embedding"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ConvTransformNetworkImageEmbeddings._add_embeddings_internal", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ConvTransformNetworkImageEmbeddings._add_embeddings_internal", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_each_embedding"], ["", "def", "forward", "(", "self", ",", "sentences", ",", "x_flair", ",", "img_obj", ",", "sentence_lens", ",", "mask", ",", "relation", "=", "None", ",", "mode", "=", "\"train\"", ")", ":", "# !!! word_seq  char_seq", "\n", "        ", "if", "relation", "is", "None", ":", "\n", "            ", "return", "self", ".", "_add_embeddings_internal", "(", "x_flair", ",", "img_obj", ",", "relation", ")", "\n", "\n", "", "self", ".", "embeddings", ".", "embed", "(", "x_flair", ")", "\n", "attention_probs", "=", "self", ".", "_add_embeddings_internal", "(", "x_flair", ",", "img_obj", ",", "relation", ")", "\n", "\n", "lengths", "=", "[", "len", "(", "sentence", ".", "tokens", ")", "for", "sentence", "in", "x_flair", "]", "\n", "longest_token_sequence_in_batch", ":", "int", "=", "max", "(", "lengths", ")", "\n", "\n", "pre_allocated_zero_tensor", "=", "torch", ".", "zeros", "(", "\n", "self", ".", "input_embeddding_size", "*", "longest_token_sequence_in_batch", ",", "\n", "dtype", "=", "torch", ".", "float", ",", "\n", ")", "\n", "\n", "all_embs", "=", "list", "(", ")", "\n", "for", "sentence", "in", "x_flair", ":", "\n", "            ", "all_embs", "+=", "[", "\n", "emb", "for", "token", "in", "sentence", "for", "emb", "in", "token", ".", "get_each_embedding", "(", ")", "\n", "]", "\n", "nb_padding_tokens", "=", "longest_token_sequence_in_batch", "-", "len", "(", "sentence", ")", "\n", "\n", "if", "nb_padding_tokens", ">", "0", ":", "\n", "                ", "t", "=", "pre_allocated_zero_tensor", "[", "\n", ":", "self", ".", "input_embeddding_size", "*", "nb_padding_tokens", "\n", "]", ".", "cuda", "(", ")", "\n", "all_embs", ".", "append", "(", "t", ")", "\n", "\n", "", "", "embed_flair", "=", "torch", ".", "cat", "(", "all_embs", ")", ".", "view", "(", "\n", "[", "\n", "len", "(", "x_flair", ")", ",", "\n", "longest_token_sequence_in_batch", ",", "\n", "self", ".", "input_embeddding_size", ",", "\n", "]", "\n", ")", "\n", "\n", "embeds", "=", "self", ".", "dropout", "(", "embed_flair", ")", "\n", "embeds", "=", "embeds", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# se bs hi+embedding_h+c", "\n", "packed_input", "=", "pack_padded_sequence", "(", "embeds", ",", "sentence_lens", ".", "numpy", "(", ")", ")", "\n", "packed_outputs", ",", "_", "=", "self", ".", "lstm", "(", "packed_input", ")", "\n", "outputs", ",", "_", "=", "pad_packed_sequence", "(", "packed_outputs", ")", "\n", "\n", "outputs", "=", "outputs", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# batch_size * seq_len * hidden_dimension*2", "\n", "outputs", "=", "self", ".", "dropout", "(", "outputs", ")", "\n", "out", "=", "self", ".", "projection", "(", "outputs", ")", "\n", "\n", "return", "out", ".", "permute", "(", "1", ",", "0", ",", "2", ")", ",", "attention_probs", "# seq_len * bs * tags", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.mner.gelu_new": [[12, 17], ["torch.tanh", "torch.tanh", "torch.tanh", "math.sqrt", "torch.pow", "torch.pow", "torch.pow"], "function", ["None"], ["def", "gelu_new", "(", "x", ")", ":", "\n", "    ", "\"\"\" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"", "\n", "return", "0.5", "*", "x", "*", "(", "1.0", "+", "torch", ".", "tanh", "(", "math", ".", "sqrt", "(", "2.0", "/", "math", ".", "pi", ")", "*", "(", "x", "+", "0.044715", "*", "torch", ".", "pow", "(", "x", ",", "3.0", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.main.parse_arguments": [[21, 72], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "cfgs.config.update_config"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.cfgs.config.update_config"], ["def", "parse_arguments", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Argument Parser for MNER'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--pre_image_obj_features_dir\"", ",", "dest", "=", "\"pre_image_obj_features_dir\"", ",", "type", "=", "str", ",", "\n", "default", "=", "'datasets/smap/rel_img/'", ")", "\n", "parser", ".", "add_argument", "(", "\"--pre_split_file\"", ",", "dest", "=", "\"pre_split_file\"", ",", "type", "=", "str", ",", "default", "=", "'datasets/smap/'", ")", "\n", "\n", "# parser.add_argument(\"--split_file\", dest=\"split_file\", type=str,", "\n", "#                         default='datasets/fudan/')", "\n", "# parser.add_argument(\"--image_obj_features_dir\", dest=\"image_obj_features_dir\", type=str,", "\n", "#                         default='datasets/fudan/ner_img/')", "\n", "\n", "parser", ".", "add_argument", "(", "\"--split_file\"", ",", "dest", "=", "\"split_file\"", ",", "type", "=", "str", ",", "\n", "default", "=", "'datasets/snap/'", ")", "\n", "parser", ".", "add_argument", "(", "\"--image_obj_features_dir\"", ",", "dest", "=", "\"image_obj_features_dir\"", ",", "type", "=", "str", ",", "\n", "default", "=", "'datasets/snap/ner_img/'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--pretrain_load\"", ",", "dest", "=", "\"pretrain_load\"", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "\"--pre_hidden_dimension\"", ",", "dest", "=", "\"pre_hidden_dimension\"", ",", "type", "=", "int", ",", "default", "=", "256", ")", "\n", "parser", ".", "add_argument", "(", "\"--cat_h_e\"", ",", "dest", "=", "\"cat_h_e\"", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "\n", "MODEL_DIR", "=", "'/home/data/wjq_new/ner/models_addpre/'", "\n", "parser", ".", "add_argument", "(", "\"--hidden_dimension\"", ",", "dest", "=", "\"hidden_dimension\"", ",", "type", "=", "int", ",", "default", "=", "512", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "dest", "=", "\"batch_size\"", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "dest", "=", "\"lr\"", ",", "type", "=", "float", ",", "default", "=", "5e-5", ")", "\n", "parser", ".", "add_argument", "(", "\"--dropout\"", ",", "dest", "=", "\"dropout\"", ",", "type", "=", "float", ",", "default", "=", "0.5", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_epochs\"", ",", "dest", "=", "\"num_epochs\"", ",", "type", "=", "int", ",", "default", "=", "40", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--n_layers\"", ",", "dest", "=", "\"n_layers\"", ",", "type", "=", "int", ",", "default", "=", "3", ")", "\n", "parser", ".", "add_argument", "(", "\"--clip_value\"", ",", "dest", "=", "\"clip_value\"", ",", "type", "=", "float", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "\"--wdecay\"", ",", "dest", "=", "\"wdecay\"", ",", "type", "=", "float", ",", "default", "=", "0.0000001", ")", "\n", "parser", ".", "add_argument", "(", "\"--step_size\"", ",", "dest", "=", "\"step_size\"", ",", "type", "=", "int", ",", "default", "=", "15", ")", "\n", "parser", ".", "add_argument", "(", "\"--gamma\"", ",", "dest", "=", "\"gamma\"", ",", "type", "=", "float", ",", "default", "=", "0.01", ")", "\n", "parser", ".", "add_argument", "(", "\"--validate_every\"", ",", "dest", "=", "\"validate_every\"", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "\"--mode\"", ",", "dest", "=", "\"mode\"", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_dir\"", ",", "dest", "=", "\"model_dir\"", ",", "type", "=", "str", ",", "default", "=", "MODEL_DIR", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_file_name\"", ",", "dest", "=", "\"model_file_name\"", ",", "type", "=", "str", ",", "default", "=", "\"model_weights.t7\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sent_maxlen\"", ",", "dest", "=", "\"sent_maxlen\"", ",", "type", "=", "int", ",", "default", "=", "35", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_maxlen\"", ",", "dest", "=", "\"word_maxlen\"", ",", "type", "=", "int", ",", "default", "=", "41", ")", "\n", "parser", ".", "add_argument", "(", "\"--regions_in_image\"", ",", "dest", "=", "\"regions_in_image\"", ",", "type", "=", "int", ",", "default", "=", "49", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--cfg'", ",", "type", "=", "str", ",", "help", "=", "'path to config file'", ",", "\n", "default", "=", "'cfgs/base_gt_boxes_4x16G.yaml'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "cfg", "is", "not", "None", ":", "\n", "        ", "update_config", "(", "args", ".", "cfg", ")", "\n", "\n", "", "return", "args", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.main.main": [[74, 129], ["main.parse_arguments", "print", "print", "print", "data_loader.DataLoader", "data_loader_bb.DataLoader", "evaluator.Evaluator", "print", "rpbert.resnet_vlbert.ResNetVLBERT", "torch.load", "rpbert.resnet_vlbert.ResNetVLBERT.state_dict", "myvlbert.state_dict.keys", "rpbert.resnet_vlbert.ResNetVLBERT.load_state_dict", "rpbert.bert_rel.BertRel", "print", "print", "trainer.Trainer", "trainer.Trainer.train", "print", "print", "k.replace().replace().replace", "len", "print", "print", "mner.MNER", "os.path.join", "model.cuda.load_state_dict", "torch.cuda.is_available", "print", "print", "evaluator.Evaluator.get_accuracy", "print", "print", "print", "print", "print", "torch.load.keys", "miss_keys.append", "torch.load", "model.cuda.cuda", "k.replace().replace", "k.replace"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.test.parse_arguments", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.evaluator.Evaluator.get_accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["", "def", "main", "(", ")", ":", "\n", "    ", "params", ",", "config", "=", "parse_arguments", "(", ")", "\n", "print", "(", "config", ")", "\n", "print", "(", "params", ")", "\n", "print", "(", "\"Constructing data loaders...\"", ")", "\n", "\n", "pre_model", "=", "None", "\n", "if", "params", ".", "pretrain_load", "==", "1", ":", "\n", "\n", "        ", "myvlbert", "=", "ResNetVLBERT", "(", "config", ")", "\n", "pretrained_bert_model", "=", "torch", ".", "load", "(", "'pretrained/bert-base-uncased/pytorch_model.bin'", ")", "\n", "new_state_dict", "=", "myvlbert", ".", "state_dict", "(", ")", "\n", "miss_keys", "=", "[", "]", "\n", "for", "k", "in", "new_state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "print", "(", "k", ")", "\n", "key", "=", "k", ".", "replace", "(", "'vlbert'", ",", "'bert'", ")", ".", "replace", "(", "'LayerNorm.weight'", ",", "'LayerNorm.gamma'", ")", ".", "replace", "(", "'LayerNorm.bias'", ",", "'LayerNorm.beta'", ")", "\n", "if", "key", "in", "pretrained_bert_model", ".", "keys", "(", ")", ":", "\n", "                ", "new_state_dict", "[", "k", "]", "=", "pretrained_bert_model", "[", "key", "]", "\n", "", "else", ":", "\n", "                ", "miss_keys", ".", "append", "(", "k", ")", "\n", "", "", "if", "len", "(", "miss_keys", ")", ">", "0", ":", "\n", "            ", "print", "(", "'miss keys: {}'", ".", "format", "(", "miss_keys", ")", ")", "\n", "", "myvlbert", ".", "load_state_dict", "(", "new_state_dict", ")", "\n", "\n", "pre_model", "=", "BertRel", "(", "params", ",", "myvlbert", ")", "\n", "print", "(", "'Load pretrain rpbert...[OK]'", ")", "\n", "\n", "", "dl", "=", "DataLoader", "(", "params", ")", "\n", "dlbb", "=", "DLbb", "(", "params", ")", "\n", "evaluator", "=", "Evaluator", "(", "params", ",", "dl", ")", "\n", "print", "(", "\"Constructing data loaders...[OK]\"", ")", "\n", "\n", "if", "params", ".", "mode", "==", "0", ":", "\n", "        ", "print", "(", "\"Training...\"", ")", "\n", "t", "=", "Trainer", "(", "params", ",", "config", ",", "dl", ",", "dlbb", ",", "evaluator", ",", "pre_model", ")", "\n", "t", ".", "train", "(", ")", "\n", "print", "(", "\"Training...[OK]\"", ")", "\n", "", "elif", "params", ".", "mode", "==", "1", ":", "\n", "        ", "print", "(", "\"Loading rpbert...\"", ")", "\n", "model", "=", "MNER", "(", "params", ")", "\n", "model_file_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "model_dir", ",", "params", ".", "model_file_name", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_file_path", ")", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "model", "=", "model", ".", "cuda", "(", ")", "\n", "", "print", "(", "\"Loading rpbert...[OK]\"", ")", "\n", "\n", "print", "(", "\"Evaluating rpbert on test set...\"", ")", "\n", "acc", ",", "f1", ",", "prec", ",", "rec", "=", "evaluator", ".", "get_accuracy", "(", "model", ",", "'test'", ")", "\n", "print", "(", "\"Accuracy : {}\"", ".", "format", "(", "acc", ")", ")", "\n", "print", "(", "\"F1 : {}\"", ".", "format", "(", "f1", ")", ")", "\n", "print", "(", "\"Precision : {}\"", ".", "format", "(", "prec", ")", ")", "\n", "print", "(", "\"Recall : {}\"", ".", "format", "(", "rec", ")", ")", "\n", "print", "(", "\"Evaluating rpbert on test set...[OK]\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader.CustomDataSet.__init__": [[29, 38], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "x", ",", "x_flair", ",", "y", ",", "img_id", ",", "s_idx", ",", "e_idx", ")", ":", "\n", "        ", "self", ".", "params", "=", "params", "\n", "self", ".", "x", "=", "x", "\n", "self", ".", "x_flair", "=", "x_flair", "\n", "self", ".", "y", "=", "y", "\n", "self", ".", "img_id", "=", "img_id", "\n", "self", ".", "s_idx", "=", "s_idx", "\n", "self", ".", "e_idx", "=", "e_idx", "\n", "self", ".", "num_of_samples", "=", "e_idx", "-", "s_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader.CustomDataSet.__len__": [[39, 41], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_of_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader.CustomDataSet.__getitem__": [[42, 57], ["os.path.join", "PIL.Image.open", "transform", "numpy.array", "image.convert.convert.convert"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "x", "=", "self", ".", "x", "[", "self", ".", "s_idx", "+", "idx", "]", "\n", "x_flair", "=", "self", ".", "x_flair", "[", "self", ".", "s_idx", "+", "idx", "]", "\n", "y", "=", "self", ".", "y", "[", "self", ".", "s_idx", "+", "idx", "]", "\n", "img_id", "=", "self", ".", "img_id", "[", "self", ".", "s_idx", "+", "idx", "]", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "image_obj_features_dir", ",", "img_id", "+", "'.jpg'", ")", "\n", "image", "=", "Image", ".", "open", "(", "path", ")", "\n", "if", "image", ".", "mode", "!=", "'RGB'", ":", "\n", "            ", "image", "=", "image", ".", "convert", "(", "'RGB'", ")", "\n", "\n", "", "image", "=", "transform", "(", "image", ")", "\n", "\n", "obj_x", "=", "np", ".", "array", "(", "image", ")", "\n", "\n", "return", "x", ",", "x_flair", ",", "y", ",", "obj_x", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader.CustomDataSet.collate": [[58, 104], ["numpy.array", "numpy.array", "numpy.array", "numpy.where().astype", "numpy.zeros", "range", "numpy.argsort", "sorted", "int", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "bool_mask.astype", "len", "len", "len", "trunc_x_flair.append", "util.to_tensor().long", "util.to_tensor", "util.to_tensor().long", "util.to_tensor().long", "util.to_tensor().int", "numpy.where", "len", "len", "len", "len", "bool_mask.any", "bool_mask.argmax", "len", "util.to_tensor", "util.to_tensor", "util.to_tensor", "util.to_tensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_tensor", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.argmax", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_tensor", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_tensor", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_tensor", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_tensor"], ["", "def", "collate", "(", "self", ",", "batch", ")", ":", "\n", "        ", "x", "=", "np", ".", "array", "(", "[", "x", "[", "0", "]", "for", "x", "in", "batch", "]", ")", "\n", "x_flair", "=", "[", "x", "[", "1", "]", "for", "x", "in", "batch", "]", "\n", "y", "=", "np", ".", "array", "(", "[", "x", "[", "2", "]", "for", "x", "in", "batch", "]", ")", "\n", "obj_x", "=", "np", ".", "array", "(", "[", "x", "[", "3", "]", "for", "x", "in", "batch", "]", ")", "\n", "\n", "bool_mask", "=", "y", "==", "0", "\n", "mask", "=", "1", "-", "bool_mask", ".", "astype", "(", "np", ".", "int", ")", "\n", "\n", "# index of first 0 in each row, if no zero then idx = -1", "\n", "zero_indices", "=", "np", ".", "where", "(", "bool_mask", ".", "any", "(", "1", ")", ",", "bool_mask", ".", "argmax", "(", "1", ")", ",", "-", "1", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "\n", "input_len", "=", "np", ".", "zeros", "(", "len", "(", "batch", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "batch", ")", ")", ":", "\n", "            ", "if", "zero_indices", "[", "i", "]", "==", "-", "1", ":", "\n", "                ", "input_len", "[", "i", "]", "=", "len", "(", "x", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                ", "input_len", "[", "i", "]", "=", "zero_indices", "[", "i", "]", "\n", "", "", "sorted_input_arg", "=", "np", ".", "argsort", "(", "-", "input_len", ")", "\n", "\n", "x", "=", "x", "[", "sorted_input_arg", "]", "\n", "x_flair", "=", "sorted", "(", "x_flair", ",", "key", "=", "lambda", "i", ":", "len", "(", "i", ")", ",", "reverse", "=", "True", ")", "\n", "\n", "y", "=", "y", "[", "sorted_input_arg", "]", "\n", "\n", "obj_x", "=", "obj_x", "[", "sorted_input_arg", "]", "\n", "mask", "=", "mask", "[", "sorted_input_arg", "]", "\n", "\n", "input_len", "=", "input_len", "[", "sorted_input_arg", "]", "\n", "\n", "max_seq_len", "=", "int", "(", "input_len", "[", "0", "]", ")", "\n", "\n", "trunc_x", "=", "np", ".", "zeros", "(", "(", "len", "(", "batch", ")", ",", "max_seq_len", ")", ")", "\n", "trunc_x_flair", "=", "[", "]", "\n", "\n", "trunc_y", "=", "np", ".", "zeros", "(", "(", "len", "(", "batch", ")", ",", "max_seq_len", ")", ")", "\n", "trunc_mask", "=", "np", ".", "zeros", "(", "(", "len", "(", "batch", ")", ",", "max_seq_len", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "batch", ")", ")", ":", "\n", "\n", "            ", "trunc_x_flair", ".", "append", "(", "x_flair", "[", "i", "]", ")", "\n", "trunc_x", "[", "i", "]", "=", "x", "[", "i", ",", ":", "max_seq_len", "]", "\n", "\n", "trunc_y", "[", "i", "]", "=", "y", "[", "i", ",", ":", "max_seq_len", "]", "\n", "trunc_mask", "[", "i", "]", "=", "mask", "[", "i", ",", ":", "max_seq_len", "]", "\n", "", "return", "to_tensor", "(", "trunc_x", ")", ".", "long", "(", ")", ",", "trunc_x_flair", ",", "to_tensor", "(", "obj_x", ")", ",", "to_tensor", "(", "trunc_y", ")", ".", "long", "(", ")", ",", "to_tensor", "(", "trunc_mask", ")", ".", "long", "(", ")", ",", "to_tensor", "(", "input_len", ")", ".", "int", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader.DataLoader.__init__": [[107, 137], ["data_loader.DataLoader.load_data", "data_loader.CustomDataSet", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "data_loader.CustomDataSet", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "data_loader.CustomDataSet", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.load_data"], ["    ", "def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "        ", "'''\n        self.x : sentence encoding with padding at word level\n        self.y : label corresponding to the words in the sentences\n        :param params:\n        '''", "\n", "self", ".", "params", "=", "params", "\n", "\n", "self", ".", "sentences", ",", "self", ".", "datasplit", ",", "self", ".", "x", ",", "self", ".", "x_flair", ",", "self", ".", "y", ",", "self", ".", "num_sentence", ",", "self", ".", "labelVoc", ",", "self", ".", "img_id", "=", "self", ".", "load_data", "(", ")", "\n", "\n", "kwargs", "=", "{", "'num_workers'", ":", "6", ",", "'pin_memory'", ":", "True", "}", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "{", "}", "\n", "\n", "dataset_train", "=", "CustomDataSet", "(", "params", ",", "self", ".", "x", ",", "self", ".", "x_flair", ",", "self", ".", "y", ",", "self", ".", "img_id", ",", "self", ".", "datasplit", "[", "0", "]", ",", "self", ".", "datasplit", "[", "1", "]", ")", "\n", "self", ".", "train_data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset_train", ",", "\n", "batch_size", "=", "self", ".", "params", ".", "batch_size", ",", "\n", "collate_fn", "=", "dataset_train", ".", "collate", ",", "\n", "shuffle", "=", "True", ",", "**", "kwargs", ")", "\n", "dataset_val", "=", "CustomDataSet", "(", "params", ",", "self", ".", "x", ",", "self", ".", "x_flair", ",", "self", ".", "y", ",", "self", ".", "img_id", ",", "self", ".", "datasplit", "[", "1", "]", ",", "self", ".", "datasplit", "[", "2", "]", ")", "\n", "self", ".", "val_data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset_val", ",", "\n", "batch_size", "=", "1", ",", "\n", "collate_fn", "=", "dataset_val", ".", "collate", ",", "\n", "shuffle", "=", "False", ",", "**", "kwargs", ")", "\n", "dataset_test", "=", "CustomDataSet", "(", "params", ",", "self", ".", "x", ",", "self", ".", "x_flair", ",", "self", ".", "y", ",", "self", ".", "img_id", ",", "self", ".", "datasplit", "[", "2", "]", ",", "self", ".", "datasplit", "[", "3", "]", ")", "\n", "self", ".", "test_data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset_test", ",", "\n", "batch_size", "=", "1", ",", "\n", "collate_fn", "=", "dataset_test", ".", "collate", ",", "\n", "shuffle", "=", "False", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader.DataLoader.load_data": [[138, 151], ["print", "data_loader.DataLoader.load_sentence", "data_loader.DataLoader.vocab_bulid", "data_loader.DataLoader.pad_sequence"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.load_sentence", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader.DataLoader.vocab_bulid", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.pad_sequence"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "print", "(", "'calculating vocabulary...'", ")", "\n", "\n", "datasplit", ",", "sentences", ",", "sent_maxlen", ",", "word_maxlen", ",", "num_sentence", ",", "img_id", "=", "self", ".", "load_sentence", "(", "\n", "'IMGID'", ",", "self", ".", "params", ".", "split_file", ",", "'train'", ",", "'dev'", ",", "'test'", ")", "\n", "\n", "labelVoc", ",", "labelVoc_inv", "=", "self", ".", "vocab_bulid", "(", "sentences", ")", "\n", "\n", "x", ",", "x_flair", ",", "y", "=", "self", ".", "pad_sequence", "(", "sentences", ",", "labelVoc", ",", "\n", "word_maxlen", "=", "self", ".", "params", ".", "word_maxlen", ",", "sent_maxlen", "=", "sent_maxlen", ")", "\n", "\n", "return", "[", "sentences", ",", "datasplit", ",", "x", ",", "x_flair", ",", "y", ",", "num_sentence", ",", "\n", "labelVoc", ",", "img_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader.DataLoader.load_sentence": [[152, 194], ["datasplit.append", "len", "print", "print", "print", "print", "print", "print", "print", "datasplit.append", "len", "len", "len", "len", "open", "os.path.join", "line.rstrip.rstrip.rstrip", "max", "sentences.append", "len", "len", "img_id.append", "sentence.append", "max", "line.rstrip.rstrip.split", "len", "str", "line.rstrip.rstrip.split"], "methods", ["None"], ["", "def", "load_sentence", "(", "self", ",", "IMAGEID", ",", "tweet_data_dir", ",", "train_name", ",", "dev_name", ",", "test_name", ")", ":", "\n", "        ", "\"\"\"\n        read the word from doc, and build sentence. every line contain a word and it's tag\n        every sentence is split with a empty line. every sentence begain with an \"IMGID:num\"\n\n        \"\"\"", "\n", "# IMAGEID='IMGID'", "\n", "img_id", "=", "[", "]", "\n", "sentences", "=", "[", "]", "\n", "sentence", "=", "[", "]", "\n", "sent_maxlen", "=", "0", "\n", "word_maxlen", "=", "0", "\n", "datasplit", "=", "[", "]", "\n", "\n", "for", "fname", "in", "(", "train_name", ",", "dev_name", ",", "test_name", ")", ":", "\n", "            ", "datasplit", ".", "append", "(", "len", "(", "img_id", ")", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "tweet_data_dir", ",", "fname", ")", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file", ":", "\n", "                ", "for", "line", "in", "file", ":", "\n", "                    ", "line", "=", "line", ".", "rstrip", "(", ")", "\n", "if", "line", "==", "''", ":", "\n", "                        ", "sent_maxlen", "=", "max", "(", "sent_maxlen", ",", "len", "(", "sentence", ")", ")", "\n", "sentences", ".", "append", "(", "sentence", ")", "\n", "sentence", "=", "[", "]", "\n", "", "else", ":", "\n", "                        ", "if", "IMAGEID", "in", "line", ":", "\n", "                            ", "num", "=", "line", "[", "6", ":", "]", "\n", "img_id", ".", "append", "(", "num", ")", "\n", "", "else", ":", "\n", "                            ", "sentence", ".", "append", "(", "line", ".", "split", "(", "'\\t'", ")", ")", "\n", "word_maxlen", "=", "max", "(", "word_maxlen", ",", "len", "(", "str", "(", "line", ".", "split", "(", ")", "[", "0", "]", ")", ")", ")", "\n", "\n", "", "", "", "", "", "datasplit", ".", "append", "(", "len", "(", "img_id", ")", ")", "\n", "num_sentence", "=", "len", "(", "sentences", ")", "\n", "\n", "print", "(", "\"datasplit\"", ",", "datasplit", ")", "\n", "print", "(", "sentences", "[", "len", "(", "sentences", ")", "-", "2", "]", ")", "\n", "print", "(", "sentences", "[", "0", "]", ")", "\n", "print", "(", "'sent_maxlen'", ",", "sent_maxlen", ")", "\n", "print", "(", "'word_maxlen'", ",", "word_maxlen", ")", "\n", "print", "(", "'number sentence'", ",", "len", "(", "sentences", ")", ")", "\n", "print", "(", "'number image'", ",", "len", "(", "img_id", ")", ")", "\n", "return", "[", "datasplit", ",", "sentences", ",", "sent_maxlen", ",", "word_maxlen", ",", "num_sentence", ",", "img_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader.DataLoader.vocab_bulid": [[195, 222], ["collections.Counter", "print", "print", "data_loader.DataLoader.label_index", "print", "len", "words.append", "labels.append", "chars.append"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.label_index"], ["", "def", "vocab_bulid", "(", "self", ",", "sentences", ")", ":", "\n", "        ", "\"\"\"\n        input:\n            sentences list,\n            the element of the list is (word, label) pair.\n        output:\n            some dictionaries.\n\n        \"\"\"", "\n", "words", "=", "[", "]", "\n", "chars", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "for", "word_label", "in", "sentence", ":", "\n", "                ", "words", ".", "append", "(", "word_label", "[", "0", "]", ")", "\n", "labels", ".", "append", "(", "word_label", "[", "1", "]", ")", "\n", "for", "char", "in", "word_label", "[", "0", "]", ":", "\n", "                    ", "chars", ".", "append", "(", "char", ")", "\n", "\n", "", "", "", "labels_counts", "=", "Counter", "(", "labels", ")", "\n", "print", "(", "'labels_counts'", ",", "len", "(", "labels_counts", ")", ")", "\n", "print", "(", "labels_counts", ")", "\n", "labelVoc_inv", ",", "labelVoc", "=", "self", ".", "label_index", "(", "labels_counts", ")", "\n", "print", "(", "'labelVoc'", ",", "labelVoc", ")", "\n", "\n", "return", "[", "labelVoc", ",", "labelVoc_inv", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader.DataLoader.label_index": [[223, 244], ["len", "len", "labels_counts.items", "labels_counts.most_common", "labelVoc.has_key", "labelVoc.setdefault", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "label_index", "(", "labels_counts", ")", ":", "\n", "        ", "\"\"\"\n           the input is the output of Counter. This function defines the (label, index) pair,\n           and it cast our datasets label to the definition (label, index) pair.\n        \"\"\"", "\n", "\n", "num_labels", "=", "len", "(", "labels_counts", ")", "\n", "labelVoc_inv", "=", "[", "x", "[", "0", "]", "for", "x", "in", "labels_counts", ".", "most_common", "(", ")", "]", "\n", "\n", "labelVoc", "=", "{", "'0'", ":", "0", ",", "\n", "'B-PER'", ":", "1", ",", "'I-PER'", ":", "2", ",", "\n", "'B-LOC'", ":", "3", ",", "'I-LOC'", ":", "4", ",", "\n", "'B-ORG'", ":", "5", ",", "'I-ORG'", ":", "6", ",", "\n", "'B-OTHER'", ":", "7", ",", "'I-OTHER'", ":", "8", ",", "\n", "'O'", ":", "9", "}", "\n", "if", "len", "(", "labelVoc", ")", "<", "num_labels", ":", "\n", "            ", "for", "key", ",", "value", "in", "labels_counts", ".", "items", "(", ")", ":", "\n", "                ", "if", "not", "labelVoc", ".", "has_key", "(", "key", ")", ":", "\n", "                    ", "labelVoc", ".", "setdefault", "(", "key", ",", "len", "(", "labelVoc", ")", ")", "\n", "", "", "", "return", "labelVoc_inv", ",", "labelVoc", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader.DataLoader.pad_sequences": [[245, 252], ["numpy.zeros", "enumerate", "numpy.zeros.astype", "min", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "pad_sequences", "(", "y", ",", "sent_maxlen", ")", ":", "\n", "        ", "padded", "=", "np", ".", "zeros", "(", "(", "len", "(", "y", ")", ",", "sent_maxlen", ")", ")", "\n", "for", "i", ",", "each", "in", "enumerate", "(", "y", ")", ":", "\n", "            ", "trunc_len", "=", "min", "(", "sent_maxlen", ",", "len", "(", "each", ")", ")", "\n", "padded", "[", "i", ",", ":", "trunc_len", "]", "=", "each", "[", ":", "trunc_len", "]", "\n", "", "return", "padded", ".", "astype", "(", "np", ".", "int32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader.DataLoader.pad_sequence": [[253, 285], ["pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_loader.DataLoader.pad_sequences", "data_loader.DataLoader.pad_sequences", "numpy.asarray", "flair.data.Sentence", "enumerate", "data_loader.DataLoader.append", "x_flair.append", "numpy.asarray.append", "flair.data.Sentence.add_token", "y_id.append", "w_id.append", "w_id.append", "word_label[].lower"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.pad_sequences", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.pad_sequences", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token"], ["", "def", "pad_sequence", "(", "self", ",", "sentences", ",", "labelVoc", ",", "word_maxlen", "=", "30", ",", "\n", "sent_maxlen", "=", "35", ")", ":", "\n", "        ", "\"\"\"\n            This function is used to pad the word into the same length, the word length is set to 30.\n            Moreover, it also pad each sentence into the same length, the length is set to 35.\n\n        \"\"\"", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "x", "=", "[", "]", "\n", "x_flair", "=", "[", "]", "\n", "y", "=", "[", "]", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "w_id", "=", "[", "]", "\n", "y_id", "=", "[", "]", "\n", "st", "=", "Sentence", "(", ")", "\n", "for", "idx", ",", "word_label", "in", "enumerate", "(", "sentence", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "w_id", ".", "append", "(", "tokenizer", ".", "vocab", "[", "word_label", "[", "0", "]", ".", "lower", "(", ")", "]", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                    ", "w_id", ".", "append", "(", "tokenizer", ".", "vocab", "[", "'[MASK]'", "]", ")", "\n", "", "st", ".", "add_token", "(", "word_label", "[", "0", "]", ")", "\n", "y_id", ".", "append", "(", "labelVoc", "[", "word_label", "[", "1", "]", "]", ")", "\n", "", "x", ".", "append", "(", "w_id", ")", "\n", "x_flair", ".", "append", "(", "st", ")", "\n", "y", ".", "append", "(", "y_id", ")", "\n", "\n", "", "y", "=", "self", ".", "pad_sequences", "(", "y", ",", "sent_maxlen", ")", "\n", "x", "=", "self", ".", "pad_sequences", "(", "x", ",", "sent_maxlen", ")", "\n", "\n", "y", "=", "np", ".", "asarray", "(", "y", ")", "\n", "\n", "return", "[", "x", ",", "x_flair", ",", "y", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.trainer.Trainer.__init__": [[24, 31], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "config", ",", "data_loader", ",", "dlbb", ",", "evaluator", ",", "pre_model", "=", "None", ")", ":", "\n", "        ", "self", ".", "params", "=", "params", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "data_loader", "=", "data_loader", "\n", "self", ".", "dlbb", "=", "dlbb", "\n", "self", ".", "evaluator", "=", "evaluator", "\n", "self", ".", "pre_model", "=", "pre_model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.trainer.Trainer.train": [[32, 136], ["len", "flair.embeddings.StackedEmbeddings", "mner.MNER", "nn.CrossEntropyLoss", "torchcrf.CRF", "torch.cuda.is_available", "dict", "dict.items", "torch.optim.Adam", "torch.optim.lr_scheduler.LambdaLR", "flair.embeddings.WordEmbeddings", "flair.embeddings.CharacterEmbeddings", "ner_model.cuda.cuda.cuda", "loss_function.cuda.cuda.cuda", "ner_model.cuda.cuda.named_parameters", "range", "timeit.default_timer", "tqdm.tqdm.tqdm", "torch.cuda.empty_cache", "tqdm.tqdm.tqdm", "torch.optim.lr_scheduler.LambdaLR.step", "torch.optim.Adam.state_dict", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "print", "torch.save", "ner_model.cuda.cuda.train", "torch.optim.Adam.zero_grad", "ner_model.cuda.cuda.pretrain_model", "util.to_variable().contiguous", "nn.CrossEntropyLoss.", "nn.CrossEntropyLoss.backward", "torch.optim.Adam.step", "ner_model.cuda.cuda.train", "ner_model.cuda.cuda.", "F.softmax", "torch.optim.Adam.zero_grad", "ner_model.cuda.cuda.", "util.to_variable().transpose().contiguous", "util.to_variable().byte().transpose", "loss.backward", "losses.append", "torch.optim.Adam.step", "print", "print", "print", "model.state_dict", "util.to_variable", "util.to_variable", "util.to_variable", "ner_model.cuda.squeeze", "util.to_variable", "util.to_variable", "util.to_variable", "util.to_variable", "util.to_variable", "util.to_variable", "F.softmax.detach", "loss_function.cuda.cuda.", "loss.data.cpu().numpy", "torch.no_grad", "trainer.Trainer.evaluator.get_accuracy", "print", "util.to_variable", "util.to_variable().transpose", "util.to_variable().byte", "print", "torch.save", "print", "numpy.asscalar", "loss.data.cpu", "numpy.asscalar", "ner_model.cuda.cuda.state_dict", "numpy.mean", "util.to_variable", "util.to_variable", "numpy.mean", "timeit.default_timer"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.evaluator.Evaluator.get_accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "num_of_tags", "=", "len", "(", "self", ".", "data_loader", ".", "labelVoc", ")", "\n", "embedding_types", "=", "[", "\n", "WordEmbeddings", "(", "\n", "'pretrained/embedding/en-fasttext-crawl-300d-1M'", ")", ",", "\n", "CharacterEmbeddings", "(", "'pretrained/embedding/datasets/common_characters_large'", ")", ",", "\n", "]", "\n", "\n", "embeddings", ":", "StackedEmbeddings", "=", "StackedEmbeddings", "(", "embeddings", "=", "embedding_types", ")", "\n", "\n", "ner_model", "=", "MNER", "(", "self", ".", "params", ",", "embeddings", ",", "self", ".", "pre_model", ",", "num_of_tags", "=", "10", ")", "\n", "loss_function_relation", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss_function", "=", "CRF", "(", "num_of_tags", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "ner_model", "=", "ner_model", ".", "cuda", "(", ")", "\n", "loss_function", "=", "loss_function", ".", "cuda", "(", ")", "\n", "\n", "", "paras", "=", "dict", "(", "ner_model", ".", "named_parameters", "(", ")", ")", "\n", "paras_new", "=", "[", "]", "\n", "for", "k", ",", "v", "in", "paras", ".", "items", "(", ")", ":", "\n", "            ", "if", "'pre_resnet'", "in", "k", "or", "'vlbert'", "in", "k", ":", "\n", "                ", "paras_new", "+=", "[", "{", "'params'", ":", "[", "v", "]", ",", "'lr'", ":", "1e-6", "}", "]", "\n", "", "else", ":", "\n", "                ", "paras_new", "+=", "[", "{", "'params'", ":", "[", "v", "]", ",", "'lr'", ":", "1e-4", "}", "]", "\n", "", "", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "paras_new", ",", "weight_decay", "=", "self", ".", "params", ".", "wdecay", ")", "\n", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "burnin_schedule", ")", "\n", "\n", "try", ":", "\n", "            ", "prev_best", "=", "0", "\n", "best_epoch", "=", "0", "\n", "for", "epoch", "in", "range", "(", "self", ".", "params", ".", "num_epochs", ")", ":", "\n", "                ", "losses", "=", "[", "]", "\n", "start_time", "=", "timer", "(", ")", "\n", "\n", "# relation\u4efb\u52a1", "\n", "for", "(", "x", ",", "x_obj", ",", "y", ",", "mask", ",", "lens", ",", "ifpairs", ")", "in", "tqdm", "(", "self", ".", "dlbb", ".", "train_phase1_loader", ")", ":", "\n", "                    ", "ner_model", ".", "train", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "pair_out", "=", "ner_model", ".", "pretrain_model", "(", "to_variable", "(", "x", ")", ",", "to_variable", "(", "x_obj", ")", ",", "lens", ",", "\n", "to_variable", "(", "mask", ")", ")", "# seq_len * bs * labels", "\n", "ifpairs", "=", "to_variable", "(", "ifpairs", ")", ".", "contiguous", "(", ")", "\n", "loss1", "=", "loss_function_relation", "(", "pair_out", ".", "squeeze", "(", "1", ")", ",", "ifpairs", ")", "\n", "loss1", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "# ner\u4efb\u52a1", "\n", "for", "(", "x", ",", "x_flair", ",", "x_obj", ",", "y", ",", "mask", ",", "lens", ")", "in", "tqdm", "(", "\n", "self", ".", "data_loader", ".", "train_data_loader", ")", ":", "\n", "                    ", "ner_model", ".", "train", "(", ")", "\n", "pair_out", "=", "ner_model", "(", "to_variable", "(", "x", ")", ",", "x_flair", ",", "to_variable", "(", "x_obj", ")", ",", "\n", "lens", ",", "to_variable", "(", "mask", ")", ")", "\n", "\n", "relation", "=", "F", ".", "softmax", "(", "pair_out", ",", "dim", "=", "-", "1", ")", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# ner\u8bad\u7ec3", "\n", "emissions", ",", "attention_probs", "=", "ner_model", "(", "to_variable", "(", "x", ")", ",", "x_flair", ",", "to_variable", "(", "x_obj", ")", ",", "\n", "lens", ",", "to_variable", "(", "mask", ")", ",", "relation", ".", "detach", "(", ")", ")", "# seq_len * bs * labels", "\n", "tags", "=", "to_variable", "(", "y", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "# seq_len * bs", "\n", "mask", "=", "to_variable", "(", "mask", ")", ".", "byte", "(", ")", ".", "transpose", "(", "0", ",", "1", ")", "# seq_len * bs", "\n", "\n", "# computing crf loss", "\n", "loss", "=", "-", "loss_function", "(", "emissions", ",", "tags", ",", "mask", "=", "mask", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "losses", ".", "append", "(", "loss", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "scheduler", ".", "step", "(", ")", "\n", "optim_state", "=", "optimizer", ".", "state_dict", "(", ")", "\n", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "# Calculate accuracy and save best rpbert", "\n", "if", "(", "epoch", "+", "1", ")", "%", "self", ".", "params", ".", "validate_every", "==", "0", ":", "\n", "\n", "                    ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                        ", "acc_dev", ",", "f1_dev", ",", "p_dev", ",", "r_dev", "=", "self", ".", "evaluator", ".", "get_accuracy", "(", "ner_model", ",", "'test'", ",", "loss_function", ")", "\n", "\n", "print", "(", "\n", "\"Epoch {} : Training Loss: {:.5f}, Acc: {:.5f}, F1: {:.5f}, Prec: {:.5f}, Rec: {:.5f}, LR: {:.5f}\"", "\n", "\"Time elapsed {:.2f} mins\"", "\n", ".", "format", "(", "epoch", "+", "1", ",", "np", ".", "asscalar", "(", "np", ".", "mean", "(", "losses", ")", ")", ",", "acc_dev", ",", "f1_dev", ",", "p_dev", ",", "r_dev", ",", "\n", "optim_state", "[", "'param_groups'", "]", "[", "0", "]", "[", "'lr'", "]", ",", "\n", "(", "timer", "(", ")", "-", "start_time", ")", "/", "60", ")", ")", "\n", "if", "f1_dev", ">", "prev_best", "and", "f1_dev", ">", "0.7", ":", "\n", "                            ", "print", "(", "\"f1-score increased....saving weights !!\"", ")", "\n", "best_epoch", "=", "epoch", "+", "1", "\n", "prev_best", "=", "f1_dev", "\n", "model_path", "=", "self", ".", "params", ".", "model_dir", "+", "\"/epoch{}_f1_{:.5f}.pth\"", ".", "format", "(", "epoch", "+", "1", ",", "f1_dev", ")", "\n", "torch", ".", "save", "(", "ner_model", ".", "state_dict", "(", ")", ",", "model_path", ")", "\n", "print", "(", "\"rpbert save in \"", "+", "model_path", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "print", "(", "\"Epoch {} : Training Loss: {:.5f}\"", ".", "format", "(", "epoch", "+", "1", ",", "np", ".", "asscalar", "(", "np", ".", "mean", "(", "losses", ")", ")", ")", ")", "\n", "", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "if", "epoch", "+", "1", "==", "self", ".", "params", ".", "num_epochs", ":", "\n", "                    ", "best_model_path", "=", "self", ".", "params", ".", "model_dir", "+", "\"/epoch{}_f1_{:.5f}.pth\"", ".", "format", "(", "best_epoch", ",", "prev_best", ")", "\n", "print", "(", "\"{} epoch get the best f1 {:.5f}\"", ".", "format", "(", "best_epoch", ",", "prev_best", ")", ")", "\n", "print", "(", "\"the rpbert is save in \"", "+", "model_path", ")", "\n", "", "", "", "except", "KeyboardInterrupt", ":", "\n", "            ", "print", "(", "\"Interrupted.. saving rpbert !!!\"", ")", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "self", ".", "params", ".", "model_dir", "+", "'/model_weights_interrupt.t7'", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.trainer.burnin_schedule": [[13, 21], ["None"], "function", ["None"], ["def", "burnin_schedule", "(", "i", ")", ":", "\n", "    ", "if", "i", "<", "10", ":", "\n", "        ", "factor", "=", "1", "\n", "", "elif", "i", "<", "20", ":", "\n", "        ", "factor", "=", "0.1", "\n", "", "else", ":", "\n", "        ", "factor", "=", "0.01", "\n", "", "return", "factor", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.CustomDataSet.__init__": [[22, 33], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "x", ",", "img_id", ",", "y", ",", "ifparis", ",", "s_idx", ",", "e_idx", ")", ":", "\n", "        ", "self", ".", "params", "=", "params", "\n", "self", ".", "x", "=", "x", "\n", "# self.img_x = img_x", "\n", "self", ".", "img_id", "=", "img_id", "\n", "self", ".", "y", "=", "y", "\n", "# self.mask_object = mask_object", "\n", "self", ".", "s_idx", "=", "s_idx", "\n", "self", ".", "e_idx", "=", "e_idx", "\n", "self", ".", "num_of_samples", "=", "e_idx", "-", "s_idx", "\n", "self", ".", "ifpairs", "=", "ifparis", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.CustomDataSet.__len__": [[34, 36], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_of_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.CustomDataSet.__getitem__": [[37, 52], ["os.path.join", "PIL.Image.open", "transform", "numpy.array", "image.convert.convert.convert"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "x", "=", "self", ".", "x", "[", "self", ".", "s_idx", "+", "idx", "]", "\n", "y", "=", "self", ".", "y", "[", "self", ".", "s_idx", "+", "idx", "]", "\n", "\n", "img_id", "=", "self", ".", "img_id", "[", "self", ".", "s_idx", "+", "idx", "]", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "pre_image_obj_features_dir", ",", "img_id", "+", "'.jpg'", ")", "\n", "image", "=", "Image", ".", "open", "(", "path", ")", "\n", "if", "image", ".", "mode", "!=", "'RGB'", ":", "\n", "            ", "image", "=", "image", ".", "convert", "(", "'RGB'", ")", "\n", "\n", "", "image", "=", "transform", "(", "image", ")", "\n", "obj_x", "=", "np", ".", "array", "(", "image", ")", "\n", "\n", "ifpairs", "=", "self", ".", "ifpairs", "[", "self", ".", "s_idx", "+", "idx", "]", "\n", "return", "x", ",", "y", ",", "obj_x", ",", "ifpairs", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.CustomDataSet.collate": [[53, 95], ["numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.where().astype", "numpy.zeros", "range", "numpy.argsort", "int", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "bool_mask.astype", "len", "len", "len", "util.to_tensor().long", "util.to_tensor", "util.to_tensor().long", "util.to_tensor().long", "util.to_tensor().int", "util.to_tensor().long", "numpy.where", "len", "len", "len", "len", "bool_mask.any", "bool_mask.argmax", "util.to_tensor", "util.to_tensor", "util.to_tensor", "util.to_tensor", "util.to_tensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_tensor", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.argmax", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_tensor", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_tensor", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_tensor", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_tensor", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_tensor"], ["", "def", "collate", "(", "self", ",", "batch", ")", ":", "\n", "        ", "x", "=", "np", ".", "array", "(", "[", "x", "[", "0", "]", "for", "x", "in", "batch", "]", ")", "\n", "y", "=", "np", ".", "array", "(", "[", "x", "[", "1", "]", "for", "x", "in", "batch", "]", ")", "\n", "\n", "obj_x", "=", "np", ".", "array", "(", "[", "x", "[", "2", "]", "for", "x", "in", "batch", "]", ")", "\n", "\n", "ifpairs", "=", "np", ".", "array", "(", "[", "x", "[", "3", "]", "for", "x", "in", "batch", "]", ")", "\n", "\n", "bool_mask", "=", "y", "==", "0", "\n", "mask", "=", "1", "-", "bool_mask", ".", "astype", "(", "np", ".", "int", ")", "\n", "\n", "# index of first 0 in each row, if no zero then idx = -1", "\n", "zero_indices", "=", "np", ".", "where", "(", "bool_mask", ".", "any", "(", "1", ")", ",", "bool_mask", ".", "argmax", "(", "1", ")", ",", "-", "1", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "input_len", "=", "np", ".", "zeros", "(", "len", "(", "batch", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "batch", ")", ")", ":", "\n", "            ", "if", "zero_indices", "[", "i", "]", "==", "-", "1", ":", "\n", "                ", "input_len", "[", "i", "]", "=", "len", "(", "y", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                ", "input_len", "[", "i", "]", "=", "zero_indices", "[", "i", "]", "\n", "", "", "sorted_input_arg", "=", "np", ".", "argsort", "(", "-", "input_len", ")", "\n", "\n", "# Sort everything according to the sequence length", "\n", "x", "=", "x", "[", "sorted_input_arg", "]", "\n", "y", "=", "y", "[", "sorted_input_arg", "]", "\n", "obj_x", "=", "obj_x", "[", "sorted_input_arg", "]", "\n", "\n", "mask", "=", "mask", "[", "sorted_input_arg", "]", "\n", "input_len", "=", "input_len", "[", "sorted_input_arg", "]", "\n", "ifpairs", "=", "ifpairs", "[", "sorted_input_arg", "]", "\n", "\n", "max_seq_len", "=", "int", "(", "input_len", "[", "0", "]", ")", "\n", "\n", "trunc_x", "=", "np", ".", "zeros", "(", "(", "len", "(", "batch", ")", ",", "max_seq_len", ")", ")", "\n", "trunc_y", "=", "np", ".", "zeros", "(", "(", "len", "(", "batch", ")", ",", "max_seq_len", ")", ")", "\n", "trunc_mask", "=", "np", ".", "zeros", "(", "(", "len", "(", "batch", ")", ",", "max_seq_len", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "batch", ")", ")", ":", "\n", "            ", "trunc_x", "[", "i", "]", "=", "x", "[", "i", ",", ":", "max_seq_len", "]", "\n", "trunc_y", "[", "i", "]", "=", "y", "[", "i", ",", ":", "max_seq_len", "]", "\n", "trunc_mask", "[", "i", "]", "=", "mask", "[", "i", ",", ":", "max_seq_len", "]", "\n", "\n", "", "return", "to_tensor", "(", "trunc_x", ")", ".", "long", "(", ")", ",", "to_tensor", "(", "obj_x", ")", ",", "to_tensor", "(", "trunc_y", ")", ".", "long", "(", ")", ",", "to_tensor", "(", "trunc_mask", ")", ".", "long", "(", ")", ",", "to_tensor", "(", "input_len", ")", ".", "int", "(", ")", ",", "to_tensor", "(", "ifpairs", ")", ".", "long", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.__init__": [[98, 117], ["data_loader_bb.DataLoader.load_data", "data_loader_bb.CustomDataSet", "torch.utils.data.DataLoader", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.load_data"], ["    ", "def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "        ", "'''\n        self.x : sentence encoding with padding at word level\n        self.y : label corresponding to the words in the sentences\n        :param params:\n        '''", "\n", "self", ".", "params", "=", "params", "\n", "\n", "self", ".", "sentences", ",", "self", ".", "datasplit", ",", "self", ".", "x", ",", "self", ".", "img_id", ",", "self", ".", "y", ",", "self", ".", "num_sentence", ",", "self", ".", "ifpairs", "=", "self", ".", "load_data", "(", ")", "\n", "kwargs", "=", "{", "'num_workers'", ":", "4", ",", "'pin_memory'", ":", "True", "}", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "{", "}", "\n", "##if for pair_out training, change to self.datasplit[0], self.datasplit[2]", "\n", "dataset_train_phase1", "=", "CustomDataSet", "(", "params", ",", "self", ".", "x", ",", "self", ".", "img_id", ",", "self", ".", "y", ",", "self", ".", "ifpairs", ",", "self", ".", "datasplit", "[", "0", "]", ",", "self", ".", "datasplit", "[", "1", "]", ")", "\n", "self", ".", "train_phase1_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset_train_phase1", ",", "\n", "batch_size", "=", "self", ".", "params", ".", "batch_size", ",", "\n", "collate_fn", "=", "dataset_train_phase1", ".", "collate", ",", "\n", "shuffle", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.load_data": [[118, 126], ["print", "data_loader_bb.DataLoader.load_sentence", "data_loader_bb.DataLoader.pad_sequence"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.load_sentence", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.pad_sequence"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "print", "(", "'calculating vocab  ulary...'", ")", "\n", "datasplit", ",", "sentences", ",", "img_id", ",", "sent_maxlen", ",", "word_maxlen", ",", "num_sentence", ",", "ifpairs", "=", "self", ".", "load_sentence", "(", "\n", "'IMGID'", ",", "self", ".", "params", ".", "pre_split_file", ",", "'textimage-data-image'", ")", "\n", "x", ",", "img_id", ",", "y", "=", "self", ".", "pad_sequence", "(", "sentences", ",", "img_id", ",", "\n", "word_maxlen", "=", "word_maxlen", ",", "sent_maxlen", "=", "sent_maxlen", ")", "\n", "return", "[", "sentences", ",", "datasplit", ",", "x", ",", "img_id", ",", "y", ",", "num_sentence", ",", "\n", "ifpairs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.load_sentence": [[127, 175], ["datasplit.append", "len", "print", "print", "print", "print", "print", "print", "print", "datasplit.append", "print", "len", "len", "len", "len", "open", "os.path.join", "line.split.split.rstrip", "max", "sentences.append", "len", "len", "line.split.split.split", "img_id.append", "ifpairs.append", "sentence.append", "max", "int", "line.split.split.split", "len", "str", "line.split.split.split"], "methods", ["None"], ["", "def", "load_sentence", "(", "self", ",", "IMAGEID", ",", "tweet_data_dir", ",", "file_name", ")", ":", "\n", "        ", "\"\"\"\n        read the word from doc, and build sentence. every line contain a word and it's tag\n        every sentence is split with a empty line. every sentence begain with an \"IMGID:num\"\n\n        \"\"\"", "\n", "# IMAGEID='IMGID'", "\n", "img_id", "=", "[", "]", "\n", "sentences", "=", "[", "]", "\n", "sentence", "=", "[", "]", "\n", "sent_maxlen", "=", "0", "\n", "word_maxlen", "=", "0", "\n", "datasplit", "=", "[", "]", "\n", "ifpairs", "=", "[", "]", "\n", "\n", "for", "fname", "in", "(", "file_name", ",", ")", ":", "\n", "            ", "datasplit", ".", "append", "(", "len", "(", "img_id", ")", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "tweet_data_dir", ",", "fname", ")", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file", ":", "\n", "                ", "for", "line", "in", "file", ":", "\n", "                    ", "line", "=", "line", ".", "rstrip", "(", ")", "\n", "if", "line", "==", "''", ":", "\n", "                        ", "sent_maxlen", "=", "max", "(", "sent_maxlen", ",", "len", "(", "sentence", ")", ")", "\n", "sentences", ".", "append", "(", "sentence", ")", "\n", "sentence", "=", "[", "]", "\n", "", "else", ":", "\n", "                        ", "if", "IMAGEID", "in", "line", ":", "\n", "                            ", "line", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "num", "=", "line", "[", "0", "]", "[", "6", ":", "]", "\n", "img_id", ".", "append", "(", "num", ")", "\n", "ifpairs", ".", "append", "(", "int", "(", "line", "[", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "                            ", "sentence", ".", "append", "(", "line", ".", "split", "(", "'\\t'", ")", ")", "\n", "word_maxlen", "=", "max", "(", "word_maxlen", ",", "len", "(", "str", "(", "line", ".", "split", "(", ")", "[", "0", "]", ")", ")", ")", "\n", "", "", "", "", "print", "(", "1", ")", "\n", "\n", "", "datasplit", ".", "append", "(", "len", "(", "img_id", ")", ")", "\n", "num_sentence", "=", "len", "(", "sentences", ")", "\n", "\n", "print", "(", "\"datasplit\"", ",", "datasplit", ")", "\n", "print", "(", "sentences", "[", "len", "(", "sentences", ")", "-", "2", "]", ")", "\n", "print", "(", "sentences", "[", "0", "]", ")", "\n", "\n", "print", "(", "'sent_maxlen'", ",", "sent_maxlen", ")", "\n", "print", "(", "'word_maxlen'", ",", "word_maxlen", ")", "\n", "print", "(", "'number sentence'", ",", "len", "(", "sentences", ")", ")", "\n", "print", "(", "'number image'", ",", "len", "(", "img_id", ")", ")", "\n", "\n", "return", "[", "datasplit", ",", "sentences", ",", "img_id", ",", "sent_maxlen", ",", "word_maxlen", ",", "num_sentence", ",", "ifpairs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.label_index": [[177, 198], ["len", "len", "labels_counts.items", "labels_counts.most_common", "labelVoc.has_key", "labelVoc.setdefault", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "label_index", "(", "labels_counts", ")", ":", "\n", "        ", "\"\"\"\n           the input is the output of Counter. This function defines the (label, index) pair,\n           and it cast our datasets label to the definition (label, index) pair.\n        \"\"\"", "\n", "\n", "num_labels", "=", "len", "(", "labels_counts", ")", "\n", "labelVoc_inv", "=", "[", "x", "[", "0", "]", "for", "x", "in", "labels_counts", ".", "most_common", "(", ")", "]", "\n", "\n", "labelVoc", "=", "{", "'0'", ":", "0", ",", "\n", "'B-PER'", ":", "1", ",", "'I-PER'", ":", "2", ",", "\n", "'B-LOC'", ":", "3", ",", "'I-LOC'", ":", "4", ",", "\n", "'B-ORG'", ":", "5", ",", "'I-ORG'", ":", "6", ",", "\n", "'B-OTHER'", ":", "7", ",", "'I-OTHER'", ":", "8", ",", "\n", "'O'", ":", "9", "}", "\n", "if", "len", "(", "labelVoc", ")", "<", "num_labels", ":", "\n", "            ", "for", "key", ",", "value", "in", "labels_counts", ".", "items", "(", ")", ":", "\n", "                ", "if", "not", "labelVoc", ".", "has_key", "(", "key", ")", ":", "\n", "                    ", "labelVoc", ".", "setdefault", "(", "key", ",", "len", "(", "labelVoc", ")", ")", "\n", "", "", "", "return", "labelVoc_inv", ",", "labelVoc", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.pad_sequences": [[199, 206], ["numpy.zeros", "enumerate", "numpy.zeros.astype", "min", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "pad_sequences", "(", "y", ",", "sent_maxlen", ")", ":", "\n", "        ", "padded", "=", "np", ".", "zeros", "(", "(", "len", "(", "y", ")", ",", "sent_maxlen", ")", ")", "\n", "for", "i", ",", "each", "in", "enumerate", "(", "y", ")", ":", "\n", "            ", "trunc_len", "=", "min", "(", "sent_maxlen", ",", "len", "(", "each", ")", ")", "\n", "padded", "[", "i", ",", ":", "trunc_len", "]", "=", "each", "[", ":", "trunc_len", "]", "\n", "", "return", "padded", ".", "astype", "(", "np", ".", "int32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.pad_sequence": [[207, 251], ["print", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_loader_bb.DataLoader.pad_sequences", "data_loader_bb.DataLoader.pad_sequences", "numpy.asarray", "numpy.asarray", "list", "list.append", "list.append", "pytorch_pretrained_bert.BertTokenizer.from_pretrained.convert_tokens_to_ids", "numpy.asarray.append", "numpy.asarray.append", "w_id.append", "pytorch_pretrained_bert.BertTokenizer.from_pretrained.tokenize", "bert_tokenization.extend", "len", "len", "list.append"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.pad_sequences", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.data_loader_bb.DataLoader.pad_sequences", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize"], ["", "def", "pad_sequence", "(", "self", ",", "sentences", ",", "img_id", ",", "word_maxlen", "=", "30", ",", "sent_maxlen", "=", "35", ")", ":", "\n", "        ", "\"\"\"\n            This function is used to pad the word into the same length, the word length is set to 30.\n            Moreover, it also pad each sentence into the same length, the length is set to 35.\n\n        \"\"\"", "\n", "\n", "print", "(", "sentences", "[", "0", "]", ")", "\n", "x", "=", "[", "]", "\n", "y", "=", "[", "]", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "w_id", "=", "[", "]", "\n", "\n", "for", "word_label", "in", "sentence", ":", "\n", "                ", "w_id", ".", "append", "(", "word_label", "[", "0", "]", ")", "\n", "", "w_id", "=", "w_id", "[", ":", "-", "1", "]", "\n", "\n", "bert_tokenization", "=", "[", "]", "\n", "\n", "for", "token", "in", "w_id", ":", "\n", "                ", "subtokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "bert_tokenization", ".", "extend", "(", "subtokens", ")", "\n", "", "if", "len", "(", "bert_tokenization", ")", ">", "sent_maxlen", ":", "\n", "                ", "sent_maxlen", "=", "len", "(", "bert_tokenization", ")", "\n", "\n", "", "tokens", "=", "list", "(", ")", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "for", "token", "in", "bert_tokenization", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "x", ".", "append", "(", "input_ids", "[", ":", "]", ")", "\n", "y", ".", "append", "(", "input_ids", "[", ":", "]", ")", "\n", "\n", "", "sent_maxlen", "+=", "2", "\n", "\n", "y", "=", "self", ".", "pad_sequences", "(", "y", ",", "sent_maxlen", ")", "\n", "x", "=", "self", ".", "pad_sequences", "(", "x", ",", "sent_maxlen", ")", "\n", "x", "=", "np", ".", "asarray", "(", "x", ")", "\n", "y", "=", "np", ".", "asarray", "(", "y", ")", "\n", "return", "[", "x", ",", "img_id", ",", "y", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.test.parse_arguments": [[27, 80], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "cfgs.config.update_config"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.cfgs.config.update_config"], ["def", "parse_arguments", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Argument Parser for MNER'", ")", "\n", "parser", ".", "add_argument", "(", "\"--pre_image_obj_features_dir\"", ",", "dest", "=", "\"pre_image_obj_features_dir\"", ",", "type", "=", "str", ",", "\n", "default", "=", "'datasets/smap/rel_img/'", ")", "\n", "parser", ".", "add_argument", "(", "\"--pre_split_file\"", ",", "dest", "=", "\"pre_split_file\"", ",", "type", "=", "str", ",", "default", "=", "'datasets/smap/'", ")", "\n", "\n", "# parser.add_argument(\"--split_file\", dest=\"split_file\", type=str,", "\n", "#                         default='datasets/fudan/')", "\n", "# parser.add_argument(\"--image_obj_features_dir\", dest=\"image_obj_features_dir\", type=str,", "\n", "#                         default='datasets/fudan/ner_img/')", "\n", "\n", "parser", ".", "add_argument", "(", "\"--split_file\"", ",", "dest", "=", "\"split_file\"", ",", "type", "=", "str", ",", "\n", "default", "=", "'datasets/snap/'", ")", "\n", "parser", ".", "add_argument", "(", "\"--image_obj_features_dir\"", ",", "dest", "=", "\"image_obj_features_dir\"", ",", "type", "=", "str", ",", "\n", "default", "=", "'datasets/snap/ner_img/'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--pretrain_load\"", ",", "dest", "=", "\"pretrain_load\"", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--pre_hidden_dimension\"", ",", "dest", "=", "\"pre_hidden_dimension\"", ",", "type", "=", "int", ",", "default", "=", "256", ")", "\n", "parser", ".", "add_argument", "(", "\"--pre_embedding_dimension\"", ",", "dest", "=", "\"pre_embedding_dimension\"", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "\"--cat_h_e\"", ",", "dest", "=", "\"cat_h_e\"", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "\n", "MODEL_DIR", "=", "'/home/data/wjq_new/ner/models_addpre/'", "\n", "\n", "parser", ".", "add_argument", "(", "\"--hidden_dimension\"", ",", "dest", "=", "\"hidden_dimension\"", ",", "type", "=", "int", ",", "default", "=", "512", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "dest", "=", "\"batch_size\"", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "dest", "=", "\"lr\"", ",", "type", "=", "float", ",", "default", "=", "5e-5", ")", "\n", "parser", ".", "add_argument", "(", "\"--dropout\"", ",", "dest", "=", "\"dropout\"", ",", "type", "=", "float", ",", "default", "=", "0.5", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_epochs\"", ",", "dest", "=", "\"num_epochs\"", ",", "type", "=", "int", ",", "default", "=", "40", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--n_layers\"", ",", "dest", "=", "\"n_layers\"", ",", "type", "=", "int", ",", "default", "=", "3", ")", "\n", "parser", ".", "add_argument", "(", "\"--clip_value\"", ",", "dest", "=", "\"clip_value\"", ",", "type", "=", "float", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "\"--wdecay\"", ",", "dest", "=", "\"wdecay\"", ",", "type", "=", "float", ",", "default", "=", "0.0000001", ")", "\n", "parser", ".", "add_argument", "(", "\"--step_size\"", ",", "dest", "=", "\"step_size\"", ",", "type", "=", "int", ",", "default", "=", "15", ")", "\n", "parser", ".", "add_argument", "(", "\"--gamma\"", ",", "dest", "=", "\"gamma\"", ",", "type", "=", "float", ",", "default", "=", "0.01", ")", "\n", "parser", ".", "add_argument", "(", "\"--validate_every\"", ",", "dest", "=", "\"validate_every\"", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "\"--mode\"", ",", "dest", "=", "\"mode\"", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_dir\"", ",", "dest", "=", "\"model_dir\"", ",", "type", "=", "str", ",", "default", "=", "MODEL_DIR", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_file_name\"", ",", "dest", "=", "\"model_file_name\"", ",", "type", "=", "str", ",", "default", "=", "\"path to your rpbert weights\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sent_maxlen\"", ",", "dest", "=", "\"sent_maxlen\"", ",", "type", "=", "int", ",", "default", "=", "35", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_maxlen\"", ",", "dest", "=", "\"word_maxlen\"", ",", "type", "=", "int", ",", "default", "=", "41", ")", "\n", "parser", ".", "add_argument", "(", "\"--regions_in_image\"", ",", "dest", "=", "\"regions_in_image\"", ",", "type", "=", "int", ",", "default", "=", "49", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--cfg'", ",", "type", "=", "str", ",", "help", "=", "'path to config file'", ",", "\n", "default", "=", "'cfgs/base_gt_boxes_4x16G.yaml'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "cfg", "is", "not", "None", ":", "\n", "        ", "update_config", "(", "args", ".", "cfg", ")", "\n", "\n", "", "return", "args", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.test.main": [[82, 127], ["test.parse_arguments", "print", "print", "print", "rpbert.resnet_vlbert.ResNetVLBERT", "rpbert.bert_rel.BertRel", "data_loader.DataLoader", "data_loader_bb.DataLoader", "evaluator.Evaluator", "print", "print", "trainer.Trainer", "trainer.Trainer.train", "print", "print", "flair.embeddings.StackedEmbeddings", "mner.MNER", "model.cuda.load_state_dict", "torch.cuda.is_available", "print", "print", "print", "print", "print", "print", "print", "flair.embeddings.WordEmbeddings", "flair.embeddings.CharacterEmbeddings", "torch.load", "model.cuda.cuda", "torch.no_grad", "evaluator.Evaluator.get_accuracy"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.test.parse_arguments", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.evaluator.Evaluator.get_accuracy"], ["", "def", "main", "(", ")", ":", "\n", "    ", "params", ",", "config", "=", "parse_arguments", "(", ")", "\n", "print", "(", "config", ")", "\n", "print", "(", "params", ")", "\n", "print", "(", "\"Constructing data loaders...\"", ")", "\n", "\n", "\n", "\n", "myvlbert", "=", "ResNetVLBERT", "(", "config", ")", "\n", "pre_model", "=", "BertRel", "(", "params", ",", "myvlbert", ")", "\n", "\n", "dl", "=", "DataLoader", "(", "params", ")", "\n", "dlbb", "=", "DLbb", "(", "params", ")", "\n", "evaluator", "=", "Evaluator", "(", "params", ",", "dl", ")", "\n", "print", "(", "\"Constructing data loaders...[OK]\"", ")", "\n", "\n", "if", "params", ".", "mode", "==", "0", ":", "\n", "        ", "print", "(", "\"Training...\"", ")", "\n", "t", "=", "Trainer", "(", "params", ",", "config", ",", "dl", ",", "dlbb", ",", "evaluator", ",", "pre_model", ")", "\n", "t", ".", "train", "(", ")", "\n", "print", "(", "\"Training...[OK]\"", ")", "\n", "", "elif", "params", ".", "mode", "==", "1", ":", "\n", "        ", "print", "(", "\"Loading rpbert...\"", ")", "\n", "embedding_types", "=", "[", "\n", "WordEmbeddings", "(", "\n", "'/media/iot538/a73dbfc5-a8a0-4021-a841-3b7d7f3fd964/mnt/xj/wnut17_advanced/pretrain/en-fasttext-crawl-300d-1M'", ")", ",", "\n", "CharacterEmbeddings", "(", "'/home/iot538/.flair/datasets/common_characters_large'", ")", ",", "\n", "]", "\n", "\n", "embeddings", ":", "StackedEmbeddings", "=", "StackedEmbeddings", "(", "embeddings", "=", "embedding_types", ")", "\n", "model", "=", "MNER", "(", "params", ",", "embeddings", ",", "pre_model", ")", "\n", "model_file_path", "=", "params", ".", "model_file_name", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_file_path", ")", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "model", "=", "model", ".", "cuda", "(", ")", "\n", "", "print", "(", "\"Loading rpbert...[OK]\"", ")", "\n", "\n", "print", "(", "\"Evaluating rpbert on test set...\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "acc", ",", "f1", ",", "prec", ",", "rec", "=", "evaluator", ".", "get_accuracy", "(", "model", ",", "'test'", ")", "\n", "", "print", "(", "\"Accuracy : {}\"", ".", "format", "(", "acc", ")", ")", "\n", "print", "(", "\"F1 : {}\"", ".", "format", "(", "f1", ")", ")", "\n", "print", "(", "\"Precision : {}\"", ".", "format", "(", "prec", ")", ")", "\n", "print", "(", "\"Recall : {}\"", ".", "format", "(", "rec", ")", ")", "\n", "print", "(", "\"Evaluating rpbert on test set...[OK]\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.util.to_tensor": [[5, 7], ["torch.from_numpy().float", "torch.from_numpy", "numpy.array"], "function", ["None"], ["def", "to_tensor", "(", "array", ")", ":", "\n", "    ", "return", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "array", ")", ")", ".", "float", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.util.to_variable": [[9, 13], ["torch.cuda.is_available", "torch.autograd.Variable", "tensor.cuda.cuda"], "function", ["None"], ["", "def", "to_variable", "(", "tensor", ",", "requires_grad", "=", "False", ")", ":", "\n", "    ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "tensor", "=", "tensor", ".", "cuda", "(", ")", "\n", "", "return", "torch", ".", "autograd", ".", "Variable", "(", "tensor", ",", "requires_grad", "=", "requires_grad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.util.my_relu": [[15, 18], ["None"], "function", ["None"], ["", "def", "my_relu", "(", "data", ")", ":", "\n", "    ", "data", "[", "data", "<", "1e-8", "]", "=", "1e-8", "\n", "return", "data", "\n", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.evaluator.Evaluator.__init__": [[8, 11], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "data_loader", ")", ":", "\n", "        ", "self", ".", "params", "=", "params", "\n", "self", ".", "data_loader", "=", "data_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.evaluator.Evaluator.get_accuracy": [[12, 47], ["model.eval", "tqdm.tqdm.tqdm", "evaluator.Evaluator.evaluate", "len", "torchcrf.CRF", "torch.cuda.is_available", "model", "torch.softmax", "torch.softmax.detach", "model", "crf.cuda.cuda.decode", "words.append", "labels.append", "labels_pred.append", "sent_lens.append", "crf.cuda.cuda.cuda", "util.to_variable", "util.to_variable", "util.to_variable", "util.to_variable", "util.to_variable", "util.to_variable", "y.cpu().numpy().squeeze", "lens.cpu().numpy", "y.cpu().numpy", "lens.cpu", "y.cpu"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.decode", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable"], ["", "def", "get_accuracy", "(", "self", ",", "model", ",", "split", ",", "crf", "=", "None", ")", ":", "\n", "        ", "if", "split", "==", "'val'", ":", "\n", "            ", "data_loader", "=", "self", ".", "data_loader", ".", "val_data_loader", "\n", "", "else", ":", "\n", "            ", "data_loader", "=", "self", ".", "data_loader", ".", "test_data_loader", "\n", "\n", "", "if", "crf", "==", "None", ":", "\n", "            ", "num_of_tags", "=", "len", "(", "self", ".", "data_loader", ".", "labelVoc", ")", "\n", "crf", "=", "CRF", "(", "num_of_tags", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "crf", "=", "crf", ".", "cuda", "(", ")", "\n", "\n", "", "", "model", ".", "eval", "(", ")", "\n", "labels_pred", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "words", "=", "[", "]", "\n", "sent_lens", "=", "[", "]", "\n", "for", "(", "x", ",", "x_flair", ",", "x_obj", ",", "y", ",", "mask", ",", "lens", ")", "in", "tqdm", "(", "data_loader", ")", ":", "\n", "            ", "pair_out", "=", "model", "(", "to_variable", "(", "x", ")", ",", "x_flair", ",", "to_variable", "(", "x_obj", ")", ",", "lens", ",", "\n", "to_variable", "(", "mask", ")", ",", "relation", "=", "None", ",", "mode", "=", "\"test\"", ")", "# seq_len * bs * labels", "\n", "relation", "=", "F", ".", "softmax", "(", "pair_out", ",", "dim", "=", "-", "1", ")", "\n", "relation_", "=", "relation", ".", "detach", "(", ")", "\n", "emissions", ",", "attention_probs", "=", "model", "(", "to_variable", "(", "x", ")", ",", "x_flair", ",", "to_variable", "(", "x_obj", ")", ",", "lens", ",", "to_variable", "(", "mask", ")", ",", "\n", "relation", "=", "relation_", ",", "mode", "=", "\"test\"", ")", "# seq_len * bs * labels", "\n", "pre_test_label_index", "=", "crf", ".", "decode", "(", "emissions", ")", "# bs * seq_len", "\n", "words", ".", "append", "(", "x", ")", "\n", "\n", "labels", ".", "append", "(", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "squeeze", "(", "0", ")", ")", "\n", "\n", "labels_pred", ".", "append", "(", "pre_test_label_index", "[", "0", "]", ")", "\n", "\n", "sent_lens", ".", "append", "(", "lens", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", ")", "\n", "\n", "\n", "", "return", "self", ".", "evaluate", "(", "labels_pred", ",", "labels", ",", "words", ",", "sent_lens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.evaluator.Evaluator.evaluate": [[48, 67], ["zip", "np.mean", "set", "set", "len", "len", "len", "evaluator.Evaluator.get_chunks", "evaluator.Evaluator.get_chunks", "zip"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.evaluator.Evaluator.get_chunks", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.evaluator.Evaluator.get_chunks"], ["", "def", "evaluate", "(", "self", ",", "labels_pred", ",", "labels", ",", "words", ",", "sents_length", ")", ":", "\n", "        ", "accs", "=", "[", "]", "\n", "correct_preds", ",", "total_correct", ",", "total_preds", "=", "0.", ",", "0.", ",", "0.", "\n", "\n", "for", "lab", ",", "lab_pred", ",", "length", ",", "word_sent", "in", "zip", "(", "labels", ",", "labels_pred", ",", "sents_length", ",", "words", ")", ":", "\n", "            ", "lab", "=", "lab", "[", ":", "length", "]", "\n", "lab_pred", "=", "lab_pred", "[", ":", "length", "]", "\n", "accs", "+=", "[", "a", "==", "b", "for", "(", "a", ",", "b", ")", "in", "zip", "(", "lab", ",", "lab_pred", ")", "]", "\n", "lab_chunks", "=", "set", "(", "self", ".", "get_chunks", "(", "lab", ",", "self", ".", "data_loader", ".", "labelVoc", ")", ")", "\n", "lab_pred_chunks", "=", "set", "(", "self", ".", "get_chunks", "(", "lab_pred", ",", "self", ".", "data_loader", ".", "labelVoc", ")", ")", "\n", "correct_preds", "+=", "len", "(", "lab_chunks", "&", "lab_pred_chunks", ")", "\n", "total_preds", "+=", "len", "(", "lab_pred_chunks", ")", "\n", "total_correct", "+=", "len", "(", "lab_chunks", ")", "\n", "\n", "", "p", "=", "correct_preds", "/", "total_preds", "if", "correct_preds", ">", "0", "else", "0", "\n", "r", "=", "correct_preds", "/", "total_correct", "if", "correct_preds", ">", "0", "else", "0", "\n", "f1", "=", "2", "*", "p", "*", "r", "/", "(", "p", "+", "r", ")", "if", "correct_preds", ">", "0", "else", "0", "\n", "acc", "=", "np", ".", "mean", "(", "accs", ")", "\n", "return", "acc", ",", "f1", ",", "p", ",", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.evaluator.Evaluator.get_chunks": [[68, 110], ["enumerate", "chunks.append", "tags.items", "chunks.append", "len", "evaluator.Evaluator.get_chunk_type", "chunks.append"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.evaluator.Evaluator.get_chunk_type"], ["", "def", "get_chunks", "(", "self", ",", "seq", ",", "tags", ")", ":", "\n", "        ", "\"\"\"\n        tags:dic{'per':1,....}\n        Args:\n            seq: [4, 4, 0, 0, ...] sequence of labels\n            tags: dict[\"O\"] = 4\n        Returns:\n            list of (chunk_type, chunk_start, chunk_end)\n        Example:\n            seq = [4, 5, 0, 3]\n            tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n            result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n        \"\"\"", "\n", "default", "=", "tags", "[", "'O'", "]", "\n", "idx_to_tag", "=", "{", "idx", ":", "tag", "for", "tag", ",", "idx", "in", "tags", ".", "items", "(", ")", "}", "\n", "chunks", "=", "[", "]", "\n", "chunk_type", ",", "chunk_start", "=", "None", ",", "None", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "seq", ")", ":", "\n", "# End of a chunk 1", "\n", "            ", "if", "tok", "==", "default", "and", "chunk_type", "is", "not", "None", ":", "\n", "# Add a chunk.", "\n", "                ", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "i", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "chunk_type", ",", "chunk_start", "=", "None", ",", "None", "\n", "\n", "# End of a chunk + start of a chunk!", "\n", "", "elif", "tok", "!=", "default", ":", "\n", "                ", "tok_chunk_class", ",", "tok_chunk_type", "=", "self", ".", "get_chunk_type", "(", "tok", ",", "idx_to_tag", ")", "\n", "if", "chunk_type", "is", "None", ":", "\n", "                    ", "chunk_type", ",", "chunk_start", "=", "tok_chunk_type", ",", "i", "\n", "", "elif", "tok_chunk_type", "!=", "chunk_type", "or", "tok_chunk_class", "==", "\"B\"", ":", "\n", "                    ", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "i", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "chunk_type", ",", "chunk_start", "=", "tok_chunk_type", ",", "i", "\n", "", "", "else", ":", "\n", "                ", "pass", "\n", "# end condition", "\n", "", "", "if", "chunk_type", "is", "not", "None", ":", "\n", "            ", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "len", "(", "seq", ")", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "\n", "", "return", "chunks", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.None.evaluator.Evaluator.get_chunk_type": [[111, 123], ["tag_name.split", "tag_name.split"], "methods", ["None"], ["", "def", "get_chunk_type", "(", "self", ",", "tok", ",", "idx_to_tag", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            tok: id of token, such as 4\n            idx_to_tag: dictionary {4: \"B-PER\", ...}\n        Returns:\n            tuple: \"B\", \"PER\"\n        \"\"\"", "\n", "tag_name", "=", "idx_to_tag", "[", "tok", "]", "\n", "tag_class", "=", "tag_name", ".", "split", "(", "'-'", ")", "[", "0", "]", "\n", "tag_type", "=", "tag_name", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", "\n", "return", "tag_class", ",", "tag_type", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.cfgs.config.update_config": [[176, 205], ["open", "easydict.EasyDict", "easydict.EasyDict.items", "yaml.load", "isinstance", "ValueError", "v.items", "tuple", "ValueError", "tuple", "float", "tuple", "isinstance", "vv.items", "vv.split", "str", "ValueError", "vvi.split"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["def", "update_config", "(", "config_file", ")", ":", "\n", "    ", "with", "open", "(", "config_file", ")", "as", "f", ":", "\n", "        ", "exp_config", "=", "edict", "(", "yaml", ".", "load", "(", "f", ")", ")", "\n", "for", "k", ",", "v", "in", "exp_config", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "in", "config", ":", "\n", "                ", "if", "isinstance", "(", "v", ",", "dict", ")", ":", "\n", "                    ", "for", "vk", ",", "vv", "in", "v", ".", "items", "(", ")", ":", "\n", "                        ", "if", "vk", "in", "config", "[", "k", "]", ":", "\n", "                            ", "if", "vk", "==", "'LR_STEP'", ":", "\n", "                                ", "config", "[", "k", "]", "[", "vk", "]", "=", "tuple", "(", "float", "(", "s", ")", "for", "s", "in", "vv", ".", "split", "(", "','", ")", ")", "\n", "", "elif", "vk", "==", "'LOSS_LOGGERS'", ":", "\n", "                                ", "config", "[", "k", "]", "[", "vk", "]", "=", "[", "tuple", "(", "str", "(", "s", ")", "for", "s", "in", "vvi", ".", "split", "(", "','", ")", ")", "for", "vvi", "in", "vv", "]", "\n", "", "elif", "vk", "==", "\"VLBERT\"", "and", "isinstance", "(", "vv", ",", "dict", ")", ":", "\n", "                                ", "for", "vvk", ",", "vvv", "in", "vv", ".", "items", "(", ")", ":", "\n", "                                    ", "if", "vvk", "in", "config", "[", "k", "]", "[", "vk", "]", ":", "\n", "                                        ", "config", "[", "k", "]", "[", "vk", "]", "[", "vvk", "]", "=", "vvv", "\n", "", "else", ":", "\n", "                                        ", "raise", "ValueError", "(", "\"key {}.{}.{} not in config.py\"", ".", "format", "(", "k", ",", "vk", ",", "vvk", ")", ")", "\n", "", "", "", "else", ":", "\n", "                                ", "config", "[", "k", "]", "[", "vk", "]", "=", "vv", "\n", "", "", "else", ":", "\n", "                            ", "raise", "ValueError", "(", "\"key {}.{} not in config.py\"", ".", "format", "(", "k", ",", "vk", ")", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "if", "k", "==", "'SCALES'", ":", "\n", "                        ", "config", "[", "k", "]", "=", "(", "tuple", "(", "v", ")", ")", "\n", "", "else", ":", "\n", "                        ", "config", "[", "k", "]", "=", "v", "\n", "", "", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"key {} not in config.py\"", ".", "format", "(", "k", ")", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.from_pretrained": [[57, 92], ["cls", "torch.load", "torch.load.items", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "logger.error", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a TransfoXLTokenizer.\n        The TransfoXLTokenizer.\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "VOCAB_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in rpbert name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find files {} \"", "\n", "\"at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", "vocab_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "vocab_dict", "=", "torch", ".", "load", "(", "resolved_vocab_file", ")", "\n", "for", "key", ",", "value", "in", "vocab_dict", ".", "items", "(", ")", ":", "\n", "            ", "tokenizer", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.__init__": [[93, 103], ["collections.Counter"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "special", "=", "[", "]", ",", "min_freq", "=", "0", ",", "max_size", "=", "None", ",", "lower_case", "=", "False", ",", "\n", "delimiter", "=", "None", ",", "vocab_file", "=", "None", ",", "never_split", "=", "(", "\"<unk>\"", ",", "\"<eos>\"", ",", "\"<formula>\"", ")", ")", ":", "\n", "        ", "self", ".", "counter", "=", "Counter", "(", ")", "\n", "self", ".", "special", "=", "special", "\n", "self", ".", "min_freq", "=", "min_freq", "\n", "self", ".", "max_size", "=", "max_size", "\n", "self", ".", "lower_case", "=", "lower_case", "\n", "self", ".", "delimiter", "=", "delimiter", "\n", "self", ".", "vocab_file", "=", "vocab_file", "\n", "self", ".", "never_split", "=", "never_split", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.count_file": [[104, 118], ["os.path.exists", "print", "io.open", "enumerate", "tokenization_transfo_xl.TransfoXLTokenizer.tokenize", "tokenization_transfo_xl.TransfoXLTokenizer.counter.update", "sents.append", "print"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize"], ["", "def", "count_file", "(", "self", ",", "path", ",", "verbose", "=", "False", ",", "add_eos", "=", "False", ")", ":", "\n", "        ", "if", "verbose", ":", "print", "(", "'counting file {} ...'", ".", "format", "(", "path", ")", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "path", ")", "\n", "\n", "sents", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "idx", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "if", "verbose", "and", "idx", ">", "0", "and", "idx", "%", "500000", "==", "0", ":", "\n", "                    ", "print", "(", "'    line {}'", ".", "format", "(", "idx", ")", ")", "\n", "", "symbols", "=", "self", ".", "tokenize", "(", "line", ",", "add_eos", "=", "add_eos", ")", "\n", "self", ".", "counter", ".", "update", "(", "symbols", ")", "\n", "sents", ".", "append", "(", "symbols", ")", "\n", "\n", "", "", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.count_sents": [[119, 128], ["enumerate", "print", "tokenization_transfo_xl.TransfoXLTokenizer.counter.update", "print", "len"], "methods", ["None"], ["", "def", "count_sents", "(", "self", ",", "sents", ",", "verbose", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n            sents : a list of sentences, each a list of tokenized symbols\n        \"\"\"", "\n", "if", "verbose", ":", "print", "(", "'counting {} sents ...'", ".", "format", "(", "len", "(", "sents", ")", ")", ")", "\n", "for", "idx", ",", "symbols", "in", "enumerate", "(", "sents", ")", ":", "\n", "            ", "if", "verbose", "and", "idx", ">", "0", "and", "idx", "%", "500000", "==", "0", ":", "\n", "                ", "print", "(", "'    line {}'", ".", "format", "(", "idx", ")", ")", "\n", "", "self", ".", "counter", ".", "update", "(", "symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer._build_from_file": [[129, 143], ["collections.OrderedDict", "io.open", "tokenization_transfo_xl.TransfoXLTokenizer.add_symbol", "ValueError", "line.strip().split", "line.strip"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.add_symbol"], ["", "", "def", "_build_from_file", "(", "self", ",", "vocab_file", ")", ":", "\n", "        ", "self", ".", "idx2sym", "=", "[", "]", "\n", "self", ".", "sym2idx", "=", "OrderedDict", "(", ")", "\n", "\n", "with", "open", "(", "vocab_file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "symb", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", "0", "]", "\n", "self", ".", "add_symbol", "(", "symb", ")", "\n", "", "", "if", "'<UNK>'", "in", "self", ".", "sym2idx", ":", "\n", "            ", "self", ".", "unk_idx", "=", "self", ".", "sym2idx", "[", "'<UNK>'", "]", "\n", "", "elif", "'<unk>'", "in", "self", ".", "sym2idx", ":", "\n", "            ", "self", ".", "unk_idx", "=", "self", ".", "sym2idx", "[", "'<unk>'", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'No <unkown> token in vocabulary'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.build_vocab": [[144, 164], ["print", "tokenization_transfo_xl.TransfoXLTokenizer._build_from_file", "print", "print", "collections.OrderedDict", "tokenization_transfo_xl.TransfoXLTokenizer.counter.most_common", "print", "tokenization_transfo_xl.TransfoXLTokenizer.add_special", "tokenization_transfo_xl.TransfoXLTokenizer.add_symbol", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer._build_from_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.add_special", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.add_symbol"], ["", "", "def", "build_vocab", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "vocab_file", ":", "\n", "            ", "print", "(", "'building vocab from {}'", ".", "format", "(", "self", ".", "vocab_file", ")", ")", "\n", "self", ".", "_build_from_file", "(", "self", ".", "vocab_file", ")", "\n", "print", "(", "'final vocab size {}'", ".", "format", "(", "len", "(", "self", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'building vocab with min_freq={}, max_size={}'", ".", "format", "(", "\n", "self", ".", "min_freq", ",", "self", ".", "max_size", ")", ")", "\n", "self", ".", "idx2sym", "=", "[", "]", "\n", "self", ".", "sym2idx", "=", "OrderedDict", "(", ")", "\n", "\n", "for", "sym", "in", "self", ".", "special", ":", "\n", "                ", "self", ".", "add_special", "(", "sym", ")", "\n", "\n", "", "for", "sym", ",", "cnt", "in", "self", ".", "counter", ".", "most_common", "(", "self", ".", "max_size", ")", ":", "\n", "                ", "if", "cnt", "<", "self", ".", "min_freq", ":", "break", "\n", "self", ".", "add_symbol", "(", "sym", ")", "\n", "\n", "", "print", "(", "'final vocab size {} from {} unique tokens'", ".", "format", "(", "\n", "len", "(", "self", ")", ",", "len", "(", "self", ".", "counter", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.encode_file": [[165, 182], ["os.path.exists", "print", "io.open", "enumerate", "torch.cat", "tokenization_transfo_xl.TransfoXLTokenizer.tokenize", "torch.cat.append", "print", "tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor"], ["", "", "def", "encode_file", "(", "self", ",", "path", ",", "ordered", "=", "False", ",", "verbose", "=", "False", ",", "add_eos", "=", "True", ",", "\n", "add_double_eos", "=", "False", ")", ":", "\n", "        ", "if", "verbose", ":", "print", "(", "'encoding file {} ...'", ".", "format", "(", "path", ")", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "path", ")", "\n", "encoded", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "idx", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "if", "verbose", "and", "idx", ">", "0", "and", "idx", "%", "500000", "==", "0", ":", "\n", "                    ", "print", "(", "'    line {}'", ".", "format", "(", "idx", ")", ")", "\n", "", "symbols", "=", "self", ".", "tokenize", "(", "line", ",", "add_eos", "=", "add_eos", ",", "\n", "add_double_eos", "=", "add_double_eos", ")", "\n", "encoded", ".", "append", "(", "self", ".", "convert_to_tensor", "(", "symbols", ")", ")", "\n", "\n", "", "", "if", "ordered", ":", "\n", "            ", "encoded", "=", "torch", ".", "cat", "(", "encoded", ")", "\n", "\n", "", "return", "encoded", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.encode_sents": [[183, 195], ["enumerate", "print", "torch.cat.append", "torch.cat", "print", "tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor"], ["", "def", "encode_sents", "(", "self", ",", "sents", ",", "ordered", "=", "False", ",", "verbose", "=", "False", ")", ":", "\n", "        ", "if", "verbose", ":", "print", "(", "'encoding {} sents ...'", ".", "format", "(", "len", "(", "sents", ")", ")", ")", "\n", "encoded", "=", "[", "]", "\n", "for", "idx", ",", "symbols", "in", "enumerate", "(", "sents", ")", ":", "\n", "            ", "if", "verbose", "and", "idx", ">", "0", "and", "idx", "%", "500000", "==", "0", ":", "\n", "                ", "print", "(", "'    line {}'", ".", "format", "(", "idx", ")", ")", "\n", "", "encoded", ".", "append", "(", "self", ".", "convert_to_tensor", "(", "symbols", ")", ")", "\n", "\n", "", "if", "ordered", ":", "\n", "            ", "encoded", "=", "torch", ".", "cat", "(", "encoded", ")", "\n", "\n", "", "return", "encoded", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.add_special": [[196, 201], ["tokenization_transfo_xl.TransfoXLTokenizer.idx2sym.append", "setattr", "len", "sym.strip"], "methods", ["None"], ["", "def", "add_special", "(", "self", ",", "sym", ")", ":", "\n", "        ", "if", "sym", "not", "in", "self", ".", "sym2idx", ":", "\n", "            ", "self", ".", "idx2sym", ".", "append", "(", "sym", ")", "\n", "self", ".", "sym2idx", "[", "sym", "]", "=", "len", "(", "self", ".", "idx2sym", ")", "-", "1", "\n", "setattr", "(", "self", ",", "'{}_idx'", ".", "format", "(", "sym", ".", "strip", "(", "'<>'", ")", ")", ",", "self", ".", "sym2idx", "[", "sym", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.add_symbol": [[202, 206], ["tokenization_transfo_xl.TransfoXLTokenizer.idx2sym.append", "len"], "methods", ["None"], ["", "", "def", "add_symbol", "(", "self", ",", "sym", ")", ":", "\n", "        ", "if", "sym", "not", "in", "self", ".", "sym2idx", ":", "\n", "            ", "self", ".", "idx2sym", ".", "append", "(", "sym", ")", "\n", "self", ".", "sym2idx", "[", "sym", "]", "=", "len", "(", "self", ".", "idx2sym", ")", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.get_sym": [[207, 210], ["len"], "methods", ["None"], ["", "", "def", "get_sym", "(", "self", ",", "idx", ")", ":", "\n", "        ", "assert", "0", "<=", "idx", "<", "len", "(", "self", ")", ",", "'Index {} out of vocabulary range'", ".", "format", "(", "idx", ")", "\n", "return", "self", ".", "idx2sym", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.get_idx": [[211, 226], ["hasattr", "tokenization_transfo_xl.TransfoXLTokenizer.sym2idx.get", "ValueError"], "methods", ["None"], ["", "def", "get_idx", "(", "self", ",", "sym", ")", ":", "\n", "        ", "if", "sym", "in", "self", ".", "sym2idx", ":", "\n", "            ", "return", "self", ".", "sym2idx", "[", "sym", "]", "\n", "", "else", ":", "\n", "# print('encounter unk {}'.format(sym))", "\n", "# assert '<eos>' not in sym", "\n", "            ", "if", "hasattr", "(", "self", ",", "'unk_idx'", ")", ":", "\n", "                ", "return", "self", ".", "sym2idx", ".", "get", "(", "sym", ",", "self", ".", "unk_idx", ")", "\n", "# Backward compatibility with pre-trained models", "\n", "", "elif", "'<unk>'", "in", "self", ".", "sym2idx", ":", "\n", "                ", "return", "self", ".", "sym2idx", "[", "'<unk>'", "]", "\n", "", "elif", "'<UNK>'", "in", "self", ".", "sym2idx", ":", "\n", "                ", "return", "self", ".", "sym2idx", "[", "'<UNK>'", "]", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "'Token not in vocabulary and no <unk> token in vocabulary for replacement'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.convert_ids_to_tokens": [[227, 230], ["tokenization_transfo_xl.TransfoXLTokenizer.get_sym"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.get_sym"], ["", "", "", "def", "convert_ids_to_tokens", "(", "self", ",", "indices", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of indices in symbols using the vocab.\"\"\"", "\n", "return", "[", "self", ".", "get_sym", "(", "idx", ")", "for", "idx", "in", "indices", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.convert_tokens_to_ids": [[231, 234], ["tokenization_transfo_xl.TransfoXLTokenizer.get_idx"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.get_idx"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "symbols", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of symbols into ids using the vocab.\"\"\"", "\n", "return", "[", "self", ".", "get_idx", "(", "sym", ")", "for", "sym", "in", "symbols", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor": [[235, 237], ["torch.LongTensor", "tokenization_transfo_xl.TransfoXLTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.convert_tokens_to_ids"], ["", "def", "convert_to_tensor", "(", "self", ",", "symbols", ")", ":", "\n", "        ", "return", "torch", ".", "LongTensor", "(", "self", ".", "convert_tokens_to_ids", "(", "symbols", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.decode": [[238, 244], ["tokenization_transfo_xl.TransfoXLTokenizer.get_sym", "tokenization_transfo_xl.TransfoXLTokenizer.get_sym"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.get_sym", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.get_sym"], ["", "def", "decode", "(", "self", ",", "indices", ",", "exclude", "=", "None", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of indices in a string.\"\"\"", "\n", "if", "exclude", "is", "None", ":", "\n", "            ", "return", "' '", ".", "join", "(", "[", "self", ".", "get_sym", "(", "idx", ")", "for", "idx", "in", "indices", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "' '", ".", "join", "(", "[", "self", ".", "get_sym", "(", "idx", ")", "for", "idx", "in", "indices", "if", "idx", "not", "in", "exclude", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.__len__": [[245, 247], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "idx2sym", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer._run_split_on_punc": [[248, 269], ["list", "len", "tokenization_transfo_xl._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "text", "in", "self", ".", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer._run_strip_accents": [[270, 280], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer._clean_text": [[281, 293], ["ord", "tokenization_transfo_xl._is_whitespace", "tokenization_transfo_xl._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.whitespace_tokenize": [[294, 304], ["text.strip.strip.strip", "text.strip.strip.split"], "methods", ["None"], ["", "def", "whitespace_tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "            ", "return", "[", "]", "\n", "", "if", "self", ".", "delimiter", "==", "''", ":", "\n", "            ", "tokens", "=", "text", "\n", "", "else", ":", "\n", "            ", "tokens", "=", "text", ".", "split", "(", "self", ".", "delimiter", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.tokenize": [[305, 324], ["tokenization_transfo_xl.TransfoXLTokenizer._clean_text", "line.strip.strip.strip", "tokenization_transfo_xl.TransfoXLTokenizer.whitespace_tokenize", "split_symbols.extend", "tokenization_transfo_xl.TransfoXLTokenizer.lower", "tokenization_transfo_xl.TransfoXLTokenizer._run_strip_accents", "tokenization_transfo_xl.TransfoXLTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "line", ",", "add_eos", "=", "False", ",", "add_double_eos", "=", "False", ")", ":", "\n", "        ", "line", "=", "self", ".", "_clean_text", "(", "line", ")", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "\n", "symbols", "=", "self", ".", "whitespace_tokenize", "(", "line", ")", "\n", "\n", "split_symbols", "=", "[", "]", "\n", "for", "symbol", "in", "symbols", ":", "\n", "            ", "if", "self", ".", "lower_case", "and", "symbol", "not", "in", "self", ".", "never_split", ":", "\n", "                ", "symbol", "=", "symbol", ".", "lower", "(", ")", "\n", "symbol", "=", "self", ".", "_run_strip_accents", "(", "symbol", ")", "\n", "", "split_symbols", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "symbol", ")", ")", "\n", "\n", "", "if", "add_double_eos", ":", "# lm1b", "\n", "            ", "return", "[", "'<S>'", "]", "+", "split_symbols", "+", "[", "'<S>'", "]", "\n", "", "elif", "add_eos", ":", "\n", "            ", "return", "split_symbols", "+", "[", "'<eos>'", "]", "\n", "", "else", ":", "\n", "            ", "return", "split_symbols", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMOrderedIterator.__init__": [[327, 348], ["data.narrow.narrow.narrow", "data.narrow.narrow.view().t().contiguous().to", "data.narrow.narrow.size", "data.narrow.narrow.view().t().contiguous", "data.narrow.narrow.view().t", "data.narrow.narrow.view"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "bsz", ",", "bptt", ",", "device", "=", "'cpu'", ",", "ext_len", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n            data -- LongTensor -- the LongTensor is strictly ordered\n        \"\"\"", "\n", "self", ".", "bsz", "=", "bsz", "\n", "self", ".", "bptt", "=", "bptt", "\n", "self", ".", "ext_len", "=", "ext_len", "if", "ext_len", "is", "not", "None", "else", "0", "\n", "\n", "self", ".", "device", "=", "device", "\n", "\n", "# Work out how cleanly we can divide the dataset into bsz parts.", "\n", "self", ".", "n_step", "=", "data", ".", "size", "(", "0", ")", "//", "bsz", "\n", "\n", "# Trim off any extra elements that wouldn't cleanly fit (remainders).", "\n", "data", "=", "data", ".", "narrow", "(", "0", ",", "0", ",", "self", ".", "n_step", "*", "bsz", ")", "\n", "\n", "# Evenly divide the data across the bsz batches.", "\n", "self", ".", "data", "=", "data", ".", "view", "(", "bsz", ",", "-", "1", ")", ".", "t", "(", ")", ".", "contiguous", "(", ")", ".", "to", "(", "device", ")", "\n", "\n", "# Number of mini-batches", "\n", "self", ".", "n_batch", "=", "(", "self", ".", "n_step", "+", "self", ".", "bptt", "-", "1", ")", "//", "self", ".", "bptt", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMOrderedIterator.get_batch": [[349, 363], ["min", "max", "data.transpose().contiguous().to", "target.transpose().contiguous().to", "data.transpose().contiguous", "target.transpose().contiguous", "tokenization_transfo_xl.LMOrderedIterator.data.size", "data.transpose", "target.transpose"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "get_batch", "(", "self", ",", "i", ",", "bptt", "=", "None", ")", ":", "\n", "        ", "if", "bptt", "is", "None", ":", "bptt", "=", "self", ".", "bptt", "\n", "seq_len", "=", "min", "(", "bptt", ",", "self", ".", "data", ".", "size", "(", "0", ")", "-", "1", "-", "i", ")", "\n", "\n", "end_idx", "=", "i", "+", "seq_len", "\n", "beg_idx", "=", "max", "(", "0", ",", "i", "-", "self", ".", "ext_len", ")", "\n", "\n", "data", "=", "self", ".", "data", "[", "beg_idx", ":", "end_idx", "]", "\n", "target", "=", "self", ".", "data", "[", "i", "+", "1", ":", "i", "+", "1", "+", "seq_len", "]", "\n", "\n", "data_out", "=", "data", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "target_out", "=", "target", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "return", "data_out", ",", "target_out", ",", "seq_len", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMOrderedIterator.get_fixlen_iter": [[364, 367], ["range", "tokenization_transfo_xl.LMOrderedIterator.data.size", "tokenization_transfo_xl.LMOrderedIterator.get_batch"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMOrderedIterator.get_batch"], ["", "def", "get_fixlen_iter", "(", "self", ",", "start", "=", "0", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "start", ",", "self", ".", "data", ".", "size", "(", "0", ")", "-", "1", ",", "self", ".", "bptt", ")", ":", "\n", "            ", "yield", "self", ".", "get_batch", "(", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMOrderedIterator.get_varlen_iter": [[368, 379], ["min", "tokenization_transfo_xl.LMOrderedIterator.get_batch", "max", "numpy.random.random", "int", "tokenization_transfo_xl.LMOrderedIterator.data.size", "numpy.random.normal"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMOrderedIterator.get_batch"], ["", "", "def", "get_varlen_iter", "(", "self", ",", "start", "=", "0", ",", "std", "=", "5", ",", "min_len", "=", "5", ",", "max_deviation", "=", "3", ")", ":", "\n", "        ", "max_len", "=", "self", ".", "bptt", "+", "max_deviation", "*", "std", "\n", "i", "=", "start", "\n", "while", "True", ":", "\n", "            ", "bptt", "=", "self", ".", "bptt", "if", "np", ".", "random", ".", "random", "(", ")", "<", "0.95", "else", "self", ".", "bptt", "/", "2.", "\n", "bptt", "=", "min", "(", "max_len", ",", "max", "(", "min_len", ",", "int", "(", "np", ".", "random", ".", "normal", "(", "bptt", ",", "std", ")", ")", ")", ")", "\n", "data", ",", "target", ",", "seq_len", "=", "self", ".", "get_batch", "(", "i", ",", "bptt", ")", "\n", "i", "+=", "seq_len", "\n", "yield", "data", ",", "target", ",", "seq_len", "\n", "if", "i", ">=", "self", ".", "data", ".", "size", "(", "0", ")", "-", "2", ":", "\n", "                ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMOrderedIterator.__iter__": [[380, 382], ["tokenization_transfo_xl.LMOrderedIterator.get_fixlen_iter"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMOrderedIterator.get_fixlen_iter"], ["", "", "", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "get_fixlen_iter", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMShuffledIterator.__init__": [[385, 397], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "bsz", ",", "bptt", ",", "device", "=", "'cpu'", ",", "ext_len", "=", "None", ",", "shuffle", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n            data -- list[LongTensor] -- there is no order among the LongTensors\n        \"\"\"", "\n", "self", ".", "data", "=", "data", "\n", "\n", "self", ".", "bsz", "=", "bsz", "\n", "self", ".", "bptt", "=", "bptt", "\n", "self", ".", "ext_len", "=", "ext_len", "if", "ext_len", "is", "not", "None", "else", "0", "\n", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMShuffledIterator.get_sent_stream": [[398, 406], ["numpy.random.permutation", "numpy.array", "len", "range", "len"], "methods", ["None"], ["", "def", "get_sent_stream", "(", "self", ")", ":", "\n", "# index iterator", "\n", "        ", "epoch_indices", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "self", ".", "data", ")", ")", "if", "self", ".", "shuffle", "else", "np", ".", "array", "(", "range", "(", "len", "(", "self", ".", "data", ")", ")", ")", "\n", "\n", "# sentence iterator", "\n", "for", "idx", "in", "epoch_indices", ":", "\n", "            ", "yield", "self", ".", "data", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMShuffledIterator.stream_iterator": [[407, 455], ["torch.LongTensor", "torch.LongTensor", "data[].fill_", "torch.LongTensor.fill_", "range", "torch.LongTensor.transpose().contiguous().to", "torch.LongTensor.transpose().contiguous().to", "min", "torch.LongTensor.resize_", "torch.LongTensor.size", "torch.LongTensor.size", "torch.LongTensor.transpose().contiguous", "torch.LongTensor.transpose().contiguous", "min", "next", "torch.LongTensor.transpose", "torch.LongTensor.transpose", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "", "def", "stream_iterator", "(", "self", ",", "sent_stream", ")", ":", "\n", "# streams for each data in the batch", "\n", "        ", "streams", "=", "[", "None", "]", "*", "self", ".", "bsz", "\n", "\n", "data", "=", "torch", ".", "LongTensor", "(", "self", ".", "bptt", ",", "self", ".", "bsz", ")", "\n", "target", "=", "torch", ".", "LongTensor", "(", "self", ".", "bptt", ",", "self", ".", "bsz", ")", "\n", "\n", "n_retain", "=", "0", "\n", "\n", "while", "True", ":", "\n", "# data   : [n_retain+bptt x bsz]", "\n", "# target : [bptt x bsz]", "\n", "            ", "data", "[", "n_retain", ":", "]", ".", "fill_", "(", "-", "1", ")", "\n", "target", ".", "fill_", "(", "-", "1", ")", "\n", "\n", "valid_batch", "=", "True", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "bsz", ")", ":", "\n", "                ", "n_filled", "=", "0", "\n", "try", ":", "\n", "                    ", "while", "n_filled", "<", "self", ".", "bptt", ":", "\n", "                        ", "if", "streams", "[", "i", "]", "is", "None", "or", "len", "(", "streams", "[", "i", "]", ")", "<=", "1", ":", "\n", "                            ", "streams", "[", "i", "]", "=", "next", "(", "sent_stream", ")", "\n", "# number of new tokens to fill in", "\n", "", "n_new", "=", "min", "(", "len", "(", "streams", "[", "i", "]", ")", "-", "1", ",", "self", ".", "bptt", "-", "n_filled", ")", "\n", "# first n_retain tokens are retained from last batch", "\n", "data", "[", "n_retain", "+", "n_filled", ":", "n_retain", "+", "n_filled", "+", "n_new", ",", "i", "]", "=", "streams", "[", "i", "]", "[", ":", "n_new", "]", "\n", "target", "[", "n_filled", ":", "n_filled", "+", "n_new", ",", "i", "]", "=", "streams", "[", "i", "]", "[", "1", ":", "n_new", "+", "1", "]", "\n", "streams", "[", "i", "]", "=", "streams", "[", "i", "]", "[", "n_new", ":", "]", "\n", "n_filled", "+=", "n_new", "\n", "", "", "except", "StopIteration", ":", "\n", "                    ", "valid_batch", "=", "False", "\n", "break", "\n", "\n", "", "", "if", "not", "valid_batch", ":", "\n", "                ", "return", "\n", "\n", "", "data_out", "=", "data", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "target_out", "=", "target", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "yield", "data_out", ",", "target_out", ",", "self", ".", "bptt", "\n", "\n", "n_retain", "=", "min", "(", "data", ".", "size", "(", "0", ")", ",", "self", ".", "ext_len", ")", "\n", "if", "n_retain", ">", "0", ":", "\n", "                ", "data", "[", ":", "n_retain", "]", "=", "data", "[", "-", "n_retain", ":", "]", "\n", "", "data", ".", "resize_", "(", "n_retain", "+", "self", ".", "bptt", ",", "data", ".", "size", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMShuffledIterator.__iter__": [[456, 462], ["tokenization_transfo_xl.LMShuffledIterator.get_sent_stream", "tokenization_transfo_xl.LMShuffledIterator.stream_iterator"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMMultiFileIterator.get_sent_stream", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMShuffledIterator.stream_iterator"], ["", "", "def", "__iter__", "(", "self", ")", ":", "\n", "# sent_stream is an iterator", "\n", "        ", "sent_stream", "=", "self", ".", "get_sent_stream", "(", ")", "\n", "\n", "for", "batch", "in", "self", ".", "stream_iterator", "(", "sent_stream", ")", ":", "\n", "            ", "yield", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMMultiFileIterator.__init__": [[465, 477], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "paths", ",", "vocab", ",", "bsz", ",", "bptt", ",", "device", "=", "'cpu'", ",", "ext_len", "=", "None", ",", "\n", "shuffle", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "paths", "=", "paths", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n", "self", ".", "bsz", "=", "bsz", "\n", "self", ".", "bptt", "=", "bptt", "\n", "self", ".", "ext_len", "=", "ext_len", "if", "ext_len", "is", "not", "None", "else", "0", "\n", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMMultiFileIterator.get_sent_stream": [[478, 485], ["tokenization_transfo_xl.LMMultiFileIterator.vocab.encode_file", "iter", "numpy.random.shuffle"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.encode_file"], ["", "def", "get_sent_stream", "(", "self", ",", "path", ")", ":", "\n", "        ", "sents", "=", "self", ".", "vocab", ".", "encode_file", "(", "path", ",", "add_double_eos", "=", "True", ")", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "np", ".", "random", ".", "shuffle", "(", "sents", ")", "\n", "", "sent_stream", "=", "iter", "(", "sents", ")", "\n", "\n", "return", "sent_stream", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMMultiFileIterator.__iter__": [[486, 495], ["numpy.random.shuffle", "tokenization_transfo_xl.LMMultiFileIterator.get_sent_stream", "tokenization_transfo_xl.LMMultiFileIterator.stream_iterator"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMMultiFileIterator.get_sent_stream", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.LMShuffledIterator.stream_iterator"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "shuffle", ":", "\n", "            ", "np", ".", "random", ".", "shuffle", "(", "self", ".", "paths", ")", "\n", "\n", "", "for", "path", "in", "self", ".", "paths", ":", "\n", "# sent_stream is an iterator", "\n", "            ", "sent_stream", "=", "self", ".", "get_sent_stream", "(", "path", ")", "\n", "for", "batch", "in", "self", ".", "stream_iterator", "(", "sent_stream", ")", ":", "\n", "                ", "yield", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLCorpus.from_pretrained": [[498, 540], ["tokenization_transfo_xl.TransfoXLTokenizer.from_pretrained", "cls", "torch.load", "torch.load.items", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "logger.error", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["    ", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a pre-processed corpus.\n        \"\"\"", "\n", "vocab", "=", "TransfoXLTokenizer", ".", "from_pretrained", "(", "pretrained_model_name_or_path", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_CORPUS_ARCHIVE_MAP", ":", "\n", "            ", "corpus_file", "=", "PRETRAINED_CORPUS_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "corpus_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "CORPUS_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_corpus_file", "=", "cached_path", "(", "corpus_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Corpus '{}' was not found in corpus list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find files {} \"", "\n", "\"at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", "corpus_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_corpus_file", "==", "corpus_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading corpus file {}\"", ".", "format", "(", "corpus_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading corpus file {} from cache at {}\"", ".", "format", "(", "\n", "corpus_file", ",", "resolved_corpus_file", ")", ")", "\n", "\n", "# Instantiate tokenizer.", "\n", "", "corpus", "=", "cls", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "corpus_dict", "=", "torch", ".", "load", "(", "resolved_corpus_file", ")", "\n", "for", "key", ",", "value", "in", "corpus_dict", ".", "items", "(", ")", ":", "\n", "            ", "corpus", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "corpus", ".", "vocab", "=", "vocab", "\n", "if", "corpus", ".", "train", "is", "not", "None", ":", "\n", "            ", "corpus", ".", "train", "=", "torch", ".", "tensor", "(", "corpus", ".", "train", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "if", "corpus", ".", "valid", "is", "not", "None", ":", "\n", "            ", "corpus", ".", "valid", "=", "torch", ".", "tensor", "(", "corpus", ".", "valid", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "if", "corpus", ".", "test", "is", "not", "None", ":", "\n", "            ", "corpus", ".", "test", "=", "torch", ".", "tensor", "(", "corpus", ".", "test", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "return", "corpus", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLCorpus.__init__": [[541, 547], ["tokenization_transfo_xl.TransfoXLTokenizer"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "TransfoXLTokenizer", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "dataset", "=", "None", "\n", "self", ".", "train", "=", "None", "\n", "self", ".", "valid", "=", "None", "\n", "self", ".", "test", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLCorpus.build_corpus": [[548, 586], ["tokenization_transfo_xl.TransfoXLCorpus.vocab.build_vocab", "tokenization_transfo_xl.TransfoXLCorpus.vocab.count_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.count_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.count_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "os.path.join", "os.path.join", "os.path.join", "tokenization_transfo_xl.TransfoXLCorpus.vocab.count_file", "os.path.join", "os.path.join", "os.path.join", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "os.path.join", "os.path.join", "glob.glob", "os.path.join", "os.path.join", "os.path.join", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "tokenization_transfo_xl.TransfoXLCorpus.vocab.encode_file", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.build_vocab", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.count_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.count_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.count_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.count_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.encode_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLTokenizer.encode_file"], ["", "def", "build_corpus", "(", "self", ",", "path", ",", "dataset", ")", ":", "\n", "        ", "self", ".", "dataset", "=", "dataset", "\n", "\n", "if", "self", ".", "dataset", "in", "[", "'ptb'", ",", "'wt2'", ",", "'enwik8'", ",", "'text8'", "]", ":", "\n", "            ", "self", ".", "vocab", ".", "count_file", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'train.txt'", ")", ")", "\n", "self", ".", "vocab", ".", "count_file", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'valid.txt'", ")", ")", "\n", "self", ".", "vocab", ".", "count_file", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'test.txt'", ")", ")", "\n", "", "elif", "self", ".", "dataset", "==", "'wt103'", ":", "\n", "            ", "self", ".", "vocab", ".", "count_file", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'train.txt'", ")", ")", "\n", "", "elif", "self", ".", "dataset", "==", "'lm1b'", ":", "\n", "            ", "train_path_pattern", "=", "os", ".", "path", ".", "join", "(", "\n", "path", ",", "'1-billion-word-language-modeling-benchmark-r13output'", ",", "\n", "'training-monolingual.tokenized.shuffled'", ",", "'news.en-*'", ")", "\n", "train_paths", "=", "glob", ".", "glob", "(", "train_path_pattern", ")", "\n", "# the vocab will load from file when build_vocab() is called", "\n", "\n", "", "self", ".", "vocab", ".", "build_vocab", "(", ")", "\n", "\n", "if", "self", ".", "dataset", "in", "[", "'ptb'", ",", "'wt2'", ",", "'wt103'", "]", ":", "\n", "            ", "self", ".", "train", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "'train.txt'", ")", ",", "ordered", "=", "True", ")", "\n", "self", ".", "valid", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "'valid.txt'", ")", ",", "ordered", "=", "True", ")", "\n", "self", ".", "test", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "'test.txt'", ")", ",", "ordered", "=", "True", ")", "\n", "", "elif", "self", ".", "dataset", "in", "[", "'enwik8'", ",", "'text8'", "]", ":", "\n", "            ", "self", ".", "train", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "'train.txt'", ")", ",", "ordered", "=", "True", ",", "add_eos", "=", "False", ")", "\n", "self", ".", "valid", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "'valid.txt'", ")", ",", "ordered", "=", "True", ",", "add_eos", "=", "False", ")", "\n", "self", ".", "test", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "'test.txt'", ")", ",", "ordered", "=", "True", ",", "add_eos", "=", "False", ")", "\n", "", "elif", "self", ".", "dataset", "==", "'lm1b'", ":", "\n", "            ", "self", ".", "train", "=", "train_paths", "\n", "self", ".", "valid", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "'valid.txt'", ")", ",", "ordered", "=", "False", ",", "add_double_eos", "=", "True", ")", "\n", "self", ".", "test", "=", "self", ".", "vocab", ".", "encode_file", "(", "\n", "os", ".", "path", ".", "join", "(", "path", ",", "'test.txt'", ")", ",", "ordered", "=", "False", ",", "add_double_eos", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.TransfoXLCorpus.get_iterator": [[587, 602], ["tokenization_transfo_xl.LMOrderedIterator", "tokenization_transfo_xl.LMMultiFileIterator", "tokenization_transfo_xl.LMOrderedIterator", "tokenization_transfo_xl.LMShuffledIterator"], "methods", ["None"], ["", "", "def", "get_iterator", "(", "self", ",", "split", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "split", "==", "'train'", ":", "\n", "            ", "if", "self", ".", "dataset", "in", "[", "'ptb'", ",", "'wt2'", ",", "'wt103'", ",", "'enwik8'", ",", "'text8'", "]", ":", "\n", "                ", "data_iter", "=", "LMOrderedIterator", "(", "self", ".", "train", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "elif", "self", ".", "dataset", "==", "'lm1b'", ":", "\n", "                ", "kwargs", "[", "'shuffle'", "]", "=", "True", "\n", "data_iter", "=", "LMMultiFileIterator", "(", "self", ".", "train", ",", "self", ".", "vocab", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "", "elif", "split", "in", "[", "'valid'", ",", "'test'", "]", ":", "\n", "            ", "data", "=", "self", ".", "valid", "if", "split", "==", "'valid'", "else", "self", ".", "test", "\n", "if", "self", ".", "dataset", "in", "[", "'ptb'", ",", "'wt2'", ",", "'wt103'", ",", "'enwik8'", ",", "'text8'", "]", ":", "\n", "                ", "data_iter", "=", "LMOrderedIterator", "(", "data", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "elif", "self", ".", "dataset", "==", "'lm1b'", ":", "\n", "                ", "data_iter", "=", "LMShuffledIterator", "(", "data", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "", "return", "data_iter", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl.get_lm_corpus": [[604, 634], ["os.path.join", "os.path.join", "os.path.exists", "print", "torch.load", "os.path.exists", "print", "print", "tokenization_transfo_xl.TransfoXLCorpus", "torch.save", "io.open", "pickle.load", "os.path.join"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["", "", "def", "get_lm_corpus", "(", "datadir", ",", "dataset", ")", ":", "\n", "    ", "fn", "=", "os", ".", "path", ".", "join", "(", "datadir", ",", "'cache.pt'", ")", "\n", "fn_pickle", "=", "os", ".", "path", ".", "join", "(", "datadir", ",", "'cache.pkl'", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "fn", ")", ":", "\n", "        ", "print", "(", "'Loading cached dataset...'", ")", "\n", "corpus", "=", "torch", ".", "load", "(", "fn_pickle", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "fn", ")", ":", "\n", "        ", "print", "(", "'Loading cached dataset from pickle...'", ")", "\n", "with", "open", "(", "fn", ",", "\"rb\"", ")", "as", "fp", ":", "\n", "            ", "corpus", "=", "pickle", ".", "load", "(", "fp", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "'Producing dataset {}...'", ".", "format", "(", "dataset", ")", ")", "\n", "kwargs", "=", "{", "}", "\n", "if", "dataset", "in", "[", "'wt103'", ",", "'wt2'", "]", ":", "\n", "            ", "kwargs", "[", "'special'", "]", "=", "[", "'<eos>'", "]", "\n", "kwargs", "[", "'lower_case'", "]", "=", "False", "\n", "", "elif", "dataset", "==", "'ptb'", ":", "\n", "            ", "kwargs", "[", "'special'", "]", "=", "[", "'<eos>'", "]", "\n", "kwargs", "[", "'lower_case'", "]", "=", "True", "\n", "", "elif", "dataset", "==", "'lm1b'", ":", "\n", "            ", "kwargs", "[", "'special'", "]", "=", "[", "]", "\n", "kwargs", "[", "'lower_case'", "]", "=", "False", "\n", "kwargs", "[", "'vocab_file'", "]", "=", "os", ".", "path", ".", "join", "(", "datadir", ",", "'1b_word_vocab.txt'", ")", "\n", "", "elif", "dataset", "in", "[", "'enwik8'", ",", "'text8'", "]", ":", "\n", "            ", "pass", "\n", "\n", "", "corpus", "=", "TransfoXLCorpus", "(", "datadir", ",", "dataset", ",", "**", "kwargs", ")", "\n", "torch", ".", "save", "(", "corpus", ",", "fn", ")", "\n", "\n", "", "return", "corpus", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl._is_whitespace": [[635, 645], ["unicodedata.category"], "function", ["None"], ["", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl._is_control": [[647, 657], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_transfo_xl._is_punctuation": [[659, 673], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertConfig.__init__": [[136, 192], ["isinstance", "json.loads.items", "isinstance", "isinstance", "io.open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this rpbert might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "\n", "and", "isinstance", "(", "vocab_size_or_config_json_file", ",", "unicode", ")", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained rpbert config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertConfig.from_dict": [[194, 201], ["modeling.BertConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertConfig.from_json_file": [[202, 208], ["cls.from_dict", "io.open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertConfig.__repr__": [[209, 211], ["str", "modeling.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertConfig.to_dict": [[212, 216], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertConfig.to_json_string": [[217, 220], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertEmbeddings.__init__": [[243, 253], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow rpbert variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertEmbeddings.forward": [[254, 269], ["input_ids.size", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "modeling.BertEmbeddings.word_embeddings", "modeling.BertEmbeddings.position_embeddings", "modeling.BertEmbeddings.token_type_embeddings", "modeling.BertEmbeddings.LayerNorm", "modeling.BertEmbeddings.dropout", "torch.zeros_like", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertSelfAttention.__init__": [[272, 287], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores": [[288, 292], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertSelfAttention.forward": [[293, 329], ["modeling.BertSelfAttention.query", "modeling.BertSelfAttention.key", "modeling.BertSelfAttention.value", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "torch.matmul", "modeling.BertSelfAttention.dropout", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling.BertSelfAttention.transpose", "math.sqrt", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Label.value", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "output_attention_probs", "=", "False", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# extended_attention_mask = attention_mask > 0", "\n", "# extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)", "\n", "# extended_attention_mask = (1.0 - extended_attention_mask) * -1e9", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "# global attn_mask_visual", "\n", "# attn_mask_visual = attention_probs", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "if", "output_attention_probs", ":", "\n", "            ", "return", "context_layer", ",", "attention_probs", "\n", "", "else", ":", "\n", "            ", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertSelfOutput.__init__": [[332, 337], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertSelfOutput.forward": [[338, 343], ["modeling.BertSelfOutput.dense", "modeling.BertSelfOutput.dropout", "modeling.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertAttention.__init__": [[346, 350], ["torch.nn.Module.__init__", "modeling.BertSelfAttention", "modeling.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertAttention.forward": [[351, 359], ["modeling.BertAttention.self", "modeling.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ",", "output_attention_probs", "=", "False", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "attention_mask", ",", "output_attention_probs", "=", "output_attention_probs", ")", "\n", "if", "output_attention_probs", ":", "\n", "            ", "self_output", ",", "attention_probs", "=", "self_output", "\n", "", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "if", "output_attention_probs", ":", "\n", "            ", "return", "attention_output", ",", "attention_probs", "\n", "", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertIntermediate.__init__": [[362, 369], ["torch.nn.Module.__init__", "torch.nn.Linear", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "config", ".", "hidden_act", ",", "unicode", ")", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertIntermediate.forward": [[370, 374], ["modeling.BertIntermediate.dense", "modeling.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertOutput.__init__": [[377, 382], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertOutput.forward": [[383, 388], ["modeling.BertOutput.dense", "modeling.BertOutput.dropout", "modeling.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertLayer.__init__": [[391, 396], ["torch.nn.Module.__init__", "modeling.BertAttention", "modeling.BertIntermediate", "modeling.BertOutput"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertLayer.forward": [[397, 407], ["modeling.BertLayer.attention", "modeling.BertLayer.intermediate", "modeling.BertLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "output_attention_probs", "=", "False", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ",", "output_attention_probs", "=", "output_attention_probs", ")", "\n", "if", "output_attention_probs", ":", "\n", "            ", "attention_output", ",", "attention_probs", "=", "attention_output", "\n", "", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "if", "output_attention_probs", ":", "\n", "            ", "return", "layer_output", ",", "attention_probs", "\n", "", "else", ":", "\n", "            ", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertEncoder.__init__": [[410, 414], ["torch.nn.Module.__init__", "modeling.BertLayer", "torch.nn.ModuleList", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer", "=", "BertLayer", "(", "config", ")", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "layer", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertEncoder.forward": [[415, 431], ["layer_module", "all_encoder_layers.append", "all_attention_probs.append", "all_encoder_layers.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "True", ",", "output_attention_probs", "=", "False", ")", ":", "\n", "        ", "all_encoder_layers", "=", "[", "]", "\n", "all_attention_probs", "=", "[", "]", "\n", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "            ", "hidden_states", "=", "layer_module", "(", "hidden_states", ",", "attention_mask", ",", "output_attention_probs", "=", "output_attention_probs", ")", "\n", "if", "output_attention_probs", ":", "\n", "                ", "hidden_states", ",", "attention_probs", "=", "hidden_states", "\n", "all_attention_probs", ".", "append", "(", "attention_probs", ")", "\n", "", "if", "output_all_encoded_layers", ":", "\n", "                ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "if", "output_attention_probs", ":", "\n", "            ", "return", "all_encoder_layers", ",", "all_attention_probs", "\n", "", "else", ":", "\n", "            ", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertPooler.__init__": [[434, 438], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertPooler.forward": [[439, 446], ["modeling.BertPooler.dense", "modeling.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the rpbert by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertPredictionHeadTransform.__init__": [[449, 457], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPredictionHeadTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "config", ".", "hidden_act", ",", "unicode", ")", ")", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "config", ".", "hidden_act", "\n", "", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertPredictionHeadTransform.forward": [[458, 463], ["modeling.BertPredictionHeadTransform.dense", "modeling.BertPredictionHeadTransform.transform_act_fn", "modeling.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertLMPredictionHead.__init__": [[466, 477], ["torch.nn.Module.__init__", "modeling.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Parameter", "bert_model_embedding_weights.size", "bert_model_embedding_weights.size", "torch.zeros", "bert_model_embedding_weights.size"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertLMPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "bert_model_embedding_weights", ".", "size", "(", "1", ")", ",", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "decoder", ".", "weight", "=", "bert_model_embedding_weights", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertLMPredictionHead.forward": [[478, 482], ["modeling.BertLMPredictionHead.transform", "modeling.BertLMPredictionHead.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "transform", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "+", "self", ".", "bias", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertOnlyMLMHead.__init__": [[485, 488], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertOnlyMLMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertOnlyMLMHead.forward": [[489, 492], ["modeling.BertOnlyMLMHead.predictions"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertOnlyNSPHead.__init__": [[495, 498], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOnlyNSPHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertOnlyNSPHead.forward": [[499, 502], ["modeling.BertOnlyNSPHead.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pooled_output", ")", ":", "\n", "        ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertPreTrainingHeads.__init__": [[505, 509], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingHeads", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertPreTrainingHeads.forward": [[510, 514], ["modeling.BertPreTrainingHeads.predictions", "modeling.BertPreTrainingHeads.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ",", "pooled_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertPreTrainedModel.__init__": [[520, 530], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "BertPreTrainedModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "BertConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"", "\n", "\"To create a rpbert from a Google pretrained rpbert use \"", "\n", "\"`rpbert = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertPreTrainedModel.init_bert_weights": [[531, 543], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_bert_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertPreTrainedModel.from_pretrained": [[544, 666], ["os.path.join", "modeling.BertConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "zip", "getattr", "torch.load.copy", "modeling.BertPreTrainedModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.from_json_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "\n", "from_tf", "=", "False", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a BertPreTrainedModel from a pre-trained rpbert file or a pytorch state dict.\n        Download and cache the pre-trained rpbert file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained rpbert to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained rpbert archive containing:\n                    . `bert_config.json` a configuration file for the rpbert\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n                - a path or url to a pretrained rpbert archive containing:\n                    . `bert_config.json` a configuration file for the rpbert\n                    . `rpbert.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "pretrained_model_name_or_path", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in rpbert name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "tempdir", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", "or", "from_tf", ":", "\n", "            ", "serialization_dir", "=", "resolved_archive_file", "\n", "", "else", ":", "\n", "# Extract archive to temp dir", "\n", "            ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "logger", ".", "info", "(", "\"extracting archive file {} to temp dir {}\"", ".", "format", "(", "\n", "resolved_archive_file", ",", "tempdir", ")", ")", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "'r:gz'", ")", "as", "archive", ":", "\n", "                ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "serialization_dir", "=", "tempdir", "\n", "# Load config", "\n", "", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", "\n", "config", "=", "BertConfig", ".", "from_json_file", "(", "config_file", ")", "\n", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "# Instantiate rpbert.", "\n", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", "and", "not", "from_tf", ":", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "WEIGHTS_NAME", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "weights_path", ",", "map_location", "=", "'cpu'", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "None", ")", "\n", "", "if", "tempdir", ":", "\n", "# Clean up temp dir", "\n", "            ", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "", "if", "from_tf", ":", "\n", "# Directly load from a TensorFlow checkpoint", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "TF_WEIGHTS_NAME", ")", "\n", "return", "load_tf_weights_in_bert", "(", "model", ",", "weights_path", ")", "\n", "# Load from a PyTorch state_dict", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "'gamma'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'gamma'", ",", "'weight'", ")", "\n", "", "if", "'beta'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'beta'", ",", "'bias'", ")", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "", "", "", "start_prefix", "=", "''", "\n", "if", "not", "hasattr", "(", "model", ",", "'bert'", ")", "and", "any", "(", "s", ".", "startswith", "(", "'bert.'", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "start_prefix", "=", "'bert.'", "\n", "", "load", "(", "model", ",", "prefix", "=", "start_prefix", ")", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights of {} not initialized from pretrained rpbert: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights from pretrained rpbert not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Error(s) in loading state_dict for {}:\\n\\t{}'", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "\"\\n\\t\"", ".", "join", "(", "error_msgs", ")", ")", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertModel.__init__": [[712, 718], ["modeling.BertPreTrainedModel.__init__", "modeling.BertEmbeddings", "modeling.BertEncoder", "modeling.BertPooler", "modeling.BertModel.apply"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertModel.forward": [[719, 749], ["torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "modeling.BertModel.embeddings", "modeling.BertModel.encoder", "modeling.BertModel.pooler", "torch.ones_like", "torch.zeros_like", "torch.ones_like.unsqueeze", "next", "modeling.BertModel.parameters"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "output_all_encoded_layers", "=", "True", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "token_type_ids", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForPreTraining.__init__": [[801, 806], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "modeling.BertPreTrainingHeads", "modeling.BertForPreTraining.apply"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForPreTraining", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForPreTraining.forward": [[807, 820], ["modeling.BertForPreTraining.bert", "modeling.BertForPreTraining.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores.view", "masked_lm_labels.view", "seq_relationship_score.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "next_sentence_label", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "sequence_output", ",", "pooled_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", "and", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "total_loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForMaskedLM.__init__": [[864, 869], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "modeling.BertOnlyMLMHead", "modeling.BertForMaskedLM.apply"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForMaskedLM", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyMLMHead", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForMaskedLM.forward": [[870, 881], ["modeling.BertForMaskedLM.bert", "modeling.BertForMaskedLM.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "modeling.BertForMaskedLM.view", "masked_lm_labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "prediction_scores", "=", "self", ".", "cls", "(", "sequence_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "masked_lm_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForNextSentencePrediction.__init__": [[926, 931], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "modeling.BertOnlyNSPHead", "modeling.BertForNextSentencePrediction.apply"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForNextSentencePrediction", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyNSPHead", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForNextSentencePrediction.forward": [[932, 943], ["modeling.BertForNextSentencePrediction.bert", "modeling.BertForNextSentencePrediction.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "modeling.BertForNextSentencePrediction.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "next_sentence_label", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "seq_relationship_score", "=", "self", ".", "cls", "(", "pooled_output", ")", "\n", "\n", "if", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "next_sentence_loss", "\n", "", "else", ":", "\n", "            ", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForSequenceClassification.__init__": [[990, 997], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForSequenceClassification.apply"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForSequenceClassification.forward": [[998, 1009], ["modeling.BertForSequenceClassification.bert", "modeling.BertForSequenceClassification.dropout", "modeling.BertForSequenceClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "modeling.BertForSequenceClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForMultipleChoice.__init__": [[1055, 1062], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForMultipleChoice.apply"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_choices", ")", ":", "\n", "        ", "super", "(", "BertForMultipleChoice", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_choices", "=", "num_choices", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForMultipleChoice.forward": [[1063, 1078], ["input_ids.view", "token_type_ids.view", "attention_mask.view", "modeling.BertForMultipleChoice.bert", "modeling.BertForMultipleChoice.dropout", "modeling.BertForMultipleChoice.classifier", "modeling.BertForMultipleChoice.view", "input_ids.size", "token_type_ids.size", "attention_mask.size", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss."], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "\n", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "flat_input_ids", ",", "flat_token_type_ids", ",", "flat_attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_choices", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "reshaped_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForTokenClassification.__init__": [[1125, 1132], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForTokenClassification.apply"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForTokenClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForTokenClassification.forward": [[1133, 1151], ["modeling.BertForTokenClassification.bert", "modeling.BertForTokenClassification.dropout", "modeling.BertForTokenClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "modeling.BertForTokenClassification.view", "labels.view", "modeling.BertForTokenClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForQuestionAnswering.__init__": [[1200, 1207], ["modeling.BertPreTrainedModel.__init__", "modeling.BertModel", "torch.nn.Linear", "modeling.BertForQuestionAnswering.apply"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "# TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version", "\n", "# self.dropout = nn.Dropout(config.hidden_dropout_prob)", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.BertForQuestionAnswering.forward": [[1208, 1233], ["modeling.BertForQuestionAnswering.bert", "modeling.BertForQuestionAnswering.qa_outputs", "modeling.BertForQuestionAnswering.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "start_logits.squeeze.squeeze.size", "start_positions.squeeze.squeeze.clamp_", "end_positions.squeeze.squeeze.clamp_", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "len", "start_positions.squeeze.squeeze.squeeze", "len", "end_positions.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.size", "end_positions.squeeze.squeeze.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension", "\n", "            ", "if", "len", "(", "start_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len", "(", "end_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our rpbert inputs, we ignore these terms", "\n", "", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.load_tf_weights_in_bert": [[56, 115], ["os.path.abspath", "print", "tf.train.list_variables", "zip", "print", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "any", "print", "torch.from_numpy", "print", "print", "re.fullmatch", "getattr", "re.split", "getattr", "len", "int", "np.transpose", "getattr", "getattr", "getattr"], "function", ["None"], ["def", "load_tf_weights_in_bert", "(", "model", ",", "tf_checkpoint_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch rpbert\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "re", "\n", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "print", "(", "\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "tf_checkpoint_path", ")", "\n", "print", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "\n", "# Load weights from TF rpbert", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "print", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ")", "\n", "\n", "", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", ".", "split", "(", "'/'", ")", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained rpbert", "\n", "if", "any", "(", "n", "in", "[", "\"adam_v\"", ",", "\"adam_m\"", "]", "for", "n", "in", "name", ")", ":", "\n", "            ", "print", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r'[A-Za-z]+_\\d+'", ",", "m_name", ")", ":", "\n", "                ", "l", "=", "re", ".", "split", "(", "r'_(\\d+)'", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "l", "=", "[", "m_name", "]", "\n", "", "if", "l", "[", "0", "]", "==", "'kernel'", "or", "l", "[", "0", "]", "==", "'gamma'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_bias'", "or", "l", "[", "0", "]", "==", "'beta'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'bias'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_weights'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "else", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "\n", "", "if", "len", "(", "l", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "l", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "if", "m_name", "[", "-", "11", ":", "]", "==", "'_embeddings'", ":", "\n", "            ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "m_name", "==", "'kernel'", ":", "\n", "            ", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "print", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.gelu": [[117, 124], ["torch.erf", "math.sqrt"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.swish": [[126, 128], ["torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2Config.__init__": [[108, 151], ["isinstance", "json.loads.items", "isinstance", "isinstance", "io.open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_size_or_config_json_file", "=", "50257", ",", "\n", "n_positions", "=", "1024", ",", "\n", "n_ctx", "=", "1024", ",", "\n", "n_embd", "=", "768", ",", "\n", "n_layer", "=", "12", ",", "\n", "n_head", "=", "12", ",", "\n", "layer_norm_epsilon", "=", "1e-5", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Constructs GPT2Config.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `GPT2Model` or a configuration json file.\n            n_positions: Number of positional embeddings.\n            n_ctx: Size of the causal mask (usually same as n_positions).\n            n_embd: Dimensionality of the embeddings and hidden states.\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            layer_norm_epsilon: epsilon to use in the layer norm layers\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "\n", "and", "isinstance", "(", "vocab_size_or_config_json_file", ",", "unicode", ")", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "n_ctx", "=", "n_ctx", "\n", "self", ".", "n_positions", "=", "n_positions", "\n", "self", ".", "n_embd", "=", "n_embd", "\n", "self", ".", "n_layer", "=", "n_layer", "\n", "self", ".", "n_head", "=", "n_head", "\n", "self", ".", "layer_norm_epsilon", "=", "layer_norm_epsilon", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained rpbert config file (str)\"", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2Config.from_dict": [[154, 161], ["modeling_gpt2.GPT2Config", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `GPT2Config` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "GPT2Config", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2Config.from_json_file": [[162, 168], ["cls.from_dict", "io.open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `GPT2Config` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2Config.__repr__": [[169, 171], ["str", "modeling_gpt2.GPT2Config.to_json_string"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2Config.to_dict": [[172, 176], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2Config.to_json_string": [[177, 180], ["json.dumps", "modeling_gpt2.GPT2Config.to_dict"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.Conv1D.__init__": [[183, 190], ["torch.Module.__init__", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.init.normal_", "torch.init.normal_", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nf", ",", "nx", ")", ":", "\n", "        ", "super", "(", "Conv1D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "nf", "=", "nf", "\n", "w", "=", "torch", ".", "empty", "(", "nx", ",", "nf", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "w", ",", "std", "=", "0.02", ")", "\n", "self", ".", "weight", "=", "Parameter", "(", "w", ")", "\n", "self", ".", "bias", "=", "Parameter", "(", "torch", ".", "zeros", "(", "nf", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.Conv1D.forward": [[191, 196], ["torch.addmm", "torch.addmm", "torch.addmm", "torch.addmm", "x.view.view.view", "x.view.view.view", "x.view.view.size", "x.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "size_out", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "nf", ",", ")", "\n", "x", "=", "torch", ".", "addmm", "(", "self", ".", "bias", ",", "x", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "-", "1", ")", ")", ",", "self", ".", "weight", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "size_out", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.Attention.__init__": [[199, 210], ["torch.Module.__init__", "modeling_gpt2.Attention.register_buffer", "modeling_gpt2.Conv1D", "modeling_gpt2.Conv1D", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nx", ",", "n_ctx", ",", "config", ",", "scale", "=", "False", ")", ":", "\n", "        ", "super", "(", "Attention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "n_state", "=", "nx", "# in Attention: n_state=768 (nx=n_embd)", "\n", "# [switch nx => n_state from Block to Attention to keep identical to TF implem]", "\n", "assert", "n_state", "%", "config", ".", "n_head", "==", "0", "\n", "self", ".", "register_buffer", "(", "\"bias\"", ",", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "n_ctx", ",", "n_ctx", ")", ")", ".", "view", "(", "1", ",", "1", ",", "n_ctx", ",", "n_ctx", ")", ")", "\n", "self", ".", "n_head", "=", "config", ".", "n_head", "\n", "self", ".", "split_size", "=", "n_state", "\n", "self", ".", "scale", "=", "scale", "\n", "self", ".", "c_attn", "=", "Conv1D", "(", "n_state", "*", "3", ",", "nx", ")", "\n", "self", ".", "c_proj", "=", "Conv1D", "(", "n_state", ",", "nx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.Attention._attn": [[211, 221], ["torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.matmul.size", "torch.Softmax", "torch.Softmax", "math.sqrt", "v.size"], "methods", ["None"], ["", "def", "_attn", "(", "self", ",", "q", ",", "k", ",", "v", ")", ":", "\n", "        ", "w", "=", "torch", ".", "matmul", "(", "q", ",", "k", ")", "\n", "if", "self", ".", "scale", ":", "\n", "            ", "w", "=", "w", "/", "math", ".", "sqrt", "(", "v", ".", "size", "(", "-", "1", ")", ")", "\n", "", "nd", ",", "ns", "=", "w", ".", "size", "(", "-", "2", ")", ",", "w", ".", "size", "(", "-", "1", ")", "\n", "b", "=", "self", ".", "bias", "[", ":", ",", ":", ",", "ns", "-", "nd", ":", "ns", ",", ":", "ns", "]", "\n", "w", "=", "w", "*", "b", "-", "1e10", "*", "(", "1", "-", "b", ")", "\n", "\n", "w", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "w", ")", "\n", "return", "torch", ".", "matmul", "(", "w", ",", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.Attention.merge_heads": [[222, 226], ["x.permute().contiguous.permute().contiguous.permute().contiguous", "x.permute().contiguous.permute().contiguous.view", "x.permute().contiguous.permute().contiguous.permute", "x.permute().contiguous.permute().contiguous.size", "x.permute().contiguous.permute().contiguous.size", "x.permute().contiguous.permute().contiguous.size"], "methods", ["None"], ["", "def", "merge_heads", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "x", ".", "size", "(", "-", "2", ")", "*", "x", ".", "size", "(", "-", "1", ")", ",", ")", "\n", "return", "x", ".", "view", "(", "*", "new_x_shape", ")", "# in Tensorflow implem: fct merge_states", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.Attention.split_heads": [[227, 234], ["x.view.view.view", "x.view.view.permute", "x.view.view.permute", "x.view.view.size", "x.view.view.size"], "methods", ["None"], ["", "def", "split_heads", "(", "self", ",", "x", ",", "k", "=", "False", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "n_head", ",", "x", ".", "size", "(", "-", "1", ")", "//", "self", ".", "n_head", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "# in Tensorflow implem: fct split_states", "\n", "if", "k", ":", "\n", "            ", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "# (batch, head, head_features, seq_length)", "\n", "", "else", ":", "\n", "            ", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "# (batch, head, seq_length, head_features)", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.Attention.forward": [[235, 250], ["modeling_gpt2.Attention.c_attn", "modeling_gpt2.Attention.split", "modeling_gpt2.Attention.split_heads", "modeling_gpt2.Attention.split_heads", "modeling_gpt2.Attention.split_heads", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "modeling_gpt2.Attention._attn", "modeling_gpt2.Attention.merge_heads", "modeling_gpt2.Attention.c_proj", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "layer_past[].transpose", "torch.cat.transpose", "torch.cat.transpose"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention.split_heads", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention.split_heads", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention.split_heads", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention._attn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention.merge_heads"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "layer_past", "=", "None", ")", ":", "\n", "        ", "x", "=", "self", ".", "c_attn", "(", "x", ")", "\n", "query", ",", "key", ",", "value", "=", "x", ".", "split", "(", "self", ".", "split_size", ",", "dim", "=", "2", ")", "\n", "query", "=", "self", ".", "split_heads", "(", "query", ")", "\n", "key", "=", "self", ".", "split_heads", "(", "key", ",", "k", "=", "True", ")", "\n", "value", "=", "self", ".", "split_heads", "(", "value", ")", "\n", "if", "layer_past", "is", "not", "None", ":", "\n", "            ", "past_key", ",", "past_value", "=", "layer_past", "[", "0", "]", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ",", "layer_past", "[", "1", "]", "# transpose back cf below", "\n", "key", "=", "torch", ".", "cat", "(", "(", "past_key", ",", "key", ")", ",", "dim", "=", "-", "1", ")", "\n", "value", "=", "torch", ".", "cat", "(", "(", "past_value", ",", "value", ")", ",", "dim", "=", "-", "2", ")", "\n", "", "present", "=", "torch", ".", "stack", "(", "(", "key", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ",", "value", ")", ")", "# transpose to have same shapes for stacking", "\n", "a", "=", "self", ".", "_attn", "(", "query", ",", "key", ",", "value", ")", "\n", "a", "=", "self", ".", "merge_heads", "(", "a", ")", "\n", "a", "=", "self", ".", "c_proj", "(", "a", ")", "\n", "return", "a", ",", "present", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.MLP.__init__": [[253, 259], ["torch.Module.__init__", "modeling_gpt2.Conv1D", "modeling_gpt2.Conv1D"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_state", ",", "config", ")", ":", "# in MLP: n_state=3072 (4 * n_embd)", "\n", "        ", "super", "(", "MLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "nx", "=", "config", ".", "n_embd", "\n", "self", ".", "c_fc", "=", "Conv1D", "(", "n_state", ",", "nx", ")", "\n", "self", ".", "c_proj", "=", "Conv1D", "(", "nx", ",", "n_state", ")", "\n", "self", ".", "act", "=", "gelu", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.MLP.forward": [[260, 264], ["modeling_gpt2.MLP.act", "modeling_gpt2.MLP.c_proj", "modeling_gpt2.MLP.c_fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "h", "=", "self", ".", "act", "(", "self", ".", "c_fc", "(", "x", ")", ")", "\n", "h2", "=", "self", ".", "c_proj", "(", "h", ")", "\n", "return", "h2", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.Block.__init__": [[267, 274], ["torch.Module.__init__", "modeling.BertLayerNorm", "modeling_gpt2.Attention", "modeling.BertLayerNorm", "modeling_gpt2.MLP"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_ctx", ",", "config", ",", "scale", "=", "False", ")", ":", "\n", "        ", "super", "(", "Block", ",", "self", ")", ".", "__init__", "(", ")", "\n", "nx", "=", "config", ".", "n_embd", "\n", "self", ".", "ln_1", "=", "LayerNorm", "(", "nx", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "nx", ",", "n_ctx", ",", "config", ",", "scale", ")", "\n", "self", ".", "ln_2", "=", "LayerNorm", "(", "nx", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "self", ".", "mlp", "=", "MLP", "(", "4", "*", "nx", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.Block.forward": [[275, 281], ["modeling_gpt2.Block.attn", "modeling_gpt2.Block.mlp", "modeling_gpt2.Block.ln_1", "modeling_gpt2.Block.ln_2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "layer_past", "=", "None", ")", ":", "\n", "        ", "a", ",", "present", "=", "self", ".", "attn", "(", "self", ".", "ln_1", "(", "x", ")", ",", "layer_past", "=", "layer_past", ")", "\n", "x", "=", "x", "+", "a", "\n", "m", "=", "self", ".", "mlp", "(", "self", ".", "ln_2", "(", "x", ")", ")", "\n", "x", "=", "x", "+", "m", "\n", "return", "x", ",", "present", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2LMHead.__init__": [[286, 290], ["torch.Module.__init__", "modeling_gpt2.GPT2LMHead.set_embeddings_weights"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTLMHead.set_embeddings_weights"], ["def", "__init__", "(", "self", ",", "model_embeddings_weights", ",", "config", ")", ":", "\n", "        ", "super", "(", "GPT2LMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "n_embd", "=", "config", ".", "n_embd", "\n", "self", ".", "set_embeddings_weights", "(", "model_embeddings_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2LMHead.set_embeddings_weights": [[291, 295], ["torch.Linear", "torch.Linear"], "methods", ["None"], ["", "def", "set_embeddings_weights", "(", "self", ",", "model_embeddings_weights", ")", ":", "\n", "        ", "embed_shape", "=", "model_embeddings_weights", ".", "shape", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "embed_shape", "[", "1", "]", ",", "embed_shape", "[", "0", "]", ",", "bias", "=", "False", ")", "\n", "self", ".", "decoder", ".", "weight", "=", "model_embeddings_weights", "# Tied weights", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2LMHead.forward": [[296, 301], ["modeling_gpt2.GPT2LMHead.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_state", ")", ":", "\n", "# Truncated Language modeling logits (we remove the last token)", "\n", "# h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)", "\n", "        ", "lm_logits", "=", "self", ".", "decoder", "(", "hidden_state", ")", "\n", "return", "lm_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2MultipleChoiceHead.__init__": [[306, 313], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "GPT2MultipleChoiceHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "n_embd", "=", "config", ".", "n_embd", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "config", ".", "n_embd", ",", "1", ")", "\n", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "linear", ".", "weight", ",", "std", "=", "0.02", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "linear", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2MultipleChoiceHead.forward": [[314, 325], ["mc_token_ids.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze().expand", "hidden_states.gather().squeeze", "modeling_gpt2.GPT2MultipleChoiceHead.linear().squeeze", "hidden_states.size", "mc_token_ids.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze", "hidden_states.gather", "modeling_gpt2.GPT2MultipleChoiceHead.linear", "mc_token_ids.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze().expand.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "mc_token_ids", ")", ":", "\n", "# Classification logits", "\n", "# hidden_state (bsz, num_choices, seq_length, hidden_size)", "\n", "# mc_token_ids (bsz, num_choices)", "\n", "        ", "mc_token_ids", "=", "mc_token_ids", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "-", "1", ",", "hidden_states", ".", "size", "(", "-", "1", ")", ")", "\n", "# (bsz, num_choices, 1, hidden_size)", "\n", "multiple_choice_h", "=", "hidden_states", ".", "gather", "(", "2", ",", "mc_token_ids", ")", ".", "squeeze", "(", "2", ")", "\n", "# (bsz, num_choices, hidden_size)", "\n", "multiple_choice_logits", "=", "self", ".", "linear", "(", "multiple_choice_h", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "# (bsz, num_choices)", "\n", "return", "multiple_choice_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2PreTrainedModel.__init__": [[332, 343], ["torch.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "GPT2PreTrainedModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "GPT2Config", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `GPT2Config`. \"", "\n", "\"To create a rpbert from a pretrained rpbert use \"", "\n", "\"`rpbert = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", "\n", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2PreTrainedModel.set_tied": [[344, 346], ["None"], "methods", ["None"], ["", "def", "set_tied", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2PreTrainedModel.init_weights": [[347, 359], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2PreTrainedModel.from_pretrained": [[360, 478], ["modeling_gpt2.GPT2Config.from_json_file", "logger.info", "cls", "torch.load.keys", "torch.load.keys", "zip", "getattr", "torch.load.copy", "torch.load.copy", "modeling_gpt2.GPT2PreTrainedModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.from_json_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "\n", "cls", ",", "pretrained_model_name_or_path", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "from_tf", "=", "False", ",", "*", "inputs", ",", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a GPT2PreTrainedModel from a pre-trained rpbert file or a pytorch state dict.\n        Download and cache the pre-trained rpbert file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained rpbert to load selected in the list of:\n                    . `openai-gpt`\n                - a path or url to a pretrained rpbert archive containing:\n                    . `gpt2_config.json` a configuration file for the rpbert\n                    . `pytorch_model.bin` a PyTorch dump of a GPT2Model instance\n                - a path or url to a pretrained rpbert archive containing:\n                    . `bert_config.json` a configuration file for the rpbert\n                    . a TensorFlow checkpoint with trained weights\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "config_file", "=", "PRETRAINED_CONFIG_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "WEIGHTS_NAME", ")", "\n", "config_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "CONFIG_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "resolved_config_file", "=", "cached_path", "(", "config_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in rpbert name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find files {} and {} \"", "\n", "\"at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\", \"", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "pretrained_model_name_or_path", ",", "\n", "archive_file", ",", "config_file", "\n", ")", "\n", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", "and", "resolved_config_file", "==", "config_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading weights file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "logger", ".", "info", "(", "\"loading configuration file {}\"", ".", "format", "(", "config_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading weights file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "logger", ".", "info", "(", "\"loading configuration file {} from cache at {}\"", ".", "format", "(", "\n", "config_file", ",", "resolved_config_file", ")", ")", "\n", "# Load config", "\n", "", "config", "=", "GPT2Config", ".", "from_json_file", "(", "resolved_config_file", ")", "\n", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "# Instantiate rpbert.", "\n", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", "and", "not", "from_tf", ":", "\n", "            ", "state_dict", "=", "torch", ".", "load", "(", "resolved_archive_file", ",", "map_location", "=", "'cpu'", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "None", ")", "\n", "", "if", "from_tf", ":", "\n", "# Directly load from a TensorFlow checkpoint (stored as NumPy array)", "\n", "            ", "return", "load_tf_weights_in_gpt2", "(", "model", ",", "resolved_archive_file", ")", "\n", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "key", ".", "endswith", "(", "\".g\"", ")", ":", "\n", "                ", "new_key", "=", "key", "[", ":", "-", "2", "]", "+", "\".weight\"", "\n", "", "elif", "key", ".", "endswith", "(", "\".b\"", ")", ":", "\n", "                ", "new_key", "=", "key", "[", ":", "-", "2", "]", "+", "\".bias\"", "\n", "", "elif", "key", ".", "endswith", "(", "\".w\"", ")", ":", "\n", "                ", "new_key", "=", "key", "[", ":", "-", "2", "]", "+", "\".weight\"", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "\"_metadata\"", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "\"\"", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", "\n", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "\".\"", ")", "\n", "\n", "", "", "", "start_model", "=", "model", "\n", "if", "hasattr", "(", "model", ",", "\"transformer\"", ")", "and", "all", "(", "not", "s", ".", "startswith", "(", "'transformer.'", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "start_model", "=", "model", ".", "transformer", "\n", "", "load", "(", "start_model", ",", "prefix", "=", "\"\"", ")", "\n", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Weights of {} not initialized from pretrained rpbert: {}\"", ".", "format", "(", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", "\n", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Weights from pretrained rpbert not used in {}: {}\"", ".", "format", "(", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", "\n", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "\n", "\"Error(s) in loading state_dict for {}:\\n\\t{}\"", ".", "format", "(", "model", ".", "__class__", ".", "__name__", ",", "\"\\n\\t\"", ".", "join", "(", "error_msgs", ")", ")", "\n", ")", "\n", "\n", "# Make sure we are still sharing the output and input embeddings after loading weights", "\n", "", "model", ".", "set_tied", "(", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2Model.__init__": [[514, 523], ["modeling_gpt2.GPT2PreTrainedModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "modeling_gpt2.Block", "torch.ModuleList", "torch.ModuleList", "modeling.BertLayerNorm", "modeling_gpt2.GPT2Model.apply", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "GPT2Model", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "wte", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "n_embd", ")", "\n", "self", ".", "wpe", "=", "nn", ".", "Embedding", "(", "config", ".", "n_positions", ",", "config", ".", "n_embd", ")", "\n", "block", "=", "Block", "(", "config", ".", "n_ctx", ",", "config", ",", "scale", "=", "True", ")", "\n", "self", ".", "h", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "block", ")", "for", "_", "in", "range", "(", "config", ".", "n_layer", ")", "]", ")", "\n", "self", ".", "ln_f", "=", "LayerNorm", "(", "config", ".", "n_embd", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "\n", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2Model.forward": [[524, 553], ["input_ids.view.view.size", "input_ids.view.view.view", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.view", "modeling_gpt2.GPT2Model.wte", "modeling_gpt2.GPT2Model.wpe", "zip", "modeling_gpt2.GPT2Model.ln_f", "[].size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "input_ids.view.view.size", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.size", "token_type_ids.view.view.view", "modeling_gpt2.GPT2Model.wte", "block", "presents.append", "modeling_gpt2.GPT2Model.view", "len", "token_type_ids.view.view.size", "modeling_gpt2.GPT2Model.size", "input_ids.view.view.size", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "position_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "past", "=", "None", ")", ":", "\n", "        ", "if", "past", "is", "None", ":", "\n", "            ", "past_length", "=", "0", "\n", "past", "=", "[", "None", "]", "*", "len", "(", "self", ".", "h", ")", "\n", "", "else", ":", "\n", "            ", "past_length", "=", "past", "[", "0", "]", "[", "0", "]", ".", "size", "(", "-", "2", ")", "\n", "", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "torch", ".", "arange", "(", "past_length", ",", "input_ids", ".", "size", "(", "-", "1", ")", "+", "past_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "\n", "", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "position_ids", "=", "position_ids", ".", "view", "(", "-", "1", ",", "position_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "\n", "inputs_embeds", "=", "self", ".", "wte", "(", "input_ids", ")", "\n", "position_embeds", "=", "self", ".", "wpe", "(", "position_ids", ")", "\n", "if", "token_type_ids", "is", "not", "None", ":", "\n", "            ", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "token_type_embeds", "=", "self", ".", "wte", "(", "token_type_ids", ")", "\n", "", "else", ":", "\n", "            ", "token_type_embeds", "=", "0", "\n", "", "hidden_states", "=", "inputs_embeds", "+", "position_embeds", "+", "token_type_embeds", "\n", "presents", "=", "[", "]", "\n", "for", "block", ",", "layer_past", "in", "zip", "(", "self", ".", "h", ",", "past", ")", ":", "\n", "            ", "hidden_states", ",", "present", "=", "block", "(", "hidden_states", ",", "layer_past", ")", "\n", "presents", ".", "append", "(", "present", ")", "\n", "", "hidden_states", "=", "self", ".", "ln_f", "(", "hidden_states", ")", "\n", "output_shape", "=", "input_shape", "+", "(", "hidden_states", ".", "size", "(", "-", "1", ")", ",", ")", "\n", "return", "hidden_states", ".", "view", "(", "*", "output_shape", ")", ",", "presents", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2LMHeadModel.__init__": [[594, 599], ["modeling_gpt2.GPT2PreTrainedModel.__init__", "modeling_gpt2.GPT2Model", "modeling_gpt2.GPT2LMHead", "modeling_gpt2.GPT2LMHeadModel.apply"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "GPT2LMHeadModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "transformer", "=", "GPT2Model", "(", "config", ")", "\n", "self", ".", "lm_head", "=", "GPT2LMHead", "(", "self", ".", "transformer", ".", "wte", ".", "weight", ",", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2LMHeadModel.set_tied": [[600, 604], ["modeling_gpt2.GPT2LMHeadModel.lm_head.set_embeddings_weights"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTLMHead.set_embeddings_weights"], ["", "def", "set_tied", "(", "self", ")", ":", "\n", "        ", "\"\"\" Make sure we are sharing the embeddings\n        \"\"\"", "\n", "self", ".", "lm_head", ".", "set_embeddings_weights", "(", "self", ".", "transformer", ".", "wte", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2LMHeadModel.forward": [[605, 613], ["modeling_gpt2.GPT2LMHeadModel.transformer", "modeling_gpt2.GPT2LMHeadModel.lm_head", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_gpt2.GPT2LMHeadModel.view", "lm_labels.view", "modeling_gpt2.GPT2LMHeadModel.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "position_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "lm_labels", "=", "None", ",", "past", "=", "None", ")", ":", "\n", "        ", "hidden_states", ",", "presents", "=", "self", ".", "transformer", "(", "input_ids", ",", "position_ids", ",", "token_type_ids", ",", "past", ")", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "hidden_states", ")", "\n", "if", "lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "loss", "=", "loss_fct", "(", "lm_logits", ".", "view", "(", "-", "1", ",", "lm_logits", ".", "size", "(", "-", "1", ")", ")", ",", "lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "return", "lm_logits", ",", "presents", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2DoubleHeadsModel.__init__": [[659, 665], ["modeling_gpt2.GPT2PreTrainedModel.__init__", "modeling_gpt2.GPT2Model", "modeling_gpt2.GPT2LMHead", "modeling_gpt2.GPT2MultipleChoiceHead", "modeling_gpt2.GPT2DoubleHeadsModel.apply"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "GPT2DoubleHeadsModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "transformer", "=", "GPT2Model", "(", "config", ")", "\n", "self", ".", "lm_head", "=", "GPT2LMHead", "(", "self", ".", "transformer", ".", "wte", ".", "weight", ",", "config", ")", "\n", "self", ".", "multiple_choice_head", "=", "GPT2MultipleChoiceHead", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2DoubleHeadsModel.set_tied": [[666, 670], ["modeling_gpt2.GPT2DoubleHeadsModel.lm_head.set_embeddings_weights"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTLMHead.set_embeddings_weights"], ["", "def", "set_tied", "(", "self", ")", ":", "\n", "        ", "\"\"\" Make sure we are sharing the embeddings\n        \"\"\"", "\n", "self", ".", "lm_head", ".", "set_embeddings_weights", "(", "self", ".", "transformer", ".", "wte", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.GPT2DoubleHeadsModel.forward": [[671, 685], ["modeling_gpt2.GPT2DoubleHeadsModel.transformer", "modeling_gpt2.GPT2DoubleHeadsModel.lm_head", "modeling_gpt2.GPT2DoubleHeadsModel.multiple_choice_head", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "losses.append", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "losses.append", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_gpt2.GPT2DoubleHeadsModel.view", "lm_labels.view", "modeling_gpt2.GPT2DoubleHeadsModel.view", "mc_labels.view", "modeling_gpt2.GPT2DoubleHeadsModel.size", "modeling_gpt2.GPT2DoubleHeadsModel.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "mc_token_ids", ",", "lm_labels", "=", "None", ",", "mc_labels", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "past", "=", "None", ")", ":", "\n", "        ", "hidden_states", ",", "presents", "=", "self", ".", "transformer", "(", "input_ids", ",", "position_ids", ",", "token_type_ids", ",", "past", ")", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "hidden_states", ")", "\n", "mc_logits", "=", "self", ".", "multiple_choice_head", "(", "hidden_states", ",", "mc_token_ids", ")", "\n", "losses", "=", "[", "]", "\n", "if", "lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "losses", ".", "append", "(", "loss_fct", "(", "lm_logits", ".", "view", "(", "-", "1", ",", "lm_logits", ".", "size", "(", "-", "1", ")", ")", ",", "lm_labels", ".", "view", "(", "-", "1", ")", ")", ")", "\n", "", "if", "mc_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "losses", ".", "append", "(", "loss_fct", "(", "mc_logits", ".", "view", "(", "-", "1", ",", "mc_logits", ".", "size", "(", "-", "1", ")", ")", ",", "mc_labels", ".", "view", "(", "-", "1", ")", ")", ")", "\n", "", "if", "losses", ":", "\n", "            ", "return", "losses", "\n", "", "return", "lm_logits", ",", "mc_logits", ",", "presents", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.load_tf_weights_in_gpt2": [[46, 98], ["os.path.abspath", "print", "tf.train.list_variables", "zip", "print", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "print", "torch.from_numpy", "torch.from_numpy", "print", "tf.train.load_variable.squeeze", "re.fullmatch", "re.split", "getattr", "len", "int", "getattr", "getattr", "getattr", "getattr"], "function", ["None"], ["def", "load_tf_weights_in_gpt2", "(", "model", ",", "gpt2_checkpoint_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch rpbert\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "re", "\n", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "print", "(", "\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "gpt2_checkpoint_path", ")", "\n", "print", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "\n", "# Load weights from TF rpbert", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "print", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ".", "squeeze", "(", ")", ")", "\n", "\n", "", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", "[", "6", ":", "]", "# skip \"rpbert/\"", "\n", "name", "=", "name", ".", "split", "(", "'/'", ")", "\n", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r'[A-Za-z]+\\d+'", ",", "m_name", ")", ":", "\n", "                ", "l", "=", "re", ".", "split", "(", "r'(\\d+)'", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "l", "=", "[", "m_name", "]", "\n", "", "if", "l", "[", "0", "]", "==", "'w'", "or", "l", "[", "0", "]", "==", "'g'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'b'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'bias'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'wpe'", "or", "l", "[", "0", "]", "==", "'wte'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "\n", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "else", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "\n", "", "if", "len", "(", "l", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "l", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "print", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.gelu": [[100, 102], ["torch.tanh", "torch.tanh", "math.sqrt", "torch.pow", "torch.pow"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "return", "0.5", "*", "x", "*", "(", "1", "+", "torch", ".", "tanh", "(", "math", ".", "sqrt", "(", "2", "/", "math", ".", "pi", ")", "*", "(", "x", "+", "0.044715", "*", "torch", ".", "pow", "(", "x", ",", "3", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch": [[26, 38], ["external.pytorch_pretrained_bert.modeling.BertConfig.from_json_file", "print", "external.pytorch_pretrained_bert.modeling.BertForPreTraining", "external.pytorch_pretrained_bert.modeling.load_tf_weights_in_bert", "print", "torch.save", "external.pytorch_pretrained_bert.modeling.BertForPreTraining.state_dict", "str"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.from_json_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling.load_tf_weights_in_bert", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save"], ["def", "convert_tf_checkpoint_to_pytorch", "(", "tf_checkpoint_path", ",", "bert_config_file", ",", "pytorch_dump_path", ")", ":", "\n", "# Initialise PyTorch rpbert", "\n", "    ", "config", "=", "BertConfig", ".", "from_json_file", "(", "bert_config_file", ")", "\n", "print", "(", "\"Building PyTorch rpbert from configuration: {}\"", ".", "format", "(", "str", "(", "config", ")", ")", ")", "\n", "model", "=", "BertForPreTraining", "(", "config", ")", "\n", "\n", "# Load weights from tf checkpoint", "\n", "load_tf_weights_in_bert", "(", "model", ",", "tf_checkpoint_path", ")", "\n", "\n", "# Save pytorch-rpbert", "\n", "print", "(", "\"Save PyTorch rpbert to {}\"", ".", "format", "(", "pytorch_dump_path", ")", ")", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "pytorch_dump_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.convert_openai_checkpoint_to_pytorch.convert_openai_checkpoint_to_pytorch": [[30, 49], ["external.pytorch_pretrained_bert.modeling_openai.OpenAIGPTModel", "external.pytorch_pretrained_bert.modeling_openai.load_tf_weights_in_openai_gpt", "print", "torch.save", "print", "external.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig", "external.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig", "external.pytorch_pretrained_bert.modeling_openai.OpenAIGPTModel.state_dict", "io.open", "f.write", "external.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.to_json_string"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.load_tf_weights_in_openai_gpt", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.to_json_string"], ["def", "convert_openai_checkpoint_to_pytorch", "(", "openai_checkpoint_folder_path", ",", "openai_config_file", ",", "pytorch_dump_folder_path", ")", ":", "\n", "# Construct rpbert", "\n", "    ", "if", "openai_config_file", "==", "\"\"", ":", "\n", "        ", "config", "=", "OpenAIGPTConfig", "(", ")", "\n", "", "else", ":", "\n", "        ", "config", "=", "OpenAIGPTConfig", "(", "openai_config_file", ")", "\n", "", "model", "=", "OpenAIGPTModel", "(", "config", ")", "\n", "\n", "# Load weights from numpy", "\n", "load_tf_weights_in_openai_gpt", "(", "model", ",", "openai_checkpoint_folder_path", ")", "\n", "\n", "# Save pytorch-rpbert", "\n", "pytorch_weights_dump_path", "=", "pytorch_dump_folder_path", "+", "'/'", "+", "WEIGHTS_NAME", "\n", "pytorch_config_dump_path", "=", "pytorch_dump_folder_path", "+", "'/'", "+", "CONFIG_NAME", "\n", "print", "(", "\"Save PyTorch rpbert to {}\"", ".", "format", "(", "pytorch_weights_dump_path", ")", ")", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "pytorch_weights_dump_path", ")", "\n", "print", "(", "\"Save configuration file to {}\"", ".", "format", "(", "pytorch_config_dump_path", ")", ")", "\n", "with", "open", "(", "pytorch_config_dump_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "config", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.optimization.BertAdam.__init__": [[59, 78], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "schedule", "=", "'warmup_linear'", ",", "\n", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-6", ",", "weight_decay", "=", "0.01", ",", "\n", "max_grad_norm", "=", "1.0", ")", ":", "\n", "        ", "if", "lr", "is", "not", "required", "and", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "schedule", "not", "in", "SCHEDULES", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid schedule parameter: {}\"", ".", "format", "(", "schedule", ")", ")", "\n", "", "if", "not", "0.0", "<=", "warmup", "<", "1.0", "and", "not", "warmup", "==", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\"", ".", "format", "(", "warmup", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b1", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b1", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b2", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b2", ")", ")", "\n", "", "if", "not", "e", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "e", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "schedule", "=", "schedule", ",", "warmup", "=", "warmup", ",", "t_total", "=", "t_total", ",", "\n", "b1", "=", "b1", ",", "b2", "=", "b2", ",", "e", "=", "e", ",", "weight_decay", "=", "weight_decay", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ")", "\n", "super", "(", "BertAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.optimization.BertAdam.get_lr": [[79, 93], ["lr.append", "len", "schedule_fct"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "lr", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "return", "[", "0", "]", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "", "lr", ".", "append", "(", "lr_scheduled", ")", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.optimization.BertAdam.step": [[94, 163], ["closure", "next_m.mul_().add_", "next_v.mul_().addcmul_", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.nn.utils.clip_grad_norm_", "next_m.mul_", "next_v.mul_", "next_v.sqrt", "schedule_fct"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the rpbert\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'next_m'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'next_v'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "next_m", ",", "next_v", "=", "state", "[", "'next_m'", "]", ",", "state", "[", "'next_v'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "# Add grad clipping", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "                    ", "clip_grad_norm_", "(", "p", ",", "group", "[", "'max_grad_norm'", "]", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "", "next_m", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "next_v", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "update", "=", "next_m", "/", "(", "next_v", ".", "sqrt", "(", ")", "+", "group", "[", "'e'", "]", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "group", "[", "'weight_decay'", "]", ">", "0.0", ":", "\n", "                    ", "update", "+=", "group", "[", "'weight_decay'", "]", "*", "p", ".", "data", "\n", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "update_with_lr", "=", "lr_scheduled", "*", "update", "\n", "p", ".", "data", ".", "add_", "(", "-", "update_with_lr", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1", "\n", "# No bias correction", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.optimization.warmup_cosine": [[23, 27], ["torch.cos"], "function", ["None"], ["def", "warmup_cosine", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "0.5", "*", "(", "1.0", "+", "torch", ".", "cos", "(", "math", ".", "pi", "*", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.optimization.warmup_constant": [[28, 32], ["None"], "function", ["None"], ["", "def", "warmup_constant", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.optimization.warmup_linear": [[33, 37], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "-", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.from_pretrained": [[88, 130], ["cls", "os.path.join", "os.path.join", "file_utils.cached_path", "file_utils.cached_path", "logger.info", "logger.info", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained rpbert file.\n        Download and cache the pre-trained rpbert file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "merges_file", "=", "PRETRAINED_MERGES_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "VOCAB_NAME", ")", "\n", "merges_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "MERGES_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "resolved_merges_file", "=", "cached_path", "(", "merges_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in rpbert name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find files {} and {} \"", "\n", "\"at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", "vocab_file", ",", "merges_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", "and", "resolved_merges_file", "==", "merges_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "logger", ".", "info", "(", "\"loading merges file {}\"", ".", "format", "(", "merges_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "logger", ".", "info", "(", "\"loading merges file {} from cache at {}\"", ".", "format", "(", "\n", "merges_file", ",", "resolved_merges_file", ")", ")", "\n", "", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained rpbert, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "resolved_merges_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.__init__": [[131, 145], ["json.load", "tokenization_gpt2.bytes_to_unicode", "dict", "regex.compile", "int", "io.open", "io.open().read().split", "tuple", "zip", "tokenization_gpt2.GPT2Tokenizer.encoder.items", "tokenization_gpt2.GPT2Tokenizer.byte_encoder.items", "merge.split", "range", "io.open().read", "len", "io.open"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.bytes_to_unicode"], ["", "def", "__init__", "(", "self", ",", "vocab_file", ",", "merges_file", ",", "errors", "=", "'replace'", ",", "max_len", "=", "None", ")", ":", "\n", "        ", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "self", ".", "encoder", "=", "json", ".", "load", "(", "open", "(", "vocab_file", ")", ")", "\n", "self", ".", "decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "encoder", ".", "items", "(", ")", "}", "\n", "self", ".", "errors", "=", "errors", "# how to handle errors in decoding", "\n", "self", ".", "byte_encoder", "=", "bytes_to_unicode", "(", ")", "\n", "self", ".", "byte_decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "byte_encoder", ".", "items", "(", ")", "}", "\n", "bpe_data", "=", "open", "(", "merges_file", ",", "encoding", "=", "'utf-8'", ")", ".", "read", "(", ")", ".", "split", "(", "'\\n'", ")", "[", "1", ":", "-", "1", "]", "\n", "bpe_merges", "=", "[", "tuple", "(", "merge", ".", "split", "(", ")", ")", "for", "merge", "in", "bpe_data", "]", "\n", "self", ".", "bpe_ranks", "=", "dict", "(", "zip", "(", "bpe_merges", ",", "range", "(", "len", "(", "bpe_merges", ")", ")", ")", ")", "\n", "self", ".", "cache", "=", "{", "}", "\n", "\n", "# Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions", "\n", "self", ".", "pat", "=", "re", ".", "compile", "(", "r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.__len__": [[146, 148], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "encoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.bpe": [[149, 189], ["tuple", "tokenization_gpt2.get_pairs", "min", "tuple", "len", "len", "tokenization_gpt2.get_pairs", "tuple.index", "tuple.extend", "tuple.append", "tuple.append", "tokenization_gpt2.GPT2Tokenizer.bpe_ranks.get", "tuple.extend", "float", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.get_pairs", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.get_pairs"], ["", "def", "bpe", "(", "self", ",", "token", ")", ":", "\n", "        ", "if", "token", "in", "self", ".", "cache", ":", "\n", "            ", "return", "self", ".", "cache", "[", "token", "]", "\n", "", "word", "=", "tuple", "(", "token", ")", "\n", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "\n", "if", "not", "pairs", ":", "\n", "            ", "return", "token", "\n", "\n", "", "while", "True", ":", "\n", "            ", "bigram", "=", "min", "(", "pairs", ",", "key", "=", "lambda", "pair", ":", "self", ".", "bpe_ranks", ".", "get", "(", "pair", ",", "float", "(", "'inf'", ")", ")", ")", "\n", "if", "bigram", "not", "in", "self", ".", "bpe_ranks", ":", "\n", "                ", "break", "\n", "", "first", ",", "second", "=", "bigram", "\n", "new_word", "=", "[", "]", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "word", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "j", "=", "word", ".", "index", "(", "first", ",", "i", ")", "\n", "new_word", ".", "extend", "(", "word", "[", "i", ":", "j", "]", ")", "\n", "i", "=", "j", "\n", "", "except", ":", "\n", "                    ", "new_word", ".", "extend", "(", "word", "[", "i", ":", "]", ")", "\n", "break", "\n", "\n", "", "if", "word", "[", "i", "]", "==", "first", "and", "i", "<", "len", "(", "word", ")", "-", "1", "and", "word", "[", "i", "+", "1", "]", "==", "second", ":", "\n", "                    ", "new_word", ".", "append", "(", "first", "+", "second", ")", "\n", "i", "+=", "2", "\n", "", "else", ":", "\n", "                    ", "new_word", ".", "append", "(", "word", "[", "i", "]", ")", "\n", "i", "+=", "1", "\n", "", "", "new_word", "=", "tuple", "(", "new_word", ")", "\n", "word", "=", "new_word", "\n", "if", "len", "(", "word", ")", "==", "1", ":", "\n", "                ", "break", "\n", "", "else", ":", "\n", "                ", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "", "", "word", "=", "' '", ".", "join", "(", "word", ")", "\n", "self", ".", "cache", "[", "token", "]", "=", "word", "\n", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode": [[190, 202], ["regex.findall", "bpe_tokens.extend", "len", "ValueError", "len", "token.encode", "tokenization_gpt2.GPT2Tokenizer.bpe().split", "tokenization_gpt2.GPT2Tokenizer.bpe"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.bpe"], ["", "def", "encode", "(", "self", ",", "text", ")", ":", "\n", "        ", "bpe_tokens", "=", "[", "]", "\n", "for", "token", "in", "re", ".", "findall", "(", "self", ".", "pat", ",", "text", ")", ":", "\n", "            ", "token", "=", "''", ".", "join", "(", "self", ".", "byte_encoder", "[", "b", "]", "for", "b", "in", "token", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "bpe_tokens", ".", "extend", "(", "self", ".", "encoder", "[", "bpe_token", "]", "for", "bpe_token", "in", "self", ".", "bpe", "(", "token", ")", ".", "split", "(", "' '", ")", ")", "\n", "", "if", "len", "(", "bpe_tokens", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this OpenAI GPT-2 rpbert ({} > {}). Running this\"", "\n", "\" sequence through the rpbert will result in indexing errors\"", ".", "format", "(", "len", "(", "bpe_tokens", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "bpe_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.decode": [[203, 207], ["bytearray().decode", "bytearray"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.decode"], ["", "def", "decode", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "text", "=", "''", ".", "join", "(", "[", "self", ".", "decoder", "[", "token", "]", "for", "token", "in", "tokens", "]", ")", "\n", "text", "=", "bytearray", "(", "[", "self", ".", "byte_decoder", "[", "c", "]", "for", "c", "in", "text", "]", ")", ".", "decode", "(", "'utf-8'", ",", "errors", "=", "self", ".", "errors", ")", "\n", "return", "text", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.bytes_to_unicode": [[49, 70], ["lru_cache", "range", "dict", "list", "chr", "zip", "list", "list", "range", "bs.append", "cs.append", "range", "range", "ord", "ord", "ord", "ord", "ord", "ord"], "function", ["None"], ["@", "lru_cache", "(", ")", "\n", "def", "bytes_to_unicode", "(", ")", ":", "\n", "    ", "\"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"", "\n", "bs", "=", "list", "(", "range", "(", "ord", "(", "\"!\"", ")", ",", "ord", "(", "\"~\"", ")", "+", "1", ")", ")", "+", "list", "(", "range", "(", "ord", "(", "\"\u00a1\"", ")", ",", "ord", "(", "\"\u00ac\"", ")", "+", "1", ")", ")", "+", "list", "(", "range", "(", "ord", "(", "\"\u00ae\"", ")", ",", "ord", "(", "\"\u00ff\"", ")", "+", "1", ")", ")", "\n", "cs", "=", "bs", "[", ":", "]", "\n", "n", "=", "0", "\n", "for", "b", "in", "range", "(", "2", "**", "8", ")", ":", "\n", "        ", "if", "b", "not", "in", "bs", ":", "\n", "            ", "bs", ".", "append", "(", "b", ")", "\n", "cs", ".", "append", "(", "2", "**", "8", "+", "n", ")", "\n", "n", "+=", "1", "\n", "", "", "cs", "=", "[", "chr", "(", "n", ")", "for", "n", "in", "cs", "]", "\n", "return", "dict", "(", "zip", "(", "bs", ",", "cs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.get_pairs": [[71, 82], ["set", "set.add"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.SearchSpace.add"], ["", "def", "get_pairs", "(", "word", ")", ":", "\n", "    ", "\"\"\"Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"", "\n", "pairs", "=", "set", "(", ")", "\n", "prev_char", "=", "word", "[", "0", "]", "\n", "for", "char", "in", "word", "[", "1", ":", "]", ":", "\n", "        ", "pairs", ".", "add", "(", "(", "prev_char", ",", "char", ")", ")", "\n", "prev_char", "=", "char", "\n", "", "return", "pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLConfig.__init__": [[186, 288], ["isinstance", "json.loads.items", "isinstance", "isinstance", "io.open", "json.loads", "modeling_transfo_xl.TransfoXLConfig.cutoffs.extend", "ValueError", "reader.read", "len", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", "=", "267735", ",", "\n", "cutoffs", "=", "[", "20000", ",", "40000", ",", "200000", "]", ",", "\n", "d_model", "=", "1024", ",", "\n", "d_embed", "=", "1024", ",", "\n", "n_head", "=", "16", ",", "\n", "d_head", "=", "64", ",", "\n", "d_inner", "=", "4096", ",", "\n", "div_val", "=", "4", ",", "\n", "pre_lnorm", "=", "False", ",", "\n", "n_layer", "=", "18", ",", "\n", "tgt_len", "=", "128", ",", "\n", "ext_len", "=", "0", ",", "\n", "mem_len", "=", "1600", ",", "\n", "clamp_len", "=", "1000", ",", "\n", "same_length", "=", "True", ",", "\n", "proj_share_all_but_first", "=", "True", ",", "\n", "attn_type", "=", "0", ",", "\n", "sample_softmax", "=", "-", "1", ",", "\n", "adaptive", "=", "True", ",", "\n", "tie_weight", "=", "True", ",", "\n", "dropout", "=", "0.1", ",", "\n", "dropatt", "=", "0.0", ",", "\n", "untie_r", "=", "True", ",", "\n", "init", "=", "\"normal\"", ",", "\n", "init_range", "=", "0.01", ",", "\n", "proj_init_std", "=", "0.01", ",", "\n", "init_std", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs TransfoXLConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `TransfoXLModel` or a configuration json file.\n            cutoffs: cutoffs for the adaptive softmax\n            d_model: Dimensionality of the rpbert's hidden states.\n            d_embed: Dimensionality of the embeddings\n            d_head: Dimensionality of the rpbert's heads.\n            div_val: divident value for adapative input and softmax\n            pre_lnorm: apply LayerNorm to the input instead of the output\n            d_inner: Inner dimension in FF\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            tgt_len: number of tokens to predict\n            ext_len: length of the extended context\n            mem_len: length of the retained previous heads\n            same_length: use the same attn length for all tokens\n            proj_share_all_but_first: True to share all but first projs, False not to share.\n            attn_type: attention type. 0 for Transformer-XL, 1 for Shaw et al, 2 for Vaswani et al, 3 for Al Rfou et al.\n            clamp_len: use the same pos embeddings after clamp_len\n            sample_softmax: number of samples in sampled softmax\n            adaptive: use adaptive softmax\n            tie_weight: tie the word embedding and softmax weights\n            dropout: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            dropatt: The dropout ratio for the attention probabilities.\n            untie_r: untie relative position biases           \n            embd_pdrop: The dropout ratio for the embeddings.\n            init: parameter initializer to use\n            init_range: parameters initialized by U(-init_range, init_range).\n            proj_init_std: parameters initialized by N(0, init_std)\n            init_std: parameters initialized by N(0, init_std)\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "\n", "and", "isinstance", "(", "vocab_size_or_config_json_file", ",", "unicode", ")", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "n_token", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "cutoffs", "=", "[", "]", "\n", "self", ".", "cutoffs", ".", "extend", "(", "cutoffs", ")", "\n", "self", ".", "tie_weight", "=", "tie_weight", "\n", "if", "proj_share_all_but_first", ":", "\n", "                ", "self", ".", "tie_projs", "=", "[", "False", "]", "+", "[", "True", "]", "*", "len", "(", "self", ".", "cutoffs", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "tie_projs", "=", "[", "False", "]", "+", "[", "False", "]", "*", "len", "(", "self", ".", "cutoffs", ")", "\n", "", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_embed", "=", "d_embed", "\n", "self", ".", "d_head", "=", "d_head", "\n", "self", ".", "d_inner", "=", "d_inner", "\n", "self", ".", "div_val", "=", "div_val", "\n", "self", ".", "pre_lnorm", "=", "pre_lnorm", "\n", "self", ".", "n_layer", "=", "n_layer", "\n", "self", ".", "n_head", "=", "n_head", "\n", "self", ".", "tgt_len", "=", "tgt_len", "\n", "self", ".", "ext_len", "=", "ext_len", "\n", "self", ".", "mem_len", "=", "mem_len", "\n", "self", ".", "same_length", "=", "same_length", "\n", "self", ".", "attn_type", "=", "attn_type", "\n", "self", ".", "clamp_len", "=", "clamp_len", "\n", "self", ".", "sample_softmax", "=", "sample_softmax", "\n", "self", ".", "adaptive", "=", "adaptive", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "dropatt", "=", "dropatt", "\n", "self", ".", "untie_r", "=", "untie_r", "\n", "self", ".", "init", "=", "init", "\n", "self", ".", "init_range", "=", "init_range", "\n", "self", ".", "proj_init_std", "=", "proj_init_std", "\n", "self", ".", "init_std", "=", "init_std", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained rpbert config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLConfig.from_dict": [[290, 297], ["modeling_transfo_xl.TransfoXLConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `TransfoXLConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "TransfoXLConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLConfig.from_json_file": [[298, 304], ["cls.from_dict", "io.open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `TransfoXLConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLConfig.__repr__": [[305, 307], ["str", "modeling_transfo_xl.TransfoXLConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLConfig.to_dict": [[308, 312], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLConfig.to_json_string": [[313, 316], ["json.dumps", "modeling_transfo_xl.TransfoXLConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.PositionalEmbedding.__init__": [[319, 326], ["torch.Module.__init__", "modeling_transfo_xl.PositionalEmbedding.register_buffer", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "demb", ")", ":", "\n", "        ", "super", "(", "PositionalEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "demb", "=", "demb", "\n", "\n", "inv_freq", "=", "1", "/", "(", "10000", "**", "(", "torch", ".", "arange", "(", "0.0", ",", "demb", ",", "2.0", ")", "/", "demb", ")", ")", "\n", "self", ".", "register_buffer", "(", "'inv_freq'", ",", "inv_freq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.PositionalEmbedding.forward": [[327, 335], ["torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pos_emb[].expand", "torch.ger.sin", "torch.ger.sin", "torch.ger.sin", "torch.ger.cos", "torch.ger.cos", "torch.ger.cos"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pos_seq", ",", "bsz", "=", "None", ")", ":", "\n", "        ", "sinusoid_inp", "=", "torch", ".", "ger", "(", "pos_seq", ",", "self", ".", "inv_freq", ")", "\n", "pos_emb", "=", "torch", ".", "cat", "(", "[", "sinusoid_inp", ".", "sin", "(", ")", ",", "sinusoid_inp", ".", "cos", "(", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "bsz", "is", "not", "None", ":", "\n", "            ", "return", "pos_emb", "[", ":", ",", "None", ",", ":", "]", ".", "expand", "(", "-", "1", ",", "bsz", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "pos_emb", "[", ":", ",", "None", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.PositionwiseFF.__init__": [[338, 355], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "modeling.BertLayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "d_inner", ",", "dropout", ",", "pre_lnorm", "=", "False", ")", ":", "\n", "        ", "super", "(", "PositionwiseFF", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_inner", "=", "d_inner", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n", "self", ".", "CoreNet", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "d_model", ",", "d_inner", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout", ")", ",", "\n", "nn", ".", "Linear", "(", "d_inner", ",", "d_model", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout", ")", ",", "\n", ")", "\n", "\n", "self", ".", "layer_norm", "=", "LayerNorm", "(", "d_model", ")", "\n", "\n", "self", ".", "pre_lnorm", "=", "pre_lnorm", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.PositionwiseFF.forward": [[356, 371], ["modeling_transfo_xl.PositionwiseFF.CoreNet", "modeling_transfo_xl.PositionwiseFF.CoreNet", "modeling_transfo_xl.PositionwiseFF.layer_norm", "modeling_transfo_xl.PositionwiseFF.layer_norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inp", ")", ":", "\n", "        ", "if", "self", ".", "pre_lnorm", ":", "\n", "##### layer normalization + positionwise feed-forward", "\n", "            ", "core_out", "=", "self", ".", "CoreNet", "(", "self", ".", "layer_norm", "(", "inp", ")", ")", "\n", "\n", "##### residual connection", "\n", "output", "=", "core_out", "+", "inp", "\n", "", "else", ":", "\n", "##### positionwise feed-forward", "\n", "            ", "core_out", "=", "self", ".", "CoreNet", "(", "inp", ")", "\n", "\n", "##### residual connection + layer normalization", "\n", "output", "=", "self", ".", "layer_norm", "(", "inp", "+", "core_out", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.MultiHeadAttn.__init__": [[373, 401], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "modeling.BertLayerNorm", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_head", ",", "d_model", ",", "d_head", ",", "dropout", ",", "dropatt", "=", "0", ",", "\n", "pre_lnorm", "=", "False", ",", "r_r_bias", "=", "None", ",", "r_w_bias", "=", "None", ")", ":", "\n", "        ", "super", "(", "MultiHeadAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "n_head", "=", "n_head", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_head", "=", "d_head", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n", "self", ".", "q_net", "=", "nn", ".", "Linear", "(", "d_model", ",", "n_head", "*", "d_head", ",", "bias", "=", "False", ")", "\n", "self", ".", "kv_net", "=", "nn", ".", "Linear", "(", "d_model", ",", "2", "*", "n_head", "*", "d_head", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropatt", "=", "nn", ".", "Dropout", "(", "dropatt", ")", "\n", "self", ".", "o_net", "=", "nn", ".", "Linear", "(", "n_head", "*", "d_head", ",", "d_model", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "layer_norm", "=", "LayerNorm", "(", "d_model", ")", "\n", "\n", "self", ".", "scale", "=", "1", "/", "(", "d_head", "**", "0.5", ")", "\n", "\n", "self", ".", "pre_lnorm", "=", "pre_lnorm", "\n", "\n", "if", "r_r_bias", "is", "None", "or", "r_w_bias", "is", "None", ":", "# Biases are not shared", "\n", "            ", "self", ".", "r_r_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "self", ".", "r_w_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "r_r_bias", "=", "r_r_bias", "\n", "self", ".", "r_w_bias", "=", "r_w_bias", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.MultiHeadAttn.forward": [[402, 452], ["modeling_transfo_xl.MultiHeadAttn.q_net", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "head_q.view.view.view", "head_k.view.view.view", "head_v.view.view.view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum.mul_", "torch.einsum.mul_", "torch.einsum.mul_", "torch.softmax", "torch.softmax", "torch.softmax", "modeling_transfo_xl.MultiHeadAttn.dropatt", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "attn_vec.contiguous().view.contiguous().view.contiguous().view", "modeling_transfo_xl.MultiHeadAttn.o_net", "modeling_transfo_xl.MultiHeadAttn.drop", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling_transfo_xl.MultiHeadAttn.layer_norm", "modeling_transfo_xl.MultiHeadAttn.kv_net", "h.size", "h.size", "modeling_transfo_xl.MultiHeadAttn.size", "modeling_transfo_xl.MultiHeadAttn.size", "modeling_transfo_xl.MultiHeadAttn.size", "modeling_transfo_xl.MultiHeadAttn.size", "attn_mask.any().item", "attn_vec.contiguous().view.contiguous().view.size", "attn_vec.contiguous().view.contiguous().view.size", "modeling_transfo_xl.MultiHeadAttn.layer_norm", "attn_mask.dim", "torch.einsum.masked_fill_", "torch.einsum.masked_fill_", "torch.einsum.masked_fill_", "attn_vec.contiguous().view.contiguous().view.contiguous", "attn_mask.any", "attn_mask.dim", "torch.einsum.masked_fill_", "torch.einsum.masked_fill_", "torch.einsum.masked_fill_", "float", "float"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "h", ",", "attn_mask", "=", "None", ",", "mems", "=", "None", ")", ":", "\n", "##### multihead attention", "\n", "# [hlen x bsz x n_head x d_head]", "\n", "\n", "        ", "if", "mems", "is", "not", "None", ":", "\n", "            ", "c", "=", "torch", ".", "cat", "(", "[", "mems", ",", "h", "]", ",", "0", ")", "\n", "", "else", ":", "\n", "            ", "c", "=", "h", "\n", "\n", "", "if", "self", ".", "pre_lnorm", ":", "\n", "##### layer normalization", "\n", "            ", "c", "=", "self", ".", "layer_norm", "(", "c", ")", "\n", "\n", "", "head_q", "=", "self", ".", "q_net", "(", "h", ")", "\n", "head_k", ",", "head_v", "=", "torch", ".", "chunk", "(", "self", ".", "kv_net", "(", "c", ")", ",", "2", ",", "-", "1", ")", "\n", "\n", "head_q", "=", "head_q", ".", "view", "(", "h", ".", "size", "(", "0", ")", ",", "h", ".", "size", "(", "1", ")", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", "\n", "head_k", "=", "head_k", ".", "view", "(", "c", ".", "size", "(", "0", ")", ",", "c", ".", "size", "(", "1", ")", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", "\n", "head_v", "=", "head_v", ".", "view", "(", "c", ".", "size", "(", "0", ")", ",", "c", ".", "size", "(", "1", ")", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", "\n", "\n", "# [qlen x klen x bsz x n_head]", "\n", "attn_score", "=", "torch", ".", "einsum", "(", "'ibnd,jbnd->ijbn'", ",", "(", "head_q", ",", "head_k", ")", ")", "\n", "attn_score", ".", "mul_", "(", "self", ".", "scale", ")", "\n", "if", "attn_mask", "is", "not", "None", "and", "attn_mask", ".", "any", "(", ")", ".", "item", "(", ")", ":", "\n", "            ", "if", "attn_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "                ", "attn_score", ".", "masked_fill_", "(", "attn_mask", "[", "None", ",", ":", ",", ":", ",", "None", "]", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "", "elif", "attn_mask", ".", "dim", "(", ")", "==", "3", ":", "\n", "                ", "attn_score", ".", "masked_fill_", "(", "attn_mask", "[", ":", ",", ":", ",", ":", ",", "None", "]", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "\n", "# [qlen x klen x bsz x n_head]", "\n", "", "", "attn_prob", "=", "F", ".", "softmax", "(", "attn_score", ",", "dim", "=", "1", ")", "\n", "attn_prob", "=", "self", ".", "dropatt", "(", "attn_prob", ")", "\n", "\n", "# [qlen x klen x bsz x n_head] + [klen x bsz x n_head x d_head] -> [qlen x bsz x n_head x d_head]", "\n", "attn_vec", "=", "torch", ".", "einsum", "(", "'ijbn,jbnd->ibnd'", ",", "(", "attn_prob", ",", "head_v", ")", ")", "\n", "attn_vec", "=", "attn_vec", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "attn_vec", ".", "size", "(", "0", ")", ",", "attn_vec", ".", "size", "(", "1", ")", ",", "self", ".", "n_head", "*", "self", ".", "d_head", ")", "\n", "\n", "##### linear projection", "\n", "attn_out", "=", "self", ".", "o_net", "(", "attn_vec", ")", "\n", "attn_out", "=", "self", ".", "drop", "(", "attn_out", ")", "\n", "\n", "if", "self", ".", "pre_lnorm", ":", "\n", "##### residual connection", "\n", "            ", "output", "=", "h", "+", "attn_out", "\n", "", "else", ":", "\n", "##### residual connection + layer normalization", "\n", "            ", "output", "=", "self", ".", "layer_norm", "(", "h", "+", "attn_out", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelMultiHeadAttn.__init__": [[454, 482], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "modeling.BertLayerNorm", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_head", ",", "d_model", ",", "d_head", ",", "dropout", ",", "dropatt", "=", "0", ",", "\n", "tgt_len", "=", "None", ",", "ext_len", "=", "None", ",", "mem_len", "=", "None", ",", "pre_lnorm", "=", "False", ",", "\n", "r_r_bias", "=", "None", ",", "r_w_bias", "=", "None", ")", ":", "\n", "        ", "super", "(", "RelMultiHeadAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "n_head", "=", "n_head", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "d_head", "=", "d_head", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n", "self", ".", "qkv_net", "=", "nn", ".", "Linear", "(", "d_model", ",", "3", "*", "n_head", "*", "d_head", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropatt", "=", "nn", ".", "Dropout", "(", "dropatt", ")", "\n", "self", ".", "o_net", "=", "nn", ".", "Linear", "(", "n_head", "*", "d_head", ",", "d_model", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "layer_norm", "=", "LayerNorm", "(", "d_model", ")", "\n", "\n", "self", ".", "scale", "=", "1", "/", "(", "d_head", "**", "0.5", ")", "\n", "\n", "self", ".", "pre_lnorm", "=", "pre_lnorm", "\n", "\n", "if", "r_r_bias", "is", "None", "or", "r_w_bias", "is", "None", ":", "# Biases are not shared", "\n", "            ", "self", ".", "r_r_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "self", ".", "r_w_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "r_r_bias", "=", "r_r_bias", "\n", "self", ".", "r_w_bias", "=", "r_w_bias", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelMultiHeadAttn._parallelogram_mask": [[483, 493], ["torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "torch.ones().byte", "min", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.ones().byte.flip", "torch.ones().byte.flip", "torch.ones().byte.flip", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["None"], ["", "", "def", "_parallelogram_mask", "(", "self", ",", "h", ",", "w", ",", "left", "=", "False", ")", ":", "\n", "        ", "mask", "=", "torch", ".", "ones", "(", "(", "h", ",", "w", ")", ")", ".", "byte", "(", ")", "\n", "m", "=", "min", "(", "h", ",", "w", ")", "\n", "mask", "[", ":", "m", ",", ":", "m", "]", "=", "torch", ".", "triu", "(", "mask", "[", ":", "m", ",", ":", "m", "]", ")", "\n", "mask", "[", "-", "m", ":", ",", "-", "m", ":", "]", "=", "torch", ".", "tril", "(", "mask", "[", "-", "m", ":", ",", "-", "m", ":", "]", ")", "\n", "\n", "if", "left", ":", "\n", "            ", "return", "mask", "\n", "", "else", ":", "\n", "            ", "return", "mask", ".", "flip", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelMultiHeadAttn._shift": [[494, 511], ["torch.cat().expand.masked_select().view", "torch.cat().expand.masked_select().view", "torch.cat().expand.masked_select().view", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "mask.flip.flip.flip", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand", "torch.cat().expand.masked_select().view.size", "torch.cat().expand.masked_select().view.size", "torch.cat().expand.masked_select", "torch.cat().expand.masked_select", "torch.cat().expand.masked_select", "torch.cat().expand.masked_select().view.size", "torch.cat().expand.masked_select().view.size", "torch.cat().expand.masked_select().view.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "", "def", "_shift", "(", "self", ",", "x", ",", "qlen", ",", "klen", ",", "mask", ",", "left", "=", "False", ")", ":", "\n", "        ", "if", "qlen", ">", "1", ":", "\n", "            ", "zero_pad", "=", "torch", ".", "zeros", "(", "(", "x", ".", "size", "(", "0", ")", ",", "qlen", "-", "1", ",", "x", ".", "size", "(", "2", ")", ",", "x", ".", "size", "(", "3", ")", ")", ",", "\n", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", "\n", "", "else", ":", "\n", "            ", "zero_pad", "=", "torch", ".", "zeros", "(", "0", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", "\n", "\n", "", "if", "left", ":", "\n", "            ", "mask", "=", "mask", ".", "flip", "(", "1", ")", "\n", "x_padded", "=", "torch", ".", "cat", "(", "[", "zero_pad", ",", "x", "]", ",", "dim", "=", "1", ")", ".", "expand", "(", "qlen", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "x_padded", "=", "torch", ".", "cat", "(", "[", "x", ",", "zero_pad", "]", ",", "dim", "=", "1", ")", ".", "expand", "(", "qlen", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "\n", "", "x", "=", "x_padded", ".", "masked_select", "(", "mask", "[", ":", ",", ":", ",", "None", ",", "None", "]", ")", ".", "view", "(", "qlen", ",", "klen", ",", "x", ".", "size", "(", "2", ")", ",", "x", ".", "size", "(", "3", ")", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelMultiHeadAttn._rel_shift": [[512, 527], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x_padded.view.view.view", "x_padded[].view_as", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "x_padded[].view_as.size", "x_padded[].view_as.size", "x_padded[].view_as.size", "x_padded[].view_as.size", "x_padded[].view_as.size", "x_padded[].view_as.size", "x_padded[].view_as.size", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "x_padded[].view_as.size", "x_padded[].view_as.size"], "methods", ["None"], ["", "def", "_rel_shift", "(", "self", ",", "x", ",", "zero_triu", "=", "False", ")", ":", "\n", "        ", "zero_pad_shape", "=", "(", "x", ".", "size", "(", "0", ")", ",", "1", ")", "+", "x", ".", "size", "(", ")", "[", "2", ":", "]", "\n", "zero_pad", "=", "torch", ".", "zeros", "(", "zero_pad_shape", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", "\n", "x_padded", "=", "torch", ".", "cat", "(", "[", "zero_pad", ",", "x", "]", ",", "dim", "=", "1", ")", "\n", "\n", "x_padded_shape", "=", "(", "x", ".", "size", "(", "1", ")", "+", "1", ",", "x", ".", "size", "(", "0", ")", ")", "+", "x", ".", "size", "(", ")", "[", "2", ":", "]", "\n", "x_padded", "=", "x_padded", ".", "view", "(", "*", "x_padded_shape", ")", "\n", "\n", "x", "=", "x_padded", "[", "1", ":", "]", ".", "view_as", "(", "x", ")", "\n", "\n", "if", "zero_triu", ":", "\n", "            ", "ones", "=", "torch", ".", "ones", "(", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "1", ")", ")", ")", "\n", "x", "=", "x", "*", "torch", ".", "tril", "(", "ones", ",", "x", ".", "size", "(", "1", ")", "-", "x", ".", "size", "(", "0", ")", ")", "[", ":", ",", ":", ",", "None", ",", "None", "]", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelMultiHeadAttn.forward": [[528, 530], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "w", ",", "r", ",", "attn_mask", "=", "None", ",", "mems", "=", "None", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.__init__": [[532, 536], ["modeling_transfo_xl.RelMultiHeadAttn.__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "RelPartialLearnableMultiHeadAttn", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "r_net", "=", "nn", ".", "Linear", "(", "self", ".", "d_model", ",", "self", ".", "n_head", "*", "self", ".", "d_head", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.forward": [[537, 611], ["w_head_k.view.view.size", "w_head_q.view.view.view", "w_head_k.view.view.view", "w_head_v.view.view.view", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn._rel_shift", "attn_score.float().masked_fill().type_as.float().masked_fill().type_as.mul_", "torch.softmax", "torch.softmax", "torch.softmax", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.dropatt", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "attn_vec.contiguous().view.contiguous().view.contiguous().view", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.o_net", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.drop", "w.size", "r.size", "w.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.r_net", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.r_net", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "attn_mask.any().item", "attn_vec.contiguous().view.contiguous().view.size", "attn_vec.contiguous().view.contiguous().view.size", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.layer_norm", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.qkv_net", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.qkv_net", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.qkv_net", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.qkv_net", "attn_mask.dim", "attn_score.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill().type_as", "attn_vec.contiguous().view.contiguous().view.contiguous", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.layer_norm", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.layer_norm", "attn_mask.any", "attn_mask.dim", "attn_score.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill().type_as", "attn_score.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill", "attn_score.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill", "attn_score.float().masked_fill().type_as.float().masked_fill().type_as.float", "attn_score.float().masked_fill().type_as.float().masked_fill().type_as.float"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelMultiHeadAttn._rel_shift"], ["", "def", "forward", "(", "self", ",", "w", ",", "r", ",", "attn_mask", "=", "None", ",", "mems", "=", "None", ")", ":", "\n", "        ", "qlen", ",", "rlen", ",", "bsz", "=", "w", ".", "size", "(", "0", ")", ",", "r", ".", "size", "(", "0", ")", ",", "w", ".", "size", "(", "1", ")", "\n", "\n", "if", "mems", "is", "not", "None", ":", "\n", "            ", "cat", "=", "torch", ".", "cat", "(", "[", "mems", ",", "w", "]", ",", "0", ")", "\n", "if", "self", ".", "pre_lnorm", ":", "\n", "                ", "w_heads", "=", "self", ".", "qkv_net", "(", "self", ".", "layer_norm", "(", "cat", ")", ")", "\n", "", "else", ":", "\n", "                ", "w_heads", "=", "self", ".", "qkv_net", "(", "cat", ")", "\n", "", "r_head_k", "=", "self", ".", "r_net", "(", "r", ")", "\n", "\n", "w_head_q", ",", "w_head_k", ",", "w_head_v", "=", "torch", ".", "chunk", "(", "w_heads", ",", "3", ",", "dim", "=", "-", "1", ")", "\n", "w_head_q", "=", "w_head_q", "[", "-", "qlen", ":", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "pre_lnorm", ":", "\n", "                ", "w_heads", "=", "self", ".", "qkv_net", "(", "self", ".", "layer_norm", "(", "w", ")", ")", "\n", "", "else", ":", "\n", "                ", "w_heads", "=", "self", ".", "qkv_net", "(", "w", ")", "\n", "", "r_head_k", "=", "self", ".", "r_net", "(", "r", ")", "\n", "\n", "w_head_q", ",", "w_head_k", ",", "w_head_v", "=", "torch", ".", "chunk", "(", "w_heads", ",", "3", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "klen", "=", "w_head_k", ".", "size", "(", "0", ")", "\n", "\n", "w_head_q", "=", "w_head_q", ".", "view", "(", "qlen", ",", "bsz", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", "# qlen x bsz x n_head x d_head", "\n", "w_head_k", "=", "w_head_k", ".", "view", "(", "klen", ",", "bsz", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", "# qlen x bsz x n_head x d_head", "\n", "w_head_v", "=", "w_head_v", ".", "view", "(", "klen", ",", "bsz", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", "# qlen x bsz x n_head x d_head", "\n", "\n", "r_head_k", "=", "r_head_k", ".", "view", "(", "rlen", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", "# qlen x n_head x d_head", "\n", "\n", "#### compute attention score", "\n", "rw_head_q", "=", "w_head_q", "+", "self", ".", "r_w_bias", "# qlen x bsz x n_head x d_head", "\n", "AC", "=", "torch", ".", "einsum", "(", "'ibnd,jbnd->ijbn'", ",", "(", "rw_head_q", ",", "w_head_k", ")", ")", "# qlen x klen x bsz x n_head", "\n", "\n", "rr_head_q", "=", "w_head_q", "+", "self", ".", "r_r_bias", "\n", "BD", "=", "torch", ".", "einsum", "(", "'ibnd,jnd->ijbn'", ",", "(", "rr_head_q", ",", "r_head_k", ")", ")", "# qlen x klen x bsz x n_head", "\n", "BD", "=", "self", ".", "_rel_shift", "(", "BD", ")", "\n", "\n", "# [qlen x klen x bsz x n_head]", "\n", "attn_score", "=", "AC", "+", "BD", "\n", "attn_score", ".", "mul_", "(", "self", ".", "scale", ")", "\n", "\n", "#### compute attention probability", "\n", "if", "attn_mask", "is", "not", "None", "and", "attn_mask", ".", "any", "(", ")", ".", "item", "(", ")", ":", "\n", "            ", "if", "attn_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "                ", "attn_score", "=", "attn_score", ".", "float", "(", ")", ".", "masked_fill", "(", "\n", "attn_mask", "[", "None", ",", ":", ",", ":", ",", "None", "]", ",", "-", "1e30", ")", ".", "type_as", "(", "attn_score", ")", "\n", "", "elif", "attn_mask", ".", "dim", "(", ")", "==", "3", ":", "\n", "                ", "attn_score", "=", "attn_score", ".", "float", "(", ")", ".", "masked_fill", "(", "\n", "attn_mask", "[", ":", ",", ":", ",", ":", ",", "None", "]", ",", "-", "1e30", ")", ".", "type_as", "(", "attn_score", ")", "\n", "\n", "# [qlen x klen x bsz x n_head]", "\n", "", "", "attn_prob", "=", "F", ".", "softmax", "(", "attn_score", ",", "dim", "=", "1", ")", "\n", "attn_prob", "=", "self", ".", "dropatt", "(", "attn_prob", ")", "\n", "\n", "#### compute attention vector", "\n", "attn_vec", "=", "torch", ".", "einsum", "(", "'ijbn,jbnd->ibnd'", ",", "(", "attn_prob", ",", "w_head_v", ")", ")", "\n", "\n", "# [qlen x bsz x n_head x d_head]", "\n", "attn_vec", "=", "attn_vec", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "attn_vec", ".", "size", "(", "0", ")", ",", "attn_vec", ".", "size", "(", "1", ")", ",", "self", ".", "n_head", "*", "self", ".", "d_head", ")", "\n", "\n", "##### linear projection", "\n", "attn_out", "=", "self", ".", "o_net", "(", "attn_vec", ")", "\n", "attn_out", "=", "self", ".", "drop", "(", "attn_out", ")", "\n", "\n", "if", "self", ".", "pre_lnorm", ":", "\n", "##### residual connection", "\n", "            ", "output", "=", "w", "+", "attn_out", "\n", "", "else", ":", "\n", "##### residual connection + layer normalization", "\n", "            ", "output", "=", "self", ".", "layer_norm", "(", "w", "+", "attn_out", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelLearnableMultiHeadAttn.__init__": [[613, 615], ["modeling_transfo_xl.RelMultiHeadAttn.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "RelLearnableMultiHeadAttn", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelLearnableMultiHeadAttn.forward": [[616, 696], ["w_head_k.view.view.size", "w_head_q.view.view.view", "w_head_k.view.view.view", "w_head_v.view.view.view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "modeling_transfo_xl.RelLearnableMultiHeadAttn._rel_shift", "attn_score.mul_", "torch.softmax", "torch.softmax", "torch.softmax", "modeling_transfo_xl.RelLearnableMultiHeadAttn.dropatt", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "attn_vec.contiguous().view.contiguous().view.contiguous().view", "modeling_transfo_xl.RelLearnableMultiHeadAttn.o_net", "modeling_transfo_xl.RelLearnableMultiHeadAttn.drop", "w.size", "w.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.cat.size", "torch.cat.size", "torch.cat.size", "r_emb[].expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "r_bias[].expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attn_mask.any().item", "attn_vec.contiguous().view.contiguous().view.size", "attn_vec.contiguous().view.contiguous().view.size", "modeling_transfo_xl.RelLearnableMultiHeadAttn.layer_norm", "modeling_transfo_xl.RelLearnableMultiHeadAttn.qkv_net", "modeling_transfo_xl.RelLearnableMultiHeadAttn.qkv_net", "modeling_transfo_xl.RelLearnableMultiHeadAttn.qkv_net", "modeling_transfo_xl.RelLearnableMultiHeadAttn.qkv_net", "attn_mask.dim", "attn_score.masked_fill_", "attn_vec.contiguous().view.contiguous().view.contiguous", "modeling_transfo_xl.RelLearnableMultiHeadAttn.layer_norm", "modeling_transfo_xl.RelLearnableMultiHeadAttn.layer_norm", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "attn_mask.any", "attn_mask.dim", "attn_score.masked_fill_", "float", "float"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelMultiHeadAttn._rel_shift"], ["", "def", "forward", "(", "self", ",", "w", ",", "r_emb", ",", "r_w_bias", ",", "r_bias", ",", "attn_mask", "=", "None", ",", "mems", "=", "None", ")", ":", "\n", "# r_emb: [klen, n_head, d_head], used for term B", "\n", "# r_w_bias: [n_head, d_head], used for term C", "\n", "# r_bias: [klen, n_head], used for term D", "\n", "\n", "        ", "qlen", ",", "bsz", "=", "w", ".", "size", "(", "0", ")", ",", "w", ".", "size", "(", "1", ")", "\n", "\n", "if", "mems", "is", "not", "None", ":", "\n", "            ", "cat", "=", "torch", ".", "cat", "(", "[", "mems", ",", "w", "]", ",", "0", ")", "\n", "if", "self", ".", "pre_lnorm", ":", "\n", "                ", "w_heads", "=", "self", ".", "qkv_net", "(", "self", ".", "layer_norm", "(", "cat", ")", ")", "\n", "", "else", ":", "\n", "                ", "w_heads", "=", "self", ".", "qkv_net", "(", "cat", ")", "\n", "", "w_head_q", ",", "w_head_k", ",", "w_head_v", "=", "torch", ".", "chunk", "(", "w_heads", ",", "3", ",", "dim", "=", "-", "1", ")", "\n", "\n", "w_head_q", "=", "w_head_q", "[", "-", "qlen", ":", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "pre_lnorm", ":", "\n", "                ", "w_heads", "=", "self", ".", "qkv_net", "(", "self", ".", "layer_norm", "(", "w", ")", ")", "\n", "", "else", ":", "\n", "                ", "w_heads", "=", "self", ".", "qkv_net", "(", "w", ")", "\n", "", "w_head_q", ",", "w_head_k", ",", "w_head_v", "=", "torch", ".", "chunk", "(", "w_heads", ",", "3", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "klen", "=", "w_head_k", ".", "size", "(", "0", ")", "\n", "\n", "w_head_q", "=", "w_head_q", ".", "view", "(", "qlen", ",", "bsz", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", "\n", "w_head_k", "=", "w_head_k", ".", "view", "(", "klen", ",", "bsz", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", "\n", "w_head_v", "=", "w_head_v", ".", "view", "(", "klen", ",", "bsz", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", "\n", "\n", "if", "klen", ">", "r_emb", ".", "size", "(", "0", ")", ":", "\n", "            ", "r_emb_pad", "=", "r_emb", "[", "0", ":", "1", "]", ".", "expand", "(", "klen", "-", "r_emb", ".", "size", "(", "0", ")", ",", "-", "1", ",", "-", "1", ")", "\n", "r_emb", "=", "torch", ".", "cat", "(", "[", "r_emb_pad", ",", "r_emb", "]", ",", "0", ")", "\n", "r_bias_pad", "=", "r_bias", "[", "0", ":", "1", "]", ".", "expand", "(", "klen", "-", "r_bias", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "r_bias", "=", "torch", ".", "cat", "(", "[", "r_bias_pad", ",", "r_bias", "]", ",", "0", ")", "\n", "", "else", ":", "\n", "            ", "r_emb", "=", "r_emb", "[", "-", "klen", ":", "]", "\n", "r_bias", "=", "r_bias", "[", "-", "klen", ":", "]", "\n", "\n", "#### compute attention score", "\n", "", "rw_head_q", "=", "w_head_q", "+", "r_w_bias", "[", "None", "]", "# qlen x bsz x n_head x d_head", "\n", "\n", "AC", "=", "torch", ".", "einsum", "(", "'ibnd,jbnd->ijbn'", ",", "(", "rw_head_q", ",", "w_head_k", ")", ")", "# qlen x klen x bsz x n_head", "\n", "B_", "=", "torch", ".", "einsum", "(", "'ibnd,jnd->ijbn'", ",", "(", "w_head_q", ",", "r_emb", ")", ")", "# qlen x klen x bsz x n_head", "\n", "D_", "=", "r_bias", "[", "None", ",", ":", ",", "None", "]", "# 1    x klen x 1   x n_head", "\n", "BD", "=", "self", ".", "_rel_shift", "(", "B_", "+", "D_", ")", "\n", "\n", "# [qlen x klen x bsz x n_head]", "\n", "attn_score", "=", "AC", "+", "BD", "\n", "attn_score", ".", "mul_", "(", "self", ".", "scale", ")", "\n", "\n", "#### compute attention probability", "\n", "if", "attn_mask", "is", "not", "None", "and", "attn_mask", ".", "any", "(", ")", ".", "item", "(", ")", ":", "\n", "            ", "if", "attn_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "                ", "attn_score", ".", "masked_fill_", "(", "attn_mask", "[", "None", ",", ":", ",", ":", ",", "None", "]", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "", "elif", "attn_mask", ".", "dim", "(", ")", "==", "3", ":", "\n", "                ", "attn_score", ".", "masked_fill_", "(", "attn_mask", "[", ":", ",", ":", ",", ":", ",", "None", "]", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "\n", "# [qlen x klen x bsz x n_head]", "\n", "", "", "attn_prob", "=", "F", ".", "softmax", "(", "attn_score", ",", "dim", "=", "1", ")", "\n", "attn_prob", "=", "self", ".", "dropatt", "(", "attn_prob", ")", "\n", "\n", "#### compute attention vector", "\n", "attn_vec", "=", "torch", ".", "einsum", "(", "'ijbn,jbnd->ibnd'", ",", "(", "attn_prob", ",", "w_head_v", ")", ")", "\n", "\n", "# [qlen x bsz x n_head x d_head]", "\n", "attn_vec", "=", "attn_vec", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "attn_vec", ".", "size", "(", "0", ")", ",", "attn_vec", ".", "size", "(", "1", ")", ",", "self", ".", "n_head", "*", "self", ".", "d_head", ")", "\n", "\n", "##### linear projection", "\n", "attn_out", "=", "self", ".", "o_net", "(", "attn_vec", ")", "\n", "attn_out", "=", "self", ".", "drop", "(", "attn_out", ")", "\n", "\n", "if", "self", ".", "pre_lnorm", ":", "\n", "##### residual connection", "\n", "            ", "output", "=", "w", "+", "attn_out", "\n", "", "else", ":", "\n", "##### residual connection + layer normalization", "\n", "            ", "output", "=", "self", ".", "layer_norm", "(", "w", "+", "attn_out", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.DecoderLayer.__init__": [[698, 704], ["torch.Module.__init__", "modeling_transfo_xl.MultiHeadAttn", "modeling_transfo_xl.PositionwiseFF", "kwargs.get"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_head", ",", "d_model", ",", "d_head", ",", "d_inner", ",", "dropout", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "DecoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "dec_attn", "=", "MultiHeadAttn", "(", "n_head", ",", "d_model", ",", "d_head", ",", "dropout", ",", "**", "kwargs", ")", "\n", "self", ".", "pos_ff", "=", "PositionwiseFF", "(", "d_model", ",", "d_inner", ",", "dropout", ",", "\n", "pre_lnorm", "=", "kwargs", ".", "get", "(", "'pre_lnorm'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.DecoderLayer.forward": [[705, 712], ["modeling_transfo_xl.DecoderLayer.dec_attn", "modeling_transfo_xl.DecoderLayer.pos_ff"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "dec_inp", ",", "dec_attn_mask", "=", "None", ",", "mems", "=", "None", ")", ":", "\n", "\n", "        ", "output", "=", "self", ".", "dec_attn", "(", "dec_inp", ",", "attn_mask", "=", "dec_attn_mask", ",", "\n", "mems", "=", "mems", ")", "\n", "output", "=", "self", ".", "pos_ff", "(", "output", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelLearnableDecoderLayer.__init__": [[714, 722], ["torch.Module.__init__", "modeling_transfo_xl.RelLearnableMultiHeadAttn", "modeling_transfo_xl.PositionwiseFF", "kwargs.get"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_head", ",", "d_model", ",", "d_head", ",", "d_inner", ",", "dropout", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "RelLearnableDecoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "dec_attn", "=", "RelLearnableMultiHeadAttn", "(", "n_head", ",", "d_model", ",", "d_head", ",", "dropout", ",", "\n", "**", "kwargs", ")", "\n", "self", ".", "pos_ff", "=", "PositionwiseFF", "(", "d_model", ",", "d_inner", ",", "dropout", ",", "\n", "pre_lnorm", "=", "kwargs", ".", "get", "(", "'pre_lnorm'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelLearnableDecoderLayer.forward": [[723, 731], ["modeling_transfo_xl.RelLearnableDecoderLayer.dec_attn", "modeling_transfo_xl.RelLearnableDecoderLayer.pos_ff"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "dec_inp", ",", "r_emb", ",", "r_w_bias", ",", "r_bias", ",", "dec_attn_mask", "=", "None", ",", "mems", "=", "None", ")", ":", "\n", "\n", "        ", "output", "=", "self", ".", "dec_attn", "(", "dec_inp", ",", "r_emb", ",", "r_w_bias", ",", "r_bias", ",", "\n", "attn_mask", "=", "dec_attn_mask", ",", "\n", "mems", "=", "mems", ")", "\n", "output", "=", "self", ".", "pos_ff", "(", "output", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelPartialLearnableDecoderLayer.__init__": [[733, 741], ["torch.Module.__init__", "modeling_transfo_xl.RelPartialLearnableMultiHeadAttn", "modeling_transfo_xl.PositionwiseFF", "kwargs.get"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_head", ",", "d_model", ",", "d_head", ",", "d_inner", ",", "dropout", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "RelPartialLearnableDecoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "dec_attn", "=", "RelPartialLearnableMultiHeadAttn", "(", "n_head", ",", "d_model", ",", "\n", "d_head", ",", "dropout", ",", "**", "kwargs", ")", "\n", "self", ".", "pos_ff", "=", "PositionwiseFF", "(", "d_model", ",", "d_inner", ",", "dropout", ",", "\n", "pre_lnorm", "=", "kwargs", ".", "get", "(", "'pre_lnorm'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.RelPartialLearnableDecoderLayer.forward": [[742, 750], ["modeling_transfo_xl.RelPartialLearnableDecoderLayer.dec_attn", "modeling_transfo_xl.RelPartialLearnableDecoderLayer.pos_ff"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "dec_inp", ",", "r", ",", "dec_attn_mask", "=", "None", ",", "mems", "=", "None", ")", ":", "\n", "\n", "        ", "output", "=", "self", ".", "dec_attn", "(", "dec_inp", ",", "r", ",", "\n", "attn_mask", "=", "dec_attn_mask", ",", "\n", "mems", "=", "mems", ")", "\n", "output", "=", "self", ".", "pos_ff", "(", "output", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.AdaptiveEmbedding.__init__": [[753, 782], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "modeling_transfo_xl.AdaptiveEmbedding.emb_layers.append", "range", "torch.Embedding", "torch.Embedding", "torch.Embedding", "modeling_transfo_xl.AdaptiveEmbedding.emb_projs.append", "len", "modeling_transfo_xl.AdaptiveEmbedding.emb_layers.append", "modeling_transfo_xl.AdaptiveEmbedding.emb_projs.append", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_token", ",", "d_embed", ",", "d_proj", ",", "cutoffs", ",", "div_val", "=", "1", ",", "\n", "sample_softmax", "=", "False", ")", ":", "\n", "        ", "super", "(", "AdaptiveEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "n_token", "=", "n_token", "\n", "self", ".", "d_embed", "=", "d_embed", "\n", "\n", "self", ".", "cutoffs", "=", "cutoffs", "+", "[", "n_token", "]", "\n", "self", ".", "div_val", "=", "div_val", "\n", "self", ".", "d_proj", "=", "d_proj", "\n", "\n", "self", ".", "emb_scale", "=", "d_proj", "**", "0.5", "\n", "\n", "self", ".", "cutoff_ends", "=", "[", "0", "]", "+", "self", ".", "cutoffs", "\n", "\n", "self", ".", "emb_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "emb_projs", "=", "nn", ".", "ParameterList", "(", ")", "\n", "if", "div_val", "==", "1", ":", "\n", "            ", "self", ".", "emb_layers", ".", "append", "(", "\n", "nn", ".", "Embedding", "(", "n_token", ",", "d_embed", ",", "sparse", "=", "sample_softmax", ">", "0", ")", "\n", ")", "\n", "if", "d_proj", "!=", "d_embed", ":", "\n", "                ", "self", ".", "emb_projs", ".", "append", "(", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "d_proj", ",", "d_embed", ")", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoffs", ")", ")", ":", "\n", "                ", "l_idx", ",", "r_idx", "=", "self", ".", "cutoff_ends", "[", "i", "]", ",", "self", ".", "cutoff_ends", "[", "i", "+", "1", "]", "\n", "d_emb_i", "=", "d_embed", "//", "(", "div_val", "**", "i", ")", "\n", "self", ".", "emb_layers", ".", "append", "(", "nn", ".", "Embedding", "(", "r_idx", "-", "l_idx", ",", "d_emb_i", ")", ")", "\n", "self", ".", "emb_projs", ".", "append", "(", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "d_proj", ",", "d_emb_i", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.AdaptiveEmbedding.forward": [[783, 814], ["torch.linear.mul_", "next", "inp.view", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range", "torch.zeros.view", "torch.zeros.view", "torch.zeros.view", "torch.linear", "torch.linear", "torch.linear", "modeling_transfo_xl.AdaptiveEmbedding.parameters", "len", "mask_i.nonzero().squeeze", "torch.linear", "torch.linear", "torch.linear", "torch.zeros.index_copy_", "torch.zeros.index_copy_", "torch.zeros.index_copy_", "inp.size", "inp.view.size", "mask_i.nonzero().squeeze.numel", "inp.view.index_select", "mask_i.nonzero"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "inp", ")", ":", "\n", "        ", "if", "self", ".", "div_val", "==", "1", ":", "\n", "            ", "embed", "=", "self", ".", "emb_layers", "[", "0", "]", "(", "inp", ")", "\n", "if", "self", ".", "d_proj", "!=", "self", ".", "d_embed", ":", "\n", "                ", "embed", "=", "F", ".", "linear", "(", "embed", ",", "self", ".", "emb_projs", "[", "0", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "param", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", "\n", "inp_flat", "=", "inp", ".", "view", "(", "-", "1", ")", "\n", "emb_flat", "=", "torch", ".", "zeros", "(", "[", "inp_flat", ".", "size", "(", "0", ")", ",", "self", ".", "d_proj", "]", ",", "\n", "dtype", "=", "param", ".", "dtype", ",", "device", "=", "param", ".", "device", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoffs", ")", ")", ":", "\n", "                ", "l_idx", ",", "r_idx", "=", "self", ".", "cutoff_ends", "[", "i", "]", ",", "self", ".", "cutoff_ends", "[", "i", "+", "1", "]", "\n", "\n", "mask_i", "=", "(", "inp_flat", ">=", "l_idx", ")", "&", "(", "inp_flat", "<", "r_idx", ")", "\n", "indices_i", "=", "mask_i", ".", "nonzero", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "if", "indices_i", ".", "numel", "(", ")", "==", "0", ":", "\n", "                    ", "continue", "\n", "\n", "", "inp_i", "=", "inp_flat", ".", "index_select", "(", "0", ",", "indices_i", ")", "-", "l_idx", "\n", "emb_i", "=", "self", ".", "emb_layers", "[", "i", "]", "(", "inp_i", ")", "\n", "emb_i", "=", "F", ".", "linear", "(", "emb_i", ",", "self", ".", "emb_projs", "[", "i", "]", ")", "\n", "\n", "emb_flat", ".", "index_copy_", "(", "0", ",", "indices_i", ",", "emb_i", ")", "\n", "\n", "", "embed_shape", "=", "inp", ".", "size", "(", ")", "+", "(", "self", ".", "d_proj", ",", ")", "\n", "embed", "=", "emb_flat", ".", "view", "(", "embed_shape", ")", "\n", "\n", "", "embed", ".", "mul_", "(", "self", ".", "emb_scale", ")", "\n", "\n", "return", "embed", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLPreTrainedModel.__init__": [[820, 830], ["torch.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "TransfoXLPreTrainedModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "TransfoXLConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `TransfoXLConfig`. \"", "\n", "\"To create a rpbert from a pretrained rpbert use \"", "\n", "\"`rpbert = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLPreTrainedModel.init_weight": [[831, 836], ["torch.init.uniform_", "torch.init.uniform_", "torch.init.uniform_", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_"], "methods", ["None"], ["", "def", "init_weight", "(", "self", ",", "weight", ")", ":", "\n", "        ", "if", "self", ".", "config", ".", "init", "==", "'uniform'", ":", "\n", "            ", "nn", ".", "init", ".", "uniform_", "(", "weight", ",", "-", "self", ".", "config", ".", "init_range", ",", "self", ".", "config", ".", "init_range", ")", "\n", "", "elif", "self", ".", "config", ".", "init", "==", "'normal'", ":", "\n", "            ", "nn", ".", "init", ".", "normal_", "(", "weight", ",", "0.0", ",", "self", ".", "config", ".", "init_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLPreTrainedModel.init_bias": [[837, 839], ["torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "", "def", "init_bias", "(", "self", ",", "bias", ")", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "bias", ",", "0.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLPreTrainedModel.init_weights": [[840, 880], ["classname.find", "hasattr", "modeling_transfo_xl.TransfoXLPreTrainedModel.init_weight", "hasattr", "modeling_transfo_xl.TransfoXLPreTrainedModel.init_bias", "classname.find", "hasattr", "range", "classname.find", "hasattr", "len", "modeling_transfo_xl.TransfoXLPreTrainedModel.init_weight", "classname.find", "hasattr", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "hasattr", "modeling_transfo_xl.TransfoXLPreTrainedModel.init_weight", "hasattr", "modeling_transfo_xl.TransfoXLPreTrainedModel.init_bias", "range", "classname.find", "hasattr", "len", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "hasattr", "modeling_transfo_xl.TransfoXLPreTrainedModel.init_bias", "classname.find", "hasattr", "hasattr", "hasattr", "hasattr", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "modeling_transfo_xl.TransfoXLPreTrainedModel.init_weight", "modeling_transfo_xl.TransfoXLPreTrainedModel.init_weight", "modeling_transfo_xl.TransfoXLPreTrainedModel.init_weight", "modeling_transfo_xl.TransfoXLPreTrainedModel.init_bias"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.init_weight", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLPreTrainedModel.init_bias", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.init_weight", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.init_weight", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLPreTrainedModel.init_bias", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLPreTrainedModel.init_bias", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.init_weight", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.init_weight", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.init_weight", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLPreTrainedModel.init_bias"], ["", "def", "init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "classname", "=", "m", ".", "__class__", ".", "__name__", "\n", "if", "classname", ".", "find", "(", "'Linear'", ")", "!=", "-", "1", ":", "\n", "            ", "if", "hasattr", "(", "m", ",", "'weight'", ")", "and", "m", ".", "weight", "is", "not", "None", ":", "\n", "                ", "self", ".", "init_weight", "(", "m", ".", "weight", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'bias'", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "self", ".", "init_bias", "(", "m", ".", "bias", ")", "\n", "", "", "elif", "classname", ".", "find", "(", "'AdaptiveEmbedding'", ")", "!=", "-", "1", ":", "\n", "            ", "if", "hasattr", "(", "m", ",", "'emb_projs'", ")", ":", "\n", "                ", "for", "i", "in", "range", "(", "len", "(", "m", ".", "emb_projs", ")", ")", ":", "\n", "                    ", "if", "m", ".", "emb_projs", "[", "i", "]", "is", "not", "None", ":", "\n", "                        ", "nn", ".", "init", ".", "normal_", "(", "m", ".", "emb_projs", "[", "i", "]", ",", "0.0", ",", "self", ".", "config", ".", "proj_init_std", ")", "\n", "", "", "", "", "elif", "classname", ".", "find", "(", "'Embedding'", ")", "!=", "-", "1", ":", "\n", "            ", "if", "hasattr", "(", "m", ",", "'weight'", ")", ":", "\n", "                ", "self", ".", "init_weight", "(", "m", ".", "weight", ")", "\n", "", "", "elif", "classname", ".", "find", "(", "'ProjectedAdaptiveLogSoftmax'", ")", "!=", "-", "1", ":", "\n", "            ", "if", "hasattr", "(", "m", ",", "'cluster_weight'", ")", "and", "m", ".", "cluster_weight", "is", "not", "None", ":", "\n", "                ", "self", ".", "init_weight", "(", "m", ".", "cluster_weight", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'cluster_bias'", ")", "and", "m", ".", "cluster_bias", "is", "not", "None", ":", "\n", "                ", "self", ".", "init_bias", "(", "m", ".", "cluster_bias", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'out_projs'", ")", ":", "\n", "                ", "for", "i", "in", "range", "(", "len", "(", "m", ".", "out_projs", ")", ")", ":", "\n", "                    ", "if", "m", ".", "out_projs", "[", "i", "]", "is", "not", "None", ":", "\n", "                        ", "nn", ".", "init", ".", "normal_", "(", "m", ".", "out_projs", "[", "i", "]", ",", "0.0", ",", "self", ".", "config", ".", "proj_init_std", ")", "\n", "", "", "", "", "elif", "classname", ".", "find", "(", "'LayerNorm'", ")", "!=", "-", "1", ":", "\n", "            ", "if", "hasattr", "(", "m", ",", "'weight'", ")", ":", "\n", "                ", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "1.0", ",", "self", ".", "config", ".", "init_std", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'bias'", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "self", ".", "init_bias", "(", "m", ".", "bias", ")", "\n", "", "", "elif", "classname", ".", "find", "(", "'TransformerLM'", ")", "!=", "-", "1", ":", "\n", "            ", "if", "hasattr", "(", "m", ",", "'r_emb'", ")", ":", "\n", "                ", "self", ".", "init_weight", "(", "m", ".", "r_emb", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'r_w_bias'", ")", ":", "\n", "                ", "self", ".", "init_weight", "(", "m", ".", "r_w_bias", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'r_r_bias'", ")", ":", "\n", "                ", "self", ".", "init_weight", "(", "m", ".", "r_r_bias", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'r_bias'", ")", ":", "\n", "                ", "self", ".", "init_bias", "(", "m", ".", "r_bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLPreTrainedModel.set_num_special_tokens": [[881, 883], ["None"], "methods", ["None"], ["", "", "", "def", "set_num_special_tokens", "(", "self", ",", "num_special_tokens", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLPreTrainedModel.from_pretrained": [[884, 981], ["modeling_transfo_xl.TransfoXLConfig.from_json_file", "logger.info", "cls", "getattr", "torch.load.copy", "torch.load.copy", "torch.load.copy", "modeling_transfo_xl.TransfoXLPreTrainedModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.from_json_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "\n", "from_tf", "=", "False", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a TransfoXLPreTrainedModel from a pre-trained rpbert file or a pytorch state dict.\n        Download and cache the pre-trained rpbert file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained rpbert to load selected in the list of:\n                    . `transfo-xl`\n                - a path or url to a pretrained rpbert archive containing:\n                    . `transfo_xl_config.json` a configuration file for the rpbert\n                    . `pytorch_model.bin` a PyTorch dump of a TransfoXLModel instance\n                - a path or url to a pretrained rpbert archive containing:\n                    . `bert_config.json` a configuration file for the rpbert\n                    . `rpbert.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "config_file", "=", "PRETRAINED_CONFIG_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "WEIGHTS_NAME", ")", "\n", "config_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "CONFIG_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "resolved_config_file", "=", "cached_path", "(", "config_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in rpbert name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find files {} and {} \"", "\n", "\"at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", "archive_file", ",", "config_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", "and", "resolved_config_file", "==", "config_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading weights file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "logger", ".", "info", "(", "\"loading configuration file {}\"", ".", "format", "(", "config_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading weights file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "logger", ".", "info", "(", "\"loading configuration file {} from cache at {}\"", ".", "format", "(", "\n", "config_file", ",", "resolved_config_file", ")", ")", "\n", "# Load config", "\n", "", "config", "=", "TransfoXLConfig", ".", "from_json_file", "(", "resolved_config_file", ")", "\n", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "# Instantiate rpbert.", "\n", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", "and", "not", "from_tf", ":", "\n", "            ", "state_dict", "=", "torch", ".", "load", "(", "resolved_archive_file", ",", "map_location", "=", "'cpu'", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "None", ")", "\n", "", "if", "from_tf", ":", "\n", "# Directly load from a TensorFlow checkpoint", "\n", "            ", "return", "load_tf_weights_in_transfo_xl", "(", "model", ",", "config", ",", "pretrained_model_name_or_path", ")", "\n", "\n", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "\n", "", "", "", "start_prefix", "=", "''", "\n", "if", "not", "hasattr", "(", "model", ",", "'transformer'", ")", "and", "any", "(", "s", ".", "startswith", "(", "'transformer.'", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "start_prefix", "=", "'transformer.'", "\n", "", "load", "(", "model", ",", "prefix", "=", "start_prefix", ")", "\n", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights of {} not initialized from pretrained rpbert: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights from pretrained rpbert not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Error(s) in loading state_dict for {}:\\n\\t{}'", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "\"\\n\\t\"", ".", "join", "(", "error_msgs", ")", ")", ")", "\n", "# Make sure we are still sharing the input and output embeddings", "\n", "", "if", "hasattr", "(", "model", ",", "'tie_weights'", ")", ":", "\n", "            ", "model", ".", "tie_weights", "(", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLModel.__init__": [[1023, 1097], ["modeling_transfo_xl.TransfoXLPreTrainedModel.__init__", "modeling_transfo_xl.AdaptiveEmbedding", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "modeling_transfo_xl.TransfoXLModel.apply", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "range", "modeling_transfo_xl.PositionalEmbedding", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "modeling_transfo_xl.TransfoXLModel.layers.append", "range", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "modeling_transfo_xl.RelPartialLearnableDecoderLayer", "modeling_transfo_xl.TransfoXLModel.layers.append", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "modeling_transfo_xl.PositionalEmbedding", "modeling_transfo_xl.RelLearnableDecoderLayer", "modeling_transfo_xl.TransfoXLModel.layers.append", "torch.Parameter", "torch.Parameter", "torch.Parameter", "modeling_transfo_xl.DecoderLayer", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "TransfoXLModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "n_token", "=", "config", ".", "n_token", "\n", "\n", "self", ".", "d_embed", "=", "config", ".", "d_embed", "\n", "self", ".", "d_model", "=", "config", ".", "d_model", "\n", "self", ".", "n_head", "=", "config", ".", "n_head", "\n", "self", ".", "d_head", "=", "config", ".", "d_head", "\n", "\n", "self", ".", "word_emb", "=", "AdaptiveEmbedding", "(", "config", ".", "n_token", ",", "config", ".", "d_embed", ",", "config", ".", "d_model", ",", "config", ".", "cutoffs", ",", "\n", "div_val", "=", "config", ".", "div_val", ")", "\n", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "config", ".", "dropout", ")", "\n", "\n", "self", ".", "n_layer", "=", "config", ".", "n_layer", "\n", "\n", "self", ".", "tgt_len", "=", "config", ".", "tgt_len", "\n", "self", ".", "mem_len", "=", "config", ".", "mem_len", "\n", "self", ".", "ext_len", "=", "config", ".", "ext_len", "\n", "self", ".", "max_klen", "=", "config", ".", "tgt_len", "+", "config", ".", "ext_len", "+", "config", ".", "mem_len", "\n", "\n", "self", ".", "attn_type", "=", "config", ".", "attn_type", "\n", "\n", "if", "not", "config", ".", "untie_r", ":", "\n", "            ", "self", ".", "r_w_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "self", ".", "r_r_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "\n", "", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "if", "config", ".", "attn_type", "==", "0", ":", "# the default attention", "\n", "            ", "for", "i", "in", "range", "(", "config", ".", "n_layer", ")", ":", "\n", "                ", "self", ".", "layers", ".", "append", "(", "\n", "RelPartialLearnableDecoderLayer", "(", "\n", "config", ".", "n_head", ",", "config", ".", "d_model", ",", "config", ".", "d_head", ",", "config", ".", "d_inner", ",", "config", ".", "dropout", ",", "\n", "tgt_len", "=", "config", ".", "tgt_len", ",", "ext_len", "=", "config", ".", "ext_len", ",", "mem_len", "=", "config", ".", "mem_len", ",", "\n", "dropatt", "=", "config", ".", "dropatt", ",", "pre_lnorm", "=", "config", ".", "pre_lnorm", ",", "\n", "r_w_bias", "=", "None", "if", "config", ".", "untie_r", "else", "self", ".", "r_w_bias", ",", "\n", "r_r_bias", "=", "None", "if", "config", ".", "untie_r", "else", "self", ".", "r_r_bias", ")", "\n", ")", "\n", "", "", "elif", "config", ".", "attn_type", "==", "1", ":", "# learnable embeddings", "\n", "            ", "for", "i", "in", "range", "(", "config", ".", "n_layer", ")", ":", "\n", "                ", "self", ".", "layers", ".", "append", "(", "\n", "RelLearnableDecoderLayer", "(", "\n", "config", ".", "n_head", ",", "config", ".", "d_model", ",", "config", ".", "d_head", ",", "config", ".", "d_inner", ",", "config", ".", "dropout", ",", "\n", "tgt_len", "=", "config", ".", "tgt_len", ",", "ext_len", "=", "config", ".", "ext_len", ",", "mem_len", "=", "config", ".", "mem_len", ",", "\n", "dropatt", "=", "config", ".", "dropatt", ",", "pre_lnorm", "=", "config", ".", "pre_lnorm", ",", "\n", "r_w_bias", "=", "None", "if", "config", ".", "untie_r", "else", "self", ".", "r_w_bias", ",", "\n", "r_r_bias", "=", "None", "if", "config", ".", "untie_r", "else", "self", ".", "r_r_bias", ")", "\n", ")", "\n", "", "", "elif", "config", ".", "attn_type", "in", "[", "2", ",", "3", "]", ":", "# absolute embeddings", "\n", "            ", "for", "i", "in", "range", "(", "config", ".", "n_layer", ")", ":", "\n", "                ", "self", ".", "layers", ".", "append", "(", "\n", "DecoderLayer", "(", "\n", "config", ".", "n_head", ",", "config", ".", "d_model", ",", "config", ".", "d_head", ",", "config", ".", "d_inner", ",", "config", ".", "dropout", ",", "\n", "dropatt", "=", "config", ".", "dropatt", ",", "pre_lnorm", "=", "config", ".", "pre_lnorm", ",", "\n", "r_w_bias", "=", "None", "if", "config", ".", "untie_r", "else", "self", ".", "r_w_bias", ",", "\n", "r_r_bias", "=", "None", "if", "config", ".", "untie_r", "else", "self", ".", "r_r_bias", ")", "\n", ")", "\n", "\n", "", "", "self", ".", "same_length", "=", "config", ".", "same_length", "\n", "self", ".", "clamp_len", "=", "config", ".", "clamp_len", "\n", "\n", "if", "self", ".", "attn_type", "==", "0", ":", "# default attention", "\n", "            ", "self", ".", "pos_emb", "=", "PositionalEmbedding", "(", "self", ".", "d_model", ")", "\n", "", "elif", "self", ".", "attn_type", "==", "1", ":", "# learnable", "\n", "            ", "self", ".", "r_emb", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "\n", "self", ".", "n_layer", ",", "self", ".", "max_klen", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "self", ".", "r_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "\n", "self", ".", "n_layer", ",", "self", ".", "max_klen", ",", "self", ".", "n_head", ")", ")", "\n", "", "elif", "self", ".", "attn_type", "==", "2", ":", "# absolute standard", "\n", "            ", "self", ".", "pos_emb", "=", "PositionalEmbedding", "(", "self", ".", "d_model", ")", "\n", "", "elif", "self", ".", "attn_type", "==", "3", ":", "# absolute deeper SA", "\n", "            ", "self", ".", "r_emb", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "\n", "self", ".", "n_layer", ",", "self", ".", "max_klen", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLModel.backward_compatible": [[1098, 1100], ["None"], "methods", ["None"], ["", "def", "backward_compatible", "(", "self", ")", ":", "\n", "        ", "self", ".", "sample_softmax", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLModel.reset_length": [[1102, 1106], ["None"], "methods", ["None"], ["", "def", "reset_length", "(", "self", ",", "tgt_len", ",", "ext_len", ",", "mem_len", ")", ":", "\n", "        ", "self", ".", "tgt_len", "=", "tgt_len", "\n", "self", ".", "mem_len", "=", "mem_len", "\n", "self", ".", "ext_len", "=", "ext_len", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLModel.init_mems": [[1107, 1119], ["next", "range", "modeling_transfo_xl.TransfoXLModel.parameters", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "mems.append", "data.size"], "methods", ["None"], ["", "def", "init_mems", "(", "self", ",", "data", ")", ":", "\n", "        ", "if", "self", ".", "mem_len", ">", "0", ":", "\n", "            ", "mems", "=", "[", "]", "\n", "param", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "n_layer", ")", ":", "\n", "                ", "empty", "=", "torch", ".", "zeros", "(", "self", ".", "mem_len", ",", "data", ".", "size", "(", "1", ")", ",", "self", ".", "config", ".", "d_model", ",", "\n", "dtype", "=", "param", ".", "dtype", ",", "device", "=", "param", ".", "device", ")", "\n", "mems", ".", "append", "(", "empty", ")", "\n", "\n", "", "return", "mems", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLModel._update_mems": [[1120, 1142], ["len", "len", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "max", "range", "max", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "new_mems.append", "cat[].detach"], "methods", ["None"], ["", "", "def", "_update_mems", "(", "self", ",", "hids", ",", "mems", ",", "qlen", ",", "mlen", ")", ":", "\n", "# does not deal with None", "\n", "        ", "if", "mems", "is", "None", ":", "return", "None", "\n", "\n", "# mems is not None", "\n", "assert", "len", "(", "hids", ")", "==", "len", "(", "mems", ")", ",", "'len(hids) != len(mems)'", "\n", "\n", "# There are `mlen + qlen` steps that can be cached into mems", "\n", "# For the next step, the last `ext_len` of the `qlen` tokens", "\n", "# will be used as the extended context. Hence, we only cache", "\n", "# the tokens from `mlen + qlen - self.ext_len - self.mem_len`", "\n", "# to `mlen + qlen - self.ext_len`.", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "new_mems", "=", "[", "]", "\n", "end_idx", "=", "mlen", "+", "max", "(", "0", ",", "qlen", "-", "0", "-", "self", ".", "ext_len", ")", "\n", "beg_idx", "=", "max", "(", "0", ",", "end_idx", "-", "self", ".", "mem_len", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "hids", ")", ")", ":", "\n", "\n", "                ", "cat", "=", "torch", ".", "cat", "(", "[", "mems", "[", "i", "]", ",", "hids", "[", "i", "]", "]", ",", "dim", "=", "0", ")", "\n", "new_mems", ".", "append", "(", "cat", "[", "beg_idx", ":", "end_idx", "]", ".", "detach", "(", ")", ")", "\n", "\n", "", "", "return", "new_mems", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLModel._forward": [[1143, 1232], ["dec_inp.size", "modeling_transfo_xl.TransfoXLModel.word_emb", "modeling_transfo_xl.TransfoXLModel.drop", "modeling_transfo_xl.TransfoXLModel._update_mems", "mems[].size", "modeling_transfo_xl.TransfoXLModel.new_ones", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "modeling_transfo_xl.TransfoXLModel.pos_emb", "modeling_transfo_xl.TransfoXLModel.drop", "modeling_transfo_xl.TransfoXLModel.drop", "enumerate", "torch.triu().byte", "torch.triu().byte", "torch.triu().byte", "torch.triu().byte", "torch.triu().byte", "torch.triu().byte", "torch.triu().byte", "torch.triu().byte", "torch.triu().byte", "torch.arange.clamp_", "torch.arange.clamp_", "torch.arange.clamp_", "hids.append", "layer", "modeling_transfo_xl.TransfoXLModel.drop", "enumerate", "hids.append", "layer", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "modeling_transfo_xl.TransfoXLModel.pos_emb", "modeling_transfo_xl.TransfoXLModel.drop", "enumerate", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.arange.clamp_", "torch.arange.clamp_", "torch.arange.clamp_", "hids.append", "layer", "modeling_transfo_xl.TransfoXLModel.drop", "enumerate", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "modeling_transfo_xl.TransfoXLModel.new_ones", "hids.append", "[].view", "layer", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.view", "torch.cat.view", "torch.cat.view", "cur_emb[].expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLModel._update_mems"], ["", "def", "_forward", "(", "self", ",", "dec_inp", ",", "mems", "=", "None", ")", ":", "\n", "        ", "qlen", ",", "bsz", "=", "dec_inp", ".", "size", "(", ")", "\n", "\n", "word_emb", "=", "self", ".", "word_emb", "(", "dec_inp", ")", "\n", "\n", "mlen", "=", "mems", "[", "0", "]", ".", "size", "(", "0", ")", "if", "mems", "is", "not", "None", "else", "0", "\n", "klen", "=", "mlen", "+", "qlen", "\n", "if", "self", ".", "same_length", ":", "\n", "            ", "all_ones", "=", "word_emb", ".", "new_ones", "(", "qlen", ",", "klen", ")", "\n", "mask_len", "=", "klen", "-", "self", ".", "mem_len", "\n", "if", "mask_len", ">", "0", ":", "\n", "                ", "mask_shift_len", "=", "qlen", "-", "mask_len", "\n", "", "else", ":", "\n", "                ", "mask_shift_len", "=", "qlen", "\n", "", "dec_attn_mask", "=", "(", "torch", ".", "triu", "(", "all_ones", ",", "1", "+", "mlen", ")", "\n", "+", "torch", ".", "tril", "(", "all_ones", ",", "-", "mask_shift_len", ")", ")", ".", "byte", "(", ")", "[", ":", ",", ":", ",", "None", "]", "# -1", "\n", "", "else", ":", "\n", "            ", "dec_attn_mask", "=", "torch", ".", "triu", "(", "\n", "word_emb", ".", "new_ones", "(", "qlen", ",", "klen", ")", ",", "diagonal", "=", "1", "+", "mlen", ")", ".", "byte", "(", ")", "[", ":", ",", ":", ",", "None", "]", "\n", "\n", "", "hids", "=", "[", "]", "\n", "if", "self", ".", "attn_type", "==", "0", ":", "# default", "\n", "            ", "pos_seq", "=", "torch", ".", "arange", "(", "klen", "-", "1", ",", "-", "1", ",", "-", "1.0", ",", "device", "=", "word_emb", ".", "device", ",", "\n", "dtype", "=", "word_emb", ".", "dtype", ")", "\n", "if", "self", ".", "clamp_len", ">", "0", ":", "\n", "                ", "pos_seq", ".", "clamp_", "(", "max", "=", "self", ".", "clamp_len", ")", "\n", "", "pos_emb", "=", "self", ".", "pos_emb", "(", "pos_seq", ")", "\n", "\n", "core_out", "=", "self", ".", "drop", "(", "word_emb", ")", "\n", "pos_emb", "=", "self", ".", "drop", "(", "pos_emb", ")", "\n", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "                ", "hids", ".", "append", "(", "core_out", ")", "\n", "mems_i", "=", "None", "if", "mems", "is", "None", "else", "mems", "[", "i", "]", "\n", "core_out", "=", "layer", "(", "core_out", ",", "pos_emb", ",", "dec_attn_mask", "=", "dec_attn_mask", ",", "mems", "=", "mems_i", ")", "\n", "", "", "elif", "self", ".", "attn_type", "==", "1", ":", "# learnable", "\n", "            ", "core_out", "=", "self", ".", "drop", "(", "word_emb", ")", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "                ", "hids", ".", "append", "(", "core_out", ")", "\n", "if", "self", ".", "clamp_len", ">", "0", ":", "\n", "                    ", "r_emb", "=", "self", ".", "r_emb", "[", "i", "]", "[", "-", "self", ".", "clamp_len", ":", "]", "\n", "r_bias", "=", "self", ".", "r_bias", "[", "i", "]", "[", "-", "self", ".", "clamp_len", ":", "]", "\n", "", "else", ":", "\n", "                    ", "r_emb", ",", "r_bias", "=", "self", ".", "r_emb", "[", "i", "]", ",", "self", ".", "r_bias", "[", "i", "]", "\n", "\n", "", "mems_i", "=", "None", "if", "mems", "is", "None", "else", "mems", "[", "i", "]", "\n", "core_out", "=", "layer", "(", "core_out", ",", "r_emb", ",", "self", ".", "r_w_bias", "[", "i", "]", ",", "\n", "r_bias", ",", "dec_attn_mask", "=", "dec_attn_mask", ",", "mems", "=", "mems_i", ")", "\n", "", "", "elif", "self", ".", "attn_type", "==", "2", ":", "# absolute", "\n", "            ", "pos_seq", "=", "torch", ".", "arange", "(", "klen", "-", "1", ",", "-", "1", ",", "-", "1.0", ",", "device", "=", "word_emb", ".", "device", ",", "\n", "dtype", "=", "word_emb", ".", "dtype", ")", "\n", "if", "self", ".", "clamp_len", ">", "0", ":", "\n", "                ", "pos_seq", ".", "clamp_", "(", "max", "=", "self", ".", "clamp_len", ")", "\n", "", "pos_emb", "=", "self", ".", "pos_emb", "(", "pos_seq", ")", "\n", "\n", "core_out", "=", "self", ".", "drop", "(", "word_emb", "+", "pos_emb", "[", "-", "qlen", ":", "]", ")", "\n", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "                ", "hids", ".", "append", "(", "core_out", ")", "\n", "mems_i", "=", "None", "if", "mems", "is", "None", "else", "mems", "[", "i", "]", "\n", "if", "mems_i", "is", "not", "None", "and", "i", "==", "0", ":", "\n", "                    ", "mems_i", "+=", "pos_emb", "[", ":", "mlen", "]", "\n", "", "core_out", "=", "layer", "(", "core_out", ",", "dec_attn_mask", "=", "dec_attn_mask", ",", "\n", "mems", "=", "mems_i", ")", "\n", "", "", "elif", "self", ".", "attn_type", "==", "3", ":", "\n", "            ", "core_out", "=", "self", ".", "drop", "(", "word_emb", ")", "\n", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "                ", "hids", ".", "append", "(", "core_out", ")", "\n", "mems_i", "=", "None", "if", "mems", "is", "None", "else", "mems", "[", "i", "]", "\n", "if", "mems_i", "is", "not", "None", "and", "mlen", ">", "0", ":", "\n", "                    ", "cur_emb", "=", "self", ".", "r_emb", "[", "i", "]", "[", ":", "-", "qlen", "]", "\n", "cur_size", "=", "cur_emb", ".", "size", "(", "0", ")", "\n", "if", "cur_size", "<", "mlen", ":", "\n", "                        ", "cur_emb_pad", "=", "cur_emb", "[", "0", ":", "1", "]", ".", "expand", "(", "mlen", "-", "cur_size", ",", "-", "1", ",", "-", "1", ")", "\n", "cur_emb", "=", "torch", ".", "cat", "(", "[", "cur_emb_pad", ",", "cur_emb", "]", ",", "0", ")", "\n", "", "else", ":", "\n", "                        ", "cur_emb", "=", "cur_emb", "[", "-", "mlen", ":", "]", "\n", "", "mems_i", "+=", "cur_emb", ".", "view", "(", "mlen", ",", "1", ",", "-", "1", ")", "\n", "", "core_out", "+=", "self", ".", "r_emb", "[", "i", "]", "[", "-", "qlen", ":", "]", ".", "view", "(", "qlen", ",", "1", ",", "-", "1", ")", "\n", "\n", "core_out", "=", "layer", "(", "core_out", ",", "dec_attn_mask", "=", "dec_attn_mask", ",", "\n", "mems", "=", "mems_i", ")", "\n", "\n", "", "", "core_out", "=", "self", ".", "drop", "(", "core_out", ")", "\n", "\n", "new_mems", "=", "self", ".", "_update_mems", "(", "hids", ",", "mems", ",", "mlen", ",", "qlen", ")", "\n", "\n", "return", "core_out", ",", "new_mems", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLModel.forward": [[1233, 1258], ["input_ids.transpose().contiguous.transpose().contiguous.transpose().contiguous", "modeling_transfo_xl.TransfoXLModel._forward", "last_hidden.transpose().contiguous.transpose().contiguous.transpose().contiguous", "modeling_transfo_xl.TransfoXLModel.init_mems", "input_ids.transpose().contiguous.transpose().contiguous.transpose", "last_hidden.transpose().contiguous.transpose().contiguous.transpose"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLModel._forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLLMHeadModel.init_mems"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "mems", "=", "None", ")", ":", "\n", "        ", "\"\"\" Params:\n                input_ids :: [bsz, len]\n                mems :: optional mems from previous forwar passes (or init_mems)\n                    list (num layers) of mem states at the entry of each layer\n                        shape :: [self.config.mem_len, bsz, self.config.d_model]\n                    Note that the first two dimensions are transposed in `mems` with regards to `input_ids` and `target`\n            Returns:\n                tuple (last_hidden, new_mems) where:\n                    new_mems: list (num layers) of mem states at the entry of each layer\n                        shape :: [self.config.mem_len, bsz, self.config.d_model]\n                    last_hidden: output of the last layer:\n                        shape :: [bsz, len, self.config.d_model]\n        \"\"\"", "\n", "# the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library", "\n", "# so we transpose here from shape [bsz, len] to shape [len, bsz]", "\n", "input_ids", "=", "input_ids", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "if", "mems", "is", "None", ":", "\n", "            ", "mems", "=", "self", ".", "init_mems", "(", "input_ids", ")", "\n", "", "last_hidden", ",", "new_mems", "=", "self", ".", "_forward", "(", "input_ids", ",", "mems", "=", "mems", ")", "\n", "\n", "# We transpose back here to shape [bsz, len, hidden_dim]", "\n", "last_hidden", "=", "last_hidden", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "return", "(", "last_hidden", ",", "new_mems", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLLMHeadModel.__init__": [[1310, 1324], ["modeling_transfo_xl.TransfoXLPreTrainedModel.__init__", "modeling_transfo_xl.TransfoXLModel", "modeling_transfo_xl.TransfoXLLMHeadModel.apply", "modeling_transfo_xl.TransfoXLLMHeadModel.tie_weights", "torch.Linear", "torch.Linear", "torch.Linear", "LogUniformSampler", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLLMHeadModel.tie_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "TransfoXLLMHeadModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "transformer", "=", "TransfoXLModel", "(", "config", ")", "\n", "self", ".", "sample_softmax", "=", "config", ".", "sample_softmax", "\n", "# use sampled softmax", "\n", "if", "config", ".", "sample_softmax", ">", "0", ":", "\n", "            ", "self", ".", "out_layer", "=", "nn", ".", "Linear", "(", "config", ".", "d_model", ",", "config", ".", "n_token", ")", "\n", "self", ".", "sampler", "=", "LogUniformSampler", "(", "config", ".", "n_token", ",", "config", ".", "sample_softmax", ")", "\n", "# use adaptive softmax (including standard softmax)", "\n", "", "else", ":", "\n", "            ", "self", ".", "crit", "=", "ProjectedAdaptiveLogSoftmax", "(", "config", ".", "n_token", ",", "config", ".", "d_embed", ",", "config", ".", "d_model", ",", "\n", "config", ".", "cutoffs", ",", "div_val", "=", "config", ".", "div_val", ")", "\n", "", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "self", ".", "tie_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLLMHeadModel.tie_weights": [[1325, 1342], ["range", "enumerate", "len"], "methods", ["None"], ["", "def", "tie_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\" Run this to be sure output and input (adaptive) softmax weights are tied \"\"\"", "\n", "# sampled softmax", "\n", "if", "self", ".", "sample_softmax", ">", "0", ":", "\n", "            ", "if", "self", ".", "config", ".", "tie_weight", ":", "\n", "                ", "self", ".", "out_layer", ".", "weight", "=", "self", ".", "transformer", ".", "word_emb", ".", "weight", "\n", "# adaptive softmax (including standard softmax)", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "config", ".", "tie_weight", ":", "\n", "                ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "crit", ".", "out_layers", ")", ")", ":", "\n", "                    ", "self", ".", "crit", ".", "out_layers", "[", "i", "]", ".", "weight", "=", "self", ".", "transformer", ".", "word_emb", ".", "emb_layers", "[", "i", "]", ".", "weight", "\n", "", "", "if", "self", ".", "config", ".", "tie_projs", ":", "\n", "                ", "for", "i", ",", "tie_proj", "in", "enumerate", "(", "self", ".", "config", ".", "tie_projs", ")", ":", "\n", "                    ", "if", "tie_proj", "and", "self", ".", "config", ".", "div_val", "==", "1", "and", "self", ".", "config", ".", "d_model", "!=", "self", ".", "config", ".", "d_embed", ":", "\n", "                        ", "self", ".", "crit", ".", "out_projs", "[", "i", "]", "=", "self", ".", "transformer", ".", "word_emb", ".", "emb_projs", "[", "0", "]", "\n", "", "elif", "tie_proj", "and", "self", ".", "config", ".", "div_val", "!=", "1", ":", "\n", "                        ", "self", ".", "crit", ".", "out_projs", "[", "i", "]", "=", "self", ".", "transformer", ".", "word_emb", ".", "emb_projs", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLLMHeadModel.reset_length": [[1343, 1345], ["modeling_transfo_xl.TransfoXLLMHeadModel.transformer.reset_length"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLLMHeadModel.reset_length"], ["", "", "", "", "", "def", "reset_length", "(", "self", ",", "tgt_len", ",", "ext_len", ",", "mem_len", ")", ":", "\n", "        ", "self", ".", "transformer", ".", "reset_length", "(", "tgt_len", ",", "ext_len", ",", "mem_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLLMHeadModel.init_mems": [[1346, 1348], ["modeling_transfo_xl.TransfoXLLMHeadModel.transformer.init_mems"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLLMHeadModel.init_mems"], ["", "def", "init_mems", "(", "self", ",", "data", ")", ":", "\n", "        ", "return", "self", ".", "transformer", ".", "init_mems", "(", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.TransfoXLLMHeadModel.forward": [[1349, 1382], ["input_ids.size", "input_ids.size", "modeling_transfo_xl.TransfoXLLMHeadModel.transformer", "modeling_transfo_xl_utilities.sample_logits", "modeling_transfo_xl.TransfoXLLMHeadModel.crit", "pred_hid.view", "softmax_output.view.view.view", "softmax_output.view.view.view", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "pred_hid.size"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.sample_logits"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "target", "=", "None", ",", "mems", "=", "None", ")", ":", "\n", "        ", "\"\"\" Params:\n                input_ids :: [bsz, len]\n                target :: [bsz, len]\n            Returns:\n                tuple(softmax_output, new_mems) where:\n                    new_mems: list (num layers) of hidden states at the entry of each layer\n                        shape :: [mem_len, bsz, self.config.d_model] :: Warning: shapes are transposed here w. regards to input_ids\n                    softmax_output: output of the (adaptive) softmax:\n                        if target is None:\n                            Negative log likelihood of shape :: [bsz, len] \n                        else:\n                            log probabilities of tokens, shape :: [bsz, len, n_tokens]\n        \"\"\"", "\n", "bsz", "=", "input_ids", ".", "size", "(", "0", ")", "\n", "tgt_len", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "\n", "last_hidden", ",", "new_mems", "=", "self", ".", "transformer", "(", "input_ids", ",", "mems", ")", "\n", "\n", "pred_hid", "=", "last_hidden", "[", ":", ",", "-", "tgt_len", ":", "]", "\n", "if", "self", ".", "sample_softmax", ">", "0", "and", "self", ".", "training", ":", "\n", "            ", "assert", "self", ".", "config", ".", "tie_weight", "\n", "logit", "=", "sample_logits", "(", "self", ".", "transformer", ".", "word_emb", ",", "self", ".", "out_layer", ".", "bias", ",", "target", ",", "pred_hid", ",", "self", ".", "sampler", ")", "\n", "softmax_output", "=", "-", "F", ".", "log_softmax", "(", "logit", ",", "-", "1", ")", "[", ":", ",", ":", ",", "0", "]", "\n", "", "else", ":", "\n", "            ", "softmax_output", "=", "self", ".", "crit", "(", "pred_hid", ".", "view", "(", "-", "1", ",", "pred_hid", ".", "size", "(", "-", "1", ")", ")", ",", "target", ")", "\n", "if", "target", "is", "None", ":", "\n", "                ", "softmax_output", "=", "softmax_output", ".", "view", "(", "bsz", ",", "tgt_len", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "softmax_output", "=", "softmax_output", ".", "view", "(", "bsz", ",", "tgt_len", ")", "\n", "\n", "# We transpose back", "\n", "", "", "return", "(", "softmax_output", ",", "new_mems", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.build_tf_to_pytorch_map": [[55, 126], ["hasattr", "enumerate", "enumerate", "tf_to_pt_map.update", "tf_to_pt_map.update", "enumerate", "zip", "tf_to_pt_map.update", "tf_to_pt_map.update", "zip", "r_r_list.append", "r_w_list.append", "tf_to_pt_map.update", "tf_to_pt_map.update", "tf_to_pt_map.update"], "function", ["None"], ["def", "build_tf_to_pytorch_map", "(", "model", ",", "config", ")", ":", "\n", "    ", "\"\"\" A map of modules from TF to PyTorch.\n        This time I use a map to keep the PyTorch rpbert as identical to the original PyTorch rpbert as possible.\n    \"\"\"", "\n", "tf_to_pt_map", "=", "{", "}", "\n", "\n", "if", "hasattr", "(", "model", ",", "'transformer'", ")", ":", "\n", "# We are loading in a TransfoXLLMHeadModel => we will load also the Adaptive Softmax", "\n", "        ", "tf_to_pt_map", ".", "update", "(", "{", "\n", "\"transformer/adaptive_softmax/cutoff_0/cluster_W\"", ":", "model", ".", "crit", ".", "cluster_weight", ",", "\n", "\"transformer/adaptive_softmax/cutoff_0/cluster_b\"", ":", "model", ".", "crit", ".", "cluster_bias", "}", ")", "\n", "for", "i", ",", "(", "out_l", ",", "proj_l", ",", "tie_proj", ")", "in", "enumerate", "(", "zip", "(", "\n", "model", ".", "crit", ".", "out_layers", ",", "\n", "model", ".", "crit", ".", "out_projs", ",", "\n", "config", ".", "tie_projs", ")", ")", ":", "\n", "            ", "layer_str", "=", "\"transformer/adaptive_softmax/cutoff_%d/\"", "%", "i", "\n", "if", "config", ".", "tie_weight", ":", "\n", "                ", "tf_to_pt_map", ".", "update", "(", "{", "\n", "layer_str", "+", "'b'", ":", "out_l", ".", "bias", "}", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "# I don't think this is implemented in the TF code", "\n", "tf_to_pt_map", ".", "update", "(", "{", "\n", "layer_str", "+", "'lookup_table'", ":", "out_l", ".", "weight", ",", "\n", "layer_str", "+", "'b'", ":", "out_l", ".", "bias", "}", ")", "\n", "", "if", "not", "tie_proj", ":", "\n", "                ", "tf_to_pt_map", ".", "update", "(", "{", "\n", "layer_str", "+", "'proj'", ":", "proj_l", "\n", "}", ")", "\n", "# Now load the rest of the transformer", "\n", "", "", "model", "=", "model", ".", "transformer", "\n", "\n", "# Embeddings", "\n", "", "for", "i", ",", "(", "embed_l", ",", "proj_l", ")", "in", "enumerate", "(", "zip", "(", "model", ".", "word_emb", ".", "emb_layers", ",", "model", ".", "word_emb", ".", "emb_projs", ")", ")", ":", "\n", "        ", "layer_str", "=", "\"transformer/adaptive_embed/cutoff_%d/\"", "%", "i", "\n", "tf_to_pt_map", ".", "update", "(", "{", "\n", "layer_str", "+", "'lookup_table'", ":", "embed_l", ".", "weight", ",", "\n", "layer_str", "+", "'proj_W'", ":", "proj_l", "\n", "}", ")", "\n", "\n", "# Transformer blocks", "\n", "", "for", "i", ",", "b", "in", "enumerate", "(", "model", ".", "layers", ")", ":", "\n", "        ", "layer_str", "=", "\"transformer/layer_%d/\"", "%", "i", "\n", "tf_to_pt_map", ".", "update", "(", "{", "\n", "layer_str", "+", "\"rel_attn/LayerNorm/gamma\"", ":", "b", ".", "dec_attn", ".", "layer_norm", ".", "weight", ",", "\n", "layer_str", "+", "\"rel_attn/LayerNorm/beta\"", ":", "b", ".", "dec_attn", ".", "layer_norm", ".", "bias", ",", "\n", "layer_str", "+", "\"rel_attn/o/kernel\"", ":", "b", ".", "dec_attn", ".", "o_net", ".", "weight", ",", "\n", "layer_str", "+", "\"rel_attn/qkv/kernel\"", ":", "b", ".", "dec_attn", ".", "qkv_net", ".", "weight", ",", "\n", "layer_str", "+", "\"rel_attn/r/kernel\"", ":", "b", ".", "dec_attn", ".", "r_net", ".", "weight", ",", "\n", "layer_str", "+", "\"ff/LayerNorm/gamma\"", ":", "b", ".", "pos_ff", ".", "layer_norm", ".", "weight", ",", "\n", "layer_str", "+", "\"ff/LayerNorm/beta\"", ":", "b", ".", "pos_ff", ".", "layer_norm", ".", "bias", ",", "\n", "layer_str", "+", "\"ff/layer_1/kernel\"", ":", "b", ".", "pos_ff", ".", "CoreNet", "[", "0", "]", ".", "weight", ",", "\n", "layer_str", "+", "\"ff/layer_1/bias\"", ":", "b", ".", "pos_ff", ".", "CoreNet", "[", "0", "]", ".", "bias", ",", "\n", "layer_str", "+", "\"ff/layer_2/kernel\"", ":", "b", ".", "pos_ff", ".", "CoreNet", "[", "3", "]", ".", "weight", ",", "\n", "layer_str", "+", "\"ff/layer_2/bias\"", ":", "b", ".", "pos_ff", ".", "CoreNet", "[", "3", "]", ".", "bias", ",", "\n", "}", ")", "\n", "\n", "# Relative positioning biases", "\n", "", "if", "config", ".", "untie_r", ":", "\n", "        ", "r_r_list", "=", "[", "]", "\n", "r_w_list", "=", "[", "]", "\n", "for", "b", "in", "model", ".", "layers", ":", "\n", "            ", "r_r_list", ".", "append", "(", "b", ".", "dec_attn", ".", "r_r_bias", ")", "\n", "r_w_list", ".", "append", "(", "b", ".", "dec_attn", ".", "r_w_bias", ")", "\n", "", "", "else", ":", "\n", "        ", "r_r_list", "=", "[", "model", ".", "r_r_bias", "]", "\n", "r_w_list", "=", "[", "model", ".", "r_w_bias", "]", "\n", "", "tf_to_pt_map", ".", "update", "(", "{", "\n", "'transformer/r_r_bias'", ":", "r_r_list", ",", "\n", "'transformer/r_w_bias'", ":", "r_w_list", "}", ")", "\n", "return", "tf_to_pt_map", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.load_tf_weights_in_transfo_xl": [[127, 181], ["modeling_transfo_xl.build_tf_to_pytorch_map", "tf.train.list_variables", "build_tf_to_pytorch_map.items", "print", "print", "tf.train.load_variable", "tf_weights.pop", "tf_weights.pop", "tf_weights.pop", "print", "np.transpose", "enumerate", "print", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "len", "len", "print", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "tf_weights.keys"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.build_tf_to_pytorch_map"], ["", "def", "load_tf_weights_in_transfo_xl", "(", "model", ",", "config", ",", "tf_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch rpbert\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "print", "(", "\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "# Build TF to PyTorch weights loading map", "\n", "", "tf_to_pt_map", "=", "build_tf_to_pytorch_map", "(", "model", ",", "config", ")", "\n", "\n", "# Load weights from TF rpbert", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "tf_weights", "=", "{", "}", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "print", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "tf_weights", "[", "name", "]", "=", "array", "\n", "\n", "", "for", "name", ",", "pointer", "in", "tf_to_pt_map", ".", "items", "(", ")", ":", "\n", "        ", "assert", "name", "in", "tf_weights", "\n", "array", "=", "tf_weights", "[", "name", "]", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained rpbert", "\n", "if", "'kernel'", "in", "name", "or", "'proj'", "in", "name", ":", "\n", "            ", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "", "if", "(", "'r_r_bias'", "in", "name", "or", "'r_w_bias'", "in", "name", ")", "and", "len", "(", "pointer", ")", ">", "1", ":", "\n", "# Here we will split the TF weigths", "\n", "            ", "assert", "len", "(", "pointer", ")", "==", "array", ".", "shape", "[", "0", "]", "\n", "for", "i", ",", "p_i", "in", "enumerate", "(", "pointer", ")", ":", "\n", "                ", "arr_i", "=", "array", "[", "i", ",", "...", "]", "\n", "try", ":", "\n", "                    ", "assert", "p_i", ".", "shape", "==", "arr_i", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "                    ", "e", ".", "args", "+=", "(", "p_i", ".", "shape", ",", "arr_i", ".", "shape", ")", "\n", "raise", "\n", "", "print", "(", "\"Initialize PyTorch weight {} for layer {}\"", ".", "format", "(", "name", ",", "i", ")", ")", "\n", "p_i", ".", "data", "=", "torch", ".", "from_numpy", "(", "arr_i", ")", "\n", "", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "                ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "print", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "tf_weights", ".", "pop", "(", "name", ",", "None", ")", "\n", "tf_weights", ".", "pop", "(", "name", "+", "'/Adam'", ",", "None", ")", "\n", "tf_weights", ".", "pop", "(", "name", "+", "'/Adam_1'", ",", "None", ")", "\n", "\n", "", "print", "(", "\"Weights not copied to PyTorch rpbert: {}\"", ".", "format", "(", "', '", ".", "join", "(", "tf_weights", ".", "keys", "(", ")", ")", ")", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.__init__": [[32, 77], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "len", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "range", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.out_layers.append", "range", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len", "torch.Linear", "torch.Linear", "torch.Linear", "len", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.out_projs.append", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.out_layers.append", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.out_projs.append", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.out_projs.append", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_token", ",", "d_embed", ",", "d_proj", ",", "cutoffs", ",", "div_val", "=", "1", ",", "\n", "keep_order", "=", "False", ")", ":", "\n", "        ", "super", "(", "ProjectedAdaptiveLogSoftmax", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "n_token", "=", "n_token", "\n", "self", ".", "d_embed", "=", "d_embed", "\n", "self", ".", "d_proj", "=", "d_proj", "\n", "\n", "self", ".", "cutoffs", "=", "cutoffs", "+", "[", "n_token", "]", "\n", "self", ".", "cutoff_ends", "=", "[", "0", "]", "+", "self", ".", "cutoffs", "\n", "self", ".", "div_val", "=", "div_val", "\n", "\n", "self", ".", "shortlist_size", "=", "self", ".", "cutoffs", "[", "0", "]", "\n", "self", ".", "n_clusters", "=", "len", "(", "self", ".", "cutoffs", ")", "-", "1", "\n", "self", ".", "head_size", "=", "self", ".", "shortlist_size", "+", "self", ".", "n_clusters", "\n", "\n", "if", "self", ".", "n_clusters", ">", "0", ":", "\n", "            ", "self", ".", "cluster_weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "n_clusters", ",", "self", ".", "d_embed", ")", ")", "\n", "self", ".", "cluster_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "n_clusters", ")", ")", "\n", "\n", "", "self", ".", "out_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "out_projs", "=", "nn", ".", "ParameterList", "(", ")", "\n", "\n", "if", "div_val", "==", "1", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoffs", ")", ")", ":", "\n", "                ", "if", "d_proj", "!=", "d_embed", ":", "\n", "                    ", "self", ".", "out_projs", ".", "append", "(", "\n", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "d_proj", ",", "d_embed", ")", ")", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "out_projs", ".", "append", "(", "None", ")", "\n", "\n", "", "", "self", ".", "out_layers", ".", "append", "(", "nn", ".", "Linear", "(", "d_embed", ",", "n_token", ")", ")", "\n", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoffs", ")", ")", ":", "\n", "                ", "l_idx", ",", "r_idx", "=", "self", ".", "cutoff_ends", "[", "i", "]", ",", "self", ".", "cutoff_ends", "[", "i", "+", "1", "]", "\n", "d_emb_i", "=", "d_embed", "//", "(", "div_val", "**", "i", ")", "\n", "\n", "self", ".", "out_projs", ".", "append", "(", "\n", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "d_proj", ",", "d_emb_i", ")", ")", "\n", ")", "\n", "\n", "self", ".", "out_layers", ".", "append", "(", "nn", ".", "Linear", "(", "d_emb_i", ",", "r_idx", "-", "l_idx", ")", ")", "\n", "\n", "", "", "self", ".", "keep_order", "=", "keep_order", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit": [[78, 91], ["torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "proj.t().contiguous", "proj.t"], "methods", ["None"], ["", "def", "_compute_logit", "(", "self", ",", "hidden", ",", "weight", ",", "bias", ",", "proj", ")", ":", "\n", "        ", "if", "proj", "is", "None", ":", "\n", "            ", "logit", "=", "F", ".", "linear", "(", "hidden", ",", "weight", ",", "bias", "=", "bias", ")", "\n", "", "else", ":", "\n", "# if CUDA_MAJOR <= 9 and CUDA_MINOR <= 1:", "\n", "            ", "proj_hid", "=", "F", ".", "linear", "(", "hidden", ",", "proj", ".", "t", "(", ")", ".", "contiguous", "(", ")", ")", "\n", "logit", "=", "F", ".", "linear", "(", "proj_hid", ",", "weight", ",", "bias", "=", "bias", ")", "\n", "# else:", "\n", "#     logit = torch.einsum('bd,de,ev->bv', (hidden, proj, weight.t()))", "\n", "#     if bias is not None:", "\n", "#         logit = logit + bias", "\n", "\n", "", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.forward": [[92, 196], ["target.view.view.view", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit", "range", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "range", "hidden.size", "target.view.view.size", "RuntimeError", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "len", "weights.append", "biases.append", "hidden.new_empty", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.log_softmax().gather().squeeze", "torch.log_softmax().gather().squeeze", "torch.log_softmax().gather().squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "mask_i.nonzero().squeeze", "torch.log_softmax.index_select", "hidden.index_select", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "head_logprob.index_select.gather().squeeze.size", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.size", "mask_i.nonzero().squeeze.numel", "target.view.view.index_select", "F.log_softmax.index_select.gather().squeeze", "torch.zeros_like.index_copy_", "torch.zeros_like.index_copy_", "torch.zeros_like.index_copy_", "out[].copy_", "torch.log_softmax().gather", "torch.log_softmax().gather", "torch.log_softmax().gather", "mask_i.nonzero", "torch.log_softmax.gather().squeeze", "hasattr", "target.view.view.unsqueeze", "F.log_softmax.index_select.gather", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax.gather", "head_logprob.index_select.gather().squeeze.size"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit"], ["", "def", "forward", "(", "self", ",", "hidden", ",", "target", "=", "None", ",", "keep_order", "=", "False", ")", ":", "\n", "        ", "'''\n            Params:\n                hidden :: [len*bsz x d_proj]\n                target :: [len*bsz]\n            Return:\n                if target is None:\n                    out :: [len*bsz] Negative log likelihood\n                else:\n                    out :: [len*bsz x n_tokens] log probabilities of tokens over the vocabulary\n            We could replace this implementation by the native PyTorch one\n            if their's had an option to set bias on all clusters in the native one.\n            here: https://github.com/pytorch/pytorch/blob/dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da/torch/nn/modules/adaptive.py#L138\n        '''", "\n", "\n", "if", "target", "is", "not", "None", ":", "\n", "            ", "target", "=", "target", ".", "view", "(", "-", "1", ")", "\n", "if", "hidden", ".", "size", "(", "0", ")", "!=", "target", ".", "size", "(", "0", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "'Input and target should have the same size '", "\n", "'in the batch dimension.'", ")", "\n", "\n", "", "", "if", "self", ".", "n_clusters", "==", "0", ":", "\n", "            ", "logit", "=", "self", ".", "_compute_logit", "(", "hidden", ",", "self", ".", "out_layers", "[", "0", "]", ".", "weight", ",", "\n", "self", ".", "out_layers", "[", "0", "]", ".", "bias", ",", "self", ".", "out_projs", "[", "0", "]", ")", "\n", "if", "target", "is", "not", "None", ":", "\n", "                ", "output", "=", "-", "F", ".", "log_softmax", "(", "logit", ",", "dim", "=", "-", "1", ")", ".", "gather", "(", "1", ",", "target", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "output", "=", "F", ".", "log_softmax", "(", "logit", ",", "dim", "=", "-", "1", ")", "\n", "", "", "else", ":", "\n", "# construct weights and biases", "\n", "            ", "weights", ",", "biases", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoffs", ")", ")", ":", "\n", "                ", "if", "self", ".", "div_val", "==", "1", ":", "\n", "                    ", "l_idx", ",", "r_idx", "=", "self", ".", "cutoff_ends", "[", "i", "]", ",", "self", ".", "cutoff_ends", "[", "i", "+", "1", "]", "\n", "weight_i", "=", "self", ".", "out_layers", "[", "0", "]", ".", "weight", "[", "l_idx", ":", "r_idx", "]", "\n", "bias_i", "=", "self", ".", "out_layers", "[", "0", "]", ".", "bias", "[", "l_idx", ":", "r_idx", "]", "\n", "", "else", ":", "\n", "                    ", "weight_i", "=", "self", ".", "out_layers", "[", "i", "]", ".", "weight", "\n", "bias_i", "=", "self", ".", "out_layers", "[", "i", "]", ".", "bias", "\n", "\n", "", "if", "i", "==", "0", ":", "\n", "                    ", "weight_i", "=", "torch", ".", "cat", "(", "\n", "[", "weight_i", ",", "self", ".", "cluster_weight", "]", ",", "dim", "=", "0", ")", "\n", "bias_i", "=", "torch", ".", "cat", "(", "\n", "[", "bias_i", ",", "self", ".", "cluster_bias", "]", ",", "dim", "=", "0", ")", "\n", "\n", "", "weights", ".", "append", "(", "weight_i", ")", "\n", "biases", ".", "append", "(", "bias_i", ")", "\n", "\n", "", "head_weight", ",", "head_bias", ",", "head_proj", "=", "weights", "[", "0", "]", ",", "biases", "[", "0", "]", ",", "self", ".", "out_projs", "[", "0", "]", "\n", "\n", "head_logit", "=", "self", ".", "_compute_logit", "(", "hidden", ",", "head_weight", ",", "head_bias", ",", "head_proj", ")", "\n", "head_logprob", "=", "F", ".", "log_softmax", "(", "head_logit", ",", "dim", "=", "1", ")", "\n", "\n", "if", "target", "is", "None", ":", "\n", "                ", "out", "=", "hidden", ".", "new_empty", "(", "(", "head_logit", ".", "size", "(", "0", ")", ",", "self", ".", "n_token", ")", ")", "\n", "", "else", ":", "\n", "                ", "out", "=", "torch", ".", "zeros_like", "(", "target", ",", "dtype", "=", "hidden", ".", "dtype", ",", "device", "=", "hidden", ".", "device", ")", "\n", "\n", "", "offset", "=", "0", "\n", "cutoff_values", "=", "[", "0", "]", "+", "self", ".", "cutoffs", "\n", "for", "i", "in", "range", "(", "len", "(", "cutoff_values", ")", "-", "1", ")", ":", "\n", "                ", "l_idx", ",", "r_idx", "=", "cutoff_values", "[", "i", "]", ",", "cutoff_values", "[", "i", "+", "1", "]", "\n", "\n", "if", "target", "is", "not", "None", ":", "\n", "                    ", "mask_i", "=", "(", "target", ">=", "l_idx", ")", "&", "(", "target", "<", "r_idx", ")", "\n", "indices_i", "=", "mask_i", ".", "nonzero", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "if", "indices_i", ".", "numel", "(", ")", "==", "0", ":", "\n", "                        ", "continue", "\n", "\n", "", "target_i", "=", "target", ".", "index_select", "(", "0", ",", "indices_i", ")", "-", "l_idx", "\n", "head_logprob_i", "=", "head_logprob", ".", "index_select", "(", "0", ",", "indices_i", ")", "\n", "hidden_i", "=", "hidden", ".", "index_select", "(", "0", ",", "indices_i", ")", "\n", "", "else", ":", "\n", "                    ", "hidden_i", "=", "hidden", "\n", "\n", "", "if", "i", "==", "0", ":", "\n", "                    ", "if", "target", "is", "not", "None", ":", "\n", "                        ", "logprob_i", "=", "head_logprob_i", ".", "gather", "(", "1", ",", "target_i", "[", ":", ",", "None", "]", ")", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "                        ", "out", "[", ":", ",", ":", "self", ".", "cutoffs", "[", "0", "]", "]", "=", "head_logprob", "[", ":", ",", ":", "self", ".", "cutoffs", "[", "0", "]", "]", "\n", "", "", "else", ":", "\n", "                    ", "weight_i", ",", "bias_i", ",", "proj_i", "=", "weights", "[", "i", "]", ",", "biases", "[", "i", "]", ",", "self", ".", "out_projs", "[", "i", "]", "\n", "\n", "tail_logit_i", "=", "self", ".", "_compute_logit", "(", "hidden_i", ",", "weight_i", ",", "bias_i", ",", "proj_i", ")", "\n", "tail_logprob_i", "=", "F", ".", "log_softmax", "(", "tail_logit_i", ",", "dim", "=", "1", ")", "\n", "cluster_prob_idx", "=", "self", ".", "cutoffs", "[", "0", "]", "+", "i", "-", "1", "# No probability for the head cluster", "\n", "if", "target", "is", "not", "None", ":", "\n", "                        ", "logprob_i", "=", "head_logprob_i", "[", ":", ",", "cluster_prob_idx", "]", "+", "tail_logprob_i", ".", "gather", "(", "1", ",", "target_i", "[", ":", ",", "None", "]", ")", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "                        ", "logprob_i", "=", "head_logprob", "[", ":", ",", "cluster_prob_idx", ",", "None", "]", "+", "tail_logprob_i", "\n", "out", "[", ":", ",", "l_idx", ":", "r_idx", "]", "=", "logprob_i", "\n", "\n", "", "", "if", "target", "is", "not", "None", ":", "\n", "                    ", "if", "(", "hasattr", "(", "self", ",", "'keep_order'", ")", "and", "self", ".", "keep_order", ")", "or", "keep_order", ":", "\n", "                        ", "out", ".", "index_copy_", "(", "0", ",", "indices_i", ",", "-", "logprob_i", ")", "\n", "", "else", ":", "\n", "                        ", "out", "[", "offset", ":", "offset", "+", "logprob_i", ".", "size", "(", "0", ")", "]", ".", "copy_", "(", "-", "logprob_i", ")", "\n", "", "offset", "+=", "logprob_i", ".", "size", "(", "0", ")", "\n", "\n", "", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.log_prob": [[198, 258], ["modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "range", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit", "hidden.new_empty", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "range", "len", "weights.append", "biases.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.size", "len", "modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit"], ["", "def", "log_prob", "(", "self", ",", "hidden", ")", ":", "\n", "        ", "r\"\"\" Computes log probabilities for all :math:`n\\_classes`\n        From: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/adaptive.py\n        Args:\n            hidden (Tensor): a minibatch of examples\n        Returns:\n            log-probabilities of for each class :math:`c`\n            in range :math:`0 <= c <= n\\_classes`, where :math:`n\\_classes` is a\n            parameter passed to ``AdaptiveLogSoftmaxWithLoss`` constructor.\n        Shape:\n            - Input: :math:`(N, in\\_features)`\n            - Output: :math:`(N, n\\_classes)`\n        \"\"\"", "\n", "if", "self", ".", "n_clusters", "==", "0", ":", "\n", "            ", "logit", "=", "self", ".", "_compute_logit", "(", "hidden", ",", "self", ".", "out_layers", "[", "0", "]", ".", "weight", ",", "\n", "self", ".", "out_layers", "[", "0", "]", ".", "bias", ",", "self", ".", "out_projs", "[", "0", "]", ")", "\n", "return", "F", ".", "log_softmax", "(", "logit", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "# construct weights and biases", "\n", "            ", "weights", ",", "biases", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoffs", ")", ")", ":", "\n", "                ", "if", "self", ".", "div_val", "==", "1", ":", "\n", "                    ", "l_idx", ",", "r_idx", "=", "self", ".", "cutoff_ends", "[", "i", "]", ",", "self", ".", "cutoff_ends", "[", "i", "+", "1", "]", "\n", "weight_i", "=", "self", ".", "out_layers", "[", "0", "]", ".", "weight", "[", "l_idx", ":", "r_idx", "]", "\n", "bias_i", "=", "self", ".", "out_layers", "[", "0", "]", ".", "bias", "[", "l_idx", ":", "r_idx", "]", "\n", "", "else", ":", "\n", "                    ", "weight_i", "=", "self", ".", "out_layers", "[", "i", "]", ".", "weight", "\n", "bias_i", "=", "self", ".", "out_layers", "[", "i", "]", ".", "bias", "\n", "\n", "", "if", "i", "==", "0", ":", "\n", "                    ", "weight_i", "=", "torch", ".", "cat", "(", "\n", "[", "weight_i", ",", "self", ".", "cluster_weight", "]", ",", "dim", "=", "0", ")", "\n", "bias_i", "=", "torch", ".", "cat", "(", "\n", "[", "bias_i", ",", "self", ".", "cluster_bias", "]", ",", "dim", "=", "0", ")", "\n", "\n", "", "weights", ".", "append", "(", "weight_i", ")", "\n", "biases", ".", "append", "(", "bias_i", ")", "\n", "\n", "", "head_weight", ",", "head_bias", ",", "head_proj", "=", "weights", "[", "0", "]", ",", "biases", "[", "0", "]", ",", "self", ".", "out_projs", "[", "0", "]", "\n", "head_logit", "=", "self", ".", "_compute_logit", "(", "hidden", ",", "head_weight", ",", "head_bias", ",", "head_proj", ")", "\n", "\n", "out", "=", "hidden", ".", "new_empty", "(", "(", "head_logit", ".", "size", "(", "0", ")", ",", "self", ".", "n_token", ")", ")", "\n", "head_logprob", "=", "F", ".", "log_softmax", "(", "head_logit", ",", "dim", "=", "1", ")", "\n", "\n", "cutoff_values", "=", "[", "0", "]", "+", "self", ".", "cutoffs", "\n", "for", "i", "in", "range", "(", "len", "(", "cutoff_values", ")", "-", "1", ")", ":", "\n", "                ", "start_idx", ",", "stop_idx", "=", "cutoff_values", "[", "i", "]", ",", "cutoff_values", "[", "i", "+", "1", "]", "\n", "\n", "if", "i", "==", "0", ":", "\n", "                    ", "out", "[", ":", ",", ":", "self", ".", "cutoffs", "[", "0", "]", "]", "=", "head_logprob", "[", ":", ",", ":", "self", ".", "cutoffs", "[", "0", "]", "]", "\n", "", "else", ":", "\n", "                    ", "weight_i", ",", "bias_i", ",", "proj_i", "=", "weights", "[", "i", "]", ",", "biases", "[", "i", "]", ",", "self", ".", "out_projs", "[", "i", "]", "\n", "\n", "tail_logit_i", "=", "self", ".", "_compute_logit", "(", "hidden", ",", "weight_i", ",", "bias_i", ",", "proj_i", ")", "\n", "tail_logprob_i", "=", "F", ".", "log_softmax", "(", "tail_logit_i", ",", "dim", "=", "1", ")", "\n", "\n", "logprob_i", "=", "head_logprob", "[", ":", ",", "-", "i", "]", "+", "tail_logprob_i", "\n", "out", "[", ":", ",", "start_idx", ",", "stop_idx", "]", "=", "logprob_i", "\n", "\n", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.LogUniformSampler.__init__": [[261, 280], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.arange().log_", "torch.arange().log_", "torch.arange().log_", "torch.arange().log_", "torch.arange().log_", "torch.arange().log_", "torch.arange().log_", "torch.arange().log_", "torch.arange().log_", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "modeling_transfo_xl_utilities.LogUniformSampler.dist.double().log1p_", "modeling_transfo_xl_utilities.LogUniformSampler.dist.double"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "range_max", ",", "n_sample", ")", ":", "\n", "        ", "\"\"\"\n        Reference : https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/candidate_sampling_ops.py\n            `P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)`\n\n        expected count can be approximated by 1 - (1 - p)^n\n        and we use a numerically stable version -expm1(num_tries * log1p(-p))\n\n        Our implementation fixes num_tries at 2 * n_sample, and the actual #samples will vary from run to run\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "range_max", "=", "range_max", "\n", "log_indices", "=", "torch", ".", "arange", "(", "1.", ",", "range_max", "+", "2.", ",", "1.", ")", ".", "log_", "(", ")", "\n", "self", ".", "dist", "=", "(", "log_indices", "[", "1", ":", "]", "-", "log_indices", "[", ":", "-", "1", "]", ")", "/", "log_indices", "[", "-", "1", "]", "\n", "# print('P', self.dist.numpy().tolist()[-30:])", "\n", "\n", "self", ".", "log_q", "=", "(", "-", "(", "-", "self", ".", "dist", ".", "double", "(", ")", ".", "log1p_", "(", ")", "*", "2", "*", "n_sample", ")", ".", "expm1_", "(", ")", ")", ".", "log_", "(", ")", ".", "float", "(", ")", "\n", "\n", "", "self", ".", "n_sample", "=", "n_sample", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.LogUniformSampler.sample": [[281, 301], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.multinomial().unique", "torch.multinomial().unique", "torch.multinomial().unique", "torch.multinomial().unique", "torch.multinomial().unique", "torch.multinomial().unique", "torch.multinomial().unique", "torch.multinomial().unique", "torch.multinomial().unique", "neg_samples.to.to.to", "modeling_transfo_xl_utilities.LogUniformSampler.log_q[].to", "modeling_transfo_xl_utilities.LogUniformSampler.log_q[].to", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "sample", "(", "self", ",", "labels", ")", ":", "\n", "        ", "\"\"\"\n            labels: [b1, b2]\n        Return\n            true_log_probs: [b1, b2]\n            samp_log_probs: [n_sample]\n            neg_samples: [n_sample]\n        \"\"\"", "\n", "\n", "# neg_samples = torch.empty(0).long()", "\n", "n_sample", "=", "self", ".", "n_sample", "\n", "n_tries", "=", "2", "*", "n_sample", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "neg_samples", "=", "torch", ".", "multinomial", "(", "self", ".", "dist", ",", "n_tries", ",", "replacement", "=", "True", ")", ".", "unique", "(", ")", "\n", "device", "=", "labels", ".", "device", "\n", "neg_samples", "=", "neg_samples", ".", "to", "(", "device", ")", "\n", "true_log_probs", "=", "self", ".", "log_q", "[", "labels", "]", ".", "to", "(", "device", ")", "\n", "samp_log_probs", "=", "self", ".", "log_q", "[", "neg_samples", "]", ".", "to", "(", "device", ")", "\n", "return", "true_log_probs", ",", "samp_log_probs", ",", "neg_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.sample_logits": [[302, 334], ["sampler.sample", "neg_samples.size", "torch.cat", "torch.cat", "torch.cat", "embedding", "all_w[].view", "all_w[].view", "all_b[].view", "sample_logits.masked_fill_", "torch.cat", "torch.cat", "torch.cat", "labels.size", "labels.size", "labels.view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.LogUniformSampler.sample", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.embedding"], ["", "", "", "def", "sample_logits", "(", "embedding", ",", "bias", ",", "labels", ",", "inputs", ",", "sampler", ")", ":", "\n", "    ", "\"\"\"\n        embedding: an nn.Embedding layer\n        bias: [n_vocab]\n        labels: [b1, b2]\n        inputs: [b1, b2, n_emb]\n        sampler: you may use a LogUniformSampler\n    Return\n        logits: [b1, b2, 1 + n_sample]\n    \"\"\"", "\n", "true_log_probs", ",", "samp_log_probs", ",", "neg_samples", "=", "sampler", ".", "sample", "(", "labels", ")", "\n", "n_sample", "=", "neg_samples", ".", "size", "(", "0", ")", "\n", "b1", ",", "b2", "=", "labels", ".", "size", "(", "0", ")", ",", "labels", ".", "size", "(", "1", ")", "\n", "all_ids", "=", "torch", ".", "cat", "(", "[", "labels", ".", "view", "(", "-", "1", ")", ",", "neg_samples", "]", ")", "\n", "all_w", "=", "embedding", "(", "all_ids", ")", "\n", "true_w", "=", "all_w", "[", ":", "-", "n_sample", "]", ".", "view", "(", "b1", ",", "b2", ",", "-", "1", ")", "\n", "sample_w", "=", "all_w", "[", "-", "n_sample", ":", "]", ".", "view", "(", "n_sample", ",", "-", "1", ")", "\n", "\n", "all_b", "=", "bias", "[", "all_ids", "]", "\n", "true_b", "=", "all_b", "[", ":", "-", "n_sample", "]", ".", "view", "(", "b1", ",", "b2", ")", "\n", "sample_b", "=", "all_b", "[", "-", "n_sample", ":", "]", "\n", "\n", "hit", "=", "(", "labels", "[", ":", ",", ":", ",", "None", "]", "==", "neg_samples", ")", ".", "detach", "(", ")", "\n", "\n", "true_logits", "=", "torch", ".", "einsum", "(", "'ijk,ijk->ij'", ",", "\n", "[", "true_w", ",", "inputs", "]", ")", "+", "true_b", "-", "true_log_probs", "\n", "sample_logits", "=", "torch", ".", "einsum", "(", "'lk,ijk->ijl'", ",", "\n", "[", "sample_w", ",", "inputs", "]", ")", "+", "sample_b", "-", "samp_log_probs", "\n", "sample_logits", ".", "masked_fill_", "(", "hit", ",", "-", "1e30", ")", "\n", "logits", "=", "torch", ".", "cat", "(", "[", "true_logits", "[", ":", ",", ":", ",", "None", "]", ",", "sample_logits", "]", ",", "-", "1", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.url_to_filename": [[39, 55], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode"], ["def", "url_to_filename", "(", "url", ",", "etag", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.filename_to_url": [[57, 81], ["os.path.join", "isinstance", "str", "os.path.exists", "EnvironmentError", "os.path.exists", "EnvironmentError", "io.open", "json.load"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["", "def", "filename_to_url", "(", "filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.cached_path": [[83, 111], ["urlparse", "isinstance", "str", "isinstance", "str", "file_utils.get_from_cache", "os.path.exists", "EnvironmentError", "ValueError"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.get_from_cache"], ["", "def", "cached_path", "(", "url_or_filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.split_s3_path": [[113, 124], ["urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ")", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.s3_request": [[126, 143], ["functools.wraps", "func", "int", "EnvironmentError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.s3_etag": [[145, 152], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ")", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.s3_get": [[154, 160], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.http_get": [[162, 172], ["requests.get", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "tqdm.tqdm.close", "int", "tqdm.tqdm.update", "temp_file.write", "len"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm"], ["", "def", "http_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.get_from_cache": [[174, 232], ["url.startswith", "file_utils.url_to_filename", "os.path.join", "isinstance", "str", "os.path.exists", "os.makedirs", "file_utils.s3_etag", "requests.head", "requests.head.headers.get", "os.path.exists", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "file_utils.s3_get", "file_utils.http_get", "io.open", "shutil.copyfileobj", "io.open", "json.dump"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.url_to_filename", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.s3_etag", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.s3_get", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.http_get"], ["", "def", "get_from_cache", "(", "url", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "cache_dir", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.read_set_from_file": [[234, 244], ["set", "io.open", "set.add", "line.rstrip"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.SearchSpace.add"], ["", "def", "read_set_from_file", "(", "filename", ")", ":", "\n", "    ", "'''\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    '''", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.file_utils.get_file_extension": [[246, 250], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ",", "dot", "=", "True", ",", "lower", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BertTokenizer.__init__": [[77, 90], ["tokenization.load_vocab", "collections.OrderedDict", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "os.path.isfile", "ValueError", "int", "tokenization.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ",", "max_len", "=", "None", ",", "\n", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"", "\n", "\"rpbert use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "ids_to_tokens", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "(", "ids", ",", "tok", ")", "for", "tok", ",", "ids", "in", "self", ".", "vocab", ".", "items", "(", ")", "]", ")", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ",", "\n", "never_split", "=", "never_split", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BertTokenizer.tokenize": [[91, 97], ["tokenization.BertTokenizer.basic_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids": [[98, 110], ["ids.append", "len", "ValueError", "len"], "methods", ["None"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "vocab", "[", "token", "]", ")", "\n", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this BERT rpbert ({} > {}). Running this\"", "\n", "\" sequence through BERT will result in indexing errors\"", ".", "format", "(", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_ids_to_tokens": [[111, 117], ["tokens.append"], "methods", ["None"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", ".", "append", "(", "self", ".", "ids_to_tokens", "[", "i", "]", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained": [[118, 155], ["os.path.isdir", "cls", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained rpbert file.\n        Download and cache the pre-trained rpbert file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "pretrained_model_name_or_path", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "vocab_file", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_file", ",", "VOCAB_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in rpbert name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "vocab_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained rpbert, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer.__init__": [[160, 170], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "do_lower_case", "=", "True", ",", "\n", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "never_split", "=", "never_split", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer.tokenize": [[171, 191], ["tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "never_split", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_strip_accents": [[192, 202], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_split_on_punc": [[203, 224], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "text", "in", "self", ".", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._tokenize_chinese_chars": [[225, 237], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._is_chinese_char": [[238, 259], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.BasicTokenizer._clean_text": [[260, 272], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__": [[277, 281], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize": [[282, 332], ["tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.load_vocab": [[50, 63], ["collections.OrderedDict", "io.open", "reader.readline", "token.strip.strip"], "function", ["None"], ["def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization.whitespace_tokenize": [[65, 72], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization._is_whitespace": [[334, 344], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization._is_control": [[346, 356], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization._is_punctuation": [[358, 372], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.__main__.main": [[2, 82], ["print", "len", "len", "len", "print", "sys.argv.pop", "sys.argv.pop", "sys.argv.pop", "convert_tf_checkpoint_to_pytorch", "convert_openai_checkpoint_to_pytorch", "print", "len", "convert_transfo_xl_checkpoint_to_pytorch", "convert_gpt2_checkpoint_to_pytorch", "sys.argv[].lower", "len", "len", "print", "print"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.convert_openai_checkpoint_to_pytorch.convert_openai_checkpoint_to_pytorch", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.convert_transfo_xl_checkpoint_to_pytorch.convert_transfo_xl_checkpoint_to_pytorch", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.convert_gpt2_checkpoint_to_pytorch.convert_gpt2_checkpoint_to_pytorch"], ["def", "main", "(", ")", ":", "\n", "    ", "import", "sys", "\n", "if", "(", "len", "(", "sys", ".", "argv", ")", "!=", "4", "and", "len", "(", "sys", ".", "argv", ")", "!=", "5", ")", "or", "sys", ".", "argv", "[", "1", "]", "not", "in", "[", "\n", "\"convert_tf_checkpoint_to_pytorch\"", ",", "\n", "\"convert_openai_checkpoint\"", ",", "\n", "\"convert_transfo_xl_checkpoint\"", ",", "\n", "\"convert_gpt2_checkpoint\"", ",", "\n", "]", ":", "\n", "        ", "print", "(", "\n", "\"Should be used as one of: \\n\"", "\n", "\">> `pytorch_pretrained_bert convert_tf_checkpoint_to_pytorch TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT`, \\n\"", "\n", "\">> `pytorch_pretrained_bert convert_openai_checkpoint OPENAI_GPT_CHECKPOINT_FOLDER_PATH PYTORCH_DUMP_OUTPUT [OPENAI_GPT_CONFIG]`, \\n\"", "\n", "\">> `pytorch_pretrained_bert convert_transfo_xl_checkpoint TF_CHECKPOINT_OR_DATASET PYTORCH_DUMP_OUTPUT [TF_CONFIG]` or \\n\"", "\n", "\">> `pytorch_pretrained_bert convert_gpt2_checkpoint TF_CHECKPOINT PYTORCH_DUMP_OUTPUT [GPT2_CONFIG]`\"", ")", "\n", "", "else", ":", "\n", "        ", "if", "sys", ".", "argv", "[", "1", "]", "==", "\"convert_tf_checkpoint_to_pytorch\"", ":", "\n", "            ", "try", ":", "\n", "                ", "from", ".", "convert_tf_checkpoint_to_pytorch", "import", "convert_tf_checkpoint_to_pytorch", "\n", "", "except", "ImportError", ":", "\n", "                ", "print", "(", "\"pytorch_pretrained_bert can only be used from the commandline to convert TensorFlow models in PyTorch, \"", "\n", "\"In that case, it requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "\n", "", "if", "len", "(", "sys", ".", "argv", ")", "!=", "5", ":", "\n", "# pylint: disable=line-too-long", "\n", "                ", "print", "(", "\"Should be used as `pytorch_pretrained_bert convert_tf_checkpoint_to_pytorch TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT`\"", ")", "\n", "", "else", ":", "\n", "                ", "PYTORCH_DUMP_OUTPUT", "=", "sys", ".", "argv", ".", "pop", "(", ")", "\n", "TF_CONFIG", "=", "sys", ".", "argv", ".", "pop", "(", ")", "\n", "TF_CHECKPOINT", "=", "sys", ".", "argv", ".", "pop", "(", ")", "\n", "convert_tf_checkpoint_to_pytorch", "(", "TF_CHECKPOINT", ",", "TF_CONFIG", ",", "PYTORCH_DUMP_OUTPUT", ")", "\n", "", "", "elif", "sys", ".", "argv", "[", "1", "]", "==", "\"convert_openai_checkpoint\"", ":", "\n", "            ", "from", ".", "convert_openai_checkpoint_to_pytorch", "import", "convert_openai_checkpoint_to_pytorch", "\n", "OPENAI_GPT_CHECKPOINT_FOLDER_PATH", "=", "sys", ".", "argv", "[", "2", "]", "\n", "PYTORCH_DUMP_OUTPUT", "=", "sys", ".", "argv", "[", "3", "]", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "5", ":", "\n", "                ", "OPENAI_GPT_CONFIG", "=", "sys", ".", "argv", "[", "4", "]", "\n", "", "else", ":", "\n", "                ", "OPENAI_GPT_CONFIG", "=", "\"\"", "\n", "", "convert_openai_checkpoint_to_pytorch", "(", "OPENAI_GPT_CHECKPOINT_FOLDER_PATH", ",", "\n", "OPENAI_GPT_CONFIG", ",", "\n", "PYTORCH_DUMP_OUTPUT", ")", "\n", "", "elif", "sys", ".", "argv", "[", "1", "]", "==", "\"convert_transfo_xl_checkpoint\"", ":", "\n", "            ", "try", ":", "\n", "                ", "from", ".", "convert_transfo_xl_checkpoint_to_pytorch", "import", "convert_transfo_xl_checkpoint_to_pytorch", "\n", "", "except", "ImportError", ":", "\n", "                ", "print", "(", "\"pytorch_pretrained_bert can only be used from the commandline to convert TensorFlow models in PyTorch, \"", "\n", "\"In that case, it requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "\n", "", "if", "'ckpt'", "in", "sys", ".", "argv", "[", "2", "]", ".", "lower", "(", ")", ":", "\n", "                ", "TF_CHECKPOINT", "=", "sys", ".", "argv", "[", "2", "]", "\n", "TF_DATASET_FILE", "=", "\"\"", "\n", "", "else", ":", "\n", "                ", "TF_DATASET_FILE", "=", "sys", ".", "argv", "[", "2", "]", "\n", "TF_CHECKPOINT", "=", "\"\"", "\n", "", "PYTORCH_DUMP_OUTPUT", "=", "sys", ".", "argv", "[", "3", "]", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "5", ":", "\n", "                ", "TF_CONFIG", "=", "sys", ".", "argv", "[", "4", "]", "\n", "", "else", ":", "\n", "                ", "TF_CONFIG", "=", "\"\"", "\n", "", "convert_transfo_xl_checkpoint_to_pytorch", "(", "TF_CHECKPOINT", ",", "TF_CONFIG", ",", "PYTORCH_DUMP_OUTPUT", ",", "TF_DATASET_FILE", ")", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "from", ".", "convert_gpt2_checkpoint_to_pytorch", "import", "convert_gpt2_checkpoint_to_pytorch", "\n", "", "except", "ImportError", ":", "\n", "                ", "print", "(", "\"pytorch_pretrained_bert can only be used from the commandline to convert TensorFlow models in PyTorch, \"", "\n", "\"In that case, it requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "\n", "", "TF_CHECKPOINT", "=", "sys", ".", "argv", "[", "2", "]", "\n", "PYTORCH_DUMP_OUTPUT", "=", "sys", ".", "argv", "[", "3", "]", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "5", ":", "\n", "                ", "TF_CONFIG", "=", "sys", ".", "argv", "[", "4", "]", "\n", "", "else", ":", "\n", "                ", "TF_CONFIG", "=", "\"\"", "\n", "", "convert_gpt2_checkpoint_to_pytorch", "(", "TF_CHECKPOINT", ",", "TF_CONFIG", ",", "PYTORCH_DUMP_OUTPUT", ")", "\n", "", "", "", "if", "__name__", "==", "'__main__'", ":", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.convert_gpt2_checkpoint_to_pytorch.convert_gpt2_checkpoint_to_pytorch": [[30, 49], ["external.pytorch_pretrained_bert.modeling_gpt2.GPT2Model", "external.pytorch_pretrained_bert.modeling_gpt2.load_tf_weights_in_gpt2", "print", "torch.save", "print", "external.pytorch_pretrained_bert.modeling_gpt2.GPT2Config", "external.pytorch_pretrained_bert.modeling_gpt2.GPT2Config", "external.pytorch_pretrained_bert.modeling_gpt2.GPT2Model.state_dict", "io.open", "f.write", "external.pytorch_pretrained_bert.modeling_gpt2.GPT2Config.to_json_string"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_gpt2.load_tf_weights_in_gpt2", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.to_json_string"], ["def", "convert_gpt2_checkpoint_to_pytorch", "(", "gpt2_checkpoint_path", ",", "gpt2_config_file", ",", "pytorch_dump_folder_path", ")", ":", "\n", "# Construct rpbert", "\n", "    ", "if", "gpt2_config_file", "==", "\"\"", ":", "\n", "        ", "config", "=", "GPT2Config", "(", ")", "\n", "", "else", ":", "\n", "        ", "config", "=", "GPT2Config", "(", "gpt2_config_file", ")", "\n", "", "model", "=", "GPT2Model", "(", "config", ")", "\n", "\n", "# Load weights from numpy", "\n", "load_tf_weights_in_gpt2", "(", "model", ",", "gpt2_checkpoint_path", ")", "\n", "\n", "# Save pytorch-rpbert", "\n", "pytorch_weights_dump_path", "=", "pytorch_dump_folder_path", "+", "'/'", "+", "WEIGHTS_NAME", "\n", "pytorch_config_dump_path", "=", "pytorch_dump_folder_path", "+", "'/'", "+", "CONFIG_NAME", "\n", "print", "(", "\"Save PyTorch rpbert to {}\"", ".", "format", "(", "pytorch_weights_dump_path", ")", ")", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "pytorch_weights_dump_path", ")", "\n", "print", "(", "\"Save configuration file to {}\"", ".", "format", "(", "pytorch_config_dump_path", ")", ")", "\n", "with", "open", "(", "pytorch_config_dump_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "config", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.convert_transfo_xl_checkpoint_to_pytorch.convert_transfo_xl_checkpoint_to_pytorch": [[47, 90], ["print", "torch.save", "corpus_dict_no_vocab.pop", "print", "torch.save", "os.path.abspath", "os.path.abspath", "print", "print", "external.pytorch_pretrained_bert.TransfoXLLMHeadModel", "external.pytorch_pretrained_bert.load_tf_weights_in_transfo_xl", "os.path.join", "os.path.join", "print", "torch.save", "print", "io.open", "pickle.load", "external.pytorch_pretrained_bert.TransfoXLConfig", "external.pytorch_pretrained_bert.TransfoXLConfig", "external.pytorch_pretrained_bert.load_tf_weights_in_transfo_xl.state_dict", "io.open", "f.write", "str", "os.path.abspath", "os.path.abspath", "external.pytorch_pretrained_bert.TransfoXLConfig.to_json_string"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl.load_tf_weights_in_transfo_xl", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.to_json_string"], ["def", "convert_transfo_xl_checkpoint_to_pytorch", "(", "tf_checkpoint_path", ",", "\n", "transfo_xl_config_file", ",", "\n", "pytorch_dump_folder_path", ",", "\n", "transfo_xl_dataset_file", ")", ":", "\n", "    ", "if", "transfo_xl_dataset_file", ":", "\n", "# Convert a pre-processed corpus (see original TensorFlow repo)", "\n", "        ", "with", "open", "(", "transfo_xl_dataset_file", ",", "\"rb\"", ")", "as", "fp", ":", "\n", "            ", "corpus", "=", "pickle", ".", "load", "(", "fp", ",", "encoding", "=", "\"latin1\"", ")", "\n", "# Save vocabulary and dataset cache as Dictionaries (should be better than pickles for the long-term)", "\n", "", "pytorch_vocab_dump_path", "=", "pytorch_dump_folder_path", "+", "'/'", "+", "VOCAB_NAME", "\n", "print", "(", "\"Save vocabulary to {}\"", ".", "format", "(", "pytorch_vocab_dump_path", ")", ")", "\n", "corpus_vocab_dict", "=", "corpus", ".", "vocab", ".", "__dict__", "\n", "torch", ".", "save", "(", "corpus_vocab_dict", ",", "pytorch_vocab_dump_path", ")", "\n", "\n", "corpus_dict_no_vocab", "=", "corpus", ".", "__dict__", "\n", "corpus_dict_no_vocab", ".", "pop", "(", "'vocab'", ",", "None", ")", "\n", "pytorch_dataset_dump_path", "=", "pytorch_dump_folder_path", "+", "'/'", "+", "CORPUS_NAME", "\n", "print", "(", "\"Save dataset to {}\"", ".", "format", "(", "pytorch_dataset_dump_path", ")", ")", "\n", "torch", ".", "save", "(", "corpus_dict_no_vocab", ",", "pytorch_dataset_dump_path", ")", "\n", "\n", "", "if", "tf_checkpoint_path", ":", "\n", "# Convert a pre-trained TensorFlow rpbert", "\n", "        ", "config_path", "=", "os", ".", "path", ".", "abspath", "(", "transfo_xl_config_file", ")", "\n", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "tf_checkpoint_path", ")", "\n", "\n", "print", "(", "\"Converting Transformer XL checkpoint from {} with config at {}\"", ".", "format", "(", "tf_path", ",", "config_path", ")", ")", "\n", "# Initialise PyTorch rpbert", "\n", "if", "transfo_xl_config_file", "==", "\"\"", ":", "\n", "            ", "config", "=", "TransfoXLConfig", "(", ")", "\n", "", "else", ":", "\n", "            ", "config", "=", "TransfoXLConfig", "(", "transfo_xl_config_file", ")", "\n", "", "print", "(", "\"Building PyTorch rpbert from configuration: {}\"", ".", "format", "(", "str", "(", "config", ")", ")", ")", "\n", "model", "=", "TransfoXLLMHeadModel", "(", "config", ")", "\n", "\n", "model", "=", "load_tf_weights_in_transfo_xl", "(", "model", ",", "config", ",", "tf_path", ")", "\n", "# Save pytorch-rpbert", "\n", "pytorch_weights_dump_path", "=", "os", ".", "path", ".", "join", "(", "pytorch_dump_folder_path", ",", "WEIGHTS_NAME", ")", "\n", "pytorch_config_dump_path", "=", "os", ".", "path", ".", "join", "(", "pytorch_dump_folder_path", ",", "CONFIG_NAME", ")", "\n", "print", "(", "\"Save PyTorch rpbert to {}\"", ".", "format", "(", "os", ".", "path", ".", "abspath", "(", "pytorch_weights_dump_path", ")", ")", ")", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "pytorch_weights_dump_path", ")", "\n", "print", "(", "\"Save configuration file to {}\"", ".", "format", "(", "os", ".", "path", ".", "abspath", "(", "pytorch_config_dump_path", ")", ")", ")", "\n", "with", "open", "(", "pytorch_config_dump_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "config", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.__init__": [[131, 192], ["isinstance", "json.loads.items", "isinstance", "isinstance", "io.open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_size_or_config_json_file", "=", "40478", ",", "\n", "n_special", "=", "0", ",", "\n", "n_positions", "=", "512", ",", "\n", "n_ctx", "=", "512", ",", "\n", "n_embd", "=", "768", ",", "\n", "n_layer", "=", "12", ",", "\n", "n_head", "=", "12", ",", "\n", "afn", "=", "\"gelu\"", ",", "\n", "resid_pdrop", "=", "0.1", ",", "\n", "embd_pdrop", "=", "0.1", ",", "\n", "attn_pdrop", "=", "0.1", ",", "\n", "layer_norm_epsilon", "=", "1e-5", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Constructs OpenAIGPTConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `OpenAIGPTModel` or a configuration json file.\n            n_special: The number of special tokens to learn during fine-tuning ('[SEP]', '[CLF]', ...)\n            n_positions: Number of positional embeddings.\n            n_ctx: Size of the causal mask (usually same as n_positions).\n            n_embd: Dimensionality of the embeddings and hidden states.\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            afn: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            resid_pdrop: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attn_pdrop: The dropout ratio for the attention\n                probabilities.\n            embd_pdrop: The dropout ratio for the embeddings.\n            layer_norm_epsilon: epsilon to use in the layer norm layers\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "\n", "and", "isinstance", "(", "vocab_size_or_config_json_file", ",", "unicode", ")", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "n_special", "=", "n_special", "\n", "self", ".", "n_ctx", "=", "n_ctx", "\n", "self", ".", "n_positions", "=", "n_positions", "\n", "self", ".", "n_embd", "=", "n_embd", "\n", "self", ".", "n_layer", "=", "n_layer", "\n", "self", ".", "n_head", "=", "n_head", "\n", "self", ".", "afn", "=", "afn", "\n", "self", ".", "resid_pdrop", "=", "resid_pdrop", "\n", "self", ".", "embd_pdrop", "=", "embd_pdrop", "\n", "self", ".", "attn_pdrop", "=", "attn_pdrop", "\n", "self", ".", "layer_norm_epsilon", "=", "layer_norm_epsilon", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained rpbert config file (str)\"", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.total_tokens_embeddings": [[195, 198], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "total_tokens_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "vocab_size", "+", "self", ".", "n_special", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.from_dict": [[199, 206], ["modeling_openai.OpenAIGPTConfig", "json_object.items"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `OpenAIGPTConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "OpenAIGPTConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.from_json_file": [[207, 213], ["cls.from_dict", "io.open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `OpenAIGPTConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.__repr__": [[214, 216], ["str", "modeling_openai.OpenAIGPTConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.to_dict": [[217, 221], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.to_json_string": [[222, 225], ["json.dumps", "modeling_openai.OpenAIGPTConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Conv1D.__init__": [[228, 239], ["torch.Module.__init__", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.init.normal_", "torch.init.normal_", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nf", ",", "rf", ",", "nx", ")", ":", "\n", "        ", "super", "(", "Conv1D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "rf", "=", "rf", "\n", "self", ".", "nf", "=", "nf", "\n", "if", "rf", "==", "1", ":", "# faster 1x1 conv", "\n", "            ", "w", "=", "torch", ".", "empty", "(", "nx", ",", "nf", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "w", ",", "std", "=", "0.02", ")", "\n", "self", ".", "weight", "=", "Parameter", "(", "w", ")", "\n", "self", ".", "bias", "=", "Parameter", "(", "torch", ".", "zeros", "(", "nf", ")", ")", "\n", "", "else", ":", "# was used to train LM", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Conv1D.forward": [[240, 248], ["torch.addmm", "torch.addmm", "torch.addmm", "torch.addmm", "x.view.view.view", "x.view.view.view", "x.view.view.size", "x.view.view.size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "rf", "==", "1", ":", "\n", "            ", "size_out", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "nf", ",", ")", "\n", "x", "=", "torch", ".", "addmm", "(", "self", ".", "bias", ",", "x", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "-", "1", ")", ")", ",", "self", ".", "weight", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "size_out", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention.__init__": [[251, 264], ["torch.Module.__init__", "modeling_openai.Attention.register_buffer", "modeling_openai.Conv1D", "modeling_openai.Conv1D", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril().view", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nx", ",", "n_ctx", ",", "config", ",", "scale", "=", "False", ")", ":", "\n", "        ", "super", "(", "Attention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "n_state", "=", "nx", "# in Attention: n_state=768 (nx=n_embd)", "\n", "# [switch nx => n_state from Block to Attention to keep identical to TF implem]", "\n", "assert", "n_state", "%", "config", ".", "n_head", "==", "0", "\n", "self", ".", "register_buffer", "(", "\"bias\"", ",", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "n_ctx", ",", "n_ctx", ")", ")", ".", "view", "(", "1", ",", "1", ",", "n_ctx", ",", "n_ctx", ")", ")", "\n", "self", ".", "n_head", "=", "config", ".", "n_head", "\n", "self", ".", "split_size", "=", "n_state", "\n", "self", ".", "scale", "=", "scale", "\n", "self", ".", "c_attn", "=", "Conv1D", "(", "n_state", "*", "3", ",", "1", ",", "nx", ")", "\n", "self", ".", "c_proj", "=", "Conv1D", "(", "n_state", ",", "1", ",", "nx", ")", "\n", "self", ".", "attn_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attn_pdrop", ")", "\n", "self", ".", "resid_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "resid_pdrop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention._attn": [[265, 277], ["torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "modeling_openai.Attention.attn_dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.Softmax", "torch.Softmax", "math.sqrt", "v.size", "modeling_openai.Attention.size", "modeling_openai.Attention.size"], "methods", ["None"], ["", "def", "_attn", "(", "self", ",", "q", ",", "k", ",", "v", ")", ":", "\n", "        ", "w", "=", "torch", ".", "matmul", "(", "q", ",", "k", ")", "\n", "if", "self", ".", "scale", ":", "\n", "            ", "w", "=", "w", "/", "math", ".", "sqrt", "(", "v", ".", "size", "(", "-", "1", ")", ")", "\n", "# w = w * self.bias + -1e9 * (1 - self.bias)  # TF implem method: mask_attn_weights", "\n", "# XD: self.b may be larger than w, so we need to crop it", "\n", "", "b", "=", "self", ".", "bias", "[", ":", ",", ":", ",", ":", "w", ".", "size", "(", "-", "2", ")", ",", ":", "w", ".", "size", "(", "-", "1", ")", "]", "\n", "w", "=", "w", "*", "b", "+", "-", "1e9", "*", "(", "1", "-", "b", ")", "\n", "\n", "w", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "w", ")", "\n", "w", "=", "self", ".", "attn_dropout", "(", "w", ")", "\n", "return", "torch", ".", "matmul", "(", "w", ",", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention.merge_heads": [[278, 282], ["x.permute().contiguous.permute().contiguous.permute().contiguous", "x.permute().contiguous.permute().contiguous.view", "x.permute().contiguous.permute().contiguous.permute", "x.permute().contiguous.permute().contiguous.size", "x.permute().contiguous.permute().contiguous.size", "x.permute().contiguous.permute().contiguous.size"], "methods", ["None"], ["", "def", "merge_heads", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "x", ".", "size", "(", "-", "2", ")", "*", "x", ".", "size", "(", "-", "1", ")", ",", ")", "\n", "return", "x", ".", "view", "(", "*", "new_x_shape", ")", "# in Tensorflow implem: fct merge_states", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention.split_heads": [[283, 290], ["x.view.view.view", "x.view.view.permute", "x.view.view.permute", "x.view.view.size", "x.view.view.size"], "methods", ["None"], ["", "def", "split_heads", "(", "self", ",", "x", ",", "k", "=", "False", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "n_head", ",", "x", ".", "size", "(", "-", "1", ")", "//", "self", ".", "n_head", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "# in Tensorflow implem: fct split_states", "\n", "if", "k", ":", "\n", "            ", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention.forward": [[291, 302], ["modeling_openai.Attention.c_attn", "modeling_openai.Attention.split", "modeling_openai.Attention.split_heads", "modeling_openai.Attention.split_heads", "modeling_openai.Attention.split_heads", "modeling_openai.Attention._attn", "modeling_openai.Attention.merge_heads", "modeling_openai.Attention.c_proj", "modeling_openai.Attention.resid_dropout"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention.split_heads", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention.split_heads", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention.split_heads", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention._attn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Attention.merge_heads"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "c_attn", "(", "x", ")", "\n", "query", ",", "key", ",", "value", "=", "x", ".", "split", "(", "self", ".", "split_size", ",", "dim", "=", "2", ")", "\n", "query", "=", "self", ".", "split_heads", "(", "query", ")", "\n", "key", "=", "self", ".", "split_heads", "(", "key", ",", "k", "=", "True", ")", "\n", "value", "=", "self", ".", "split_heads", "(", "value", ")", "\n", "a", "=", "self", ".", "_attn", "(", "query", ",", "key", ",", "value", ")", "\n", "a", "=", "self", ".", "merge_heads", "(", "a", ")", "\n", "a", "=", "self", ".", "c_proj", "(", "a", ")", "\n", "a", "=", "self", ".", "resid_dropout", "(", "a", ")", "\n", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.MLP.__init__": [[305, 312], ["torch.Module.__init__", "modeling_openai.Conv1D", "modeling_openai.Conv1D", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_state", ",", "config", ")", ":", "# in MLP: n_state=3072 (4 * n_embd)", "\n", "        ", "super", "(", "MLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "nx", "=", "config", ".", "n_embd", "\n", "self", ".", "c_fc", "=", "Conv1D", "(", "n_state", ",", "1", ",", "nx", ")", "\n", "self", ".", "c_proj", "=", "Conv1D", "(", "nx", ",", "1", ",", "n_state", ")", "\n", "self", ".", "act", "=", "ACT_FNS", "[", "config", ".", "afn", "]", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "resid_pdrop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.MLP.forward": [[313, 317], ["modeling_openai.MLP.act", "modeling_openai.MLP.c_proj", "modeling_openai.MLP.dropout", "modeling_openai.MLP.c_fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "h", "=", "self", ".", "act", "(", "self", ".", "c_fc", "(", "x", ")", ")", "\n", "h2", "=", "self", ".", "c_proj", "(", "h", ")", "\n", "return", "self", ".", "dropout", "(", "h2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Block.__init__": [[320, 327], ["torch.Module.__init__", "modeling_openai.Attention", "modeling.BertLayerNorm", "modeling_openai.MLP", "modeling.BertLayerNorm"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_ctx", ",", "config", ",", "scale", "=", "False", ")", ":", "\n", "        ", "super", "(", "Block", ",", "self", ")", ".", "__init__", "(", ")", "\n", "nx", "=", "config", ".", "n_embd", "\n", "self", ".", "attn", "=", "Attention", "(", "nx", ",", "n_ctx", ",", "config", ",", "scale", ")", "\n", "self", ".", "ln_1", "=", "LayerNorm", "(", "nx", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "self", ".", "mlp", "=", "MLP", "(", "4", "*", "nx", ",", "config", ")", "\n", "self", ".", "ln_2", "=", "LayerNorm", "(", "nx", ",", "eps", "=", "config", ".", "layer_norm_epsilon", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.Block.forward": [[328, 334], ["modeling_openai.Block.attn", "modeling_openai.Block.ln_1", "modeling_openai.Block.mlp", "modeling_openai.Block.ln_2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "a", "=", "self", ".", "attn", "(", "x", ")", "\n", "n", "=", "self", ".", "ln_1", "(", "x", "+", "a", ")", "\n", "m", "=", "self", ".", "mlp", "(", "n", ")", "\n", "h", "=", "self", ".", "ln_2", "(", "n", "+", "m", ")", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTLMHead.__init__": [[339, 343], ["torch.Module.__init__", "modeling_openai.OpenAIGPTLMHead.set_embeddings_weights"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTLMHead.set_embeddings_weights"], ["def", "__init__", "(", "self", ",", "model_embeddings_weights", ",", "config", ")", ":", "\n", "        ", "super", "(", "OpenAIGPTLMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "n_embd", "=", "config", ".", "n_embd", "\n", "self", ".", "set_embeddings_weights", "(", "model_embeddings_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTLMHead.set_embeddings_weights": [[344, 348], ["torch.Linear", "torch.Linear"], "methods", ["None"], ["", "def", "set_embeddings_weights", "(", "self", ",", "model_embeddings_weights", ")", ":", "\n", "        ", "embed_shape", "=", "model_embeddings_weights", ".", "shape", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "embed_shape", "[", "1", "]", ",", "embed_shape", "[", "0", "]", ",", "bias", "=", "False", ")", "\n", "self", ".", "decoder", ".", "weight", "=", "model_embeddings_weights", "# Tied weights", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTLMHead.forward": [[349, 354], ["modeling_openai.OpenAIGPTLMHead.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_state", ")", ":", "\n", "# Truncated Language modeling logits (we remove the last token)", "\n", "# h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)", "\n", "        ", "lm_logits", "=", "self", ".", "decoder", "(", "hidden_state", ")", "\n", "return", "lm_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTMultipleChoiceHead.__init__": [[359, 368], ["torch.Module.__init__", "torch.Dropout2d", "torch.Dropout2d", "torch.Linear", "torch.Linear", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "OpenAIGPTMultipleChoiceHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "n_embd", "=", "config", ".", "n_embd", "\n", "# self.multiple_choice_token = multiple_choice_token", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout2d", "(", "config", ".", "resid_pdrop", ")", "# To reproduce the noise_shape parameter of TF implementation", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "config", ".", "n_embd", ",", "1", ")", "\n", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "linear", ".", "weight", ",", "std", "=", "0.02", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "linear", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTMultipleChoiceHead.forward": [[369, 380], ["mc_token_ids.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze().expand", "hidden_states.gather().squeeze", "modeling_openai.OpenAIGPTMultipleChoiceHead.linear().squeeze", "hidden_states.size", "mc_token_ids.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze", "hidden_states.gather", "modeling_openai.OpenAIGPTMultipleChoiceHead.linear", "mc_token_ids.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze().expand.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "mc_token_ids", ")", ":", "\n", "# Classification logits", "\n", "# hidden_state (bsz, num_choices, seq_length, hidden_size)", "\n", "# mc_token_ids (bsz, num_choices)", "\n", "        ", "mc_token_ids", "=", "mc_token_ids", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "-", "1", ",", "hidden_states", ".", "size", "(", "-", "1", ")", ")", "\n", "# (bsz, num_choices, 1, hidden_size)", "\n", "multiple_choice_h", "=", "hidden_states", ".", "gather", "(", "2", ",", "mc_token_ids", ")", ".", "squeeze", "(", "2", ")", "\n", "# (bsz, num_choices, hidden_size)", "\n", "multiple_choice_logits", "=", "self", ".", "linear", "(", "multiple_choice_h", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "# (bsz, num_choices)", "\n", "return", "multiple_choice_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTPreTrainedModel.__init__": [[387, 398], ["torch.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "OpenAIGPTPreTrainedModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "OpenAIGPTConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `OpenAIGPTConfig`. \"", "\n", "\"To create a rpbert from a pretrained rpbert use \"", "\n", "\"`rpbert = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", "\n", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTPreTrainedModel.init_weights": [[399, 411], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTPreTrainedModel.set_num_special_tokens": [[412, 414], ["None"], "methods", ["None"], ["", "", "def", "set_num_special_tokens", "(", "self", ",", "num_special_tokens", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTPreTrainedModel.from_pretrained": [[415, 534], ["modeling_openai.OpenAIGPTConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "torch.load.keys", "zip", "getattr", "torch.load.copy", "torch.load.copy", "modeling_openai.OpenAIGPTPreTrainedModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTConfig.from_json_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "\n", "cls", ",", "pretrained_model_name_or_path", ",", "num_special_tokens", "=", "None", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "from_tf", "=", "False", ",", "*", "inputs", ",", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a OpenAIGPTPreTrainedModel from a pre-trained rpbert file or a pytorch state dict.\n        Download and cache the pre-trained rpbert file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained rpbert to load selected in the list of:\n                    . `openai-gpt`\n                - a path or url to a pretrained rpbert archive containing:\n                    . `openai_gpt_config.json` a configuration file for the rpbert\n                    . `pytorch_model.bin` a PyTorch dump of a OpenAIGPTModel instance\n                - a path or url to a pretrained rpbert archive containing:\n                    . `bert_config.json` a configuration file for the rpbert\n                    . a series of NumPy files containing OpenAI TensorFlow trained weights\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "config_file", "=", "PRETRAINED_CONFIG_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "WEIGHTS_NAME", ")", "\n", "config_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "CONFIG_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "resolved_config_file", "=", "cached_path", "(", "config_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in rpbert name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find files {} and {} \"", "\n", "\"at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\", \"", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "pretrained_model_name_or_path", ",", "\n", "archive_file", ",", "config_file", "\n", ")", "\n", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", "and", "resolved_config_file", "==", "config_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading weights file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "logger", ".", "info", "(", "\"loading configuration file {}\"", ".", "format", "(", "config_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading weights file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "logger", ".", "info", "(", "\"loading configuration file {} from cache at {}\"", ".", "format", "(", "\n", "config_file", ",", "resolved_config_file", ")", ")", "\n", "# Load config", "\n", "", "config", "=", "OpenAIGPTConfig", ".", "from_json_file", "(", "resolved_config_file", ")", "\n", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "# Instantiate rpbert.", "\n", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", "and", "not", "from_tf", ":", "\n", "            ", "state_dict", "=", "torch", ".", "load", "(", "resolved_archive_file", ",", "map_location", "=", "'cpu'", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "None", ")", "\n", "", "if", "from_tf", ":", "\n", "# Directly load from a TensorFlow checkpoint (stored as NumPy array)", "\n", "            ", "return", "load_tf_weights_in_openai_gpt", "(", "model", ",", "resolved_archive_file", ")", "\n", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "key", ".", "endswith", "(", "\".g\"", ")", ":", "\n", "                ", "new_key", "=", "key", "[", ":", "-", "2", "]", "+", "\".weight\"", "\n", "", "elif", "key", ".", "endswith", "(", "\".b\"", ")", ":", "\n", "                ", "new_key", "=", "key", "[", ":", "-", "2", "]", "+", "\".bias\"", "\n", "", "elif", "key", ".", "endswith", "(", "\".w\"", ")", ":", "\n", "                ", "new_key", "=", "key", "[", ":", "-", "2", "]", "+", "\".weight\"", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "\"_metadata\"", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "\"\"", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", "\n", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "\".\"", ")", "\n", "\n", "", "", "", "start_model", "=", "model", "\n", "if", "hasattr", "(", "model", ",", "\"transformer\"", ")", "and", "all", "(", "not", "s", ".", "startswith", "(", "'transformer.'", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "start_model", "=", "model", ".", "transformer", "\n", "", "load", "(", "start_model", ",", "prefix", "=", "\"\"", ")", "\n", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Weights of {} not initialized from pretrained rpbert: {}\"", ".", "format", "(", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", "\n", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Weights from pretrained rpbert not used in {}: {}\"", ".", "format", "(", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", "\n", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "\n", "\"Error(s) in loading state_dict for {}:\\n\\t{}\"", ".", "format", "(", "model", ".", "__class__", ".", "__name__", ",", "\"\\n\\t\"", ".", "join", "(", "error_msgs", ")", ")", "\n", ")", "\n", "\n", "# Add additional embeddings for special tokens if needed", "\n", "# This step also make sure we are still sharing the output and input embeddings after loading weights", "\n", "", "model", ".", "set_num_special_tokens", "(", "num_special_tokens", "if", "num_special_tokens", "is", "not", "None", "else", "config", ".", "n_special", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTModel.__init__": [[587, 597], ["modeling_openai.OpenAIGPTPreTrainedModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Dropout", "torch.Dropout", "modeling_openai.Block", "torch.ModuleList", "torch.ModuleList", "modeling_openai.OpenAIGPTModel.apply", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "OpenAIGPTModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "num_tokens", "=", "config", ".", "vocab_size", "+", "config", ".", "n_special", "\n", "self", ".", "tokens_embed", "=", "nn", ".", "Embedding", "(", "num_tokens", ",", "config", ".", "n_embd", ")", "\n", "self", ".", "positions_embed", "=", "nn", ".", "Embedding", "(", "config", ".", "n_positions", ",", "config", ".", "n_embd", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "config", ".", "embd_pdrop", ")", "\n", "block", "=", "Block", "(", "config", ".", "n_ctx", ",", "config", ",", "scale", "=", "True", ")", "\n", "self", ".", "h", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "block", ")", "for", "_", "in", "range", "(", "config", ".", "n_layer", ")", "]", ")", "\n", "\n", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "# nn.init.normal_(self.embed.weight, std=0.02)", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTModel.set_num_special_tokens": [[599, 613], ["torch.Embedding", "torch.Embedding", "modeling_openai.OpenAIGPTModel.init_weights"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.BaseModel.init_weights"], ["", "def", "set_num_special_tokens", "(", "self", ",", "num_special_tokens", ")", ":", "\n", "        ", "\" Update input embeddings with new embedding matrice if needed \"", "\n", "if", "self", ".", "config", ".", "n_special", "==", "num_special_tokens", ":", "\n", "            ", "return", "\n", "# Update config", "\n", "", "self", ".", "config", ".", "n_special", "=", "num_special_tokens", "\n", "# # Build new embeddings and initialize", "\n", "old_embed", "=", "self", ".", "tokens_embed", "\n", "self", ".", "tokens_embed", "=", "nn", ".", "Embedding", "(", "self", ".", "config", ".", "total_tokens_embeddings", ",", "self", ".", "config", ".", "n_embd", ")", "\n", "# Initialize all new embeddings (in particular the special tokens)", "\n", "self", ".", "init_weights", "(", "self", ".", "tokens_embed", ")", "\n", "# Copy word and positional embeddings from the previous weights", "\n", "self", ".", "tokens_embed", ".", "weight", ".", "data", "[", ":", "self", ".", "config", ".", "vocab_size", ",", ":", "]", "=", "old_embed", ".", "weight", ".", "data", "[", ":", "self", ".", "config", ".", "vocab_size", ",", ":", "]", "\n", "self", ".", "tokens_embed", ".", "weight", ".", "data", "[", "-", "self", ".", "config", ".", "n_positions", ":", ",", ":", "]", "=", "old_embed", ".", "weight", ".", "data", "[", "-", "self", ".", "config", ".", "n_positions", ":", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTModel.forward": [[614, 641], ["input_ids.view.view.size", "input_ids.view.view.view", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.view", "modeling_openai.OpenAIGPTModel.tokens_embed", "modeling_openai.OpenAIGPTModel.positions_embed", "block.view", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "input_ids.view.view.size", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.size", "token_type_ids.view.view.view", "modeling_openai.OpenAIGPTModel.tokens_embed", "block", "input_ids.view.view.size", "token_type_ids.view.view.size", "block.size", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "position_ids", "=", "None", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "if", "position_ids", "is", "None", ":", "\n", "# This was used when we had a single embedding matrice from position and token embeddings", "\n", "# start = self.config.vocab_size + self.config.n_special", "\n", "# end = start + input_ids.size(-1)", "\n", "# position_ids = torch.arange(start, end, dtype=torch.long, device=input_ids.device)", "\n", "            ", "position_ids", "=", "torch", ".", "arange", "(", "input_ids", ".", "size", "(", "-", "1", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "\n", "", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "position_ids", "=", "position_ids", ".", "view", "(", "-", "1", ",", "position_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "\n", "inputs_embeds", "=", "self", ".", "tokens_embed", "(", "input_ids", ")", "\n", "position_embeds", "=", "self", ".", "positions_embed", "(", "position_ids", ")", "\n", "if", "token_type_ids", "is", "not", "None", ":", "\n", "            ", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "token_type_embeds", "=", "self", ".", "tokens_embed", "(", "token_type_ids", ")", "\n", "", "else", ":", "\n", "            ", "token_type_embeds", "=", "0", "\n", "# Add the position information to the input embeddings", "\n", "# h = e.sum(dim=2)", "\n", "", "hidden_states", "=", "inputs_embeds", "+", "position_embeds", "+", "token_type_embeds", "\n", "for", "block", "in", "self", ".", "h", ":", "\n", "            ", "hidden_states", "=", "block", "(", "hidden_states", ")", "\n", "", "output_shape", "=", "input_shape", "+", "(", "hidden_states", ".", "size", "(", "-", "1", ")", ",", ")", "\n", "return", "hidden_states", ".", "view", "(", "*", "output_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTLMHeadModel.__init__": [[699, 704], ["modeling_openai.OpenAIGPTPreTrainedModel.__init__", "modeling_openai.OpenAIGPTModel", "modeling_openai.OpenAIGPTLMHead", "modeling_openai.OpenAIGPTLMHeadModel.apply"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "OpenAIGPTLMHeadModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "transformer", "=", "OpenAIGPTModel", "(", "config", ")", "\n", "self", ".", "lm_head", "=", "OpenAIGPTLMHead", "(", "self", ".", "transformer", ".", "tokens_embed", ".", "weight", ",", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTLMHeadModel.set_num_special_tokens": [[705, 711], ["modeling_openai.OpenAIGPTLMHeadModel.transformer.set_num_special_tokens", "modeling_openai.OpenAIGPTLMHeadModel.lm_head.set_embeddings_weights"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTDoubleHeadsModel.set_num_special_tokens", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTLMHead.set_embeddings_weights"], ["", "def", "set_num_special_tokens", "(", "self", ",", "num_special_tokens", ")", ":", "\n", "        ", "\"\"\" Update input and output embeddings with new embedding matrice\n            Make sure we are sharing the embeddings\n        \"\"\"", "\n", "self", ".", "transformer", ".", "set_num_special_tokens", "(", "num_special_tokens", ")", "\n", "self", ".", "lm_head", ".", "set_embeddings_weights", "(", "self", ".", "transformer", ".", "tokens_embed", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTLMHeadModel.forward": [[712, 720], ["modeling_openai.OpenAIGPTLMHeadModel.transformer", "modeling_openai.OpenAIGPTLMHeadModel.lm_head", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_openai.OpenAIGPTLMHeadModel.view", "lm_labels.view", "modeling_openai.OpenAIGPTLMHeadModel.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "position_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "lm_labels", "=", "None", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "transformer", "(", "input_ids", ",", "position_ids", ",", "token_type_ids", ")", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "hidden_states", ")", "\n", "if", "lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "loss", "=", "loss_fct", "(", "lm_logits", ".", "view", "(", "-", "1", ",", "lm_logits", ".", "size", "(", "-", "1", ")", ")", ",", "lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "return", "lm_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTDoubleHeadsModel.__init__": [[783, 789], ["modeling_openai.OpenAIGPTPreTrainedModel.__init__", "modeling_openai.OpenAIGPTModel", "modeling_openai.OpenAIGPTLMHead", "modeling_openai.OpenAIGPTMultipleChoiceHead", "modeling_openai.OpenAIGPTDoubleHeadsModel.apply"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "OpenAIGPTDoubleHeadsModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "transformer", "=", "OpenAIGPTModel", "(", "config", ")", "\n", "self", ".", "lm_head", "=", "OpenAIGPTLMHead", "(", "self", ".", "transformer", ".", "tokens_embed", ".", "weight", ",", "config", ")", "\n", "self", ".", "multiple_choice_head", "=", "OpenAIGPTMultipleChoiceHead", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTDoubleHeadsModel.set_num_special_tokens": [[790, 796], ["modeling_openai.OpenAIGPTDoubleHeadsModel.transformer.set_num_special_tokens", "modeling_openai.OpenAIGPTDoubleHeadsModel.lm_head.set_embeddings_weights"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTDoubleHeadsModel.set_num_special_tokens", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTLMHead.set_embeddings_weights"], ["", "def", "set_num_special_tokens", "(", "self", ",", "num_special_tokens", ")", ":", "\n", "        ", "\"\"\" Update input and output embeddings with new embedding matrice\n            Make sure we are sharing the embeddings\n        \"\"\"", "\n", "self", ".", "transformer", ".", "set_num_special_tokens", "(", "num_special_tokens", ")", "\n", "self", ".", "lm_head", ".", "set_embeddings_weights", "(", "self", ".", "transformer", ".", "tokens_embed", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.OpenAIGPTDoubleHeadsModel.forward": [[797, 811], ["modeling_openai.OpenAIGPTDoubleHeadsModel.transformer", "modeling_openai.OpenAIGPTDoubleHeadsModel.lm_head", "modeling_openai.OpenAIGPTDoubleHeadsModel.multiple_choice_head", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "losses.append", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "losses.append", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_openai.OpenAIGPTDoubleHeadsModel.view", "lm_labels.view", "modeling_openai.OpenAIGPTDoubleHeadsModel.view", "mc_labels.view", "modeling_openai.OpenAIGPTDoubleHeadsModel.size", "modeling_openai.OpenAIGPTDoubleHeadsModel.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "mc_token_ids", ",", "lm_labels", "=", "None", ",", "mc_labels", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "transformer", "(", "input_ids", ",", "position_ids", ",", "token_type_ids", ")", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "hidden_states", ")", "\n", "mc_logits", "=", "self", ".", "multiple_choice_head", "(", "hidden_states", ",", "mc_token_ids", ")", "\n", "losses", "=", "[", "]", "\n", "if", "lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "losses", ".", "append", "(", "loss_fct", "(", "lm_logits", ".", "view", "(", "-", "1", ",", "lm_logits", ".", "size", "(", "-", "1", ")", ")", ",", "lm_labels", ".", "view", "(", "-", "1", ")", ")", ")", "\n", "", "if", "mc_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "losses", ".", "append", "(", "loss_fct", "(", "mc_logits", ".", "view", "(", "-", "1", ",", "mc_logits", ".", "size", "(", "-", "1", ")", ")", ",", "mc_labels", ".", "view", "(", "-", "1", ")", ")", ")", "\n", "", "if", "losses", ":", "\n", "            ", "return", "losses", "\n", "", "return", "lm_logits", ",", "mc_logits", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.load_tf_weights_in_openai_gpt": [[46, 114], ["print", "json.load", "json.load", "np.cumsum", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "json.load.pop", "init_params.pop", "init_params.pop", "zip", "io.open", "io.open", "np.load", "np.split", "param.reshape", "arr.squeeze", "name.split.split", "print", "torch.from_numpy", "torch.from_numpy", "np.prod", "range", "np.concatenate", "zip", "re.fullmatch", "re.split", "getattr", "len", "int", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["def", "load_tf_weights_in_openai_gpt", "(", "model", ",", "openai_checkpoint_folder_path", ")", ":", "\n", "    ", "\"\"\" Load tf pre-trained weights in a pytorch rpbert (from NumPy arrays here)\n    \"\"\"", "\n", "import", "re", "\n", "import", "numpy", "as", "np", "\n", "print", "(", "\"Loading weights...\"", ")", "\n", "names", "=", "json", ".", "load", "(", "open", "(", "openai_checkpoint_folder_path", "+", "'/parameters_names.json'", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", ")", "\n", "shapes", "=", "json", ".", "load", "(", "open", "(", "openai_checkpoint_folder_path", "+", "'/params_shapes.json'", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", ")", "\n", "offsets", "=", "np", ".", "cumsum", "(", "[", "np", ".", "prod", "(", "shape", ")", "for", "shape", "in", "shapes", "]", ")", "\n", "init_params", "=", "[", "np", ".", "load", "(", "openai_checkpoint_folder_path", "+", "'/params_{}.npy'", ".", "format", "(", "n", ")", ")", "for", "n", "in", "range", "(", "10", ")", "]", "\n", "init_params", "=", "np", ".", "split", "(", "np", ".", "concatenate", "(", "init_params", ",", "0", ")", ",", "offsets", ")", "[", ":", "-", "1", "]", "\n", "init_params", "=", "[", "param", ".", "reshape", "(", "shape", ")", "for", "param", ",", "shape", "in", "zip", "(", "init_params", ",", "shapes", ")", "]", "\n", "\n", "# This was used when we had a single embedding matrix for positions and tokens", "\n", "# init_params[0] = np.concatenate([init_params[1], init_params[0]], 0)", "\n", "# del init_params[1]", "\n", "init_params", "=", "[", "arr", ".", "squeeze", "(", ")", "for", "arr", "in", "init_params", "]", "\n", "\n", "try", ":", "\n", "        ", "assert", "model", ".", "tokens_embed", ".", "weight", ".", "shape", "==", "init_params", "[", "1", "]", ".", "shape", "\n", "assert", "model", ".", "positions_embed", ".", "weight", ".", "shape", "==", "init_params", "[", "0", "]", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "        ", "e", ".", "args", "+=", "(", "model", ".", "tokens_embed", ".", "weight", ".", "shape", ",", "init_params", "[", "1", "]", ".", "shape", ")", "\n", "e", ".", "args", "+=", "(", "model", ".", "positions_embed", ".", "weight", ".", "shape", ",", "init_params", "[", "0", "]", ".", "shape", ")", "\n", "raise", "\n", "\n", "", "model", ".", "tokens_embed", ".", "weight", ".", "data", "=", "torch", ".", "from_numpy", "(", "init_params", "[", "1", "]", ")", "\n", "model", ".", "positions_embed", ".", "weight", ".", "data", "=", "torch", ".", "from_numpy", "(", "init_params", "[", "0", "]", ")", "\n", "names", ".", "pop", "(", "0", ")", "\n", "# Pop position and token embedding arrays", "\n", "init_params", ".", "pop", "(", "0", ")", "\n", "init_params", ".", "pop", "(", "0", ")", "\n", "\n", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "init_params", ")", ":", "# names[1:n_transfer], init_params[1:n_transfer]):", "\n", "        ", "name", "=", "name", "[", "6", ":", "]", "# skip \"rpbert/\"", "\n", "assert", "name", "[", "-", "2", ":", "]", "==", "\":0\"", "\n", "name", "=", "name", "[", ":", "-", "2", "]", "\n", "name", "=", "name", ".", "split", "(", "'/'", ")", "\n", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r'[A-Za-z]+\\d+'", ",", "m_name", ")", ":", "\n", "                ", "l", "=", "re", ".", "split", "(", "r'(\\d+)'", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "l", "=", "[", "m_name", "]", "\n", "", "if", "l", "[", "0", "]", "==", "'g'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'b'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'bias'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'w'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "else", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "\n", "", "if", "len", "(", "l", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "l", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "print", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.gelu": [[116, 118], ["torch.tanh", "torch.tanh", "math.sqrt", "torch.pow", "torch.pow"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "return", "0.5", "*", "x", "*", "(", "1", "+", "torch", ".", "tanh", "(", "math", ".", "sqrt", "(", "2", "/", "math", ".", "pi", ")", "*", "(", "x", "+", "0.044715", "*", "torch", ".", "pow", "(", "x", ",", "3", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_openai.swish": [[120, 122], ["torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.optimization_openai.OpenAIAdam.__init__": [[45, 64], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "schedule", "=", "'warmup_linear'", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "\n", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-8", ",", "weight_decay", "=", "0", ",", "\n", "vector_l2", "=", "False", ",", "max_grad_norm", "=", "-", "1", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "lr", "is", "not", "required", "and", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "schedule", "not", "in", "SCHEDULES", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid schedule parameter: {}\"", ".", "format", "(", "schedule", ")", ")", "\n", "", "if", "not", "0.0", "<=", "warmup", "<", "1.0", "and", "not", "warmup", "==", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\"", ".", "format", "(", "warmup", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b1", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b1 parameter: {}\"", ".", "format", "(", "b1", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b2", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b2 parameter: {}\"", ".", "format", "(", "b2", ")", ")", "\n", "", "if", "not", "e", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "e", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "schedule", "=", "schedule", ",", "warmup", "=", "warmup", ",", "t_total", "=", "t_total", ",", "\n", "b1", "=", "b1", ",", "b2", "=", "b2", ",", "e", "=", "e", ",", "weight_decay", "=", "weight_decay", ",", "vector_l2", "=", "vector_l2", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ")", "\n", "super", "(", "OpenAIAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.optimization_openai.OpenAIAdam.get_lr": [[65, 79], ["lr.append", "len", "schedule_fct"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "lr", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "return", "[", "0", "]", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "", "lr", ".", "append", "(", "lr_scheduled", ")", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.optimization_openai.OpenAIAdam.step": [[80, 141], ["closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "exp_avg_sq.sqrt().add_", "p.data.addcdiv_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.nn.utils.clip_grad_norm_", "p.data.add_", "exp_avg.mul_", "exp_avg_sq.mul_", "exp_avg_sq.sqrt", "schedule_fct", "math.sqrt", "len", "p.size"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the rpbert\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# Add grad clipping", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "                    ", "clip_grad_norm_", "(", "p", ",", "group", "[", "'max_grad_norm'", "]", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'e'", "]", ")", "\n", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "\n", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "step_size", "=", "lr_scheduled", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", "\n", "p", ".", "data", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "\n", "# Add weight decay at the end (fixed version)", "\n", "if", "(", "len", "(", "p", ".", "size", "(", ")", ")", ">", "1", "or", "group", "[", "'vector_l2'", "]", ")", "and", "group", "[", "'weight_decay'", "]", ">", "0", ":", "\n", "                    ", "p", ".", "data", ".", "add_", "(", "-", "lr_scheduled", "*", "group", "[", "'weight_decay'", "]", ",", "p", ".", "data", ")", "\n", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.optimization_openai.warmup_cosine": [[23, 26], ["torch.cos"], "function", ["None"], ["def", "warmup_cosine", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "s", "=", "1", "if", "x", "<=", "warmup", "else", "0", "\n", "return", "s", "*", "(", "x", "/", "warmup", ")", "+", "(", "1", "-", "s", ")", "*", "(", "0.5", "*", "(", "1", "+", "torch", ".", "cos", "(", "math", ".", "pi", "*", "x", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.optimization_openai.warmup_constant": [[27, 30], ["None"], "function", ["None"], ["", "def", "warmup_constant", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "s", "=", "1", "if", "x", "<=", "warmup", "else", "0", "\n", "return", "s", "*", "(", "x", "/", "warmup", ")", "+", "(", "1", "-", "s", ")", "*", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.optimization_openai.warmup_linear": [[31, 34], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "s", "=", "1", "if", "x", "<=", "warmup", "else", "0", "\n", "return", "(", "s", "*", "(", "x", "/", "warmup", ")", "+", "(", "1", "-", "s", ")", ")", "*", "(", "1", "-", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained": [[80, 122], ["cls", "os.path.join", "os.path.join", "file_utils.cached_path", "file_utils.cached_path", "logger.info", "logger.info", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained rpbert file.\n        Download and cache the pre-trained rpbert file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "merges_file", "=", "PRETRAINED_MERGES_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "VOCAB_NAME", ")", "\n", "merges_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "MERGES_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "resolved_merges_file", "=", "cached_path", "(", "merges_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in rpbert name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find files {} and {} \"", "\n", "\"at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", "vocab_file", ",", "merges_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", "and", "resolved_merges_file", "==", "merges_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "logger", ".", "info", "(", "\"loading merges file {}\"", ".", "format", "(", "merges_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "logger", ".", "info", "(", "\"loading merges file {} from cache at {}\"", ".", "format", "(", "\n", "merges_file", ",", "resolved_merges_file", ")", ")", "\n", "", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained rpbert, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "resolved_merges_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.__init__": [[123, 143], ["json.load", "dict", "tokenization_openai.OpenAIGPTTokenizer.set_special_tokens", "spacy.load", "int", "io.open", "io.open().read().split", "tuple", "zip", "logger.warning", "tokenization.BasicTokenizer", "tokenization_openai.OpenAIGPTTokenizer.encoder.items", "merge.split", "range", "io.open().read", "len", "io.open"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.set_special_tokens", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["", "def", "__init__", "(", "self", ",", "vocab_file", ",", "merges_file", ",", "special_tokens", "=", "None", ",", "max_len", "=", "None", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "import", "ftfy", "\n", "import", "spacy", "\n", "self", ".", "nlp", "=", "spacy", ".", "load", "(", "'en'", ",", "disable", "=", "[", "'parser'", ",", "'tagger'", ",", "'ner'", ",", "'textcat'", "]", ")", "\n", "self", ".", "fix_text", "=", "ftfy", ".", "fix_text", "\n", "", "except", "ImportError", ":", "\n", "            ", "logger", ".", "warning", "(", "\"ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\"", ")", "\n", "self", ".", "nlp", "=", "BasicTokenizer", "(", "do_lower_case", "=", "True", ",", "\n", "never_split", "=", "special_tokens", "if", "special_tokens", "is", "not", "None", "else", "[", "]", ")", "\n", "self", ".", "fix_text", "=", "None", "\n", "\n", "", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "self", ".", "encoder", "=", "json", ".", "load", "(", "open", "(", "vocab_file", ",", "encoding", "=", "\"utf-8\"", ")", ")", "\n", "self", ".", "decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "encoder", ".", "items", "(", ")", "}", "\n", "merges", "=", "open", "(", "merges_file", ",", "encoding", "=", "'utf-8'", ")", ".", "read", "(", ")", ".", "split", "(", "'\\n'", ")", "[", "1", ":", "-", "1", "]", "\n", "merges", "=", "[", "tuple", "(", "merge", ".", "split", "(", ")", ")", "for", "merge", "in", "merges", "]", "\n", "self", ".", "bpe_ranks", "=", "dict", "(", "zip", "(", "merges", ",", "range", "(", "len", "(", "merges", ")", ")", ")", ")", "\n", "self", ".", "cache", "=", "{", "}", "\n", "self", ".", "set_special_tokens", "(", "special_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.__len__": [[144, 146], ["len", "len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "encoder", ")", "+", "len", "(", "self", ".", "special_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.set_special_tokens": [[147, 162], ["dict", "logger.info", "tokenization_openai.OpenAIGPTTokenizer.special_tokens.items", "enumerate", "len"], "methods", ["None"], ["", "def", "set_special_tokens", "(", "self", ",", "special_tokens", ")", ":", "\n", "        ", "\"\"\" Add a list of additional tokens to the encoder.\n            The additional tokens are indexed starting from the last index of the\n            current vocabulary in the order of the `special_tokens` list.\n        \"\"\"", "\n", "if", "not", "special_tokens", ":", "\n", "            ", "self", ".", "special_tokens", "=", "{", "}", "\n", "self", ".", "special_tokens_decoder", "=", "{", "}", "\n", "return", "\n", "", "self", ".", "special_tokens", "=", "dict", "(", "(", "tok", ",", "len", "(", "self", ".", "encoder", ")", "+", "i", ")", "for", "i", ",", "tok", "in", "enumerate", "(", "special_tokens", ")", ")", "\n", "self", ".", "special_tokens_decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "special_tokens", ".", "items", "(", ")", "}", "\n", "if", "self", ".", "fix_text", "is", "None", ":", "\n", "# Using BERT's BasicTokenizer: we can update the tokenizer", "\n", "            ", "self", ".", "nlp", ".", "never_split", "=", "special_tokens", "\n", "", "logger", ".", "info", "(", "\"Special tokens {}\"", ".", "format", "(", "self", ".", "special_tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.bpe": [[163, 205], ["tokenization_openai.get_pairs", "tuple", "min", "tuple", "len", "len", "tokenization_openai.get_pairs", "word.index", "tuple.extend", "tuple.append", "tuple.append", "tokenization_openai.OpenAIGPTTokenizer.bpe_ranks.get", "tuple.extend", "float", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.get_pairs", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.get_pairs"], ["", "def", "bpe", "(", "self", ",", "token", ")", ":", "\n", "        ", "word", "=", "tuple", "(", "token", "[", ":", "-", "1", "]", ")", "+", "(", "token", "[", "-", "1", "]", "+", "'</w>'", ",", ")", "\n", "if", "token", "in", "self", ".", "cache", ":", "\n", "            ", "return", "self", ".", "cache", "[", "token", "]", "\n", "", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "\n", "if", "not", "pairs", ":", "\n", "            ", "return", "token", "+", "'</w>'", "\n", "\n", "", "while", "True", ":", "\n", "            ", "bigram", "=", "min", "(", "pairs", ",", "key", "=", "lambda", "pair", ":", "self", ".", "bpe_ranks", ".", "get", "(", "pair", ",", "float", "(", "'inf'", ")", ")", ")", "\n", "if", "bigram", "not", "in", "self", ".", "bpe_ranks", ":", "\n", "                ", "break", "\n", "", "first", ",", "second", "=", "bigram", "\n", "new_word", "=", "[", "]", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "word", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "j", "=", "word", ".", "index", "(", "first", ",", "i", ")", "\n", "new_word", ".", "extend", "(", "word", "[", "i", ":", "j", "]", ")", "\n", "i", "=", "j", "\n", "", "except", ":", "\n", "                    ", "new_word", ".", "extend", "(", "word", "[", "i", ":", "]", ")", "\n", "break", "\n", "\n", "", "if", "word", "[", "i", "]", "==", "first", "and", "i", "<", "len", "(", "word", ")", "-", "1", "and", "word", "[", "i", "+", "1", "]", "==", "second", ":", "\n", "                    ", "new_word", ".", "append", "(", "first", "+", "second", ")", "\n", "i", "+=", "2", "\n", "", "else", ":", "\n", "                    ", "new_word", ".", "append", "(", "word", "[", "i", "]", ")", "\n", "i", "+=", "1", "\n", "", "", "new_word", "=", "tuple", "(", "new_word", ")", "\n", "word", "=", "new_word", "\n", "if", "len", "(", "word", ")", "==", "1", ":", "\n", "                ", "break", "\n", "", "else", ":", "\n", "                ", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "", "", "word", "=", "' '", ".", "join", "(", "word", ")", "\n", "if", "word", "==", "'\\n  </w>'", ":", "\n", "            ", "word", "=", "'\\n</w>'", "\n", "", "self", ".", "cache", "[", "token", "]", "=", "word", "\n", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.tokenize": [[206, 220], ["tokenization_openai.OpenAIGPTTokenizer.nlp.tokenize", "tokenization_openai.OpenAIGPTTokenizer.nlp", "split_tokens.extend", "tokenization_openai.text_standardize", "split_tokens.extend", "tokenization_openai.OpenAIGPTTokenizer.fix_text", "tokenization_openai.OpenAIGPTTokenizer.bpe().split", "tokenization_openai.OpenAIGPTTokenizer.bpe().split", "tokenization_openai.OpenAIGPTTokenizer.bpe", "tokenization_openai.OpenAIGPTTokenizer.bpe", "token.text.lower"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.text_standardize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.bpe", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.bpe"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\" Tokenize a string. \"\"\"", "\n", "split_tokens", "=", "[", "]", "\n", "if", "self", ".", "fix_text", "is", "None", ":", "\n", "# Using BERT's BasicTokenizer", "\n", "            ", "text", "=", "self", ".", "nlp", ".", "tokenize", "(", "text", ")", "\n", "for", "token", "in", "text", ":", "\n", "                ", "split_tokens", ".", "extend", "(", "[", "t", "for", "t", "in", "self", ".", "bpe", "(", "token", ")", ".", "split", "(", "' '", ")", "]", ")", "\n", "", "", "else", ":", "\n", "# Using SpaCy & ftfy (original tokenization process of OpenAI GPT)", "\n", "            ", "text", "=", "self", ".", "nlp", "(", "text_standardize", "(", "self", ".", "fix_text", "(", "text", ")", ")", ")", "\n", "for", "token", "in", "text", ":", "\n", "                ", "split_tokens", ".", "extend", "(", "[", "t", "for", "t", "in", "self", ".", "bpe", "(", "token", ".", "text", ".", "lower", "(", ")", ")", ".", "split", "(", "' '", ")", "]", ")", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.convert_tokens_to_ids": [[221, 241], ["isinstance", "len", "ValueError", "isinstance", "tokenization_openai.OpenAIGPTTokenizer.encoder.get", "ids.append", "ids.append", "tokenization_openai.OpenAIGPTTokenizer.encoder.get", "len"], "methods", ["None"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\" Converts a sequence of tokens into ids using the vocab. \"\"\"", "\n", "ids", "=", "[", "]", "\n", "if", "isinstance", "(", "tokens", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "tokens", ",", "unicode", ")", ")", ":", "\n", "            ", "if", "tokens", "in", "self", ".", "special_tokens", ":", "\n", "                ", "return", "self", ".", "special_tokens", "[", "tokens", "]", "\n", "", "else", ":", "\n", "                ", "return", "self", ".", "encoder", ".", "get", "(", "tokens", ",", "0", ")", "\n", "", "", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "token", "in", "self", ".", "special_tokens", ":", "\n", "                ", "ids", ".", "append", "(", "self", ".", "special_tokens", "[", "token", "]", ")", "\n", "", "else", ":", "\n", "                ", "ids", ".", "append", "(", "self", ".", "encoder", ".", "get", "(", "token", ",", "0", ")", ")", "\n", "", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this OpenAI GPT rpbert ({} > {}). Running this\"", "\n", "\" sequence through the rpbert will result in indexing errors\"", ".", "format", "(", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.convert_ids_to_tokens": [[242, 252], ["tokens.append", "tokens.append"], "methods", ["None"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ",", "skip_special_tokens", "=", "False", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in BPE tokens using the vocab.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "if", "i", "in", "self", ".", "special_tokens_decoder", ":", "\n", "                ", "if", "not", "skip_special_tokens", ":", "\n", "                    ", "tokens", ".", "append", "(", "self", ".", "special_tokens_decoder", "[", "i", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "tokens", ".", "append", "(", "self", ".", "decoder", "[", "i", "]", ")", "\n", "", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.decode": [[253, 264], ["tokenization_openai.OpenAIGPTTokenizer.convert_ids_to_tokens", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.convert_ids_to_tokens"], ["", "def", "decode", "(", "self", ",", "ids", ",", "skip_special_tokens", "=", "False", ",", "clean_up_tokenization_spaces", "=", "False", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in a string.\"\"\"", "\n", "tokens", "=", "self", ".", "convert_ids_to_tokens", "(", "ids", ",", "skip_special_tokens", "=", "skip_special_tokens", ")", "\n", "out_string", "=", "''", ".", "join", "(", "tokens", ")", ".", "replace", "(", "'</w>'", ",", "' '", ")", ".", "strip", "(", ")", "\n", "if", "clean_up_tokenization_spaces", ":", "\n", "            ", "out_string", "=", "out_string", ".", "replace", "(", "'<unk>'", ",", "''", ")", "\n", "out_string", "=", "out_string", ".", "replace", "(", "' .'", ",", "'.'", ")", ".", "replace", "(", "' ?'", ",", "'?'", ")", ".", "replace", "(", "' !'", ",", "'!'", ")", ".", "replace", "(", "' ,'", ",", "','", ")", ".", "replace", "(", "' ,'", ",", "','", "\n", ")", ".", "replace", "(", "\" n't\"", ",", "\"n't\"", ")", ".", "replace", "(", "\" 'm\"", ",", "\"'m\"", ")", ".", "replace", "(", "\" 're\"", ",", "\"'re\"", ")", ".", "replace", "(", "\" do not\"", ",", "\" don't\"", "\n", ")", ".", "replace", "(", "\" 's\"", ",", "\"'s\"", ")", ".", "replace", "(", "\" t \"", ",", "\"'t \"", ")", ".", "replace", "(", "\" s \"", ",", "\"'s \"", ")", ".", "replace", "(", "\" m \"", ",", "\"'m \"", "\n", ")", ".", "replace", "(", "\" 've\"", ",", "\"'ve\"", ")", "\n", "", "return", "out_string", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.get_pairs": [[45, 56], ["set", "set.add"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.SearchSpace.add"], ["def", "get_pairs", "(", "word", ")", ":", "\n", "    ", "\"\"\"\n    Return set of symbol pairs in a word.\n    word is represented as tuple of symbols (symbols being variable-length strings)\n    \"\"\"", "\n", "pairs", "=", "set", "(", ")", "\n", "prev_char", "=", "word", "[", "0", "]", "\n", "for", "char", "in", "word", "[", "1", ":", "]", ":", "\n", "        ", "pairs", ".", "add", "(", "(", "prev_char", ",", "char", ")", ")", "\n", "prev_char", "=", "char", "\n", "", "return", "pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.text_standardize": [[57, 71], ["re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub", "re.sub", "re.sub", "re.sub.strip"], "function", ["None"], ["", "def", "text_standardize", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    fixes some issues the spacy tokenizer had on books corpus\n    also does some whitespace standardization\n    \"\"\"", "\n", "text", "=", "text", ".", "replace", "(", "'\u2014'", ",", "'-'", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\u2013'", ",", "'-'", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\u2015'", ",", "'-'", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\u2026'", ",", "'...'", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\u00b4'", ",", "\"'\"", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'''(-+|~+|!+|\"+|;+|\\?+|\\++|,+|\\)+|\\(+|\\\\+|\\/+|\\*+|\\[+|\\]+|}+|{+|\\|+|_+)'''", ",", "r' \\1 '", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'\\s*\\n\\s*'", ",", "' \\n '", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'[^\\S\\n]+'", ",", "' '", ",", "text", ")", "\n", "return", "text", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.samplers.FlairSampler.set_dataset": [[13, 19], ["len"], "methods", ["None"], ["    ", "def", "set_dataset", "(", "self", ",", "data_source", ")", ":", "\n", "        ", "\"\"\"Initialize by passing a block_size and a plus_window parameter.\n        :param data_source: dataset to sample from\n        \"\"\"", "\n", "self", ".", "data_source", "=", "data_source", "\n", "self", ".", "num_samples", "=", "len", "(", "self", ".", "data_source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.samplers.FlairSampler.__len__": [[20, 22], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.samplers.ImbalancedClassificationDatasetSampler.__init__": [[28, 30], ["torch.utils.data.sampler.Sampler.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "ImbalancedClassificationDatasetSampler", ",", "self", ")", ".", "__init__", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.samplers.ImbalancedClassificationDatasetSampler.set_dataset": [[31, 54], ["len", "list", "collections.defaultdict", "torch.DoubleTensor", "range", "len"], "methods", ["None"], ["", "def", "set_dataset", "(", "self", ",", "data_source", ":", "FlairDataset", ")", ":", "\n", "        ", "\"\"\"\n        Initialize by passing a classification dataset with labels, i.e. either TextClassificationDataSet or\n        :param data_source:\n        \"\"\"", "\n", "self", ".", "data_source", "=", "data_source", "\n", "self", ".", "num_samples", "=", "len", "(", "self", ".", "data_source", ")", "\n", "self", ".", "indices", "=", "list", "(", "range", "(", "len", "(", "data_source", ")", ")", ")", "\n", "\n", "# first determine the distribution of classes in the dataset", "\n", "label_count", "=", "defaultdict", "(", "int", ")", "\n", "for", "sentence", "in", "data_source", ":", "\n", "            ", "for", "label", "in", "sentence", ".", "labels", ":", "\n", "                ", "label_count", "[", "label", ".", "value", "]", "+=", "1", "\n", "\n", "# weight for each sample", "\n", "", "", "offset", "=", "0", "\n", "weights", "=", "[", "\n", "1.0", "/", "(", "offset", "+", "label_count", "[", "data_source", "[", "idx", "]", ".", "labels", "[", "0", "]", ".", "value", "]", ")", "\n", "for", "idx", "in", "self", ".", "indices", "\n", "]", "\n", "\n", "self", ".", "weights", "=", "torch", ".", "DoubleTensor", "(", "weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.samplers.ImbalancedClassificationDatasetSampler.__iter__": [[55, 59], ["torch.multinomial"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "(", "\n", "self", ".", "indices", "[", "i", "]", "\n", "for", "i", "in", "torch", ".", "multinomial", "(", "self", ".", "weights", ",", "self", ".", "num_samples", ",", "replacement", "=", "True", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.samplers.ChunkSampler.__init__": [[67, 72], ["torch.utils.data.sampler.Sampler.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "block_size", "=", "5", ",", "plus_window", "=", "5", ")", ":", "\n", "        ", "super", "(", "ChunkSampler", ",", "self", ")", ".", "__init__", "(", "None", ")", "\n", "self", ".", "block_size", "=", "block_size", "\n", "self", ".", "plus_window", "=", "plus_window", "\n", "self", ".", "data_source", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.samplers.ChunkSampler.__iter__": [[73, 89], ["list", "log.info", "random.shuffle", "iter", "range", "random.randint", "len", "range", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "data", "=", "list", "(", "range", "(", "len", "(", "self", ".", "data_source", ")", ")", ")", "\n", "\n", "blocksize", "=", "self", ".", "block_size", "+", "random", ".", "randint", "(", "0", ",", "self", ".", "plus_window", ")", "\n", "\n", "log", ".", "info", "(", "\n", "f\"Chunk sampling with blocksize = {blocksize} ({self.block_size} + {self.plus_window})\"", "\n", ")", "\n", "\n", "# Create blocks", "\n", "blocks", "=", "[", "data", "[", "i", ":", "i", "+", "blocksize", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "data", ")", ",", "blocksize", ")", "]", "\n", "# shuffle the blocks", "\n", "random", ".", "shuffle", "(", "blocks", ")", "\n", "# concatenate the shuffled blocks", "\n", "data", "[", ":", "]", "=", "[", "b", "for", "bs", "in", "blocks", "for", "b", "in", "bs", "]", "\n", "return", "iter", "(", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.samplers.ExpandingChunkSampler.__init__": [[96, 104], ["torch.utils.data.sampler.Sampler.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "step", "=", "3", ")", ":", "\n", "        ", "\"\"\"Initialize by passing a block_size and a plus_window parameter.\n        :param data_source: dataset to sample from\n        \"\"\"", "\n", "super", "(", "ExpandingChunkSampler", ",", "self", ")", ".", "__init__", "(", "None", ")", "\n", "self", ".", "block_size", "=", "1", "\n", "self", ".", "epoch_count", "=", "0", "\n", "self", ".", "step", "=", "step", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.samplers.ExpandingChunkSampler.__iter__": [[105, 125], ["list", "log.info", "random.shuffle", "iter", "range", "len", "range", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "self", ".", "epoch_count", "+=", "1", "\n", "\n", "data", "=", "list", "(", "range", "(", "len", "(", "self", ".", "data_source", ")", ")", ")", "\n", "\n", "log", ".", "info", "(", "f\"Chunk sampling with blocksize = {self.block_size}\"", ")", "\n", "\n", "# Create blocks", "\n", "blocks", "=", "[", "\n", "data", "[", "i", ":", "i", "+", "self", ".", "block_size", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "data", ")", ",", "self", ".", "block_size", ")", "\n", "]", "\n", "# shuffle the blocks", "\n", "random", ".", "shuffle", "(", "blocks", ")", "\n", "# concatenate the shuffled blocks", "\n", "data", "[", ":", "]", "=", "[", "b", "for", "bs", "in", "blocks", "for", "b", "in", "bs", "]", "\n", "\n", "if", "self", ".", "epoch_count", "%", "self", ".", "step", "==", "0", ":", "\n", "            ", "self", ".", "block_size", "+=", "1", "\n", "\n", "", "return", "iter", "(", "data", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_corpora": [[119, 126], ["deprecated.deprecated.deprecated", "flair.data.MultiCorpus", "data_fetcher.NLPTaskDataFetcher.load_corpus"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_corpus"], ["    ", "@", "staticmethod", "\n", "@", "deprecated", "(", "version", "=", "\"0.4.1\"", ",", "reason", "=", "\"Use 'flair.datasets' instead.\"", ")", "\n", "def", "load_corpora", "(", "\n", "tasks", ":", "List", "[", "Union", "[", "NLPTask", ",", "str", "]", "]", ",", "base_path", ":", "Path", "=", "None", "\n", ")", "->", "MultiCorpus", ":", "\n", "        ", "return", "MultiCorpus", "(", "\n", "[", "NLPTaskDataFetcher", ".", "load_corpus", "(", "task", ",", "base_path", ")", "for", "task", "in", "tasks", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_corpus": [[128, 254], ["deprecated.deprecated.deprecated", "task.startswith", "type", "data_fetcher.NLPTaskDataFetcher.download_dataset", "type", "pathlib.Path", "task.lower", "data_fetcher.NLPTaskDataFetcher.load_column_corpus", "data_fetcher.NLPTaskDataFetcher.load_column_corpus", "data_fetcher.NLPTaskDataFetcher.load_column_corpus", "task.startswith", "data_fetcher.NLPTaskDataFetcher.load_column_corpus", "data_fetcher.NLPTaskDataFetcher.load_column_corpus", "data_fetcher.NLPTaskDataFetcher.load_column_corpus", "data_fetcher.NLPTaskDataFetcher.load_column_corpus", "task.startswith", "data_fetcher.NLPTaskDataFetcher.load_ud_corpus", "data_fetcher.NLPTaskDataFetcher.load_classification_corpus", "data_fetcher.NLPTaskDataFetcher.load_column_corpus", "data_fetcher.NLPTaskDataFetcher.load_classification_corpus", "pathlib.Path", "type"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.download_dataset", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_column_corpus", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_column_corpus", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_column_corpus", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_column_corpus", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_column_corpus", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_column_corpus", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_column_corpus", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_ud_corpus", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_classification_corpus", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_column_corpus", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_classification_corpus"], ["", "@", "staticmethod", "\n", "@", "deprecated", "(", "version", "=", "\"0.4.1\"", ",", "reason", "=", "\"Use 'flair.datasets' instead.\"", ")", "\n", "def", "load_corpus", "(", "task", ":", "Union", "[", "NLPTask", ",", "str", "]", ",", "base_path", ":", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "Corpus", ":", "\n", "        ", "\"\"\"\n        Helper function to fetch a Corpus for a specific NLPTask. For this to work you need to first download\n        and put into the appropriate folder structure the corresponding NLP task data. The tutorials on\n        https://github.com/zalandoresearch/flair give more info on how to do this. Alternatively, you can use this\n        code to create your own data fetchers.\n        :param task: specification of the NLPTask you wish to get\n        :param base_path: path to data folder containing tasks sub folders\n        :return: a Corpus consisting of train, dev and test data\n        \"\"\"", "\n", "\n", "# first, try to fetch dataset online", "\n", "if", "type", "(", "task", ")", "is", "NLPTask", ":", "\n", "            ", "NLPTaskDataFetcher", ".", "download_dataset", "(", "task", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "\n", "", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# get string value if enum is passed", "\n", "", "task", "=", "task", ".", "value", "if", "type", "(", "task", ")", "is", "NLPTask", "else", "task", "\n", "\n", "data_folder", "=", "base_path", "/", "task", ".", "lower", "(", ")", "\n", "\n", "# the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)", "\n", "if", "task", "==", "NLPTask", ".", "CONLL_2000", ".", "value", ":", "\n", "            ", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"np\"", "}", "\n", "\n", "return", "NLPTaskDataFetcher", ".", "load_column_corpus", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_biloes", "=", "\"np\"", "\n", ")", "\n", "\n", "# many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag", "\n", "", "if", "(", "\n", "task", "==", "NLPTask", ".", "CONLL_03", ".", "value", "\n", "or", "task", "==", "NLPTask", ".", "ONTONER", ".", "value", "\n", "or", "task", "==", "NLPTask", ".", "FASHION", ".", "value", "\n", ")", ":", "\n", "            ", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"np\"", ",", "3", ":", "\"ner\"", "}", "\n", "\n", "return", "NLPTaskDataFetcher", ".", "load_column_corpus", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_biloes", "=", "\"ner\"", "\n", ")", "\n", "\n", "# the CoNLL 03 task for German has an additional lemma column", "\n", "", "if", "task", "==", "NLPTask", ".", "CONLL_03_GERMAN", ".", "value", ":", "\n", "            ", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"lemma\"", ",", "2", ":", "\"pos\"", ",", "3", ":", "\"np\"", ",", "4", ":", "\"ner\"", "}", "\n", "\n", "return", "NLPTaskDataFetcher", ".", "load_column_corpus", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_biloes", "=", "\"ner\"", "\n", ")", "\n", "\n", "# the CoNLL 03 task for Dutch has no NP column", "\n", "", "if", "task", "==", "NLPTask", ".", "CONLL_03_DUTCH", ".", "value", "or", "task", ".", "startswith", "(", "\"wikiner\"", ")", ":", "\n", "            ", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "return", "NLPTaskDataFetcher", ".", "load_column_corpus", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_biloes", "=", "\"ner\"", "\n", ")", "\n", "\n", "# the CoNLL 03 task for Spanish only has two columns", "\n", "", "if", "task", "==", "NLPTask", ".", "CONLL_03_SPANISH", ".", "value", "or", "task", "==", "NLPTask", ".", "WNUT_17", ".", "value", ":", "\n", "            ", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"ner\"", "}", "\n", "\n", "return", "NLPTaskDataFetcher", ".", "load_column_corpus", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_biloes", "=", "\"ner\"", "\n", ")", "\n", "\n", "# the GERMEVAL task only has two columns: text and ner", "\n", "", "if", "task", "==", "NLPTask", ".", "GERMEVAL", ".", "value", ":", "\n", "            ", "columns", "=", "{", "1", ":", "\"text\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "return", "NLPTaskDataFetcher", ".", "load_column_corpus", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_biloes", "=", "\"ner\"", "\n", ")", "\n", "\n", "# WSD tasks may be put into this column format", "\n", "", "if", "task", "==", "NLPTask", ".", "WSD", ".", "value", ":", "\n", "            ", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"lemma\"", ",", "2", ":", "\"pos\"", ",", "3", ":", "\"sense\"", "}", "\n", "return", "NLPTaskDataFetcher", ".", "load_column_corpus", "(", "\n", "data_folder", ",", "\n", "columns", ",", "\n", "train_file", "=", "\"semcor.tsv\"", ",", "\n", "test_file", "=", "\"semeval2015.tsv\"", ",", "\n", ")", "\n", "\n", "# the UD corpora follow the CoNLL-U format, for which we have a special reader", "\n", "", "if", "task", ".", "startswith", "(", "\"ud_\"", ")", "or", "task", "in", "[", "\n", "NLPTask", ".", "ONTONOTES", ".", "value", ",", "\n", "NLPTask", ".", "CONLL_12", ".", "value", ",", "\n", "NLPTask", ".", "PENN", ".", "value", ",", "\n", "]", ":", "\n", "            ", "return", "NLPTaskDataFetcher", ".", "load_ud_corpus", "(", "data_folder", ")", "\n", "\n", "# for text classifiers, we use our own special format", "\n", "", "if", "task", "in", "[", "\n", "NLPTask", ".", "IMDB", ".", "value", ",", "\n", "NLPTask", ".", "AG_NEWS", ".", "value", ",", "\n", "NLPTask", ".", "TREC_6", ".", "value", ",", "\n", "NLPTask", ".", "TREC_50", ".", "value", ",", "\n", "NLPTask", ".", "REGRESSION", ".", "value", ",", "\n", "]", ":", "\n", "            ", "tokenizer", ":", "Callable", "[", "[", "str", "]", ",", "List", "[", "Token", "]", "]", "=", "space_tokenizer", "if", "task", "in", "[", "\n", "NLPTask", ".", "TREC_6", ".", "value", ",", "\n", "NLPTask", ".", "TREC_50", ".", "value", ",", "\n", "]", "else", "segtok_tokenizer", "\n", "\n", "return", "NLPTaskDataFetcher", ".", "load_classification_corpus", "(", "\n", "data_folder", ",", "tokenizer", "=", "tokenizer", "\n", ")", "\n", "\n", "# NER corpus for Basque", "\n", "", "if", "task", "==", "NLPTask", ".", "NER_BASQUE", ".", "value", ":", "\n", "            ", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"ner\"", "}", "\n", "return", "NLPTaskDataFetcher", ".", "load_column_corpus", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_biloes", "=", "\"ner\"", "\n", ")", "\n", "\n", "", "if", "task", ".", "startswith", "(", "\"wassa\"", ")", ":", "\n", "            ", "return", "NLPTaskDataFetcher", ".", "load_classification_corpus", "(", "\n", "data_folder", ",", "tokenizer", "=", "segtok_tokenizer", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_column_corpus": [[256, 355], ["deprecated.deprecated.deprecated", "log.info", "log.info", "log.info", "log.info", "data_fetcher.NLPTaskDataFetcher.read_column_data", "flair.data.Corpus", "type", "pathlib.Path", "data_folder.iterdir", "data_fetcher.NLPTaskDataFetcher.read_column_data", "data_fetcher.NLPTaskDataFetcher.read_column_data", "file_name.endswith", "data_folder.iterdir", "sentence.convert_tag_scheme", "file_name.endswith", "data_fetcher.NLPTaskDataFetcher.__sample", "data_fetcher.NLPTaskDataFetcher.__sample", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.read_column_data", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.read_column_data", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.read_column_data", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.convert_tag_scheme", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.__sample", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.__sample"], ["", "", "@", "staticmethod", "\n", "@", "deprecated", "(", "version", "=", "\"0.4.1\"", ",", "reason", "=", "\"Use 'flair.datasets' instead.\"", ")", "\n", "def", "load_column_corpus", "(", "\n", "data_folder", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "column_format", ":", "Dict", "[", "int", ",", "str", "]", ",", "\n", "train_file", "=", "None", ",", "\n", "test_file", "=", "None", ",", "\n", "dev_file", "=", "None", ",", "\n", "tag_to_biloes", "=", "None", ",", "\n", ")", "->", "Corpus", ":", "\n", "        ", "\"\"\"\n        Helper function to get a Corpus from CoNLL column-formatted task data such as CoNLL03 or CoNLL2000.\n\n        :param data_folder: base folder with the task data\n        :param column_format: a map specifying the column format\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :param tag_to_biloes: whether to convert to BILOES tagging scheme\n        :return: a Corpus with annotated train, dev and test data\n        \"\"\"", "\n", "\n", "if", "type", "(", "data_folder", ")", "==", "str", ":", "\n", "            ", "data_folder", ":", "Path", "=", "Path", "(", "data_folder", ")", "\n", "\n", "", "if", "train_file", "is", "not", "None", ":", "\n", "            ", "train_file", "=", "data_folder", "/", "train_file", "\n", "", "if", "test_file", "is", "not", "None", ":", "\n", "            ", "test_file", "=", "data_folder", "/", "test_file", "\n", "", "if", "dev_file", "is", "not", "None", ":", "\n", "            ", "dev_file", "=", "data_folder", "/", "dev_file", "\n", "\n", "# automatically identify train / test / dev files", "\n", "", "if", "train_file", "is", "None", ":", "\n", "            ", "for", "file", "in", "data_folder", ".", "iterdir", "(", ")", ":", "\n", "                ", "file_name", "=", "file", ".", "name", "\n", "if", "file_name", ".", "endswith", "(", "\".gz\"", ")", ":", "\n", "                    ", "continue", "\n", "", "if", "\"train\"", "in", "file_name", "and", "not", "\"54019\"", "in", "file_name", ":", "\n", "                    ", "train_file", "=", "file", "\n", "", "if", "\"dev\"", "in", "file_name", ":", "\n", "                    ", "dev_file", "=", "file", "\n", "", "if", "\"testa\"", "in", "file_name", ":", "\n", "                    ", "dev_file", "=", "file", "\n", "", "if", "\"testb\"", "in", "file_name", ":", "\n", "                    ", "test_file", "=", "file", "\n", "\n", "# if no test file is found, take any file with 'test' in name", "\n", "", "", "if", "test_file", "is", "None", ":", "\n", "                ", "for", "file", "in", "data_folder", ".", "iterdir", "(", ")", ":", "\n", "                    ", "file_name", "=", "file", ".", "name", "\n", "if", "file_name", ".", "endswith", "(", "\".gz\"", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "\"test\"", "in", "file_name", ":", "\n", "                        ", "test_file", "=", "file", "\n", "\n", "", "", "", "", "log", ".", "info", "(", "\"Reading data from {}\"", ".", "format", "(", "data_folder", ")", ")", "\n", "log", ".", "info", "(", "\"Train: {}\"", ".", "format", "(", "train_file", ")", ")", "\n", "log", ".", "info", "(", "\"Dev: {}\"", ".", "format", "(", "dev_file", ")", ")", "\n", "log", ".", "info", "(", "\"Test: {}\"", ".", "format", "(", "test_file", ")", ")", "\n", "\n", "# get train and test data", "\n", "sentences_train", ":", "List", "[", "Sentence", "]", "=", "NLPTaskDataFetcher", ".", "read_column_data", "(", "\n", "train_file", ",", "column_format", "\n", ")", "\n", "\n", "# read in test file if exists, otherwise sample 10% of train data as test dataset", "\n", "if", "test_file", "is", "not", "None", ":", "\n", "            ", "sentences_test", ":", "List", "[", "Sentence", "]", "=", "NLPTaskDataFetcher", ".", "read_column_data", "(", "\n", "test_file", ",", "column_format", "\n", ")", "\n", "", "else", ":", "\n", "            ", "sentences_test", ":", "List", "[", "Sentence", "]", "=", "[", "\n", "sentences_train", "[", "i", "]", "\n", "for", "i", "in", "NLPTaskDataFetcher", ".", "__sample", "(", "len", "(", "sentences_train", ")", ",", "0.1", ")", "\n", "]", "\n", "sentences_train", "=", "[", "x", "for", "x", "in", "sentences_train", "if", "x", "not", "in", "sentences_test", "]", "\n", "\n", "# read in dev file if exists, otherwise sample 10% of train data as dev dataset", "\n", "", "if", "dev_file", "is", "not", "None", ":", "\n", "            ", "sentences_dev", ":", "List", "[", "Sentence", "]", "=", "NLPTaskDataFetcher", ".", "read_column_data", "(", "\n", "dev_file", ",", "column_format", "\n", ")", "\n", "", "else", ":", "\n", "            ", "sentences_dev", ":", "List", "[", "Sentence", "]", "=", "[", "\n", "sentences_train", "[", "i", "]", "\n", "for", "i", "in", "NLPTaskDataFetcher", ".", "__sample", "(", "len", "(", "sentences_train", ")", ",", "0.1", ")", "\n", "]", "\n", "sentences_train", "=", "[", "x", "for", "x", "in", "sentences_train", "if", "x", "not", "in", "sentences_dev", "]", "\n", "\n", "", "if", "tag_to_biloes", "is", "not", "None", ":", "\n", "# convert tag scheme to iobes", "\n", "            ", "for", "sentence", "in", "sentences_train", "+", "sentences_test", "+", "sentences_dev", ":", "\n", "                ", "sentence", ".", "convert_tag_scheme", "(", "\n", "tag_type", "=", "tag_to_biloes", ",", "target_scheme", "=", "\"iobes\"", "\n", ")", "\n", "\n", "", "", "return", "Corpus", "(", "\n", "sentences_train", ",", "sentences_dev", ",", "sentences_test", ",", "name", "=", "data_folder", ".", "name", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_ud_corpus": [[357, 397], ["deprecated.deprecated.deprecated", "log.info", "log.info", "log.info", "log.info", "data_fetcher.NLPTaskDataFetcher.read_conll_ud", "data_fetcher.NLPTaskDataFetcher.read_conll_ud", "data_fetcher.NLPTaskDataFetcher.read_conll_ud", "flair.data.Corpus", "data_folder.iterdir"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.read_conll_ud", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.read_conll_ud", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.read_conll_ud"], ["", "@", "staticmethod", "\n", "@", "deprecated", "(", "version", "=", "\"0.4.1\"", ",", "reason", "=", "\"Use 'flair.datasets' instead.\"", ")", "\n", "def", "load_ud_corpus", "(", "\n", "data_folder", ":", "Union", "[", "str", ",", "Path", "]", ",", "train_file", "=", "None", ",", "test_file", "=", "None", ",", "dev_file", "=", "None", "\n", ")", "->", "Corpus", ":", "\n", "        ", "\"\"\"\n        Helper function to get a Corpus from CoNLL-U column-formatted task data such as the UD corpora\n\n        :param data_folder: base folder with the task data\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :return: a Corpus with annotated train, dev and test data\n        \"\"\"", "\n", "# automatically identify train / test / dev files", "\n", "if", "train_file", "is", "None", ":", "\n", "            ", "for", "file", "in", "data_folder", ".", "iterdir", "(", ")", ":", "\n", "                ", "file_name", "=", "file", ".", "name", "\n", "if", "\"train\"", "in", "file_name", ":", "\n", "                    ", "train_file", "=", "file", "\n", "", "if", "\"test\"", "in", "file_name", ":", "\n", "                    ", "test_file", "=", "file", "\n", "", "if", "\"dev\"", "in", "file_name", ":", "\n", "                    ", "dev_file", "=", "file", "\n", "", "if", "\"testa\"", "in", "file_name", ":", "\n", "                    ", "dev_file", "=", "file", "\n", "", "if", "\"testb\"", "in", "file_name", ":", "\n", "                    ", "test_file", "=", "file", "\n", "\n", "", "", "", "log", ".", "info", "(", "\"Reading data from {}\"", ".", "format", "(", "data_folder", ")", ")", "\n", "log", ".", "info", "(", "\"Train: {}\"", ".", "format", "(", "train_file", ")", ")", "\n", "log", ".", "info", "(", "\"Dev: {}\"", ".", "format", "(", "dev_file", ")", ")", "\n", "log", ".", "info", "(", "\"Test: {}\"", ".", "format", "(", "test_file", ")", ")", "\n", "\n", "sentences_train", ":", "List", "[", "Sentence", "]", "=", "NLPTaskDataFetcher", ".", "read_conll_ud", "(", "train_file", ")", "\n", "sentences_test", ":", "List", "[", "Sentence", "]", "=", "NLPTaskDataFetcher", ".", "read_conll_ud", "(", "test_file", ")", "\n", "sentences_dev", ":", "List", "[", "Sentence", "]", "=", "NLPTaskDataFetcher", ".", "read_conll_ud", "(", "dev_file", ")", "\n", "\n", "return", "Corpus", "(", "\n", "sentences_train", ",", "sentences_dev", ",", "sentences_test", ",", "name", "=", "data_folder", ".", "name", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.load_classification_corpus": [[399, 474], ["deprecated.deprecated.deprecated", "log.info", "log.info", "log.info", "log.info", "data_fetcher.NLPTaskDataFetcher.read_text_classification_file", "data_fetcher.NLPTaskDataFetcher.read_text_classification_file", "flair.data.Corpus", "type", "pathlib.Path", "data_folder.iterdir", "data_fetcher.NLPTaskDataFetcher.read_text_classification_file", "data_fetcher.NLPTaskDataFetcher.__sample", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.read_text_classification_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.read_text_classification_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.read_text_classification_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.__sample"], ["", "@", "staticmethod", "\n", "@", "deprecated", "(", "version", "=", "\"0.4.1\"", ",", "reason", "=", "\"Use 'flair.datasets' instead.\"", ")", "\n", "def", "load_classification_corpus", "(", "\n", "data_folder", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "train_file", "=", "None", ",", "\n", "test_file", "=", "None", ",", "\n", "dev_file", "=", "None", ",", "\n", "tokenizer", ":", "Callable", "[", "[", "str", "]", ",", "List", "[", "Token", "]", "]", "=", "segtok_tokenizer", ",", "\n", "max_tokens_per_doc", "=", "-", "1", ",", "\n", ")", "->", "Corpus", ":", "\n", "        ", "\"\"\"\n        Helper function to get a Corpus from text classification-formatted task data\n\n        :param data_folder: base folder with the task data\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :return: a Corpus with annotated train, dev and test data\n        \"\"\"", "\n", "\n", "if", "type", "(", "data_folder", ")", "==", "str", ":", "\n", "            ", "data_folder", ":", "Path", "=", "Path", "(", "data_folder", ")", "\n", "\n", "", "if", "train_file", "is", "not", "None", ":", "\n", "            ", "train_file", "=", "data_folder", "/", "train_file", "\n", "", "if", "test_file", "is", "not", "None", ":", "\n", "            ", "test_file", "=", "data_folder", "/", "test_file", "\n", "", "if", "dev_file", "is", "not", "None", ":", "\n", "            ", "dev_file", "=", "data_folder", "/", "dev_file", "\n", "\n", "# automatically identify train / test / dev files", "\n", "", "if", "train_file", "is", "None", ":", "\n", "            ", "for", "file", "in", "data_folder", ".", "iterdir", "(", ")", ":", "\n", "                ", "file_name", "=", "file", ".", "name", "\n", "if", "\"train\"", "in", "file_name", ":", "\n", "                    ", "train_file", "=", "file", "\n", "", "if", "\"test\"", "in", "file_name", ":", "\n", "                    ", "test_file", "=", "file", "\n", "", "if", "\"dev\"", "in", "file_name", ":", "\n", "                    ", "dev_file", "=", "file", "\n", "", "if", "\"testa\"", "in", "file_name", ":", "\n", "                    ", "dev_file", "=", "file", "\n", "", "if", "\"testb\"", "in", "file_name", ":", "\n", "                    ", "test_file", "=", "file", "\n", "\n", "", "", "", "log", ".", "info", "(", "\"Reading data from {}\"", ".", "format", "(", "data_folder", ")", ")", "\n", "log", ".", "info", "(", "\"Train: {}\"", ".", "format", "(", "train_file", ")", ")", "\n", "log", ".", "info", "(", "\"Dev: {}\"", ".", "format", "(", "dev_file", ")", ")", "\n", "log", ".", "info", "(", "\"Test: {}\"", ".", "format", "(", "test_file", ")", ")", "\n", "\n", "sentences_train", ":", "List", "[", "\n", "Sentence", "\n", "]", "=", "NLPTaskDataFetcher", ".", "read_text_classification_file", "(", "\n", "train_file", ",", "tokenizer", "=", "tokenizer", ",", "max_tokens_per_doc", "=", "max_tokens_per_doc", "\n", ")", "\n", "sentences_test", ":", "List", "[", "\n", "Sentence", "\n", "]", "=", "NLPTaskDataFetcher", ".", "read_text_classification_file", "(", "\n", "test_file", ",", "tokenizer", "=", "tokenizer", ",", "max_tokens_per_doc", "=", "max_tokens_per_doc", "\n", ")", "\n", "\n", "if", "dev_file", "is", "not", "None", ":", "\n", "            ", "sentences_dev", ":", "List", "[", "\n", "Sentence", "\n", "]", "=", "NLPTaskDataFetcher", ".", "read_text_classification_file", "(", "\n", "dev_file", ",", "tokenizer", "=", "tokenizer", ",", "max_tokens_per_doc", "=", "max_tokens_per_doc", "\n", ")", "\n", "", "else", ":", "\n", "            ", "sentences_dev", ":", "List", "[", "Sentence", "]", "=", "[", "\n", "sentences_train", "[", "i", "]", "\n", "for", "i", "in", "NLPTaskDataFetcher", ".", "__sample", "(", "len", "(", "sentences_train", ")", ",", "0.1", ")", "\n", "]", "\n", "sentences_train", "=", "[", "x", "for", "x", "in", "sentences_train", "if", "x", "not", "in", "sentences_dev", "]", "\n", "\n", "", "return", "Corpus", "(", "sentences_train", ",", "sentences_dev", ",", "sentences_test", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.read_text_classification_file": [[475, 520], ["deprecated.deprecated.deprecated", "open", "str", "line.split", "range", "line[].strip", "len", "words[].startswith", "flair.data.Sentence", "words[].replace", "labels.append", "len", "sentences.append", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "@", "deprecated", "(", "version", "=", "\"0.4.1\"", ",", "reason", "=", "\"Use 'flair.datasets' instead.\"", ")", "\n", "def", "read_text_classification_file", "(", "\n", "path_to_file", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "max_tokens_per_doc", "=", "-", "1", ",", "\n", "tokenizer", "=", "segtok_tokenizer", ",", "\n", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "\"\"\"\n        Reads a data file for text classification. The file should contain one document/text per line.\n        The line should have the following format:\n        __label__<class_name> <text>\n        If you have a multi class task, you can have as many labels as you want at the beginning of the line, e.g.,\n        __label__<class_name_1> __label__<class_name_2> <text>\n        :param path_to_file: the path to the data file\n        :param max_tokens_per_doc: Takes at most this amount of tokens per document. If set to -1 all documents are taken as is.\n        :return: list of sentences\n        \"\"\"", "\n", "label_prefix", "=", "\"__label__\"", "\n", "sentences", "=", "[", "]", "\n", "\n", "with", "open", "(", "str", "(", "path_to_file", ")", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "words", "=", "line", ".", "split", "(", ")", "\n", "\n", "labels", "=", "[", "]", "\n", "l_len", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "words", ")", ")", ":", "\n", "                    ", "if", "words", "[", "i", "]", ".", "startswith", "(", "label_prefix", ")", ":", "\n", "                        ", "l_len", "+=", "len", "(", "words", "[", "i", "]", ")", "+", "1", "\n", "label", "=", "words", "[", "i", "]", ".", "replace", "(", "label_prefix", ",", "\"\"", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "", "else", ":", "\n", "                        ", "break", "\n", "\n", "", "", "text", "=", "line", "[", "l_len", ":", "]", ".", "strip", "(", ")", "\n", "\n", "if", "text", "and", "labels", ":", "\n", "                    ", "sentence", "=", "Sentence", "(", "text", ",", "labels", "=", "labels", ",", "use_tokenizer", "=", "tokenizer", ")", "\n", "if", "len", "(", "sentence", ")", ">", "max_tokens_per_doc", "and", "max_tokens_per_doc", ">", "0", ":", "\n", "                        ", "sentence", ".", "tokens", "=", "sentence", ".", "tokens", "[", ":", "max_tokens_per_doc", "]", "\n", "", "if", "len", "(", "sentence", ".", "tokens", ")", ">", "0", ":", "\n", "                        ", "sentences", ".", "append", "(", "sentence", ")", "\n", "\n", "", "", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.read_column_data": [[521, 587], ["deprecated.deprecated.deprecated", "flair.data.Sentence", "open().read().strip().split", "line.startswith", "len", "sentence.infer_space_after", "sentences.append", "log.info", "open().read().strip().split", "line.strip().replace", "flair.data.Sentence", "re.split", "flair.data.Token", "sentence.add_token", "open().read().strip", "len", "sentence.infer_space_after", "sentences.append", "open().read().strip", "line.strip", "len", "open().read", "flair.data.Token.add_tag", "open().read", "open", "str", "open", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.infer_space_after", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.infer_space_after", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tag"], ["", "@", "staticmethod", "\n", "@", "deprecated", "(", "version", "=", "\"0.4.1\"", ",", "reason", "=", "\"Use 'flair.datasets' instead.\"", ")", "\n", "def", "read_column_data", "(", "\n", "path_to_column_file", ":", "Path", ",", "\n", "column_name_map", ":", "Dict", "[", "int", ",", "str", "]", ",", "\n", "infer_whitespace_after", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Reads a file in column format and produces a list of Sentence with tokenlevel annotation as specified in the\n        column_name_map. For instance, by passing \"{0: 'text', 1: 'pos', 2: 'np', 3: 'ner'}\" as column_name_map you\n        specify that the first column is the text (lexical value) of the token, the second the PoS tag, the third\n        the chunk and the forth the NER tag.\n        :param path_to_column_file: the path to the column file\n        :param column_name_map: a map of column number to token annotation name\n        :param infer_whitespace_after: if True, tries to infer whitespace_after field for Token\n        :return: list of sentences\n        \"\"\"", "\n", "sentences", ":", "List", "[", "Sentence", "]", "=", "[", "]", "\n", "\n", "try", ":", "\n", "            ", "lines", ":", "List", "[", "str", "]", "=", "open", "(", "\n", "str", "(", "path_to_column_file", ")", ",", "encoding", "=", "\"utf-8\"", "\n", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "", "except", ":", "\n", "            ", "log", ".", "info", "(", "\n", "'UTF-8 can\\'t read: {} ... using \"latin-1\" instead.'", ".", "format", "(", "\n", "path_to_column_file", "\n", ")", "\n", ")", "\n", "lines", ":", "List", "[", "str", "]", "=", "open", "(", "\n", "str", "(", "path_to_column_file", ")", ",", "encoding", "=", "\"latin1\"", "\n", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "\n", "# most data sets have the token text in the first column, if not, pass 'text' as column", "\n", "", "text_column", ":", "int", "=", "0", "\n", "for", "column", "in", "column_name_map", ":", "\n", "            ", "if", "column_name_map", "[", "column", "]", "==", "\"text\"", ":", "\n", "                ", "text_column", "=", "column", "\n", "\n", "", "", "sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "\n", "            ", "if", "line", ".", "startswith", "(", "\"#\"", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "line", ".", "strip", "(", ")", ".", "replace", "(", "\"\ufeff\"", ",", "\"\"", ")", "==", "\"\"", ":", "\n", "                ", "if", "len", "(", "sentence", ")", ">", "0", ":", "\n", "                    ", "sentence", ".", "infer_space_after", "(", ")", "\n", "sentences", ".", "append", "(", "sentence", ")", "\n", "", "sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "\n", "", "else", ":", "\n", "                ", "fields", ":", "List", "[", "str", "]", "=", "re", ".", "split", "(", "\"\\s+\"", ",", "line", ")", "\n", "token", "=", "Token", "(", "fields", "[", "text_column", "]", ")", "\n", "for", "column", "in", "column_name_map", ":", "\n", "                    ", "if", "len", "(", "fields", ")", ">", "column", ":", "\n", "                        ", "if", "column", "!=", "text_column", ":", "\n", "                            ", "token", ".", "add_tag", "(", "column_name_map", "[", "column", "]", ",", "fields", "[", "column", "]", ")", "\n", "\n", "", "", "", "sentence", ".", "add_token", "(", "token", ")", "\n", "\n", "", "", "if", "len", "(", "sentence", ".", "tokens", ")", ">", "0", ":", "\n", "            ", "sentence", ".", "infer_space_after", "(", ")", "\n", "sentences", ".", "append", "(", "sentence", ")", "\n", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.read_conll_ud": [[588, 638], ["deprecated.deprecated.deprecated", "open().read().strip().split", "flair.data.Sentence", "re.split", "len", "sentences.append", "open().read().strip", "flair.data.Sentence", "line.startswith", "len", "sentences.append", "open().read", "flair.data.Token", "flair.data.Token.add_tag", "flair.data.Token.add_tag", "flair.data.Token.add_tag", "flair.data.Token.add_tag", "str().split", "sentence.add_token", "open", "str", "str", "str", "str", "flair.data.Token.add_tag", "flair.data.Token.add_tag", "int", "str", "[].lower", "len", "str", "str", "morph.split", "morph.split"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tag", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tag", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tag", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tag", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tag", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tag"], ["", "@", "staticmethod", "\n", "@", "deprecated", "(", "version", "=", "\"0.4.1\"", ",", "reason", "=", "\"Use 'flair.datasets' instead.\"", ")", "\n", "def", "read_conll_ud", "(", "path_to_conll_file", ":", "Path", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "\"\"\"\n       Reads a file in CoNLL-U format and produces a list of Sentence with full morphosyntactic annotation\n       :param path_to_conll_file: the path to the conll-u file\n       :return: list of sentences\n       \"\"\"", "\n", "sentences", ":", "List", "[", "Sentence", "]", "=", "[", "]", "\n", "\n", "lines", ":", "List", "[", "str", "]", "=", "open", "(", "\n", "path_to_conll_file", ",", "encoding", "=", "\"utf-8\"", "\n", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "\n", "sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "\n", "            ", "fields", ":", "List", "[", "str", "]", "=", "re", ".", "split", "(", "\"\\t+\"", ",", "line", ")", "\n", "if", "line", "==", "\"\"", ":", "\n", "                ", "if", "len", "(", "sentence", ")", ">", "0", ":", "\n", "                    ", "sentences", ".", "append", "(", "sentence", ")", "\n", "", "sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "\n", "", "elif", "line", ".", "startswith", "(", "\"#\"", ")", ":", "\n", "                ", "continue", "\n", "", "elif", "\".\"", "in", "fields", "[", "0", "]", ":", "\n", "                ", "continue", "\n", "", "elif", "\"-\"", "in", "fields", "[", "0", "]", ":", "\n", "                ", "continue", "\n", "", "else", ":", "\n", "                ", "token", "=", "Token", "(", "fields", "[", "1", "]", ",", "head_id", "=", "int", "(", "fields", "[", "6", "]", ")", ")", "\n", "token", ".", "add_tag", "(", "\"lemma\"", ",", "str", "(", "fields", "[", "2", "]", ")", ")", "\n", "token", ".", "add_tag", "(", "\"upos\"", ",", "str", "(", "fields", "[", "3", "]", ")", ")", "\n", "token", ".", "add_tag", "(", "\"pos\"", ",", "str", "(", "fields", "[", "4", "]", ")", ")", "\n", "token", ".", "add_tag", "(", "\"dependency\"", ",", "str", "(", "fields", "[", "7", "]", ")", ")", "\n", "\n", "for", "morph", "in", "str", "(", "fields", "[", "5", "]", ")", ".", "split", "(", "\"|\"", ")", ":", "\n", "                    ", "if", "not", "\"=\"", "in", "morph", ":", "\n", "                        ", "continue", "\n", "", "token", ".", "add_tag", "(", "morph", ".", "split", "(", "\"=\"", ")", "[", "0", "]", ".", "lower", "(", ")", ",", "morph", ".", "split", "(", "\"=\"", ")", "[", "1", "]", ")", "\n", "\n", "", "if", "len", "(", "fields", ")", ">", "10", "and", "str", "(", "fields", "[", "10", "]", ")", "==", "\"Y\"", ":", "\n", "                    ", "token", ".", "add_tag", "(", "\"frame\"", ",", "str", "(", "fields", "[", "11", "]", ")", ")", "\n", "\n", "", "sentence", ".", "add_token", "(", "token", ")", "\n", "\n", "", "", "if", "len", "(", "sentence", ".", "tokens", ")", ">", "0", ":", "\n", "            ", "sentences", ".", "append", "(", "sentence", ")", "\n", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.__sample": [[639, 646], ["round", "random.sample", "range"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.modeling_transfo_xl_utilities.LogUniformSampler.sample"], ["", "@", "staticmethod", "\n", "def", "__sample", "(", "total_number_of_sentences", ":", "int", ",", "percentage", ":", "float", "=", "0.1", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "import", "random", "\n", "\n", "sample_size", ":", "int", "=", "round", "(", "total_number_of_sentences", "*", "percentage", ")", "\n", "sample", "=", "random", ".", "sample", "(", "range", "(", "1", ",", "total_number_of_sentences", ")", ",", "sample_size", ")", "\n", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data_fetcher.NLPTaskDataFetcher.download_dataset": [[647, 1379], ["task.value.startswith", "task.value.startswith", "task.value.startswith", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "data_file.is_file", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "data_file.is_file", "flair.file_utils.cached_path", "data_file.is_file", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "data_file.is_file", "zip", "data_file.is_file", "flair.file_utils.cached_path", "bz2.BZ2File", "new_train_file.is_file", "gzip.open", "gzip.open", "pathlib.Path", "tarfile.open", "pathlib.Path", "tarfile.open", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "open", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "data_file.is_file", "flair.file_utils.cached_path", "os.remove", "pathlib.Path", "pathlib.Path", "pathlib.Path", "open", "pathlib.Path", "pathlib.Path", "pathlib.Path", "open", "shutil.copyfileobj", "open", "shutil.copyfileobj", "pathlib.Path", "f_in.extract", "shutil.move", "pathlib.Path", "open", "pathlib.Path", "pathlib.Path", "line.rstrip.rstrip.decode", "line.rstrip.rstrip.split", "open", "pathlib.Path", "pathlib.Path", "f_in.extractall", "pathlib.Path", "open", "out.write", "pathlib.Path", "pathlib.Path", "open", "next", "open", "f_out.write", "open", "current_path.iterdir", "line.rstrip.rstrip.rstrip", "line.rstrip.rstrip.split", "write_fp.write", "pathlib.Path", "line.rstrip.rstrip.split", "out.write", "f_in.read", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "file_name.is_file", "file_name.name.endswith", "f_p.write", "old_label.split", "word.split", "f_in.getmembers", "fields[].rstrip", "file_name.open().read", "file_name.open"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.decode"], ["", "@", "staticmethod", "\n", "def", "download_dataset", "(", "task", ":", "NLPTask", ")", ":", "\n", "\n", "# conll 2000 chunking task", "\n", "        ", "if", "task", "==", "NLPTask", ".", "CONLL_2000", ":", "\n", "            ", "conll_2000_path", "=", "\"https://www.clips.uantwerpen.be/conll2000/chunking/\"", "\n", "data_file", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "task", ".", "value", "/", "\"train.txt\"", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "                ", "cached_path", "(", "\n", "f\"{conll_2000_path}train.txt.gz\"", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{conll_2000_path}test.txt.gz\"", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", "\n", ")", "\n", "import", "gzip", ",", "shutil", "\n", "\n", "with", "gzip", ".", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "task", ".", "value", "/", "\"train.txt.gz\"", ",", "\n", "\"rb\"", ",", "\n", ")", "as", "f_in", ":", "\n", "                    ", "with", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "task", ".", "value", "/", "\"train.txt\"", ",", "\n", "\"wb\"", ",", "\n", ")", "as", "f_out", ":", "\n", "                        ", "shutil", ".", "copyfileobj", "(", "f_in", ",", "f_out", ")", "\n", "", "", "with", "gzip", ".", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "task", ".", "value", "/", "\"test.txt.gz\"", ",", "\n", "\"rb\"", ",", "\n", ")", "as", "f_in", ":", "\n", "                    ", "with", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "task", ".", "value", "/", "\"test.txt\"", ",", "\n", "\"wb\"", ",", "\n", ")", "as", "f_out", ":", "\n", "                        ", "shutil", ".", "copyfileobj", "(", "f_in", ",", "f_out", ")", "\n", "\n", "", "", "", "", "if", "task", "==", "NLPTask", ".", "NER_BASQUE", ":", "\n", "            ", "ner_basque_path", "=", "\"http://ixa2.si.ehu.eus/eiec/\"", "\n", "data_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "task", ".", "value", "\n", "data_file", "=", "data_path", "/", "\"named_ent_eu.train\"", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "                ", "cached_path", "(", "\n", "f\"{ner_basque_path}/eiec_v1.0.tgz\"", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", "\n", ")", "\n", "import", "tarfile", ",", "shutil", "\n", "\n", "with", "tarfile", ".", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "task", ".", "value", "/", "\"eiec_v1.0.tgz\"", ",", "\n", "\"r:gz\"", ",", "\n", ")", "as", "f_in", ":", "\n", "                    ", "corpus_files", "=", "(", "\n", "\"eiec_v1.0/named_ent_eu.train\"", ",", "\n", "\"eiec_v1.0/named_ent_eu.test\"", ",", "\n", ")", "\n", "for", "corpus_file", "in", "corpus_files", ":", "\n", "                        ", "f_in", ".", "extract", "(", "corpus_file", ",", "data_path", ")", "\n", "shutil", ".", "move", "(", "f\"{data_path}/{corpus_file}\"", ",", "data_path", ")", "\n", "\n", "", "", "", "", "if", "task", "==", "NLPTask", ".", "IMDB", ":", "\n", "            ", "imdb_acl_path", "=", "(", "\n", "\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"", "\n", ")", "\n", "data_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "task", ".", "value", "\n", "data_file", "=", "data_path", "/", "\"train.txt\"", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "                ", "cached_path", "(", "imdb_acl_path", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ")", "\n", "import", "tarfile", "\n", "\n", "with", "tarfile", ".", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "\n", "/", "\"datasets\"", "\n", "/", "task", ".", "value", "\n", "/", "\"aclImdb_v1.tar.gz\"", ",", "\n", "\"r:gz\"", ",", "\n", ")", "as", "f_in", ":", "\n", "                    ", "datasets", "=", "[", "\"train\"", ",", "\"test\"", "]", "\n", "labels", "=", "[", "\"pos\"", ",", "\"neg\"", "]", "\n", "\n", "for", "label", "in", "labels", ":", "\n", "                        ", "for", "dataset", "in", "datasets", ":", "\n", "                            ", "f_in", ".", "extractall", "(", "\n", "data_path", ",", "\n", "members", "=", "[", "\n", "m", "\n", "for", "m", "in", "f_in", ".", "getmembers", "(", ")", "\n", "if", "f\"{dataset}/{label}\"", "in", "m", ".", "name", "\n", "]", ",", "\n", ")", "\n", "with", "open", "(", "f\"{data_path}/{dataset}.txt\"", ",", "\"at\"", ")", "as", "f_p", ":", "\n", "                                ", "current_path", "=", "data_path", "/", "\"aclImdb\"", "/", "dataset", "/", "label", "\n", "for", "file_name", "in", "current_path", ".", "iterdir", "(", ")", ":", "\n", "                                    ", "if", "file_name", ".", "is_file", "(", ")", "and", "file_name", ".", "name", ".", "endswith", "(", "\n", "\".txt\"", "\n", ")", ":", "\n", "                                        ", "f_p", ".", "write", "(", "\n", "f\"__label__{label} \"", "\n", "+", "file_name", ".", "open", "(", "\n", "\"rt\"", ",", "encoding", "=", "\"utf-8\"", "\n", ")", ".", "read", "(", ")", "\n", "+", "\"\\n\"", "\n", ")", "\n", "\n", "# Support both TREC-6 and TREC-50", "\n", "", "", "", "", "", "", "", "", "if", "task", ".", "value", ".", "startswith", "(", "\"trec\"", ")", ":", "\n", "            ", "trec_path", "=", "\"http://cogcomp.org/Data/QA/QC/\"", "\n", "\n", "original_filenames", "=", "[", "\"train_5500.label\"", ",", "\"TREC_10.label\"", "]", "\n", "new_filenames", "=", "[", "\"train.txt\"", ",", "\"test.txt\"", "]", "\n", "for", "original_filename", "in", "original_filenames", ":", "\n", "                ", "cached_path", "(", "\n", "f\"{trec_path}{original_filename}\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", "/", "\"original\"", ",", "\n", ")", "\n", "\n", "", "data_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "task", ".", "value", "\n", "data_file", "=", "data_path", "/", "new_filenames", "[", "0", "]", "\n", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "                ", "for", "original_filename", ",", "new_filename", "in", "zip", "(", "\n", "original_filenames", ",", "new_filenames", "\n", ")", ":", "\n", "                    ", "with", "open", "(", "\n", "data_path", "/", "\"original\"", "/", "original_filename", ",", "\n", "\"rt\"", ",", "\n", "encoding", "=", "\"latin1\"", ",", "\n", ")", "as", "open_fp", ":", "\n", "                        ", "with", "open", "(", "\n", "data_path", "/", "new_filename", ",", "\"wt\"", ",", "encoding", "=", "\"utf-8\"", "\n", ")", "as", "write_fp", ":", "\n", "                            ", "for", "line", "in", "open_fp", ":", "\n", "                                ", "line", "=", "line", ".", "rstrip", "(", ")", "\n", "fields", "=", "line", ".", "split", "(", ")", "\n", "old_label", "=", "fields", "[", "0", "]", "\n", "question", "=", "\" \"", ".", "join", "(", "fields", "[", "1", ":", "]", ")", "\n", "\n", "# Create flair compatible labels", "\n", "# TREC-6 : NUM:dist -> __label__NUM", "\n", "# TREC-50: NUM:dist -> __label__NUM:dist", "\n", "new_label", "=", "\"__label__\"", "\n", "new_label", "+=", "(", "\n", "old_label", ".", "split", "(", "\":\"", ")", "[", "0", "]", "\n", "if", "task", ".", "value", "==", "\"trec-6\"", "\n", "else", "old_label", "\n", ")", "\n", "\n", "write_fp", ".", "write", "(", "f\"{new_label} {question}\\n\"", ")", "\n", "\n", "", "", "", "", "", "", "if", "task", "==", "NLPTask", ".", "WNUT_17", ":", "\n", "            ", "wnut_path", "=", "\"https://noisy-text.github.io/2017/files/\"", "\n", "cached_path", "(", "f\"{wnut_path}wnut17train.conll\"", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ")", "\n", "cached_path", "(", "f\"{wnut_path}emerging.dev.conll\"", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ")", "\n", "cached_path", "(", "\n", "f\"{wnut_path}emerging.test.annotated\"", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", "\n", ")", "\n", "\n", "# Wikiner NER task", "\n", "", "wikiner_path", "=", "(", "\n", "\"https://raw.githubusercontent.com/dice-group/FOX/master/input/Wikiner/\"", "\n", ")", "\n", "if", "task", ".", "value", ".", "startswith", "(", "\"wikiner\"", ")", ":", "\n", "            ", "lc", "=", "\"\"", "\n", "if", "task", "==", "NLPTask", ".", "WIKINER_ENGLISH", ":", "\n", "                ", "lc", "=", "\"en\"", "\n", "", "if", "task", "==", "NLPTask", ".", "WIKINER_GERMAN", ":", "\n", "                ", "lc", "=", "\"de\"", "\n", "", "if", "task", "==", "NLPTask", ".", "WIKINER_DUTCH", ":", "\n", "                ", "lc", "=", "\"nl\"", "\n", "", "if", "task", "==", "NLPTask", ".", "WIKINER_FRENCH", ":", "\n", "                ", "lc", "=", "\"fr\"", "\n", "", "if", "task", "==", "NLPTask", ".", "WIKINER_ITALIAN", ":", "\n", "                ", "lc", "=", "\"it\"", "\n", "", "if", "task", "==", "NLPTask", ".", "WIKINER_SPANISH", ":", "\n", "                ", "lc", "=", "\"es\"", "\n", "", "if", "task", "==", "NLPTask", ".", "WIKINER_PORTUGUESE", ":", "\n", "                ", "lc", "=", "\"pt\"", "\n", "", "if", "task", "==", "NLPTask", ".", "WIKINER_POLISH", ":", "\n", "                ", "lc", "=", "\"pl\"", "\n", "", "if", "task", "==", "NLPTask", ".", "WIKINER_RUSSIAN", ":", "\n", "                ", "lc", "=", "\"ru\"", "\n", "\n", "", "data_file", "=", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "\n", "/", "\"datasets\"", "\n", "/", "task", ".", "value", "\n", "/", "f\"aij-wikiner-{lc}-wp3.train\"", "\n", ")", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "\n", "                ", "cached_path", "(", "\n", "f\"{wikiner_path}aij-wikiner-{lc}-wp3.bz2\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "import", "bz2", ",", "shutil", "\n", "\n", "# unpack and write out in CoNLL column-like format", "\n", "bz_file", "=", "bz2", ".", "BZ2File", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "\n", "/", "\"datasets\"", "\n", "/", "task", ".", "value", "\n", "/", "f\"aij-wikiner-{lc}-wp3.bz2\"", ",", "\n", "\"rb\"", ",", "\n", ")", "\n", "with", "bz_file", "as", "f", ",", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "\n", "/", "\"datasets\"", "\n", "/", "task", ".", "value", "\n", "/", "f\"aij-wikiner-{lc}-wp3.train\"", ",", "\n", "\"w\"", ",", "\n", ")", "as", "out", ":", "\n", "                    ", "for", "line", "in", "f", ":", "\n", "                        ", "line", "=", "line", ".", "decode", "(", "\"utf-8\"", ")", "\n", "words", "=", "line", ".", "split", "(", "\" \"", ")", "\n", "for", "word", "in", "words", ":", "\n", "                            ", "out", ".", "write", "(", "\"\\t\"", ".", "join", "(", "word", ".", "split", "(", "\"|\"", ")", ")", "+", "\"\\n\"", ")", "\n", "\n", "# CoNLL 02/03 NER", "\n", "", "", "", "", "", "conll_02_path", "=", "\"https://www.clips.uantwerpen.be/conll2002/ner/data/\"", "\n", "if", "task", "==", "NLPTask", ".", "CONLL_03_DUTCH", ":", "\n", "            ", "cached_path", "(", "f\"{conll_02_path}ned.testa\"", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ")", "\n", "cached_path", "(", "f\"{conll_02_path}ned.testb\"", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ")", "\n", "cached_path", "(", "f\"{conll_02_path}ned.train\"", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ")", "\n", "", "if", "task", "==", "NLPTask", ".", "CONLL_03_SPANISH", ":", "\n", "            ", "cached_path", "(", "f\"{conll_02_path}esp.testa\"", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ")", "\n", "cached_path", "(", "f\"{conll_02_path}esp.testb\"", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ")", "\n", "cached_path", "(", "f\"{conll_02_path}esp.train\"", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ")", "\n", "\n", "# universal dependencies", "\n", "", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/\"", "\n", "# --- UD Germanic", "\n", "if", "task", "==", "NLPTask", ".", "UD_ENGLISH", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_English-EWT/master/en_ewt-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_English-EWT/master/en_ewt-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_English-EWT/master/en_ewt-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_GERMAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_German-GSD/master/de_gsd-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_German-GSD/master/de_gsd-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_German-GSD/master/de_gsd-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_DUTCH", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Dutch-Alpino/master/nl_alpino-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Dutch-Alpino/master/nl_alpino-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Dutch-Alpino/master/nl_alpino-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "# --- UD Romance", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_FRENCH", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_French-GSD/master/fr_gsd-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_French-GSD/master/fr_gsd-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_French-GSD/master/fr_gsd-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_ITALIAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Italian-ISDT/master/it_isdt-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Italian-ISDT/master/it_isdt-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Italian-ISDT/master/it_isdt-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_SPANISH", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Spanish-GSD/master/es_gsd-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Spanish-GSD/master/es_gsd-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Spanish-GSD/master/es_gsd-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_PORTUGUESE", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Portuguese-Bosque/blob/master/pt_bosque-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Portuguese-Bosque/blob/master/pt_bosque-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Portuguese-Bosque/blob/master/pt_bosque-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_ROMANIAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Romanian-RRT/master/ro_rrt-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Romanian-RRT/master/ro_rrt-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Romanian-RRT/master/ro_rrt-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_CATALAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Catalan-AnCora/master/ca_ancora-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Catalan-AnCora/master/ca_ancora-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Catalan-AnCora/master/ca_ancora-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "# --- UD West-Slavic", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_POLISH", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Polish-LFG/master/pl_lfg-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Polish-LFG/master/pl_lfg-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Polish-LFG/master/pl_lfg-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_CZECH", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Czech-PDT/master/cs_pdt-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Czech-PDT/master/cs_pdt-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Czech-PDT/master/cs_pdt-ud-train-l.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_SLOVAK", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Slovak-SNK/master/sk_snk-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Slovak-SNK/master/sk_snk-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Slovak-SNK/master/sk_snk-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "# --- UD Scandinavian", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_SWEDISH", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Swedish-Talbanken/master/sv_talbanken-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Swedish-Talbanken/master/sv_talbanken-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Swedish-Talbanken/master/sv_talbanken-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_DANISH", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Danish-DDT/master/da_ddt-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Danish-DDT/master/da_ddt-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Danish-DDT/master/da_ddt-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_NORWEGIAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Norwegian-Bokmaal/master/no_bokmaal-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Norwegian-Bokmaal/master/no_bokmaal-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Norwegian-Bokmaal/master/no_bokmaal-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_FINNISH", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Finnish-TDT/master/fi_tdt-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Finnish-TDT/master/fi_tdt-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Finnish-TDT/master/fi_tdt-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "# --- UD South-Slavic", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_SLOVENIAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Slovenian-SSJ/master/sl_ssj-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Slovenian-SSJ/master/sl_ssj-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Slovenian-SSJ/master/sl_ssj-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_CROATIAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Croatian-SET/master/hr_set-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Croatian-SET/master/hr_set-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Croatian-SET/master/hr_set-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_SERBIAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Serbian-SET/master/sr_set-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Serbian-SET/master/sr_set-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Serbian-SET/master/sr_set-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_BULGARIAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Bulgarian-BTB/master/bg_btb-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Bulgarian-BTB/master/bg_btb-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Bulgarian-BTB/master/bg_btb-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "# --- UD Asian", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_ARABIC", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Arabic-PADT/master/ar_padt-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Arabic-PADT/master/ar_padt-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Arabic-PADT/master/ar_padt-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_HEBREW", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Hebrew-HTB/master/he_htb-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Hebrew-HTB/master/he_htb-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Hebrew-HTB/master/he_htb-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_TURKISH", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Turkish-IMST/master/tr_imst-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Turkish-IMST/master/tr_imst-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Turkish-IMST/master/tr_imst-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_PERSIAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Persian-Seraji/master/fa_seraji-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Persian-Seraji/master/fa_seraji-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Persian-Seraji/master/fa_seraji-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_RUSSIAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Russian-SynTagRus/master/ru_syntagrus-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_HINDI", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Hindi-HDTB/master/hi_hdtb-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Hindi-HDTB/master/hi_hdtb-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Hindi-HDTB/master/hi_hdtb-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_INDONESIAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Indonesian-GSD/master/id_gsd-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Indonesian-GSD/master/id_gsd-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Indonesian-GSD/master/id_gsd-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_JAPANESE", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Japanese-GSD/master/ja_gsd-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Japanese-GSD/master/ja_gsd-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Japanese-GSD/master/ja_gsd-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_CHINESE", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Chinese-GSD/master/zh_gsd-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Chinese-GSD/master/zh_gsd-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Chinese-GSD/master/zh_gsd-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_KOREAN", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Korean-Kaist/master/ko_kaist-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Korean-Kaist/master/ko_kaist-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Korean-Kaist/master/ko_kaist-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", "==", "NLPTask", ".", "UD_BASQUE", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_Basque-BDT/master/eu_bdt-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Basque-BDT/master/eu_bdt-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_Basque-BDT/master/eu_bdt-ud-train.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "\n", "", "if", "task", ".", "value", ".", "startswith", "(", "\"wassa\"", ")", ":", "\n", "\n", "            ", "emotion", "=", "task", ".", "value", "[", "6", ":", "]", "\n", "\n", "for", "split", "in", "[", "\"train\"", ",", "\"dev\"", ",", "\"test\"", "]", ":", "\n", "\n", "                ", "data_file", "=", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "\n", "/", "\"datasets\"", "\n", "/", "task", ".", "value", "\n", "/", "f\"{emotion}-{split}.txt\"", "\n", ")", "\n", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "\n", "                    ", "if", "split", "==", "\"train\"", ":", "\n", "                        ", "url", "=", "f\"http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/{emotion}-ratings-0to1.train.txt\"", "\n", "", "if", "split", "==", "\"dev\"", ":", "\n", "                        ", "url", "=", "f\"http://saifmohammad.com/WebDocs/EmoInt%20Dev%20Data%20With%20Gold/{emotion}-ratings-0to1.dev.gold.txt\"", "\n", "", "if", "split", "==", "\"test\"", ":", "\n", "                        ", "url", "=", "f\"http://saifmohammad.com/WebDocs/EmoInt%20Test%20Gold%20Data/{emotion}-ratings-0to1.test.gold.txt\"", "\n", "\n", "", "path", "=", "cached_path", "(", "url", ",", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ")", "\n", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "                        ", "with", "open", "(", "data_file", ",", "\"w\"", ")", "as", "out", ":", "\n", "                            ", "next", "(", "f", ")", "\n", "for", "line", "in", "f", ":", "\n", "                                ", "fields", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "out", ".", "write", "(", "\n", "f\"__label__{fields[3].rstrip()} {fields[1]}\\n\"", "\n", ")", "\n", "\n", "", "", "", "os", ".", "remove", "(", "path", ")", "\n", "\n", "", "", "", "if", "task", "==", "NLPTask", ".", "UD_GERMAN_HDT", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}UD_German-HDT/dev/de_hdt-ud-dev.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_German-HDT/dev/de_hdt-ud-test.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_German-HDT/dev/de_hdt-ud-train-a.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", "/", "\"original\"", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}UD_German-HDT/dev/de_hdt-ud-train-b.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "task", ".", "value", "/", "\"original\"", ",", "\n", ")", "\n", "data_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "task", ".", "value", "\n", "\n", "train_filenames", "=", "[", "\"de_hdt-ud-train-a.conllu\"", ",", "\"de_hdt-ud-train-b.conllu\"", "]", "\n", "\n", "new_train_file", ":", "Path", "=", "data_path", "/", "\"de_hdt-ud-train-all.conllu\"", "\n", "\n", "if", "not", "new_train_file", ".", "is_file", "(", ")", ":", "\n", "                ", "with", "open", "(", "new_train_file", ",", "\"wt\"", ")", "as", "f_out", ":", "\n", "                    ", "for", "train_filename", "in", "train_filenames", ":", "\n", "                        ", "with", "open", "(", "\n", "data_path", "/", "\"original\"", "/", "train_filename", ",", "\"rt\"", "\n", ")", "as", "f_in", ":", "\n", "                            ", "f_out", ".", "write", "(", "f_in", ".", "read", "(", ")", ")", "\n", "", "", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.set_default_mininterval": [[245, 248], ["None"], "methods", ["None"], ["\n", "", "def", "get_file_extension", "(", "path", ",", "dot", "=", "True", ",", "lower", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.set_slower_interval": [[249, 261], ["None"], "methods", ["None"], ["return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm": [[262, 267], ["tqdm.tqdm.tqdm"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm"], []], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.load_big_file": [[25, 37], ["logger.info", "open", "mmap.mmap", "f_in.close", "f_in.fileno"], "function", ["None"], ["", "except", "ImportError", ":", "\n", "    ", "from", "urlparse", "import", "urlparse", "\n", "\n", "", "try", ":", "\n", "    ", "from", "pathlib", "import", "Path", "\n", "PYTORCH_PRETRAINED_BERT_CACHE", "=", "Path", "(", "os", ".", "getenv", "(", "'PYTORCH_PRETRAINED_BERT_CACHE'", ",", "\n", "Path", ".", "home", "(", ")", "/", "'.pytorch_pretrained_bert'", ")", ")", "\n", "", "except", "AttributeError", ":", "\n", "    ", "PYTORCH_PRETRAINED_BERT_CACHE", "=", "os", ".", "getenv", "(", "'PYTORCH_PRETRAINED_BERT_CACHE'", ",", "\n", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "expanduser", "(", "\"~\"", ")", ",", "'.pytorch_pretrained_bert'", ")", ")", "\n", "\n", "", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "# pylint: disable=invalid-name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.url_to_filename": [[39, 56], ["url.encode", "base64.b64encode", "base64.b64encode.decode", "etag.replace.replace"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.decode"], ["def", "url_to_filename", "(", "url", ",", "etag", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.filename_to_url": [[58, 73], ["decoded.encode", "base64.b64decode", "filename.split", "base64.b64decode.decode"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.decode"], ["    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path": [[75, 99], ["urllib.parse.urlparse", "pathlib.Path", "file_utils.get_from_cache", "pathlib.Path().exists", "pathlib.Path", "FileNotFoundError", "ValueError", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.get_from_cache"], ["", "with", "open", "(", "meta_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n", "\n", "", "def", "cached_path", "(", "url_or_filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.unzip_file": [[102, 108], ["ZipFile", "zipObj.extractall"], "function", ["None"], ["", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.download_file": [[110, 143], ["cache_dir.mkdir", "re.sub", "print", "tempfile.mkstemp", "logger.info", "requests.get", "requests.get.headers.get", "file_utils.Tqdm.tqdm", "Tqdm.tqdm.close", "logger.info", "shutil.copyfile", "logger.info", "os.close", "os.remove", "Tqdm.tqdm.close", "int", "open", "requests.get.iter_content", "str", "Tqdm.tqdm.update", "temp_file.write", "len"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm"], ["        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n", "\n", "", "", "def", "split_s3_path", "(", "url", ")", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n", "\n", "", "def", "s3_request", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.get_from_cache": [[146, 195], ["cache_dir.mkdir", "re.sub", "cache_path.exists", "requests.head", "IOError", "cache_path.exists", "tempfile.mkstemp", "logger.info", "requests.get", "requests.get.headers.get", "file_utils.Tqdm.tqdm", "Tqdm.tqdm.close", "logger.info", "shutil.copyfile", "logger.info", "os.close", "os.remove", "int", "open", "requests.get.iter_content", "str", "Tqdm.tqdm.update", "temp_file.write", "len"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm"], ["def", "s3_etag", "(", "url", ")", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n", "\n", "", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n", "\n", "", "def", "http_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n", "\n", "", "def", "get_from_cache", "(", "url", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "cache_dir", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.open_inside_zip": [[197, 211], ["file_utils.cached_path", "zipfile.ZipFile", "typing.cast", "zipfile.ZipFile.open", "io.TextIOWrapper", "zipfile.ZipFile.namelist", "file_utils.get_the_only_file_in_the_archive"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.get_the_only_file_in_the_archive"], ["", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.get_the_only_file_in_the_archive": [[213, 228], ["len", "ValueError", "file_utils.format_embeddings_file_uri"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.format_embeddings_file_uri"], ["\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.format_embeddings_file_uri": [[230, 236], ["None"], "function", ["None"], ["\n", "", "", "return", "cache_path", "\n", "\n", "\n", "", "def", "read_set_from_file", "(", "filename", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.WordEmbeddingsStore.__init__": [[61, 85], ["dict", "inference_utils.WordEmbeddingsStore._get_store_filename", "print", "inference_utils.SqliteWordEmbeddingsStoreBackend", "inference_utils.LmdbWordEmbeddingsStoreBackend", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.WordEmbeddingsStore._get_store_filename"], ["def", "__init__", "(", "self", ",", "embedding", ",", "backend", "=", "'sqlite'", ",", "verbose", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        :param embedding: Flair WordEmbeddings instance.\n        :param backend: cache database backend name e.g ``'sqlite'``, ``'lmdb'``.\n                        Default value is ``'sqlite'``.\n        :param verbose: If `True` print information on standard output\n        \"\"\"", "\n", "# some non-used parameter to allow print", "\n", "self", ".", "_modules", "=", "dict", "(", ")", "\n", "self", ".", "items", "=", "\"\"", "\n", "\n", "# get db filename from embedding name", "\n", "self", ".", "name", "=", "embedding", ".", "name", "\n", "self", ".", "store_filename", "=", "WordEmbeddingsStore", ".", "_get_store_filename", "(", "embedding", ",", "backend", ")", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "\"store filename:\"", ",", "self", ".", "store_filename", ")", "\n", "\n", "", "if", "backend", "==", "'sqlite'", ":", "\n", "            ", "self", ".", "backend", "=", "SqliteWordEmbeddingsStoreBackend", "(", "embedding", ",", "verbose", ")", "\n", "", "elif", "backend", "==", "'lmdb'", ":", "\n", "            ", "self", ".", "backend", "=", "LmdbWordEmbeddingsStoreBackend", "(", "embedding", ",", "verbose", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f'The given backend \"{backend}\" is not available.'", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.WordEmbeddingsStore._get_vector": [[87, 89], ["inference_utils.WordEmbeddingsStore.backend._get_vector"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.LmdbWordEmbeddingsStoreBackend._get_vector"], ["", "", "def", "_get_vector", "(", "self", ",", "word", "=", "\"house\"", ")", ":", "\n", "        ", "return", "self", ".", "backend", ".", "_get_vector", "(", "word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.WordEmbeddingsStore.embed": [[90, 95], ["torch.tensor", "token.set_embedding", "inference_utils.WordEmbeddingsStore._get_vector", "token.text.lower"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.LmdbWordEmbeddingsStoreBackend._get_vector"], ["", "def", "embed", "(", "self", ",", "sentences", ")", ":", "\n", "        ", "for", "sentence", "in", "sentences", ":", "\n", "            ", "for", "token", "in", "sentence", ":", "\n", "                ", "t", "=", "torch", ".", "tensor", "(", "self", ".", "_get_vector", "(", "word", "=", "token", ".", "text", ".", "lower", "(", ")", ")", ")", "\n", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.WordEmbeddingsStore._get_store_filename": [[96, 104], ["re.findall", "str"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "_get_store_filename", "(", "embedding", ",", "backend", "=", "'sqlite'", ")", ":", "\n", "        ", "\"\"\"\n        get the filename of the store\n        \"\"\"", "\n", "embedding_filename", "=", "re", ".", "findall", "(", "\".flair(/.*)\"", ",", "embedding", ".", "name", ")", "[", "0", "]", "\n", "store_filename", "=", "str", "(", "flair", ".", "cache_root", ")", "+", "embedding_filename", "+", "\".\"", "+", "backend", "\n", "return", "store_filename", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.WordEmbeddingsStore.create_stores": [[105, 115], ["type", "inference_utils.WordEmbeddingsStore"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "create_stores", "(", "model", ",", "backend", "=", "'sqlite'", ")", ":", "\n", "        ", "\"\"\"\n        creates database versions of all word embeddings in the rpbert and\n        deletes the original vectors to save memory\n        \"\"\"", "\n", "for", "embedding", "in", "model", ".", "embeddings", ".", "embeddings", ":", "\n", "            ", "if", "type", "(", "embedding", ")", "==", "WordEmbeddings", ":", "\n", "                ", "WordEmbeddingsStore", "(", "embedding", ",", "backend", ")", "\n", "del", "embedding", ".", "precomputed_word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.WordEmbeddingsStore.load_stores": [[116, 124], ["enumerate", "type", "inference_utils.WordEmbeddingsStore"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "load_stores", "(", "model", ",", "backend", "=", "'sqlite'", ")", ":", "\n", "        ", "\"\"\"\n        loads the db versions of all word embeddings in the rpbert\n        \"\"\"", "\n", "for", "i", ",", "embedding", "in", "enumerate", "(", "model", ".", "embeddings", ".", "embeddings", ")", ":", "\n", "            ", "if", "type", "(", "embedding", ")", "==", "WordEmbeddings", ":", "\n", "                ", "model", ".", "embeddings", ".", "embeddings", "[", "i", "]", "=", "WordEmbeddingsStore", "(", "embedding", ",", "backend", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.WordEmbeddingsStore.delete_stores": [[125, 138], ["inference_utils.WordEmbeddingsStore._get_store_filename", "os.path.isfile", "print", "os.remove", "os.path.isdir", "print", "shutil.rmtree"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.WordEmbeddingsStore._get_store_filename"], ["", "", "", "@", "staticmethod", "\n", "def", "delete_stores", "(", "model", ",", "backend", "=", "'sqlite'", ")", ":", "\n", "        ", "\"\"\"\n        deletes the db versions of all word embeddings\n        \"\"\"", "\n", "for", "embedding", "in", "model", ".", "embeddings", ".", "embeddings", ":", "\n", "            ", "store_filename", "=", "WordEmbeddingsStore", ".", "_get_store_filename", "(", "embedding", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "store_filename", ")", ":", "\n", "                ", "print", "(", "\"delete store:\"", ",", "store_filename", ")", "\n", "os", ".", "remove", "(", "store_filename", ")", "\n", "", "elif", "os", ".", "path", ".", "isdir", "(", "store_filename", ")", ":", "\n", "                ", "print", "(", "\"delete store:\"", ",", "store_filename", ")", "\n", "shutil", ".", "rmtree", "(", "store_filename", ",", "ignore_errors", "=", "False", ",", "onerror", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.WordEmbeddingsStoreBackend.__init__": [[141, 147], ["inference_utils.WordEmbeddingsStore._get_store_filename", "print"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.WordEmbeddingsStore._get_store_filename"], ["    ", "def", "__init__", "(", "self", ",", "embedding", ",", "backend", ",", "verbose", "=", "True", ")", ":", "\n", "# get db filename from embedding name", "\n", "        ", "self", ".", "name", "=", "embedding", ".", "name", "\n", "self", ".", "store_filename", "=", "WordEmbeddingsStore", ".", "_get_store_filename", "(", "embedding", ",", "backend", ")", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "\"store filename:\"", ",", "self", ".", "store_filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.WordEmbeddingsStoreBackend._get_vector": [[148, 150], ["None"], "methods", ["None"], ["", "", "def", "_get_vector", "(", "self", ",", "word", "=", "\"house\"", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.SqliteWordEmbeddingsStoreBackend.__init__": [[153, 186], ["inference_utils.WordEmbeddingsStoreBackend.__init__", "os.path.isfile", "sqlite3.connect", "inference_utils.SqliteWordEmbeddingsStoreBackend.db.execute", "inference_utils.SqliteWordEmbeddingsStoreBackend.db.execute", "inference_utils.SqliteWordEmbeddingsStoreBackend.db.executemany", "inference_utils.SqliteWordEmbeddingsStoreBackend.db.execute", "inference_utils.SqliteWordEmbeddingsStoreBackend.db.execute", "inference_utils.SqliteWordEmbeddingsStoreBackend.db.commit", "inference_utils.SqliteWordEmbeddingsStoreBackend.db.close", "sqlite3.connect", "inference_utils.SqliteWordEmbeddingsStoreBackend.db.cursor", "inference_utils.SqliteWordEmbeddingsStoreBackend.execute", "list", "print", "tqdm.tqdm.tqdm", "len", "pwe.get_vector().tolist", "pwe.vocab.keys", "pwe.get_vector", "range", "str", "range", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm"], ["    ", "def", "__init__", "(", "self", ",", "embedding", ",", "verbose", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "embedding", ",", "'sqlite'", ",", "verbose", ")", "\n", "# if embedding database already exists", "\n", "if", "os", ".", "path", ".", "isfile", "(", "self", ".", "store_filename", ")", ":", "\n", "            ", "self", ".", "db", "=", "sqlite3", ".", "connect", "(", "self", ".", "store_filename", ")", "\n", "cursor", "=", "self", ".", "db", ".", "cursor", "(", ")", "\n", "cursor", ".", "execute", "(", "\"SELECT * FROM embedding LIMIT 1;\"", ")", "\n", "result", "=", "list", "(", "cursor", ")", "\n", "self", ".", "k", "=", "len", "(", "result", "[", "0", "]", ")", "-", "1", "\n", "return", "\n", "\n", "# otherwise, push embedding to database", "\n", "", "self", ".", "db", "=", "sqlite3", ".", "connect", "(", "self", ".", "store_filename", ")", "\n", "pwe", "=", "embedding", ".", "precomputed_word_embeddings", "\n", "self", ".", "k", "=", "pwe", ".", "vector_size", "\n", "self", ".", "db", ".", "execute", "(", "f\"DROP TABLE IF EXISTS embedding;\"", ")", "\n", "self", ".", "db", ".", "execute", "(", "\n", "f\"CREATE TABLE embedding(word text,{','.join('v' + str(i) + ' float' for i in range(self.k))});\"", "\n", ")", "\n", "vectors_it", "=", "(", "\n", "[", "word", "]", "+", "pwe", ".", "get_vector", "(", "word", ")", ".", "tolist", "(", ")", "for", "word", "in", "pwe", ".", "vocab", ".", "keys", "(", ")", "\n", ")", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "\"load vectors to store\"", ")", "\n", "", "self", ".", "db", ".", "executemany", "(", "\n", "f\"INSERT INTO embedding(word,{','.join('v' + str(i) for i in range(self.k))}) \\\n        values ({','.join(['?'] * (1 + self.k))})\"", ",", "\n", "tqdm", "(", "vectors_it", ")", ",", "\n", ")", "\n", "self", ".", "db", ".", "execute", "(", "f\"DROP INDEX IF EXISTS embedding_index;\"", ")", "\n", "self", ".", "db", ".", "execute", "(", "f\"CREATE INDEX embedding_index ON embedding(word);\"", ")", "\n", "self", ".", "db", ".", "commit", "(", ")", "\n", "self", ".", "db", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.SqliteWordEmbeddingsStoreBackend._get_vector": [[187, 197], ["sqlite3.connect", "sqlite3.connect.cursor", "word.replace.replace.replace", "sqlite3.connect.cursor.execute", "list", "sqlite3.connect.close"], "methods", ["None"], ["", "def", "_get_vector", "(", "self", ",", "word", "=", "\"house\"", ")", ":", "\n", "        ", "db", "=", "sqlite3", ".", "connect", "(", "self", ".", "store_filename", ")", "\n", "cursor", "=", "db", ".", "cursor", "(", ")", "\n", "word", "=", "word", ".", "replace", "(", "'\"'", ",", "''", ")", "\n", "cursor", ".", "execute", "(", "f'SELECT * FROM embedding WHERE word=\"{word}\";'", ")", "\n", "result", "=", "list", "(", "cursor", ")", "\n", "db", ".", "close", "(", ")", "\n", "if", "not", "result", ":", "\n", "            ", "return", "[", "0.0", "]", "*", "self", ".", "k", "\n", "", "return", "result", "[", "0", "]", "[", "1", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.LmdbWordEmbeddingsStoreBackend.__init__": [[200, 240], ["inference_utils.WordEmbeddingsStoreBackend.__init__", "os.path.isdir", "os.makedirs", "lmdb.open", "inference_utils.LmdbWordEmbeddingsStoreBackend.env.begin", "tqdm.tqdm.tqdm", "inference_utils.LmdbWordEmbeddingsStoreBackend.commit", "lmdb.open", "print", "pwe.vocab.keys", "pwe.get_vector", "log.warning", "log.warning", "log.warning", "log.warning", "len", "inference_utils.LmdbWordEmbeddingsStoreBackend.env.max_key_size", "inference_utils.LmdbWordEmbeddingsStoreBackend.put", "inference_utils.LmdbWordEmbeddingsStoreBackend.env.begin", "inference_utils.LmdbWordEmbeddingsStoreBackend.cursor", "inference_utils.LmdbWordEmbeddingsStoreBackend.cursor.close", "word.encode", "word.encode", "pickle.dumps", "pickle.loads"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode"], ["    ", "def", "__init__", "(", "self", ",", "embedding", ",", "verbose", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "embedding", ",", "'lmdb'", ",", "verbose", ")", "\n", "try", ":", "\n", "            ", "import", "lmdb", "\n", "# if embedding database already exists", "\n", "load_db", "=", "True", "\n", "if", "os", ".", "path", ".", "isdir", "(", "self", ".", "store_filename", ")", ":", "\n", "# open the database in read mode", "\n", "                ", "self", ".", "env", "=", "lmdb", ".", "open", "(", "self", ".", "store_filename", ",", "readonly", "=", "True", ",", "max_readers", "=", "2048", ",", "max_spare_txns", "=", "4", ")", "\n", "if", "self", ".", "env", ":", "\n", "# we need to set self.k", "\n", "                    ", "with", "self", ".", "env", ".", "begin", "(", ")", "as", "txn", ":", "\n", "                        ", "cursor", "=", "txn", ".", "cursor", "(", ")", "\n", "for", "key", ",", "value", "in", "cursor", ":", "\n", "                            ", "vector", "=", "pickle", ".", "loads", "(", "value", ")", "\n", "self", ".", "k", "=", "vector", ".", "shape", "[", "0", "]", "\n", "break", "\n", "", "cursor", ".", "close", "(", ")", "\n", "", "return", "\n", "# create and load the database in write mode", "\n", "", "", "os", ".", "makedirs", "(", "self", ".", "store_filename", ",", "exist_ok", "=", "True", ")", "\n", "pwe", "=", "embedding", ".", "precomputed_word_embeddings", "\n", "self", ".", "k", "=", "pwe", ".", "vector_size", "\n", "self", ".", "env", "=", "lmdb", ".", "open", "(", "self", ".", "store_filename", ",", "map_size", "=", "DEFAULT_MAP_SIZE", ")", "\n", "if", "verbose", ":", "\n", "                ", "print", "(", "\"load vectors to store\"", ")", "\n", "", "txn", "=", "self", ".", "env", ".", "begin", "(", "write", "=", "True", ")", "\n", "for", "word", "in", "tqdm", "(", "pwe", ".", "vocab", ".", "keys", "(", ")", ")", ":", "\n", "                ", "vector", "=", "pwe", ".", "get_vector", "(", "word", ")", "\n", "if", "len", "(", "word", ".", "encode", "(", "encoding", "=", "'UTF-8'", ")", ")", "<", "self", ".", "env", ".", "max_key_size", "(", ")", ":", "\n", "                    ", "txn", ".", "put", "(", "word", ".", "encode", "(", "encoding", "=", "'UTF-8'", ")", ",", "pickle", ".", "dumps", "(", "vector", ")", ")", "\n", "", "", "txn", ".", "commit", "(", ")", "\n", "return", "\n", "", "except", "ModuleNotFoundError", ":", "\n", "            ", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "log", ".", "warning", "(", "'ATTENTION! The library \"lmdb\" is not installed!'", ")", "\n", "log", ".", "warning", "(", "\n", "'To use LMDB, please first install with \"pip install lmdb\"'", "\n", ")", "\n", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.LmdbWordEmbeddingsStoreBackend._get_vector": [[241, 267], ["inference_utils.LmdbWordEmbeddingsStoreBackend.env.begin", "txn.get", "inference_utils.LmdbWordEmbeddingsStoreBackend.env.close", "lmdb.open", "inference_utils.LmdbWordEmbeddingsStoreBackend._get_vector", "log.warning", "log.warning", "log.warning", "log.warning", "numpy.zeros", "word.encode", "pickle.loads", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.inference_utils.LmdbWordEmbeddingsStoreBackend._get_vector", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode"], ["", "", "def", "_get_vector", "(", "self", ",", "word", "=", "\"house\"", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "import", "lmdb", "\n", "with", "self", ".", "env", ".", "begin", "(", ")", "as", "txn", ":", "\n", "                ", "vector", "=", "txn", ".", "get", "(", "word", ".", "encode", "(", "encoding", "=", "'UTF-8'", ")", ")", "\n", "if", "vector", ":", "\n", "                    ", "word_vector", "=", "pickle", ".", "loads", "(", "vector", ")", "\n", "vector", "=", "None", "\n", "", "else", ":", "\n", "                    ", "word_vector", "=", "np", ".", "zeros", "(", "(", "self", ".", "k", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "", "", "except", "lmdb", ".", "Error", ":", "\n", "# no idea why, but we need to close and reopen the environment to avoid", "\n", "# mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot", "\n", "# when opening new transaction !", "\n", "            ", "self", ".", "env", ".", "close", "(", ")", "\n", "self", ".", "env", "=", "lmdb", ".", "open", "(", "self", ".", "store_filename", ",", "readonly", "=", "True", ",", "max_readers", "=", "2048", ",", "max_spare_txns", "=", "2", ",", "lock", "=", "False", ")", "\n", "return", "self", ".", "_get_vector", "(", "word", ")", "\n", "", "except", "ModuleNotFoundError", ":", "\n", "            ", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "log", ".", "warning", "(", "'ATTENTION! The library \"lmdb\" is not installed!'", ")", "\n", "log", ".", "warning", "(", "\n", "'To use LMDB, please first install with \"pip install lmdb\"'", "\n", ")", "\n", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "word_vector", "=", "np", ".", "zeros", "(", "(", "self", ".", "k", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "return", "word_vector", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.__init__": [[28, 37], ["data.Dictionary.add_item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.add_item"], ["def", "__init__", "(", "self", ",", "add_unk", "=", "True", ")", ":", "\n", "# init dictionaries", "\n", "        ", "self", ".", "item2idx", ":", "Dict", "[", "str", ",", "int", "]", "=", "{", "}", "\n", "self", ".", "idx2item", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "self", ".", "multi_label", ":", "bool", "=", "False", "\n", "\n", "# in order to deal with unknown tokens, add <unk>", "\n", "if", "add_unk", ":", "\n", "            ", "self", ".", "add_item", "(", "\"<unk>\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.add_item": [[38, 49], ["item.encode.encode.encode", "data.Dictionary.idx2item.append", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode"], ["", "", "def", "add_item", "(", "self", ",", "item", ":", "str", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        add string - if already in dictionary returns its ID. if not in dictionary, it will get a new ID.\n        :param item: a string for which to assign an id.\n        :return: ID of string\n        \"\"\"", "\n", "item", "=", "item", ".", "encode", "(", "\"utf-8\"", ")", "\n", "if", "item", "not", "in", "self", ".", "item2idx", ":", "\n", "            ", "self", ".", "idx2item", ".", "append", "(", "item", ")", "\n", "self", ".", "item2idx", "[", "item", "]", "=", "len", "(", "self", ".", "idx2item", ")", "-", "1", "\n", "", "return", "self", ".", "item2idx", "[", "item", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item": [[50, 61], ["item.encode.encode.encode", "data.Dictionary.item2idx.keys"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode"], ["", "def", "get_idx_for_item", "(", "self", ",", "item", ":", "str", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        returns the ID of the string, otherwise 0\n        :param item: string for which ID is requested\n        :return: ID of string, otherwise 0\n        \"\"\"", "\n", "item", "=", "item", ".", "encode", "(", "\"utf-8\"", ")", "\n", "if", "item", "in", "self", ".", "item2idx", ".", "keys", "(", ")", ":", "\n", "            ", "return", "self", ".", "item2idx", "[", "item", "]", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_items": [[62, 80], ["isinstance", "list", "hasattr", "dict", "collections.defaultdict", "operator.itemgetter", "key.decode", "data.Dictionary.item2idx.items"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.decode"], ["", "", "def", "get_idx_for_items", "(", "self", ",", "items", ":", "List", "[", "str", "]", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        returns the IDs for each item of the list of string, otherwise 0 if not found\n        :param items: List of string for which IDs are requested\n        :return: List of ID of strings\n        \"\"\"", "\n", "if", "not", "hasattr", "(", "self", ",", "\"item2idx_not_encoded\"", ")", ":", "\n", "            ", "d", "=", "dict", "(", "\n", "[", "(", "key", ".", "decode", "(", "\"UTF-8\"", ")", ",", "value", ")", "for", "key", ",", "value", "in", "self", ".", "item2idx", ".", "items", "(", ")", "]", "\n", ")", "\n", "self", ".", "item2idx_not_encoded", "=", "defaultdict", "(", "int", ",", "d", ")", "\n", "\n", "", "if", "not", "items", ":", "\n", "            ", "return", "[", "]", "\n", "", "results", "=", "itemgetter", "(", "*", "items", ")", "(", "self", ".", "item2idx_not_encoded", ")", "\n", "if", "isinstance", "(", "results", ",", "int", ")", ":", "\n", "            ", "return", "[", "results", "]", "\n", "", "return", "list", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_items": [[81, 86], ["items.append", "item.decode"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.decode"], ["", "def", "get_items", "(", "self", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "items", "=", "[", "]", "\n", "for", "item", "in", "self", ".", "idx2item", ":", "\n", "            ", "items", ".", "append", "(", "item", ".", "decode", "(", "\"UTF-8\"", ")", ")", "\n", "", "return", "items", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.__len__": [[87, 89], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "idx2item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_item_for_index": [[90, 92], ["data.Dictionary.idx2item[].decode"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.decode"], ["", "def", "get_item_for_index", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "self", ".", "idx2item", "[", "idx", "]", ".", "decode", "(", "\"UTF-8\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.save": [[93, 99], ["open", "pickle.dump"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "savefile", ")", ":", "\n", "        ", "import", "pickle", "\n", "\n", "with", "open", "(", "savefile", ",", "\"wb\"", ")", "as", "f", ":", "\n", "            ", "mappings", "=", "{", "\"idx2item\"", ":", "self", ".", "idx2item", ",", "\"item2idx\"", ":", "self", ".", "item2idx", "}", "\n", "pickle", ".", "dump", "(", "mappings", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.load_from_file": [[100, 112], ["data.Dictionary", "open", "pickle.load"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["", "", "@", "classmethod", "\n", "def", "load_from_file", "(", "cls", ",", "filename", ":", "str", ")", ":", "\n", "        ", "import", "pickle", "\n", "\n", "dictionary", ":", "Dictionary", "=", "Dictionary", "(", ")", "\n", "with", "open", "(", "filename", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "mappings", "=", "pickle", ".", "load", "(", "f", ",", "encoding", "=", "\"latin1\"", ")", "\n", "idx2item", "=", "mappings", "[", "\"idx2item\"", "]", "\n", "item2idx", "=", "mappings", "[", "\"item2idx\"", "]", "\n", "dictionary", ".", "item2idx", "=", "item2idx", "\n", "dictionary", ".", "idx2item", "=", "idx2item", "\n", "", "return", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.load": [[113, 133], ["data.Dictionary.load_from_file", "cached_path", "data.Dictionary.load_from_file", "cached_path", "data.Dictionary.load_from_file", "cached_path", "data.Dictionary.load_from_file"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.load_from_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.load_from_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.load_from_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.load_from_file"], ["", "@", "classmethod", "\n", "def", "load", "(", "cls", ",", "name", ":", "str", ")", ":", "\n", "        ", "from", "flair", ".", "file_utils", "import", "cached_path", "\n", "\n", "if", "name", "==", "\"chars\"", "or", "name", "==", "\"common-chars\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models/common_characters\"", "\n", "char_dict", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "\"datasets\"", ")", "\n", "return", "Dictionary", ".", "load_from_file", "(", "char_dict", ")", "\n", "\n", "", "if", "name", "==", "\"chars-large\"", "or", "name", "==", "\"common-chars-large\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models/common_characters_large\"", "\n", "char_dict", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "\"datasets\"", ")", "\n", "return", "Dictionary", ".", "load_from_file", "(", "char_dict", ")", "\n", "\n", "", "if", "name", "==", "\"chars-xl\"", "or", "name", "==", "\"common-chars-xl\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models/common_characters_xl\"", "\n", "char_dict", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "\"datasets\"", ")", "\n", "return", "Dictionary", ".", "load_from_file", "(", "char_dict", ")", "\n", "\n", "", "return", "Dictionary", ".", "load_from_file", "(", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.__str__": [[134, 137], ["data.Dictionary.get_item_for_index", "len", "range", "min", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_item_for_index"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "tags", "=", "', '", ".", "join", "(", "self", ".", "get_item_for_index", "(", "i", ")", "for", "i", "in", "range", "(", "min", "(", "len", "(", "self", ")", ",", "30", ")", ")", ")", "\n", "return", "f\"Dictionary with {len(self)} tags: {tags}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Label.__init__": [[145, 149], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "value", ":", "str", ",", "score", ":", "float", "=", "1.0", ")", ":", "\n", "        ", "self", ".", "value", "=", "value", "\n", "self", ".", "score", "=", "score", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Label.value": [[154, 162], ["ValueError"], "methods", ["None"], ["", "@", "value", ".", "setter", "\n", "def", "value", "(", "self", ",", "value", ")", ":", "\n", "        ", "if", "not", "value", "and", "value", "!=", "\"\"", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Incorrect label value provided. Label value needs to be set.\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_value", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Label.score": [[167, 173], ["None"], "methods", ["None"], ["", "@", "score", ".", "setter", "\n", "def", "score", "(", "self", ",", "score", ")", ":", "\n", "        ", "if", "0.0", "<=", "score", "<=", "1.0", ":", "\n", "            ", "self", ".", "_score", "=", "score", "\n", "", "else", ":", "\n", "            ", "self", ".", "_score", "=", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Label.to_dict": [[174, 176], ["None"], "methods", ["None"], ["", "", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "\"value\"", ":", "self", ".", "value", ",", "\"confidence\"", ":", "self", ".", "score", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Label.__str__": [[177, 179], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "f\"{self._value} ({self._score:.4f})\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Label.__repr__": [[180, 182], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "f\"{self._value} ({self._score:.4f})\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.__init__": [[192, 194], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "annotation_layers", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.embedding": [[195, 199], ["None"], "methods", ["None"], ["", "@", "property", "\n", "@", "abstractmethod", "\n", "def", "embedding", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.to": [[200, 203], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "to", "(", "self", ",", "device", ":", "str", ",", "pin_memory", ":", "bool", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.clear_embeddings": [[204, 207], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "clear_embeddings", "(", "self", ",", "embedding_names", ":", "List", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label": [[208, 216], ["data.DataPoint.annotation_layers[].append", "data.Label", "data.Label"], "methods", ["None"], ["", "def", "add_label", "(", "self", ",", "label_type", ":", "str", ",", "value", ":", "str", ",", "score", ":", "float", "=", "1.", ")", ":", "\n", "\n", "        ", "if", "label_type", "not", "in", "self", ".", "annotation_layers", ":", "\n", "            ", "self", ".", "annotation_layers", "[", "label_type", "]", "=", "[", "Label", "(", "value", ",", "score", ")", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "annotation_layers", "[", "label_type", "]", ".", "append", "(", "Label", "(", "value", ",", "score", ")", ")", "\n", "\n", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.set_label": [[217, 221], ["data.Label"], "methods", ["None"], ["", "def", "set_label", "(", "self", ",", "label_type", ":", "str", ",", "value", ":", "str", ",", "score", ":", "float", "=", "1.", ")", ":", "\n", "        ", "self", ".", "annotation_layers", "[", "label_type", "]", "=", "[", "Label", "(", "value", ",", "score", ")", "]", "\n", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.get_labels": [[222, 227], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ",", "label_type", ":", "str", "=", "None", ")", ":", "\n", "        ", "if", "label_type", "is", "None", ":", "\n", "            ", "return", "self", ".", "labels", "\n", "\n", "", "return", "self", ".", "annotation_layers", "[", "label_type", "]", "if", "label_type", "in", "self", ".", "annotation_layers", "else", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.labels": [[228, 234], ["data.DataPoint.annotation_layers.keys", "all_labels.extend"], "methods", ["None"], ["", "@", "property", "\n", "def", "labels", "(", "self", ")", "->", "List", "[", "Label", "]", ":", "\n", "        ", "all_labels", "=", "[", "]", "\n", "for", "key", "in", "self", ".", "annotation_layers", ".", "keys", "(", ")", ":", "\n", "            ", "all_labels", ".", "extend", "(", "self", ".", "annotation_layers", "[", "key", "]", ")", "\n", "", "return", "all_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPair.__init__": [[237, 241], ["data.DataPoint.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "first", ":", "DataPoint", ",", "second", ":", "DataPoint", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "first", "=", "first", "\n", "self", ".", "second", "=", "second", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPair.to": [[242, 245], ["data.DataPair.first.to", "data.DataPair.second.to"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "to", "(", "self", ",", "device", ":", "str", ",", "pin_memory", ":", "bool", "=", "False", ")", ":", "\n", "        ", "self", ".", "first", ".", "to", "(", "device", ",", "pin_memory", ")", "\n", "self", ".", "second", ".", "to", "(", "device", ",", "pin_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPair.clear_embeddings": [[246, 249], ["data.DataPair.first.clear_embeddings", "data.DataPair.second.clear_embeddings"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.clear_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.clear_embeddings"], ["", "def", "clear_embeddings", "(", "self", ",", "embedding_names", ":", "List", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "self", ".", "first", ".", "clear_embeddings", "(", "embedding_names", ")", "\n", "self", ".", "second", ".", "clear_embeddings", "(", "embedding_names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPair.embedding": [[250, 253], ["torch.cat"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding", "(", "self", ")", ":", "\n", "        ", "return", "torch", ".", "cat", "(", "[", "self", ".", "first", ".", "embedding", ",", "self", ".", "second", ".", "embedding", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPair.__str__": [[254, 256], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "f\"DataPair:\\n \u2212 First {self.first}\\n \u2212 Second {self.second}\\n \u2212 Labels: {self.labels}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPair.to_plain_string": [[257, 259], ["None"], "methods", ["None"], ["", "def", "to_plain_string", "(", "self", ")", ":", "\n", "        ", "return", "f\"DataPair: First {self.first}  ||  Second {self.second}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPair.__len__": [[260, 262], ["len", "len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "first", ")", "+", "len", "(", "self", ".", "second", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.__init__": [[270, 293], ["data.DataPoint.__init__", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "text", ":", "str", ",", "\n", "idx", ":", "int", "=", "None", ",", "\n", "head_id", ":", "int", "=", "None", ",", "\n", "whitespace_after", ":", "bool", "=", "True", ",", "\n", "start_position", ":", "int", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "text", ":", "str", "=", "text", "\n", "self", ".", "idx", ":", "int", "=", "idx", "\n", "self", ".", "head_id", ":", "int", "=", "head_id", "\n", "self", ".", "whitespace_after", ":", "bool", "=", "whitespace_after", "\n", "\n", "self", ".", "start_pos", "=", "start_position", "\n", "self", ".", "end_pos", "=", "(", "\n", "start_position", "+", "len", "(", "text", ")", "if", "start_position", "is", "not", "None", "else", "None", "\n", ")", "\n", "\n", "self", ".", "sentence", ":", "Sentence", "=", "None", "\n", "self", ".", "_embeddings", ":", "Dict", "=", "{", "}", "\n", "self", ".", "tags_proba_dist", ":", "Dict", "[", "str", ",", "List", "[", "Label", "]", "]", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tag_label": [[294, 296], ["data.Token.set_label"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.set_label"], ["", "def", "add_tag_label", "(", "self", ",", "tag_type", ":", "str", ",", "tag", ":", "Label", ")", ":", "\n", "        ", "self", ".", "set_label", "(", "tag_type", ",", "tag", ".", "value", ",", "tag", ".", "score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tags_proba_dist": [[297, 299], ["None"], "methods", ["None"], ["", "def", "add_tags_proba_dist", "(", "self", ",", "tag_type", ":", "str", ",", "tags", ":", "List", "[", "Label", "]", ")", ":", "\n", "        ", "self", ".", "tags_proba_dist", "[", "tag_type", "]", "=", "tags", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tag": [[300, 302], ["data.Token.set_label"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.set_label"], ["", "def", "add_tag", "(", "self", ",", "tag_type", ":", "str", ",", "tag_value", ":", "str", ",", "confidence", "=", "1.0", ")", ":", "\n", "        ", "self", ".", "set_label", "(", "tag_type", ",", "tag_value", ",", "confidence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag": [[303, 306], ["len", "data.Label", "data.Token.get_labels", "data.Token.get_labels"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.get_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.get_labels"], ["", "def", "get_tag", "(", "self", ",", "label_type", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "get_labels", "(", "label_type", ")", ")", "==", "0", ":", "return", "Label", "(", "''", ")", "\n", "return", "self", ".", "get_labels", "(", "label_type", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tags_proba_dist": [[307, 311], ["None"], "methods", ["None"], ["", "def", "get_tags_proba_dist", "(", "self", ",", "tag_type", ":", "str", ")", "->", "List", "[", "Label", "]", ":", "\n", "        ", "if", "tag_type", "in", "self", ".", "tags_proba_dist", ":", "\n", "            ", "return", "self", ".", "tags_proba_dist", "[", "tag_type", "]", "\n", "", "return", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_head": [[312, 314], ["data.Token.sentence.get_token"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token"], ["", "def", "get_head", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sentence", ".", "get_token", "(", "self", ".", "head_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.set_embedding": [[315, 322], ["vector.to.to.to", "len", "next", "data.Token._embeddings.keys", "iter", "data.Token._embeddings.values"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "set_embedding", "(", "self", ",", "name", ":", "str", ",", "vector", ":", "torch", ".", "tensor", ")", ":", "\n", "        ", "device", "=", "flair", ".", "device", "\n", "if", "(", "flair", ".", "embedding_storage_mode", "==", "\"cpu\"", ")", "and", "len", "(", "self", ".", "_embeddings", ".", "keys", "(", ")", ")", ">", "0", ":", "\n", "            ", "device", "=", "next", "(", "iter", "(", "self", ".", "_embeddings", ".", "values", "(", ")", ")", ")", ".", "device", "\n", "", "if", "device", "!=", "vector", ".", "device", ":", "\n", "            ", "vector", "=", "vector", ".", "to", "(", "device", ")", "\n", "", "self", ".", "_embeddings", "[", "name", "]", "=", "vector", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.to": [[323, 332], ["data.Token._embeddings.items", "str", "str", "vector.to().pin_memory", "vector.to", "vector.to"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "to", "(", "self", ",", "device", ":", "str", ",", "pin_memory", ":", "bool", "=", "False", ")", ":", "\n", "        ", "for", "name", ",", "vector", "in", "self", ".", "_embeddings", ".", "items", "(", ")", ":", "\n", "            ", "if", "str", "(", "vector", ".", "device", ")", "!=", "str", "(", "device", ")", ":", "\n", "                ", "if", "pin_memory", ":", "\n", "                    ", "self", ".", "_embeddings", "[", "name", "]", "=", "vector", ".", "to", "(", "\n", "device", ",", "non_blocking", "=", "True", "\n", ")", ".", "pin_memory", "(", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_embeddings", "[", "name", "]", "=", "vector", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.clear_embeddings": [[333, 340], ["data.Token._embeddings.keys"], "methods", ["None"], ["", "", "", "", "def", "clear_embeddings", "(", "self", ",", "embedding_names", ":", "List", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "if", "embedding_names", "is", "None", ":", "\n", "            ", "self", ".", "_embeddings", ":", "Dict", "=", "{", "}", "\n", "", "else", ":", "\n", "            ", "for", "name", "in", "embedding_names", ":", "\n", "                ", "if", "name", "in", "self", ".", "_embeddings", ".", "keys", "(", ")", ":", "\n", "                    ", "del", "self", ".", "_embeddings", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_each_embedding": [[341, 349], ["sorted", "data.Token._embeddings.keys", "data.Token._embeddings[].to", "embeddings.append", "embed.to.to.to"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "", "", "", "def", "get_each_embedding", "(", "self", ")", "->", "torch", ".", "tensor", ":", "\n", "        ", "embeddings", "=", "[", "]", "\n", "for", "embed", "in", "sorted", "(", "self", ".", "_embeddings", ".", "keys", "(", ")", ")", ":", "\n", "            ", "embed", "=", "self", ".", "_embeddings", "[", "embed", "]", ".", "to", "(", "flair", ".", "device", ")", "\n", "if", "(", "flair", ".", "embedding_storage_mode", "==", "\"cpu\"", ")", "and", "embed", ".", "device", "!=", "flair", ".", "device", ":", "\n", "                ", "embed", "=", "embed", ".", "to", "(", "flair", ".", "device", ")", "\n", "", "embeddings", ".", "append", "(", "embed", ")", "\n", "", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_embedding": [[350, 357], ["data.Token.get_each_embedding", "torch.tensor", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_each_embedding"], ["", "def", "get_embedding", "(", "self", ")", "->", "torch", ".", "tensor", ":", "\n", "        ", "embeddings", "=", "self", ".", "get_each_embedding", "(", ")", "\n", "\n", "if", "embeddings", ":", "\n", "            ", "return", "torch", ".", "cat", "(", "embeddings", ",", "dim", "=", "0", ")", "\n", "\n", "", "return", "torch", ".", "tensor", "(", "[", "]", ",", "device", "=", "flair", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.start_position": [[358, 361], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "start_position", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "start_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.end_position": [[362, 365], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "end_position", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "end_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.embedding": [[366, 369], ["data.Token.get_embedding"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding"], ["", "@", "property", "\n", "def", "embedding", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "get_embedding", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.__str__": [[370, 375], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "(", "\n", "\"Token: {} {}\"", ".", "format", "(", "self", ".", "idx", ",", "self", ".", "text", ")", "\n", "if", "self", ".", "idx", "is", "not", "None", "\n", "else", "\"Token: {}\"", ".", "format", "(", "self", ".", "text", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.__repr__": [[377, 382], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "(", "\n", "\"Token: {} {}\"", ".", "format", "(", "self", ".", "idx", ",", "self", ".", "text", ")", "\n", "if", "self", ".", "idx", "is", "not", "None", "\n", "else", "\"Token: {}\"", ".", "format", "(", "self", ".", "text", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Span.__init__": [[390, 401], ["data.DataPoint.__init__", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "tokens", ":", "List", "[", "Token", "]", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "start_pos", "=", "None", "\n", "self", ".", "end_pos", "=", "None", "\n", "\n", "if", "tokens", ":", "\n", "            ", "self", ".", "start_pos", "=", "tokens", "[", "0", "]", ".", "start_position", "\n", "self", ".", "end_pos", "=", "tokens", "[", "len", "(", "tokens", ")", "-", "1", "]", ".", "end_position", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Span.text": [[402, 405], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "text", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "\" \"", ".", "join", "(", "[", "t", ".", "text", "for", "t", "in", "self", ".", "tokens", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Span.to_original_text": [[406, 420], ["len"], "methods", ["None"], ["", "def", "to_original_text", "(", "self", ")", "->", "str", ":", "\n", "        ", "pos", "=", "self", ".", "tokens", "[", "0", "]", ".", "start_pos", "\n", "if", "pos", "is", "None", ":", "\n", "            ", "return", "\" \"", ".", "join", "(", "[", "t", ".", "text", "for", "t", "in", "self", ".", "tokens", "]", ")", "\n", "", "str", "=", "\"\"", "\n", "for", "t", "in", "self", ".", "tokens", ":", "\n", "            ", "while", "t", ".", "start_pos", "!=", "pos", ":", "\n", "                ", "str", "+=", "\" \"", "\n", "pos", "+=", "1", "\n", "\n", "", "str", "+=", "t", ".", "text", "\n", "pos", "+=", "len", "(", "t", ".", "text", ")", "\n", "\n", "", "return", "str", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Span.to_dict": [[421, 427], ["data.Span.to_original_text"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_original_text"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "\"text\"", ":", "self", ".", "to_original_text", "(", ")", ",", "\n", "\"start_pos\"", ":", "self", ".", "start_pos", ",", "\n", "\"end_pos\"", ":", "self", ".", "end_pos", ",", "\n", "\"labels\"", ":", "self", ".", "labels", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Span.__str__": [[429, 435], ["str", "str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "ids", "=", "\",\"", ".", "join", "(", "[", "str", "(", "t", ".", "idx", ")", "for", "t", "in", "self", ".", "tokens", "]", ")", "\n", "label_string", "=", "\" \"", ".", "join", "(", "[", "str", "(", "label", ")", "for", "label", "in", "self", ".", "labels", "]", ")", "\n", "labels", "=", "f'   [\u2212 Labels: {label_string}]'", "if", "self", ".", "labels", "is", "not", "None", "else", "\"\"", "\n", "return", "(", "\n", "'Span [{}]: \"{}\"{}'", ".", "format", "(", "ids", ",", "self", ".", "text", ",", "labels", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Span.__repr__": [[437, 443], ["str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "        ", "ids", "=", "\",\"", ".", "join", "(", "[", "str", "(", "t", ".", "idx", ")", "for", "t", "in", "self", ".", "tokens", "]", ")", "\n", "return", "(", "\n", "'<{}-span ({}): \"{}\">'", ".", "format", "(", "self", ".", "tag", ",", "ids", ",", "self", ".", "text", ")", "\n", "if", "self", ".", "tag", "is", "not", "None", "\n", "else", "'<span ({}): \"{}\">'", ".", "format", "(", "ids", ",", "self", ".", "text", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Span.tag": [[445, 448], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "tag", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "labels", "[", "0", "]", ".", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Span.score": [[449, 452], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "score", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "labels", "[", "0", "]", ".", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.__init__": [[459, 499], ["data.DataPoint.__init__", "type", "data.Sentence._restore_windows_1252_characters", "log.warning", "data.Sentence.add_token", "tokenizer"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence._restore_windows_1252_characters", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token"], ["def", "__init__", "(", "\n", "self", ",", "\n", "text", ":", "str", "=", "None", ",", "\n", "use_tokenizer", ":", "Union", "[", "bool", ",", "Callable", "[", "[", "str", "]", ",", "List", "[", "Token", "]", "]", "]", "=", "False", ",", "\n", "language_code", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Class to hold all meta related to a text (tokens, predictions, language code, ...)\n        :param text: original string\n        :param use_tokenizer: a custom tokenizer (default is space based tokenizer,\n        more advanced options are segtok_tokenizer to use segtok or build_spacy_tokenizer to use Spacy library\n        if available). Check the code of space_tokenizer to implement your own (if you need it).\n        If instead of providing a function, this parameter is just set to True, segtok will be used.\n        :param labels:\n        :param language_code:\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tokens", ":", "List", "[", "Token", "]", "=", "[", "]", "\n", "\n", "self", ".", "_embeddings", ":", "Dict", "=", "{", "}", "\n", "\n", "self", ".", "language_code", ":", "str", "=", "language_code", "\n", "\n", "tokenizer", "=", "use_tokenizer", "\n", "if", "type", "(", "use_tokenizer", ")", "==", "bool", ":", "\n", "            ", "tokenizer", "=", "segtok_tokenizer", "if", "use_tokenizer", "else", "space_tokenizer", "\n", "\n", "# if text is passed, instantiate sentence with tokens (words)", "\n", "", "if", "text", "is", "not", "None", ":", "\n", "            ", "text", "=", "self", ".", "_restore_windows_1252_characters", "(", "text", ")", "\n", "[", "self", ".", "add_token", "(", "token", ")", "for", "token", "in", "tokenizer", "(", "text", ")", "]", "\n", "\n", "# log a warning if the dataset is empty", "\n", "", "if", "text", "==", "\"\"", ":", "\n", "            ", "log", ".", "warning", "(", "\n", "\"ACHTUNG: An empty Sentence was created! Are there empty strings in your dataset?\"", "\n", ")", "\n", "\n", "", "self", ".", "tokenized", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token": [[500, 504], ["None"], "methods", ["None"], ["", "def", "get_token", "(", "self", ",", "token_id", ":", "int", ")", "->", "Token", ":", "\n", "        ", "for", "token", "in", "self", ".", "tokens", ":", "\n", "            ", "if", "token", ".", "idx", "==", "token_id", ":", "\n", "                ", "return", "token", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token": [[505, 516], ["data.Sentence.tokens.append", "type", "data.Token", "len"], "methods", ["None"], ["", "", "", "def", "add_token", "(", "self", ",", "token", ":", "Union", "[", "Token", ",", "str", "]", ")", ":", "\n", "\n", "        ", "if", "type", "(", "token", ")", "is", "str", ":", "\n", "            ", "token", "=", "Token", "(", "token", ")", "\n", "\n", "", "self", ".", "tokens", ".", "append", "(", "token", ")", "\n", "\n", "# set token idx if not set", "\n", "token", ".", "sentence", "=", "self", "\n", "if", "token", ".", "idx", "is", "None", ":", "\n", "            ", "token", ".", "idx", "=", "len", "(", "self", ".", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_label_names": [[517, 522], ["label_names.append"], "methods", ["None"], ["", "", "def", "get_label_names", "(", "self", ")", ":", "\n", "        ", "label_names", "=", "[", "]", "\n", "for", "label", "in", "self", ".", "labels", ":", "\n", "            ", "label_names", ".", "append", "(", "label", ".", "value", ")", "\n", "", "return", "label_names", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_spans": [[523, 596], ["collections.defaultdict", "token.get_tag", "len", "collections.defaultdict", "current_span.append", "sum", "len", "data.Span", "data.DataPoint.add_label", "spans.append", "len", "sum", "len", "data.Span", "data.DataPoint.add_label", "spans.append", "t.get_labels", "t.get_labels", "sorted", "sorted", "collections.defaultdict.items", "collections.defaultdict.items"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.get_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.get_labels"], ["", "def", "get_spans", "(", "self", ",", "label_type", ":", "str", ",", "min_score", "=", "-", "1", ")", "->", "List", "[", "Span", "]", ":", "\n", "\n", "        ", "spans", ":", "List", "[", "Span", "]", "=", "[", "]", "\n", "\n", "current_span", "=", "[", "]", "\n", "\n", "tags", "=", "defaultdict", "(", "lambda", ":", "0.0", ")", "\n", "\n", "previous_tag_value", ":", "str", "=", "\"O\"", "\n", "for", "token", "in", "self", ":", "\n", "\n", "            ", "tag", ":", "Label", "=", "token", ".", "get_tag", "(", "label_type", ")", "\n", "tag_value", "=", "tag", ".", "value", "\n", "\n", "# non-set tags are OUT tags", "\n", "if", "tag_value", "==", "\"\"", "or", "tag_value", "==", "\"O\"", ":", "\n", "                ", "tag_value", "=", "\"O-\"", "\n", "\n", "# anything that is not a BIOES tag is a SINGLE tag", "\n", "", "if", "tag_value", "[", "0", ":", "2", "]", "not", "in", "[", "\"B-\"", ",", "\"I-\"", ",", "\"O-\"", ",", "\"E-\"", ",", "\"S-\"", "]", ":", "\n", "                ", "tag_value", "=", "\"S-\"", "+", "tag_value", "\n", "\n", "# anything that is not OUT is IN", "\n", "", "in_span", "=", "False", "\n", "if", "tag_value", "[", "0", ":", "2", "]", "not", "in", "[", "\"O-\"", "]", ":", "\n", "                ", "in_span", "=", "True", "\n", "\n", "# single and begin tags start a new span", "\n", "", "starts_new_span", "=", "False", "\n", "if", "tag_value", "[", "0", ":", "2", "]", "in", "[", "\"B-\"", ",", "\"S-\"", "]", ":", "\n", "                ", "starts_new_span", "=", "True", "\n", "\n", "", "if", "(", "\n", "previous_tag_value", "[", "0", ":", "2", "]", "in", "[", "\"S-\"", "]", "\n", "and", "previous_tag_value", "[", "2", ":", "]", "!=", "tag_value", "[", "2", ":", "]", "\n", "and", "in_span", "\n", ")", ":", "\n", "                ", "starts_new_span", "=", "True", "\n", "\n", "", "if", "(", "starts_new_span", "or", "not", "in_span", ")", "and", "len", "(", "current_span", ")", ">", "0", ":", "\n", "                ", "scores", "=", "[", "t", ".", "get_labels", "(", "label_type", ")", "[", "0", "]", ".", "score", "for", "t", "in", "current_span", "]", "\n", "span_score", "=", "sum", "(", "scores", ")", "/", "len", "(", "scores", ")", "\n", "if", "span_score", ">", "min_score", ":", "\n", "                    ", "span", "=", "Span", "(", "current_span", ")", "\n", "span", ".", "add_label", "(", "\n", "label_type", "=", "label_type", ",", "\n", "value", "=", "sorted", "(", "tags", ".", "items", "(", ")", ",", "key", "=", "lambda", "k_v", ":", "k_v", "[", "1", "]", ",", "reverse", "=", "True", ")", "[", "0", "]", "[", "0", "]", ",", "\n", "score", "=", "span_score", ")", "\n", "spans", ".", "append", "(", "span", ")", "\n", "\n", "", "current_span", "=", "[", "]", "\n", "tags", "=", "defaultdict", "(", "lambda", ":", "0.0", ")", "\n", "\n", "", "if", "in_span", ":", "\n", "                ", "current_span", ".", "append", "(", "token", ")", "\n", "weight", "=", "1.1", "if", "starts_new_span", "else", "1.0", "\n", "tags", "[", "tag_value", "[", "2", ":", "]", "]", "+=", "weight", "\n", "\n", "# remember previous tag", "\n", "", "previous_tag_value", "=", "tag_value", "\n", "\n", "", "if", "len", "(", "current_span", ")", ">", "0", ":", "\n", "            ", "scores", "=", "[", "t", ".", "get_labels", "(", "label_type", ")", "[", "0", "]", ".", "score", "for", "t", "in", "current_span", "]", "\n", "span_score", "=", "sum", "(", "scores", ")", "/", "len", "(", "scores", ")", "\n", "if", "span_score", ">", "min_score", ":", "\n", "                ", "span", "=", "Span", "(", "current_span", ")", "\n", "span", ".", "add_label", "(", "\n", "label_type", "=", "label_type", ",", "\n", "value", "=", "sorted", "(", "tags", ".", "items", "(", ")", ",", "key", "=", "lambda", "k_v", ":", "k_v", "[", "1", "]", ",", "reverse", "=", "True", ")", "[", "0", "]", "[", "0", "]", ",", "\n", "score", "=", "span_score", ")", "\n", "spans", ".", "append", "(", "span", ")", "\n", "\n", "", "", "return", "spans", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.embedding": [[597, 600], ["data.Sentence.get_embedding"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding"], ["", "@", "property", "\n", "def", "embedding", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "get_embedding", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.set_embedding": [[601, 608], ["vector.to.to.to", "len", "next", "data.Sentence._embeddings.keys", "iter", "data.Sentence._embeddings.values"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "set_embedding", "(", "self", ",", "name", ":", "str", ",", "vector", ":", "torch", ".", "tensor", ")", ":", "\n", "        ", "device", "=", "flair", ".", "device", "\n", "if", "(", "flair", ".", "embedding_storage_mode", "==", "\"cpu\"", ")", "and", "len", "(", "self", ".", "_embeddings", ".", "keys", "(", ")", ")", ">", "0", ":", "\n", "            ", "device", "=", "next", "(", "iter", "(", "self", ".", "_embeddings", ".", "values", "(", ")", ")", ")", ".", "device", "\n", "", "if", "device", "!=", "vector", ".", "device", ":", "\n", "            ", "vector", "=", "vector", ".", "to", "(", "device", ")", "\n", "", "self", ".", "_embeddings", "[", "name", "]", "=", "vector", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_embedding": [[609, 619], ["sorted", "torch.Tensor", "data.Sentence._embeddings.keys", "embeddings.append", "torch.cat"], "methods", ["None"], ["", "def", "get_embedding", "(", "self", ")", "->", "torch", ".", "tensor", ":", "\n", "        ", "embeddings", "=", "[", "]", "\n", "for", "embed", "in", "sorted", "(", "self", ".", "_embeddings", ".", "keys", "(", ")", ")", ":", "\n", "            ", "embedding", "=", "self", ".", "_embeddings", "[", "embed", "]", "\n", "embeddings", ".", "append", "(", "embedding", ")", "\n", "\n", "", "if", "embeddings", ":", "\n", "            ", "return", "torch", ".", "cat", "(", "embeddings", ",", "dim", "=", "0", ")", "\n", "\n", "", "return", "torch", ".", "Tensor", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to": [[620, 635], ["data.Sentence._embeddings.items", "token.to", "str", "str", "vector.to().pin_memory", "vector.to", "vector.to"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "to", "(", "self", ",", "device", ":", "str", ",", "pin_memory", ":", "bool", "=", "False", ")", ":", "\n", "\n", "# move sentence embeddings to device", "\n", "        ", "for", "name", ",", "vector", "in", "self", ".", "_embeddings", ".", "items", "(", ")", ":", "\n", "            ", "if", "str", "(", "vector", ".", "device", ")", "!=", "str", "(", "device", ")", ":", "\n", "                ", "if", "pin_memory", ":", "\n", "                    ", "self", ".", "_embeddings", "[", "name", "]", "=", "vector", ".", "to", "(", "\n", "device", ",", "non_blocking", "=", "True", "\n", ")", ".", "pin_memory", "(", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_embeddings", "[", "name", "]", "=", "vector", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n", "# move token embeddings to device", "\n", "", "", "", "for", "token", "in", "self", ":", "\n", "            ", "token", ".", "to", "(", "device", ",", "pin_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.clear_embeddings": [[636, 649], ["token.clear_embeddings", "data.Sentence._embeddings.keys"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.clear_embeddings"], ["", "", "def", "clear_embeddings", "(", "self", ",", "embedding_names", ":", "List", "[", "str", "]", "=", "None", ")", ":", "\n", "\n", "# clear sentence embeddings", "\n", "        ", "if", "embedding_names", "is", "None", ":", "\n", "            ", "self", ".", "_embeddings", ":", "Dict", "=", "{", "}", "\n", "", "else", ":", "\n", "            ", "for", "name", "in", "embedding_names", ":", "\n", "                ", "if", "name", "in", "self", ".", "_embeddings", ".", "keys", "(", ")", ":", "\n", "                    ", "del", "self", ".", "_embeddings", "[", "name", "]", "\n", "\n", "# clear token embeddings", "\n", "", "", "", "for", "token", "in", "self", ":", "\n", "            ", "token", ".", "clear_embeddings", "(", "embedding_names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tagged_string": [[650, 669], ["list.append", "token.annotation_layers.keys", "tags.append", "list.append", "token.get_labels", "token.get_labels"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.get_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.get_labels"], ["", "", "def", "to_tagged_string", "(", "self", ",", "main_tag", "=", "None", ")", "->", "str", ":", "\n", "        ", "list", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "tokens", ":", "\n", "            ", "list", ".", "append", "(", "token", ".", "text", ")", "\n", "\n", "tags", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "for", "label_type", "in", "token", ".", "annotation_layers", ".", "keys", "(", ")", ":", "\n", "\n", "                ", "if", "main_tag", "is", "not", "None", "and", "main_tag", "!=", "label_type", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "token", ".", "get_labels", "(", "label_type", ")", "[", "0", "]", ".", "value", "==", "\"O\"", ":", "\n", "                    ", "continue", "\n", "\n", "", "tags", ".", "append", "(", "token", ".", "get_labels", "(", "label_type", ")", "[", "0", "]", ".", "value", ")", "\n", "", "all_tags", "=", "\"<\"", "+", "\"/\"", ".", "join", "(", "tags", ")", "+", "\">\"", "\n", "if", "all_tags", "!=", "\"<>\"", ":", "\n", "                ", "list", ".", "append", "(", "all_tags", ")", "\n", "", "", "return", "\" \"", ".", "join", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string": [[670, 676], ["None"], "methods", ["None"], ["", "def", "to_tokenized_string", "(", "self", ")", "->", "str", ":", "\n", "\n", "        ", "if", "self", ".", "tokenized", "is", "None", ":", "\n", "            ", "self", ".", "tokenized", "=", "\" \"", ".", "join", "(", "[", "t", ".", "text", "for", "t", "in", "self", ".", "tokens", "]", ")", "\n", "\n", "", "return", "self", ".", "tokenized", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_plain_string": [[677, 684], ["plain.rstrip"], "methods", ["None"], ["", "def", "to_plain_string", "(", "self", ")", ":", "\n", "        ", "plain", "=", "\"\"", "\n", "for", "token", "in", "self", ".", "tokens", ":", "\n", "            ", "plain", "+=", "token", ".", "text", "\n", "if", "token", ".", "whitespace_after", ":", "\n", "                ", "plain", "+=", "\" \"", "\n", "", "", "return", "plain", ".", "rstrip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.convert_tag_scheme": [[685, 700], ["enumerate", "iob_iobes.append", "data.iob2", "data.iob2", "data.iob_iobes", "data.Sentence.tokens[].set_label", "token.get_tag"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.iob2", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.iob2", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.iob_iobes", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.set_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag"], ["", "def", "convert_tag_scheme", "(", "self", ",", "tag_type", ":", "str", "=", "\"ner\"", ",", "target_scheme", ":", "str", "=", "\"iob\"", ")", ":", "\n", "\n", "        ", "tags", ":", "List", "[", "Label", "]", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "tokens", ":", "\n", "            ", "tags", ".", "append", "(", "token", ".", "get_tag", "(", "tag_type", ")", ")", "\n", "\n", "", "if", "target_scheme", "==", "\"iob\"", ":", "\n", "            ", "iob2", "(", "tags", ")", "\n", "\n", "", "if", "target_scheme", "==", "\"iobes\"", ":", "\n", "            ", "iob2", "(", "tags", ")", "\n", "tags", "=", "iob_iobes", "(", "tags", ")", "\n", "\n", "", "for", "index", ",", "tag", "in", "enumerate", "(", "tags", ")", ":", "\n", "            ", "self", ".", "tokens", "[", "index", "]", ".", "set_label", "(", "tag_type", ",", "tag", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.infer_space_after": [[701, 732], ["token.text.startswith"], "methods", ["None"], ["", "", "def", "infer_space_after", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Heuristics in case you wish to infer whitespace_after values for tokenized text. This is useful for some old NLP\n        tasks (such as CoNLL-03 and CoNLL-2000) that provide only tokenized data with no info of original whitespacing.\n        :return:\n        \"\"\"", "\n", "last_token", "=", "None", "\n", "quote_count", ":", "int", "=", "0", "\n", "# infer whitespace after field", "\n", "\n", "for", "token", "in", "self", ".", "tokens", ":", "\n", "            ", "if", "token", ".", "text", "==", "'\"'", ":", "\n", "                ", "quote_count", "+=", "1", "\n", "if", "quote_count", "%", "2", "!=", "0", ":", "\n", "                    ", "token", ".", "whitespace_after", "=", "False", "\n", "", "elif", "last_token", "is", "not", "None", ":", "\n", "                    ", "last_token", ".", "whitespace_after", "=", "False", "\n", "\n", "", "", "if", "last_token", "is", "not", "None", ":", "\n", "\n", "                ", "if", "token", ".", "text", "in", "[", "\".\"", ",", "\":\"", ",", "\",\"", ",", "\";\"", ",", "\")\"", ",", "\"n't\"", ",", "\"!\"", ",", "\"?\"", "]", ":", "\n", "                    ", "last_token", ".", "whitespace_after", "=", "False", "\n", "\n", "", "if", "token", ".", "text", ".", "startswith", "(", "\"'\"", ")", ":", "\n", "                    ", "last_token", ".", "whitespace_after", "=", "False", "\n", "\n", "", "", "if", "token", ".", "text", "in", "[", "\"(\"", "]", ":", "\n", "                ", "token", ".", "whitespace_after", "=", "False", "\n", "\n", "", "last_token", "=", "token", "\n", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_original_text": [[733, 747], ["len", "len"], "methods", ["None"], ["", "def", "to_original_text", "(", "self", ")", "->", "str", ":", "\n", "        ", "if", "len", "(", "self", ".", "tokens", ")", ">", "0", "and", "(", "self", ".", "tokens", "[", "0", "]", ".", "start_pos", "is", "None", ")", ":", "\n", "            ", "return", "\" \"", ".", "join", "(", "[", "t", ".", "text", "for", "t", "in", "self", ".", "tokens", "]", ")", "\n", "", "str", "=", "\"\"", "\n", "pos", "=", "0", "\n", "for", "t", "in", "self", ".", "tokens", ":", "\n", "            ", "while", "t", ".", "start_pos", "!=", "pos", ":", "\n", "                ", "str", "+=", "\" \"", "\n", "pos", "+=", "1", "\n", "\n", "", "str", "+=", "t", ".", "text", "\n", "pos", "+=", "len", "(", "t", ".", "text", ")", "\n", "\n", "", "return", "str", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_dict": [[748, 758], ["data.Sentence.to_original_text", "span.to_dict", "l.to_dict", "data.Sentence.get_spans"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_original_text", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_dict", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_dict", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_spans"], ["", "def", "to_dict", "(", "self", ",", "tag_type", ":", "str", "=", "None", ")", ":", "\n", "        ", "labels", "=", "[", "]", "\n", "entities", "=", "[", "]", "\n", "\n", "if", "tag_type", ":", "\n", "            ", "entities", "=", "[", "span", ".", "to_dict", "(", ")", "for", "span", "in", "self", ".", "get_spans", "(", "tag_type", ")", "]", "\n", "", "if", "self", ".", "labels", ":", "\n", "            ", "labels", "=", "[", "l", ".", "to_dict", "(", ")", "for", "l", "in", "self", ".", "labels", "]", "\n", "\n", "", "return", "{", "\"text\"", ":", "self", ".", "to_original_text", "(", ")", ",", "\"labels\"", ":", "labels", ",", "\"entities\"", ":", "entities", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.__getitem__": [[759, 761], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ":", "int", ")", "->", "Token", ":", "\n", "        ", "return", "self", ".", "tokens", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.__iter__": [[762, 764], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.__repr__": [[765, 776], ["data.Sentence.to_tagged_string", "data.Sentence.to_tokenized_string", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tagged_string", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "tagged_string", "=", "self", ".", "to_tagged_string", "(", ")", "\n", "tokenized_string", "=", "self", ".", "to_tokenized_string", "(", ")", "\n", "\n", "# add Sentence labels to output if they exist", "\n", "sentence_labels", "=", "f\"  \u2212 Sentence-Labels: {self.annotation_layers}\"", "if", "self", ".", "annotation_layers", "!=", "{", "}", "else", "\"\"", "\n", "\n", "# add Token labels to output if they exist", "\n", "token_labels", "=", "f'  \u2212 Token-Labels: \"{tagged_string}\"'", "if", "tokenized_string", "!=", "tagged_string", "else", "\"\"", "\n", "\n", "return", "f'Sentence: \"{tokenized_string}\"   [\u2212 Tokens: {len(self)}{token_labels}{sentence_labels}]'", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.__copy__": [[777, 790], ["data.Sentence", "data.Token", "data.Sentence.add_token", "data.DataPoint.add_label", "token.get_tag", "token.get_tag"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag"], ["", "def", "__copy__", "(", "self", ")", ":", "\n", "        ", "s", "=", "Sentence", "(", ")", "\n", "for", "token", "in", "self", ".", "tokens", ":", "\n", "            ", "nt", "=", "Token", "(", "token", ".", "text", ")", "\n", "for", "tag_type", "in", "token", ".", "tags", ":", "\n", "                ", "nt", ".", "add_label", "(", "\n", "tag_type", ",", "\n", "token", ".", "get_tag", "(", "tag_type", ")", ".", "value", ",", "\n", "token", ".", "get_tag", "(", "tag_type", ")", ".", "score", ",", "\n", ")", "\n", "\n", "", "s", ".", "add_token", "(", "nt", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.__str__": [[791, 803], ["data.Sentence.to_tagged_string", "data.Sentence.to_tokenized_string", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tagged_string", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "\n", "        ", "tagged_string", "=", "self", ".", "to_tagged_string", "(", ")", "\n", "tokenized_string", "=", "self", ".", "to_tokenized_string", "(", ")", "\n", "\n", "# add Sentence labels to output if they exist", "\n", "sentence_labels", "=", "f\"  \u2212 Sentence-Labels: {self.annotation_layers}\"", "if", "self", ".", "annotation_layers", "!=", "{", "}", "else", "\"\"", "\n", "\n", "# add Token labels to output if they exist", "\n", "token_labels", "=", "f'  \u2212 Token-Labels: \"{tagged_string}\"'", "if", "tokenized_string", "!=", "tagged_string", "else", "\"\"", "\n", "\n", "return", "f'Sentence: \"{tokenized_string}\"   [\u2212 Tokens: {len(self)}{token_labels}{sentence_labels}]'", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.__len__": [[804, 806], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_language_code": [[807, 817], ["langdetect.detect", "data.Sentence.to_plain_string"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_plain_string"], ["", "def", "get_language_code", "(", "self", ")", "->", "str", ":", "\n", "        ", "if", "self", ".", "language_code", "is", "None", ":", "\n", "            ", "import", "langdetect", "\n", "\n", "try", ":", "\n", "                ", "self", ".", "language_code", "=", "langdetect", ".", "detect", "(", "self", ".", "to_plain_string", "(", ")", ")", "\n", "", "except", ":", "\n", "                ", "self", ".", "language_code", "=", "\"en\"", "\n", "\n", "", "", "return", "self", ".", "language_code", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence._restore_windows_1252_characters": [[818, 828], ["re.sub", "bytes().decode", "bytes", "ord", "match.group"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.decode"], ["", "@", "staticmethod", "\n", "def", "_restore_windows_1252_characters", "(", "text", ":", "str", ")", "->", "str", ":", "\n", "        ", "def", "to_windows_1252", "(", "match", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "return", "bytes", "(", "[", "ord", "(", "match", ".", "group", "(", "0", ")", ")", "]", ")", ".", "decode", "(", "\"windows-1252\"", ")", "\n", "", "except", "UnicodeDecodeError", ":", "\n", "# No character at the corresponding code point: remove it", "\n", "                ", "return", "\"\"", "\n", "\n", "", "", "return", "re", ".", "sub", "(", "r\"[\\u0080-\\u0099]\"", ",", "to_windows_1252", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.__init__": [[832, 838], ["data.DataPoint.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data", "=", "None", ",", "imageURL", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "_embeddings", ":", "Dict", "=", "{", "}", "\n", "self", ".", "imageURL", "=", "imageURL", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.embedding": [[839, 842], ["data.Image.get_embedding"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding"], ["", "@", "property", "\n", "def", "embedding", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "get_embedding", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.__str__": [[843, 849], ["data.Image.data.size"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "\n", "        ", "image_repr", "=", "self", ".", "data", ".", "size", "(", ")", "if", "self", ".", "data", "else", "\"\"", "\n", "image_url", "=", "self", ".", "imageURL", "if", "self", ".", "imageURL", "else", "\"\"", "\n", "\n", "return", "f\"Image: {image_repr} {image_url}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding": [[850, 859], ["torch.tensor", "torch.cat", "sorted", "data.Image._embeddings.keys"], "methods", ["None"], ["", "def", "get_embedding", "(", "self", ")", "->", "torch", ".", "tensor", ":", "\n", "        ", "embeddings", "=", "[", "\n", "self", ".", "_embeddings", "[", "embed", "]", "for", "embed", "in", "sorted", "(", "self", ".", "_embeddings", ".", "keys", "(", ")", ")", "\n", "]", "\n", "\n", "if", "embeddings", ":", "\n", "            ", "return", "torch", ".", "cat", "(", "embeddings", ",", "dim", "=", "0", ")", "\n", "\n", "", "return", "torch", ".", "tensor", "(", "[", "]", ",", "device", "=", "flair", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding": [[860, 867], ["vector.to.to.to", "len", "next", "data.Image._embeddings.keys", "iter", "data.Image._embeddings.values"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "set_embedding", "(", "self", ",", "name", ":", "str", ",", "vector", ":", "torch", ".", "tensor", ")", ":", "\n", "        ", "device", "=", "flair", ".", "device", "\n", "if", "(", "flair", ".", "embedding_storage_mode", "==", "\"cpu\"", ")", "and", "len", "(", "self", ".", "_embeddings", ".", "keys", "(", ")", ")", ">", "0", ":", "\n", "            ", "device", "=", "next", "(", "iter", "(", "self", ".", "_embeddings", ".", "values", "(", ")", ")", ")", ".", "device", "\n", "", "if", "device", "!=", "vector", ".", "device", ":", "\n", "            ", "vector", "=", "vector", ".", "to", "(", "device", ")", "\n", "", "self", ".", "_embeddings", "[", "name", "]", "=", "vector", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to": [[868, 877], ["data.Image._embeddings.items", "str", "str", "vector.to().pin_memory", "vector.to", "vector.to"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "to", "(", "self", ",", "device", ":", "str", ",", "pin_memory", ":", "bool", "=", "False", ")", ":", "\n", "        ", "for", "name", ",", "vector", "in", "self", ".", "_embeddings", ".", "items", "(", ")", ":", "\n", "            ", "if", "str", "(", "vector", ".", "device", ")", "!=", "str", "(", "device", ")", ":", "\n", "                ", "if", "pin_memory", ":", "\n", "                    ", "self", ".", "_embeddings", "[", "name", "]", "=", "vector", ".", "to", "(", "\n", "device", ",", "non_blocking", "=", "True", "\n", ")", ".", "pin_memory", "(", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_embeddings", "[", "name", "]", "=", "vector", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.clear_embeddings": [[878, 885], ["data.Image._embeddings.keys"], "methods", ["None"], ["", "", "", "", "def", "clear_embeddings", "(", "self", ",", "embedding_names", ":", "List", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "if", "embedding_names", "is", "None", ":", "\n", "            ", "self", ".", "_embeddings", ":", "Dict", "=", "{", "}", "\n", "", "else", ":", "\n", "            ", "for", "name", "in", "embedding_names", ":", "\n", "                ", "if", "name", "in", "self", ".", "_embeddings", ".", "keys", "(", ")", ":", "\n", "                    ", "del", "self", ".", "_embeddings", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.FlairDataset.is_in_memory": [[888, 891], ["None"], "methods", ["None"], ["    ", "@", "abstractmethod", "\n", "def", "is_in_memory", "(", "self", ")", "->", "bool", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.__init__": [[894, 924], ["len", "round", "torch.utils.data.random_split", "len", "round", "torch.utils.data.random_split"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "train", ":", "FlairDataset", ",", "\n", "dev", ":", "FlairDataset", "=", "None", ",", "\n", "test", ":", "FlairDataset", "=", "None", ",", "\n", "name", ":", "str", "=", "\"corpus\"", ",", "\n", ")", ":", "\n", "# set name", "\n", "        ", "self", ".", "name", ":", "str", "=", "name", "\n", "\n", "# sample test data if none is provided", "\n", "if", "test", "is", "None", ":", "\n", "            ", "train_length", "=", "len", "(", "train", ")", "\n", "test_size", ":", "int", "=", "round", "(", "train_length", "/", "10", ")", "\n", "splits", "=", "random_split", "(", "train", ",", "[", "train_length", "-", "test_size", ",", "test_size", "]", ")", "\n", "train", "=", "splits", "[", "0", "]", "\n", "test", "=", "splits", "[", "1", "]", "\n", "\n", "# sample dev data if none is provided", "\n", "", "if", "dev", "is", "None", ":", "\n", "            ", "train_length", "=", "len", "(", "train", ")", "\n", "dev_size", ":", "int", "=", "round", "(", "train_length", "/", "10", ")", "\n", "splits", "=", "random_split", "(", "train", ",", "[", "train_length", "-", "dev_size", ",", "dev_size", "]", ")", "\n", "train", "=", "splits", "[", "0", "]", "\n", "dev", "=", "splits", "[", "1", "]", "\n", "\n", "# set train dev and test data", "\n", "", "self", ".", "_train", ":", "FlairDataset", "=", "train", "\n", "self", ".", "_test", ":", "FlairDataset", "=", "test", "\n", "self", ".", "_dev", ":", "FlairDataset", "=", "dev", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.train": [[925, 928], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "train", "(", "self", ")", "->", "FlairDataset", ":", "\n", "        ", "return", "self", ".", "_train", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.dev": [[929, 932], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "dev", "(", "self", ")", "->", "FlairDataset", ":", "\n", "        ", "return", "self", ".", "_dev", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.test": [[933, 936], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "test", "(", "self", ")", "->", "FlairDataset", ":", "\n", "        ", "return", "self", ".", "_test", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.downsample": [[937, 945], ["data.Corpus._downsample_to_proportion", "data.Corpus._downsample_to_proportion", "data.Corpus._downsample_to_proportion"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._downsample_to_proportion", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._downsample_to_proportion", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._downsample_to_proportion"], ["", "def", "downsample", "(", "self", ",", "percentage", ":", "float", "=", "0.1", ",", "only_downsample_train", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "_train", "=", "self", ".", "_downsample_to_proportion", "(", "self", ".", "train", ",", "percentage", ")", "\n", "if", "not", "only_downsample_train", ":", "\n", "            ", "self", ".", "_dev", "=", "self", ".", "_downsample_to_proportion", "(", "self", ".", "dev", ",", "percentage", ")", "\n", "self", ".", "_test", "=", "self", ".", "_downsample_to_proportion", "(", "self", ".", "test", ",", "percentage", ")", "\n", "\n", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.filter_empty_sentences": [[946, 952], ["log.info", "data.Corpus._filter_empty_sentences", "data.Corpus._filter_empty_sentences", "data.Corpus._filter_empty_sentences", "log.info"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._filter_empty_sentences", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._filter_empty_sentences", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._filter_empty_sentences"], ["", "def", "filter_empty_sentences", "(", "self", ")", ":", "\n", "        ", "log", ".", "info", "(", "\"Filtering empty sentences\"", ")", "\n", "self", ".", "_train", "=", "Corpus", ".", "_filter_empty_sentences", "(", "self", ".", "_train", ")", "\n", "self", ".", "_test", "=", "Corpus", ".", "_filter_empty_sentences", "(", "self", ".", "_test", ")", "\n", "self", ".", "_dev", "=", "Corpus", ".", "_filter_empty_sentences", "(", "self", ".", "_dev", ")", "\n", "log", ".", "info", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._filter_empty_sentences": [[953, 975], ["DataLoader", "torch.utils.data.dataset.Subset", "len", "empty_sentence_indices.append", "non_empty_sentence_indices.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_filter_empty_sentences", "(", "dataset", ")", "->", "Dataset", ":", "\n", "\n", "# find out empty sentence indices", "\n", "        ", "empty_sentence_indices", "=", "[", "]", "\n", "non_empty_sentence_indices", "=", "[", "]", "\n", "index", "=", "0", "\n", "\n", "from", "flair", ".", "datasets", "import", "DataLoader", "\n", "\n", "for", "batch", "in", "DataLoader", "(", "dataset", ")", ":", "\n", "            ", "for", "sentence", "in", "batch", ":", "\n", "                ", "if", "len", "(", "sentence", ")", "==", "0", ":", "\n", "                    ", "empty_sentence_indices", ".", "append", "(", "index", ")", "\n", "", "else", ":", "\n", "                    ", "non_empty_sentence_indices", ".", "append", "(", "index", ")", "\n", "", "index", "+=", "1", "\n", "\n", "# create subset of non-empty sentence indices", "\n", "", "", "subset", "=", "Subset", "(", "dataset", ",", "non_empty_sentence_indices", ")", "\n", "\n", "return", "subset", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.make_vocab_dictionary": [[976, 994], ["data.Corpus._get_most_common_tokens", "data.Dictionary", "vocab_dictionary.add_item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._get_most_common_tokens", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.add_item"], ["", "def", "make_vocab_dictionary", "(", "self", ",", "max_tokens", "=", "-", "1", ",", "min_freq", "=", "1", ")", "->", "Dictionary", ":", "\n", "        ", "\"\"\"\n        Creates a dictionary of all tokens contained in the corpus.\n        By defining `max_tokens` you can set the maximum number of tokens that should be contained in the dictionary.\n        If there are more than `max_tokens` tokens in the corpus, the most frequent tokens are added first.\n        If `min_freq` is set the a value greater than 1 only tokens occurring more than `min_freq` times are considered\n        to be added to the dictionary.\n        :param max_tokens: the maximum number of tokens that should be added to the dictionary (-1 = take all tokens)\n        :param min_freq: a token needs to occur at least `min_freq` times to be added to the dictionary (-1 = there is no limitation)\n        :return: dictionary of tokens\n        \"\"\"", "\n", "tokens", "=", "self", ".", "_get_most_common_tokens", "(", "max_tokens", ",", "min_freq", ")", "\n", "\n", "vocab_dictionary", ":", "Dictionary", "=", "Dictionary", "(", ")", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "vocab_dictionary", ".", "add_item", "(", "token", ")", "\n", "\n", "", "return", "vocab_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._get_most_common_tokens": [[995, 1007], ["collections.Counter", "tokens_and_frequencies.most_common.most_common.most_common", "data.Corpus._get_all_tokens", "tokens.append", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._get_all_tokens"], ["", "def", "_get_most_common_tokens", "(", "self", ",", "max_tokens", ",", "min_freq", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "tokens_and_frequencies", "=", "Counter", "(", "self", ".", "_get_all_tokens", "(", ")", ")", "\n", "tokens_and_frequencies", "=", "tokens_and_frequencies", ".", "most_common", "(", ")", "\n", "\n", "tokens", "=", "[", "]", "\n", "for", "token", ",", "freq", "in", "tokens_and_frequencies", ":", "\n", "            ", "if", "(", "min_freq", "!=", "-", "1", "and", "freq", "<", "min_freq", ")", "or", "(", "\n", "max_tokens", "!=", "-", "1", "and", "len", "(", "tokens", ")", "==", "max_tokens", "\n", ")", ":", "\n", "                ", "break", "\n", "", "tokens", ".", "append", "(", "token", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._get_all_tokens": [[1008, 1012], ["list", "list", "map", "map"], "methods", ["None"], ["", "def", "_get_all_tokens", "(", "self", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "tokens", "=", "list", "(", "map", "(", "(", "lambda", "s", ":", "s", ".", "tokens", ")", ",", "self", ".", "train", ")", ")", "\n", "tokens", "=", "[", "token", "for", "sublist", "in", "tokens", "for", "token", "in", "sublist", "]", "\n", "return", "list", "(", "map", "(", "(", "lambda", "t", ":", "t", ".", "text", ")", ",", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._downsample_to_proportion": [[1013, 1019], ["round", "torch.utils.data.random_split", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_downsample_to_proportion", "(", "dataset", ":", "Dataset", ",", "proportion", ":", "float", ")", ":", "\n", "\n", "        ", "sampled_size", ":", "int", "=", "round", "(", "len", "(", "dataset", ")", "*", "proportion", ")", "\n", "splits", "=", "random_split", "(", "dataset", ",", "[", "len", "(", "dataset", ")", "-", "sampled_size", ",", "sampled_size", "]", ")", "\n", "return", "splits", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.obtain_statistics": [[1020, 1037], ["data.Corpus._obtain_statistics_for", "data.Corpus._obtain_statistics_for", "data.Corpus._obtain_statistics_for", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._obtain_statistics_for", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._obtain_statistics_for", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._obtain_statistics_for"], ["", "def", "obtain_statistics", "(", "\n", "self", ",", "label_type", ":", "str", "=", "None", ",", "pretty_print", ":", "bool", "=", "True", "\n", ")", "->", "dict", ":", "\n", "        ", "\"\"\"\n        Print statistics about the class distribution (only labels of sentences are taken into account) and sentence\n        sizes.\n        \"\"\"", "\n", "json_string", "=", "{", "\n", "\"TRAIN\"", ":", "self", ".", "_obtain_statistics_for", "(", "self", ".", "train", ",", "\"TRAIN\"", ",", "label_type", ")", ",", "\n", "\"TEST\"", ":", "self", ".", "_obtain_statistics_for", "(", "self", ".", "test", ",", "\"TEST\"", ",", "label_type", ")", ",", "\n", "\"DEV\"", ":", "self", ".", "_obtain_statistics_for", "(", "self", ".", "dev", ",", "\"DEV\"", ",", "label_type", ")", ",", "\n", "}", "\n", "if", "pretty_print", ":", "\n", "            ", "import", "json", "\n", "\n", "json_string", "=", "json", ".", "dumps", "(", "json_string", ",", "indent", "=", "4", ")", "\n", "", "return", "json_string", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._obtain_statistics_for": [[1038, 1065], ["data.Corpus._count_sentence_labels", "data.Corpus._count_token_labels", "data.Corpus._get_tokens_per_sentence", "data.Corpus._count_sentence_labels", "data.Corpus._count_token_labels", "len", "len", "sum", "min", "max", "sum", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._count_sentence_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._count_token_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._get_tokens_per_sentence", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._count_sentence_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._count_token_labels"], ["", "@", "staticmethod", "\n", "def", "_obtain_statistics_for", "(", "sentences", ",", "name", ",", "tag_type", ")", "->", "dict", ":", "\n", "        ", "if", "len", "(", "sentences", ")", "==", "0", ":", "\n", "            ", "return", "{", "}", "\n", "\n", "", "classes_to_count", "=", "Corpus", ".", "_count_sentence_labels", "(", "sentences", ")", "\n", "tags_to_count", "=", "Corpus", ".", "_count_token_labels", "(", "sentences", ",", "tag_type", ")", "\n", "tokens_per_sentence", "=", "Corpus", ".", "_get_tokens_per_sentence", "(", "sentences", ")", "\n", "\n", "label_size_dict", "=", "{", "}", "\n", "for", "l", ",", "c", "in", "classes_to_count", ".", "items", "(", ")", ":", "\n", "            ", "label_size_dict", "[", "l", "]", "=", "c", "\n", "\n", "", "tag_size_dict", "=", "{", "}", "\n", "for", "l", ",", "c", "in", "tags_to_count", ".", "items", "(", ")", ":", "\n", "            ", "tag_size_dict", "[", "l", "]", "=", "c", "\n", "\n", "", "return", "{", "\n", "\"dataset\"", ":", "name", ",", "\n", "\"total_number_of_documents\"", ":", "len", "(", "sentences", ")", ",", "\n", "\"number_of_documents_per_class\"", ":", "label_size_dict", ",", "\n", "\"number_of_tokens_per_tag\"", ":", "tag_size_dict", ",", "\n", "\"number_of_tokens\"", ":", "{", "\n", "\"total\"", ":", "sum", "(", "tokens_per_sentence", ")", ",", "\n", "\"min\"", ":", "min", "(", "tokens_per_sentence", ")", ",", "\n", "\"max\"", ":", "max", "(", "tokens_per_sentence", ")", ",", "\n", "\"avg\"", ":", "sum", "(", "tokens_per_sentence", ")", "/", "len", "(", "sentences", ")", ",", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._get_tokens_per_sentence": [[1068, 1071], ["list", "map", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_tokens_per_sentence", "(", "sentences", ")", ":", "\n", "        ", "return", "list", "(", "map", "(", "lambda", "x", ":", "len", "(", "x", ".", "tokens", ")", ",", "sentences", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._count_sentence_labels": [[1072, 1079], ["collections.defaultdict"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_count_sentence_labels", "(", "sentences", ")", ":", "\n", "        ", "label_count", "=", "defaultdict", "(", "lambda", ":", "0", ")", "\n", "for", "sent", "in", "sentences", ":", "\n", "            ", "for", "label", "in", "sent", ".", "labels", ":", "\n", "                ", "label_count", "[", "label", ".", "value", "]", "+=", "1", "\n", "", "", "return", "label_count", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus._count_token_labels": [[1080, 1089], ["collections.defaultdict", "token.annotation_layers.keys", "token.get_tag"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag"], ["", "@", "staticmethod", "\n", "def", "_count_token_labels", "(", "sentences", ",", "label_type", ")", ":", "\n", "        ", "label_count", "=", "defaultdict", "(", "lambda", ":", "0", ")", "\n", "for", "sent", "in", "sentences", ":", "\n", "            ", "for", "token", "in", "sent", ".", "tokens", ":", "\n", "                ", "if", "label_type", "in", "token", ".", "annotation_layers", ".", "keys", "(", ")", ":", "\n", "                    ", "label", "=", "token", ".", "get_tag", "(", "label_type", ")", "\n", "label_count", "[", "label", ".", "value", "]", "+=", "1", "\n", "", "", "", "return", "label_count", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.__str__": [[1090, 1095], ["len", "len", "len"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "\"Corpus: %d train + %d dev + %d test sentences\"", "%", "(", "\n", "len", "(", "self", ".", "train", ")", ",", "\n", "len", "(", "self", ".", "dev", ")", ",", "\n", "len", "(", "self", ".", "test", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.make_label_dictionary": [[1097, 1133], ["data.Dictionary", "DataLoader", "log.info", "flair.file_utils.Tqdm.tqdm", "log.info", "iter", "isinstance", "sentence.get_labels", "label_dictionary.add_item", "token.get_labels", "len", "label_dictionary.add_item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.get_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.add_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.get_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.add_item"], ["", "def", "make_label_dictionary", "(", "self", ",", "label_type", ":", "str", "=", "None", ")", "->", "Dictionary", ":", "\n", "        ", "\"\"\"\n        Creates a dictionary of all labels assigned to the sentences in the corpus.\n        :return: dictionary of labels\n        \"\"\"", "\n", "label_dictionary", ":", "Dictionary", "=", "Dictionary", "(", "add_unk", "=", "False", ")", "\n", "label_dictionary", ".", "multi_label", "=", "False", "\n", "\n", "from", "flair", ".", "datasets", "import", "DataLoader", "\n", "\n", "loader", "=", "DataLoader", "(", "self", ".", "train", ",", "batch_size", "=", "1", ")", "\n", "\n", "log", ".", "info", "(", "\"Computing label dictionary. Progress:\"", ")", "\n", "for", "batch", "in", "Tqdm", ".", "tqdm", "(", "iter", "(", "loader", ")", ")", ":", "\n", "\n", "            ", "for", "sentence", "in", "batch", ":", "\n", "\n", "# check if sentence itself has labels", "\n", "                ", "labels", "=", "sentence", ".", "get_labels", "(", "label_type", ")", "if", "label_type", "is", "not", "None", "else", "sentence", ".", "labels", "\n", "\n", "for", "label", "in", "labels", ":", "\n", "                    ", "label_dictionary", ".", "add_item", "(", "label", ".", "value", ")", "\n", "\n", "# check for labels of words", "\n", "", "if", "isinstance", "(", "sentence", ",", "Sentence", ")", ":", "\n", "                    ", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "                        ", "for", "label", "in", "token", ".", "get_labels", "(", "label_type", ")", ":", "\n", "                            ", "label_dictionary", ".", "add_item", "(", "label", ".", "value", ")", "\n", "\n", "", "", "", "if", "not", "label_dictionary", ".", "multi_label", ":", "\n", "                    ", "if", "len", "(", "labels", ")", ">", "1", ":", "\n", "                        ", "label_dictionary", ".", "multi_label", "=", "True", "\n", "\n", "", "", "", "", "log", ".", "info", "(", "label_dictionary", ".", "idx2item", ")", "\n", "\n", "return", "label_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.get_label_distribution": [[1134, 1140], ["collections.defaultdict"], "methods", ["None"], ["", "def", "get_label_distribution", "(", "self", ")", ":", "\n", "        ", "class_to_count", "=", "defaultdict", "(", "lambda", ":", "0", ")", "\n", "for", "sent", "in", "self", ".", "train", ":", "\n", "            ", "for", "label", "in", "sent", ".", "labels", ":", "\n", "                ", "class_to_count", "[", "label", ".", "value", "]", "+=", "1", "\n", "", "", "return", "class_to_count", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.get_all_sentences": [[1141, 1143], ["torch.utils.data.dataset.ConcatDataset"], "methods", ["None"], ["", "def", "get_all_sentences", "(", "self", ")", "->", "Dataset", ":", "\n", "        ", "return", "ConcatDataset", "(", "[", "self", ".", "train", ",", "self", ".", "dev", ",", "self", ".", "test", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.make_tag_dictionary": [[1144, 1155], ["data.Dictionary", "tag_dictionary.add_item", "data.Corpus.get_all_sentences", "tag_dictionary.add_item", "tag_dictionary.add_item", "tag_dictionary.add_item", "token.get_tag"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.add_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.get_all_sentences", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.add_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.add_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.add_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag"], ["", "def", "make_tag_dictionary", "(", "self", ",", "tag_type", ":", "str", ")", "->", "Dictionary", ":", "\n", "\n", "# Make the tag dictionary", "\n", "        ", "tag_dictionary", ":", "Dictionary", "=", "Dictionary", "(", ")", "\n", "tag_dictionary", ".", "add_item", "(", "\"O\"", ")", "\n", "for", "sentence", "in", "self", ".", "get_all_sentences", "(", ")", ":", "\n", "            ", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "                ", "tag_dictionary", ".", "add_item", "(", "token", ".", "get_tag", "(", "tag_type", ")", ".", "value", ")", "\n", "", "", "tag_dictionary", ".", "add_item", "(", "\"<START>\"", ")", "\n", "tag_dictionary", ".", "add_item", "(", "\"<STOP>\"", ")", "\n", "return", "tag_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.MultiCorpus.__init__": [[1158, 1166], ["data.Corpus.__init__", "torch.utils.data.dataset.ConcatDataset", "torch.utils.data.dataset.ConcatDataset", "torch.utils.data.dataset.ConcatDataset"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "corpora", ":", "List", "[", "Corpus", "]", ",", "name", ":", "str", "=", "\"multicorpus\"", ")", ":", "\n", "        ", "self", ".", "corpora", ":", "List", "[", "Corpus", "]", "=", "corpora", "\n", "\n", "super", "(", "MultiCorpus", ",", "self", ")", ".", "__init__", "(", "\n", "ConcatDataset", "(", "[", "corpus", ".", "train", "for", "corpus", "in", "self", ".", "corpora", "]", ")", ",", "\n", "ConcatDataset", "(", "[", "corpus", ".", "dev", "for", "corpus", "in", "self", ".", "corpora", "]", ")", ",", "\n", "ConcatDataset", "(", "[", "corpus", ".", "test", "for", "corpus", "in", "self", ".", "corpora", "]", ")", ",", "\n", "name", "=", "name", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.MultiCorpus.__str__": [[1168, 1170], ["str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "\"\\n\"", ".", "join", "(", "[", "str", "(", "corpus", ")", "for", "corpus", "in", "self", ".", "corpora", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.iob2": [[1172, 1192], ["enumerate", "tag.value.split", "len"], "function", ["None"], ["", "", "def", "iob2", "(", "tags", ")", ":", "\n", "    ", "\"\"\"\n    Check that tags have a valid IOB format.\n    Tags in IOB1 format are converted to IOB2.\n    \"\"\"", "\n", "for", "i", ",", "tag", "in", "enumerate", "(", "tags", ")", ":", "\n", "        ", "if", "tag", ".", "value", "==", "\"O\"", ":", "\n", "            ", "continue", "\n", "", "split", "=", "tag", ".", "value", ".", "split", "(", "\"-\"", ")", "\n", "if", "len", "(", "split", ")", "!=", "2", "or", "split", "[", "0", "]", "not", "in", "[", "\"I\"", ",", "\"B\"", "]", ":", "\n", "            ", "return", "False", "\n", "", "if", "split", "[", "0", "]", "==", "\"B\"", ":", "\n", "            ", "continue", "\n", "", "elif", "i", "==", "0", "or", "tags", "[", "i", "-", "1", "]", ".", "value", "==", "\"O\"", ":", "# conversion IOB1 to IOB2", "\n", "            ", "tags", "[", "i", "]", ".", "value", "=", "\"B\"", "+", "tag", ".", "value", "[", "1", ":", "]", "\n", "", "elif", "tags", "[", "i", "-", "1", "]", ".", "value", "[", "1", ":", "]", "==", "tag", ".", "value", "[", "1", ":", "]", ":", "\n", "            ", "continue", "\n", "", "else", ":", "# conversion IOB1 to IOB2", "\n", "            ", "tags", "[", "i", "]", ".", "value", "=", "\"B\"", "+", "tag", ".", "value", "[", "1", ":", "]", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.iob_iobes": [[1194, 1215], ["enumerate", "new_tags.append", "tag.value.split", "new_tags.append", "new_tags.append", "Exception", "len", "tag.value.replace", "tag.value.split", "new_tags.append", "new_tags.append", "tags[].value.split", "len", "tag.value.replace", "tags[].value.split"], "function", ["None"], ["", "def", "iob_iobes", "(", "tags", ")", ":", "\n", "    ", "\"\"\"\n    IOB -> IOBES\n    \"\"\"", "\n", "new_tags", "=", "[", "]", "\n", "for", "i", ",", "tag", "in", "enumerate", "(", "tags", ")", ":", "\n", "        ", "if", "tag", ".", "value", "==", "\"O\"", ":", "\n", "            ", "new_tags", ".", "append", "(", "tag", ".", "value", ")", "\n", "", "elif", "tag", ".", "value", ".", "split", "(", "\"-\"", ")", "[", "0", "]", "==", "\"B\"", ":", "\n", "            ", "if", "i", "+", "1", "!=", "len", "(", "tags", ")", "and", "tags", "[", "i", "+", "1", "]", ".", "value", ".", "split", "(", "\"-\"", ")", "[", "0", "]", "==", "\"I\"", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ".", "value", ")", "\n", "", "else", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ".", "value", ".", "replace", "(", "\"B-\"", ",", "\"S-\"", ")", ")", "\n", "", "", "elif", "tag", ".", "value", ".", "split", "(", "\"-\"", ")", "[", "0", "]", "==", "\"I\"", ":", "\n", "            ", "if", "i", "+", "1", "<", "len", "(", "tags", ")", "and", "tags", "[", "i", "+", "1", "]", ".", "value", ".", "split", "(", "\"-\"", ")", "[", "0", "]", "==", "\"I\"", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ".", "value", ")", "\n", "", "else", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ".", "value", ".", "replace", "(", "\"I-\"", ",", "\"E-\"", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Invalid IOB format!\"", ")", "\n", "", "", "return", "new_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.space_tokenizer": [[1217, 1245], ["enumerate", "len", "tokens.append", "len", "data.Token", "len", "tokens.append", "len", "data.Token"], "function", ["None"], ["", "def", "space_tokenizer", "(", "text", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "    ", "\"\"\"\n    Tokenizer based on space character only.\n    \"\"\"", "\n", "tokens", ":", "List", "[", "Token", "]", "=", "[", "]", "\n", "word", "=", "\"\"", "\n", "index", "=", "-", "1", "\n", "for", "index", ",", "char", "in", "enumerate", "(", "text", ")", ":", "\n", "        ", "if", "char", "==", "\" \"", ":", "\n", "            ", "if", "len", "(", "word", ")", ">", "0", ":", "\n", "                ", "start_position", "=", "index", "-", "len", "(", "word", ")", "\n", "tokens", ".", "append", "(", "\n", "Token", "(", "\n", "text", "=", "word", ",", "start_position", "=", "start_position", ",", "whitespace_after", "=", "True", "\n", ")", "\n", ")", "\n", "\n", "", "word", "=", "\"\"", "\n", "", "else", ":", "\n", "            ", "word", "+=", "char", "\n", "# increment for last token in sentence if not followed by whitespace", "\n", "", "", "index", "+=", "1", "\n", "if", "len", "(", "word", ")", ">", "0", ":", "\n", "        ", "start_position", "=", "index", "-", "len", "(", "word", ")", "\n", "tokens", ".", "append", "(", "\n", "Token", "(", "text", "=", "word", ",", "start_position", "=", "start_position", ",", "whitespace_after", "=", "False", ")", "\n", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.build_japanese_tokenizer": [[1247, 1312], ["konoha.SentenceTokenizer", "konoha.WordTokenizer", "tokenizer.lower", "NotImplementedError", "konoha.SentenceTokenizer.tokenize", "log.warning", "log.warning", "log.warning", "log.warning", "log.warning", "log.warning", "konoha.WordTokenizer.tokenize", "words.extend", "data.Token", "tokens.append", "list", "index", "len", "map"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize"], ["", "def", "build_japanese_tokenizer", "(", "tokenizer", ":", "str", "=", "\"MeCab\"", ")", ":", "\n", "    ", "if", "tokenizer", ".", "lower", "(", ")", "!=", "\"mecab\"", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Currently, MeCab is only supported.\"", ")", "\n", "\n", "", "try", ":", "\n", "        ", "import", "konoha", "\n", "", "except", "ModuleNotFoundError", ":", "\n", "        ", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "log", ".", "warning", "(", "'ATTENTION! The library \"konoha\" is not installed!'", ")", "\n", "log", ".", "warning", "(", "\n", "'To use Japanese tokenizer, please first install with the following steps:'", "\n", ")", "\n", "log", ".", "warning", "(", "\n", "'- Install mecab with \"sudo apt install mecab libmecab-dev mecab-ipadic\"'", "\n", ")", "\n", "log", ".", "warning", "(", "'- Install konoha with \"pip install konoha[mecab]\"'", ")", "\n", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "pass", "\n", "\n", "", "sentence_tokenizer", "=", "konoha", ".", "SentenceTokenizer", "(", ")", "\n", "word_tokenizer", "=", "konoha", ".", "WordTokenizer", "(", "tokenizer", ")", "\n", "\n", "def", "tokenizer", "(", "text", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "        ", "\"\"\"\n        Tokenizer using konoha, a third party library which supports\n        multiple Japanese tokenizer such as MeCab, KyTea and SudachiPy.\n        \"\"\"", "\n", "tokens", ":", "List", "[", "Token", "]", "=", "[", "]", "\n", "words", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "sentences", "=", "sentence_tokenizer", ".", "tokenize", "(", "text", ")", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "konoha_tokens", "=", "word_tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "words", ".", "extend", "(", "list", "(", "map", "(", "str", ",", "konoha_tokens", ")", ")", ")", "\n", "\n", "# determine offsets for whitespace_after field", "\n", "", "index", "=", "text", ".", "index", "\n", "current_offset", "=", "0", "\n", "previous_word_offset", "=", "-", "1", "\n", "previous_token", "=", "None", "\n", "for", "word", "in", "words", ":", "\n", "            ", "try", ":", "\n", "                ", "word_offset", "=", "index", "(", "word", ",", "current_offset", ")", "\n", "start_position", "=", "word_offset", "\n", "", "except", ":", "\n", "                ", "word_offset", "=", "previous_word_offset", "+", "1", "\n", "start_position", "=", "(", "\n", "current_offset", "+", "1", "if", "current_offset", ">", "0", "else", "current_offset", "\n", ")", "\n", "\n", "", "token", "=", "Token", "(", "\n", "text", "=", "word", ",", "start_position", "=", "start_position", ",", "whitespace_after", "=", "True", "\n", ")", "\n", "tokens", ".", "append", "(", "token", ")", "\n", "\n", "if", "(", "previous_token", "is", "not", "None", ")", "and", "word_offset", "-", "1", "==", "previous_word_offset", ":", "\n", "                ", "previous_token", ".", "whitespace_after", "=", "False", "\n", "\n", "", "current_offset", "=", "word_offset", "+", "len", "(", "word", ")", "\n", "previous_word_offset", "=", "current_offset", "-", "1", "\n", "previous_token", "=", "token", "\n", "\n", "", "return", "tokens", "\n", "\n", "", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.segtok_tokenizer": [[1314, 1356], ["segtok.segmenter.split_single", "segtok.tokenizer.split_contractions", "words.extend", "segtok.tokenizer.word_tokenizer", "index", "data.Token", "tokens.append", "len"], "function", ["None"], ["", "def", "segtok_tokenizer", "(", "text", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "    ", "\"\"\"\n    Tokenizer using segtok, a third party library dedicated to rules-based Indo-European languages.\n    https://github.com/fnl/segtok\n    \"\"\"", "\n", "tokens", ":", "List", "[", "Token", "]", "=", "[", "]", "\n", "\n", "words", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "sentences", "=", "split_single", "(", "text", ")", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "contractions", "=", "split_contractions", "(", "word_tokenizer", "(", "sentence", ")", ")", "\n", "words", ".", "extend", "(", "contractions", ")", "\n", "\n", "# determine offsets for whitespace_after field", "\n", "", "index", "=", "text", ".", "index", "\n", "current_offset", "=", "0", "\n", "previous_word_offset", "=", "-", "1", "\n", "previous_token", "=", "None", "\n", "for", "word", "in", "words", ":", "\n", "        ", "try", ":", "\n", "            ", "word_offset", "=", "index", "(", "word", ",", "current_offset", ")", "\n", "start_position", "=", "word_offset", "\n", "", "except", ":", "\n", "            ", "word_offset", "=", "previous_word_offset", "+", "1", "\n", "start_position", "=", "(", "\n", "current_offset", "+", "1", "if", "current_offset", ">", "0", "else", "current_offset", "\n", ")", "\n", "\n", "", "if", "word", ":", "\n", "            ", "token", "=", "Token", "(", "\n", "text", "=", "word", ",", "start_position", "=", "start_position", ",", "whitespace_after", "=", "True", "\n", ")", "\n", "tokens", ".", "append", "(", "token", ")", "\n", "\n", "", "if", "(", "previous_token", "is", "not", "None", ")", "and", "word_offset", "-", "1", "==", "previous_word_offset", ":", "\n", "            ", "previous_token", ".", "whitespace_after", "=", "False", "\n", "\n", "", "current_offset", "=", "word_offset", "+", "len", "(", "word", ")", "\n", "previous_word_offset", "=", "current_offset", "-", "1", "\n", "previous_token", "=", "token", "\n", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.build_spacy_tokenizer": [[1358, 1396], ["model.make_doc", "ImportError", "data.Token", "tokens.append", "len"], "function", ["None"], ["", "def", "build_spacy_tokenizer", "(", "model", ")", "->", "Callable", "[", "[", "str", "]", ",", "List", "[", "Token", "]", "]", ":", "\n", "    ", "\"\"\"\n    Wrap Spacy rpbert to build a tokenizer for the Sentence class.\n    :param model a Spacy V2 rpbert\n    :return a tokenizer function to provide to Sentence class constructor\n    \"\"\"", "\n", "try", ":", "\n", "        ", "from", "spacy", ".", "language", "import", "Language", "\n", "from", "spacy", ".", "tokens", ".", "doc", "import", "Doc", "\n", "from", "spacy", ".", "tokens", ".", "token", "import", "Token", "as", "SpacyToken", "\n", "", "except", "ImportError", ":", "\n", "        ", "raise", "ImportError", "(", "\n", "\"Please install Spacy v2.0 or better before using the Spacy tokenizer, otherwise you can use segtok_tokenizer as advanced tokenizer.\"", "\n", ")", "\n", "\n", "", "model", ":", "Language", "=", "model", "\n", "\n", "def", "tokenizer", "(", "text", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "        ", "doc", ":", "Doc", "=", "model", ".", "make_doc", "(", "text", ")", "\n", "previous_token", "=", "None", "\n", "tokens", ":", "List", "[", "Token", "]", "=", "[", "]", "\n", "for", "word", "in", "doc", ":", "\n", "            ", "word", ":", "SpacyToken", "=", "word", "\n", "token", "=", "Token", "(", "\n", "text", "=", "word", ".", "text", ",", "start_position", "=", "word", ".", "idx", ",", "whitespace_after", "=", "True", "\n", ")", "\n", "tokens", ".", "append", "(", "token", ")", "\n", "\n", "if", "(", "previous_token", "is", "not", "None", ")", "and", "(", "\n", "token", ".", "start_pos", "-", "1", "\n", "==", "previous_token", ".", "start_pos", "+", "len", "(", "previous_token", ".", "text", ")", "\n", ")", ":", "\n", "                ", "previous_token", ".", "whitespace_after", "=", "False", "\n", "\n", "", "previous_token", "=", "token", "\n", "", "return", "tokens", "\n", "\n", "", "return", "tokenizer", "\n", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.Embeddings.embedding_length": [[64, 69], ["None"], "methods", ["None"], ["@", "property", "\n", "@", "abstractmethod", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Returns the length of the embedding vector.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.Embeddings.embedding_type": [[70, 74], ["None"], "methods", ["None"], ["", "@", "property", "\n", "@", "abstractmethod", "\n", "def", "embedding_type", "(", "self", ")", "->", "str", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.Embeddings.embed": [[75, 99], ["embeddings.Embeddings._add_embeddings_internal", "type", "type", "sentence._embeddings.keys", "token._embeddings.keys"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ConvTransformNetworkImageEmbeddings._add_embeddings_internal"], ["", "def", "embed", "(", "self", ",", "sentences", ":", "Union", "[", "Sentence", ",", "List", "[", "Sentence", "]", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "\"\"\"Add embeddings to all words in a list of sentences. If embeddings are already added, updates only if embeddings\n        are non-static.\"\"\"", "\n", "\n", "# if only one sentence is passed, convert to list of sentence", "\n", "if", "(", "type", "(", "sentences", ")", "is", "Sentence", ")", "or", "(", "type", "(", "sentences", ")", "is", "Image", ")", ":", "\n", "            ", "sentences", "=", "[", "sentences", "]", "\n", "\n", "", "everything_embedded", ":", "bool", "=", "True", "\n", "\n", "if", "self", ".", "embedding_type", "==", "\"word-level\"", ":", "\n", "            ", "for", "sentence", "in", "sentences", ":", "\n", "                ", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "                    ", "if", "self", ".", "name", "not", "in", "token", ".", "_embeddings", ".", "keys", "(", ")", ":", "\n", "                        ", "everything_embedded", "=", "False", "\n", "", "", "", "", "else", ":", "\n", "            ", "for", "sentence", "in", "sentences", ":", "\n", "                ", "if", "self", ".", "name", "not", "in", "sentence", ".", "_embeddings", ".", "keys", "(", ")", ":", "\n", "                    ", "everything_embedded", "=", "False", "\n", "\n", "", "", "", "if", "not", "everything_embedded", "or", "not", "self", ".", "static_embeddings", ":", "\n", "            ", "self", ".", "_add_embeddings_internal", "(", "sentences", ")", "\n", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.Embeddings._add_embeddings_internal": [[100, 104], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "\"\"\"Private method for adding embeddings to all words in a list of sentences.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TokenEmbeddings.embedding_length": [[109, 114], ["None"], "methods", ["None"], ["@", "property", "\n", "@", "abstractmethod", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Returns the length of the embedding vector.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TokenEmbeddings.embedding_type": [[115, 118], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_type", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "\"word-level\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentEmbeddings.embedding_length": [[123, 128], ["None"], "methods", ["None"], ["@", "property", "\n", "@", "abstractmethod", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Returns the length of the embedding vector.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentEmbeddings.embedding_type": [[129, 132], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_type", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "\"sentence-level\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ImageEmbeddings.embedding_length": [[135, 140], ["None"], "methods", ["None"], ["    ", "@", "property", "\n", "@", "abstractmethod", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Returns the length of the embedding vector.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ImageEmbeddings.embedding_type": [[141, 144], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_type", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "\"image-level\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.StackedEmbeddings.__init__": [[149, 168], ["super().__init__", "enumerate", "embeddings.StackedEmbeddings.add_module", "str", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "embeddings", ":", "List", "[", "TokenEmbeddings", "]", ")", ":", "\n", "        ", "\"\"\"The constructor takes a list of embeddings to be combined.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "\n", "# IMPORTANT: add embeddings as torch modules", "\n", "for", "i", ",", "embedding", "in", "enumerate", "(", "embeddings", ")", ":", "\n", "            ", "embedding", ".", "name", "=", "f\"{str(i)}-{embedding.name}\"", "\n", "self", ".", "add_module", "(", "f\"list_embedding_{str(i)}\"", ",", "embedding", ")", "\n", "\n", "", "self", ".", "name", ":", "str", "=", "\"Stack\"", "\n", "self", ".", "static_embeddings", ":", "bool", "=", "True", "\n", "\n", "self", ".", "__embedding_type", ":", "str", "=", "embeddings", "[", "0", "]", ".", "embedding_type", "\n", "\n", "self", ".", "__embedding_length", ":", "int", "=", "0", "\n", "for", "embedding", "in", "embeddings", ":", "\n", "            ", "self", ".", "__embedding_length", "+=", "embedding", ".", "embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.StackedEmbeddings.embed": [[169, 178], ["type", "embedding.embed"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed"], ["", "", "def", "embed", "(", "\n", "self", ",", "sentences", ":", "Union", "[", "Sentence", ",", "List", "[", "Sentence", "]", "]", ",", "static_embeddings", ":", "bool", "=", "True", "\n", ")", ":", "\n", "# if only one sentence is passed, convert to list of sentence", "\n", "        ", "if", "type", "(", "sentences", ")", "is", "Sentence", ":", "\n", "            ", "sentences", "=", "[", "sentences", "]", "\n", "\n", "", "for", "embedding", "in", "self", ".", "embeddings", ":", "\n", "            ", "embedding", ".", "embed", "(", "sentences", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.StackedEmbeddings.embedding_type": [[179, 182], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "embedding_type", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "__embedding_type", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.StackedEmbeddings.embedding_length": [[183, 186], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.StackedEmbeddings._add_embeddings_internal": [[187, 193], ["embedding._add_embeddings_internal"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ConvTransformNetworkImageEmbeddings._add_embeddings_internal"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "\n", "        ", "for", "embedding", "in", "self", ".", "embeddings", ":", "\n", "            ", "embedding", ".", "_add_embeddings_internal", "(", "sentences", ")", "\n", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.StackedEmbeddings.__str__": [[194, 196], ["str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "f'StackedEmbeddings [{\",\".join([str(e) for e in self.embeddings])}]'", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.WordEmbeddings.__init__": [[201, 339], ["pathlib.Path", "str", "super().__init__", "file_utils.cached_path", "file_utils.cached_path", "str().endswith", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path", "gensim.models.KeyedVectors.load_word2vec_format", "gensim.models.KeyedVectors.load", "gensim.models.word2vec.Word2Vec.load", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path", "str", "str", "str", "str", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "len", "file_utils.cached_path", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.endswith", "file_utils.cached_path", "file_utils.cached_path", "len", "file_utils.cached_path.endswith", "file_utils.cached_path", "file_utils.cached_path", "file_utils.cached_path.lower", "len", "pathlib.Path().exists", "ValueError", "file_utils.cached_path.lower", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["def", "__init__", "(", "self", ",", "embeddings", ":", "str", ",", "field", ":", "str", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Initializes classic word embeddings. Constructor downloads required files if not there.\n        :param embeddings: one of: 'glove', 'extvec', 'crawl' or two-letter language code or custom\n        If you want to use a custom embedding file, just pass the path to the embeddings as embeddings variable.\n        \"\"\"", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "\n", "old_base_path", "=", "(", "\n", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/\"", "\n", ")", "\n", "base_path", "=", "(", "\n", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/\"", "\n", ")", "\n", "embeddings_path_v4", "=", "(", "\n", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/\"", "\n", ")", "\n", "embeddings_path_v4_1", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/\"", "\n", "\n", "cache_dir", "=", "Path", "(", "\"embeddings\"", ")", "\n", "\n", "# GLOVE embeddings", "\n", "if", "embeddings", ".", "lower", "(", ")", "==", "\"glove\"", "or", "embeddings", ".", "lower", "(", ")", "==", "\"en-glove\"", ":", "\n", "            ", "cached_path", "(", "f\"{old_base_path}glove.gensim.vectors.npy\"", ",", "cache_dir", "=", "cache_dir", ")", "\n", "embeddings", "=", "cached_path", "(", "\n", "f\"{old_base_path}glove.gensim\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "\n", "# TURIAN embeddings", "\n", "", "elif", "embeddings", ".", "lower", "(", ")", "==", "\"turian\"", "or", "embeddings", ".", "lower", "(", ")", "==", "\"en-turian\"", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{embeddings_path_v4_1}turian.vectors.npy\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "embeddings", "=", "cached_path", "(", "\n", "f\"{embeddings_path_v4_1}turian\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "\n", "# KOMNINOS embeddings", "\n", "", "elif", "embeddings", ".", "lower", "(", ")", "==", "\"extvec\"", "or", "embeddings", ".", "lower", "(", ")", "==", "\"en-extvec\"", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{old_base_path}extvec.gensim.vectors.npy\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "embeddings", "=", "cached_path", "(", "\n", "f\"{old_base_path}extvec.gensim\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "\n", "# FT-CRAWL embeddings", "\n", "", "elif", "embeddings", ".", "lower", "(", ")", "==", "\"crawl\"", "or", "embeddings", ".", "lower", "(", ")", "==", "\"en-crawl\"", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{base_path}en-fasttext-crawl-300d-1M.vectors.npy\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "embeddings", "=", "cached_path", "(", "\n", "f\"{base_path}en-fasttext-crawl-300d-1M\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "\n", "# FT-CRAWL embeddings", "\n", "", "elif", "(", "\n", "embeddings", ".", "lower", "(", ")", "==", "\"news\"", "\n", "or", "embeddings", ".", "lower", "(", ")", "==", "\"en-news\"", "\n", "or", "embeddings", ".", "lower", "(", ")", "==", "\"en\"", "\n", ")", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{base_path}en-fasttext-news-300d-1M.vectors.npy\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "embeddings", "=", "cached_path", "(", "\n", "f\"{base_path}en-fasttext-news-300d-1M\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "\n", "# twitter embeddings", "\n", "", "elif", "embeddings", ".", "lower", "(", ")", "==", "\"twitter\"", "or", "embeddings", ".", "lower", "(", ")", "==", "\"en-twitter\"", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{old_base_path}twitter.gensim.vectors.npy\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "embeddings", "=", "cached_path", "(", "\n", "f\"{old_base_path}twitter.gensim\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "\n", "# two-letter language code wiki embeddings", "\n", "", "elif", "len", "(", "embeddings", ".", "lower", "(", ")", ")", "==", "2", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{embeddings_path_v4}{embeddings}-wiki-fasttext-300d-1M.vectors.npy\"", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", ")", "\n", "embeddings", "=", "cached_path", "(", "\n", "f\"{embeddings_path_v4}{embeddings}-wiki-fasttext-300d-1M\"", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", ")", "\n", "\n", "# two-letter language code wiki embeddings", "\n", "", "elif", "len", "(", "embeddings", ".", "lower", "(", ")", ")", "==", "7", "and", "embeddings", ".", "endswith", "(", "\"-wiki\"", ")", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{embeddings_path_v4}{embeddings[:2]}-wiki-fasttext-300d-1M.vectors.npy\"", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", ")", "\n", "embeddings", "=", "cached_path", "(", "\n", "f\"{embeddings_path_v4}{embeddings[:2]}-wiki-fasttext-300d-1M\"", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", ")", "\n", "\n", "# two-letter language code crawl embeddings", "\n", "", "elif", "len", "(", "embeddings", ".", "lower", "(", ")", ")", "==", "8", "and", "embeddings", ".", "endswith", "(", "\"-crawl\"", ")", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{embeddings_path_v4}{embeddings[:2]}-crawl-fasttext-300d-1M.vectors.npy\"", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", ")", "\n", "embeddings", "=", "cached_path", "(", "\n", "f\"{embeddings_path_v4}{embeddings[:2]}-crawl-fasttext-300d-1M\"", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", ")", "\n", "\n", "", "elif", "not", "Path", "(", "embeddings", ")", ".", "exists", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f'The given embeddings \"{embeddings}\" is not available or is not a valid path.'", "\n", ")", "\n", "\n", "", "self", ".", "name", ":", "str", "=", "str", "(", "embeddings", ")", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n", "try", ":", "\n", "            ", "if", "str", "(", "embeddings", ")", ".", "endswith", "(", "\".bin\"", ")", ":", "\n", "                ", "self", ".", "precomputed_word_embeddings", "=", "gensim", ".", "models", ".", "KeyedVectors", ".", "load_word2vec_format", "(", "\n", "str", "(", "embeddings", ")", ",", "binary", "=", "True", "\n", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "precomputed_word_embeddings", "=", "gensim", ".", "models", ".", "KeyedVectors", ".", "load", "(", "\n", "str", "(", "embeddings", ")", "\n", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "precomputed_word_embeddings", "=", "gensim", ".", "models", ".", "word2vec", ".", "Word2Vec", ".", "load", "(", "\n", "str", "(", "embeddings", ")", "\n", ")", "\n", "\n", "", "self", ".", "field", "=", "field", "\n", "\n", "self", ".", "__embedding_length", ":", "int", "=", "self", ".", "precomputed_word_embeddings", ".", "vector_size", "\n", "# self.unk = np.random.uniform(-0.25, 0.25, self.embedding_length)", "\n", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.WordEmbeddings.embedding_length": [[340, 343], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.WordEmbeddings.get_cached_vec": [[344, 366], ["functools.lru_cache", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "word.lower", "re.sub", "word.lower", "word.lower", "re.sub", "numpy.zeros", "re.sub", "word.lower", "word.lower", "re.sub", "word.lower"], "methods", ["None"], ["", "@", "lru_cache", "(", "maxsize", "=", "10000", ",", "typed", "=", "False", ")", "\n", "def", "get_cached_vec", "(", "self", ",", "word", ":", "str", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "word", "in", "self", ".", "precomputed_word_embeddings", ":", "\n", "            ", "word_embedding", "=", "self", ".", "precomputed_word_embeddings", "[", "word", "]", "\n", "", "elif", "word", ".", "lower", "(", ")", "in", "self", ".", "precomputed_word_embeddings", ":", "\n", "            ", "word_embedding", "=", "self", ".", "precomputed_word_embeddings", "[", "word", ".", "lower", "(", ")", "]", "\n", "", "elif", "re", ".", "sub", "(", "r\"\\d\"", ",", "\"#\"", ",", "word", ".", "lower", "(", ")", ")", "in", "self", ".", "precomputed_word_embeddings", ":", "\n", "            ", "word_embedding", "=", "self", ".", "precomputed_word_embeddings", "[", "\n", "re", ".", "sub", "(", "r\"\\d\"", ",", "\"#\"", ",", "word", ".", "lower", "(", ")", ")", "\n", "]", "\n", "", "elif", "re", ".", "sub", "(", "r\"\\d\"", ",", "\"0\"", ",", "word", ".", "lower", "(", ")", ")", "in", "self", ".", "precomputed_word_embeddings", ":", "\n", "            ", "word_embedding", "=", "self", ".", "precomputed_word_embeddings", "[", "\n", "re", ".", "sub", "(", "r\"\\d\"", ",", "\"0\"", ",", "word", ".", "lower", "(", ")", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "word_embedding", "=", "np", ".", "zeros", "(", "self", ".", "embedding_length", ",", "dtype", "=", "\"float\"", ")", "\n", "# word_embedding = np.random.uniform(-0.25, 0.25, self.embedding_length)", "\n", "\n", "", "word_embedding", "=", "torch", ".", "tensor", "(", "\n", "word_embedding", ",", "device", "=", "flair", ".", "device", ",", "dtype", "=", "torch", ".", "float", "\n", ")", "\n", "return", "word_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.WordEmbeddings._add_embeddings_internal": [[367, 383], ["enumerate", "zip", "range", "embeddings.WordEmbeddings.get_cached_vec", "token.set_embedding", "len", "token.get_tag"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.MuseCrosslingualEmbeddings.get_cached_vec", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "\n", "        ", "for", "i", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "\n", "            ", "for", "token", ",", "token_idx", "in", "zip", "(", "sentence", ".", "tokens", ",", "range", "(", "len", "(", "sentence", ".", "tokens", ")", ")", ")", ":", "\n", "\n", "                ", "if", "\"field\"", "not", "in", "self", ".", "__dict__", "or", "self", ".", "field", "is", "None", ":", "\n", "                    ", "word", "=", "token", ".", "text", "\n", "", "else", ":", "\n", "                    ", "word", "=", "token", ".", "get_tag", "(", "self", ".", "field", ")", ".", "value", "\n", "\n", "", "word_embedding", "=", "self", ".", "get_cached_vec", "(", "word", "=", "word", ")", "\n", "\n", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "word_embedding", ")", "\n", "\n", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.WordEmbeddings.__str__": [[384, 386], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.WordEmbeddings.extra_repr": [[387, 393], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "# fix serialized models", "\n", "        ", "if", "\"embeddings\"", "not", "in", "self", ".", "__dict__", ":", "\n", "            ", "self", ".", "embeddings", "=", "self", ".", "name", "\n", "\n", "", "return", "f\"'{self.embeddings}'\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.FastTextEmbeddings.__init__": [[398, 431], ["pathlib.Path", "str", "gensim.models.FastText.load_fasttext_format", "super().__init__", "file_utils.cached_path", "str", "pathlib.Path().exists", "ValueError", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["def", "__init__", "(", "self", ",", "embeddings", ":", "str", ",", "use_local", ":", "bool", "=", "True", ",", "field", ":", "str", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Initializes fasttext word embeddings. Constructor downloads required embedding file and stores in cache\n        if use_local is False.\n\n        :param embeddings: path to your embeddings '.bin' file\n        :param use_local: set this to False if you are using embeddings from a remote source\n        \"\"\"", "\n", "\n", "cache_dir", "=", "Path", "(", "\"embeddings\"", ")", "\n", "\n", "if", "use_local", ":", "\n", "            ", "if", "not", "Path", "(", "embeddings", ")", ".", "exists", "(", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f'The given embeddings \"{embeddings}\" is not available or is not a valid path.'", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "embeddings", "=", "cached_path", "(", "f\"{embeddings}\"", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "", "self", ".", "embeddings", "=", "embeddings", "\n", "\n", "self", ".", "name", ":", "str", "=", "str", "(", "embeddings", ")", "\n", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n", "self", ".", "precomputed_word_embeddings", "=", "gensim", ".", "models", ".", "FastText", ".", "load_fasttext_format", "(", "\n", "str", "(", "embeddings", ")", "\n", ")", "\n", "\n", "self", ".", "__embedding_length", ":", "int", "=", "self", ".", "precomputed_word_embeddings", ".", "vector_size", "\n", "\n", "self", ".", "field", "=", "field", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.FastTextEmbeddings.embedding_length": [[432, 435], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.FastTextEmbeddings.get_cached_vec": [[436, 447], ["functools.lru_cache", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "numpy.zeros"], "methods", ["None"], ["", "@", "lru_cache", "(", "maxsize", "=", "10000", ",", "typed", "=", "False", ")", "\n", "def", "get_cached_vec", "(", "self", ",", "word", ":", "str", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "try", ":", "\n", "            ", "word_embedding", "=", "self", ".", "precomputed_word_embeddings", "[", "word", "]", "\n", "", "except", ":", "\n", "            ", "word_embedding", "=", "np", ".", "zeros", "(", "self", ".", "embedding_length", ",", "dtype", "=", "\"float\"", ")", "\n", "\n", "", "word_embedding", "=", "torch", ".", "tensor", "(", "\n", "word_embedding", ",", "device", "=", "flair", ".", "device", ",", "dtype", "=", "torch", ".", "float", "\n", ")", "\n", "return", "word_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.FastTextEmbeddings._add_embeddings_internal": [[448, 464], ["enumerate", "zip", "range", "embeddings.FastTextEmbeddings.get_cached_vec", "token.set_embedding", "len", "token.get_tag"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.MuseCrosslingualEmbeddings.get_cached_vec", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "\n", "        ", "for", "i", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "\n", "            ", "for", "token", ",", "token_idx", "in", "zip", "(", "sentence", ".", "tokens", ",", "range", "(", "len", "(", "sentence", ".", "tokens", ")", ")", ")", ":", "\n", "\n", "                ", "if", "\"field\"", "not", "in", "self", ".", "__dict__", "or", "self", ".", "field", "is", "None", ":", "\n", "                    ", "word", "=", "token", ".", "text", "\n", "", "else", ":", "\n", "                    ", "word", "=", "token", ".", "get_tag", "(", "self", ".", "field", ")", ".", "value", "\n", "\n", "", "word_embedding", "=", "self", ".", "get_cached_vec", "(", "word", ")", "\n", "\n", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "word_embedding", ")", "\n", "\n", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.FastTextEmbeddings.__str__": [[465, 467], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.FastTextEmbeddings.extra_repr": [[468, 470], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "f\"'{self.embeddings}'\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OneHotEmbeddings.__init__": [[475, 528], ["super().__init__", "list", "data.Dictionary", "print", "print", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "embeddings.OneHotEmbeddings.to", "map", "collections.Counter().most_common", "collections.Counter().most_common", "list.append", "embeddings.OneHotEmbeddings.vocab_dictionary.add_item", "len", "collections.Counter", "collections.Counter", "len", "list", "list", "map", "map", "t.get_tag"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.add_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag"], ["def", "__init__", "(", "\n", "self", ",", "\n", "corpus", ":", "Corpus", ",", "\n", "field", ":", "str", "=", "\"text\"", ",", "\n", "embedding_length", ":", "int", "=", "300", ",", "\n", "min_freq", ":", "int", "=", "3", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Initializes one-hot encoded word embeddings and a trainable embedding layer\n        :param corpus: you need to pass a Corpus in order to construct the vocabulary\n        :param field: by default, the 'text' of tokens is embedded, but you can also embed tags such as 'pos'\n        :param embedding_length: dimensionality of the trainable embedding layer\n        :param min_freq: minimum frequency of a word to become part of the vocabulary\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "name", "=", "\"one-hot\"", "\n", "self", ".", "static_embeddings", "=", "False", "\n", "self", ".", "min_freq", "=", "min_freq", "\n", "self", ".", "field", "=", "field", "\n", "\n", "tokens", "=", "list", "(", "map", "(", "(", "lambda", "s", ":", "s", ".", "tokens", ")", ",", "corpus", ".", "train", ")", ")", "\n", "tokens", "=", "[", "token", "for", "sublist", "in", "tokens", "for", "token", "in", "sublist", "]", "\n", "\n", "if", "field", "==", "\"text\"", ":", "\n", "            ", "most_common", "=", "Counter", "(", "list", "(", "map", "(", "(", "lambda", "t", ":", "t", ".", "text", ")", ",", "tokens", ")", ")", ")", ".", "most_common", "(", ")", "\n", "", "else", ":", "\n", "            ", "most_common", "=", "Counter", "(", "\n", "list", "(", "map", "(", "(", "lambda", "t", ":", "t", ".", "get_tag", "(", "field", ")", ".", "value", ")", ",", "tokens", ")", ")", "\n", ")", ".", "most_common", "(", ")", "\n", "\n", "", "tokens", "=", "[", "]", "\n", "for", "token", ",", "freq", "in", "most_common", ":", "\n", "            ", "if", "freq", "<", "min_freq", ":", "\n", "                ", "break", "\n", "", "tokens", ".", "append", "(", "token", ")", "\n", "\n", "", "self", ".", "vocab_dictionary", ":", "Dictionary", "=", "Dictionary", "(", ")", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "self", ".", "vocab_dictionary", ".", "add_item", "(", "token", ")", "\n", "\n", "# max_tokens = 500", "\n", "", "self", ".", "__embedding_length", "=", "embedding_length", "\n", "\n", "print", "(", "self", ".", "vocab_dictionary", ".", "idx2item", ")", "\n", "print", "(", "f\"vocabulary size of {len(self.vocab_dictionary)}\"", ")", "\n", "\n", "# rpbert architecture", "\n", "self", ".", "embedding_layer", "=", "torch", ".", "nn", ".", "Embedding", "(", "\n", "len", "(", "self", ".", "vocab_dictionary", ")", ",", "self", ".", "__embedding_length", "\n", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "embedding_layer", ".", "weight", ")", "\n", "\n", "self", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OneHotEmbeddings.embedding_length": [[529, 532], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OneHotEmbeddings._add_embeddings_internal": [[533, 565], ["enumerate", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "embeddings.OneHotEmbeddings.embedding_layer.forward", "torch.tensor().to.extend", "torch.tensor().to.extend", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "token.set_embedding", "embeddings.OneHotEmbeddings.vocab_dictionary.get_idx_for_item", "embeddings.OneHotEmbeddings.vocab_dictionary.get_idx_for_item", "t.get_tag"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "\n", "        ", "one_hot_sentences", "=", "[", "]", "\n", "for", "i", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "\n", "            ", "if", "self", ".", "field", "==", "\"text\"", ":", "\n", "                ", "context_idxs", "=", "[", "\n", "self", ".", "vocab_dictionary", ".", "get_idx_for_item", "(", "t", ".", "text", ")", "\n", "for", "t", "in", "sentence", ".", "tokens", "\n", "]", "\n", "", "else", ":", "\n", "                ", "context_idxs", "=", "[", "\n", "self", ".", "vocab_dictionary", ".", "get_idx_for_item", "(", "t", ".", "get_tag", "(", "self", ".", "field", ")", ".", "value", ")", "\n", "for", "t", "in", "sentence", ".", "tokens", "\n", "]", "\n", "\n", "", "one_hot_sentences", ".", "extend", "(", "context_idxs", ")", "\n", "\n", "", "one_hot_sentences", "=", "torch", ".", "tensor", "(", "one_hot_sentences", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "\n", "flair", ".", "device", "\n", ")", "\n", "\n", "embedded", "=", "self", ".", "embedding_layer", ".", "forward", "(", "one_hot_sentences", ")", "\n", "\n", "index", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "for", "token", "in", "sentence", ":", "\n", "                ", "embedding", "=", "embedded", "[", "index", "]", "\n", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "embedding", ")", "\n", "index", "+=", "1", "\n", "\n", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OneHotEmbeddings.__str__": [[566, 568], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OneHotEmbeddings.extra_repr": [[569, 571], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "\"min_freq={}\"", ".", "format", "(", "self", ".", "min_freq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.HashEmbeddings.__init__": [[576, 596], ["super().__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "embeddings.HashEmbeddings.to"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["def", "__init__", "(", "\n", "self", ",", "num_embeddings", ":", "int", "=", "1000", ",", "embedding_length", ":", "int", "=", "300", ",", "hash_method", "=", "\"md5\"", "\n", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "name", "=", "\"hash\"", "\n", "self", ".", "static_embeddings", "=", "False", "\n", "\n", "self", ".", "__num_embeddings", "=", "num_embeddings", "\n", "self", ".", "__embedding_length", "=", "embedding_length", "\n", "\n", "self", ".", "__hash_method", "=", "hash_method", "\n", "\n", "# rpbert architecture", "\n", "self", ".", "embedding_layer", "=", "torch", ".", "nn", ".", "Embedding", "(", "\n", "self", ".", "__num_embeddings", ",", "self", ".", "__embedding_length", "\n", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "embedding_layer", ".", "weight", ")", "\n", "\n", "self", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.HashEmbeddings.num_embeddings": [[597, 600], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_embeddings", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__num_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.HashEmbeddings.embedding_length": [[601, 604], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.HashEmbeddings._add_embeddings_internal": [[605, 629], ["enumerate", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "embeddings.HashEmbeddings.embedding_layer.forward", "hashlib.new", "hashlib.new.update", "torch.tensor().to.extend", "torch.tensor().to.extend", "bytes", "int", "embeddings.HashEmbeddings._add_embeddings_internal.get_idx_for_item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "def", "get_idx_for_item", "(", "text", ")", ":", "\n", "            ", "hash_function", "=", "hashlib", ".", "new", "(", "self", ".", "__hash_method", ")", "\n", "hash_function", ".", "update", "(", "bytes", "(", "str", "(", "text", ")", ",", "\"utf-8\"", ")", ")", "\n", "return", "int", "(", "hash_function", ".", "hexdigest", "(", ")", ",", "16", ")", "%", "self", ".", "__num_embeddings", "\n", "\n", "", "hash_sentences", "=", "[", "]", "\n", "for", "i", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "            ", "context_idxs", "=", "[", "get_idx_for_item", "(", "t", ".", "text", ")", "for", "t", "in", "sentence", ".", "tokens", "]", "\n", "\n", "hash_sentences", ".", "extend", "(", "context_idxs", ")", "\n", "\n", "", "hash_sentences", "=", "torch", ".", "tensor", "(", "hash_sentences", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "embedded", "=", "self", ".", "embedding_layer", ".", "forward", "(", "hash_sentences", ")", "\n", "\n", "index", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "for", "token", "in", "sentence", ":", "\n", "                ", "embedding", "=", "embedded", "[", "index", "]", "\n", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "embedding", ")", "\n", "index", "+=", "1", "\n", "\n", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.HashEmbeddings.__str__": [[630, 632], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.MuseCrosslingualEmbeddings.__init__": [[635, 641], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", ")", ":", "\n", "        ", "self", ".", "name", ":", "str", "=", "f\"muse-crosslingual\"", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "self", ".", "__embedding_length", ":", "int", "=", "300", "\n", "self", ".", "language_embeddings", "=", "{", "}", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.MuseCrosslingualEmbeddings.get_cached_vec": [[642, 659], ["functools.lru_cache", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "word.lower", "re.sub", "word.lower", "word.lower", "re.sub", "numpy.zeros", "re.sub", "word.lower", "word.lower", "re.sub", "word.lower"], "methods", ["None"], ["", "@", "lru_cache", "(", "maxsize", "=", "10000", ",", "typed", "=", "False", ")", "\n", "def", "get_cached_vec", "(", "self", ",", "language_code", ":", "str", ",", "word", ":", "str", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "current_embedding_model", "=", "self", ".", "language_embeddings", "[", "language_code", "]", "\n", "if", "word", "in", "current_embedding_model", ":", "\n", "            ", "word_embedding", "=", "current_embedding_model", "[", "word", "]", "\n", "", "elif", "word", ".", "lower", "(", ")", "in", "current_embedding_model", ":", "\n", "            ", "word_embedding", "=", "current_embedding_model", "[", "word", ".", "lower", "(", ")", "]", "\n", "", "elif", "re", ".", "sub", "(", "r\"\\d\"", ",", "\"#\"", ",", "word", ".", "lower", "(", ")", ")", "in", "current_embedding_model", ":", "\n", "            ", "word_embedding", "=", "current_embedding_model", "[", "re", ".", "sub", "(", "r\"\\d\"", ",", "\"#\"", ",", "word", ".", "lower", "(", ")", ")", "]", "\n", "", "elif", "re", ".", "sub", "(", "r\"\\d\"", ",", "\"0\"", ",", "word", ".", "lower", "(", ")", ")", "in", "current_embedding_model", ":", "\n", "            ", "word_embedding", "=", "current_embedding_model", "[", "re", ".", "sub", "(", "r\"\\d\"", ",", "\"0\"", ",", "word", ".", "lower", "(", ")", ")", "]", "\n", "", "else", ":", "\n", "            ", "word_embedding", "=", "np", ".", "zeros", "(", "self", ".", "embedding_length", ",", "dtype", "=", "\"float\"", ")", "\n", "", "word_embedding", "=", "torch", ".", "tensor", "(", "\n", "word_embedding", ",", "device", "=", "flair", ".", "device", ",", "dtype", "=", "torch", ".", "float", "\n", ")", "\n", "return", "word_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.MuseCrosslingualEmbeddings._add_embeddings_internal": [[660, 725], ["enumerate", "sentence.get_language_code", "zip", "log.info", "file_utils.cached_path", "file_utils.cached_path", "gensim.models.KeyedVectors.load", "range", "embeddings.MuseCrosslingualEmbeddings.get_cached_vec", "token.set_embedding", "pathlib.Path", "str", "len", "token.get_tag"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_language_code", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.MuseCrosslingualEmbeddings.get_cached_vec", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "\n", "        ", "for", "i", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "\n", "            ", "language_code", "=", "sentence", ".", "get_language_code", "(", ")", "\n", "supported", "=", "[", "\n", "\"en\"", ",", "\n", "\"de\"", ",", "\n", "\"bg\"", ",", "\n", "\"ca\"", ",", "\n", "\"hr\"", ",", "\n", "\"cs\"", ",", "\n", "\"da\"", ",", "\n", "\"nl\"", ",", "\n", "\"et\"", ",", "\n", "\"fi\"", ",", "\n", "\"fr\"", ",", "\n", "\"el\"", ",", "\n", "\"he\"", ",", "\n", "\"hu\"", ",", "\n", "\"id\"", ",", "\n", "\"it\"", ",", "\n", "\"mk\"", ",", "\n", "\"no\"", ",", "\n", "\"pl\"", ",", "\n", "\"pt\"", ",", "\n", "\"ro\"", ",", "\n", "\"ru\"", ",", "\n", "\"sk\"", ",", "\n", "]", "\n", "if", "language_code", "not", "in", "supported", ":", "\n", "                ", "language_code", "=", "\"en\"", "\n", "\n", "", "if", "language_code", "not", "in", "self", ".", "language_embeddings", ":", "\n", "                ", "log", ".", "info", "(", "f\"Loading up MUSE embeddings for '{language_code}'!\"", ")", "\n", "# download if necessary", "\n", "webpath", "=", "\"https://alan-nlp.s3.eu-central-1.amazonaws.com/resources/embeddings-muse\"", "\n", "cache_dir", "=", "Path", "(", "\"embeddings\"", ")", "/", "\"MUSE\"", "\n", "cached_path", "(", "\n", "f\"{webpath}/muse.{language_code}.vec.gensim.vectors.npy\"", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", ")", "\n", "embeddings_file", "=", "cached_path", "(", "\n", "f\"{webpath}/muse.{language_code}.vec.gensim\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "\n", "# load the rpbert", "\n", "self", ".", "language_embeddings", "[", "\n", "language_code", "\n", "]", "=", "gensim", ".", "models", ".", "KeyedVectors", ".", "load", "(", "str", "(", "embeddings_file", ")", ")", "\n", "\n", "", "for", "token", ",", "token_idx", "in", "zip", "(", "sentence", ".", "tokens", ",", "range", "(", "len", "(", "sentence", ".", "tokens", ")", ")", ")", ":", "\n", "\n", "                ", "if", "\"field\"", "not", "in", "self", ".", "__dict__", "or", "self", ".", "field", "is", "None", ":", "\n", "                    ", "word", "=", "token", ".", "text", "\n", "", "else", ":", "\n", "                    ", "word", "=", "token", ".", "get_tag", "(", "self", ".", "field", ")", ".", "value", "\n", "\n", "", "word_embedding", "=", "self", ".", "get_cached_vec", "(", "\n", "language_code", "=", "language_code", ",", "word", "=", "word", "\n", ")", "\n", "\n", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "word_embedding", ")", "\n", "\n", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.MuseCrosslingualEmbeddings.embedding_length": [[726, 729], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.MuseCrosslingualEmbeddings.__str__": [[730, 732], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BytePairEmbeddings.__init__": [[735, 752], ["bpemb.BPEmb", "super().__init__", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "language", ":", "str", ",", "\n", "dim", ":", "int", "=", "50", ",", "\n", "syllables", ":", "int", "=", "100000", ",", "\n", "cache_dir", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"embeddings\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Initializes BP embeddings. Constructor downloads required files if not there.\n        \"\"\"", "\n", "\n", "self", ".", "name", ":", "str", "=", "f\"bpe-{language}-{syllables}-{dim}\"", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "self", ".", "embedder", "=", "BPEmb", "(", "lang", "=", "language", ",", "vs", "=", "syllables", ",", "dim", "=", "dim", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "self", ".", "__embedding_length", ":", "int", "=", "self", ".", "embedder", ".", "emb", ".", "vector_size", "*", "2", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BytePairEmbeddings.embedding_length": [[753, 756], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BytePairEmbeddings._add_embeddings_internal": [[757, 784], ["enumerate", "zip", "range", "len", "word.strip", "token.set_embedding", "embeddings.BytePairEmbeddings.BytePairEmbeddings.embedder.embed", "numpy.concatenate", "token.set_embedding", "token.get_tag", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "word.lower", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "\n", "        ", "for", "i", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "\n", "            ", "for", "token", ",", "token_idx", "in", "zip", "(", "sentence", ".", "tokens", ",", "range", "(", "len", "(", "sentence", ".", "tokens", ")", ")", ")", ":", "\n", "\n", "                ", "if", "\"field\"", "not", "in", "self", ".", "__dict__", "or", "self", ".", "field", "is", "None", ":", "\n", "                    ", "word", "=", "token", ".", "text", "\n", "", "else", ":", "\n", "                    ", "word", "=", "token", ".", "get_tag", "(", "self", ".", "field", ")", ".", "value", "\n", "\n", "", "if", "word", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "# empty words get no embedding", "\n", "                    ", "token", ".", "set_embedding", "(", "\n", "self", ".", "name", ",", "torch", ".", "zeros", "(", "self", ".", "embedding_length", ",", "dtype", "=", "torch", ".", "float", ")", "\n", ")", "\n", "", "else", ":", "\n", "# all other words get embedded", "\n", "                    ", "embeddings", "=", "self", ".", "embedder", ".", "embed", "(", "word", ".", "lower", "(", ")", ")", "\n", "embedding", "=", "np", ".", "concatenate", "(", "\n", "(", "embeddings", "[", "0", "]", ",", "embeddings", "[", "len", "(", "embeddings", ")", "-", "1", "]", ")", "\n", ")", "\n", "token", ".", "set_embedding", "(", "\n", "self", ".", "name", ",", "torch", ".", "tensor", "(", "embedding", ",", "dtype", "=", "torch", ".", "float", ")", "\n", ")", "\n", "\n", "", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BytePairEmbeddings.__str__": [[785, 787], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BytePairEmbeddings.extra_repr": [[788, 790], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "\"rpbert={}\"", ".", "format", "(", "self", ".", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ELMoEmbeddings.__init__": [[796, 856], ["super().__init__", "re.fullmatch", "allennlp.commands.elmo.ElmoEmbedder", "data.Sentence", "dummy_sentence.add_token", "embeddings.ELMoEmbeddings.embed", "len", "str", "int", "data.Token", "embedded_dummy[].get_token().get_embedding", "log.warning", "log.warning", "log.warning", "log.warning", "str", "str().split", "embedded_dummy[].get_token", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token"], ["def", "__init__", "(", "\n", "self", ",", "model", ":", "str", "=", "\"original\"", ",", "options_file", ":", "str", "=", "None", ",", "weight_file", ":", "str", "=", "None", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "try", ":", "\n", "            ", "import", "allennlp", ".", "commands", ".", "elmo", "\n", "", "except", "ModuleNotFoundError", ":", "\n", "            ", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "log", ".", "warning", "(", "'ATTENTION! The library \"allennlp\" is not installed!'", ")", "\n", "log", ".", "warning", "(", "\n", "'To use ELMoEmbeddings, please first install with \"pip install allennlp\"'", "\n", ")", "\n", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "pass", "\n", "\n", "", "self", ".", "name", "=", "\"elmo-\"", "+", "model", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n", "if", "not", "options_file", "or", "not", "weight_file", ":", "\n", "# the default rpbert for ELMo is the 'original' rpbert, which is very large", "\n", "            ", "options_file", "=", "allennlp", ".", "commands", ".", "elmo", ".", "DEFAULT_OPTIONS_FILE", "\n", "weight_file", "=", "allennlp", ".", "commands", ".", "elmo", ".", "DEFAULT_WEIGHT_FILE", "\n", "# alternatively, a small, medium or portuguese rpbert can be selected by passing the appropriate mode name", "\n", "if", "model", "==", "\"small\"", ":", "\n", "                ", "options_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json\"", "\n", "weight_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5\"", "\n", "", "if", "model", "==", "\"medium\"", ":", "\n", "                ", "options_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x2048_256_2048cnn_1xhighway/elmo_2x2048_256_2048cnn_1xhighway_options.json\"", "\n", "weight_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x2048_256_2048cnn_1xhighway/elmo_2x2048_256_2048cnn_1xhighway_weights.hdf5\"", "\n", "", "if", "model", "in", "[", "\"large\"", ",", "\"5.5B\"", "]", ":", "\n", "                ", "options_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\"", "\n", "weight_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\"", "\n", "", "if", "model", "==", "\"pt\"", "or", "model", "==", "\"portuguese\"", ":", "\n", "                ", "options_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pt/elmo_pt_options.json\"", "\n", "weight_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pt/elmo_pt_weights.hdf5\"", "\n", "", "if", "model", "==", "\"pubmed\"", ":", "\n", "                ", "options_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pubmed/elmo_2x4096_512_2048cnn_2xhighway_options.json\"", "\n", "weight_file", "=", "\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pubmed/elmo_2x4096_512_2048cnn_2xhighway_weights_PubMed_only.hdf5\"", "\n", "\n", "# put on Cuda if available", "\n", "", "", "from", "flair", "import", "device", "\n", "\n", "if", "re", ".", "fullmatch", "(", "r\"cuda:[0-9]+\"", ",", "str", "(", "device", ")", ")", ":", "\n", "            ", "cuda_device", "=", "int", "(", "str", "(", "device", ")", ".", "split", "(", "\":\"", ")", "[", "-", "1", "]", ")", "\n", "", "elif", "str", "(", "device", ")", "==", "\"cpu\"", ":", "\n", "            ", "cuda_device", "=", "-", "1", "\n", "", "else", ":", "\n", "            ", "cuda_device", "=", "0", "\n", "\n", "", "self", ".", "ee", "=", "allennlp", ".", "commands", ".", "elmo", ".", "ElmoEmbedder", "(", "\n", "options_file", "=", "options_file", ",", "weight_file", "=", "weight_file", ",", "cuda_device", "=", "cuda_device", "\n", ")", "\n", "\n", "# embed a dummy sentence to determine embedding_length", "\n", "dummy_sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "dummy_sentence", ".", "add_token", "(", "Token", "(", "\"hello\"", ")", ")", "\n", "embedded_dummy", "=", "self", ".", "embed", "(", "dummy_sentence", ")", "\n", "self", ".", "__embedding_length", ":", "int", "=", "len", "(", "\n", "embedded_dummy", "[", "0", "]", ".", "get_token", "(", "1", ")", ".", "get_embedding", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ELMoEmbeddings.embedding_length": [[858, 861], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ELMoEmbeddings._add_embeddings_internal": [[862, 887], ["embeddings.ELMoEmbeddings.ELMoEmbeddings.ee.embed_batch", "enumerate", "sentence_words.append", "zip", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "token.set_embedding", "len", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "\n", "        ", "sentence_words", ":", "List", "[", "List", "[", "str", "]", "]", "=", "[", "]", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "sentence_words", ".", "append", "(", "[", "token", ".", "text", "for", "token", "in", "sentence", "]", ")", "\n", "\n", "", "embeddings", "=", "self", ".", "ee", ".", "embed_batch", "(", "sentence_words", ")", "\n", "\n", "for", "i", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "\n", "            ", "sentence_embeddings", "=", "embeddings", "[", "i", "]", "\n", "\n", "for", "token", ",", "token_idx", "in", "zip", "(", "sentence", ".", "tokens", ",", "range", "(", "len", "(", "sentence", ".", "tokens", ")", ")", ")", ":", "\n", "                ", "word_embedding", "=", "torch", ".", "cat", "(", "\n", "[", "\n", "torch", ".", "FloatTensor", "(", "sentence_embeddings", "[", "0", ",", "token_idx", ",", ":", "]", ")", ",", "\n", "torch", ".", "FloatTensor", "(", "sentence_embeddings", "[", "1", ",", "token_idx", ",", ":", "]", ")", ",", "\n", "torch", ".", "FloatTensor", "(", "sentence_embeddings", "[", "2", ",", "token_idx", ",", ":", "]", ")", ",", "\n", "]", ",", "\n", "0", ",", "\n", ")", "\n", "\n", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "word_embedding", ")", "\n", "\n", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ELMoEmbeddings.extra_repr": [[888, 890], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "\"rpbert={}\"", ".", "format", "(", "self", ".", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ELMoEmbeddings.__str__": [[891, 893], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ELMoTransformerEmbeddings.__init__": [[898, 940], ["deprecated.deprecated.deprecated", "super().__init__", "BidirectionalLanguageModelTokenEmbedder", "embeddings.ELMoTransformerEmbeddings.lm_embedder.to", "ELMoTokenCharactersIndexer", "data.Sentence", "dummy_sentence.add_token", "embeddings.ELMoTransformerEmbeddings.embed", "len", "data.Token", "embedded_dummy[].get_token().get_embedding", "log.warning", "log.warning", "log.warning", "log.warning", "embedded_dummy[].get_token"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token"], ["@", "deprecated", "(", "\n", "version", "=", "\"0.4.2\"", ",", "\n", "reason", "=", "\"Not possible to load or save ELMo Transformer models. @stefan-it is working on it.\"", ",", "\n", ")", "\n", "def", "__init__", "(", "self", ",", "model_file", ":", "str", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "try", ":", "\n", "            ", "from", "allennlp", ".", "modules", ".", "token_embedders", ".", "bidirectional_language_model_token_embedder", "import", "(", "\n", "BidirectionalLanguageModelTokenEmbedder", ",", "\n", ")", "\n", "from", "allennlp", ".", "data", ".", "token_indexers", ".", "elmo_indexer", "import", "(", "\n", "ELMoTokenCharactersIndexer", ",", "\n", ")", "\n", "", "except", "ModuleNotFoundError", ":", "\n", "            ", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "log", ".", "warning", "(", "'ATTENTION! The library \"allennlp\" is not installed!'", ")", "\n", "log", ".", "warning", "(", "\n", "\"To use ELMoTransformerEmbeddings, please first install a recent version from https://github.com/allenai/allennlp\"", "\n", ")", "\n", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "pass", "\n", "\n", "", "self", ".", "name", "=", "\"elmo-transformer\"", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "self", ".", "lm_embedder", "=", "BidirectionalLanguageModelTokenEmbedder", "(", "\n", "archive_file", "=", "model_file", ",", "\n", "dropout", "=", "0.2", ",", "\n", "bos_eos_tokens", "=", "(", "\"<S>\"", ",", "\"</S>\"", ")", ",", "\n", "remove_bos_eos", "=", "True", ",", "\n", "requires_grad", "=", "False", ",", "\n", ")", "\n", "self", ".", "lm_embedder", "=", "self", ".", "lm_embedder", ".", "to", "(", "device", "=", "flair", ".", "device", ")", "\n", "self", ".", "vocab", "=", "self", ".", "lm_embedder", ".", "_lm", ".", "vocab", "\n", "self", ".", "indexer", "=", "ELMoTokenCharactersIndexer", "(", ")", "\n", "\n", "# embed a dummy sentence to determine embedding_length", "\n", "dummy_sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "dummy_sentence", ".", "add_token", "(", "Token", "(", "\"hello\"", ")", ")", "\n", "embedded_dummy", "=", "self", ".", "embed", "(", "dummy_sentence", ")", "\n", "self", ".", "__embedding_length", ":", "int", "=", "len", "(", "\n", "embedded_dummy", "[", "0", "]", ".", "get_token", "(", "1", ")", ".", "get_embedding", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ELMoTransformerEmbeddings.embedding_length": [[942, 945], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ELMoTransformerEmbeddings._add_embeddings_internal": [[946, 968], ["torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "indices_tensor.to.to.to", "[].detach().cpu().numpy", "zip", "indexer.tokens_to_indices", "range", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "token.set_embedding", "[].detach().cpu", "len", "allen_nlp_token.Token", "[].detach", "[].detach().cpu().numpy.ELMoTransformerEmbeddings.lm_embedder"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "# Avoid conflicts with flair's Token class", "\n", "        ", "import", "allennlp", ".", "data", ".", "tokenizers", ".", "token", "as", "allen_nlp_token", "\n", "\n", "indexer", "=", "self", ".", "indexer", "\n", "vocab", "=", "self", ".", "vocab", "\n", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "character_indices", "=", "indexer", ".", "tokens_to_indices", "(", "\n", "[", "allen_nlp_token", ".", "Token", "(", "token", ".", "text", ")", "for", "token", "in", "sentence", "]", ",", "vocab", ",", "\"elmo\"", "\n", ")", "[", "\"elmo\"", "]", "\n", "\n", "indices_tensor", "=", "torch", ".", "LongTensor", "(", "[", "character_indices", "]", ")", "\n", "indices_tensor", "=", "indices_tensor", ".", "to", "(", "device", "=", "flair", ".", "device", ")", "\n", "embeddings", "=", "self", ".", "lm_embedder", "(", "indices_tensor", ")", "[", "0", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "for", "token", ",", "token_idx", "in", "zip", "(", "sentence", ".", "tokens", ",", "range", "(", "len", "(", "sentence", ".", "tokens", ")", ")", ")", ":", "\n", "                ", "embedding", "=", "embeddings", "[", "token_idx", "]", "\n", "word_embedding", "=", "torch", ".", "FloatTensor", "(", "embedding", ")", "\n", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "word_embedding", ")", "\n", "\n", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ELMoTransformerEmbeddings.extra_repr": [[969, 971], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "\"rpbert={}\"", ".", "format", "(", "self", ".", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ELMoTransformerEmbeddings.__str__": [[972, 974], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ScalarMix.__init__": [[987, 1009], ["super().__init__", "torch.nn.ParameterList", "torch.nn.ParameterList", "torch.nn.Parameter", "torch.nn.Parameter", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.nn.Parameter", "torch.nn.Parameter", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "range", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["def", "__init__", "(", "self", ",", "mixture_size", ":", "int", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Inits scalar mix implementation.\n        ``mixture = gamma * sum(s_k * tensor_k)`` where ``s = softmax(w)``, with ``w`` and ``gamma`` scalar parameters.\n        :param mixture_size: size of mixtures (usually the number of layers)\n        \"\"\"", "\n", "super", "(", "ScalarMix", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mixture_size", "=", "mixture_size", "\n", "\n", "initial_scalar_parameters", "=", "[", "0.0", "]", "*", "mixture_size", "\n", "\n", "self", ".", "scalar_parameters", "=", "ParameterList", "(", "\n", "[", "\n", "Parameter", "(", "\n", "torch", ".", "FloatTensor", "(", "[", "initial_scalar_parameters", "[", "i", "]", "]", ")", ".", "to", "(", "flair", ".", "device", ")", ",", "\n", "requires_grad", "=", "False", ",", "\n", ")", "\n", "for", "i", "in", "range", "(", "mixture_size", ")", "\n", "]", "\n", ")", "\n", "self", ".", "gamma", "=", "Parameter", "(", "\n", "torch", ".", "FloatTensor", "(", "[", "1.0", "]", ")", ".", "to", "(", "flair", ".", "device", ")", ",", "requires_grad", "=", "False", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ScalarMix.forward": [[1011, 1034], ["torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.split", "torch.split", "torch.split", "torch.split", "zip", "len", "log.error", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pieces.append", "sum", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tensors", ":", "List", "[", "torch", ".", "Tensor", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Computes a weighted average of the ``tensors``.  The input tensors an be any shape\n        with at least two dimensions, but must all be the same shape.\n        :param tensors: list of input tensors\n        :return: computed weighted average of input tensors\n        \"\"\"", "\n", "if", "len", "(", "tensors", ")", "!=", "self", ".", "mixture_size", ":", "\n", "            ", "log", ".", "error", "(", "\n", "\"{} tensors were passed, but the module was initialized to mix {} tensors.\"", ".", "format", "(", "\n", "len", "(", "tensors", ")", ",", "self", ".", "mixture_size", "\n", ")", "\n", ")", "\n", "\n", "", "normed_weights", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "\n", "torch", ".", "cat", "(", "[", "parameter", "for", "parameter", "in", "self", ".", "scalar_parameters", "]", ")", ",", "dim", "=", "0", "\n", ")", "\n", "normed_weights", "=", "torch", ".", "split", "(", "normed_weights", ",", "split_size_or_sections", "=", "1", ")", "\n", "\n", "pieces", "=", "[", "]", "\n", "for", "weight", ",", "tensor", "in", "zip", "(", "normed_weights", ",", "tensors", ")", ":", "\n", "            ", "pieces", ".", "append", "(", "weight", "*", "tensor", ")", "\n", "", "return", "self", ".", "gamma", "*", "sum", "(", "pieces", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerXLEmbeddings.__init__": [[1226, 1256], ["super().__init__", "transformers.TransfoXLTokenizer.from_pretrained", "transformers.TransfoXLModel.from_pretrained", "data.Sentence", "dummy_sentence.add_token", "embeddings.TransformerXLEmbeddings.embed", "len", "int", "data.Token", "embedded_dummy[].get_token().get_embedding", "layers.split", "embedded_dummy[].get_token"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "pretrained_model_name_or_path", ":", "str", "=", "\"transfo-xl-wt103\"", ",", "\n", "layers", ":", "str", "=", "\"1,2,3\"", ",", "\n", "use_scalar_mix", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Transformer-XL embeddings, as proposed in Dai et al., 2019.\n        :param pretrained_model_name_or_path: name or path of Transformer-XL rpbert\n        :param layers: comma-separated list of layers\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tokenizer", "=", "TransfoXLTokenizer", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "\n", ")", "\n", "self", ".", "model", "=", "TransfoXLModel", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "=", "pretrained_model_name_or_path", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", ")", "\n", "self", ".", "name", "=", "pretrained_model_name_or_path", "\n", "self", ".", "layers", ":", "List", "[", "int", "]", "=", "[", "int", "(", "layer", ")", "for", "layer", "in", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "self", ".", "use_scalar_mix", "=", "use_scalar_mix", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n", "dummy_sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "dummy_sentence", ".", "add_token", "(", "Token", "(", "\"hello\"", ")", ")", "\n", "embedded_dummy", "=", "self", ".", "embed", "(", "dummy_sentence", ")", "\n", "self", ".", "__embedding_length", ":", "int", "=", "len", "(", "\n", "embedded_dummy", "[", "0", "]", ".", "get_token", "(", "1", ")", ".", "get_embedding", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerXLEmbeddings.embedding_length": [[1258, 1261], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerXLEmbeddings._add_embeddings_internal": [[1262, 1278], ["embeddings.TransformerXLEmbeddings.model.to", "embeddings.TransformerXLEmbeddings.model.eval", "embeddings._get_transformer_sentence_embeddings"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._get_transformer_sentence_embeddings"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "self", ".", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "sentences", "=", "_get_transformer_sentence_embeddings", "(", "\n", "sentences", "=", "sentences", ",", "\n", "tokenizer", "=", "self", ".", "tokenizer", ",", "\n", "model", "=", "self", ".", "model", ",", "\n", "name", "=", "self", ".", "name", ",", "\n", "layers", "=", "self", ".", "layers", ",", "\n", "pooling_operation", "=", "\"first\"", ",", "\n", "use_scalar_mix", "=", "self", ".", "use_scalar_mix", ",", "\n", "eos_token", "=", "\"<eos>\"", ",", "\n", ")", "\n", "\n", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerXLEmbeddings.extra_repr": [[1279, 1281], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "\"rpbert={}\"", ".", "format", "(", "self", ".", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerXLEmbeddings.__str__": [[1282, 1284], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLNetEmbeddings.__init__": [[1287, 1318], ["super().__init__", "transformers.XLNetTokenizer.from_pretrained", "transformers.XLNetModel.from_pretrained", "data.Sentence", "dummy_sentence.add_token", "embeddings.XLNetEmbeddings.embed", "len", "int", "data.Token", "embedded_dummy[].get_token().get_embedding", "layers.split", "embedded_dummy[].get_token"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "pretrained_model_name_or_path", ":", "str", "=", "\"xlnet-large-cased\"", ",", "\n", "layers", ":", "str", "=", "\"1\"", ",", "\n", "pooling_operation", ":", "str", "=", "\"first_last\"", ",", "\n", "use_scalar_mix", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"XLNet embeddings, as proposed in Yang et al., 2019.\n        :param pretrained_model_name_or_path: name or path of XLNet rpbert\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tokenizer", "=", "XLNetTokenizer", ".", "from_pretrained", "(", "pretrained_model_name_or_path", ")", "\n", "self", ".", "model", "=", "XLNetModel", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "=", "pretrained_model_name_or_path", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", ")", "\n", "self", ".", "name", "=", "pretrained_model_name_or_path", "\n", "self", ".", "layers", ":", "List", "[", "int", "]", "=", "[", "int", "(", "layer", ")", "for", "layer", "in", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "self", ".", "pooling_operation", "=", "pooling_operation", "\n", "self", ".", "use_scalar_mix", "=", "use_scalar_mix", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n", "dummy_sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "dummy_sentence", ".", "add_token", "(", "Token", "(", "\"hello\"", ")", ")", "\n", "embedded_dummy", "=", "self", ".", "embed", "(", "dummy_sentence", ")", "\n", "self", ".", "__embedding_length", ":", "int", "=", "len", "(", "\n", "embedded_dummy", "[", "0", "]", ".", "get_token", "(", "1", ")", ".", "get_embedding", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLNetEmbeddings.embedding_length": [[1320, 1323], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLNetEmbeddings._add_embeddings_internal": [[1324, 1341], ["embeddings.XLNetEmbeddings.model.to", "embeddings.XLNetEmbeddings.model.eval", "embeddings._get_transformer_sentence_embeddings"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._get_transformer_sentence_embeddings"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "self", ".", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "sentences", "=", "_get_transformer_sentence_embeddings", "(", "\n", "sentences", "=", "sentences", ",", "\n", "tokenizer", "=", "self", ".", "tokenizer", ",", "\n", "model", "=", "self", ".", "model", ",", "\n", "name", "=", "self", ".", "name", ",", "\n", "layers", "=", "self", ".", "layers", ",", "\n", "pooling_operation", "=", "self", ".", "pooling_operation", ",", "\n", "use_scalar_mix", "=", "self", ".", "use_scalar_mix", ",", "\n", "bos_token", "=", "\"<s>\"", ",", "\n", "eos_token", "=", "\"</s>\"", ",", "\n", ")", "\n", "\n", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLNetEmbeddings.extra_repr": [[1342, 1344], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "\"rpbert={}\"", ".", "format", "(", "self", ".", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLNetEmbeddings.__str__": [[1345, 1347], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLMEmbeddings.__init__": [[1350, 1382], ["super().__init__", "transformers.XLMTokenizer.from_pretrained", "transformers.XLMModel.from_pretrained", "data.Sentence", "dummy_sentence.add_token", "embeddings.XLMEmbeddings.embed", "len", "int", "data.Token", "embedded_dummy[].get_token().get_embedding", "layers.split", "embedded_dummy[].get_token"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "pretrained_model_name_or_path", ":", "str", "=", "\"xlm-mlm-en-2048\"", ",", "\n", "layers", ":", "str", "=", "\"1\"", ",", "\n", "pooling_operation", ":", "str", "=", "\"first_last\"", ",", "\n", "use_scalar_mix", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        XLM embeddings, as proposed in Guillaume et al., 2019.\n        :param pretrained_model_name_or_path: name or path of XLM rpbert\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tokenizer", "=", "XLMTokenizer", ".", "from_pretrained", "(", "pretrained_model_name_or_path", ")", "\n", "self", ".", "model", "=", "XLMModel", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "=", "pretrained_model_name_or_path", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", ")", "\n", "self", ".", "name", "=", "pretrained_model_name_or_path", "\n", "self", ".", "layers", ":", "List", "[", "int", "]", "=", "[", "int", "(", "layer", ")", "for", "layer", "in", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "self", ".", "pooling_operation", "=", "pooling_operation", "\n", "self", ".", "use_scalar_mix", "=", "use_scalar_mix", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n", "dummy_sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "dummy_sentence", ".", "add_token", "(", "Token", "(", "\"hello\"", ")", ")", "\n", "embedded_dummy", "=", "self", ".", "embed", "(", "dummy_sentence", ")", "\n", "self", ".", "__embedding_length", ":", "int", "=", "len", "(", "\n", "embedded_dummy", "[", "0", "]", ".", "get_token", "(", "1", ")", ".", "get_embedding", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLMEmbeddings.embedding_length": [[1384, 1387], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLMEmbeddings._add_embeddings_internal": [[1388, 1405], ["embeddings.XLMEmbeddings.model.to", "embeddings.XLMEmbeddings.model.eval", "embeddings._get_transformer_sentence_embeddings"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._get_transformer_sentence_embeddings"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "self", ".", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "sentences", "=", "_get_transformer_sentence_embeddings", "(", "\n", "sentences", "=", "sentences", ",", "\n", "tokenizer", "=", "self", ".", "tokenizer", ",", "\n", "model", "=", "self", ".", "model", ",", "\n", "name", "=", "self", ".", "name", ",", "\n", "layers", "=", "self", ".", "layers", ",", "\n", "pooling_operation", "=", "self", ".", "pooling_operation", ",", "\n", "use_scalar_mix", "=", "self", ".", "use_scalar_mix", ",", "\n", "bos_token", "=", "\"<s>\"", ",", "\n", "eos_token", "=", "\"</s>\"", ",", "\n", ")", "\n", "\n", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLMEmbeddings.extra_repr": [[1406, 1408], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "\"rpbert={}\"", ".", "format", "(", "self", ".", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLMEmbeddings.__str__": [[1409, 1411], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OpenAIGPTEmbeddings.__init__": [[1414, 1447], ["super().__init__", "transformers.OpenAIGPTTokenizer.from_pretrained", "transformers.OpenAIGPTModel.from_pretrained", "data.Sentence", "dummy_sentence.add_token", "embeddings.OpenAIGPTEmbeddings.embed", "len", "int", "data.Token", "embedded_dummy[].get_token().get_embedding", "layers.split", "embedded_dummy[].get_token"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "pretrained_model_name_or_path", ":", "str", "=", "\"openai-gpt\"", ",", "\n", "layers", ":", "str", "=", "\"1\"", ",", "\n", "pooling_operation", ":", "str", "=", "\"first_last\"", ",", "\n", "use_scalar_mix", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"OpenAI GPT embeddings, as proposed in Radford et al. 2018.\n        :param pretrained_model_name_or_path: name or path of OpenAI GPT rpbert\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tokenizer", "=", "OpenAIGPTTokenizer", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "\n", ")", "\n", "self", ".", "model", "=", "OpenAIGPTModel", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "=", "pretrained_model_name_or_path", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", ")", "\n", "self", ".", "name", "=", "pretrained_model_name_or_path", "\n", "self", ".", "layers", ":", "List", "[", "int", "]", "=", "[", "int", "(", "layer", ")", "for", "layer", "in", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "self", ".", "pooling_operation", "=", "pooling_operation", "\n", "self", ".", "use_scalar_mix", "=", "use_scalar_mix", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n", "dummy_sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "dummy_sentence", ".", "add_token", "(", "Token", "(", "\"hello\"", ")", ")", "\n", "embedded_dummy", "=", "self", ".", "embed", "(", "dummy_sentence", ")", "\n", "self", ".", "__embedding_length", ":", "int", "=", "len", "(", "\n", "embedded_dummy", "[", "0", "]", ".", "get_token", "(", "1", ")", ".", "get_embedding", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OpenAIGPTEmbeddings.embedding_length": [[1449, 1452], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OpenAIGPTEmbeddings._add_embeddings_internal": [[1453, 1468], ["embeddings.OpenAIGPTEmbeddings.model.to", "embeddings.OpenAIGPTEmbeddings.model.eval", "embeddings._get_transformer_sentence_embeddings"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._get_transformer_sentence_embeddings"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "self", ".", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "sentences", "=", "_get_transformer_sentence_embeddings", "(", "\n", "sentences", "=", "sentences", ",", "\n", "tokenizer", "=", "self", ".", "tokenizer", ",", "\n", "model", "=", "self", ".", "model", ",", "\n", "name", "=", "self", ".", "name", ",", "\n", "layers", "=", "self", ".", "layers", ",", "\n", "pooling_operation", "=", "self", ".", "pooling_operation", ",", "\n", "use_scalar_mix", "=", "self", ".", "use_scalar_mix", ",", "\n", ")", "\n", "\n", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OpenAIGPTEmbeddings.extra_repr": [[1469, 1471], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "\"rpbert={}\"", ".", "format", "(", "self", ".", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OpenAIGPTEmbeddings.__str__": [[1472, 1474], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OpenAIGPT2Embeddings.__init__": [[1477, 1508], ["super().__init__", "transformers.GPT2Tokenizer.from_pretrained", "transformers.GPT2Model.from_pretrained", "data.Sentence", "dummy_sentence.add_token", "embeddings.OpenAIGPT2Embeddings.embed", "len", "int", "data.Token", "embedded_dummy[].get_token().get_embedding", "layers.split", "embedded_dummy[].get_token"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "pretrained_model_name_or_path", ":", "str", "=", "\"gpt2-medium\"", ",", "\n", "layers", ":", "str", "=", "\"1\"", ",", "\n", "pooling_operation", ":", "str", "=", "\"first_last\"", ",", "\n", "use_scalar_mix", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"OpenAI GPT-2 embeddings, as proposed in Radford et al. 2019.\n        :param pretrained_model_name_or_path: name or path of OpenAI GPT-2 rpbert\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "pretrained_model_name_or_path", ")", "\n", "self", ".", "model", "=", "GPT2Model", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "=", "pretrained_model_name_or_path", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", ")", "\n", "self", ".", "name", "=", "pretrained_model_name_or_path", "\n", "self", ".", "layers", ":", "List", "[", "int", "]", "=", "[", "int", "(", "layer", ")", "for", "layer", "in", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "self", ".", "pooling_operation", "=", "pooling_operation", "\n", "self", ".", "use_scalar_mix", "=", "use_scalar_mix", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n", "dummy_sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "dummy_sentence", ".", "add_token", "(", "Token", "(", "\"hello\"", ")", ")", "\n", "embedded_dummy", "=", "self", ".", "embed", "(", "dummy_sentence", ")", "\n", "self", ".", "__embedding_length", ":", "int", "=", "len", "(", "\n", "embedded_dummy", "[", "0", "]", ".", "get_token", "(", "1", ")", ".", "get_embedding", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OpenAIGPT2Embeddings.embedding_length": [[1510, 1513], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.OpenAIGPT2Embeddings._add_embeddings_internal": [[1514, 1531], ["embeddings.OpenAIGPT2Embeddings.model.to", "embeddings.OpenAIGPT2Embeddings.model.eval", "embeddings._get_transformer_sentence_embeddings"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._get_transformer_sentence_embeddings"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "self", ".", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "sentences", "=", "_get_transformer_sentence_embeddings", "(", "\n", "sentences", "=", "sentences", ",", "\n", "tokenizer", "=", "self", ".", "tokenizer", ",", "\n", "model", "=", "self", ".", "model", ",", "\n", "name", "=", "self", ".", "name", ",", "\n", "layers", "=", "self", ".", "layers", ",", "\n", "pooling_operation", "=", "self", ".", "pooling_operation", ",", "\n", "use_scalar_mix", "=", "self", ".", "use_scalar_mix", ",", "\n", "bos_token", "=", "\"<|endoftext|>\"", ",", "\n", "eos_token", "=", "\"<|endoftext|>\"", ",", "\n", ")", "\n", "\n", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.RoBERTaEmbeddings.__init__": [[1534, 1565], ["super().__init__", "transformers.RobertaTokenizer.from_pretrained", "transformers.RobertaModel.from_pretrained", "data.Sentence", "dummy_sentence.add_token", "embeddings.RoBERTaEmbeddings.embed", "len", "int", "data.Token", "embedded_dummy[].get_token().get_embedding", "layers.split", "embedded_dummy[].get_token"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "pretrained_model_name_or_path", ":", "str", "=", "\"roberta-base\"", ",", "\n", "layers", ":", "str", "=", "\"-1\"", ",", "\n", "pooling_operation", ":", "str", "=", "\"first\"", ",", "\n", "use_scalar_mix", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"RoBERTa, as proposed by Liu et al. 2019.\n        :param pretrained_model_name_or_path: name or path of RoBERTa rpbert\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "pretrained_model_name_or_path", ")", "\n", "self", ".", "model", "=", "RobertaModel", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "=", "pretrained_model_name_or_path", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", ")", "\n", "self", ".", "name", "=", "pretrained_model_name_or_path", "\n", "self", ".", "layers", ":", "List", "[", "int", "]", "=", "[", "int", "(", "layer", ")", "for", "layer", "in", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "self", ".", "pooling_operation", "=", "pooling_operation", "\n", "self", ".", "use_scalar_mix", "=", "use_scalar_mix", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n", "dummy_sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "dummy_sentence", ".", "add_token", "(", "Token", "(", "\"hello\"", ")", ")", "\n", "embedded_dummy", "=", "self", ".", "embed", "(", "dummy_sentence", ")", "\n", "self", ".", "__embedding_length", ":", "int", "=", "len", "(", "\n", "embedded_dummy", "[", "0", "]", ".", "get_token", "(", "1", ")", ".", "get_embedding", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.RoBERTaEmbeddings.embedding_length": [[1567, 1570], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.RoBERTaEmbeddings._add_embeddings_internal": [[1571, 1588], ["embeddings.RoBERTaEmbeddings.model.to", "embeddings.RoBERTaEmbeddings.model.eval", "embeddings._get_transformer_sentence_embeddings"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._get_transformer_sentence_embeddings"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "self", ".", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "sentences", "=", "_get_transformer_sentence_embeddings", "(", "\n", "sentences", "=", "sentences", ",", "\n", "tokenizer", "=", "self", ".", "tokenizer", ",", "\n", "model", "=", "self", ".", "model", ",", "\n", "name", "=", "self", ".", "name", ",", "\n", "layers", "=", "self", ".", "layers", ",", "\n", "pooling_operation", "=", "self", ".", "pooling_operation", ",", "\n", "use_scalar_mix", "=", "self", ".", "use_scalar_mix", ",", "\n", "bos_token", "=", "\"<s>\"", ",", "\n", "eos_token", "=", "\"</s>\"", ",", "\n", ")", "\n", "\n", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CamembertEmbeddings.__init__": [[1591, 1624], ["super().__init__", "transformers.CamembertTokenizer.from_pretrained", "transformers.CamembertModel.from_pretrained", "data.Sentence", "dummy_sentence.add_token", "embeddings.CamembertEmbeddings.embed", "len", "int", "data.Token", "embedded_dummy[].get_token().get_embedding", "layers.split", "embedded_dummy[].get_token"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "pretrained_model_name_or_path", ":", "str", "=", "\"camembert-base\"", ",", "\n", "layers", ":", "str", "=", "\"-1\"", ",", "\n", "pooling_operation", ":", "str", "=", "\"first\"", ",", "\n", "use_scalar_mix", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"CamemBERT, a Tasty French Language Model, as proposed by Martin et al. 2019.\n        :param pretrained_model_name_or_path: name or path of RoBERTa rpbert\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tokenizer", "=", "CamembertTokenizer", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "\n", ")", "\n", "self", ".", "model", "=", "CamembertModel", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "=", "pretrained_model_name_or_path", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", ")", "\n", "self", ".", "name", "=", "pretrained_model_name_or_path", "\n", "self", ".", "layers", ":", "List", "[", "int", "]", "=", "[", "int", "(", "layer", ")", "for", "layer", "in", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "self", ".", "pooling_operation", "=", "pooling_operation", "\n", "self", ".", "use_scalar_mix", "=", "use_scalar_mix", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n", "dummy_sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "dummy_sentence", ".", "add_token", "(", "Token", "(", "\"hello\"", ")", ")", "\n", "embedded_dummy", "=", "self", ".", "embed", "(", "dummy_sentence", ")", "\n", "self", ".", "__embedding_length", ":", "int", "=", "len", "(", "\n", "embedded_dummy", "[", "0", "]", ".", "get_token", "(", "1", ")", ".", "get_embedding", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CamembertEmbeddings.__getstate__": [[1626, 1630], ["embeddings.CamembertEmbeddings.__dict__.copy"], "methods", ["None"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "state", "=", "self", ".", "__dict__", ".", "copy", "(", ")", "\n", "state", "[", "\"tokenizer\"", "]", "=", "None", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CamembertEmbeddings.__setstate__": [[1631, 1637], ["transformers.CamembertTokenizer.from_pretrained", "embeddings.CamembertEmbeddings.name.split"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained"], ["", "def", "__setstate__", "(", "self", ",", "d", ")", ":", "\n", "        ", "self", ".", "__dict__", "=", "d", "\n", "\n", "# 1-camembert-base -> camembert-base", "\n", "self", ".", "tokenizer", "=", "self", ".", "tokenizer", "=", "CamembertTokenizer", ".", "from_pretrained", "(", "\n", "\"-\"", ".", "join", "(", "self", ".", "name", ".", "split", "(", "\"-\"", ")", "[", "1", ":", "]", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CamembertEmbeddings.embedding_length": [[1639, 1642], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CamembertEmbeddings._add_embeddings_internal": [[1643, 1660], ["embeddings.CamembertEmbeddings.model.to", "embeddings.CamembertEmbeddings.model.eval", "embeddings._get_transformer_sentence_embeddings"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._get_transformer_sentence_embeddings"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "self", ".", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "sentences", "=", "_get_transformer_sentence_embeddings", "(", "\n", "sentences", "=", "sentences", ",", "\n", "tokenizer", "=", "self", ".", "tokenizer", ",", "\n", "model", "=", "self", ".", "model", ",", "\n", "name", "=", "self", ".", "name", ",", "\n", "layers", "=", "self", ".", "layers", ",", "\n", "pooling_operation", "=", "self", ".", "pooling_operation", ",", "\n", "use_scalar_mix", "=", "self", ".", "use_scalar_mix", ",", "\n", "bos_token", "=", "\"<s>\"", ",", "\n", "eos_token", "=", "\"</s>\"", ",", "\n", ")", "\n", "\n", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLMRobertaEmbeddings.__init__": [[1663, 1696], ["super().__init__", "transformers.XLMRobertaTokenizer.from_pretrained", "transformers.XLMRobertaModel.from_pretrained", "data.Sentence", "dummy_sentence.add_token", "embeddings.XLMRobertaEmbeddings.embed", "len", "int", "data.Token", "embedded_dummy[].get_token().get_embedding", "layers.split", "embedded_dummy[].get_token"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "pretrained_model_name_or_path", ":", "str", "=", "\"xlm-roberta-large\"", ",", "\n", "layers", ":", "str", "=", "\"-1\"", ",", "\n", "pooling_operation", ":", "str", "=", "\"first\"", ",", "\n", "use_scalar_mix", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"XLM-RoBERTa as proposed by Conneau et al. 2019.\n        :param pretrained_model_name_or_path: name or path of XLM-R rpbert\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tokenizer", "=", "XLMRobertaTokenizer", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "\n", ")", "\n", "self", ".", "model", "=", "XLMRobertaModel", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "=", "pretrained_model_name_or_path", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", ")", "\n", "self", ".", "name", "=", "pretrained_model_name_or_path", "\n", "self", ".", "layers", ":", "List", "[", "int", "]", "=", "[", "int", "(", "layer", ")", "for", "layer", "in", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "self", ".", "pooling_operation", "=", "pooling_operation", "\n", "self", ".", "use_scalar_mix", "=", "use_scalar_mix", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n", "dummy_sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "dummy_sentence", ".", "add_token", "(", "Token", "(", "\"hello\"", ")", ")", "\n", "embedded_dummy", "=", "self", ".", "embed", "(", "dummy_sentence", ")", "\n", "self", ".", "__embedding_length", ":", "int", "=", "len", "(", "\n", "embedded_dummy", "[", "0", "]", ".", "get_token", "(", "1", ")", ".", "get_embedding", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLMRobertaEmbeddings.__getstate__": [[1698, 1702], ["embeddings.XLMRobertaEmbeddings.__dict__.copy"], "methods", ["None"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "state", "=", "self", ".", "__dict__", ".", "copy", "(", ")", "\n", "state", "[", "\"tokenizer\"", "]", "=", "None", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLMRobertaEmbeddings.__setstate__": [[1703, 1709], ["transformers.XLMRobertaTokenizer.from_pretrained", "embeddings.XLMRobertaEmbeddings.name.split"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained"], ["", "def", "__setstate__", "(", "self", ",", "d", ")", ":", "\n", "        ", "self", ".", "__dict__", "=", "d", "\n", "\n", "# 1-xlm-roberta-large -> xlm-roberta-large", "\n", "self", ".", "tokenizer", "=", "self", ".", "tokenizer", "=", "XLMRobertaTokenizer", ".", "from_pretrained", "(", "\n", "\"-\"", ".", "join", "(", "self", ".", "name", ".", "split", "(", "\"-\"", ")", "[", "1", ":", "]", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLMRobertaEmbeddings.embedding_length": [[1711, 1714], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.XLMRobertaEmbeddings._add_embeddings_internal": [[1715, 1732], ["embeddings.XLMRobertaEmbeddings.model.to", "embeddings.XLMRobertaEmbeddings.model.eval", "embeddings._get_transformer_sentence_embeddings"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._get_transformer_sentence_embeddings"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "self", ".", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "sentences", "=", "_get_transformer_sentence_embeddings", "(", "\n", "sentences", "=", "sentences", ",", "\n", "tokenizer", "=", "self", ".", "tokenizer", ",", "\n", "model", "=", "self", ".", "model", ",", "\n", "name", "=", "self", ".", "name", ",", "\n", "layers", "=", "self", ".", "layers", ",", "\n", "pooling_operation", "=", "self", ".", "pooling_operation", ",", "\n", "use_scalar_mix", "=", "self", ".", "use_scalar_mix", ",", "\n", "bos_token", "=", "\"<s>\"", ",", "\n", "eos_token", "=", "\"</s>\"", ",", "\n", ")", "\n", "\n", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CharacterEmbeddings.__init__": [[1737, 1772], ["super().__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "embeddings.CharacterEmbeddings.to", "data.Dictionary.load", "data.Dictionary.load_from_file", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.load_from_file"], ["def", "__init__", "(", "\n", "self", ",", "\n", "path_to_char_dict", ":", "str", "=", "None", ",", "\n", "char_embedding_dim", ":", "int", "=", "25", ",", "\n", "hidden_size_char", ":", "int", "=", "25", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Uses the default character dictionary if none provided.\"\"\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "name", "=", "\"Char\"", "\n", "self", ".", "static_embeddings", "=", "False", "\n", "\n", "# use list of common characters if none provided", "\n", "if", "path_to_char_dict", "is", "None", ":", "\n", "            ", "self", ".", "char_dictionary", ":", "Dictionary", "=", "Dictionary", ".", "load", "(", "\"common-chars\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "char_dictionary", ":", "Dictionary", "=", "Dictionary", ".", "load_from_file", "(", "\n", "path_to_char_dict", "\n", ")", "\n", "\n", "", "self", ".", "char_embedding_dim", ":", "int", "=", "char_embedding_dim", "\n", "self", ".", "hidden_size_char", ":", "int", "=", "hidden_size_char", "\n", "self", ".", "char_embedding", "=", "torch", ".", "nn", ".", "Embedding", "(", "\n", "len", "(", "self", ".", "char_dictionary", ".", "item2idx", ")", ",", "self", ".", "char_embedding_dim", "\n", ")", "\n", "self", ".", "char_rnn", "=", "torch", ".", "nn", ".", "LSTM", "(", "\n", "self", ".", "char_embedding_dim", ",", "\n", "self", ".", "hidden_size_char", ",", "\n", "num_layers", "=", "1", ",", "\n", "bidirectional", "=", "True", ",", "\n", ")", "\n", "\n", "self", ".", "__embedding_length", "=", "self", ".", "hidden_size_char", "*", "2", "\n", "\n", "self", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CharacterEmbeddings.embedding_length": [[1773, 1776], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CharacterEmbeddings._add_embeddings_internal": [[1777, 1839], ["sorted", "enumerate", "max", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "embeddings.CharacterEmbeddings.char_embedding().transpose", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "embeddings.CharacterEmbeddings.char_rnn", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "outputs.transpose.transpose.transpose", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "torch.zeros.clone", "torch.zeros.clone", "range", "enumerate", "tokens_char_indices.append", "enumerate", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.zeros.clone.size", "token.set_embedding", "embeddings.CharacterEmbeddings.char_dictionary.get_idx_for_item", "len", "embeddings.CharacterEmbeddings.char_embedding", "outputs.transpose.transpose.size", "outputs.transpose.transpose.size", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", ":", "\n", "\n", "        ", "for", "sentence", "in", "sentences", ":", "\n", "\n", "            ", "tokens_char_indices", "=", "[", "]", "\n", "\n", "# translate words in sentence into ints using dictionary", "\n", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "                ", "char_indices", "=", "[", "\n", "self", ".", "char_dictionary", ".", "get_idx_for_item", "(", "char", ")", "for", "char", "in", "token", ".", "text", "\n", "]", "\n", "tokens_char_indices", ".", "append", "(", "char_indices", ")", "\n", "\n", "# sort words by length, for batching and masking", "\n", "", "tokens_sorted_by_length", "=", "sorted", "(", "\n", "tokens_char_indices", ",", "key", "=", "lambda", "p", ":", "len", "(", "p", ")", ",", "reverse", "=", "True", "\n", ")", "\n", "d", "=", "{", "}", "\n", "for", "i", ",", "ci", "in", "enumerate", "(", "tokens_char_indices", ")", ":", "\n", "                ", "for", "j", ",", "cj", "in", "enumerate", "(", "tokens_sorted_by_length", ")", ":", "\n", "                    ", "if", "ci", "==", "cj", ":", "\n", "                        ", "d", "[", "j", "]", "=", "i", "\n", "continue", "\n", "", "", "", "chars2_length", "=", "[", "len", "(", "c", ")", "for", "c", "in", "tokens_sorted_by_length", "]", "\n", "longest_token_in_sentence", "=", "max", "(", "chars2_length", ")", "\n", "tokens_mask", "=", "torch", ".", "zeros", "(", "\n", "(", "len", "(", "tokens_sorted_by_length", ")", ",", "longest_token_in_sentence", ")", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "flair", ".", "device", ",", "\n", ")", "\n", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "tokens_sorted_by_length", ")", ":", "\n", "                ", "tokens_mask", "[", "i", ",", ":", "chars2_length", "[", "i", "]", "]", "=", "torch", ".", "tensor", "(", "\n", "c", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "flair", ".", "device", "\n", ")", "\n", "\n", "# chars for rnn processing", "\n", "", "chars", "=", "tokens_mask", "\n", "\n", "character_embeddings", "=", "self", ".", "char_embedding", "(", "chars", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "packed", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "\n", "character_embeddings", ",", "chars2_length", "\n", ")", "\n", "\n", "lstm_out", ",", "self", ".", "hidden", "=", "self", ".", "char_rnn", "(", "packed", ")", "\n", "\n", "outputs", ",", "output_lengths", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "lstm_out", ")", "\n", "outputs", "=", "outputs", ".", "transpose", "(", "0", ",", "1", ")", "\n", "chars_embeds_temp", "=", "torch", ".", "zeros", "(", "\n", "(", "outputs", ".", "size", "(", "0", ")", ",", "outputs", ".", "size", "(", "2", ")", ")", ",", "\n", "dtype", "=", "torch", ".", "float", ",", "\n", "device", "=", "flair", ".", "device", ",", "\n", ")", "\n", "for", "i", ",", "index", "in", "enumerate", "(", "output_lengths", ")", ":", "\n", "                ", "chars_embeds_temp", "[", "i", "]", "=", "outputs", "[", "i", ",", "index", "-", "1", "]", "\n", "", "character_embeddings", "=", "chars_embeds_temp", ".", "clone", "(", ")", "\n", "for", "i", "in", "range", "(", "character_embeddings", ".", "size", "(", "0", ")", ")", ":", "\n", "                ", "character_embeddings", "[", "d", "[", "i", "]", "]", "=", "chars_embeds_temp", "[", "i", "]", "\n", "\n", "", "for", "token_number", ",", "token", "in", "enumerate", "(", "sentence", ".", "tokens", ")", ":", "\n", "                ", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "character_embeddings", "[", "token_number", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CharacterEmbeddings.__str__": [[1840, 1842], ["None"], "methods", ["None"], ["", "", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.FlairEmbeddings.__init__": [[1847, 2029], ["super().__init__", "pathlib.Path", "data.Sentence", "dummy_sentence.add_token", "embeddings.FlairEmbeddings.embed", "len", "embeddings.FlairEmbeddings.eval", "type", "type", "LanguageModel.load_language_model", "str", "data.Token", "embedded_dummy[].get_token().get_embedding", "file_utils.cached_path.lower", "file_utils.cached_path", "embeddings.replace_with_language_code", "file_utils.cached_path", "embedded_dummy[].get_token", "file_utils.cached_path.lower", "pathlib.Path().exists", "ValueError", "embeddings.replace_with_language_code", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.load_language_model", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.replace_with_language_code", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.replace_with_language_code"], ["def", "__init__", "(", "self", ",", "model", ",", "fine_tune", ":", "bool", "=", "False", ",", "chars_per_chunk", ":", "int", "=", "512", ")", ":", "\n", "        ", "\"\"\"\n        initializes contextual string embeddings using a character-level language rpbert.\n        :param model: rpbert string, one of 'news-forward', 'news-backward', 'news-forward-fast', 'news-backward-fast',\n                'mix-forward', 'mix-backward', 'german-forward', 'german-backward', 'polish-backward', 'polish-forward'\n                depending on which character language rpbert is desired.\n        :param fine_tune: if set to True, the gradient will propagate into the language rpbert. This dramatically slows down\n                training and often leads to overfitting, so use with caution.\n        :param  chars_per_chunk: max number of chars per rnn pass to control speed/memory tradeoff. Higher means faster but requires\n                more memory. Lower means slower but less memory.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "cache_dir", "=", "Path", "(", "\"embeddings\"", ")", "\n", "\n", "aws_path", ":", "str", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources\"", "\n", "hu_path", ":", "str", "=", "\"https://flair.informatik.hu-berlin.de/resources\"", "\n", "\n", "self", ".", "PRETRAINED_MODEL_ARCHIVE_MAP", "=", "{", "\n", "# multilingual models", "\n", "\"multi-forward\"", ":", "f\"{aws_path}/embeddings-v0.4.3/lm-jw300-forward-v0.1.pt\"", ",", "\n", "\"multi-backward\"", ":", "f\"{aws_path}/embeddings-v0.4.3/lm-jw300-backward-v0.1.pt\"", ",", "\n", "\"multi-v0-forward\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-multi-forward-v0.1.pt\"", ",", "\n", "\"multi-v0-backward\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-multi-backward-v0.1.pt\"", ",", "\n", "\"multi-v0-forward-fast\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-multi-forward-fast-v0.1.pt\"", ",", "\n", "\"multi-v0-backward-fast\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-multi-backward-fast-v0.1.pt\"", ",", "\n", "# English models", "\n", "\"en-forward\"", ":", "f\"{aws_path}/embeddings-v0.4.1/big-news-forward--h2048-l1-d0.05-lr30-0.25-20/news-forward-0.4.1.pt\"", ",", "\n", "\"en-backward\"", ":", "f\"{aws_path}/embeddings-v0.4.1/big-news-backward--h2048-l1-d0.05-lr30-0.25-20/news-backward-0.4.1.pt\"", ",", "\n", "\"en-forward-fast\"", ":", "f\"{aws_path}/embeddings/lm-news-english-forward-1024-v0.2rc.pt\"", ",", "\n", "\"en-backward-fast\"", ":", "f\"{aws_path}/embeddings/lm-news-english-backward-1024-v0.2rc.pt\"", ",", "\n", "\"news-forward\"", ":", "f\"{aws_path}/embeddings-v0.4.1/big-news-forward--h2048-l1-d0.05-lr30-0.25-20/news-forward-0.4.1.pt\"", ",", "\n", "\"news-backward\"", ":", "f\"{aws_path}/embeddings-v0.4.1/big-news-backward--h2048-l1-d0.05-lr30-0.25-20/news-backward-0.4.1.pt\"", ",", "\n", "\"news-forward-fast\"", ":", "f\"{aws_path}/embeddings/lm-news-english-forward-1024-v0.2rc.pt\"", ",", "\n", "\"news-backward-fast\"", ":", "f\"{aws_path}/embeddings/lm-news-english-backward-1024-v0.2rc.pt\"", ",", "\n", "\"mix-forward\"", ":", "f\"{aws_path}/embeddings/lm-mix-english-forward-v0.2rc.pt\"", ",", "\n", "\"mix-backward\"", ":", "f\"{aws_path}/embeddings/lm-mix-english-backward-v0.2rc.pt\"", ",", "\n", "# Arabic", "\n", "\"ar-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-ar-opus-large-forward-v0.1.pt\"", ",", "\n", "\"ar-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-ar-opus-large-backward-v0.1.pt\"", ",", "\n", "# Bulgarian", "\n", "\"bg-forward-fast\"", ":", "f\"{aws_path}/embeddings-v0.3/lm-bg-small-forward-v0.1.pt\"", ",", "\n", "\"bg-backward-fast\"", ":", "f\"{aws_path}/embeddings-v0.3/lm-bg-small-backward-v0.1.pt\"", ",", "\n", "\"bg-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-bg-opus-large-forward-v0.1.pt\"", ",", "\n", "\"bg-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-bg-opus-large-backward-v0.1.pt\"", ",", "\n", "# Czech", "\n", "\"cs-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-cs-opus-large-forward-v0.1.pt\"", ",", "\n", "\"cs-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-cs-opus-large-backward-v0.1.pt\"", ",", "\n", "\"cs-v0-forward\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-cs-large-forward-v0.1.pt\"", ",", "\n", "\"cs-v0-backward\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-cs-large-backward-v0.1.pt\"", ",", "\n", "# Danish", "\n", "\"da-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-da-opus-large-forward-v0.1.pt\"", ",", "\n", "\"da-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-da-opus-large-backward-v0.1.pt\"", ",", "\n", "# German", "\n", "\"de-forward\"", ":", "f\"{aws_path}/embeddings/lm-mix-german-forward-v0.2rc.pt\"", ",", "\n", "\"de-backward\"", ":", "f\"{aws_path}/embeddings/lm-mix-german-backward-v0.2rc.pt\"", ",", "\n", "\"de-historic-ha-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-historic-hamburger-anzeiger-forward-v0.1.pt\"", ",", "\n", "\"de-historic-ha-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-historic-hamburger-anzeiger-backward-v0.1.pt\"", ",", "\n", "\"de-historic-wz-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-historic-wiener-zeitung-forward-v0.1.pt\"", ",", "\n", "\"de-historic-wz-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-historic-wiener-zeitung-backward-v0.1.pt\"", ",", "\n", "\"de-historic-rw-forward\"", ":", "f\"{hu_path}/embeddings/redewiedergabe_lm_forward.pt\"", ",", "\n", "\"de-historic-rw-backward\"", ":", "f\"{hu_path}/embeddings/redewiedergabe_lm_backward.pt\"", ",", "\n", "# Spanish", "\n", "\"es-forward\"", ":", "f\"{aws_path}/embeddings-v0.4/language_model_es_forward_long/lm-es-forward.pt\"", ",", "\n", "\"es-backward\"", ":", "f\"{aws_path}/embeddings-v0.4/language_model_es_backward_long/lm-es-backward.pt\"", ",", "\n", "\"es-forward-fast\"", ":", "f\"{aws_path}/embeddings-v0.4/language_model_es_forward/lm-es-forward-fast.pt\"", ",", "\n", "\"es-backward-fast\"", ":", "f\"{aws_path}/embeddings-v0.4/language_model_es_backward/lm-es-backward-fast.pt\"", ",", "\n", "# Basque", "\n", "\"eu-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-eu-opus-large-forward-v0.2.pt\"", ",", "\n", "\"eu-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-eu-opus-large-backward-v0.2.pt\"", ",", "\n", "\"eu-v1-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-eu-opus-large-forward-v0.1.pt\"", ",", "\n", "\"eu-v1-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-eu-opus-large-backward-v0.1.pt\"", ",", "\n", "\"eu-v0-forward\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-eu-large-forward-v0.1.pt\"", ",", "\n", "\"eu-v0-backward\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-eu-large-backward-v0.1.pt\"", ",", "\n", "# Persian", "\n", "\"fa-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-fa-opus-large-forward-v0.1.pt\"", ",", "\n", "\"fa-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-fa-opus-large-backward-v0.1.pt\"", ",", "\n", "# Finnish", "\n", "\"fi-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-fi-opus-large-forward-v0.1.pt\"", ",", "\n", "\"fi-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-fi-opus-large-backward-v0.1.pt\"", ",", "\n", "# French", "\n", "\"fr-forward\"", ":", "f\"{aws_path}/embeddings/lm-fr-charlm-forward.pt\"", ",", "\n", "\"fr-backward\"", ":", "f\"{aws_path}/embeddings/lm-fr-charlm-backward.pt\"", ",", "\n", "# Hebrew", "\n", "\"he-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-he-opus-large-forward-v0.1.pt\"", ",", "\n", "\"he-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-he-opus-large-backward-v0.1.pt\"", ",", "\n", "# Hindi", "\n", "\"hi-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-hi-opus-large-forward-v0.1.pt\"", ",", "\n", "\"hi-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-hi-opus-large-backward-v0.1.pt\"", ",", "\n", "# Croatian", "\n", "\"hr-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-hr-opus-large-forward-v0.1.pt\"", ",", "\n", "\"hr-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-hr-opus-large-backward-v0.1.pt\"", ",", "\n", "# Indonesian", "\n", "\"id-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-id-opus-large-forward-v0.1.pt\"", ",", "\n", "\"id-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-id-opus-large-backward-v0.1.pt\"", ",", "\n", "# Italian", "\n", "\"it-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-it-opus-large-forward-v0.1.pt\"", ",", "\n", "\"it-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-it-opus-large-backward-v0.1.pt\"", ",", "\n", "# Japanese", "\n", "\"ja-forward\"", ":", "f\"{aws_path}/embeddings-v0.4.1/lm__char-forward__ja-wikipedia-3GB/japanese-forward.pt\"", ",", "\n", "\"ja-backward\"", ":", "f\"{aws_path}/embeddings-v0.4.1/lm__char-backward__ja-wikipedia-3GB/japanese-backward.pt\"", ",", "\n", "# Malayalam", "\n", "\"ml-forward\"", ":", "f\"https://raw.githubusercontent.com/qburst/models-repository/master/FlairMalayalamModels/ml-forward.pt\"", ",", "\n", "\"ml-backward\"", ":", "f\"https://raw.githubusercontent.com/qburst/models-repository/master/FlairMalayalamModels/ml-backward.pt\"", ",", "\n", "# Dutch", "\n", "\"nl-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-nl-opus-large-forward-v0.1.pt\"", ",", "\n", "\"nl-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-nl-opus-large-backward-v0.1.pt\"", ",", "\n", "\"nl-v0-forward\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-nl-large-forward-v0.1.pt\"", ",", "\n", "\"nl-v0-backward\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-nl-large-backward-v0.1.pt\"", ",", "\n", "# Norwegian", "\n", "\"no-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-no-opus-large-forward-v0.1.pt\"", ",", "\n", "\"no-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-no-opus-large-backward-v0.1.pt\"", ",", "\n", "# Polish", "\n", "\"pl-forward\"", ":", "f\"{aws_path}/embeddings/lm-polish-forward-v0.2.pt\"", ",", "\n", "\"pl-backward\"", ":", "f\"{aws_path}/embeddings/lm-polish-backward-v0.2.pt\"", ",", "\n", "\"pl-opus-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-pl-opus-large-forward-v0.1.pt\"", ",", "\n", "\"pl-opus-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-pl-opus-large-backward-v0.1.pt\"", ",", "\n", "# Portuguese", "\n", "\"pt-forward\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-pt-forward.pt\"", ",", "\n", "\"pt-backward\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-pt-backward.pt\"", ",", "\n", "# Pubmed", "\n", "\"pubmed-forward\"", ":", "f\"{aws_path}/embeddings-v0.4.1/pubmed-2015-fw-lm.pt\"", ",", "\n", "\"pubmed-backward\"", ":", "f\"{aws_path}/embeddings-v0.4.1/pubmed-2015-bw-lm.pt\"", ",", "\n", "# Slovenian", "\n", "\"sl-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-sl-opus-large-forward-v0.1.pt\"", ",", "\n", "\"sl-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-sl-opus-large-backward-v0.1.pt\"", ",", "\n", "\"sl-v0-forward\"", ":", "f\"{aws_path}/embeddings-v0.3/lm-sl-large-forward-v0.1.pt\"", ",", "\n", "\"sl-v0-backward\"", ":", "f\"{aws_path}/embeddings-v0.3/lm-sl-large-backward-v0.1.pt\"", ",", "\n", "# Swedish", "\n", "\"sv-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-sv-opus-large-forward-v0.1.pt\"", ",", "\n", "\"sv-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-sv-opus-large-backward-v0.1.pt\"", ",", "\n", "\"sv-v0-forward\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-sv-large-forward-v0.1.pt\"", ",", "\n", "\"sv-v0-backward\"", ":", "f\"{aws_path}/embeddings-v0.4/lm-sv-large-backward-v0.1.pt\"", ",", "\n", "# Tamil", "\n", "\"ta-forward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-ta-opus-large-forward-v0.1.pt\"", ",", "\n", "\"ta-backward\"", ":", "f\"{aws_path}/embeddings-stefan-it/lm-ta-opus-large-backward-v0.1.pt\"", ",", "\n", "}", "\n", "\n", "if", "type", "(", "model", ")", "==", "str", ":", "\n", "\n", "# load rpbert if in pretrained rpbert map", "\n", "            ", "if", "model", ".", "lower", "(", ")", "in", "self", ".", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "                ", "base_path", "=", "self", ".", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "model", ".", "lower", "(", ")", "]", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "", "elif", "replace_with_language_code", "(", "model", ")", "in", "self", ".", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "                ", "base_path", "=", "self", ".", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "\n", "replace_with_language_code", "(", "model", ")", "\n", "]", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "", "elif", "not", "Path", "(", "model", ")", ".", "exists", "(", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f'The given rpbert \"{model}\" is not available or is not a valid path.'", "\n", ")", "\n", "\n", "", "", "from", "flair", ".", "models", "import", "LanguageModel", "\n", "\n", "if", "type", "(", "model", ")", "==", "LanguageModel", ":", "\n", "            ", "self", ".", "lm", ":", "LanguageModel", "=", "model", "\n", "self", ".", "name", "=", "f\"Task-LSTM-{self.lm.hidden_size}-{self.lm.nlayers}-{self.lm.is_forward_lm}\"", "\n", "", "else", ":", "\n", "            ", "self", ".", "lm", ":", "LanguageModel", "=", "LanguageModel", ".", "load_language_model", "(", "model", ")", "\n", "self", ".", "name", "=", "str", "(", "model", ")", "\n", "\n", "# embeddings are static if we don't do finetuning", "\n", "", "self", ".", "fine_tune", "=", "fine_tune", "\n", "self", ".", "static_embeddings", "=", "not", "fine_tune", "\n", "\n", "self", ".", "is_forward_lm", ":", "bool", "=", "self", ".", "lm", ".", "is_forward_lm", "\n", "self", ".", "chars_per_chunk", ":", "int", "=", "chars_per_chunk", "\n", "\n", "# embed a dummy sentence to determine embedding_length", "\n", "dummy_sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "dummy_sentence", ".", "add_token", "(", "Token", "(", "\"hello\"", ")", ")", "\n", "embedded_dummy", "=", "self", ".", "embed", "(", "dummy_sentence", ")", "\n", "self", ".", "__embedding_length", ":", "int", "=", "len", "(", "\n", "embedded_dummy", "[", "0", "]", ".", "get_token", "(", "1", ")", ".", "get_embedding", "(", ")", "\n", ")", "\n", "\n", "# set to eval mode", "\n", "self", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.FlairEmbeddings.train": [[2030, 2042], ["super().train"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train"], ["", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "\n", "# make compatible with serialized models (TODO: remove)", "\n", "        ", "if", "\"fine_tune\"", "not", "in", "self", ".", "__dict__", ":", "\n", "            ", "self", ".", "fine_tune", "=", "False", "\n", "", "if", "\"chars_per_chunk\"", "not", "in", "self", ".", "__dict__", ":", "\n", "            ", "self", ".", "chars_per_chunk", "=", "512", "\n", "\n", "", "if", "not", "self", ".", "fine_tune", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "super", "(", "FlairEmbeddings", ",", "self", ")", ".", "train", "(", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.FlairEmbeddings.embedding_length": [[2043, 2046], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.FlairEmbeddings._add_embeddings_internal": [[2047, 2101], ["torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "embeddings.FlairEmbeddings.lm.get_representation", "enumerate", "sentence.to_tokenized_string", "all_hidden_states_in_lm.detach.detach.detach", "sentence.to_tokenized_string", "len", "len", "len", "len", "len", "token.set_embedding", "embedding.clone.clone.clone"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.get_representation", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "\n", "# gradients are enable if fine-tuning is enabled", "\n", "        ", "gradient_context", "=", "torch", ".", "enable_grad", "(", ")", "if", "self", ".", "fine_tune", "else", "torch", ".", "no_grad", "(", ")", "\n", "\n", "with", "gradient_context", ":", "\n", "\n", "# if this is not possible, use LM to generate embedding. First, get text sentences", "\n", "            ", "text_sentences", "=", "[", "sentence", ".", "to_tokenized_string", "(", ")", "for", "sentence", "in", "sentences", "]", "\n", "\n", "start_marker", "=", "\"\\n\"", "\n", "end_marker", "=", "\" \"", "\n", "\n", "# get hidden states from language rpbert", "\n", "all_hidden_states_in_lm", "=", "self", ".", "lm", ".", "get_representation", "(", "\n", "text_sentences", ",", "start_marker", ",", "end_marker", ",", "self", ".", "chars_per_chunk", "\n", ")", "\n", "\n", "if", "not", "self", ".", "fine_tune", ":", "\n", "                ", "all_hidden_states_in_lm", "=", "all_hidden_states_in_lm", ".", "detach", "(", ")", "\n", "\n", "# take first or last hidden states from language rpbert as word representation", "\n", "", "for", "i", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "                ", "sentence_text", "=", "sentence", ".", "to_tokenized_string", "(", ")", "\n", "\n", "offset_forward", ":", "int", "=", "len", "(", "start_marker", ")", "\n", "offset_backward", ":", "int", "=", "len", "(", "sentence_text", ")", "+", "len", "(", "start_marker", ")", "\n", "\n", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "\n", "                    ", "offset_forward", "+=", "len", "(", "token", ".", "text", ")", "\n", "\n", "if", "self", ".", "is_forward_lm", ":", "\n", "                        ", "offset", "=", "offset_forward", "\n", "", "else", ":", "\n", "                        ", "offset", "=", "offset_backward", "\n", "\n", "", "embedding", "=", "all_hidden_states_in_lm", "[", "offset", ",", "i", ",", ":", "]", "\n", "\n", "# if self.tokenized_lm or token.whitespace_after:", "\n", "offset_forward", "+=", "1", "\n", "offset_backward", "-=", "1", "\n", "\n", "offset_backward", "-=", "len", "(", "token", ".", "text", ")", "\n", "\n", "# only clone if optimization mode is 'gpu'", "\n", "if", "flair", ".", "embedding_storage_mode", "==", "\"gpu\"", ":", "\n", "                        ", "embedding", "=", "embedding", ".", "clone", "(", ")", "\n", "\n", "", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "embedding", ")", "\n", "\n", "", "", "del", "all_hidden_states_in_lm", "\n", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.FlairEmbeddings.__str__": [[2102, 2104], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.PooledFlairEmbeddings.__init__": [[2107, 2149], ["super().__init__", "type", "embeddings.FlairEmbeddings"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "contextual_embeddings", ":", "Union", "[", "str", ",", "FlairEmbeddings", "]", ",", "\n", "pooling", ":", "str", "=", "\"min\"", ",", "\n", "only_capitalized", ":", "bool", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# use the character language rpbert embeddings as basis", "\n", "if", "type", "(", "contextual_embeddings", ")", "is", "str", ":", "\n", "            ", "self", ".", "context_embeddings", ":", "FlairEmbeddings", "=", "FlairEmbeddings", "(", "\n", "contextual_embeddings", ",", "**", "kwargs", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "context_embeddings", ":", "FlairEmbeddings", "=", "contextual_embeddings", "\n", "\n", "# length is twice the original character LM embedding length", "\n", "", "self", ".", "embedding_length", "=", "self", ".", "context_embeddings", ".", "embedding_length", "*", "2", "\n", "self", ".", "name", "=", "self", ".", "context_embeddings", ".", "name", "+", "\"-context\"", "\n", "\n", "# these fields are for the embedding memory", "\n", "self", ".", "word_embeddings", "=", "{", "}", "\n", "self", ".", "word_count", "=", "{", "}", "\n", "\n", "# whether to add only capitalized words to memory (faster runtime and lower memory consumption)", "\n", "self", ".", "only_capitalized", "=", "only_capitalized", "\n", "\n", "# we re-compute embeddings dynamically at each epoch", "\n", "self", ".", "static_embeddings", "=", "False", "\n", "\n", "# set the memory method", "\n", "self", ".", "pooling", "=", "pooling", "\n", "if", "pooling", "==", "\"mean\"", ":", "\n", "            ", "self", ".", "aggregate_op", "=", "torch", ".", "add", "\n", "", "elif", "pooling", "==", "\"fade\"", ":", "\n", "            ", "self", ".", "aggregate_op", "=", "torch", ".", "add", "\n", "", "elif", "pooling", "==", "\"max\"", ":", "\n", "            ", "self", ".", "aggregate_op", "=", "torch", ".", "max", "\n", "", "elif", "pooling", "==", "\"min\"", ":", "\n", "            ", "self", ".", "aggregate_op", "=", "torch", ".", "min", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.PooledFlairEmbeddings.train": [[2150, 2157], ["super().train", "print"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train"], ["", "", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "train", "(", "mode", "=", "mode", ")", "\n", "if", "mode", ":", "\n", "# memory is wiped each time we do a training run", "\n", "            ", "print", "(", "\"train mode resetting embeddings\"", ")", "\n", "self", ".", "word_embeddings", "=", "{", "}", "\n", "self", ".", "word_count", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.PooledFlairEmbeddings._add_embeddings_internal": [[2158, 2200], ["embeddings.PooledFlairEmbeddings.context_embeddings.embed", "token._embeddings[].cpu", "token.set_embedding", "token.text[].isupper", "embeddings.PooledFlairEmbeddings.aggregate_op"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding"], ["", "", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "\n", "        ", "self", ".", "context_embeddings", ".", "embed", "(", "sentences", ")", "\n", "\n", "# if we keep a pooling, it needs to be updated continuously", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "\n", "# update embedding", "\n", "                ", "local_embedding", "=", "token", ".", "_embeddings", "[", "self", ".", "context_embeddings", ".", "name", "]", ".", "cpu", "(", ")", "\n", "\n", "# check token.text is empty or not", "\n", "if", "token", ".", "text", ":", "\n", "                    ", "if", "token", ".", "text", "[", "0", "]", ".", "isupper", "(", ")", "or", "not", "self", ".", "only_capitalized", ":", "\n", "\n", "                        ", "if", "token", ".", "text", "not", "in", "self", ".", "word_embeddings", ":", "\n", "                            ", "self", ".", "word_embeddings", "[", "token", ".", "text", "]", "=", "local_embedding", "\n", "self", ".", "word_count", "[", "token", ".", "text", "]", "=", "1", "\n", "", "else", ":", "\n", "                            ", "aggregated_embedding", "=", "self", ".", "aggregate_op", "(", "\n", "self", ".", "word_embeddings", "[", "token", ".", "text", "]", ",", "local_embedding", "\n", ")", "\n", "if", "self", ".", "pooling", "==", "\"fade\"", ":", "\n", "                                ", "aggregated_embedding", "/=", "2", "\n", "", "self", ".", "word_embeddings", "[", "token", ".", "text", "]", "=", "aggregated_embedding", "\n", "self", ".", "word_count", "[", "token", ".", "text", "]", "+=", "1", "\n", "\n", "# add embeddings after updating", "\n", "", "", "", "", "", "for", "sentence", "in", "sentences", ":", "\n", "            ", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "                ", "if", "token", ".", "text", "in", "self", ".", "word_embeddings", ":", "\n", "                    ", "base", "=", "(", "\n", "self", ".", "word_embeddings", "[", "token", ".", "text", "]", "/", "self", ".", "word_count", "[", "token", ".", "text", "]", "\n", "if", "self", ".", "pooling", "==", "\"mean\"", "\n", "else", "self", ".", "word_embeddings", "[", "token", ".", "text", "]", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "base", "=", "token", ".", "_embeddings", "[", "self", ".", "context_embeddings", ".", "name", "]", "\n", "\n", "", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "base", ")", "\n", "\n", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.PooledFlairEmbeddings.embedding_length": [[2201, 2203], ["None"], "methods", ["None"], ["", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.PooledFlairEmbeddings.__setstate__": [[2204, 2210], ["embeddings.PooledFlairEmbeddings.word_embeddings[].cpu"], "methods", ["None"], ["", "def", "__setstate__", "(", "self", ",", "d", ")", ":", "\n", "        ", "self", ".", "__dict__", "=", "d", "\n", "\n", "if", "flair", ".", "device", "!=", "'cpu'", ":", "\n", "            ", "for", "key", "in", "self", ".", "word_embeddings", ":", "\n", "                ", "self", ".", "word_embeddings", "[", "key", "]", "=", "self", ".", "word_embeddings", "[", "key", "]", ".", "cpu", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerWordEmbeddings.__init__": [[2213, 2268], ["super().__init__", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_pretrained", "str", "embeddings.TransformerWordEmbeddings.model.eval", "embeddings.TransformerWordEmbeddings.model.to", "embeddings.TransformerWordEmbeddings.special_tokens.append", "embeddings.TransformerWordEmbeddings.special_tokens.append", "isinstance", "isinstance", "isinstance", "int", "layers.split"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "str", "=", "\"bert-base-uncased\"", ",", "\n", "layers", ":", "str", "=", "\"-1,-2,-3,-4\"", ",", "\n", "pooling_operation", ":", "str", "=", "\"first\"", ",", "\n", "batch_size", ":", "int", "=", "1", ",", "\n", "use_scalar_mix", ":", "bool", "=", "False", ",", "\n", "fine_tune", ":", "bool", "=", "False", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Bidirectional transformer embeddings of words from various transformer architectures.\n        :param model: name of transformer rpbert (see https://huggingface.co/transformers/pretrained_models.html for\n        options)\n        :param layers: string indicating which layers to take for embedding (-1 is topmost layer)\n        :param pooling_operation: how to get from token piece embeddings to token embedding. Either take the first\n        subtoken ('first'), the last subtoken ('last'), both first and last ('first_last') or a mean over all ('mean')\n        :param batch_size: How many sentence to push through transformer at once. Set to 1 by default since transformer\n        models tend to be huge.\n        :param use_scalar_mix: If True, uses a scalar mix of layers as embedding\n        :param fine_tune: If True, allows transformers to be fine-tuned during training\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# load tokenizer and transformer rpbert", "\n", "self", ".", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model", ")", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model", ",", "output_hidden_states", "=", "True", ")", "\n", "self", ".", "model", "=", "AutoModel", ".", "from_pretrained", "(", "model", ",", "config", "=", "config", ")", "\n", "\n", "# rpbert name", "\n", "self", ".", "name", "=", "str", "(", "model", ")", "\n", "\n", "# when initializing, embeddings are in eval mode by default", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "# embedding parameters", "\n", "self", ".", "layer_indexes", "=", "[", "int", "(", "x", ")", "for", "x", "in", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "self", ".", "pooling_operation", "=", "pooling_operation", "\n", "self", ".", "use_scalar_mix", "=", "use_scalar_mix", "\n", "self", ".", "fine_tune", "=", "fine_tune", "\n", "self", ".", "static_embeddings", "=", "not", "self", ".", "fine_tune", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "\n", "self", ".", "special_tokens", "=", "[", "]", "\n", "self", ".", "special_tokens", ".", "append", "(", "self", ".", "tokenizer", ".", "bos_token", ")", "\n", "self", ".", "special_tokens", ".", "append", "(", "self", ".", "tokenizer", ".", "cls_token", ")", "\n", "\n", "# most models have an intial BOS token, except for XLNet, T5 and GPT2", "\n", "self", ".", "begin_offset", "=", "1", "\n", "if", "isinstance", "(", "self", ".", "tokenizer", ",", "XLNetTokenizer", ")", ":", "\n", "            ", "self", ".", "begin_offset", "=", "0", "\n", "", "if", "isinstance", "(", "self", ".", "tokenizer", ",", "T5Tokenizer", ")", ":", "\n", "            ", "self", ".", "begin_offset", "=", "0", "\n", "", "if", "isinstance", "(", "self", ".", "tokenizer", ",", "GPT2Tokenizer", ")", ":", "\n", "            ", "self", ".", "begin_offset", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerWordEmbeddings._add_embeddings_internal": [[2269, 2281], ["embeddings.TransformerWordEmbeddings._add_embeddings_to_sentences", "range", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerDocumentEmbeddings._add_embeddings_to_sentences"], ["", "", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "\"\"\"Add embeddings to all words in a list of sentences.\"\"\"", "\n", "\n", "# split into micro batches of size self.batch_size before pushing through transformer", "\n", "sentence_batches", "=", "[", "sentences", "[", "i", "*", "self", ".", "batch_size", ":", "(", "i", "+", "1", ")", "*", "self", ".", "batch_size", "]", "\n", "for", "i", "in", "range", "(", "(", "len", "(", "sentences", ")", "+", "self", ".", "batch_size", "-", "1", ")", "//", "self", ".", "batch_size", ")", "]", "\n", "\n", "# embed each micro-batch", "\n", "for", "batch", "in", "sentence_batches", ":", "\n", "            ", "self", ".", "_add_embeddings_to_sentences", "(", "batch", ")", "\n", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerWordEmbeddings._add_embeddings_to_sentences": [[2282, 2411], ["len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "embeddings.TransformerWordEmbeddings.tokenizer.encode", "subtokenized_sentences.append", "embeddings.TransformerWordEmbeddings.tokenizer.convert_ids_to_tokens", "iter", "next", "enumerate", "subtokenized_sentences_token_lengths.append", "max", "len", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "embeddings.TransformerWordEmbeddings.model", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "sentence.to_tokenized_string", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "re.sub", "re.sub", "re.sub", "re.sub", "len", "len", "zip", "enumerate", "reconstructed_token.lower", "next.text.lower", "token_subtoken_lengths.append", "zip", "next.set_embedding", "len", "len", "next", "subtoken_embeddings.append", "embeddings.ScalarMix", "ScalarMix.", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "embedding.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.convert_ids_to_tokens", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding"], ["", "def", "_add_embeddings_to_sentences", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", ":", "\n", "        ", "\"\"\"Match subtokenization to Flair tokenization and extract embeddings from transformers for each token.\"\"\"", "\n", "\n", "# first, subtokenize each sentence and find out into how many subtokens each token was divided", "\n", "subtokenized_sentences", "=", "[", "]", "\n", "subtokenized_sentences_token_lengths", "=", "[", "]", "\n", "\n", "for", "sentence", "in", "sentences", ":", "\n", "\n", "# subtokenize sentence", "\n", "            ", "subtokenized_sentence", "=", "self", ".", "tokenizer", ".", "encode", "(", "sentence", ".", "to_tokenized_string", "(", ")", ",", "add_special_tokens", "=", "True", ")", "\n", "subtokenized_sentences", ".", "append", "(", "torch", ".", "tensor", "(", "subtokenized_sentence", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "subtokens", "=", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "subtokenized_sentence", ")", "\n", "\n", "word_iterator", "=", "iter", "(", "sentence", ")", "\n", "token", "=", "next", "(", "word_iterator", ")", "\n", "\n", "token_subtoken_lengths", "=", "[", "]", "\n", "reconstructed_token", "=", "''", "\n", "subtoken_count", "=", "0", "\n", "\n", "# iterate over subtokens and reconstruct tokens", "\n", "for", "subtoken_id", ",", "subtoken", "in", "enumerate", "(", "subtokens", ")", ":", "\n", "\n", "                ", "subtoken_count", "+=", "1", "\n", "\n", "# remove special markup", "\n", "subtoken", "=", "re", ".", "sub", "(", "'^\u0120'", ",", "''", ",", "subtoken", ")", "# RoBERTa models", "\n", "subtoken", "=", "re", ".", "sub", "(", "'^##'", ",", "''", ",", "subtoken", ")", "# BERT models", "\n", "subtoken", "=", "re", ".", "sub", "(", "'^\u2581'", ",", "''", ",", "subtoken", ")", "# XLNet models", "\n", "subtoken", "=", "re", ".", "sub", "(", "'</w>$'", ",", "''", ",", "subtoken", ")", "# XLM models", "\n", "\n", "# append subtoken to reconstruct token", "\n", "reconstructed_token", "=", "reconstructed_token", "+", "subtoken", "\n", "\n", "# check if reconstructed token is special begin token ([CLS] or similar)", "\n", "if", "reconstructed_token", "in", "self", ".", "special_tokens", "and", "subtoken_id", "==", "0", ":", "\n", "                    ", "reconstructed_token", "=", "''", "\n", "subtoken_count", "=", "0", "\n", "\n", "# check if reconstructed token is the same as current token", "\n", "", "if", "reconstructed_token", ".", "lower", "(", ")", "==", "token", ".", "text", ".", "lower", "(", ")", ":", "\n", "\n", "# if so, add subtoken count", "\n", "                    ", "token_subtoken_lengths", ".", "append", "(", "subtoken_count", ")", "\n", "\n", "# reset subtoken count and reconstructed token", "\n", "reconstructed_token", "=", "''", "\n", "subtoken_count", "=", "0", "\n", "\n", "# break from loop if all tokens are accounted for", "\n", "if", "len", "(", "token_subtoken_lengths", ")", "<", "len", "(", "sentence", ")", ":", "\n", "                        ", "token", "=", "next", "(", "word_iterator", ")", "\n", "", "else", ":", "\n", "                        ", "break", "\n", "\n", "", "", "", "subtokenized_sentences_token_lengths", ".", "append", "(", "token_subtoken_lengths", ")", "\n", "\n", "# find longest sentence in batch", "\n", "", "longest_sequence_in_batch", ":", "int", "=", "len", "(", "max", "(", "subtokenized_sentences", ",", "key", "=", "len", ")", ")", "\n", "\n", "# initialize batch tensors and mask", "\n", "input_ids", "=", "torch", ".", "zeros", "(", "\n", "[", "len", "(", "sentences", ")", ",", "longest_sequence_in_batch", "]", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "flair", ".", "device", ",", "\n", ")", "\n", "mask", "=", "torch", ".", "zeros", "(", "\n", "[", "len", "(", "sentences", ")", ",", "longest_sequence_in_batch", "]", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "flair", ".", "device", ",", "\n", ")", "\n", "for", "s_id", ",", "sentence", "in", "enumerate", "(", "subtokenized_sentences", ")", ":", "\n", "            ", "sequence_length", "=", "len", "(", "sentence", ")", "\n", "input_ids", "[", "s_id", "]", "[", ":", "sequence_length", "]", "=", "sentence", "\n", "mask", "[", "s_id", "]", "[", ":", "sequence_length", "]", "=", "torch", ".", "ones", "(", "sequence_length", ")", "\n", "\n", "# put encoded batch through transformer rpbert to get all hidden states of all encoder layers", "\n", "", "hidden_states", "=", "self", ".", "model", "(", "input_ids", ",", "attention_mask", "=", "mask", ")", "[", "-", "1", "]", "\n", "\n", "# gradients are enabled if fine-tuning is enabled", "\n", "gradient_context", "=", "torch", ".", "enable_grad", "(", ")", "if", "(", "self", ".", "fine_tune", "and", "self", ".", "training", ")", "else", "torch", ".", "no_grad", "(", ")", "\n", "\n", "with", "gradient_context", ":", "\n", "\n", "# iterate over all subtokenized sentences", "\n", "            ", "for", "sentence_idx", ",", "(", "sentence", ",", "subtoken_lengths", ")", "in", "enumerate", "(", "zip", "(", "sentences", ",", "subtokenized_sentences_token_lengths", ")", ")", ":", "\n", "\n", "                ", "subword_start_idx", "=", "self", ".", "begin_offset", "\n", "\n", "# for each token, get embedding", "\n", "for", "token_idx", ",", "(", "token", ",", "number_of_subtokens", ")", "in", "enumerate", "(", "zip", "(", "sentence", ",", "subtoken_lengths", ")", ")", ":", "\n", "\n", "                    ", "subword_end_idx", "=", "subword_start_idx", "+", "number_of_subtokens", "\n", "\n", "subtoken_embeddings", ":", "List", "[", "torch", ".", "FloatTensor", "]", "=", "[", "]", "\n", "\n", "# get states from all selected layers, aggregate with pooling operation", "\n", "for", "layer", "in", "self", ".", "layer_indexes", ":", "\n", "                        ", "current_embeddings", "=", "hidden_states", "[", "layer", "]", "[", "sentence_idx", "]", "[", "subword_start_idx", ":", "subword_end_idx", "]", "\n", "\n", "if", "self", ".", "pooling_operation", "==", "\"first\"", ":", "\n", "                            ", "final_embedding", ":", "torch", ".", "FloatTensor", "=", "current_embeddings", "[", "0", "]", "\n", "\n", "", "if", "self", ".", "pooling_operation", "==", "\"last\"", ":", "\n", "                            ", "final_embedding", ":", "torch", ".", "FloatTensor", "=", "current_embeddings", "[", "-", "1", "]", "\n", "\n", "", "if", "self", ".", "pooling_operation", "==", "\"first_last\"", ":", "\n", "                            ", "final_embedding", ":", "torch", ".", "Tensor", "=", "torch", ".", "cat", "(", "[", "current_embeddings", "[", "0", "]", ",", "current_embeddings", "[", "-", "1", "]", "]", ")", "\n", "\n", "", "if", "self", ".", "pooling_operation", "==", "\"mean\"", ":", "\n", "                            ", "all_embeddings", ":", "List", "[", "torch", ".", "FloatTensor", "]", "=", "[", "\n", "embedding", ".", "unsqueeze", "(", "0", ")", "for", "embedding", "in", "current_embeddings", "\n", "]", "\n", "final_embedding", ":", "torch", ".", "Tensor", "=", "torch", ".", "mean", "(", "torch", ".", "cat", "(", "all_embeddings", ",", "dim", "=", "0", ")", ",", "dim", "=", "0", ")", "\n", "\n", "", "subtoken_embeddings", ".", "append", "(", "final_embedding", ")", "\n", "\n", "# use scalar mix of embeddings if so selected", "\n", "", "if", "self", ".", "use_scalar_mix", ":", "\n", "                        ", "sm", "=", "ScalarMix", "(", "mixture_size", "=", "len", "(", "subtoken_embeddings", ")", ")", "\n", "sm_embeddings", "=", "sm", "(", "subtoken_embeddings", ")", "\n", "\n", "subtoken_embeddings", "=", "[", "sm_embeddings", "]", "\n", "\n", "# set the extracted embedding for the token", "\n", "", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "torch", ".", "cat", "(", "subtoken_embeddings", ")", ")", "\n", "\n", "subword_start_idx", "+=", "number_of_subtokens", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerWordEmbeddings.embedding_length": [[2412, 2425], ["len"], "methods", ["None"], ["", "", "", "", "@", "property", "\n", "@", "abstractmethod", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Returns the length of the embedding vector.\"\"\"", "\n", "\n", "if", "not", "self", ".", "use_scalar_mix", ":", "\n", "            ", "length", "=", "len", "(", "self", ".", "layer_indexes", ")", "*", "self", ".", "model", ".", "config", ".", "hidden_size", "\n", "", "else", ":", "\n", "            ", "length", "=", "self", ".", "model", ".", "config", ".", "hidden_size", "\n", "\n", "", "if", "self", ".", "pooling_operation", "==", "'first_last'", ":", "length", "*=", "2", "\n", "\n", "return", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerDocumentEmbeddings.__init__": [[2428, 2471], ["super().__init__", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_pretrained", "str", "embeddings.TransformerDocumentEmbeddings.model.eval", "embeddings.TransformerDocumentEmbeddings.model.to", "int", "isinstance", "isinstance", "layers.split"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "str", "=", "\"bert-base-uncased\"", ",", "\n", "fine_tune", ":", "bool", "=", "True", ",", "\n", "batch_size", ":", "int", "=", "1", ",", "\n", "layers", ":", "str", "=", "\"-1\"", ",", "\n", "use_scalar_mix", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Bidirectional transformer embeddings of words from various transformer architectures.\n        :param model: name of transformer rpbert (see https://huggingface.co/transformers/pretrained_models.html for\n        options)\n        :param fine_tune: If True, allows transformers to be fine-tuned during training\n        :param batch_size: How many sentence to push through transformer at once. Set to 1 by default since transformer\n        models tend to be huge.\n        :param layers: string indicating which layers to take for embedding (-1 is topmost layer)\n        :param use_scalar_mix: If True, uses a scalar mix of layers as embedding\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# load tokenizer and transformer rpbert", "\n", "self", ".", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model", ")", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model", ",", "output_hidden_states", "=", "True", ")", "\n", "self", ".", "model", "=", "AutoModel", ".", "from_pretrained", "(", "model", ",", "config", "=", "config", ")", "\n", "\n", "# rpbert name", "\n", "self", ".", "name", "=", "str", "(", "model", ")", "\n", "\n", "# when initializing, embeddings are in eval mode by default", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "# embedding parameters", "\n", "self", ".", "layer_indexes", "=", "[", "int", "(", "x", ")", "for", "x", "in", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "self", ".", "use_scalar_mix", "=", "use_scalar_mix", "\n", "self", ".", "fine_tune", "=", "fine_tune", "\n", "self", ".", "static_embeddings", "=", "not", "self", ".", "fine_tune", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "\n", "# most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial", "\n", "self", ".", "initial_cls_token", ":", "bool", "=", "False", "\n", "if", "isinstance", "(", "self", ".", "tokenizer", ",", "BertTokenizer", ")", "or", "isinstance", "(", "self", ".", "tokenizer", ",", "AlbertTokenizer", ")", ":", "\n", "            ", "self", ".", "initial_cls_token", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerDocumentEmbeddings._add_embeddings_internal": [[2472, 2484], ["embeddings.TransformerDocumentEmbeddings._add_embeddings_to_sentences", "range", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerDocumentEmbeddings._add_embeddings_to_sentences"], ["", "", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "\"\"\"Add embeddings to all words in a list of sentences.\"\"\"", "\n", "\n", "# using list comprehension", "\n", "sentence_batches", "=", "[", "sentences", "[", "i", "*", "self", ".", "batch_size", ":", "(", "i", "+", "1", ")", "*", "self", ".", "batch_size", "]", "\n", "for", "i", "in", "range", "(", "(", "len", "(", "sentences", ")", "+", "self", ".", "batch_size", "-", "1", ")", "//", "self", ".", "batch_size", ")", "]", "\n", "\n", "for", "batch", "in", "sentence_batches", ":", "\n", "\n", "            ", "self", ".", "_add_embeddings_to_sentences", "(", "batch", ")", "\n", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerDocumentEmbeddings._add_embeddings_to_sentences": [[2485, 2545], ["torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "enumerate", "embeddings.TransformerDocumentEmbeddings.tokenizer.encode", "subtokenized_sentences.append", "max", "len", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "zip", "sentence.set_embedding", "sentence.to_tokenized_string", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len", "len", "embeddings.TransformerDocumentEmbeddings.model", "embeddings.TransformerDocumentEmbeddings.model", "embeddings.ScalarMix", "ScalarMix.", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_gpt2.GPT2Tokenizer.encode", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string"], ["", "def", "_add_embeddings_to_sentences", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", ":", "\n", "        ", "\"\"\"Extract sentence embedding from CLS token or similar and add to Sentence object.\"\"\"", "\n", "\n", "# gradients are enabled if fine-tuning is enabled", "\n", "gradient_context", "=", "torch", ".", "enable_grad", "(", ")", "if", "(", "self", ".", "fine_tune", "and", "self", ".", "training", ")", "else", "torch", ".", "no_grad", "(", ")", "\n", "\n", "with", "gradient_context", ":", "\n", "\n", "# first, subtokenize each sentence and find out into how many subtokens each token was divided", "\n", "            ", "subtokenized_sentences", "=", "[", "]", "\n", "\n", "# subtokenize sentences", "\n", "for", "sentence", "in", "sentences", ":", "\n", "# tokenize and truncate to 512 subtokens (TODO: check better truncation strategies)", "\n", "                ", "subtokenized_sentence", "=", "self", ".", "tokenizer", ".", "encode", "(", "sentence", ".", "to_tokenized_string", "(", ")", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "max_length", "=", "512", ")", "\n", "subtokenized_sentences", ".", "append", "(", "\n", "torch", ".", "tensor", "(", "subtokenized_sentence", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "flair", ".", "device", ")", ")", "\n", "\n", "# find longest sentence in batch", "\n", "", "longest_sequence_in_batch", ":", "int", "=", "len", "(", "max", "(", "subtokenized_sentences", ",", "key", "=", "len", ")", ")", "\n", "\n", "# initialize batch tensors and mask", "\n", "input_ids", "=", "torch", ".", "zeros", "(", "\n", "[", "len", "(", "sentences", ")", ",", "longest_sequence_in_batch", "]", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "flair", ".", "device", ",", "\n", ")", "\n", "mask", "=", "torch", ".", "zeros", "(", "\n", "[", "len", "(", "sentences", ")", ",", "longest_sequence_in_batch", "]", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "flair", ".", "device", ",", "\n", ")", "\n", "for", "s_id", ",", "sentence", "in", "enumerate", "(", "subtokenized_sentences", ")", ":", "\n", "                ", "sequence_length", "=", "len", "(", "sentence", ")", "\n", "input_ids", "[", "s_id", "]", "[", ":", "sequence_length", "]", "=", "sentence", "\n", "mask", "[", "s_id", "]", "[", ":", "sequence_length", "]", "=", "torch", ".", "ones", "(", "sequence_length", ")", "\n", "\n", "# put encoded batch through transformer rpbert to get all hidden states of all encoder layers", "\n", "", "hidden_states", "=", "self", ".", "model", "(", "input_ids", ",", "attention_mask", "=", "mask", ")", "[", "-", "1", "]", "if", "len", "(", "sentences", ")", ">", "1", "else", "self", ".", "model", "(", "input_ids", ")", "[", "-", "1", "]", "\n", "\n", "# iterate over all subtokenized sentences", "\n", "for", "sentence_idx", ",", "(", "sentence", ",", "subtokens", ")", "in", "enumerate", "(", "zip", "(", "sentences", ",", "subtokenized_sentences", ")", ")", ":", "\n", "\n", "                ", "index_of_CLS_token", "=", "0", "if", "self", ".", "initial_cls_token", "else", "len", "(", "subtokens", ")", "-", "1", "\n", "\n", "cls_embeddings_all_layers", ":", "List", "[", "torch", ".", "FloatTensor", "]", "=", "[", "hidden_states", "[", "layer", "]", "[", "sentence_idx", "]", "[", "index_of_CLS_token", "]", "for", "layer", "in", "self", ".", "layer_indexes", "]", "\n", "\n", "# use scalar mix of embeddings if so selected", "\n", "if", "self", ".", "use_scalar_mix", ":", "\n", "                    ", "sm", "=", "ScalarMix", "(", "mixture_size", "=", "len", "(", "cls_embeddings_all_layers", ")", ")", "\n", "sm_embeddings", "=", "sm", "(", "cls_embeddings_all_layers", ")", "\n", "\n", "cls_embeddings_all_layers", "=", "[", "sm_embeddings", "]", "\n", "\n", "# set the extracted embedding for the token", "\n", "", "sentence", ".", "set_embedding", "(", "self", ".", "name", ",", "torch", ".", "cat", "(", "cls_embeddings_all_layers", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.TransformerDocumentEmbeddings.embedding_length": [[2546, 2554], ["len"], "methods", ["None"], ["", "", "", "@", "property", "\n", "@", "abstractmethod", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Returns the length of the embedding vector.\"\"\"", "\n", "return", "(", "\n", "len", "(", "self", ".", "layer_indexes", ")", "*", "self", ".", "model", ".", "config", ".", "hidden_size", "\n", "if", "not", "self", ".", "use_scalar_mix", "\n", "else", "self", ".", "model", ".", "config", ".", "hidden_size", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BertEmbeddings.__init__": [[2558, 2608], ["super().__init__", "str", "DistilBertTokenizer.from_pretrained", "DistilBertModel.from_pretrained", "int", "transformers.AlbertTokenizer.from_pretrained", "transformers.AlbertModel.from_pretrained", "transformers.BertTokenizer.from_pretrained", "transformers.BertModel.from_pretrained", "layers.split", "log.warning", "log.warning", "log.warning"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "bert_model_or_path", ":", "str", "=", "\"bert-base-uncased\"", ",", "\n", "layers", ":", "str", "=", "\"-1,-2,-3,-4\"", ",", "\n", "pooling_operation", ":", "str", "=", "\"first\"", ",", "\n", "use_scalar_mix", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Bidirectional transformer embeddings of words, as proposed in Devlin et al., 2018.\n        :param bert_model_or_path: name of BERT rpbert ('') or directory path containing custom rpbert, configuration file\n        and vocab file (names of three files should be - config.json, pytorch_model.bin/rpbert.chkpt, vocab.txt)\n        :param layers: string indicating which layers to take for embedding\n        :param pooling_operation: how to get from token piece embeddings to token embedding. Either pool them and take\n        the average ('mean') or use first word piece embedding as token embedding ('first)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "\"distilbert\"", "in", "bert_model_or_path", ":", "\n", "            ", "try", ":", "\n", "                ", "from", "transformers", "import", "DistilBertTokenizer", ",", "DistilBertModel", "\n", "", "except", "ImportError", ":", "\n", "                ", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "log", ".", "warning", "(", "\n", "\"ATTENTION! To use DistilBert, please first install a recent version of transformers!\"", "\n", ")", "\n", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "pass", "\n", "\n", "", "self", ".", "tokenizer", "=", "DistilBertTokenizer", ".", "from_pretrained", "(", "bert_model_or_path", ")", "\n", "self", ".", "model", "=", "DistilBertModel", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "=", "bert_model_or_path", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", ")", "\n", "", "elif", "\"albert\"", "in", "bert_model_or_path", ":", "\n", "            ", "self", ".", "tokenizer", "=", "AlbertTokenizer", ".", "from_pretrained", "(", "bert_model_or_path", ")", "\n", "self", ".", "model", "=", "AlbertModel", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "=", "bert_model_or_path", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_or_path", ")", "\n", "self", ".", "model", "=", "BertModel", ".", "from_pretrained", "(", "\n", "pretrained_model_name_or_path", "=", "bert_model_or_path", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", ")", "\n", "", "self", ".", "layer_indexes", "=", "[", "int", "(", "x", ")", "for", "x", "in", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "self", ".", "pooling_operation", "=", "pooling_operation", "\n", "self", ".", "use_scalar_mix", "=", "use_scalar_mix", "\n", "self", ".", "name", "=", "str", "(", "bert_model_or_path", ")", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BertEmbeddings._convert_sentences_to_features": [[2628, 2681], ["enumerate", "tokens.append", "input_type_ids.append", "tokens.append", "input_type_ids.append", "embeddings.BertEmbeddings.tokenizer.convert_tokens_to_ids", "features.append", "embeddings.BertEmbeddings.tokenizer.tokenize", "bert_tokenization.extend", "len", "len", "tokens.append", "input_type_ids.append", "len", "len", "embeddings.BertEmbeddings.append", "input_mask.append", "input_type_ids.append", "BertEmbeddings.BertInputFeatures"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize"], ["", "", "def", "_convert_sentences_to_features", "(", "\n", "self", ",", "sentences", ",", "max_sequence_length", ":", "int", "\n", ")", "->", "[", "BertInputFeatures", "]", ":", "\n", "\n", "        ", "max_sequence_length", "=", "max_sequence_length", "+", "2", "\n", "\n", "features", ":", "List", "[", "BertEmbeddings", ".", "BertInputFeatures", "]", "=", "[", "]", "\n", "for", "(", "sentence_index", ",", "sentence", ")", "in", "enumerate", "(", "sentences", ")", ":", "\n", "\n", "            ", "bert_tokenization", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "token_subtoken_count", ":", "Dict", "[", "int", ",", "int", "]", "=", "{", "}", "\n", "\n", "for", "token", "in", "sentence", ":", "\n", "                ", "subtokens", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "token", ".", "text", ")", "\n", "bert_tokenization", ".", "extend", "(", "subtokens", ")", "\n", "token_subtoken_count", "[", "token", ".", "idx", "]", "=", "len", "(", "subtokens", ")", "\n", "\n", "", "if", "len", "(", "bert_tokenization", ")", ">", "max_sequence_length", "-", "2", ":", "\n", "                ", "bert_tokenization", "=", "bert_tokenization", "[", "0", ":", "(", "max_sequence_length", "-", "2", ")", "]", "\n", "\n", "", "tokens", "=", "[", "]", "\n", "input_type_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "bert_tokenization", ":", "\n", "                ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "input_ids", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_sequence_length", ":", "\n", "                ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "BertEmbeddings", ".", "BertInputFeatures", "(", "\n", "unique_id", "=", "sentence_index", ",", "\n", "tokens", "=", "tokens", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "input_type_ids", "=", "input_type_ids", ",", "\n", "token_subtoken_count", "=", "token_subtoken_count", ",", "\n", ")", "\n", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BertEmbeddings._add_embeddings_internal": [[2682, 2762], ["len", "embeddings.BertEmbeddings._convert_sentences_to_features", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "embeddings.BertEmbeddings.model.to", "embeddings.BertEmbeddings.model.eval", "max", "embeddings.BertEmbeddings.model", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "enumerate", "embeddings.BertEmbeddings.tokenizer.tokenize", "subtoken_embeddings.append", "sentence.to_tokenized_string", "all_layers.append", "embeddings.ScalarMix", "ScalarMix.", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "token.set_embedding", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "token.set_embedding", "embedding.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "int"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BertEmbeddings._convert_sentences_to_features", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "\"\"\"Add embeddings to all words in a list of sentences. If embeddings are already added,\n        updates only if embeddings are non-static.\"\"\"", "\n", "\n", "# first, find longest sentence in batch", "\n", "longest_sentence_in_batch", ":", "int", "=", "len", "(", "\n", "max", "(", "\n", "[", "\n", "self", ".", "tokenizer", ".", "tokenize", "(", "sentence", ".", "to_tokenized_string", "(", ")", ")", "\n", "for", "sentence", "in", "sentences", "\n", "]", ",", "\n", "key", "=", "len", ",", "\n", ")", "\n", ")", "\n", "\n", "# prepare id maps for BERT rpbert", "\n", "features", "=", "self", ".", "_convert_sentences_to_features", "(", "\n", "sentences", ",", "longest_sentence_in_batch", "\n", ")", "\n", "all_input_ids", "=", "torch", ".", "LongTensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "features", "]", ")", ".", "to", "(", "\n", "flair", ".", "device", "\n", ")", "\n", "all_input_masks", "=", "torch", ".", "LongTensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "features", "]", ")", ".", "to", "(", "\n", "flair", ".", "device", "\n", ")", "\n", "\n", "# put encoded batch through BERT rpbert to get all hidden states of all encoder layers", "\n", "self", ".", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "all_encoder_layers", "=", "self", ".", "model", "(", "all_input_ids", ",", "attention_mask", "=", "all_input_masks", ")", "[", "\n", "-", "1", "\n", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "            ", "for", "sentence_index", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "\n", "                ", "feature", "=", "features", "[", "sentence_index", "]", "\n", "\n", "# get aggregated embeddings for each BERT-subtoken in sentence", "\n", "subtoken_embeddings", "=", "[", "]", "\n", "for", "token_index", ",", "_", "in", "enumerate", "(", "feature", ".", "tokens", ")", ":", "\n", "                    ", "all_layers", "=", "[", "]", "\n", "for", "layer_index", "in", "self", ".", "layer_indexes", ":", "\n", "                        ", "layer_output", "=", "all_encoder_layers", "[", "int", "(", "layer_index", ")", "]", "[", "\n", "sentence_index", "\n", "]", "\n", "all_layers", ".", "append", "(", "layer_output", "[", "token_index", "]", ")", "\n", "\n", "", "if", "self", ".", "use_scalar_mix", ":", "\n", "                        ", "sm", "=", "ScalarMix", "(", "mixture_size", "=", "len", "(", "all_layers", ")", ")", "\n", "sm_embeddings", "=", "sm", "(", "all_layers", ")", "\n", "all_layers", "=", "[", "sm_embeddings", "]", "\n", "\n", "", "subtoken_embeddings", ".", "append", "(", "torch", ".", "cat", "(", "all_layers", ")", ")", "\n", "\n", "# get the current sentence object", "\n", "", "token_idx", "=", "0", "\n", "for", "token", "in", "sentence", ":", "\n", "# add concatenated embedding to sentence", "\n", "                    ", "token_idx", "+=", "1", "\n", "\n", "if", "self", ".", "pooling_operation", "==", "\"first\"", ":", "\n", "# use first subword embedding if pooling operation is 'first'", "\n", "                        ", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "subtoken_embeddings", "[", "token_idx", "]", ")", "\n", "", "else", ":", "\n", "# otherwise, do a mean over all subwords in token", "\n", "                        ", "embeddings", "=", "subtoken_embeddings", "[", "\n", "token_idx", ":", "token_idx", "\n", "+", "feature", ".", "token_subtoken_count", "[", "token", ".", "idx", "]", "\n", "]", "\n", "embeddings", "=", "[", "\n", "embedding", ".", "unsqueeze", "(", "0", ")", "for", "embedding", "in", "embeddings", "\n", "]", "\n", "mean", "=", "torch", ".", "mean", "(", "torch", ".", "cat", "(", "embeddings", ",", "dim", "=", "0", ")", ",", "dim", "=", "0", ")", "\n", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "mean", ")", "\n", "\n", "", "token_idx", "+=", "feature", ".", "token_subtoken_count", "[", "token", ".", "idx", "]", "-", "1", "\n", "\n", "", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BertEmbeddings.embedding_length": [[2763, 2771], ["len"], "methods", ["None"], ["", "@", "property", "\n", "@", "abstractmethod", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Returns the length of the embedding vector.\"\"\"", "\n", "return", "(", "\n", "len", "(", "self", ".", "layer_indexes", ")", "*", "self", ".", "model", ".", "config", ".", "hidden_size", "\n", "if", "not", "self", ".", "use_scalar_mix", "\n", "else", "self", ".", "model", ".", "config", ".", "hidden_size", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CharLMEmbeddings.__init__": [[2778, 2961], ["deprecated.deprecated.deprecated", "super().__init__", "pathlib.Path", "str", "LanguageModel.load_language_model", "data.Sentence", "dummy_sentence.add_token", "embeddings.CharLMEmbeddings.embed", "len", "embeddings.CharLMEmbeddings.eval", "file_utils.cached_path.lower", "file_utils.cached_path", "SqliteDict", "data.Token", "embedded_dummy[].get_token().get_embedding", "file_utils.cached_path.lower", "file_utils.cached_path", "pathlib.Path", "str", "file_utils.cached_path.lower", "file_utils.cached_path", "embedded_dummy[].get_token", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "pathlib.Path().exists", "ValueError", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.load_language_model", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["@", "deprecated", "(", "version", "=", "\"0.4\"", ",", "reason", "=", "\"Use 'FlairEmbeddings' instead.\"", ")", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "str", ",", "\n", "detach", ":", "bool", "=", "True", ",", "\n", "use_cache", ":", "bool", "=", "False", ",", "\n", "cache_directory", ":", "Path", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        initializes contextual string embeddings using a character-level language rpbert.\n        :param model: rpbert string, one of 'news-forward', 'news-backward', 'news-forward-fast', 'news-backward-fast',\n                'mix-forward', 'mix-backward', 'german-forward', 'german-backward', 'polish-backward', 'polish-forward'\n                depending on which character language rpbert is desired.\n        :param detach: if set to False, the gradient will propagate into the language rpbert. this dramatically slows down\n                training and often leads to worse results, so not recommended.\n        :param use_cache: if set to False, will not write embeddings to file for later retrieval. this saves disk space but will\n                not allow re-use of once computed embeddings that do not fit into memory\n        :param cache_directory: if cache_directory is not set, the cache will be written to ~/.flair/embeddings. otherwise the cache\n                is written to the provided directory.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "cache_dir", "=", "Path", "(", "\"embeddings\"", ")", "\n", "\n", "# multilingual forward (English, German, French, Italian, Dutch, Polish)", "\n", "if", "model", ".", "lower", "(", ")", "==", "\"multi-forward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-forward-v0.1.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "# multilingual backward  (English, German, French, Italian, Dutch, Polish)", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"multi-backward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-backward-v0.1.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# news-english-forward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"news-forward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-v0.2rc.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# news-english-backward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"news-backward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-v0.2rc.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# news-english-forward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"news-forward-fast\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-1024-v0.2rc.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# news-english-backward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"news-backward-fast\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-1024-v0.2rc.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# mix-english-forward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"mix-forward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-english-forward-v0.2rc.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# mix-english-backward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"mix-backward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-english-backward-v0.2rc.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# mix-german-forward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"german-forward\"", "or", "model", ".", "lower", "(", ")", "==", "\"de-forward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-german-forward-v0.2rc.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# mix-german-backward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"german-backward\"", "or", "model", ".", "lower", "(", ")", "==", "\"de-backward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-german-backward-v0.2rc.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# common crawl Polish forward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"polish-forward\"", "or", "model", ".", "lower", "(", ")", "==", "\"pl-forward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-polish-forward-v0.2.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# common crawl Polish backward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"polish-backward\"", "or", "model", ".", "lower", "(", ")", "==", "\"pl-backward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-polish-backward-v0.2.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# Slovenian forward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"slovenian-forward\"", "or", "model", ".", "lower", "(", ")", "==", "\"sl-forward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-sl-large-forward-v0.1.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "# Slovenian backward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"slovenian-backward\"", "or", "model", ".", "lower", "(", ")", "==", "\"sl-backward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-sl-large-backward-v0.1.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# Bulgarian forward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"bulgarian-forward\"", "or", "model", ".", "lower", "(", ")", "==", "\"bg-forward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-bg-small-forward-v0.1.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "# Bulgarian backward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"bulgarian-backward\"", "or", "model", ".", "lower", "(", ")", "==", "\"bg-backward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-bg-small-backward-v0.1.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# Dutch forward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"dutch-forward\"", "or", "model", ".", "lower", "(", ")", "==", "\"nl-forward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-nl-large-forward-v0.1.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "# Dutch backward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"dutch-backward\"", "or", "model", ".", "lower", "(", ")", "==", "\"nl-backward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-nl-large-backward-v0.1.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# Swedish forward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"swedish-forward\"", "or", "model", ".", "lower", "(", ")", "==", "\"sv-forward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-sv-large-forward-v0.1.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "# Swedish backward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"swedish-backward\"", "or", "model", ".", "lower", "(", ")", "==", "\"sv-backward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-sv-large-backward-v0.1.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# French forward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"french-forward\"", "or", "model", ".", "lower", "(", ")", "==", "\"fr-forward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-fr-charlm-forward.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "# French backward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"french-backward\"", "or", "model", ".", "lower", "(", ")", "==", "\"fr-backward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-fr-charlm-backward.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# Czech forward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"czech-forward\"", "or", "model", ".", "lower", "(", ")", "==", "\"cs-forward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-cs-large-forward-v0.1.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "# Czech backward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"czech-backward\"", "or", "model", ".", "lower", "(", ")", "==", "\"cs-backward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-cs-large-backward-v0.1.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "# Portuguese forward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"portuguese-forward\"", "or", "model", ".", "lower", "(", ")", "==", "\"pt-forward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-pt-forward.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "# Portuguese backward", "\n", "", "elif", "model", ".", "lower", "(", ")", "==", "\"portuguese-backward\"", "or", "model", ".", "lower", "(", ")", "==", "\"pt-backward\"", ":", "\n", "            ", "base_path", "=", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-pt-backward.pt\"", "\n", "model", "=", "cached_path", "(", "base_path", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "", "elif", "not", "Path", "(", "model", ")", ".", "exists", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f'The given rpbert \"{model}\" is not available or is not a valid path.'", "\n", ")", "\n", "\n", "", "self", ".", "name", "=", "str", "(", "model", ")", "\n", "self", ".", "static_embeddings", "=", "detach", "\n", "\n", "from", "flair", ".", "models", "import", "LanguageModel", "\n", "\n", "self", ".", "lm", "=", "LanguageModel", ".", "load_language_model", "(", "model", ")", "\n", "self", ".", "detach", "=", "detach", "\n", "\n", "self", ".", "is_forward_lm", ":", "bool", "=", "self", ".", "lm", ".", "is_forward_lm", "\n", "\n", "# initialize cache if use_cache set", "\n", "self", ".", "cache", "=", "None", "\n", "if", "use_cache", ":", "\n", "            ", "cache_path", "=", "(", "\n", "Path", "(", "f\"{self.name}-tmp-cache.sqllite\"", ")", "\n", "if", "not", "cache_directory", "\n", "else", "cache_directory", "/", "f\"{self.name}-tmp-cache.sqllite\"", "\n", ")", "\n", "from", "sqlitedict", "import", "SqliteDict", "\n", "\n", "self", ".", "cache", "=", "SqliteDict", "(", "str", "(", "cache_path", ")", ",", "autocommit", "=", "True", ")", "\n", "\n", "# embed a dummy sentence to determine embedding_length", "\n", "", "dummy_sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "dummy_sentence", ".", "add_token", "(", "Token", "(", "\"hello\"", ")", ")", "\n", "embedded_dummy", "=", "self", ".", "embed", "(", "dummy_sentence", ")", "\n", "self", ".", "__embedding_length", ":", "int", "=", "len", "(", "\n", "embedded_dummy", "[", "0", "]", ".", "get_token", "(", "1", ")", ".", "get_embedding", "(", ")", "\n", ")", "\n", "\n", "# set to eval mode", "\n", "self", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CharLMEmbeddings.train": [[2962, 2964], ["None"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CharLMEmbeddings.__getstate__": [[2965, 2973], ["embeddings.CharLMEmbeddings.__dict__.copy"], "methods", ["None"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "# Copy the object's state from self.__dict__ which contains", "\n", "# all our instance attributes. Always use the dict.copy()", "\n", "# method to avoid modifying the original state.", "\n", "        ", "state", "=", "self", ".", "__dict__", ".", "copy", "(", ")", "\n", "# Remove the unpicklable entries.", "\n", "state", "[", "\"cache\"", "]", "=", "None", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CharLMEmbeddings.embedding_length": [[2974, 2977], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CharLMEmbeddings._add_embeddings_internal": [[2978, 3043], ["embeddings.CharLMEmbeddings.CharLMEmbeddings.lm.get_representation", "enumerate", "sentence.to_tokenized_string", "sentence.to_tokenized_string", "len", "sentence.to_tokenized_string", "embeddings.CharLMEmbeddings.CharLMEmbeddings.cache.get", "len", "len", "len", "len", "token.set_embedding", "zip", "token._embeddings[].tolist", "token.set_embedding", "sentence.to_tokenized_string", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.get_representation", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_tokenized_string"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "\n", "# if cache is used, try setting embeddings from cache first", "\n", "        ", "if", "\"cache\"", "in", "self", ".", "__dict__", "and", "self", ".", "cache", "is", "not", "None", ":", "\n", "\n", "# try populating embeddings from cache", "\n", "            ", "all_embeddings_retrieved_from_cache", ":", "bool", "=", "True", "\n", "for", "sentence", "in", "sentences", ":", "\n", "                ", "key", "=", "sentence", ".", "to_tokenized_string", "(", ")", "\n", "embeddings", "=", "self", ".", "cache", ".", "get", "(", "key", ")", "\n", "\n", "if", "not", "embeddings", ":", "\n", "                    ", "all_embeddings_retrieved_from_cache", "=", "False", "\n", "break", "\n", "", "else", ":", "\n", "                    ", "for", "token", ",", "embedding", "in", "zip", "(", "sentence", ",", "embeddings", ")", ":", "\n", "                        ", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "torch", ".", "FloatTensor", "(", "embedding", ")", ")", "\n", "\n", "", "", "", "if", "all_embeddings_retrieved_from_cache", ":", "\n", "                ", "return", "sentences", "\n", "\n", "# if this is not possible, use LM to generate embedding. First, get text sentences", "\n", "", "", "text_sentences", "=", "[", "sentence", ".", "to_tokenized_string", "(", ")", "for", "sentence", "in", "sentences", "]", "\n", "\n", "start_marker", "=", "\"\\n\"", "\n", "end_marker", "=", "\" \"", "\n", "\n", "# get hidden states from language rpbert", "\n", "all_hidden_states_in_lm", "=", "self", ".", "lm", ".", "get_representation", "(", "\n", "text_sentences", ",", "start_marker", ",", "end_marker", ",", "self", ".", "chars_per_chunk", "\n", ")", "\n", "\n", "# take first or last hidden states from language rpbert as word representation", "\n", "for", "i", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "            ", "sentence_text", "=", "sentence", ".", "to_tokenized_string", "(", ")", "\n", "\n", "offset_forward", ":", "int", "=", "len", "(", "start_marker", ")", "\n", "offset_backward", ":", "int", "=", "len", "(", "sentence_text", ")", "+", "len", "(", "start_marker", ")", "\n", "\n", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "\n", "                ", "offset_forward", "+=", "len", "(", "token", ".", "text", ")", "\n", "\n", "if", "self", ".", "is_forward_lm", ":", "\n", "                    ", "offset", "=", "offset_forward", "\n", "", "else", ":", "\n", "                    ", "offset", "=", "offset_backward", "\n", "\n", "", "embedding", "=", "all_hidden_states_in_lm", "[", "offset", ",", "i", ",", ":", "]", "\n", "\n", "# if self.tokenized_lm or token.whitespace_after:", "\n", "offset_forward", "+=", "1", "\n", "offset_backward", "-=", "1", "\n", "\n", "offset_backward", "-=", "len", "(", "token", ".", "text", ")", "\n", "\n", "token", ".", "set_embedding", "(", "self", ".", "name", ",", "embedding", ")", "\n", "\n", "", "", "if", "\"cache\"", "in", "self", ".", "__dict__", "and", "self", ".", "cache", "is", "not", "None", ":", "\n", "            ", "for", "sentence", "in", "sentences", ":", "\n", "                ", "self", ".", "cache", "[", "sentence", ".", "to_tokenized_string", "(", ")", "]", "=", "[", "\n", "token", ".", "_embeddings", "[", "self", ".", "name", "]", ".", "tolist", "(", ")", "for", "token", "in", "sentence", "\n", "]", "\n", "\n", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.CharLMEmbeddings.__str__": [[3044, 3046], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentMeanEmbeddings.__init__": [[3049, 3065], ["deprecated.deprecated.deprecated", "super().__init__", "embeddings.StackedEmbeddings", "embeddings.DocumentMeanEmbeddings.to"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["    ", "@", "deprecated", "(", "\n", "version", "=", "\"0.3.1\"", ",", "\n", "reason", "=", "\"The functionality of this class is moved to 'DocumentPoolEmbeddings'\"", ",", "\n", ")", "\n", "def", "__init__", "(", "self", ",", "token_embeddings", ":", "List", "[", "TokenEmbeddings", "]", ")", ":", "\n", "        ", "\"\"\"The constructor takes a list of embeddings to be combined.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embeddings", ":", "StackedEmbeddings", "=", "StackedEmbeddings", "(", "\n", "embeddings", "=", "token_embeddings", "\n", ")", "\n", "self", ".", "name", ":", "str", "=", "\"document_mean\"", "\n", "\n", "self", ".", "__embedding_length", ":", "int", "=", "self", ".", "embeddings", ".", "embedding_length", "\n", "\n", "self", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentMeanEmbeddings.embedding_length": [[3066, 3069], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentMeanEmbeddings.embed": [[3070, 3098], ["type", "embeddings.DocumentMeanEmbeddings.embeddings.embed", "sentence._embeddings.keys", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "sentence.set_embedding", "torch.cat().to.append", "torch.cat().to.append", "token.get_embedding().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "token.get_embedding"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding"], ["", "def", "embed", "(", "self", ",", "sentences", ":", "Union", "[", "List", "[", "Sentence", "]", ",", "Sentence", "]", ")", ":", "\n", "        ", "\"\"\"Add embeddings to every sentence in the given list of sentences. If embeddings are already added, updates\n        only if embeddings are non-static.\"\"\"", "\n", "\n", "everything_embedded", ":", "bool", "=", "True", "\n", "\n", "# if only one sentence is passed, convert to list of sentence", "\n", "if", "type", "(", "sentences", ")", "is", "Sentence", ":", "\n", "            ", "sentences", "=", "[", "sentences", "]", "\n", "\n", "", "for", "sentence", "in", "sentences", ":", "\n", "            ", "if", "self", ".", "name", "not", "in", "sentence", ".", "_embeddings", ".", "keys", "(", ")", ":", "\n", "                ", "everything_embedded", "=", "False", "\n", "\n", "", "", "if", "not", "everything_embedded", ":", "\n", "\n", "            ", "self", ".", "embeddings", ".", "embed", "(", "sentences", ")", "\n", "\n", "for", "sentence", "in", "sentences", ":", "\n", "                ", "word_embeddings", "=", "[", "]", "\n", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "                    ", "word_embeddings", ".", "append", "(", "token", ".", "get_embedding", "(", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "", "word_embeddings", "=", "torch", ".", "cat", "(", "word_embeddings", ",", "dim", "=", "0", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "mean_embedding", "=", "torch", ".", "mean", "(", "word_embeddings", ",", "0", ")", "\n", "\n", "sentence", ".", "set_embedding", "(", "self", ".", "name", ",", "mean_embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentMeanEmbeddings._add_embeddings_internal": [[3099, 3101], ["None"], "methods", ["None"], ["", "", "", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentPoolEmbeddings.__init__": [[3104, 3147], ["super().__init__", "embeddings.StackedEmbeddings", "embeddings.DocumentPoolEmbeddings.to", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "embeddings.DocumentPoolEmbeddings.embedding_flex.weight.data.copy_", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "embeddings", ":", "List", "[", "TokenEmbeddings", "]", ",", "\n", "fine_tune_mode", "=", "\"linear\"", ",", "\n", "pooling", ":", "str", "=", "\"mean\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"The constructor takes a list of embeddings to be combined.\n        :param embeddings: a list of token embeddings\n        :param pooling: a string which can any value from ['mean', 'max', 'min']\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embeddings", ":", "StackedEmbeddings", "=", "StackedEmbeddings", "(", "embeddings", "=", "embeddings", ")", "\n", "self", ".", "__embedding_length", "=", "self", ".", "embeddings", ".", "embedding_length", "\n", "\n", "# optional fine-tuning on top of embedding layer", "\n", "self", ".", "fine_tune_mode", "=", "fine_tune_mode", "\n", "if", "self", ".", "fine_tune_mode", "in", "[", "\"nonlinear\"", ",", "\"linear\"", "]", ":", "\n", "            ", "self", ".", "embedding_flex", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "self", ".", "embedding_length", ",", "self", ".", "embedding_length", ",", "bias", "=", "False", "\n", ")", "\n", "self", ".", "embedding_flex", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "eye", "(", "self", ".", "embedding_length", ")", ")", "\n", "\n", "", "if", "self", ".", "fine_tune_mode", "in", "[", "\"nonlinear\"", "]", ":", "\n", "            ", "self", ".", "embedding_flex_nonlinear", "=", "torch", ".", "nn", ".", "ReLU", "(", "self", ".", "embedding_length", ")", "\n", "self", ".", "embedding_flex_nonlinear_map", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "self", ".", "embedding_length", ",", "self", ".", "embedding_length", "\n", ")", "\n", "\n", "", "self", ".", "__embedding_length", ":", "int", "=", "self", ".", "embeddings", ".", "embedding_length", "\n", "\n", "self", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "self", ".", "pooling", "=", "pooling", "\n", "if", "self", ".", "pooling", "==", "\"mean\"", ":", "\n", "            ", "self", ".", "pool_op", "=", "torch", ".", "mean", "\n", "", "elif", "pooling", "==", "\"max\"", ":", "\n", "            ", "self", ".", "pool_op", "=", "torch", ".", "max", "\n", "", "elif", "pooling", "==", "\"min\"", ":", "\n", "            ", "self", ".", "pool_op", "=", "torch", ".", "min", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Pooling operation for {self.mode!r} is not defined\"", ")", "\n", "", "self", ".", "name", ":", "str", "=", "f\"document_{self.pooling}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentPoolEmbeddings.embedding_length": [[3148, 3151], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentPoolEmbeddings.embed": [[3152, 3182], ["isinstance", "embeddings.DocumentPoolEmbeddings.embeddings.embed", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "sentence.set_embedding", "embeddings.DocumentPoolEmbeddings.append", "embeddings.DocumentPoolEmbeddings.embedding_flex", "embeddings.DocumentPoolEmbeddings.embedding_flex_nonlinear", "embeddings.DocumentPoolEmbeddings.embedding_flex_nonlinear_map", "embeddings.DocumentPoolEmbeddings.pool_op", "embeddings.DocumentPoolEmbeddings.pool_op", "token.get_embedding().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "token.get_embedding"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding"], ["", "def", "embed", "(", "self", ",", "sentences", ":", "Union", "[", "List", "[", "Sentence", "]", ",", "Sentence", "]", ")", ":", "\n", "        ", "\"\"\"Add embeddings to every sentence in the given list of sentences. If embeddings are already added, updates\n        only if embeddings are non-static.\"\"\"", "\n", "\n", "# if only one sentence is passed, convert to list of sentence", "\n", "if", "isinstance", "(", "sentences", ",", "Sentence", ")", ":", "\n", "            ", "sentences", "=", "[", "sentences", "]", "\n", "\n", "", "self", ".", "embeddings", ".", "embed", "(", "sentences", ")", "\n", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "word_embeddings", "=", "[", "]", "\n", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "                ", "word_embeddings", ".", "append", "(", "token", ".", "get_embedding", "(", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "", "word_embeddings", "=", "torch", ".", "cat", "(", "word_embeddings", ",", "dim", "=", "0", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "if", "self", ".", "fine_tune_mode", "in", "[", "\"nonlinear\"", ",", "\"linear\"", "]", ":", "\n", "                ", "word_embeddings", "=", "self", ".", "embedding_flex", "(", "word_embeddings", ")", "\n", "\n", "", "if", "self", ".", "fine_tune_mode", "in", "[", "\"nonlinear\"", "]", ":", "\n", "                ", "word_embeddings", "=", "self", ".", "embedding_flex_nonlinear", "(", "word_embeddings", ")", "\n", "word_embeddings", "=", "self", ".", "embedding_flex_nonlinear_map", "(", "word_embeddings", ")", "\n", "\n", "", "if", "self", ".", "pooling", "==", "\"mean\"", ":", "\n", "                ", "pooled_embedding", "=", "self", ".", "pool_op", "(", "word_embeddings", ",", "0", ")", "\n", "", "else", ":", "\n", "                ", "pooled_embedding", ",", "_", "=", "self", ".", "pool_op", "(", "word_embeddings", ",", "0", ")", "\n", "\n", "", "sentence", ".", "set_embedding", "(", "self", ".", "name", ",", "pooled_embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentPoolEmbeddings._add_embeddings_internal": [[3183, 3185], ["None"], "methods", ["None"], ["", "", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentPoolEmbeddings.extra_repr": [[3186, 3188], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "f\"fine_tune_mode={self.fine_tune_mode}, pooling={self.pooling}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentRNNEmbeddings.__init__": [[3191, 3276], ["super().__init__", "embeddings.StackedEmbeddings", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "embeddings.DocumentRNNEmbeddings.to", "embeddings.DocumentRNNEmbeddings.eval", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.GRU", "torch.nn.GRU", "torch.nn.GRU", "torch.nn.GRU", "embeddings.DocumentRNNEmbeddings.rnn._get_name", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "nn.LockedDropout", "nn.WordDropout"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "embeddings", ":", "List", "[", "TokenEmbeddings", "]", ",", "\n", "hidden_size", "=", "128", ",", "\n", "rnn_layers", "=", "1", ",", "\n", "reproject_words", ":", "bool", "=", "True", ",", "\n", "reproject_words_dimension", ":", "int", "=", "None", ",", "\n", "bidirectional", ":", "bool", "=", "False", ",", "\n", "dropout", ":", "float", "=", "0.5", ",", "\n", "word_dropout", ":", "float", "=", "0.0", ",", "\n", "locked_dropout", ":", "float", "=", "0.0", ",", "\n", "rnn_type", "=", "\"GRU\"", ",", "\n", "fine_tune", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"The constructor takes a list of embeddings to be combined.\n        :param embeddings: a list of token embeddings\n        :param hidden_size: the number of hidden states in the rnn\n        :param rnn_layers: the number of layers for the rnn\n        :param reproject_words: boolean value, indicating whether to reproject the token embeddings in a separate linear\n        layer before putting them into the rnn or not\n        :param reproject_words_dimension: output dimension of reprojecting token embeddings. If None the same output\n        dimension as before will be taken.\n        :param bidirectional: boolean value, indicating whether to use a bidirectional rnn or not\n        :param dropout: the dropout value to be used\n        :param word_dropout: the word dropout value to be used, if 0.0 word dropout is not used\n        :param locked_dropout: the locked dropout value to be used, if 0.0 locked dropout is not used\n        :param rnn_type: 'GRU' or 'LSTM'\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embeddings", ":", "StackedEmbeddings", "=", "StackedEmbeddings", "(", "embeddings", "=", "embeddings", ")", "\n", "\n", "self", ".", "rnn_type", "=", "rnn_type", "\n", "\n", "self", ".", "reproject_words", "=", "reproject_words", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "\n", "self", ".", "length_of_all_token_embeddings", ":", "int", "=", "self", ".", "embeddings", ".", "embedding_length", "\n", "\n", "self", ".", "static_embeddings", "=", "False", "if", "fine_tune", "else", "True", "\n", "\n", "self", ".", "__embedding_length", ":", "int", "=", "hidden_size", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "self", ".", "__embedding_length", "*=", "4", "\n", "\n", "", "self", ".", "embeddings_dimension", ":", "int", "=", "self", ".", "length_of_all_token_embeddings", "\n", "if", "self", ".", "reproject_words", "and", "reproject_words_dimension", "is", "not", "None", ":", "\n", "            ", "self", ".", "embeddings_dimension", "=", "reproject_words_dimension", "\n", "\n", "", "self", ".", "word_reprojection_map", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "self", ".", "length_of_all_token_embeddings", ",", "self", ".", "embeddings_dimension", "\n", ")", "\n", "\n", "# bidirectional RNN on top of embedding layer", "\n", "if", "rnn_type", "==", "\"LSTM\"", ":", "\n", "            ", "self", ".", "rnn", "=", "torch", ".", "nn", ".", "LSTM", "(", "\n", "self", ".", "embeddings_dimension", ",", "\n", "hidden_size", ",", "\n", "num_layers", "=", "rnn_layers", ",", "\n", "bidirectional", "=", "self", ".", "bidirectional", ",", "\n", "batch_first", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "rnn", "=", "torch", ".", "nn", ".", "GRU", "(", "\n", "self", ".", "embeddings_dimension", ",", "\n", "hidden_size", ",", "\n", "num_layers", "=", "rnn_layers", ",", "\n", "bidirectional", "=", "self", ".", "bidirectional", ",", "\n", "batch_first", "=", "True", ",", "\n", ")", "\n", "\n", "", "self", ".", "name", "=", "\"document_\"", "+", "self", ".", "rnn", ".", "_get_name", "(", ")", "\n", "\n", "# dropouts", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "if", "dropout", ">", "0.0", "else", "None", "\n", "self", ".", "locked_dropout", "=", "(", "\n", "LockedDropout", "(", "locked_dropout", ")", "if", "locked_dropout", ">", "0.0", "else", "None", "\n", ")", "\n", "self", ".", "word_dropout", "=", "WordDropout", "(", "word_dropout", ")", "if", "word_dropout", ">", "0.0", "else", "None", "\n", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "word_reprojection_map", ".", "weight", ")", "\n", "\n", "self", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "self", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentRNNEmbeddings.embedding_length": [[3277, 3280], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentRNNEmbeddings._add_embeddings_internal": [[3281, 3368], ["embeddings.DocumentRNNEmbeddings.rnn.zero_grad", "embeddings.DocumentRNNEmbeddings.embeddings.embed", "max", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "list", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "embeddings.DocumentRNNEmbeddings.rnn", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "enumerate", "hasattr", "hasattr", "type", "len", "embeddings.DocumentRNNEmbeddings.dropout", "embeddings.DocumentRNNEmbeddings.locked_dropout", "embeddings.DocumentRNNEmbeddings.word_dropout", "embeddings.DocumentRNNEmbeddings.word_reprojection_map", "embeddings.DocumentRNNEmbeddings.dropout", "embeddings.DocumentRNNEmbeddings.locked_dropout", "sentence.set_embedding", "len", "all_embs.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "embedding.detach.detach.detach", "token.get_each_embedding"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_each_embedding"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "Union", "[", "List", "[", "Sentence", "]", ",", "Sentence", "]", ")", ":", "\n", "        ", "\"\"\"Add embeddings to all sentences in the given list of sentences. If embeddings are already added, update\n         only if embeddings are non-static.\"\"\"", "\n", "\n", "# TODO: remove in future versions", "\n", "if", "not", "hasattr", "(", "self", ",", "\"locked_dropout\"", ")", ":", "\n", "            ", "self", ".", "locked_dropout", "=", "None", "\n", "", "if", "not", "hasattr", "(", "self", ",", "\"word_dropout\"", ")", ":", "\n", "            ", "self", ".", "word_dropout", "=", "None", "\n", "\n", "", "if", "type", "(", "sentences", ")", "is", "Sentence", ":", "\n", "            ", "sentences", "=", "[", "sentences", "]", "\n", "\n", "", "self", ".", "rnn", ".", "zero_grad", "(", ")", "\n", "\n", "# embed words in the sentence", "\n", "self", ".", "embeddings", ".", "embed", "(", "sentences", ")", "\n", "\n", "lengths", ":", "List", "[", "int", "]", "=", "[", "len", "(", "sentence", ".", "tokens", ")", "for", "sentence", "in", "sentences", "]", "\n", "longest_token_sequence_in_batch", ":", "int", "=", "max", "(", "lengths", ")", "\n", "\n", "pre_allocated_zero_tensor", "=", "torch", ".", "zeros", "(", "\n", "self", ".", "embeddings", ".", "embedding_length", "*", "longest_token_sequence_in_batch", ",", "\n", "dtype", "=", "torch", ".", "float", ",", "\n", "device", "=", "flair", ".", "device", ",", "\n", ")", "\n", "\n", "all_embs", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "list", "(", ")", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "all_embs", "+=", "[", "\n", "emb", "for", "token", "in", "sentence", "for", "emb", "in", "token", ".", "get_each_embedding", "(", ")", "\n", "]", "\n", "nb_padding_tokens", "=", "longest_token_sequence_in_batch", "-", "len", "(", "sentence", ")", "\n", "\n", "if", "nb_padding_tokens", ">", "0", ":", "\n", "                ", "t", "=", "pre_allocated_zero_tensor", "[", "\n", ":", "self", ".", "embeddings", ".", "embedding_length", "*", "nb_padding_tokens", "\n", "]", "\n", "all_embs", ".", "append", "(", "t", ")", "\n", "\n", "", "", "sentence_tensor", "=", "torch", ".", "cat", "(", "all_embs", ")", ".", "view", "(", "\n", "[", "\n", "len", "(", "sentences", ")", ",", "\n", "longest_token_sequence_in_batch", ",", "\n", "self", ".", "embeddings", ".", "embedding_length", ",", "\n", "]", "\n", ")", "\n", "\n", "# before-RNN dropout", "\n", "if", "self", ".", "dropout", ":", "\n", "            ", "sentence_tensor", "=", "self", ".", "dropout", "(", "sentence_tensor", ")", "\n", "", "if", "self", ".", "locked_dropout", ":", "\n", "            ", "sentence_tensor", "=", "self", ".", "locked_dropout", "(", "sentence_tensor", ")", "\n", "", "if", "self", ".", "word_dropout", ":", "\n", "            ", "sentence_tensor", "=", "self", ".", "word_dropout", "(", "sentence_tensor", ")", "\n", "\n", "# reproject if set", "\n", "", "if", "self", ".", "reproject_words", ":", "\n", "            ", "sentence_tensor", "=", "self", ".", "word_reprojection_map", "(", "sentence_tensor", ")", "\n", "\n", "# push through RNN", "\n", "", "packed", "=", "pack_padded_sequence", "(", "\n", "sentence_tensor", ",", "lengths", ",", "enforce_sorted", "=", "False", ",", "batch_first", "=", "True", "\n", ")", "\n", "rnn_out", ",", "hidden", "=", "self", ".", "rnn", "(", "packed", ")", "\n", "outputs", ",", "output_lengths", "=", "pad_packed_sequence", "(", "rnn_out", ",", "batch_first", "=", "True", ")", "\n", "\n", "# after-RNN dropout", "\n", "if", "self", ".", "dropout", ":", "\n", "            ", "outputs", "=", "self", ".", "dropout", "(", "outputs", ")", "\n", "", "if", "self", ".", "locked_dropout", ":", "\n", "            ", "outputs", "=", "self", ".", "locked_dropout", "(", "outputs", ")", "\n", "\n", "# extract embeddings from RNN", "\n", "", "for", "sentence_no", ",", "length", "in", "enumerate", "(", "lengths", ")", ":", "\n", "            ", "last_rep", "=", "outputs", "[", "sentence_no", ",", "length", "-", "1", "]", "\n", "\n", "embedding", "=", "last_rep", "\n", "if", "self", ".", "bidirectional", ":", "\n", "                ", "first_rep", "=", "outputs", "[", "sentence_no", ",", "0", "]", "\n", "embedding", "=", "torch", ".", "cat", "(", "[", "first_rep", ",", "last_rep", "]", ",", "0", ")", "\n", "\n", "", "if", "self", ".", "static_embeddings", ":", "\n", "                ", "embedding", "=", "embedding", ".", "detach", "(", ")", "\n", "\n", "", "sentence", "=", "sentences", "[", "sentence_no", "]", "\n", "sentence", ".", "set_embedding", "(", "self", ".", "name", ",", "embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentRNNEmbeddings._apply": [[3369, 3402], ["int", "embeddings.DocumentRNNEmbeddings.children", "super()._apply", "torch.__version__.replace().split", "torch.__version__.replace().split", "torch.__version__.replace().split", "torch.__version__.replace().split", "info.isdigit", "isinstance", "child_module._apply", "range", "setattr", "torch.__version__.replace", "torch.__version__.replace", "torch.__version__.replace", "torch.__version__.replace", "range", "_flat_weights_names.extend", "x.format"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel._apply", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel._apply"], ["", "", "def", "_apply", "(", "self", ",", "fn", ")", ":", "\n", "        ", "major", ",", "minor", ",", "build", ",", "*", "_", "=", "(", "int", "(", "info", ")", "\n", "for", "info", "in", "torch", ".", "__version__", ".", "replace", "(", "\"+\"", ",", "\".\"", ")", ".", "split", "(", "'.'", ")", "if", "info", ".", "isdigit", "(", ")", ")", "\n", "\n", "# fixed RNN change format for torch 1.4.0", "\n", "if", "major", ">=", "1", "and", "minor", ">=", "4", ":", "\n", "            ", "for", "child_module", "in", "self", ".", "children", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "child_module", ",", "torch", ".", "nn", ".", "RNNBase", ")", ":", "\n", "                    ", "_flat_weights_names", "=", "[", "]", "\n", "num_direction", "=", "None", "\n", "\n", "if", "child_module", ".", "__dict__", "[", "\"bidirectional\"", "]", ":", "\n", "                        ", "num_direction", "=", "2", "\n", "", "else", ":", "\n", "                        ", "num_direction", "=", "1", "\n", "", "for", "layer", "in", "range", "(", "child_module", ".", "__dict__", "[", "\"num_layers\"", "]", ")", ":", "\n", "                        ", "for", "direction", "in", "range", "(", "num_direction", ")", ":", "\n", "                            ", "suffix", "=", "\"_reverse\"", "if", "direction", "==", "1", "else", "\"\"", "\n", "param_names", "=", "[", "\"weight_ih_l{}{}\"", ",", "\"weight_hh_l{}{}\"", "]", "\n", "if", "child_module", ".", "__dict__", "[", "\"bias\"", "]", ":", "\n", "                                ", "param_names", "+=", "[", "\"bias_ih_l{}{}\"", ",", "\"bias_hh_l{}{}\"", "]", "\n", "", "param_names", "=", "[", "\n", "x", ".", "format", "(", "layer", ",", "suffix", ")", "for", "x", "in", "param_names", "\n", "]", "\n", "_flat_weights_names", ".", "extend", "(", "param_names", ")", "\n", "\n", "", "", "setattr", "(", "child_module", ",", "\"_flat_weights_names\"", ",", "\n", "_flat_weights_names", ")", "\n", "\n", "", "child_module", ".", "_apply", "(", "fn", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "super", "(", ")", ".", "_apply", "(", "fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.__init__": [[3409, 3478], ["super().__init__", "embeddings.StackedEmbeddings", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.GRU", "torch.nn.GRU", "torch.nn.GRU", "torch.nn.GRU", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "embeddings.DocumentLSTMEmbeddings.to", "nn.LockedDropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "nn.WordDropout"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "embeddings", ":", "List", "[", "TokenEmbeddings", "]", ",", "\n", "hidden_size", "=", "128", ",", "\n", "rnn_layers", "=", "1", ",", "\n", "reproject_words", ":", "bool", "=", "True", ",", "\n", "reproject_words_dimension", ":", "int", "=", "None", ",", "\n", "bidirectional", ":", "bool", "=", "False", ",", "\n", "dropout", ":", "float", "=", "0.5", ",", "\n", "word_dropout", ":", "float", "=", "0.0", ",", "\n", "locked_dropout", ":", "float", "=", "0.0", ",", "\n", ")", ":", "\n", "        ", "\"\"\"The constructor takes a list of embeddings to be combined.\n        :param embeddings: a list of token embeddings\n        :param hidden_size: the number of hidden states in the lstm\n        :param rnn_layers: the number of layers for the lstm\n        :param reproject_words: boolean value, indicating whether to reproject the token embeddings in a separate linear\n        layer before putting them into the lstm or not\n        :param reproject_words_dimension: output dimension of reprojecting token embeddings. If None the same output\n        dimension as before will be taken.\n        :param bidirectional: boolean value, indicating whether to use a bidirectional lstm or not\n        :param dropout: the dropout value to be used\n        :param word_dropout: the word dropout value to be used, if 0.0 word dropout is not used\n        :param locked_dropout: the locked dropout value to be used, if 0.0 locked dropout is not used\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embeddings", ":", "StackedEmbeddings", "=", "StackedEmbeddings", "(", "embeddings", "=", "embeddings", ")", "\n", "\n", "self", ".", "reproject_words", "=", "reproject_words", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "\n", "self", ".", "length_of_all_token_embeddings", ":", "int", "=", "self", ".", "embeddings", ".", "embedding_length", "\n", "\n", "self", ".", "name", "=", "\"document_lstm\"", "\n", "self", ".", "static_embeddings", "=", "False", "\n", "\n", "self", ".", "__embedding_length", ":", "int", "=", "hidden_size", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "self", ".", "__embedding_length", "*=", "4", "\n", "\n", "", "self", ".", "embeddings_dimension", ":", "int", "=", "self", ".", "length_of_all_token_embeddings", "\n", "if", "self", ".", "reproject_words", "and", "reproject_words_dimension", "is", "not", "None", ":", "\n", "            ", "self", ".", "embeddings_dimension", "=", "reproject_words_dimension", "\n", "\n", "# bidirectional LSTM on top of embedding layer", "\n", "", "self", ".", "word_reprojection_map", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "self", ".", "length_of_all_token_embeddings", ",", "self", ".", "embeddings_dimension", "\n", ")", "\n", "self", ".", "rnn", "=", "torch", ".", "nn", ".", "GRU", "(", "\n", "self", ".", "embeddings_dimension", ",", "\n", "hidden_size", ",", "\n", "num_layers", "=", "rnn_layers", ",", "\n", "bidirectional", "=", "self", ".", "bidirectional", ",", "\n", ")", "\n", "\n", "# dropouts", "\n", "if", "locked_dropout", ">", "0.0", ":", "\n", "            ", "self", ".", "dropout", ":", "torch", ".", "nn", ".", "Module", "=", "LockedDropout", "(", "locked_dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "", "self", ".", "use_word_dropout", ":", "bool", "=", "word_dropout", ">", "0.0", "\n", "if", "self", ".", "use_word_dropout", ":", "\n", "            ", "self", ".", "word_dropout", "=", "WordDropout", "(", "word_dropout", ")", "\n", "\n", "", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "word_reprojection_map", ".", "weight", ")", "\n", "\n", "self", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embedding_length": [[3479, 3482], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed": [[3483, 3567], ["embeddings.DocumentLSTMEmbeddings.rnn.zero_grad", "sentences.sort", "embeddings.DocumentLSTMEmbeddings.embeddings.embed", "len", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "embeddings.DocumentLSTMEmbeddings.dropout", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "embeddings.DocumentLSTMEmbeddings.rnn.flatten_parameters", "embeddings.DocumentLSTMEmbeddings.rnn", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "embeddings.DocumentLSTMEmbeddings.dropout", "enumerate", "type", "lengths.append", "zip", "range", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "all_sentence_tensors.append", "embeddings.DocumentLSTMEmbeddings.word_dropout", "embeddings.DocumentLSTMEmbeddings.word_reprojection_map", "sentence.set_embedding", "len", "range", "word_embeddings.append", "word_embeddings.append", "sentence_states.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "len", "token.get_embedding().unsqueeze", "len", "torch.zeros().unsqueeze().to", "torch.zeros().unsqueeze().to", "torch.zeros().unsqueeze().to", "torch.zeros().unsqueeze().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "token.get_embedding", "torch.zeros().unsqueeze", "torch.zeros().unsqueeze", "torch.zeros().unsqueeze", "torch.zeros().unsqueeze", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.get_embedding"], ["", "def", "embed", "(", "self", ",", "sentences", ":", "Union", "[", "List", "[", "Sentence", "]", ",", "Sentence", "]", ")", ":", "\n", "        ", "\"\"\"Add embeddings to all sentences in the given list of sentences. If embeddings are already added, update\n         only if embeddings are non-static.\"\"\"", "\n", "\n", "if", "type", "(", "sentences", ")", "is", "Sentence", ":", "\n", "            ", "sentences", "=", "[", "sentences", "]", "\n", "\n", "", "self", ".", "rnn", ".", "zero_grad", "(", ")", "\n", "\n", "sentences", ".", "sort", "(", "key", "=", "lambda", "x", ":", "len", "(", "x", ")", ",", "reverse", "=", "True", ")", "\n", "\n", "self", ".", "embeddings", ".", "embed", "(", "sentences", ")", "\n", "\n", "# first, sort sentences by number of tokens", "\n", "longest_token_sequence_in_batch", ":", "int", "=", "len", "(", "sentences", "[", "0", "]", ")", "\n", "\n", "all_sentence_tensors", "=", "[", "]", "\n", "lengths", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "\n", "# go through each sentence in batch", "\n", "for", "i", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "\n", "            ", "lengths", ".", "append", "(", "len", "(", "sentence", ".", "tokens", ")", ")", "\n", "\n", "word_embeddings", "=", "[", "]", "\n", "\n", "for", "token", ",", "token_idx", "in", "zip", "(", "sentence", ".", "tokens", ",", "range", "(", "len", "(", "sentence", ".", "tokens", ")", ")", ")", ":", "\n", "                ", "word_embeddings", ".", "append", "(", "token", ".", "get_embedding", "(", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "# PADDING: pad shorter sentences out", "\n", "", "for", "add", "in", "range", "(", "longest_token_sequence_in_batch", "-", "len", "(", "sentence", ".", "tokens", ")", ")", ":", "\n", "                ", "word_embeddings", ".", "append", "(", "\n", "torch", ".", "zeros", "(", "\n", "self", ".", "length_of_all_token_embeddings", ",", "dtype", "=", "torch", ".", "float", "\n", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", ")", "\n", "\n", "", "word_embeddings_tensor", "=", "torch", ".", "cat", "(", "word_embeddings", ",", "0", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "sentence_states", "=", "word_embeddings_tensor", "\n", "\n", "# ADD TO SENTENCE LIST: add the representation", "\n", "all_sentence_tensors", ".", "append", "(", "sentence_states", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "# --------------------------------------------------------------------", "\n", "# GET REPRESENTATION FOR ENTIRE BATCH", "\n", "# --------------------------------------------------------------------", "\n", "", "sentence_tensor", "=", "torch", ".", "cat", "(", "all_sentence_tensors", ",", "1", ")", "\n", "\n", "# --------------------------------------------------------------------", "\n", "# FF PART", "\n", "# --------------------------------------------------------------------", "\n", "# use word dropout if set", "\n", "if", "self", ".", "use_word_dropout", ":", "\n", "            ", "sentence_tensor", "=", "self", ".", "word_dropout", "(", "sentence_tensor", ")", "\n", "\n", "", "if", "self", ".", "reproject_words", ":", "\n", "            ", "sentence_tensor", "=", "self", ".", "word_reprojection_map", "(", "sentence_tensor", ")", "\n", "\n", "", "sentence_tensor", "=", "self", ".", "dropout", "(", "sentence_tensor", ")", "\n", "\n", "packed", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "sentence_tensor", ",", "lengths", ")", "\n", "\n", "self", ".", "rnn", ".", "flatten_parameters", "(", ")", "\n", "\n", "lstm_out", ",", "hidden", "=", "self", ".", "rnn", "(", "packed", ")", "\n", "\n", "outputs", ",", "output_lengths", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "lstm_out", ")", "\n", "\n", "outputs", "=", "self", ".", "dropout", "(", "outputs", ")", "\n", "\n", "# --------------------------------------------------------------------", "\n", "# EXTRACT EMBEDDINGS FROM LSTM", "\n", "# --------------------------------------------------------------------", "\n", "for", "sentence_no", ",", "length", "in", "enumerate", "(", "lengths", ")", ":", "\n", "            ", "last_rep", "=", "outputs", "[", "length", "-", "1", ",", "sentence_no", "]", "\n", "\n", "embedding", "=", "last_rep", "\n", "if", "self", ".", "bidirectional", ":", "\n", "                ", "first_rep", "=", "outputs", "[", "0", ",", "sentence_no", "]", "\n", "embedding", "=", "torch", ".", "cat", "(", "[", "first_rep", ",", "last_rep", "]", ",", "0", ")", "\n", "\n", "", "sentence", "=", "sentences", "[", "sentence_no", "]", "\n", "sentence", ".", "set_embedding", "(", "self", ".", "name", ",", "embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings._add_embeddings_internal": [[3568, 3570], ["None"], "methods", ["None"], ["", "", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLMEmbeddings.__init__": [[3573, 3587], ["super().__init__", "enumerate", "sum", "embeddings.DocumentLMEmbeddings.add_module"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "flair_embeddings", ":", "List", "[", "FlairEmbeddings", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embeddings", "=", "flair_embeddings", "\n", "self", ".", "name", "=", "\"document_lm\"", "\n", "\n", "# IMPORTANT: add embeddings as torch modules", "\n", "for", "i", ",", "embedding", "in", "enumerate", "(", "flair_embeddings", ")", ":", "\n", "            ", "self", ".", "add_module", "(", "\"lm_embedding_{}\"", ".", "format", "(", "i", ")", ",", "embedding", ")", "\n", "if", "not", "embedding", ".", "static_embeddings", ":", "\n", "                ", "self", ".", "static_embeddings", "=", "False", "\n", "\n", "", "", "self", ".", "_embedding_length", ":", "int", "=", "sum", "(", "\n", "embedding", ".", "embedding_length", "for", "embedding", "in", "flair_embeddings", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLMEmbeddings.embedding_length": [[3589, 3592], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLMEmbeddings._add_embeddings_internal": [[3593, 3616], ["type", "embedding.embed", "sentence.set_embedding", "sentence.set_embedding", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", ":", "\n", "        ", "if", "type", "(", "sentences", ")", "is", "Sentence", ":", "\n", "            ", "sentences", "=", "[", "sentences", "]", "\n", "\n", "", "for", "embedding", "in", "self", ".", "embeddings", ":", "\n", "            ", "embedding", ".", "embed", "(", "sentences", ")", "\n", "\n", "# iterate over sentences", "\n", "for", "sentence", "in", "sentences", ":", "\n", "                ", "sentence", ":", "Sentence", "=", "sentence", "\n", "\n", "# if its a forward LM, take last state", "\n", "if", "embedding", ".", "is_forward_lm", ":", "\n", "                    ", "sentence", ".", "set_embedding", "(", "\n", "embedding", ".", "name", ",", "\n", "sentence", "[", "len", "(", "sentence", ")", "-", "1", "]", ".", "_embeddings", "[", "embedding", ".", "name", "]", ",", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "sentence", ".", "set_embedding", "(", "\n", "embedding", ".", "name", ",", "sentence", "[", "0", "]", ".", "_embeddings", "[", "embedding", ".", "name", "]", "\n", ")", "\n", "\n", "", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.NILCEmbeddings.__init__": [[3619, 3664], ["str", "log.info", "gensim.models.KeyedVectors.load_word2vec_format", "file_utils.cached_path.WordEmbeddings.__init__", "pathlib.Path", "file_utils.cached_path.lower", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path", "file_utils.open_inside_zip", "file_utils.cached_path.lower", "file_utils.cached_path", "file_utils.cached_path", "str", "pathlib.Path().exists", "ValueError", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.open_inside_zip", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["    ", "def", "__init__", "(", "self", ",", "embeddings", ":", "str", ",", "model", ":", "str", "=", "\"skip\"", ",", "size", ":", "int", "=", "100", ")", ":", "\n", "        ", "\"\"\"\n        Initializes portuguese classic word embeddings trained by NILC Lab (http://www.nilc.icmc.usp.br/embeddings).\n        Constructor downloads required files if not there.\n        :param embeddings: one of: 'fasttext', 'glove', 'wang2vec' or 'word2vec'\n        :param model: one of: 'skip' or 'cbow'. This is not applicable to glove.\n        :param size: one of: 50, 100, 300, 600 or 1000.\n        \"\"\"", "\n", "\n", "base_path", "=", "\"http://143.107.183.175:22980/download.php?file=embeddings/\"", "\n", "\n", "cache_dir", "=", "Path", "(", "\"embeddings\"", ")", "/", "embeddings", ".", "lower", "(", ")", "\n", "\n", "# GLOVE embeddings", "\n", "if", "embeddings", ".", "lower", "(", ")", "==", "\"glove\"", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{base_path}{embeddings}/{embeddings}_s{size}.zip\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "embeddings", "=", "cached_path", "(", "\n", "f\"{base_path}{embeddings}/{embeddings}_s{size}.zip\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "\n", "", "elif", "embeddings", ".", "lower", "(", ")", "in", "[", "\"fasttext\"", ",", "\"wang2vec\"", ",", "\"word2vec\"", "]", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{base_path}{embeddings}/{model}_s{size}.zip\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "embeddings", "=", "cached_path", "(", "\n", "f\"{base_path}{embeddings}/{model}_s{size}.zip\"", ",", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "\n", "", "elif", "not", "Path", "(", "embeddings", ")", ".", "exists", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f'The given embeddings \"{embeddings}\" is not available or is not a valid path.'", "\n", ")", "\n", "\n", "", "self", ".", "name", ":", "str", "=", "str", "(", "embeddings", ")", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "\n", "log", ".", "info", "(", "\"Reading embeddings from %s\"", "%", "embeddings", ")", "\n", "self", ".", "precomputed_word_embeddings", "=", "gensim", ".", "models", ".", "KeyedVectors", ".", "load_word2vec_format", "(", "\n", "open_inside_zip", "(", "str", "(", "embeddings", ")", ",", "cache_dir", "=", "cache_dir", ")", "\n", ")", "\n", "\n", "self", ".", "__embedding_length", ":", "int", "=", "self", ".", "precomputed_word_embeddings", ".", "vector_size", "\n", "super", "(", "TokenEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.NILCEmbeddings.embedding_length": [[3665, 3668], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.NILCEmbeddings.__str__": [[3669, 3671], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.IdentityImageEmbeddings.__init__": [[3674, 3683], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "transforms", ")", ":", "\n", "        ", "import", "PIL", "as", "pythonimagelib", "\n", "\n", "self", ".", "PIL", "=", "pythonimagelib", "\n", "self", ".", "name", "=", "\"Identity\"", "\n", "self", ".", "transforms", "=", "transforms", "\n", "self", ".", "__embedding_length", "=", "None", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.IdentityImageEmbeddings._add_embeddings_internal": [[3684, 3689], ["embeddings.IdentityImageEmbeddings.PIL.Image.open", "embeddings.IdentityImageEmbeddings.load", "image.set_embedding", "embeddings.IdentityImageEmbeddings.transforms"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "images", ":", "List", "[", "Image", "]", ")", "->", "List", "[", "Image", "]", ":", "\n", "        ", "for", "image", "in", "images", ":", "\n", "            ", "image_data", "=", "self", ".", "PIL", ".", "Image", ".", "open", "(", "image", ".", "imageURL", ")", "\n", "image_data", ".", "load", "(", ")", "\n", "image", ".", "set_embedding", "(", "self", ".", "name", ",", "self", ".", "transforms", "(", "image_data", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.IdentityImageEmbeddings.embedding_length": [[3690, 3693], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.IdentityImageEmbeddings.__str__": [[3694, 3696], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.PrecomputedImageEmbeddings.__init__": [[3699, 3705], ["len", "super().__init__", "list", "embeddings.PrecomputedImageEmbeddings.url2tensor_dict.values"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "url2tensor_dict", ",", "name", ")", ":", "\n", "        ", "self", ".", "url2tensor_dict", "=", "url2tensor_dict", "\n", "self", ".", "name", "=", "name", "\n", "self", ".", "__embedding_length", "=", "len", "(", "list", "(", "self", ".", "url2tensor_dict", ".", "values", "(", ")", ")", "[", "0", "]", ")", "\n", "self", ".", "static_embeddings", "=", "True", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.PrecomputedImageEmbeddings._add_embeddings_internal": [[3706, 3713], ["image.set_embedding", "image.set_embedding", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "images", ":", "List", "[", "Image", "]", ")", "->", "List", "[", "Image", "]", ":", "\n", "        ", "for", "image", "in", "images", ":", "\n", "            ", "if", "image", ".", "imageURL", "in", "self", ".", "url2tensor_dict", ":", "\n", "                ", "image", ".", "set_embedding", "(", "self", ".", "name", ",", "self", ".", "url2tensor_dict", "[", "image", ".", "imageURL", "]", ")", "\n", "", "else", ":", "\n", "                ", "image", ".", "set_embedding", "(", "\n", "self", ".", "name", ",", "torch", ".", "zeros", "(", "self", ".", "__embedding_length", ",", "device", "=", "flair", ".", "device", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.PrecomputedImageEmbeddings.embedding_length": [[3715, 3718], ["None"], "methods", ["None"], ["", "", "", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.PrecomputedImageEmbeddings.__str__": [[3719, 3721], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.NetworkImageEmbeddings.__init__": [[3724, 3771], ["super().__init__", "torchvision.transforms.Compose", "torchvision.transforms.ToTensor", "model_constructor", "model_features", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "Exception", "log.warning", "log.warning", "log.warning", "log.warning", "torchvision.transforms.Normalize", "model_constructor.children", "list", "list", "torch.nn.AdaptiveAvgPool2d", "torch.nn.AdaptiveAvgPool2d", "torch.nn.AdaptiveAvgPool2d", "torch.nn.AdaptiveAvgPool2d"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "name", ",", "pretrained", "=", "True", ",", "transforms", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "try", ":", "\n", "            ", "import", "torchvision", "as", "torchvision", "\n", "", "except", "ModuleNotFoundError", ":", "\n", "            ", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "log", ".", "warning", "(", "'ATTENTION! The library \"torchvision\" is not installed!'", ")", "\n", "log", ".", "warning", "(", "\n", "'To use convnets pretraned on ImageNet, please first install with \"pip install torchvision\"'", "\n", ")", "\n", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "pass", "\n", "\n", "", "model_info", "=", "{", "\n", "\"resnet50\"", ":", "(", "torchvision", ".", "models", ".", "resnet50", ",", "lambda", "x", ":", "list", "(", "x", ")", "[", ":", "-", "1", "]", ",", "2048", ")", ",", "\n", "\"mobilenet_v2\"", ":", "(", "\n", "torchvision", ".", "models", ".", "mobilenet_v2", ",", "\n", "lambda", "x", ":", "list", "(", "x", ")", "[", ":", "-", "1", "]", "+", "[", "torch", ".", "nn", ".", "AdaptiveAvgPool2d", "(", "(", "1", ",", "1", ")", ")", "]", ",", "\n", "1280", ",", "\n", ")", ",", "\n", "}", "\n", "\n", "transforms", "=", "[", "]", "if", "transforms", "is", "None", "else", "transforms", "\n", "transforms", "+=", "[", "torchvision", ".", "transforms", ".", "ToTensor", "(", ")", "]", "\n", "if", "pretrained", ":", "\n", "            ", "imagenet_mean", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", "\n", "imagenet_std", "=", "[", "0.229", ",", "0.224", ",", "0.225", "]", "\n", "transforms", "+=", "[", "\n", "torchvision", ".", "transforms", ".", "Normalize", "(", "mean", "=", "imagenet_mean", ",", "std", "=", "imagenet_std", ")", "\n", "]", "\n", "", "self", ".", "transforms", "=", "torchvision", ".", "transforms", ".", "Compose", "(", "transforms", ")", "\n", "\n", "if", "name", "in", "model_info", ":", "\n", "            ", "model_constructor", "=", "model_info", "[", "name", "]", "[", "0", "]", "\n", "model_features", "=", "model_info", "[", "name", "]", "[", "1", "]", "\n", "embedding_length", "=", "model_info", "[", "name", "]", "[", "2", "]", "\n", "\n", "net", "=", "model_constructor", "(", "pretrained", "=", "pretrained", ")", "\n", "modules", "=", "model_features", "(", "net", ".", "children", "(", ")", ")", "\n", "self", ".", "features", "=", "torch", ".", "nn", ".", "Sequential", "(", "*", "modules", ")", "\n", "\n", "self", ".", "__embedding_length", "=", "embedding_length", "\n", "\n", "self", ".", "name", "=", "name", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "f\"Image embeddings {name} not available.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.NetworkImageEmbeddings._add_embeddings_internal": [[3772, 3786], ["torch.stack", "torch.stack", "torch.stack", "torch.stack", "embeddings.NetworkImageEmbeddings.features", "enumerate", "embeddings.NetworkImageEmbeddings.view", "embeddings.NetworkImageEmbeddings.dim", "Exception", "image.set_embedding", "embeddings.NetworkImageEmbeddings.transforms", "embeddings.NetworkImageEmbeddings.dim", "embeddings.NetworkImageEmbeddings.dim"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding"], ["", "", "def", "_add_embeddings_internal", "(", "self", ",", "images", ":", "List", "[", "Image", "]", ")", "->", "List", "[", "Image", "]", ":", "\n", "        ", "image_tensor", "=", "torch", ".", "stack", "(", "[", "self", ".", "transforms", "(", "image", ".", "data", ")", "for", "image", "in", "images", "]", ")", "\n", "image_embeddings", "=", "self", ".", "features", "(", "image_tensor", ")", "\n", "image_embeddings", "=", "(", "\n", "image_embeddings", ".", "view", "(", "image_embeddings", ".", "shape", "[", ":", "2", "]", ")", "\n", "if", "image_embeddings", ".", "dim", "(", ")", "==", "4", "\n", "else", "image_embeddings", "\n", ")", "\n", "if", "image_embeddings", ".", "dim", "(", ")", "!=", "2", ":", "\n", "            ", "raise", "Exception", "(", "\n", "f\"Unknown embedding shape of length {image_embeddings.dim()}\"", "\n", ")", "\n", "", "for", "image_id", ",", "image", "in", "enumerate", "(", "images", ")", ":", "\n", "            ", "image", ".", "set_embedding", "(", "self", ".", "name", ",", "image_embeddings", "[", "image_id", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.NetworkImageEmbeddings.embedding_length": [[3787, 3790], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "__embedding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.NetworkImageEmbeddings.__str__": [[3791, 3793], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ConvTransformNetworkImageEmbeddings.__init__": [[3796, 3885], ["super().__init__", "convnet_arch.extend", "enumerate", "convnet_arch.append", "torch.nn.Sequential", "torch.nn.Sequential", "convnet_arch.append", "zip", "convnet_arch.append", "convnet_arch.append", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.TransformerEncoderLayer", "torch.nn.TransformerEncoderLayer", "torch.nn.TransformerEncoder", "torch.nn.TransformerEncoder", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Dropout2d", "torch.nn.Dropout2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.MaxPool2d", "torch.nn.MaxPool2d", "convnet_arch.append", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.ReLU", "torch.nn.ReLU", "str", "convnet_arch.append", "torch.nn.Dropout2d", "torch.nn.Dropout2d", "torch.nn.MaxPool2d", "torch.nn.MaxPool2d", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "feats_in", ",", "convnet_parms", ",", "posnet_parms", ",", "transformer_parms", ")", ":", "\n", "        ", "super", "(", "ConvTransformNetworkImageEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "adaptive_pool_func_map", "=", "{", "\"max\"", ":", "AdaptiveMaxPool2d", ",", "\"avg\"", ":", "AdaptiveAvgPool2d", "}", "\n", "\n", "convnet_arch", "=", "(", "\n", "[", "]", "\n", "if", "convnet_parms", "[", "\"dropout\"", "]", "[", "0", "]", "<=", "0", "\n", "else", "[", "Dropout2d", "(", "convnet_parms", "[", "\"dropout\"", "]", "[", "0", "]", ")", "]", "\n", ")", "\n", "convnet_arch", ".", "extend", "(", "\n", "[", "\n", "Conv2d", "(", "\n", "in_channels", "=", "feats_in", ",", "\n", "out_channels", "=", "convnet_parms", "[", "\"n_feats_out\"", "]", "[", "0", "]", ",", "\n", "kernel_size", "=", "convnet_parms", "[", "\"kernel_sizes\"", "]", "[", "0", "]", ",", "\n", "padding", "=", "convnet_parms", "[", "\"kernel_sizes\"", "]", "[", "0", "]", "[", "0", "]", "//", "2", ",", "\n", "stride", "=", "convnet_parms", "[", "\"strides\"", "]", "[", "0", "]", ",", "\n", "groups", "=", "convnet_parms", "[", "\"groups\"", "]", "[", "0", "]", ",", "\n", ")", ",", "\n", "ReLU", "(", ")", ",", "\n", "]", "\n", ")", "\n", "if", "\"0\"", "in", "convnet_parms", "[", "\"pool_layers_map\"", "]", ":", "\n", "            ", "convnet_arch", ".", "append", "(", "\n", "MaxPool2d", "(", "kernel_size", "=", "convnet_parms", "[", "\"pool_layers_map\"", "]", "[", "\"0\"", "]", ")", "\n", ")", "\n", "", "for", "layer_id", ",", "(", "kernel_size", ",", "n_in", ",", "n_out", ",", "groups", ",", "stride", ",", "dropout", ")", "in", "enumerate", "(", "\n", "zip", "(", "\n", "convnet_parms", "[", "\"kernel_sizes\"", "]", "[", "1", ":", "]", ",", "\n", "convnet_parms", "[", "\"n_feats_out\"", "]", "[", ":", "-", "1", "]", ",", "\n", "convnet_parms", "[", "\"n_feats_out\"", "]", "[", "1", ":", "]", ",", "\n", "convnet_parms", "[", "\"groups\"", "]", "[", "1", ":", "]", ",", "\n", "convnet_parms", "[", "\"strides\"", "]", "[", "1", ":", "]", ",", "\n", "convnet_parms", "[", "\"dropout\"", "]", "[", "1", ":", "]", ",", "\n", ")", "\n", ")", ":", "\n", "            ", "if", "dropout", ">", "0", ":", "\n", "                ", "convnet_arch", ".", "append", "(", "Dropout2d", "(", "dropout", ")", ")", "\n", "", "convnet_arch", ".", "append", "(", "\n", "Conv2d", "(", "\n", "in_channels", "=", "n_in", ",", "\n", "out_channels", "=", "n_out", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "padding", "=", "kernel_size", "[", "0", "]", "//", "2", ",", "\n", "stride", "=", "stride", ",", "\n", "groups", "=", "groups", ",", "\n", ")", "\n", ")", "\n", "convnet_arch", ".", "append", "(", "ReLU", "(", ")", ")", "\n", "if", "str", "(", "layer_id", "+", "1", ")", "in", "convnet_parms", "[", "\"pool_layers_map\"", "]", ":", "\n", "                ", "convnet_arch", ".", "append", "(", "\n", "MaxPool2d", "(", "\n", "kernel_size", "=", "convnet_parms", "[", "\"pool_layers_map\"", "]", "[", "str", "(", "layer_id", "+", "1", ")", "]", "\n", ")", "\n", ")", "\n", "", "", "convnet_arch", ".", "append", "(", "\n", "adaptive_pool_func_map", "[", "convnet_parms", "[", "\"adaptive_pool_func\"", "]", "]", "(", "\n", "output_size", "=", "convnet_parms", "[", "\"output_size\"", "]", "\n", ")", "\n", ")", "\n", "self", ".", "conv_features", "=", "Sequential", "(", "*", "convnet_arch", ")", "\n", "conv_feat_dim", "=", "convnet_parms", "[", "\"n_feats_out\"", "]", "[", "-", "1", "]", "\n", "if", "posnet_parms", "is", "not", "None", "and", "transformer_parms", "is", "not", "None", ":", "\n", "            ", "self", ".", "use_transformer", "=", "True", "\n", "if", "posnet_parms", "[", "\"nonlinear\"", "]", ":", "\n", "                ", "posnet_arch", "=", "[", "\n", "Linear", "(", "2", ",", "posnet_parms", "[", "\"n_hidden\"", "]", ")", ",", "\n", "ReLU", "(", ")", ",", "\n", "Linear", "(", "posnet_parms", "[", "\"n_hidden\"", "]", ",", "conv_feat_dim", ")", ",", "\n", "]", "\n", "", "else", ":", "\n", "                ", "posnet_arch", "=", "[", "Linear", "(", "2", ",", "conv_feat_dim", ")", "]", "\n", "", "self", ".", "position_features", "=", "Sequential", "(", "*", "posnet_arch", ")", "\n", "transformer_layer", "=", "TransformerEncoderLayer", "(", "\n", "d_model", "=", "conv_feat_dim", ",", "**", "transformer_parms", "[", "\"transformer_encoder_parms\"", "]", "\n", ")", "\n", "self", ".", "transformer", "=", "TransformerEncoder", "(", "\n", "transformer_layer", ",", "num_layers", "=", "transformer_parms", "[", "\"n_blocks\"", "]", "\n", ")", "\n", "# <cls> token initially set to 1/D, so it attends to all image features equally", "\n", "self", ".", "cls_token", "=", "Parameter", "(", "torch", ".", "ones", "(", "conv_feat_dim", ",", "1", ")", "/", "conv_feat_dim", ")", "\n", "self", ".", "_feat_dim", "=", "conv_feat_dim", "\n", "", "else", ":", "\n", "            ", "self", ".", "use_transformer", "=", "False", "\n", "self", ".", "_feat_dim", "=", "(", "\n", "convnet_parms", "[", "\"output_size\"", "]", "[", "0", "]", "\n", "*", "convnet_parms", "[", "\"output_size\"", "]", "[", "1", "]", "\n", "*", "conv_feat_dim", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ConvTransformNetworkImageEmbeddings.forward": [[3887, 3925], ["embeddings.ConvTransformNetworkImageEmbeddings.conv_features", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "y.unsqueeze.unsqueeze.view().transpose", "y.unsqueeze.unsqueeze.type().to", "embeddings.ConvTransformNetworkImageEmbeddings.position_features().transpose().view", "y.unsqueeze.unsqueeze.unsqueeze", "torch.layer_norm.view", "torch.layer_norm().permute", "torch.layer_norm().permute", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.layer_norm.view().transpose().view", "embeddings.ConvTransformNetworkImageEmbeddings.transformer", "torch.layer_norm.view", "torch.layer_norm", "torch.layer_norm", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "y.unsqueeze.unsqueeze.view", "y.unsqueeze.unsqueeze.type", "embeddings.ConvTransformNetworkImageEmbeddings.position_features().transpose", "torch.layer_norm", "torch.layer_norm", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.layer_norm.view().transpose", "torch.layer_norm.permute", "embeddings.ConvTransformNetworkImageEmbeddings.position_features", "torch.layer_norm.view", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv_features", "(", "x", ")", "# [b, d, h, w]", "\n", "b", ",", "d", ",", "h", ",", "w", "=", "x", ".", "shape", "\n", "if", "self", ".", "use_transformer", ":", "\n", "# add positional encodings", "\n", "            ", "y", "=", "torch", ".", "stack", "(", "\n", "[", "\n", "torch", ".", "cat", "(", "[", "torch", ".", "arange", "(", "h", ")", ".", "unsqueeze", "(", "1", ")", "]", "*", "w", ",", "dim", "=", "1", ")", ",", "\n", "torch", ".", "cat", "(", "[", "torch", ".", "arange", "(", "w", ")", ".", "unsqueeze", "(", "0", ")", "]", "*", "h", ",", "dim", "=", "0", ")", ",", "\n", "]", "\n", ")", "# [2, h, w", "\n", "y", "=", "y", ".", "view", "(", "[", "2", ",", "h", "*", "w", "]", ")", ".", "transpose", "(", "1", ",", "0", ")", "# [h*w, 2]", "\n", "y", "=", "y", ".", "type", "(", "torch", ".", "float32", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "y", "=", "(", "\n", "self", ".", "position_features", "(", "y", ")", ".", "transpose", "(", "1", ",", "0", ")", ".", "view", "(", "[", "d", ",", "h", ",", "w", "]", ")", "\n", ")", "# [h*w, d] => [d, h, w]", "\n", "y", "=", "y", ".", "unsqueeze", "(", "dim", "=", "0", ")", "# [1, d, h, w]", "\n", "x", "=", "x", "+", "y", "# [b, d, h, w] + [1, d, h, w] => [b, d, h, w]", "\n", "# reshape the pixels into the sequence", "\n", "x", "=", "x", ".", "view", "(", "[", "b", ",", "d", ",", "h", "*", "w", "]", ")", "# [b, d, h*w]", "\n", "# layer norm after convolution and positional encodings", "\n", "x", "=", "F", ".", "layer_norm", "(", "x", ".", "permute", "(", "[", "0", ",", "2", ",", "1", "]", ")", ",", "(", "d", ",", ")", ")", ".", "permute", "(", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "# add <cls> token", "\n", "x", "=", "torch", ".", "cat", "(", "\n", "[", "x", ",", "torch", ".", "stack", "(", "[", "self", ".", "cls_token", "]", "*", "b", ")", "]", ",", "dim", "=", "2", "\n", ")", "# [b, d, h*w+1]", "\n", "# transformer requires input in the shape [h*w+1, b, d]", "\n", "x", "=", "(", "\n", "x", ".", "view", "(", "[", "b", "*", "d", ",", "h", "*", "w", "+", "1", "]", ")", ".", "transpose", "(", "1", ",", "0", ")", ".", "view", "(", "[", "h", "*", "w", "+", "1", ",", "b", ",", "d", "]", ")", "\n", ")", "# [b, d, h*w+1] => [b*d, h*w+1] => [h*w+1, b*d] => [h*w+1, b*d]", "\n", "x", "=", "self", ".", "transformer", "(", "x", ")", "# [h*w+1, b, d]", "\n", "# the output is an embedding of <cls> token", "\n", "x", "=", "x", "[", "-", "1", ",", ":", ",", ":", "]", "# [b, d]", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", ".", "view", "(", "[", "-", "1", ",", "self", ".", "_feat_dim", "]", ")", "\n", "x", "=", "F", ".", "layer_norm", "(", "x", ",", "(", "self", ".", "_feat_dim", ",", ")", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ConvTransformNetworkImageEmbeddings._add_embeddings_internal": [[3926, 3931], ["torch.stack", "torch.stack", "torch.stack", "torch.stack", "embeddings.ConvTransformNetworkImageEmbeddings.forward", "enumerate", "image.set_embedding"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding"], ["", "def", "_add_embeddings_internal", "(", "self", ",", "images", ":", "List", "[", "Image", "]", ")", "->", "List", "[", "Image", "]", ":", "\n", "        ", "image_tensor", "=", "torch", ".", "stack", "(", "[", "image", ".", "data", "for", "image", "in", "images", "]", ")", "\n", "image_embeddings", "=", "self", ".", "forward", "(", "image_tensor", ")", "\n", "for", "image_id", ",", "image", "in", "enumerate", "(", "images", ")", ":", "\n", "            ", "image", ".", "set_embedding", "(", "self", ".", "name", ",", "image_embeddings", "[", "image_id", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ConvTransformNetworkImageEmbeddings.embedding_length": [[3932, 3935], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "embedding_length", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_feat_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.ConvTransformNetworkImageEmbeddings.__str__": [[3936, 3938], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BPEmbSerializable.__getstate__": [[3969, 3975], ["embeddings.BPEmbSerializable.__dict__.copy", "open().read", "open"], "methods", ["None"], ["    ", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "state", "=", "self", ".", "__dict__", ".", "copy", "(", ")", "\n", "# save the sentence piece rpbert as binary file (not as path which may change)", "\n", "state", "[", "\"spm_model_binary\"", "]", "=", "open", "(", "self", ".", "model_file", ",", "mode", "=", "\"rb\"", ")", ".", "read", "(", ")", "\n", "state", "[", "\"spm\"", "]", "=", "None", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.BPEmbSerializable.__setstate__": [[3976, 3997], ["embeddings.BPEmbSerializable.model_tpl.format", "sentencepiece_load", "pathlib.Path", "embeddings.BPEmbSerializable._load_file", "os.path.exists", "os.makedirs", "open", "out.write"], "methods", ["None"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "from", "bpemb", ".", "util", "import", "sentencepiece_load", "\n", "\n", "model_file", "=", "self", ".", "model_tpl", ".", "format", "(", "lang", "=", "state", "[", "\"lang\"", "]", ",", "vs", "=", "state", "[", "\"vs\"", "]", ")", "\n", "self", ".", "__dict__", "=", "state", "\n", "\n", "# write out the binary sentence piece rpbert into the expected directory", "\n", "self", ".", "cache_dir", ":", "Path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"embeddings\"", "\n", "if", "\"spm_model_binary\"", "in", "self", ".", "__dict__", ":", "\n", "# if the rpbert was saved as binary and it is not found on disk, write to appropriate path", "\n", "            ", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "cache_dir", "/", "state", "[", "\"lang\"", "]", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "self", ".", "cache_dir", "/", "state", "[", "\"lang\"", "]", ")", "\n", "", "self", ".", "model_file", "=", "self", ".", "cache_dir", "/", "model_file", "\n", "with", "open", "(", "self", ".", "model_file", ",", "\"wb\"", ")", "as", "out", ":", "\n", "                ", "out", ".", "write", "(", "self", ".", "__dict__", "[", "\"spm_model_binary\"", "]", ")", "\n", "", "", "else", ":", "\n", "# otherwise, use normal process and potentially trigger another download", "\n", "            ", "self", ".", "model_file", "=", "self", ".", "_load_file", "(", "model_file", ")", "\n", "\n", "# once the modes if there, load it with sentence piece", "\n", "", "state", "[", "\"spm\"", "]", "=", "sentencepiece_load", "(", "self", ".", "model_file", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._extract_embeddings": [[1036, 1086], ["subtoken_embeddings.append", "embeddings.ScalarMix", "ScalarMix.", "torch.cat", "torch.cat", "len", "torch.mean", "torch.mean", "embedding.unsqueeze", "torch.cat", "torch.cat"], "function", ["None"], ["", "", "def", "_extract_embeddings", "(", "\n", "hidden_states", ":", "List", "[", "torch", ".", "FloatTensor", "]", ",", "\n", "layers", ":", "List", "[", "int", "]", ",", "\n", "pooling_operation", ":", "str", ",", "\n", "subword_start_idx", ":", "int", ",", "\n", "subword_end_idx", ":", "int", ",", "\n", "use_scalar_mix", ":", "bool", "=", "False", ",", "\n", ")", "->", "List", "[", "torch", ".", "FloatTensor", "]", ":", "\n", "    ", "\"\"\"\n    Extracts subword embeddings from specified layers from hidden states.\n    :param hidden_states: list of hidden states from rpbert\n    :param layers: list of layers\n    :param pooling_operation: pooling operation for subword embeddings (supported: first, last, first_last and mean)\n    :param subword_start_idx: defines start index for subword\n    :param subword_end_idx: defines end index for subword\n    :param use_scalar_mix: determines, if scalar mix should be used\n    :return: list of extracted subword embeddings\n    \"\"\"", "\n", "subtoken_embeddings", ":", "List", "[", "torch", ".", "FloatTensor", "]", "=", "[", "]", "\n", "\n", "for", "layer", "in", "layers", ":", "\n", "        ", "current_embeddings", "=", "hidden_states", "[", "layer", "]", "[", "0", "]", "[", "subword_start_idx", ":", "subword_end_idx", "]", "\n", "\n", "first_embedding", ":", "torch", ".", "FloatTensor", "=", "current_embeddings", "[", "0", "]", "\n", "if", "pooling_operation", "==", "\"first_last\"", ":", "\n", "            ", "last_embedding", ":", "torch", ".", "FloatTensor", "=", "current_embeddings", "[", "-", "1", "]", "\n", "final_embedding", ":", "torch", ".", "FloatTensor", "=", "torch", ".", "cat", "(", "\n", "[", "first_embedding", ",", "last_embedding", "]", "\n", ")", "\n", "", "elif", "pooling_operation", "==", "\"last\"", ":", "\n", "            ", "final_embedding", ":", "torch", ".", "FloatTensor", "=", "current_embeddings", "[", "-", "1", "]", "\n", "", "elif", "pooling_operation", "==", "\"mean\"", ":", "\n", "            ", "all_embeddings", ":", "List", "[", "torch", ".", "FloatTensor", "]", "=", "[", "\n", "embedding", ".", "unsqueeze", "(", "0", ")", "for", "embedding", "in", "current_embeddings", "\n", "]", "\n", "final_embedding", ":", "torch", ".", "FloatTensor", "=", "torch", ".", "mean", "(", "\n", "torch", ".", "cat", "(", "all_embeddings", ",", "dim", "=", "0", ")", ",", "dim", "=", "0", "\n", ")", "\n", "", "else", ":", "\n", "            ", "final_embedding", ":", "torch", ".", "FloatTensor", "=", "first_embedding", "\n", "\n", "", "subtoken_embeddings", ".", "append", "(", "final_embedding", ")", "\n", "\n", "", "if", "use_scalar_mix", ":", "\n", "        ", "sm", "=", "ScalarMix", "(", "mixture_size", "=", "len", "(", "subtoken_embeddings", ")", ")", "\n", "sm_embeddings", "=", "sm", "(", "subtoken_embeddings", ")", "\n", "\n", "subtoken_embeddings", "=", "[", "sm_embeddings", "]", "\n", "\n", "", "return", "subtoken_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._build_token_subwords_mapping": [[1088, 1112], ["tokenizer.tokenize", "tokens.append", "len"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize"], ["", "def", "_build_token_subwords_mapping", "(", "\n", "sentence", ":", "Sentence", ",", "tokenizer", ":", "PreTrainedTokenizer", "\n", ")", "->", "Tuple", "[", "Dict", "[", "int", ",", "int", "]", ",", "str", "]", ":", "\n", "    ", "\"\"\" Builds a dictionary that stores the following information:\n    Token index (key) and number of corresponding subwords (value) for a sentence.\n\n    :param sentence: input sentence\n    :param tokenizer: Transformers tokenization object\n    :return: dictionary of token index to corresponding number of subwords, tokenized string\n    \"\"\"", "\n", "token_subwords_mapping", ":", "Dict", "[", "int", ",", "int", "]", "=", "{", "}", "\n", "\n", "tokens", "=", "[", "]", "\n", "\n", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "        ", "token_text", "=", "token", ".", "text", "\n", "\n", "subwords", "=", "tokenizer", ".", "tokenize", "(", "token_text", ")", "\n", "\n", "tokens", ".", "append", "(", "token", ".", "text", "if", "subwords", "else", "tokenizer", ".", "unk_token", ")", "\n", "\n", "token_subwords_mapping", "[", "token", ".", "idx", "]", "=", "len", "(", "subwords", ")", "if", "subwords", "else", "1", "\n", "\n", "", "return", "token_subwords_mapping", ",", "\" \"", ".", "join", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._build_token_subwords_mapping_gpt2": [[1114, 1143], ["tokens.append", "tokenizer.tokenize", "len", "tokenizer.tokenize"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize"], ["", "def", "_build_token_subwords_mapping_gpt2", "(", "\n", "sentence", ":", "Sentence", ",", "tokenizer", ":", "PreTrainedTokenizer", "\n", ")", "->", "Tuple", "[", "Dict", "[", "int", ",", "int", "]", ",", "str", "]", ":", "\n", "    ", "\"\"\" Builds a dictionary that stores the following information:\n    Token index (key) and number of corresponding subwords (value) for a sentence.\n\n    :param sentence: input sentence\n    :param tokenizer: Transformers tokenization object\n    :return: dictionary of token index to corresponding number of subwords, tokenized string\n    \"\"\"", "\n", "token_subwords_mapping", ":", "Dict", "[", "int", ",", "int", "]", "=", "{", "}", "\n", "\n", "tokens", "=", "[", "]", "\n", "\n", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "# Dummy token is needed to get the actually token tokenized correctly with special ``\u0120`` symbol", "\n", "\n", "        ", "if", "token", ".", "idx", "==", "1", ":", "\n", "            ", "token_text", "=", "token", ".", "text", "\n", "subwords", "=", "tokenizer", ".", "tokenize", "(", "token_text", ")", "\n", "", "else", ":", "\n", "            ", "token_text", "=", "\"X \"", "+", "token", ".", "text", "\n", "subwords", "=", "tokenizer", ".", "tokenize", "(", "token_text", ")", "[", "1", ":", "]", "\n", "\n", "", "tokens", ".", "append", "(", "token", ".", "text", "if", "subwords", "else", "tokenizer", ".", "unk_token", ")", "\n", "\n", "token_subwords_mapping", "[", "token", ".", "idx", "]", "=", "len", "(", "subwords", ")", "if", "subwords", "else", "1", "\n", "\n", "", "return", "token_subwords_mapping", ",", "\" \"", ".", "join", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._get_transformer_sentence_embeddings": [[1145, 1223], ["torch.no_grad", "torch.no_grad", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "torch.tensor", "torch.tensor", "tokens_tensor.to.to", "embeddings._build_token_subwords_mapping_gpt2", "embeddings._build_token_subwords_mapping", "model", "embeddings._extract_embeddings", "torch.cat", "torch.cat", "token.set_embedding"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._build_token_subwords_mapping_gpt2", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._build_token_subwords_mapping", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings._extract_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.set_embedding"], ["", "def", "_get_transformer_sentence_embeddings", "(", "\n", "sentences", ":", "List", "[", "Sentence", "]", ",", "\n", "tokenizer", ":", "PreTrainedTokenizer", ",", "\n", "model", ":", "PreTrainedModel", ",", "\n", "name", ":", "str", ",", "\n", "layers", ":", "List", "[", "int", "]", ",", "\n", "pooling_operation", ":", "str", ",", "\n", "use_scalar_mix", ":", "bool", ",", "\n", "bos_token", ":", "str", "=", "None", ",", "\n", "eos_token", ":", "str", "=", "None", ",", "\n", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "    ", "\"\"\"\n    Builds sentence embeddings for Transformer-based architectures.\n    :param sentences: input sentences\n    :param tokenizer: tokenization object\n    :param model: rpbert object\n    :param name: name of the Transformer-based rpbert\n    :param layers: list of layers\n    :param pooling_operation: defines pooling operation for subword extraction\n    :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n    :param bos_token: defines begin of sentence token (used for left padding)\n    :param eos_token: defines end of sentence token (used for right padding)\n    :return: list of sentences (each token of a sentence is now embedded)\n    \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "sentence", "in", "sentences", ":", "\n", "            ", "token_subwords_mapping", ":", "Dict", "[", "int", ",", "int", "]", "=", "{", "}", "\n", "\n", "if", "(", "\"gpt2\"", "in", "name", "or", "\"roberta\"", "in", "name", ")", "and", "\"xlm\"", "not", "in", "name", ":", "\n", "                ", "(", "\n", "token_subwords_mapping", ",", "\n", "tokenized_string", ",", "\n", ")", "=", "_build_token_subwords_mapping_gpt2", "(", "\n", "sentence", "=", "sentence", ",", "tokenizer", "=", "tokenizer", "\n", ")", "\n", "", "else", ":", "\n", "                ", "(", "\n", "token_subwords_mapping", ",", "\n", "tokenized_string", ",", "\n", ")", "=", "_build_token_subwords_mapping", "(", "\n", "sentence", "=", "sentence", ",", "tokenizer", "=", "tokenizer", "\n", ")", "\n", "\n", "", "subwords", "=", "tokenizer", ".", "tokenize", "(", "tokenized_string", ")", "\n", "\n", "offset", "=", "0", "\n", "\n", "if", "bos_token", ":", "\n", "                ", "subwords", "=", "[", "bos_token", "]", "+", "subwords", "\n", "offset", "=", "1", "\n", "\n", "", "if", "eos_token", ":", "\n", "                ", "subwords", "=", "subwords", "+", "[", "eos_token", "]", "\n", "\n", "", "indexed_tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "subwords", ")", "\n", "tokens_tensor", "=", "torch", ".", "tensor", "(", "[", "indexed_tokens", "]", ")", "\n", "tokens_tensor", "=", "tokens_tensor", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "hidden_states", "=", "model", "(", "tokens_tensor", ")", "[", "-", "1", "]", "\n", "\n", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "                ", "len_subwords", "=", "token_subwords_mapping", "[", "token", ".", "idx", "]", "\n", "\n", "subtoken_embeddings", "=", "_extract_embeddings", "(", "\n", "hidden_states", "=", "hidden_states", ",", "\n", "layers", "=", "layers", ",", "\n", "pooling_operation", "=", "pooling_operation", ",", "\n", "subword_start_idx", "=", "offset", ",", "\n", "subword_end_idx", "=", "offset", "+", "len_subwords", ",", "\n", "use_scalar_mix", "=", "use_scalar_mix", ",", "\n", ")", "\n", "\n", "offset", "+=", "len_subwords", "\n", "\n", "final_subtoken_embedding", "=", "torch", ".", "cat", "(", "subtoken_embeddings", ")", "\n", "token", ".", "set_embedding", "(", "name", ",", "final_subtoken_embedding", ")", "\n", "\n", "", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.replace_with_language_code": [[3940, 3965], ["string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace"], "function", ["None"], ["", "", "def", "replace_with_language_code", "(", "string", ":", "str", ")", ":", "\n", "    ", "string", "=", "string", ".", "replace", "(", "\"arabic-\"", ",", "\"ar-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"basque-\"", ",", "\"eu-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"bulgarian-\"", ",", "\"bg-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"croatian-\"", ",", "\"hr-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"czech-\"", ",", "\"cs-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"danish-\"", ",", "\"da-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"dutch-\"", ",", "\"nl-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"farsi-\"", ",", "\"fa-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"persian-\"", ",", "\"fa-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"finnish-\"", ",", "\"fi-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"french-\"", ",", "\"fr-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"german-\"", ",", "\"de-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"hebrew-\"", ",", "\"he-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"hindi-\"", ",", "\"hi-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"indonesian-\"", ",", "\"id-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"italian-\"", ",", "\"it-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"japanese-\"", ",", "\"ja-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"norwegian-\"", ",", "\"no\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"polish-\"", ",", "\"pl-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"portuguese-\"", ",", "\"pt-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"slovenian-\"", ",", "\"sl-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"spanish-\"", ",", "\"es-\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"swedish-\"", ",", "\"sv-\"", ")", "\n", "return", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.forward_loss": [[21, 27], ["None"], "methods", ["None"], ["@", "abstractmethod", "\n", "def", "forward_loss", "(", "\n", "self", ",", "data_points", ":", "Union", "[", "List", "[", "DataPoint", "]", ",", "DataPoint", "]", "\n", ")", "->", "torch", ".", "tensor", ":", "\n", "        ", "\"\"\"Performs a forward pass and returns a loss tensor for backpropagation. Implement this to enable training.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.evaluate": [[28, 44], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "evaluate", "(", "\n", "self", ",", "\n", "data_loader", ":", "DataLoader", ",", "\n", "out_path", ":", "Path", "=", "None", ",", "\n", "embedding_storage_mode", ":", "str", "=", "\"none\"", ",", "\n", ")", "->", "(", "Result", ",", "float", ")", ":", "\n", "        ", "\"\"\"Evaluates the rpbert. Returns a Result object containing evaluation\n        results and a loss value. Implement this to enable evaluation.\n        :param data_loader: DataLoader that iterates over dataset to be evaluated\n        :param out_path: Optional output path to store predictions\n        :param embedding_storage_mode: One of 'none', 'cpu' or 'gpu'. 'none' means all embeddings are deleted and\n        freshly recomputed, 'cpu' means all embeddings are stored on CPU, or 'gpu' means all embeddings are stored on GPU\n        :return: Returns a Tuple consisting of a Result object and a loss float value\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model._get_state_dict": [[45, 50], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "_get_state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the state dictionary for this rpbert. Implementing this enables the save() and save_checkpoint()\n        functionality.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model._init_model_with_state_dict": [[51, 57], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "@", "abstractmethod", "\n", "def", "_init_model_with_state_dict", "(", "state", ")", ":", "\n", "        ", "\"\"\"Initialize the rpbert from a state dictionary. Implementing this enables the load() and load_checkpoint()\n        functionality.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model._fetch_model": [[58, 62], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "@", "abstractmethod", "\n", "def", "_fetch_model", "(", "model_name", ")", "->", "str", ":", "\n", "        ", "return", "model_name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.save": [[63, 71], ["nn.Model._get_state_dict", "torch.save", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._get_state_dict", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save"], ["", "def", "save", "(", "self", ",", "model_file", ":", "Union", "[", "str", ",", "Path", "]", ")", ":", "\n", "        ", "\"\"\"\n        Saves the current rpbert to the provided file.\n        :param model_file: the rpbert file\n        \"\"\"", "\n", "model_state", "=", "self", ".", "_get_state_dict", "(", ")", "\n", "\n", "torch", ".", "save", "(", "model_state", ",", "str", "(", "model_file", ")", ",", "pickle_protocol", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load": [[72, 94], ["cls._fetch_model", "cls._init_model_with_state_dict", "cls._init_model_with_state_dict.eval", "cls._init_model_with_state_dict.to", "str", "warnings.catch_warnings", "warnings.filterwarnings", "flair.file_utils.load_big_file", "torch.load", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._fetch_model", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._init_model_with_state_dict", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.load_big_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["", "@", "classmethod", "\n", "def", "load", "(", "cls", ",", "model", ":", "Union", "[", "str", ",", "Path", "]", ")", ":", "\n", "        ", "\"\"\"\n        Loads the rpbert from the given file.\n        :param model: the rpbert file\n        :return: the loaded text classifier rpbert\n        \"\"\"", "\n", "model_file", "=", "cls", ".", "_fetch_model", "(", "str", "(", "model", ")", ")", "\n", "\n", "with", "warnings", ".", "catch_warnings", "(", ")", ":", "\n", "            ", "warnings", ".", "filterwarnings", "(", "\"ignore\"", ")", "\n", "# load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups", "\n", "# see https://github.com/zalandoresearch/flair/issues/351", "\n", "f", "=", "file_utils", ".", "load_big_file", "(", "str", "(", "model_file", ")", ")", "\n", "state", "=", "torch", ".", "load", "(", "f", ",", "map_location", "=", "flair", ".", "device", ")", "\n", "\n", "", "model", "=", "cls", ".", "_init_model_with_state_dict", "(", "state", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.LockedDropout.__init__": [[101, 106], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "dropout_rate", "=", "0.5", ",", "batch_first", "=", "True", ",", "inplace", "=", "False", ")", ":", "\n", "        ", "super", "(", "LockedDropout", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout_rate", "=", "dropout_rate", "\n", "self", ".", "batch_first", "=", "batch_first", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.LockedDropout.forward": [[107, 119], ["mask.expand_as.expand_as.expand_as", "x.data.new().bernoulli_", "x.data.new().bernoulli_", "torch.autograd.Variable", "x.data.new", "x.data.new", "x.size", "x.size", "x.size", "x.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "not", "self", ".", "training", "or", "not", "self", ".", "dropout_rate", ":", "\n", "            ", "return", "x", "\n", "\n", "", "if", "not", "self", ".", "batch_first", ":", "\n", "            ", "m", "=", "x", ".", "data", ".", "new", "(", "1", ",", "x", ".", "size", "(", "1", ")", ",", "x", ".", "size", "(", "2", ")", ")", ".", "bernoulli_", "(", "1", "-", "self", ".", "dropout_rate", ")", "\n", "", "else", ":", "\n", "            ", "m", "=", "x", ".", "data", ".", "new", "(", "x", ".", "size", "(", "0", ")", ",", "1", ",", "x", ".", "size", "(", "2", ")", ")", ".", "bernoulli_", "(", "1", "-", "self", ".", "dropout_rate", ")", "\n", "\n", "", "mask", "=", "torch", ".", "autograd", ".", "Variable", "(", "m", ",", "requires_grad", "=", "False", ")", "/", "(", "1", "-", "self", ".", "dropout_rate", ")", "\n", "mask", "=", "mask", ".", "expand_as", "(", "x", ")", "\n", "return", "mask", "*", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.LockedDropout.extra_repr": [[120, 123], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "inplace_str", "=", "\", inplace\"", "if", "self", ".", "inplace", "else", "\"\"", "\n", "return", "\"p={}{}\"", ".", "format", "(", "self", ".", "dropout_rate", ",", "inplace_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.WordDropout.__init__": [[130, 134], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "dropout_rate", "=", "0.05", ",", "inplace", "=", "False", ")", ":", "\n", "        ", "super", "(", "WordDropout", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout_rate", "=", "dropout_rate", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.WordDropout.forward": [[135, 143], ["x.data.new().bernoulli_", "torch.autograd.Variable", "x.data.new", "x.size", "x.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "not", "self", ".", "training", "or", "not", "self", ".", "dropout_rate", ":", "\n", "            ", "return", "x", "\n", "\n", "", "m", "=", "x", ".", "data", ".", "new", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "1", ")", ",", "1", ")", ".", "bernoulli_", "(", "1", "-", "self", ".", "dropout_rate", ")", "\n", "\n", "mask", "=", "torch", ".", "autograd", ".", "Variable", "(", "m", ",", "requires_grad", "=", "False", ")", "\n", "return", "mask", "*", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.WordDropout.extra_repr": [[144, 147], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "inplace_str", "=", "\", inplace\"", "if", "self", ".", "inplace", "else", "\"\"", "\n", "return", "\"p={}{}\"", ".", "format", "(", "self", ".", "dropout_rate", ",", "inplace_str", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Result.__init__": [[17, 24], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "main_score", ":", "float", ",", "log_header", ":", "str", ",", "log_line", ":", "str", ",", "detailed_results", ":", "str", "\n", ")", ":", "\n", "        ", "self", ".", "main_score", ":", "float", "=", "main_score", "\n", "self", ".", "log_header", ":", "str", "=", "log_header", "\n", "self", ".", "log_line", ":", "str", "=", "log_line", "\n", "self", ".", "detailed_results", ":", "str", "=", "detailed_results", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.__init__": [[27, 35], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "name", ",", "beta", "=", "1", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "beta", "=", "beta", "\n", "\n", "self", ".", "_tps", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_fps", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_tns", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_fns", "=", "defaultdict", "(", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_tp": [[36, 38], ["None"], "methods", ["None"], ["", "def", "add_tp", "(", "self", ",", "class_name", ")", ":", "\n", "        ", "self", ".", "_tps", "[", "class_name", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_tn": [[39, 41], ["None"], "methods", ["None"], ["", "def", "add_tn", "(", "self", ",", "class_name", ")", ":", "\n", "        ", "self", ".", "_tns", "[", "class_name", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_fp": [[42, 44], ["None"], "methods", ["None"], ["", "def", "add_fp", "(", "self", ",", "class_name", ")", ":", "\n", "        ", "self", ".", "_fps", "[", "class_name", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_fn": [[45, 47], ["None"], "methods", ["None"], ["", "def", "add_fn", "(", "self", ",", "class_name", ")", ":", "\n", "        ", "self", ".", "_fns", "[", "class_name", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp": [[48, 52], ["sum", "training_utils.Metric.get_classes"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_classes"], ["", "def", "get_tp", "(", "self", ",", "class_name", "=", "None", ")", ":", "\n", "        ", "if", "class_name", "is", "None", ":", "\n", "            ", "return", "sum", "(", "[", "self", ".", "_tps", "[", "class_name", "]", "for", "class_name", "in", "self", ".", "get_classes", "(", ")", "]", ")", "\n", "", "return", "self", ".", "_tps", "[", "class_name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tn": [[53, 57], ["sum", "training_utils.Metric.get_classes"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_classes"], ["", "def", "get_tn", "(", "self", ",", "class_name", "=", "None", ")", ":", "\n", "        ", "if", "class_name", "is", "None", ":", "\n", "            ", "return", "sum", "(", "[", "self", ".", "_tns", "[", "class_name", "]", "for", "class_name", "in", "self", ".", "get_classes", "(", ")", "]", ")", "\n", "", "return", "self", ".", "_tns", "[", "class_name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fp": [[58, 62], ["sum", "training_utils.Metric.get_classes"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_classes"], ["", "def", "get_fp", "(", "self", ",", "class_name", "=", "None", ")", ":", "\n", "        ", "if", "class_name", "is", "None", ":", "\n", "            ", "return", "sum", "(", "[", "self", ".", "_fps", "[", "class_name", "]", "for", "class_name", "in", "self", ".", "get_classes", "(", ")", "]", ")", "\n", "", "return", "self", ".", "_fps", "[", "class_name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fn": [[63, 67], ["sum", "training_utils.Metric.get_classes"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_classes"], ["", "def", "get_fn", "(", "self", ",", "class_name", "=", "None", ")", ":", "\n", "        ", "if", "class_name", "is", "None", ":", "\n", "            ", "return", "sum", "(", "[", "self", ".", "_fns", "[", "class_name", "]", "for", "class_name", "in", "self", ".", "get_classes", "(", ")", "]", ")", "\n", "", "return", "self", ".", "_fns", "[", "class_name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.precision": [[68, 75], ["training_utils.Metric.get_tp", "training_utils.Metric.get_fp", "training_utils.Metric.get_tp", "training_utils.Metric.get_tp", "training_utils.Metric.get_fp"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fp"], ["", "def", "precision", "(", "self", ",", "class_name", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "get_tp", "(", "class_name", ")", "+", "self", ".", "get_fp", "(", "class_name", ")", ">", "0", ":", "\n", "            ", "return", "(", "\n", "self", ".", "get_tp", "(", "class_name", ")", "\n", "/", "(", "self", ".", "get_tp", "(", "class_name", ")", "+", "self", ".", "get_fp", "(", "class_name", ")", ")", "\n", ")", "\n", "", "return", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.recall": [[76, 83], ["training_utils.Metric.get_tp", "training_utils.Metric.get_fn", "training_utils.Metric.get_tp", "training_utils.Metric.get_tp", "training_utils.Metric.get_fn"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fn"], ["", "def", "recall", "(", "self", ",", "class_name", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "get_tp", "(", "class_name", ")", "+", "self", ".", "get_fn", "(", "class_name", ")", ">", "0", ":", "\n", "            ", "return", "(", "\n", "self", ".", "get_tp", "(", "class_name", ")", "\n", "/", "(", "self", ".", "get_tp", "(", "class_name", ")", "+", "self", ".", "get_fn", "(", "class_name", ")", ")", "\n", ")", "\n", "", "return", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.f_score": [[84, 92], ["training_utils.Metric.precision", "training_utils.Metric.recall", "training_utils.Metric.recall", "training_utils.Metric.precision", "training_utils.Metric.recall", "training_utils.Metric.precision"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.precision", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.recall", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.recall", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.precision", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.recall", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.precision"], ["", "def", "f_score", "(", "self", ",", "class_name", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "precision", "(", "class_name", ")", "+", "self", ".", "recall", "(", "class_name", ")", ">", "0", ":", "\n", "            ", "return", "(", "\n", "(", "1", "+", "self", ".", "beta", "*", "self", ".", "beta", ")", "\n", "*", "(", "self", ".", "precision", "(", "class_name", ")", "*", "self", ".", "recall", "(", "class_name", ")", ")", "\n", "/", "(", "self", ".", "precision", "(", "class_name", ")", "*", "self", ".", "beta", "*", "self", ".", "beta", "+", "self", ".", "recall", "(", "class_name", ")", ")", "\n", ")", "\n", "", "return", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.accuracy": [[93, 108], ["training_utils.Metric.get_fn", "training_utils.Metric.get_tp", "training_utils.Metric.get_fp", "training_utils.Metric.get_tp", "training_utils.Metric.get_tn", "training_utils.Metric.get_tn", "training_utils.Metric.get_fn", "training_utils.Metric.get_tp", "training_utils.Metric.get_fp"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fp"], ["", "def", "accuracy", "(", "self", ",", "class_name", "=", "None", ")", ":", "\n", "        ", "if", "(", "\n", "self", ".", "get_tp", "(", "class_name", ")", "+", "self", ".", "get_fp", "(", "class_name", ")", "+", "self", ".", "get_fn", "(", "class_name", ")", "\n", ">", "0", "\n", ")", ":", "\n", "            ", "return", "(", "\n", "(", "self", ".", "get_tp", "(", "class_name", ")", "+", "self", ".", "get_tn", "(", "class_name", ")", ")", "\n", "/", "(", "\n", "self", ".", "get_tp", "(", "class_name", ")", "\n", "+", "self", ".", "get_fp", "(", "class_name", ")", "\n", "+", "self", ".", "get_fn", "(", "class_name", ")", "\n", "+", "self", ".", "get_tn", "(", "class_name", ")", "\n", ")", "\n", ")", "\n", "", "return", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.micro_avg_f_score": [[109, 111], ["training_utils.Metric.f_score"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.f_score"], ["", "def", "micro_avg_f_score", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "f_score", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.macro_avg_f_score": [[112, 118], ["training_utils.Metric.f_score", "len", "sum", "len", "training_utils.Metric.get_classes"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_classes"], ["", "def", "macro_avg_f_score", "(", "self", ")", ":", "\n", "        ", "class_f_scores", "=", "[", "self", ".", "f_score", "(", "class_name", ")", "for", "class_name", "in", "self", ".", "get_classes", "(", ")", "]", "\n", "if", "len", "(", "class_f_scores", ")", "==", "0", ":", "\n", "            ", "return", "0.0", "\n", "", "macro_f_score", "=", "sum", "(", "class_f_scores", ")", "/", "len", "(", "class_f_scores", ")", "\n", "return", "macro_f_score", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.micro_avg_accuracy": [[119, 121], ["training_utils.Metric.accuracy"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.accuracy"], ["", "def", "micro_avg_accuracy", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "accuracy", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.macro_avg_accuracy": [[122, 131], ["training_utils.Metric.accuracy", "len", "training_utils.Metric.get_classes", "sum", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_classes"], ["", "def", "macro_avg_accuracy", "(", "self", ")", ":", "\n", "        ", "class_accuracy", "=", "[", "\n", "self", ".", "accuracy", "(", "class_name", ")", "for", "class_name", "in", "self", ".", "get_classes", "(", ")", "\n", "]", "\n", "\n", "if", "len", "(", "class_accuracy", ")", ">", "0", ":", "\n", "            ", "return", "sum", "(", "class_accuracy", ")", "/", "len", "(", "class_accuracy", ")", "\n", "\n", "", "return", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_classes": [[132, 151], ["set", "set.sort", "itertools.chain", "list", "training_utils.Metric._tps.keys", "training_utils.Metric._fps.keys", "training_utils.Metric._tns.keys", "training_utils.Metric._fns.keys"], "methods", ["None"], ["", "def", "get_classes", "(", "self", ")", "->", "List", ":", "\n", "        ", "all_classes", "=", "set", "(", "\n", "itertools", ".", "chain", "(", "\n", "*", "[", "\n", "list", "(", "keys", ")", "\n", "for", "keys", "in", "[", "\n", "self", ".", "_tps", ".", "keys", "(", ")", ",", "\n", "self", ".", "_fps", ".", "keys", "(", ")", ",", "\n", "self", ".", "_tns", ".", "keys", "(", ")", ",", "\n", "self", ".", "_fns", ".", "keys", "(", ")", ",", "\n", "]", "\n", "]", "\n", ")", "\n", ")", "\n", "all_classes", "=", "[", "\n", "class_name", "for", "class_name", "in", "all_classes", "if", "class_name", "is", "not", "None", "\n", "]", "\n", "all_classes", ".", "sort", "(", ")", "\n", "return", "all_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.to_tsv": [[152, 155], ["training_utils.Metric.precision", "training_utils.Metric.recall", "training_utils.Metric.accuracy", "training_utils.Metric.micro_avg_f_score"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.precision", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.recall", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.micro_avg_f_score"], ["", "def", "to_tsv", "(", "self", ")", ":", "\n", "        ", "return", "\"{}\\t{}\\t{}\\t{}\"", ".", "format", "(", "\n", "self", ".", "precision", "(", ")", ",", "self", ".", "recall", "(", ")", ",", "self", ".", "accuracy", "(", ")", ",", "self", ".", "micro_avg_f_score", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.tsv_header": [[157, 163], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "tsv_header", "(", "prefix", "=", "None", ")", ":", "\n", "        ", "if", "prefix", ":", "\n", "            ", "return", "\"{0}_PRECISION\\t{0}_RECALL\\t{0}_ACCURACY\\t{0}_F-SCORE\"", ".", "format", "(", "prefix", ")", "\n", "\n", "", "return", "\"PRECISION\\tRECALL\\tACCURACY\\tF-SCORE\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.to_empty_tsv": [[164, 167], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "to_empty_tsv", "(", ")", ":", "\n", "        ", "return", "\"\\t_\\t_\\t_\\t_\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.__str__": [[168, 186], ["training_utils.Metric.get_classes", "training_utils.Metric.get_tp", "training_utils.Metric.get_fp", "training_utils.Metric.get_fn", "training_utils.Metric.get_tn", "training_utils.Metric.precision", "training_utils.Metric.recall", "training_utils.Metric.accuracy", "training_utils.Metric.f_score"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_classes", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.precision", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.recall", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.f_score"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "all_classes", "=", "self", ".", "get_classes", "(", ")", "\n", "all_classes", "=", "[", "None", "]", "+", "all_classes", "\n", "all_lines", "=", "[", "\n", "\"{0:<10}\\ttp: {1} - fp: {2} - fn: {3} - tn: {4} - precision: {5:.4f} - recall: {6:.4f} - accuracy: {7:.4f} - f1-score: {8:.4f}\"", ".", "format", "(", "\n", "self", ".", "name", "if", "class_name", "is", "None", "else", "class_name", ",", "\n", "self", ".", "get_tp", "(", "class_name", ")", ",", "\n", "self", ".", "get_fp", "(", "class_name", ")", ",", "\n", "self", ".", "get_fn", "(", "class_name", ")", ",", "\n", "self", ".", "get_tn", "(", "class_name", ")", ",", "\n", "self", ".", "precision", "(", "class_name", ")", ",", "\n", "self", ".", "recall", "(", "class_name", ")", ",", "\n", "self", ".", "accuracy", "(", "class_name", ")", ",", "\n", "self", ".", "f_score", "(", "class_name", ")", ",", "\n", ")", "\n", "for", "class_name", "in", "all_classes", "\n", "]", "\n", "return", "\"\\n\"", ".", "join", "(", "all_lines", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.__init__": [[189, 194], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "name", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "\n", "self", ".", "true", "=", "[", "]", "\n", "self", ".", "pred", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_squared_error": [[195, 197], ["sklearn.metrics.mean_squared_error"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_squared_error"], ["", "def", "mean_squared_error", "(", "self", ")", ":", "\n", "        ", "return", "mean_squared_error", "(", "self", ".", "true", ",", "self", ".", "pred", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_absolute_error": [[198, 200], ["sklearn.metrics.mean_absolute_error"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_absolute_error"], ["", "def", "mean_absolute_error", "(", "self", ")", ":", "\n", "        ", "return", "mean_absolute_error", "(", "self", ".", "true", ",", "self", ".", "pred", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.pearsonr": [[201, 203], ["scipy.stats.pearsonr"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.pearsonr"], ["", "def", "pearsonr", "(", "self", ")", ":", "\n", "        ", "return", "pearsonr", "(", "self", ".", "true", ",", "self", ".", "pred", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.spearmanr": [[204, 206], ["scipy.stats.spearmanr"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.spearmanr"], ["", "def", "spearmanr", "(", "self", ")", ":", "\n", "        ", "return", "spearmanr", "(", "self", ".", "true", ",", "self", ".", "pred", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.micro_avg_f_score": [[208, 210], ["training_utils.MetricRegression.mean_squared_error"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_squared_error"], ["", "def", "micro_avg_f_score", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "mean_squared_error", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.to_tsv": [[211, 217], ["training_utils.MetricRegression.mean_squared_error", "training_utils.MetricRegression.mean_absolute_error", "training_utils.MetricRegression.pearsonr", "training_utils.MetricRegression.spearmanr"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_squared_error", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_absolute_error", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.pearsonr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.spearmanr"], ["", "def", "to_tsv", "(", "self", ")", ":", "\n", "        ", "return", "\"{}\\t{}\\t{}\\t{}\"", ".", "format", "(", "\n", "self", ".", "mean_squared_error", "(", ")", ",", "\n", "self", ".", "mean_absolute_error", "(", ")", ",", "\n", "self", ".", "pearsonr", "(", ")", ",", "\n", "self", ".", "spearmanr", "(", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.tsv_header": [[219, 227], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "tsv_header", "(", "prefix", "=", "None", ")", ":", "\n", "        ", "if", "prefix", ":", "\n", "            ", "return", "\"{0}_MEAN_SQUARED_ERROR\\t{0}_MEAN_ABSOLUTE_ERROR\\t{0}_PEARSON\\t{0}_SPEARMAN\"", ".", "format", "(", "\n", "prefix", "\n", ")", "\n", "\n", "", "return", "\"MEAN_SQUARED_ERROR\\tMEAN_ABSOLUTE_ERROR\\tPEARSON\\tSPEARMAN\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.to_empty_tsv": [[228, 231], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "to_empty_tsv", "(", ")", ":", "\n", "        ", "return", "\"\\t_\\t_\\t_\\t_\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.__str__": [[232, 240], ["training_utils.MetricRegression.mean_squared_error", "training_utils.MetricRegression.mean_absolute_error", "training_utils.MetricRegression.pearsonr", "training_utils.MetricRegression.spearmanr"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_squared_error", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_absolute_error", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.pearsonr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.spearmanr"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "line", "=", "\"mean squared error: {0:.4f} - mean absolute error: {1:.4f} - pearson: {2:.4f} - spearman: {3:.4f}\"", ".", "format", "(", "\n", "self", ".", "mean_squared_error", "(", ")", ",", "\n", "self", ".", "mean_absolute_error", "(", ")", ",", "\n", "self", ".", "pearsonr", "(", ")", ",", "\n", "self", ".", "spearmanr", "(", ")", ",", "\n", ")", "\n", "return", "line", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.WeightExtractor.__init__": [[251, 255], ["training_utils.init_output_file", "collections.defaultdict", "collections.defaultdict", "list"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.init_output_file"], ["    ", "def", "__init__", "(", "self", ",", "directory", ":", "Path", ",", "number_of_weights", ":", "int", "=", "10", ")", ":", "\n", "        ", "self", ".", "weights_file", "=", "init_output_file", "(", "directory", ",", "\"weights.txt\"", ")", "\n", "self", ".", "weights_dict", "=", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "lambda", ":", "list", "(", ")", ")", ")", "\n", "self", ".", "number_of_weights", "=", "number_of_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.WeightExtractor.extract_weights": [[256, 276], ["state_dict.keys", "min", "range", "functools.reduce", "training_utils.WeightExtractor._init_weights_index", "vec.item", "list", "open", "f.write", "vec.size", "float"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.WeightExtractor._init_weights_index"], ["", "def", "extract_weights", "(", "self", ",", "state_dict", ",", "iteration", ")", ":", "\n", "        ", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "\n", "            ", "vec", "=", "state_dict", "[", "key", "]", "\n", "weights_to_watch", "=", "min", "(", "\n", "self", ".", "number_of_weights", ",", "reduce", "(", "lambda", "x", ",", "y", ":", "x", "*", "y", ",", "list", "(", "vec", ".", "size", "(", ")", ")", ")", "\n", ")", "\n", "\n", "if", "key", "not", "in", "self", ".", "weights_dict", ":", "\n", "                ", "self", ".", "_init_weights_index", "(", "key", ",", "state_dict", ",", "weights_to_watch", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "weights_to_watch", ")", ":", "\n", "                ", "vec", "=", "state_dict", "[", "key", "]", "\n", "for", "index", "in", "self", ".", "weights_dict", "[", "key", "]", "[", "i", "]", ":", "\n", "                    ", "vec", "=", "vec", "[", "index", "]", "\n", "\n", "", "value", "=", "vec", ".", "item", "(", ")", "\n", "\n", "with", "open", "(", "self", ".", "weights_file", ",", "\"a\"", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "\"{}\\t{}\\t{}\\t{}\\n\"", ".", "format", "(", "iteration", ",", "key", ",", "i", ",", "float", "(", "value", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.WeightExtractor._init_weights_index": [[277, 295], ["len", "range", "len", "random.randint", "cur_indices.append", "list", "vec.size", "indices.values", "len"], "methods", ["None"], ["", "", "", "", "def", "_init_weights_index", "(", "self", ",", "key", ",", "state_dict", ",", "weights_to_watch", ")", ":", "\n", "        ", "indices", "=", "{", "}", "\n", "\n", "i", "=", "0", "\n", "while", "len", "(", "indices", ")", "<", "weights_to_watch", ":", "\n", "            ", "vec", "=", "state_dict", "[", "key", "]", "\n", "cur_indices", "=", "[", "]", "\n", "\n", "for", "x", "in", "range", "(", "len", "(", "vec", ".", "size", "(", ")", ")", ")", ":", "\n", "                ", "index", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "vec", ")", "-", "1", ")", "\n", "vec", "=", "vec", "[", "index", "]", "\n", "cur_indices", ".", "append", "(", "index", ")", "\n", "\n", "", "if", "cur_indices", "not", "in", "list", "(", "indices", ".", "values", "(", ")", ")", ":", "\n", "                ", "indices", "[", "i", "]", "=", "cur_indices", "\n", "i", "+=", "1", "\n", "\n", "", "", "self", ".", "weights_dict", "[", "key", "]", "=", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.init_output_file": [[297, 309], ["base_path.mkdir", "open().close", "open"], "function", ["None"], ["", "", "def", "init_output_file", "(", "base_path", ":", "Path", ",", "file_name", ":", "str", ")", "->", "Path", ":", "\n", "    ", "\"\"\"\n    Creates a local file.\n    :param base_path: the path to the directory\n    :param file_name: the file name\n    :return: the created file\n    \"\"\"", "\n", "base_path", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "\n", "file", "=", "base_path", "/", "file_name", "\n", "open", "(", "file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", ".", "close", "(", ")", "\n", "return", "file", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.convert_labels_to_one_hot": [[311, 323], ["label_dict.get_items"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_items"], ["", "def", "convert_labels_to_one_hot", "(", "\n", "label_list", ":", "List", "[", "List", "[", "str", "]", "]", ",", "label_dict", ":", "Dictionary", "\n", ")", "->", "List", "[", "List", "[", "int", "]", "]", ":", "\n", "    ", "\"\"\"\n    Convert list of labels (strings) to a one hot list.\n    :param label_list: list of labels\n    :param label_dict: label dictionary\n    :return: converted label list\n    \"\"\"", "\n", "return", "[", "\n", "[", "1", "if", "l", "in", "labels", "else", "0", "for", "l", "in", "label_dict", ".", "get_items", "(", ")", "]", "\n", "for", "labels", "in", "label_list", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line": [[326, 328], ["log.info"], "function", ["None"], ["", "def", "log_line", "(", "log", ")", ":", "\n", "    ", "log", ".", "info", "(", "\"-\"", "*", "100", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.add_file_handler": [[330, 338], ["training_utils.init_output_file", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.Formatter", "logging.FileHandler.setFormatter", "log.addHandler"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.init_output_file"], ["", "def", "add_file_handler", "(", "log", ",", "output_file", ")", ":", "\n", "    ", "init_output_file", "(", "output_file", ".", "parents", "[", "0", "]", ",", "output_file", ".", "name", ")", "\n", "fh", "=", "logging", ".", "FileHandler", "(", "output_file", ",", "mode", "=", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "fh", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "\"%(asctime)-15s %(message)s\"", ")", "\n", "fh", ".", "setFormatter", "(", "formatter", ")", "\n", "log", ".", "addHandler", "(", "fh", ")", "\n", "return", "fh", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings": [[340, 368], ["sentence.clear_embeddings", "type", "[]._embeddings.items", "sentence.clear_embeddings", "sentence.to", "str", "delete_keys.append"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.clear_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.clear_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "store_embeddings", "(", "sentences", ":", "List", "[", "Sentence", "]", ",", "storage_mode", ":", "str", ")", ":", "\n", "\n", "# if memory mode option 'none' delete everything", "\n", "    ", "if", "storage_mode", "==", "\"none\"", ":", "\n", "        ", "for", "sentence", "in", "sentences", ":", "\n", "            ", "sentence", ".", "clear_embeddings", "(", ")", "\n", "\n", "# else delete only dynamic embeddings (otherwise autograd will keep everything in memory)", "\n", "", "", "else", ":", "\n", "# find out which ones are dynamic embeddings", "\n", "        ", "delete_keys", "=", "[", "]", "\n", "if", "type", "(", "sentences", "[", "0", "]", ")", "==", "Sentence", ":", "\n", "            ", "for", "name", ",", "vector", "in", "sentences", "[", "0", "]", "[", "0", "]", ".", "_embeddings", ".", "items", "(", ")", ":", "\n", "                ", "if", "sentences", "[", "0", "]", "[", "0", "]", ".", "_embeddings", "[", "name", "]", ".", "requires_grad", ":", "\n", "                    ", "delete_keys", ".", "append", "(", "name", ")", "\n", "\n", "# find out which ones are dynamic embeddings", "\n", "", "", "", "for", "sentence", "in", "sentences", ":", "\n", "            ", "sentence", ".", "clear_embeddings", "(", "delete_keys", ")", "\n", "\n", "# memory management - option 1: send everything to CPU (pin to memory if we train on GPU)", "\n", "", "", "if", "storage_mode", "==", "\"cpu\"", ":", "\n", "        ", "pin_memory", "=", "False", "if", "str", "(", "flair", ".", "device", ")", "==", "\"cpu\"", "else", "True", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "sentence", ".", "to", "(", "\"cpu\"", ",", "pin_memory", "=", "pin_memory", ")", "\n", "\n", "# record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)", "\n", "", "", "flair", ".", "embedding_storage_mode", "=", "storage_mode", "\n", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.SGDW.__init__": [[64, 90], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "params", ",", "\n", "lr", "=", "required", ",", "\n", "momentum", "=", "0", ",", "\n", "dampening", "=", "0", ",", "\n", "weight_decay", "=", "0", ",", "\n", "nesterov", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "lr", "is", "not", "required", "and", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "momentum", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid momentum value: {}\"", ".", "format", "(", "momentum", ")", ")", "\n", "", "if", "weight_decay", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "\n", "momentum", "=", "momentum", ",", "\n", "dampening", "=", "dampening", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "nesterov", "=", "nesterov", ",", "\n", ")", "\n", "if", "nesterov", "and", "(", "momentum", "<=", "0", "or", "dampening", "!=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Nesterov momentum requires a momentum and zero dampening\"", ")", "\n", "", "super", "(", "SGDW", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.SGDW.__setstate__": [[91, 95], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.AdamW.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "SGDW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "\"nesterov\"", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.SGDW.step": [[96, 137], ["closure", "p.data.add_", "p.data.add_", "torch.zeros_like", "torch.zeros_like.mul_().add_", "torch.zeros_like.mul_().add_", "d_p.add.add.add", "torch.zeros_like.mul_", "torch.zeros_like.mul_"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.SearchSpace.add"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the rpbert\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "weight_decay", "=", "group", "[", "\"weight_decay\"", "]", "\n", "momentum", "=", "group", "[", "\"momentum\"", "]", "\n", "dampening", "=", "group", "[", "\"dampening\"", "]", "\n", "nesterov", "=", "group", "[", "\"nesterov\"", "]", "\n", "\n", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "d_p", "=", "p", ".", "grad", ".", "data", "\n", "\n", "if", "momentum", "!=", "0", ":", "\n", "                    ", "param_state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "\"momentum_buffer\"", "not", "in", "param_state", ":", "\n", "                        ", "buf", "=", "param_state", "[", "\"momentum_buffer\"", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "buf", ".", "mul_", "(", "momentum", ")", ".", "add_", "(", "d_p", ")", "\n", "", "else", ":", "\n", "                        ", "buf", "=", "param_state", "[", "\"momentum_buffer\"", "]", "\n", "buf", ".", "mul_", "(", "momentum", ")", ".", "add_", "(", "1", "-", "dampening", ",", "d_p", ")", "\n", "", "if", "nesterov", ":", "\n", "                        ", "d_p", "=", "d_p", ".", "add", "(", "momentum", ",", "buf", ")", "\n", "", "else", ":", "\n", "                        ", "d_p", "=", "buf", "\n", "\n", "", "", "if", "weight_decay", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "add_", "(", "-", "weight_decay", ",", "p", ".", "data", ")", "\n", "\n", "", "p", ".", "data", ".", "add_", "(", "-", "group", "[", "\"lr\"", "]", ",", "d_p", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.AdamW.__init__": [[167, 188], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "params", ",", "\n", "lr", "=", "1e-3", ",", "\n", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "\n", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "\n", "amsgrad", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", "\n", ")", "\n", "super", "(", "AdamW", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.AdamW.__setstate__": [[189, 193], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.AdamW.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "\"amsgrad\"", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.AdamW.step": [[194, 257], ["closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.data.addcdiv_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "p.data.add_", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "math.sqrt", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt"], "methods", ["None"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the rpbert\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "\"Adam does not support sparse gradients, please consider SparseAdam instead\"", "\n", ")", "\n", "", "amsgrad", "=", "group", "[", "\"amsgrad\"", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "\"step\"", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "\"exp_avg\"", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "\"max_exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "\"exp_avg\"", "]", ",", "state", "[", "\"exp_avg_sq\"", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_sq", "=", "state", "[", "\"max_exp_avg_sq\"", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "\"betas\"", "]", "\n", "\n", "state", "[", "\"step\"", "]", "+=", "1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "\"eps\"", "]", ")", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "\"eps\"", "]", ")", "\n", "\n", "", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "\"step\"", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "\"step\"", "]", "\n", "step_size", "=", "group", "[", "\"lr\"", "]", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", "\n", "if", "group", "[", "\"weight_decay\"", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "add_", "(", "-", "group", "[", "\"weight_decay\"", "]", ",", "p", ".", "data", ")", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ExpAnnealLR.__init__": [[271, 275], ["torch.optim.lr_scheduler._LRScheduler.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "optimizer", ",", "end_lr", ",", "iterations", ",", "last_epoch", "=", "-", "1", ")", ":", "\n", "        ", "self", ".", "end_lr", "=", "end_lr", "\n", "self", ".", "iterations", "=", "iterations", "\n", "super", "(", "ExpAnnealLR", ",", "self", ")", ".", "__init__", "(", "optimizer", ",", "last_epoch", "=", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ExpAnnealLR.get_lr": [[276, 280], ["None"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "iteration", "=", "self", ".", "last_epoch", "+", "1", "\n", "pct", "=", "iteration", "/", "self", ".", "iterations", "\n", "return", "[", "base_lr", "*", "(", "self", ".", "end_lr", "/", "base_lr", ")", "**", "pct", "for", "base_lr", "in", "self", ".", "base_lrs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step": [[336, 357], ["optim.ReduceLRWDOnPlateau.is_better", "optim.ReduceLRWDOnPlateau._reduce_lr", "optim.ReduceLRWDOnPlateau._reduce_weight_decay"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau._reduce_weight_decay"], ["def", "step", "(", "self", ",", "metrics", ",", "epoch", "=", "None", ")", ":", "\n", "        ", "current", "=", "metrics", "\n", "if", "epoch", "is", "None", ":", "\n", "            ", "epoch", "=", "self", ".", "last_epoch", "=", "self", ".", "last_epoch", "+", "1", "\n", "", "self", ".", "last_epoch", "=", "epoch", "\n", "\n", "if", "self", ".", "is_better", "(", "current", ",", "self", ".", "best", ")", ":", "\n", "            ", "self", ".", "best", "=", "current", "\n", "self", ".", "num_bad_epochs", "=", "0", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_bad_epochs", "+=", "1", "\n", "\n", "", "if", "self", ".", "in_cooldown", ":", "\n", "            ", "self", ".", "cooldown_counter", "-=", "1", "\n", "self", ".", "num_bad_epochs", "=", "0", "# ignore any bad epochs in cooldown", "\n", "\n", "", "if", "self", ".", "num_bad_epochs", ">", "self", ".", "patience", ":", "\n", "            ", "self", ".", "_reduce_lr", "(", "epoch", ")", "\n", "self", ".", "_reduce_weight_decay", "(", "epoch", ")", "\n", "self", ".", "cooldown_counter", "=", "self", ".", "cooldown", "\n", "self", ".", "num_bad_epochs", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau._reduce_weight_decay": [[358, 368], ["enumerate", "float", "max", "log.info"], "methods", ["None"], ["", "", "def", "_reduce_weight_decay", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "for", "i", ",", "param_group", "in", "enumerate", "(", "self", ".", "optimizer", ".", "param_groups", ")", ":", "\n", "            ", "if", "param_group", "[", "\"weight_decay\"", "]", "!=", "0", ":", "\n", "                ", "old_weight_decay", "=", "float", "(", "param_group", "[", "\"weight_decay\"", "]", ")", "\n", "new_weight_decay", "=", "max", "(", "old_weight_decay", "*", "self", ".", "factor", ",", "self", ".", "min_lrs", "[", "i", "]", ")", "\n", "if", "old_weight_decay", "-", "new_weight_decay", ">", "self", ".", "eps", ":", "\n", "                    ", "param_group", "[", "\"weight_decay\"", "]", "=", "new_weight_decay", "\n", "if", "self", ".", "verbose", ":", "\n", "                        ", "log", ".", "info", "(", "\n", "f\"Epoch {epoch}: reducing weight decay factor of group {i} to {new_weight_decay:.4e}.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.DataLoader.__init__": [[22, 63], ["super().__init__", "type", "type", "isinstance", "flair_dataset.is_in_memory", "type"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.ColumnDataset.is_in_memory"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "dataset", ",", "\n", "batch_size", "=", "1", ",", "\n", "shuffle", "=", "False", ",", "\n", "sampler", "=", "None", ",", "\n", "batch_sampler", "=", "None", ",", "\n", "num_workers", "=", "8", ",", "\n", "drop_last", "=", "False", ",", "\n", "timeout", "=", "0", ",", "\n", "worker_init_fn", "=", "None", ",", "\n", ")", ":", "\n", "\n", "# in certain cases, multi-CPU data loading makes no sense and slows", "\n", "# everything down. For this reason, we detect if a dataset is in-memory:", "\n", "# if so, num_workers is set to 0 for faster processing", "\n", "        ", "flair_dataset", "=", "dataset", "\n", "while", "True", ":", "\n", "            ", "if", "type", "(", "flair_dataset", ")", "is", "Subset", ":", "\n", "                ", "flair_dataset", "=", "flair_dataset", ".", "dataset", "\n", "", "elif", "type", "(", "flair_dataset", ")", "is", "ConcatDataset", ":", "\n", "                ", "flair_dataset", "=", "flair_dataset", ".", "datasets", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "break", "\n", "\n", "", "", "if", "type", "(", "flair_dataset", ")", "is", "list", ":", "\n", "            ", "num_workers", "=", "0", "\n", "", "elif", "isinstance", "(", "flair_dataset", ",", "FlairDataset", ")", "and", "flair_dataset", ".", "is_in_memory", "(", ")", ":", "\n", "            ", "num_workers", "=", "0", "\n", "\n", "", "super", "(", "DataLoader", ",", "self", ")", ".", "__init__", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "sampler", "=", "sampler", ",", "\n", "batch_sampler", "=", "batch_sampler", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "collate_fn", "=", "list", ",", "\n", "drop_last", "=", "drop_last", ",", "\n", "timeout", "=", "timeout", ",", "\n", "worker_init_fn", "=", "worker_init_fn", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.SentenceDataset.__init__": [[71, 80], ["type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sentences", ":", "Union", "[", "Sentence", ",", "List", "[", "Sentence", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate SentenceDataset\n        :param sentences: Sentence or List of Sentence that make up SentenceDataset\n        \"\"\"", "\n", "# cast to list if necessary", "\n", "if", "type", "(", "sentences", ")", "==", "Sentence", ":", "\n", "            ", "sentences", "=", "[", "sentences", "]", "\n", "", "self", ".", "sentences", "=", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.SentenceDataset.is_in_memory": [[81, 84], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "is_in_memory", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.SentenceDataset.__len__": [[85, 87], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "sentences", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.SentenceDataset.__getitem__": [[88, 90], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", "=", "0", ")", "->", "Sentence", ":", "\n", "        ", "return", "self", ".", "sentences", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.StringDataset.__init__": [[97, 115], ["type"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "texts", ":", "Union", "[", "str", ",", "List", "[", "str", "]", "]", ",", "\n", "use_tokenizer", ":", "Union", "[", "bool", ",", "Callable", "[", "[", "str", "]", ",", "List", "[", "Token", "]", "]", "]", "=", "space_tokenizer", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate StringDataset\n        :param texts: a string or List of string that make up StringDataset\n        :param use_tokenizer: a custom tokenizer (default is space based tokenizer,\n        more advanced options are segtok_tokenizer to use segtok or build_spacy_tokenizer to use Spacy library\n        if available). Check the code of space_tokenizer to implement your own (if you need it).\n        If instead of providing a function, this parameter is just set to True, segtok will be used.\n        \"\"\"", "\n", "# cast to list if necessary", "\n", "if", "type", "(", "texts", ")", "==", "Sentence", ":", "\n", "            ", "texts", "=", "[", "texts", "]", "\n", "", "self", ".", "texts", "=", "texts", "\n", "self", ".", "use_tokenizer", "=", "use_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.StringDataset.is_in_memory": [[116, 119], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "is_in_memory", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.StringDataset.__len__": [[120, 122], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "texts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.StringDataset.__getitem__": [[123, 126], ["flair.data.Sentence"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", "=", "0", ")", "->", "Sentence", ":", "\n", "        ", "text", "=", "self", ".", "texts", "[", "index", "]", "\n", "return", "Sentence", "(", "text", ",", "use_tokenizer", "=", "self", ".", "use_tokenizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.MongoDataset.__init__": [[129, 216], ["pymongo.MongoClient", "base.MongoDataset.__cursor.find", "base.MongoDataset.__cursor.find().distinct", "base.MongoDataset.__cursor.count_documents", "log.warning", "log.warning", "log.warning", "log.warning", "base.MongoDataset._parse_document_to_sentence", "kwargs", "base.MongoDataset.sentences.append", "base.MongoDataset.__cursor.find", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.MongoDataset._parse_document_to_sentence"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "query", ":", "str", ",", "\n", "host", ":", "str", ",", "\n", "port", ":", "int", ",", "\n", "database", ":", "str", ",", "\n", "collection", ":", "str", ",", "\n", "text_field", ":", "str", ",", "\n", "categories_field", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "max_tokens_per_doc", ":", "int", "=", "-", "1", ",", "\n", "max_chars_per_doc", ":", "int", "=", "-", "1", ",", "\n", "tokenizer", "=", "segtok_tokenizer", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Reads Mongo collections. Each collection should contain one document/text per item.\n\n        Each item should have the following format:\n        {\n        'Beskrivning': 'Abrahamsby. G\u00e5rd i Gottr\u00f6ra sn, L\u00e5nghundra hd, Stockholms l\u00e4n, n\u00e4ra L\u00e5ngsj\u00f6n.',\n        'L\u00e4n':'Stockholms l\u00e4n',\n        'H\u00e4rad': 'L\u00e5nghundra',\n        'F\u00f6rsamling': 'Gottr\u00f6ra',\n        'Plats': 'Abrahamsby'\n        }\n\n        :param query: Query, e.g. {'L\u00e4n': 'Stockholms l\u00e4n'}\n        :param host: Host, e.g. 'localhost',\n        :param port: Port, e.g. 27017\n        :param database: Database, e.g. 'rosenberg',\n        :param collection: Collection, e.g. 'book',\n        :param text_field: Text field, e.g. 'Beskrivning',\n        :param categories_field: List of category fields, e.g ['L\u00e4n', 'H\u00e4rad', 'Tingslag', 'F\u00f6rsamling', 'Plats'],\n        :param max_tokens_per_doc: Takes at most this amount of tokens per document. If set to -1 all documents are taken as is.\n        :param max_tokens_per_doc: If set, truncates each Sentence to a maximum number of Tokens\n        :param max_chars_per_doc: If set, truncates each Sentence to a maximum number of chars\n        :param in_memory: If True, keeps dataset as Sentences in memory, otherwise only keeps strings\n        :return: list of sentences\n        \"\"\"", "\n", "\n", "# first, check if pymongo is installed", "\n", "try", ":", "\n", "            ", "import", "pymongo", "\n", "", "except", "ModuleNotFoundError", ":", "\n", "            ", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "log", ".", "warning", "(", "'ATTENTION! The library \"pymongo\" is not installed!'", ")", "\n", "log", ".", "warning", "(", "\n", "'To use MongoDataset, please first install with \"pip install pymongo\"'", "\n", ")", "\n", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "pass", "\n", "\n", "", "self", ".", "in_memory", "=", "in_memory", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "\n", "if", "self", ".", "in_memory", ":", "\n", "            ", "self", ".", "sentences", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "indices", "=", "[", "]", "\n", "\n", "", "self", ".", "total_sentence_count", ":", "int", "=", "0", "\n", "self", ".", "max_chars_per_doc", "=", "max_chars_per_doc", "\n", "self", ".", "max_tokens_per_doc", "=", "max_tokens_per_doc", "\n", "\n", "self", ".", "__connection", "=", "pymongo", ".", "MongoClient", "(", "host", ",", "port", ")", "\n", "self", ".", "__cursor", "=", "self", ".", "__connection", "[", "database", "]", "[", "collection", "]", "\n", "\n", "self", ".", "text", "=", "text_field", "\n", "self", ".", "categories", "=", "categories_field", "if", "categories_field", "is", "not", "None", "else", "[", "]", "\n", "\n", "start", "=", "0", "\n", "\n", "kwargs", "=", "lambda", "start", ":", "{", "\"filter\"", ":", "query", ",", "\"skip\"", ":", "start", ",", "\"limit\"", ":", "0", "}", "\n", "\n", "if", "self", ".", "in_memory", ":", "\n", "            ", "for", "document", "in", "self", ".", "__cursor", ".", "find", "(", "**", "kwargs", "(", "start", ")", ")", ":", "\n", "                ", "sentence", "=", "self", ".", "_parse_document_to_sentence", "(", "\n", "document", "[", "self", ".", "text", "]", ",", "\n", "[", "document", "[", "_", "]", "if", "_", "in", "document", "else", "\"\"", "for", "_", "in", "self", ".", "categories", "]", ",", "\n", "tokenizer", ",", "\n", ")", "\n", "if", "sentence", "is", "not", "None", "and", "len", "(", "sentence", ".", "tokens", ")", ">", "0", ":", "\n", "                    ", "self", ".", "sentences", ".", "append", "(", "sentence", ")", "\n", "self", ".", "total_sentence_count", "+=", "1", "\n", "", "", "", "else", ":", "\n", "            ", "self", ".", "indices", "=", "self", ".", "__cursor", ".", "find", "(", ")", ".", "distinct", "(", "\"_id\"", ")", "\n", "self", ".", "total_sentence_count", "=", "self", ".", "__cursor", ".", "count_documents", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.MongoDataset._parse_document_to_sentence": [[217, 233], ["flair.data.Sentence", "min", "len"], "methods", ["None"], ["", "", "def", "_parse_document_to_sentence", "(", "\n", "self", ",", "text", ":", "str", ",", "labels", ":", "List", "[", "str", "]", ",", "tokenizer", ":", "Callable", "[", "[", "str", "]", ",", "List", "[", "Token", "]", "]", "\n", ")", ":", "\n", "        ", "if", "self", ".", "max_chars_per_doc", ">", "0", ":", "\n", "            ", "text", "=", "text", "[", ":", "self", ".", "max_chars_per_doc", "]", "\n", "\n", "", "if", "text", "and", "labels", ":", "\n", "            ", "sentence", "=", "Sentence", "(", "text", ",", "labels", "=", "labels", ",", "use_tokenizer", "=", "tokenizer", ")", "\n", "\n", "if", "self", ".", "max_tokens_per_doc", ">", "0", ":", "\n", "                ", "sentence", ".", "tokens", "=", "sentence", ".", "tokens", "[", "\n", ":", "min", "(", "len", "(", "sentence", ")", ",", "self", ".", "max_tokens_per_doc", ")", "\n", "]", "\n", "\n", "", "return", "sentence", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.MongoDataset.is_in_memory": [[234, 236], ["None"], "methods", ["None"], ["", "def", "is_in_memory", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "self", ".", "in_memory", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.MongoDataset.__len__": [[237, 239], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total_sentence_count", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.MongoDataset.__getitem__": [[240, 251], ["base.MongoDataset.__cursor.find_one", "base.MongoDataset._parse_document_to_sentence"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.MongoDataset._parse_document_to_sentence"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", "=", "0", ")", "->", "Sentence", ":", "\n", "        ", "if", "self", ".", "in_memory", ":", "\n", "            ", "return", "self", ".", "sentences", "[", "index", "]", "\n", "", "else", ":", "\n", "            ", "document", "=", "self", ".", "__cursor", ".", "find_one", "(", "{", "\"_id\"", ":", "index", "}", ")", "\n", "sentence", "=", "self", ".", "_parse_document_to_sentence", "(", "\n", "document", "[", "self", ".", "text", "]", ",", "\n", "[", "document", "[", "_", "]", "if", "_", "in", "document", "else", "\"\"", "for", "_", "in", "self", ".", "categories", "]", ",", "\n", "self", ".", "tokenizer", ",", "\n", ")", "\n", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.find_train_dev_test_files": [[253, 296], ["log.info", "log.info", "log.info", "log.info", "type", "pathlib.Path", "data_folder.iterdir", "data_folder.iterdir", "suffixes_to_ignore.isdisjoint", "suffixes_to_ignore.isdisjoint"], "function", ["None"], ["", "", "", "def", "find_train_dev_test_files", "(", "data_folder", ",", "dev_file", ",", "test_file", ",", "train_file", ")", ":", "\n", "    ", "if", "type", "(", "data_folder", ")", "==", "str", ":", "\n", "        ", "data_folder", ":", "Path", "=", "Path", "(", "data_folder", ")", "\n", "\n", "", "if", "train_file", "is", "not", "None", ":", "\n", "        ", "train_file", "=", "data_folder", "/", "train_file", "\n", "", "if", "test_file", "is", "not", "None", ":", "\n", "        ", "test_file", "=", "data_folder", "/", "test_file", "\n", "", "if", "dev_file", "is", "not", "None", ":", "\n", "        ", "dev_file", "=", "data_folder", "/", "dev_file", "\n", "\n", "", "suffixes_to_ignore", "=", "{", "\".gz\"", ",", "\".swp\"", "}", "\n", "\n", "# automatically identify train / test / dev files", "\n", "if", "train_file", "is", "None", ":", "\n", "        ", "for", "file", "in", "data_folder", ".", "iterdir", "(", ")", ":", "\n", "            ", "file_name", "=", "file", ".", "name", "\n", "if", "not", "suffixes_to_ignore", ".", "isdisjoint", "(", "file", ".", "suffixes", ")", ":", "\n", "                ", "continue", "\n", "", "if", "\"train\"", "in", "file_name", "and", "not", "\"54019\"", "in", "file_name", ":", "\n", "                ", "train_file", "=", "file", "\n", "", "if", "\"dev\"", "in", "file_name", ":", "\n", "                ", "dev_file", "=", "file", "\n", "", "if", "\"testa\"", "in", "file_name", ":", "\n", "                ", "dev_file", "=", "file", "\n", "", "if", "\"testb\"", "in", "file_name", ":", "\n", "                ", "test_file", "=", "file", "\n", "\n", "# if no test file is found, take any file with 'test' in name", "\n", "", "", "if", "test_file", "is", "None", ":", "\n", "            ", "for", "file", "in", "data_folder", ".", "iterdir", "(", ")", ":", "\n", "                ", "file_name", "=", "file", ".", "name", "\n", "if", "not", "suffixes_to_ignore", ".", "isdisjoint", "(", "file", ".", "suffixes", ")", ":", "\n", "                    ", "continue", "\n", "", "if", "\"test\"", "in", "file_name", ":", "\n", "                    ", "test_file", "=", "file", "\n", "\n", "", "", "", "", "log", ".", "info", "(", "\"Reading data from {}\"", ".", "format", "(", "data_folder", ")", ")", "\n", "log", ".", "info", "(", "\"Train: {}\"", ".", "format", "(", "train_file", ")", ")", "\n", "log", ".", "info", "(", "\"Dev: {}\"", ".", "format", "(", "dev_file", ")", ")", "\n", "log", ".", "info", "(", "\"Test: {}\"", ".", "format", "(", "test_file", ")", ")", "\n", "\n", "return", "dev_file", ",", "test_file", ",", "train_file", "\n", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UniversalDependenciesCorpus.__init__": [[20, 54], ["flair.datasets.base.find_train_dev_test_files", "treebanks.UniversalDependenciesDataset", "treebanks.UniversalDependenciesDataset", "treebanks.UniversalDependenciesDataset", "flair.data.Corpus.__init__", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.find_train_dev_test_files", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "data_folder", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "train_file", "=", "None", ",", "\n", "test_file", "=", "None", ",", "\n", "dev_file", "=", "None", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiates a Corpus from CoNLL-U column-formatted task data such as the UD corpora\n\n        :param data_folder: base folder with the task data\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :param in_memory: If set to True, keeps full dataset in memory, otherwise does disk reads\n        :return: a Corpus with annotated train, dev and test data\n        \"\"\"", "\n", "\n", "# find train, dev and test files if not specified", "\n", "dev_file", ",", "test_file", ",", "train_file", "=", "find_train_dev_test_files", "(", "data_folder", ",", "dev_file", ",", "test_file", ",", "train_file", ")", "\n", "\n", "# get train data", "\n", "train", "=", "UniversalDependenciesDataset", "(", "train_file", ",", "in_memory", "=", "in_memory", ")", "\n", "\n", "# get test data", "\n", "test", "=", "UniversalDependenciesDataset", "(", "test_file", ",", "in_memory", "=", "in_memory", ")", "\n", "\n", "# get dev data", "\n", "dev", "=", "UniversalDependenciesDataset", "(", "dev_file", ",", "in_memory", "=", "in_memory", ")", "\n", "\n", "super", "(", "UniversalDependenciesCorpus", ",", "self", ")", ".", "__init__", "(", "\n", "train", ",", "dev", ",", "test", ",", "name", "=", "str", "(", "data_folder", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UniversalDependenciesDataset.__init__": [[58, 131], ["path_to_conll_file.exists", "open", "file.readline", "flair.data.Sentence", "str", "file.readline.strip", "re.split", "file.readline", "len", "flair.data.Sentence", "file.readline.startswith", "treebanks.UniversalDependenciesDataset.sentences.append", "treebanks.UniversalDependenciesDataset.indices.append", "len", "file.readline", "treebanks.UniversalDependenciesDataset.sentences.append", "treebanks.UniversalDependenciesDataset.indices.append", "file.tell", "file.readline", "file.readline", "flair.data.Token", "flair.data.Token.add_label", "flair.data.Token.add_label", "flair.data.Token.add_label", "flair.data.Token.add_label", "str().split", "sentence.add_token", "str", "str", "str", "str", "flair.data.Token.add_label", "flair.data.Token.add_label", "int", "len", "str", "[].lower", "len", "str", "str", "morph.split", "morph.split"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label"], ["    ", "def", "__init__", "(", "self", ",", "path_to_conll_file", ":", "Path", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Instantiates a column dataset in CoNLL-U format.\n\n        :param path_to_conll_file: Path to the CoNLL-U formatted file\n        :param in_memory: If set to True, keeps full dataset in memory, otherwise does disk reads\n        \"\"\"", "\n", "assert", "path_to_conll_file", ".", "exists", "(", ")", "\n", "\n", "self", ".", "in_memory", "=", "in_memory", "\n", "self", ".", "path_to_conll_file", "=", "path_to_conll_file", "\n", "self", ".", "total_sentence_count", ":", "int", "=", "0", "\n", "\n", "if", "self", ".", "in_memory", ":", "\n", "            ", "self", ".", "sentences", ":", "List", "[", "Sentence", "]", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "indices", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "\n", "", "with", "open", "(", "str", "(", "self", ".", "path_to_conll_file", ")", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "\n", "            ", "line", "=", "file", ".", "readline", "(", ")", "\n", "position", "=", "0", "\n", "sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "while", "line", ":", "\n", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "fields", ":", "List", "[", "str", "]", "=", "re", ".", "split", "(", "\"\\t+\"", ",", "line", ")", "\n", "if", "line", "==", "\"\"", ":", "\n", "                    ", "if", "len", "(", "sentence", ")", ">", "0", ":", "\n", "                        ", "self", ".", "total_sentence_count", "+=", "1", "\n", "if", "self", ".", "in_memory", ":", "\n", "                            ", "self", ".", "sentences", ".", "append", "(", "sentence", ")", "\n", "", "else", ":", "\n", "                            ", "self", ".", "indices", ".", "append", "(", "position", ")", "\n", "position", "=", "file", ".", "tell", "(", ")", "\n", "", "", "sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "\n", "", "elif", "line", ".", "startswith", "(", "\"#\"", ")", ":", "\n", "                    ", "line", "=", "file", ".", "readline", "(", ")", "\n", "continue", "\n", "", "elif", "\".\"", "in", "fields", "[", "0", "]", ":", "\n", "                    ", "line", "=", "file", ".", "readline", "(", ")", "\n", "continue", "\n", "", "elif", "\"-\"", "in", "fields", "[", "0", "]", ":", "\n", "                    ", "line", "=", "file", ".", "readline", "(", ")", "\n", "continue", "\n", "", "else", ":", "\n", "                    ", "token", "=", "Token", "(", "fields", "[", "1", "]", ",", "head_id", "=", "int", "(", "fields", "[", "6", "]", ")", ")", "\n", "token", ".", "add_label", "(", "\"lemma\"", ",", "str", "(", "fields", "[", "2", "]", ")", ")", "\n", "token", ".", "add_label", "(", "\"upos\"", ",", "str", "(", "fields", "[", "3", "]", ")", ")", "\n", "token", ".", "add_label", "(", "\"pos\"", ",", "str", "(", "fields", "[", "4", "]", ")", ")", "\n", "token", ".", "add_label", "(", "\"dependency\"", ",", "str", "(", "fields", "[", "7", "]", ")", ")", "\n", "\n", "if", "len", "(", "fields", ")", ">", "9", "and", "'SpaceAfter=No'", "in", "fields", "[", "9", "]", ":", "\n", "                        ", "token", ".", "whitespace_after", "=", "False", "\n", "\n", "", "for", "morph", "in", "str", "(", "fields", "[", "5", "]", ")", ".", "split", "(", "\"|\"", ")", ":", "\n", "                        ", "if", "\"=\"", "not", "in", "morph", ":", "\n", "                            ", "continue", "\n", "", "token", ".", "add_label", "(", "morph", ".", "split", "(", "\"=\"", ")", "[", "0", "]", ".", "lower", "(", ")", ",", "morph", ".", "split", "(", "\"=\"", ")", "[", "1", "]", ")", "\n", "\n", "", "if", "len", "(", "fields", ")", ">", "10", "and", "str", "(", "fields", "[", "10", "]", ")", "==", "\"Y\"", ":", "\n", "                        ", "token", ".", "add_label", "(", "\"frame\"", ",", "str", "(", "fields", "[", "11", "]", ")", ")", "\n", "\n", "", "sentence", ".", "add_token", "(", "token", ")", "\n", "\n", "", "line", "=", "file", ".", "readline", "(", ")", "\n", "", "if", "len", "(", "sentence", ".", "tokens", ")", ">", "0", ":", "\n", "                ", "self", ".", "total_sentence_count", "+=", "1", "\n", "if", "self", ".", "in_memory", ":", "\n", "                    ", "self", ".", "sentences", ".", "append", "(", "sentence", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "indices", ".", "append", "(", "position", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UniversalDependenciesDataset.is_in_memory": [[132, 134], ["None"], "methods", ["None"], ["", "", "", "", "def", "is_in_memory", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "self", ".", "in_memory", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UniversalDependenciesDataset.__len__": [[135, 137], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total_sentence_count", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UniversalDependenciesDataset.__getitem__": [[138, 188], ["open", "file.seek", "file.readline", "flair.data.Sentence", "str", "file.readline.strip", "re.split", "file.readline", "file.readline.startswith", "len", "file.readline", "file.readline", "file.readline", "flair.data.Token", "flair.data.Token.add_label", "flair.data.Token.add_label", "flair.data.Token.add_label", "flair.data.Token.add_label", "str().split", "sentence.add_token", "str", "str", "str", "str", "flair.data.Token.add_label", "flair.data.Token.add_label", "int", "len", "str", "[].lower", "len", "str", "str", "morph.split", "morph.split"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", "=", "0", ")", "->", "Sentence", ":", "\n", "\n", "        ", "if", "self", ".", "in_memory", ":", "\n", "            ", "sentence", "=", "self", ".", "sentences", "[", "index", "]", "\n", "", "else", ":", "\n", "            ", "with", "open", "(", "str", "(", "self", ".", "path_to_conll_file", ")", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "                ", "file", ".", "seek", "(", "self", ".", "indices", "[", "index", "]", ")", "\n", "line", "=", "file", ".", "readline", "(", ")", "\n", "sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "while", "line", ":", "\n", "\n", "                    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "fields", ":", "List", "[", "str", "]", "=", "re", ".", "split", "(", "\"\\t+\"", ",", "line", ")", "\n", "if", "line", "==", "\"\"", ":", "\n", "                        ", "if", "len", "(", "sentence", ")", ">", "0", ":", "\n", "                            ", "break", "\n", "\n", "", "", "elif", "line", ".", "startswith", "(", "\"#\"", ")", ":", "\n", "                        ", "line", "=", "file", ".", "readline", "(", ")", "\n", "continue", "\n", "", "elif", "\".\"", "in", "fields", "[", "0", "]", ":", "\n", "                        ", "line", "=", "file", ".", "readline", "(", ")", "\n", "continue", "\n", "", "elif", "\"-\"", "in", "fields", "[", "0", "]", ":", "\n", "                        ", "line", "=", "file", ".", "readline", "(", ")", "\n", "continue", "\n", "", "else", ":", "\n", "                        ", "token", "=", "Token", "(", "fields", "[", "1", "]", ",", "head_id", "=", "int", "(", "fields", "[", "6", "]", ")", ")", "\n", "token", ".", "add_label", "(", "\"lemma\"", ",", "str", "(", "fields", "[", "2", "]", ")", ")", "\n", "token", ".", "add_label", "(", "\"upos\"", ",", "str", "(", "fields", "[", "3", "]", ")", ")", "\n", "token", ".", "add_label", "(", "\"pos\"", ",", "str", "(", "fields", "[", "4", "]", ")", ")", "\n", "token", ".", "add_label", "(", "\"dependency\"", ",", "str", "(", "fields", "[", "7", "]", ")", ")", "\n", "\n", "if", "len", "(", "fields", ")", ">", "9", "and", "'SpaceAfter=No'", "in", "fields", "[", "9", "]", ":", "\n", "                            ", "token", ".", "whitespace_after", "=", "False", "\n", "\n", "", "for", "morph", "in", "str", "(", "fields", "[", "5", "]", ")", ".", "split", "(", "\"|\"", ")", ":", "\n", "                            ", "if", "\"=\"", "not", "in", "morph", ":", "\n", "                                ", "continue", "\n", "", "token", ".", "add_label", "(", "\n", "morph", ".", "split", "(", "\"=\"", ")", "[", "0", "]", ".", "lower", "(", ")", ",", "morph", ".", "split", "(", "\"=\"", ")", "[", "1", "]", "\n", ")", "\n", "\n", "", "if", "len", "(", "fields", ")", ">", "10", "and", "str", "(", "fields", "[", "10", "]", ")", "==", "\"Y\"", ":", "\n", "                            ", "token", ".", "add_label", "(", "\"frame\"", ",", "str", "(", "fields", "[", "11", "]", ")", ")", "\n", "\n", "", "sentence", ".", "add_token", "(", "token", ")", "\n", "\n", "", "line", "=", "file", ".", "readline", "(", ")", "\n", "", "", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_ENGLISH.__init__": [[191, 215], ["treebanks.UD_ENGLISH.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "web_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master\"", "\n", "cached_path", "(", "f\"{web_path}/en_ewt-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{web_path}/en_ewt-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{web_path}/en_ewt-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_ENGLISH", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_GERMAN.__init__": [[218, 240], ["treebanks.UD_GERMAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/master\"", "\n", "cached_path", "(", "f\"{ud_path}/de_gsd-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/de_gsd-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/de_gsd-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_GERMAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_GERMAN_HDT.__init__": [[243, 286], ["treebanks.UD_GERMAN_HDT.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "flair.file_utils.cached_path", "new_train_file.is_file", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "open", "pathlib.Path", "open", "f_out.write", "f_in.read"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "False", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "(", "\n", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_German-HDT/dev\"", "\n", ")", "\n", "cached_path", "(", "f\"{ud_path}/de_hdt-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/de_hdt-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "\n", "train_filenames", "=", "[", "\n", "\"de_hdt-ud-train-a-1.conllu\"", ",", "\n", "\"de_hdt-ud-train-a-2.conllu\"", ",", "\n", "\"de_hdt-ud-train-b-1.conllu\"", ",", "\n", "\"de_hdt-ud-train-b-2.conllu\"", ",", "\n", "]", "\n", "\n", "for", "train_file", "in", "train_filenames", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ud_path}/{train_file}\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "\"original\"", "\n", ")", "\n", "\n", "", "data_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "\n", "\n", "new_train_file", ":", "Path", "=", "data_path", "/", "\"de_hdt-ud-train-all.conllu\"", "\n", "\n", "if", "not", "new_train_file", ".", "is_file", "(", ")", ":", "\n", "            ", "with", "open", "(", "new_train_file", ",", "\"wt\"", ")", "as", "f_out", ":", "\n", "                ", "for", "train_filename", "in", "train_filenames", ":", "\n", "                    ", "with", "open", "(", "data_path", "/", "\"original\"", "/", "train_filename", ",", "\"rt\"", ")", "as", "f_in", ":", "\n", "                        ", "f_out", ".", "write", "(", "f_in", ".", "read", "(", ")", ")", "\n", "\n", "", "", "", "", "super", "(", "UD_GERMAN_HDT", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_DUTCH.__init__": [[289, 315], ["treebanks.UD_DUTCH.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Dutch-Alpino/master\"", "\n", "cached_path", "(", "\n", "f\"{ud_path}/nl_alpino-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/nl_alpino-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/nl_alpino-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_DUTCH", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_FRENCH.__init__": [[318, 339], ["treebanks.UD_FRENCH.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/master\"", "\n", "cached_path", "(", "f\"{ud_path}/fr_gsd-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/fr_gsd-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/fr_gsd-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "super", "(", "UD_FRENCH", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_ITALIAN.__init__": [[342, 365], ["treebanks.UD_ITALIAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Italian-ISDT/master\"", "\n", "cached_path", "(", "f\"{ud_path}/it_isdt-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/it_isdt-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/it_isdt-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "super", "(", "UD_ITALIAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_SPANISH.__init__": [[368, 389], ["treebanks.UD_SPANISH.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Spanish-GSD/master\"", "\n", "cached_path", "(", "f\"{ud_path}/es_gsd-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/es_gsd-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/es_gsd-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "super", "(", "UD_SPANISH", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_PORTUGUESE.__init__": [[392, 417], ["treebanks.UD_PORTUGUESE.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Portuguese-Bosque/master\"", "\n", "cached_path", "(", "\n", "f\"{ud_path}/pt_bosque-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/pt_bosque-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/pt_bosque-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "super", "(", "UD_PORTUGUESE", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_ROMANIAN.__init__": [[420, 441], ["treebanks.UD_ROMANIAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Romanian-RRT/master\"", "\n", "cached_path", "(", "f\"{ud_path}/ro_rrt-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/ro_rrt-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ro_rrt-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "super", "(", "UD_ROMANIAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_CATALAN.__init__": [[444, 469], ["treebanks.UD_CATALAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Catalan-AnCora/master\"", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ca_ancora-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ca_ancora-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ca_ancora-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "super", "(", "UD_CATALAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_POLISH.__init__": [[472, 494], ["treebanks.UD_POLISH.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Polish-LFG/master\"", "\n", "cached_path", "(", "f\"{ud_path}/pl_lfg-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/pl_lfg-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/pl_lfg-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_POLISH", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_CZECH.__init__": [[497, 547], ["treebanks.UD_CZECH.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "new_train_file.is_file", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "open", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "open", "f_out.write", "f_in.read"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "False", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Czech-PDT/master\"", "\n", "cached_path", "(", "f\"{ud_path}/cs_pdt-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/cs_pdt-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/cs_pdt-ud-train-c.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "\"original\"", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/cs_pdt-ud-train-l.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "\"original\"", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/cs_pdt-ud-train-m.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "\"original\"", ",", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/cs_pdt-ud-train-v.conllu\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "\"original\"", ",", "\n", ")", "\n", "data_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "\n", "\n", "train_filenames", "=", "[", "\n", "\"cs_pdt-ud-train-c.conllu\"", ",", "\n", "\"cs_pdt-ud-train-l.conllu\"", ",", "\n", "\"cs_pdt-ud-train-m.conllu\"", ",", "\n", "\"cs_pdt-ud-train-v.conllu\"", ",", "\n", "]", "\n", "\n", "new_train_file", ":", "Path", "=", "data_path", "/", "\"cs_pdt-ud-train-all.conllu\"", "\n", "\n", "if", "not", "new_train_file", ".", "is_file", "(", ")", ":", "\n", "            ", "with", "open", "(", "new_train_file", ",", "\"wt\"", ")", "as", "f_out", ":", "\n", "                ", "for", "train_filename", "in", "train_filenames", ":", "\n", "                    ", "with", "open", "(", "data_path", "/", "\"original\"", "/", "train_filename", ",", "\"rt\"", ")", "as", "f_in", ":", "\n", "                        ", "f_out", ".", "write", "(", "f_in", ".", "read", "(", ")", ")", "\n", "", "", "", "", "super", "(", "UD_CZECH", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_SLOVAK.__init__": [[550, 572], ["treebanks.UD_SLOVAK.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Slovak-SNK/master\"", "\n", "cached_path", "(", "f\"{ud_path}/sk_snk-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/sk_snk-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/sk_snk-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_SLOVAK", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_SWEDISH.__init__": [[575, 601], ["treebanks.UD_SWEDISH.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Swedish-Talbanken/master\"", "\n", "cached_path", "(", "\n", "f\"{ud_path}/sv_talbanken-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/sv_talbanken-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/sv_talbanken-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_SWEDISH", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_DANISH.__init__": [[604, 626], ["treebanks.UD_DANISH.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Danish-DDT/master\"", "\n", "cached_path", "(", "f\"{ud_path}/da_ddt-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/da_ddt-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/da_ddt-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_DANISH", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_NORWEGIAN.__init__": [[629, 655], ["treebanks.UD_NORWEGIAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Norwegian-Bokmaal/master\"", "\n", "cached_path", "(", "\n", "f\"{ud_path}/no_bokmaal-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/no_bokmaal-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/no_bokmaal-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_NORWEGIAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_FINNISH.__init__": [[658, 680], ["treebanks.UD_FINNISH.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Finnish-TDT/master\"", "\n", "cached_path", "(", "f\"{ud_path}/fi_tdt-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/fi_tdt-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/fi_tdt-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_FINNISH", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_SLOVENIAN.__init__": [[683, 705], ["treebanks.UD_SLOVENIAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Slovenian-SSJ/master\"", "\n", "cached_path", "(", "f\"{ud_path}/sl_ssj-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/sl_ssj-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/sl_ssj-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_SLOVENIAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_CROATIAN.__init__": [[708, 730], ["treebanks.UD_CROATIAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Croatian-SET/master\"", "\n", "cached_path", "(", "f\"{ud_path}/hr_set-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/hr_set-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/hr_set-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_CROATIAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_SERBIAN.__init__": [[733, 755], ["treebanks.UD_SERBIAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Serbian-SET/master\"", "\n", "cached_path", "(", "f\"{ud_path}/sr_set-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/sr_set-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/sr_set-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_SERBIAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_BULGARIAN.__init__": [[758, 780], ["treebanks.UD_BULGARIAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Bulgarian-BTB/master\"", "\n", "cached_path", "(", "f\"{ud_path}/bg_btb-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/bg_btb-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/bg_btb-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_BULGARIAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_ARABIC.__init__": [[783, 806], ["treebanks.UD_ARABIC.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Arabic-PADT/master\"", "\n", "cached_path", "(", "f\"{ud_path}/ar_padt-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ar_padt-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ar_padt-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "super", "(", "UD_ARABIC", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_HEBREW.__init__": [[809, 830], ["treebanks.UD_HEBREW.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Hebrew-HTB/master\"", "\n", "cached_path", "(", "f\"{ud_path}/he_htb-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/he_htb-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/he_htb-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "super", "(", "UD_HEBREW", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_TURKISH.__init__": [[833, 857], ["treebanks.UD_TURKISH.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Turkish-IMST/master\"", "\n", "cached_path", "(", "f\"{ud_path}/tr_imst-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/tr_imst-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/tr_imst-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_TURKISH", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_PERSIAN.__init__": [[860, 886], ["treebanks.UD_PERSIAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Persian-Seraji/master\"", "\n", "cached_path", "(", "\n", "f\"{ud_path}/fa_seraji-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/fa_seraji-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/fa_seraji-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_PERSIAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_RUSSIAN.__init__": [[889, 915], ["treebanks.UD_RUSSIAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master\"", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ru_syntagrus-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ru_syntagrus-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ru_syntagrus-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_RUSSIAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_HINDI.__init__": [[918, 942], ["treebanks.UD_HINDI.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Hindi-HDTB/master\"", "\n", "cached_path", "(", "f\"{ud_path}/hi_hdtb-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/hi_hdtb-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/hi_hdtb-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_HINDI", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_INDONESIAN.__init__": [[945, 967], ["treebanks.UD_INDONESIAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Indonesian-GSD/master\"", "\n", "cached_path", "(", "f\"{ud_path}/id_gsd-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/id_gsd-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/id_gsd-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_INDONESIAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_JAPANESE.__init__": [[970, 992], ["treebanks.UD_JAPANESE.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Japanese-GSD/master\"", "\n", "cached_path", "(", "f\"{ud_path}/ja_gsd-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/ja_gsd-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ja_gsd-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_JAPANESE", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_CHINESE.__init__": [[995, 1017], ["treebanks.UD_CHINESE.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSD/master\"", "\n", "cached_path", "(", "f\"{ud_path}/zh_gsd-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/zh_gsd-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/zh_gsd-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_CHINESE", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_KOREAN.__init__": [[1020, 1046], ["treebanks.UD_KOREAN.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Korean-Kaist/master\"", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ko_kaist-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ko_kaist-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/ko_kaist-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_KOREAN", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.treebanks.UD_BASQUE.__init__": [[1049, 1071], ["treebanks.UD_BASQUE.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "treebanks.UniversalDependenciesCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "in_memory", ":", "bool", "=", "True", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ud_path", "=", "\"https://raw.githubusercontent.com/UniversalDependencies/UD_Basque-BDT/master\"", "\n", "cached_path", "(", "f\"{ud_path}/eu_bdt-ud-dev.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{ud_path}/eu_bdt-ud-test.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{ud_path}/eu_bdt-ud-train.conllu\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "UD_BASQUE", ",", "self", ")", ".", "__init__", "(", "data_folder", ",", "in_memory", "=", "in_memory", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.ClassificationCorpus.__init__": [[21, 90], ["flair.datasets.base.find_train_dev_test_files", "document_classification.ClassificationDataset", "flair.data.Corpus.__init__", "document_classification.ClassificationDataset", "document_classification.ClassificationDataset", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.find_train_dev_test_files", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "data_folder", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "label_type", ":", "str", "=", "'class'", ",", "\n", "train_file", "=", "None", ",", "\n", "test_file", "=", "None", ",", "\n", "dev_file", "=", "None", ",", "\n", "tokenizer", ":", "Callable", "[", "[", "str", "]", ",", "List", "[", "Token", "]", "]", "=", "space_tokenizer", ",", "\n", "truncate_to_max_tokens", ":", "int", "=", "-", "1", ",", "\n", "truncate_to_max_chars", ":", "int", "=", "-", "1", ",", "\n", "filter_if_longer_than", ":", "int", "=", "-", "1", ",", "\n", "in_memory", ":", "bool", "=", "False", ",", "\n", "encoding", ":", "str", "=", "'utf-8'", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiates a Corpus from text classification-formatted task data\n\n        :param data_folder: base folder with the task data\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :param use_tokenizer: If True, tokenizes the dataset, otherwise uses whitespace tokenization\n        :param truncate_to_max_tokens: If set, truncates each Sentence to a maximum number of Tokens\n        :param truncate_to_max_chars: If set, truncates each Sentence to a maximum number of chars\n        :param in_memory: If True, keeps dataset as Sentences in memory, otherwise only keeps strings\n        :return: a Corpus with annotated train, dev and test data\n        \"\"\"", "\n", "\n", "# find train, dev and test files if not specified", "\n", "dev_file", ",", "test_file", ",", "train_file", "=", "find_train_dev_test_files", "(", "data_folder", ",", "dev_file", ",", "test_file", ",", "train_file", ")", "\n", "\n", "train", ":", "FlairDataset", "=", "ClassificationDataset", "(", "\n", "train_file", ",", "\n", "label_type", "=", "label_type", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "truncate_to_max_tokens", "=", "truncate_to_max_tokens", ",", "\n", "truncate_to_max_chars", "=", "truncate_to_max_chars", ",", "\n", "filter_if_longer_than", "=", "filter_if_longer_than", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", "encoding", "=", "encoding", ",", "\n", ")", "\n", "\n", "# use test_file to create test split if available", "\n", "test", ":", "FlairDataset", "=", "ClassificationDataset", "(", "\n", "test_file", ",", "\n", "label_type", "=", "label_type", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "truncate_to_max_tokens", "=", "truncate_to_max_tokens", ",", "\n", "truncate_to_max_chars", "=", "truncate_to_max_chars", ",", "\n", "filter_if_longer_than", "=", "filter_if_longer_than", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", "encoding", "=", "encoding", ",", "\n", ")", "if", "test_file", "is", "not", "None", "else", "None", "\n", "\n", "# use dev_file to create test split if available", "\n", "dev", ":", "FlairDataset", "=", "ClassificationDataset", "(", "\n", "dev_file", ",", "\n", "label_type", "=", "label_type", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "truncate_to_max_tokens", "=", "truncate_to_max_tokens", ",", "\n", "truncate_to_max_chars", "=", "truncate_to_max_chars", ",", "\n", "filter_if_longer_than", "=", "filter_if_longer_than", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", "encoding", "=", "encoding", ",", "\n", ")", "if", "dev_file", "is", "not", "None", "else", "None", "\n", "\n", "super", "(", "ClassificationCorpus", ",", "self", ")", ".", "__init__", "(", "\n", "train", ",", "dev", ",", "test", ",", "name", "=", "str", "(", "data_folder", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.ClassificationDataset.__init__": [[94, 168], ["path_to_file.exists", "type", "pathlib.Path", "open", "f.readline", "str", "f.tell", "f.readline", "f.tell", "f.readline", "len", "f.tell", "f.readline", "document_classification.ClassificationDataset._parse_line_to_sentence", "document_classification.ClassificationDataset.indices.append", "f.readline.split", "document_classification.ClassificationDataset.sentences.append", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.ClassificationDataset._parse_line_to_sentence"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "path_to_file", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "label_type", ":", "str", "=", "'class'", ",", "\n", "truncate_to_max_tokens", "=", "-", "1", ",", "\n", "truncate_to_max_chars", "=", "-", "1", ",", "\n", "filter_if_longer_than", ":", "int", "=", "-", "1", ",", "\n", "tokenizer", "=", "segtok_tokenizer", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", "encoding", ":", "str", "=", "'utf-8'", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Reads a data file for text classification. The file should contain one document/text per line.\n        The line should have the following format:\n        __label__<class_name> <text>\n        If you have a multi class task, you can have as many labels as you want at the beginning of the line, e.g.,\n        __label__<class_name_1> __label__<class_name_2> <text>\n        :param path_to_file: the path to the data file\n        :param truncate_to_max_tokens: Takes at most this amount of tokens per document. If set to -1 all documents are taken as is.\n        :param max_tokens_per_doc: If set, truncates each Sentence to a maximum number of Tokens\n        :param truncate_to_max_chars: If set, truncates each Sentence to a maximum number of chars\n        :param in_memory: If True, keeps dataset as Sentences in memory, otherwise only keeps strings\n        :return: list of sentences\n        \"\"\"", "\n", "if", "type", "(", "path_to_file", ")", "==", "str", ":", "\n", "            ", "path_to_file", ":", "Path", "=", "Path", "(", "path_to_file", ")", "\n", "\n", "", "assert", "path_to_file", ".", "exists", "(", ")", "\n", "\n", "self", ".", "label_prefix", "=", "\"__label__\"", "\n", "self", ".", "label_type", "=", "label_type", "\n", "\n", "self", ".", "in_memory", "=", "in_memory", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "\n", "if", "self", ".", "in_memory", ":", "\n", "            ", "self", ".", "sentences", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "indices", "=", "[", "]", "\n", "\n", "", "self", ".", "total_sentence_count", ":", "int", "=", "0", "\n", "self", ".", "truncate_to_max_chars", "=", "truncate_to_max_chars", "\n", "self", ".", "truncate_to_max_tokens", "=", "truncate_to_max_tokens", "\n", "self", ".", "filter_if_longer_than", "=", "filter_if_longer_than", "\n", "\n", "self", ".", "path_to_file", "=", "path_to_file", "\n", "\n", "with", "open", "(", "str", "(", "path_to_file", ")", ",", "encoding", "=", "encoding", ")", "as", "f", ":", "\n", "            ", "line", "=", "f", ".", "readline", "(", ")", "\n", "position", "=", "0", "\n", "while", "line", ":", "\n", "                ", "if", "\"__label__\"", "not", "in", "line", "or", "\" \"", "not", "in", "line", ":", "\n", "                    ", "position", "=", "f", ".", "tell", "(", ")", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "continue", "\n", "\n", "", "if", "0", "<", "self", ".", "filter_if_longer_than", "<", "len", "(", "line", ".", "split", "(", "' '", ")", ")", ":", "\n", "                    ", "position", "=", "f", ".", "tell", "(", ")", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "continue", "\n", "\n", "", "if", "self", ".", "in_memory", ":", "\n", "                    ", "sentence", "=", "self", ".", "_parse_line_to_sentence", "(", "\n", "line", ",", "self", ".", "label_prefix", ",", "tokenizer", "\n", ")", "\n", "if", "sentence", "is", "not", "None", "and", "len", "(", "sentence", ".", "tokens", ")", ">", "0", ":", "\n", "                        ", "self", ".", "sentences", ".", "append", "(", "sentence", ")", "\n", "self", ".", "total_sentence_count", "+=", "1", "\n", "", "", "else", ":", "\n", "                    ", "self", ".", "indices", ".", "append", "(", "position", ")", "\n", "self", ".", "total_sentence_count", "+=", "1", "\n", "\n", "", "position", "=", "f", ".", "tell", "(", ")", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.ClassificationDataset._parse_line_to_sentence": [[169, 204], ["line.split", "range", "line[].strip", "len", "words[].startswith", "flair.data.Sentence", "words[].replace", "labels.append", "flair.data.Sentence.add_label", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label"], ["", "", "", "def", "_parse_line_to_sentence", "(", "\n", "self", ",", "line", ":", "str", ",", "label_prefix", ":", "str", ",", "tokenizer", ":", "Callable", "[", "[", "str", "]", ",", "List", "[", "Token", "]", "]", "\n", ")", ":", "\n", "        ", "words", "=", "line", ".", "split", "(", ")", "\n", "\n", "labels", "=", "[", "]", "\n", "l_len", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "words", ")", ")", ":", "\n", "            ", "if", "words", "[", "i", "]", ".", "startswith", "(", "label_prefix", ")", ":", "\n", "                ", "l_len", "+=", "len", "(", "words", "[", "i", "]", ")", "+", "1", "\n", "label", "=", "words", "[", "i", "]", ".", "replace", "(", "label_prefix", ",", "\"\"", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "", "else", ":", "\n", "                ", "break", "\n", "\n", "", "", "text", "=", "line", "[", "l_len", ":", "]", ".", "strip", "(", ")", "\n", "\n", "if", "self", ".", "truncate_to_max_chars", ">", "0", ":", "\n", "            ", "text", "=", "text", "[", ":", "self", ".", "truncate_to_max_chars", "]", "\n", "\n", "", "if", "text", "and", "labels", ":", "\n", "            ", "sentence", "=", "Sentence", "(", "text", ",", "use_tokenizer", "=", "tokenizer", ")", "\n", "\n", "for", "label", "in", "labels", ":", "\n", "                ", "sentence", ".", "add_label", "(", "self", ".", "label_type", ",", "label", ")", "\n", "\n", "", "if", "(", "\n", "sentence", "is", "not", "None", "\n", "and", "0", "<", "self", ".", "truncate_to_max_tokens", "<", "len", "(", "sentence", ")", "\n", ")", ":", "\n", "                ", "sentence", ".", "tokens", "=", "sentence", ".", "tokens", "[", ":", "self", ".", "truncate_to_max_tokens", "]", "\n", "\n", "", "return", "sentence", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.ClassificationDataset.is_in_memory": [[205, 207], ["None"], "methods", ["None"], ["", "def", "is_in_memory", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "self", ".", "in_memory", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.ClassificationDataset.__len__": [[208, 210], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total_sentence_count", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.ClassificationDataset.__getitem__": [[211, 223], ["open", "file.seek", "file.readline", "document_classification.ClassificationDataset._parse_line_to_sentence", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.ClassificationDataset._parse_line_to_sentence"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", "=", "0", ")", "->", "Sentence", ":", "\n", "        ", "if", "self", ".", "in_memory", ":", "\n", "            ", "return", "self", ".", "sentences", "[", "index", "]", "\n", "", "else", ":", "\n", "\n", "            ", "with", "open", "(", "str", "(", "self", ".", "path_to_file", ")", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "                ", "file", ".", "seek", "(", "self", ".", "indices", "[", "index", "]", ")", "\n", "line", "=", "file", ".", "readline", "(", ")", "\n", "sentence", "=", "self", ".", "_parse_line_to_sentence", "(", "\n", "line", ",", "self", ".", "label_prefix", ",", "self", ".", "tokenizer", "\n", ")", "\n", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.CSVClassificationCorpus.__init__": [[226, 303], ["flair.datasets.base.find_train_dev_test_files", "document_classification.CSVClassificationDataset", "flair.data.Corpus.__init__", "document_classification.CSVClassificationDataset", "document_classification.CSVClassificationDataset", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.find_train_dev_test_files", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "data_folder", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "column_name_map", ":", "Dict", "[", "int", ",", "str", "]", ",", "\n", "label_type", ":", "str", "=", "'class'", ",", "\n", "train_file", "=", "None", ",", "\n", "test_file", "=", "None", ",", "\n", "dev_file", "=", "None", ",", "\n", "tokenizer", ":", "Callable", "[", "[", "str", "]", ",", "List", "[", "Token", "]", "]", "=", "segtok_tokenizer", ",", "\n", "max_tokens_per_doc", "=", "-", "1", ",", "\n", "max_chars_per_doc", "=", "-", "1", ",", "\n", "in_memory", ":", "bool", "=", "False", ",", "\n", "skip_header", ":", "bool", "=", "False", ",", "\n", "encoding", ":", "str", "=", "'utf-8'", ",", "\n", "**", "fmtparams", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiates a Corpus for text classification from CSV column formatted data\n\n        :param data_folder: base folder with the task data\n        :param column_name_map: a column name map that indicates which column is text and which the label(s)\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :param max_tokens_per_doc: If set, truncates each Sentence to a maximum number of Tokens\n        :param max_chars_per_doc: If set, truncates each Sentence to a maximum number of chars\n        :param use_tokenizer: If True, tokenizes the dataset, otherwise uses whitespace tokenization\n        :param in_memory: If True, keeps dataset as Sentences in memory, otherwise only keeps strings\n        :param fmtparams: additional parameters for the CSV file reader\n        :return: a Corpus with annotated train, dev and test data\n        \"\"\"", "\n", "\n", "# find train, dev and test files if not specified", "\n", "dev_file", ",", "test_file", ",", "train_file", "=", "find_train_dev_test_files", "(", "data_folder", ",", "dev_file", ",", "test_file", ",", "train_file", ")", "\n", "\n", "train", ":", "FlairDataset", "=", "CSVClassificationDataset", "(", "\n", "train_file", ",", "\n", "column_name_map", ",", "\n", "label_type", "=", "label_type", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_tokens_per_doc", "=", "max_tokens_per_doc", ",", "\n", "max_chars_per_doc", "=", "max_chars_per_doc", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", "skip_header", "=", "skip_header", ",", "\n", "encoding", "=", "encoding", ",", "\n", "**", "fmtparams", ",", "\n", ")", "\n", "\n", "test", ":", "FlairDataset", "=", "CSVClassificationDataset", "(", "\n", "test_file", ",", "\n", "column_name_map", ",", "\n", "label_type", "=", "label_type", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_tokens_per_doc", "=", "max_tokens_per_doc", ",", "\n", "max_chars_per_doc", "=", "max_chars_per_doc", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", "skip_header", "=", "skip_header", ",", "\n", "encoding", "=", "encoding", ",", "\n", "**", "fmtparams", ",", "\n", ")", "if", "test_file", "is", "not", "None", "else", "None", "\n", "\n", "dev", ":", "FlairDataset", "=", "CSVClassificationDataset", "(", "\n", "dev_file", ",", "\n", "column_name_map", ",", "\n", "label_type", "=", "label_type", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_tokens_per_doc", "=", "max_tokens_per_doc", ",", "\n", "max_chars_per_doc", "=", "max_chars_per_doc", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", "skip_header", "=", "skip_header", ",", "\n", "encoding", "=", "encoding", ",", "\n", "**", "fmtparams", ",", "\n", ")", "if", "dev_file", "is", "not", "None", "else", "None", "\n", "\n", "super", "(", "CSVClassificationCorpus", ",", "self", ")", ".", "__init__", "(", "\n", "train", ",", "dev", ",", "test", ",", "name", "=", "str", "(", "data_folder", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.CSVClassificationDataset.__init__": [[305, 415], ["path_to_file.exists", "type", "pathlib.Path", "open", "csv.reader", "document_classification.CSVClassificationDataset.text_columns.append", "next", "flair.data.Sentence", "document_classification.CSVClassificationDataset.sentences.append", "document_classification.CSVClassificationDataset.raw_data.append", "len", "document_classification.CSVClassificationDataset.column_name_map[].startswith", "len", "document_classification.CSVClassificationDataset.column_name_map[].startswith", "flair.data.Sentence.add_label"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "path_to_file", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "column_name_map", ":", "Dict", "[", "int", ",", "str", "]", ",", "\n", "label_type", ":", "str", "=", "\"class\"", ",", "\n", "max_tokens_per_doc", ":", "int", "=", "-", "1", ",", "\n", "max_chars_per_doc", ":", "int", "=", "-", "1", ",", "\n", "tokenizer", "=", "segtok_tokenizer", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", "skip_header", ":", "bool", "=", "False", ",", "\n", "encoding", ":", "str", "=", "'utf-8'", ",", "\n", "**", "fmtparams", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiates a Dataset for text classification from CSV column formatted data\n\n        :param path_to_file: path to the file with the CSV data\n        :param column_name_map: a column name map that indicates which column is text and which the label(s)\n        :param max_tokens_per_doc: If set, truncates each Sentence to a maximum number of Tokens\n        :param max_chars_per_doc: If set, truncates each Sentence to a maximum number of chars\n        :param use_tokenizer: If True, tokenizes the dataset, otherwise uses whitespace tokenization\n        :param in_memory: If True, keeps dataset as Sentences in memory, otherwise only keeps strings\n        :param skip_header: If True, skips first line because it is header\n        :param fmtparams: additional parameters for the CSV file reader\n        :return: a Corpus with annotated train, dev and test data\n        \"\"\"", "\n", "\n", "if", "type", "(", "path_to_file", ")", "==", "str", ":", "\n", "            ", "path_to_file", ":", "Path", "=", "Path", "(", "path_to_file", ")", "\n", "\n", "", "assert", "path_to_file", ".", "exists", "(", ")", "\n", "\n", "# variables", "\n", "self", ".", "path_to_file", "=", "path_to_file", "\n", "self", ".", "in_memory", "=", "in_memory", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "column_name_map", "=", "column_name_map", "\n", "self", ".", "max_tokens_per_doc", "=", "max_tokens_per_doc", "\n", "self", ".", "max_chars_per_doc", "=", "max_chars_per_doc", "\n", "\n", "self", ".", "label_type", "=", "label_type", "\n", "\n", "# different handling of in_memory data than streaming data", "\n", "if", "self", ".", "in_memory", ":", "\n", "            ", "self", ".", "sentences", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "raw_data", "=", "[", "]", "\n", "\n", "", "self", ".", "total_sentence_count", ":", "int", "=", "0", "\n", "\n", "# most data sets have the token text in the first column, if not, pass 'text' as column", "\n", "self", ".", "text_columns", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "for", "column", "in", "column_name_map", ":", "\n", "            ", "if", "column_name_map", "[", "column", "]", "==", "\"text\"", ":", "\n", "                ", "self", ".", "text_columns", ".", "append", "(", "column", ")", "\n", "\n", "", "", "with", "open", "(", "self", ".", "path_to_file", ",", "encoding", "=", "encoding", ")", "as", "csv_file", ":", "\n", "\n", "            ", "csv_reader", "=", "csv", ".", "reader", "(", "csv_file", ",", "**", "fmtparams", ")", "\n", "\n", "if", "skip_header", ":", "\n", "                ", "next", "(", "csv_reader", ",", "None", ")", "# skip the headers", "\n", "\n", "", "for", "row", "in", "csv_reader", ":", "\n", "\n", "# test if format is OK", "\n", "                ", "wrong_format", "=", "False", "\n", "for", "text_column", "in", "self", ".", "text_columns", ":", "\n", "                    ", "if", "text_column", ">=", "len", "(", "row", ")", ":", "\n", "                        ", "wrong_format", "=", "True", "\n", "\n", "", "", "if", "wrong_format", ":", "\n", "                    ", "continue", "\n", "\n", "# test if at least one label given", "\n", "", "has_label", "=", "False", "\n", "for", "column", "in", "self", ".", "column_name_map", ":", "\n", "                    ", "if", "self", ".", "column_name_map", "[", "column", "]", ".", "startswith", "(", "\"label\"", ")", "and", "row", "[", "column", "]", ":", "\n", "                        ", "has_label", "=", "True", "\n", "break", "\n", "\n", "", "", "if", "not", "has_label", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "self", ".", "in_memory", ":", "\n", "\n", "                    ", "text", "=", "\" \"", ".", "join", "(", "\n", "[", "row", "[", "text_column", "]", "for", "text_column", "in", "self", ".", "text_columns", "]", "\n", ")", "\n", "\n", "if", "self", ".", "max_chars_per_doc", ">", "0", ":", "\n", "                        ", "text", "=", "text", "[", ":", "self", ".", "max_chars_per_doc", "]", "\n", "\n", "", "sentence", "=", "Sentence", "(", "text", ",", "use_tokenizer", "=", "self", ".", "tokenizer", ")", "\n", "\n", "for", "column", "in", "self", ".", "column_name_map", ":", "\n", "                        ", "if", "(", "\n", "self", ".", "column_name_map", "[", "column", "]", ".", "startswith", "(", "\"label\"", ")", "\n", "and", "row", "[", "column", "]", "\n", ")", ":", "\n", "                            ", "sentence", ".", "add_label", "(", "label_type", ",", "row", "[", "column", "]", ")", "\n", "\n", "", "", "if", "0", "<", "self", ".", "max_tokens_per_doc", "<", "len", "(", "sentence", ")", ":", "\n", "                        ", "sentence", ".", "tokens", "=", "sentence", ".", "tokens", "[", ":", "self", ".", "max_tokens_per_doc", "]", "\n", "", "self", ".", "sentences", ".", "append", "(", "sentence", ")", "\n", "\n", "", "else", ":", "\n", "                    ", "self", ".", "raw_data", ".", "append", "(", "row", ")", "\n", "\n", "", "self", ".", "total_sentence_count", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.CSVClassificationDataset.is_in_memory": [[416, 418], ["None"], "methods", ["None"], ["", "", "", "def", "is_in_memory", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "self", ".", "in_memory", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.CSVClassificationDataset.__len__": [[419, 421], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total_sentence_count", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.CSVClassificationDataset.__getitem__": [[422, 442], ["flair.data.Sentence", "len", "document_classification.CSVClassificationDataset.column_name_map[].startswith", "flair.data.Sentence.add_label"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", "=", "0", ")", "->", "Sentence", ":", "\n", "        ", "if", "self", ".", "in_memory", ":", "\n", "            ", "return", "self", ".", "sentences", "[", "index", "]", "\n", "", "else", ":", "\n", "            ", "row", "=", "self", ".", "raw_data", "[", "index", "]", "\n", "\n", "text", "=", "\" \"", ".", "join", "(", "[", "row", "[", "text_column", "]", "for", "text_column", "in", "self", ".", "text_columns", "]", ")", "\n", "\n", "if", "self", ".", "max_chars_per_doc", ">", "0", ":", "\n", "                ", "text", "=", "text", "[", ":", "self", ".", "max_chars_per_doc", "]", "\n", "\n", "", "sentence", "=", "Sentence", "(", "text", ",", "use_tokenizer", "=", "self", ".", "tokenizer", ")", "\n", "for", "column", "in", "self", ".", "column_name_map", ":", "\n", "                ", "if", "self", ".", "column_name_map", "[", "column", "]", ".", "startswith", "(", "\"label\"", ")", "and", "row", "[", "column", "]", ":", "\n", "                    ", "sentence", ".", "add_label", "(", "self", ".", "label_type", ",", "row", "[", "column", "]", ")", "\n", "\n", "", "", "if", "0", "<", "self", ".", "max_tokens_per_doc", "<", "len", "(", "sentence", ")", ":", "\n", "                ", "sentence", ".", "tokens", "=", "sentence", ".", "tokens", "[", ":", "self", ".", "max_tokens_per_doc", "]", "\n", "\n", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.IMDB.__init__": [[445, 504], ["document_classification.ClassificationCorpus.__init__", "type", "pathlib.Path", "document_classification.IMDB.__class__.__name__.lower", "data_file.is_file", "flair.file_utils.cached_path", "pathlib.Path", "pathlib.Path", "tarfile.open", "pathlib.Path", "f_in.extractall", "open", "current_path.iterdir", "pathlib.Path", "file_name.is_file", "file_name.name.endswith", "f_p.write", "f_in.getmembers", "file_name.open().read", "file_name.open"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "rebalance_corpus", ":", "bool", "=", "True", ",", "**", "corpusargs", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "+", "'_v2'", "\n", "\n", "if", "rebalance_corpus", ":", "\n", "            ", "dataset_name", "=", "dataset_name", "+", "'-rebalanced'", "\n", "\n", "# default dataset folder is the cache root", "\n", "", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "imdb_acl_path", "=", "\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"", "\n", "data_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "\n", "data_file", "=", "data_path", "/", "\"train.txt\"", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "            ", "cached_path", "(", "imdb_acl_path", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "import", "tarfile", "\n", "\n", "with", "tarfile", ".", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "\n", "/", "\"datasets\"", "\n", "/", "dataset_name", "\n", "/", "\"aclImdb_v1.tar.gz\"", ",", "\n", "\"r:gz\"", ",", "\n", ")", "as", "f_in", ":", "\n", "                ", "datasets", "=", "[", "\"train\"", ",", "\"test\"", "]", "\n", "labels", "=", "[", "\"pos\"", ",", "\"neg\"", "]", "\n", "\n", "for", "label", "in", "labels", ":", "\n", "                    ", "for", "dataset", "in", "datasets", ":", "\n", "                        ", "f_in", ".", "extractall", "(", "\n", "data_path", ",", "\n", "members", "=", "[", "\n", "m", "\n", "for", "m", "in", "f_in", ".", "getmembers", "(", ")", "\n", "if", "f\"{dataset}/{label}\"", "in", "m", ".", "name", "\n", "]", ",", "\n", ")", "\n", "with", "open", "(", "f\"{data_path}/train-all.txt\"", ",", "\"at\"", ")", "as", "f_p", ":", "\n", "                            ", "current_path", "=", "data_path", "/", "\"aclImdb\"", "/", "dataset", "/", "label", "\n", "for", "file_name", "in", "current_path", ".", "iterdir", "(", ")", ":", "\n", "                                ", "if", "file_name", ".", "is_file", "(", ")", "and", "file_name", ".", "name", ".", "endswith", "(", "\n", "\".txt\"", "\n", ")", ":", "\n", "                                    ", "if", "label", "==", "\"pos\"", ":", "sentiment_label", "=", "'POSITIVE'", "\n", "if", "label", "==", "\"neg\"", ":", "sentiment_label", "=", "'NEGATIVE'", "\n", "f_p", ".", "write", "(", "\n", "f\"__label__{sentiment_label} \"", "\n", "+", "file_name", ".", "open", "(", "\"rt\"", ",", "encoding", "=", "\"utf-8\"", ")", ".", "read", "(", ")", "\n", "+", "\"\\n\"", "\n", ")", "\n", "\n", "", "", "", "", "", "", "", "super", "(", "IMDB", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "tokenizer", "=", "space_tokenizer", ",", "**", "corpusargs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.NEWSGROUPS.__init__": [[508, 592], ["document_classification.NEWSGROUPS.__class__.__name__.lower", "document_classification.ClassificationCorpus.__init__", "type", "pathlib.Path", "data_file.is_file", "flair.file_utils.cached_path", "pathlib.Path", "pathlib.Path", "tarfile.open", "pathlib.Path", "f_in.extractall", "open", "current_path.iterdir", "file_name.is_file", "pathlib.Path", "f_p.write", "f_in.getmembers", "file_name.open().read().replace", "file_name.open().read", "file_name.open"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "**", "corpusargs", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "twenty_newsgroups_path", "=", "(", "\n", "\"http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\"", "\n", ")", "\n", "data_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "\n", "data_file", "=", "data_path", "/", "\"20news-bydate-train.txt\"", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "            ", "cached_path", "(", "\n", "twenty_newsgroups_path", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "\"original\"", "\n", ")", "\n", "\n", "import", "tarfile", "\n", "\n", "with", "tarfile", ".", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "\n", "/", "\"datasets\"", "\n", "/", "dataset_name", "\n", "/", "\"original\"", "\n", "/", "\"20news-bydate.tar.gz\"", ",", "\n", "\"r:gz\"", ",", "\n", ")", "as", "f_in", ":", "\n", "                ", "datasets", "=", "[", "\"20news-bydate-test\"", ",", "\"20news-bydate-train\"", "]", "\n", "labels", "=", "[", "\n", "\"alt.atheism\"", ",", "\n", "\"comp.graphics\"", ",", "\n", "\"comp.os.ms-windows.misc\"", ",", "\n", "\"comp.sys.ibm.pc.hardware\"", ",", "\n", "\"comp.sys.mac.hardware\"", ",", "\n", "\"comp.windows.x\"", ",", "\n", "\"misc.forsale\"", ",", "\n", "\"rec.autos\"", ",", "\n", "\"rec.motorcycles\"", ",", "\n", "\"rec.sport.baseball\"", ",", "\n", "\"rec.sport.hockey\"", ",", "\n", "\"sci.crypt\"", ",", "\n", "\"sci.electronics\"", ",", "\n", "\"sci.med\"", ",", "\n", "\"sci.space\"", ",", "\n", "\"soc.religion.christian\"", ",", "\n", "\"talk.politics.guns\"", ",", "\n", "\"talk.politics.mideast\"", ",", "\n", "\"talk.politics.misc\"", ",", "\n", "\"talk.religion.misc\"", ",", "\n", "]", "\n", "\n", "for", "label", "in", "labels", ":", "\n", "                    ", "for", "dataset", "in", "datasets", ":", "\n", "                        ", "f_in", ".", "extractall", "(", "\n", "data_path", "/", "\"original\"", ",", "\n", "members", "=", "[", "\n", "m", "\n", "for", "m", "in", "f_in", ".", "getmembers", "(", ")", "\n", "if", "f\"{dataset}/{label}\"", "in", "m", ".", "name", "\n", "]", ",", "\n", ")", "\n", "with", "open", "(", "\n", "f\"{data_path}/{dataset}.txt\"", ",", "\"at\"", ",", "encoding", "=", "\"utf-8\"", "\n", ")", "as", "f_p", ":", "\n", "                            ", "current_path", "=", "data_path", "/", "\"original\"", "/", "dataset", "/", "label", "\n", "for", "file_name", "in", "current_path", ".", "iterdir", "(", ")", ":", "\n", "                                ", "if", "file_name", ".", "is_file", "(", ")", ":", "\n", "                                    ", "f_p", ".", "write", "(", "\n", "f\"__label__{label} \"", "\n", "+", "file_name", ".", "open", "(", "\"rt\"", ",", "encoding", "=", "\"latin1\"", ")", "\n", ".", "read", "(", ")", "\n", ".", "replace", "(", "\"\\n\"", ",", "\" <n> \"", ")", "\n", "+", "\"\\n\"", "\n", ")", "\n", "\n", "", "", "", "", "", "", "", "super", "(", "NEWSGROUPS", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "tokenizer", "=", "space_tokenizer", ",", "**", "corpusargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.SENTEVAL_CR.__init__": [[596, 632], ["document_classification.SENTEVAL_CR.__class__.__name__.lower", "document_classification.ClassificationCorpus.__init__", "flair.file_utils.cached_path", "flair.file_utils.unzip_file", "pathlib.Path", "os.path.exists", "os.makedirs", "open", "pathlib.Path", "pathlib.Path", "open", "open", "train_file.write", "train_file.write"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.unzip_file"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "**", "corpusargs", ",", "\n", ")", ":", "\n", "# this dataset name", "\n", "        ", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "data_folder", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "if", "not", "(", "data_folder", "/", "\"train.txt\"", ")", ".", "is_file", "(", ")", ":", "\n", "\n", "# download senteval datasets if necessary und unzip", "\n", "            ", "senteval_path", "=", "\"https://dl.fbaipublicfiles.com/senteval/senteval_data/datasmall_NB_ACL12.zip\"", "\n", "cached_path", "(", "senteval_path", ",", "Path", "(", "\"datasets\"", ")", "/", "\"senteval\"", ")", "\n", "senteval_folder", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "\"senteval\"", "\n", "unzip_file", "(", "senteval_folder", "/", "\"datasmall_NB_ACL12.zip\"", ",", "senteval_folder", ")", "\n", "\n", "# create dataset directory if necessary", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_folder", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "data_folder", ")", "\n", "\n", "# create train.txt file by iterating over pos and neg file", "\n", "", "with", "open", "(", "data_folder", "/", "\"train.txt\"", ",", "\"a\"", ")", "as", "train_file", ":", "\n", "\n", "                ", "with", "open", "(", "senteval_folder", "/", "\"data\"", "/", "\"customerr\"", "/", "\"custrev.pos\"", ",", "encoding", "=", "\"latin1\"", ")", "as", "file", ":", "\n", "                    ", "for", "line", "in", "file", ":", "\n", "                        ", "train_file", ".", "write", "(", "f\"__label__POSITIVE {line}\"", ")", "\n", "\n", "", "", "with", "open", "(", "senteval_folder", "/", "\"data\"", "/", "\"customerr\"", "/", "\"custrev.neg\"", ",", "encoding", "=", "\"latin1\"", ")", "as", "file", ":", "\n", "                    ", "for", "line", "in", "file", ":", "\n", "                        ", "train_file", ".", "write", "(", "f\"__label__NEGATIVE {line}\"", ")", "\n", "\n", "", "", "", "", "super", "(", "SENTEVAL_CR", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "label_type", "=", "'sentiment'", ",", "tokenizer", "=", "segtok_tokenizer", ",", "**", "corpusargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.SENTEVAL_MR.__init__": [[636, 672], ["document_classification.SENTEVAL_MR.__class__.__name__.lower", "document_classification.ClassificationCorpus.__init__", "flair.file_utils.cached_path", "flair.file_utils.unzip_file", "pathlib.Path", "os.path.exists", "os.makedirs", "open", "pathlib.Path", "pathlib.Path", "open", "open", "train_file.write", "train_file.write"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.unzip_file"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "**", "corpusargs", "\n", ")", ":", "\n", "# this dataset name", "\n", "        ", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "data_folder", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "if", "not", "(", "data_folder", "/", "\"train.txt\"", ")", ".", "is_file", "(", ")", ":", "\n", "\n", "# download senteval datasets if necessary und unzip", "\n", "            ", "senteval_path", "=", "\"https://dl.fbaipublicfiles.com/senteval/senteval_data/datasmall_NB_ACL12.zip\"", "\n", "cached_path", "(", "senteval_path", ",", "Path", "(", "\"datasets\"", ")", "/", "\"senteval\"", ")", "\n", "senteval_folder", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "\"senteval\"", "\n", "unzip_file", "(", "senteval_folder", "/", "\"datasmall_NB_ACL12.zip\"", ",", "senteval_folder", ")", "\n", "\n", "# create dataset directory if necessary", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_folder", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "data_folder", ")", "\n", "\n", "# create train.txt file by iterating over pos and neg file", "\n", "", "with", "open", "(", "data_folder", "/", "\"train.txt\"", ",", "\"a\"", ")", "as", "train_file", ":", "\n", "\n", "                ", "with", "open", "(", "senteval_folder", "/", "\"data\"", "/", "\"rt10662\"", "/", "\"rt-polarity.pos\"", ",", "encoding", "=", "\"latin1\"", ")", "as", "file", ":", "\n", "                    ", "for", "line", "in", "file", ":", "\n", "                        ", "train_file", ".", "write", "(", "f\"__label__POSITIVE {line}\"", ")", "\n", "\n", "", "", "with", "open", "(", "senteval_folder", "/", "\"data\"", "/", "\"rt10662\"", "/", "\"rt-polarity.neg\"", ",", "encoding", "=", "\"latin1\"", ")", "as", "file", ":", "\n", "                    ", "for", "line", "in", "file", ":", "\n", "                        ", "train_file", ".", "write", "(", "f\"__label__NEGATIVE {line}\"", ")", "\n", "\n", "", "", "", "", "super", "(", "SENTEVAL_MR", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "label_type", "=", "'sentiment'", ",", "tokenizer", "=", "segtok_tokenizer", ",", "**", "corpusargs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.SENTEVAL_SUBJ.__init__": [[676, 712], ["document_classification.SENTEVAL_SUBJ.__class__.__name__.lower", "document_classification.ClassificationCorpus.__init__", "flair.file_utils.cached_path", "flair.file_utils.unzip_file", "pathlib.Path", "os.path.exists", "os.makedirs", "open", "pathlib.Path", "pathlib.Path", "open", "open", "train_file.write", "train_file.write"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.unzip_file"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "**", "corpusargs", ",", "\n", ")", ":", "\n", "# this dataset name", "\n", "        ", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "data_folder", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "if", "not", "(", "data_folder", "/", "\"train.txt\"", ")", ".", "is_file", "(", ")", ":", "\n", "\n", "# download senteval datasets if necessary und unzip", "\n", "            ", "senteval_path", "=", "\"https://dl.fbaipublicfiles.com/senteval/senteval_data/datasmall_NB_ACL12.zip\"", "\n", "cached_path", "(", "senteval_path", ",", "Path", "(", "\"datasets\"", ")", "/", "\"senteval\"", ")", "\n", "senteval_folder", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "\"senteval\"", "\n", "unzip_file", "(", "senteval_folder", "/", "\"datasmall_NB_ACL12.zip\"", ",", "senteval_folder", ")", "\n", "\n", "# create dataset directory if necessary", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_folder", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "data_folder", ")", "\n", "\n", "# create train.txt file by iterating over pos and neg file", "\n", "", "with", "open", "(", "data_folder", "/", "\"train.txt\"", ",", "\"a\"", ")", "as", "train_file", ":", "\n", "\n", "                ", "with", "open", "(", "senteval_folder", "/", "\"data\"", "/", "\"subj\"", "/", "\"subj.subjective\"", ",", "encoding", "=", "\"latin1\"", ")", "as", "file", ":", "\n", "                    ", "for", "line", "in", "file", ":", "\n", "                        ", "train_file", ".", "write", "(", "f\"__label__SUBJECTIVE {line}\"", ")", "\n", "\n", "", "", "with", "open", "(", "senteval_folder", "/", "\"data\"", "/", "\"subj\"", "/", "\"subj.objective\"", ",", "encoding", "=", "\"latin1\"", ")", "as", "file", ":", "\n", "                    ", "for", "line", "in", "file", ":", "\n", "                        ", "train_file", ".", "write", "(", "f\"__label__OBJECTIVE {line}\"", ")", "\n", "\n", "", "", "", "", "super", "(", "SENTEVAL_SUBJ", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "label_type", "=", "'objectivity'", ",", "tokenizer", "=", "segtok_tokenizer", ",", "**", "corpusargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.SENTEVAL_MPQA.__init__": [[716, 752], ["document_classification.SENTEVAL_MPQA.__class__.__name__.lower", "document_classification.ClassificationCorpus.__init__", "flair.file_utils.cached_path", "flair.file_utils.unzip_file", "pathlib.Path", "os.path.exists", "os.makedirs", "open", "pathlib.Path", "pathlib.Path", "open", "open", "train_file.write", "train_file.write"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.unzip_file"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "**", "corpusargs", ",", "\n", ")", ":", "\n", "# this dataset name", "\n", "        ", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "data_folder", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "if", "not", "(", "data_folder", "/", "\"train.txt\"", ")", ".", "is_file", "(", ")", ":", "\n", "\n", "# download senteval datasets if necessary und unzip", "\n", "            ", "senteval_path", "=", "\"https://dl.fbaipublicfiles.com/senteval/senteval_data/datasmall_NB_ACL12.zip\"", "\n", "cached_path", "(", "senteval_path", ",", "Path", "(", "\"datasets\"", ")", "/", "\"senteval\"", ")", "\n", "senteval_folder", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "\"senteval\"", "\n", "unzip_file", "(", "senteval_folder", "/", "\"datasmall_NB_ACL12.zip\"", ",", "senteval_folder", ")", "\n", "\n", "# create dataset directory if necessary", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_folder", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "data_folder", ")", "\n", "\n", "# create train.txt file by iterating over pos and neg file", "\n", "", "with", "open", "(", "data_folder", "/", "\"train.txt\"", ",", "\"a\"", ")", "as", "train_file", ":", "\n", "\n", "                ", "with", "open", "(", "senteval_folder", "/", "\"data\"", "/", "\"mpqa\"", "/", "\"mpqa.pos\"", ",", "encoding", "=", "\"latin1\"", ")", "as", "file", ":", "\n", "                    ", "for", "line", "in", "file", ":", "\n", "                        ", "train_file", ".", "write", "(", "f\"__label__POSITIVE {line}\"", ")", "\n", "\n", "", "", "with", "open", "(", "senteval_folder", "/", "\"data\"", "/", "\"mpqa\"", "/", "\"mpqa.neg\"", ",", "encoding", "=", "\"latin1\"", ")", "as", "file", ":", "\n", "                    ", "for", "line", "in", "file", ":", "\n", "                        ", "train_file", ".", "write", "(", "f\"__label__NEGATIVE {line}\"", ")", "\n", "\n", "", "", "", "", "super", "(", "SENTEVAL_MPQA", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "label_type", "=", "'sentiment'", ",", "tokenizer", "=", "segtok_tokenizer", ",", "**", "corpusargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.SENTEVAL_SST_BINARY.__init__": [[756, 785], ["document_classification.ClassificationCorpus.__init__", "document_classification.SENTEVAL_SST_BINARY.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "pathlib.Path", "open", "open", "line.split", "out_file.write", "pathlib.Path", "pathlib.Path", "pathlib.Path", "fields[].rstrip"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "**", "corpusargs", "\n", ")", ":", "\n", "# this dataset name", "\n", "        ", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "+", "'_v2'", "\n", "\n", "# default dataset folder is the cache root", "\n", "data_folder", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "if", "not", "(", "data_folder", "/", "\"train.txt\"", ")", ".", "is_file", "(", ")", ":", "\n", "\n", "# download senteval datasets if necessary und unzip", "\n", "            ", "cached_path", "(", "'https://raw.githubusercontent.com/PrincetonML/SIF/master/data/sentiment-train'", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "'raw'", ")", "\n", "cached_path", "(", "'https://raw.githubusercontent.com/PrincetonML/SIF/master/data/sentiment-test'", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "'raw'", ")", "\n", "cached_path", "(", "'https://raw.githubusercontent.com/PrincetonML/SIF/master/data/sentiment-dev'", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "'raw'", ")", "\n", "\n", "# create train.txt file by iterating over pos and neg file", "\n", "with", "open", "(", "data_folder", "/", "\"train.txt\"", ",", "\"a\"", ")", "as", "out_file", ",", "open", "(", "data_folder", "/", "'raw'", "/", "\"sentiment-train\"", ")", "as", "in_file", ":", "\n", "                ", "for", "line", "in", "in_file", ":", "\n", "                    ", "fields", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "label", "=", "'POSITIVE'", "if", "fields", "[", "1", "]", ".", "rstrip", "(", ")", "==", "'1'", "else", "'NEGATIVE'", "\n", "out_file", ".", "write", "(", "f\"__label__{label} {fields[0]}\\n\"", ")", "\n", "\n", "", "", "", "super", "(", "SENTEVAL_SST_BINARY", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "\n", "tokenizer", "=", "segtok_tokenizer", ",", "\n", "**", "corpusargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.SENTEVAL_SST_GRANULAR.__init__": [[789, 819], ["document_classification.SENTEVAL_SST_GRANULAR.__class__.__name__.lower", "document_classification.ClassificationCorpus.__init__", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "pathlib.Path", "open", "pathlib.Path", "pathlib.Path", "pathlib.Path", "open", "train_file.write"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "**", "corpusargs", ",", "\n", ")", ":", "\n", "# this dataset name", "\n", "        ", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "data_folder", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "if", "not", "(", "data_folder", "/", "\"train.txt\"", ")", ".", "is_file", "(", ")", ":", "\n", "\n", "# download senteval datasets if necessary und unzip", "\n", "            ", "cached_path", "(", "'https://raw.githubusercontent.com/AcademiaSinicaNLPLab/sentiment_dataset/master/data/stsa.fine.train'", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "'raw'", ")", "\n", "cached_path", "(", "'https://raw.githubusercontent.com/AcademiaSinicaNLPLab/sentiment_dataset/master/data/stsa.fine.test'", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "'raw'", ")", "\n", "cached_path", "(", "'https://raw.githubusercontent.com/AcademiaSinicaNLPLab/sentiment_dataset/master/data/stsa.fine.dev'", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "'raw'", ")", "\n", "\n", "# convert to FastText format", "\n", "for", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", ":", "\n", "                ", "with", "open", "(", "data_folder", "/", "f\"{split}.txt\"", ",", "\"w\"", ")", "as", "train_file", ":", "\n", "\n", "                    ", "with", "open", "(", "data_folder", "/", "'raw'", "/", "f'stsa.fine.{split}'", ",", "encoding", "=", "\"latin1\"", ")", "as", "file", ":", "\n", "                        ", "for", "line", "in", "file", ":", "\n", "                            ", "train_file", ".", "write", "(", "f\"__label__{line[0]} {line[2:]}\"", ")", "\n", "\n", "", "", "", "", "", "super", "(", "SENTEVAL_SST_GRANULAR", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "\n", "tokenizer", "=", "segtok_tokenizer", ",", "\n", "**", "corpusargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.TREC_50.__init__": [[823, 877], ["document_classification.TREC_50.__class__.__name__.lower", "document_classification.ClassificationCorpus.__init__", "type", "pathlib.Path", "flair.file_utils.cached_path", "data_file.is_file", "zip", "pathlib.Path", "open", "pathlib.Path", "open", "line.rstrip.rstrip.rstrip", "line.rstrip.rstrip.split", "write_fp.write"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "**", "corpusargs", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "trec_path", "=", "\"https://cogcomp.seas.upenn.edu/Data/QA/QC/\"", "\n", "\n", "original_filenames", "=", "[", "\"train_5500.label\"", ",", "\"TREC_10.label\"", "]", "\n", "new_filenames", "=", "[", "\"train.txt\"", ",", "\"test.txt\"", "]", "\n", "for", "original_filename", "in", "original_filenames", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{trec_path}{original_filename}\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "\"original\"", ",", "\n", ")", "\n", "\n", "", "data_file", "=", "data_folder", "/", "new_filenames", "[", "0", "]", "\n", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "            ", "for", "original_filename", ",", "new_filename", "in", "zip", "(", "\n", "original_filenames", ",", "new_filenames", "\n", ")", ":", "\n", "                ", "with", "open", "(", "\n", "data_folder", "/", "\"original\"", "/", "original_filename", ",", "\n", "\"rt\"", ",", "\n", "encoding", "=", "\"latin1\"", ",", "\n", ")", "as", "open_fp", ":", "\n", "                    ", "with", "open", "(", "\n", "data_folder", "/", "new_filename", ",", "\"wt\"", ",", "encoding", "=", "\"utf-8\"", "\n", ")", "as", "write_fp", ":", "\n", "                        ", "for", "line", "in", "open_fp", ":", "\n", "                            ", "line", "=", "line", ".", "rstrip", "(", ")", "\n", "fields", "=", "line", ".", "split", "(", ")", "\n", "old_label", "=", "fields", "[", "0", "]", "\n", "question", "=", "\" \"", ".", "join", "(", "fields", "[", "1", ":", "]", ")", "\n", "\n", "# Create flair compatible labels", "\n", "# TREC-6 : NUM:dist -> __label__NUM", "\n", "# TREC-50: NUM:dist -> __label__NUM:dist", "\n", "new_label", "=", "\"__label__\"", "\n", "new_label", "+=", "old_label", "\n", "\n", "write_fp", ".", "write", "(", "f\"{new_label} {question}\\n\"", ")", "\n", "\n", "", "", "", "", "", "super", "(", "TREC_50", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "tokenizer", "=", "space_tokenizer", ",", "**", "corpusargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.TREC_6.__init__": [[881, 935], ["document_classification.TREC_6.__class__.__name__.lower", "document_classification.ClassificationCorpus.__init__", "type", "pathlib.Path", "flair.file_utils.cached_path", "data_file.is_file", "zip", "pathlib.Path", "open", "pathlib.Path", "open", "line.rstrip.rstrip.rstrip", "line.rstrip.rstrip.split", "write_fp.write", "old_label.split"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "**", "corpusargs", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "trec_path", "=", "\"https://cogcomp.seas.upenn.edu/Data/QA/QC/\"", "\n", "\n", "original_filenames", "=", "[", "\"train_5500.label\"", ",", "\"TREC_10.label\"", "]", "\n", "new_filenames", "=", "[", "\"train.txt\"", ",", "\"test.txt\"", "]", "\n", "for", "original_filename", "in", "original_filenames", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{trec_path}{original_filename}\"", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "/", "\"original\"", ",", "\n", ")", "\n", "\n", "", "data_file", "=", "data_folder", "/", "new_filenames", "[", "0", "]", "\n", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "            ", "for", "original_filename", ",", "new_filename", "in", "zip", "(", "\n", "original_filenames", ",", "new_filenames", "\n", ")", ":", "\n", "                ", "with", "open", "(", "\n", "data_folder", "/", "\"original\"", "/", "original_filename", ",", "\n", "\"rt\"", ",", "\n", "encoding", "=", "\"latin1\"", ",", "\n", ")", "as", "open_fp", ":", "\n", "                    ", "with", "open", "(", "\n", "data_folder", "/", "new_filename", ",", "\"wt\"", ",", "encoding", "=", "\"utf-8\"", "\n", ")", "as", "write_fp", ":", "\n", "                        ", "for", "line", "in", "open_fp", ":", "\n", "                            ", "line", "=", "line", ".", "rstrip", "(", ")", "\n", "fields", "=", "line", ".", "split", "(", ")", "\n", "old_label", "=", "fields", "[", "0", "]", "\n", "question", "=", "\" \"", ".", "join", "(", "fields", "[", "1", ":", "]", ")", "\n", "\n", "# Create flair compatible labels", "\n", "# TREC-6 : NUM:dist -> __label__NUM", "\n", "# TREC-50: NUM:dist -> __label__NUM:dist", "\n", "new_label", "=", "\"__label__\"", "\n", "new_label", "+=", "old_label", ".", "split", "(", "\":\"", ")", "[", "0", "]", "\n", "\n", "write_fp", ".", "write", "(", "f\"{new_label} {question}\\n\"", ")", "\n", "\n", "", "", "", "", "", "super", "(", "TREC_6", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "label_type", "=", "'question_type'", ",", "tokenizer", "=", "space_tokenizer", ",", "**", "corpusargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.WASSA_ANGER.__init__": [[965, 983], ["document_classification.WASSA_ANGER.__class__.__name__.lower", "document_classification._download_wassa_if_not_there", "document_classification.ClassificationCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification._download_wassa_if_not_there", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "**", "corpusargs", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wassa_if_not_there", "(", "\"anger\"", ",", "data_folder", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WASSA_ANGER", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "tokenizer", "=", "space_tokenizer", ",", "**", "corpusargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.WASSA_FEAR.__init__": [[987, 1005], ["document_classification.WASSA_FEAR.__class__.__name__.lower", "document_classification._download_wassa_if_not_there", "document_classification.ClassificationCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification._download_wassa_if_not_there", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "**", "corpusargs", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wassa_if_not_there", "(", "\"fear\"", ",", "data_folder", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WASSA_FEAR", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "tokenizer", "=", "space_tokenizer", ",", "**", "corpusargs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.WASSA_JOY.__init__": [[1009, 1027], ["document_classification.WASSA_JOY.__class__.__name__.lower", "document_classification._download_wassa_if_not_there", "document_classification.ClassificationCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification._download_wassa_if_not_there", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "**", "corpusargs", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wassa_if_not_there", "(", "\"joy\"", ",", "data_folder", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WASSA_JOY", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "tokenizer", "=", "space_tokenizer", ",", "**", "corpusargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification.WASSA_SADNESS.__init__": [[1031, 1049], ["document_classification.WASSA_SADNESS.__class__.__name__.lower", "document_classification._download_wassa_if_not_there", "document_classification.ClassificationCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification._download_wassa_if_not_there", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "**", "corpusargs", ")", ":", "\n", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# this dataset name", "\n", "", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wassa_if_not_there", "(", "\"sadness\"", ",", "data_folder", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WASSA_SADNESS", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "tokenizer", "=", "space_tokenizer", ",", "**", "corpusargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.document_classification._download_wassa_if_not_there": [[938, 962], ["data_file.is_file", "flair.file_utils.cached_path", "os.remove", "open", "pathlib.Path", "open", "next", "line.split", "out.write", "fields[].rstrip"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["", "", "def", "_download_wassa_if_not_there", "(", "emotion", ",", "data_folder", ",", "dataset_name", ")", ":", "\n", "    ", "for", "split", "in", "[", "\"train\"", ",", "\"dev\"", ",", "\"test\"", "]", ":", "\n", "\n", "        ", "data_file", "=", "data_folder", "/", "f\"{emotion}-{split}.txt\"", "\n", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "\n", "            ", "if", "split", "==", "\"train\"", ":", "\n", "                ", "url", "=", "f\"http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/{emotion}-ratings-0to1.train.txt\"", "\n", "", "if", "split", "==", "\"dev\"", ":", "\n", "                ", "url", "=", "f\"http://saifmohammad.com/WebDocs/EmoInt%20Dev%20Data%20With%20Gold/{emotion}-ratings-0to1.dev.gold.txt\"", "\n", "", "if", "split", "==", "\"test\"", ":", "\n", "                ", "url", "=", "f\"http://saifmohammad.com/WebDocs/EmoInt%20Test%20Gold%20Data/{emotion}-ratings-0to1.test.gold.txt\"", "\n", "\n", "", "path", "=", "cached_path", "(", "url", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "with", "open", "(", "data_file", ",", "\"w\"", ")", "as", "out", ":", "\n", "                    ", "next", "(", "f", ")", "\n", "for", "line", "in", "f", ":", "\n", "                        ", "fields", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "out", ".", "write", "(", "f\"__label__{fields[3].rstrip()} {fields[1]}\\n\"", ")", "\n", "\n", "", "", "", "os", ".", "remove", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.ColumnCorpus.__init__": [[15, 82], ["flair.datasets.base.find_train_dev_test_files", "sequence_labeling.ColumnDataset", "flair.data.Corpus.__init__", "sequence_labeling.ColumnDataset", "sequence_labeling.ColumnDataset", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.base.find_train_dev_test_files", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "data_folder", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "column_format", ":", "Dict", "[", "int", ",", "str", "]", ",", "\n", "train_file", "=", "None", ",", "\n", "test_file", "=", "None", ",", "\n", "dev_file", "=", "None", ",", "\n", "tag_to_bioes", "=", "None", ",", "\n", "comment_symbol", ":", "str", "=", "None", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", "encoding", ":", "str", "=", "\"utf-8\"", ",", "\n", "document_separator_token", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiates a Corpus from CoNLL column-formatted task data such as CoNLL03 or CoNLL2000.\n\n        :param data_folder: base folder with the task data\n        :param column_format: a map specifying the column format\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :param tag_to_bioes: whether to convert to BIOES tagging scheme\n        :param comment_symbol: if set, lines that begin with this symbol are treated as comments\n        :param in_memory: If set to True, the dataset is kept in memory as Sentence objects, otherwise does disk reads\n        :param document_separator_token: If provided, multiple sentences are read into one object. Provide the string token\n        that indicates that a new document begins\n        :return: a Corpus with annotated train, dev and test data\n        \"\"\"", "\n", "\n", "# find train, dev and test files if not specified", "\n", "dev_file", ",", "test_file", ",", "train_file", "=", "find_train_dev_test_files", "(", "data_folder", ",", "dev_file", ",", "test_file", ",", "train_file", ")", "\n", "\n", "# get train data", "\n", "train", "=", "ColumnDataset", "(", "\n", "train_file", ",", "\n", "column_format", ",", "\n", "tag_to_bioes", ",", "\n", "encoding", "=", "encoding", ",", "\n", "comment_symbol", "=", "comment_symbol", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", "document_separator_token", "=", "document_separator_token", ",", "\n", ")", "\n", "\n", "# read in test file if exists", "\n", "test", "=", "ColumnDataset", "(", "\n", "test_file", ",", "\n", "column_format", ",", "\n", "tag_to_bioes", ",", "\n", "encoding", "=", "encoding", ",", "\n", "comment_symbol", "=", "comment_symbol", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", "document_separator_token", "=", "document_separator_token", ",", "\n", ")", "if", "test_file", "is", "not", "None", "else", "None", "\n", "\n", "# read in dev file if exists", "\n", "dev", "=", "ColumnDataset", "(", "\n", "dev_file", ",", "\n", "column_format", ",", "\n", "tag_to_bioes", ",", "\n", "encoding", "=", "encoding", ",", "\n", "comment_symbol", "=", "comment_symbol", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", "document_separator_token", "=", "document_separator_token", ",", "\n", ")", "if", "dev_file", "is", "not", "None", "else", "None", "\n", "\n", "super", "(", "ColumnCorpus", ",", "self", ")", ".", "__init__", "(", "train", ",", "dev", ",", "test", ",", "name", "=", "str", "(", "data_folder", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.ColumnDataset.__init__": [[85, 187], ["path_to_column_file.exists", "flair.data.Sentence", "open", "f.readline", "sentence.infer_space_after", "str", "sequence_labeling.ColumnDataset.__line_completes_sentence", "f.readline", "sequence_labeling.ColumnDataset.sentences.append", "sequence_labeling.ColumnDataset.indices.append", "f.readline.startswith", "f.readline", "flair.data.Sentence", "sentence.infer_space_after", "re.split", "flair.data.Token", "sequence_labeling.ColumnDataset.sentences.append", "sequence_labeling.ColumnDataset.indices.append", "f.tell", "f.readline.isspace", "sentence.add_token", "f.readline.isspace", "sentence.convert_tag_scheme", "len", "flair.data.Token.add_label"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.infer_space_after", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.ColumnDataset.__line_completes_sentence", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.infer_space_after", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.convert_tag_scheme", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "path_to_column_file", ":", "Path", ",", "\n", "column_name_map", ":", "Dict", "[", "int", ",", "str", "]", ",", "\n", "tag_to_bioes", ":", "str", "=", "None", ",", "\n", "comment_symbol", ":", "str", "=", "None", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", "document_separator_token", ":", "str", "=", "None", ",", "\n", "encoding", ":", "str", "=", "\"utf-8\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiates a column dataset (typically used for sequence labeling or word-level prediction).\n\n        :param path_to_column_file: path to the file with the column-formatted data\n        :param column_name_map: a map specifying the column format\n        :param tag_to_bioes: whether to convert to BIOES tagging scheme\n        :param comment_symbol: if set, lines that begin with this symbol are treated as comments\n        :param in_memory: If set to True, the dataset is kept in memory as Sentence objects, otherwise does disk reads\n        :param document_separator_token: If provided, multiple sentences are read into one object. Provide the string token\n        that indicates that a new document begins\n        \"\"\"", "\n", "assert", "path_to_column_file", ".", "exists", "(", ")", "\n", "self", ".", "path_to_column_file", "=", "path_to_column_file", "\n", "self", ".", "tag_to_bioes", "=", "tag_to_bioes", "\n", "self", ".", "column_name_map", "=", "column_name_map", "\n", "self", ".", "comment_symbol", "=", "comment_symbol", "\n", "self", ".", "document_separator_token", "=", "document_separator_token", "\n", "\n", "# store either Sentence objects in memory, or only file offsets", "\n", "self", ".", "in_memory", "=", "in_memory", "\n", "if", "self", ".", "in_memory", ":", "\n", "            ", "self", ".", "sentences", ":", "List", "[", "Sentence", "]", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "indices", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "\n", "", "self", ".", "total_sentence_count", ":", "int", "=", "0", "\n", "\n", "# most data sets have the token text in the first column, if not, pass 'text' as column", "\n", "self", ".", "text_column", ":", "int", "=", "0", "\n", "for", "column", "in", "self", ".", "column_name_map", ":", "\n", "            ", "if", "column_name_map", "[", "column", "]", "==", "\"text\"", ":", "\n", "                ", "self", ".", "text_column", "=", "column", "\n", "\n", "# determine encoding of text file", "\n", "", "", "self", ".", "encoding", "=", "encoding", "\n", "\n", "sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "sentence_started", ":", "bool", "=", "False", "\n", "with", "open", "(", "str", "(", "self", ".", "path_to_column_file", ")", ",", "encoding", "=", "self", ".", "encoding", ")", "as", "f", ":", "\n", "\n", "            ", "line", "=", "f", ".", "readline", "(", ")", "\n", "position", "=", "0", "\n", "\n", "while", "line", ":", "\n", "\n", "                ", "if", "self", ".", "comment_symbol", "is", "not", "None", "and", "line", ".", "startswith", "(", "comment_symbol", ")", ":", "\n", "                    ", "line", "=", "f", ".", "readline", "(", ")", "\n", "continue", "\n", "\n", "", "if", "self", ".", "__line_completes_sentence", "(", "line", ")", ":", "\n", "\n", "                    ", "if", "sentence_started", ":", "\n", "\n", "                        ", "sentence", ".", "infer_space_after", "(", ")", "\n", "if", "self", ".", "in_memory", ":", "\n", "                            ", "if", "self", ".", "tag_to_bioes", "is", "not", "None", ":", "\n", "                                ", "sentence", ".", "convert_tag_scheme", "(", "\n", "tag_type", "=", "self", ".", "tag_to_bioes", ",", "target_scheme", "=", "\"iobes\"", "\n", ")", "\n", "", "self", ".", "sentences", ".", "append", "(", "sentence", ")", "\n", "", "else", ":", "\n", "                            ", "self", ".", "indices", ".", "append", "(", "position", ")", "\n", "position", "=", "f", ".", "tell", "(", ")", "\n", "", "self", ".", "total_sentence_count", "+=", "1", "\n", "", "sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "sentence_started", "=", "False", "\n", "\n", "", "elif", "self", ".", "in_memory", ":", "\n", "                    ", "fields", ":", "List", "[", "str", "]", "=", "re", ".", "split", "(", "\"\\s+\"", ",", "line", ")", "\n", "token", "=", "Token", "(", "fields", "[", "self", ".", "text_column", "]", ")", "\n", "for", "column", "in", "column_name_map", ":", "\n", "                        ", "if", "len", "(", "fields", ")", ">", "column", ":", "\n", "                            ", "if", "column", "!=", "self", ".", "text_column", ":", "\n", "                                ", "token", ".", "add_label", "(", "\n", "self", ".", "column_name_map", "[", "column", "]", ",", "fields", "[", "column", "]", "\n", ")", "\n", "\n", "", "", "", "if", "not", "line", ".", "isspace", "(", ")", ":", "\n", "                        ", "sentence", ".", "add_token", "(", "token", ")", "\n", "sentence_started", "=", "True", "\n", "", "", "elif", "not", "line", ".", "isspace", "(", ")", ":", "\n", "                    ", "sentence_started", "=", "True", "\n", "\n", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "\n", "", "", "if", "sentence_started", ":", "\n", "            ", "sentence", ".", "infer_space_after", "(", ")", "\n", "if", "self", ".", "in_memory", ":", "\n", "                ", "self", ".", "sentences", ".", "append", "(", "sentence", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "indices", ".", "append", "(", "position", ")", "\n", "", "self", ".", "total_sentence_count", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.ColumnDataset.__line_completes_sentence": [[188, 197], ["line.isspace", "re.split", "len"], "methods", ["None"], ["", "", "def", "__line_completes_sentence", "(", "self", ",", "line", ":", "str", ")", "->", "bool", ":", "\n", "        ", "sentence_completed", "=", "line", ".", "isspace", "(", ")", "\n", "if", "self", ".", "document_separator_token", ":", "\n", "            ", "sentence_completed", "=", "False", "\n", "fields", ":", "List", "[", "str", "]", "=", "re", ".", "split", "(", "\"\\s+\"", ",", "line", ")", "\n", "if", "len", "(", "fields", ")", ">=", "self", ".", "text_column", ":", "\n", "                ", "if", "fields", "[", "self", ".", "text_column", "]", "==", "self", ".", "document_separator_token", ":", "\n", "                    ", "sentence_completed", "=", "True", "\n", "", "", "", "return", "sentence_completed", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.ColumnDataset.is_in_memory": [[198, 200], ["None"], "methods", ["None"], ["", "def", "is_in_memory", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "self", ".", "in_memory", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.ColumnDataset.__len__": [[201, 203], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total_sentence_count", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.ColumnDataset.__getitem__": [[204, 245], ["open", "file.seek", "file.readline", "flair.data.Sentence", "str", "sequence_labeling.ColumnDataset.__line_completes_sentence", "file.readline", "file.readline.startswith", "file.readline", "re.split", "flair.data.Token", "len", "sentence.infer_space_after", "file.readline.isspace", "sentence.add_token", "sentence.convert_tag_scheme", "len", "flair.data.Token.add_label"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.ColumnDataset.__line_completes_sentence", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.infer_space_after", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.add_token", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.convert_tag_scheme", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", "=", "0", ")", "->", "Sentence", ":", "\n", "\n", "        ", "if", "self", ".", "in_memory", ":", "\n", "            ", "sentence", "=", "self", ".", "sentences", "[", "index", "]", "\n", "\n", "", "else", ":", "\n", "            ", "with", "open", "(", "str", "(", "self", ".", "path_to_column_file", ")", ",", "encoding", "=", "self", ".", "encoding", ")", "as", "file", ":", "\n", "                ", "file", ".", "seek", "(", "self", ".", "indices", "[", "index", "]", ")", "\n", "line", "=", "file", ".", "readline", "(", ")", "\n", "sentence", ":", "Sentence", "=", "Sentence", "(", ")", "\n", "while", "line", ":", "\n", "                    ", "if", "self", ".", "comment_symbol", "is", "not", "None", "and", "line", ".", "startswith", "(", "\n", "self", ".", "comment_symbol", "\n", ")", ":", "\n", "                        ", "line", "=", "file", ".", "readline", "(", ")", "\n", "continue", "\n", "\n", "", "if", "self", ".", "__line_completes_sentence", "(", "line", ")", ":", "\n", "                        ", "if", "len", "(", "sentence", ")", ">", "0", ":", "\n", "                            ", "sentence", ".", "infer_space_after", "(", ")", "\n", "if", "self", ".", "tag_to_bioes", "is", "not", "None", ":", "\n", "                                ", "sentence", ".", "convert_tag_scheme", "(", "\n", "tag_type", "=", "self", ".", "tag_to_bioes", ",", "target_scheme", "=", "\"iobes\"", "\n", ")", "\n", "", "return", "sentence", "\n", "\n", "", "", "else", ":", "\n", "                        ", "fields", ":", "List", "[", "str", "]", "=", "re", ".", "split", "(", "\"\\s+\"", ",", "line", ")", "\n", "token", "=", "Token", "(", "fields", "[", "self", ".", "text_column", "]", ")", "\n", "for", "column", "in", "self", ".", "column_name_map", ":", "\n", "                            ", "if", "len", "(", "fields", ")", ">", "column", ":", "\n", "                                ", "if", "column", "!=", "self", ".", "text_column", ":", "\n", "                                    ", "token", ".", "add_label", "(", "\n", "self", ".", "column_name_map", "[", "column", "]", ",", "fields", "[", "column", "]", "\n", ")", "\n", "\n", "", "", "", "if", "not", "line", ".", "isspace", "(", ")", ":", "\n", "                            ", "sentence", ".", "add_token", "(", "token", ")", "\n", "\n", "", "", "line", "=", "file", ".", "readline", "(", ")", "\n", "", "", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.CONLL_03.__init__": [[248, 294], ["sequence_labeling.CONLL_03.__class__.__name__.lower", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "data_folder.exists", "log.warning", "log.warning", "log.warning", "log.warning", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", "document_as_sequence", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Initialize the CoNLL-03 corpus. This is only possible if you've manually downloaded it to your machine.\n        Obtain the corpus from https://www.clips.uantwerpen.be/conll2003/ner/ and put it into some folder. Then point\n        the base_path parameter in the constructor to this folder\n        :param base_path: Path to the CoNLL-03 corpus on your machine\n        :param tag_to_bioes: NER by default, need not be changed, but you could also select 'pos' or 'np' to predict\n        POS tags or chunks respectively\n        :param in_memory: If True, keeps dataset in memory giving speedups in training.\n        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object\n        \"\"\"", "\n", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"np\"", ",", "3", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# check if data there", "\n", "if", "not", "data_folder", ".", "exists", "(", ")", ":", "\n", "            ", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "log", ".", "warning", "(", "f'ACHTUNG: CoNLL-03 dataset not found at \"{data_folder}\".'", ")", "\n", "log", ".", "warning", "(", "\n", "'Instructions for obtaining the data can be found here: https://www.clips.uantwerpen.be/conll2003/ner/\"'", "\n", ")", "\n", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "\n", "", "super", "(", "CONLL_03", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "\n", "columns", ",", "\n", "tag_to_bioes", "=", "tag_to_bioes", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", "document_separator_token", "=", "None", "if", "not", "document_as_sequence", "else", "\"-DOCSTART-\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.CONLL_03_GERMAN.__init__": [[298, 344], ["sequence_labeling.CONLL_03_GERMAN.__class__.__name__.lower", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "data_folder.exists", "log.warning", "log.warning", "log.warning", "log.warning", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", "document_as_sequence", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Initialize the CoNLL-03 corpus for German. This is only possible if you've manually downloaded it to your machine.\n        Obtain the corpus from https://www.clips.uantwerpen.be/conll2003/ner/ and put it into some folder. Then point\n        the base_path parameter in the constructor to this folder\n        :param base_path: Path to the CoNLL-03 corpus on your machine\n        :param tag_to_bioes: NER by default, need not be changed, but you could also select 'lemma', 'pos' or 'np' to predict\n        word lemmas, POS tags or chunks respectively\n        :param in_memory: If True, keeps dataset in memory giving speedups in training.\n        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object\n        \"\"\"", "\n", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"lemma\"", ",", "2", ":", "\"pos\"", ",", "3", ":", "\"np\"", ",", "4", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# check if data there", "\n", "if", "not", "data_folder", ".", "exists", "(", ")", ":", "\n", "            ", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "log", ".", "warning", "(", "f'ACHTUNG: CoNLL-03 dataset not found at \"{data_folder}\".'", ")", "\n", "log", ".", "warning", "(", "\n", "'Instructions for obtaining the data can be found here: https://www.clips.uantwerpen.be/conll2003/ner/\"'", "\n", ")", "\n", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "\n", "", "super", "(", "CONLL_03_GERMAN", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "\n", "columns", ",", "\n", "tag_to_bioes", "=", "tag_to_bioes", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", "document_separator_token", "=", "None", "if", "not", "document_as_sequence", "else", "\"-DOCSTART-\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.CONLL_03_DUTCH.__init__": [[348, 392], ["sequence_labeling.CONLL_03_DUTCH.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", "document_as_sequence", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Initialize the CoNLL-03 corpus for Dutch. The first time you call this constructor it will automatically\n        download the dataset.\n        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this\n        to point to a different folder but typically this should not be necessary.\n        :param tag_to_bioes: NER by default, need not be changed, but you could also select 'pos' to predict\n        POS tags instead\n        :param in_memory: If True, keeps dataset in memory giving speedups in training.\n        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object\n        \"\"\"", "\n", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "conll_02_path", "=", "\"https://www.clips.uantwerpen.be/conll2002/ner/data/\"", "\n", "cached_path", "(", "f\"{conll_02_path}ned.testa\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{conll_02_path}ned.testb\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{conll_02_path}ned.train\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "\n", "super", "(", "CONLL_03_DUTCH", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "\n", "columns", ",", "\n", "tag_to_bioes", "=", "tag_to_bioes", ",", "\n", "encoding", "=", "\"latin-1\"", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", "document_separator_token", "=", "None", "if", "not", "document_as_sequence", "else", "\"-DOCSTART-\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.CONLL_03_SPANISH.__init__": [[396, 437], ["sequence_labeling.CONLL_03_SPANISH.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Initialize the CoNLL-03 corpus for Spanish. The first time you call this constructor it will automatically\n        download the dataset.\n        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this\n        to point to a different folder but typically this should not be necessary.\n        :param tag_to_bioes: NER by default, should not be changed\n        :param in_memory: If True, keeps dataset in memory giving speedups in training.\n        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object\n        \"\"\"", "\n", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "conll_02_path", "=", "\"https://www.clips.uantwerpen.be/conll2002/ner/data/\"", "\n", "cached_path", "(", "f\"{conll_02_path}esp.testa\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{conll_02_path}esp.testb\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{conll_02_path}esp.train\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "\n", "super", "(", "CONLL_03_SPANISH", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "\n", "columns", ",", "\n", "tag_to_bioes", "=", "tag_to_bioes", ",", "\n", "encoding", "=", "\"latin-1\"", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.CONLL_2000.__init__": [[441, 501], ["sequence_labeling.CONLL_2000.__class__.__name__.lower", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "data_file.is_file", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "pathlib.Path", "gzip.open", "gzip.open", "pathlib.Path", "pathlib.Path", "pathlib.Path", "open", "shutil.copyfileobj", "open", "shutil.copyfileobj", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"np\"", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Initialize the CoNLL-2000 corpus for English chunking.\n        The first time you call this constructor it will automatically download the dataset.\n        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this\n        to point to a different folder but typically this should not be necessary.\n        :param tag_to_bioes: 'np' by default, should not be changed, but you can set 'pos' instead to predict POS tags\n        :param in_memory: If True, keeps dataset in memory giving speedups in training.\n        \"\"\"", "\n", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"np\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "conll_2000_path", "=", "\"https://www.clips.uantwerpen.be/conll2000/chunking/\"", "\n", "data_file", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "/", "\"train.txt\"", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{conll_2000_path}train.txt.gz\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "cached_path", "(", "\n", "f\"{conll_2000_path}test.txt.gz\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "import", "gzip", ",", "shutil", "\n", "\n", "with", "gzip", ".", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "/", "\"train.txt.gz\"", ",", "\n", "\"rb\"", ",", "\n", ")", "as", "f_in", ":", "\n", "                ", "with", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "/", "\"train.txt\"", ",", "\n", "\"wb\"", ",", "\n", ")", "as", "f_out", ":", "\n", "                    ", "shutil", ".", "copyfileobj", "(", "f_in", ",", "f_out", ")", "\n", "", "", "with", "gzip", ".", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "/", "\"test.txt.gz\"", ",", "\"rb\"", "\n", ")", "as", "f_in", ":", "\n", "                ", "with", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "/", "\"test.txt\"", ",", "\n", "\"wb\"", ",", "\n", ")", "as", "f_out", ":", "\n", "                    ", "shutil", ".", "copyfileobj", "(", "f_in", ",", "f_out", ")", "\n", "\n", "", "", "", "super", "(", "CONLL_2000", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "in_memory", "=", "in_memory", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.DANE.__init__": [[505, 556], ["sequence_labeling.DANE.__class__.__name__.lower", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "train_data_file.is_file", "flair.file_utils.cached_path", "pathlib.Path", "pathlib.Path", "ZipFile", "zip_file.extractall", "print", "pathlib.Path", "open", "open", "file.writelines", "lines.append", "line.startswith", "lines.append", "line.replace().replace", "line.replace"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "1", ":", "'text'", ",", "3", ":", "'pos'", ",", "9", ":", "'ner'", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "data_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "\n", "train_data_file", "=", "data_path", "/", "\"ddt.train.conllu\"", "\n", "if", "not", "train_data_file", ".", "is_file", "(", ")", ":", "\n", "            ", "temp_file", "=", "cached_path", "(", "\n", "'https://danlp.s3.eu-central-1.amazonaws.com/datasets/ddt.zip'", ",", "\n", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "from", "zipfile", "import", "ZipFile", "\n", "\n", "with", "ZipFile", "(", "temp_file", ",", "'r'", ")", "as", "zip_file", ":", "\n", "                ", "zip_file", ".", "extractall", "(", "path", "=", "data_path", ")", "\n", "\n", "# Remove CoNLL-U meta information in the last column", "\n", "", "for", "part", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", ":", "\n", "                ", "lines", "=", "[", "]", "\n", "data_file", "=", "\"ddt.{}.conllu\"", ".", "format", "(", "part", ")", "\n", "with", "open", "(", "data_path", "/", "data_file", ",", "'r'", ")", "as", "file", ":", "\n", "                    ", "for", "line", "in", "file", ":", "\n", "                        ", "if", "line", ".", "startswith", "(", "\"#\"", ")", "or", "line", "==", "\"\\n\"", ":", "\n", "                            ", "lines", ".", "append", "(", "line", ")", "\n", "", "lines", ".", "append", "(", "line", ".", "replace", "(", "\"name=\"", ",", "\"\"", ")", ".", "replace", "(", "\"|SpaceAfter=No\"", ",", "\"\"", ")", ")", "\n", "\n", "", "", "with", "open", "(", "data_path", "/", "data_file", ",", "'w'", ")", "as", "file", ":", "\n", "                    ", "file", ".", "writelines", "(", "lines", ")", "\n", "\n", "", "print", "(", "data_path", "/", "data_file", ")", "\n", "\n", "", "", "super", "(", "DANE", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "\n", "in_memory", "=", "in_memory", ",", "comment_symbol", "=", "\"#\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.GERMEVAL_14.__init__": [[560, 603], ["sequence_labeling.GERMEVAL_14.__class__.__name__.lower", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "data_folder.exists", "log.warning", "log.warning", "log.warning", "log.warning", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Initialize the GermEval NER corpus for German. This is only possible if you've manually downloaded it to your\n        machine. Obtain the corpus from https://sites.google.com/site/germeval2014ner/home/ and put it into some folder.\n        Then point the base_path parameter in the constructor to this folder\n        :param base_path: Path to the GermEval corpus on your machine\n        :param tag_to_bioes: 'ner' by default, should not be changed.\n        :param in_memory:If True, keeps dataset in memory giving speedups in training.\n        \"\"\"", "\n", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "1", ":", "\"text\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# check if data there", "\n", "if", "not", "data_folder", ".", "exists", "(", ")", ":", "\n", "\n", "            ", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "log", ".", "warning", "(", "f'ACHTUNG: GermEval-14 dataset not found at \"{data_folder}\".'", ")", "\n", "log", ".", "warning", "(", "\n", "'Instructions for obtaining the data can be found here: https://sites.google.com/site/germeval2014ner/home/\"'", "\n", ")", "\n", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "", "super", "(", "GERMEVAL_14", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "\n", "columns", ",", "\n", "tag_to_bioes", "=", "tag_to_bioes", ",", "\n", "comment_symbol", "=", "\"#\"", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.NER_BASQUE.__init__": [[607, 651], ["sequence_labeling.NER_BASQUE.__class__.__name__.lower", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "data_file.is_file", "flair.file_utils.cached_path", "pathlib.Path", "pathlib.Path", "tarfile.open", "pathlib.Path", "f_in.extract", "shutil.move", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "ner_basque_path", "=", "\"http://ixa2.si.ehu.eus/eiec/\"", "\n", "data_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "\n", "data_file", "=", "data_path", "/", "\"named_ent_eu.train\"", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "            ", "cached_path", "(", "\n", "f\"{ner_basque_path}/eiec_v1.0.tgz\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "import", "tarfile", ",", "shutil", "\n", "\n", "with", "tarfile", ".", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "/", "dataset_name", "/", "\"eiec_v1.0.tgz\"", ",", "\n", "\"r:gz\"", ",", "\n", ")", "as", "f_in", ":", "\n", "                ", "corpus_files", "=", "(", "\n", "\"eiec_v1.0/named_ent_eu.train\"", ",", "\n", "\"eiec_v1.0/named_ent_eu.test\"", ",", "\n", ")", "\n", "for", "corpus_file", "in", "corpus_files", ":", "\n", "                    ", "f_in", ".", "extract", "(", "corpus_file", ",", "data_path", ")", "\n", "shutil", ".", "move", "(", "f\"{data_path}/{corpus_file}\"", ",", "data_path", ")", "\n", "\n", "", "", "", "super", "(", "NER_BASQUE", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "in_memory", "=", "in_memory", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.WIKINER_ENGLISH.__init__": [[655, 680], ["sequence_labeling.WIKINER_ENGLISH.__class__.__name__.lower", "sequence_labeling._download_wikiner", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling._download_wikiner", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wikiner", "(", "\"en\"", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WIKINER_ENGLISH", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "in_memory", "=", "in_memory", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.WIKINER_GERMAN.__init__": [[684, 709], ["sequence_labeling.WIKINER_GERMAN.__class__.__name__.lower", "sequence_labeling._download_wikiner", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling._download_wikiner", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wikiner", "(", "\"de\"", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WIKINER_GERMAN", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "in_memory", "=", "in_memory", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.WIKINER_DUTCH.__init__": [[713, 738], ["sequence_labeling.WIKINER_DUTCH.__class__.__name__.lower", "sequence_labeling._download_wikiner", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling._download_wikiner", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wikiner", "(", "\"nl\"", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WIKINER_DUTCH", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "in_memory", "=", "in_memory", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.WIKINER_FRENCH.__init__": [[742, 767], ["sequence_labeling.WIKINER_FRENCH.__class__.__name__.lower", "sequence_labeling._download_wikiner", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling._download_wikiner", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wikiner", "(", "\"fr\"", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WIKINER_FRENCH", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "in_memory", "=", "in_memory", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.WIKINER_ITALIAN.__init__": [[771, 796], ["sequence_labeling.WIKINER_ITALIAN.__class__.__name__.lower", "sequence_labeling._download_wikiner", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling._download_wikiner", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wikiner", "(", "\"it\"", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WIKINER_ITALIAN", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "in_memory", "=", "in_memory", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.WIKINER_SPANISH.__init__": [[800, 825], ["sequence_labeling.WIKINER_SPANISH.__class__.__name__.lower", "sequence_labeling._download_wikiner", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling._download_wikiner", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wikiner", "(", "\"es\"", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WIKINER_SPANISH", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "in_memory", "=", "in_memory", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.WIKINER_PORTUGUESE.__init__": [[829, 854], ["sequence_labeling.WIKINER_PORTUGUESE.__class__.__name__.lower", "sequence_labeling._download_wikiner", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling._download_wikiner", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wikiner", "(", "\"pt\"", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WIKINER_PORTUGUESE", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "in_memory", "=", "in_memory", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.WIKINER_POLISH.__init__": [[858, 883], ["sequence_labeling.WIKINER_POLISH.__class__.__name__.lower", "sequence_labeling._download_wikiner", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling._download_wikiner", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wikiner", "(", "\"pl\"", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WIKINER_POLISH", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "in_memory", "=", "in_memory", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.WIKINER_RUSSIAN.__init__": [[887, 912], ["sequence_labeling.WIKINER_RUSSIAN.__class__.__name__.lower", "sequence_labeling._download_wikiner", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling._download_wikiner", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"pos\"", ",", "2", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "_download_wikiner", "(", "\"ru\"", ",", "dataset_name", ")", "\n", "\n", "super", "(", "WIKINER_RUSSIAN", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "in_memory", "=", "in_memory", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling.WNUT_17.__init__": [[916, 946], ["sequence_labeling.WNUT_17.__class__.__name__.lower", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "flair.file_utils.cached_path", "sequence_labeling.ColumnCorpus.__init__", "type", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "tag_to_bioes", ":", "str", "=", "\"ner\"", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "==", "str", ":", "\n", "            ", "base_path", ":", "Path", "=", "Path", "(", "base_path", ")", "\n", "\n", "# column format", "\n", "", "columns", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"ner\"", "}", "\n", "\n", "# this dataset name", "\n", "dataset_name", "=", "self", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "\n", "\n", "# default dataset folder is the cache root", "\n", "if", "not", "base_path", ":", "\n", "            ", "base_path", "=", "Path", "(", "flair", ".", "cache_root", ")", "/", "\"datasets\"", "\n", "", "data_folder", "=", "base_path", "/", "dataset_name", "\n", "\n", "# download data if necessary", "\n", "wnut_path", "=", "\"https://noisy-text.github.io/2017/files/\"", "\n", "cached_path", "(", "f\"{wnut_path}wnut17train.conll\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "f\"{wnut_path}emerging.dev.conll\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", ")", "\n", "cached_path", "(", "\n", "f\"{wnut_path}emerging.test.annotated\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "\n", "super", "(", "WNUT_17", ",", "self", ")", ".", "__init__", "(", "\n", "data_folder", ",", "columns", ",", "tag_to_bioes", "=", "tag_to_bioes", ",", "in_memory", "=", "in_memory", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.sequence_labeling._download_wikiner": [[949, 989], ["data_file.is_file", "flair.file_utils.cached_path", "bz2.BZ2File", "open", "pathlib.Path", "pathlib.Path", "line.decode.decode", "line.decode.split", "out.write", "pathlib.Path", "pathlib.Path", "word.split"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.decode"], ["", "", "def", "_download_wikiner", "(", "language_code", ":", "str", ",", "dataset_name", ":", "str", ")", ":", "\n", "# download data if necessary", "\n", "    ", "wikiner_path", "=", "(", "\n", "\"https://raw.githubusercontent.com/dice-group/FOX/master/input/Wikiner/\"", "\n", ")", "\n", "lc", "=", "language_code", "\n", "\n", "data_file", "=", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "\n", "/", "\"datasets\"", "\n", "/", "dataset_name", "\n", "/", "f\"aij-wikiner-{lc}-wp3.train\"", "\n", ")", "\n", "if", "not", "data_file", ".", "is_file", "(", ")", ":", "\n", "\n", "        ", "cached_path", "(", "\n", "f\"{wikiner_path}aij-wikiner-{lc}-wp3.bz2\"", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset_name", "\n", ")", "\n", "import", "bz2", ",", "shutil", "\n", "\n", "# unpack and write out in CoNLL column-like format", "\n", "bz_file", "=", "bz2", ".", "BZ2File", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "\n", "/", "\"datasets\"", "\n", "/", "dataset_name", "\n", "/", "f\"aij-wikiner-{lc}-wp3.bz2\"", ",", "\n", "\"rb\"", ",", "\n", ")", "\n", "with", "bz_file", "as", "f", ",", "open", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "\n", "/", "\"datasets\"", "\n", "/", "dataset_name", "\n", "/", "f\"aij-wikiner-{lc}-wp3.train\"", ",", "\n", "\"w\"", ",", "\n", ")", "as", "out", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "line", "=", "line", ".", "decode", "(", "\"utf-8\"", ")", "\n", "words", "=", "line", ".", "split", "(", "\" \"", ")", "\n", "for", "word", "in", "words", ":", "\n", "                    ", "out", ".", "write", "(", "\"\\t\"", ".", "join", "(", "word", ".", "split", "(", "\"|\"", ")", ")", "+", "\"\\n\"", ")", "", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.text_image.FeideggerCorpus.__init__": [[27, 61], ["flair.file_utils.cached_path", "json.load", "os.path.join", "tqdm.tqdm.tqdm", "text_image.FeideggerDataset", "list", "torch.utils.data.dataset.Subset", "list", "torch.utils.data.dataset.Subset", "list", "torch.utils.data.dataset.Subset", "flair.data.Corpus.__init__", "open", "os.path.dirname", "os.path.isdir", "os.mkdir", "os.path.basename", "os.path.join", "pathlib.Path", "os.path.isfile", "urllib.request.urlretrieve", "numpy.where", "numpy.where", "numpy.where", "numpy.in1d", "numpy.in1d", "numpy.in1d", "list", "range"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "dataset", "=", "\"feidegger\"", "\n", "\n", "# cache Feidegger config file", "\n", "json_link", "=", "\"https://raw.githubusercontent.com/zalandoresearch/feidegger/master/data/FEIDEGGER_release_1.1.json\"", "\n", "json_local_path", "=", "cached_path", "(", "json_link", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset", ")", "\n", "\n", "# cache Feidegger images", "\n", "dataset_info", "=", "json", ".", "load", "(", "open", "(", "json_local_path", ",", "\"r\"", ")", ")", "\n", "images_cache_folder", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "json_local_path", ")", ",", "\"images\"", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "images_cache_folder", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "images_cache_folder", ")", "\n", "", "for", "image_info", "in", "tqdm", "(", "dataset_info", ")", ":", "\n", "            ", "name", "=", "os", ".", "path", ".", "basename", "(", "image_info", "[", "\"url\"", "]", ")", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "images_cache_folder", ",", "name", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "filename", ")", ":", "\n", "                ", "urllib", ".", "request", ".", "urlretrieve", "(", "image_info", "[", "\"url\"", "]", ",", "filename", ")", "\n", "# replace image URL with local cached file", "\n", "", "image_info", "[", "\"url\"", "]", "=", "filename", "\n", "\n", "", "feidegger_dataset", ":", "Dataset", "=", "FeideggerDataset", "(", "dataset_info", ",", "**", "kwargs", ")", "\n", "\n", "train_indices", "=", "list", "(", "\n", "np", ".", "where", "(", "np", ".", "in1d", "(", "feidegger_dataset", ".", "split", ",", "list", "(", "range", "(", "8", ")", ")", ")", ")", "[", "0", "]", "\n", ")", "\n", "train", "=", "torch", ".", "utils", ".", "data", ".", "dataset", ".", "Subset", "(", "feidegger_dataset", ",", "train_indices", ")", "\n", "\n", "dev_indices", "=", "list", "(", "np", ".", "where", "(", "np", ".", "in1d", "(", "feidegger_dataset", ".", "split", ",", "[", "8", "]", ")", ")", "[", "0", "]", ")", "\n", "dev", "=", "torch", ".", "utils", ".", "data", ".", "dataset", ".", "Subset", "(", "feidegger_dataset", ",", "dev_indices", ")", "\n", "\n", "test_indices", "=", "list", "(", "np", ".", "where", "(", "np", ".", "in1d", "(", "feidegger_dataset", ".", "split", ",", "[", "9", "]", ")", ")", "[", "0", "]", ")", "\n", "test", "=", "torch", ".", "utils", ".", "data", ".", "dataset", ".", "Subset", "(", "feidegger_dataset", ",", "test_indices", ")", "\n", "\n", "super", "(", "FeideggerCorpus", ",", "self", ")", ".", "__init__", "(", "train", ",", "dev", ",", "test", ",", "name", "=", "\"feidegger\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.text_image.FeideggerDataset.__init__": [[64, 82], ["flair.data.FlairDataset.__init__", "flair.data.Image", "x.lower", "text_image.FeideggerDataset.data_points.append", "text_image.FeideggerDataset.split.append", "flair.data.DataPair", "int", "flair.data.Sentence", "preprocessor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset_info", ",", "in_memory", ":", "bool", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "FeideggerDataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "data_points", ":", "List", "[", "DataPair", "]", "=", "[", "]", "\n", "self", ".", "split", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "\n", "preprocessor", "=", "lambda", "x", ":", "x", "\n", "if", "\"lowercase\"", "in", "kwargs", "and", "kwargs", "[", "\"lowercase\"", "]", ":", "\n", "            ", "preprocessor", "=", "lambda", "x", ":", "x", ".", "lower", "(", ")", "\n", "\n", "", "for", "image_info", "in", "dataset_info", ":", "\n", "            ", "image", "=", "Image", "(", "imageURL", "=", "image_info", "[", "\"url\"", "]", ")", "\n", "for", "caption", "in", "image_info", "[", "\"descriptions\"", "]", ":", "\n", "# append Sentence-Image data point", "\n", "                ", "self", ".", "data_points", ".", "append", "(", "\n", "DataPair", "(", "Sentence", "(", "preprocessor", "(", "caption", ")", ",", "use_tokenizer", "=", "True", ")", ",", "image", ")", "\n", ")", "\n", "self", ".", "split", ".", "append", "(", "int", "(", "image_info", "[", "\"split\"", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.text_image.FeideggerDataset.__len__": [[83, 85], ["len"], "methods", ["None"], ["", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data_points", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.text_image.FeideggerDataset.__getitem__": [[86, 88], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", "=", "0", ")", "->", "DataPair", ":", "\n", "        ", "return", "self", ".", "data_points", "[", "index", "]", "", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.text_text.ParallelTextCorpus.__init__": [[18, 48], ["text_text.ParallelTextDataset", "flair.data.Corpus.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "source_file", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "target_file", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "name", ":", "str", "=", "None", ",", "\n", "use_tokenizer", ":", "bool", "=", "True", ",", "\n", "max_tokens_per_doc", "=", "-", "1", ",", "\n", "max_chars_per_doc", "=", "-", "1", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiates a Corpus for text classification from CSV column formatted data\n\n        :param data_folder: base folder with the task data\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :return: a Corpus with annotated train, dev and test data\n        \"\"\"", "\n", "\n", "train", ":", "FlairDataset", "=", "ParallelTextDataset", "(", "\n", "source_file", ",", "\n", "target_file", ",", "\n", "use_tokenizer", "=", "use_tokenizer", ",", "\n", "max_tokens_per_doc", "=", "max_tokens_per_doc", ",", "\n", "max_chars_per_doc", "=", "max_chars_per_doc", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", ")", "\n", "\n", "super", "(", "ParallelTextCorpus", ",", "self", ")", ".", "__init__", "(", "train", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.text_text.OpusParallelCorpus.__init__": [[51, 115], ["text_text.ParallelTextCorpus.__init__", "log.error", "l1_file.exists", "flair.file_utils.cached_path", "flair.file_utils.unzip_file", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.unzip_file"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "dataset", ":", "str", ",", "\n", "l1", ":", "str", ",", "\n", "l2", ":", "str", ",", "\n", "use_tokenizer", ":", "bool", "=", "True", ",", "\n", "max_tokens_per_doc", "=", "-", "1", ",", "\n", "max_chars_per_doc", "=", "-", "1", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Instantiates a Parallel Corpus from OPUS (http://opus.nlpl.eu/)\n        :param dataset: Name of the dataset (one of \"tatoeba\")\n        :param l1: Language code of first language in pair (\"en\", \"de\", etc.)\n        :param l2: Language code of second language in pair (\"en\", \"de\", etc.)\n        :param use_tokenizer: Whether or not to use in-built tokenizer\n        :param max_tokens_per_doc: If set, shortens sentences to this maximum number of tokens\n        :param max_chars_per_doc: If set, shortens sentences to this maximum number of characters\n        :param in_memory: If True, keeps dataset fully in memory\n        \"\"\"", "\n", "\n", "if", "l1", ">", "l2", ":", "\n", "            ", "l1", ",", "l2", "=", "l2", ",", "l1", "\n", "\n", "# check if dataset is supported", "\n", "", "supported_datasets", "=", "[", "\"tatoeba\"", "]", "\n", "if", "dataset", "not", "in", "supported_datasets", ":", "\n", "            ", "log", ".", "error", "(", "f\"Dataset must be one of: {supported_datasets}\"", ")", "\n", "\n", "# set file names", "\n", "", "if", "dataset", "==", "\"tatoeba\"", ":", "\n", "            ", "link", "=", "f\"https://object.pouta.csc.fi/OPUS-Tatoeba/v20190709/moses/{l1}-{l2}.txt.zip\"", "\n", "\n", "l1_file", "=", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "\n", "/", "\"datasets\"", "\n", "/", "dataset", "\n", "/", "f\"{l1}-{l2}\"", "\n", "/", "f\"Tatoeba.{l1}-{l2}.{l1}\"", "\n", ")", "\n", "l2_file", "=", "(", "\n", "Path", "(", "flair", ".", "cache_root", ")", "\n", "/", "\"datasets\"", "\n", "/", "dataset", "\n", "/", "f\"{l1}-{l2}\"", "\n", "/", "f\"Tatoeba.{l1}-{l2}.{l2}\"", "\n", ")", "\n", "\n", "# download and unzip in file structure if necessary", "\n", "", "if", "not", "l1_file", ".", "exists", "(", ")", ":", "\n", "            ", "path", "=", "cached_path", "(", "link", ",", "Path", "(", "\"datasets\"", ")", "/", "dataset", "/", "f\"{l1}-{l2}\"", ")", "\n", "unzip_file", "(", "\n", "path", ",", "Path", "(", "flair", ".", "cache_root", ")", "/", "Path", "(", "\"datasets\"", ")", "/", "dataset", "/", "f\"{l1}-{l2}\"", "\n", ")", "\n", "\n", "# instantiate corpus", "\n", "", "super", "(", "OpusParallelCorpus", ",", "self", ")", ".", "__init__", "(", "\n", "l1_file", ",", "\n", "l2_file", ",", "\n", "name", "=", "f\"{dataset}-{l1_file}-{l2_file}\"", ",", "\n", "use_tokenizer", "=", "use_tokenizer", ",", "\n", "max_tokens_per_doc", "=", "max_tokens_per_doc", ",", "\n", "max_chars_per_doc", "=", "max_chars_per_doc", ",", "\n", "in_memory", "=", "in_memory", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.text_text.ParallelTextDataset.__init__": [[119, 178], ["path_to_source.exists", "path_to_target.exists", "type", "pathlib.Path", "type", "pathlib.Path", "open", "open", "source_file.readline", "target_file.readline", "str", "str", "source_file.readline", "target_file.readline", "source_file.readline.strip", "target_file.readline.strip", "text_text.ParallelTextDataset._make_bi_sentence", "text_text.ParallelTextDataset.bi_sentences.append", "text_text.ParallelTextDataset.source_lines.append", "text_text.ParallelTextDataset.target_lines.append"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.text_text.ParallelTextDataset._make_bi_sentence"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "path_to_source", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "path_to_target", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "max_tokens_per_doc", "=", "-", "1", ",", "\n", "max_chars_per_doc", "=", "-", "1", ",", "\n", "use_tokenizer", "=", "True", ",", "\n", "in_memory", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "path_to_source", ")", "==", "str", ":", "\n", "            ", "path_to_source", ":", "Path", "=", "Path", "(", "path_to_source", ")", "\n", "", "if", "type", "(", "path_to_target", ")", "==", "str", ":", "\n", "            ", "path_to_target", ":", "Path", "=", "Path", "(", "path_to_target", ")", "\n", "\n", "", "assert", "path_to_source", ".", "exists", "(", ")", "\n", "assert", "path_to_target", ".", "exists", "(", ")", "\n", "\n", "self", ".", "in_memory", "=", "in_memory", "\n", "\n", "self", ".", "use_tokenizer", "=", "use_tokenizer", "\n", "self", ".", "max_tokens_per_doc", "=", "max_tokens_per_doc", "\n", "\n", "self", ".", "total_sentence_count", ":", "int", "=", "0", "\n", "\n", "if", "self", ".", "in_memory", ":", "\n", "            ", "self", ".", "bi_sentences", ":", "List", "[", "DataPair", "]", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "source_lines", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "self", ".", "target_lines", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "", "with", "open", "(", "str", "(", "path_to_source", ")", ",", "encoding", "=", "\"utf-8\"", ")", "as", "source_file", ",", "open", "(", "\n", "str", "(", "path_to_target", ")", ",", "encoding", "=", "\"utf-8\"", "\n", ")", "as", "target_file", ":", "\n", "\n", "            ", "source_line", "=", "source_file", ".", "readline", "(", ")", "\n", "target_line", "=", "target_file", ".", "readline", "(", ")", "\n", "\n", "while", "source_line", "and", "target_line", ":", "\n", "\n", "                ", "source_line", "=", "source_file", ".", "readline", "(", ")", "\n", "target_line", "=", "target_file", ".", "readline", "(", ")", "\n", "\n", "if", "source_line", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "                    ", "continue", "\n", "", "if", "target_line", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "max_chars_per_doc", ">", "0", ":", "\n", "                    ", "source_line", "=", "source_line", "[", ":", "max_chars_per_doc", "]", "\n", "target_line", "=", "target_line", "[", ":", "max_chars_per_doc", "]", "\n", "\n", "", "if", "self", ".", "in_memory", ":", "\n", "                    ", "bi_sentence", "=", "self", ".", "_make_bi_sentence", "(", "source_line", ",", "target_line", ")", "\n", "self", ".", "bi_sentences", ".", "append", "(", "bi_sentence", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "source_lines", ".", "append", "(", "source_line", ")", "\n", "self", ".", "target_lines", ".", "append", "(", "target_line", ")", "\n", "\n", "", "self", ".", "total_sentence_count", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.text_text.ParallelTextDataset._make_bi_sentence": [[179, 189], ["flair.data.Sentence", "flair.data.Sentence", "flair.data.DataPair"], "methods", ["None"], ["", "", "", "def", "_make_bi_sentence", "(", "self", ",", "source_line", ":", "str", ",", "target_line", ":", "str", ")", ":", "\n", "\n", "        ", "source_sentence", "=", "Sentence", "(", "source_line", ",", "use_tokenizer", "=", "self", ".", "use_tokenizer", ")", "\n", "target_sentence", "=", "Sentence", "(", "target_line", ",", "use_tokenizer", "=", "self", ".", "use_tokenizer", ")", "\n", "\n", "if", "self", ".", "max_tokens_per_doc", ">", "0", ":", "\n", "            ", "source_sentence", ".", "tokens", "=", "source_sentence", ".", "tokens", "[", ":", "self", ".", "max_tokens_per_doc", "]", "\n", "target_sentence", ".", "tokens", "=", "target_sentence", ".", "tokens", "[", ":", "self", ".", "max_tokens_per_doc", "]", "\n", "\n", "", "return", "DataPair", "(", "source_sentence", ",", "target_sentence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.text_text.ParallelTextDataset.__len__": [[190, 192], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total_sentence_count", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.text_text.ParallelTextDataset.__getitem__": [[193, 199], ["text_text.ParallelTextDataset._make_bi_sentence"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.datasets.text_text.ParallelTextDataset._make_bi_sentence"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", "=", "0", ")", "->", "DataPair", ":", "\n", "        ", "if", "self", ".", "in_memory", ":", "\n", "            ", "return", "self", ".", "bi_sentences", "[", "index", "]", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_make_bi_sentence", "(", "\n", "self", ".", "source_lines", "[", "index", "]", ",", "self", ".", "target_lines", "[", "index", "]", "\n", ")", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.SearchSpace.__init__": [[38, 40], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "search_space", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.SearchSpace.add": [[41, 43], ["func"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "parameter", ":", "Parameter", ",", "func", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "search_space", "[", "parameter", ".", "value", "]", "=", "func", "(", "parameter", ".", "value", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.SearchSpace.get_search_space": [[44, 46], ["hyperopt.hp.choice"], "methods", ["None"], ["", "def", "get_search_space", "(", "self", ")", ":", "\n", "        ", "return", "hp", ".", "choice", "(", "\"parameters\"", ",", "[", "self", ".", "search_space", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.ParamSelector.__init__": [[49, 70], ["flair.training_utils.init_output_file", "type", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.init_output_file"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "corpus", ":", "Corpus", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "max_epochs", ":", "int", ",", "\n", "evaluation_metric", ":", "EvaluationMetric", ",", "\n", "training_runs", ":", "int", ",", "\n", "optimization_value", ":", "OptimizationValue", ",", "\n", ")", ":", "\n", "        ", "if", "type", "(", "base_path", ")", "is", "str", ":", "\n", "            ", "base_path", "=", "Path", "(", "base_path", ")", "\n", "\n", "", "self", ".", "corpus", "=", "corpus", "\n", "self", ".", "max_epochs", "=", "max_epochs", "\n", "self", ".", "base_path", "=", "base_path", "\n", "self", ".", "evaluation_metric", "=", "evaluation_metric", "\n", "self", ".", "run", "=", "1", "\n", "self", ".", "training_runs", "=", "training_runs", "\n", "self", ".", "optimization_value", "=", "optimization_value", "\n", "\n", "self", ".", "param_selection_file", "=", "init_output_file", "(", "base_path", ",", "\"param_selection.txt\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.ParamSelector._set_up_model": [[71, 74], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "_set_up_model", "(", "self", ",", "params", ":", "dict", ")", "->", "flair", ".", "nn", ".", "Model", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.ParamSelector._objective": [[75, 158], ["flair.training_utils.log_line", "log.info", "log.info", "params.items", "flair.training_utils.log_line", "param_selection.ParamSelector.corpus.get_all_sentences", "range", "flair.training_utils.log_line", "log.info", "params.items", "log.info", "log.info", "log.info", "flair.training_utils.log_line", "isinstance", "log.info", "sent.clear_embeddings", "flair.training_utils.log_line", "log.info", "param_selection.ParamSelector._set_up_model", "flair.trainers.ModelTrainer", "trainer.train", "numpy.var", "scores.append", "vars.append", "sum", "float", "sum", "float", "isinstance", "log.info", "open", "f.write", "params.items", "f.write", "f.write", "f.write", "f.write", "list", "sum", "float", "len", "len", "isinstance", "f.write", "map", "len", "str", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.get_all_sentences", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.clear_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.TextClassifierParamSelector._set_up_model", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train"], ["", "def", "_objective", "(", "self", ",", "params", ":", "dict", ")", ":", "\n", "        ", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "f\"Evaluation run: {self.run}\"", ")", "\n", "log", ".", "info", "(", "f\"Evaluating parameter combination:\"", ")", "\n", "for", "k", ",", "v", "in", "params", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "v", ",", "Tuple", ")", ":", "\n", "                ", "v", "=", "\",\"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "v", "]", ")", "\n", "", "log", ".", "info", "(", "f\"\\t{k}: {str(v)}\"", ")", "\n", "", "log_line", "(", "log", ")", "\n", "\n", "for", "sent", "in", "self", ".", "corpus", ".", "get_all_sentences", "(", ")", ":", "\n", "            ", "sent", ".", "clear_embeddings", "(", ")", "\n", "\n", "", "scores", "=", "[", "]", "\n", "vars", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "self", ".", "training_runs", ")", ":", "\n", "            ", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "f\"Training run: {i + 1}\"", ")", "\n", "\n", "model", "=", "self", ".", "_set_up_model", "(", "params", ")", "\n", "\n", "training_params", "=", "{", "\n", "key", ":", "params", "[", "key", "]", "for", "key", "in", "params", "if", "key", "in", "TRAINING_PARAMETERS", "\n", "}", "\n", "model_trainer_parameters", "=", "{", "\n", "key", ":", "params", "[", "key", "]", "for", "key", "in", "params", "if", "key", "in", "MODEL_TRAINER_PARAMETERS", "\n", "}", "\n", "\n", "trainer", ":", "ModelTrainer", "=", "ModelTrainer", "(", "\n", "model", ",", "self", ".", "corpus", ",", "**", "model_trainer_parameters", "\n", ")", "\n", "\n", "result", "=", "trainer", ".", "train", "(", "\n", "self", ".", "base_path", ",", "\n", "max_epochs", "=", "self", ".", "max_epochs", ",", "\n", "param_selection_mode", "=", "True", ",", "\n", "**", "training_params", ",", "\n", ")", "\n", "\n", "# take the average over the last three scores of training", "\n", "if", "self", ".", "optimization_value", "==", "OptimizationValue", ".", "DEV_LOSS", ":", "\n", "                ", "curr_scores", "=", "result", "[", "\"dev_loss_history\"", "]", "[", "-", "3", ":", "]", "\n", "", "else", ":", "\n", "                ", "curr_scores", "=", "list", "(", "\n", "map", "(", "lambda", "s", ":", "1", "-", "s", ",", "result", "[", "\"dev_score_history\"", "]", "[", "-", "3", ":", "]", ")", "\n", ")", "\n", "\n", "", "score", "=", "sum", "(", "curr_scores", ")", "/", "float", "(", "len", "(", "curr_scores", ")", ")", "\n", "var", "=", "np", ".", "var", "(", "curr_scores", ")", "\n", "scores", ".", "append", "(", "score", ")", "\n", "vars", ".", "append", "(", "var", ")", "\n", "\n", "# take average over the scores from the different training runs", "\n", "", "final_score", "=", "sum", "(", "scores", ")", "/", "float", "(", "len", "(", "scores", ")", ")", "\n", "final_var", "=", "sum", "(", "vars", ")", "/", "float", "(", "len", "(", "vars", ")", ")", "\n", "\n", "test_score", "=", "result", "[", "\"test_score\"", "]", "\n", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "f\"Done evaluating parameter combination:\"", ")", "\n", "for", "k", ",", "v", "in", "params", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "v", ",", "Tuple", ")", ":", "\n", "                ", "v", "=", "\",\"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "v", "]", ")", "\n", "", "log", ".", "info", "(", "f\"\\t{k}: {v}\"", ")", "\n", "", "log", ".", "info", "(", "f\"{self.optimization_value.value}: {final_score}\"", ")", "\n", "log", ".", "info", "(", "f\"variance: {final_var}\"", ")", "\n", "log", ".", "info", "(", "f\"test_score: {test_score}\\n\"", ")", "\n", "log_line", "(", "log", ")", "\n", "\n", "with", "open", "(", "self", ".", "param_selection_file", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "f\"evaluation run {self.run}\\n\"", ")", "\n", "for", "k", ",", "v", "in", "params", ".", "items", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "v", ",", "Tuple", ")", ":", "\n", "                    ", "v", "=", "\",\"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "v", "]", ")", "\n", "", "f", ".", "write", "(", "f\"\\t{k}: {str(v)}\\n\"", ")", "\n", "", "f", ".", "write", "(", "f\"{self.optimization_value.value}: {final_score}\\n\"", ")", "\n", "f", ".", "write", "(", "f\"variance: {final_var}\\n\"", ")", "\n", "f", ".", "write", "(", "f\"test_score: {test_score}\\n\"", ")", "\n", "f", ".", "write", "(", "\"-\"", "*", "100", "+", "\"\\n\"", ")", "\n", "\n", "", "self", ".", "run", "+=", "1", "\n", "\n", "return", "{", "\"status\"", ":", "\"ok\"", ",", "\"loss\"", ":", "final_score", ",", "\"loss_variance\"", ":", "final_var", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.ParamSelector.optimize": [[159, 178], ["hyperopt.fmin", "flair.training_utils.log_line", "log.info", "log.info", "hyperopt.fmin.items", "flair.training_utils.log_line", "log.info", "open", "f.write", "hyperopt.fmin.items", "isinstance", "f.write", "str", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line"], ["", "def", "optimize", "(", "self", ",", "space", ":", "SearchSpace", ",", "max_evals", "=", "100", ")", ":", "\n", "        ", "search_space", "=", "space", ".", "search_space", "\n", "best", "=", "fmin", "(", "\n", "self", ".", "_objective", ",", "search_space", ",", "algo", "=", "tpe", ".", "suggest", ",", "max_evals", "=", "max_evals", "\n", ")", "\n", "\n", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "\"Optimizing parameter configuration done.\"", ")", "\n", "log", ".", "info", "(", "\"Best parameter configuration found:\"", ")", "\n", "for", "k", ",", "v", "in", "best", ".", "items", "(", ")", ":", "\n", "            ", "log", ".", "info", "(", "f\"\\t{k}: {v}\"", ")", "\n", "", "log_line", "(", "log", ")", "\n", "\n", "with", "open", "(", "self", ".", "param_selection_file", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"best parameter combination\\n\"", ")", "\n", "for", "k", ",", "v", "in", "best", ".", "items", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "v", ",", "Tuple", ")", ":", "\n", "                    ", "v", "=", "\",\"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "v", "]", ")", "\n", "", "f", ".", "write", "(", "f\"\\t{k}: {str(v)}\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.SequenceTaggerParamSelector.__init__": [[181, 211], ["param_selection.ParamSelector.__init__", "param_selection.SequenceTaggerParamSelector.corpus.make_tag_dictionary"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.make_tag_dictionary"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "corpus", ":", "Corpus", ",", "\n", "tag_type", ":", "str", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "max_epochs", ":", "int", "=", "50", ",", "\n", "evaluation_metric", ":", "EvaluationMetric", "=", "EvaluationMetric", ".", "MICRO_F1_SCORE", ",", "\n", "training_runs", ":", "int", "=", "1", ",", "\n", "optimization_value", ":", "OptimizationValue", "=", "OptimizationValue", ".", "DEV_LOSS", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        :param corpus: the corpus\n        :param tag_type: tag type to use\n        :param base_path: the path to the result folder (results will be written to that folder)\n        :param max_epochs: number of epochs to perform on every evaluation run\n        :param evaluation_metric: evaluation metric used during training\n        :param training_runs: number of training runs per evaluation run\n        :param optimization_value: value to optimize\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "corpus", ",", "\n", "base_path", ",", "\n", "max_epochs", ",", "\n", "evaluation_metric", ",", "\n", "training_runs", ",", "\n", "optimization_value", ",", "\n", ")", "\n", "\n", "self", ".", "tag_type", "=", "tag_type", "\n", "self", ".", "tag_dictionary", "=", "self", ".", "corpus", ".", "make_tag_dictionary", "(", "self", ".", "tag_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.SequenceTaggerParamSelector._set_up_model": [[212, 223], ["flair.models.SequenceTagger"], "methods", ["None"], ["", "def", "_set_up_model", "(", "self", ",", "params", ":", "dict", ")", ":", "\n", "        ", "sequence_tagger_params", "=", "{", "\n", "key", ":", "params", "[", "key", "]", "for", "key", "in", "params", "if", "key", "in", "SEQUENCE_TAGGER_PARAMETERS", "\n", "}", "\n", "\n", "tagger", ":", "SequenceTagger", "=", "SequenceTagger", "(", "\n", "tag_dictionary", "=", "self", ".", "tag_dictionary", ",", "\n", "tag_type", "=", "self", ".", "tag_type", ",", "\n", "**", "sequence_tagger_params", ",", "\n", ")", "\n", "return", "tagger", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.TextClassifierParamSelector.__init__": [[226, 260], ["param_selection.ParamSelector.__init__", "param_selection.TextClassifierParamSelector.corpus.make_label_dictionary"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.make_label_dictionary"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "corpus", ":", "Corpus", ",", "\n", "multi_label", ":", "bool", ",", "\n", "base_path", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "document_embedding_type", ":", "str", ",", "\n", "max_epochs", ":", "int", "=", "50", ",", "\n", "evaluation_metric", ":", "EvaluationMetric", "=", "EvaluationMetric", ".", "MICRO_F1_SCORE", ",", "\n", "training_runs", ":", "int", "=", "1", ",", "\n", "optimization_value", ":", "OptimizationValue", "=", "OptimizationValue", ".", "DEV_LOSS", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        :param corpus: the corpus\n        :param multi_label: true, if the dataset is multi label, false otherwise\n        :param base_path: the path to the result folder (results will be written to that folder)\n        :param document_embedding_type: either 'lstm', 'mean', 'min', or 'max'\n        :param max_epochs: number of epochs to perform on every evaluation run\n        :param evaluation_metric: evaluation metric used during training\n        :param training_runs: number of training runs per evaluation run\n        :param optimization_value: value to optimize\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "corpus", ",", "\n", "base_path", ",", "\n", "max_epochs", ",", "\n", "evaluation_metric", ",", "\n", "training_runs", ",", "\n", "optimization_value", ",", "\n", ")", "\n", "\n", "self", ".", "multi_label", "=", "multi_label", "\n", "self", ".", "document_embedding_type", "=", "document_embedding_type", "\n", "\n", "self", ".", "label_dictionary", "=", "self", ".", "corpus", ".", "make_label_dictionary", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.hyperparameter.param_selection.TextClassifierParamSelector._set_up_model": [[261, 278], ["flair.models.TextClassifier", "flair.embeddings.DocumentRNNEmbeddings", "flair.embeddings.DocumentPoolEmbeddings"], "methods", ["None"], ["", "def", "_set_up_model", "(", "self", ",", "params", ":", "dict", ")", ":", "\n", "        ", "embdding_params", "=", "{", "\n", "key", ":", "params", "[", "key", "]", "for", "key", "in", "params", "if", "key", "in", "DOCUMENT_EMBEDDING_PARAMETERS", "\n", "}", "\n", "\n", "if", "self", ".", "document_embedding_type", "==", "\"lstm\"", ":", "\n", "            ", "document_embedding", "=", "DocumentRNNEmbeddings", "(", "**", "embdding_params", ")", "\n", "", "else", ":", "\n", "            ", "document_embedding", "=", "DocumentPoolEmbeddings", "(", "**", "embdding_params", ")", "\n", "\n", "", "text_classifier", ":", "TextClassifier", "=", "TextClassifier", "(", "\n", "label_dictionary", "=", "self", ".", "label_dictionary", ",", "\n", "multi_label", "=", "self", ".", "multi_label", ",", "\n", "document_embeddings", "=", "document_embedding", ",", "\n", ")", "\n", "\n", "return", "text_classifier", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer.ModelTrainer.__init__": [[39, 60], ["None"], "methods", ["None"], ["\n", "embeddings", ":", "StackedEmbeddings", "=", "StackedEmbeddings", "(", "embeddings", "=", "embedding_types", ")", "\n", "\n", "ner_model", "=", "MNER", "(", "self", ".", "params", ",", "embeddings", ",", "self", ".", "pre_model", ",", "num_of_tags", "=", "10", ")", "\n", "loss_function_relation", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss_function", "=", "CRF", "(", "num_of_tags", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "ner_model", "=", "ner_model", ".", "cuda", "(", ")", "\n", "loss_function", "=", "loss_function", ".", "cuda", "(", ")", "\n", "\n", "", "paras", "=", "dict", "(", "ner_model", ".", "named_parameters", "(", ")", ")", "\n", "paras_new", "=", "[", "]", "\n", "for", "k", ",", "v", "in", "paras", ".", "items", "(", ")", ":", "\n", "            ", "if", "'pre_resnet'", "in", "k", "or", "'vlbert'", "in", "k", ":", "\n", "                ", "paras_new", "+=", "[", "{", "'params'", ":", "[", "v", "]", ",", "'lr'", ":", "1e-6", "}", "]", "\n", "", "else", ":", "\n", "                ", "paras_new", "+=", "[", "{", "'params'", ":", "[", "v", "]", ",", "'lr'", ":", "1e-4", "}", "]", "\n", "", "", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "paras_new", ",", "weight_decay", "=", "self", ".", "params", ".", "wdecay", ")", "\n", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "burnin_schedule", ")", "\n", "\n", "try", ":", "\n", "            ", "prev_best", "=", "0", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer.ModelTrainer.train": [[61, 582], ["flair.training_utils.add_file_handler", "flair.training_utils.add_file_handler", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "log.info", "log.info", "log.info", "log.info", "log.info", "log.info", "log.info", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.init_output_file", "flair.training_utils.init_output_file", "flair.training_utils.WeightExtractor", "flair.training_utils.WeightExtractor", "trainer.ModelTrainer.optimizer", "torch.optim.lr_scheduler.ReduceLROnPlateau", "log.removeHandler", "type", "pathlib.Path", "isinstance", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.warning", "trainer.ModelTrainer.model.parameters", "amp.initialize", "torch.utils.data.dataset.ConcatDataset", "inspect.isclass", "sampler.set_dataset", "range", "trainer.ModelTrainer.final_test", "log.info", "SummaryWriter.close", "SummaryWriter", "RuntimeError", "RuntimeError", "len", "int", "list", "torch.utils.data.dataset.Subset", "sampler.", "flair.training_utils.log_line", "flair.training_utils.log_line", "trainer.ModelTrainer.model.state_dict", "trainer.ModelTrainer.model.state_dict", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "trainer.ModelTrainer.model.train", "len", "max", "enumerate", "trainer.ModelTrainer.model.eval", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "scheduler.step", "train_loss_history.append", "log.info", "trainer.ModelTrainer.model.save", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.warning", "flair.training_utils.log_line", "flair.training_utils.log_line", "range", "list", "random.shuffle", "torch.utils.data.dataset.Subset", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "int", "time.time", "trainer.ModelTrainer.model.zero_grad", "optimizer.zero_grad", "torch.nn.utils.clip_grad_norm_", "optimizer.step", "trainer.ModelTrainer.item", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "SummaryWriter.add_scalar", "trainer.ModelTrainer.model.evaluate", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "trainer.ModelTrainer.model.evaluate", "log.info", "trainer.ModelTrainer.model.evaluate", "log.info", "dev_score_history.append", "dev_loss_history.append", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "trainer.ModelTrainer.model.evaluate", "log.info", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "open", "f.write", "f.write", "trainer.ModelTrainer.save_checkpoint", "print", "trainer.ModelTrainer.model.save", "trainer.ModelTrainer.model.state_dict", "trainer.ModelTrainer.model.load_state_dict", "trainer.ModelTrainer.model.save", "trainer.ModelTrainer.model.load_state_dict", "SummaryWriter.close", "log.info", "trainer.ModelTrainer.model.save", "log.info", "len", "range", "trainer.ModelTrainer.model.load_state_dict", "trainer.ModelTrainer.model.load_state_dict", "len", "trainer.ModelTrainer.model.forward_loss", "trainer.ModelTrainer.model.parameters", "time.time", "log.info", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "SummaryWriter.add_scalar", "SummaryWriter.add_scalar", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "SummaryWriter.add_scalar", "SummaryWriter.add_scalar", "f.write", "trainer.ModelTrainer.model.load().state_dict", "trainer.ModelTrainer.model.load().state_dict", "trainer.ModelTrainer.backward", "flair.training_utils.WeightExtractor.extract_weights", "flair.training_utils.WeightExtractor.extract_weights", "f.write", "f.write", "f.write", "f.write", "range", "amp.scale_loss", "scaled_loss.backward", "trainer.ModelTrainer.model.state_dict", "round", "round", "round", "datetime.datetime.now", "trainer.ModelTrainer.model.load", "trainer.ModelTrainer.model.load", "len", "train_eval_result.log_header.split", "train_part_eval_result.log_header.split", "dev_eval_result.log_header.split", "test_eval_result.log_header.split"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.add_file_handler", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.add_file_handler", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.init_output_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.init_output_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.initialize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.samplers.ImbalancedClassificationDatasetSampler.set_dataset", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.final_test", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save_checkpoint", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.forward_loss", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.WeightExtractor.extract_weights", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.WeightExtractor.extract_weights", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["best_epoch", "=", "0", "\n", "for", "epoch", "in", "range", "(", "self", ".", "params", ".", "num_epochs", ")", ":", "\n", "                ", "losses", "=", "[", "]", "\n", "start_time", "=", "timer", "(", ")", "\n", "\n", "# relation\u4efb\u52a1", "\n", "for", "(", "x", ",", "x_obj", ",", "y", ",", "mask", ",", "lens", ",", "ifpairs", ")", "in", "tqdm", "(", "self", ".", "dlbb", ".", "train_phase1_loader", ")", ":", "\n", "                    ", "ner_model", ".", "train", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "pair_out", "=", "ner_model", ".", "pretrain_model", "(", "to_variable", "(", "x", ")", ",", "to_variable", "(", "x_obj", ")", ",", "lens", ",", "\n", "to_variable", "(", "mask", ")", ")", "# seq_len * bs * labels", "\n", "ifpairs", "=", "to_variable", "(", "ifpairs", ")", ".", "contiguous", "(", ")", "\n", "loss1", "=", "loss_function_relation", "(", "pair_out", ".", "squeeze", "(", "1", ")", ",", "ifpairs", ")", "\n", "loss1", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "# ner\u4efb\u52a1", "\n", "for", "(", "x", ",", "x_flair", ",", "x_obj", ",", "y", ",", "mask", ",", "lens", ")", "in", "tqdm", "(", "\n", "self", ".", "data_loader", ".", "train_data_loader", ")", ":", "\n", "                    ", "ner_model", ".", "train", "(", ")", "\n", "pair_out", "=", "ner_model", "(", "to_variable", "(", "x", ")", ",", "x_flair", ",", "to_variable", "(", "x_obj", ")", ",", "\n", "lens", ",", "to_variable", "(", "mask", ")", ")", "\n", "\n", "relation", "=", "F", ".", "softmax", "(", "pair_out", ",", "dim", "=", "-", "1", ")", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# ner\u8bad\u7ec3", "\n", "emissions", ",", "attention_probs", "=", "ner_model", "(", "to_variable", "(", "x", ")", ",", "x_flair", ",", "to_variable", "(", "x_obj", ")", ",", "\n", "lens", ",", "to_variable", "(", "mask", ")", ",", "relation", ".", "detach", "(", ")", ")", "# seq_len * bs * labels", "\n", "tags", "=", "to_variable", "(", "y", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "# seq_len * bs", "\n", "mask", "=", "to_variable", "(", "mask", ")", ".", "byte", "(", ")", ".", "transpose", "(", "0", ",", "1", ")", "# seq_len * bs", "\n", "\n", "# computing crf loss", "\n", "loss", "=", "-", "loss_function", "(", "emissions", ",", "tags", ",", "mask", "=", "mask", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "losses", ".", "append", "(", "loss", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "scheduler", ".", "step", "(", ")", "\n", "optim_state", "=", "optimizer", ".", "state_dict", "(", ")", "\n", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "# Calculate accuracy and save best rpbert", "\n", "if", "(", "epoch", "+", "1", ")", "%", "self", ".", "params", ".", "validate_every", "==", "0", ":", "\n", "\n", "                    ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                        ", "acc_dev", ",", "f1_dev", ",", "p_dev", ",", "r_dev", "=", "self", ".", "evaluator", ".", "get_accuracy", "(", "ner_model", ",", "'test'", ",", "loss_function", ")", "\n", "\n", "print", "(", "\n", "\"Epoch {} : Training Loss: {:.5f}, Acc: {:.5f}, F1: {:.5f}, Prec: {:.5f}, Rec: {:.5f}, LR: {:.5f}\"", "\n", "\"Time elapsed {:.2f} mins\"", "\n", ".", "format", "(", "epoch", "+", "1", ",", "np", ".", "asscalar", "(", "np", ".", "mean", "(", "losses", ")", ")", ",", "acc_dev", ",", "f1_dev", ",", "p_dev", ",", "r_dev", ",", "\n", "optim_state", "[", "'param_groups'", "]", "[", "0", "]", "[", "'lr'", "]", ",", "\n", "(", "timer", "(", ")", "-", "start_time", ")", "/", "60", ")", ")", "\n", "if", "f1_dev", ">", "prev_best", "and", "f1_dev", ">", "0.7", ":", "\n", "                            ", "print", "(", "\"f1-score increased....saving weights !!\"", ")", "\n", "best_epoch", "=", "epoch", "+", "1", "\n", "prev_best", "=", "f1_dev", "\n", "model_path", "=", "self", ".", "params", ".", "model_dir", "+", "\"/epoch{}_f1_{:.5f}.pth\"", ".", "format", "(", "epoch", "+", "1", ",", "f1_dev", ")", "\n", "torch", ".", "save", "(", "ner_model", ".", "state_dict", "(", ")", ",", "model_path", ")", "\n", "print", "(", "\"rpbert save in \"", "+", "model_path", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "print", "(", "\"Epoch {} : Training Loss: {:.5f}\"", ".", "format", "(", "epoch", "+", "1", ",", "np", ".", "asscalar", "(", "np", ".", "mean", "(", "losses", ")", ")", ")", ")", "\n", "", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "if", "epoch", "+", "1", "==", "self", ".", "params", ".", "num_epochs", ":", "\n", "                    ", "best_model_path", "=", "self", ".", "params", ".", "model_dir", "+", "\"/epoch{}_f1_{:.5f}.pth\"", ".", "format", "(", "best_epoch", ",", "prev_best", ")", "\n", "print", "(", "\"{} epoch get the best f1 {:.5f}\"", ".", "format", "(", "best_epoch", ",", "prev_best", ")", ")", "\n", "print", "(", "\"the rpbert is save in \"", "+", "model_path", ")", "\n", "", "", "", "except", "KeyboardInterrupt", ":", "\n", "            ", "print", "(", "\"Interrupted.. saving rpbert !!!\"", ")", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "self", ".", "params", ".", "model_dir", "+", "'/model_weights_interrupt.t7'", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer.ModelTrainer.save_checkpoint": [[584, 589], ["torch.save", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save"], []], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer.ModelTrainer.load_checkpoint": [[590, 595], ["torch.load"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], []], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer.ModelTrainer.final_test": [[596, 641], ["flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "trainer.ModelTrainer.model.eval", "trainer.ModelTrainer.model.evaluate", "log.info", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "trainer.ModelTrainer.model.load", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "type", "flair.training_utils.log_line", "flair.training_utils.log_line", "trainer.ModelTrainer.model.evaluate", "flair.datasets.DataLoader", "flair.datasets.DataLoader"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate"], []], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer.ModelTrainer.find_learning_rate": [[642, 734], ["flair.training_utils.init_output_file", "flair.training_utils.init_output_file", "trainer.ModelTrainer.optimizer", "flair.optim.ExpAnnealLR", "flair.optim.ExpAnnealLR", "trainer.ModelTrainer.model.state_dict", "trainer.ModelTrainer.model.train", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "pathlib.Path", "type", "pathlib.Path", "open", "f.write", "trainer.ModelTrainer.model.parameters", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "trainer.ModelTrainer.model.load_state_dict", "trainer.ModelTrainer.model.to", "trainer.ModelTrainer.model.forward_loss", "trainer.ModelTrainer.zero_grad", "trainer.ModelTrainer.backward", "torch.nn.utils.clip_grad_norm_", "trainer.ModelTrainer.step", "flair.optim.ExpAnnealLR.step", "flair.optim.ExpAnnealLR.step", "print", "trainer.ModelTrainer.item", "trainer.ModelTrainer.model.parameters", "flair.optim.ExpAnnealLR.get_lr", "flair.optim.ExpAnnealLR.get_lr", "flair.optim.ExpAnnealLR.get_lr", "flair.optim.ExpAnnealLR.get_lr", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "open", "f.write", "torch.isnan", "str", "datetime.datetime.now"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.init_output_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.init_output_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.forward_loss", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ExpAnnealLR.get_lr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ExpAnnealLR.get_lr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ExpAnnealLR.get_lr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ExpAnnealLR.get_lr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line"], []], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.__init__": [[26, 52], ["path.exists", "path.is_dir", "sorted", "path.iterdir", "f.exists"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "path", ":", "Path", ",", "\n", "dictionary", ":", "Dictionary", ",", "\n", "expand_vocab", ":", "bool", "=", "False", ",", "\n", "forward", ":", "bool", "=", "True", ",", "\n", "split_on_char", ":", "bool", "=", "True", ",", "\n", "random_case_flip", ":", "bool", "=", "True", ",", "\n", "shuffle_lines", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "\n", "        ", "assert", "path", ".", "exists", "(", ")", "\n", "\n", "self", ".", "files", "=", "None", "\n", "self", ".", "path", "=", "path", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "split_on_char", "=", "split_on_char", "\n", "self", ".", "forward", "=", "forward", "\n", "self", ".", "random_case_flip", "=", "random_case_flip", "\n", "self", ".", "expand_vocab", "=", "expand_vocab", "\n", "self", ".", "shuffle_lines", "=", "shuffle_lines", "\n", "\n", "if", "path", ".", "is_dir", "(", ")", ":", "\n", "            ", "self", ".", "files", "=", "sorted", "(", "[", "f", "for", "f", "in", "path", ".", "iterdir", "(", ")", "if", "f", ".", "exists", "(", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "files", "=", "[", "path", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.__len__": [[53, 55], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "files", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.__getitem__": [[56, 63], ["language_model_trainer.TextDataset.charsplit"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.charsplit"], ["", "def", "__getitem__", "(", "self", ",", "index", "=", "0", ")", "->", "torch", ".", "tensor", ":", "\n", "        ", "return", "self", ".", "charsplit", "(", "\n", "self", ".", "files", "[", "index", "]", ",", "\n", "self", ".", "expand_vocab", ",", "\n", "self", ".", "forward", ",", "\n", "self", ".", "split_on_char", ",", "\n", "self", ".", "random_case_flip", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.charsplit": [[65, 134], ["path.exists", "open().readlines", "log.info", "torch.zeros", "random.shuffle", "log.info", "len", "open", "list", "language_model_trainer.TextDataset.split", "len", "language_model_trainer.TextDataset.dictionary.add_item", "language_model_trainer.TextDataset.random_casechange", "list", "language_model_trainer.TextDataset.split", "language_model_trainer.TextDataset.dictionary.get_idx_for_item", "language_model_trainer.TextDataset.random_casechange", "list", "language_model_trainer.TextDataset.split", "language_model_trainer.TextDataset.dictionary.get_idx_for_item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.add_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.random_casechange", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.random_casechange", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item"], ["", "def", "charsplit", "(", "\n", "self", ",", "\n", "path", ":", "Path", ",", "\n", "expand_vocab", "=", "False", ",", "\n", "forward", "=", "True", ",", "\n", "split_on_char", "=", "True", ",", "\n", "random_case_flip", "=", "True", ",", "\n", ")", "->", "torch", ".", "tensor", ":", "\n", "\n", "        ", "\"\"\"Tokenizes a text file on character basis.\"\"\"", "\n", "assert", "path", ".", "exists", "(", ")", "\n", "\n", "lines", "=", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", ".", "readlines", "(", ")", "\n", "log", ".", "info", "(", "f\"read text file with {len(lines)} lines\"", ")", "\n", "if", "self", ".", "shuffle_lines", ":", "\n", "            ", "random", ".", "shuffle", "(", "lines", ")", "\n", "log", ".", "info", "(", "f\"shuffled\"", ")", "\n", "\n", "", "tokens", "=", "0", "\n", "for", "line", "in", "lines", ":", "\n", "\n", "            ", "if", "split_on_char", ":", "\n", "                ", "chars", "=", "list", "(", "line", ")", "\n", "", "else", ":", "\n", "                ", "chars", "=", "line", ".", "split", "(", ")", "\n", "\n", "", "tokens", "+=", "len", "(", "chars", ")", "\n", "\n", "# Add chars to the dictionary", "\n", "if", "expand_vocab", ":", "\n", "                ", "for", "char", "in", "chars", ":", "\n", "                    ", "self", ".", "dictionary", ".", "add_item", "(", "char", ")", "\n", "\n", "", "", "", "ids", "=", "torch", ".", "zeros", "(", "tokens", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "forward", ":", "\n", "# charsplit file content", "\n", "            ", "token", "=", "0", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "if", "random_case_flip", ":", "\n", "                    ", "line", "=", "self", ".", "random_casechange", "(", "line", ")", "\n", "\n", "", "if", "split_on_char", ":", "\n", "                    ", "chars", "=", "list", "(", "line", ")", "\n", "", "else", ":", "\n", "                    ", "chars", "=", "line", ".", "split", "(", ")", "\n", "\n", "", "for", "char", "in", "chars", ":", "\n", "                    ", "if", "token", ">=", "tokens", ":", "\n", "                        ", "break", "\n", "", "ids", "[", "token", "]", "=", "self", ".", "dictionary", ".", "get_idx_for_item", "(", "char", ")", "\n", "token", "+=", "1", "\n", "", "", "", "else", ":", "\n", "# charsplit file content", "\n", "            ", "token", "=", "tokens", "-", "1", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "if", "random_case_flip", ":", "\n", "                    ", "line", "=", "self", ".", "random_casechange", "(", "line", ")", "\n", "\n", "", "if", "split_on_char", ":", "\n", "                    ", "chars", "=", "list", "(", "line", ")", "\n", "", "else", ":", "\n", "                    ", "chars", "=", "line", ".", "split", "(", ")", "\n", "\n", "", "for", "char", "in", "chars", ":", "\n", "                    ", "if", "token", ">=", "tokens", ":", "\n", "                        ", "break", "\n", "", "ids", "[", "token", "]", "=", "self", ".", "dictionary", ".", "get_idx_for_item", "(", "char", ")", "\n", "token", "-=", "1", "\n", "", "", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.random_casechange": [[135, 143], ["random.randint", "line.upper.upper.lower", "line.upper.upper.upper"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "random_casechange", "(", "line", ":", "str", ")", "->", "str", ":", "\n", "        ", "no", "=", "random", ".", "randint", "(", "0", ",", "99", ")", "\n", "if", "no", "is", "0", ":", "\n", "            ", "line", "=", "line", ".", "lower", "(", ")", "\n", "", "if", "no", "is", "1", ":", "\n", "            ", "line", "=", "line", ".", "upper", "(", ")", "\n", "", "return", "line", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextDataset.tokenize": [[144, 167], ["path.exists", "open", "open", "torch.zeros", "len", "line.split", "language_model_trainer.TextDataset.dictionary.add_word", "line.split"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "path", ":", "Path", ")", ":", "\n", "        ", "\"\"\"Tokenizes a text file.\"\"\"", "\n", "assert", "path", ".", "exists", "(", ")", "\n", "# Add words to the dictionary", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "tokens", "=", "0", "\n", "for", "line", "in", "f", ":", "\n", "                ", "words", "=", "line", ".", "split", "(", ")", "+", "[", "\"<eos>\"", "]", "\n", "tokens", "+=", "len", "(", "words", ")", "\n", "for", "word", "in", "words", ":", "\n", "                    ", "self", ".", "dictionary", ".", "add_word", "(", "word", ")", "\n", "\n", "# Tokenize file content", "\n", "", "", "", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "ids", "=", "torch", ".", "zeros", "(", "tokens", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "flair", ".", "device", ")", "\n", "token", "=", "0", "\n", "for", "line", "in", "f", ":", "\n", "                ", "words", "=", "line", ".", "split", "(", ")", "+", "[", "\"<eos>\"", "]", "\n", "for", "word", "in", "words", ":", "\n", "                    ", "ids", "[", "token", "]", "=", "self", ".", "dictionary", ".", "word2idx", "[", "word", "]", "\n", "token", "+=", "1", "\n", "\n", "", "", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.TextCorpus.__init__": [[170, 217], ["language_model_trainer.TextDataset", "type", "pathlib.Path", "language_model_trainer.TextDataset", "language_model_trainer.TextDataset"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "path", ":", "Union", "[", "Path", ",", "str", "]", ",", "\n", "dictionary", ":", "Dictionary", ",", "\n", "forward", ":", "bool", "=", "True", ",", "\n", "character_level", ":", "bool", "=", "True", ",", "\n", "random_case_flip", ":", "bool", "=", "True", ",", "\n", "shuffle_lines", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "self", ".", "dictionary", ":", "Dictionary", "=", "dictionary", "\n", "self", ".", "forward", "=", "forward", "\n", "self", ".", "split_on_char", "=", "character_level", "\n", "self", ".", "random_case_flip", "=", "random_case_flip", "\n", "self", ".", "shuffle_lines", "=", "shuffle_lines", "\n", "\n", "if", "type", "(", "path", ")", "==", "str", ":", "\n", "            ", "path", "=", "Path", "(", "path", ")", "\n", "\n", "", "self", ".", "train", "=", "TextDataset", "(", "\n", "path", "/", "\"train\"", ",", "\n", "dictionary", ",", "\n", "False", ",", "\n", "self", ".", "forward", ",", "\n", "self", ".", "split_on_char", ",", "\n", "self", ".", "random_case_flip", ",", "\n", "shuffle_lines", "=", "self", ".", "shuffle_lines", ",", "\n", ")", "\n", "\n", "# TextDataset returns a list. valid and test are only one file, so return the first element", "\n", "self", ".", "valid", "=", "TextDataset", "(", "\n", "path", "/", "\"valid.txt\"", ",", "\n", "dictionary", ",", "\n", "False", ",", "\n", "self", ".", "forward", ",", "\n", "self", ".", "split_on_char", ",", "\n", "self", ".", "random_case_flip", ",", "\n", "shuffle_lines", "=", "False", ",", "\n", ")", "[", "0", "]", "\n", "self", ".", "test", "=", "TextDataset", "(", "\n", "path", "/", "\"test.txt\"", ",", "\n", "dictionary", ",", "\n", "False", ",", "\n", "self", ".", "forward", ",", "\n", "self", ".", "split_on_char", ",", "\n", "self", ".", "random_case_flip", ",", "\n", "shuffle_lines", "=", "False", ",", "\n", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer.__init__": [[220, 242], ["torch.nn.CrossEntropyLoss"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "LanguageModel", ",", "\n", "corpus", ":", "TextCorpus", ",", "\n", "optimizer", ":", "Optimizer", "=", "SGD", ",", "\n", "test_mode", ":", "bool", "=", "False", ",", "\n", "epoch", ":", "int", "=", "0", ",", "\n", "split", ":", "int", "=", "0", ",", "\n", "loss", ":", "float", "=", "10000", ",", "\n", "optimizer_state", ":", "dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "model", ":", "LanguageModel", "=", "model", "\n", "self", ".", "optimizer", ":", "Optimizer", "=", "optimizer", "\n", "self", ".", "corpus", ":", "TextCorpus", "=", "corpus", "\n", "self", ".", "test_mode", ":", "bool", "=", "test_mode", "\n", "\n", "self", ".", "loss_function", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "self", ".", "log_interval", "=", "100", "\n", "self", ".", "epoch", "=", "epoch", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "loss", "=", "loss", "\n", "self", ".", "optimizer_state", "=", "optimizer_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer.train": [[243, 499], ["flair.training_utils.add_file_handler", "len", "language_model_trainer.LanguageModelTrainer._batchify", "pathlib.Path.mkdir", "language_model_trainer.LanguageModelTrainer._batchify", "language_model_trainer.LanguageModelTrainer.evaluate", "log.info", "log.info", "type", "pathlib.Path", "language_model_trainer.LanguageModelTrainer.size", "RuntimeError", "language_model_trainer.LanguageModelTrainer.optimizer", "isinstance", "torch.utils.data.DataLoader", "range", "math.exp", "open", "myfile.write", "RuntimeError", "RuntimeError", "language_model_trainer.LanguageModelTrainer.model.parameters", "language_model_trainer.LanguageModelTrainer.load_state_dict", "ReduceLRWDOnPlateau", "ReduceLROnPlateau", "amp.initialize", "time.time", "enumerate", "log.info", "log.info", "log.info", "torch.utils.data.DataLoader", "language_model_trainer.LanguageModelTrainer.model.save_checkpoint", "log.info", "time.time", "language_model_trainer.LanguageModelTrainer._batchify", "log.info", "language_model_trainer.LanguageModelTrainer.model.train", "language_model_trainer.LanguageModelTrainer.model.init_hidden", "len", "time.time", "enumerate", "log.info", "language_model_trainer.LanguageModelTrainer.model.eval", "language_model_trainer.LanguageModelTrainer.evaluate", "scheduler.step", "log.info", "log.info", "log.info", "log.info", "log.info", "train_slice.flatten", "range", "language_model_trainer.LanguageModelTrainer._get_batch", "language_model_trainer.LanguageModelTrainer.model.zero_grad", "language_model_trainer.LanguageModelTrainer.zero_grad", "language_model_trainer.LanguageModelTrainer.model.forward", "language_model_trainer.LanguageModelTrainer.loss_function", "torch.nn.utils.clip_grad_norm_", "language_model_trainer.LanguageModelTrainer.step", "language_model_trainer.LanguageModelTrainer._repackage_hidden", "language_model_trainer.LanguageModelTrainer.model.generate_text", "language_model_trainer.LanguageModelTrainer.model.save_checkpoint", "language_model_trainer.LanguageModelTrainer.model.save", "math.exp", "open", "myfile.write", "torch.cuda.is_available", "log.info", "Exception", "output.view", "language_model_trainer.LanguageModelTrainer.backward", "language_model_trainer.LanguageModelTrainer.model.parameters", "log.info", "time.time", "time.time", "time.time", "datetime.datetime.now", "language_model_trainer.LanguageModelTrainer.size", "amp.scale_loss", "scaled_loss.backward", "total_loss.item", "time.time", "math.exp", "time.time", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.add_file_handler", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer._batchify", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer._batchify", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.initialize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save_checkpoint", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer._batchify", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.init_hidden", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer._get_batch", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer._repackage_hidden", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.generate_text", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save_checkpoint", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save"], ["", "def", "train", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "Path", ",", "str", "]", ",", "\n", "sequence_length", ":", "int", ",", "\n", "learning_rate", ":", "float", "=", "20", ",", "\n", "mini_batch_size", ":", "int", "=", "100", ",", "\n", "anneal_factor", ":", "float", "=", "0.25", ",", "\n", "patience", ":", "int", "=", "10", ",", "\n", "clip", "=", "0.25", ",", "\n", "max_epochs", ":", "int", "=", "1000", ",", "\n", "checkpoint", ":", "bool", "=", "False", ",", "\n", "grow_to_sequence_length", ":", "int", "=", "0", ",", "\n", "num_workers", ":", "int", "=", "2", ",", "\n", "use_amp", ":", "bool", "=", "False", ",", "\n", "amp_opt_level", ":", "str", "=", "\"O1\"", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "\n", "        ", "if", "use_amp", ":", "\n", "            ", "if", "sys", ".", "version_info", "<", "(", "3", ",", "0", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Apex currently only supports Python 3. Aborting.\"", ")", "\n", "", "if", "amp", "is", "None", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"Failed to import apex. Please install apex from https://www.github.com/nvidia/apex \"", "\n", "\"to enable mixed-precision training.\"", "\n", ")", "\n", "\n", "# cast string to Path", "\n", "", "", "if", "type", "(", "base_path", ")", "is", "str", ":", "\n", "            ", "base_path", "=", "Path", "(", "base_path", ")", "\n", "\n", "", "add_file_handler", "(", "log", ",", "base_path", "/", "\"training.log\"", ")", "\n", "\n", "number_of_splits", ":", "int", "=", "len", "(", "self", ".", "corpus", ".", "train", ")", "\n", "\n", "val_data", "=", "self", ".", "_batchify", "(", "self", ".", "corpus", ".", "valid", ",", "mini_batch_size", ")", "\n", "\n", "# error message if the validation dataset is too small", "\n", "if", "val_data", ".", "size", "(", "0", ")", "==", "1", ":", "\n", "            ", "raise", "RuntimeError", "(", "\n", "f\"ERROR: Your validation dataset is too small. For your mini_batch_size, the data needs to \"", "\n", "f\"consist of at least {mini_batch_size * 2} characters!\"", "\n", ")", "\n", "\n", "", "base_path", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "loss_txt", "=", "base_path", "/", "\"loss.txt\"", "\n", "savefile", "=", "base_path", "/", "\"best-lm.pt\"", "\n", "\n", "try", ":", "\n", "            ", "best_val_loss", "=", "self", ".", "loss", "\n", "optimizer", "=", "self", ".", "optimizer", "(", "\n", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "learning_rate", ",", "**", "kwargs", "\n", ")", "\n", "if", "self", ".", "optimizer_state", "is", "not", "None", ":", "\n", "                ", "optimizer", ".", "load_state_dict", "(", "self", ".", "optimizer_state", ")", "\n", "\n", "", "if", "isinstance", "(", "optimizer", ",", "(", "AdamW", ",", "SGDW", ")", ")", ":", "\n", "                ", "scheduler", ":", "ReduceLRWDOnPlateau", "=", "ReduceLRWDOnPlateau", "(", "\n", "optimizer", ",", "verbose", "=", "True", ",", "factor", "=", "anneal_factor", ",", "patience", "=", "patience", "\n", ")", "\n", "", "else", ":", "\n", "                ", "scheduler", ":", "ReduceLROnPlateau", "=", "ReduceLROnPlateau", "(", "\n", "optimizer", ",", "verbose", "=", "True", ",", "factor", "=", "anneal_factor", ",", "patience", "=", "patience", "\n", ")", "\n", "\n", "", "if", "use_amp", ":", "\n", "                ", "self", ".", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "\n", "self", ".", "model", ",", "optimizer", ",", "opt_level", "=", "amp_opt_level", "\n", ")", "\n", "\n", "", "training_generator", "=", "DataLoader", "(", "\n", "self", ".", "corpus", ".", "train", ",", "shuffle", "=", "False", ",", "num_workers", "=", "num_workers", "\n", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "self", ".", "epoch", ",", "max_epochs", ")", ":", "\n", "                ", "epoch_start_time", "=", "time", ".", "time", "(", ")", "\n", "# Shuffle training files randomly after serially iterating through corpus one", "\n", "if", "epoch", ">", "0", ":", "\n", "                    ", "training_generator", "=", "DataLoader", "(", "\n", "self", ".", "corpus", ".", "train", ",", "shuffle", "=", "True", ",", "num_workers", "=", "num_workers", "\n", ")", "\n", "self", ".", "model", ".", "save_checkpoint", "(", "\n", "base_path", "/", "f\"epoch_{epoch}.pt\"", ",", "\n", "optimizer", ",", "\n", "epoch", ",", "\n", "0", ",", "\n", "best_val_loss", ",", "\n", ")", "\n", "\n", "# iterate through training data, starting at self.split (for checkpointing)", "\n", "", "for", "curr_split", ",", "train_slice", "in", "enumerate", "(", "\n", "training_generator", ",", "self", ".", "split", "\n", ")", ":", "\n", "\n", "                    ", "if", "sequence_length", "<", "grow_to_sequence_length", ":", "\n", "                        ", "sequence_length", "+=", "1", "\n", "", "log", ".", "info", "(", "f\"Sequence length is {sequence_length}\"", ")", "\n", "\n", "split_start_time", "=", "time", ".", "time", "(", ")", "\n", "# off by one for printing", "\n", "curr_split", "+=", "1", "\n", "train_data", "=", "self", ".", "_batchify", "(", "train_slice", ".", "flatten", "(", ")", ",", "mini_batch_size", ")", "\n", "\n", "log", ".", "info", "(", "\n", "\"Split %d\"", "%", "curr_split", "\n", "+", "\"\\t - ({:%H:%M:%S})\"", ".", "format", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", ")", "\n", "\n", "for", "group", "in", "optimizer", ".", "param_groups", ":", "\n", "                        ", "learning_rate", "=", "group", "[", "\"lr\"", "]", "\n", "\n", "# go into train mode", "\n", "", "self", ".", "model", ".", "train", "(", ")", "\n", "\n", "# reset variables", "\n", "hidden", "=", "self", ".", "model", ".", "init_hidden", "(", "mini_batch_size", ")", "\n", "\n", "# not really sure what this does", "\n", "ntokens", "=", "len", "(", "self", ".", "corpus", ".", "dictionary", ")", "\n", "\n", "total_loss", "=", "0", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "for", "batch", ",", "i", "in", "enumerate", "(", "\n", "range", "(", "0", ",", "train_data", ".", "size", "(", "0", ")", "-", "1", ",", "sequence_length", ")", "\n", ")", ":", "\n", "                        ", "data", ",", "targets", "=", "self", ".", "_get_batch", "(", "train_data", ",", "i", ",", "sequence_length", ")", "\n", "\n", "if", "not", "data", ".", "is_cuda", "and", "cuda", ".", "is_available", "(", ")", ":", "\n", "                            ", "log", ".", "info", "(", "\n", "\"Batch %d is not on CUDA, training will be very slow\"", "\n", "%", "(", "batch", ")", "\n", ")", "\n", "raise", "Exception", "(", "\"data isnt on cuda\"", ")", "\n", "\n", "", "self", ".", "model", ".", "zero_grad", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# do the forward pass in the rpbert", "\n", "output", ",", "rnn_output", ",", "hidden", "=", "self", ".", "model", ".", "forward", "(", "data", ",", "hidden", ")", "\n", "\n", "# try to predict the targets", "\n", "loss", "=", "self", ".", "loss_function", "(", "output", ".", "view", "(", "-", "1", ",", "ntokens", ")", ",", "targets", ")", "\n", "# Backward", "\n", "if", "use_amp", ":", "\n", "                            ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                                ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                            ", "loss", ".", "backward", "(", ")", "\n", "\n", "# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.", "\n", "", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "clip", ")", "\n", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "total_loss", "+=", "loss", ".", "data", "\n", "\n", "# We detach the hidden state from how it was previously produced.", "\n", "# If we didn't, the rpbert would try backpropagating all the way to start of the dataset.", "\n", "hidden", "=", "self", ".", "_repackage_hidden", "(", "hidden", ")", "\n", "\n", "# explicitly remove loss to clear up memory", "\n", "del", "loss", ",", "output", ",", "rnn_output", "\n", "\n", "if", "batch", "%", "self", ".", "log_interval", "==", "0", "and", "batch", ">", "0", ":", "\n", "                            ", "cur_loss", "=", "total_loss", ".", "item", "(", ")", "/", "self", ".", "log_interval", "\n", "elapsed", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "log", ".", "info", "(", "\n", "\"| split {:3d} /{:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | \"", "\n", "\"loss {:5.2f} | ppl {:8.2f}\"", ".", "format", "(", "\n", "curr_split", ",", "\n", "number_of_splits", ",", "\n", "batch", ",", "\n", "len", "(", "train_data", ")", "//", "sequence_length", ",", "\n", "elapsed", "*", "1000", "/", "self", ".", "log_interval", ",", "\n", "cur_loss", ",", "\n", "math", ".", "exp", "(", "cur_loss", ")", ",", "\n", ")", "\n", ")", "\n", "total_loss", "=", "0", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "", "", "log", ".", "info", "(", "\n", "\"%d seconds for train split %d\"", "\n", "%", "(", "time", ".", "time", "(", ")", "-", "split_start_time", ",", "curr_split", ")", "\n", ")", "\n", "\n", "###############################################################################", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "val_loss", "=", "self", ".", "evaluate", "(", "val_data", ",", "mini_batch_size", ",", "sequence_length", ")", "\n", "scheduler", ".", "step", "(", "val_loss", ")", "\n", "\n", "log", ".", "info", "(", "\"best loss so far {:5.2f}\"", ".", "format", "(", "best_val_loss", ")", ")", "\n", "\n", "log", ".", "info", "(", "self", ".", "model", ".", "generate_text", "(", ")", ")", "\n", "\n", "if", "checkpoint", ":", "\n", "                        ", "self", ".", "model", ".", "save_checkpoint", "(", "\n", "base_path", "/", "\"checkpoint.pt\"", ",", "\n", "optimizer", ",", "\n", "epoch", ",", "\n", "curr_split", ",", "\n", "best_val_loss", ",", "\n", ")", "\n", "\n", "# Save the rpbert if the validation loss is the best we've seen so far.", "\n", "", "if", "val_loss", "<", "best_val_loss", ":", "\n", "                        ", "self", ".", "model", ".", "best_score", "=", "best_val_loss", "\n", "self", ".", "model", ".", "save", "(", "savefile", ")", "\n", "best_val_loss", "=", "val_loss", "\n", "\n", "###############################################################################", "\n", "# print info", "\n", "###############################################################################", "\n", "", "log", ".", "info", "(", "\"-\"", "*", "89", ")", "\n", "\n", "summary", "=", "(", "\n", "\"| end of split {:3d} /{:3d} | epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | \"", "\n", "\"valid ppl {:8.2f} | learning rate {:3.4f}\"", ".", "format", "(", "\n", "curr_split", ",", "\n", "number_of_splits", ",", "\n", "epoch", "+", "1", ",", "\n", "(", "time", ".", "time", "(", ")", "-", "split_start_time", ")", ",", "\n", "val_loss", ",", "\n", "math", ".", "exp", "(", "val_loss", ")", ",", "\n", "learning_rate", ",", "\n", ")", "\n", ")", "\n", "\n", "with", "open", "(", "loss_txt", ",", "\"a\"", ")", "as", "myfile", ":", "\n", "                        ", "myfile", ".", "write", "(", "\"%s\\n\"", "%", "summary", ")", "\n", "\n", "", "log", ".", "info", "(", "summary", ")", "\n", "log", ".", "info", "(", "\"-\"", "*", "89", ")", "\n", "\n", "", "log", ".", "info", "(", "\"Epoch time: %.2f\"", "%", "(", "time", ".", "time", "(", ")", "-", "epoch_start_time", ")", ")", "\n", "\n", "", "", "except", "KeyboardInterrupt", ":", "\n", "            ", "log", ".", "info", "(", "\"-\"", "*", "89", ")", "\n", "log", ".", "info", "(", "\"Exiting from training early\"", ")", "\n", "\n", "###############################################################################", "\n", "# final testing", "\n", "###############################################################################", "\n", "", "test_data", "=", "self", ".", "_batchify", "(", "self", ".", "corpus", ".", "test", ",", "mini_batch_size", ")", "\n", "test_loss", "=", "self", ".", "evaluate", "(", "test_data", ",", "mini_batch_size", ",", "sequence_length", ")", "\n", "\n", "summary", "=", "\"TEST: valid loss {:5.2f} | valid ppl {:8.2f}\"", ".", "format", "(", "\n", "test_loss", ",", "math", ".", "exp", "(", "test_loss", ")", "\n", ")", "\n", "with", "open", "(", "loss_txt", ",", "\"a\"", ")", "as", "myfile", ":", "\n", "            ", "myfile", ".", "write", "(", "\"%s\\n\"", "%", "summary", ")", "\n", "\n", "", "log", ".", "info", "(", "summary", ")", "\n", "log", ".", "info", "(", "\"-\"", "*", "89", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer.evaluate": [[500, 517], ["language_model_trainer.LanguageModelTrainer.model.eval", "torch.no_grad", "len", "language_model_trainer.LanguageModelTrainer.model.init_hidden", "range", "language_model_trainer.LanguageModelTrainer._get_batch", "language_model_trainer.LanguageModelTrainer.model.forward", "prediction.view", "language_model_trainer.LanguageModelTrainer._repackage_hidden", "total_loss.item", "len", "data_source.size", "len", "language_model_trainer.LanguageModelTrainer.loss_function"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.init_hidden", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer._get_batch", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer._repackage_hidden"], ["", "def", "evaluate", "(", "self", ",", "data_source", ",", "eval_batch_size", ",", "sequence_length", ")", ":", "\n", "# Turn on evaluation mode which disables dropout.", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "total_loss", "=", "0", "\n", "ntokens", "=", "len", "(", "self", ".", "corpus", ".", "dictionary", ")", "\n", "\n", "hidden", "=", "self", ".", "model", ".", "init_hidden", "(", "eval_batch_size", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "data_source", ".", "size", "(", "0", ")", "-", "1", ",", "sequence_length", ")", ":", "\n", "                ", "data", ",", "targets", "=", "self", ".", "_get_batch", "(", "data_source", ",", "i", ",", "sequence_length", ")", "\n", "prediction", ",", "rnn_output", ",", "hidden", "=", "self", ".", "model", ".", "forward", "(", "data", ",", "hidden", ")", "\n", "output_flat", "=", "prediction", ".", "view", "(", "-", "1", ",", "ntokens", ")", "\n", "total_loss", "+=", "len", "(", "data", ")", "*", "self", ".", "loss_function", "(", "output_flat", ",", "targets", ")", ".", "data", "\n", "hidden", "=", "self", ".", "_repackage_hidden", "(", "hidden", ")", "\n", "", "return", "total_loss", ".", "item", "(", ")", "/", "len", "(", "data_source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer._batchify": [[518, 527], ["data.view().t().contiguous.view().t().contiguous.narrow", "data.view().t().contiguous.view().t().contiguous.view().t().contiguous", "data.view().t().contiguous.view().t().contiguous.size", "data.view().t().contiguous.view().t().contiguous.view().t", "data.view().t().contiguous.view().t().contiguous.view"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "_batchify", "(", "data", ",", "batch_size", ")", ":", "\n", "# Work out how cleanly we can divide the dataset into bsz parts.", "\n", "        ", "nbatch", "=", "data", ".", "size", "(", "0", ")", "//", "batch_size", "\n", "# Trim off any extra elements that wouldn't cleanly fit (remainders).", "\n", "data", "=", "data", ".", "narrow", "(", "0", ",", "0", ",", "nbatch", "*", "batch_size", ")", "\n", "# Evenly divide the data across the bsz batches.", "\n", "data", "=", "data", ".", "view", "(", "batch_size", ",", "-", "1", ")", ".", "t", "(", ")", ".", "contiguous", "(", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer._get_batch": [[528, 539], ["min", "source[].clone().detach", "source[].view().clone().detach", "data.to.to.to", "target.to.to.to", "source[].clone", "source[].view().clone", "len", "source[].view"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "@", "staticmethod", "\n", "def", "_get_batch", "(", "source", ",", "i", ",", "sequence_length", ")", ":", "\n", "        ", "seq_len", "=", "min", "(", "sequence_length", ",", "len", "(", "source", ")", "-", "1", "-", "i", ")", "\n", "\n", "data", "=", "source", "[", "i", ":", "i", "+", "seq_len", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "target", "=", "source", "[", "i", "+", "1", ":", "i", "+", "1", "+", "seq_len", "]", ".", "view", "(", "-", "1", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "\n", "data", "=", "data", ".", "to", "(", "flair", ".", "device", ")", "\n", "target", "=", "target", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "return", "data", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer._repackage_hidden": [[540, 544], ["tuple", "v.clone().detach", "v.clone"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_repackage_hidden", "(", "h", ")", ":", "\n", "        ", "\"\"\"Wraps hidden states in new tensors, to detach them from their history.\"\"\"", "\n", "return", "tuple", "(", "v", ".", "clone", "(", ")", ".", "detach", "(", ")", "for", "v", "in", "h", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.language_model_trainer.LanguageModelTrainer.load_from_checkpoint": [[545, 558], ["flair.models.LanguageModel.load_checkpoint", "language_model_trainer.LanguageModelTrainer"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.load_checkpoint"], ["", "@", "staticmethod", "\n", "def", "load_from_checkpoint", "(", "\n", "checkpoint_file", ":", "Path", ",", "corpus", ":", "TextCorpus", ",", "optimizer", ":", "Optimizer", "=", "SGD", "\n", ")", ":", "\n", "        ", "checkpoint", "=", "LanguageModel", ".", "load_checkpoint", "(", "checkpoint_file", ")", "\n", "return", "LanguageModelTrainer", "(", "\n", "checkpoint", "[", "\"rpbert\"", "]", ",", "\n", "corpus", ",", "\n", "optimizer", ",", "\n", "epoch", "=", "checkpoint", "[", "\"epoch\"", "]", ",", "\n", "split", "=", "checkpoint", "[", "\"split\"", "]", ",", "\n", "loss", "=", "checkpoint", "[", "\"loss\"", "]", ",", "\n", "optimizer_state", "=", "checkpoint", "[", "\"optimizer_state_dict\"", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.__init__": [[39, 60], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "flair", ".", "nn", ".", "Model", ",", "\n", "corpus", ":", "Corpus", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", "=", "SGD", ",", "\n", "epoch", ":", "int", "=", "0", ",", "\n", "use_tensorboard", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Initialize a rpbert trainer\n        :param model: The rpbert that you want to train. The rpbert should inherit from flair.nn.Model\n        :param corpus: The dataset used to train the rpbert, should be of type Corpus\n        :param optimizer: The optimizer to use (typically SGD or Adam)\n        :param epoch: The starting epoch (normally 0 but could be higher if you continue training rpbert)\n        :param use_tensorboard: If True, writes out tensorboard information\n        \"\"\"", "\n", "self", ".", "model", ":", "flair", ".", "nn", ".", "Model", "=", "model", "\n", "self", ".", "corpus", ":", "Corpus", "=", "corpus", "\n", "self", ".", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", "=", "optimizer", "\n", "self", ".", "epoch", ":", "int", "=", "epoch", "\n", "self", ".", "use_tensorboard", ":", "bool", "=", "use_tensorboard", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train": [[61, 582], ["flair.training_utils.add_file_handler", "flair.training_utils.add_file_handler", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "log.info", "log.info", "log.info", "log.info", "log.info", "log.info", "log.info", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.init_output_file", "flair.training_utils.init_output_file", "flair.training_utils.WeightExtractor", "flair.training_utils.WeightExtractor", "trainer_p.ModelTrainer.optimizer", "torch.optim.lr_scheduler.ReduceLROnPlateau", "log.removeHandler", "type", "pathlib.Path", "isinstance", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.warning", "trainer_p.ModelTrainer.model.module.parameters", "amp.initialize", "torch.utils.data.dataset.ConcatDataset", "inspect.isclass", "sampler.set_dataset", "range", "trainer_p.ModelTrainer.final_test", "log.info", "SummaryWriter.close", "SummaryWriter", "RuntimeError", "RuntimeError", "len", "int", "list", "torch.utils.data.dataset.Subset", "sampler.", "flair.training_utils.log_line", "flair.training_utils.log_line", "trainer_p.ModelTrainer.model.module.state_dict", "trainer_p.ModelTrainer.model.module.state_dict", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "trainer_p.ModelTrainer.model.train", "len", "max", "enumerate", "trainer_p.ModelTrainer.model.eval", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "scheduler.step", "train_loss_history.append", "log.info", "trainer_p.ModelTrainer.model.module.save", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.warning", "flair.training_utils.log_line", "flair.training_utils.log_line", "range", "list", "random.shuffle", "torch.utils.data.dataset.Subset", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "int", "time.time", "trainer_p.ModelTrainer.model.zero_grad", "optimizer.zero_grad", "torch.nn.utils.clip_grad_norm_", "optimizer.step", "trainer_p.ModelTrainer.item", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "SummaryWriter.add_scalar", "trainer_p.ModelTrainer.model.module.evaluate", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "trainer_p.ModelTrainer.model.module.evaluate", "log.info", "trainer_p.ModelTrainer.model.module.evaluate", "log.info", "dev_score_history.append", "dev_loss_history.append", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "trainer_p.ModelTrainer.model.module.evaluate", "log.info", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "open", "f.write", "f.write", "trainer_p.ModelTrainer.save_checkpoint", "print", "trainer_p.ModelTrainer.model.module.save", "trainer_p.ModelTrainer.model.state_dict", "trainer_p.ModelTrainer.model.module.load_state_dict", "trainer_p.ModelTrainer.model.module.save", "trainer_p.ModelTrainer.model.module.load_state_dict", "SummaryWriter.close", "log.info", "trainer_p.ModelTrainer.model.module.save", "log.info", "len", "range", "trainer_p.ModelTrainer.model.module.load_state_dict", "trainer_p.ModelTrainer.model.module.load_state_dict", "len", "trainer_p.ModelTrainer.model.module.forward_loss", "trainer_p.ModelTrainer.model.module.parameters", "time.time", "log.info", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "SummaryWriter.add_scalar", "SummaryWriter.add_scalar", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "SummaryWriter.add_scalar", "SummaryWriter.add_scalar", "f.write", "trainer_p.ModelTrainer.model.module.load().state_dict", "trainer_p.ModelTrainer.model.module.load().state_dict", "trainer_p.ModelTrainer.backward", "flair.training_utils.WeightExtractor.extract_weights", "flair.training_utils.WeightExtractor.extract_weights", "f.write", "f.write", "f.write", "f.write", "range", "amp.scale_loss", "scaled_loss.backward", "trainer_p.ModelTrainer.model.module.state_dict", "round", "round", "round", "datetime.datetime.now", "trainer_p.ModelTrainer.model.module.load", "trainer_p.ModelTrainer.model.module.load", "len", "train_eval_result.log_header.split", "train_part_eval_result.log_header.split", "dev_eval_result.log_header.split", "test_eval_result.log_header.split"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.add_file_handler", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.add_file_handler", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.init_output_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.init_output_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.initialize", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.samplers.ImbalancedClassificationDatasetSampler.set_dataset", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.final_test", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save_checkpoint", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.forward_loss", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.WeightExtractor.extract_weights", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.WeightExtractor.extract_weights", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["", "def", "train", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "Path", ",", "str", "]", ",", "\n", "learning_rate", ":", "float", "=", "0.1", ",", "\n", "mini_batch_size", ":", "int", "=", "32", ",", "\n", "mini_batch_chunk_size", ":", "int", "=", "None", ",", "\n", "max_epochs", ":", "int", "=", "100", ",", "\n", "anneal_factor", ":", "float", "=", "0.5", ",", "\n", "patience", ":", "int", "=", "3", ",", "\n", "min_learning_rate", ":", "float", "=", "0.0001", ",", "\n", "train_with_dev", ":", "bool", "=", "False", ",", "\n", "monitor_train", ":", "bool", "=", "False", ",", "\n", "monitor_test", ":", "bool", "=", "False", ",", "\n", "embeddings_storage_mode", ":", "str", "=", "\"cpu\"", ",", "\n", "checkpoint", ":", "bool", "=", "False", ",", "\n", "save_final_model", ":", "bool", "=", "True", ",", "\n", "anneal_with_restarts", ":", "bool", "=", "False", ",", "\n", "anneal_with_prestarts", ":", "bool", "=", "False", ",", "\n", "batch_growth_annealing", ":", "bool", "=", "False", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "param_selection_mode", ":", "bool", "=", "False", ",", "\n", "num_workers", ":", "int", "=", "6", ",", "\n", "sampler", "=", "None", ",", "\n", "use_amp", ":", "bool", "=", "False", ",", "\n", "amp_opt_level", ":", "str", "=", "\"O1\"", ",", "\n", "eval_on_train_fraction", "=", "0.0", ",", "\n", "eval_on_train_shuffle", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "dict", ":", "\n", "        ", "\"\"\"\n        Trains any class that implements the flair.nn.Model interface.\n        :param base_path: Main path to which all output during training is logged and models are saved\n        :param learning_rate: Initial learning rate\n        :param mini_batch_size: Size of mini-batches during training\n        :param mini_batch_chunk_size: If mini-batches are larger than this number, they get broken down into chunks of this size for processing purposes\n        :param max_epochs: Maximum number of epochs to train. Terminates training if this number is surpassed.\n        :param anneal_factor: The factor by which the learning rate is annealed\n        :param patience: Patience is the number of epochs with no improvement the Trainer waits\n         until annealing the learning rate\n        :param min_learning_rate: If the learning rate falls below this threshold, training terminates\n        :param train_with_dev: If True, training is performed using both train+dev data\n        :param monitor_train: If True, training data is evaluated at end of each epoch\n        :param monitor_test: If True, test data is evaluated at end of each epoch\n        :param embeddings_storage_mode: One of 'none' (all embeddings are deleted and freshly recomputed),\n        'cpu' (embeddings are stored on CPU) or 'gpu' (embeddings are stored on GPU)\n        :param checkpoint: If True, a full checkpoint is saved at end of each epoch\n        :param save_final_model: If True, final rpbert is saved\n        :param anneal_with_restarts: If True, the last best rpbert is restored when annealing the learning rate\n        :param shuffle: If True, data is shuffled during training\n        :param param_selection_mode: If True, testing is performed against dev data. Use this mode when doing\n        parameter selection.\n        :param num_workers: Number of workers in your data loader.\n        :param sampler: You can pass a data sampler here for special sampling of data.\n        :param eval_on_train_fraction: the fraction of train data to do the evaluation on,\n        if 0. the evaluation is not performed on fraction of training data,\n        if 'dev' the size is determined from dev set size\n        :param eval_on_train_shuffle: if True the train data fraction is determined on the start of training\n        and kept fixed during training, otherwise it's sampled at beginning of each epoch\n        :param kwargs: Other arguments for the Optimizer\n        :return:\n        \"\"\"", "\n", "\n", "if", "self", ".", "use_tensorboard", ":", "\n", "            ", "try", ":", "\n", "                ", "from", "torch", ".", "utils", ".", "tensorboard", "import", "SummaryWriter", "\n", "\n", "writer", "=", "SummaryWriter", "(", ")", "\n", "", "except", ":", "\n", "                ", "log_line", "(", "log", ")", "\n", "log", ".", "warning", "(", "\n", "\"ATTENTION! PyTorch >= 1.1.0 and pillow are required for TensorBoard support!\"", "\n", ")", "\n", "log_line", "(", "log", ")", "\n", "self", ".", "use_tensorboard", "=", "False", "\n", "pass", "\n", "\n", "", "", "if", "use_amp", ":", "\n", "            ", "if", "sys", ".", "version_info", "<", "(", "3", ",", "0", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Apex currently only supports Python 3. Aborting.\"", ")", "\n", "", "if", "amp", "is", "None", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"Failed to import apex. Please install apex from https://www.github.com/nvidia/apex \"", "\n", "\"to enable mixed-precision training.\"", "\n", ")", "\n", "\n", "", "", "if", "mini_batch_chunk_size", "is", "None", ":", "\n", "            ", "mini_batch_chunk_size", "=", "mini_batch_size", "\n", "\n", "# cast string to Path", "\n", "", "if", "type", "(", "base_path", ")", "is", "str", ":", "\n", "            ", "base_path", "=", "Path", "(", "base_path", ")", "\n", "\n", "", "log_handler", "=", "add_file_handler", "(", "log", ",", "base_path", "/", "\"training.log\"", ")", "\n", "\n", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "f'Model: \"{self.model.module}\"'", ")", "\n", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "f'Corpus: \"{self.corpus}\"'", ")", "\n", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "\"Parameters:\"", ")", "\n", "log", ".", "info", "(", "f' - learning_rate: \"{learning_rate}\"'", ")", "\n", "log", ".", "info", "(", "f' - mini_batch_size: \"{mini_batch_size}\"'", ")", "\n", "log", ".", "info", "(", "f' - patience: \"{patience}\"'", ")", "\n", "log", ".", "info", "(", "f' - anneal_factor: \"{anneal_factor}\"'", ")", "\n", "log", ".", "info", "(", "f' - max_epochs: \"{max_epochs}\"'", ")", "\n", "log", ".", "info", "(", "f' - shuffle: \"{shuffle}\"'", ")", "\n", "log", ".", "info", "(", "f' - train_with_dev: \"{train_with_dev}\"'", ")", "\n", "log", ".", "info", "(", "f' - batch_growth_annealing: \"{batch_growth_annealing}\"'", ")", "\n", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "f'Model training base path: \"{base_path}\"'", ")", "\n", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "f\"Device: {flair.device}\"", ")", "\n", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "f\"Embeddings storage mode: {embeddings_storage_mode}\"", ")", "\n", "if", "isinstance", "(", "self", ".", "model", ".", "module", ",", "SequenceTagger", ")", "and", "self", ".", "model", ".", "module", ".", "weight_dict", "and", "self", ".", "model", ".", "module", ".", "use_crf", ":", "\n", "            ", "log_line", "(", "log", ")", "\n", "log", ".", "warning", "(", "f'WARNING: Specified class weights will not take effect when using CRF'", ")", "\n", "\n", "# determine what splits (train, dev, test) to evaluate and log", "\n", "", "log_train", "=", "True", "if", "monitor_train", "else", "False", "\n", "log_test", "=", "(", "\n", "True", "\n", "if", "(", "not", "param_selection_mode", "and", "self", ".", "corpus", ".", "test", "and", "monitor_test", ")", "\n", "else", "False", "\n", ")", "\n", "log_dev", "=", "True", "if", "not", "train_with_dev", "else", "False", "\n", "log_train_part", "=", "(", "\n", "True", "\n", "if", "(", "eval_on_train_fraction", "==", "\"dev\"", "or", "eval_on_train_fraction", ">", "0.0", ")", "\n", "else", "False", "\n", ")", "\n", "\n", "if", "log_train_part", ":", "\n", "            ", "train_part_size", "=", "(", "\n", "len", "(", "self", ".", "corpus", ".", "dev", ")", "\n", "if", "eval_on_train_fraction", "==", "\"dev\"", "\n", "else", "int", "(", "len", "(", "self", ".", "corpus", ".", "train", ")", "*", "eval_on_train_fraction", ")", "\n", ")", "\n", "assert", "train_part_size", ">", "0", "\n", "if", "not", "eval_on_train_shuffle", ":", "\n", "                ", "train_part_indices", "=", "list", "(", "range", "(", "train_part_size", ")", ")", "\n", "train_part", "=", "torch", ".", "utils", ".", "data", ".", "dataset", ".", "Subset", "(", "\n", "self", ".", "corpus", ".", "train", ",", "train_part_indices", "\n", ")", "\n", "\n", "# prepare loss logging file and set up header", "\n", "", "", "loss_txt", "=", "init_output_file", "(", "base_path", ",", "\"loss.tsv\"", ")", "\n", "\n", "weight_extractor", "=", "WeightExtractor", "(", "base_path", ")", "\n", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", "=", "self", ".", "optimizer", "(", "\n", "self", ".", "model", ".", "module", ".", "parameters", "(", ")", ",", "lr", "=", "learning_rate", ",", "**", "kwargs", "\n", ")", "\n", "\n", "if", "use_amp", ":", "\n", "            ", "self", ".", "model", ".", "module", ",", "optimizer", "=", "amp", ".", "initialize", "(", "\n", "self", ".", "model", ".", "module", ",", "optimizer", ",", "opt_level", "=", "amp_opt_level", "\n", ")", "\n", "\n", "# minimize training loss if training with dev data, else maximize dev score", "\n", "", "anneal_mode", "=", "\"min\"", "if", "train_with_dev", "else", "\"max\"", "\n", "\n", "scheduler", ":", "ReduceLROnPlateau", "=", "ReduceLROnPlateau", "(", "\n", "optimizer", ",", "\n", "factor", "=", "anneal_factor", ",", "\n", "patience", "=", "patience", ",", "\n", "mode", "=", "anneal_mode", ",", "\n", "verbose", "=", "True", ",", "\n", ")", "\n", "\n", "train_data", "=", "self", ".", "corpus", ".", "train", "\n", "\n", "# if training also uses dev data, include in training set", "\n", "if", "train_with_dev", ":", "\n", "            ", "train_data", "=", "ConcatDataset", "(", "[", "self", ".", "corpus", ".", "train", ",", "self", ".", "corpus", ".", "dev", "]", ")", "\n", "\n", "# initialize sampler if provided", "\n", "", "if", "sampler", "is", "not", "None", ":", "\n", "# init with default values if only class is provided", "\n", "            ", "if", "inspect", ".", "isclass", "(", "sampler", ")", ":", "\n", "                ", "sampler", "=", "sampler", "(", ")", "\n", "# set dataset to sample from", "\n", "", "sampler", ".", "set_dataset", "(", "train_data", ")", "\n", "shuffle", "=", "False", "\n", "\n", "", "dev_score_history", "=", "[", "]", "\n", "dev_loss_history", "=", "[", "]", "\n", "train_loss_history", "=", "[", "]", "\n", "\n", "micro_batch_size", "=", "mini_batch_chunk_size", "\n", "\n", "# At any point you can hit Ctrl + C to break out of training early.", "\n", "try", ":", "\n", "            ", "previous_learning_rate", "=", "learning_rate", "\n", "\n", "for", "self", ".", "epoch", "in", "range", "(", "self", ".", "epoch", "+", "1", ",", "max_epochs", "+", "1", ")", ":", "\n", "                ", "log_line", "(", "log", ")", "\n", "\n", "last_epoch_model_state_dict", "=", "self", ".", "model", ".", "module", ".", "state_dict", "(", ")", "\n", "best_state_dict", "=", "self", ".", "model", ".", "module", ".", "state_dict", "(", ")", "\n", "\n", "if", "eval_on_train_shuffle", ":", "\n", "                    ", "train_part_indices", "=", "list", "(", "range", "(", "self", ".", "corpus", ".", "train", ")", ")", "\n", "random", ".", "shuffle", "(", "train_part_indices", ")", "\n", "train_part_indices", "=", "train_part_indices", "[", ":", "train_part_size", "]", "\n", "train_part", "=", "torch", ".", "utils", ".", "data", ".", "dataset", ".", "Subset", "(", "\n", "self", ".", "corpus", ".", "train", ",", "train_part_indices", "\n", ")", "\n", "\n", "# get new learning rate", "\n", "", "for", "group", "in", "optimizer", ".", "param_groups", ":", "\n", "                    ", "learning_rate", "=", "group", "[", "\"lr\"", "]", "\n", "\n", "", "if", "learning_rate", "!=", "previous_learning_rate", "and", "batch_growth_annealing", ":", "\n", "                    ", "mini_batch_size", "*=", "2", "\n", "\n", "# reload last best rpbert if annealing with restarts is enabled", "\n", "", "if", "(", "\n", "learning_rate", "!=", "previous_learning_rate", "\n", "and", "(", "base_path", "/", "\"best-rpbert.pt\"", ")", ".", "exists", "(", ")", "\n", ")", ":", "\n", "                    ", "log", ".", "info", "(", "\"resetting to best rpbert\"", ")", "\n", "if", "anneal_with_restarts", ":", "\n", "                        ", "self", ".", "model", ".", "module", ".", "load_state_dict", "(", "\n", "self", ".", "model", ".", "module", ".", "load", "(", "base_path", "/", "\"best-rpbert.pt\"", ")", ".", "state_dict", "(", ")", "\n", ")", "\n", "", "if", "anneal_with_prestarts", ":", "\n", "                        ", "self", ".", "model", ".", "module", ".", "load_state_dict", "(", "\n", "self", ".", "model", ".", "module", ".", "load", "(", "base_path", "/", "\"pre-best-rpbert.pt\"", ")", ".", "state_dict", "(", ")", "\n", ")", "\n", "\n", "\n", "", "", "previous_learning_rate", "=", "learning_rate", "\n", "\n", "# stop training if learning rate becomes too small", "\n", "if", "learning_rate", "<", "min_learning_rate", ":", "\n", "                    ", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "\"learning rate too small - quitting training!\"", ")", "\n", "log_line", "(", "log", ")", "\n", "break", "\n", "\n", "", "batch_loader", "=", "DataLoader", "(", "\n", "train_data", ",", "\n", "batch_size", "=", "mini_batch_size", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "sampler", "=", "sampler", ",", "\n", ")", "\n", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "\n", "train_loss", ":", "float", "=", "0", "\n", "\n", "seen_batches", "=", "0", "\n", "total_number_of_batches", "=", "len", "(", "batch_loader", ")", "\n", "\n", "modulo", "=", "max", "(", "1", ",", "int", "(", "total_number_of_batches", "/", "10", ")", ")", "\n", "\n", "# process mini-batches", "\n", "batch_time", "=", "0", "\n", "for", "batch_no", ",", "batch", "in", "enumerate", "(", "batch_loader", ")", ":", "\n", "                    ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "# zero the gradients on the rpbert and optimizer", "\n", "self", ".", "model", ".", "zero_grad", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# if necessary, make batch_steps", "\n", "batch_steps", "=", "[", "batch", "]", "\n", "if", "len", "(", "batch", ")", ">", "micro_batch_size", ":", "\n", "                        ", "batch_steps", "=", "[", "\n", "batch", "[", "x", ":", "x", "+", "micro_batch_size", "]", "\n", "for", "x", "in", "range", "(", "0", ",", "len", "(", "batch", ")", ",", "micro_batch_size", ")", "\n", "]", "\n", "\n", "# forward and backward for batch", "\n", "", "for", "batch_step", "in", "batch_steps", ":", "\n", "\n", "# forward pass", "\n", "                        ", "loss", "=", "self", ".", "model", ".", "module", ".", "forward_loss", "(", "batch_step", ")", "\n", "\n", "# Backward", "\n", "if", "use_amp", ":", "\n", "                            ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                                ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                            ", "loss", ".", "backward", "(", ")", "\n", "\n", "# do the optimizer step", "\n", "", "", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "model", ".", "module", ".", "parameters", "(", ")", ",", "5.0", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "seen_batches", "+=", "1", "\n", "train_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "# depending on memory mode, embeddings are moved to CPU, GPU or deleted", "\n", "store_embeddings", "(", "batch", ",", "embeddings_storage_mode", ")", "\n", "\n", "batch_time", "+=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "if", "seen_batches", "%", "modulo", "==", "0", ":", "\n", "                        ", "log", ".", "info", "(", "\n", "f\"epoch {self.epoch} - iter {seen_batches}/{total_number_of_batches} - loss \"", "\n", "f\"{train_loss / seen_batches:.8f} - samples/sec: {mini_batch_size * modulo / batch_time:.2f}\"", "\n", ")", "\n", "batch_time", "=", "0", "\n", "iteration", "=", "self", ".", "epoch", "*", "total_number_of_batches", "+", "batch_no", "\n", "if", "not", "param_selection_mode", ":", "\n", "                            ", "weight_extractor", ".", "extract_weights", "(", "\n", "self", ".", "model", ".", "module", ".", "state_dict", "(", ")", ",", "iteration", "\n", ")", "\n", "\n", "", "", "", "train_loss", "/=", "seen_batches", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "\n", "f\"EPOCH {self.epoch} done: loss {train_loss:.4f} - lr {learning_rate:.7f}\"", "\n", ")", "\n", "\n", "if", "self", ".", "use_tensorboard", ":", "\n", "                    ", "writer", ".", "add_scalar", "(", "\"train_loss\"", ",", "train_loss", ",", "self", ".", "epoch", ")", "\n", "\n", "# anneal against train loss if training with dev, otherwise anneal against dev score", "\n", "", "current_score", "=", "train_loss", "\n", "\n", "# evaluate on train / dev / test split depending on training settings", "\n", "result_line", ":", "str", "=", "\"\"", "\n", "\n", "if", "log_train", ":", "\n", "                    ", "train_eval_result", ",", "train_loss", "=", "self", ".", "model", ".", "module", ".", "evaluate", "(", "\n", "DataLoader", "(", "\n", "self", ".", "corpus", ".", "train", ",", "\n", "batch_size", "=", "mini_batch_chunk_size", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", ")", ",", "\n", "embedding_storage_mode", "=", "embeddings_storage_mode", ",", "\n", ")", "\n", "result_line", "+=", "f\"\\t{train_eval_result.log_line}\"", "\n", "\n", "# depending on memory mode, embeddings are moved to CPU, GPU or deleted", "\n", "store_embeddings", "(", "self", ".", "corpus", ".", "train", ",", "embeddings_storage_mode", ")", "\n", "\n", "", "if", "log_train_part", ":", "\n", "                    ", "train_part_eval_result", ",", "train_part_loss", "=", "self", ".", "model", ".", "module", ".", "evaluate", "(", "\n", "DataLoader", "(", "\n", "train_part", ",", "\n", "batch_size", "=", "mini_batch_chunk_size", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", ")", ",", "\n", "embedding_storage_mode", "=", "embeddings_storage_mode", ",", "\n", ")", "\n", "result_line", "+=", "(", "\n", "f\"\\t{train_part_loss}\\t{train_part_eval_result.log_line}\"", "\n", ")", "\n", "log", ".", "info", "(", "\n", "f\"TRAIN_SPLIT : loss {train_part_loss} - score {round(train_part_eval_result.main_score, 4)}\"", "\n", ")", "\n", "\n", "", "if", "log_dev", ":", "\n", "                    ", "dev_eval_result", ",", "dev_loss", "=", "self", ".", "model", ".", "module", ".", "evaluate", "(", "\n", "DataLoader", "(", "\n", "self", ".", "corpus", ".", "dev", ",", "\n", "batch_size", "=", "mini_batch_chunk_size", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", ")", ",", "\n", "embedding_storage_mode", "=", "embeddings_storage_mode", ",", "\n", ")", "\n", "result_line", "+=", "f\"\\t{dev_loss}\\t{dev_eval_result.log_line}\"", "\n", "log", ".", "info", "(", "\n", "f\"DEV : loss {dev_loss} - score {round(dev_eval_result.main_score, 4)}\"", "\n", ")", "\n", "# calculate scores using dev data if available", "\n", "# append dev score to score history", "\n", "dev_score_history", ".", "append", "(", "dev_eval_result", ".", "main_score", ")", "\n", "dev_loss_history", ".", "append", "(", "dev_loss", ")", "\n", "\n", "current_score", "=", "dev_eval_result", ".", "main_score", "\n", "\n", "# depending on memory mode, embeddings are moved to CPU, GPU or deleted", "\n", "store_embeddings", "(", "self", ".", "corpus", ".", "dev", ",", "embeddings_storage_mode", ")", "\n", "\n", "if", "self", ".", "use_tensorboard", ":", "\n", "                        ", "writer", ".", "add_scalar", "(", "\"dev_loss\"", ",", "dev_loss", ",", "self", ".", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "\n", "\"dev_score\"", ",", "dev_eval_result", ".", "main_score", ",", "self", ".", "epoch", "\n", ")", "\n", "\n", "", "", "if", "log_test", ":", "\n", "                    ", "test_eval_result", ",", "test_loss", "=", "self", ".", "model", ".", "module", ".", "evaluate", "(", "\n", "DataLoader", "(", "\n", "self", ".", "corpus", ".", "test", ",", "\n", "batch_size", "=", "mini_batch_chunk_size", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", ")", ",", "\n", "base_path", "/", "\"test.tsv\"", ",", "\n", "embedding_storage_mode", "=", "embeddings_storage_mode", ",", "\n", ")", "\n", "result_line", "+=", "f\"\\t{test_loss}\\t{test_eval_result.log_line}\"", "\n", "log", ".", "info", "(", "\n", "f\"TEST : loss {test_loss} - score {round(test_eval_result.main_score, 4)}\"", "\n", ")", "\n", "\n", "# depending on memory mode, embeddings are moved to CPU, GPU or deleted", "\n", "store_embeddings", "(", "self", ".", "corpus", ".", "test", ",", "embeddings_storage_mode", ")", "\n", "\n", "if", "self", ".", "use_tensorboard", ":", "\n", "                        ", "writer", ".", "add_scalar", "(", "\"test_loss\"", ",", "test_loss", ",", "self", ".", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "\n", "\"test_score\"", ",", "test_eval_result", ".", "main_score", ",", "self", ".", "epoch", "\n", ")", "\n", "\n", "# determine learning rate annealing through scheduler", "\n", "", "", "scheduler", ".", "step", "(", "current_score", ")", "\n", "\n", "train_loss_history", ".", "append", "(", "train_loss", ")", "\n", "\n", "# determine bad epoch number", "\n", "try", ":", "\n", "                    ", "bad_epochs", "=", "scheduler", ".", "num_bad_epochs", "\n", "", "except", ":", "\n", "                    ", "bad_epochs", "=", "0", "\n", "", "for", "group", "in", "optimizer", ".", "param_groups", ":", "\n", "                    ", "new_learning_rate", "=", "group", "[", "\"lr\"", "]", "\n", "", "if", "new_learning_rate", "!=", "previous_learning_rate", ":", "\n", "                    ", "bad_epochs", "=", "patience", "+", "1", "\n", "\n", "# log bad epochs", "\n", "", "log", ".", "info", "(", "f\"BAD EPOCHS (no improvement): {bad_epochs}\"", ")", "\n", "\n", "# output log file", "\n", "with", "open", "(", "loss_txt", ",", "\"a\"", ")", "as", "f", ":", "\n", "\n", "# make headers on first epoch", "\n", "                    ", "if", "self", ".", "epoch", "==", "1", ":", "\n", "                        ", "f", ".", "write", "(", "\n", "f\"EPOCH\\tTIMESTAMP\\tBAD_EPOCHS\\tLEARNING_RATE\\tTRAIN_LOSS\"", "\n", ")", "\n", "\n", "if", "log_train", ":", "\n", "                            ", "f", ".", "write", "(", "\n", "\"\\tTRAIN_\"", "\n", "+", "\"\\tTRAIN_\"", ".", "join", "(", "\n", "train_eval_result", ".", "log_header", ".", "split", "(", "\"\\t\"", ")", "\n", ")", "\n", ")", "\n", "", "if", "log_train_part", ":", "\n", "                            ", "f", ".", "write", "(", "\n", "\"\\tTRAIN_PART_LOSS\\tTRAIN_PART_\"", "\n", "+", "\"\\tTRAIN_PART_\"", ".", "join", "(", "\n", "train_part_eval_result", ".", "log_header", ".", "split", "(", "\"\\t\"", ")", "\n", ")", "\n", ")", "\n", "", "if", "log_dev", ":", "\n", "                            ", "f", ".", "write", "(", "\n", "\"\\tDEV_LOSS\\tDEV_\"", "\n", "+", "\"\\tDEV_\"", ".", "join", "(", "dev_eval_result", ".", "log_header", ".", "split", "(", "\"\\t\"", ")", ")", "\n", ")", "\n", "", "if", "log_test", ":", "\n", "                            ", "f", ".", "write", "(", "\n", "\"\\tTEST_LOSS\\tTEST_\"", "\n", "+", "\"\\tTEST_\"", ".", "join", "(", "\n", "test_eval_result", ".", "log_header", ".", "split", "(", "\"\\t\"", ")", "\n", ")", "\n", ")", "\n", "\n", "", "", "f", ".", "write", "(", "\n", "f\"\\n{self.epoch}\\t{datetime.datetime.now():%H:%M:%S}\\t{bad_epochs}\\t{learning_rate:.4f}\\t{train_loss}\"", "\n", ")", "\n", "f", ".", "write", "(", "result_line", ")", "\n", "\n", "# if checkpoint is enabled, save rpbert at each epoch", "\n", "", "if", "checkpoint", "and", "not", "param_selection_mode", ":", "\n", "                    ", "self", ".", "save_checkpoint", "(", "base_path", "/", "\"checkpoint.pt\"", ")", "\n", "\n", "# if we use dev data, remember best rpbert based on dev evaluation score", "\n", "", "if", "(", "\n", "(", "not", "train_with_dev", "or", "anneal_with_restarts", "or", "anneal_with_prestarts", ")", "\n", "and", "not", "param_selection_mode", "\n", "and", "current_score", "==", "scheduler", ".", "best", "\n", ")", ":", "\n", "                    ", "print", "(", "\"saving best rpbert\"", ")", "\n", "self", ".", "model", ".", "module", ".", "save", "(", "base_path", "/", "\"best-rpbert.pt\"", ")", "\n", "current_state_dict", "=", "self", ".", "model", ".", "state_dict", "(", ")", "\n", "self", ".", "model", ".", "module", ".", "load_state_dict", "(", "last_epoch_model_state_dict", ")", "\n", "self", ".", "model", ".", "module", ".", "save", "(", "base_path", "/", "\"pre-best-rpbert.pt\"", ")", "\n", "self", ".", "model", ".", "module", ".", "load_state_dict", "(", "current_state_dict", ")", "\n", "\n", "# if we do not use dev data for rpbert selection, save final rpbert", "\n", "", "", "if", "save_final_model", "and", "not", "param_selection_mode", ":", "\n", "                ", "self", ".", "model", ".", "module", ".", "save", "(", "base_path", "/", "\"final-rpbert.pt\"", ")", "\n", "\n", "", "", "except", "KeyboardInterrupt", ":", "\n", "            ", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "\"Exiting from training early.\"", ")", "\n", "\n", "if", "self", ".", "use_tensorboard", ":", "\n", "                ", "writer", ".", "close", "(", ")", "\n", "\n", "", "if", "not", "param_selection_mode", ":", "\n", "                ", "log", ".", "info", "(", "\"Saving rpbert ...\"", ")", "\n", "self", ".", "model", ".", "module", ".", "save", "(", "base_path", "/", "\"final-rpbert.pt\"", ")", "\n", "log", ".", "info", "(", "\"Done.\"", ")", "\n", "\n", "# test best rpbert if test data is present", "\n", "", "", "if", "self", ".", "corpus", ".", "test", ":", "\n", "            ", "final_score", "=", "self", ".", "final_test", "(", "base_path", ",", "mini_batch_chunk_size", ",", "num_workers", ")", "\n", "", "else", ":", "\n", "            ", "final_score", "=", "0", "\n", "log", ".", "info", "(", "\"Test data not provided setting final score to 0\"", ")", "\n", "\n", "", "log", ".", "removeHandler", "(", "log_handler", ")", "\n", "\n", "if", "self", ".", "use_tensorboard", ":", "\n", "            ", "writer", ".", "close", "(", ")", "\n", "\n", "", "return", "{", "\n", "\"test_score\"", ":", "final_score", ",", "\n", "\"dev_score_history\"", ":", "dev_score_history", ",", "\n", "\"train_loss_history\"", ":", "train_loss_history", ",", "\n", "\"dev_loss_history\"", ":", "dev_loss_history", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.save_checkpoint": [[584, 589], ["torch.save", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save"], ["", "def", "save_checkpoint", "(", "self", ",", "model_file", ":", "Union", "[", "str", ",", "Path", "]", ")", ":", "\n", "        ", "corpus", "=", "self", ".", "corpus", "\n", "self", ".", "corpus", "=", "None", "\n", "torch", ".", "save", "(", "self", ",", "str", "(", "model_file", ")", ",", "pickle_protocol", "=", "4", ")", "\n", "self", ".", "corpus", "=", "corpus", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.load_checkpoint": [[590, 595], ["torch.load"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["", "@", "classmethod", "\n", "def", "load_checkpoint", "(", "cls", ",", "checkpoint", ":", "Union", "[", "Path", ",", "str", "]", ",", "corpus", ":", "Corpus", ")", ":", "\n", "        ", "model", ":", "ModelTrainer", "=", "torch", ".", "load", "(", "checkpoint", ",", "map_location", "=", "flair", ".", "device", ")", "\n", "model", ".", "corpus", "=", "corpus", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.final_test": [[596, 641], ["flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "trainer_p.ModelTrainer.model.eval", "trainer_p.ModelTrainer.model.module.evaluate", "log.info", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "trainer_p.ModelTrainer.model.load", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "type", "flair.training_utils.log_line", "flair.training_utils.log_line", "trainer_p.ModelTrainer.model.module.evaluate", "flair.datasets.DataLoader", "flair.datasets.DataLoader"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate"], ["", "def", "final_test", "(", "\n", "self", ",", "base_path", ":", "Path", ",", "eval_mini_batch_size", ":", "int", ",", "num_workers", ":", "int", "=", "8", "\n", ")", ":", "\n", "\n", "        ", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "\"Testing using best rpbert ...\"", ")", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "if", "(", "base_path", "/", "\"best-rpbert.pt\"", ")", ".", "exists", "(", ")", ":", "\n", "            ", "self", ".", "model", "=", "self", ".", "model", ".", "load", "(", "base_path", "/", "\"best-rpbert.pt\"", ")", "\n", "\n", "", "test_results", ",", "test_loss", "=", "self", ".", "model", ".", "module", ".", "evaluate", "(", "\n", "DataLoader", "(", "\n", "self", ".", "corpus", ".", "test", ",", "\n", "batch_size", "=", "eval_mini_batch_size", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", ")", ",", "\n", "out_path", "=", "base_path", "/", "\"test.tsv\"", ",", "\n", "embedding_storage_mode", "=", "\"none\"", ",", "\n", ")", "\n", "\n", "test_results", ":", "Result", "=", "test_results", "\n", "log", ".", "info", "(", "test_results", ".", "log_line", ")", "\n", "log", ".", "info", "(", "test_results", ".", "detailed_results", ")", "\n", "log_line", "(", "log", ")", "\n", "\n", "# if we are training over multiple datasets, do evaluation for each", "\n", "if", "type", "(", "self", ".", "corpus", ")", "is", "MultiCorpus", ":", "\n", "            ", "for", "subcorpus", "in", "self", ".", "corpus", ".", "corpora", ":", "\n", "                ", "log_line", "(", "log", ")", "\n", "self", ".", "model", ".", "module", ".", "evaluate", "(", "\n", "DataLoader", "(", "\n", "subcorpus", ".", "test", ",", "\n", "batch_size", "=", "eval_mini_batch_size", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", ")", ",", "\n", "out_path", "=", "base_path", "/", "f\"{subcorpus.name}-test.tsv\"", ",", "\n", "embedding_storage_mode", "=", "\"none\"", ",", "\n", ")", "\n", "\n", "# get and return the final test score of best rpbert", "\n", "", "", "final_score", "=", "test_results", ".", "main_score", "\n", "\n", "return", "final_score", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.find_learning_rate": [[642, 734], ["flair.training_utils.init_output_file", "flair.training_utils.init_output_file", "trainer_p.ModelTrainer.optimizer", "flair.optim.ExpAnnealLR", "flair.optim.ExpAnnealLR", "trainer_p.ModelTrainer.model.module.state_dict", "trainer_p.ModelTrainer.model.train", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "flair.training_utils.log_line", "flair.training_utils.log_line", "pathlib.Path", "type", "pathlib.Path", "open", "f.write", "trainer_p.ModelTrainer.model.module.parameters", "flair.datasets.DataLoader", "flair.datasets.DataLoader", "trainer_p.ModelTrainer.model.module.load_state_dict", "trainer_p.ModelTrainer.model.to", "trainer_p.ModelTrainer.model.module.forward_loss", "trainer_p.ModelTrainer.zero_grad", "trainer_p.ModelTrainer.backward", "torch.nn.utils.clip_grad_norm_", "trainer_p.ModelTrainer.step", "flair.optim.ExpAnnealLR.step", "flair.optim.ExpAnnealLR.step", "print", "trainer_p.ModelTrainer.item", "trainer_p.ModelTrainer.model.module.parameters", "flair.optim.ExpAnnealLR.get_lr", "flair.optim.ExpAnnealLR.get_lr", "flair.optim.ExpAnnealLR.get_lr", "flair.optim.ExpAnnealLR.get_lr", "flair.training_utils.log_line", "flair.training_utils.log_line", "log.info", "open", "f.write", "torch.isnan", "str", "datetime.datetime.now"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.init_output_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.init_output_file", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.trainers.trainer_p.ModelTrainer.train", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.forward_loss", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ReduceLRWDOnPlateau.step", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ExpAnnealLR.get_lr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ExpAnnealLR.get_lr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ExpAnnealLR.get_lr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.optim.ExpAnnealLR.get_lr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.log_line"], ["", "def", "find_learning_rate", "(", "\n", "self", ",", "\n", "base_path", ":", "Union", "[", "Path", ",", "str", "]", ",", "\n", "file_name", ":", "str", "=", "\"learning_rate.tsv\"", ",", "\n", "start_learning_rate", ":", "float", "=", "1e-7", ",", "\n", "end_learning_rate", ":", "float", "=", "10", ",", "\n", "iterations", ":", "int", "=", "100", ",", "\n", "mini_batch_size", ":", "int", "=", "32", ",", "\n", "stop_early", ":", "bool", "=", "True", ",", "\n", "smoothing_factor", ":", "float", "=", "0.98", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Path", ":", "\n", "        ", "best_loss", "=", "None", "\n", "moving_avg_loss", "=", "0", "\n", "\n", "# cast string to Path", "\n", "if", "type", "(", "base_path", ")", "is", "str", ":", "\n", "            ", "base_path", "=", "Path", "(", "base_path", ")", "\n", "", "learning_rate_tsv", "=", "init_output_file", "(", "base_path", ",", "file_name", ")", "\n", "\n", "with", "open", "(", "learning_rate_tsv", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"ITERATION\\tTIMESTAMP\\tLEARNING_RATE\\tTRAIN_LOSS\\n\"", ")", "\n", "\n", "", "optimizer", "=", "self", ".", "optimizer", "(", "\n", "self", ".", "model", ".", "module", ".", "parameters", "(", ")", ",", "lr", "=", "start_learning_rate", ",", "**", "kwargs", "\n", ")", "\n", "\n", "train_data", "=", "self", ".", "corpus", ".", "train", "\n", "\n", "scheduler", "=", "ExpAnnealLR", "(", "optimizer", ",", "end_learning_rate", ",", "iterations", ")", "\n", "\n", "model_state", "=", "self", ".", "model", ".", "module", ".", "state_dict", "(", ")", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "\n", "step", "=", "0", "\n", "while", "step", "<", "iterations", ":", "\n", "            ", "batch_loader", "=", "DataLoader", "(", "\n", "train_data", ",", "batch_size", "=", "mini_batch_size", ",", "shuffle", "=", "True", "\n", ")", "\n", "for", "batch", "in", "batch_loader", ":", "\n", "                ", "step", "+=", "1", "\n", "\n", "# forward pass", "\n", "loss", "=", "self", ".", "model", ".", "module", ".", "forward_loss", "(", "batch", ")", "\n", "\n", "# update optimizer and scheduler", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "model", ".", "module", ".", "parameters", "(", ")", ",", "5.0", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", "step", ")", "\n", "\n", "print", "(", "scheduler", ".", "get_lr", "(", ")", ")", "\n", "learning_rate", "=", "scheduler", ".", "get_lr", "(", ")", "[", "0", "]", "\n", "\n", "loss_item", "=", "loss", ".", "item", "(", ")", "\n", "if", "step", "==", "1", ":", "\n", "                    ", "best_loss", "=", "loss_item", "\n", "", "else", ":", "\n", "                    ", "if", "smoothing_factor", ">", "0", ":", "\n", "                        ", "moving_avg_loss", "=", "(", "\n", "smoothing_factor", "*", "moving_avg_loss", "\n", "+", "(", "1", "-", "smoothing_factor", ")", "*", "loss_item", "\n", ")", "\n", "loss_item", "=", "moving_avg_loss", "/", "(", "\n", "1", "-", "smoothing_factor", "**", "(", "step", "+", "1", ")", "\n", ")", "\n", "", "if", "loss_item", "<", "best_loss", ":", "\n", "                        ", "best_loss", "=", "loss", "\n", "\n", "", "", "if", "step", ">", "iterations", ":", "\n", "                    ", "break", "\n", "\n", "", "if", "stop_early", "and", "(", "loss_item", ">", "4", "*", "best_loss", "or", "torch", ".", "isnan", "(", "loss", ")", ")", ":", "\n", "                    ", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "\"loss diverged - stopping early!\"", ")", "\n", "step", "=", "iterations", "\n", "break", "\n", "\n", "", "with", "open", "(", "str", "(", "learning_rate_tsv", ")", ",", "\"a\"", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "\n", "f\"{step}\\t{datetime.datetime.now():%H:%M:%S}\\t{learning_rate}\\t{loss_item}\\n\"", "\n", ")", "\n", "\n", "", "", "self", ".", "model", ".", "module", ".", "load_state_dict", "(", "model_state", ")", "\n", "self", ".", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "", "log_line", "(", "log", ")", "\n", "log", ".", "info", "(", "f\"learning rate finder finished - plot {learning_rate_tsv}\"", ")", "\n", "log_line", "(", "log", ")", "\n", "\n", "return", "Path", "(", "learning_rate_tsv", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.const._const.__setattr__": [[11, 19], ["const._const.__dict__.keys", "const._const.ConstError", "name.isupper", "const._const.ConstCaseError"], "methods", ["None"], ["", "def", "__setattr__", "(", "self", ",", "name", ",", "value", ")", ":", "\n", "#if self.__dict__.haskey(name):", "\n", "        ", "if", "name", "in", "self", ".", "__dict__", ".", "keys", "(", ")", ":", "\n", "            ", "raise", "self", ".", "ConstError", "(", "'Can not change const.{0}'", ".", "format", "(", "name", ")", ")", "\n", "", "if", "not", "name", ".", "isupper", "(", ")", ":", "\n", "            ", "raise", "self", ".", "ConstCaseError", "(", "\n", "'const name {0} is not all uppercase.'", ".", "format", "(", "name", ")", ")", "\n", "", "self", ".", "__dict__", "[", "name", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier.__init__": [[33, 95], ["super().__init__", "torch.Linear", "torch.Linear", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "text_classification_model.TextClassifier.to", "len", "enumerate", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "len", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "text_classification_model.TextClassifier.label_dictionary.get_items", "range", "loss_weights.keys", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_items"], ["def", "__init__", "(", "\n", "self", ",", "\n", "document_embeddings", ":", "flair", ".", "embeddings", ".", "DocumentEmbeddings", ",", "\n", "label_dictionary", ":", "Dictionary", ",", "\n", "label_type", ":", "str", "=", "None", ",", "\n", "multi_label", ":", "bool", "=", "None", ",", "\n", "multi_label_threshold", ":", "float", "=", "0.5", ",", "\n", "beta", ":", "float", "=", "1.0", ",", "\n", "loss_weights", ":", "Dict", "[", "str", ",", "float", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Initializes a TextClassifier\n        :param document_embeddings: embeddings used to embed each data point\n        :param label_dictionary: dictionary of labels you want to predict\n        :param multi_label: auto-detected by default, but you can set this to True to force multi-label prediction\n        or False to force single-label prediction\n        :param multi_label_threshold: If multi-label you can set the threshold to make predictions\n        :param beta: Parameter for F-beta score for evaluation and training annealing\n        :param loss_weights: Dictionary of weights for labels for the loss function\n        (if any label's weight is unspecified it will default to 1.0)\n        \"\"\"", "\n", "\n", "super", "(", "TextClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "document_embeddings", ":", "flair", ".", "embeddings", ".", "DocumentRNNEmbeddings", "=", "document_embeddings", "\n", "self", ".", "label_dictionary", ":", "Dictionary", "=", "label_dictionary", "\n", "self", ".", "label_type", "=", "label_type", "\n", "\n", "if", "multi_label", "is", "not", "None", ":", "\n", "            ", "self", ".", "multi_label", "=", "multi_label", "\n", "", "else", ":", "\n", "            ", "self", ".", "multi_label", "=", "self", ".", "label_dictionary", ".", "multi_label", "\n", "\n", "", "self", ".", "multi_label_threshold", "=", "multi_label_threshold", "\n", "\n", "self", ".", "beta", "=", "beta", "\n", "\n", "self", ".", "weight_dict", "=", "loss_weights", "\n", "# Initialize the weight tensor", "\n", "if", "loss_weights", "is", "not", "None", ":", "\n", "            ", "n_classes", "=", "len", "(", "self", ".", "label_dictionary", ")", "\n", "weight_list", "=", "[", "1.", "for", "i", "in", "range", "(", "n_classes", ")", "]", "\n", "for", "i", ",", "tag", "in", "enumerate", "(", "self", ".", "label_dictionary", ".", "get_items", "(", ")", ")", ":", "\n", "                ", "if", "tag", "in", "loss_weights", ".", "keys", "(", ")", ":", "\n", "                    ", "weight_list", "[", "i", "]", "=", "loss_weights", "[", "tag", "]", "\n", "", "", "self", ".", "loss_weights", "=", "torch", ".", "FloatTensor", "(", "weight_list", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss_weights", "=", "None", "\n", "\n", "", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "\n", "self", ".", "document_embeddings", ".", "embedding_length", ",", "len", "(", "self", ".", "label_dictionary", ")", "\n", ")", "\n", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "decoder", ".", "weight", ")", "\n", "\n", "if", "self", ".", "multi_label", ":", "\n", "            ", "self", ".", "loss_function", "=", "nn", ".", "BCEWithLogitsLoss", "(", "weight", "=", "self", ".", "loss_weights", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss_function", "=", "nn", ".", "CrossEntropyLoss", "(", "weight", "=", "self", ".", "loss_weights", ")", "\n", "\n", "# auto-spawn on GPU if available", "\n", "", "self", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier.forward": [[96, 108], ["text_classification_model.TextClassifier.document_embeddings.embed", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "text_classification_model.TextClassifier.decoder", "sentence.embedding.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "forward", "(", "self", ",", "sentences", ")", ":", "\n", "\n", "        ", "self", ".", "document_embeddings", ".", "embed", "(", "sentences", ")", "\n", "\n", "text_embedding_list", "=", "[", "\n", "sentence", ".", "embedding", ".", "unsqueeze", "(", "0", ")", "for", "sentence", "in", "sentences", "\n", "]", "\n", "text_embedding_tensor", "=", "torch", ".", "cat", "(", "text_embedding_list", ",", "0", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "label_scores", "=", "self", ".", "decoder", "(", "text_embedding_tensor", ")", "\n", "\n", "return", "label_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._get_state_dict": [[109, 119], ["text_classification_model.TextClassifier.state_dict"], "methods", ["None"], ["", "def", "_get_state_dict", "(", "self", ")", ":", "\n", "        ", "model_state", "=", "{", "\n", "\"state_dict\"", ":", "self", ".", "state_dict", "(", ")", ",", "\n", "\"document_embeddings\"", ":", "self", ".", "document_embeddings", ",", "\n", "\"label_dictionary\"", ":", "self", ".", "label_dictionary", ",", "\n", "\"multi_label\"", ":", "self", ".", "multi_label", ",", "\n", "\"beta\"", ":", "self", ".", "beta", ",", "\n", "\"weight_dict\"", ":", "self", ".", "weight_dict", ",", "\n", "}", "\n", "return", "model_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._init_model_with_state_dict": [[120, 137], ["text_classification_model.TextClassifier", "TextClassifier.load_state_dict", "state.keys", "state.keys", "state.keys"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_init_model_with_state_dict", "(", "state", ")", ":", "\n", "        ", "beta", "=", "1.0", "if", "\"beta\"", "not", "in", "state", ".", "keys", "(", ")", "else", "state", "[", "\"beta\"", "]", "\n", "weights", "=", "None", "if", "\"weight_dict\"", "not", "in", "state", ".", "keys", "(", ")", "else", "state", "[", "\"weight_dict\"", "]", "\n", "label_type", "=", "None", "if", "\"label_type\"", "not", "in", "state", ".", "keys", "(", ")", "else", "state", "[", "\"label_type\"", "]", "\n", "\n", "model", "=", "TextClassifier", "(", "\n", "document_embeddings", "=", "state", "[", "\"document_embeddings\"", "]", ",", "\n", "label_dictionary", "=", "state", "[", "\"label_dictionary\"", "]", ",", "\n", "label_type", "=", "label_type", ",", "\n", "multi_label", "=", "state", "[", "\"multi_label\"", "]", ",", "\n", "beta", "=", "beta", ",", "\n", "loss_weights", "=", "weights", ",", "\n", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "state", "[", "\"state_dict\"", "]", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier.forward_loss": [[138, 145], ["text_classification_model.TextClassifier.forward", "text_classification_model.TextClassifier._calculate_loss"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._calculate_loss"], ["", "def", "forward_loss", "(", "\n", "self", ",", "data_points", ":", "Union", "[", "List", "[", "Sentence", "]", ",", "Sentence", "]", "\n", ")", "->", "torch", ".", "tensor", ":", "\n", "\n", "        ", "scores", "=", "self", ".", "forward", "(", "data_points", ")", "\n", "\n", "return", "self", ".", "_calculate_loss", "(", "scores", ",", "data_points", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._calculate_loss": [[146, 152], ["text_classification_model.TextClassifier.loss_function", "text_classification_model.TextClassifier._labels_to_one_hot", "text_classification_model.TextClassifier._labels_to_indices"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._labels_to_one_hot", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_regression_model.TextRegressor._labels_to_indices"], ["", "def", "_calculate_loss", "(", "self", ",", "scores", ",", "data_points", ")", ":", "\n", "\n", "        ", "labels", "=", "self", ".", "_labels_to_one_hot", "(", "data_points", ")", "if", "self", ".", "multi_label", "else", "self", ".", "_labels_to_indices", "(", "data_points", ")", "\n", "\n", "return", "self", ".", "loss_function", "(", "scores", ",", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier.predict": [[153, 248], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "isinstance", "sorted", "sorted", "isinstance", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "enumerate", "isinstance", "isinstance", "log.warning", "len", "range", "range", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "flair.datasets.SentenceDataset", "flair.datasets.SentenceDataset", "flair.datasets.StringDataset", "flair.datasets.StringDataset", "tqdm.tqdm.tqdm", "text_classification_model.TextClassifier.forward", "text_classification_model.TextClassifier._obtain_labels", "zip", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "len", "len", "len", "len", "tqdm.tqdm.tqdm.set_description", "len", "sentence.add_label", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._obtain_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.add_label"], ["", "def", "predict", "(", "\n", "self", ",", "\n", "sentences", ":", "Union", "[", "List", "[", "Sentence", "]", ",", "Sentence", ",", "List", "[", "str", "]", ",", "str", "]", ",", "\n", "mini_batch_size", ":", "int", "=", "32", ",", "\n", "embedding_storage_mode", "=", "\"none\"", ",", "\n", "multi_class_prob", ":", "bool", "=", "False", ",", "\n", "verbose", ":", "bool", "=", "False", ",", "\n", "use_tokenizer", ":", "Union", "[", "bool", ",", "Callable", "[", "[", "str", "]", ",", "List", "[", "Token", "]", "]", "]", "=", "space_tokenizer", ",", "\n", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "\"\"\"\n        Predicts the class labels for the given sentences. The labels are directly added to the sentences.\n        :param sentences: list of sentences\n        :param mini_batch_size: mini batch size to use\n        :param embedding_storage_mode: 'none' for the minimum memory footprint, 'cpu' to store embeddings in Ram,\n        'gpu' to store embeddings in GPU memory.\n        :param multi_class_prob : return probability for all class for multiclass\n        :param verbose: set to True to display a progress bar\n        :param use_tokenizer: a custom tokenizer when string are provided (default is space based tokenizer).\n        :return: the list of sentences containing the labels\n        \"\"\"", "\n", "predicted_label_type", "=", "self", ".", "label_type", "if", "self", ".", "label_type", "is", "not", "None", "else", "'class'", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "not", "sentences", ":", "\n", "                ", "return", "sentences", "\n", "\n", "", "if", "isinstance", "(", "sentences", ",", "DataPoint", ")", "or", "isinstance", "(", "sentences", ",", "str", ")", ":", "\n", "                ", "sentences", "=", "[", "sentences", "]", "\n", "\n", "", "if", "(", "flair", ".", "device", ".", "type", "==", "\"cuda\"", ")", "and", "embedding_storage_mode", "==", "\"cpu\"", ":", "\n", "                ", "log", ".", "warning", "(", "\n", "\"You are inferring on GPU with parameter 'embedding_storage_mode' set to 'cpu'.\"", "\n", "\"This option will slow down your inference, usually 'none' (default value) \"", "\n", "\"is a better choice.\"", "\n", ")", "\n", "\n", "# filter empty sentences", "\n", "", "if", "isinstance", "(", "sentences", "[", "0", "]", ",", "Sentence", ")", ":", "\n", "                ", "sentences", "=", "[", "sentence", "for", "sentence", "in", "sentences", "if", "len", "(", "sentence", ")", ">", "0", "]", "\n", "", "if", "len", "(", "sentences", ")", "==", "0", ":", "return", "sentences", "\n", "\n", "# reverse sort all sequences by their length", "\n", "rev_order_len_index", "=", "sorted", "(", "\n", "range", "(", "len", "(", "sentences", ")", ")", ",", "key", "=", "lambda", "k", ":", "len", "(", "sentences", "[", "k", "]", ")", ",", "reverse", "=", "True", "\n", ")", "\n", "original_order_index", "=", "sorted", "(", "\n", "range", "(", "len", "(", "rev_order_len_index", ")", ")", ",", "key", "=", "lambda", "k", ":", "rev_order_len_index", "[", "k", "]", "\n", ")", "\n", "\n", "reordered_sentences", ":", "List", "[", "Union", "[", "DataPoint", ",", "str", "]", "]", "=", "[", "\n", "sentences", "[", "index", "]", "for", "index", "in", "rev_order_len_index", "\n", "]", "\n", "\n", "if", "isinstance", "(", "sentences", "[", "0", "]", ",", "DataPoint", ")", ":", "\n", "# remove previous embeddings", "\n", "                ", "store_embeddings", "(", "reordered_sentences", ",", "\"none\"", ")", "\n", "dataset", "=", "SentenceDataset", "(", "reordered_sentences", ")", "\n", "", "else", ":", "\n", "                ", "dataset", "=", "StringDataset", "(", "\n", "reordered_sentences", ",", "use_tokenizer", "=", "use_tokenizer", "\n", ")", "\n", "", "dataloader", "=", "DataLoader", "(", "\n", "dataset", "=", "dataset", ",", "batch_size", "=", "mini_batch_size", ",", "collate_fn", "=", "lambda", "x", ":", "x", "\n", ")", "\n", "\n", "# progress bar for verbosity", "\n", "if", "verbose", ":", "\n", "                ", "dataloader", "=", "tqdm", "(", "dataloader", ")", "\n", "\n", "", "results", ":", "List", "[", "Sentence", "]", "=", "[", "]", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "                ", "if", "verbose", ":", "\n", "                    ", "dataloader", ".", "set_description", "(", "f\"Inferencing on batch {i}\"", ")", "\n", "", "results", "+=", "batch", "\n", "# stop if all sentences are empty", "\n", "if", "not", "batch", ":", "\n", "                    ", "continue", "\n", "\n", "", "scores", "=", "self", ".", "forward", "(", "batch", ")", "\n", "predicted_labels", "=", "self", ".", "_obtain_labels", "(", "\n", "scores", ",", "predict_prob", "=", "multi_class_prob", "\n", ")", "\n", "\n", "for", "(", "sentence", ",", "labels", ")", "in", "zip", "(", "batch", ",", "predicted_labels", ")", ":", "\n", "                    ", "for", "label", "in", "labels", ":", "\n", "                        ", "sentence", ".", "add_label", "(", "predicted_label_type", ",", "label", ".", "value", ",", "label", ".", "score", ")", "\n", "\n", "# clearing token embeddings to save memory", "\n", "", "", "store_embeddings", "(", "batch", ",", "storage_mode", "=", "embedding_storage_mode", ")", "\n", "\n", "", "results", ":", "List", "[", "Union", "[", "Sentence", ",", "str", "]", "]", "=", "[", "\n", "results", "[", "index", "]", "for", "index", "in", "original_order_index", "\n", "]", "\n", "assert", "len", "(", "sentences", ")", "==", "len", "(", "results", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier.evaluate": [[249, 346], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "flair.training_utils.Metric", "flair.training_utils.Metric", "flair.training_utils.Metric.get_classes", "flair.training_utils.Metric.get_classes", "flair.training_utils.Result", "flair.training_utils.Result", "text_classification_model.TextClassifier.forward", "text_classification_model.TextClassifier._obtain_labels", "text_classification_model.TextClassifier._calculate_loss", "text_classification_model.TextClassifier.label_dictionary.get_items", "zip", "zip", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "sent.to_plain_string", "sentence.get_labels", "lines.append", "flair.training_utils.Metric.micro_avg_accuracy", "flair.training_utils.Metric.micro_avg_accuracy", "flair.training_utils.Metric.micro_avg_f_score", "flair.training_utils.Metric.micro_avg_f_score", "flair.training_utils.Metric.macro_avg_accuracy", "flair.training_utils.Metric.macro_avg_accuracy", "flair.training_utils.Metric.macro_avg_f_score", "flair.training_utils.Metric.macro_avg_f_score", "flair.training_utils.Metric.micro_avg_accuracy", "flair.training_utils.Metric.micro_avg_accuracy", "open", "outfile.write", "flair.training_utils.Metric.get_tp", "flair.training_utils.Metric.get_tp", "flair.training_utils.Metric.get_fp", "flair.training_utils.Metric.get_fp", "flair.training_utils.Metric.get_fn", "flair.training_utils.Metric.get_fn", "flair.training_utils.Metric.get_tn", "flair.training_utils.Metric.get_tn", "flair.training_utils.Metric.precision", "flair.training_utils.Metric.precision", "flair.training_utils.Metric.recall", "flair.training_utils.Metric.recall", "flair.training_utils.Metric.accuracy", "flair.training_utils.Metric.accuracy", "flair.training_utils.Metric.f_score", "flair.training_utils.Metric.f_score", "flair.training_utils.Metric.add_tp", "flair.training_utils.Metric.add_tp", "flair.training_utils.Metric.precision", "flair.training_utils.Metric.precision", "flair.training_utils.Metric.recall", "flair.training_utils.Metric.recall", "flair.training_utils.Metric.micro_avg_f_score", "flair.training_utils.Metric.micro_avg_f_score", "flair.training_utils.Metric.add_fp", "flair.training_utils.Metric.add_fp", "flair.training_utils.Metric.add_fn", "flair.training_utils.Metric.add_fn", "flair.training_utils.Metric.add_tn", "flair.training_utils.Metric.add_tn"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_classes", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_classes", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._obtain_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._calculate_loss", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_items", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_plain_string", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.get_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.micro_avg_accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.micro_avg_accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.micro_avg_f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.micro_avg_f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.macro_avg_accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.macro_avg_accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.macro_avg_f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.macro_avg_f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.micro_avg_accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.micro_avg_accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.precision", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.precision", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.recall", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.recall", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.precision", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.precision", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.recall", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.recall", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.micro_avg_f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.micro_avg_f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_fp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_fp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_fn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_fn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_tn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_tn"], ["", "", "def", "evaluate", "(", "\n", "self", ",", "\n", "data_loader", ":", "DataLoader", ",", "\n", "out_path", ":", "Path", "=", "None", ",", "\n", "embedding_storage_mode", ":", "str", "=", "\"none\"", ",", "\n", ")", "->", "(", "Result", ",", "float", ")", ":", "\n", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "eval_loss", "=", "0", "\n", "\n", "metric", "=", "Metric", "(", "\"Evaluation\"", ",", "beta", "=", "self", ".", "beta", ")", "\n", "\n", "lines", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "batch_count", ":", "int", "=", "0", "\n", "for", "batch", "in", "data_loader", ":", "\n", "\n", "                ", "batch_count", "+=", "1", "\n", "\n", "scores", "=", "self", ".", "forward", "(", "batch", ")", "\n", "predictions", "=", "self", ".", "_obtain_labels", "(", "scores", ")", "\n", "loss", "=", "self", ".", "_calculate_loss", "(", "scores", ",", "batch", ")", "\n", "\n", "eval_loss", "+=", "loss", "\n", "\n", "sentences_for_batch", "=", "[", "sent", ".", "to_plain_string", "(", ")", "for", "sent", "in", "batch", "]", "\n", "\n", "true_values_for_batch", "=", "[", "sentence", ".", "get_labels", "(", "self", ".", "label_type", ")", "for", "sentence", "in", "batch", "]", "\n", "available_labels", "=", "self", ".", "label_dictionary", ".", "get_items", "(", ")", "\n", "\n", "for", "sentence", ",", "prediction", ",", "true_value", "in", "zip", "(", "\n", "sentences_for_batch", ",", "\n", "predictions", ",", "\n", "true_values_for_batch", ",", "\n", ")", ":", "\n", "                    ", "eval_line", "=", "\"{}\\t{}\\t{}\\n\"", ".", "format", "(", "\n", "sentence", ",", "true_value", ",", "prediction", "\n", ")", "\n", "lines", ".", "append", "(", "eval_line", ")", "\n", "\n", "", "for", "predictions_for_sentence", ",", "true_values_for_sentence", "in", "zip", "(", "\n", "predictions", ",", "true_values_for_batch", "\n", ")", ":", "\n", "\n", "                    ", "true_values_for_sentence", "=", "[", "label", ".", "value", "for", "label", "in", "true_values_for_sentence", "]", "\n", "predictions_for_sentence", "=", "[", "label", ".", "value", "for", "label", "in", "predictions_for_sentence", "]", "\n", "\n", "for", "label", "in", "available_labels", ":", "\n", "                        ", "if", "(", "\n", "label", "in", "predictions_for_sentence", "\n", "and", "label", "in", "true_values_for_sentence", "\n", ")", ":", "\n", "                            ", "metric", ".", "add_tp", "(", "label", ")", "\n", "", "elif", "(", "\n", "label", "in", "predictions_for_sentence", "\n", "and", "label", "not", "in", "true_values_for_sentence", "\n", ")", ":", "\n", "                            ", "metric", ".", "add_fp", "(", "label", ")", "\n", "", "elif", "(", "\n", "label", "not", "in", "predictions_for_sentence", "\n", "and", "label", "in", "true_values_for_sentence", "\n", ")", ":", "\n", "                            ", "metric", ".", "add_fn", "(", "label", ")", "\n", "", "elif", "(", "\n", "label", "not", "in", "predictions_for_sentence", "\n", "and", "label", "not", "in", "true_values_for_sentence", "\n", ")", ":", "\n", "                            ", "metric", ".", "add_tn", "(", "label", ")", "\n", "\n", "", "", "", "store_embeddings", "(", "batch", ",", "embedding_storage_mode", ")", "\n", "\n", "", "eval_loss", "/=", "batch_count", "\n", "\n", "detailed_result", "=", "(", "\n", "f\"\\nMICRO_AVG: acc {metric.micro_avg_accuracy()} - f1-score {metric.micro_avg_f_score()}\"", "\n", "f\"\\nMACRO_AVG: acc {metric.macro_avg_accuracy()} - f1-score {metric.macro_avg_f_score()}\"", "\n", ")", "\n", "for", "class_name", "in", "metric", ".", "get_classes", "(", ")", ":", "\n", "                ", "detailed_result", "+=", "(", "\n", "f\"\\n{class_name:<10} tp: {metric.get_tp(class_name)} - fp: {metric.get_fp(class_name)} - \"", "\n", "f\"fn: {metric.get_fn(class_name)} - tn: {metric.get_tn(class_name)} - precision: \"", "\n", "f\"{metric.precision(class_name):.4f} - recall: {metric.recall(class_name):.4f} - \"", "\n", "f\"accuracy: {metric.accuracy(class_name):.4f} - f1-score: \"", "\n", "f\"{metric.f_score(class_name):.4f}\"", "\n", ")", "\n", "\n", "", "result", "=", "Result", "(", "\n", "main_score", "=", "metric", ".", "micro_avg_accuracy", "(", ")", ",", "\n", "log_line", "=", "f\"{metric.precision()}\\t{metric.recall()}\\t{metric.micro_avg_f_score()}\"", ",", "\n", "log_header", "=", "\"PRECISION\\tRECALL\\tF1\"", ",", "\n", "detailed_results", "=", "detailed_result", ",", "\n", ")", "\n", "\n", "if", "out_path", "is", "not", "None", ":", "\n", "                ", "with", "open", "(", "out_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "outfile", ":", "\n", "                    ", "outfile", ".", "write", "(", "\"\"", ".", "join", "(", "lines", ")", ")", "\n", "\n", "", "", "return", "result", ",", "eval_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._filter_empty_sentences": [[347, 357], ["len", "len", "log.warning", "len", "len"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "_filter_empty_sentences", "(", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "filtered_sentences", "=", "[", "sentence", "for", "sentence", "in", "sentences", "if", "sentence", ".", "tokens", "]", "\n", "if", "len", "(", "sentences", ")", "!=", "len", "(", "filtered_sentences", ")", ":", "\n", "            ", "log", ".", "warning", "(", "\n", "\"Ignore {} sentence(s) with no tokens.\"", ".", "format", "(", "\n", "len", "(", "sentences", ")", "-", "len", "(", "filtered_sentences", ")", "\n", ")", "\n", ")", "\n", "", "return", "filtered_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._obtain_labels": [[358, 374], ["text_classification_model.TextClassifier._get_single_label", "text_classification_model.TextClassifier._get_multi_label", "text_classification_model.TextClassifier._predict_label_prob"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._get_single_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._get_multi_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._predict_label_prob"], ["", "def", "_obtain_labels", "(", "\n", "self", ",", "scores", ":", "List", "[", "List", "[", "float", "]", "]", ",", "predict_prob", ":", "bool", "=", "False", "\n", ")", "->", "List", "[", "List", "[", "Label", "]", "]", ":", "\n", "        ", "\"\"\"\n        Predicts the labels of sentences.\n        :param scores: the prediction scores from the rpbert\n        :return: list of predicted labels\n        \"\"\"", "\n", "\n", "if", "self", ".", "multi_label", ":", "\n", "            ", "return", "[", "self", ".", "_get_multi_label", "(", "s", ")", "for", "s", "in", "scores", "]", "\n", "\n", "", "elif", "predict_prob", ":", "\n", "            ", "return", "[", "self", ".", "_predict_label_prob", "(", "s", ")", "for", "s", "in", "scores", "]", "\n", "\n", "", "return", "[", "self", ".", "_get_single_label", "(", "s", ")", "for", "s", "in", "scores", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._get_multi_label": [[375, 387], ["torch.nn.Sigmoid", "torch.nn.Sigmoid", "torch.nn.Sigmoid", "torch.nn.Sigmoid", "list", "enumerate", "map", "text_classification_model.TextClassifier.label_dictionary.get_item_for_index", "labels.append", "torch.nn.Sigmoid.", "torch.nn.Sigmoid.", "flair.data.Label", "flair.data.Label", "conf.item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_item_for_index"], ["", "def", "_get_multi_label", "(", "self", ",", "label_scores", ")", "->", "List", "[", "Label", "]", ":", "\n", "        ", "labels", "=", "[", "]", "\n", "\n", "sigmoid", "=", "torch", ".", "nn", ".", "Sigmoid", "(", ")", "\n", "\n", "results", "=", "list", "(", "map", "(", "lambda", "x", ":", "sigmoid", "(", "x", ")", ",", "label_scores", ")", ")", "\n", "for", "idx", ",", "conf", "in", "enumerate", "(", "results", ")", ":", "\n", "            ", "if", "conf", ">", "self", ".", "multi_label_threshold", ":", "\n", "                ", "label", "=", "self", ".", "label_dictionary", ".", "get_item_for_index", "(", "idx", ")", "\n", "labels", ".", "append", "(", "Label", "(", "label", ",", "conf", ".", "item", "(", ")", ")", ")", "\n", "\n", "", "", "return", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._get_single_label": [[388, 394], ["torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.max", "torch.max", "torch.max", "torch.max", "text_classification_model.TextClassifier.label_dictionary.get_item_for_index", "idx.item", "flair.data.Label", "flair.data.Label", "conf.item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_item_for_index"], ["", "def", "_get_single_label", "(", "self", ",", "label_scores", ")", "->", "List", "[", "Label", "]", ":", "\n", "        ", "softmax", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "label_scores", ",", "dim", "=", "0", ")", "\n", "conf", ",", "idx", "=", "torch", ".", "max", "(", "softmax", ",", "0", ")", "\n", "label", "=", "self", ".", "label_dictionary", ".", "get_item_for_index", "(", "idx", ".", "item", "(", ")", ")", "\n", "\n", "return", "[", "Label", "(", "label", ",", "conf", ".", "item", "(", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._predict_label_prob": [[395, 402], ["torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "enumerate", "text_classification_model.TextClassifier.label_dictionary.get_item_for_index", "label_probs.append", "flair.data.Label", "flair.data.Label", "conf.item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_item_for_index"], ["", "def", "_predict_label_prob", "(", "self", ",", "label_scores", ")", "->", "List", "[", "Label", "]", ":", "\n", "        ", "softmax", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "label_scores", ",", "dim", "=", "0", ")", "\n", "label_probs", "=", "[", "]", "\n", "for", "idx", ",", "conf", "in", "enumerate", "(", "softmax", ")", ":", "\n", "            ", "label", "=", "self", ".", "label_dictionary", ".", "get_item_for_index", "(", "idx", ")", "\n", "label_probs", ".", "append", "(", "Label", "(", "label", ",", "conf", ".", "item", "(", ")", ")", ")", "\n", "", "return", "label_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._labels_to_one_hot": [[403, 413], ["flair.training_utils.convert_labels_to_one_hot", "flair.training_utils.convert_labels_to_one_hot", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "label_list.append", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "sentence.get_labels"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.convert_labels_to_one_hot", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.convert_labels_to_one_hot", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.get_labels"], ["", "def", "_labels_to_one_hot", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", ":", "\n", "\n", "        ", "label_list", "=", "[", "]", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "label_list", ".", "append", "(", "[", "label", ".", "value", "for", "label", "in", "sentence", ".", "get_labels", "(", "self", ".", "label_type", ")", "]", ")", "\n", "\n", "", "one_hot", "=", "convert_labels_to_one_hot", "(", "label_list", ",", "self", ".", "label_dictionary", ")", "\n", "one_hot", "=", "[", "torch", ".", "FloatTensor", "(", "l", ")", ".", "unsqueeze", "(", "0", ")", "for", "l", "in", "one_hot", "]", "\n", "one_hot", "=", "torch", ".", "cat", "(", "one_hot", ",", "0", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "return", "one_hot", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._labels_to_indices": [[414, 429], ["torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "text_classification_model.TextClassifier.label_dictionary.get_idx_for_item", "sentence.get_labels"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.DataPoint.get_labels"], ["", "def", "_labels_to_indices", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", ":", "\n", "\n", "        ", "indices", "=", "[", "\n", "torch", ".", "LongTensor", "(", "\n", "[", "\n", "self", ".", "label_dictionary", ".", "get_idx_for_item", "(", "label", ".", "value", ")", "\n", "for", "label", "in", "sentence", ".", "get_labels", "(", "self", ".", "label_type", ")", "\n", "]", "\n", ")", "\n", "for", "sentence", "in", "sentences", "\n", "]", "\n", "\n", "vec", "=", "torch", ".", "cat", "(", "indices", ",", "0", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "return", "vec", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier._fetch_model": [[430, 455], ["pathlib.Path", "flair.file_utils.cached_path", "flair.file_utils.cached_path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["", "@", "staticmethod", "\n", "def", "_fetch_model", "(", "model_name", ")", "->", "str", ":", "\n", "\n", "        ", "model_map", "=", "{", "}", "\n", "aws_resource_path", "=", "(", "\n", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4\"", "\n", ")", "\n", "\n", "model_map", "[", "\"de-offensive-language\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path", ",", "\n", "\"classy-offensive-de-rnn-cuda%3A0\"", ",", "\n", "\"germ-eval-2018-task-1-v0.4.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"en-sentiment\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "aws_resource_path", ",", "\"classy-imdb-en-rnn-cuda%3A0\"", ",", "\"imdb-v0.4.pt\"", "]", "\n", ")", "\n", "\n", "cache_dir", "=", "Path", "(", "\"models\"", ")", "\n", "if", "model_name", "in", "model_map", ":", "\n", "            ", "model_name", "=", "cached_path", "(", "model_map", "[", "model_name", "]", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "", "return", "model_name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_classification_model.TextClassifier.__str__": [[456, 461], ["super().__str__().rstrip", "super().__str__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.__str__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "super", "(", "flair", ".", "nn", ".", "Model", ",", "self", ")", ".", "__str__", "(", ")", ".", "rstrip", "(", "')'", ")", "+", "f'  (beta): {self.beta}\\n'", "+", "f'  (weights): {self.weight_dict}\\n'", "+", "f'  (weight_tensor) {self.loss_weights}\\n)'", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.__init__": [[18, 62], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Embedding", "torch.Embedding", "language_model.LanguageModel.init_weights", "language_model.LanguageModel.to", "len", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "language_model.LanguageModel.initialize", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.BaseModel.init_weights", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.initialize"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dictionary", ":", "Dictionary", ",", "\n", "is_forward_lm", ":", "bool", ",", "\n", "hidden_size", ":", "int", ",", "\n", "nlayers", ":", "int", ",", "\n", "embedding_size", ":", "int", "=", "100", ",", "\n", "nout", "=", "None", ",", "\n", "dropout", "=", "0.1", ",", "\n", ")", ":", "\n", "\n", "        ", "super", "(", "LanguageModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "is_forward_lm", ":", "bool", "=", "is_forward_lm", "\n", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "embedding_size", "=", "embedding_size", "\n", "self", ".", "nlayers", "=", "nlayers", "\n", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "encoder", "=", "nn", ".", "Embedding", "(", "len", "(", "dictionary", ")", ",", "embedding_size", ")", "\n", "\n", "if", "nlayers", "==", "1", ":", "\n", "            ", "self", ".", "rnn", "=", "nn", ".", "LSTM", "(", "embedding_size", ",", "hidden_size", ",", "nlayers", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "rnn", "=", "nn", ".", "LSTM", "(", "embedding_size", ",", "hidden_size", ",", "nlayers", ",", "dropout", "=", "dropout", ")", "\n", "\n", "", "self", ".", "hidden", "=", "None", "\n", "\n", "self", ".", "nout", "=", "nout", "\n", "if", "nout", "is", "not", "None", ":", "\n", "            ", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "nout", ")", "\n", "self", ".", "initialize", "(", "self", ".", "proj", ".", "weight", ")", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "nout", ",", "len", "(", "dictionary", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "proj", "=", "None", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "len", "(", "dictionary", ")", ")", "\n", "\n", "", "self", ".", "init_weights", "(", ")", "\n", "\n", "# auto-spawn on GPU if available", "\n", "self", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.init_weights": [[63, 68], ["language_model.LanguageModel.encoder.weight.detach().uniform_", "language_model.LanguageModel.decoder.bias.detach().fill_", "language_model.LanguageModel.decoder.weight.detach().uniform_", "language_model.LanguageModel.encoder.weight.detach", "language_model.LanguageModel.decoder.bias.detach", "language_model.LanguageModel.decoder.weight.detach"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "initrange", "=", "0.1", "\n", "self", ".", "encoder", ".", "weight", ".", "detach", "(", ")", ".", "uniform_", "(", "-", "initrange", ",", "initrange", ")", "\n", "self", ".", "decoder", ".", "bias", ".", "detach", "(", ")", ".", "fill_", "(", "0", ")", "\n", "self", ".", "decoder", ".", "weight", ".", "detach", "(", ")", ".", "uniform_", "(", "-", "initrange", ",", "initrange", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.set_hidden": [[69, 71], ["None"], "methods", ["None"], ["", "def", "set_hidden", "(", "self", ",", "hidden", ")", ":", "\n", "        ", "self", ".", "hidden", "=", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.forward": [[72, 93], ["language_model.LanguageModel.encoder", "language_model.LanguageModel.drop", "language_model.LanguageModel.rnn.flatten_parameters", "language_model.LanguageModel.rnn", "language_model.LanguageModel.drop", "language_model.LanguageModel.decoder", "language_model.LanguageModel.proj", "language_model.LanguageModel.view", "language_model.LanguageModel.view", "language_model.LanguageModel.size", "language_model.LanguageModel.size", "language_model.LanguageModel.size", "language_model.LanguageModel.size", "language_model.LanguageModel.size", "language_model.LanguageModel.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "hidden", ",", "ordered_sequence_lengths", "=", "None", ")", ":", "\n", "        ", "encoded", "=", "self", ".", "encoder", "(", "input", ")", "\n", "emb", "=", "self", ".", "drop", "(", "encoded", ")", "\n", "\n", "self", ".", "rnn", ".", "flatten_parameters", "(", ")", "\n", "\n", "output", ",", "hidden", "=", "self", ".", "rnn", "(", "emb", ",", "hidden", ")", "\n", "\n", "if", "self", ".", "proj", "is", "not", "None", ":", "\n", "            ", "output", "=", "self", ".", "proj", "(", "output", ")", "\n", "\n", "", "output", "=", "self", ".", "drop", "(", "output", ")", "\n", "\n", "decoded", "=", "self", ".", "decoder", "(", "\n", "output", ".", "view", "(", "output", ".", "size", "(", "0", ")", "*", "output", ".", "size", "(", "1", ")", ",", "output", ".", "size", "(", "2", ")", ")", "\n", ")", "\n", "\n", "return", "(", "\n", "decoded", ".", "view", "(", "output", ".", "size", "(", "0", ")", ",", "output", ".", "size", "(", "1", ")", ",", "decoded", ".", "size", "(", "1", ")", ")", ",", "\n", "output", ",", "\n", "hidden", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.init_hidden": [[95, 100], ["next().detach", "next().detach.new().zero_().clone().detach", "next().detach.new().zero_().clone().detach", "next", "language_model.LanguageModel.parameters", "next().detach.new().zero_().clone", "next().detach.new().zero_().clone", "next().detach.new().zero_", "next().detach.new().zero_", "next().detach.new", "next().detach.new"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "bsz", ")", ":", "\n", "        ", "weight", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "detach", "(", ")", "\n", "return", "(", "\n", "weight", ".", "new", "(", "self", ".", "nlayers", ",", "bsz", ",", "self", ".", "hidden_size", ")", ".", "zero_", "(", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", ",", "\n", "weight", ".", "new", "(", "self", ".", "nlayers", ",", "bsz", ",", "self", ".", "hidden_size", ")", ".", "zero_", "(", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.get_representation": [[102, 162], ["len", "range", "chunks.append", "language_model.LanguageModel.init_hidden", "language_model.LanguageModel.dictionary.get_idx_for_item", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "max", "padded_strings.append", "len", "chunks.append", "len", "len", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "batches.append", "batch.transpose.transpose.transpose", "language_model.LanguageModel.forward", "output_parts.append", "len", "max", "language_model.LanguageModel.dictionary.get_idx_for_items", "sequences_as_char_indices.append", "list", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.init_hidden", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_items"], ["", "def", "get_representation", "(", "\n", "self", ",", "\n", "strings", ":", "List", "[", "str", "]", ",", "\n", "start_marker", ":", "str", ",", "\n", "end_marker", ":", "str", ",", "\n", "chars_per_chunk", ":", "int", "=", "512", ",", "\n", ")", ":", "\n", "\n", "        ", "len_longest_str", ":", "int", "=", "len", "(", "max", "(", "strings", ",", "key", "=", "len", ")", ")", "\n", "\n", "# pad strings with whitespaces to longest sentence", "\n", "padded_strings", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "for", "string", "in", "strings", ":", "\n", "            ", "if", "not", "self", ".", "is_forward_lm", ":", "\n", "                ", "string", "=", "string", "[", ":", ":", "-", "1", "]", "\n", "\n", "", "padded", "=", "f\"{start_marker}{string}{end_marker}\"", "\n", "padded_strings", ".", "append", "(", "padded", ")", "\n", "\n", "# cut up the input into chunks of max charlength = chunk_size", "\n", "", "chunks", "=", "[", "]", "\n", "splice_begin", "=", "0", "\n", "longest_padded_str", ":", "int", "=", "len_longest_str", "+", "len", "(", "start_marker", ")", "+", "len", "(", "end_marker", ")", "\n", "for", "splice_end", "in", "range", "(", "chars_per_chunk", ",", "longest_padded_str", ",", "chars_per_chunk", ")", ":", "\n", "            ", "chunks", ".", "append", "(", "[", "text", "[", "splice_begin", ":", "splice_end", "]", "for", "text", "in", "padded_strings", "]", ")", "\n", "splice_begin", "=", "splice_end", "\n", "\n", "", "chunks", ".", "append", "(", "\n", "[", "text", "[", "splice_begin", ":", "longest_padded_str", "]", "for", "text", "in", "padded_strings", "]", "\n", ")", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "len", "(", "chunks", "[", "0", "]", ")", ")", "\n", "\n", "padding_char_index", "=", "self", ".", "dictionary", ".", "get_idx_for_item", "(", "\" \"", ")", "\n", "\n", "batches", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "# push each chunk through the RNN language rpbert", "\n", "for", "chunk", "in", "chunks", ":", "\n", "            ", "len_longest_chunk", ":", "int", "=", "len", "(", "max", "(", "chunk", ",", "key", "=", "len", ")", ")", "\n", "sequences_as_char_indices", ":", "List", "[", "List", "[", "int", "]", "]", "=", "[", "]", "\n", "for", "string", "in", "chunk", ":", "\n", "                ", "char_indices", "=", "self", ".", "dictionary", ".", "get_idx_for_items", "(", "list", "(", "string", ")", ")", "\n", "char_indices", "+=", "[", "padding_char_index", "]", "*", "(", "len_longest_chunk", "-", "len", "(", "string", ")", ")", "\n", "\n", "sequences_as_char_indices", ".", "append", "(", "char_indices", ")", "\n", "", "t", "=", "torch", ".", "tensor", "(", "sequences_as_char_indices", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "\n", "device", "=", "flair", ".", "device", ",", "non_blocking", "=", "True", "\n", ")", "\n", "batches", ".", "append", "(", "t", ")", "\n", "\n", "", "output_parts", "=", "[", "]", "\n", "for", "batch", "in", "batches", ":", "\n", "            ", "batch", "=", "batch", ".", "transpose", "(", "0", ",", "1", ")", "\n", "_", ",", "rnn_output", ",", "hidden", "=", "self", ".", "forward", "(", "batch", ",", "hidden", ")", "\n", "output_parts", ".", "append", "(", "rnn_output", ")", "\n", "\n", "# concatenate all chunks to make final output", "\n", "", "output", "=", "torch", ".", "cat", "(", "output_parts", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.get_output": [[163, 171], ["torch.LongTensor().transpose", "torch.LongTensor().transpose", "torch.LongTensor().transpose", "torch.LongTensor().transpose", "language_model.LanguageModel.init_hidden", "language_model.LanguageModel.forward", "language_model.LanguageModel.repackage_hidden", "language_model.LanguageModel.dictionary.get_idx_for_item", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.init_hidden", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.repackage_hidden", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item"], ["", "def", "get_output", "(", "self", ",", "text", ":", "str", ")", ":", "\n", "        ", "char_indices", "=", "[", "self", ".", "dictionary", ".", "get_idx_for_item", "(", "char", ")", "for", "char", "in", "text", "]", "\n", "input_vector", "=", "torch", ".", "LongTensor", "(", "[", "char_indices", "]", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "1", ")", "\n", "prediction", ",", "rnn_output", ",", "hidden", "=", "self", ".", "forward", "(", "input_vector", ",", "hidden", ")", "\n", "\n", "return", "self", ".", "repackage_hidden", "(", "hidden", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.repackage_hidden": [[172, 178], ["type", "h.clone().detach", "tuple", "h.clone", "language_model.LanguageModel.repackage_hidden"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.repackage_hidden"], ["", "def", "repackage_hidden", "(", "self", ",", "h", ")", ":", "\n", "        ", "\"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"", "\n", "if", "type", "(", "h", ")", "==", "torch", ".", "Tensor", ":", "\n", "            ", "return", "h", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "tuple", "(", "self", ".", "repackage_hidden", "(", "v", ")", "for", "v", "in", "h", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.initialize": [[179, 184], ["matrix.size", "math.sqrt", "matrix.detach().uniform_", "matrix.detach"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "initialize", "(", "matrix", ")", ":", "\n", "        ", "in_", ",", "out_", "=", "matrix", ".", "size", "(", ")", "\n", "stdv", "=", "math", ".", "sqrt", "(", "3.0", "/", "(", "in_", "+", "out_", ")", ")", "\n", "matrix", ".", "detach", "(", ")", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.load_language_model": [[185, 204], ["torch.load", "torch.load", "torch.load", "torch.load", "language_model.LanguageModel", "LanguageModel.load_state_dict", "LanguageModel.eval", "LanguageModel.to", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "@", "classmethod", "\n", "def", "load_language_model", "(", "cls", ",", "model_file", ":", "Union", "[", "Path", ",", "str", "]", ")", ":", "\n", "\n", "        ", "state", "=", "torch", ".", "load", "(", "str", "(", "model_file", ")", ",", "map_location", "=", "flair", ".", "device", ")", "\n", "\n", "model", "=", "LanguageModel", "(", "\n", "state", "[", "\"dictionary\"", "]", ",", "\n", "state", "[", "\"is_forward_lm\"", "]", ",", "\n", "state", "[", "\"hidden_size\"", "]", ",", "\n", "state", "[", "\"nlayers\"", "]", ",", "\n", "state", "[", "\"embedding_size\"", "]", ",", "\n", "state", "[", "\"nout\"", "]", ",", "\n", "state", "[", "\"dropout\"", "]", ",", "\n", ")", "\n", "model", ".", "load_state_dict", "(", "state", "[", "\"state_dict\"", "]", ")", "\n", "model", ".", "eval", "(", ")", "\n", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.load_checkpoint": [[205, 235], ["torch.load", "torch.load", "torch.load", "torch.load", "language_model.LanguageModel", "LanguageModel.load_state_dict", "LanguageModel.eval", "LanguageModel.to", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "@", "classmethod", "\n", "def", "load_checkpoint", "(", "cls", ",", "model_file", ":", "Path", ")", ":", "\n", "        ", "state", "=", "torch", ".", "load", "(", "str", "(", "model_file", ")", ",", "map_location", "=", "flair", ".", "device", ")", "\n", "\n", "epoch", "=", "state", "[", "\"epoch\"", "]", "if", "\"epoch\"", "in", "state", "else", "None", "\n", "split", "=", "state", "[", "\"split\"", "]", "if", "\"split\"", "in", "state", "else", "None", "\n", "loss", "=", "state", "[", "\"loss\"", "]", "if", "\"loss\"", "in", "state", "else", "None", "\n", "optimizer_state_dict", "=", "(", "\n", "state", "[", "\"optimizer_state_dict\"", "]", "if", "\"optimizer_state_dict\"", "in", "state", "else", "None", "\n", ")", "\n", "\n", "model", "=", "LanguageModel", "(", "\n", "state", "[", "\"dictionary\"", "]", ",", "\n", "state", "[", "\"is_forward_lm\"", "]", ",", "\n", "state", "[", "\"hidden_size\"", "]", ",", "\n", "state", "[", "\"nlayers\"", "]", ",", "\n", "state", "[", "\"embedding_size\"", "]", ",", "\n", "state", "[", "\"nout\"", "]", ",", "\n", "state", "[", "\"dropout\"", "]", ",", "\n", ")", "\n", "model", ".", "load_state_dict", "(", "state", "[", "\"state_dict\"", "]", ")", "\n", "model", ".", "eval", "(", ")", "\n", "model", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "return", "{", "\n", "\"rpbert\"", ":", "model", ",", "\n", "\"epoch\"", ":", "epoch", ",", "\n", "\"split\"", ":", "split", ",", "\n", "\"loss\"", ":", "loss", ",", "\n", "\"optimizer_state_dict\"", ":", "optimizer_state_dict", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save_checkpoint": [[237, 256], ["torch.save", "torch.save", "torch.save", "torch.save", "language_model.LanguageModel.state_dict", "optimizer.state_dict", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save"], ["", "def", "save_checkpoint", "(", "\n", "self", ",", "file", ":", "Path", ",", "optimizer", ":", "Optimizer", ",", "epoch", ":", "int", ",", "split", ":", "int", ",", "loss", ":", "float", "\n", ")", ":", "\n", "        ", "model_state", "=", "{", "\n", "\"state_dict\"", ":", "self", ".", "state_dict", "(", ")", ",", "\n", "\"dictionary\"", ":", "self", ".", "dictionary", ",", "\n", "\"is_forward_lm\"", ":", "self", ".", "is_forward_lm", ",", "\n", "\"hidden_size\"", ":", "self", ".", "hidden_size", ",", "\n", "\"nlayers\"", ":", "self", ".", "nlayers", ",", "\n", "\"embedding_size\"", ":", "self", ".", "embedding_size", ",", "\n", "\"nout\"", ":", "self", ".", "nout", ",", "\n", "\"dropout\"", ":", "self", ".", "dropout", ",", "\n", "\"optimizer_state_dict\"", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "\"epoch\"", ":", "epoch", ",", "\n", "\"split\"", ":", "split", ",", "\n", "\"loss\"", ":", "loss", ",", "\n", "}", "\n", "\n", "torch", ".", "save", "(", "model_state", ",", "str", "(", "file", ")", ",", "pickle_protocol", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save": [[257, 270], ["torch.save", "torch.save", "torch.save", "torch.save", "language_model.LanguageModel.state_dict", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.save"], ["", "def", "save", "(", "self", ",", "file", ":", "Path", ")", ":", "\n", "        ", "model_state", "=", "{", "\n", "\"state_dict\"", ":", "self", ".", "state_dict", "(", ")", ",", "\n", "\"dictionary\"", ":", "self", ".", "dictionary", ",", "\n", "\"is_forward_lm\"", ":", "self", ".", "is_forward_lm", ",", "\n", "\"hidden_size\"", ":", "self", ".", "hidden_size", ",", "\n", "\"nlayers\"", ":", "self", ".", "nlayers", ",", "\n", "\"embedding_size\"", ":", "self", ".", "embedding_size", ",", "\n", "\"nout\"", ":", "self", ".", "nout", ",", "\n", "\"dropout\"", ":", "self", ".", "dropout", ",", "\n", "}", "\n", "\n", "torch", ".", "save", "(", "model_state", ",", "str", "(", "file", ")", ",", "pickle_protocol", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.generate_text": [[271, 359], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "language_model.LanguageModel.init_hidden", "torch.tensor().unsqueeze().unsqueeze", "torch.tensor().unsqueeze().unsqueeze", "torch.tensor().unsqueeze().unsqueeze", "torch.tensor().unsqueeze().unsqueeze", "range", "log_prob.item.item.item", "len", "len", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "language_model.LanguageModel.forward", "torch.tensor.detach().unsqueeze().unsqueeze.to", "language_model.LanguageModel.forward", "prediction.div.div.squeeze().detach", "prediction.div.div.div", "torch.max", "torch.max", "torch.max", "torch.max", "prediction.div.div.exp().cpu", "torch.tensor.detach().unsqueeze().unsqueeze", "torch.tensor.detach().unsqueeze().unsqueeze", "idx2item[].decode", "characters.append", "char_tensors.append", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze().unsqueeze", "torch.tensor().unsqueeze().unsqueeze", "torch.tensor().unsqueeze().unsqueeze", "torch.tensor().unsqueeze().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prediction.div.div.squeeze", "prediction.div.div.exp", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor.detach().unsqueeze", "torch.tensor.detach().unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "language_model.LanguageModel.dictionary.get_idx_for_item", "torch.tensor.detach", "torch.tensor.detach", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "language_model.LanguageModel.dictionary.get_idx_for_item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.init_hidden", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.decode", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item"], ["", "def", "generate_text", "(", "\n", "self", ",", "\n", "prefix", ":", "str", "=", "\"\\n\"", ",", "\n", "number_of_characters", ":", "int", "=", "1000", ",", "\n", "temperature", ":", "float", "=", "1.0", ",", "\n", "break_on_suffix", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "str", ",", "float", "]", ":", "\n", "\n", "        ", "if", "prefix", "==", "\"\"", ":", "\n", "            ", "prefix", "=", "\"\\n\"", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "characters", "=", "[", "]", "\n", "\n", "idx2item", "=", "self", ".", "dictionary", ".", "idx2item", "\n", "\n", "# initial hidden state", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "1", ")", "\n", "\n", "if", "len", "(", "prefix", ")", ">", "1", ":", "\n", "\n", "                ", "char_tensors", "=", "[", "]", "\n", "for", "character", "in", "prefix", "[", ":", "-", "1", "]", ":", "\n", "                    ", "char_tensors", ".", "append", "(", "\n", "torch", ".", "tensor", "(", "self", ".", "dictionary", ".", "get_idx_for_item", "(", "character", ")", ")", "\n", ".", "unsqueeze", "(", "0", ")", "\n", ".", "unsqueeze", "(", "0", ")", "\n", ")", "\n", "\n", "", "input", "=", "torch", ".", "cat", "(", "char_tensors", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "prediction", ",", "_", ",", "hidden", "=", "self", ".", "forward", "(", "input", ",", "hidden", ")", "\n", "\n", "", "input", "=", "(", "\n", "torch", ".", "tensor", "(", "self", ".", "dictionary", ".", "get_idx_for_item", "(", "prefix", "[", "-", "1", "]", ")", ")", "\n", ".", "unsqueeze", "(", "0", ")", "\n", ".", "unsqueeze", "(", "0", ")", "\n", ")", "\n", "\n", "log_prob", "=", "0.0", "\n", "\n", "for", "i", "in", "range", "(", "number_of_characters", ")", ":", "\n", "\n", "                ", "input", "=", "input", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "# get predicted weights", "\n", "prediction", ",", "_", ",", "hidden", "=", "self", ".", "forward", "(", "input", ",", "hidden", ")", "\n", "prediction", "=", "prediction", ".", "squeeze", "(", ")", ".", "detach", "(", ")", "\n", "decoder_output", "=", "prediction", "\n", "\n", "# divide by temperature", "\n", "prediction", "=", "prediction", ".", "div", "(", "temperature", ")", "\n", "\n", "# to prevent overflow problem with small temperature values, substract largest value from all", "\n", "# this makes a vector in which the largest value is 0", "\n", "max", "=", "torch", ".", "max", "(", "prediction", ")", "\n", "prediction", "-=", "max", "\n", "\n", "# compute word weights with exponential function", "\n", "word_weights", "=", "prediction", ".", "exp", "(", ")", ".", "cpu", "(", ")", "\n", "\n", "# try sampling multinomial distribution for next character", "\n", "try", ":", "\n", "                    ", "word_idx", "=", "torch", ".", "multinomial", "(", "word_weights", ",", "1", ")", "[", "0", "]", "\n", "", "except", ":", "\n", "                    ", "word_idx", "=", "torch", ".", "tensor", "(", "0", ")", "\n", "\n", "# print(word_idx)", "\n", "", "prob", "=", "decoder_output", "[", "word_idx", "]", "\n", "log_prob", "+=", "prob", "\n", "\n", "input", "=", "word_idx", ".", "detach", "(", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "word", "=", "idx2item", "[", "word_idx", "]", ".", "decode", "(", "\"UTF-8\"", ")", "\n", "characters", ".", "append", "(", "word", ")", "\n", "\n", "if", "break_on_suffix", "is", "not", "None", ":", "\n", "                    ", "if", "\"\"", ".", "join", "(", "characters", ")", ".", "endswith", "(", "break_on_suffix", ")", ":", "\n", "                        ", "break", "\n", "\n", "", "", "", "text", "=", "prefix", "+", "\"\"", ".", "join", "(", "characters", ")", "\n", "\n", "log_prob", "=", "log_prob", ".", "item", "(", ")", "\n", "log_prob", "/=", "len", "(", "characters", ")", "\n", "\n", "if", "not", "self", ".", "is_forward_lm", ":", "\n", "                ", "text", "=", "text", "[", ":", ":", "-", "1", "]", "\n", "\n", "", "return", "text", ",", "log_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.calculate_perplexity": [[360, 391], ["torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "input.to.to.to", "language_model.LanguageModel.init_hidden", "language_model.LanguageModel.forward", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "targets.to.to.to", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.item", "torch.nn.CrossEntropyLoss.item", "math.exp", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "language_model.LanguageModel.dictionary.get_idx_for_item", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction.view", "language_model.LanguageModel.dictionary.get_idx_for_item", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.init_hidden", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item"], ["", "", "def", "calculate_perplexity", "(", "self", ",", "text", ":", "str", ")", "->", "float", ":", "\n", "\n", "        ", "if", "not", "self", ".", "is_forward_lm", ":", "\n", "            ", "text", "=", "text", "[", ":", ":", "-", "1", "]", "\n", "\n", "# input ids", "\n", "", "input", "=", "torch", ".", "tensor", "(", "\n", "[", "self", ".", "dictionary", ".", "get_idx_for_item", "(", "char", ")", "for", "char", "in", "text", "[", ":", "-", "1", "]", "]", "\n", ")", ".", "unsqueeze", "(", "1", ")", "\n", "input", "=", "input", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "# push list of character IDs through rpbert", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "1", ")", "\n", "prediction", ",", "_", ",", "hidden", "=", "self", ".", "forward", "(", "input", ",", "hidden", ")", "\n", "\n", "# the target is always the next character", "\n", "targets", "=", "torch", ".", "tensor", "(", "\n", "[", "self", ".", "dictionary", ".", "get_idx_for_item", "(", "char", ")", "for", "char", "in", "text", "[", "1", ":", "]", "]", "\n", ")", "\n", "targets", "=", "targets", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "# use cross entropy loss to compare output of forward pass with targets", "\n", "cross_entroy_loss", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "cross_entroy_loss", "(", "\n", "prediction", ".", "view", "(", "-", "1", ",", "len", "(", "self", ".", "dictionary", ")", ")", ",", "targets", "\n", ")", ".", "item", "(", ")", "\n", "\n", "# exponentiate cross-entropy loss to calculate perplexity", "\n", "perplexity", "=", "math", ".", "exp", "(", "loss", ")", "\n", "\n", "return", "perplexity", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel._apply": [[392, 425], ["int", "language_model.LanguageModel.children", "super()._apply", "torch.__version__.replace().split", "torch.__version__.replace().split", "torch.__version__.replace().split", "torch.__version__.replace().split", "info.isdigit", "isinstance", "child_module._apply", "range", "setattr", "torch.__version__.replace", "torch.__version__.replace", "torch.__version__.replace", "torch.__version__.replace", "range", "_flat_weights_names.extend", "x.format"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel._apply", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel._apply"], ["", "def", "_apply", "(", "self", ",", "fn", ")", ":", "\n", "        ", "major", ",", "minor", ",", "build", ",", "*", "_", "=", "(", "int", "(", "info", ")", "\n", "for", "info", "in", "torch", ".", "__version__", ".", "replace", "(", "\"+\"", ",", "\".\"", ")", ".", "split", "(", "'.'", ")", "if", "info", ".", "isdigit", "(", ")", ")", "\n", "\n", "# fixed RNN change format for torch 1.4.0", "\n", "if", "major", ">=", "1", "and", "minor", ">=", "4", ":", "\n", "            ", "for", "child_module", "in", "self", ".", "children", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "child_module", ",", "torch", ".", "nn", ".", "RNNBase", ")", ":", "\n", "                    ", "_flat_weights_names", "=", "[", "]", "\n", "num_direction", "=", "None", "\n", "\n", "if", "child_module", ".", "__dict__", "[", "\"bidirectional\"", "]", ":", "\n", "                        ", "num_direction", "=", "2", "\n", "", "else", ":", "\n", "                        ", "num_direction", "=", "1", "\n", "", "for", "layer", "in", "range", "(", "child_module", ".", "__dict__", "[", "\"num_layers\"", "]", ")", ":", "\n", "                        ", "for", "direction", "in", "range", "(", "num_direction", ")", ":", "\n", "                            ", "suffix", "=", "\"_reverse\"", "if", "direction", "==", "1", "else", "\"\"", "\n", "param_names", "=", "[", "\"weight_ih_l{}{}\"", ",", "\"weight_hh_l{}{}\"", "]", "\n", "if", "child_module", ".", "__dict__", "[", "\"bias\"", "]", ":", "\n", "                                ", "param_names", "+=", "[", "\"bias_ih_l{}{}\"", ",", "\"bias_hh_l{}{}\"", "]", "\n", "", "param_names", "=", "[", "\n", "x", ".", "format", "(", "layer", ",", "suffix", ")", "for", "x", "in", "param_names", "\n", "]", "\n", "_flat_weights_names", ".", "extend", "(", "param_names", ")", "\n", "\n", "", "", "setattr", "(", "child_module", ",", "\"_flat_weights_names\"", ",", "\n", "_flat_weights_names", ")", "\n", "\n", "", "child_module", ".", "_apply", "(", "fn", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "super", "(", ")", ".", "_apply", "(", "fn", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityMeasure.forward": [[24, 27], ["None"], "methods", ["None"], ["    ", "@", "abstractmethod", "\n", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SliceReshaper.__init__": [[31, 36], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "begin", ",", "end", "=", "None", ",", "shape", "=", "None", ")", ":", "\n", "        ", "super", "(", "SliceReshaper", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "begin", "=", "begin", "\n", "self", ".", "end", "=", "end", "\n", "self", ".", "shape", "=", "shape", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SliceReshaper.forward": [[37, 41], ["x.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "[", ":", ",", "self", ".", "begin", "]", "if", "self", ".", "end", "is", "None", "else", "x", "[", ":", ",", "self", ".", "begin", ":", "self", ".", "end", "]", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "*", "self", ".", "shape", ")", "if", "self", ".", "shape", "is", "not", "None", "else", "x", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.ModelSimilarity.__init__": [[51, 54], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "model", ")", ":", "\n", "# rpbert is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}", "\n", "        ", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.ModelSimilarity.forward": [[55, 72], ["parameter_map.items", "layer_model", "isinstance", "param_slice_reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "model_parameters", "=", "x", "[", "0", "]", "\n", "model_inputs", "=", "x", "[", "1", "]", "\n", "\n", "cur_outputs", "=", "model_inputs", "\n", "for", "layer_model", ",", "parameter_map", "in", "self", ".", "model", ":", "\n", "            ", "param_dict", "=", "{", "}", "\n", "for", "param_name", ",", "param_slice_reshape", "in", "parameter_map", ".", "items", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "param_slice_reshape", ",", "SliceReshaper", ")", ":", "\n", "                    ", "val", "=", "param_slice_reshape", "(", "model_parameters", ")", "\n", "", "else", ":", "\n", "                    ", "val", "=", "param_slice_reshape", "\n", "", "param_dict", "[", "param_name", "]", "=", "val", "\n", "", "cur_outputs", "=", "layer_model", "(", "cur_outputs", ",", "**", "param_dict", ")", "\n", "\n", "", "return", "cur_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.CosineSimilarity.forward": [[80, 91], ["torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "input_modality_0", "=", "x", "[", "0", "]", "\n", "input_modality_1", "=", "x", "[", "1", "]", "\n", "\n", "# normalize the embeddings", "\n", "input_modality_0_norms", "=", "torch", ".", "norm", "(", "input_modality_0", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "input_modality_1_norms", "=", "torch", ".", "norm", "(", "input_modality_1", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "return", "torch", ".", "matmul", "(", "\n", "input_modality_0", "/", "input_modality_0_norms", ",", "\n", "(", "input_modality_1", "/", "input_modality_1_norms", ")", ".", "t", "(", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLoss.__init__": [[96, 98], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "SimilarityLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLoss.forward": [[99, 102], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "forward", "(", "self", ",", "inputs", ",", "targets", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.PairwiseBCELoss.__init__": [[109, 112], ["similarity_learning_model.SimilarityLoss.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "balanced", "=", "False", ")", ":", "\n", "        ", "super", "(", "PairwiseBCELoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "balanced", "=", "balanced", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.PairwiseBCELoss.forward": [[113, 125], ["torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits.mean", "torch.ones_like().to", "torch.ones_like().to", "torch.ones_like().to", "torch.ones_like().to", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "targets", ")", ":", "\n", "        ", "n", "=", "inputs", ".", "shape", "[", "0", "]", "\n", "neg_targets", "=", "torch", ".", "ones_like", "(", "targets", ")", ".", "to", "(", "flair", ".", "device", ")", "-", "targets", "\n", "# we want that logits for corresponding pairs are high, and for non-corresponding low", "\n", "bce_loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "inputs", ",", "targets", ",", "reduction", "=", "\"none\"", ")", "\n", "if", "self", ".", "balanced", ":", "\n", "# TODO: this assumes eye matrix", "\n", "            ", "weight_matrix", "=", "n", "*", "(", "targets", "/", "2.0", "+", "neg_targets", "/", "(", "2.0", "*", "(", "n", "-", "1", ")", ")", ")", "\n", "bce_loss", "*=", "weight_matrix", "\n", "", "loss", "=", "bce_loss", ".", "mean", "(", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.RankingLoss.__init__": [[132, 136], ["similarity_learning_model.SimilarityLoss.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["def", "__init__", "(", "self", ",", "margin", "=", "0.1", ",", "direction_weights", "=", "[", "0.5", ",", "0.5", "]", ")", ":", "\n", "        ", "super", "(", "RankingLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "margin", "=", "margin", "\n", "self", ".", "direction_weights", "=", "direction_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.RankingLoss.forward": [[137, 156], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "targets", ")", ":", "\n", "        ", "n", "=", "inputs", ".", "shape", "[", "0", "]", "\n", "neg_targets", "=", "torch", ".", "ones_like", "(", "targets", ")", "-", "targets", "\n", "# loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa", "\n", "ranking_loss_matrix_01", "=", "neg_targets", "*", "F", ".", "relu", "(", "\n", "self", ".", "margin", "+", "inputs", "-", "torch", ".", "diag", "(", "inputs", ")", ".", "view", "(", "n", ",", "1", ")", "\n", ")", "\n", "ranking_loss_matrix_10", "=", "neg_targets", "*", "F", ".", "relu", "(", "\n", "self", ".", "margin", "+", "inputs", "-", "torch", ".", "diag", "(", "inputs", ")", ".", "view", "(", "1", ",", "n", ")", "\n", ")", "\n", "neg_targets_01_sum", "=", "torch", ".", "sum", "(", "neg_targets", ",", "dim", "=", "1", ")", "\n", "neg_targets_10_sum", "=", "torch", ".", "sum", "(", "neg_targets", ",", "dim", "=", "0", ")", "\n", "loss", "=", "self", ".", "direction_weights", "[", "0", "]", "*", "torch", ".", "mean", "(", "\n", "torch", ".", "sum", "(", "ranking_loss_matrix_01", "/", "neg_targets_01_sum", ",", "dim", "=", "1", ")", "\n", ")", "+", "self", ".", "direction_weights", "[", "1", "]", "*", "torch", ".", "mean", "(", "\n", "torch", ".", "sum", "(", "ranking_loss_matrix_10", "/", "neg_targets_10_sum", ",", "dim", "=", "0", ")", "\n", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLearner.__init__": [[160, 186], ["super().__init__", "similarity_learning_model.SimilarityLearner.to"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "source_embeddings", ":", "Embeddings", ",", "\n", "target_embeddings", ":", "Embeddings", ",", "\n", "similarity_measure", ":", "SimilarityMeasure", ",", "\n", "similarity_loss", ":", "SimilarityLoss", ",", "\n", "eval_device", "=", "flair", ".", "device", ",", "\n", "source_mapping", ":", "torch", ".", "nn", ".", "Module", "=", "None", ",", "\n", "target_mapping", ":", "torch", ".", "nn", ".", "Module", "=", "None", ",", "\n", "recall_at_points", ":", "List", "[", "int", "]", "=", "[", "1", ",", "5", ",", "10", ",", "20", "]", ",", "\n", "recall_at_points_weights", ":", "List", "[", "float", "]", "=", "[", "0.4", ",", "0.3", ",", "0.2", ",", "0.1", "]", ",", "\n", "interleave_embedding_updates", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", "SimilarityLearner", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "source_embeddings", ":", "Embeddings", "=", "source_embeddings", "\n", "self", ".", "target_embeddings", ":", "Embeddings", "=", "target_embeddings", "\n", "self", ".", "source_mapping", ":", "torch", ".", "nn", ".", "Module", "=", "source_mapping", "\n", "self", ".", "target_mapping", ":", "torch", ".", "nn", ".", "Module", "=", "target_mapping", "\n", "self", ".", "similarity_measure", ":", "SimilarityMeasure", "=", "similarity_measure", "\n", "self", ".", "similarity_loss", ":", "SimilarityLoss", "=", "similarity_loss", "\n", "self", ".", "eval_device", "=", "eval_device", "\n", "self", ".", "recall_at_points", ":", "List", "[", "int", "]", "=", "recall_at_points", "\n", "self", ".", "recall_at_points_weights", ":", "List", "[", "float", "]", "=", "recall_at_points_weights", "\n", "self", ".", "interleave_embedding_updates", "=", "interleave_embedding_updates", "\n", "\n", "self", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLearner._embed_source": [[187, 202], ["similarity_learning_model.SimilarityLearner.source_embeddings.embed", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.stack().to", "type", "similarity_learning_model.SimilarityLearner.source_mapping", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "_embed_source", "(", "self", ",", "data_points", ")", ":", "\n", "\n", "        ", "if", "type", "(", "data_points", "[", "0", "]", ")", "==", "DataPair", ":", "\n", "            ", "data_points", "=", "[", "point", ".", "first", "for", "point", "in", "data_points", "]", "\n", "\n", "", "self", ".", "source_embeddings", ".", "embed", "(", "data_points", ")", "\n", "\n", "source_embedding_tensor", "=", "torch", ".", "stack", "(", "\n", "[", "point", ".", "embedding", "for", "point", "in", "data_points", "]", "\n", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "if", "self", ".", "source_mapping", "is", "not", "None", ":", "\n", "            ", "source_embedding_tensor", "=", "self", ".", "source_mapping", "(", "source_embedding_tensor", ")", "\n", "\n", "", "return", "source_embedding_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLearner._embed_target": [[203, 218], ["similarity_learning_model.SimilarityLearner.target_embeddings.embed", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.stack().to", "type", "similarity_learning_model.SimilarityLearner.target_mapping", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "_embed_target", "(", "self", ",", "data_points", ")", ":", "\n", "\n", "        ", "if", "type", "(", "data_points", "[", "0", "]", ")", "==", "DataPair", ":", "\n", "            ", "data_points", "=", "[", "point", ".", "second", "for", "point", "in", "data_points", "]", "\n", "\n", "", "self", ".", "target_embeddings", ".", "embed", "(", "data_points", ")", "\n", "\n", "target_embedding_tensor", "=", "torch", ".", "stack", "(", "\n", "[", "point", ".", "embedding", "for", "point", "in", "data_points", "]", "\n", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "if", "self", ".", "target_mapping", "is", "not", "None", ":", "\n", "            ", "target_embedding_tensor", "=", "self", ".", "target_mapping", "(", "target_embedding_tensor", ")", "\n", "\n", "", "return", "target_embedding_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLearner.get_similarity": [[219, 227], ["similarity_learning_model.SimilarityLearner.similarity_measure.forward"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward"], ["", "def", "get_similarity", "(", "self", ",", "modality_0_embeddings", ",", "modality_1_embeddings", ")", ":", "\n", "        ", "\"\"\"\n        :param modality_0_embeddings: embeddings of first modality, a tensor of shape [n0, d0]\n        :param modality_1_embeddings: embeddings of second modality, a tensor of shape [n1, d1]\n        :return: a similarity matrix of shape [n0, n1]\n        \"\"\"", "\n", "return", "self", ".", "similarity_measure", ".", "forward", "(", "\n", "[", "modality_0_embeddings", ",", "modality_1_embeddings", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLearner.forward_loss": [[229, 271], ["similarity_learning_model.SimilarityLearner._embed_source", "similarity_learning_model.SimilarityLearner._embed_target", "similarity_learning_model.SimilarityLearner.similarity_measure.forward", "enumerate", "torch.zeros_like().to", "torch.zeros_like().to", "torch.zeros_like().to", "torch.zeros_like().to", "similarity_learning_model.SimilarityLearner.similarity_loss", "torch.randint().item", "torch.randint().item", "torch.randint().item", "torch.randint().item", "similarity_learning_model.SimilarityLearner.forward_loss.add_to_index_map"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLearner._embed_source", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLearner._embed_target", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "forward_loss", "(", "\n", "self", ",", "data_points", ":", "Union", "[", "List", "[", "DataPoint", "]", ",", "DataPoint", "]", "\n", ")", "->", "torch", ".", "tensor", ":", "\n", "        ", "mapped_source_embeddings", "=", "self", ".", "_embed_source", "(", "data_points", ")", "\n", "mapped_target_embeddings", "=", "self", ".", "_embed_target", "(", "data_points", ")", "\n", "\n", "if", "self", ".", "interleave_embedding_updates", ":", "\n", "# 1/3 only source branch of rpbert, 1/3 only target branch of rpbert, 1/3 both", "\n", "            ", "detach_modality_id", "=", "torch", ".", "randint", "(", "0", ",", "3", ",", "(", "1", ",", ")", ")", ".", "item", "(", ")", "\n", "if", "detach_modality_id", "==", "0", ":", "\n", "                ", "mapped_source_embeddings", ".", "detach", "(", ")", "\n", "", "elif", "detach_modality_id", "==", "1", ":", "\n", "                ", "mapped_target_embeddings", ".", "detach", "(", ")", "\n", "\n", "", "", "similarity_matrix", "=", "self", ".", "similarity_measure", ".", "forward", "(", "\n", "(", "mapped_source_embeddings", ",", "mapped_target_embeddings", ")", "\n", ")", "\n", "\n", "def", "add_to_index_map", "(", "hashmap", ",", "key", ",", "val", ")", ":", "\n", "            ", "if", "key", "not", "in", "hashmap", ":", "\n", "                ", "hashmap", "[", "key", "]", "=", "[", "val", "]", "\n", "", "else", ":", "\n", "                ", "hashmap", "[", "key", "]", "+=", "[", "val", "]", "\n", "\n", "", "", "index_map", "=", "{", "\"first\"", ":", "{", "}", ",", "\"second\"", ":", "{", "}", "}", "\n", "for", "data_point_id", ",", "data_point", "in", "enumerate", "(", "data_points", ")", ":", "\n", "            ", "add_to_index_map", "(", "index_map", "[", "\"first\"", "]", ",", "str", "(", "data_point", ".", "first", ")", ",", "data_point_id", ")", "\n", "add_to_index_map", "(", "index_map", "[", "\"second\"", "]", ",", "str", "(", "data_point", ".", "second", ")", ",", "data_point_id", ")", "\n", "\n", "", "targets", "=", "torch", ".", "zeros_like", "(", "similarity_matrix", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "for", "data_point", "in", "data_points", ":", "\n", "            ", "first_indices", "=", "index_map", "[", "\"first\"", "]", "[", "str", "(", "data_point", ".", "first", ")", "]", "\n", "second_indices", "=", "index_map", "[", "\"second\"", "]", "[", "str", "(", "data_point", ".", "second", ")", "]", "\n", "for", "first_index", ",", "second_index", "in", "itertools", ".", "product", "(", "\n", "first_indices", ",", "second_indices", "\n", ")", ":", "\n", "                ", "targets", "[", "first_index", ",", "second_index", "]", "=", "1.0", "\n", "\n", "", "", "loss", "=", "self", ".", "similarity_loss", "(", "similarity_matrix", ",", "targets", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLearner.evaluate": [[272, 361], ["numpy.array", "numpy.median", "sum", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "numpy.mean", "flair.training_utils.Result", "flair.training_utils.store_embeddings", "len", "similarity_learning_model.SimilarityLearner._embed_source", "similarity_learning_model.SimilarityLearner.to", "similarity_learning_model.SimilarityLearner.similarity_measure.forward", "torch.argsort", "torch.argsort", "torch.argsort", "torch.argsort", "numpy.array.extend", "flair.training_utils.store_embeddings", "str", "str", "torch.cat.append", "torch.cat.append", "torch.argsort", "torch.argsort", "torch.argsort", "torch.argsort", "batch_gt_ranks.tolist", "str", "zip", "zip", "str", "len", "target_inputs.append", "similarity_learning_model.SimilarityLearner._embed_target().to", "str", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "str", "similarity_learning_model.SimilarityLearner._embed_target"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLearner._embed_source", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLearner._embed_target"], ["", "def", "evaluate", "(", "\n", "self", ",", "\n", "data_loader", ":", "DataLoader", ",", "\n", "out_path", ":", "Path", "=", "None", ",", "\n", "embedding_storage_mode", "=", "\"none\"", ",", "\n", ")", "->", "(", "Result", ",", "float", ")", ":", "\n", "# assumes that for each data pair there's at least one embedding per modality", "\n", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# pre-compute embeddings for all targets in evaluation dataset", "\n", "            ", "target_index", "=", "{", "}", "\n", "all_target_embeddings", "=", "[", "]", "\n", "for", "data_points", "in", "data_loader", ":", "\n", "                ", "target_inputs", "=", "[", "]", "\n", "for", "data_point", "in", "data_points", ":", "\n", "                    ", "if", "str", "(", "data_point", ".", "second", ")", "not", "in", "target_index", ":", "\n", "                        ", "target_index", "[", "str", "(", "data_point", ".", "second", ")", "]", "=", "len", "(", "target_index", ")", "\n", "target_inputs", ".", "append", "(", "data_point", ")", "\n", "", "", "if", "target_inputs", ":", "\n", "                    ", "all_target_embeddings", ".", "append", "(", "\n", "self", ".", "_embed_target", "(", "target_inputs", ")", ".", "to", "(", "self", ".", "eval_device", ")", "\n", ")", "\n", "", "store_embeddings", "(", "data_points", ",", "embedding_storage_mode", ")", "\n", "", "all_target_embeddings", "=", "torch", ".", "cat", "(", "all_target_embeddings", ",", "dim", "=", "0", ")", "# [n0, d0]", "\n", "assert", "len", "(", "target_index", ")", "==", "all_target_embeddings", ".", "shape", "[", "0", "]", "\n", "\n", "ranks", "=", "[", "]", "\n", "for", "data_points", "in", "data_loader", ":", "\n", "                ", "batch_embeddings", "=", "self", ".", "_embed_source", "(", "data_points", ")", "\n", "\n", "batch_source_embeddings", "=", "batch_embeddings", ".", "to", "(", "self", ".", "eval_device", ")", "\n", "# compute the similarity", "\n", "batch_similarity_matrix", "=", "self", ".", "similarity_measure", ".", "forward", "(", "\n", "[", "batch_source_embeddings", ",", "all_target_embeddings", "]", "\n", ")", "\n", "\n", "# sort the similarity matrix across modality 1", "\n", "batch_modality_1_argsort", "=", "torch", ".", "argsort", "(", "\n", "batch_similarity_matrix", ",", "descending", "=", "True", ",", "dim", "=", "1", "\n", ")", "\n", "\n", "# get the ranks, so +1 to start counting ranks from 1", "\n", "batch_modality_1_ranks", "=", "(", "\n", "torch", ".", "argsort", "(", "batch_modality_1_argsort", ",", "dim", "=", "1", ")", "+", "1", "\n", ")", "\n", "\n", "batch_target_indices", "=", "[", "\n", "target_index", "[", "str", "(", "data_point", ".", "second", ")", "]", "for", "data_point", "in", "data_points", "\n", "]", "\n", "\n", "batch_gt_ranks", "=", "batch_modality_1_ranks", "[", "\n", "torch", ".", "arange", "(", "batch_similarity_matrix", ".", "shape", "[", "0", "]", ")", ",", "\n", "torch", ".", "tensor", "(", "batch_target_indices", ")", ",", "\n", "]", "\n", "ranks", ".", "extend", "(", "batch_gt_ranks", ".", "tolist", "(", ")", ")", "\n", "\n", "store_embeddings", "(", "data_points", ",", "embedding_storage_mode", ")", "\n", "\n", "", "", "ranks", "=", "np", ".", "array", "(", "ranks", ")", "\n", "median_rank", "=", "np", ".", "median", "(", "ranks", ")", "\n", "recall_at", "=", "{", "k", ":", "np", ".", "mean", "(", "ranks", "<=", "k", ")", "for", "k", "in", "self", ".", "recall_at_points", "}", "\n", "\n", "results_header", "=", "[", "\"Median rank\"", "]", "+", "[", "\n", "\"Recall@top\"", "+", "str", "(", "r", ")", "for", "r", "in", "self", ".", "recall_at_points", "\n", "]", "\n", "results_header_str", "=", "\"\\t\"", ".", "join", "(", "results_header", ")", "\n", "epoch_results", "=", "[", "str", "(", "median_rank", ")", "]", "+", "[", "\n", "str", "(", "recall_at", "[", "k", "]", ")", "for", "k", "in", "self", ".", "recall_at_points", "\n", "]", "\n", "epoch_results_str", "=", "\"\\t\"", ".", "join", "(", "epoch_results", ")", "\n", "detailed_results", "=", "\", \"", ".", "join", "(", "\n", "[", "f\"{h}={v}\"", "for", "h", ",", "v", "in", "zip", "(", "results_header", ",", "epoch_results", ")", "]", "\n", ")", "\n", "\n", "validated_measure", "=", "sum", "(", "\n", "[", "\n", "recall_at", "[", "r", "]", "*", "w", "\n", "for", "r", ",", "w", "in", "zip", "(", "self", ".", "recall_at_points", ",", "self", ".", "recall_at_points_weights", ")", "\n", "]", "\n", ")", "\n", "\n", "return", "(", "\n", "Result", "(", "\n", "validated_measure", ",", "\n", "results_header_str", ",", "\n", "epoch_results_str", ",", "\n", "detailed_results", ",", "\n", ")", ",", "\n", "0", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLearner._get_state_dict": [[363, 377], ["similarity_learning_model.SimilarityLearner.state_dict"], "methods", ["None"], ["", "def", "_get_state_dict", "(", "self", ")", ":", "\n", "        ", "model_state", "=", "{", "\n", "\"state_dict\"", ":", "self", ".", "state_dict", "(", ")", ",", "\n", "\"input_modality_0_embedding\"", ":", "self", ".", "source_embeddings", ",", "\n", "\"input_modality_1_embedding\"", ":", "self", ".", "target_embeddings", ",", "\n", "\"similarity_measure\"", ":", "self", ".", "similarity_measure", ",", "\n", "\"similarity_loss\"", ":", "self", ".", "similarity_loss", ",", "\n", "\"source_mapping\"", ":", "self", ".", "source_mapping", ",", "\n", "\"target_mapping\"", ":", "self", ".", "target_mapping", ",", "\n", "\"eval_device\"", ":", "self", ".", "eval_device", ",", "\n", "\"recall_at_points\"", ":", "self", ".", "recall_at_points", ",", "\n", "\"recall_at_points_weights\"", ":", "self", ".", "recall_at_points_weights", ",", "\n", "}", "\n", "return", "model_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.similarity_learning_model.SimilarityLearner._init_model_with_state_dict": [[378, 398], ["similarity_learning_model.SimilarityLearner", "SimilarityLearner.load_state_dict"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_init_model_with_state_dict", "(", "state", ")", ":", "\n", "# The conversion from old rpbert's constructor interface", "\n", "        ", "if", "\"input_embeddings\"", "in", "state", ":", "\n", "            ", "state", "[", "\"input_modality_0_embedding\"", "]", "=", "state", "[", "\"input_embeddings\"", "]", "[", "0", "]", "\n", "state", "[", "\"input_modality_1_embedding\"", "]", "=", "state", "[", "\"input_embeddings\"", "]", "[", "1", "]", "\n", "", "model", "=", "SimilarityLearner", "(", "\n", "source_embeddings", "=", "state", "[", "\"input_modality_0_embedding\"", "]", ",", "\n", "target_embeddings", "=", "state", "[", "\"input_modality_1_embedding\"", "]", ",", "\n", "source_mapping", "=", "state", "[", "\"source_mapping\"", "]", ",", "\n", "target_mapping", "=", "state", "[", "\"target_mapping\"", "]", ",", "\n", "similarity_measure", "=", "state", "[", "\"similarity_measure\"", "]", ",", "\n", "similarity_loss", "=", "state", "[", "\"similarity_loss\"", "]", ",", "\n", "eval_device", "=", "state", "[", "\"eval_device\"", "]", ",", "\n", "recall_at_points", "=", "state", "[", "\"recall_at_points\"", "]", ",", "\n", "recall_at_points_weights", "=", "state", "[", "\"recall_at_points_weights\"", "]", ",", "\n", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "state", "[", "\"state_dict\"", "]", ")", "\n", "return", "model", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_regression_model.TextRegressor.__init__": [[18, 29], ["super().__init__", "log.info", "torch.MSELoss", "torch.MSELoss", "flair.data.Dictionary", "flair.data.Dictionary", "flair.data.Dictionary", "flair.data.Dictionary"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "document_embeddings", ":", "flair", ".", "embeddings", ".", "DocumentEmbeddings", ")", ":", "\n", "\n", "        ", "super", "(", "TextRegressor", ",", "self", ")", ".", "__init__", "(", "\n", "document_embeddings", "=", "document_embeddings", ",", "\n", "label_dictionary", "=", "flair", ".", "data", ".", "Dictionary", "(", ")", ",", "\n", "multi_label", "=", "False", ",", "\n", ")", "\n", "\n", "log", ".", "info", "(", "\"Using REGRESSION - experimental\"", ")", "\n", "\n", "self", ".", "loss_function", "=", "nn", ".", "MSELoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_regression_model.TextRegressor._labels_to_indices": [[30, 41], ["torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "float"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "def", "_labels_to_indices", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", ":", "\n", "        ", "indices", "=", "[", "\n", "torch", ".", "tensor", "(", "\n", "[", "float", "(", "label", ".", "value", ")", "for", "label", "in", "sentence", ".", "labels", "]", ",", "dtype", "=", "torch", ".", "float", "\n", ")", "\n", "for", "sentence", "in", "sentences", "\n", "]", "\n", "\n", "vec", "=", "torch", ".", "cat", "(", "indices", ",", "0", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "return", "vec", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_regression_model.TextRegressor.predict": [[42, 73], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "text_regression_model.TextRegressor._filter_empty_sentences", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "type", "text_regression_model.TextRegressor.forward", "zip", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "range", "text_regression_model.TextRegressor.tolist", "len", "flair.data.Label", "flair.data.Label", "str"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._filter_empty_sentences", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings"], ["", "def", "predict", "(", "\n", "self", ",", "\n", "sentences", ":", "Union", "[", "Sentence", ",", "List", "[", "Sentence", "]", "]", ",", "\n", "mini_batch_size", ":", "int", "=", "32", ",", "\n", "embedding_storage_mode", "=", "\"none\"", ",", "\n", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "type", "(", "sentences", ")", "is", "Sentence", ":", "\n", "                ", "sentences", "=", "[", "sentences", "]", "\n", "\n", "", "filtered_sentences", "=", "self", ".", "_filter_empty_sentences", "(", "sentences", ")", "\n", "\n", "# remove previous embeddings", "\n", "store_embeddings", "(", "filtered_sentences", ",", "\"none\"", ")", "\n", "\n", "batches", "=", "[", "\n", "filtered_sentences", "[", "x", ":", "x", "+", "mini_batch_size", "]", "\n", "for", "x", "in", "range", "(", "0", ",", "len", "(", "filtered_sentences", ")", ",", "mini_batch_size", ")", "\n", "]", "\n", "\n", "for", "batch", "in", "batches", ":", "\n", "                ", "scores", "=", "self", ".", "forward", "(", "batch", ")", "\n", "\n", "for", "(", "sentence", ",", "score", ")", "in", "zip", "(", "batch", ",", "scores", ".", "tolist", "(", ")", ")", ":", "\n", "                    ", "sentence", ".", "labels", "=", "[", "Label", "(", "value", "=", "str", "(", "score", "[", "0", "]", ")", ")", "]", "\n", "\n", "# clearing token embeddings to save memory", "\n", "", "store_embeddings", "(", "batch", ",", "storage_mode", "=", "embedding_storage_mode", ")", "\n", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_regression_model.TextRegressor._calculate_loss": [[74, 84], ["text_regression_model.TextRegressor.loss_function", "scores.squeeze", "text_regression_model.TextRegressor._labels_to_indices"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_regression_model.TextRegressor._labels_to_indices"], ["", "", "def", "_calculate_loss", "(", "\n", "self", ",", "scores", ":", "torch", ".", "tensor", ",", "sentences", ":", "List", "[", "Sentence", "]", "\n", ")", "->", "torch", ".", "tensor", ":", "\n", "        ", "\"\"\"\n        Calculates the loss.\n        :param scores: the prediction scores from the rpbert\n        :param sentences: list of sentences\n        :return: loss value\n        \"\"\"", "\n", "return", "self", ".", "loss_function", "(", "scores", ".", "squeeze", "(", "1", ")", ",", "self", ".", "_labels_to_indices", "(", "sentences", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_regression_model.TextRegressor.forward_labels_and_loss": [[85, 92], ["text_regression_model.TextRegressor.forward", "text_regression_model.TextRegressor._calculate_loss"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._calculate_loss"], ["", "def", "forward_labels_and_loss", "(", "\n", "self", ",", "sentences", ":", "Union", "[", "Sentence", ",", "List", "[", "Sentence", "]", "]", "\n", ")", "->", "(", "List", "[", "List", "[", "float", "]", "]", ",", "torch", ".", "tensor", ")", ":", "\n", "\n", "        ", "scores", "=", "self", ".", "forward", "(", "sentences", ")", "\n", "loss", "=", "self", ".", "_calculate_loss", "(", "scores", ",", "sentences", ")", "\n", "return", "scores", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_regression_model.TextRegressor.evaluate": [[93, 164], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "flair.training_utils.MetricRegression", "flair.training_utils.MetricRegression", "enumerate", "flair.training_utils.Result", "flair.training_utils.Result", "isinstance", "text_regression_model.TextRegressor.forward_labels_and_loss", "flair.training_utils.MetricRegression.true.extend", "flair.training_utils.MetricRegression.true.extend", "flair.training_utils.MetricRegression.pred.extend", "flair.training_utils.MetricRegression.pred.extend", "zip", "flair.training_utils.store_embeddings", "flair.training_utils.store_embeddings", "flair.training_utils.MetricRegression.pearsonr", "flair.training_utils.MetricRegression.pearsonr", "lines.append", "open", "outfile.write", "flair.training_utils.MetricRegression.mean_squared_error", "flair.training_utils.MetricRegression.mean_squared_error", "flair.training_utils.MetricRegression.spearmanr", "flair.training_utils.MetricRegression.spearmanr", "flair.training_utils.MetricRegression.pearsonr", "flair.training_utils.MetricRegression.pearsonr", "flair.training_utils.MetricRegression.mean_squared_error", "flair.training_utils.MetricRegression.mean_squared_error", "flair.training_utils.MetricRegression.mean_absolute_error", "flair.training_utils.MetricRegression.mean_absolute_error", "flair.training_utils.MetricRegression.pearsonr", "flair.training_utils.MetricRegression.pearsonr", "flair.training_utils.MetricRegression.spearmanr", "flair.training_utils.MetricRegression.spearmanr", "true_values.append", "type", "results.append", "results.append", "sentence.to_original_text", "float", "float", "float"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_regression_model.TextRegressor.forward_labels_and_loss", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.pearsonr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.pearsonr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_squared_error", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_squared_error", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.spearmanr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.spearmanr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.pearsonr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.pearsonr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_squared_error", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_squared_error", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_absolute_error", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.mean_absolute_error", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.pearsonr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.pearsonr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.spearmanr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.spearmanr", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_original_text"], ["", "def", "evaluate", "(", "\n", "self", ",", "\n", "data_loader", ":", "DataLoader", ",", "\n", "out_path", ":", "Path", "=", "None", ",", "\n", "embedding_storage_mode", ":", "str", "=", "\"none\"", ",", "\n", ")", "->", "(", "Result", ",", "float", ")", ":", "\n", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "eval_loss", "=", "0", "\n", "\n", "metric", "=", "MetricRegression", "(", "\"Evaluation\"", ")", "\n", "\n", "lines", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "total_count", "=", "0", "\n", "for", "batch_nr", ",", "batch", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "\n", "                ", "if", "isinstance", "(", "batch", ",", "Sentence", ")", ":", "\n", "                    ", "batch", "=", "[", "batch", "]", "\n", "\n", "", "scores", ",", "loss", "=", "self", ".", "forward_labels_and_loss", "(", "batch", ")", "\n", "\n", "true_values", "=", "[", "]", "\n", "for", "sentence", "in", "batch", ":", "\n", "                    ", "total_count", "+=", "1", "\n", "for", "label", "in", "sentence", ".", "labels", ":", "\n", "                        ", "true_values", ".", "append", "(", "float", "(", "label", ".", "value", ")", ")", "\n", "\n", "", "", "results", "=", "[", "]", "\n", "for", "score", "in", "scores", ":", "\n", "                    ", "if", "type", "(", "score", "[", "0", "]", ")", "is", "Label", ":", "\n", "                        ", "results", ".", "append", "(", "float", "(", "score", "[", "0", "]", ".", "score", ")", ")", "\n", "", "else", ":", "\n", "                        ", "results", ".", "append", "(", "float", "(", "score", "[", "0", "]", ")", ")", "\n", "\n", "", "", "eval_loss", "+=", "loss", "\n", "\n", "metric", ".", "true", ".", "extend", "(", "true_values", ")", "\n", "metric", ".", "pred", ".", "extend", "(", "results", ")", "\n", "\n", "for", "sentence", ",", "prediction", ",", "true_value", "in", "zip", "(", "\n", "batch", ",", "results", ",", "true_values", "\n", ")", ":", "\n", "                    ", "eval_line", "=", "\"{}\\t{}\\t{}\\n\"", ".", "format", "(", "\n", "sentence", ".", "to_original_text", "(", ")", ",", "true_value", ",", "prediction", "\n", ")", "\n", "lines", ".", "append", "(", "eval_line", ")", "\n", "\n", "", "store_embeddings", "(", "batch", ",", "embedding_storage_mode", ")", "\n", "\n", "", "eval_loss", "/=", "total_count", "\n", "\n", "##TODO: not saving lines yet", "\n", "if", "out_path", "is", "not", "None", ":", "\n", "                ", "with", "open", "(", "out_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "outfile", ":", "\n", "                    ", "outfile", ".", "write", "(", "\"\"", ".", "join", "(", "lines", ")", ")", "\n", "\n", "", "", "log_line", "=", "f\"{metric.mean_squared_error()}\\t{metric.spearmanr()}\\t{metric.pearsonr()}\"", "\n", "log_header", "=", "\"MSE\\tSPEARMAN\\tPEARSON\"", "\n", "\n", "detailed_result", "=", "(", "\n", "f\"AVG: mse: {metric.mean_squared_error():.4f} - \"", "\n", "f\"mae: {metric.mean_absolute_error():.4f} - \"", "\n", "f\"pearson: {metric.pearsonr():.4f} - \"", "\n", "f\"spearman: {metric.spearmanr():.4f}\"", "\n", ")", "\n", "\n", "result", ":", "Result", "=", "Result", "(", "\n", "metric", ".", "pearsonr", "(", ")", ",", "log_header", ",", "log_line", ",", "detailed_result", "\n", ")", "\n", "\n", "return", "result", ",", "eval_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_regression_model.TextRegressor._get_state_dict": [[165, 171], ["text_regression_model.TextRegressor.state_dict"], "methods", ["None"], ["", "", "def", "_get_state_dict", "(", "self", ")", ":", "\n", "        ", "model_state", "=", "{", "\n", "\"state_dict\"", ":", "self", ".", "state_dict", "(", ")", ",", "\n", "\"document_embeddings\"", ":", "self", ".", "document_embeddings", ",", "\n", "}", "\n", "return", "model_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.text_regression_model.TextRegressor._init_model_with_state_dict": [[172, 179], ["text_regression_model.TextRegressor", "TextRegressor.load_state_dict"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_init_model_with_state_dict", "(", "state", ")", ":", "\n", "\n", "        ", "model", "=", "TextRegressor", "(", "document_embeddings", "=", "state", "[", "\"document_embeddings\"", "]", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "state", "[", "\"state_dict\"", "]", ")", "\n", "return", "model", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.__init__": [[66, 218], ["super().__init__", "len", "sequence_tagger_model.SequenceTagger.to", "len", "enumerate", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "flair.nn.WordDropout", "flair.nn.LockedDropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_items", "len", "len", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "sequence_tagger_model.SequenceTagger.transitions.detach", "sequence_tagger_model.SequenceTagger.transitions.detach", "range", "loss_weights.keys", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "getattr", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_idx_for_item", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_idx_for_item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_items", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "hidden_size", ":", "int", ",", "\n", "embeddings", ":", "TokenEmbeddings", ",", "\n", "tag_dictionary", ":", "Dictionary", ",", "\n", "tag_type", ":", "str", ",", "\n", "use_crf", ":", "bool", "=", "True", ",", "\n", "use_rnn", ":", "bool", "=", "True", ",", "\n", "rnn_layers", ":", "int", "=", "1", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "word_dropout", ":", "float", "=", "0.05", ",", "\n", "locked_dropout", ":", "float", "=", "0.5", ",", "\n", "train_initial_hidden_state", ":", "bool", "=", "False", ",", "\n", "rnn_type", ":", "str", "=", "\"LSTM\"", ",", "\n", "pickle_module", ":", "str", "=", "\"pickle\"", ",", "\n", "beta", ":", "float", "=", "1.0", ",", "\n", "loss_weights", ":", "Dict", "[", "str", ",", "float", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Initializes a SequenceTagger\n        :param hidden_size: number of hidden states in RNN\n        :param embeddings: word embeddings used in tagger\n        :param tag_dictionary: dictionary of tags you want to predict\n        :param tag_type: string identifier for tag type\n        :param use_crf: if True use CRF decoder, else project directly to tag space\n        :param use_rnn: if True use RNN layer, otherwise use word embeddings directly\n        :param rnn_layers: number of RNN layers\n        :param dropout: dropout probability\n        :param word_dropout: word dropout probability\n        :param locked_dropout: locked dropout probability\n        :param train_initial_hidden_state: if True, trains initial hidden state of RNN\n        :param beta: Parameter for F-beta score for evaluation and training annealing\n        :param loss_weights: Dictionary of weights for classes (tags) for the loss function\n        (if any tag's weight is unspecified it will default to 1.0)\n\n        \"\"\"", "\n", "\n", "super", "(", "SequenceTagger", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "use_rnn", "=", "use_rnn", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "use_crf", ":", "bool", "=", "use_crf", "\n", "self", ".", "rnn_layers", ":", "int", "=", "rnn_layers", "\n", "\n", "self", ".", "trained_epochs", ":", "int", "=", "0", "\n", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "\n", "# set the dictionaries", "\n", "self", ".", "tag_dictionary", ":", "Dictionary", "=", "tag_dictionary", "\n", "self", ".", "tag_type", ":", "str", "=", "tag_type", "\n", "self", ".", "tagset_size", ":", "int", "=", "len", "(", "tag_dictionary", ")", "\n", "\n", "self", ".", "beta", "=", "beta", "\n", "\n", "self", ".", "weight_dict", "=", "loss_weights", "\n", "# Initialize the weight tensor", "\n", "if", "loss_weights", "is", "not", "None", ":", "\n", "            ", "n_classes", "=", "len", "(", "self", ".", "tag_dictionary", ")", "\n", "weight_list", "=", "[", "1.", "for", "i", "in", "range", "(", "n_classes", ")", "]", "\n", "for", "i", ",", "tag", "in", "enumerate", "(", "self", ".", "tag_dictionary", ".", "get_items", "(", ")", ")", ":", "\n", "                ", "if", "tag", "in", "loss_weights", ".", "keys", "(", ")", ":", "\n", "                    ", "weight_list", "[", "i", "]", "=", "loss_weights", "[", "tag", "]", "\n", "", "", "self", ".", "loss_weights", "=", "torch", ".", "FloatTensor", "(", "weight_list", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss_weights", "=", "None", "\n", "\n", "# initialize the network architecture", "\n", "", "self", ".", "nlayers", ":", "int", "=", "rnn_layers", "\n", "self", ".", "hidden_word", "=", "None", "\n", "\n", "# dropouts", "\n", "self", ".", "use_dropout", ":", "float", "=", "dropout", "\n", "self", ".", "use_word_dropout", ":", "float", "=", "word_dropout", "\n", "self", ".", "use_locked_dropout", ":", "float", "=", "locked_dropout", "\n", "\n", "self", ".", "pickle_module", "=", "pickle_module", "\n", "\n", "if", "dropout", ">", "0.0", ":", "\n", "            ", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "", "if", "word_dropout", ">", "0.0", ":", "\n", "            ", "self", ".", "word_dropout", "=", "flair", ".", "nn", ".", "WordDropout", "(", "word_dropout", ")", "\n", "\n", "", "if", "locked_dropout", ">", "0.0", ":", "\n", "            ", "self", ".", "locked_dropout", "=", "flair", ".", "nn", ".", "LockedDropout", "(", "locked_dropout", ")", "\n", "\n", "", "rnn_input_dim", ":", "int", "=", "self", ".", "embeddings", ".", "embedding_length", "\n", "\n", "self", ".", "relearn_embeddings", ":", "bool", "=", "True", "\n", "\n", "if", "self", ".", "relearn_embeddings", ":", "\n", "            ", "self", ".", "embedding2nn", "=", "torch", ".", "nn", ".", "Linear", "(", "rnn_input_dim", ",", "rnn_input_dim", ")", "\n", "\n", "", "self", ".", "train_initial_hidden_state", "=", "train_initial_hidden_state", "\n", "self", ".", "bidirectional", "=", "True", "\n", "self", ".", "rnn_type", "=", "rnn_type", "\n", "\n", "# bidirectional LSTM on top of embedding layer", "\n", "if", "self", ".", "use_rnn", ":", "\n", "            ", "num_directions", "=", "2", "if", "self", ".", "bidirectional", "else", "1", "\n", "\n", "if", "self", ".", "rnn_type", "in", "[", "\"LSTM\"", ",", "\"GRU\"", "]", ":", "\n", "\n", "                ", "self", ".", "rnn", "=", "getattr", "(", "torch", ".", "nn", ",", "self", ".", "rnn_type", ")", "(", "\n", "rnn_input_dim", ",", "\n", "hidden_size", ",", "\n", "num_layers", "=", "self", ".", "nlayers", ",", "\n", "dropout", "=", "0.0", "if", "self", ".", "nlayers", "==", "1", "else", "0.5", ",", "\n", "bidirectional", "=", "True", ",", "\n", "batch_first", "=", "True", ",", "\n", ")", "\n", "# Create initial hidden state and initialize it", "\n", "if", "self", ".", "train_initial_hidden_state", ":", "\n", "                    ", "self", ".", "hs_initializer", "=", "torch", ".", "nn", ".", "init", ".", "xavier_normal_", "\n", "\n", "self", ".", "lstm_init_h", "=", "Parameter", "(", "\n", "torch", ".", "randn", "(", "self", ".", "nlayers", "*", "num_directions", ",", "self", ".", "hidden_size", ")", ",", "\n", "requires_grad", "=", "True", ",", "\n", ")", "\n", "\n", "self", ".", "lstm_init_c", "=", "Parameter", "(", "\n", "torch", ".", "randn", "(", "self", ".", "nlayers", "*", "num_directions", ",", "self", ".", "hidden_size", ")", ",", "\n", "requires_grad", "=", "True", ",", "\n", ")", "\n", "\n", "# TODO: Decide how to initialize the hidden state variables", "\n", "# self.hs_initializer(self.lstm_init_h)", "\n", "# self.hs_initializer(self.lstm_init_c)", "\n", "\n", "# final linear map to tag space", "\n", "", "", "self", ".", "linear", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "hidden_size", "*", "num_directions", ",", "len", "(", "tag_dictionary", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "self", ".", "embeddings", ".", "embedding_length", ",", "len", "(", "tag_dictionary", ")", "\n", ")", "\n", "\n", "", "if", "self", ".", "use_crf", ":", "\n", "            ", "self", ".", "transitions", "=", "torch", ".", "nn", ".", "Parameter", "(", "\n", "torch", ".", "randn", "(", "self", ".", "tagset_size", ",", "self", ".", "tagset_size", ")", "\n", ")", "\n", "\n", "self", ".", "transitions", ".", "detach", "(", ")", "[", "\n", "self", ".", "tag_dictionary", ".", "get_idx_for_item", "(", "START_TAG", ")", ",", ":", "\n", "]", "=", "-", "10000", "\n", "\n", "self", ".", "transitions", ".", "detach", "(", ")", "[", "\n", ":", ",", "self", ".", "tag_dictionary", ".", "get_idx_for_item", "(", "STOP_TAG", ")", "\n", "]", "=", "-", "10000", "\n", "\n", "", "self", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._get_state_dict": [[219, 237], ["sequence_tagger_model.SequenceTagger.state_dict"], "methods", ["None"], ["", "def", "_get_state_dict", "(", "self", ")", ":", "\n", "        ", "model_state", "=", "{", "\n", "\"state_dict\"", ":", "self", ".", "state_dict", "(", ")", ",", "\n", "\"embeddings\"", ":", "self", ".", "embeddings", ",", "\n", "\"hidden_size\"", ":", "self", ".", "hidden_size", ",", "\n", "\"train_initial_hidden_state\"", ":", "self", ".", "train_initial_hidden_state", ",", "\n", "\"tag_dictionary\"", ":", "self", ".", "tag_dictionary", ",", "\n", "\"tag_type\"", ":", "self", ".", "tag_type", ",", "\n", "\"use_crf\"", ":", "self", ".", "use_crf", ",", "\n", "\"use_rnn\"", ":", "self", ".", "use_rnn", ",", "\n", "\"rnn_layers\"", ":", "self", ".", "rnn_layers", ",", "\n", "\"use_word_dropout\"", ":", "self", ".", "use_word_dropout", ",", "\n", "\"use_locked_dropout\"", ":", "self", ".", "use_locked_dropout", ",", "\n", "\"rnn_type\"", ":", "self", ".", "rnn_type", ",", "\n", "\"beta\"", ":", "self", ".", "beta", ",", "\n", "\"weight_dict\"", ":", "self", ".", "weight_dict", ",", "\n", "}", "\n", "return", "model_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._init_model_with_state_dict": [[238, 277], ["sequence_tagger_model.SequenceTagger", "SequenceTagger.load_state_dict", "state.keys", "state.keys", "state.keys", "state.keys", "state.keys", "state.keys", "state.keys"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_init_model_with_state_dict", "(", "state", ")", ":", "\n", "\n", "        ", "rnn_type", "=", "\"LSTM\"", "if", "\"rnn_type\"", "not", "in", "state", ".", "keys", "(", ")", "else", "state", "[", "\"rnn_type\"", "]", "\n", "use_dropout", "=", "0.0", "if", "\"use_dropout\"", "not", "in", "state", ".", "keys", "(", ")", "else", "state", "[", "\"use_dropout\"", "]", "\n", "use_word_dropout", "=", "(", "\n", "0.0", "if", "\"use_word_dropout\"", "not", "in", "state", ".", "keys", "(", ")", "else", "state", "[", "\"use_word_dropout\"", "]", "\n", ")", "\n", "use_locked_dropout", "=", "(", "\n", "0.0", "\n", "if", "\"use_locked_dropout\"", "not", "in", "state", ".", "keys", "(", ")", "\n", "else", "state", "[", "\"use_locked_dropout\"", "]", "\n", ")", "\n", "train_initial_hidden_state", "=", "(", "\n", "False", "\n", "if", "\"train_initial_hidden_state\"", "not", "in", "state", ".", "keys", "(", ")", "\n", "else", "state", "[", "\"train_initial_hidden_state\"", "]", "\n", ")", "\n", "beta", "=", "1.0", "if", "\"beta\"", "not", "in", "state", ".", "keys", "(", ")", "else", "state", "[", "\"beta\"", "]", "\n", "weights", "=", "None", "if", "\"weight_dict\"", "not", "in", "state", ".", "keys", "(", ")", "else", "state", "[", "\"weight_dict\"", "]", "\n", "\n", "model", "=", "SequenceTagger", "(", "\n", "hidden_size", "=", "state", "[", "\"hidden_size\"", "]", ",", "\n", "embeddings", "=", "state", "[", "\"embeddings\"", "]", ",", "\n", "tag_dictionary", "=", "state", "[", "\"tag_dictionary\"", "]", ",", "\n", "tag_type", "=", "state", "[", "\"tag_type\"", "]", ",", "\n", "use_crf", "=", "state", "[", "\"use_crf\"", "]", ",", "\n", "use_rnn", "=", "state", "[", "\"use_rnn\"", "]", ",", "\n", "rnn_layers", "=", "state", "[", "\"rnn_layers\"", "]", ",", "\n", "dropout", "=", "use_dropout", ",", "\n", "word_dropout", "=", "use_word_dropout", ",", "\n", "locked_dropout", "=", "use_locked_dropout", ",", "\n", "train_initial_hidden_state", "=", "train_initial_hidden_state", ",", "\n", "rnn_type", "=", "rnn_type", ",", "\n", "beta", "=", "beta", ",", "\n", "loss_weights", "=", "weights", ",", "\n", ")", "\n", "model", ".", "load_state_dict", "(", "state", "[", "\"state_dict\"", "]", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.predict": [[278, 383], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sorted", "sorted", "isinstance", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "enumerate", "isinstance", "isinstance", "log.warning", "range", "range", "flair.training_utils.store_embeddings", "flair.datasets.SentenceDataset", "flair.datasets.StringDataset", "sequence_tagger_model.SequenceTagger.transitions.detach().cpu().numpy", "tqdm.tqdm.tqdm", "sequence_tagger_model.SequenceTagger._filter_empty_sentences", "sequence_tagger_model.SequenceTagger.forward", "sequence_tagger_model.SequenceTagger._obtain_labels", "zip", "zip", "flair.training_utils.store_embeddings", "len", "len", "len", "len", "tqdm.tqdm.tqdm.set_description", "zip", "zip", "len", "sequence_tagger_model.SequenceTagger.transitions.detach().cpu", "token.add_tag_label", "token.add_tags_proba_dist", "sequence_tagger_model.SequenceTagger.transitions.detach"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._filter_empty_sentences", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._obtain_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tag_label", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tags_proba_dist"], ["", "def", "predict", "(", "\n", "self", ",", "\n", "sentences", ":", "Union", "[", "List", "[", "Sentence", "]", ",", "Sentence", ",", "List", "[", "str", "]", ",", "str", "]", ",", "\n", "mini_batch_size", "=", "32", ",", "\n", "embedding_storage_mode", "=", "\"none\"", ",", "\n", "all_tag_prob", ":", "bool", "=", "False", ",", "\n", "verbose", ":", "bool", "=", "False", ",", "\n", "use_tokenizer", ":", "Union", "[", "bool", ",", "Callable", "[", "[", "str", "]", ",", "List", "[", "Token", "]", "]", "]", "=", "space_tokenizer", ",", "\n", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "\"\"\"\n        Predict sequence tags for Named Entity Recognition task\n        :param sentences: a Sentence or a string or a List of Sentence or a List of string.\n        :param mini_batch_size: size of the minibatch, usually bigger is more rapid but consume more memory,\n        up to a point when it has no more effect.\n        :param embedding_storage_mode: 'none' for the minimum memory footprint, 'cpu' to store embeddings in Ram,\n        'gpu' to store embeddings in GPU memory.\n        :param all_tag_prob: True to compute the score for each tag on each token,\n        otherwise only the score of the best tag is returned\n        :param verbose: set to True to display a progress bar\n        :param use_tokenizer: a custom tokenizer when string are provided (default is space based tokenizer).\n        :return: List of Sentence enriched by the predicted tags\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "not", "sentences", ":", "\n", "                ", "return", "sentences", "\n", "\n", "", "if", "isinstance", "(", "sentences", ",", "Sentence", ")", "or", "isinstance", "(", "sentences", ",", "str", ")", ":", "\n", "                ", "sentences", "=", "[", "sentences", "]", "\n", "\n", "", "if", "(", "flair", ".", "device", ".", "type", "==", "\"cuda\"", ")", "and", "embedding_storage_mode", "==", "\"cpu\"", ":", "\n", "                ", "log", ".", "warning", "(", "\n", "\"You are inferring on GPU with parameter 'embedding_storage_mode' set to 'cpu'.\"", "\n", "\"This option will slow down your inference, usually 'none' (default value) \"", "\n", "\"is a better choice.\"", "\n", ")", "\n", "\n", "# reverse sort all sequences by their length", "\n", "", "rev_order_len_index", "=", "sorted", "(", "\n", "range", "(", "len", "(", "sentences", ")", ")", ",", "key", "=", "lambda", "k", ":", "len", "(", "sentences", "[", "k", "]", ")", ",", "reverse", "=", "True", "\n", ")", "\n", "original_order_index", "=", "sorted", "(", "\n", "range", "(", "len", "(", "rev_order_len_index", ")", ")", ",", "key", "=", "lambda", "k", ":", "rev_order_len_index", "[", "k", "]", "\n", ")", "\n", "\n", "reordered_sentences", ":", "List", "[", "Union", "[", "Sentence", ",", "str", "]", "]", "=", "[", "\n", "sentences", "[", "index", "]", "for", "index", "in", "rev_order_len_index", "\n", "]", "\n", "\n", "if", "isinstance", "(", "sentences", "[", "0", "]", ",", "Sentence", ")", ":", "\n", "# remove previous embeddings", "\n", "                ", "store_embeddings", "(", "reordered_sentences", ",", "\"none\"", ")", "\n", "dataset", "=", "SentenceDataset", "(", "reordered_sentences", ")", "\n", "", "else", ":", "\n", "                ", "dataset", "=", "StringDataset", "(", "\n", "reordered_sentences", ",", "use_tokenizer", "=", "use_tokenizer", "\n", ")", "\n", "", "dataloader", "=", "DataLoader", "(", "\n", "dataset", "=", "dataset", ",", "batch_size", "=", "mini_batch_size", ",", "collate_fn", "=", "lambda", "x", ":", "x", "\n", ")", "\n", "\n", "if", "self", ".", "use_crf", ":", "\n", "                ", "transitions", "=", "self", ".", "transitions", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                ", "transitions", "=", "None", "\n", "\n", "# progress bar for verbosity", "\n", "", "if", "verbose", ":", "\n", "                ", "dataloader", "=", "tqdm", "(", "dataloader", ")", "\n", "\n", "", "results", ":", "List", "[", "Sentence", "]", "=", "[", "]", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "\n", "                ", "if", "verbose", ":", "\n", "                    ", "dataloader", ".", "set_description", "(", "f\"Inferencing on batch {i}\"", ")", "\n", "", "results", "+=", "batch", "\n", "batch", "=", "self", ".", "_filter_empty_sentences", "(", "batch", ")", "\n", "# stop if all sentences are empty", "\n", "if", "not", "batch", ":", "\n", "                    ", "continue", "\n", "\n", "", "feature", ":", "torch", ".", "Tensor", "=", "self", ".", "forward", "(", "batch", ")", "\n", "tags", ",", "all_tags", "=", "self", ".", "_obtain_labels", "(", "\n", "feature", "=", "feature", ",", "\n", "batch_sentences", "=", "batch", ",", "\n", "transitions", "=", "transitions", ",", "\n", "get_all_tags", "=", "all_tag_prob", ",", "\n", ")", "\n", "\n", "for", "(", "sentence", ",", "sent_tags", ")", "in", "zip", "(", "batch", ",", "tags", ")", ":", "\n", "                    ", "for", "(", "token", ",", "tag", ")", "in", "zip", "(", "sentence", ".", "tokens", ",", "sent_tags", ")", ":", "\n", "                        ", "token", ".", "add_tag_label", "(", "self", ".", "tag_type", ",", "tag", ")", "\n", "\n", "# all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided", "\n", "", "", "for", "(", "sentence", ",", "sent_all_tags", ")", "in", "zip", "(", "batch", ",", "all_tags", ")", ":", "\n", "                    ", "for", "(", "token", ",", "token_all_tags", ")", "in", "zip", "(", "sentence", ".", "tokens", ",", "sent_all_tags", ")", ":", "\n", "                        ", "token", ".", "add_tags_proba_dist", "(", "self", ".", "tag_type", ",", "token_all_tags", ")", "\n", "\n", "# clearing token embeddings to save memory", "\n", "", "", "store_embeddings", "(", "batch", ",", "storage_mode", "=", "embedding_storage_mode", ")", "\n", "\n", "", "results", ":", "List", "[", "Union", "[", "Sentence", ",", "str", "]", "]", "=", "[", "\n", "results", "[", "index", "]", "for", "index", "in", "original_order_index", "\n", "]", "\n", "assert", "len", "(", "sentences", ")", "==", "len", "(", "results", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.evaluate": [[384, 490], ["type", "pathlib.Path", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "flair.training_utils.Metric", "flair.training_utils.Metric.get_classes", "flair.training_utils.Result", "sequence_tagger_model.SequenceTagger.transitions.detach().cpu().numpy", "zip", "flair.training_utils.store_embeddings", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sequence_tagger_model.SequenceTagger.forward", "sequence_tagger_model.SequenceTagger._calculate_loss", "sequence_tagger_model.SequenceTagger._obtain_labels", "zip", "lines.append", "open", "outfile.write", "flair.training_utils.Metric.micro_avg_accuracy", "flair.training_utils.Metric.micro_avg_f_score", "flair.training_utils.Metric.macro_avg_accuracy", "flair.training_utils.Metric.macro_avg_f_score", "flair.training_utils.Metric.micro_avg_f_score", "sequence_tagger_model.SequenceTagger.transitions.detach().cpu", "token.add_tag", "lines.append", "flair.training_utils.Metric.get_tp", "flair.training_utils.Metric.get_fp", "flair.training_utils.Metric.get_fn", "flair.training_utils.Metric.get_tn", "flair.training_utils.Metric.precision", "flair.training_utils.Metric.recall", "flair.training_utils.Metric.accuracy", "flair.training_utils.Metric.f_score", "sentence.get_spans", "sentence.get_spans", "flair.training_utils.Metric.add_tp", "flair.training_utils.Metric.add_fp", "flair.training_utils.Metric.add_fn", "flair.training_utils.Metric.add_tn", "flair.training_utils.Metric.precision", "flair.training_utils.Metric.recall", "flair.training_utils.Metric.micro_avg_f_score", "sequence_tagger_model.SequenceTagger.transitions.detach", "token.get_tag"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_classes", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.store_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._calculate_loss", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._obtain_labels", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.micro_avg_accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.micro_avg_f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.macro_avg_accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.macro_avg_f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.micro_avg_f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.add_tag", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_fn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.get_tn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.precision", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.recall", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.accuracy", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_spans", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_spans", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_tp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_fp", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_fn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.add_tn", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.precision", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.Metric.recall", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.training_utils.MetricRegression.micro_avg_f_score", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag"], ["", "", "def", "evaluate", "(", "\n", "self", ",", "\n", "data_loader", ":", "DataLoader", ",", "\n", "out_path", ":", "Path", "=", "None", ",", "\n", "embedding_storage_mode", ":", "str", "=", "\"none\"", ",", "\n", ")", "->", "(", "Result", ",", "float", ")", ":", "\n", "\n", "        ", "if", "type", "(", "out_path", ")", "==", "str", ":", "\n", "            ", "out_path", "=", "Path", "(", "out_path", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "eval_loss", "=", "0", "\n", "\n", "batch_no", ":", "int", "=", "0", "\n", "\n", "metric", "=", "Metric", "(", "\"Evaluation\"", ",", "beta", "=", "self", ".", "beta", ")", "\n", "\n", "lines", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "if", "self", ".", "use_crf", ":", "\n", "                ", "transitions", "=", "self", ".", "transitions", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                ", "transitions", "=", "None", "\n", "\n", "", "for", "batch", "in", "data_loader", ":", "\n", "                ", "batch_no", "+=", "1", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "features", "=", "self", ".", "forward", "(", "batch", ")", "\n", "loss", "=", "self", ".", "_calculate_loss", "(", "features", ",", "batch", ")", "\n", "tags", ",", "_", "=", "self", ".", "_obtain_labels", "(", "\n", "feature", "=", "features", ",", "\n", "batch_sentences", "=", "batch", ",", "\n", "transitions", "=", "transitions", ",", "\n", "get_all_tags", "=", "False", ",", "\n", ")", "\n", "\n", "", "eval_loss", "+=", "loss", "\n", "\n", "for", "(", "sentence", ",", "sent_tags", ")", "in", "zip", "(", "batch", ",", "tags", ")", ":", "\n", "                    ", "for", "(", "token", ",", "tag", ")", "in", "zip", "(", "sentence", ".", "tokens", ",", "sent_tags", ")", ":", "\n", "                        ", "token", ":", "Token", "=", "token", "\n", "token", ".", "add_tag", "(", "\"predicted\"", ",", "tag", ".", "value", ",", "tag", ".", "score", ")", "\n", "\n", "# append both to file for evaluation", "\n", "eval_line", "=", "\"{} {} {} {}\\n\"", ".", "format", "(", "\n", "token", ".", "text", ",", "\n", "token", ".", "get_tag", "(", "self", ".", "tag_type", ")", ".", "value", ",", "\n", "tag", ".", "value", ",", "\n", "tag", ".", "score", ",", "\n", ")", "\n", "lines", ".", "append", "(", "eval_line", ")", "\n", "", "lines", ".", "append", "(", "\"\\n\"", ")", "\n", "\n", "", "for", "sentence", "in", "batch", ":", "\n", "# make list of gold tags", "\n", "                    ", "gold_tags", "=", "[", "\n", "(", "tag", ".", "tag", ",", "tag", ".", "text", ")", "for", "tag", "in", "sentence", ".", "get_spans", "(", "self", ".", "tag_type", ")", "\n", "]", "\n", "# make list of predicted tags", "\n", "predicted_tags", "=", "[", "\n", "(", "tag", ".", "tag", ",", "tag", ".", "text", ")", "for", "tag", "in", "sentence", ".", "get_spans", "(", "\"predicted\"", ")", "\n", "]", "\n", "\n", "# check for true positives, false positives and false negatives", "\n", "for", "tag", ",", "prediction", "in", "predicted_tags", ":", "\n", "                        ", "if", "(", "tag", ",", "prediction", ")", "in", "gold_tags", ":", "\n", "                            ", "metric", ".", "add_tp", "(", "tag", ")", "\n", "", "else", ":", "\n", "                            ", "metric", ".", "add_fp", "(", "tag", ")", "\n", "\n", "", "", "for", "tag", ",", "gold", "in", "gold_tags", ":", "\n", "                        ", "if", "(", "tag", ",", "gold", ")", "not", "in", "predicted_tags", ":", "\n", "                            ", "metric", ".", "add_fn", "(", "tag", ")", "\n", "", "else", ":", "\n", "                            ", "metric", ".", "add_tn", "(", "tag", ")", "\n", "\n", "", "", "", "store_embeddings", "(", "batch", ",", "embedding_storage_mode", ")", "\n", "\n", "", "eval_loss", "/=", "batch_no", "\n", "\n", "if", "out_path", "is", "not", "None", ":", "\n", "                ", "with", "open", "(", "out_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "outfile", ":", "\n", "                    ", "outfile", ".", "write", "(", "\"\"", ".", "join", "(", "lines", ")", ")", "\n", "\n", "", "", "detailed_result", "=", "(", "\n", "f\"\\nMICRO_AVG: acc {metric.micro_avg_accuracy():.4f} - f1-score {metric.micro_avg_f_score():.4f}\"", "\n", "f\"\\nMACRO_AVG: acc {metric.macro_avg_accuracy():.4f} - f1-score {metric.macro_avg_f_score():.4f}\"", "\n", ")", "\n", "for", "class_name", "in", "metric", ".", "get_classes", "(", ")", ":", "\n", "                ", "detailed_result", "+=", "(", "\n", "f\"\\n{class_name:<10} tp: {metric.get_tp(class_name)} - fp: {metric.get_fp(class_name)} - \"", "\n", "f\"fn: {metric.get_fn(class_name)} - tn: {metric.get_tn(class_name)} - precision: \"", "\n", "f\"{metric.precision(class_name):.4f} - recall: {metric.recall(class_name):.4f} - \"", "\n", "f\"accuracy: {metric.accuracy(class_name):.4f} - f1-score: \"", "\n", "f\"{metric.f_score(class_name):.4f}\"", "\n", ")", "\n", "\n", "", "result", "=", "Result", "(", "\n", "main_score", "=", "metric", ".", "micro_avg_f_score", "(", ")", ",", "\n", "log_line", "=", "f\"{metric.precision():.4f}\\t{metric.recall():.4f}\\t{metric.micro_avg_f_score():.4f}\"", ",", "\n", "log_header", "=", "\"PRECISION\\tRECALL\\tF1\"", ",", "\n", "detailed_results", "=", "detailed_result", ",", "\n", ")", "\n", "\n", "return", "result", ",", "eval_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.forward_loss": [[491, 496], ["sequence_tagger_model.SequenceTagger.forward", "sequence_tagger_model.SequenceTagger._calculate_loss"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._calculate_loss"], ["", "", "def", "forward_loss", "(", "\n", "self", ",", "data_points", ":", "Union", "[", "List", "[", "Sentence", "]", ",", "Sentence", "]", ",", "sort", "=", "True", "\n", ")", "->", "torch", ".", "tensor", ":", "\n", "        ", "features", "=", "self", ".", "forward", "(", "data_points", ")", "\n", "return", "self", ".", "_calculate_loss", "(", "features", ",", "data_points", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.forward": [[497, 574], ["sequence_tagger_model.SequenceTagger.embeddings.embed", "max", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "list", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "sequence_tagger_model.SequenceTagger.linear", "len", "sequence_tagger_model.SequenceTagger.dropout", "sequence_tagger_model.SequenceTagger.word_dropout", "sequence_tagger_model.SequenceTagger.locked_dropout", "sequence_tagger_model.SequenceTagger.embedding2nn", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "len", "list.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "sequence_tagger_model.SequenceTagger.rnn", "sequence_tagger_model.SequenceTagger.rnn", "sequence_tagger_model.SequenceTagger.dropout", "sequence_tagger_model.SequenceTagger.locked_dropout", "token.get_each_embedding", "sequence_tagger_model.SequenceTagger.lstm_init_h.unsqueeze().repeat", "sequence_tagger_model.SequenceTagger.lstm_init_c.unsqueeze().repeat", "len", "len", "sequence_tagger_model.SequenceTagger.lstm_init_h.unsqueeze", "sequence_tagger_model.SequenceTagger.lstm_init_c.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_each_embedding"], ["", "def", "forward", "(", "self", ",", "sentences", ":", "List", "[", "Sentence", "]", ")", ":", "\n", "\n", "        ", "self", ".", "embeddings", ".", "embed", "(", "sentences", ")", "\n", "\n", "lengths", ":", "List", "[", "int", "]", "=", "[", "len", "(", "sentence", ".", "tokens", ")", "for", "sentence", "in", "sentences", "]", "\n", "longest_token_sequence_in_batch", ":", "int", "=", "max", "(", "lengths", ")", "\n", "\n", "pre_allocated_zero_tensor", "=", "torch", ".", "zeros", "(", "\n", "self", ".", "embeddings", ".", "embedding_length", "*", "longest_token_sequence_in_batch", ",", "\n", "dtype", "=", "torch", ".", "float", ",", "\n", "device", "=", "flair", ".", "device", ",", "\n", ")", "\n", "\n", "all_embs", "=", "list", "(", ")", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "all_embs", "+=", "[", "\n", "emb", "for", "token", "in", "sentence", "for", "emb", "in", "token", ".", "get_each_embedding", "(", ")", "\n", "]", "\n", "nb_padding_tokens", "=", "longest_token_sequence_in_batch", "-", "len", "(", "sentence", ")", "\n", "\n", "if", "nb_padding_tokens", ">", "0", ":", "\n", "                ", "t", "=", "pre_allocated_zero_tensor", "[", "\n", ":", "self", ".", "embeddings", ".", "embedding_length", "*", "nb_padding_tokens", "\n", "]", "\n", "all_embs", ".", "append", "(", "t", ")", "\n", "\n", "", "", "sentence_tensor", "=", "torch", ".", "cat", "(", "all_embs", ")", ".", "view", "(", "\n", "[", "\n", "len", "(", "sentences", ")", ",", "\n", "longest_token_sequence_in_batch", ",", "\n", "self", ".", "embeddings", ".", "embedding_length", ",", "\n", "]", "\n", ")", "\n", "\n", "# --------------------------------------------------------------------", "\n", "# FF PART", "\n", "# --------------------------------------------------------------------", "\n", "if", "self", ".", "use_dropout", ">", "0.0", ":", "\n", "            ", "sentence_tensor", "=", "self", ".", "dropout", "(", "sentence_tensor", ")", "\n", "", "if", "self", ".", "use_word_dropout", ">", "0.0", ":", "\n", "            ", "sentence_tensor", "=", "self", ".", "word_dropout", "(", "sentence_tensor", ")", "\n", "", "if", "self", ".", "use_locked_dropout", ">", "0.0", ":", "\n", "            ", "sentence_tensor", "=", "self", ".", "locked_dropout", "(", "sentence_tensor", ")", "\n", "\n", "", "if", "self", ".", "relearn_embeddings", ":", "\n", "            ", "sentence_tensor", "=", "self", ".", "embedding2nn", "(", "sentence_tensor", ")", "\n", "\n", "", "if", "self", ".", "use_rnn", ":", "\n", "            ", "packed", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "\n", "sentence_tensor", ",", "lengths", ",", "enforce_sorted", "=", "False", ",", "batch_first", "=", "True", "\n", ")", "\n", "\n", "# if initial hidden state is trainable, use this state", "\n", "if", "self", ".", "train_initial_hidden_state", ":", "\n", "                ", "initial_hidden_state", "=", "[", "\n", "self", ".", "lstm_init_h", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "len", "(", "sentences", ")", ",", "1", ")", ",", "\n", "self", ".", "lstm_init_c", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "len", "(", "sentences", ")", ",", "1", ")", ",", "\n", "]", "\n", "rnn_output", ",", "hidden", "=", "self", ".", "rnn", "(", "packed", ",", "initial_hidden_state", ")", "\n", "", "else", ":", "\n", "                ", "rnn_output", ",", "hidden", "=", "self", ".", "rnn", "(", "packed", ")", "\n", "\n", "", "sentence_tensor", ",", "output_lengths", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "\n", "rnn_output", ",", "batch_first", "=", "True", "\n", ")", "\n", "\n", "if", "self", ".", "use_dropout", ">", "0.0", ":", "\n", "                ", "sentence_tensor", "=", "self", ".", "dropout", "(", "sentence_tensor", ")", "\n", "# word dropout only before LSTM - TODO: more experimentation needed", "\n", "# if self.use_word_dropout > 0.0:", "\n", "#     sentence_tensor = self.word_dropout(sentence_tensor)", "\n", "", "if", "self", ".", "use_locked_dropout", ">", "0.0", ":", "\n", "                ", "sentence_tensor", "=", "self", ".", "locked_dropout", "(", "sentence_tensor", ")", "\n", "\n", "", "", "features", "=", "self", ".", "linear", "(", "sentence_tensor", ")", "\n", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._score_sentence": [[575, 607], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "start[].repeat", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "stop[].repeat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "range", "len", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_idx_for_item", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_idx_for_item", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_idx_for_item", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "range"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item"], ["", "def", "_score_sentence", "(", "self", ",", "feats", ",", "tags", ",", "lens_", ")", ":", "\n", "\n", "        ", "start", "=", "torch", ".", "tensor", "(", "\n", "[", "self", ".", "tag_dictionary", ".", "get_idx_for_item", "(", "START_TAG", ")", "]", ",", "device", "=", "flair", ".", "device", "\n", ")", "\n", "start", "=", "start", "[", "None", ",", ":", "]", ".", "repeat", "(", "tags", ".", "shape", "[", "0", "]", ",", "1", ")", "\n", "\n", "stop", "=", "torch", ".", "tensor", "(", "\n", "[", "self", ".", "tag_dictionary", ".", "get_idx_for_item", "(", "STOP_TAG", ")", "]", ",", "device", "=", "flair", ".", "device", "\n", ")", "\n", "stop", "=", "stop", "[", "None", ",", ":", "]", ".", "repeat", "(", "tags", ".", "shape", "[", "0", "]", ",", "1", ")", "\n", "\n", "pad_start_tags", "=", "torch", ".", "cat", "(", "[", "start", ",", "tags", "]", ",", "1", ")", "\n", "pad_stop_tags", "=", "torch", ".", "cat", "(", "[", "tags", ",", "stop", "]", ",", "1", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "lens_", ")", ")", ":", "\n", "            ", "pad_stop_tags", "[", "i", ",", "lens_", "[", "i", "]", ":", "]", "=", "self", ".", "tag_dictionary", ".", "get_idx_for_item", "(", "\n", "STOP_TAG", "\n", ")", "\n", "\n", "", "score", "=", "torch", ".", "FloatTensor", "(", "feats", ".", "shape", "[", "0", "]", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "for", "i", "in", "range", "(", "feats", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "r", "=", "torch", ".", "LongTensor", "(", "range", "(", "lens_", "[", "i", "]", ")", ")", ".", "to", "(", "flair", ".", "device", ")", "\n", "\n", "score", "[", "i", "]", "=", "torch", ".", "sum", "(", "\n", "self", ".", "transitions", "[", "\n", "pad_stop_tags", "[", "i", ",", ":", "lens_", "[", "i", "]", "+", "1", "]", ",", "pad_start_tags", "[", "i", ",", ":", "lens_", "[", "i", "]", "+", "1", "]", "\n", "]", "\n", ")", "+", "torch", ".", "sum", "(", "feats", "[", "i", ",", "r", ",", "tags", "[", "i", ",", ":", "lens_", "[", "i", "]", "]", "]", ")", "\n", "\n", "", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._calculate_loss": [[608, 647], ["enumerate", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "tag_list.append", "sequence_tagger_model.pad_tensors", "sequence_tagger_model.SequenceTagger._forward_alg", "sequence_tagger_model.SequenceTagger._score_sentence", "score.mean", "zip", "len", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_idx_for_item", "torch.nn.functional.cross_entropy", "torch.nn.functional.cross_entropy", "torch.nn.functional.cross_entropy", "torch.nn.functional.cross_entropy", "torch.nn.functional.cross_entropy", "torch.nn.functional.cross_entropy", "torch.nn.functional.cross_entropy", "torch.nn.functional.cross_entropy", "torch.nn.functional.cross_entropy", "token.get_tag"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.pad_tensors", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._forward_alg", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._score_sentence", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Token.get_tag"], ["", "def", "_calculate_loss", "(", "\n", "self", ",", "features", ":", "torch", ".", "tensor", ",", "sentences", ":", "List", "[", "Sentence", "]", "\n", ")", "->", "float", ":", "\n", "\n", "        ", "lengths", ":", "List", "[", "int", "]", "=", "[", "len", "(", "sentence", ".", "tokens", ")", "for", "sentence", "in", "sentences", "]", "\n", "\n", "tag_list", ":", "List", "=", "[", "]", "\n", "for", "s_id", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "# get the tags in this sentence", "\n", "            ", "tag_idx", ":", "List", "[", "int", "]", "=", "[", "\n", "self", ".", "tag_dictionary", ".", "get_idx_for_item", "(", "token", ".", "get_tag", "(", "self", ".", "tag_type", ")", ".", "value", ")", "\n", "for", "token", "in", "sentence", "\n", "]", "\n", "# add tags as tensor", "\n", "tag", "=", "torch", ".", "tensor", "(", "tag_idx", ",", "device", "=", "flair", ".", "device", ")", "\n", "tag_list", ".", "append", "(", "tag", ")", "\n", "\n", "", "if", "self", ".", "use_crf", ":", "\n", "# pad tags if using batch-CRF decoder", "\n", "            ", "tags", ",", "_", "=", "pad_tensors", "(", "tag_list", ")", "\n", "\n", "forward_score", "=", "self", ".", "_forward_alg", "(", "features", ",", "lengths", ")", "\n", "gold_score", "=", "self", ".", "_score_sentence", "(", "features", ",", "tags", ",", "lengths", ")", "\n", "\n", "score", "=", "forward_score", "-", "gold_score", "\n", "\n", "return", "score", ".", "mean", "(", ")", "\n", "\n", "", "else", ":", "\n", "            ", "score", "=", "0", "\n", "for", "sentence_feats", ",", "sentence_tags", ",", "sentence_length", "in", "zip", "(", "\n", "features", ",", "tag_list", ",", "lengths", "\n", ")", ":", "\n", "                ", "sentence_feats", "=", "sentence_feats", "[", ":", "sentence_length", "]", "\n", "score", "+=", "torch", ".", "nn", ".", "functional", ".", "cross_entropy", "(", "\n", "sentence_feats", ",", "sentence_tags", ",", "weight", "=", "self", ".", "loss_weights", "\n", ")", "\n", "", "score", "/=", "len", "(", "features", ")", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._obtain_labels": [[648, 710], ["zip.cpu", "zip", "len", "zip.numpy", "enumerate", "torch.softmax().cpu", "torch.softmax().cpu", "torch.softmax().cpu", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "zip", "tags.append", "sequence_tagger_model.SequenceTagger._viterbi_decode", "score[].tolist", "prediction[].tolist", "softmax[].tolist", "all_tags.append", "torch.softmax", "torch.softmax", "torch.softmax", "flair.data.Label", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_item_for_index", "zip", "flair.data.Label", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_item_for_index", "enumerate"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._viterbi_decode", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_item_for_index", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_item_for_index"], ["", "", "def", "_obtain_labels", "(", "\n", "self", ",", "\n", "feature", ":", "torch", ".", "Tensor", ",", "\n", "batch_sentences", ":", "List", "[", "Sentence", "]", ",", "\n", "transitions", ":", "Optional", "[", "np", ".", "ndarray", "]", ",", "\n", "get_all_tags", ":", "bool", ",", "\n", ")", "->", "(", "List", "[", "List", "[", "Label", "]", "]", ",", "List", "[", "List", "[", "List", "[", "Label", "]", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Returns a tuple of two lists:\n         - The first list corresponds to the most likely `Label` per token in each sentence.\n         - The second list contains a probability distribution over all `Labels` for each token\n           in a sentence for all sentences.\n        \"\"\"", "\n", "\n", "lengths", ":", "List", "[", "int", "]", "=", "[", "len", "(", "sentence", ".", "tokens", ")", "for", "sentence", "in", "batch_sentences", "]", "\n", "\n", "tags", "=", "[", "]", "\n", "all_tags", "=", "[", "]", "\n", "feature", "=", "feature", ".", "cpu", "(", ")", "\n", "if", "self", ".", "use_crf", ":", "\n", "            ", "feature", "=", "feature", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "            ", "for", "index", ",", "length", "in", "enumerate", "(", "lengths", ")", ":", "\n", "                ", "feature", "[", "index", ",", "length", ":", "]", "=", "0", "\n", "", "softmax_batch", "=", "F", ".", "softmax", "(", "feature", ",", "dim", "=", "2", ")", ".", "cpu", "(", ")", "\n", "scores_batch", ",", "prediction_batch", "=", "torch", ".", "max", "(", "softmax_batch", ",", "dim", "=", "2", ")", "\n", "feature", "=", "zip", "(", "softmax_batch", ",", "scores_batch", ",", "prediction_batch", ")", "\n", "\n", "", "for", "feats", ",", "length", "in", "zip", "(", "feature", ",", "lengths", ")", ":", "\n", "            ", "if", "self", ".", "use_crf", ":", "\n", "                ", "confidences", ",", "tag_seq", ",", "scores", "=", "self", ".", "_viterbi_decode", "(", "\n", "feats", "=", "feats", "[", ":", "length", "]", ",", "\n", "transitions", "=", "transitions", ",", "\n", "all_scores", "=", "get_all_tags", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "softmax", ",", "score", ",", "prediction", "=", "feats", "\n", "confidences", "=", "score", "[", ":", "length", "]", ".", "tolist", "(", ")", "\n", "tag_seq", "=", "prediction", "[", ":", "length", "]", ".", "tolist", "(", ")", "\n", "scores", "=", "softmax", "[", ":", "length", "]", ".", "tolist", "(", ")", "\n", "\n", "", "tags", ".", "append", "(", "\n", "[", "\n", "Label", "(", "self", ".", "tag_dictionary", ".", "get_item_for_index", "(", "tag", ")", ",", "conf", ")", "\n", "for", "conf", ",", "tag", "in", "zip", "(", "confidences", ",", "tag_seq", ")", "\n", "]", "\n", ")", "\n", "\n", "if", "get_all_tags", ":", "\n", "                ", "all_tags", ".", "append", "(", "\n", "[", "\n", "[", "\n", "Label", "(", "\n", "self", ".", "tag_dictionary", ".", "get_item_for_index", "(", "score_id", ")", ",", "score", "\n", ")", "\n", "for", "score_id", ",", "score", "in", "enumerate", "(", "score_dist", ")", "\n", "]", "\n", "for", "score_dist", "in", "scores", "\n", "]", "\n", ")", "\n", "\n", "", "", "return", "tags", ",", "all_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._softmax": [[711, 717], ["numpy.exp", "x.max", "numpy.exp.sum"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_softmax", "(", "x", ",", "axis", ")", ":", "\n", "# reduce raw values to avoid NaN during exp", "\n", "        ", "x_norm", "=", "x", "-", "x", ".", "max", "(", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "y", "=", "np", ".", "exp", "(", "x_norm", ")", "\n", "return", "y", "/", "y", ".", "sum", "(", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._viterbi_decode": [[718, 787], ["sequence_tagger_model.SequenceTagger.tag_dictionary.get_idx_for_item", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_idx_for_item", "numpy.empty", "numpy.empty", "numpy.expand_dims().astype", "enumerate", "terminal_var.argmax", "reversed", "best_path.pop", "best_path.reverse", "sequence_tagger_model.SequenceTagger._softmax", "numpy.max", "numpy.zeros", "next_tag_var.argmax", "forward_var.squeeze", "best_path.append", "enumerate", "numpy.max.tolist", "numpy.zeros.tolist", "numpy.expand_dims", "zip", "numpy.repeat", "tag_scores.argmax", "numpy.arange", "type", "tag_id.item", "tag_scores.argmax", "tag_scores.argmax", "type", "tag_scores.argmax", "tag_id.item", "tag_id.item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.argmax", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._softmax", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.argmax", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.argmax", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.argmax", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.argmax", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.argmax"], ["", "def", "_viterbi_decode", "(", "\n", "self", ",", "feats", ":", "np", ".", "ndarray", ",", "transitions", ":", "np", ".", "ndarray", ",", "all_scores", ":", "bool", "\n", ")", ":", "\n", "        ", "id_start", "=", "self", ".", "tag_dictionary", ".", "get_idx_for_item", "(", "START_TAG", ")", "\n", "id_stop", "=", "self", ".", "tag_dictionary", ".", "get_idx_for_item", "(", "STOP_TAG", ")", "\n", "\n", "backpointers", "=", "np", ".", "empty", "(", "shape", "=", "(", "feats", ".", "shape", "[", "0", "]", ",", "self", ".", "tagset_size", ")", ",", "dtype", "=", "np", ".", "int_", ")", "\n", "backscores", "=", "np", ".", "empty", "(", "\n", "shape", "=", "(", "feats", ".", "shape", "[", "0", "]", ",", "self", ".", "tagset_size", ")", ",", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "\n", "init_vvars", "=", "np", ".", "expand_dims", "(", "\n", "np", ".", "repeat", "(", "-", "10000.0", ",", "self", ".", "tagset_size", ")", ",", "axis", "=", "0", "\n", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "init_vvars", "[", "0", "]", "[", "id_start", "]", "=", "0", "\n", "\n", "forward_var", "=", "init_vvars", "\n", "for", "index", ",", "feat", "in", "enumerate", "(", "feats", ")", ":", "\n", "# broadcasting will do the job of reshaping and is more efficient than calling repeat", "\n", "            ", "next_tag_var", "=", "forward_var", "+", "transitions", "\n", "bptrs_t", "=", "next_tag_var", ".", "argmax", "(", "axis", "=", "1", ")", "\n", "viterbivars_t", "=", "next_tag_var", "[", "np", ".", "arange", "(", "bptrs_t", ".", "shape", "[", "0", "]", ")", ",", "bptrs_t", "]", "\n", "forward_var", "=", "viterbivars_t", "+", "feat", "\n", "backscores", "[", "index", "]", "=", "forward_var", "\n", "forward_var", "=", "forward_var", "[", "np", ".", "newaxis", ",", ":", "]", "\n", "backpointers", "[", "index", "]", "=", "bptrs_t", "\n", "\n", "", "terminal_var", "=", "forward_var", ".", "squeeze", "(", ")", "+", "transitions", "[", "id_stop", "]", "\n", "terminal_var", "[", "id_stop", "]", "=", "-", "10000.0", "\n", "terminal_var", "[", "id_start", "]", "=", "-", "10000.0", "\n", "best_tag_id", "=", "terminal_var", ".", "argmax", "(", ")", "\n", "\n", "best_path", "=", "[", "best_tag_id", "]", "\n", "for", "bptrs_t", "in", "reversed", "(", "backpointers", ")", ":", "\n", "            ", "best_tag_id", "=", "bptrs_t", "[", "best_tag_id", "]", "\n", "best_path", ".", "append", "(", "best_tag_id", ")", "\n", "\n", "", "start", "=", "best_path", ".", "pop", "(", ")", "\n", "assert", "start", "==", "id_start", "\n", "best_path", ".", "reverse", "(", ")", "\n", "\n", "best_scores_softmax", "=", "self", ".", "_softmax", "(", "backscores", ",", "axis", "=", "1", ")", "\n", "best_scores_np", "=", "np", ".", "max", "(", "best_scores_softmax", ",", "axis", "=", "1", ")", "\n", "\n", "# default value", "\n", "all_scores_np", "=", "np", ".", "zeros", "(", "0", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "if", "all_scores", ":", "\n", "            ", "all_scores_np", "=", "best_scores_softmax", "\n", "for", "index", ",", "(", "tag_id", ",", "tag_scores", ")", "in", "enumerate", "(", "zip", "(", "best_path", ",", "all_scores_np", ")", ")", ":", "\n", "                ", "if", "type", "(", "tag_id", ")", "!=", "int", "and", "tag_id", ".", "item", "(", ")", "!=", "tag_scores", ".", "argmax", "(", ")", ":", "\n", "                    ", "swap_index_score", "=", "tag_scores", ".", "argmax", "(", ")", "\n", "(", "\n", "all_scores_np", "[", "index", "]", "[", "tag_id", ".", "item", "(", ")", "]", ",", "\n", "all_scores_np", "[", "index", "]", "[", "swap_index_score", "]", ",", "\n", ")", "=", "(", "\n", "all_scores_np", "[", "index", "]", "[", "swap_index_score", "]", ",", "\n", "all_scores_np", "[", "index", "]", "[", "tag_id", ".", "item", "(", ")", "]", ",", "\n", ")", "\n", "", "elif", "type", "(", "tag_id", ")", "==", "int", "and", "tag_id", "!=", "tag_scores", ".", "argmax", "(", ")", ":", "\n", "                    ", "swap_index_score", "=", "tag_scores", ".", "argmax", "(", ")", "\n", "(", "\n", "all_scores_np", "[", "index", "]", "[", "tag_id", "]", ",", "\n", "all_scores_np", "[", "index", "]", "[", "swap_index_score", "]", ",", "\n", ")", "=", "(", "\n", "all_scores_np", "[", "index", "]", "[", "swap_index_score", "]", ",", "\n", "all_scores_np", "[", "index", "]", "[", "tag_id", "]", ",", "\n", ")", "\n", "\n", "", "", "", "return", "best_scores_np", ".", "tolist", "(", ")", ",", "best_path", ",", "all_scores_np", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._forward_alg": [[788, 840], ["torch.FloatTensor().fill_", "torch.FloatTensor().fill_", "torch.FloatTensor().fill_", "torch.FloatTensor().fill_", "torch.FloatTensor().fill_", "torch.FloatTensor().fill_", "torch.FloatTensor().fill_", "torch.FloatTensor().fill_", "torch.FloatTensor().fill_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "init_alphas[].repeat", "sequence_tagger_model.SequenceTagger.transitions.view().repeat", "range", "sequence_tagger_model.log_sum_exp_batch", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.zeros.clone", "torch.zeros.clone", "torch.zeros.clone", "[].repeat", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_idx_for_item", "sequence_tagger_model.SequenceTagger.transitions.view", "[].repeat().transpose", "max_tag_var[].repeat", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "emit_score[].repeat", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "range", "[].repeat", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_idx_for_item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.log_sum_exp_batch", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_idx_for_item"], ["", "def", "_forward_alg", "(", "self", ",", "feats", ",", "lens_", ")", ":", "\n", "\n", "        ", "init_alphas", "=", "torch", ".", "FloatTensor", "(", "self", ".", "tagset_size", ")", ".", "fill_", "(", "-", "10000.0", ")", "\n", "init_alphas", "[", "self", ".", "tag_dictionary", ".", "get_idx_for_item", "(", "START_TAG", ")", "]", "=", "0.0", "\n", "\n", "forward_var", "=", "torch", ".", "zeros", "(", "\n", "feats", ".", "shape", "[", "0", "]", ",", "\n", "feats", ".", "shape", "[", "1", "]", "+", "1", ",", "\n", "feats", ".", "shape", "[", "2", "]", ",", "\n", "dtype", "=", "torch", ".", "float", ",", "\n", "device", "=", "flair", ".", "device", ",", "\n", ")", "\n", "\n", "forward_var", "[", ":", ",", "0", ",", ":", "]", "=", "init_alphas", "[", "None", ",", ":", "]", ".", "repeat", "(", "feats", ".", "shape", "[", "0", "]", ",", "1", ")", "\n", "\n", "transitions", "=", "self", ".", "transitions", ".", "view", "(", "\n", "1", ",", "self", ".", "transitions", ".", "shape", "[", "0", "]", ",", "self", ".", "transitions", ".", "shape", "[", "1", "]", "\n", ")", ".", "repeat", "(", "feats", ".", "shape", "[", "0", "]", ",", "1", ",", "1", ")", "\n", "\n", "for", "i", "in", "range", "(", "feats", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "emit_score", "=", "feats", "[", ":", ",", "i", ",", ":", "]", "\n", "\n", "tag_var", "=", "(", "\n", "emit_score", "[", ":", ",", ":", ",", "None", "]", ".", "repeat", "(", "1", ",", "1", ",", "transitions", ".", "shape", "[", "2", "]", ")", "\n", "+", "transitions", "\n", "+", "forward_var", "[", ":", ",", "i", ",", ":", "]", "[", ":", ",", ":", ",", "None", "]", "\n", ".", "repeat", "(", "1", ",", "1", ",", "transitions", ".", "shape", "[", "2", "]", ")", "\n", ".", "transpose", "(", "2", ",", "1", ")", "\n", ")", "\n", "\n", "max_tag_var", ",", "_", "=", "torch", ".", "max", "(", "tag_var", ",", "dim", "=", "2", ")", "\n", "\n", "tag_var", "=", "tag_var", "-", "max_tag_var", "[", ":", ",", ":", ",", "None", "]", ".", "repeat", "(", "\n", "1", ",", "1", ",", "transitions", ".", "shape", "[", "2", "]", "\n", ")", "\n", "\n", "agg_", "=", "torch", ".", "log", "(", "torch", ".", "sum", "(", "torch", ".", "exp", "(", "tag_var", ")", ",", "dim", "=", "2", ")", ")", "\n", "\n", "cloned", "=", "forward_var", ".", "clone", "(", ")", "\n", "cloned", "[", ":", ",", "i", "+", "1", ",", ":", "]", "=", "max_tag_var", "+", "agg_", "\n", "\n", "forward_var", "=", "cloned", "\n", "\n", "", "forward_var", "=", "forward_var", "[", "range", "(", "forward_var", ".", "shape", "[", "0", "]", ")", ",", "lens_", ",", ":", "]", "\n", "\n", "terminal_var", "=", "forward_var", "+", "self", ".", "transitions", "[", "\n", "self", ".", "tag_dictionary", ".", "get_idx_for_item", "(", "STOP_TAG", ")", "\n", "]", "[", "None", ",", ":", "]", ".", "repeat", "(", "forward_var", ".", "shape", "[", "0", "]", ",", "1", ")", "\n", "\n", "alpha", "=", "log_sum_exp_batch", "(", "terminal_var", ")", "\n", "\n", "return", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._filter_empty_sentences": [[841, 849], ["len", "len", "log.warning", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_filter_empty_sentences", "(", "sentences", ":", "List", "[", "Sentence", "]", ")", "->", "List", "[", "Sentence", "]", ":", "\n", "        ", "filtered_sentences", "=", "[", "sentence", "for", "sentence", "in", "sentences", "if", "sentence", ".", "tokens", "]", "\n", "if", "len", "(", "sentences", ")", "!=", "len", "(", "filtered_sentences", ")", ":", "\n", "            ", "log", ".", "warning", "(", "\n", "f\"Ignore {len(sentences) - len(filtered_sentences)} sentence(s) with no tokens.\"", "\n", ")", "\n", "", "return", "filtered_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._filter_empty_string": [[850, 858], ["len", "len", "log.warning", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_filter_empty_string", "(", "texts", ":", "List", "[", "str", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "filtered_texts", "=", "[", "text", "for", "text", "in", "texts", "if", "text", "]", "\n", "if", "len", "(", "texts", ")", "!=", "len", "(", "filtered_texts", ")", ":", "\n", "            ", "log", ".", "warning", "(", "\n", "f\"Ignore {len(texts) - len(filtered_texts)} string(s) with no tokens.\"", "\n", ")", "\n", "", "return", "filtered_texts", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger._fetch_model": [[859, 1017], ["pathlib.Path", "flair.file_utils.cached_path"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.cached_path"], ["", "@", "staticmethod", "\n", "def", "_fetch_model", "(", "model_name", ")", "->", "str", ":", "\n", "\n", "        ", "model_map", "=", "{", "}", "\n", "\n", "aws_resource_path_v04", "=", "(", "\n", "\"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4\"", "\n", ")", "\n", "\n", "model_map", "[", "\"ner\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "aws_resource_path_v04", ",", "\"NER-conll03-english\"", ",", "\"en-ner-conll03-v0.4.pt\"", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"ner-fast\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path_v04", ",", "\n", "\"NER-conll03--h256-l1-b32-p3-0.5-%2Bglove%2Bnews-forward-fast%2Bnews-backward-fast-normal-locked0.5-word0.05--release_4\"", ",", "\n", "\"en-ner-fast-conll03-v0.4.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"ner-ontonotes\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path_v04", ",", "\n", "\"release-ner-ontonotes-0\"", ",", "\n", "\"en-ner-ontonotes-v0.4.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"ner-ontonotes-fast\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path_v04", ",", "\n", "\"release-ner-ontonotes-fast-0\"", ",", "\n", "\"en-ner-ontonotes-fast-v0.4.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "for", "key", "in", "[", "\"ner-multi\"", ",", "\"multi-ner\"", "]", ":", "\n", "            ", "model_map", "[", "key", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path_v04", ",", "\n", "\"release-quadner-512-l2-multi-embed\"", ",", "\n", "\"quadner-large.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "", "for", "key", "in", "[", "\"ner-multi-fast\"", ",", "\"multi-ner-fast\"", "]", ":", "\n", "            ", "model_map", "[", "key", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "aws_resource_path_v04", ",", "\"NER-multi-fast\"", ",", "\"ner-multi-fast.pt\"", "]", "\n", ")", "\n", "\n", "", "for", "key", "in", "[", "\"ner-multi-fast-learn\"", ",", "\"multi-ner-fast-learn\"", "]", ":", "\n", "            ", "model_map", "[", "key", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path_v04", ",", "\n", "\"NER-multi-fast-evolve\"", ",", "\n", "\"ner-multi-fast-learn.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "", "model_map", "[", "\"pos\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path_v04", ",", "\n", "\"POS-ontonotes--h256-l1-b32-p3-0.5-%2Bglove%2Bnews-forward%2Bnews-backward-normal-locked0.5-word0.05--v0.4_0\"", ",", "\n", "\"en-pos-ontonotes-v0.4.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"pos-fast\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path_v04", ",", "\n", "\"release-pos-fast-0\"", ",", "\n", "\"en-pos-ontonotes-fast-v0.4.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "for", "key", "in", "[", "\"pos-multi\"", ",", "\"multi-pos\"", "]", ":", "\n", "            ", "model_map", "[", "key", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path_v04", ",", "\n", "\"release-dodekapos-512-l2-multi\"", ",", "\n", "\"pos-multi-v0.1.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "", "for", "key", "in", "[", "\"pos-multi-fast\"", ",", "\"multi-pos-fast\"", "]", ":", "\n", "            ", "model_map", "[", "key", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "aws_resource_path_v04", ",", "\"UPOS-multi-fast\"", ",", "\"pos-multi-fast.pt\"", "]", "\n", ")", "\n", "\n", "", "model_map", "[", "\"frame\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "aws_resource_path_v04", ",", "\"release-frame-1\"", ",", "\"en-frame-ontonotes-v0.4.pt\"", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"frame-fast\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path_v04", ",", "\n", "\"release-frame-fast-0\"", ",", "\n", "\"en-frame-ontonotes-fast-v0.4.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"chunk\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path_v04", ",", "\n", "\"NP-conll2000--h256-l1-b32-p3-0.5-%2Bnews-forward%2Bnews-backward-normal-locked0.5-word0.05--v0.4_0\"", ",", "\n", "\"en-chunk-conll2000-v0.4.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"chunk-fast\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path_v04", ",", "\n", "\"release-chunk-fast-0\"", ",", "\n", "\"en-chunk-conll2000-fast-v0.4.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"da-pos\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "aws_resource_path_v04", ",", "\"POS-danish\"", ",", "\"da-pos-v0.1.pt\"", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"da-ner\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "aws_resource_path_v04", ",", "\"NER-danish\"", ",", "\"da-ner-v0.1.pt\"", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"de-pos\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "aws_resource_path_v04", ",", "\"release-de-pos-0\"", ",", "\"de-pos-ud-hdt-v0.4.pt\"", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"de-pos-fine-grained\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "\n", "aws_resource_path_v04", ",", "\n", "\"POS-fine-grained-german-tweets\"", ",", "\n", "\"de-pos-twitter-v0.1.pt\"", ",", "\n", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"de-ner\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "aws_resource_path_v04", ",", "\"release-de-ner-0\"", ",", "\"de-ner-conll03-v0.4.pt\"", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"de-ner-germeval\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "aws_resource_path_v04", ",", "\"NER-germeval\"", ",", "\"de-ner-germeval-0.4.1.pt\"", "]", "\n", ")", "\n", "\n", "model_map", "[", "\"fr-ner\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "aws_resource_path_v04", ",", "\"release-fr-ner-0\"", ",", "\"fr-ner-wikiner-0.4.pt\"", "]", "\n", ")", "\n", "model_map", "[", "\"nl-ner\"", "]", "=", "\"/\"", ".", "join", "(", "\n", "[", "aws_resource_path_v04", ",", "\"NER-conll2002-dutch\"", ",", "\"nl-ner-conll02-v0.1.pt\"", "]", "\n", ")", "\n", "\n", "cache_dir", "=", "Path", "(", "\"models\"", ")", "\n", "if", "model_name", "in", "model_map", ":", "\n", "            ", "model_name", "=", "cached_path", "(", "model_map", "[", "model_name", "]", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "", "return", "model_name", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.get_transition_matrix": [[1018, 1030], ["enumerate", "print", "enumerate", "data.append", "tabulate.tabulate.tabulate", "data.append", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_item_for_index", "sequence_tagger_model.SequenceTagger.tag_dictionary.get_item_for_index", "column.item"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_item_for_index", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Dictionary.get_item_for_index"], ["", "def", "get_transition_matrix", "(", "self", ")", ":", "\n", "        ", "data", "=", "[", "]", "\n", "for", "to_idx", ",", "row", "in", "enumerate", "(", "self", ".", "transitions", ")", ":", "\n", "            ", "for", "from_idx", ",", "column", "in", "enumerate", "(", "row", ")", ":", "\n", "                ", "row", "=", "[", "\n", "self", ".", "tag_dictionary", ".", "get_item_for_index", "(", "from_idx", ")", ",", "\n", "self", ".", "tag_dictionary", ".", "get_item_for_index", "(", "to_idx", ")", ",", "\n", "column", ".", "item", "(", ")", ",", "\n", "]", "\n", "data", ".", "append", "(", "row", ")", "\n", "", "data", ".", "append", "(", "[", "\"----\"", "]", ")", "\n", "", "print", "(", "tabulate", "(", "data", ",", "headers", "=", "[", "\"FROM\"", ",", "\"TO\"", ",", "\"SCORE\"", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.__str__": [[1031, 1036], ["super().__str__().rstrip", "super().__str__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.SequenceTagger.__str__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "super", "(", "flair", ".", "nn", ".", "Model", ",", "self", ")", ".", "__str__", "(", ")", ".", "rstrip", "(", "')'", ")", "+", "f'  (beta): {self.beta}\\n'", "+", "f'  (weights): {self.weight_dict}\\n'", "+", "f'  (weight_tensor) {self.loss_weights}\\n)'", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.to_scalar": [[27, 29], ["var.view().detach().tolist", "var.view().detach", "var.view"], "function", ["None"], ["def", "to_scalar", "(", "var", ")", ":", "\n", "    ", "return", "var", ".", "view", "(", "-", "1", ")", ".", "detach", "(", ")", ".", "tolist", "(", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.argmax": [[31, 34], ["torch.max", "torch.max", "torch.max", "sequence_tagger_model.to_scalar"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.to_scalar"], ["", "def", "argmax", "(", "vec", ")", ":", "\n", "    ", "_", ",", "idx", "=", "torch", ".", "max", "(", "vec", ",", "1", ")", "\n", "return", "to_scalar", "(", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.log_sum_exp": [[36, 40], ["max_score.view().expand", "torch.log", "torch.log", "torch.log", "max_score.view", "vec.size", "torch.sum", "torch.sum", "torch.sum", "sequence_tagger_model.argmax", "torch.exp", "torch.exp", "torch.exp"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.argmax"], ["", "def", "log_sum_exp", "(", "vec", ")", ":", "\n", "    ", "max_score", "=", "vec", "[", "0", ",", "argmax", "(", "vec", ")", "]", "\n", "max_score_broadcast", "=", "max_score", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand", "(", "1", ",", "vec", ".", "size", "(", ")", "[", "1", "]", ")", "\n", "return", "max_score", "+", "torch", ".", "log", "(", "torch", ".", "sum", "(", "torch", ".", "exp", "(", "vec", "-", "max_score_broadcast", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.argmax_batch": [[42, 45], ["torch.max", "torch.max", "torch.max"], "function", ["None"], ["", "def", "argmax_batch", "(", "vecs", ")", ":", "\n", "    ", "_", ",", "idx", "=", "torch", ".", "max", "(", "vecs", ",", "1", ")", "\n", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.log_sum_exp_batch": [[47, 52], ["maxi[].repeat", "torch.log", "torch.log", "torch.log", "torch.max", "torch.max", "torch.max", "torch.sum", "torch.sum", "torch.sum", "torch.exp", "torch.exp", "torch.exp"], "function", ["None"], ["", "def", "log_sum_exp_batch", "(", "vecs", ")", ":", "\n", "    ", "maxi", "=", "torch", ".", "max", "(", "vecs", ",", "1", ")", "[", "0", "]", "\n", "maxi_bc", "=", "maxi", "[", ":", ",", "None", "]", ".", "repeat", "(", "1", ",", "vecs", ".", "shape", "[", "1", "]", ")", "\n", "recti_", "=", "torch", ".", "log", "(", "torch", ".", "sum", "(", "torch", ".", "exp", "(", "vecs", "-", "maxi_bc", ")", ",", "1", ")", ")", "\n", "return", "maxi", "+", "recti_", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.sequence_tagger_model.pad_tensors": [[54, 63], ["max", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "list", "len"], "function", ["None"], ["", "def", "pad_tensors", "(", "tensor_list", ")", ":", "\n", "    ", "ml", "=", "max", "(", "[", "x", ".", "shape", "[", "0", "]", "for", "x", "in", "tensor_list", "]", ")", "\n", "shape", "=", "[", "len", "(", "tensor_list", ")", ",", "ml", "]", "+", "list", "(", "tensor_list", "[", "0", "]", ".", "shape", "[", "1", ":", "]", ")", "\n", "template", "=", "torch", ".", "zeros", "(", "*", "shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "flair", ".", "device", ")", "\n", "lens_", "=", "[", "x", ".", "shape", "[", "0", "]", "for", "x", "in", "tensor_list", "]", "\n", "for", "i", ",", "tensor", "in", "enumerate", "(", "tensor_list", ")", ":", "\n", "        ", "template", "[", "i", ",", ":", "lens_", "[", "i", "]", "]", "=", "tensor", "\n", "\n", "", "return", "template", ",", "lens_", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold._Transform.__init__": [[7, 9], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold._Transform.fit": [[10, 12], ["manifold._Transform.transform.fit_transform"], "methods", ["None"], ["", "def", "fit", "(", "self", ",", "X", ")", ":", "\n", "        ", "return", "self", ".", "transform", ".", "fit_transform", "(", "X", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.tSNE.__init__": [[15, 19], ["manifold._Transform.__init__", "sklearn.manifold.TSNE"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "transform", "=", "TSNE", "(", "n_components", "=", "2", ",", "verbose", "=", "1", ",", "perplexity", "=", "40", ",", "n_iter", "=", "300", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.visualize_word_emeddings": [[22, 30], ["manifold.Visualizer.prepare_word_embeddings", "manifold.Visualizer.word_contexts", "manifold.tSNE", "manifold._Transform.fit", "manifold.Visualizer.visualize"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.prepare_word_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.word_contexts", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold._Transform.fit", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.visualize"], ["    ", "def", "visualize_word_emeddings", "(", "self", ",", "embeddings", ",", "sentences", ",", "output_file", ")", ":", "\n", "        ", "X", "=", "self", ".", "prepare_word_embeddings", "(", "embeddings", ",", "sentences", ")", "\n", "contexts", "=", "self", ".", "word_contexts", "(", "sentences", ")", "\n", "\n", "trans_", "=", "tSNE", "(", ")", "\n", "reduced", "=", "trans_", ".", "fit", "(", "X", ")", "\n", "\n", "self", ".", "visualize", "(", "reduced", ",", "contexts", ",", "output_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.visualize_char_emeddings": [[31, 39], ["manifold.Visualizer.prepare_char_embeddings", "manifold.Visualizer.char_contexts", "manifold.tSNE", "manifold._Transform.fit", "manifold.Visualizer.visualize"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.prepare_char_embeddings", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.char_contexts", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold._Transform.fit", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.visualize"], ["", "def", "visualize_char_emeddings", "(", "self", ",", "embeddings", ",", "sentences", ",", "output_file", ")", ":", "\n", "        ", "X", "=", "self", ".", "prepare_char_embeddings", "(", "embeddings", ",", "sentences", ")", "\n", "contexts", "=", "self", ".", "char_contexts", "(", "sentences", ")", "\n", "\n", "trans_", "=", "tSNE", "(", ")", "\n", "reduced", "=", "trans_", ".", "fit", "(", "X", ")", "\n", "\n", "self", ".", "visualize", "(", "reduced", ",", "contexts", ",", "output_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.prepare_word_embeddings": [[40, 53], ["tqdm.tqdm", "numpy.concatenate", "embeddings.embed", "enumerate", "numpy.concatenate.append", "token.embedding.detach().numpy", "token.embedding.detach"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.embeddings.DocumentLSTMEmbeddings.embed"], ["", "@", "staticmethod", "\n", "def", "prepare_word_embeddings", "(", "embeddings", ",", "sentences", ")", ":", "\n", "        ", "X", "=", "[", "]", "\n", "\n", "for", "sentence", "in", "tqdm", ".", "tqdm", "(", "sentences", ")", ":", "\n", "            ", "embeddings", ".", "embed", "(", "sentence", ")", "\n", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "sentence", ")", ":", "\n", "                ", "X", ".", "append", "(", "token", ".", "embedding", ".", "detach", "(", ")", ".", "numpy", "(", ")", "[", "None", ",", ":", "]", ")", "\n", "\n", "", "", "X", "=", "numpy", ".", "concatenate", "(", "X", ",", "0", ")", "\n", "\n", "return", "X", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.word_contexts": [[54, 71], ["enumerate", "contexts.append", "max", "min", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "word_contexts", "(", "sentences", ")", ":", "\n", "        ", "contexts", "=", "[", "]", "\n", "\n", "for", "sentence", "in", "sentences", ":", "\n", "\n", "            ", "strs", "=", "[", "x", ".", "text", "for", "x", "in", "sentence", ".", "tokens", "]", "\n", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "strs", ")", ":", "\n", "                ", "prop", "=", "'<b><font color=\"red\"> {token} </font></b>'", ".", "format", "(", "token", "=", "token", ")", "\n", "\n", "prop", "=", "\" \"", ".", "join", "(", "strs", "[", "max", "(", "i", "-", "4", ",", "0", ")", ":", "i", "]", ")", "+", "prop", "\n", "prop", "=", "prop", "+", "\" \"", ".", "join", "(", "strs", "[", "i", "+", "1", ":", "min", "(", "len", "(", "strs", ")", ",", "i", "+", "5", ")", "]", ")", "\n", "\n", "contexts", ".", "append", "(", "\"<p>\"", "+", "prop", "+", "\"</p>\"", ")", "\n", "\n", "", "", "return", "contexts", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.prepare_char_embeddings": [[72, 85], ["tqdm.tqdm", "numpy.concatenate", "embeddings.lm.get_representation", "numpy.concatenate.append", "embeddings.lm.get_representation.squeeze().detach().numpy", "embeddings.lm.get_representation.squeeze().detach", "embeddings.lm.get_representation.squeeze"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.file_utils.Tqdm.tqdm", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.models.language_model.LanguageModel.get_representation"], ["", "@", "staticmethod", "\n", "def", "prepare_char_embeddings", "(", "embeddings", ",", "sentences", ")", ":", "\n", "        ", "X", "=", "[", "]", "\n", "\n", "for", "sentence", "in", "tqdm", ".", "tqdm", "(", "sentences", ")", ":", "\n", "            ", "sentence", "=", "\" \"", ".", "join", "(", "[", "x", ".", "text", "for", "x", "in", "sentence", "]", ")", "\n", "\n", "hidden", "=", "embeddings", ".", "lm", ".", "get_representation", "(", "[", "sentence", "]", ",", "\"\"", ",", "\"\"", ")", "\n", "X", ".", "append", "(", "hidden", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "X", "=", "numpy", ".", "concatenate", "(", "X", ",", "0", ")", "\n", "\n", "return", "X", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.char_contexts": [[86, 105], ["enumerate", "contexts.append", "max", "min", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "char_contexts", "(", "sentences", ")", ":", "\n", "        ", "contexts", "=", "[", "]", "\n", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "sentence", "=", "\" \"", ".", "join", "(", "[", "token", ".", "text", "for", "token", "in", "sentence", "]", ")", "\n", "\n", "for", "i", ",", "char", "in", "enumerate", "(", "sentence", ")", ":", "\n", "                ", "context", "=", "'<span style=\"background-color: yellow\"><b>{}</b></span>'", ".", "format", "(", "\n", "char", "\n", ")", "\n", "context", "=", "\"\"", ".", "join", "(", "sentence", "[", "max", "(", "i", "-", "30", ",", "0", ")", ":", "i", "]", ")", "+", "context", "\n", "context", "=", "context", "+", "\"\"", ".", "join", "(", "\n", "sentence", "[", "i", "+", "1", ":", "min", "(", "len", "(", "sentence", ")", ",", "i", "+", "30", ")", "]", "\n", ")", "\n", "\n", "contexts", ".", "append", "(", "context", ")", "\n", "\n", "", "", "return", "contexts", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.manifold.Visualizer.visualize": [[106, 130], ["matplotlib.pyplot.subplots", "ax.grid", "ax.plot", "ax.set_xlabel", "ax.set_ylabel", "ax.set_title", "mpld3.plugins.PointHTMLTooltip", "mpld3.plugins.connect", "mpld3.save_html"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "visualize", "(", "X", ",", "contexts", ",", "file", ")", ":", "\n", "        ", "import", "matplotlib", ".", "pyplot", "\n", "import", "mpld3", "\n", "\n", "fig", ",", "ax", "=", "matplotlib", ".", "pyplot", ".", "subplots", "(", ")", "\n", "\n", "ax", ".", "grid", "(", "True", ",", "alpha", "=", "0.3", ")", "\n", "\n", "points", "=", "ax", ".", "plot", "(", "\n", "X", "[", ":", ",", "0", "]", ",", "X", "[", ":", ",", "1", "]", ",", "\"o\"", ",", "color", "=", "\"b\"", ",", "mec", "=", "\"k\"", ",", "ms", "=", "5", ",", "mew", "=", "1", ",", "alpha", "=", "0.6", "\n", ")", "\n", "\n", "ax", ".", "set_xlabel", "(", "\"x\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"y\"", ")", "\n", "ax", ".", "set_title", "(", "\"Hover mouse to reveal context\"", ",", "size", "=", "20", ")", "\n", "\n", "tooltip", "=", "mpld3", ".", "plugins", ".", "PointHTMLTooltip", "(", "\n", "points", "[", "0", "]", ",", "contexts", ",", "voffset", "=", "10", ",", "hoffset", "=", "10", "\n", ")", "\n", "\n", "mpld3", ".", "plugins", ".", "connect", "(", "fig", ",", "tooltip", ")", "\n", "\n", "mpld3", ".", "save_html", "(", "fig", ",", "file", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.training_curves.Plotter._extract_evaluation_data": [[29, 77], ["open", "csv.reader", "next", "score.upper.upper.upper", "log.warning", "log.warning", "log.warning", "log.warning", "next.index", "next.index", "next.index", "[].append", "[].append", "[].append", "float", "float", "float"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "_extract_evaluation_data", "(", "file_name", ":", "Path", ",", "score", ":", "str", "=", "\"F1\"", ")", "->", "dict", ":", "\n", "        ", "training_curves", "=", "{", "\n", "\"train\"", ":", "{", "\"loss\"", ":", "[", "]", ",", "\"score\"", ":", "[", "]", "}", ",", "\n", "\"test\"", ":", "{", "\"loss\"", ":", "[", "]", ",", "\"score\"", ":", "[", "]", "}", ",", "\n", "\"dev\"", ":", "{", "\"loss\"", ":", "[", "]", ",", "\"score\"", ":", "[", "]", "}", ",", "\n", "}", "\n", "\n", "with", "open", "(", "file_name", ",", "\"r\"", ")", "as", "tsvin", ":", "\n", "            ", "tsvin", "=", "csv", ".", "reader", "(", "tsvin", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "\n", "# determine the column index of loss, f-score and accuracy for train, dev and test split", "\n", "row", "=", "next", "(", "tsvin", ",", "None", ")", "\n", "\n", "score", "=", "score", ".", "upper", "(", ")", "\n", "\n", "if", "f\"TEST_{score}\"", "not", "in", "row", ":", "\n", "                ", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "log", ".", "warning", "(", "f\"WARNING: No {score} found for test split in this data.\"", ")", "\n", "log", ".", "warning", "(", "\n", "f\"Are you sure you want to plot {score} and not another value?\"", "\n", ")", "\n", "log", ".", "warning", "(", "\"-\"", "*", "100", ")", "\n", "\n", "", "TRAIN_SCORE", "=", "(", "\n", "row", ".", "index", "(", "f\"TRAIN_{score}\"", ")", "if", "f\"TRAIN_{score}\"", "in", "row", "else", "None", "\n", ")", "\n", "DEV_SCORE", "=", "row", ".", "index", "(", "f\"DEV_{score}\"", ")", "if", "f\"DEV_{score}\"", "in", "row", "else", "None", "\n", "TEST_SCORE", "=", "row", ".", "index", "(", "f\"TEST_{score}\"", ")", "if", "f\"TEST_{score}\"", "in", "row", "else", "None", "\n", "\n", "# then get all relevant values from the tsv", "\n", "for", "row", "in", "tsvin", ":", "\n", "\n", "                ", "if", "TRAIN_SCORE", "is", "not", "None", ":", "\n", "                    ", "if", "row", "[", "TRAIN_SCORE", "]", "!=", "\"_\"", ":", "\n", "                        ", "training_curves", "[", "\"train\"", "]", "[", "\"score\"", "]", ".", "append", "(", "\n", "float", "(", "row", "[", "TRAIN_SCORE", "]", ")", "\n", ")", "\n", "\n", "", "", "if", "DEV_SCORE", "is", "not", "None", ":", "\n", "                    ", "if", "row", "[", "DEV_SCORE", "]", "!=", "\"_\"", ":", "\n", "                        ", "training_curves", "[", "\"dev\"", "]", "[", "\"score\"", "]", ".", "append", "(", "float", "(", "row", "[", "DEV_SCORE", "]", ")", ")", "\n", "\n", "", "", "if", "TEST_SCORE", "is", "not", "None", ":", "\n", "                    ", "if", "row", "[", "TEST_SCORE", "]", "!=", "\"_\"", ":", "\n", "                        ", "training_curves", "[", "\"test\"", "]", "[", "\"score\"", "]", ".", "append", "(", "float", "(", "row", "[", "TEST_SCORE", "]", ")", ")", "\n", "\n", "", "", "", "", "return", "training_curves", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.training_curves.Plotter._extract_weight_data": [[78, 93], ["collections.defaultdict", "open", "csv.reader", "collections.defaultdict", "float", "[].append", "list"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_extract_weight_data", "(", "file_name", ":", "Path", ")", "->", "dict", ":", "\n", "        ", "weights", "=", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "lambda", ":", "list", "(", ")", ")", ")", "\n", "\n", "with", "open", "(", "file_name", ",", "\"r\"", ")", "as", "tsvin", ":", "\n", "            ", "tsvin", "=", "csv", ".", "reader", "(", "tsvin", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "\n", "for", "row", "in", "tsvin", ":", "\n", "                ", "name", "=", "row", "[", "WEIGHT_NAME", "]", "\n", "param", "=", "row", "[", "WEIGHT_NUMBER", "]", "\n", "value", "=", "float", "(", "row", "[", "WEIGHT_VALUE", "]", ")", "\n", "\n", "weights", "[", "name", "]", "[", "param", "]", ".", "append", "(", "value", ")", "\n", "\n", "", "", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.training_curves.Plotter._extract_learning_rate": [[94, 113], ["open", "csv.reader", "next", "next.index", "next.index", "losses.append", "lrs.append", "float", "float"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_extract_learning_rate", "(", "file_name", ":", "Path", ")", ":", "\n", "        ", "lrs", "=", "[", "]", "\n", "losses", "=", "[", "]", "\n", "\n", "with", "open", "(", "file_name", ",", "\"r\"", ")", "as", "tsvin", ":", "\n", "            ", "tsvin", "=", "csv", ".", "reader", "(", "tsvin", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "row", "=", "next", "(", "tsvin", ",", "None", ")", "\n", "LEARNING_RATE", "=", "row", ".", "index", "(", "\"LEARNING_RATE\"", ")", "\n", "TRAIN_LOSS", "=", "row", ".", "index", "(", "\"TRAIN_LOSS\"", ")", "\n", "\n", "# then get all relevant values from the tsv", "\n", "for", "row", "in", "tsvin", ":", "\n", "                ", "if", "row", "[", "TRAIN_LOSS", "]", "!=", "\"_\"", ":", "\n", "                    ", "losses", ".", "append", "(", "float", "(", "row", "[", "TRAIN_LOSS", "]", ")", ")", "\n", "", "if", "row", "[", "LEARNING_RATE", "]", "!=", "\"_\"", ":", "\n", "                    ", "lrs", ".", "append", "(", "float", "(", "row", "[", "LEARNING_RATE", "]", ")", ")", "\n", "\n", "", "", "", "return", "lrs", ",", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.training_curves.Plotter.plot_weights": [[114, 163], ["training_curves.Plotter._extract_weight_data", "len", "max", "matplotlib.figure", "matplotlib.subplots", "training_curves.Plotter.items", "f.subplots_adjust", "matplotlib.tight_layout", "matplotlib.savefig", "print", "matplotlib.close", "type", "pathlib.Path", "int", "axarr[].set_title", "values.items", "axarr[].set_yticks", "axarr[].set_xticks", "axarr[].set_yticks", "axarr[].set_xticks", "math.ceil", "axarr[].plot", "numpy.arange", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.training_curves.Plotter._extract_weight_data"], ["", "def", "plot_weights", "(", "self", ",", "file_name", ":", "Union", "[", "str", ",", "Path", "]", ")", ":", "\n", "        ", "if", "type", "(", "file_name", ")", "is", "str", ":", "\n", "            ", "file_name", "=", "Path", "(", "file_name", ")", "\n", "\n", "", "weights", "=", "self", ".", "_extract_weight_data", "(", "file_name", ")", "\n", "\n", "total", "=", "len", "(", "weights", ")", "\n", "columns", "=", "2", "\n", "rows", "=", "max", "(", "2", ",", "int", "(", "math", ".", "ceil", "(", "total", "/", "columns", ")", ")", ")", "\n", "# print(rows)", "\n", "\n", "# figsize = (16, 16)", "\n", "if", "rows", "!=", "columns", ":", "\n", "            ", "figsize", "=", "(", "8", ",", "rows", "+", "0", ")", "\n", "\n", "", "fig", "=", "plt", ".", "figure", "(", ")", "\n", "f", ",", "axarr", "=", "plt", ".", "subplots", "(", "rows", ",", "columns", ",", "figsize", "=", "figsize", ")", "\n", "\n", "c", "=", "0", "\n", "r", "=", "0", "\n", "for", "name", ",", "values", "in", "weights", ".", "items", "(", ")", ":", "\n", "# plot i", "\n", "            ", "axarr", "[", "r", ",", "c", "]", ".", "set_title", "(", "name", ",", "fontsize", "=", "6", ")", "\n", "for", "_", ",", "v", "in", "values", ".", "items", "(", ")", ":", "\n", "                ", "axarr", "[", "r", ",", "c", "]", ".", "plot", "(", "np", ".", "arange", "(", "0", ",", "len", "(", "v", ")", ")", ",", "v", ",", "linewidth", "=", "0.35", ")", "\n", "", "axarr", "[", "r", ",", "c", "]", ".", "set_yticks", "(", "[", "]", ")", "\n", "axarr", "[", "r", ",", "c", "]", ".", "set_xticks", "(", "[", "]", ")", "\n", "c", "+=", "1", "\n", "if", "c", "==", "columns", ":", "\n", "                ", "c", "=", "0", "\n", "r", "+=", "1", "\n", "\n", "", "", "while", "r", "!=", "rows", "and", "c", "!=", "columns", ":", "\n", "            ", "axarr", "[", "r", ",", "c", "]", ".", "set_yticks", "(", "[", "]", ")", "\n", "axarr", "[", "r", ",", "c", "]", ".", "set_xticks", "(", "[", "]", ")", "\n", "c", "+=", "1", "\n", "if", "c", "==", "columns", ":", "\n", "                ", "c", "=", "0", "\n", "r", "+=", "1", "\n", "\n", "# save plots", "\n", "", "", "f", ".", "subplots_adjust", "(", "hspace", "=", "0.5", ")", "\n", "plt", ".", "tight_layout", "(", "pad", "=", "1.0", ")", "\n", "path", "=", "file_name", ".", "parent", "/", "\"weights.png\"", "\n", "plt", ".", "savefig", "(", "path", ",", "dpi", "=", "300", ")", "\n", "print", "(", "\n", "f\"Weights plots are saved in {path}\"", "\n", ")", "# to let user know the path of the save plots", "\n", "plt", ".", "close", "(", "fig", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.training_curves.Plotter.plot_training_curves": [[164, 205], ["matplotlib.figure", "enumerate", "matplotlib.tight_layout", "matplotlib.savefig", "print", "matplotlib.show", "matplotlib.close", "type", "pathlib.Path", "training_curves.Plotter.Plotter._extract_evaluation_data", "matplotlib.subplot", "matplotlib.legend", "matplotlib.ylabel", "matplotlib.xlabel", "len", "numpy.arange", "matplotlib.plot", "numpy.arange", "matplotlib.plot", "numpy.arange", "matplotlib.plot", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.training_curves.Plotter._extract_evaluation_data"], ["", "def", "plot_training_curves", "(", "\n", "self", ",", "file_name", ":", "Union", "[", "str", ",", "Path", "]", ",", "plot_values", ":", "List", "[", "str", "]", "=", "[", "\"loss\"", ",", "\"F1\"", "]", "\n", ")", ":", "\n", "        ", "if", "type", "(", "file_name", ")", "is", "str", ":", "\n", "            ", "file_name", "=", "Path", "(", "file_name", ")", "\n", "\n", "", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "15", ",", "10", ")", ")", "\n", "\n", "for", "plot_no", ",", "plot_value", "in", "enumerate", "(", "plot_values", ")", ":", "\n", "\n", "            ", "training_curves", "=", "self", ".", "_extract_evaluation_data", "(", "file_name", ",", "plot_value", ")", "\n", "\n", "plt", ".", "subplot", "(", "len", "(", "plot_values", ")", ",", "1", ",", "plot_no", "+", "1", ")", "\n", "if", "training_curves", "[", "\"train\"", "]", "[", "\"score\"", "]", ":", "\n", "                ", "x", "=", "np", ".", "arange", "(", "0", ",", "len", "(", "training_curves", "[", "\"train\"", "]", "[", "\"score\"", "]", ")", ")", "\n", "plt", ".", "plot", "(", "\n", "x", ",", "training_curves", "[", "\"train\"", "]", "[", "\"score\"", "]", ",", "label", "=", "f\"training {plot_value}\"", "\n", ")", "\n", "", "if", "training_curves", "[", "\"dev\"", "]", "[", "\"score\"", "]", ":", "\n", "                ", "x", "=", "np", ".", "arange", "(", "0", ",", "len", "(", "training_curves", "[", "\"dev\"", "]", "[", "\"score\"", "]", ")", ")", "\n", "plt", ".", "plot", "(", "\n", "x", ",", "training_curves", "[", "\"dev\"", "]", "[", "\"score\"", "]", ",", "label", "=", "f\"validation {plot_value}\"", "\n", ")", "\n", "", "if", "training_curves", "[", "\"test\"", "]", "[", "\"score\"", "]", ":", "\n", "                ", "x", "=", "np", ".", "arange", "(", "0", ",", "len", "(", "training_curves", "[", "\"test\"", "]", "[", "\"score\"", "]", ")", ")", "\n", "plt", ".", "plot", "(", "\n", "x", ",", "training_curves", "[", "\"test\"", "]", "[", "\"score\"", "]", ",", "label", "=", "f\"test {plot_value}\"", "\n", ")", "\n", "", "plt", ".", "legend", "(", "bbox_to_anchor", "=", "(", "1.04", ",", "0", ")", ",", "loc", "=", "\"lower left\"", ",", "borderaxespad", "=", "0", ")", "\n", "plt", ".", "ylabel", "(", "plot_value", ")", "\n", "plt", ".", "xlabel", "(", "\"epochs\"", ")", "\n", "\n", "# save plots", "\n", "", "plt", ".", "tight_layout", "(", "pad", "=", "1.0", ")", "\n", "path", "=", "file_name", ".", "parent", "/", "\"training.png\"", "\n", "plt", ".", "savefig", "(", "path", ",", "dpi", "=", "300", ")", "\n", "print", "(", "\n", "f\"Loss and F1 plots are saved in {path}\"", "\n", ")", "# to let user know the path of the save plots", "\n", "plt", ".", "show", "(", "block", "=", "False", ")", "# to have the plots displayed when user run this module", "\n", "plt", ".", "close", "(", "fig", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.training_curves.Plotter.plot_learning_rate": [[206, 234], ["training_curves.Plotter._extract_learning_rate", "matplotlib.subplots", "ax.plot", "ax.set_ylabel", "ax.set_xlabel", "ax.set_xscale", "ax.xaxis.set_major_formatter", "matplotlib.tight_layout", "matplotlib.savefig", "print", "matplotlib.show", "matplotlib.close", "type", "pathlib.Path", "matplotlib.FormatStrFormatter"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.training_curves.Plotter._extract_learning_rate"], ["", "def", "plot_learning_rate", "(", "\n", "self", ",", "file_name", ":", "Union", "[", "str", ",", "Path", "]", ",", "skip_first", ":", "int", "=", "10", ",", "skip_last", ":", "int", "=", "5", "\n", ")", ":", "\n", "        ", "if", "type", "(", "file_name", ")", "is", "str", ":", "\n", "            ", "file_name", "=", "Path", "(", "file_name", ")", "\n", "\n", "", "lrs", ",", "losses", "=", "self", ".", "_extract_learning_rate", "(", "file_name", ")", "\n", "lrs", "=", "lrs", "[", "skip_first", ":", "-", "skip_last", "]", "if", "skip_last", ">", "0", "else", "lrs", "[", "skip_first", ":", "]", "\n", "losses", "=", "losses", "[", "skip_first", ":", "-", "skip_last", "]", "if", "skip_last", ">", "0", "else", "losses", "[", "skip_first", ":", "]", "\n", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "1", ",", "1", ")", "\n", "ax", ".", "plot", "(", "lrs", ",", "losses", ")", "\n", "ax", ".", "set_ylabel", "(", "\"Loss\"", ")", "\n", "ax", ".", "set_xlabel", "(", "\"Learning Rate\"", ")", "\n", "ax", ".", "set_xscale", "(", "\"log\"", ")", "\n", "ax", ".", "xaxis", ".", "set_major_formatter", "(", "plt", ".", "FormatStrFormatter", "(", "\"%.0e\"", ")", ")", "\n", "\n", "# plt.show()", "\n", "\n", "# save plot", "\n", "plt", ".", "tight_layout", "(", "pad", "=", "1.0", ")", "\n", "path", "=", "file_name", ".", "parent", "/", "\"learning_rate.png\"", "\n", "plt", ".", "savefig", "(", "path", ",", "dpi", "=", "300", ")", "\n", "print", "(", "\n", "f\"Learning_rate plots are saved in {path}\"", "\n", ")", "# to let user know the path of the save plots", "\n", "plt", ".", "show", "(", "block", "=", "True", ")", "# to have the plots displayed when user run this module", "\n", "plt", ".", "close", "(", "fig", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.activations.Highlighter.__init__": [[5, 32], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "color_map", "=", "[", "\n", "\"#ff0000\"", ",", "\n", "\"#ff4000\"", ",", "\n", "\"#ff8000\"", ",", "\n", "\"#ffbf00\"", ",", "\n", "\"#ffff00\"", ",", "\n", "\"#bfff00\"", ",", "\n", "\"#80ff00\"", ",", "\n", "\"#40ff00\"", ",", "\n", "\"#00ff00\"", ",", "\n", "\"#00ff40\"", ",", "\n", "\"#00ff80\"", ",", "\n", "\"#00ffbf\"", ",", "\n", "\"#00ffff\"", ",", "\n", "\"#00bfff\"", ",", "\n", "\"#0080ff\"", ",", "\n", "\"#0040ff\"", ",", "\n", "\"#0000ff\"", ",", "\n", "\"#4000ff\"", ",", "\n", "\"#8000ff\"", ",", "\n", "\"#bf00ff\"", ",", "\n", "\"#ff00ff\"", ",", "\n", "\"#ff00bf\"", ",", "\n", "\"#ff0080\"", ",", "\n", "\"#ff0040\"", ",", "\n", "\"#ff0000\"", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.activations.Highlighter.highlight": [[34, 61], ["activation.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "numpy.array", "enumerate", "enumerate", "len", "list", "zip", "activations.Highlighter._render", "activation.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "max", "min", "numpy.arange", "colors.append", "list", "min", "max", "colors.append", "activation.detach().cpu().numpy.detach().cpu().numpy.detach", "len", "numpy.where"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.activations.Highlighter._render"], ["", "def", "highlight", "(", "self", ",", "activation", ",", "text", ")", ":", "\n", "        ", "activation", "=", "activation", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "step_size", "=", "(", "max", "(", "activation", ")", "-", "min", "(", "activation", ")", ")", "/", "len", "(", "self", ".", "color_map", ")", "\n", "\n", "lookup", "=", "numpy", ".", "array", "(", "\n", "list", "(", "numpy", ".", "arange", "(", "min", "(", "activation", ")", ",", "max", "(", "activation", ")", ",", "step_size", ")", ")", "\n", ")", "\n", "\n", "colors", "=", "[", "]", "\n", "\n", "for", "i", ",", "act", "in", "enumerate", "(", "activation", ")", ":", "\n", "\n", "            ", "try", ":", "\n", "                ", "colors", ".", "append", "(", "self", ".", "color_map", "[", "numpy", ".", "where", "(", "act", ">", "lookup", ")", "[", "0", "]", "[", "-", "1", "]", "]", ")", "\n", "", "except", "IndexError", ":", "\n", "                ", "colors", ".", "append", "(", "len", "(", "self", ".", "color_map", ")", "-", "1", ")", "\n", "\n", "", "", "str_", "=", "\"<br><br>\"", "\n", "\n", "for", "i", ",", "(", "char", ",", "color", ")", "in", "enumerate", "(", "zip", "(", "list", "(", "text", ")", ",", "colors", ")", ")", ":", "\n", "            ", "str_", "+=", "self", ".", "_render", "(", "char", ",", "color", ")", "\n", "\n", "if", "i", "%", "100", "==", "0", "and", "i", ">", "0", ":", "\n", "                ", "str_", "+=", "\"<br>\"", "\n", "\n", "", "", "return", "str_", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.activations.Highlighter.highlight_selection": [[62, 76], ["numpy.random.choice", "activations.Highlighter.highlight", "open", "f.write"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.activations.Highlighter.highlight"], ["", "def", "highlight_selection", "(", "\n", "self", ",", "activations", ",", "text", ",", "file_", "=", "\"resources/data/highlight.html\"", ",", "n", "=", "10", "\n", ")", ":", "\n", "\n", "        ", "ix", "=", "numpy", ".", "random", ".", "choice", "(", "activations", ".", "shape", "[", "1", "]", ",", "size", "=", "n", ")", "\n", "\n", "rendered", "=", "\"\"", "\n", "\n", "for", "i", "in", "ix", ":", "\n", "\n", "            ", "rendered", "+=", "self", ".", "highlight", "(", "activations", "[", ":", ",", "i", "]", ",", "text", ")", "\n", "\n", "", "with", "open", "(", "file_", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "rendered", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.activations.Highlighter._render": [[77, 80], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "_render", "(", "char", ",", "color", ")", ":", "\n", "        ", "return", "'<span style=\"background-color: {}\">{}</span>'", ".", "format", "(", "color", ",", "char", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.ner_html.split_to_spans": [[27, 40], ["s.to_original_text", "s.get_spans", "spans.append", "spans.append", "spans.append", "len", "len"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.to_original_text", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Sentence.get_spans"], ["def", "split_to_spans", "(", "s", ":", "Sentence", ")", ":", "\n", "    ", "orig", "=", "s", ".", "to_original_text", "(", ")", "\n", "last_idx", "=", "0", "\n", "spans", "=", "[", "]", "\n", "tagged_ents", "=", "s", ".", "get_spans", "(", "\"ner\"", ")", "\n", "for", "ent", "in", "tagged_ents", ":", "\n", "        ", "if", "last_idx", "!=", "ent", ".", "start_pos", ":", "\n", "            ", "spans", ".", "append", "(", "(", "orig", "[", "last_idx", ":", "ent", ".", "start_pos", "]", ",", "None", ")", ")", "\n", "", "spans", ".", "append", "(", "(", "ent", ".", "text", ",", "ent", ".", "tag", ")", ")", "\n", "last_idx", "=", "ent", ".", "end_pos", "\n", "", "if", "last_idx", "<", "len", "(", "orig", ")", "-", "1", ":", "\n", "        ", "spans", ".", "append", "(", "(", "orig", "[", "last_idx", ":", "len", "(", "orig", ")", "]", ",", "None", ")", ")", "\n", "", "return", "spans", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.ner_html.render_ner_html": [[42, 87], ["isinstance", "ner_html.split_to_spans", "list", "PARAGRAPH.format", "sentences_html.append", "HTML_PAGE.format", "html.escape().replace", "list.append", "TAGGED_ENTITY.format", "html.escape", "colors.get"], "function", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.visual.ner_html.split_to_spans"], ["", "def", "render_ner_html", "(", "\n", "sentences", ":", "Union", "[", "List", "[", "Sentence", "]", ",", "Sentence", "]", ",", "\n", "title", ":", "str", "=", "\"Flair\"", ",", "\n", "colors", "=", "{", "\n", "\"PER\"", ":", "\"#F7FF53\"", ",", "\n", "\"ORG\"", ":", "\"#E8902E\"", ",", "\n", "\"LOC\"", ":", "\"#FF40A3\"", ",", "\n", "\"MISC\"", ":", "\"#4647EB\"", ",", "\n", "\"O\"", ":", "\"#ddd\"", ",", "\n", "}", ",", "\n", "default_color", ":", "str", "=", "\"#ddd\"", ",", "\n", "wrap_page", "=", "True", ",", "\n", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    :param sentences: single sentence or list of sentences to convert to HTML\n    :param title: title of the HTML page\n    :param colors: dict where keys are tags and values are color HTML codes\n    :param default_color: color to use if colors parameter is missing a tag\n    :param wrap_page: if True method returns result of processing sentences wrapped by &lt;html&gt; and &lt;body&gt; tags, otherwise - without these tags\n    :return: HTML as a string\n    \"\"\"", "\n", "if", "isinstance", "(", "sentences", ",", "Sentence", ")", ":", "\n", "        ", "sentences", "=", "[", "sentences", "]", "\n", "", "sentences_html", "=", "[", "]", "\n", "for", "s", "in", "sentences", ":", "\n", "        ", "spans", "=", "split_to_spans", "(", "s", ")", "\n", "spans_html", "=", "list", "(", ")", "\n", "for", "fragment", ",", "tag", "in", "spans", ":", "\n", "            ", "escaped_fragment", "=", "html", ".", "escape", "(", "fragment", ")", ".", "replace", "(", "\"\\n\"", ",", "\"<br/>\"", ")", "\n", "if", "tag", ":", "\n", "                ", "escaped_fragment", "=", "TAGGED_ENTITY", ".", "format", "(", "\n", "entity", "=", "escaped_fragment", ",", "\n", "label", "=", "tag", ",", "\n", "color", "=", "colors", ".", "get", "(", "tag", ",", "default_color", ")", ",", "\n", ")", "\n", "", "spans_html", ".", "append", "(", "escaped_fragment", ")", "\n", "", "line", "=", "PARAGRAPH", ".", "format", "(", "sentence", "=", "\"\"", ".", "join", "(", "spans_html", ")", ")", "\n", "sentences_html", ".", "append", "(", "line", ")", "\n", "\n", "", "final_text", "=", "\"\"", ".", "join", "(", "sentences_html", ")", "\n", "\n", "if", "wrap_page", ":", "\n", "        ", "return", "HTML_PAGE", ".", "format", "(", "text", "=", "final_text", ",", "title", "=", "title", ")", "\n", "", "else", ":", "\n", "        ", "return", "final_text", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.BasicBlock.__init__": [[20, 37], ["torch.Module.__init__", "resnet_model.conv3x3", "norm_layer", "torch.ReLU", "torch.ReLU", "resnet_model.conv3x3", "norm_layer", "ValueError", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.conv3x3", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.conv3x3"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "groups", "=", "1", ",", "\n", "base_width", "=", "64", ",", "dilation", "=", "1", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "BasicBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "nn", ".", "BatchNorm2d", "\n", "", "if", "groups", "!=", "1", "or", "base_width", "!=", "64", ":", "\n", "            ", "raise", "ValueError", "(", "'BasicBlock only supports groups=1 and base_width=64'", ")", "\n", "", "if", "dilation", ">", "1", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Dilation > 1 not supported in BasicBlock\"", ")", "\n", "# Both self.conv1 and self.downsample layers downsample the input when stride != 1", "\n", "", "self", ".", "conv1", "=", "conv3x3", "(", "inplanes", ",", "planes", ",", "stride", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "planes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv2", "=", "conv3x3", "(", "planes", ",", "planes", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "planes", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.BasicBlock.forward": [[38, 55], ["resnet_model.BasicBlock.conv1", "resnet_model.BasicBlock.bn1", "resnet_model.BasicBlock.relu", "resnet_model.BasicBlock.conv2", "resnet_model.BasicBlock.bn2", "resnet_model.BasicBlock.relu", "resnet_model.BasicBlock.downsample"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "identity", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.Bottleneck.__init__": [[60, 76], ["torch.Module.__init__", "resnet_model.conv1x1", "norm_layer", "resnet_model.conv3x3", "norm_layer", "resnet_model.conv1x1", "norm_layer", "torch.ReLU", "torch.ReLU", "int"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.conv1x1", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.conv3x3", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.conv1x1"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "groups", "=", "1", ",", "\n", "base_width", "=", "64", ",", "dilation", "=", "1", ",", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "Bottleneck", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "nn", ".", "BatchNorm2d", "\n", "", "width", "=", "int", "(", "planes", "*", "(", "base_width", "/", "64.", ")", ")", "*", "groups", "\n", "# Both self.conv2 and self.downsample layers downsample the input when stride != 1", "\n", "self", ".", "conv1", "=", "conv1x1", "(", "inplanes", ",", "width", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "width", ")", "\n", "self", ".", "conv2", "=", "conv3x3", "(", "width", ",", "width", ",", "stride", ",", "groups", ",", "dilation", ")", "\n", "self", ".", "bn2", "=", "norm_layer", "(", "width", ")", "\n", "self", ".", "conv3", "=", "conv1x1", "(", "width", ",", "planes", "*", "self", ".", "expansion", ")", "\n", "self", ".", "bn3", "=", "norm_layer", "(", "planes", "*", "self", ".", "expansion", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.Bottleneck.forward": [[77, 98], ["resnet_model.Bottleneck.conv1", "resnet_model.Bottleneck.bn1", "resnet_model.Bottleneck.relu", "resnet_model.Bottleneck.conv2", "resnet_model.Bottleneck.bn2", "resnet_model.Bottleneck.relu", "resnet_model.Bottleneck.conv3", "resnet_model.Bottleneck.bn3", "resnet_model.Bottleneck.relu", "resnet_model.Bottleneck.downsample"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Corpus.downsample"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "out", "=", "self", ".", "bn3", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "identity", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.ResNet.__init__": [[102, 153], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.MaxPool2d", "torch.MaxPool2d", "resnet_model.ResNet._make_layer", "resnet_model.ResNet._make_layer", "resnet_model.ResNet._make_layer", "resnet_model.ResNet._make_layer", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "torch.Linear", "torch.Linear", "resnet_model.ResNet.modules", "len", "ValueError", "isinstance", "resnet_model.ResNet.modules", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "isinstance", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.ResNet._make_layer", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.ResNet._make_layer", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.ResNet._make_layer", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.ResNet._make_layer"], ["    ", "def", "__init__", "(", "self", ",", "block", ",", "layers", ",", "num_classes", "=", "1000", ",", "zero_init_residual", "=", "False", ",", "\n", "groups", "=", "1", ",", "width_per_group", "=", "64", ",", "replace_stride_with_dilation", "=", "None", ",", "\n", "norm_layer", "=", "None", ")", ":", "\n", "        ", "super", "(", "ResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_layer", "is", "None", ":", "\n", "            ", "norm_layer", "=", "nn", ".", "BatchNorm2d", "\n", "", "self", ".", "_norm_layer", "=", "norm_layer", "\n", "\n", "self", ".", "inplanes", "=", "64", "\n", "self", ".", "dilation", "=", "1", "\n", "if", "replace_stride_with_dilation", "is", "None", ":", "\n", "# each element in the tuple indicates if we should replace", "\n", "# the 2x2 stride with a dilated convolution instead", "\n", "            ", "replace_stride_with_dilation", "=", "[", "False", ",", "False", ",", "False", "]", "\n", "", "if", "len", "(", "replace_stride_with_dilation", ")", "!=", "3", ":", "\n", "            ", "raise", "ValueError", "(", "\"replace_stride_with_dilation should be None \"", "\n", "\"or a 3-element tuple, got {}\"", ".", "format", "(", "replace_stride_with_dilation", ")", ")", "\n", "", "self", ".", "groups", "=", "groups", "\n", "self", ".", "base_width", "=", "width_per_group", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "3", ",", "self", ".", "inplanes", ",", "kernel_size", "=", "7", ",", "stride", "=", "2", ",", "padding", "=", "3", ",", "\n", "bias", "=", "False", ")", "\n", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "self", ".", "inplanes", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "maxpool", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", "\n", "self", ".", "layer1", "=", "self", ".", "_make_layer", "(", "block", ",", "64", ",", "layers", "[", "0", "]", ")", "\n", "self", ".", "layer2", "=", "self", ".", "_make_layer", "(", "block", ",", "128", ",", "layers", "[", "1", "]", ",", "stride", "=", "2", ",", "\n", "dilate", "=", "replace_stride_with_dilation", "[", "0", "]", ")", "\n", "self", ".", "layer3", "=", "self", ".", "_make_layer", "(", "block", ",", "256", ",", "layers", "[", "2", "]", ",", "stride", "=", "2", ",", "\n", "dilate", "=", "replace_stride_with_dilation", "[", "1", "]", ")", "\n", "self", ".", "layer4", "=", "self", ".", "_make_layer", "(", "block", ",", "512", ",", "layers", "[", "3", "]", ",", "stride", "=", "2", ",", "\n", "dilate", "=", "replace_stride_with_dilation", "[", "2", "]", ")", "\n", "self", ".", "avgpool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "(", "1", ",", "1", ")", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "512", "*", "block", ".", "expansion", ",", "num_classes", ")", "\n", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "(", "nn", ".", "BatchNorm2d", ",", "nn", ".", "GroupNorm", ")", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n", "# Zero-initialize the last BN in each residual branch,", "\n", "# so that the residual branch starts with zeros, and each residual block behaves like an identity.", "\n", "# This improves the rpbert by 0.2~0.3% according to https://arxiv.org/abs/1706.02677", "\n", "", "", "if", "zero_init_residual", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "Bottleneck", ")", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bn3", ".", "weight", ",", "0", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "BasicBlock", ")", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bn2", ".", "weight", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.ResNet._make_layer": [[154, 177], ["layers.append", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "block", "layers.append", "resnet_model.conv1x1", "norm_layer", "block"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.conv1x1"], ["", "", "", "", "def", "_make_layer", "(", "self", ",", "block", ",", "planes", ",", "blocks", ",", "stride", "=", "1", ",", "dilate", "=", "False", ")", ":", "\n", "        ", "norm_layer", "=", "self", ".", "_norm_layer", "\n", "downsample", "=", "None", "\n", "previous_dilation", "=", "self", ".", "dilation", "\n", "if", "dilate", ":", "\n", "            ", "self", ".", "dilation", "*=", "stride", "\n", "stride", "=", "1", "\n", "", "if", "stride", "!=", "1", "or", "self", ".", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "nn", ".", "Sequential", "(", "\n", "conv1x1", "(", "self", ".", "inplanes", ",", "planes", "*", "block", ".", "expansion", ",", "stride", ")", ",", "\n", "norm_layer", "(", "planes", "*", "block", ".", "expansion", ")", ",", "\n", ")", "\n", "\n", "", "layers", "=", "[", "]", "\n", "layers", ".", "append", "(", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "stride", ",", "downsample", ",", "self", ".", "groups", ",", "\n", "self", ".", "base_width", ",", "previous_dilation", ",", "norm_layer", ")", ")", "\n", "self", ".", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "_", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "groups", "=", "self", ".", "groups", ",", "\n", "base_width", "=", "self", ".", "base_width", ",", "dilation", "=", "self", ".", "dilation", ",", "\n", "norm_layer", "=", "norm_layer", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.ResNet.forward": [[178, 194], ["resnet_model.ResNet.conv1", "resnet_model.ResNet.bn1", "resnet_model.ResNet.relu", "resnet_model.ResNet.maxpool", "resnet_model.ResNet.layer1", "resnet_model.ResNet.layer2", "resnet_model.ResNet.layer3", "resnet_model.ResNet.layer4"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "maxpool", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "layer1", "(", "x", ")", "\n", "x", "=", "self", ".", "layer2", "(", "x", ")", "\n", "x", "=", "self", ".", "layer3", "(", "x", ")", "\n", "x", "=", "self", ".", "layer4", "(", "x", ")", "\n", "\n", "# x = self.avgpool(x)", "\n", "# x = torch.flatten(x, 1)", "\n", "# x = self.fc(x)", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.conv3x3": [[6, 10], ["torch.Conv2d"], "function", ["None"], ["def", "conv3x3", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ",", "groups", "=", "1", ",", "dilation", "=", "1", ")", ":", "\n", "    ", "\"\"\"3x3 convolution with padding\"\"\"", "\n", "return", "nn", ".", "Conv2d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "dilation", ",", "groups", "=", "groups", ",", "bias", "=", "False", ",", "dilation", "=", "dilation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.conv1x1": [[12, 15], ["torch.Conv2d"], "function", ["None"], ["", "def", "conv1x1", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ")", ":", "\n", "    ", "\"\"\"1x1 convolution\"\"\"", "\n", "return", "nn", ".", "Conv2d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.resnet34": [[196, 198], ["resnet_model.ResNet"], "function", ["None"], ["", "", "def", "resnet34", "(", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "ResNet", "(", "BasicBlock", ",", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.resnet152": [[200, 202], ["resnet_model.ResNet"], "function", ["None"], ["", "def", "resnet152", "(", "pretrained", "=", "False", ",", "progress", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "ResNet", "(", "Bottleneck", ",", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_vlbert.ResNetVLBERT.__init__": [[13, 28], ["rpbert.module.Module.__init__", "rpbert.resnet_model.resnet152", "resnet_vlbert.ResNetVLBERT.pre_resnet.load_state_dict", "print", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Embedding", "torch.Embedding", "torch.Embedding", "external.pytorch_pretrained_bert.BertTokenizer.from_pretrained", "rpbert.visual_linguistic_bert.VisualLinguisticBert", "resnet_vlbert.ResNetVLBERT.init_weight", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_model.resnet152", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.pytorch_pretrained_bert.tokenization_openai.OpenAIGPTTokenizer.from_pretrained", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.init_weight", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.nn.Model.load"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "\n", "        ", "super", "(", "ResNetVLBERT", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "pre_resnet", "=", "resnet152", "(", ")", "\n", "self", ".", "pre_resnet", ".", "load_state_dict", "(", "torch", ".", "load", "(", "'/home/data/datasets/resnet152-b121ed2d.pth'", ")", ")", "\n", "print", "(", "'load resnet152 pretrained rpbert'", ")", "\n", "self", ".", "object_visual_embeddings", "=", "nn", ".", "Linear", "(", "2048", ",", "config", ".", "NETWORK", ".", "VLBERT", ".", "hidden_size", ")", "\n", "self", ".", "object_linguistic_embeddings", "=", "nn", ".", "Embedding", "(", "1", ",", "config", ".", "NETWORK", ".", "VLBERT", ".", "hidden_size", ")", "\n", "self", ".", "image_feature_bn_eval", "=", "config", ".", "NETWORK", ".", "IMAGE_FROZEN_BN", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "config", ".", "NETWORK", ".", "BERT_MODEL_NAME", ")", "\n", "self", ".", "vlbert", "=", "VisualLinguisticBert", "(", "config", ".", "NETWORK", ".", "VLBERT", ")", "\n", "\n", "# init weights", "\n", "self", ".", "init_weight", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_vlbert.ResNetVLBERT.init_weight": [[29, 33], ["resnet_vlbert.ResNetVLBERT.object_linguistic_embeddings.weight.data.normal_"], "methods", ["None"], ["", "def", "init_weight", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "object_linguistic_embeddings", "is", "not", "None", ":", "\n", "            ", "self", ".", "object_linguistic_embeddings", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "\n", "std", "=", "self", ".", "config", ".", "NETWORK", ".", "VLBERT", ".", "initializer_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_vlbert.ResNetVLBERT.fix_params": [[34, 39], ["resnet_vlbert.ResNetVLBERT.image_feature_extractor.parameters", "resnet_vlbert.ResNetVLBERT.vlbert.parameters"], "methods", ["None"], ["", "", "def", "fix_params", "(", "self", ")", ":", "\n", "        ", "for", "param", "in", "self", ".", "image_feature_extractor", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "param", "in", "self", ".", "vlbert", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.resnet_vlbert.ResNetVLBERT.forward": [[40, 83], ["resnet_vlbert.ResNetVLBERT.pre_resnet", "img_feature.view().transpose.view().transpose.view().transpose", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "expression.new_zeros", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "expression.new_zeros.new_zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "resnet_vlbert.ResNetVLBERT.object_visual_embeddings", "resnet_vlbert.ResNetVLBERT.object_linguistic_embeddings", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "resnet_vlbert.ResNetVLBERT.vlbert", "torch.ones.new_zeros().long", "torch.ones.new_zeros().long", "torch.ones.new_zeros().long", "img_feature.view().transpose.view().transpose.view", "torch.ones.new_zeros", "torch.ones.new_zeros", "torch.ones.new_zeros"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "\n", "image", ",", "\n", "expression", ",", "\n", "relation", "=", "None", ")", ":", "\n", "\n", "        ", "batch_size", "=", "expression", ".", "shape", "[", "0", "]", "\n", "images", "=", "image", "\n", "\n", "img_feature", "=", "self", ".", "pre_resnet", "(", "images", ")", "\n", "img_feature", "=", "img_feature", ".", "view", "(", "batch_size", ",", "2048", ",", "7", "*", "7", ")", ".", "transpose", "(", "2", ",", "1", ")", "\n", "\n", "box_mask", "=", "torch", ".", "ones", "(", "(", "batch_size", ",", "49", ")", ",", "dtype", "=", "torch", ".", "bool", ",", "device", "=", "expression", ".", "device", ")", "\n", "\n", "text_input_ids", "=", "expression", ".", "new_zeros", "(", "(", "expression", ".", "shape", "[", "0", "]", ",", "expression", ".", "shape", "[", "1", "]", ")", ")", "\n", "text_input_ids", "[", ":", ",", ":", "]", "=", "expression", "\n", "_sep_pos", "=", "(", "text_input_ids", ">", "0", ")", ".", "sum", "(", "1", ")", "\n", "_batch_inds", "=", "torch", ".", "arange", "(", "expression", ".", "shape", "[", "0", "]", ",", "device", "=", "expression", ".", "device", ")", "\n", "text_token_type_ids", "=", "text_input_ids", ".", "new_zeros", "(", "text_input_ids", ".", "shape", ")", "\n", "text_mask", "=", "text_input_ids", ">", "0", "\n", "text_visual_embeddings", "=", "torch", ".", "zeros", "(", "\n", "(", "text_input_ids", ".", "shape", "[", "0", "]", ",", "text_input_ids", ".", "shape", "[", "1", "]", ",", "self", ".", "config", ".", "NETWORK", ".", "VLBERT", ".", "hidden_size", ")", ",", "\n", "device", "=", "expression", ".", "device", ",", "\n", ")", "\n", "object_visual_embedding", "=", "self", ".", "object_visual_embeddings", "(", "img_feature", ")", "\n", "object_linguistic_embeddings", "=", "self", ".", "object_linguistic_embeddings", "(", "\n", "box_mask", ".", "new_zeros", "(", "(", "box_mask", ".", "shape", "[", "0", "]", ",", "box_mask", ".", "shape", "[", "1", "]", ")", ")", ".", "long", "(", ")", "\n", ")", "\n", "object_vl_embeddings", "=", "torch", ".", "cat", "(", "(", "object_visual_embedding", ",", "object_linguistic_embeddings", ")", ",", "-", "1", ")", "\n", "\n", "###########################################", "\n", "\n", "# Visual Linguistic BERT", "\n", "\n", "hidden_states_text", ",", "hidden_states_regions", ",", "_", ",", "attention_probs", "=", "self", ".", "vlbert", "(", "text_input_ids", ",", "\n", "text_token_type_ids", ",", "\n", "text_visual_embeddings", ",", "\n", "text_mask", ",", "\n", "object_vl_embeddings", ",", "\n", "box_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "\n", "output_text_and_object_separately", "=", "True", ",", "\n", "relation", "=", "relation", ")", "\n", "return", "hidden_states_text", ",", "attention_probs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.__init__": [[9, 12], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "Module", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.init_weight": [[13, 15], ["NotImplementedError"], "methods", ["None"], ["", "def", "init_weight", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.fix_params": [[16, 18], ["NotImplementedError"], "methods", ["None"], ["", "def", "fix_params", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.forward": [[19, 25], ["module.Module.preprocess", "module.Module.train_forward", "module.Module.inference_forward"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.preprocess", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.train_forward", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.inference_forward"], ["", "def", "forward", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "inputs", ",", "kwargs", "=", "self", ".", "preprocess", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "self", ".", "training", ":", "\n", "            ", "return", "self", ".", "train_forward", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "inference_forward", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.train_forward": [[26, 43], ["None"], "methods", ["None"], ["", "", "def", "train_forward", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        def train_forward(self, data, label, **kwargs):\n            # this is a toy example for 1 output, 2 loss function\n\n            output = None\n            loss1 = torch.tensor(0.0)\n            loss2 = torch.tensor(0.0)\n\n            outputs = {'output': output,\n                       'loss1': loss1,\n                       'loss2': loss2}\n            loss = loss1 + loss2\n\n            return outputs, loss\n        \"\"\"", "\n", "raise", "NotImplemented", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.inference_forward": [[44, 52], ["None"], "methods", ["None"], ["", "def", "inference_forward", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        def inference_forward(self, data, **kwargs):\n            output = None\n            outputs = {'output': output}\n            return outputs\n        \"\"\"", "\n", "raise", "NotImplemented", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.preprocess": [[53, 58], ["module.Module.train_preprocess", "module.Module.inference_preprocess"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.train_preprocess", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.inference_preprocess"], ["", "def", "preprocess", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "self", ".", "training", ":", "\n", "            ", "return", "self", ".", "train_preprocess", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "inference_preprocess", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.train_preprocess": [[59, 61], ["None"], "methods", ["None"], ["", "", "def", "train_preprocess", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "inputs", ",", "kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.module.Module.inference_preprocess": [[62, 64], ["None"], "methods", ["None"], ["", "def", "inference_preprocess", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "inputs", ",", "kwargs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.bert_rel.BertRel.__init__": [[23, 28], ["super().__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "myvlbert", ")", ":", "\n", "        ", "super", "(", "BertRel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "params", "=", "params", "\n", "self", ".", "mybert", "=", "myvlbert", "\n", "self", ".", "pair_preject", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "768", ",", "out_features", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.bert_rel.BertRel.forward": [[29, 41], ["bert_rel.BertRel.mybert", "bert_rel.BertRel.pair_preject"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sentence", ",", "img_obj", ",", "sentence_lens", ",", "mask", ",", "mode", "=", "\"train\"", ")", ":", "\n", "# Get the text features", "\n", "# u = self.text_encoder(sentence, sentence_lens, chars)", "\n", "# batch_size = sentence.shape[0]", "\n", "# if mode == \"test\":", "\n", "#     batch_size = 1", "\n", "        ", "all_encoder_layers", ",", "attention_probs", "=", "self", ".", "mybert", "(", "img_obj", ",", "sentence", ")", "\n", "# print(all_encoder_layers.shape)", "\n", "pair_out", "=", "self", ".", "pair_preject", "(", "all_encoder_layers", "[", ":", ",", ":", "1", ",", ":", "]", ")", "\n", "# pair_out = F.relu(pair_out)", "\n", "# print(pair_out.shape)", "\n", "return", "pair_out", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.bert_rel.gelu_new": [[15, 20], ["torch.tanh", "torch.tanh", "torch.tanh", "math.sqrt", "torch.pow", "torch.pow", "torch.pow"], "function", ["None"], ["def", "gelu_new", "(", "x", ")", ":", "\n", "    ", "\"\"\" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"", "\n", "return", "0.5", "*", "x", "*", "(", "1.0", "+", "torch", ".", "tanh", "(", "math", ".", "sqrt", "(", "2.0", "/", "math", ".", "pi", ")", "*", "(", "x", "+", "0.044715", "*", "torch", ".", "pow", "(", "x", ",", "3.0", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_tensor": [[5, 7], ["torch.from_numpy().float", "torch.from_numpy", "numpy.array"], "function", ["None"], ["def", "to_tensor", "(", "array", ")", ":", "\n", "    ", "return", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "array", ")", ")", ".", "float", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.util.to_variable": [[9, 13], ["torch.cuda.is_available", "torch.autograd.Variable", "tensor.cuda.cuda"], "function", ["None"], ["", "def", "to_variable", "(", "tensor", ",", "requires_grad", "=", "False", ")", ":", "\n", "    ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "tensor", "=", "tensor", ".", "cuda", "(", ")", "\n", "", "return", "torch", ".", "autograd", ".", "Variable", "(", "tensor", ",", "requires_grad", "=", "requires_grad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.BaseModel.__init__": [[10, 13], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "super", "(", "BaseModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.BaseModel.init_weights": [[14, 26], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.BaseModel.forward": [[27, 29], ["None"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "raise", "NotImplemented", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__": [[32, 81], ["visual_linguistic_bert.BaseModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "external.pytorch_pretrained_bert.modeling.BertLayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout", "external.pytorch_pretrained_bert.modeling.BertEncoder", "visual_linguistic_bert.VisualLinguisticBert.apply", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "external.pytorch_pretrained_bert.modeling.BertLayerNorm", "external.pytorch_pretrained_bert.modeling.BertLayerNorm", "torch.Parameter", "torch.Parameter", "torch.Parameter", "visual_linguistic_bert.VisualLinguisticBert.register_parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "visual_linguistic_bert.VisualLinguisticBert.register_parameter", "external.pytorch_pretrained_bert.modeling.BertPooler", "visual_linguistic_bert.VisualLinguisticBert.visual_ln_text.weight.data.fill_", "visual_linguistic_bert.VisualLinguisticBert.visual_ln_object.weight.data.fill_", "visual_linguistic_bert.VisualLinguisticBert.word_embeddings.parameters", "torch.Embedding", "torch.Embedding", "torch.Embedding", "visual_linguistic_bert.VisualLinguisticBert.special_word_embeddings.weight.data.copy_", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "VisualLinguisticBert", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "config", "=", "config", "\n", "\n", "# embeddings", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "end_embedding", "=", "nn", ".", "Embedding", "(", "1", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "embedding_LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "embedding_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# for compatibility of roberta", "\n", "self", ".", "position_padding_idx", "=", "config", ".", "position_padding_idx", "\n", "\n", "# visual transform", "\n", "self", ".", "visual_1x1_text", "=", "None", "\n", "self", ".", "visual_1x1_object", "=", "None", "\n", "if", "config", ".", "visual_size", "!=", "config", ".", "hidden_size", ":", "\n", "            ", "self", ".", "visual_1x1_text", "=", "nn", ".", "Linear", "(", "config", ".", "visual_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "visual_1x1_object", "=", "nn", ".", "Linear", "(", "config", ".", "visual_size", ",", "config", ".", "hidden_size", ")", "\n", "", "if", "config", ".", "visual_ln", ":", "\n", "            ", "self", ".", "visual_ln_text", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "visual_ln_object", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "", "else", ":", "\n", "            ", "visual_scale_text", "=", "nn", ".", "Parameter", "(", "torch", ".", "as_tensor", "(", "self", ".", "config", ".", "visual_scale_text_init", ",", "dtype", "=", "torch", ".", "float", ")", ",", "\n", "requires_grad", "=", "True", ")", "\n", "self", ".", "register_parameter", "(", "'visual_scale_text'", ",", "visual_scale_text", ")", "\n", "visual_scale_object", "=", "nn", ".", "Parameter", "(", "torch", ".", "as_tensor", "(", "self", ".", "config", ".", "visual_scale_object_init", ",", "dtype", "=", "torch", ".", "float", ")", ",", "\n", "requires_grad", "=", "True", ")", "\n", "self", ".", "register_parameter", "(", "'visual_scale_object'", ",", "visual_scale_object", ")", "\n", "\n", "", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "\n", "if", "self", ".", "config", ".", "with_pooler", ":", "\n", "            ", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "\n", "# init weights", "\n", "", "self", ".", "apply", "(", "self", ".", "init_weights", ")", "\n", "if", "config", ".", "visual_ln", ":", "\n", "            ", "self", ".", "visual_ln_text", ".", "weight", ".", "data", ".", "fill_", "(", "self", ".", "config", ".", "visual_scale_text_init", ")", "\n", "self", ".", "visual_ln_object", ".", "weight", ".", "data", ".", "fill_", "(", "self", ".", "config", ".", "visual_scale_object_init", ")", "\n", "\n", "", "if", "config", ".", "word_embedding_frozen", ":", "\n", "            ", "for", "p", "in", "self", ".", "word_embeddings", ".", "parameters", "(", ")", ":", "\n", "                ", "p", ".", "requires_grad", "=", "False", "\n", "", "self", ".", "special_word_embeddings", "=", "nn", ".", "Embedding", "(", "NUM_SPECIAL_WORDS", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "special_word_embeddings", ".", "weight", ".", "data", ".", "copy_", "(", "self", ".", "word_embeddings", ".", "weight", ".", "data", "[", ":", "NUM_SPECIAL_WORDS", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.word_embeddings_wrapper": [[82, 90], ["visual_linguistic_bert.VisualLinguisticBert.word_embeddings", "visual_linguistic_bert.VisualLinguisticBert.special_word_embeddings", "visual_linguistic_bert.VisualLinguisticBert.word_embeddings"], "methods", ["None"], ["", "", "def", "word_embeddings_wrapper", "(", "self", ",", "input_ids", ")", ":", "\n", "        ", "if", "self", ".", "config", ".", "word_embedding_frozen", ":", "\n", "            ", "word_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "word_embeddings", "[", "input_ids", "<", "NUM_SPECIAL_WORDS", "]", "=", "self", ".", "special_word_embeddings", "(", "input_ids", "[", "input_ids", "<", "NUM_SPECIAL_WORDS", "]", ")", "\n", "return", "word_embeddings", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.forward": [[91, 175], ["visual_linguistic_bert.VisualLinguisticBert.embedding", "attention_mask.unsqueeze().unsqueeze.unsqueeze().unsqueeze.unsqueeze().unsqueeze", "attention_mask.unsqueeze().unsqueeze.unsqueeze().unsqueeze.squeeze().transpose().to", "extended_attention_mask.to.to.to", "visual_linguistic_bert.VisualLinguisticBert.encoder", "visual_linguistic_bert.VisualLinguisticBert.encoder", "visual_linguistic_bert.VisualLinguisticBert.pooler", "attention_mask.unsqueeze().unsqueeze.unsqueeze().unsqueeze.unsqueeze", "attention_mask.unsqueeze().unsqueeze.unsqueeze().unsqueeze.squeeze().transpose", "encoded_layer.new_zeros", "encoded_layers_text.append", "encoded_layers_object.append", "next", "next", "attention_mask.unsqueeze().unsqueeze.unsqueeze().unsqueeze.squeeze", "visual_linguistic_bert.VisualLinguisticBert.parameters", "visual_linguistic_bert.VisualLinguisticBert.parameters"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.embedding", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to", "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.flair.data.Image.to"], ["", "", "def", "forward", "(", "self", ",", "\n", "text_input_ids", ",", "\n", "text_token_type_ids", ",", "\n", "text_visual_embeddings", ",", "\n", "text_mask", ",", "\n", "object_vl_embeddings", ",", "\n", "object_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "\n", "output_text_and_object_separately", "=", "False", ",", "\n", "output_attention_probs", "=", "True", ",", "\n", "relation", "=", "None", ")", ":", "\n", "\n", "# get seamless concatenate embeddings and mask", "\n", "        ", "embedding_output", ",", "attention_mask", ",", "text_mask_new", ",", "object_mask_new", "=", "self", ".", "embedding", "(", "text_input_ids", ",", "\n", "text_token_type_ids", ",", "\n", "text_visual_embeddings", ",", "\n", "text_mask", ",", "\n", "object_vl_embeddings", ",", "\n", "object_mask", ",", "\n", "relation", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "embedding_mask", "=", "attention_mask", ".", "squeeze", "(", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "\n", "embedding_output", "=", "embedding_output", "*", "embedding_mask", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "attention_mask", ">", "0", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "# extended_attention_mask = 1.0 - extended_attention_mask", "\n", "# extended_attention_mask[extended_attention_mask == 0] = -1e9", "\n", "# extended_attention_mask = torch.log(", "\n", "#     extended_attention_mask", "\n", "# )", "\n", "\n", "if", "output_attention_probs", ":", "\n", "            ", "encoded_layers", ",", "attention_probs", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "\n", "output_attention_probs", "=", "output_attention_probs", ")", "\n", "", "else", ":", "\n", "            ", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "\n", "output_attention_probs", "=", "output_attention_probs", ")", "\n", "", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "config", ".", "with_pooler", "else", "None", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "\n", "", "if", "output_text_and_object_separately", ":", "\n", "            ", "if", "not", "output_all_encoded_layers", ":", "\n", "                ", "encoded_layers", "=", "[", "encoded_layers", "]", "\n", "", "encoded_layers_text", "=", "[", "]", "\n", "encoded_layers_object", "=", "[", "]", "\n", "for", "encoded_layer", "in", "encoded_layers", ":", "\n", "                ", "max_text_len", "=", "text_input_ids", ".", "shape", "[", "1", "]", "\n", "max_object_len", "=", "object_vl_embeddings", ".", "shape", "[", "1", "]", "\n", "encoded_layer_text", "=", "encoded_layer", "[", ":", ",", ":", "max_text_len", "]", "\n", "encoded_layer_object", "=", "encoded_layer", ".", "new_zeros", "(", "\n", "(", "encoded_layer", ".", "shape", "[", "0", "]", ",", "max_object_len", ",", "encoded_layer", ".", "shape", "[", "2", "]", ")", ")", "\n", "encoded_layer_object", "[", "object_mask", "]", "=", "encoded_layer", "[", "object_mask_new", "]", "\n", "encoded_layers_text", ".", "append", "(", "encoded_layer_text", ")", "\n", "encoded_layers_object", ".", "append", "(", "encoded_layer_object", ")", "\n", "", "if", "not", "output_all_encoded_layers", ":", "\n", "                ", "encoded_layers_text", "=", "encoded_layers_text", "[", "0", "]", "\n", "encoded_layers_object", "=", "encoded_layers_object", "[", "0", "]", "\n", "", "if", "output_attention_probs", ":", "\n", "                ", "return", "encoded_layers_text", ",", "encoded_layers_object", ",", "pooled_output", ",", "attention_probs", "\n", "", "else", ":", "\n", "                ", "return", "encoded_layers_text", ",", "encoded_layers_object", ",", "pooled_output", "\n", "", "", "else", ":", "\n", "            ", "if", "output_attention_probs", ":", "\n", "                ", "return", "encoded_layers", ",", "pooled_output", ",", "attention_probs", "\n", "", "else", ":", "\n", "                ", "return", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.embedding": [[176, 253], ["visual_linguistic_bert.VisualLinguisticBert.word_embeddings_wrapper", "text_vl_embeddings.size", "text_vl_embeddings.size", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "text_mask.sum", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "text_vl_embeddings.new_zeros", "text_token_type_ids.new_zeros", "visual_linguistic_bert.VisualLinguisticBert.token_type_embeddings", "visual_linguistic_bert.VisualLinguisticBert.position_embeddings", "text_mask.new_zeros", "visual_linguistic_bert.VisualLinguisticBert.embedding_LayerNorm", "visual_linguistic_bert.VisualLinguisticBert.embedding_dropout", "visual_linguistic_bert.VisualLinguisticBert.visual_1x1_text", "visual_linguistic_bert.VisualLinguisticBert.visual_ln_text", "visual_linguistic_bert.VisualLinguisticBert.visual_1x1_object", "visual_linguistic_bert.VisualLinguisticBert.visual_ln_object", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "relation[].view", "relation.repeat.repeat.repeat", "object_mask.sum", "text_mask.sum", "object_mask.sum", "text_mask.sum.expand"], "methods", ["home.repos.pwc.inspect_result.Multimodal-NER_RpBERT.rpbert.visual_linguistic_bert.VisualLinguisticBert.word_embeddings_wrapper"], ["", "", "", "def", "embedding", "(", "self", ",", "\n", "text_input_ids", ",", "\n", "text_token_type_ids", ",", "\n", "text_visual_embeddings", ",", "\n", "text_mask", ",", "\n", "object_vl_embeddings", ",", "\n", "object_mask", ",", "\n", "relation", "=", "None", ")", ":", "\n", "\n", "        ", "text_linguistic_embedding", "=", "self", ".", "word_embeddings_wrapper", "(", "text_input_ids", ")", "\n", "if", "self", ".", "visual_1x1_text", "is", "not", "None", ":", "\n", "            ", "text_visual_embeddings", "=", "self", ".", "visual_1x1_text", "(", "text_visual_embeddings", ")", "\n", "", "if", "self", ".", "config", ".", "visual_ln", ":", "\n", "            ", "text_visual_embeddings", "=", "self", ".", "visual_ln_text", "(", "text_visual_embeddings", ")", "\n", "", "else", ":", "\n", "            ", "text_visual_embeddings", "*=", "self", ".", "visual_scale_text", "\n", "", "text_vl_embeddings", "=", "text_linguistic_embedding", "+", "text_visual_embeddings", "\n", "\n", "object_visual_embeddings", "=", "object_vl_embeddings", "[", ":", ",", ":", ",", ":", "self", ".", "config", ".", "visual_size", "]", "\n", "if", "self", ".", "visual_1x1_object", "is", "not", "None", ":", "\n", "            ", "object_visual_embeddings", "=", "self", ".", "visual_1x1_object", "(", "object_visual_embeddings", ")", "\n", "", "if", "self", ".", "config", ".", "visual_ln", ":", "\n", "            ", "object_visual_embeddings", "=", "self", ".", "visual_ln_object", "(", "object_visual_embeddings", ")", "\n", "", "else", ":", "\n", "            ", "object_visual_embeddings", "*=", "self", ".", "visual_scale_object", "\n", "", "object_linguistic_embeddings", "=", "object_vl_embeddings", "[", ":", ",", ":", ",", "self", ".", "config", ".", "visual_size", ":", "]", "\n", "object_vl_embeddings", "=", "object_linguistic_embeddings", "+", "object_visual_embeddings", "\n", "\n", "bs", "=", "text_vl_embeddings", ".", "size", "(", "0", ")", "\n", "vl_embed_size", "=", "text_vl_embeddings", ".", "size", "(", "-", "1", ")", "\n", "max_length", "=", "(", "text_mask", ".", "sum", "(", "1", ")", "+", "object_mask", ".", "sum", "(", "1", ")", ")", ".", "max", "(", ")", "\n", "grid_ind", ",", "grid_pos", "=", "torch", ".", "meshgrid", "(", "torch", ".", "arange", "(", "bs", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "text_vl_embeddings", ".", "device", ")", ",", "\n", "torch", ".", "arange", "(", "max_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "text_vl_embeddings", ".", "device", ")", ")", "\n", "text_end", "=", "text_mask", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "object_end", "=", "text_end", "+", "object_mask", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", "-", "1", "\n", "\n", "# seamlessly concatenate visual linguistic embeddings of text and object", "\n", "_zero_id", "=", "torch", ".", "zeros", "(", "(", "bs", ",", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "text_vl_embeddings", ".", "device", ")", "\n", "vl_embeddings", "=", "text_vl_embeddings", ".", "new_zeros", "(", "(", "bs", ",", "max_length", ",", "vl_embed_size", ")", ")", "\n", "vl_embeddings", "[", "grid_pos", "<", "text_end", "]", "=", "text_vl_embeddings", "[", "text_mask", "]", "\n", "vl_embeddings", "[", "(", "grid_pos", ">=", "text_end", ")", "&", "(", "grid_pos", "<=", "object_end", ")", "]", "=", "object_vl_embeddings", "[", "object_mask", "]", "\n", "# vl_embeddings[grid_pos == object_end] = self.end_embedding(_zero_id)", "\n", "\n", "# token type embeddings/ segment embeddings", "\n", "token_type_ids", "=", "text_token_type_ids", ".", "new_zeros", "(", "(", "bs", ",", "max_length", ")", ")", "\n", "token_type_ids", "[", "grid_pos", "<", "text_end", "]", "=", "1", "\n", "token_type_ids", "[", "(", "grid_pos", ">=", "text_end", ")", "&", "(", "grid_pos", "<=", "object_end", ")", "]", "=", "2", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "# position embeddings", "\n", "position_ids", "=", "grid_pos", "+", "self", ".", "position_padding_idx", "+", "1", "\n", "if", "self", ".", "config", ".", "obj_pos_id_relative", ":", "\n", "            ", "position_ids", "[", "(", "grid_pos", ">=", "text_end", ")", "&", "(", "grid_pos", "<=", "object_end", ")", "]", "=", "text_end", ".", "expand", "(", "(", "bs", ",", "max_length", ")", ")", "[", "(", "grid_pos", ">=", "text_end", ")", "&", "(", "grid_pos", "<=", "object_end", ")", "]", "+", "self", ".", "position_padding_idx", "+", "1", "\n", "# position_ids[grid_pos == object_end] = (text_end + 1).squeeze(1) + self.position_padding_idx + 1", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "\"Don't use position id 510/511 for objects and [END]!!!\"", "\n", "position_ids", "[", "(", "grid_pos", ">=", "text_end", ")", "&", "(", "grid_pos", "<=", "object_end", ")", "]", "=", "self", ".", "config", ".", "max_position_embeddings", "-", "2", "\n", "# position_ids[grid_pos == object_end] = self.config.max_position_embeddings - 1", "\n", "\n", "", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "mask", "=", "text_mask", ".", "new_zeros", "(", "(", "bs", ",", "max_length", ")", ")", "\n", "if", "relation", "is", "None", ":", "\n", "            ", "mask", "[", "grid_pos", "<=", "object_end", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "relation", "=", "relation", "[", ":", ",", ":", ",", ":", "1", "]", ".", "view", "(", "bs", ",", "1", ")", "\n", "relation", "=", "relation", ".", "repeat", "(", "1", ",", "mask", ".", "shape", "[", "1", "]", ")", "\n", "mask", "=", "relation", "\n", "mask", "[", "grid_pos", "<", "text_end", "]", "=", "1", "\n", "mask", "[", "grid_pos", ">", "object_end", "]", "=", "0", "\n", "\n", "", "embeddings", "=", "vl_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "embedding_LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "embedding_dropout", "(", "embeddings", ")", "\n", "\n", "return", "embeddings", ",", "mask", ",", "grid_pos", "<", "text_end", ",", "(", "grid_pos", ">=", "text_end", ")", "&", "(", "grid_pos", "<=", "object_end", ")", "", "", "", ""]]}