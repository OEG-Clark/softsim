{"home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._load_words": [[536, 551], ["conll18_ud_eval.load_conllu", "w.split", "len", "lines.append", "lines.append", "lines.append", "int", "int", "len"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.load_conllu"], ["    ", "@", "staticmethod", "\n", "def", "_load_words", "(", "words", ")", ":", "\n", "        ", "\"\"\"Prepare fake CoNLL-U files with fake HEAD to prevent multiple roots errors.\"\"\"", "\n", "lines", ",", "num_words", "=", "[", "]", ",", "0", "\n", "for", "w", "in", "words", ":", "\n", "            ", "parts", "=", "w", ".", "split", "(", "\" \"", ")", "\n", "if", "len", "(", "parts", ")", "==", "1", ":", "\n", "                ", "num_words", "+=", "1", "\n", "lines", ".", "append", "(", "\"{}\\t{}\\t_\\t_\\t_\\t_\\t{}\\t_\\t_\\t_\"", ".", "format", "(", "num_words", ",", "parts", "[", "0", "]", ",", "int", "(", "num_words", ">", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "lines", ".", "append", "(", "\"{}-{}\\t{}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_\"", ".", "format", "(", "num_words", "+", "1", ",", "num_words", "+", "len", "(", "parts", ")", "-", "1", ",", "parts", "[", "0", "]", ")", ")", "\n", "for", "part", "in", "parts", "[", "1", ":", "]", ":", "\n", "                    ", "num_words", "+=", "1", "\n", "lines", ".", "append", "(", "\"{}\\t{}\\t_\\t_\\t_\\t_\\t{}\\t_\\t_\\t_\"", ".", "format", "(", "num_words", ",", "part", ",", "int", "(", "num_words", ">", "1", ")", ")", ")", "\n", "", "", "", "return", "load_conllu", "(", "(", "io", ".", "StringIO", "if", "sys", ".", "version_info", ">=", "(", "3", ",", "0", ")", "else", "io", ".", "BytesIO", ")", "(", "\"\\n\"", ".", "join", "(", "lines", "+", "[", "\"\\n\"", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_exception": [[552, 554], ["conll18_ud_eval.TestAlignment.assertRaises", "conll18_ud_eval.TestAlignment._load_words", "conll18_ud_eval.TestAlignment._load_words"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._load_words", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._load_words"], ["", "def", "_test_exception", "(", "self", ",", "gold", ",", "system", ")", ":", "\n", "        ", "self", ".", "assertRaises", "(", "UDError", ",", "evaluate", ",", "self", ".", "_load_words", "(", "gold", ")", ",", "self", ".", "_load_words", "(", "system", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok": [[555, 561], ["conll18_ud_eval.evaluate", "sum", "sum", "conll18_ud_eval.TestAlignment.assertEqual", "conll18_ud_eval.TestAlignment._load_words", "conll18_ud_eval.TestAlignment._load_words", "max", "max", "len", "len", "word.split", "word.split"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.evaluate", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._load_words", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._load_words"], ["", "def", "_test_ok", "(", "self", ",", "gold", ",", "system", ",", "correct", ")", ":", "\n", "        ", "metrics", "=", "evaluate", "(", "self", ".", "_load_words", "(", "gold", ")", ",", "self", ".", "_load_words", "(", "system", ")", ")", "\n", "gold_words", "=", "sum", "(", "(", "max", "(", "1", ",", "len", "(", "word", ".", "split", "(", "\" \"", ")", ")", "-", "1", ")", "for", "word", "in", "gold", ")", ")", "\n", "system_words", "=", "sum", "(", "(", "max", "(", "1", ",", "len", "(", "word", ".", "split", "(", "\" \"", ")", ")", "-", "1", ")", "for", "word", "in", "system", ")", ")", "\n", "self", ".", "assertEqual", "(", "(", "metrics", "[", "\"Words\"", "]", ".", "precision", ",", "metrics", "[", "\"Words\"", "]", ".", "recall", ",", "metrics", "[", "\"Words\"", "]", ".", "f1", ")", ",", "\n", "(", "correct", "/", "system_words", ",", "correct", "/", "gold_words", ",", "2", "*", "correct", "/", "(", "gold_words", "+", "system_words", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment.test_exception": [[562, 564], ["conll18_ud_eval.TestAlignment._test_exception"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_exception"], ["", "def", "test_exception", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_exception", "(", "[", "\"a\"", "]", ",", "[", "\"b\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment.test_equal": [[565, 568], ["conll18_ud_eval.TestAlignment._test_ok", "conll18_ud_eval.TestAlignment._test_ok"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok"], ["", "def", "test_equal", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_ok", "(", "[", "\"a\"", "]", ",", "[", "\"a\"", "]", ",", "1", ")", "\n", "self", ".", "_test_ok", "(", "[", "\"a\"", ",", "\"b\"", ",", "\"c\"", "]", ",", "[", "\"a\"", ",", "\"b\"", ",", "\"c\"", "]", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment.test_equal_with_multiword": [[569, 574], ["conll18_ud_eval.TestAlignment._test_ok", "conll18_ud_eval.TestAlignment._test_ok", "conll18_ud_eval.TestAlignment._test_ok", "conll18_ud_eval.TestAlignment._test_ok"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok"], ["", "def", "test_equal_with_multiword", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_ok", "(", "[", "\"abc a b c\"", "]", ",", "[", "\"a\"", ",", "\"b\"", ",", "\"c\"", "]", ",", "3", ")", "\n", "self", ".", "_test_ok", "(", "[", "\"a\"", ",", "\"bc b c\"", ",", "\"d\"", "]", ",", "[", "\"a\"", ",", "\"b\"", ",", "\"c\"", ",", "\"d\"", "]", ",", "4", ")", "\n", "self", ".", "_test_ok", "(", "[", "\"abcd a b c d\"", "]", ",", "[", "\"ab a b\"", ",", "\"cd c d\"", "]", ",", "4", ")", "\n", "self", ".", "_test_ok", "(", "[", "\"abc a b c\"", ",", "\"de d e\"", "]", ",", "[", "\"a\"", ",", "\"bcd b c d\"", ",", "\"e\"", "]", ",", "5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment.test_alignment": [[575, 583], ["conll18_ud_eval.TestAlignment._test_ok", "conll18_ud_eval.TestAlignment._test_ok", "conll18_ud_eval.TestAlignment._test_ok", "conll18_ud_eval.TestAlignment._test_ok", "conll18_ud_eval.TestAlignment._test_ok", "conll18_ud_eval.TestAlignment._test_ok", "conll18_ud_eval.TestAlignment._test_ok"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.TestAlignment._test_ok"], ["", "def", "test_alignment", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_ok", "(", "[", "\"abcd\"", "]", ",", "[", "\"a\"", ",", "\"b\"", ",", "\"c\"", ",", "\"d\"", "]", ",", "0", ")", "\n", "self", ".", "_test_ok", "(", "[", "\"abc\"", ",", "\"d\"", "]", ",", "[", "\"a\"", ",", "\"b\"", ",", "\"c\"", ",", "\"d\"", "]", ",", "1", ")", "\n", "self", ".", "_test_ok", "(", "[", "\"a\"", ",", "\"bc\"", ",", "\"d\"", "]", ",", "[", "\"a\"", ",", "\"b\"", ",", "\"c\"", ",", "\"d\"", "]", ",", "2", ")", "\n", "self", ".", "_test_ok", "(", "[", "\"a\"", ",", "\"bc b c\"", ",", "\"d\"", "]", ",", "[", "\"a\"", ",", "\"b\"", ",", "\"cd\"", "]", ",", "2", ")", "\n", "self", ".", "_test_ok", "(", "[", "\"abc a BX c\"", ",", "\"def d EX f\"", "]", ",", "[", "\"ab a b\"", ",", "\"cd c d\"", ",", "\"ef e f\"", "]", ",", "4", ")", "\n", "self", ".", "_test_ok", "(", "[", "\"ab a b\"", ",", "\"cd bc d\"", "]", ",", "[", "\"a\"", ",", "\"bc\"", ",", "\"d\"", "]", ",", "2", ")", "\n", "self", ".", "_test_ok", "(", "[", "\"a\"", ",", "\"bc b c\"", ",", "\"d\"", "]", ",", "[", "\"ab AX BX\"", ",", "\"cd CX a\"", "]", ",", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval._decode": [[127, 129], ["text.decode", "isinstance"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.decode"], ["", "def", "_decode", "(", "text", ")", ":", "\n", "    ", "return", "text", "if", "sys", ".", "version_info", "[", "0", "]", ">=", "3", "or", "not", "isinstance", "(", "text", ",", "str", ")", "else", "text", ".", "decode", "(", "\"utf-8\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval._encode": [[130, 132], ["text.encode", "isinstance"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.encode"], ["", "def", "_encode", "(", "text", ")", ":", "\n", "    ", "return", "text", "if", "sys", ".", "version_info", "[", "0", "]", ">=", "3", "or", "not", "isinstance", "(", "text", ",", "unicode", ")", "else", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.load_conllu": [[134, 282], ["UDRepresentation", "file.readline", "conll18_ud_eval._decode", "_decode.split", "UDRepresentation.characters.extend", "UDRepresentation.tokens.append", "len", "conll18_ud_eval.UDError", "_decode.rstrip", "_decode.startswith", "UDRepresentation.sentences.append", "len", "len", "conll18_ud_eval.UDError", "filter", "conll18_ud_eval.UDError", "UDSpan", "range", "UDRepresentation.words.append", "sorted", "columns[].split", "UDSpan", "conll18_ud_eval.load_conllu.process_word"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval._decode"], ["", "def", "load_conllu", "(", "file", ")", ":", "\n", "# Internal representation classes", "\n", "    ", "class", "UDRepresentation", ":", "\n", "        ", "def", "__init__", "(", "self", ")", ":", "\n", "# Characters of all the tokens in the whole file.", "\n", "# Whitespace between tokens is not included.", "\n", "            ", "self", ".", "characters", "=", "[", "]", "\n", "# List of UDSpan instances with start&end indices into `characters`.", "\n", "self", ".", "tokens", "=", "[", "]", "\n", "# List of UDWord instances.", "\n", "self", ".", "words", "=", "[", "]", "\n", "# List of UDSpan instances with start&end indices into `characters`.", "\n", "self", ".", "sentences", "=", "[", "]", "\n", "", "", "class", "UDSpan", ":", "\n", "        ", "def", "__init__", "(", "self", ",", "start", ",", "end", ")", ":", "\n", "            ", "self", ".", "start", "=", "start", "\n", "# Note that self.end marks the first position **after the end** of span,", "\n", "# so we can use characters[start:end] or range(start, end).", "\n", "self", ".", "end", "=", "end", "\n", "", "", "class", "UDWord", ":", "\n", "        ", "def", "__init__", "(", "self", ",", "span", ",", "columns", ",", "is_multiword", ")", ":", "\n", "# Span of this word (or MWT, see below) within ud_representation.characters.", "\n", "            ", "self", ".", "span", "=", "span", "\n", "# 10 columns of the CoNLL-U file: ID, FORM, LEMMA,...", "\n", "self", ".", "columns", "=", "columns", "\n", "# is_multiword==True means that this word is part of a multi-word token.", "\n", "# In that case, self.span marks the span of the whole multi-word token.", "\n", "self", ".", "is_multiword", "=", "is_multiword", "\n", "# Reference to the UDWord instance representing the HEAD (or None if root).", "\n", "self", ".", "parent", "=", "None", "\n", "# List of references to UDWord instances representing functional-deprel children.", "\n", "self", ".", "functional_children", "=", "[", "]", "\n", "# Only consider universal FEATS.", "\n", "self", ".", "columns", "[", "FEATS", "]", "=", "\"|\"", ".", "join", "(", "sorted", "(", "feat", "for", "feat", "in", "columns", "[", "FEATS", "]", ".", "split", "(", "\"|\"", ")", "\n", "if", "feat", ".", "split", "(", "\"=\"", ",", "1", ")", "[", "0", "]", "in", "UNIVERSAL_FEATURES", ")", ")", "\n", "# Let's ignore language-specific deprel subtypes.", "\n", "self", ".", "columns", "[", "DEPREL", "]", "=", "columns", "[", "DEPREL", "]", ".", "split", "(", "\":\"", ")", "[", "0", "]", "\n", "# Precompute which deprels are CONTENT_DEPRELS and which FUNCTIONAL_DEPRELS", "\n", "self", ".", "is_content_deprel", "=", "self", ".", "columns", "[", "DEPREL", "]", "in", "CONTENT_DEPRELS", "\n", "self", ".", "is_functional_deprel", "=", "self", ".", "columns", "[", "DEPREL", "]", "in", "FUNCTIONAL_DEPRELS", "\n", "\n", "", "", "ud", "=", "UDRepresentation", "(", ")", "\n", "\n", "# Load the CoNLL-U file", "\n", "index", ",", "sentence_start", "=", "0", ",", "None", "\n", "while", "True", ":", "\n", "        ", "line", "=", "file", ".", "readline", "(", ")", "\n", "if", "not", "line", ":", "\n", "            ", "break", "\n", "", "line", "=", "_decode", "(", "line", ".", "rstrip", "(", "\"\\r\\n\"", ")", ")", "\n", "\n", "# Handle sentence start boundaries", "\n", "if", "sentence_start", "is", "None", ":", "\n", "# Skip comments", "\n", "            ", "if", "line", ".", "startswith", "(", "\"#\"", ")", ":", "\n", "                ", "continue", "\n", "# Start a new sentence", "\n", "", "ud", ".", "sentences", ".", "append", "(", "UDSpan", "(", "index", ",", "0", ")", ")", "\n", "sentence_start", "=", "len", "(", "ud", ".", "words", ")", "\n", "", "if", "not", "line", ":", "\n", "# Add parent and children UDWord links and check there are no cycles", "\n", "            ", "def", "process_word", "(", "word", ")", ":", "\n", "                ", "if", "word", ".", "parent", "==", "\"remapping\"", ":", "\n", "                    ", "raise", "UDError", "(", "\"There is a cycle in a sentence\"", ")", "\n", "", "if", "word", ".", "parent", "is", "None", ":", "\n", "                    ", "head", "=", "int", "(", "word", ".", "columns", "[", "HEAD", "]", ")", "\n", "if", "head", "<", "0", "or", "head", ">", "len", "(", "ud", ".", "words", ")", "-", "sentence_start", ":", "\n", "                        ", "raise", "UDError", "(", "\"HEAD '{}' points outside of the sentence\"", ".", "format", "(", "_encode", "(", "word", ".", "columns", "[", "HEAD", "]", ")", ")", ")", "\n", "", "if", "head", ":", "\n", "                        ", "parent", "=", "ud", ".", "words", "[", "sentence_start", "+", "head", "-", "1", "]", "\n", "word", ".", "parent", "=", "\"remapping\"", "\n", "process_word", "(", "parent", ")", "\n", "word", ".", "parent", "=", "parent", "\n", "\n", "", "", "", "for", "word", "in", "ud", ".", "words", "[", "sentence_start", ":", "]", ":", "\n", "                ", "process_word", "(", "word", ")", "\n", "# func_children cannot be assigned within process_word", "\n", "# because it is called recursively and may result in adding one child twice.", "\n", "", "for", "word", "in", "ud", ".", "words", "[", "sentence_start", ":", "]", ":", "\n", "                ", "if", "word", ".", "parent", "and", "word", ".", "is_functional_deprel", ":", "\n", "                    ", "word", ".", "parent", ".", "functional_children", ".", "append", "(", "word", ")", "\n", "\n", "# Check there is a single root node", "\n", "", "", "if", "len", "(", "[", "word", "for", "word", "in", "ud", ".", "words", "[", "sentence_start", ":", "]", "if", "word", ".", "parent", "is", "None", "]", ")", "!=", "1", ":", "\n", "                ", "raise", "UDError", "(", "\"There are multiple roots in a sentence\"", ")", "\n", "\n", "# End the sentence", "\n", "", "ud", ".", "sentences", "[", "-", "1", "]", ".", "end", "=", "index", "\n", "sentence_start", "=", "None", "\n", "continue", "\n", "\n", "# Read next token/word", "\n", "", "columns", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "len", "(", "columns", ")", "!=", "10", ":", "\n", "            ", "raise", "UDError", "(", "\"The CoNLL-U line does not contain 10 tab-separated columns: '{}'\"", ".", "format", "(", "_encode", "(", "line", ")", ")", ")", "\n", "\n", "# Skip empty nodes", "\n", "", "if", "\".\"", "in", "columns", "[", "ID", "]", ":", "\n", "            ", "continue", "\n", "\n", "# Delete spaces from FORM, so gold.characters == system.characters", "\n", "# even if one of them tokenizes the space. Use any Unicode character", "\n", "# with category Zs.", "\n", "", "columns", "[", "FORM", "]", "=", "\"\"", ".", "join", "(", "filter", "(", "lambda", "c", ":", "unicodedata", ".", "category", "(", "c", ")", "!=", "\"Zs\"", ",", "columns", "[", "FORM", "]", ")", ")", "\n", "if", "not", "columns", "[", "FORM", "]", ":", "\n", "            ", "raise", "UDError", "(", "\"There is an empty FORM in the CoNLL-U file\"", ")", "\n", "\n", "# Save token", "\n", "", "ud", ".", "characters", ".", "extend", "(", "columns", "[", "FORM", "]", ")", "\n", "ud", ".", "tokens", ".", "append", "(", "UDSpan", "(", "index", ",", "index", "+", "len", "(", "columns", "[", "FORM", "]", ")", ")", ")", "\n", "index", "+=", "len", "(", "columns", "[", "FORM", "]", ")", "\n", "\n", "# Handle multi-word tokens to save word(s)", "\n", "if", "\"-\"", "in", "columns", "[", "ID", "]", ":", "\n", "            ", "try", ":", "\n", "                ", "start", ",", "end", "=", "map", "(", "int", ",", "columns", "[", "ID", "]", ".", "split", "(", "\"-\"", ")", ")", "\n", "", "except", ":", "\n", "                ", "raise", "UDError", "(", "\"Cannot parse multi-word token ID '{}'\"", ".", "format", "(", "_encode", "(", "columns", "[", "ID", "]", ")", ")", ")", "\n", "\n", "", "for", "_", "in", "range", "(", "start", ",", "end", "+", "1", ")", ":", "\n", "                ", "word_line", "=", "_decode", "(", "file", ".", "readline", "(", ")", ".", "rstrip", "(", "\"\\r\\n\"", ")", ")", "\n", "word_columns", "=", "word_line", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "len", "(", "word_columns", ")", "!=", "10", ":", "\n", "                    ", "raise", "UDError", "(", "\"The CoNLL-U line does not contain 10 tab-separated columns: '{}'\"", ".", "format", "(", "_encode", "(", "word_line", ")", ")", ")", "\n", "", "ud", ".", "words", ".", "append", "(", "UDWord", "(", "ud", ".", "tokens", "[", "-", "1", "]", ",", "word_columns", ",", "is_multiword", "=", "True", ")", ")", "\n", "# Basic tokens/words", "\n", "", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "word_id", "=", "int", "(", "columns", "[", "ID", "]", ")", "\n", "", "except", ":", "\n", "                ", "raise", "UDError", "(", "\"Cannot parse word ID '{}'\"", ".", "format", "(", "_encode", "(", "columns", "[", "ID", "]", ")", ")", ")", "\n", "", "if", "word_id", "!=", "len", "(", "ud", ".", "words", ")", "-", "sentence_start", "+", "1", ":", "\n", "                ", "raise", "UDError", "(", "\"Incorrect word ID '{}' for word '{}', expected '{}'\"", ".", "format", "(", "\n", "_encode", "(", "columns", "[", "ID", "]", ")", ",", "_encode", "(", "columns", "[", "FORM", "]", ")", ",", "len", "(", "ud", ".", "words", ")", "-", "sentence_start", "+", "1", ")", ")", "\n", "\n", "", "try", ":", "\n", "                ", "head_id", "=", "int", "(", "columns", "[", "HEAD", "]", ")", "\n", "", "except", ":", "\n", "                ", "raise", "UDError", "(", "\"Cannot parse HEAD '{}'\"", ".", "format", "(", "_encode", "(", "columns", "[", "HEAD", "]", ")", ")", ")", "\n", "", "if", "head_id", "<", "0", ":", "\n", "                ", "raise", "UDError", "(", "\"HEAD cannot be negative\"", ")", "\n", "\n", "", "ud", ".", "words", ".", "append", "(", "UDWord", "(", "ud", ".", "tokens", "[", "-", "1", "]", ",", "columns", ",", "is_multiword", "=", "False", ")", ")", "\n", "\n", "", "", "if", "sentence_start", "is", "not", "None", ":", "\n", "        ", "raise", "UDError", "(", "\"The CoNLL-U file does not end with empty line\"", ")", "\n", "\n", "", "return", "ud", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.evaluate": [[284, 473], ["conll18_ud_eval.evaluate.align_words"], "function", ["None"], ["", "def", "evaluate", "(", "gold_ud", ",", "system_ud", ")", ":", "\n", "    ", "class", "Score", ":", "\n", "        ", "def", "__init__", "(", "self", ",", "gold_total", ",", "system_total", ",", "correct", ",", "aligned_total", "=", "None", ")", ":", "\n", "            ", "self", ".", "correct", "=", "correct", "\n", "self", ".", "gold_total", "=", "gold_total", "\n", "self", ".", "system_total", "=", "system_total", "\n", "self", ".", "aligned_total", "=", "aligned_total", "\n", "self", ".", "precision", "=", "correct", "/", "system_total", "if", "system_total", "else", "0.0", "\n", "self", ".", "recall", "=", "correct", "/", "gold_total", "if", "gold_total", "else", "0.0", "\n", "self", ".", "f1", "=", "2", "*", "correct", "/", "(", "system_total", "+", "gold_total", ")", "if", "system_total", "+", "gold_total", "else", "0.0", "\n", "self", ".", "aligned_accuracy", "=", "correct", "/", "aligned_total", "if", "aligned_total", "else", "aligned_total", "\n", "", "", "class", "AlignmentWord", ":", "\n", "        ", "def", "__init__", "(", "self", ",", "gold_word", ",", "system_word", ")", ":", "\n", "            ", "self", ".", "gold_word", "=", "gold_word", "\n", "self", ".", "system_word", "=", "system_word", "\n", "", "", "class", "Alignment", ":", "\n", "        ", "def", "__init__", "(", "self", ",", "gold_words", ",", "system_words", ")", ":", "\n", "            ", "self", ".", "gold_words", "=", "gold_words", "\n", "self", ".", "system_words", "=", "system_words", "\n", "self", ".", "matched_words", "=", "[", "]", "\n", "self", ".", "matched_words_map", "=", "{", "}", "\n", "", "def", "append_aligned_words", "(", "self", ",", "gold_word", ",", "system_word", ")", ":", "\n", "            ", "self", ".", "matched_words", ".", "append", "(", "AlignmentWord", "(", "gold_word", ",", "system_word", ")", ")", "\n", "self", ".", "matched_words_map", "[", "system_word", "]", "=", "gold_word", "\n", "\n", "", "", "def", "spans_score", "(", "gold_spans", ",", "system_spans", ")", ":", "\n", "        ", "correct", ",", "gi", ",", "si", "=", "0", ",", "0", ",", "0", "\n", "while", "gi", "<", "len", "(", "gold_spans", ")", "and", "si", "<", "len", "(", "system_spans", ")", ":", "\n", "            ", "if", "system_spans", "[", "si", "]", ".", "start", "<", "gold_spans", "[", "gi", "]", ".", "start", ":", "\n", "                ", "si", "+=", "1", "\n", "", "elif", "gold_spans", "[", "gi", "]", ".", "start", "<", "system_spans", "[", "si", "]", ".", "start", ":", "\n", "                ", "gi", "+=", "1", "\n", "", "else", ":", "\n", "                ", "correct", "+=", "gold_spans", "[", "gi", "]", ".", "end", "==", "system_spans", "[", "si", "]", ".", "end", "\n", "si", "+=", "1", "\n", "gi", "+=", "1", "\n", "\n", "", "", "return", "Score", "(", "len", "(", "gold_spans", ")", ",", "len", "(", "system_spans", ")", ",", "correct", ")", "\n", "\n", "", "def", "alignment_score", "(", "alignment", ",", "key_fn", "=", "None", ",", "filter_fn", "=", "None", ")", ":", "\n", "        ", "if", "filter_fn", "is", "not", "None", ":", "\n", "            ", "gold", "=", "sum", "(", "1", "for", "gold", "in", "alignment", ".", "gold_words", "if", "filter_fn", "(", "gold", ")", ")", "\n", "system", "=", "sum", "(", "1", "for", "system", "in", "alignment", ".", "system_words", "if", "filter_fn", "(", "system", ")", ")", "\n", "aligned", "=", "sum", "(", "1", "for", "word", "in", "alignment", ".", "matched_words", "if", "filter_fn", "(", "word", ".", "gold_word", ")", ")", "\n", "", "else", ":", "\n", "            ", "gold", "=", "len", "(", "alignment", ".", "gold_words", ")", "\n", "system", "=", "len", "(", "alignment", ".", "system_words", ")", "\n", "aligned", "=", "len", "(", "alignment", ".", "matched_words", ")", "\n", "\n", "", "if", "key_fn", "is", "None", ":", "\n", "# Return score for whole aligned words", "\n", "            ", "return", "Score", "(", "gold", ",", "system", ",", "aligned", ")", "\n", "\n", "", "def", "gold_aligned_gold", "(", "word", ")", ":", "\n", "            ", "return", "word", "\n", "", "def", "gold_aligned_system", "(", "word", ")", ":", "\n", "            ", "return", "alignment", ".", "matched_words_map", ".", "get", "(", "word", ",", "\"NotAligned\"", ")", "if", "word", "is", "not", "None", "else", "None", "\n", "", "correct", "=", "0", "\n", "for", "words", "in", "alignment", ".", "matched_words", ":", "\n", "            ", "if", "filter_fn", "is", "None", "or", "filter_fn", "(", "words", ".", "gold_word", ")", ":", "\n", "                ", "if", "key_fn", "(", "words", ".", "gold_word", ",", "gold_aligned_gold", ")", "==", "key_fn", "(", "words", ".", "system_word", ",", "gold_aligned_system", ")", ":", "\n", "                    ", "correct", "+=", "1", "\n", "\n", "", "", "", "return", "Score", "(", "gold", ",", "system", ",", "correct", ",", "aligned", ")", "\n", "\n", "", "def", "beyond_end", "(", "words", ",", "i", ",", "multiword_span_end", ")", ":", "\n", "        ", "if", "i", ">=", "len", "(", "words", ")", ":", "\n", "            ", "return", "True", "\n", "", "if", "words", "[", "i", "]", ".", "is_multiword", ":", "\n", "            ", "return", "words", "[", "i", "]", ".", "span", ".", "start", ">=", "multiword_span_end", "\n", "", "return", "words", "[", "i", "]", ".", "span", ".", "end", ">", "multiword_span_end", "\n", "\n", "", "def", "extend_end", "(", "word", ",", "multiword_span_end", ")", ":", "\n", "        ", "if", "word", ".", "is_multiword", "and", "word", ".", "span", ".", "end", ">", "multiword_span_end", ":", "\n", "            ", "return", "word", ".", "span", ".", "end", "\n", "", "return", "multiword_span_end", "\n", "\n", "", "def", "find_multiword_span", "(", "gold_words", ",", "system_words", ",", "gi", ",", "si", ")", ":", "\n", "# We know gold_words[gi].is_multiword or system_words[si].is_multiword.", "\n", "# Find the start of the multiword span (gs, ss), so the multiword span is minimal.", "\n", "# Initialize multiword_span_end characters index.", "\n", "        ", "if", "gold_words", "[", "gi", "]", ".", "is_multiword", ":", "\n", "            ", "multiword_span_end", "=", "gold_words", "[", "gi", "]", ".", "span", ".", "end", "\n", "if", "not", "system_words", "[", "si", "]", ".", "is_multiword", "and", "system_words", "[", "si", "]", ".", "span", ".", "start", "<", "gold_words", "[", "gi", "]", ".", "span", ".", "start", ":", "\n", "                ", "si", "+=", "1", "\n", "", "", "else", ":", "# if system_words[si].is_multiword", "\n", "            ", "multiword_span_end", "=", "system_words", "[", "si", "]", ".", "span", ".", "end", "\n", "if", "not", "gold_words", "[", "gi", "]", ".", "is_multiword", "and", "gold_words", "[", "gi", "]", ".", "span", ".", "start", "<", "system_words", "[", "si", "]", ".", "span", ".", "start", ":", "\n", "                ", "gi", "+=", "1", "\n", "", "", "gs", ",", "ss", "=", "gi", ",", "si", "\n", "\n", "# Find the end of the multiword span", "\n", "# (so both gi and si are pointing to the word following the multiword span end).", "\n", "while", "not", "beyond_end", "(", "gold_words", ",", "gi", ",", "multiword_span_end", ")", "or", "not", "beyond_end", "(", "system_words", ",", "si", ",", "multiword_span_end", ")", ":", "\n", "            ", "if", "gi", "<", "len", "(", "gold_words", ")", "and", "(", "si", ">=", "len", "(", "system_words", ")", "or", "\n", "gold_words", "[", "gi", "]", ".", "span", ".", "start", "<=", "system_words", "[", "si", "]", ".", "span", ".", "start", ")", ":", "\n", "                ", "multiword_span_end", "=", "extend_end", "(", "gold_words", "[", "gi", "]", ",", "multiword_span_end", ")", "\n", "gi", "+=", "1", "\n", "", "else", ":", "\n", "                ", "multiword_span_end", "=", "extend_end", "(", "system_words", "[", "si", "]", ",", "multiword_span_end", ")", "\n", "si", "+=", "1", "\n", "", "", "return", "gs", ",", "ss", ",", "gi", ",", "si", "\n", "\n", "", "def", "compute_lcs", "(", "gold_words", ",", "system_words", ",", "gi", ",", "si", ",", "gs", ",", "ss", ")", ":", "\n", "        ", "lcs", "=", "[", "[", "0", "]", "*", "(", "si", "-", "ss", ")", "for", "i", "in", "range", "(", "gi", "-", "gs", ")", "]", "\n", "for", "g", "in", "reversed", "(", "range", "(", "gi", "-", "gs", ")", ")", ":", "\n", "            ", "for", "s", "in", "reversed", "(", "range", "(", "si", "-", "ss", ")", ")", ":", "\n", "                ", "if", "gold_words", "[", "gs", "+", "g", "]", ".", "columns", "[", "FORM", "]", ".", "lower", "(", ")", "==", "system_words", "[", "ss", "+", "s", "]", ".", "columns", "[", "FORM", "]", ".", "lower", "(", ")", ":", "\n", "                    ", "lcs", "[", "g", "]", "[", "s", "]", "=", "1", "+", "(", "lcs", "[", "g", "+", "1", "]", "[", "s", "+", "1", "]", "if", "g", "+", "1", "<", "gi", "-", "gs", "and", "s", "+", "1", "<", "si", "-", "ss", "else", "0", ")", "\n", "", "lcs", "[", "g", "]", "[", "s", "]", "=", "max", "(", "lcs", "[", "g", "]", "[", "s", "]", ",", "lcs", "[", "g", "+", "1", "]", "[", "s", "]", "if", "g", "+", "1", "<", "gi", "-", "gs", "else", "0", ")", "\n", "lcs", "[", "g", "]", "[", "s", "]", "=", "max", "(", "lcs", "[", "g", "]", "[", "s", "]", ",", "lcs", "[", "g", "]", "[", "s", "+", "1", "]", "if", "s", "+", "1", "<", "si", "-", "ss", "else", "0", ")", "\n", "", "", "return", "lcs", "\n", "\n", "", "def", "align_words", "(", "gold_words", ",", "system_words", ")", ":", "\n", "        ", "alignment", "=", "Alignment", "(", "gold_words", ",", "system_words", ")", "\n", "\n", "gi", ",", "si", "=", "0", ",", "0", "\n", "while", "gi", "<", "len", "(", "gold_words", ")", "and", "si", "<", "len", "(", "system_words", ")", ":", "\n", "            ", "if", "gold_words", "[", "gi", "]", ".", "is_multiword", "or", "system_words", "[", "si", "]", ".", "is_multiword", ":", "\n", "# A: Multi-word tokens => align via LCS within the whole \"multiword span\".", "\n", "                ", "gs", ",", "ss", ",", "gi", ",", "si", "=", "find_multiword_span", "(", "gold_words", ",", "system_words", ",", "gi", ",", "si", ")", "\n", "\n", "if", "si", ">", "ss", "and", "gi", ">", "gs", ":", "\n", "                    ", "lcs", "=", "compute_lcs", "(", "gold_words", ",", "system_words", ",", "gi", ",", "si", ",", "gs", ",", "ss", ")", "\n", "\n", "# Store aligned words", "\n", "s", ",", "g", "=", "0", ",", "0", "\n", "while", "g", "<", "gi", "-", "gs", "and", "s", "<", "si", "-", "ss", ":", "\n", "                        ", "if", "gold_words", "[", "gs", "+", "g", "]", ".", "columns", "[", "FORM", "]", ".", "lower", "(", ")", "==", "system_words", "[", "ss", "+", "s", "]", ".", "columns", "[", "FORM", "]", ".", "lower", "(", ")", ":", "\n", "                            ", "alignment", ".", "append_aligned_words", "(", "gold_words", "[", "gs", "+", "g", "]", ",", "system_words", "[", "ss", "+", "s", "]", ")", "\n", "g", "+=", "1", "\n", "s", "+=", "1", "\n", "", "elif", "lcs", "[", "g", "]", "[", "s", "]", "==", "(", "lcs", "[", "g", "+", "1", "]", "[", "s", "]", "if", "g", "+", "1", "<", "gi", "-", "gs", "else", "0", ")", ":", "\n", "                            ", "g", "+=", "1", "\n", "", "else", ":", "\n", "                            ", "s", "+=", "1", "\n", "", "", "", "", "else", ":", "\n", "# B: No multi-word token => align according to spans.", "\n", "                ", "if", "(", "gold_words", "[", "gi", "]", ".", "span", ".", "start", ",", "gold_words", "[", "gi", "]", ".", "span", ".", "end", ")", "==", "(", "system_words", "[", "si", "]", ".", "span", ".", "start", ",", "system_words", "[", "si", "]", ".", "span", ".", "end", ")", ":", "\n", "                    ", "alignment", ".", "append_aligned_words", "(", "gold_words", "[", "gi", "]", ",", "system_words", "[", "si", "]", ")", "\n", "gi", "+=", "1", "\n", "si", "+=", "1", "\n", "", "elif", "gold_words", "[", "gi", "]", ".", "span", ".", "start", "<=", "system_words", "[", "si", "]", ".", "span", ".", "start", ":", "\n", "                    ", "gi", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "si", "+=", "1", "\n", "\n", "", "", "", "return", "alignment", "\n", "\n", "# Check that the underlying character sequences do match.", "\n", "", "if", "gold_ud", ".", "characters", "!=", "system_ud", ".", "characters", ":", "\n", "        ", "index", "=", "0", "\n", "while", "index", "<", "len", "(", "gold_ud", ".", "characters", ")", "and", "index", "<", "len", "(", "system_ud", ".", "characters", ")", "and", "gold_ud", ".", "characters", "[", "index", "]", "==", "system_ud", ".", "characters", "[", "index", "]", ":", "\n", "            ", "index", "+=", "1", "\n", "\n", "", "raise", "UDError", "(", "\n", "\"The concatenation of tokens in gold file and in system file differ!\\n\"", "+", "\n", "\"First 20 differing characters in gold file: '{}' and system file: '{}'\"", ".", "format", "(", "\n", "\"\"", ".", "join", "(", "map", "(", "_encode", ",", "gold_ud", ".", "characters", "[", "index", ":", "index", "+", "20", "]", ")", ")", ",", "\n", "\"\"", ".", "join", "(", "map", "(", "_encode", ",", "system_ud", ".", "characters", "[", "index", ":", "index", "+", "20", "]", ")", ")", "\n", ")", "\n", ")", "\n", "\n", "# Align words", "\n", "", "alignment", "=", "align_words", "(", "gold_ud", ".", "words", ",", "system_ud", ".", "words", ")", "\n", "\n", "# Compute the F1-scores", "\n", "return", "{", "\n", "\"Tokens\"", ":", "spans_score", "(", "gold_ud", ".", "tokens", ",", "system_ud", ".", "tokens", ")", ",", "\n", "\"Sentences\"", ":", "spans_score", "(", "gold_ud", ".", "sentences", ",", "system_ud", ".", "sentences", ")", ",", "\n", "\"Words\"", ":", "alignment_score", "(", "alignment", ")", ",", "\n", "\"UPOS\"", ":", "alignment_score", "(", "alignment", ",", "lambda", "w", ",", "_", ":", "w", ".", "columns", "[", "UPOS", "]", ")", ",", "\n", "\"XPOS\"", ":", "alignment_score", "(", "alignment", ",", "lambda", "w", ",", "_", ":", "w", ".", "columns", "[", "XPOS", "]", ")", ",", "\n", "\"UFeats\"", ":", "alignment_score", "(", "alignment", ",", "lambda", "w", ",", "_", ":", "w", ".", "columns", "[", "FEATS", "]", ")", ",", "\n", "\"AllTags\"", ":", "alignment_score", "(", "alignment", ",", "lambda", "w", ",", "_", ":", "(", "w", ".", "columns", "[", "UPOS", "]", ",", "w", ".", "columns", "[", "XPOS", "]", ",", "w", ".", "columns", "[", "FEATS", "]", ")", ")", ",", "\n", "\"Lemmas\"", ":", "alignment_score", "(", "alignment", ",", "lambda", "w", ",", "ga", ":", "w", ".", "columns", "[", "LEMMA", "]", "if", "ga", "(", "w", ")", ".", "columns", "[", "LEMMA", "]", "!=", "\"_\"", "else", "\"_\"", ")", ",", "\n", "\"UAS\"", ":", "alignment_score", "(", "alignment", ",", "lambda", "w", ",", "ga", ":", "ga", "(", "w", ".", "parent", ")", ")", ",", "\n", "\"LAS\"", ":", "alignment_score", "(", "alignment", ",", "lambda", "w", ",", "ga", ":", "(", "ga", "(", "w", ".", "parent", ")", ",", "w", ".", "columns", "[", "DEPREL", "]", ")", ")", ",", "\n", "\"CLAS\"", ":", "alignment_score", "(", "alignment", ",", "lambda", "w", ",", "ga", ":", "(", "ga", "(", "w", ".", "parent", ")", ",", "w", ".", "columns", "[", "DEPREL", "]", ")", ",", "\n", "filter_fn", "=", "lambda", "w", ":", "w", ".", "is_content_deprel", ")", ",", "\n", "\"MLAS\"", ":", "alignment_score", "(", "alignment", ",", "lambda", "w", ",", "ga", ":", "(", "ga", "(", "w", ".", "parent", ")", ",", "w", ".", "columns", "[", "DEPREL", "]", ",", "w", ".", "columns", "[", "UPOS", "]", ",", "w", ".", "columns", "[", "FEATS", "]", ",", "\n", "[", "(", "ga", "(", "c", ")", ",", "c", ".", "columns", "[", "DEPREL", "]", ",", "c", ".", "columns", "[", "UPOS", "]", ",", "c", ".", "columns", "[", "FEATS", "]", ")", "\n", "for", "c", "in", "w", ".", "functional_children", "]", ")", ",", "\n", "filter_fn", "=", "lambda", "w", ":", "w", ".", "is_content_deprel", ")", ",", "\n", "\"BLEX\"", ":", "alignment_score", "(", "alignment", ",", "lambda", "w", ",", "ga", ":", "(", "ga", "(", "w", ".", "parent", ")", ",", "w", ".", "columns", "[", "DEPREL", "]", ",", "\n", "w", ".", "columns", "[", "LEMMA", "]", "if", "ga", "(", "w", ")", ".", "columns", "[", "LEMMA", "]", "!=", "\"_\"", "else", "\"_\"", ")", ",", "\n", "filter_fn", "=", "lambda", "w", ":", "w", ".", "is_content_deprel", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.load_conllu_file": [[476, 479], ["open", "conll18_ud_eval.load_conllu"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.load_conllu"], ["", "def", "load_conllu_file", "(", "path", ")", ":", "\n", "    ", "_file", "=", "open", "(", "path", ",", "mode", "=", "\"r\"", ",", "**", "(", "{", "\"encoding\"", ":", "\"utf-8\"", "}", "if", "sys", ".", "version_info", ">=", "(", "3", ",", "0", ")", "else", "{", "}", ")", ")", "\n", "return", "load_conllu", "(", "_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.evaluate_wrapper": [[480, 485], ["conll18_ud_eval.load_conllu_file", "conll18_ud_eval.load_conllu_file", "conll18_ud_eval.evaluate"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.load_conllu_file", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.load_conllu_file", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.evaluate"], ["", "def", "evaluate_wrapper", "(", "args", ")", ":", "\n", "# Load CoNLL-U files", "\n", "    ", "gold_ud", "=", "load_conllu_file", "(", "args", ".", "gold_file", ")", "\n", "system_ud", "=", "load_conllu_file", "(", "args", ".", "system_file", ")", "\n", "return", "evaluate", "(", "gold_ud", ",", "system_ud", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.main": [[486, 529], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "conll18_ud_eval.evaluate_wrapper", "print", "print", "print", "print", "print", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.parse_args", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.evaluate_wrapper"], ["", "def", "main", "(", ")", ":", "\n", "# Parse arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"gold_file\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Name of the CoNLL-U file with the gold data.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"system_file\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Name of the CoNLL-U file with the predicted data.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--verbose\"", ",", "\"-v\"", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Print all metrics.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--counts\"", ",", "\"-c\"", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Print raw counts of correct/gold/system/aligned words instead of prec/rec/F1 for all metrics.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# Evaluate", "\n", "evaluation", "=", "evaluate_wrapper", "(", "args", ")", "\n", "\n", "# Print the evaluation", "\n", "if", "not", "args", ".", "verbose", "and", "not", "args", ".", "counts", ":", "\n", "        ", "print", "(", "\"LAS F1 Score: {:.2f}\"", ".", "format", "(", "100", "*", "evaluation", "[", "\"LAS\"", "]", ".", "f1", ")", ")", "\n", "print", "(", "\"MLAS Score: {:.2f}\"", ".", "format", "(", "100", "*", "evaluation", "[", "\"MLAS\"", "]", ".", "f1", ")", ")", "\n", "print", "(", "\"BLEX Score: {:.2f}\"", ".", "format", "(", "100", "*", "evaluation", "[", "\"BLEX\"", "]", ".", "f1", ")", ")", "\n", "", "else", ":", "\n", "        ", "if", "args", ".", "counts", ":", "\n", "            ", "print", "(", "\"Metric     | Correct   |      Gold | Predicted | Aligned\"", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Metric     | Precision |    Recall |  F1 Score | AligndAcc\"", ")", "\n", "", "print", "(", "\"-----------+-----------+-----------+-----------+-----------\"", ")", "\n", "for", "metric", "in", "[", "\"Tokens\"", ",", "\"Sentences\"", ",", "\"Words\"", ",", "\"UPOS\"", ",", "\"XPOS\"", ",", "\"UFeats\"", ",", "\"AllTags\"", ",", "\"Lemmas\"", ",", "\"UAS\"", ",", "\"LAS\"", ",", "\"CLAS\"", ",", "\"MLAS\"", ",", "\"BLEX\"", "]", ":", "\n", "            ", "if", "args", ".", "counts", ":", "\n", "                ", "print", "(", "\"{:11}|{:10} |{:10} |{:10} |{:10}\"", ".", "format", "(", "\n", "metric", ",", "\n", "evaluation", "[", "metric", "]", ".", "correct", ",", "\n", "evaluation", "[", "metric", "]", ".", "gold_total", ",", "\n", "evaluation", "[", "metric", "]", ".", "system_total", ",", "\n", "evaluation", "[", "metric", "]", ".", "aligned_total", "or", "(", "evaluation", "[", "metric", "]", ".", "correct", "if", "metric", "==", "\"Words\"", "else", "\"\"", ")", "\n", ")", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"{:11}|{:10.2f} |{:10.2f} |{:10.2f} |{}\"", ".", "format", "(", "\n", "metric", ",", "\n", "100", "*", "evaluation", "[", "metric", "]", ".", "precision", ",", "\n", "100", "*", "evaluation", "[", "metric", "]", ".", "recall", ",", "\n", "100", "*", "evaluation", "[", "metric", "]", ".", "f1", ",", "\n", "\"{:10.2f}\"", ".", "format", "(", "100", "*", "evaluation", "[", "metric", "]", ".", "aligned_accuracy", ")", "if", "evaluation", "[", "metric", "]", ".", "aligned_accuracy", "is", "not", "None", "else", "\"\"", "\n", ")", ")", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.paired_bootstrap.load_file": [[8, 13], ["lexenlem.models.common.conll.CoNLLFile", "conll.CoNLLFile.get"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get"], ["def", "load_file", "(", "filename", ")", ":", "\n", "    ", "\"\"\"Loads the CONLL file\"\"\"", "\n", "conll_file", "=", "conll", ".", "CoNLLFile", "(", "filename", ")", "\n", "data", "=", "conll_file", ".", "get", "(", "[", "'lemma'", "]", ",", "as_sentences", "=", "True", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.paired_bootstrap.system_score": [[15, 32], ["range", "len", "len", "len", "range", "len"], "function", ["None"], ["", "def", "system_score", "(", "gold", ":", "List", "[", "str", "]", ",", "system", ":", "List", "[", "str", "]", ")", "->", "int", ":", "\n", "    ", "\"\"\"Returns the accuracy of the system\"\"\"", "\n", "assert", "len", "(", "gold", ")", "==", "len", "(", "system", ")", ",", "\"The gold and system predictions are not of the same length.\"", "\n", "\n", "score", "=", "0", "\n", "total", "=", "0", "\n", "correct", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "gold", ")", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "len", "(", "gold", "[", "i", "]", ")", ")", ":", "\n", "            ", "total", "+=", "1", "\n", "if", "gold", "[", "i", "]", "[", "j", "]", "==", "system", "[", "i", "]", "[", "j", "]", ":", "\n", "                ", "correct", "+=", "1", "\n", "\n", "", "", "", "score", "=", "correct", "/", "total", "\n", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.paired_bootstrap.paired_bootstrap": [[34, 98], ["collections.defaultdict", "collections.defaultdict", "len", "list", "int", "int", "print", "print", "range", "sorted", "enumerate", "range", "print", "range", "len", "range", "sorted", "final.append", "print", "len", "len", "numpy.random.choice", "paired_bootstrap.system_score", "system_scores[].append", "zip", "print", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.paired_bootstrap.system_score"], ["", "def", "paired_bootstrap", "(", "gold", ":", "List", "[", "str", "]", ",", "systems", ":", "List", "[", "NamedTuple", "]", ",", "\n", "num_samples", ":", "int", "=", "1000", ",", "\n", "confidence", ":", "int", "=", "95", ")", "->", "None", ":", "\n", "    ", "\"\"\"Evaluate two systems with the paired bootstrap method\n\n    :param gold: The golden labels\n    :param systems: Systems to compare\n    :param num_samples: The number of random bootstrap samples\n    \"\"\"", "\n", "for", "system", "in", "systems", ":", "\n", "        ", "assert", "len", "(", "gold", ")", "==", "len", "(", "system", ".", "sents", ")", ",", "f\"The gold ({len(gold)}) and {system.name} ({len(system.sents)}) predictions are not of the same length.\"", "\n", "\n", "", "system_scores", "=", "defaultdict", "(", "list", ")", "\n", "wins", "=", "defaultdict", "(", "int", ")", "\n", "\n", "n", "=", "len", "(", "gold", ")", "\n", "idx", "=", "list", "(", "range", "(", "n", ")", ")", "\n", "alpha", "=", "(", "1", "-", "confidence", "/", "100", ")", "/", "2", "\n", "index_lo", "=", "int", "(", "alpha", "*", "(", "num_samples", "-", "1", ")", ")", "\n", "index_hi", "=", "num_samples", "-", "1", "-", "index_lo", "\n", "index_mid", "=", "int", "(", "num_samples", "/", "2", ")", "\n", "\n", "# Resampling the sets for each system and comparing with the gold", "\n", "print", "(", "'Resampling...'", ")", "\n", "for", "system", "in", "systems", ":", "\n", "        ", "print", "(", "f'Resampling {system.name}...'", ")", "\n", "for", "i", "in", "range", "(", "num_samples", ")", ":", "\n", "            ", "idx_shuffled", "=", "np", ".", "random", ".", "choice", "(", "idx", ",", "size", "=", "n", ")", "\n", "gold_shuffled", "=", "[", "gold", "[", "i", "]", "for", "i", "in", "idx_shuffled", "]", "\n", "system_shuffled", "=", "[", "system", ".", "sents", "[", "i", "]", "for", "i", "in", "idx_shuffled", "]", "\n", "score", "=", "system_score", "(", "gold_shuffled", ",", "system_shuffled", ")", "\n", "system_scores", "[", "system", ".", "name", "]", ".", "append", "(", "score", ")", "\n", "\n", "# Counting the wins of each system and raking them from the highers to lowest", "\n", "# Average accuracy is used for sorting", "\n", "", "", "final", "=", "[", "]", "\n", "print", "(", "'Ranking the systems...'", ")", "\n", "for", "i_system", "in", "range", "(", "len", "(", "systems", ")", ")", ":", "\n", "        ", "i_name", "=", "systems", "[", "i_system", "]", ".", "name", "\n", "for", "j_system", "in", "range", "(", "i_system", ")", ":", "\n", "            ", "j_name", "=", "systems", "[", "j_system", "]", ".", "name", "\n", "for", "i", ",", "j", "in", "zip", "(", "system_scores", "[", "i_name", "]", ",", "system_scores", "[", "j_name", "]", ")", ":", "\n", "                ", "if", "i", ">", "j", ":", "\n", "                    ", "wins", "[", "(", "i_name", ",", "j_name", ")", "]", "+=", "1", "\n", "", "elif", "i", "<", "j", ":", "\n", "                    ", "wins", "[", "(", "j_name", ",", "i_name", ")", "]", "+=", "1", "\n", "", "", "", "scores", "=", "sorted", "(", "system_scores", "[", "i_name", "]", ")", "\n", "final", ".", "append", "(", "[", "i_name", ",", "scores", "[", "index_mid", "]", ",", "scores", "[", "index_hi", "]", ",", "scores", "[", "index_lo", "]", "]", ")", "\n", "\n", "", "sorted_systems", "=", "sorted", "(", "final", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "for", "rank", ",", "results", "in", "enumerate", "(", "sorted_systems", ")", ":", "\n", "        ", "system_name", ",", "mid", ",", "hi", ",", "lo", "=", "results", "\n", "if", "rank", "<", "len", "(", "systems", ")", "-", "1", ":", "\n", "            ", "lower_rank_system", "=", "sorted_systems", "[", "rank", "+", "1", "]", "[", "0", "]", "\n", "p_value", "=", "(", "wins", "[", "(", "lower_rank_system", ",", "system_name", ")", "]", "+", "1", ")", "/", "(", "num_samples", "+", "1", ")", "\n", "p_string", "=", "f'p={p_value:.3f}'", "\n", "", "else", ":", "\n", "            ", "p_value", "=", "1", "\n", "p_string", "=", "''", "\n", "", "print", "(", "f'{rank+1:2d}. {system_name:>30} {100*mid:5.2f} \u00b1{50*(hi-lo):5.2f} ({100*lo:5.2f} .. {100*hi:5.2f}) {p_string}'", ")", "\n", "if", "p_value", "<", "(", "1", "-", "confidence", "/", "100", ")", ":", "\n", "            ", "print", "(", "'-'", "*", "72", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.paired_bootstrap.main": [[100, 140], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "numpy.random.seed", "pathlib.Path", "collections.namedtuple", "isinstance", "pathlib.Path.iterdir", "paired_bootstrap.paired_bootstrap", "filename.endswith", "paired_bootstrap.load_file", "child.is_dir", "paired_bootstrap.load_file", "child.stem.replace", "systems.append", "collections.namedtuple.", "list", "FileNotFoundError", "child.glob"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.parse_args", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.paired_bootstrap.paired_bootstrap", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.load_file", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.load_file"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--gold_file\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Name of the CoNLL-U file with the gold data.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--systems\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Folder with the systems results.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--confidence\"", ",", "\"-c\"", ",", "default", "=", "95", ",", "type", "=", "int", ",", "\n", "help", "=", "\"X-percent confidence interval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_samples\"", ",", "\"-n\"", ",", "default", "=", "1000", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The number of random bootstrap samples.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "\"-s\"", ",", "default", "=", "1234", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Seed for random generator.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "gold_file", "=", "args", ".", "gold_file", "\n", "systems_dir", "=", "Path", "(", "args", ".", "systems", ")", "\n", "\n", "System", "=", "namedtuple", "(", "'System'", ",", "[", "'name'", ",", "'sents'", "]", ")", "\n", "systems", "=", "[", "]", "\n", "\n", "# Parse the corresponding .conllu files", "\n", "if", "isinstance", "(", "gold_file", ",", "str", ")", ":", "\n", "        ", "filename", "=", "gold_file", "\n", "assert", "filename", ".", "endswith", "(", "'conllu'", ")", ",", "f\"{filename} must be conllu file.\"", "\n", "gold_data", "=", "load_file", "(", "filename", ")", "\n", "\n", "", "for", "child", "in", "systems_dir", ".", "iterdir", "(", ")", ":", "\n", "        ", "if", "child", ".", "is_dir", "(", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "filename", "=", "list", "(", "child", ".", "glob", "(", "'*.conllu'", ")", ")", "[", "0", "]", "\n", "", "except", "IndexError", ":", "\n", "                ", "raise", "FileNotFoundError", "(", "'The folder should not be empty!'", ")", "\n", "", "system_data", "=", "load_file", "(", "filename", ")", "\n", "system_name", "=", "child", ".", "stem", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "systems", ".", "append", "(", "System", "(", "system_name", ",", "system_data", ")", ")", "\n", "\n", "", "", "paired_bootstrap", "(", "gold_data", ",", "systems", ",", "\n", "args", ".", "num_samples", ",", "args", ".", "confidence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemmatizers.apertium.lemmatize": [[4, 11], ["apertium.analyze", "list", "list", "collections.OrderedDict.fromkeys", "x.split", "str().split", "str"], "function", ["None"], ["def", "lemmatize", "(", "word", ",", "lang", ")", ":", "\n", "    ", "analysis", "=", "apertium", ".", "analyze", "(", "lang", ",", "word", ")", "\n", "if", "analysis", ":", "\n", "        ", "lemmas", "=", "list", "(", "OrderedDict", ".", "fromkeys", "(", "[", "x", ".", "split", "(", "'<'", ")", "[", "0", "]", "for", "x", "in", "str", "(", "analysis", "[", "0", "]", ")", ".", "split", "(", "'/'", ")", "[", "1", ":", "]", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "lemmas", "=", "[", "]", "\n", "", "return", "list", "(", "''", ".", "join", "(", "lemmas", ")", ")", "", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemmatizers.pymorphy.lemmatize": [[5, 10], ["set", "morph.parse", "list", "set.add"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.set"], ["def", "lemmatize", "(", "token", ")", ":", "\n", "    ", "normal_forms", "=", "set", "(", ")", "\n", "for", "p", "in", "morph", ".", "parse", "(", "token", ")", ":", "\n", "        ", "normal_forms", ".", "add", "(", "p", ".", "normal_form", ")", "\n", "", "return", "list", "(", "normal_forms", ")", "", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemmatizers.vabamorf.lemmatize": [[3, 5], ["estnltk.Text().lemmas[].split", "estnltk.Text"], "function", ["None"], ["def", "lemmatize", "(", "token", ")", ":", "\n", "    ", "return", "Text", "(", "token", ")", ".", "lemmas", "[", "0", "]", ".", "split", "(", "'|'", ")", "", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.parse_args": [[29, 83], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "len", "torch.cuda.is_available"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "'data/lemma'", ",", "help", "=", "'Directory for all lemma data.'", ")", "\n", "parser", ".", "add_argument", "(", "'--unimorph_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'Directory of unimorph file'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_file'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "'Input file for data loader.'", ")", "\n", "parser", ".", "add_argument", "(", "'--eval_file'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "'Input file for data loader.'", ")", "\n", "parser", ".", "add_argument", "(", "'--output_file'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "'Output CoNLL-U file.'", ")", "\n", "parser", ".", "add_argument", "(", "'--gold_file'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "'Output CoNLL-U file.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--mode'", ",", "default", "=", "'train'", ",", "choices", "=", "[", "'train'", ",", "'predict'", "]", ")", "\n", "parser", ".", "add_argument", "(", "'--lang'", ",", "type", "=", "str", ",", "help", "=", "'Language'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--no_dict'", ",", "dest", "=", "'ensemble_dict'", ",", "action", "=", "'store_false'", ",", "help", "=", "'Do not ensemble dictionary with seq2seq. By default use ensemble.'", ")", "\n", "parser", ".", "add_argument", "(", "'--dict_only'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Only train a dictionary-based lemmatizer.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--hidden_dim'", ",", "type", "=", "int", ",", "default", "=", "200", ")", "\n", "parser", ".", "add_argument", "(", "'--emb_dim'", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'--num_layers'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--emb_dropout'", ",", "type", "=", "float", ",", "default", "=", "0.5", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.5", ")", "\n", "parser", ".", "add_argument", "(", "'--max_dec_len'", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'--beam_size'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--attn_type'", ",", "default", "=", "'soft'", ",", "choices", "=", "[", "'soft'", ",", "'mlp'", ",", "'linear'", ",", "'deep'", "]", ",", "help", "=", "'Attention type'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_edit'", ",", "dest", "=", "'edit'", ",", "action", "=", "'store_false'", ",", "help", "=", "'Do not use edit classifier in lemmatization. By default use an edit classifier.'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_morph'", ",", "dest", "=", "'morph'", ",", "action", "=", "'store_false'", ",", "help", "=", "'Do not use morphological tags as inputs. By default use pos and morphological tags.'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_pos'", ",", "dest", "=", "'pos'", ",", "action", "=", "'store_false'", ",", "help", "=", "'Do not use pos tags as inputs. By default use pos and morphological tags.'", ")", "\n", "parser", ".", "add_argument", "(", "'--lemmatizer'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "'Name of the outer lemmatizer function'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_pos_lexicon'", ",", "dest", "=", "'use_pos'", ",", "action", "=", "'store_false'", ",", "help", "=", "'Do not use word-pos dictionary in the lexicon'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_word_lexicon'", ",", "dest", "=", "'use_word'", ",", "action", "=", "'store_false'", ",", "help", "=", "'Do not use word dictionary in the lexicon'", ")", "\n", "parser", ".", "add_argument", "(", "'--lexicon_dropout'", ",", "type", "=", "float", ",", "default", "=", "0.8", ",", "help", "=", "'Probability to drop the word from the lexicon'", ")", "\n", "parser", ".", "add_argument", "(", "'--eos_after'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Put <EOS> symbol after all the inputs. Otherwise put it after the end of token'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_edit'", ",", "type", "=", "int", ",", "default", "=", "len", "(", "edit", ".", "EDIT_TO_ID", ")", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "1.0", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--sample_train'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'Subsample training data.'", ")", "\n", "parser", ".", "add_argument", "(", "'--optim'", ",", "type", "=", "str", ",", "default", "=", "'adam'", ",", "help", "=", "'sgd, adagrad, adam or adamax.'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ",", "help", "=", "'Learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr_decay'", ",", "type", "=", "float", ",", "default", "=", "0.9", ")", "\n", "parser", ".", "add_argument", "(", "'--decay_epoch'", ",", "type", "=", "int", ",", "default", "=", "30", ",", "help", "=", "\"Decay the lr starting from this epoch.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_epoch'", ",", "type", "=", "int", ",", "default", "=", "60", ")", "\n", "parser", ".", "add_argument", "(", "'--early_stop'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "\"Stop training if dev score doesn't improve after the specified number of epochs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--min_epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "\"Minimum number of epochs to train before early stopping gets applied.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'--max_grad_norm'", ",", "type", "=", "float", ",", "default", "=", "5.0", ",", "help", "=", "'Gradient clipping.'", ")", "\n", "parser", ".", "add_argument", "(", "'--log_step'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "help", "=", "'Print log every k steps.'", ")", "\n", "parser", ".", "add_argument", "(", "'--model_dir'", ",", "type", "=", "str", ",", "default", "=", "'saved_models/lemma'", ",", "help", "=", "'Root dir for saving models.'", ")", "\n", "parser", ".", "add_argument", "(", "'--log_attn'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Log attention output.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1234", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "type", "=", "bool", ",", "default", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", ")", "\n", "parser", ".", "add_argument", "(", "'--cpu'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Ignore CUDA.'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.main": [[84, 106], ["lemmatizer_cmb.parse_args", "torch.manual_seed", "numpy.random.seed", "random.seed", "vars", "print", "lemmatizer_cmb.train", "lemmatizer_cmb.evaluate", "torch.cuda.manual_seed"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.parse_args", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.train", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.evaluate"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "cpu", ":", "\n", "        ", "args", ".", "cuda", "=", "False", "\n", "", "elif", "args", ".", "cuda", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "", "args", "=", "vars", "(", "args", ")", "\n", "print", "(", "\"Running lemmatizer in {} mode\"", ".", "format", "(", "args", "[", "'mode'", "]", ")", ")", "\n", "\n", "# manually correct for training epochs", "\n", "if", "args", "[", "'lang'", "]", "in", "[", "'cs_pdt'", ",", "'ru_syntagrus'", ",", "'de_hdt'", "]", ":", "\n", "        ", "args", "[", "'num_epoch'", "]", "=", "30", "\n", "\n", "", "if", "args", "[", "'mode'", "]", "==", "'train'", ":", "\n", "        ", "train", "(", "args", ")", "\n", "", "else", ":", "\n", "        ", "evaluate", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.train": [[107, 240], ["print", "lexenlem.models.lemma.data.DataLoaderCombined", "lexenlem.models.lemma.data.DataLoaderCombined", "lexenlem.models.common.utils.ensure_dir", "lexenlem.models.common.utils.print_config", "lexenlem.models.lemma.trainer.TrainerCombined", "print", "lexenlem.models.lemma.trainer.TrainerCombined.train_dict", "print", "lexenlem.models.lemma.trainer.TrainerCombined.predict_dict", "lexenlem.models.lemma.data.DataLoaderCombined.conll.write_conll_with_lemmas", "lexenlem.models.lemma.scorer.score", "print", "args.get", "print", "sys.exit", "lexenlem.models.lemma.data.DataLoaderCombined.conll.get", "lexenlem.models.lemma.data.DataLoaderCombined.conll.get", "lexenlem.models.lemma.trainer.TrainerCombined.save", "print", "len", "time.time", "range", "print", "print", "print", "print", "importlib.import_module", "len", "len", "len", "enumerate", "print", "enumerate", "lexenlem.models.lemma.trainer.TrainerCombined.postprocess", "args.get", "lexenlem.models.lemma.data.DataLoaderCombined.conll.write_conll_with_lemmas", "lexenlem.models.lemma.scorer.score", "print", "print", "time.time", "lexenlem.models.lemma.trainer.TrainerCombined.update", "time.time", "lexenlem.models.lemma.trainer.TrainerCombined.predict", "lexenlem.models.lemma.data.DataLoaderCombined.conll.get", "print", "lexenlem.models.lemma.trainer.TrainerCombined.ensemble", "lexenlem.models.lemma.trainer.TrainerCombined.save", "print", "print", "lexenlem.models.lemma.trainer.TrainerCombined.update_lr", "max", "numpy.argmax", "print", "print", "lexenlem.models.lemma.data.DataLoaderCombined.conll.get", "max", "max", "print", "time.time", "format_str.format", "time.time", "format_str_dev.format", "datetime.datetime.now().strftime", "datetime.datetime.now().strftime", "datetime.datetime.now", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.ensure_dir", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.print_config", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.train_dict", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.predict_dict", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.write_conll_with_lemmas", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.scorer.score", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.save", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.postprocess", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.write_conll_with_lemmas", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.scorer.score", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.update", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.predict", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.ensemble", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.save", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.update_lr", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get"], ["", "", "def", "train", "(", "args", ")", ":", "\n", "# load data", "\n", "    ", "print", "(", "\"[Loading data with batch size {}...]\"", ".", "format", "(", "args", "[", "'batch_size'", "]", ")", ")", "\n", "if", "args", "[", "'lemmatizer'", "]", "is", "None", ":", "\n", "        ", "lemmatizer", "=", "None", "\n", "", "elif", "args", "[", "'lemmatizer'", "]", "==", "'lexicon'", ":", "\n", "        ", "print", "(", "\"[Using the lexicon...]\"", ")", "\n", "lemmatizer", "=", "'lexicon'", "\n", "", "else", ":", "\n", "        ", "print", "(", "f\"[Loading the {args['lemmatizer']} lemmatizer...]\"", ")", "\n", "lemmatizer", "=", "importlib", ".", "import_module", "(", "'lexenlem.lemmatizers.'", "+", "args", "[", "'lemmatizer'", "]", ")", "\n", "", "train_batch", "=", "DataLoaderCombined", "(", "args", "[", "'train_file'", "]", ",", "args", "[", "'batch_size'", "]", ",", "args", ",", "lemmatizer", "=", "lemmatizer", ",", "evaluation", "=", "False", ")", "\n", "vocab", "=", "train_batch", ".", "vocab", "\n", "if", "args", "[", "'lemmatizer'", "]", "==", "'lexicon'", ":", "\n", "        ", "lemmatizer", "=", "train_batch", ".", "lemmatizer", "\n", "", "args", "[", "'vocab_size'", "]", "=", "vocab", "[", "'combined'", "]", ".", "size", "\n", "dev_batch", "=", "DataLoaderCombined", "(", "args", "[", "'eval_file'", "]", ",", "args", "[", "'batch_size'", "]", ",", "args", ",", "lemmatizer", "=", "lemmatizer", ",", "vocab", "=", "vocab", ",", "evaluation", "=", "True", ")", "\n", "\n", "utils", ".", "ensure_dir", "(", "args", "[", "'model_dir'", "]", ")", "\n", "model_file", "=", "'{}/{}_lemmatizer.pt'", ".", "format", "(", "args", "[", "'model_dir'", "]", ",", "args", "[", "'lang'", "]", ")", "\n", "\n", "# pred and gold path", "\n", "system_pred_file", "=", "args", "[", "'output_file'", "]", "\n", "gold_file", "=", "args", "[", "'gold_file'", "]", "\n", "\n", "utils", ".", "print_config", "(", "args", ")", "\n", "\n", "# skip training if the language does not have training or dev data", "\n", "if", "len", "(", "train_batch", ")", "==", "0", "or", "len", "(", "dev_batch", ")", "==", "0", ":", "\n", "        ", "print", "(", "\"[Skip training because no data available...]\"", ")", "\n", "sys", ".", "exit", "(", "0", ")", "\n", "\n", "# start training", "\n", "# train a dictionary-based lemmatizer", "\n", "", "trainer", "=", "TrainerCombined", "(", "args", "=", "args", ",", "vocab", "=", "vocab", ",", "use_cuda", "=", "args", "[", "'cuda'", "]", ")", "\n", "if", "args", "[", "'lemmatizer'", "]", "==", "'lexicon'", ":", "\n", "        ", "trainer", ".", "lexicon", "=", "train_batch", ".", "lemmatizer", "\n", "", "print", "(", "\"[Training dictionary-based lemmatizer...]\"", ")", "\n", "trainer", ".", "train_dict", "(", "train_batch", ".", "conll", ".", "get", "(", "[", "'word'", ",", "'upos'", ",", "'lemma'", "]", ")", ")", "\n", "print", "(", "\"Evaluating on dev set...\"", ")", "\n", "dev_preds", "=", "trainer", ".", "predict_dict", "(", "dev_batch", ".", "conll", ".", "get", "(", "[", "'word'", ",", "'upos'", "]", ")", ")", "\n", "dev_batch", ".", "conll", ".", "write_conll_with_lemmas", "(", "dev_preds", ",", "system_pred_file", ")", "\n", "_", ",", "_", ",", "dev_f", "=", "scorer", ".", "score", "(", "system_pred_file", ",", "gold_file", ")", "\n", "print", "(", "\"Dev F1 = {:.2f}\"", ".", "format", "(", "dev_f", "*", "100", ")", ")", "\n", "\n", "if", "args", ".", "get", "(", "'dict_only'", ",", "False", ")", ":", "\n", "# save dictionaries", "\n", "        ", "trainer", ".", "save", "(", "model_file", ")", "\n", "", "else", ":", "\n", "# train a seq2seq model", "\n", "        ", "print", "(", "\"[Training seq2seq-based lemmatizer...]\"", ")", "\n", "global_step", "=", "0", "\n", "max_steps", "=", "len", "(", "train_batch", ")", "*", "args", "[", "'num_epoch'", "]", "\n", "max_dev_steps", "=", "len", "(", "dev_batch", ")", "\n", "dev_score_history", "=", "[", "]", "\n", "devs_without_improvements", "=", "0", "\n", "best_dev_preds", "=", "[", "]", "\n", "current_lr", "=", "args", "[", "'lr'", "]", "\n", "global_start_time", "=", "time", ".", "time", "(", ")", "\n", "format_str", "=", "'{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'", "\n", "format_str_dev", "=", "'{}: step {}/{} (epoch {}/{}) ({:.3f} sec/batch)'", "\n", "\n", "# start training", "\n", "for", "epoch", "in", "range", "(", "1", ",", "args", "[", "'num_epoch'", "]", "+", "1", ")", ":", "\n", "            ", "dev_step", "=", "0", "\n", "train_loss", "=", "0", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "train_batch", ")", ":", "\n", "                ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "global_step", "+=", "1", "\n", "loss", "=", "trainer", ".", "update", "(", "batch", ",", "eval", "=", "False", ")", "# update step", "\n", "train_loss", "+=", "loss", "\n", "if", "global_step", "%", "args", "[", "'log_step'", "]", "==", "0", ":", "\n", "                    ", "duration", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "print", "(", "format_str", ".", "format", "(", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d %H:%M:%S\"", ")", ",", "global_step", ",", "max_steps", ",", "epoch", ",", "args", "[", "'num_epoch'", "]", ",", "loss", ",", "duration", ",", "current_lr", ")", ")", "\n", "\n", "# eval on dev", "\n", "", "", "print", "(", "\"Evaluating on dev set...\"", ")", "\n", "dev_preds", "=", "[", "]", "\n", "dev_edits", "=", "[", "]", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "dev_batch", ")", ":", "\n", "                ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "dev_step", "+=", "1", "\n", "preds", ",", "edits", ",", "_", "=", "trainer", ".", "predict", "(", "batch", ",", "args", "[", "'beam_size'", "]", ")", "\n", "dev_preds", "+=", "preds", "\n", "if", "edits", "is", "not", "None", ":", "\n", "                    ", "dev_edits", "+=", "edits", "\n", "", "if", "dev_step", "%", "args", "[", "'log_step'", "]", "==", "0", ":", "\n", "                    ", "duration", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "print", "(", "format_str_dev", ".", "format", "(", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d %H:%M:%S\"", ")", ",", "dev_step", ",", "max_dev_steps", ",", "epoch", ",", "args", "[", "'num_epoch'", "]", ",", "duration", ")", ")", "\n", "", "", "dev_preds", "=", "trainer", ".", "postprocess", "(", "dev_batch", ".", "conll", ".", "get", "(", "[", "'word'", "]", ")", ",", "dev_preds", ",", "edits", "=", "dev_edits", ")", "\n", "\n", "# try ensembling with dict if necessary", "\n", "if", "args", ".", "get", "(", "'ensemble_dict'", ",", "False", ")", ":", "\n", "                ", "print", "(", "\"[Ensembling dict with seq2seq model...]\"", ")", "\n", "dev_preds", "=", "trainer", ".", "ensemble", "(", "dev_batch", ".", "conll", ".", "get", "(", "[", "'word'", ",", "'upos'", "]", ")", ",", "dev_preds", ")", "\n", "", "dev_batch", ".", "conll", ".", "write_conll_with_lemmas", "(", "dev_preds", ",", "system_pred_file", ")", "\n", "_", ",", "_", ",", "dev_score", "=", "scorer", ".", "score", "(", "system_pred_file", ",", "gold_file", ")", "\n", "\n", "train_loss", "=", "train_loss", "/", "train_batch", ".", "num_examples", "*", "args", "[", "'batch_size'", "]", "# avg loss per batch", "\n", "print", "(", "\"epoch {}: train_loss = {:.6f}, dev_score = {:.4f}\"", ".", "format", "(", "epoch", ",", "train_loss", ",", "dev_score", ")", ")", "\n", "\n", "# save best model", "\n", "if", "epoch", "==", "1", "or", "dev_score", ">", "max", "(", "dev_score_history", ")", ":", "\n", "                ", "trainer", ".", "save", "(", "model_file", ")", "\n", "print", "(", "\"new best model saved.\"", ")", "\n", "best_dev_preds", "=", "dev_preds", "\n", "\n", "# early stopping", "\n", "", "if", "epoch", ">", "args", "[", "'min_epochs'", "]", ":", "\n", "                ", "if", "dev_score", "<=", "max", "(", "dev_score_history", ")", ":", "\n", "                    ", "devs_without_improvements", "+=", "1", "\n", "print", "(", "\"{} epochs since last dev score improvement.\"", ".", "format", "(", "devs_without_improvements", ")", ")", "\n", "", "else", ":", "\n", "                    ", "devs_without_improvements", "=", "0", "\n", "", "", "if", "devs_without_improvements", ">", "args", "[", "'early_stop'", "]", ":", "\n", "                ", "print", "(", "\"No dev score improvements for {} epocs. Stopping training...\"", ".", "format", "(", "args", "[", "'early_stop'", "]", ")", ")", "\n", "break", "\n", "\n", "# lr schedule", "\n", "", "if", "epoch", ">", "args", "[", "'decay_epoch'", "]", "and", "dev_score", "<=", "dev_score_history", "[", "-", "1", "]", "and", "args", "[", "'optim'", "]", "in", "[", "'sgd'", ",", "'adagrad'", "]", ":", "\n", "                ", "current_lr", "*=", "args", "[", "'lr_decay'", "]", "\n", "trainer", ".", "update_lr", "(", "current_lr", ")", "\n", "\n", "", "dev_score_history", "+=", "[", "dev_score", "]", "\n", "print", "(", "\"\"", ")", "\n", "\n", "", "print", "(", "\"Training ended with {} epochs.\"", ".", "format", "(", "epoch", ")", ")", "\n", "\n", "best_f", ",", "best_epoch", "=", "max", "(", "dev_score_history", ")", "*", "100", ",", "np", ".", "argmax", "(", "dev_score_history", ")", "+", "1", "\n", "print", "(", "\"Best dev F1 = {:.2f}, at epoch = {}\"", ".", "format", "(", "best_f", ",", "best_epoch", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.evaluate": [[241, 335], ["lexenlem.models.lemma.trainer.TrainerCombined", "print", "print", "lexenlem.models.lemma.data.DataLoaderCombined", "lexenlem.models.lemma.trainer.TrainerCombined.predict_dict", "loaded_args.get", "lexenlem.models.lemma.data.DataLoaderCombined.conll.write_conll_with_lemmas", "len", "print", "print", "print", "sys.exit", "lexenlem.models.lemma.data.DataLoaderCombined.conll.get", "print", "enumerate", "lexenlem.models.lemma.trainer.TrainerCombined.postprocess", "loaded_args.get", "lexenlem.models.lemma.scorer.score", "print", "print", "k.endswith", "k.endswith", "print", "lexenlem.models.lemma.trainer.TrainerCombined.predict", "print", "numpy.savez", "lexenlem.models.lemma.data.DataLoaderCombined.conll.get", "print", "lexenlem.models.lemma.trainer.TrainerCombined.ensemble", "print", "lexenlem.models.common.lexicon.ExtendedLexicon", "print", "importlib.import_module", "attns.items", "lexenlem.models.lemma.data.DataLoaderCombined.conll.get", "importlib.import_module", "numpy.concatenate", "numpy.vstack", "numpy.vstack", "numpy.concatenate", "numpy.concatenate", "numpy.hstack", "numpy.hstack", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.predict_dict", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.write_conll_with_lemmas", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.postprocess", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.scorer.score", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.predict", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.ensemble", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get"], ["", "", "def", "evaluate", "(", "args", ")", ":", "\n", "# file paths", "\n", "    ", "system_pred_file", "=", "args", "[", "'output_file'", "]", "\n", "gold_file", "=", "args", "[", "'gold_file'", "]", "\n", "model_file", "=", "'{}/{}_lemmatizer.pt'", ".", "format", "(", "args", "[", "'model_dir'", "]", ",", "args", "[", "'lang'", "]", ")", "\n", "\n", "# load model", "\n", "use_cuda", "=", "args", "[", "'cuda'", "]", "and", "not", "args", "[", "'cpu'", "]", "\n", "trainer", "=", "TrainerCombined", "(", "model_file", "=", "model_file", ",", "use_cuda", "=", "use_cuda", ")", "\n", "loaded_args", ",", "vocab", "=", "trainer", ".", "args", ",", "trainer", ".", "vocab", "\n", "\n", "for", "k", "in", "args", ":", "\n", "        ", "if", "k", ".", "endswith", "(", "'_dir'", ")", "or", "k", ".", "endswith", "(", "'_file'", ")", "or", "k", "in", "[", "'shorthand'", "]", ":", "\n", "            ", "loaded_args", "[", "k", "]", "=", "args", "[", "k", "]", "\n", "\n", "", "", "print", "(", "\"[Loading the outer lemmatizer...]\"", ")", "\n", "if", "args", "[", "'lemmatizer'", "]", "!=", "loaded_args", "[", "'lemmatizer'", "]", "and", "loaded_args", "[", "'lemmatizer'", "]", "is", "None", ":", "\n", "        ", "loaded_args", "[", "'lemmatizer'", "]", "=", "args", "[", "'lemmatizer'", "]", "\n", "", "if", "loaded_args", "[", "'lemmatizer'", "]", "==", "'lexicon'", "and", "args", "[", "'lemmatizer'", "]", "not", "in", "[", "'lexicon'", ",", "None", "]", ":", "\n", "        ", "loaded_args", "[", "'lemmatizer'", "]", "=", "'lexicon_extended'", "\n", "", "if", "loaded_args", "[", "'lemmatizer'", "]", "is", "None", ":", "\n", "        ", "lemmatizer", "=", "None", "\n", "", "elif", "loaded_args", "[", "'lemmatizer'", "]", "==", "'lexicon'", ":", "\n", "        ", "print", "(", "\"[Using the lexicon...]\"", ")", "\n", "lemmatizer", "=", "trainer", ".", "lexicon", "\n", "", "elif", "loaded_args", "[", "'lemmatizer'", "]", "==", "'lexicon_extended'", ":", "\n", "        ", "print", "(", "f\"[Loading the Lexicon extended with the {args['lemmatizer']} lemmatizer...]\"", ")", "\n", "lemmatizer", "=", "ExtendedLexicon", "(", "trainer", ".", "lexicon", ",", "importlib", ".", "import_module", "(", "'lexenlem.lemmatizers.'", "+", "args", "[", "'lemmatizer'", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "f\"[Loading the {loaded_args['lemmatizer']} lemmatizer...]\"", ")", "\n", "lemmatizer", "=", "importlib", ".", "import_module", "(", "'lexenlem.lemmatizers.'", "+", "loaded_args", "[", "'lemmatizer'", "]", ")", "\n", "\n", "# laod data", "\n", "", "print", "(", "\"Loading data with batch size {}...\"", ".", "format", "(", "args", "[", "'batch_size'", "]", ")", ")", "\n", "batch", "=", "DataLoaderCombined", "(", "args", "[", "'eval_file'", "]", ",", "args", "[", "'batch_size'", "]", ",", "loaded_args", ",", "lemmatizer", "=", "lemmatizer", ",", "vocab", "=", "vocab", ",", "evaluation", "=", "True", ")", "\n", "\n", "# skip eval if dev data does not exist", "\n", "if", "len", "(", "batch", ")", "==", "0", ":", "\n", "        ", "print", "(", "\"Skip evaluation because no dev data is available...\"", ")", "\n", "print", "(", "\"Lemma score:\"", ")", "\n", "print", "(", "\"{} \"", ".", "format", "(", "args", "[", "'lang'", "]", ")", ")", "\n", "sys", ".", "exit", "(", "0", ")", "\n", "\n", "", "dict_preds", "=", "trainer", ".", "predict_dict", "(", "batch", ".", "conll", ".", "get", "(", "[", "'word'", ",", "'upos'", "]", ")", ")", "\n", "\n", "if", "loaded_args", ".", "get", "(", "'dict_only'", ",", "False", ")", ":", "\n", "        ", "preds", "=", "dict_preds", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Running the seq2seq model...\"", ")", "\n", "preds", "=", "[", "]", "\n", "edits", "=", "[", "]", "\n", "log_attns", "=", "{", "}", "\n", "for", "i", ",", "b", "in", "enumerate", "(", "batch", ")", ":", "\n", "            ", "ps", ",", "es", ",", "attns", "=", "trainer", ".", "predict", "(", "b", ",", "args", "[", "'beam_size'", "]", ",", "log_attn", "=", "args", "[", "'log_attn'", "]", ")", "\n", "if", "attns", ":", "\n", "                ", "for", "k", ",", "_", "in", "attns", ".", "items", "(", ")", ":", "\n", "                    ", "if", "k", "in", "log_attns", ":", "\n", "                        ", "if", "k", "==", "'attns'", ":", "\n", "                            ", "if", "log_attns", "[", "k", "]", ".", "shape", "[", "0", "]", ">", "attns", "[", "k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                                ", "attns", "[", "k", "]", "=", "np", ".", "vstack", "(", "(", "attns", "[", "k", "]", ",", "np", ".", "zeros", "(", "(", "log_attns", "[", "k", "]", ".", "shape", "[", "0", "]", "-", "attns", "[", "k", "]", ".", "shape", "[", "0", "]", ",", "attns", "[", "k", "]", ".", "shape", "[", "1", "]", ",", "attns", "[", "k", "]", ".", "shape", "[", "2", "]", ")", ")", ")", ")", "\n", "", "else", ":", "\n", "                                ", "log_attns", "[", "k", "]", "=", "np", ".", "vstack", "(", "(", "log_attns", "[", "k", "]", ",", "np", ".", "zeros", "(", "(", "attns", "[", "k", "]", ".", "shape", "[", "0", "]", "-", "log_attns", "[", "k", "]", ".", "shape", "[", "0", "]", ",", "log_attns", "[", "k", "]", ".", "shape", "[", "1", "]", ",", "log_attns", "[", "k", "]", ".", "shape", "[", "2", "]", ")", ")", ")", ")", "\n", "", "log_attns", "[", "k", "]", "=", "np", ".", "concatenate", "(", "[", "log_attns", "[", "k", "]", ",", "attns", "[", "k", "]", "]", ",", "axis", "=", "2", ")", "\n", "", "elif", "k", "==", "'all_hyp'", ":", "\n", "                            ", "log_attns", "[", "k", "]", "=", "np", ".", "concatenate", "(", "[", "log_attns", "[", "k", "]", ",", "attns", "[", "k", "]", "]", ",", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "                            ", "if", "log_attns", "[", "k", "]", ".", "shape", "[", "1", "]", ">", "attns", "[", "k", "]", ".", "shape", "[", "1", "]", ":", "\n", "                                ", "attns", "[", "k", "]", "=", "np", ".", "hstack", "(", "(", "attns", "[", "k", "]", ",", "np", ".", "zeros", "(", "(", "attns", "[", "k", "]", ".", "shape", "[", "0", "]", ",", "log_attns", "[", "k", "]", ".", "shape", "[", "1", "]", "-", "attns", "[", "k", "]", ".", "shape", "[", "1", "]", ")", ")", ")", ")", "\n", "", "else", ":", "\n", "                                ", "log_attns", "[", "k", "]", "=", "np", ".", "hstack", "(", "(", "log_attns", "[", "k", "]", ",", "np", ".", "zeros", "(", "(", "log_attns", "[", "k", "]", ".", "shape", "[", "0", "]", ",", "attns", "[", "k", "]", ".", "shape", "[", "1", "]", "-", "log_attns", "[", "k", "]", ".", "shape", "[", "1", "]", ")", ")", ")", ")", "\n", "", "log_attns", "[", "k", "]", "=", "np", ".", "concatenate", "(", "[", "log_attns", "[", "k", "]", ",", "attns", "[", "k", "]", "]", ",", "axis", "=", "0", ")", "\n", "", "", "else", ":", "\n", "                        ", "log_attns", "[", "k", "]", "=", "attns", "[", "k", "]", "\n", "", "", "", "preds", "+=", "ps", "\n", "if", "es", "is", "not", "None", ":", "\n", "                ", "edits", "+=", "es", "\n", "", "", "if", "args", "[", "'log_attn'", "]", ":", "\n", "            ", "lem_name", "=", "loaded_args", "[", "'lemmatizer'", "]", "if", "loaded_args", "[", "'lemmatizer'", "]", "is", "not", "None", "else", "'nolexicon'", "\n", "fname", "=", "''", ".", "join", "(", "[", "args", "[", "'lang'", "]", ",", "'_'", ",", "lem_name", "]", ")", "\n", "print", "(", "f'[Logging attention to {fname}.npz...]'", ")", "\n", "np", ".", "savez", "(", "fname", ",", "**", "log_attns", ")", "\n", "", "preds", "=", "trainer", ".", "postprocess", "(", "batch", ".", "conll", ".", "get", "(", "[", "'word'", "]", ")", ",", "preds", ",", "edits", "=", "edits", ")", "\n", "\n", "if", "loaded_args", ".", "get", "(", "'ensemble_dict'", ",", "False", ")", ":", "\n", "            ", "print", "(", "\"[Ensembling dict with seq2seq lemmatizer...]\"", ")", "\n", "preds", "=", "trainer", ".", "ensemble", "(", "batch", ".", "conll", ".", "get", "(", "[", "'word'", ",", "'upos'", "]", ")", ",", "preds", ")", "\n", "\n", "# write to file and score", "\n", "", "", "batch", ".", "conll", ".", "write_conll_with_lemmas", "(", "preds", ",", "system_pred_file", ")", "\n", "if", "gold_file", "is", "not", "None", ":", "\n", "        ", "_", ",", "_", ",", "score", "=", "scorer", ".", "score", "(", "system_pred_file", ",", "gold_file", ")", "\n", "\n", "print", "(", "\"Lemma score:\"", ")", "\n", "print", "(", "\"{} {:.2f}\"", ".", "format", "(", "args", "[", "'lang'", "]", ",", "score", "*", "100", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.__init__": [[38, 65], ["trainer.Trainer.load", "dict", "dict", "trainer.Trainer.args.get", "lexenlem.models.common.utils.get_optimizer", "lexenlem.models.common.seq2seq_model.Seq2SeqModel", "lexenlem.models.common.loss.MixLoss", "print", "lexenlem.models.common.loss.SequenceLoss", "trainer.Trainer.model.cuda", "trainer.Trainer.crit.cuda", "trainer.Trainer.model.cpu", "trainer.Trainer.crit.cpu", "trainer.Trainer.model.parameters"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.load", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.get_optimizer", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.loss.SequenceLoss", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cpu", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cpu"], ["def", "__init__", "(", "self", ",", "args", "=", "None", ",", "vocab", "=", "None", ",", "emb_matrix", "=", "None", ",", "model_file", "=", "None", ",", "use_cuda", "=", "False", ")", ":", "\n", "        ", "self", ".", "use_cuda", "=", "use_cuda", "\n", "if", "model_file", "is", "not", "None", ":", "\n", "# load everything from file", "\n", "            ", "self", ".", "load", "(", "model_file", ",", "use_cuda", ")", "\n", "", "else", ":", "\n", "# build model from scratch", "\n", "            ", "self", ".", "args", "=", "args", "\n", "self", ".", "model", "=", "None", "if", "args", "[", "'dict_only'", "]", "else", "Seq2SeqModel", "(", "args", ",", "vocab", ",", "emb_matrix", "=", "emb_matrix", ",", "use_cuda", "=", "use_cuda", ")", "\n", "self", ".", "vocab", "=", "vocab", "\n", "# dict-based components", "\n", "self", ".", "word_dict", "=", "dict", "(", ")", "\n", "self", ".", "composite_dict", "=", "dict", "(", ")", "\n", "", "if", "not", "self", ".", "args", "[", "'dict_only'", "]", ":", "\n", "            ", "if", "self", ".", "args", ".", "get", "(", "'edit'", ",", "False", ")", ":", "\n", "                ", "self", ".", "crit", "=", "loss", ".", "MixLoss", "(", "self", ".", "vocab", "[", "'char'", "]", ".", "size", ",", "self", ".", "args", "[", "'alpha'", "]", ")", "\n", "print", "(", "\"[Running seq2seq lemmatizer with edit classifier]\"", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "crit", "=", "loss", ".", "SequenceLoss", "(", "self", ".", "vocab", "[", "'char'", "]", ".", "size", ")", "\n", "", "self", ".", "parameters", "=", "[", "p", "for", "p", "in", "self", ".", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "if", "use_cuda", ":", "\n", "                ", "self", ".", "model", ".", "cuda", "(", ")", "\n", "self", ".", "crit", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "model", ".", "cpu", "(", ")", "\n", "self", ".", "crit", ".", "cpu", "(", ")", "\n", "", "self", ".", "optimizer", "=", "utils", ".", "get_optimizer", "(", "self", ".", "args", "[", "'optim'", "]", ",", "self", ".", "parameters", ",", "self", ".", "args", "[", "'lr'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.update": [[66, 90], ["trainer.unpack_batch", "trainer.Trainer.model", "trainer.Trainer.args.get", "trainer.Trainer.data.item", "trainer.Trainer.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "trainer.Trainer.optimizer.step", "trainer.Trainer.model.eval", "trainer.Trainer.model.train", "trainer.Trainer.optimizer.zero_grad", "trainer.Trainer.crit", "trainer.Trainer.crit", "trainer.Trainer.model.parameters", "log_probs.view", "tgt_out.view", "log_probs.view", "tgt_out.view"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.unpack_batch", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.train"], ["", "", "def", "update", "(", "self", ",", "batch", ",", "eval", "=", "False", ")", ":", "\n", "        ", "inputs", ",", "orig_idx", "=", "unpack_batch", "(", "batch", ",", "self", ".", "use_cuda", ")", "\n", "src", ",", "src_mask", ",", "tgt_in", ",", "tgt_out", ",", "pos", ",", "feats", ",", "lem", ",", "lem_mask", ",", "edits", "=", "inputs", "\n", "\n", "if", "eval", ":", "\n", "            ", "self", ".", "model", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "", "log_probs", ",", "edit_logits", "=", "self", ".", "model", "(", "src", ",", "src_mask", ",", "tgt_in", ",", "pos", ",", "feats", ",", "lem", ",", "lem_mask", ")", "\n", "if", "self", ".", "args", ".", "get", "(", "'edit'", ",", "False", ")", ":", "\n", "            ", "assert", "edit_logits", "is", "not", "None", "\n", "loss", "=", "self", ".", "crit", "(", "log_probs", ".", "view", "(", "-", "1", ",", "self", ".", "vocab", "[", "'char'", "]", ".", "size", ")", ",", "tgt_out", ".", "view", "(", "-", "1", ")", ",", "edit_logits", ",", "edits", ")", "\n", "", "else", ":", "\n", "            ", "loss", "=", "self", ".", "crit", "(", "log_probs", ".", "view", "(", "-", "1", ",", "self", ".", "vocab", "[", "'char'", "]", ".", "size", ")", ",", "tgt_out", ".", "view", "(", "-", "1", ")", ")", "\n", "", "loss_val", "=", "loss", ".", "data", ".", "item", "(", ")", "\n", "if", "eval", ":", "\n", "            ", "return", "loss_val", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "self", ".", "args", "[", "'max_grad_norm'", "]", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "return", "loss_val", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.predict": [[91, 109], ["trainer.unpack_batch", "trainer.Trainer.model.eval", "src.size", "trainer.Trainer.model.predict", "lexenlem.models.common.utils.prune_decoded_seqs", "lexenlem.models.common.utils.unsort", "trainer.Trainer.args.get", "trainer.Trainer.vocab[].unmap", "numpy.argmax().reshape().tolist", "lexenlem.models.common.utils.unsort", "numpy.argmax().reshape", "numpy.argmax", "edit_logits.data.cpu().numpy", "edit_logits.data.cpu"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.unpack_batch", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.predict", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.prune_decoded_seqs", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.unsort", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.unmap", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.unsort", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cpu"], ["", "def", "predict", "(", "self", ",", "batch", ",", "beam_size", "=", "1", ")", ":", "\n", "        ", "inputs", ",", "orig_idx", "=", "unpack_batch", "(", "batch", ",", "self", ".", "use_cuda", ")", "\n", "src", ",", "src_mask", ",", "tgt_in", ",", "tgt_out", ",", "pos", ",", "feats", ",", "lem", ",", "lem_mask", ",", "edits", "=", "inputs", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "batch_size", "=", "src", ".", "size", "(", "0", ")", "\n", "preds", ",", "edit_logits", "=", "self", ".", "model", ".", "predict", "(", "src", ",", "src_mask", ",", "pos", "=", "pos", ",", "feats", "=", "feats", ",", "lem", "=", "lem", ",", "lem_mask", "=", "lem_mask", ",", "beam_size", "=", "beam_size", ")", "\n", "pred_seqs", "=", "[", "self", ".", "vocab", "[", "'char'", "]", ".", "unmap", "(", "ids", ")", "for", "ids", "in", "preds", "]", "# unmap to tokens", "\n", "pred_seqs", "=", "utils", ".", "prune_decoded_seqs", "(", "pred_seqs", ")", "\n", "pred_tokens", "=", "[", "\"\"", ".", "join", "(", "seq", ")", "for", "seq", "in", "pred_seqs", "]", "# join chars to be tokens", "\n", "pred_tokens", "=", "utils", ".", "unsort", "(", "pred_tokens", ",", "orig_idx", ")", "\n", "if", "self", ".", "args", ".", "get", "(", "'edit'", ",", "False", ")", ":", "\n", "            ", "assert", "edit_logits", "is", "not", "None", "\n", "edits", "=", "np", ".", "argmax", "(", "edit_logits", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "1", ")", ".", "reshape", "(", "[", "batch_size", "]", ")", ".", "tolist", "(", ")", "\n", "edits", "=", "utils", ".", "unsort", "(", "edits", ",", "orig_idx", ")", "\n", "", "else", ":", "\n", "            ", "edits", "=", "None", "\n", "", "return", "pred_tokens", ",", "edits", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.postprocess": [[110, 130], ["trainer.Trainer.args.get", "zip", "len", "len", "zip", "len", "len", "lexenlem.models.lemma.edit.edit_word", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.edit.edit_word"], ["", "def", "postprocess", "(", "self", ",", "words", ",", "preds", ",", "edits", "=", "None", ")", ":", "\n", "        ", "\"\"\" Postprocess, mainly for handing edits. \"\"\"", "\n", "assert", "len", "(", "words", ")", "==", "len", "(", "preds", ")", ",", "\"Lemma predictions must have same length as words.\"", "\n", "edited", "=", "[", "]", "\n", "if", "self", ".", "args", ".", "get", "(", "'edit'", ",", "False", ")", ":", "\n", "            ", "assert", "edits", "is", "not", "None", "and", "len", "(", "words", ")", "==", "len", "(", "edits", ")", "\n", "for", "w", ",", "p", ",", "e", "in", "zip", "(", "words", ",", "preds", ",", "edits", ")", ":", "\n", "                ", "lem", "=", "edit", ".", "edit_word", "(", "w", ",", "p", ",", "e", ")", "\n", "edited", "+=", "[", "lem", "]", "\n", "", "", "else", ":", "\n", "            ", "edited", "=", "preds", "# do not edit", "\n", "# final sanity check", "\n", "", "assert", "len", "(", "edited", ")", "==", "len", "(", "words", ")", "\n", "final", "=", "[", "]", "\n", "for", "lem", ",", "w", "in", "zip", "(", "edited", ",", "words", ")", ":", "\n", "            ", "if", "len", "(", "lem", ")", "==", "0", "or", "constant", ".", "UNK", "in", "lem", ":", "\n", "                ", "final", "+=", "[", "w", "]", "# invalid prediction, fall back to word", "\n", "", "else", ":", "\n", "                ", "final", "+=", "[", "lem", "]", "\n", "", "", "return", "final", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.update_lr": [[131, 133], ["lexenlem.models.common.utils.change_lr"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.change_lr"], ["", "def", "update_lr", "(", "self", ",", "new_lr", ")", ":", "\n", "        ", "utils", ".", "change_lr", "(", "self", ".", "optimizer", ",", "new_lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.train_dict": [[134, 147], ["collections.Counter", "collections.Counter.update", "collections.Counter.most_common"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.update"], ["", "def", "train_dict", "(", "self", ",", "triples", ")", ":", "\n", "        ", "\"\"\" Train a dict lemmatizer given training (word, pos, lemma) triples. \"\"\"", "\n", "# accumulate counter", "\n", "ctr", "=", "Counter", "(", ")", "\n", "ctr", ".", "update", "(", "[", "(", "p", "[", "0", "]", ",", "p", "[", "1", "]", ",", "p", "[", "2", "]", ")", "for", "p", "in", "triples", "]", ")", "\n", "# find the most frequent mappings", "\n", "for", "p", ",", "_", "in", "ctr", ".", "most_common", "(", ")", ":", "\n", "            ", "w", ",", "pos", ",", "l", "=", "p", "\n", "if", "(", "w", ",", "pos", ")", "not", "in", "self", ".", "composite_dict", ":", "\n", "                ", "self", ".", "composite_dict", "[", "(", "w", ",", "pos", ")", "]", "=", "l", "\n", "", "if", "w", "not", "in", "self", ".", "word_dict", ":", "\n", "                ", "self", ".", "word_dict", "[", "w", "]", "=", "l", "\n", "", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.predict_dict": [[148, 163], ["None"], "methods", ["None"], ["", "def", "predict_dict", "(", "self", ",", "pairs", ",", "ignore_empty", "=", "False", ")", ":", "\n", "        ", "\"\"\" Predict a list of lemmas using the dict model given (word, pos) pairs. \"\"\"", "\n", "lemmas", "=", "[", "]", "\n", "for", "p", "in", "pairs", ":", "\n", "            ", "w", ",", "pos", "=", "p", "\n", "if", "(", "w", ",", "pos", ")", "in", "self", ".", "composite_dict", ":", "\n", "                ", "lemmas", "+=", "[", "self", ".", "composite_dict", "[", "(", "w", ",", "pos", ")", "]", "]", "\n", "", "elif", "w", "in", "self", ".", "word_dict", ":", "\n", "                ", "lemmas", "+=", "[", "self", ".", "word_dict", "[", "w", "]", "]", "\n", "", "else", ":", "\n", "                ", "if", "ignore_empty", ":", "\n", "                    ", "lemmas", "+=", "[", "''", "]", "\n", "", "else", ":", "\n", "                    ", "lemmas", "+=", "[", "w", "]", "\n", "", "", "", "return", "lemmas", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.skip_seq2seq": [[164, 177], ["skip.append", "skip.append", "skip.append"], "methods", ["None"], ["", "def", "skip_seq2seq", "(", "self", ",", "pairs", ")", ":", "\n", "        ", "\"\"\" Determine if we can skip the seq2seq module when ensembling with the frequency lexicon. \"\"\"", "\n", "\n", "skip", "=", "[", "]", "\n", "for", "p", "in", "pairs", ":", "\n", "            ", "w", ",", "pos", "=", "p", "\n", "if", "(", "w", ",", "pos", ")", "in", "self", ".", "composite_dict", ":", "\n", "                ", "skip", ".", "append", "(", "True", ")", "\n", "", "elif", "w", "in", "self", ".", "word_dict", ":", "\n", "                ", "skip", ".", "append", "(", "True", ")", "\n", "", "else", ":", "\n", "                ", "skip", ".", "append", "(", "False", ")", "\n", "", "", "return", "skip", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.ensemble": [[178, 191], ["zip", "len", "len"], "methods", ["None"], ["", "def", "ensemble", "(", "self", ",", "pairs", ",", "other_preds", ")", ":", "\n", "        ", "\"\"\" Ensemble the dict with statitical model predictions. \"\"\"", "\n", "lemmas", "=", "[", "]", "\n", "assert", "len", "(", "pairs", ")", "==", "len", "(", "other_preds", ")", "\n", "for", "p", ",", "pred", "in", "zip", "(", "pairs", ",", "other_preds", ")", ":", "\n", "            ", "w", ",", "pos", "=", "p", "\n", "if", "(", "w", ",", "pos", ")", "in", "self", ".", "composite_dict", ":", "\n", "                ", "lemmas", "+=", "[", "self", ".", "composite_dict", "[", "(", "w", ",", "pos", ")", "]", "]", "\n", "", "elif", "w", "in", "self", ".", "word_dict", ":", "\n", "                ", "lemmas", "+=", "[", "self", ".", "word_dict", "[", "w", "]", "]", "\n", "", "else", ":", "\n", "                ", "lemmas", "+=", "[", "pred", "]", "\n", "", "", "return", "lemmas", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.save": [[192, 204], ["trainer.Trainer.vocab.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "print", "trainer.Trainer.model.state_dict", "print"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.state_dict", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.save", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.save", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.save", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.save", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.state_dict"], ["", "def", "save", "(", "self", ",", "filename", ")", ":", "\n", "        ", "params", "=", "{", "\n", "'model'", ":", "self", ".", "model", ".", "state_dict", "(", ")", "if", "self", ".", "model", "is", "not", "None", "else", "None", ",", "\n", "'dicts'", ":", "(", "self", ".", "word_dict", ",", "self", ".", "composite_dict", ")", ",", "\n", "'vocab'", ":", "self", ".", "vocab", ".", "state_dict", "(", ")", ",", "\n", "'config'", ":", "self", ".", "args", "\n", "}", "\n", "try", ":", "\n", "            ", "torch", ".", "save", "(", "params", ",", "filename", ")", "\n", "print", "(", "\"model saved to {}\"", ".", "format", "(", "filename", ")", ")", "\n", "", "except", "BaseException", ":", "\n", "            ", "print", "(", "\"[Warning: Saving failed... continuing anyway.]\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.Trainer.load": [[205, 219], ["lexenlem.models.lemma.vocab.MultiVocab.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load", "lexenlem.models.common.seq2seq_model.Seq2SeqModelCombined", "trainer.Trainer.model.load_state_dict", "print", "sys.exit"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.load_state_dict", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.load", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.load", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.load", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.load", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.load_state_dict"], ["", "", "def", "load", "(", "self", ",", "filename", ",", "use_cuda", "=", "False", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "filename", ",", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "", "except", "BaseException", ":", "\n", "            ", "print", "(", "\"Cannot load model from {}\"", ".", "format", "(", "filename", ")", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "", "self", ".", "args", "=", "checkpoint", "[", "'config'", "]", "\n", "self", ".", "word_dict", ",", "self", ".", "composite_dict", "=", "checkpoint", "[", "'dicts'", "]", "\n", "self", ".", "vocab", "=", "MultiVocab", ".", "load_state_dict", "(", "checkpoint", "[", "'vocab'", "]", ")", "\n", "if", "not", "self", ".", "args", "[", "'dict_only'", "]", ":", "\n", "            ", "self", ".", "model", "=", "Seq2SeqModelCombined", "(", "self", ".", "args", ",", "self", ".", "vocab", ",", "use_cuda", "=", "use_cuda", ")", "\n", "self", ".", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model'", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "model", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.__init__": [[222, 251], ["trainer.TrainerCombined.load", "dict", "dict", "trainer.TrainerCombined.args.get", "lexenlem.models.common.utils.get_optimizer", "lexenlem.models.common.seq2seq_model.Seq2SeqModelCombined", "lexenlem.models.common.loss.MixLoss", "print", "lexenlem.models.common.loss.SequenceLoss", "trainer.TrainerCombined.model.cuda", "trainer.TrainerCombined.crit.cuda", "trainer.TrainerCombined.model.cpu", "trainer.TrainerCombined.crit.cpu", "trainer.TrainerCombined.model.parameters"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.load", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.get_optimizer", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.loss.SequenceLoss", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cpu", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cpu"], ["def", "__init__", "(", "self", ",", "args", "=", "None", ",", "vocab", "=", "None", ",", "emb_matrix", "=", "None", ",", "model_file", "=", "None", ",", "use_cuda", "=", "False", ")", ":", "\n", "        ", "self", ".", "use_cuda", "=", "use_cuda", "\n", "if", "model_file", "is", "not", "None", ":", "\n", "# load everything from file", "\n", "            ", "self", ".", "load", "(", "model_file", ",", "use_cuda", ")", "\n", "", "else", ":", "\n", "# build model from scratch", "\n", "            ", "self", ".", "args", "=", "args", "\n", "self", ".", "model", "=", "None", "if", "args", "[", "'dict_only'", "]", "else", "Seq2SeqModelCombined", "(", "args", ",", "vocab", ",", "emb_matrix", "=", "emb_matrix", ",", "use_cuda", "=", "use_cuda", ")", "\n", "self", ".", "vocab", "=", "vocab", "\n", "# dict-based components", "\n", "self", ".", "word_dict", "=", "dict", "(", ")", "\n", "self", ".", "composite_dict", "=", "dict", "(", ")", "\n", "# lexicon", "\n", "self", ".", "lexicon", "=", "None", "\n", "", "if", "not", "self", ".", "args", "[", "'dict_only'", "]", ":", "\n", "            ", "if", "self", ".", "args", ".", "get", "(", "'edit'", ",", "False", ")", ":", "\n", "                ", "self", ".", "crit", "=", "loss", ".", "MixLoss", "(", "self", ".", "vocab", "[", "'combined'", "]", ".", "size", ",", "self", ".", "args", "[", "'alpha'", "]", ")", "\n", "print", "(", "\"[Running seq2seq lemmatizer with edit classifier]\"", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "crit", "=", "loss", ".", "SequenceLoss", "(", "self", ".", "vocab", "[", "'combined'", "]", ".", "size", ")", "\n", "", "self", ".", "parameters", "=", "[", "p", "for", "p", "in", "self", ".", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "if", "use_cuda", ":", "\n", "                ", "self", ".", "model", ".", "cuda", "(", ")", "\n", "self", ".", "crit", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "model", ".", "cpu", "(", ")", "\n", "self", ".", "crit", ".", "cpu", "(", ")", "\n", "", "self", ".", "optimizer", "=", "utils", ".", "get_optimizer", "(", "self", ".", "args", "[", "'optim'", "]", ",", "self", ".", "parameters", ",", "self", ".", "args", "[", "'lr'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.update": [[252, 276], ["trainer.unpack_batch_combined", "trainer.TrainerCombined.model", "trainer.TrainerCombined.args.get", "trainer.TrainerCombined.data.item", "trainer.TrainerCombined.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "trainer.TrainerCombined.optimizer.step", "trainer.TrainerCombined.model.eval", "trainer.TrainerCombined.model.train", "trainer.TrainerCombined.optimizer.zero_grad", "trainer.TrainerCombined.crit", "trainer.TrainerCombined.crit", "trainer.TrainerCombined.model.parameters", "log_probs.view", "tgt_out.view", "log_probs.view", "tgt_out.view"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.unpack_batch_combined", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.train"], ["", "", "def", "update", "(", "self", ",", "batch", ",", "eval", "=", "False", ")", ":", "\n", "        ", "inputs", ",", "_", "=", "unpack_batch_combined", "(", "batch", ",", "self", ".", "use_cuda", ")", "\n", "src", ",", "src_mask", ",", "lem", ",", "lem_mask", ",", "tgt_in", ",", "tgt_out", ",", "edits", "=", "inputs", "\n", "\n", "if", "eval", ":", "\n", "            ", "self", ".", "model", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "", "log_probs", ",", "edit_logits", "=", "self", ".", "model", "(", "src", ",", "src_mask", ",", "tgt_in", ",", "lem", ",", "lem_mask", ")", "\n", "if", "self", ".", "args", ".", "get", "(", "'edit'", ",", "False", ")", ":", "\n", "            ", "assert", "edit_logits", "is", "not", "None", "\n", "loss", "=", "self", ".", "crit", "(", "log_probs", ".", "view", "(", "-", "1", ",", "self", ".", "vocab", "[", "'combined'", "]", ".", "size", ")", ",", "tgt_out", ".", "view", "(", "-", "1", ")", ",", "edit_logits", ",", "edits", ")", "\n", "", "else", ":", "\n", "            ", "loss", "=", "self", ".", "crit", "(", "log_probs", ".", "view", "(", "-", "1", ",", "self", ".", "vocab", "[", "'combined'", "]", ".", "size", ")", ",", "tgt_out", ".", "view", "(", "-", "1", ")", ")", "\n", "", "loss_val", "=", "loss", ".", "data", ".", "item", "(", ")", "\n", "if", "eval", ":", "\n", "            ", "return", "loss_val", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "self", ".", "args", "[", "'max_grad_norm'", "]", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "return", "loss_val", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.predict": [[277, 295], ["trainer.unpack_batch_combined", "trainer.TrainerCombined.model.eval", "src.size", "trainer.TrainerCombined.model.predict", "lexenlem.models.common.utils.prune_decoded_seqs", "lexenlem.models.common.utils.unsort", "trainer.TrainerCombined.args.get", "trainer.TrainerCombined.vocab[].unmap", "numpy.argmax().reshape().tolist", "lexenlem.models.common.utils.unsort", "numpy.argmax().reshape", "numpy.argmax", "edit_logits.data.cpu().numpy", "edit_logits.data.cpu"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.unpack_batch_combined", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.predict", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.prune_decoded_seqs", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.unsort", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.unmap", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.unsort", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cpu"], ["", "def", "predict", "(", "self", ",", "batch", ",", "beam_size", "=", "1", ",", "log_attn", "=", "False", ")", ":", "\n", "        ", "inputs", ",", "orig_idx", "=", "unpack_batch_combined", "(", "batch", ",", "self", ".", "use_cuda", ")", "\n", "src", ",", "src_mask", ",", "lem", ",", "lem_mask", ",", "_", ",", "_", ",", "edits", "=", "inputs", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "batch_size", "=", "src", ".", "size", "(", "0", ")", "\n", "preds", ",", "edit_logits", ",", "log_attns", "=", "self", ".", "model", ".", "predict", "(", "src", ",", "src_mask", ",", "lem", ",", "lem_mask", ",", "beam_size", "=", "beam_size", ",", "log_attn", "=", "log_attn", ")", "\n", "pred_seqs", "=", "[", "self", ".", "vocab", "[", "'combined'", "]", ".", "unmap", "(", "ids", ")", "for", "ids", "in", "preds", "]", "# unmap to tokens", "\n", "pred_seqs", "=", "utils", ".", "prune_decoded_seqs", "(", "pred_seqs", ")", "\n", "pred_tokens", "=", "[", "\"\"", ".", "join", "(", "seq", ")", "for", "seq", "in", "pred_seqs", "]", "# join chars to be tokens", "\n", "pred_tokens", "=", "utils", ".", "unsort", "(", "pred_tokens", ",", "orig_idx", ")", "\n", "if", "self", ".", "args", ".", "get", "(", "'edit'", ",", "False", ")", ":", "\n", "            ", "assert", "edit_logits", "is", "not", "None", "\n", "edits", "=", "np", ".", "argmax", "(", "edit_logits", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "1", ")", ".", "reshape", "(", "[", "batch_size", "]", ")", ".", "tolist", "(", ")", "\n", "edits", "=", "utils", ".", "unsort", "(", "edits", ",", "orig_idx", ")", "\n", "", "else", ":", "\n", "            ", "edits", "=", "None", "\n", "", "return", "pred_tokens", ",", "edits", ",", "log_attns", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.save": [[296, 309], ["trainer.TrainerCombined.vocab.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "print", "trainer.TrainerCombined.model.state_dict", "print"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.state_dict", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.save", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.save", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.save", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.save", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.state_dict"], ["", "def", "save", "(", "self", ",", "filename", ")", ":", "\n", "        ", "params", "=", "{", "\n", "'model'", ":", "self", ".", "model", ".", "state_dict", "(", ")", "if", "self", ".", "model", "is", "not", "None", "else", "None", ",", "\n", "'dicts'", ":", "(", "self", ".", "word_dict", ",", "self", ".", "composite_dict", ")", ",", "\n", "'vocab'", ":", "self", ".", "vocab", ".", "state_dict", "(", ")", ",", "\n", "'config'", ":", "self", ".", "args", ",", "\n", "'lexicon'", ":", "self", ".", "lexicon", "\n", "}", "\n", "try", ":", "\n", "            ", "torch", ".", "save", "(", "params", ",", "filename", ")", "\n", "print", "(", "\"model saved to {}\"", ".", "format", "(", "filename", ")", ")", "\n", "", "except", "BaseException", ":", "\n", "            ", "print", "(", "\"[Warning: Saving failed... continuing anyway.]\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.load": [[310, 325], ["lexenlem.models.lemma.vocab.MultiVocab.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load", "lexenlem.models.common.seq2seq_model.Seq2SeqModelCombined", "trainer.TrainerCombined.model.load_state_dict", "print", "sys.exit"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.load_state_dict", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.load", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.load", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.load", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.load", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.load_state_dict"], ["", "", "def", "load", "(", "self", ",", "filename", ",", "use_cuda", "=", "False", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "filename", ",", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "", "except", "BaseException", ":", "\n", "            ", "print", "(", "\"Cannot load model from {}\"", ".", "format", "(", "filename", ")", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "", "self", ".", "args", "=", "checkpoint", "[", "'config'", "]", "\n", "self", ".", "word_dict", ",", "self", ".", "composite_dict", "=", "checkpoint", "[", "'dicts'", "]", "\n", "self", ".", "vocab", "=", "MultiVocab", ".", "load_state_dict", "(", "checkpoint", "[", "'vocab'", "]", ")", "\n", "self", ".", "lexicon", "=", "checkpoint", "[", "'lexicon'", "]", "\n", "if", "not", "self", ".", "args", "[", "'dict_only'", "]", ":", "\n", "            ", "self", ".", "model", "=", "Seq2SeqModelCombined", "(", "self", ".", "args", ",", "self", ".", "vocab", ",", "use_cuda", "=", "use_cuda", ")", "\n", "self", ".", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model'", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "model", "=", "None", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.unpack_batch": [[18, 26], ["b.cuda"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda"], ["def", "unpack_batch", "(", "batch", ",", "use_cuda", ")", ":", "\n", "    ", "\"\"\" Unpack a batch from the data loader. \"\"\"", "\n", "if", "use_cuda", ":", "\n", "        ", "inputs", "=", "[", "b", ".", "cuda", "(", ")", "if", "b", "is", "not", "None", "else", "None", "for", "b", "in", "batch", "[", ":", "9", "]", "]", "\n", "", "else", ":", "\n", "        ", "inputs", "=", "[", "b", "if", "b", "is", "not", "None", "else", "None", "for", "b", "in", "batch", "[", ":", "9", "]", "]", "\n", "", "orig_idx", "=", "batch", "[", "9", "]", "\n", "return", "inputs", ",", "orig_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.unpack_batch_combined": [[27, 35], ["b.cuda"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda"], ["", "def", "unpack_batch_combined", "(", "batch", ",", "use_cuda", ")", ":", "\n", "    ", "\"\"\" Unpack a batch from the data loader. \"\"\"", "\n", "if", "use_cuda", ":", "\n", "        ", "inputs", "=", "[", "b", ".", "cuda", "(", ")", "if", "b", "is", "not", "None", "else", "None", "for", "b", "in", "batch", "[", ":", "7", "]", "]", "\n", "", "else", ":", "\n", "        ", "inputs", "=", "[", "b", "if", "b", "is", "not", "None", "else", "None", "for", "b", "in", "batch", "[", ":", "7", "]", "]", "\n", "", "orig_idx", "=", "batch", "[", "7", "]", "\n", "return", "inputs", ",", "orig_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.__init__": [[18, 81], ["args.get", "args.get", "print", "print", "isinstance", "random.sample.DataLoaderCombined.preprocess", "len", "filename.endswith", "random.sample.DataLoaderCombined.load_file", "isinstance", "dict", "random.sample.DataLoaderCombined.init_vocab", "lexenlem.models.lemma.vocab.MultiVocab", "int", "random.sample", "print", "print", "lexenlem.models.common.lexicon.Lexicon", "random.sample.DataLoaderCombined.lemmatizer.init_lexicon", "list", "random.shuffle", "random.sample.DataLoaderCombined.load_doc", "len", "len", "args.get", "range", "range", "zip", "len", "args.get", "args.get", "len", "len"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.preprocess", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.load_file", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.init_vocab", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.Lexicon.init_lexicon", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.load_doc", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get"], ["    ", "def", "__init__", "(", "self", ",", "input_src", ",", "batch_size", ",", "args", ",", "lemmatizer", "=", "None", ",", "vocab", "=", "None", ",", "evaluation", "=", "False", ",", "conll_only", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "        ", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "eval", "=", "evaluation", "\n", "self", ".", "shuffled", "=", "not", "self", ".", "eval", "\n", "\n", "self", ".", "lemmatizer", "=", "lemmatizer", "\n", "self", ".", "morph", "=", "args", ".", "get", "(", "'morph'", ",", "True", ")", "\n", "self", ".", "pos", "=", "args", ".", "get", "(", "'pos'", ",", "True", ")", "\n", "print", "(", "'Using FEATS:'", ",", "self", ".", "morph", ")", "\n", "print", "(", "'Using POS:'", ",", "self", ".", "pos", ")", "\n", "\n", "# check if input source is a file or a Document object", "\n", "if", "isinstance", "(", "input_src", ",", "str", ")", ":", "\n", "            ", "filename", "=", "input_src", "\n", "assert", "filename", ".", "endswith", "(", "'conllu'", ")", ",", "\"Loaded file must be conllu file.\"", "\n", "self", ".", "conll", ",", "data", "=", "self", ".", "load_file", "(", "filename", ")", "\n", "", "elif", "isinstance", "(", "input_src", ",", "Document", ")", ":", "\n", "            ", "filename", "=", "None", "\n", "doc", "=", "input_src", "\n", "self", ".", "conll", ",", "data", "=", "self", ".", "load_doc", "(", "doc", ")", "\n", "\n", "", "if", "conll_only", ":", "# only load conll file", "\n", "            ", "return", "\n", "\n", "", "if", "skip", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "data", ")", "==", "len", "(", "skip", ")", "\n", "data", "=", "[", "x", "for", "x", ",", "y", "in", "zip", "(", "data", ",", "skip", ")", "if", "not", "y", "]", "\n", "\n", "# handle vocab", "\n", "", "if", "vocab", "is", "not", "None", ":", "\n", "            ", "self", ".", "vocab", "=", "vocab", "\n", "", "else", ":", "\n", "            ", "self", ".", "vocab", "=", "dict", "(", ")", "\n", "combined_vocab", "=", "self", ".", "init_vocab", "(", "data", ")", "\n", "self", ".", "vocab", "=", "MultiVocab", "(", "{", "'combined'", ":", "combined_vocab", "}", ")", "\n", "\n", "# filter and sample data", "\n", "", "if", "args", ".", "get", "(", "'sample_train'", ",", "1.0", ")", "<", "1.0", "and", "not", "self", ".", "eval", ":", "\n", "            ", "keep", "=", "int", "(", "args", "[", "'sample_train'", "]", "*", "len", "(", "data", ")", ")", "\n", "data", "=", "random", ".", "sample", "(", "data", ",", "keep", ")", "\n", "print", "(", "\"Subsample training set with rate {:g}\"", ".", "format", "(", "args", "[", "'sample_train'", "]", ")", ")", "\n", "\n", "", "if", "lemmatizer", "==", "'lexicon'", ":", "\n", "            ", "print", "(", "\"Building the lexicon...\"", ")", "\n", "self", ".", "lemmatizer", "=", "Lexicon", "(", "\n", "unimorph", "=", "args", "[", "'unimorph_dir'", "]", ",", "\n", "use_pos", "=", "args", ".", "get", "(", "'use_pos'", ",", "False", ")", ",", "\n", "use_word", "=", "args", ".", "get", "(", "'use_word'", ",", "False", ")", "\n", ")", "\n", "self", ".", "lemmatizer", ".", "init_lexicon", "(", "data", ")", "\n", "\n", "", "data", "=", "self", ".", "preprocess", "(", "data", ",", "self", ".", "vocab", "[", "'combined'", "]", ",", "args", ")", "\n", "# shuffle for training", "\n", "if", "self", ".", "shuffled", ":", "\n", "            ", "indices", "=", "list", "(", "range", "(", "len", "(", "data", ")", ")", ")", "\n", "random", ".", "shuffle", "(", "indices", ")", "\n", "data", "=", "[", "data", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "", "self", ".", "num_examples", "=", "len", "(", "data", ")", "\n", "\n", "# chunk into batches", "\n", "data", "=", "[", "data", "[", "i", ":", "i", "+", "batch_size", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "data", ")", ",", "batch_size", ")", "]", "\n", "self", ".", "data", "=", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.make_feats_data": [[82, 91], ["feats_data.extend", "feats_data.append", "feats.split"], "methods", ["None"], ["", "def", "make_feats_data", "(", "self", ",", "data", ",", "feats_idx", "=", "3", ")", ":", "\n", "        ", "feats_data", "=", "[", "]", "\n", "for", "d", "in", "data", ":", "\n", "            ", "feats", "=", "d", "[", "feats_idx", "]", "\n", "if", "'|'", "in", "feats", ":", "\n", "                ", "feats_data", ".", "extend", "(", "feats", ".", "split", "(", "'|'", ")", ")", "\n", "", "else", ":", "\n", "                ", "feats_data", ".", "append", "(", "feats", ")", "\n", "", "", "return", "feats_data", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.init_vocab": [[92, 100], ["list", "data.DataLoaderCombined.make_feats_data", "lexenlem.models.lemma.vocab.Vocab"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.make_feats_data"], ["", "def", "init_vocab", "(", "self", ",", "data", ")", ":", "\n", "        ", "assert", "self", ".", "eval", "is", "False", ",", "\"Vocab file must exist for evaluation\"", "\n", "char_data", "=", "list", "(", "\"\"", ".", "join", "(", "d", "[", "0", "]", "+", "d", "[", "2", "]", "for", "d", "in", "data", ")", ")", "\n", "pos_data", "=", "[", "'POS='", "+", "d", "[", "1", "]", "for", "d", "in", "data", "]", "\n", "feats_data", "=", "self", ".", "make_feats_data", "(", "data", ")", "\n", "combined_data", "=", "char_data", "+", "pos_data", "+", "feats_data", "\n", "combined_vocab", "=", "Vocab", "(", "combined_data", ",", "self", ".", "args", "[", "'lang'", "]", ")", "\n", "return", "combined_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.preprocess": [[101, 146], ["args.get", "list", "combined_vocab.map", "combined_vocab.map", "list", "combined_vocab.map", "combined_vocab.map", "processed.append", "feats.extend", "feats.append", "lexenlem.models.lemma.edit.get_edit_type", "d[].split", "type", "data.DataLoaderCombined.lemmatizer.lemmatize", "data.DataLoaderCombined.lemmatizer.lemmatize", "data.DataLoaderCombined.lemmatizer.lemmatize", "args[].split"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.map", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.map", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.map", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.map", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.edit.get_edit_type", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.ExtendedLexicon.lemmatize", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.ExtendedLexicon.lemmatize", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.ExtendedLexicon.lemmatize"], ["", "def", "preprocess", "(", "self", ",", "data", ",", "combined_vocab", ",", "args", ")", ":", "\n", "        ", "processed", "=", "[", "]", "\n", "eos_after", "=", "args", ".", "get", "(", "'eos_after'", ",", "False", ")", "\n", "for", "d", "in", "data", ":", "\n", "            ", "edit_type", "=", "edit", ".", "EDIT_TO_ID", "[", "edit", ".", "get_edit_type", "(", "d", "[", "0", "]", ",", "d", "[", "2", "]", ")", "]", "\n", "src", "=", "list", "(", "d", "[", "0", "]", ")", "\n", "if", "eos_after", ":", "\n", "                ", "src", "=", "[", "constant", ".", "SOS", "]", "+", "src", "\n", "", "else", ":", "\n", "                ", "src", "=", "[", "constant", ".", "SOS", "]", "+", "src", "+", "[", "constant", ".", "EOS", "]", "\n", "", "pos", "=", "[", "'POS='", "+", "d", "[", "1", "]", "]", "\n", "feats", "=", "[", "]", "\n", "if", "'|'", "in", "d", "[", "3", "]", ":", "\n", "                ", "feats", ".", "extend", "(", "d", "[", "3", "]", ".", "split", "(", "'|'", ")", ")", "\n", "", "else", ":", "\n", "                ", "feats", ".", "append", "(", "d", "[", "3", "]", ")", "\n", "", "inp", "=", "src", "\n", "if", "self", ".", "pos", ":", "\n", "                ", "inp", "+=", "pos", "\n", "", "if", "self", ".", "morph", ":", "\n", "                ", "inp", "+=", "feats", "\n", "", "if", "eos_after", ":", "\n", "                ", "inp", "+=", "[", "constant", ".", "EOS", "]", "\n", "", "inp", "=", "combined_vocab", ".", "map", "(", "inp", ")", "\n", "processed_sent", "=", "[", "inp", "]", "\n", "if", "self", ".", "lemmatizer", "is", "None", ":", "\n", "                ", "lem", "=", "[", "constant", ".", "SOS", ",", "constant", ".", "EOS", "]", "\n", "", "else", ":", "\n", "                ", "if", "type", "(", "self", ".", "lemmatizer", ")", "in", "[", "Lexicon", ",", "ExtendedLexicon", "]", ":", "\n", "                    ", "lem", "=", "self", ".", "lemmatizer", ".", "lemmatize", "(", "d", "[", "0", "]", ",", "d", "[", "1", "]", ")", "\n", "", "elif", "args", "[", "'lemmatizer'", "]", "==", "'apertium'", ":", "\n", "                    ", "lem", "=", "self", ".", "lemmatizer", ".", "lemmatize", "(", "d", "[", "0", "]", ",", "args", "[", "'lang'", "]", ".", "split", "(", "'_'", ")", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lem", "=", "self", ".", "lemmatizer", ".", "lemmatize", "(", "d", "[", "0", "]", ")", "\n", "", "lem", "=", "[", "constant", ".", "SOS", "]", "+", "lem", "+", "[", "constant", ".", "EOS", "]", "\n", "", "lem", "=", "combined_vocab", ".", "map", "(", "lem", ")", "\n", "processed_sent", "+=", "[", "lem", "]", "\n", "tgt", "=", "list", "(", "d", "[", "2", "]", ")", "\n", "tgt_in", "=", "combined_vocab", ".", "map", "(", "[", "constant", ".", "SOS", "]", "+", "tgt", ")", "\n", "tgt_out", "=", "combined_vocab", ".", "map", "(", "tgt", "+", "[", "constant", ".", "EOS", "]", ")", "\n", "processed_sent", "+=", "[", "tgt_in", "]", "\n", "processed_sent", "+=", "[", "tgt_out", "]", "\n", "processed_sent", "+=", "[", "edit_type", "]", "\n", "processed", ".", "append", "(", "processed_sent", ")", "\n", "", "return", "processed", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.__len__": [[147, 149], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.__getitem__": [[150, 177], ["len", "list", "lexenlem.models.common.data.sort_all", "lexenlem.models.common.data.get_long_tensor", "torch.eq", "lexenlem.models.common.data.get_long_tensor", "torch.eq", "lexenlem.models.common.data.get_long_tensor", "lexenlem.models.common.data.get_long_tensor", "torch.LongTensor", "isinstance", "zip", "len", "len", "lexenlem.models.common.data.get_long_tensor.size", "lexenlem.models.common.data.get_long_tensor.size", "len"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.data.sort_all", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.data.get_long_tensor", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.data.get_long_tensor", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.data.get_long_tensor", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.data.get_long_tensor", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "__getitem__", "(", "self", ",", "key", ")", ":", "\n", "        ", "\"\"\" Get a batch with index. \"\"\"", "\n", "if", "not", "isinstance", "(", "key", ",", "int", ")", ":", "\n", "            ", "raise", "TypeError", "\n", "", "if", "key", "<", "0", "or", "key", ">=", "len", "(", "self", ".", "data", ")", ":", "\n", "            ", "raise", "IndexError", "\n", "", "batch", "=", "self", ".", "data", "[", "key", "]", "\n", "batch_size", "=", "len", "(", "batch", ")", "\n", "batch", "=", "list", "(", "zip", "(", "*", "batch", ")", ")", "\n", "assert", "len", "(", "batch", ")", "==", "5", "\n", "\n", "# sort all fields by lens for easy RNN operations", "\n", "lens", "=", "[", "len", "(", "x", ")", "for", "x", "in", "batch", "[", "0", "]", "]", "\n", "batch", ",", "orig_idx", "=", "sort_all", "(", "batch", ",", "lens", ")", "\n", "\n", "# convert to tensors", "\n", "src", "=", "batch", "[", "0", "]", "\n", "src", "=", "get_long_tensor", "(", "src", ",", "batch_size", ")", "\n", "src_mask", "=", "torch", ".", "eq", "(", "src", ",", "constant", ".", "PAD_ID", ")", "\n", "lem", "=", "batch", "[", "1", "]", "\n", "lem", "=", "get_long_tensor", "(", "lem", ",", "batch_size", ")", "\n", "lem_mask", "=", "torch", ".", "eq", "(", "lem", ",", "constant", ".", "PAD_ID", ")", "\n", "tgt_in", "=", "get_long_tensor", "(", "batch", "[", "2", "]", ",", "batch_size", ")", "\n", "tgt_out", "=", "get_long_tensor", "(", "batch", "[", "3", "]", ",", "batch_size", ")", "\n", "edits", "=", "torch", ".", "LongTensor", "(", "batch", "[", "4", "]", ")", "\n", "assert", "tgt_in", ".", "size", "(", "1", ")", "==", "tgt_out", ".", "size", "(", "1", ")", ",", "\"Target input and output sequence sizes do not match.\"", "\n", "return", "src", ",", "src_mask", ",", "lem", ",", "lem_mask", ",", "tgt_in", ",", "tgt_out", ",", "edits", ",", "orig_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.__iter__": [[178, 181], ["range", "data.DataLoaderCombined.__len__", "data.DataLoaderCombined.__getitem__"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.__len__", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__getitem__"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "__len__", "(", ")", ")", ":", "\n", "            ", "yield", "self", ".", "__getitem__", "(", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.load_file": [[182, 186], ["lexenlem.models.common.conll.CoNLLFile", "lexenlem.models.common.conll.CoNLLFile.get"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get"], ["", "", "def", "load_file", "(", "self", ",", "filename", ")", ":", "\n", "        ", "conll_file", "=", "conll", ".", "CoNLLFile", "(", "filename", ")", "\n", "data", "=", "conll_file", ".", "get", "(", "[", "'word'", ",", "'upos'", ",", "'lemma'", ",", "'feats'", "]", ")", "\n", "return", "conll_file", ",", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.data.DataLoaderCombined.load_doc": [[187, 190], ["doc.conll_file.get"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get"], ["", "def", "load_doc", "(", "self", ",", "doc", ")", ":", "\n", "        ", "data", "=", "doc", ".", "conll_file", ".", "get", "(", "[", "'word'", ",", "'upos'", ",", "'lemma'", ",", "'feats'", "]", ")", "\n", "return", "doc", ".", "conll_file", ",", "data", "", "", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.edit.get_edit_type": [[7, 14], ["word.lower"], "function", ["None"], ["def", "get_edit_type", "(", "word", ",", "lemma", ")", ":", "\n", "    ", "\"\"\" Calculate edit types. \"\"\"", "\n", "if", "lemma", "==", "word", ":", "\n", "        ", "return", "'identity'", "\n", "", "elif", "lemma", "==", "word", ".", "lower", "(", ")", ":", "\n", "        ", "return", "'lower'", "\n", "", "return", "'none'", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.edit.edit_word": [[15, 27], ["word.lower", "Exception"], "function", ["None"], ["", "def", "edit_word", "(", "word", ",", "pred", ",", "edit_id", ")", ":", "\n", "    ", "\"\"\"\n    Edit a word, given edit and seq2seq predictions.\n    \"\"\"", "\n", "if", "edit_id", "==", "1", ":", "\n", "        ", "return", "word", "\n", "", "elif", "edit_id", "==", "2", ":", "\n", "        ", "return", "word", ".", "lower", "(", ")", "\n", "", "elif", "edit_id", "==", "0", ":", "\n", "        ", "return", "pred", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "\"Unrecognized edit ID: {}\"", ".", "format", "(", "edit_id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.scorer.score": [[6, 14], ["lexenlem.utils.conll18_ud_eval.load_conllu_file", "lexenlem.utils.conll18_ud_eval.load_conllu_file", "lexenlem.utils.conll18_ud_eval.evaluate"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.load_conllu_file", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.load_conllu_file", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.evaluate"], ["def", "score", "(", "system_conllu_file", ",", "gold_conllu_file", ")", ":", "\n", "    ", "\"\"\" Wrapper for lemma scorer. \"\"\"", "\n", "gold_ud", "=", "ud_eval", ".", "load_conllu_file", "(", "gold_conllu_file", ")", "\n", "system_ud", "=", "ud_eval", ".", "load_conllu_file", "(", "system_conllu_file", ")", "\n", "evaluation", "=", "ud_eval", ".", "evaluate", "(", "gold_ud", ",", "system_ud", ")", "\n", "el", "=", "evaluation", "[", "\"Lemmas\"", "]", "\n", "p", ",", "r", ",", "f", "=", "el", ".", "precision", ",", "el", ".", "recall", ",", "el", ".", "f1", "\n", "return", "p", ",", "r", ",", "f", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.vocab.Vocab.build_vocab": [[7, 11], ["collections.Counter", "list", "sorted", "enumerate", "list", "collections.Counter.keys"], "methods", ["None"], ["    ", "def", "build_vocab", "(", "self", ")", ":", "\n", "        ", "counter", "=", "Counter", "(", "self", ".", "data", ")", "\n", "self", ".", "_id2unit", "=", "VOCAB_PREFIX", "+", "list", "(", "sorted", "(", "list", "(", "counter", ".", "keys", "(", ")", ")", ",", "key", "=", "lambda", "k", ":", "counter", "[", "k", "]", ",", "reverse", "=", "True", ")", ")", "\n", "self", ".", "_unit2id", "=", "{", "w", ":", "i", "for", "i", ",", "w", "in", "enumerate", "(", "self", ".", "_id2unit", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.vocab.MultiVocab.load_state_dict": [[13, 22], ["cls", "state_dict.items", "FeatureVocab.load_state_dict", "Vocab.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.load_state_dict", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.load_state_dict"], ["    ", "@", "classmethod", "\n", "def", "load_state_dict", "(", "cls", ",", "state_dict", ")", ":", "\n", "        ", "new", "=", "cls", "(", ")", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "==", "'feats'", ":", "\n", "                ", "new", "[", "k", "]", "=", "FeatureVocab", ".", "load_state_dict", "(", "v", ")", "\n", "", "else", ":", "\n", "                ", "new", "[", "k", "]", "=", "Vocab", ".", "load_state_dict", "(", "v", ")", "\n", "", "", "return", "new", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.vocab.FeatureVocab.__init__": [[24, 26], ["lexenlem.models.common.vocab.CompositeVocab.__init__"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data", "=", "None", ",", "lang", "=", "\"\"", ",", "idx", "=", "0", ",", "sep", "=", "\"|\"", ",", "keyed", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "data", ",", "lang", ",", "idx", "=", "idx", ",", "sep", "=", "sep", ",", "keyed", "=", "keyed", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.__init__": [[24, 44], ["beam.Beam.tt.FloatTensor().zero_", "beam.Beam.tt.LongTensor().fill_", "beam.Beam.tt.FloatTensor", "beam.Beam.tt.LongTensor"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "size", ",", "cuda", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "size", "=", "size", "\n", "self", ".", "done", "=", "False", "\n", "\n", "self", ".", "tt", "=", "torch", ".", "cuda", "if", "cuda", "else", "torch", "\n", "\n", "# The score for each translation on the beam.", "\n", "self", ".", "scores", "=", "self", ".", "tt", ".", "FloatTensor", "(", "size", ")", ".", "zero_", "(", ")", "\n", "self", ".", "allScores", "=", "[", "]", "\n", "\n", "# The backpointers at each time-step.", "\n", "self", ".", "prevKs", "=", "[", "]", "\n", "\n", "# The outputs at each time-step.", "\n", "self", ".", "nextYs", "=", "[", "self", ".", "tt", ".", "LongTensor", "(", "size", ")", ".", "fill_", "(", "constant", ".", "PAD_ID", ")", "]", "\n", "self", ".", "nextYs", "[", "0", "]", "[", "0", "]", "=", "constant", ".", "SOS_ID", "\n", "\n", "# The copy indices for each time", "\n", "self", ".", "copy", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.get_current_state": [[45, 48], ["None"], "methods", ["None"], ["", "def", "get_current_state", "(", "self", ")", ":", "\n", "        ", "\"Get the outputs for the current timestep.\"", "\n", "return", "self", ".", "nextYs", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.get_current_origin": [[49, 52], ["None"], "methods", ["None"], ["", "def", "get_current_origin", "(", "self", ")", ":", "\n", "        ", "\"Get the backpointers for the current timestep.\"", "\n", "return", "self", ".", "prevKs", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.advance": [[53, 96], ["wordLk.size", "beamLk.view", "beamLk.view.topk", "beam.Beam.allScores.append", "beam.Beam.prevKs.append", "beam.Beam.nextYs.append", "len", "beam.Beam.copy.append", "beam.Beam.allScores.append", "beam.Beam.scores.unsqueeze().expand_as", "copy_indices.index_select", "beam.Beam.scores.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "advance", "(", "self", ",", "wordLk", ",", "copy_indices", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Given prob over words for every last beam `wordLk` and attention\n        `attnOut`: Compute and update the beam search.\n\n        Parameters:\n\n        * `wordLk`- probs of advancing from the last step (K x words)\n        * `copy_indices` - copy indices (K x ctx_len)\n\n        Returns: True if beam search is complete.\n        \"\"\"", "\n", "if", "self", ".", "done", ":", "\n", "            ", "return", "True", "\n", "", "numWords", "=", "wordLk", ".", "size", "(", "1", ")", "\n", "\n", "# Sum the previous scores.", "\n", "if", "len", "(", "self", ".", "prevKs", ")", ">", "0", ":", "\n", "            ", "beamLk", "=", "wordLk", "+", "self", ".", "scores", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "wordLk", ")", "\n", "", "else", ":", "\n", "# first step, expand from the first position", "\n", "            ", "beamLk", "=", "wordLk", "[", "0", "]", "\n", "\n", "", "flatBeamLk", "=", "beamLk", ".", "view", "(", "-", "1", ")", "\n", "\n", "bestScores", ",", "bestScoresId", "=", "flatBeamLk", ".", "topk", "(", "self", ".", "size", ",", "0", ",", "True", ",", "True", ")", "\n", "self", ".", "allScores", ".", "append", "(", "self", ".", "scores", ")", "\n", "self", ".", "scores", "=", "bestScores", "\n", "\n", "# bestScoresId is flattened beam x word array, so calculate which", "\n", "# word and beam each score came from", "\n", "prevK", "=", "bestScoresId", "/", "numWords", "\n", "self", ".", "prevKs", ".", "append", "(", "prevK", ")", "\n", "self", ".", "nextYs", ".", "append", "(", "bestScoresId", "-", "prevK", "*", "numWords", ")", "\n", "if", "copy_indices", "is", "not", "None", ":", "\n", "            ", "self", ".", "copy", ".", "append", "(", "copy_indices", ".", "index_select", "(", "0", ",", "prevK", ")", ")", "\n", "\n", "# End condition is when top-of-beam is EOS.", "\n", "", "if", "self", ".", "nextYs", "[", "-", "1", "]", "[", "0", "]", "==", "constant", ".", "EOS_ID", ":", "\n", "            ", "self", ".", "done", "=", "True", "\n", "self", ".", "allScores", ".", "append", "(", "self", ".", "scores", ")", "\n", "\n", "", "return", "self", ".", "done", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.sort_best": [[97, 99], ["torch.sort"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.sort"], ["", "def", "sort_best", "(", "self", ")", ":", "\n", "        ", "return", "torch", ".", "sort", "(", "self", ".", "scores", ",", "0", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.get_best": [[100, 104], ["beam.Beam.sortBest"], "methods", ["None"], ["", "def", "get_best", "(", "self", ")", ":", "\n", "        ", "\"Get the score of the best in the beam.\"", "\n", "scores", ",", "ids", "=", "self", ".", "sortBest", "(", ")", "\n", "return", "scores", "[", "1", "]", ",", "ids", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.get_hyp": [[105, 131], ["range", "enumerate", "hyp.append", "len", "len", "cpy.append"], "methods", ["None"], ["", "def", "get_hyp", "(", "self", ",", "k", ")", ":", "\n", "        ", "\"\"\"\n        Walk back to construct the full hypothesis.\n\n        Parameters:\n\n             * `k` - the position in the beam to construct.\n\n         Returns: The hypothesis\n        \"\"\"", "\n", "hyp", "=", "[", "]", "\n", "cpy", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "self", ".", "prevKs", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "hyp", ".", "append", "(", "self", ".", "nextYs", "[", "j", "+", "1", "]", "[", "k", "]", ")", "\n", "if", "len", "(", "self", ".", "copy", ")", ">", "0", ":", "\n", "                ", "cpy", ".", "append", "(", "self", ".", "copy", "[", "j", "]", "[", "k", "]", ")", "\n", "", "k", "=", "self", ".", "prevKs", "[", "j", "]", "[", "k", "]", "\n", "\n", "", "hyp", "=", "hyp", "[", ":", ":", "-", "1", "]", "\n", "cpy", "=", "cpy", "[", ":", ":", "-", "1", "]", "\n", "# postprocess: if cpy index is not -1, use cpy index instead of hyp word", "\n", "for", "i", ",", "cidx", "in", "enumerate", "(", "cpy", ")", ":", "\n", "            ", "if", "cidx", ">=", "0", ":", "\n", "                ", "hyp", "[", "i", "]", "=", "-", "(", "cidx", "+", "1", ")", "# make index 1-based and flip it for token generation", "\n", "\n", "", "", "return", "hyp", "\n", "", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.__init__": [[12, 26], ["Exception", "os.path.exists", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "filename", "=", "None", ",", "input_str", "=", "None", ",", "ignore_gapping", "=", "True", ")", ":", "\n", "# If ignore_gapping is True, all words that are gap fillers (identified with a period in", "\n", "# the sentence index) will be ignored.", "\n", "\n", "        ", "self", ".", "ignore_gapping", "=", "ignore_gapping", "\n", "if", "filename", "is", "not", "None", "and", "not", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "            ", "raise", "Exception", "(", "\"File not found at: \"", "+", "filename", ")", "\n", "", "if", "filename", "is", "None", ":", "\n", "            ", "assert", "input_str", "is", "not", "None", "and", "len", "(", "input_str", ")", ">", "0", "\n", "self", ".", "_file", "=", "input_str", "\n", "self", ".", "_from_str", "=", "True", "\n", "", "else", ":", "\n", "            ", "self", ".", "_file", "=", "filename", "\n", "self", ".", "_from_str", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.load_all": [[27, 31], ["None"], "methods", ["None"], ["", "", "def", "load_all", "(", "self", ")", ":", "\n", "        ", "\"\"\" Trigger all lazy initializations so that the file is loaded.\"\"\"", "\n", "_", "=", "self", ".", "sents", "\n", "_", "=", "self", ".", "num_words", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.load_conll": [[32, 60], ["io.StringIO", "open", "len", "sents.append", "line.strip.strip.strip", "len", "line.strip.strip.startswith", "line.strip.strip.split", "len", "sents.append", "len"], "methods", ["None"], ["", "def", "load_conll", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Load data into a list of sentences, where each sentence is a list of lines,\n        and each line is a list of conllu fields.\n        \"\"\"", "\n", "sents", ",", "cache", "=", "[", "]", ",", "[", "]", "\n", "if", "self", ".", "_from_str", ":", "\n", "            ", "infile", "=", "io", ".", "StringIO", "(", "self", ".", "file", ")", "\n", "", "else", ":", "\n", "            ", "infile", "=", "open", "(", "self", ".", "file", ",", "encoding", "=", "'utf-8'", ")", "\n", "", "with", "infile", ":", "\n", "            ", "for", "line", "in", "infile", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "                    ", "if", "len", "(", "cache", ")", ">", "0", ":", "\n", "                        ", "sents", ".", "append", "(", "cache", ")", "\n", "cache", "=", "[", "]", "\n", "", "", "else", ":", "\n", "                    ", "if", "line", ".", "startswith", "(", "'#'", ")", ":", "# skip comment line", "\n", "                        ", "continue", "\n", "", "array", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "if", "self", ".", "ignore_gapping", "and", "'.'", "in", "array", "[", "0", "]", ":", "\n", "                        ", "continue", "\n", "", "assert", "len", "(", "array", ")", "==", "FIELD_NUM", "\n", "cache", "+=", "[", "array", "]", "\n", "", "", "", "if", "len", "(", "cache", ")", ">", "0", ":", "\n", "            ", "sents", ".", "append", "(", "cache", ")", "\n", "", "return", "sents", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.file": [[61, 64], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "file", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_file", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.sents": [[65, 70], ["hasattr", "conll.CoNLLFile.load_conll"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.load_conll"], ["", "@", "property", "\n", "def", "sents", "(", "self", ")", ":", "\n", "        ", "if", "not", "hasattr", "(", "self", ",", "'_sents'", ")", ":", "\n", "            ", "self", ".", "_sents", "=", "self", ".", "load_conll", "(", ")", "\n", "", "return", "self", ".", "_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.__len__": [[71, 73], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "sents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.num_words": [[74, 85], ["hasattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_words", "(", "self", ")", ":", "\n", "        ", "\"\"\" Num of total words, after multi-word expansion.\"\"\"", "\n", "if", "not", "hasattr", "(", "self", ",", "'_num_words'", ")", ":", "\n", "            ", "n", "=", "0", "\n", "for", "sent", "in", "self", ".", "sents", ":", "\n", "                ", "for", "ln", "in", "sent", ":", "\n", "                    ", "if", "'-'", "not", "in", "ln", "[", "0", "]", ":", "\n", "                        ", "n", "+=", "1", "\n", "", "", "", "self", ".", "_num_words", "=", "n", "\n", "", "return", "self", ".", "_num_words", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get": [[86, 110], ["isinstance", "len", "results.append", "f.lower", "len"], "methods", ["None"], ["", "def", "get", "(", "self", ",", "fields", ",", "as_sentences", "=", "False", ")", ":", "\n", "        ", "\"\"\" Get fields from a list of field names. If only one field name is provided, return a list\n        of that field; if more than one, return a list of list. Note that all returned fields are after\n        multi-word expansion.\n        \"\"\"", "\n", "assert", "isinstance", "(", "fields", ",", "list", ")", ",", "\"Must provide field names as a list.\"", "\n", "assert", "len", "(", "fields", ")", ">=", "1", ",", "\"Must have at least one field.\"", "\n", "field_idxs", "=", "[", "FIELD_TO_IDX", "[", "f", ".", "lower", "(", ")", "]", "for", "f", "in", "fields", "]", "\n", "results", "=", "[", "]", "\n", "for", "sent", "in", "self", ".", "sents", ":", "\n", "            ", "cursent", "=", "[", "]", "\n", "for", "ln", "in", "sent", ":", "\n", "                ", "if", "'-'", "in", "ln", "[", "0", "]", ":", "# skip", "\n", "                    ", "continue", "\n", "", "if", "len", "(", "field_idxs", ")", "==", "1", ":", "\n", "                    ", "cursent", "+=", "[", "ln", "[", "field_idxs", "[", "0", "]", "]", "]", "\n", "", "else", ":", "\n", "                    ", "cursent", "+=", "[", "[", "ln", "[", "fid", "]", "for", "fid", "in", "field_idxs", "]", "]", "\n", "\n", "", "", "if", "as_sentences", ":", "\n", "                ", "results", ".", "append", "(", "cursent", ")", "\n", "", "else", ":", "\n", "                ", "results", "+=", "cursent", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.set": [[111, 131], ["isinstance", "isinstance", "len", "len", "f.lower", "len", "zip"], "methods", ["None"], ["", "def", "set", "(", "self", ",", "fields", ",", "contents", ")", ":", "\n", "        ", "\"\"\" Set fields based on contents. If only one field (singleton list) is provided, then a list of content will be expected; otherwise a list of list of contents will be expected.\n        \"\"\"", "\n", "assert", "isinstance", "(", "fields", ",", "list", ")", ",", "\"Must provide field names as a list.\"", "\n", "assert", "isinstance", "(", "contents", ",", "list", ")", ",", "\"Must provide contents as a list (one item per line).\"", "\n", "assert", "len", "(", "fields", ")", ">=", "1", ",", "\"Must have at least one field.\"", "\n", "assert", "self", ".", "num_words", "==", "len", "(", "contents", ")", ",", "\"Contents must have the same number as the original file.\"", "\n", "field_idxs", "=", "[", "FIELD_TO_IDX", "[", "f", ".", "lower", "(", ")", "]", "for", "f", "in", "fields", "]", "\n", "cidx", "=", "0", "\n", "for", "sent", "in", "self", ".", "sents", ":", "\n", "            ", "for", "ln", "in", "sent", ":", "\n", "                ", "if", "'-'", "in", "ln", "[", "0", "]", ":", "\n", "                    ", "continue", "\n", "", "if", "len", "(", "field_idxs", ")", "==", "1", ":", "\n", "                    ", "ln", "[", "field_idxs", "[", "0", "]", "]", "=", "contents", "[", "cidx", "]", "\n", "", "else", ":", "\n", "                    ", "for", "fid", ",", "ct", "in", "zip", "(", "field_idxs", ",", "contents", "[", "cidx", "]", ")", ":", "\n", "                        ", "ln", "[", "fid", "]", "=", "ct", "\n", "", "", "cidx", "+=", "1", "\n", "", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.write_conll": [[132, 139], ["conll.CoNLLFile.conll_as_string", "open", "outfile.write"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.conll_as_string"], ["", "def", "write_conll", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\" Write current conll contents to file.\n        \"\"\"", "\n", "conll_string", "=", "self", ".", "conll_as_string", "(", ")", "\n", "with", "open", "(", "filename", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "outfile", ":", "\n", "            ", "outfile", ".", "write", "(", "conll_string", ")", "\n", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.conll_as_string": [[140, 149], ["None"], "methods", ["None"], ["", "def", "conll_as_string", "(", "self", ")", ":", "\n", "        ", "\"\"\" Return current conll contents as string\n        \"\"\"", "\n", "return_string", "=", "\"\"", "\n", "for", "sent", "in", "self", ".", "sents", ":", "\n", "            ", "for", "ln", "in", "sent", ":", "\n", "                ", "return_string", "+=", "(", "\"\\t\"", ".", "join", "(", "ln", ")", "+", "\"\\n\"", ")", "\n", "", "return_string", "+=", "\"\\n\"", "\n", "", "return", "return_string", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.write_conll_with_lemmas": [[150, 167], ["len", "open", "print", "print", "len"], "methods", ["None"], ["", "def", "write_conll_with_lemmas", "(", "self", ",", "lemmas", ",", "filename", ")", ":", "\n", "        ", "\"\"\" Write a new conll file, but use the new lemmas to replace the old ones.\"\"\"", "\n", "assert", "self", ".", "num_words", "==", "len", "(", "lemmas", ")", ",", "\"Num of lemmas does not match the number in original data file.\"", "\n", "lemma_idx", "=", "FIELD_TO_IDX", "[", "'lemma'", "]", "\n", "idx", "=", "0", "\n", "with", "open", "(", "filename", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "outfile", ":", "\n", "            ", "for", "sent", "in", "self", ".", "sents", ":", "\n", "                ", "for", "ln", "in", "sent", ":", "\n", "                    ", "if", "'-'", "not", "in", "ln", "[", "0", "]", ":", "# do not process if it is a mwt line", "\n", "                        ", "lm", "=", "lemmas", "[", "idx", "]", "\n", "if", "len", "(", "lm", ")", "==", "0", ":", "\n", "                            ", "lm", "=", "'_'", "\n", "", "ln", "[", "lemma_idx", "]", "=", "lm", "\n", "idx", "+=", "1", "\n", "", "print", "(", "\"\\t\"", ".", "join", "(", "ln", ")", ",", "file", "=", "outfile", ")", "\n", "", "print", "(", "\"\"", ",", "file", "=", "outfile", ")", "\n", "", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get_mwt_expansions": [[168, 195], ["int", "int", "int", "ln[].split"], "methods", ["None"], ["", "def", "get_mwt_expansions", "(", "self", ")", ":", "\n", "        ", "word_idx", "=", "FIELD_TO_IDX", "[", "'word'", "]", "\n", "expansions", "=", "[", "]", "\n", "src", "=", "''", "\n", "dst", "=", "[", "]", "\n", "for", "sent", "in", "self", ".", "sents", ":", "\n", "            ", "mwt_begin", "=", "0", "\n", "mwt_end", "=", "-", "1", "\n", "for", "ln", "in", "sent", ":", "\n", "                ", "if", "'.'", "in", "ln", "[", "0", "]", ":", "\n", "# skip ellipsis", "\n", "                    ", "continue", "\n", "\n", "", "if", "'-'", "in", "ln", "[", "0", "]", ":", "\n", "                    ", "mwt_begin", ",", "mwt_end", "=", "[", "int", "(", "x", ")", "for", "x", "in", "ln", "[", "0", "]", ".", "split", "(", "'-'", ")", "]", "\n", "src", "=", "ln", "[", "word_idx", "]", "\n", "continue", "\n", "\n", "", "if", "mwt_begin", "<=", "int", "(", "ln", "[", "0", "]", ")", "<", "mwt_end", ":", "\n", "                    ", "dst", "+=", "[", "ln", "[", "word_idx", "]", "]", "\n", "", "elif", "int", "(", "ln", "[", "0", "]", ")", "==", "mwt_end", ":", "\n", "                    ", "dst", "+=", "[", "ln", "[", "word_idx", "]", "]", "\n", "expansions", "+=", "[", "[", "src", ",", "' '", ".", "join", "(", "dst", ")", "]", "]", "\n", "src", "=", "''", "\n", "dst", "=", "[", "]", "\n", "\n", "", "", "", "return", "expansions", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get_mwt_expansion_cands": [[196, 205], ["None"], "methods", ["None"], ["", "def", "get_mwt_expansion_cands", "(", "self", ")", ":", "\n", "        ", "word_idx", "=", "FIELD_TO_IDX", "[", "'word'", "]", "\n", "cands", "=", "[", "]", "\n", "for", "sent", "in", "self", ".", "sents", ":", "\n", "            ", "for", "ln", "in", "sent", ":", "\n", "                ", "if", "\"MWT=Yes\"", "in", "ln", "[", "-", "1", "]", ":", "\n", "                    ", "cands", "+=", "[", "ln", "[", "word_idx", "]", "]", "\n", "\n", "", "", "", "return", "cands", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.write_conll_with_mwt_expansions": [[206, 232], ["print", "len", "len", "print", "print", "enumerate", "print", "expansions[].split", "len", "len", "enumerate", "str", "str"], "methods", ["None"], ["", "def", "write_conll_with_mwt_expansions", "(", "self", ",", "expansions", ",", "output_file", ")", ":", "\n", "        ", "\"\"\" Expands MWTs predicted by the tokenizer and write to file. This method replaces the head column with a right branching tree. \"\"\"", "\n", "idx", "=", "0", "\n", "count", "=", "0", "\n", "\n", "for", "sent", "in", "self", ".", "sents", ":", "\n", "            ", "for", "ln", "in", "sent", ":", "\n", "                ", "idx", "+=", "1", "\n", "if", "\"MWT=Yes\"", "not", "in", "ln", "[", "-", "1", "]", ":", "\n", "                    ", "print", "(", "\"{}\\t{}\"", ".", "format", "(", "idx", ",", "\"\\t\"", ".", "join", "(", "ln", "[", "1", ":", "6", "]", "+", "[", "str", "(", "idx", "-", "1", ")", "]", "+", "ln", "[", "7", ":", "]", ")", ")", ",", "file", "=", "output_file", ")", "\n", "", "else", ":", "\n", "# print MWT expansion", "\n", "                    ", "expanded", "=", "[", "x", "for", "x", "in", "expansions", "[", "count", "]", ".", "split", "(", "' '", ")", "if", "len", "(", "x", ")", ">", "0", "]", "\n", "count", "+=", "1", "\n", "endidx", "=", "idx", "+", "len", "(", "expanded", ")", "-", "1", "\n", "\n", "print", "(", "\"{}-{}\\t{}\"", ".", "format", "(", "idx", ",", "endidx", ",", "\"\\t\"", ".", "join", "(", "[", "'_'", "if", "i", "==", "5", "or", "i", "==", "8", "else", "x", "for", "i", ",", "x", "in", "enumerate", "(", "ln", "[", "1", ":", "]", ")", "]", ")", ")", ",", "file", "=", "output_file", ")", "\n", "for", "e_i", ",", "e_word", "in", "enumerate", "(", "expanded", ")", ":", "\n", "                        ", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "idx", "+", "e_i", ",", "e_word", ",", "\"\\t\"", ".", "join", "(", "[", "'_'", "]", "*", "4", "+", "[", "str", "(", "idx", "+", "e_i", "-", "1", ")", "]", "+", "[", "'_'", "]", "*", "3", ")", ")", ",", "file", "=", "output_file", ")", "\n", "", "idx", "=", "endidx", "\n", "\n", "", "", "print", "(", "\"\"", ",", "file", "=", "output_file", ")", "\n", "idx", "=", "0", "\n", "\n", "", "assert", "count", "==", "len", "(", "expansions", ")", ",", "\"{} {} {}\"", ".", "format", "(", "count", ",", "len", "(", "expansions", ")", ",", "expansions", ")", "\n", "return", "\n", "", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.Lexicon.__init__": [[7, 19], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "print", "print"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "unimorph", "=", "False", ",", "use_pos", "=", "True", ",", "use_word", "=", "True", ")", ":", "\n", "        ", "self", ".", "pos_lexicon", "=", "defaultdict", "(", "str", ")", "\n", "self", ".", "word_lexicon", "=", "defaultdict", "(", "str", ")", "\n", "self", ".", "unimorph_lexicon", "=", "defaultdict", "(", "str", ")", "\n", "self", ".", "unimorph", "=", "unimorph", "\n", "self", ".", "use_word", "=", "use_word", "\n", "self", ".", "use_pos", "=", "use_pos", "\n", "\n", "if", "self", ".", "use_pos", ":", "\n", "            ", "print", "(", "'[Using the word-pos lexicon...]'", ")", "\n", "", "if", "self", ".", "use_word", ":", "\n", "            ", "print", "(", "'[Using the word lexicon...]'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.Lexicon.init_unimorph": [[20, 33], ["collections.Counter", "collections.Counter.most_common", "open", "line.split.split.strip", "line.split.split.split", "len", "line[].split"], "methods", ["None"], ["", "", "def", "init_unimorph", "(", "self", ")", ":", "\n", "        ", "ctr", "=", "Counter", "(", ")", "\n", "with", "open", "(", "self", ".", "unimorph", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                    ", "line", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "if", "len", "(", "line", "[", "1", "]", ".", "split", "(", ")", ")", "==", "1", ":", "\n", "                        ", "lemma", ",", "word", "=", "line", "[", "0", "]", ",", "line", "[", "1", "]", "\n", "ctr", "[", "(", "lemma", ",", "word", ")", "]", "+=", "1", "\n", "", "", "", "", "for", "entry", ",", "_", "in", "ctr", ".", "most_common", "(", ")", ":", "\n", "            ", "lemma", ",", "word", "=", "entry", "\n", "self", ".", "unimorph_lexicon", "[", "word", "]", "+=", "lemma", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.Lexicon.init_lexicon": [[34, 44], ["collections.Counter", "collections.Counter.update", "collections.Counter.most_common", "print", "lexicon.Lexicon.init_unimorph"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.update", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.Lexicon.init_unimorph"], ["", "", "def", "init_lexicon", "(", "self", ",", "data", ")", ":", "\n", "        ", "ctr", "=", "Counter", "(", ")", "\n", "ctr", ".", "update", "(", "[", "(", "d", "[", "0", "]", ",", "d", "[", "1", "]", ",", "d", "[", "2", "]", ")", "for", "d", "in", "data", "]", ")", "\n", "for", "entry", ",", "_", "in", "ctr", ".", "most_common", "(", ")", ":", "\n", "            ", "word", ",", "pos", ",", "lemma", "=", "entry", "\n", "self", ".", "pos_lexicon", "[", "(", "word", ",", "pos", ")", "]", "+=", "lemma", "\n", "self", ".", "word_lexicon", "[", "word", "]", "+=", "lemma", "\n", "", "if", "self", ".", "unimorph", ":", "\n", "            ", "print", "(", "\"[Loading the Unimorph lexicon...]\"", ")", "\n", "self", ".", "init_unimorph", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.Lexicon.lemmatize": [[45, 53], ["list", "list", "list"], "methods", ["None"], ["", "", "def", "lemmatize", "(", "self", ",", "word", ",", "pos", ")", ":", "\n", "        ", "if", "(", "word", ",", "pos", ")", "in", "self", ".", "pos_lexicon", "and", "self", ".", "use_pos", ":", "\n", "            ", "return", "list", "(", "self", ".", "pos_lexicon", "[", "(", "word", ",", "pos", ")", "]", ")", "\n", "", "if", "self", ".", "unimorph", "and", "word", "in", "self", ".", "unimorph_lexicon", ":", "\n", "            ", "return", "list", "(", "self", ".", "unimorph_lexicon", "[", "word", "]", ")", "\n", "", "if", "word", "in", "self", ".", "word_lexicon", "and", "self", ".", "use_word", ":", "\n", "            ", "return", "list", "(", "self", ".", "word_lexicon", "[", "word", "]", ")", "\n", "", "return", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.ExtendedLexicon.__init__": [[56, 59], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "lexicon", ",", "extension", ")", ":", "\n", "        ", "self", ".", "lexicon", "=", "lexicon", "\n", "self", ".", "extension", "=", "extension", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.ExtendedLexicon.lemmatize": [[60, 65], ["lexicon.ExtendedLexicon.lexicon.lemmatize", "lexicon.ExtendedLexicon.extension.lemmatize"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.ExtendedLexicon.lemmatize", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.lexicon.ExtendedLexicon.lemmatize"], ["", "def", "lemmatize", "(", "self", ",", "word", ",", "pos", ")", ":", "\n", "        ", "candidate", "=", "self", ".", "lexicon", ".", "lemmatize", "(", "word", ",", "pos", ")", "\n", "if", "candidate", "==", "[", "]", ":", "\n", "            ", "candidate", "=", "self", ".", "extension", ".", "lemmatize", "(", "word", ")", "\n", "", "return", "candidate", "", "", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.BasicAttention.__init__": [[16, 24], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Softmax", "torch.Softmax"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__"], ["def", "__init__", "(", "self", ",", "dim", ")", ":", "\n", "        ", "super", "(", "BasicAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear_in", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "linear_c", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "linear_v", "=", "nn", ".", "Linear", "(", "dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "linear_out", "=", "nn", ".", "Linear", "(", "dim", "*", "2", ",", "dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "sm", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.BasicAttention.forward": [[25, 51], ["context.size", "context.size", "context.size", "seq2seq_modules.BasicAttention.linear_in", "seq2seq_modules.BasicAttention.linear_c().view", "seq2seq_modules.BasicAttention.tanh", "seq2seq_modules.BasicAttention.linear_v().view", "seq2seq_modules.BasicAttention.sm", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seq2seq_modules.BasicAttention.tanh", "seq2seq_modules.BasicAttention.unsqueeze().expand_as", "seq2seq_modules.BasicAttention.masked_fill_", "seq2seq_modules.BasicAttention.linear_out", "seq2seq_modules.BasicAttention.linear_c", "seq2seq_modules.BasicAttention.linear_v", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "context.contiguous().view", "seq2seq_modules.BasicAttention.unsqueeze", "seq2seq_modules.BasicAttention.view", "seq2seq_modules.BasicAttention.unsqueeze", "context.contiguous"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "context", ",", "mask", "=", "None", ",", "attn_only", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        input: batch x dim\n        context: batch x sourceL x dim\n        \"\"\"", "\n", "batch_size", "=", "context", ".", "size", "(", "0", ")", "\n", "source_len", "=", "context", ".", "size", "(", "1", ")", "\n", "dim", "=", "context", ".", "size", "(", "2", ")", "\n", "target", "=", "self", ".", "linear_in", "(", "input", ")", "# batch x dim", "\n", "source", "=", "self", ".", "linear_c", "(", "context", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "dim", ")", ")", ".", "view", "(", "batch_size", ",", "source_len", ",", "dim", ")", "\n", "attn", "=", "target", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "context", ")", "+", "source", "\n", "attn", "=", "self", ".", "tanh", "(", "attn", ")", "# batch x sourceL x dim", "\n", "attn", "=", "self", ".", "linear_v", "(", "attn", ".", "view", "(", "-", "1", ",", "dim", ")", ")", ".", "view", "(", "batch_size", ",", "source_len", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "attn", ".", "masked_fill_", "(", "mask", ",", "-", "constant", ".", "INFINITY_NUMBER", ")", "\n", "\n", "", "attn", "=", "self", ".", "sm", "(", "attn", ")", "\n", "if", "attn_only", ":", "\n", "            ", "return", "attn", "\n", "\n", "", "weighted_context", "=", "torch", ".", "bmm", "(", "attn", ".", "unsqueeze", "(", "1", ")", ",", "context", ")", ".", "squeeze", "(", "1", ")", "\n", "h_tilde", "=", "torch", ".", "cat", "(", "(", "weighted_context", ",", "input", ")", ",", "1", ")", "\n", "h_tilde", "=", "self", ".", "tanh", "(", "self", ".", "linear_out", "(", "h_tilde", ")", ")", "\n", "\n", "return", "h_tilde", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.SoftDotAttention.__init__": [[59, 67], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__"], ["def", "__init__", "(", "self", ",", "dim", ")", ":", "\n", "        ", "\"\"\"Initialize layer.\"\"\"", "\n", "super", "(", "SoftDotAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear_in", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "sm", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "self", ".", "linear_out", "=", "nn", ".", "Linear", "(", "dim", "*", "2", ",", "dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "mask", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.SoftDotAttention.forward": [[68, 96], ["seq2seq_modules.SoftDotAttention.linear_in().unsqueeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "seq2seq_modules.SoftDotAttention.sm", "seq2seq_modules.SoftDotAttention.view", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seq2seq_modules.SoftDotAttention.tanh", "seq2seq_modules.SoftDotAttention.masked_fill_", "seq2seq_modules.SoftDotAttention.size", "seq2seq_modules.SoftDotAttention.size", "seq2seq_modules.SoftDotAttention.linear_out", "seq2seq_modules.SoftDotAttention.linear_in", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "mask.size", "seq2seq_modules.SoftDotAttention.size", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "context", ",", "mask", "=", "None", ",", "attn_only", "=", "False", ")", ":", "\n", "        ", "\"\"\"Propogate input through the network.\n\n        input: batch x dim\n        context: batch x sourceL x dim\n        \"\"\"", "\n", "target", "=", "self", ".", "linear_in", "(", "input", ")", ".", "unsqueeze", "(", "2", ")", "# batch x dim x 1", "\n", "\n", "# Get attention", "\n", "attn", "=", "torch", ".", "bmm", "(", "context", ",", "target", ")", ".", "squeeze", "(", "2", ")", "# batch x sourceL", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "# sett the padding attention logits to -inf", "\n", "            ", "assert", "mask", ".", "size", "(", ")", "==", "attn", ".", "size", "(", ")", ",", "\"Mask size must match the attention size!\"", "\n", "attn", ".", "masked_fill_", "(", "mask", ",", "-", "constant", ".", "INFINITY_NUMBER", ")", "\n", "\n", "", "attn", "=", "self", ".", "sm", "(", "attn", ")", "\n", "if", "attn_only", ":", "\n", "            ", "return", "attn", "\n", "\n", "", "attn3", "=", "attn", ".", "view", "(", "attn", ".", "size", "(", "0", ")", ",", "1", ",", "attn", ".", "size", "(", "1", ")", ")", "# batch x 1 x sourceL", "\n", "\n", "weighted_context", "=", "torch", ".", "bmm", "(", "attn3", ",", "context", ")", ".", "squeeze", "(", "1", ")", "# batch x dim", "\n", "h_tilde", "=", "torch", ".", "cat", "(", "(", "weighted_context", ",", "input", ")", ",", "1", ")", "\n", "\n", "h_tilde", "=", "self", ".", "tanh", "(", "self", ".", "linear_out", "(", "h_tilde", ")", ")", "\n", "\n", "return", "h_tilde", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.MultiHeadAttention.__init__": [[102, 116], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "num_heads", ")", ":", "\n", "        ", "super", "(", "MultiHeadAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "\n", "assert", "d_model", "%", "self", ".", "num_heads", "==", "0", "\n", "\n", "self", ".", "depth", "=", "d_model", "\n", "\n", "self", ".", "wq", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "wk", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "wv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.MultiHeadAttention.forward": [[117, 119], ["seq2seq_modules.MultiHeadAttention.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "context", ",", "mask", "=", "None", ",", "attn_only", "=", "False", ")", ":", "\n", "        ", "target", "=", "self", ".", "linear", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.LinearAttention.__init__": [[126, 133], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__"], ["def", "__init__", "(", "self", ",", "dim", ")", ":", "\n", "        ", "super", "(", "LinearAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "dim", "*", "3", ",", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "linear_out", "=", "nn", ".", "Linear", "(", "dim", "*", "2", ",", "dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "sm", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "mask", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.LinearAttention.forward": [[134, 162], ["context.size", "context.size", "context.size", "input.unsqueeze().expand_as().contiguous().view", "context.contiguous().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seq2seq_modules.LinearAttention.linear().view", "seq2seq_modules.LinearAttention.sm", "seq2seq_modules.LinearAttention.view", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seq2seq_modules.LinearAttention.tanh", "seq2seq_modules.LinearAttention.masked_fill_", "seq2seq_modules.LinearAttention.linear_out", "input.unsqueeze().expand_as().contiguous", "context.contiguous", "input.unsqueeze().expand_as().contiguous().view.mul", "seq2seq_modules.LinearAttention.linear", "mask.size", "seq2seq_modules.LinearAttention.size", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "input.unsqueeze().expand_as", "input.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "context", ",", "mask", "=", "None", ",", "attn_only", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        input: batch x dim\n        context: batch x sourceL x dim\n        \"\"\"", "\n", "batch_size", "=", "context", ".", "size", "(", "0", ")", "\n", "source_len", "=", "context", ".", "size", "(", "1", ")", "\n", "dim", "=", "context", ".", "size", "(", "2", ")", "\n", "u", "=", "input", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "context", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "dim", ")", "# batch*sourceL x dim", "\n", "v", "=", "context", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "dim", ")", "\n", "attn_in", "=", "torch", ".", "cat", "(", "(", "u", ",", "v", ",", "u", ".", "mul", "(", "v", ")", ")", ",", "1", ")", "\n", "attn", "=", "self", ".", "linear", "(", "attn_in", ")", ".", "view", "(", "batch_size", ",", "source_len", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "# sett the padding attention logits to -inf", "\n", "            ", "assert", "mask", ".", "size", "(", ")", "==", "attn", ".", "size", "(", ")", ",", "\"Mask size must match the attention size!\"", "\n", "attn", ".", "masked_fill_", "(", "mask", ",", "-", "constant", ".", "INFINITY_NUMBER", ")", "\n", "\n", "", "attn", "=", "self", ".", "sm", "(", "attn", ")", "\n", "if", "attn_only", ":", "\n", "            ", "return", "attn", "\n", "\n", "", "attn3", "=", "attn", ".", "view", "(", "batch_size", ",", "1", ",", "source_len", ")", "# batch x 1 x sourceL", "\n", "\n", "weighted_context", "=", "torch", ".", "bmm", "(", "attn3", ",", "context", ")", ".", "squeeze", "(", "1", ")", "# batch x dim", "\n", "h_tilde", "=", "torch", ".", "cat", "(", "(", "weighted_context", ",", "input", ")", ",", "1", ")", "\n", "h_tilde", "=", "self", ".", "tanh", "(", "self", ".", "linear_out", "(", "h_tilde", ")", ")", "\n", "return", "h_tilde", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.DeepAttention.__init__": [[170, 179], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Softmax", "torch.Softmax", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__"], ["def", "__init__", "(", "self", ",", "dim", ")", ":", "\n", "        ", "super", "(", "DeepAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear_in", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "linear_v", "=", "nn", ".", "Linear", "(", "dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "linear_out", "=", "nn", ".", "Linear", "(", "dim", "*", "2", ",", "dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "sm", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "mask", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.DeepAttention.forward": [[180, 208], ["context.size", "context.size", "context.size", "input.unsqueeze().expand_as().contiguous().view", "seq2seq_modules.DeepAttention.relu", "seq2seq_modules.DeepAttention.relu", "seq2seq_modules.DeepAttention.linear_v().view", "seq2seq_modules.DeepAttention.sm", "seq2seq_modules.DeepAttention.view", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seq2seq_modules.DeepAttention.tanh", "seq2seq_modules.DeepAttention.linear_in", "seq2seq_modules.DeepAttention.linear_in", "seq2seq_modules.DeepAttention.masked_fill_", "seq2seq_modules.DeepAttention.linear_out", "input.unsqueeze().expand_as().contiguous", "context.contiguous().view", "seq2seq_modules.DeepAttention.linear_v", "mask.size", "seq2seq_modules.DeepAttention.size", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "seq2seq_modules.DeepAttention.mul", "input.unsqueeze().expand_as", "context.contiguous", "input.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "context", ",", "mask", "=", "None", ",", "attn_only", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        input: batch x dim\n        context: batch x sourceL x dim\n        \"\"\"", "\n", "batch_size", "=", "context", ".", "size", "(", "0", ")", "\n", "source_len", "=", "context", ".", "size", "(", "1", ")", "\n", "dim", "=", "context", ".", "size", "(", "2", ")", "\n", "u", "=", "input", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "context", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "dim", ")", "# batch*sourceL x dim", "\n", "u", "=", "self", ".", "relu", "(", "self", ".", "linear_in", "(", "u", ")", ")", "\n", "v", "=", "self", ".", "relu", "(", "self", ".", "linear_in", "(", "context", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "dim", ")", ")", ")", "\n", "attn", "=", "self", ".", "linear_v", "(", "u", ".", "mul", "(", "v", ")", ")", ".", "view", "(", "batch_size", ",", "source_len", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "# sett the padding attention logits to -inf", "\n", "            ", "assert", "mask", ".", "size", "(", ")", "==", "attn", ".", "size", "(", ")", ",", "\"Mask size must match the attention size!\"", "\n", "attn", ".", "masked_fill_", "(", "mask", ",", "-", "constant", ".", "INFINITY_NUMBER", ")", "\n", "\n", "", "attn", "=", "self", ".", "sm", "(", "attn", ")", "\n", "if", "attn_only", ":", "\n", "            ", "return", "attn", "\n", "\n", "", "attn3", "=", "attn", ".", "view", "(", "batch_size", ",", "1", ",", "source_len", ")", "# batch x 1 x sourceL", "\n", "\n", "weighted_context", "=", "torch", ".", "bmm", "(", "attn3", ",", "context", ")", ".", "squeeze", "(", "1", ")", "# batch x dim", "\n", "h_tilde", "=", "torch", ".", "cat", "(", "(", "weighted_context", ",", "input", ")", ",", "1", ")", "\n", "h_tilde", "=", "self", ".", "tanh", "(", "self", ".", "linear_out", "(", "h_tilde", ")", ")", "\n", "return", "h_tilde", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.LSTMAttention.__init__": [[212, 232], ["torch.Module.__init__", "torch.LSTMCell", "torch.LSTMCell", "print", "seq2seq_modules.SoftDotAttention", "seq2seq_modules.BasicAttention", "seq2seq_modules.LinearAttention", "seq2seq_modules.DeepAttention", "Exception"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "batch_first", "=", "True", ",", "attn_type", "=", "'soft'", ")", ":", "\n", "        ", "\"\"\"Initialize params.\"\"\"", "\n", "super", "(", "LSTMAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "batch_first", "=", "batch_first", "\n", "\n", "self", ".", "lstm_cell", "=", "nn", ".", "LSTMCell", "(", "input_size", ",", "hidden_size", ")", "\n", "\n", "if", "attn_type", "==", "'soft'", ":", "\n", "            ", "self", ".", "attention_layer", "=", "SoftDotAttention", "(", "hidden_size", ")", "\n", "", "elif", "attn_type", "==", "'mlp'", ":", "\n", "            ", "self", ".", "attention_layer", "=", "BasicAttention", "(", "hidden_size", ")", "\n", "", "elif", "attn_type", "==", "'linear'", ":", "\n", "            ", "self", ".", "attention_layer", "=", "LinearAttention", "(", "hidden_size", ")", "\n", "", "elif", "attn_type", "==", "'deep'", ":", "\n", "            ", "self", ".", "attention_layer", "=", "DeepAttention", "(", "hidden_size", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unsupported LSTM attention type: {}\"", ".", "format", "(", "attn_type", ")", ")", "\n", "", "print", "(", "\"Using {} attention for LSTM.\"", ".", "format", "(", "attn_type", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.LSTMAttention.forward": [[233, 254], ["range", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "input.transpose.transpose.transpose", "input.transpose.transpose.size", "seq2seq_modules.LSTMAttention.lstm_cell", "seq2seq_modules.LSTMAttention.attention_layer", "output.transpose.transpose.append", "input.transpose.transpose.size", "output.transpose.transpose.transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "output[].size"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "hidden", ",", "ctx", ",", "ctx_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"Propogate input through the network.\"\"\"", "\n", "if", "self", ".", "batch_first", ":", "\n", "            ", "input", "=", "input", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "#print('dec_input:', input.size())", "\n", "#print('dec_hidden:', hidden[0].size(), hidden[1].size())", "\n", "\n", "", "output", "=", "[", "]", "\n", "steps", "=", "range", "(", "input", ".", "size", "(", "0", ")", ")", "\n", "for", "i", "in", "steps", ":", "\n", "            ", "hidden", "=", "self", ".", "lstm_cell", "(", "input", "[", "i", "]", ",", "hidden", ")", "\n", "hy", ",", "cy", "=", "hidden", "\n", "h_tilde", ",", "alpha", "=", "self", ".", "attention_layer", "(", "hy", ",", "ctx", ",", "mask", "=", "ctx_mask", ")", "\n", "output", ".", "append", "(", "h_tilde", ")", "\n", "", "output", "=", "torch", ".", "cat", "(", "output", ",", "0", ")", ".", "view", "(", "input", ".", "size", "(", "0", ")", ",", "*", "output", "[", "0", "]", ".", "size", "(", ")", ")", "\n", "\n", "if", "self", ".", "batch_first", ":", "\n", "            ", "output", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "", "return", "output", ",", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.LSTMDoubleAttention.__init__": [[259, 263], ["seq2seq_modules.LSTMAttention.__init__", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "batch_first", "=", "True", ",", "attn_type", "=", "'soft'", ")", ":", "\n", "        ", "\"\"\"Initialize params.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "input_size", ",", "hidden_size", ",", "batch_first", ",", "attn_type", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "hidden_size", "*", "2", ",", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_modules.LSTMDoubleAttention.forward": [[265, 291], ["range", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "input.transpose.transpose.transpose", "input.transpose.transpose.size", "seq2seq_modules.LSTMDoubleAttention.lstm_cell", "seq2seq_modules.LSTMDoubleAttention.attention_layer", "seq2seq_modules.LSTMDoubleAttention.attention_layer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seq2seq_modules.LSTMDoubleAttention.linear", "output.transpose.transpose.append", "input.transpose.transpose.size", "output.transpose.transpose.transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "output[].size"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "hidden", ",", "src_ctx", ",", "lex_ctx", ",", "ctx_mask", "=", "None", ",", "lex_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"Propogate input through the network.\"\"\"", "\n", "if", "self", ".", "batch_first", ":", "\n", "            ", "input", "=", "input", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "#print('dec_input:', input.size())", "\n", "#print('dec_hidden:', hidden[0].size(), hidden[1].size())", "\n", "\n", "", "output", "=", "[", "]", "\n", "steps", "=", "range", "(", "input", ".", "size", "(", "0", ")", ")", "\n", "for", "i", "in", "steps", ":", "\n", "            ", "hidden", "=", "self", ".", "lstm_cell", "(", "input", "[", "i", "]", ",", "hidden", ")", "\n", "hy", ",", "_", "=", "hidden", "\n", "h_tilde0", ",", "attn0", "=", "self", ".", "attention_layer", "(", "hy", ",", "src_ctx", ",", "mask", "=", "ctx_mask", ")", "\n", "h_tilde1", ",", "attn1", "=", "self", ".", "attention_layer", "(", "hy", ",", "lex_ctx", ",", "mask", "=", "lex_mask", ")", "\n", "h_tilde", "=", "torch", ".", "cat", "(", "(", "h_tilde0", ",", "h_tilde1", ")", ",", "1", ")", "\n", "h_tilde", "=", "self", ".", "linear", "(", "h_tilde", ")", "\n", "output", ".", "append", "(", "h_tilde", ")", "\n", "", "output", "=", "torch", ".", "cat", "(", "output", ",", "0", ")", ".", "view", "(", "input", ".", "size", "(", "0", ")", ",", "*", "output", "[", "0", "]", ".", "size", "(", ")", ")", "\n", "\n", "if", "self", ".", "batch_first", ":", "\n", "            ", "output", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "", "attn", "=", "(", "attn0", ",", "attn1", ")", "\n", "\n", "return", "output", ",", "hidden", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.data.map_to_ids": [[9, 12], ["None"], "function", ["None"], ["from", "lexenlem", ".", "models", ".", "common", ".", "data", "import", "map_to_ids", ",", "get_long_tensor", ",", "get_float_tensor", ",", "sort_all", "\n", "from", "lexenlem", ".", "models", ".", "common", "import", "conll", "\n", "from", "lexenlem", ".", "models", ".", "lemma", ".", "vocab", "import", "Vocab", ",", "MultiVocab", ",", "FeatureVocab", "\n", "from", "lexenlem", ".", "models", ".", "lemma", "import", "edit", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.data.get_long_tensor": [[13, 24], ["isinstance", "torch.LongTensor().fill_", "enumerate", "sizes.append", "torch.LongTensor", "max", "torch.LongTensor", "len", "len"], "function", ["None"], ["from", "lexenlem", ".", "models", ".", "common", ".", "doc", "import", "Document", "\n", "from", "lexenlem", ".", "models", ".", "common", ".", "lexicon", "import", "Lexicon", ",", "ExtendedLexicon", "\n", "import", "json", "\n", "\n", "class", "DataLoaderCombined", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "input_src", ",", "batch_size", ",", "args", ",", "lemmatizer", "=", "None", ",", "vocab", "=", "None", ",", "evaluation", "=", "False", ",", "conll_only", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "        ", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "eval", "=", "evaluation", "\n", "self", ".", "shuffled", "=", "not", "self", ".", "eval", "\n", "\n", "self", ".", "lemmatizer", "=", "lemmatizer", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.data.get_float_tensor": [[25, 34], ["max", "len", "torch.FloatTensor().zero_", "enumerate", "torch.FloatTensor", "len", "torch.FloatTensor", "len"], "function", ["None"], ["self", ".", "morph", "=", "args", ".", "get", "(", "'morph'", ",", "True", ")", "\n", "self", ".", "pos", "=", "args", ".", "get", "(", "'pos'", ",", "True", ")", "\n", "print", "(", "'Using FEATS:'", ",", "self", ".", "morph", ")", "\n", "print", "(", "'Using POS:'", ",", "self", ".", "pos", ")", "\n", "\n", "# check if input source is a file or a Document object", "\n", "if", "isinstance", "(", "input_src", ",", "str", ")", ":", "\n", "            ", "filename", "=", "input_src", "\n", "assert", "filename", ".", "endswith", "(", "'conllu'", ")", ",", "\"Loaded file must be conllu file.\"", "\n", "self", ".", "conll", ",", "data", "=", "self", ".", "load_file", "(", "filename", ")", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.data.sort_all": [[35, 40], ["list", "list", "zip", "range", "len", "sorted", "zip"], "function", ["None"], ["", "elif", "isinstance", "(", "input_src", ",", "Document", ")", ":", "\n", "            ", "filename", "=", "None", "\n", "doc", "=", "input_src", "\n", "self", ".", "conll", ",", "data", "=", "self", ".", "load_doc", "(", "doc", ")", "\n", "\n", "", "if", "conll_only", ":", "# only load conll file", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.loss.MixLoss.__init__": [[21, 27], ["torch.Module.__init__", "loss.SequenceLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.loss.SequenceLoss"], ["def", "__init__", "(", "self", ",", "vocab_size", ",", "alpha", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seq_loss", "=", "SequenceLoss", "(", "vocab_size", ")", "\n", "self", ".", "ce_loss", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "assert", "alpha", ">=", "0", "\n", "self", ".", "alpha", "=", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.loss.MixLoss.forward": [[28, 33], ["loss.MixLoss.seq_loss", "loss.MixLoss.ce_loss"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "seq_inputs", ",", "seq_targets", ",", "class_inputs", ",", "class_targets", ")", ":", "\n", "        ", "sl", "=", "self", ".", "seq_loss", "(", "seq_inputs", ",", "seq_targets", ")", "\n", "cel", "=", "self", ".", "ce_loss", "(", "class_inputs", ",", "class_targets", ")", "\n", "loss", "=", "sl", "+", "self", ".", "alpha", "*", "cel", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.loss.MaxEntropySequenceLoss.__init__": [[41, 47], ["torch.Module.__init__", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.NLLLoss", "torch.NLLLoss"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__"], ["def", "__init__", "(", "self", ",", "vocab_size", ",", "alpha", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "weight", "=", "torch", ".", "ones", "(", "vocab_size", ")", "\n", "weight", "[", "constant", ".", "PAD_ID", "]", "=", "0", "\n", "self", ".", "nll", "=", "nn", ".", "NLLLoss", "(", "weight", ")", "\n", "self", ".", "alpha", "=", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.loss.MaxEntropySequenceLoss.forward": [[48, 62], ["loss.MaxEntropySequenceLoss.nll", "targets.eq().unsqueeze().expand_as", "inputs.clone().masked_fill_", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "inputs.size", "targets.size", "torch.exp.mul().sum", "torch.exp.mul().sum", "inputs.size", "targets.eq().unsqueeze", "inputs.clone", "torch.exp.mul", "torch.exp.mul", "targets.eq"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "targets", ")", ":", "\n", "        ", "\"\"\"\n        inputs: [N, C]\n        targets: [N]\n        \"\"\"", "\n", "assert", "inputs", ".", "size", "(", "0", ")", "==", "targets", ".", "size", "(", "0", ")", "\n", "nll_loss", "=", "self", ".", "nll", "(", "inputs", ",", "targets", ")", "\n", "# entropy loss", "\n", "mask", "=", "targets", ".", "eq", "(", "constant", ".", "PAD_ID", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "inputs", ")", "\n", "masked_inputs", "=", "inputs", ".", "clone", "(", ")", ".", "masked_fill_", "(", "mask", ",", "0.0", ")", "\n", "p", "=", "torch", ".", "exp", "(", "masked_inputs", ")", "\n", "ent_loss", "=", "p", ".", "mul", "(", "masked_inputs", ")", ".", "sum", "(", ")", "/", "inputs", ".", "size", "(", "0", ")", "# average over minibatch", "\n", "loss", "=", "nll_loss", "+", "self", ".", "alpha", "*", "ent_loss", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.loss.SequenceLoss": [[10, 15], ["torch.ones", "torch.ones", "torch.NLLLoss"], "function", ["None"], ["def", "SequenceLoss", "(", "vocab_size", ")", ":", "\n", "    ", "weight", "=", "torch", ".", "ones", "(", "vocab_size", ")", "\n", "weight", "[", "constant", ".", "PAD_ID", "]", "=", "0", "\n", "crit", "=", "nn", ".", "NLLLoss", "(", "weight", ")", "\n", "return", "crit", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.__init__": [[21, 90], ["torch.nn.Module.__init__", "args.get", "args.get", "print", "print", "args.get", "args.get", "args.get", "args.get", "args.get", "args.get", "args.get", "args.get", "args.get", "args.get", "args.get", "args.get", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LSTM", "torch.nn.LSTM", "lexenlem.models.common.seq2seq_modules.LSTMAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "seq2seq_model.Seq2SeqModel.init_weights", "print", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Dropout", "torch.nn.Dropout", "print", "torch.nn.ModuleList", "torch.nn.ModuleList", "seq2seq_model.Seq2SeqModel.vocab[].lens", "torch.nn.Dropout", "torch.nn.Dropout", "print", "torch.nn.Sequential", "torch.nn.Sequential", "seq2seq_model.Seq2SeqModel.SOS_tensor.cuda", "print", "seq2seq_model.Seq2SeqModel.feats_embedding.append", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Embedding", "torch.nn.Embedding"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.init_weights", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.lens", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab", ",", "emb_matrix", "=", "None", ",", "use_cuda", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab_size", "=", "args", "[", "'vocab_size'", "]", "\n", "self", ".", "emb_dim", "=", "args", "[", "'emb_dim'", "]", "\n", "self", ".", "hidden_dim", "=", "args", "[", "'hidden_dim'", "]", "\n", "self", ".", "nlayers", "=", "args", "[", "'num_layers'", "]", "# encoder layers, decoder layers = 1", "\n", "self", ".", "emb_dropout", "=", "args", ".", "get", "(", "'emb_dropout'", ",", "0.0", ")", "\n", "self", ".", "dropout", "=", "args", "[", "'dropout'", "]", "\n", "self", ".", "pad_token", "=", "constant", ".", "PAD_ID", "\n", "self", ".", "max_dec_len", "=", "args", "[", "'max_dec_len'", "]", "\n", "self", ".", "use_cuda", "=", "use_cuda", "\n", "self", ".", "top", "=", "args", ".", "get", "(", "'top'", ",", "1e10", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "emb_matrix", "=", "emb_matrix", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n", "print", "(", "\"Building an attentional Seq2Seq model...\"", ")", "\n", "print", "(", "\"Using a Bi-LSTM encoder\"", ")", "\n", "self", ".", "num_directions", "=", "2", "\n", "self", ".", "enc_hidden_dim", "=", "self", ".", "hidden_dim", "//", "2", "\n", "self", ".", "dec_hidden_dim", "=", "self", ".", "hidden_dim", "\n", "\n", "self", ".", "use_pos", "=", "args", ".", "get", "(", "'pos'", ",", "False", ")", "\n", "self", ".", "pos_first", "=", "args", ".", "get", "(", "'pos_first'", ",", "False", ")", "\n", "self", ".", "pos_dim", "=", "args", ".", "get", "(", "'pos_dim'", ",", "0", ")", "\n", "self", ".", "pos_vocab_size", "=", "args", ".", "get", "(", "'pos_vocab_size'", ",", "0", ")", "\n", "self", ".", "pos_dropout", "=", "args", ".", "get", "(", "'pos_dropout'", ",", "0", ")", "\n", "self", ".", "use_feats", "=", "args", ".", "get", "(", "'feats'", ",", "False", ")", "\n", "self", ".", "feats_first", "=", "args", ".", "get", "(", "'feats_first'", ",", "False", ")", "\n", "self", ".", "feats_vocab_size", "=", "args", ".", "get", "(", "'feats_vocab_size'", ",", "0", ")", "\n", "self", ".", "edit", "=", "args", ".", "get", "(", "'edit'", ",", "False", ")", "\n", "self", ".", "num_edit", "=", "args", ".", "get", "(", "'num_edit'", ",", "0", ")", "\n", "\n", "self", ".", "use_vabamorf", "=", "args", ".", "get", "(", "'vabamorf'", ",", "False", ")", "\n", "self", ".", "vabamorf_first", "=", "args", ".", "get", "(", "'vabamorf_first'", ",", "False", ")", "\n", "\n", "self", ".", "emb_drop", "=", "nn", ".", "Dropout", "(", "self", ".", "emb_dropout", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "self", ".", "dropout", ")", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", ",", "self", ".", "emb_dim", ",", "self", ".", "pad_token", ")", "\n", "self", ".", "encoder", "=", "nn", ".", "LSTM", "(", "self", ".", "emb_dim", ",", "self", ".", "enc_hidden_dim", ",", "self", ".", "nlayers", ",", "bidirectional", "=", "True", ",", "batch_first", "=", "True", ",", "dropout", "=", "self", ".", "dropout", "if", "self", ".", "nlayers", ">", "1", "else", "0", ")", "\n", "self", ".", "decoder", "=", "LSTMAttention", "(", "self", ".", "emb_dim", ",", "self", ".", "dec_hidden_dim", ",", "batch_first", "=", "True", ",", "attn_type", "=", "self", ".", "args", "[", "'attn_type'", "]", ")", "\n", "self", ".", "dec2vocab", "=", "nn", ".", "Linear", "(", "self", ".", "dec_hidden_dim", ",", "self", ".", "vocab_size", ")", "\n", "if", "self", ".", "use_pos", "and", "self", ".", "pos_dim", ">", "0", ":", "\n", "            ", "print", "(", "\"Using POS in encoder\"", ")", "\n", "if", "self", ".", "pos_first", ":", "\n", "                ", "print", "(", "\"Putting POS before source in the encoder\"", ")", "\n", "", "self", ".", "pos_embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "pos_vocab_size", ",", "self", ".", "pos_dim", ",", "self", ".", "pad_token", ")", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "self", ".", "pos_dropout", ")", "\n", "", "if", "self", ".", "use_feats", "and", "self", ".", "pos_dim", ">", "0", ":", "\n", "            ", "print", "(", "\"Using FEATS in encoder\"", ")", "\n", "self", ".", "feats_embedding", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "l", "in", "self", ".", "vocab", "[", "'feats'", "]", ".", "lens", "(", ")", ":", "\n", "                ", "self", ".", "feats_embedding", ".", "append", "(", "nn", ".", "Embedding", "(", "l", ",", "self", ".", "pos_dim", ",", "self", ".", "pad_token", ")", ")", "\n", "", "self", ".", "feats_drop", "=", "nn", ".", "Dropout", "(", "self", ".", "pos_dropout", ")", "\n", "", "if", "self", ".", "use_vabamorf", ":", "\n", "            ", "print", "(", "\"Using Vabamorf lemmas in the encoder\"", ")", "\n", "", "if", "self", ".", "edit", ":", "\n", "            ", "edit_hidden", "=", "self", ".", "hidden_dim", "//", "2", "\n", "self", ".", "edit_clf", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "hidden_dim", ",", "edit_hidden", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "edit_hidden", ",", "self", ".", "num_edit", ")", ")", "\n", "\n", "", "self", ".", "SOS_tensor", "=", "torch", ".", "LongTensor", "(", "[", "constant", ".", "SOS_ID", "]", ")", "\n", "self", ".", "SOS_tensor", "=", "self", ".", "SOS_tensor", ".", "cuda", "(", ")", "if", "self", ".", "use_cuda", "else", "self", ".", "SOS_tensor", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.init_weights": [[91, 117], ["isinstance", "seq2seq_model.Seq2SeqModel.embedding.weight.data.copy_", "seq2seq_model.Seq2SeqModel.embedding.weight.data.uniform_", "print", "seq2seq_model.Seq2SeqModel.pos_embedding.weight.data.uniform_", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "seq2seq_model.Seq2SeqModel.emb_matrix.size", "print", "seq2seq_model.Seq2SeqModel.embedding.weight.register_hook", "print", "fe.weight.data.uniform_", "lexenlem.models.common.utils.keep_partial_grad"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.keep_partial_grad"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "# initialize embeddings", "\n", "        ", "init_range", "=", "constant", ".", "EMB_INIT_RANGE", "\n", "if", "self", ".", "emb_matrix", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "emb_matrix", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "self", ".", "emb_matrix", "=", "torch", ".", "from_numpy", "(", "self", ".", "emb_matrix", ")", "\n", "", "assert", "self", ".", "emb_matrix", ".", "size", "(", ")", "==", "(", "self", ".", "vocab_size", ",", "self", ".", "emb_dim", ")", ",", "\"Input embedding matrix must match size: {} x {}\"", ".", "format", "(", "self", ".", "vocab_size", ",", "self", ".", "emb_dim", ")", "\n", "self", ".", "embedding", ".", "weight", ".", "data", ".", "copy_", "(", "self", ".", "emb_matrix", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "embedding", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "init_range", ",", "init_range", ")", "\n", "# decide finetuning", "\n", "", "if", "self", ".", "top", "<=", "0", ":", "\n", "            ", "print", "(", "\"Do not finetune embedding layer.\"", ")", "\n", "self", ".", "embedding", ".", "weight", ".", "requires_grad", "=", "False", "\n", "", "elif", "self", ".", "top", "<", "self", ".", "vocab_size", ":", "\n", "            ", "print", "(", "\"Finetune top {} embeddings.\"", ".", "format", "(", "self", ".", "top", ")", ")", "\n", "self", ".", "embedding", ".", "weight", ".", "register_hook", "(", "lambda", "x", ":", "utils", ".", "keep_partial_grad", "(", "x", ",", "self", ".", "top", ")", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Finetune all embeddings.\"", ")", "\n", "# initialize pos embeddings", "\n", "", "if", "self", ".", "use_pos", ":", "\n", "            ", "self", ".", "pos_embedding", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "init_range", ",", "init_range", ")", "\n", "", "if", "self", ".", "use_feats", ":", "\n", "            ", "for", "fe", "in", "self", ".", "feats_embedding", ":", "\n", "                ", "fe", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "init_range", ",", "init_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda": [[118, 121], ["super().cuda"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda"], ["", "", "", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "cuda", "(", ")", "\n", "self", ".", "use_cuda", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cpu": [[122, 125], ["super().cpu"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cpu"], ["", "def", "cpu", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "cpu", "(", ")", "\n", "self", ".", "use_cuda", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.zero_state": [[126, 133], ["inputs.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros.cuda", "torch.zeros.cuda", "torch.zeros.cuda", "torch.zeros.cuda"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda"], ["", "def", "zero_state", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "batch_size", "=", "inputs", ".", "size", "(", "0", ")", "\n", "h0", "=", "torch", ".", "zeros", "(", "self", ".", "encoder", ".", "num_layers", "*", "2", ",", "batch_size", ",", "self", ".", "enc_hidden_dim", ",", "requires_grad", "=", "False", ")", "\n", "c0", "=", "torch", ".", "zeros", "(", "self", ".", "encoder", ".", "num_layers", "*", "2", ",", "batch_size", ",", "self", ".", "enc_hidden_dim", ",", "requires_grad", "=", "False", ")", "\n", "if", "self", ".", "use_cuda", ":", "\n", "            ", "return", "h0", ".", "cuda", "(", ")", ",", "c0", ".", "cuda", "(", ")", "\n", "", "return", "h0", ",", "c0", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.encode": [[134, 148], ["seq2seq_model.Seq2SeqModel.zero_state", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "seq2seq_model.Seq2SeqModel.encoder", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.zero_state"], ["", "def", "encode", "(", "self", ",", "enc_inputs", ",", "lens", ")", ":", "\n", "        ", "\"\"\" Encode source sequence. \"\"\"", "\n", "self", ".", "h0", ",", "self", ".", "c0", "=", "self", ".", "zero_state", "(", "enc_inputs", ")", "\n", "\n", "#print('h0, c0:', self.h0.size(), self.c0.size())", "\n", "\n", "packed_inputs", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "enc_inputs", ",", "lens", ",", "batch_first", "=", "True", ")", "\n", "packed_h_in", ",", "(", "hn", ",", "cn", ")", "=", "self", ".", "encoder", "(", "packed_inputs", ",", "(", "self", ".", "h0", ",", "self", ".", "c0", ")", ")", "\n", "h_in", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed_h_in", ",", "batch_first", "=", "True", ")", "\n", "#print('hn, cn (original):', hn.size(), cn.size())", "\n", "#print('hn[-1], hn[-2]:', hn[-1].size(), hn[-2].size())", "\n", "hn", "=", "torch", ".", "cat", "(", "(", "hn", "[", "-", "1", "]", ",", "hn", "[", "-", "2", "]", ")", ",", "1", ")", "\n", "cn", "=", "torch", ".", "cat", "(", "(", "cn", "[", "-", "1", "]", ",", "cn", "[", "-", "2", "]", ")", ",", "1", ")", "\n", "return", "h_in", ",", "(", "hn", ",", "cn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.decode": [[149, 159], ["seq2seq_model.Seq2SeqModel.decoder", "h_out.contiguous().view", "seq2seq_model.Seq2SeqModel.dec2vocab", "decoder_logits.view.view.view", "seq2seq_model.Seq2SeqModel.get_log_prob", "h_out.size", "h_out.size", "h_out.contiguous", "h_out.size", "h_out.size"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.get_log_prob", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "decode", "(", "self", ",", "dec_inputs", ",", "hn", ",", "cn", ",", "ctx", ",", "ctx_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\" Decode a step, based on context encoding and source context states.\"\"\"", "\n", "dec_hidden", "=", "(", "hn", ",", "cn", ")", "\n", "h_out", ",", "dec_hidden", "=", "self", ".", "decoder", "(", "dec_inputs", ",", "dec_hidden", ",", "ctx", ",", "ctx_mask", ")", "\n", "\n", "h_out_reshape", "=", "h_out", ".", "contiguous", "(", ")", ".", "view", "(", "h_out", ".", "size", "(", "0", ")", "*", "h_out", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "decoder_logits", "=", "self", ".", "dec2vocab", "(", "h_out_reshape", ")", "\n", "decoder_logits", "=", "decoder_logits", ".", "view", "(", "h_out", ".", "size", "(", "0", ")", ",", "h_out", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "log_probs", "=", "self", ".", "get_log_prob", "(", "decoder_logits", ")", "\n", "return", "log_probs", ",", "dec_hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.forward": [[160, 200], ["seq2seq_model.Seq2SeqModel.emb_drop", "seq2seq_model.Seq2SeqModel.emb_drop", "list", "seq2seq_model.Seq2SeqModel.encode", "seq2seq_model.Seq2SeqModel.decode", "seq2seq_model.Seq2SeqModel.embedding", "seq2seq_model.Seq2SeqModel.embedding", "src_mask.data.eq().long().sum", "seq2seq_model.Seq2SeqModel.pos_drop", "range", "seq2seq_model.Seq2SeqModel.emb_drop", "seq2seq_model.Seq2SeqModel.edit_clf", "seq2seq_model.Seq2SeqModel.pos_embedding", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "seq2seq_model.Seq2SeqModel.feats_drop", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seq2seq_model.Seq2SeqModel.embedding", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "src_mask.data.eq().long", "seq2seq_model.Seq2SeqModel.unsqueeze", "seq2seq_model.Seq2SeqModel.unsqueeze", "src_mask.data.eq"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.encode", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.decode"], ["", "def", "forward", "(", "self", ",", "src", ",", "src_mask", ",", "tgt_in", ",", "pos", "=", "None", ",", "feats", "=", "None", ",", "lem", "=", "None", ",", "lem_mask", "=", "None", ")", ":", "\n", "# prepare for encoder/decoder", "\n", "        ", "enc_inputs", "=", "self", ".", "emb_drop", "(", "self", ".", "embedding", "(", "src", ")", ")", "\n", "dec_inputs", "=", "self", ".", "emb_drop", "(", "self", ".", "embedding", "(", "tgt_in", ")", ")", "\n", "src_lens", "=", "list", "(", "src_mask", ".", "data", ".", "eq", "(", "constant", ".", "PAD_ID", ")", ".", "long", "(", ")", ".", "sum", "(", "1", ")", ")", "\n", "if", "self", ".", "use_pos", ":", "\n", "# append pos to the end of src sequence", "\n", "            ", "assert", "pos", "is", "not", "None", "\n", "pos_inputs", "=", "self", ".", "pos_drop", "(", "self", ".", "pos_embedding", "(", "pos", ")", ")", "\n", "if", "self", ".", "pos_first", ":", "\n", "                ", "enc_inputs", "=", "torch", ".", "cat", "(", "[", "pos_inputs", ".", "unsqueeze", "(", "1", ")", ",", "enc_inputs", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "enc_inputs", "=", "torch", ".", "cat", "(", "[", "enc_inputs", ",", "pos_inputs", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "", "if", "self", ".", "use_feats", ":", "\n", "            ", "assert", "feats", "is", "not", "None", "\n", "feats_inputs", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "vocab", "[", "'feats'", "]", ")", ")", ":", "\n", "                ", "feats_inputs", "+=", "self", ".", "feats_drop", "(", "self", ".", "feats_embedding", "[", "i", "]", "(", "feats", "[", ":", ",", ":", ",", "i", "]", ")", ")", "\n", "", "if", "self", ".", "feats_first", ":", "\n", "                ", "enc_inputs", "=", "torch", ".", "cat", "(", "[", "feats_inputs", ",", "enc_inputs", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "enc_inputs", "=", "torch", ".", "cat", "(", "[", "enc_inputs", ",", "feats_inputs", "]", ",", "dim", "=", "1", ")", "\n", "", "", "if", "self", ".", "use_vabamorf", ":", "\n", "            ", "assert", "lem", "is", "not", "None", "\n", "assert", "lem_mask", "is", "not", "None", "\n", "vabamorf_inputs", "=", "self", ".", "emb_drop", "(", "self", ".", "embedding", "(", "lem", ")", ")", "\n", "if", "self", ".", "vabamorf_first", ":", "\n", "                ", "enc_inputs", "=", "torch", ".", "cat", "(", "[", "vabamorf_inputs", ",", "enc_inputs", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "enc_inputs", "=", "torch", ".", "cat", "(", "[", "enc_inputs", ",", "vabamorf_inputs", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "", "h_in", ",", "(", "hn", ",", "cn", ")", "=", "self", ".", "encode", "(", "enc_inputs", ",", "src_lens", ")", "\n", "\n", "if", "self", ".", "edit", ":", "\n", "            ", "edit_logits", "=", "self", ".", "edit_clf", "(", "hn", ")", "\n", "", "else", ":", "\n", "            ", "edit_logits", "=", "None", "\n", "\n", "", "log_probs", ",", "_", "=", "self", ".", "decode", "(", "dec_inputs", ",", "hn", ",", "cn", ",", "h_in", ",", "src_mask", ")", "\n", "return", "log_probs", ",", "edit_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.get_log_prob": [[201, 207], ["logits.view", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax.view", "logits.dim", "logits.size", "logits.size", "logits.size"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "get_log_prob", "(", "self", ",", "logits", ")", ":", "\n", "        ", "logits_reshape", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "vocab_size", ")", "\n", "log_probs", "=", "F", ".", "log_softmax", "(", "logits_reshape", ",", "dim", "=", "1", ")", "\n", "if", "logits", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "return", "log_probs", "\n", "", "return", "log_probs", ".", "view", "(", "logits", ".", "size", "(", "0", ")", ",", "logits", ".", "size", "(", "1", ")", ",", "logits", ".", "size", "(", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.predict": [[208, 293], ["seq2seq_model.Seq2SeqModel.embedding", "torch.cat.size", "torch.cat.size", "list", "seq2seq_model.Seq2SeqModel.encode", "range", "range", "src_mask.repeat.repeat.data.eq().long().sum", "seq2seq_model.Seq2SeqModel.pos_drop", "range", "seq2seq_model.Seq2SeqModel.emb_drop", "seq2seq_model.Seq2SeqModel.edit_clf", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "h_in.data.repeat.data.repeat.data.repeat", "src_mask.repeat.repeat.repeat", "hn.data.repeat.data.repeat.data.repeat", "cn.data.repeat.data.repeat.data.repeat", "lexenlem.models.common.beam.Beam", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "seq2seq_model.Seq2SeqModel.embedding", "seq2seq_model.Seq2SeqModel.decode", "log_probs.view().transpose().contiguous.view().transpose().contiguous.view().transpose().contiguous", "range", "beam[].sort_best", "beam[].get_hyp", "lexenlem.models.common.utils.prune_hyp", "seq2seq_model.Seq2SeqModel.pos_embedding", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "seq2seq_model.Seq2SeqModel.feats_drop", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seq2seq_model.Seq2SeqModel.embedding", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "e.size", "s.data.copy_", "beam[].advance", "seq2seq_model.Seq2SeqModel.predict.update_state"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.encode", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.decode", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.sort_best", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.get_hyp", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.prune_hyp", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.advance"], ["", "def", "predict", "(", "self", ",", "src", ",", "src_mask", ",", "pos", "=", "None", ",", "feats", "=", "None", ",", "lem", "=", "None", ",", "lem_mask", "=", "None", ",", "beam_size", "=", "5", ")", ":", "\n", "        ", "\"\"\" Predict with beam search. \"\"\"", "\n", "enc_inputs", "=", "self", ".", "embedding", "(", "src", ")", "\n", "batch_size", "=", "enc_inputs", ".", "size", "(", "0", ")", "\n", "src_lens", "=", "list", "(", "src_mask", ".", "data", ".", "eq", "(", "constant", ".", "PAD_ID", ")", ".", "long", "(", ")", ".", "sum", "(", "1", ")", ")", "\n", "if", "self", ".", "use_pos", ":", "\n", "            ", "assert", "pos", "is", "not", "None", "\n", "pos_inputs", "=", "self", ".", "pos_drop", "(", "self", ".", "pos_embedding", "(", "pos", ")", ")", "\n", "if", "self", ".", "pos_first", ":", "\n", "                ", "enc_inputs", "=", "torch", ".", "cat", "(", "[", "pos_inputs", ".", "unsqueeze", "(", "1", ")", ",", "enc_inputs", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "enc_inputs", "=", "torch", ".", "cat", "(", "[", "enc_inputs", ",", "pos_inputs", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "", "if", "self", ".", "use_feats", ":", "\n", "            ", "assert", "feats", "is", "not", "None", "\n", "feats_inputs", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "vocab", "[", "'feats'", "]", ")", ")", ":", "\n", "                ", "feats_inputs", "+=", "self", ".", "feats_drop", "(", "self", ".", "feats_embedding", "[", "i", "]", "(", "feats", "[", ":", ",", ":", ",", "i", "]", ")", ")", "\n", "", "if", "self", ".", "feats_first", ":", "\n", "                ", "enc_inputs", "=", "torch", ".", "cat", "(", "[", "feats_inputs", ",", "enc_inputs", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "enc_inputs", "=", "torch", ".", "cat", "(", "[", "enc_inputs", ",", "feats_inputs", "]", ",", "dim", "=", "1", ")", "\n", "", "", "if", "self", ".", "use_vabamorf", ":", "\n", "            ", "assert", "lem", "is", "not", "None", "\n", "assert", "lem_mask", "is", "not", "None", "\n", "vabamorf_inputs", "=", "self", ".", "emb_drop", "(", "self", ".", "embedding", "(", "lem", ")", ")", "\n", "if", "self", ".", "vabamorf_first", ":", "\n", "                ", "enc_inputs", "=", "torch", ".", "cat", "(", "[", "vabamorf_inputs", ",", "enc_inputs", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "enc_inputs", "=", "torch", ".", "cat", "(", "[", "enc_inputs", ",", "vabamorf_inputs", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# (1) encode source", "\n", "", "", "h_in", ",", "(", "hn", ",", "cn", ")", "=", "self", ".", "encode", "(", "enc_inputs", ",", "src_lens", ")", "\n", "\n", "if", "self", ".", "edit", ":", "\n", "            ", "edit_logits", "=", "self", ".", "edit_clf", "(", "hn", ")", "\n", "", "else", ":", "\n", "            ", "edit_logits", "=", "None", "\n", "\n", "# (2) set up beam", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "h_in", "=", "h_in", ".", "data", ".", "repeat", "(", "beam_size", ",", "1", ",", "1", ")", "# repeat data for beam search", "\n", "src_mask", "=", "src_mask", ".", "repeat", "(", "beam_size", ",", "1", ")", "\n", "# repeat decoder hidden states", "\n", "hn", "=", "hn", ".", "data", ".", "repeat", "(", "beam_size", ",", "1", ")", "\n", "cn", "=", "cn", ".", "data", ".", "repeat", "(", "beam_size", ",", "1", ")", "\n", "", "beam", "=", "[", "Beam", "(", "beam_size", ",", "self", ".", "use_cuda", ")", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "def", "update_state", "(", "states", ",", "idx", ",", "positions", ",", "beam_size", ")", ":", "\n", "            ", "\"\"\" Select the states according to back pointers. \"\"\"", "\n", "for", "e", "in", "states", ":", "\n", "                ", "br", ",", "d", "=", "e", ".", "size", "(", ")", "\n", "s", "=", "e", ".", "contiguous", "(", ")", ".", "view", "(", "beam_size", ",", "br", "//", "beam_size", ",", "d", ")", "[", ":", ",", "idx", "]", "\n", "s", ".", "data", ".", "copy_", "(", "s", ".", "data", ".", "index_select", "(", "0", ",", "positions", ")", ")", "\n", "\n", "# (3) main loop", "\n", "", "", "for", "i", "in", "range", "(", "self", ".", "max_dec_len", ")", ":", "\n", "            ", "dec_inputs", "=", "torch", ".", "stack", "(", "[", "b", ".", "get_current_state", "(", ")", "for", "b", "in", "beam", "]", ")", ".", "t", "(", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "dec_inputs", "=", "self", ".", "embedding", "(", "dec_inputs", ")", "\n", "log_probs", ",", "(", "hn", ",", "cn", ")", "=", "self", ".", "decode", "(", "dec_inputs", ",", "hn", ",", "cn", ",", "h_in", ",", "src_mask", ")", "\n", "log_probs", "=", "log_probs", ".", "view", "(", "beam_size", ",", "batch_size", ",", "-", "1", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "# [batch, beam, V]", "\n", "\n", "# advance each beam", "\n", "done", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "is_done", "=", "beam", "[", "b", "]", ".", "advance", "(", "log_probs", ".", "data", "[", "b", "]", ")", "\n", "if", "is_done", ":", "\n", "                    ", "done", "+=", "[", "b", "]", "\n", "# update beam state", "\n", "", "update_state", "(", "(", "hn", ",", "cn", ")", ",", "b", ",", "beam", "[", "b", "]", ".", "get_current_origin", "(", ")", ",", "beam_size", ")", "\n", "\n", "", "if", "len", "(", "done", ")", "==", "batch_size", ":", "\n", "                ", "break", "\n", "\n", "# back trace and find hypothesis", "\n", "", "", "all_hyp", ",", "all_scores", "=", "[", "]", ",", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "scores", ",", "ks", "=", "beam", "[", "b", "]", ".", "sort_best", "(", ")", "\n", "all_scores", "+=", "[", "scores", "[", "0", "]", "]", "\n", "k", "=", "ks", "[", "0", "]", "\n", "hyp", "=", "beam", "[", "b", "]", ".", "get_hyp", "(", "k", ")", "\n", "hyp", "=", "utils", ".", "prune_hyp", "(", "hyp", ")", "\n", "all_hyp", "+=", "[", "hyp", "]", "\n", "\n", "", "return", "all_hyp", ",", "edit_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.__init__": [[299, 353], ["seq2seq_model.Seq2SeqModel.__init__", "args.get", "args.get", "print", "print", "print", "print", "args.get", "args.get", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "lexenlem.models.common.seq2seq_modules.LSTMDoubleAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "seq2seq_model.Seq2SeqModelCombined.init_weights", "torch.nn.Sequential", "torch.nn.Sequential", "seq2seq_model.Seq2SeqModelCombined.SOS_tensor.cuda", "args.get", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.init_weights", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get"], ["def", "__init__", "(", "self", ",", "args", ",", "vocab", ",", "emb_matrix", "=", "None", ",", "use_cuda", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "vocab", ",", "emb_matrix", ",", "use_cuda", ")", "\n", "self", ".", "vocab_size", "=", "args", "[", "'vocab_size'", "]", "\n", "self", ".", "emb_dim", "=", "args", "[", "'emb_dim'", "]", "\n", "self", ".", "hidden_dim", "=", "args", "[", "'hidden_dim'", "]", "\n", "self", ".", "nlayers", "=", "args", "[", "'num_layers'", "]", "# encoder layers, decoder layers = 1", "\n", "self", ".", "emb_dropout", "=", "args", ".", "get", "(", "'emb_dropout'", ",", "0.0", ")", "\n", "self", ".", "dropout", "=", "args", "[", "'dropout'", "]", "\n", "self", ".", "is_lexicon", "=", "False", "if", "args", ".", "get", "(", "'lemmatizer'", ",", "None", ")", "is", "None", "else", "True", "\n", "self", ".", "lexicon_dropout", "=", "args", "[", "'lexicon_dropout'", "]", "\n", "self", ".", "pad_token", "=", "constant", ".", "PAD_ID", "\n", "self", ".", "max_dec_len", "=", "args", "[", "'max_dec_len'", "]", "\n", "self", ".", "use_cuda", "=", "use_cuda", "\n", "self", ".", "top", "=", "args", ".", "get", "(", "'top'", ",", "1e10", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "emb_matrix", "=", "emb_matrix", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "log_attn", "=", "args", "[", "'log_attn'", "]", "\n", "\n", "print", "(", "\"Building an attentional Seq2Seq model...\"", ")", "\n", "print", "(", "\"Using a Bi-LSTM encoder\"", ")", "\n", "print", "(", "\"Using a lexicon:\"", ",", "self", ".", "is_lexicon", ")", "\n", "print", "(", "\"Lexicon dropout:\"", ",", "self", ".", "lexicon_dropout", ")", "\n", "self", ".", "num_directions", "=", "2", "\n", "self", ".", "enc_hidden_dim", "=", "self", ".", "hidden_dim", "//", "2", "\n", "self", ".", "dec_hidden_dim", "=", "self", ".", "hidden_dim", "\n", "\n", "self", ".", "edit", "=", "args", ".", "get", "(", "'edit'", ",", "False", ")", "\n", "self", ".", "num_edit", "=", "args", ".", "get", "(", "'num_edit'", ",", "0", ")", "\n", "\n", "self", ".", "emb_drop", "=", "nn", ".", "Dropout", "(", "self", ".", "emb_dropout", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "self", ".", "dropout", ")", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", ",", "self", ".", "emb_dim", ",", "self", ".", "pad_token", ")", "\n", "self", ".", "encoder", "=", "nn", ".", "LSTM", "(", "self", ".", "emb_dim", ",", "self", ".", "enc_hidden_dim", ",", "self", ".", "nlayers", ",", "bidirectional", "=", "True", ",", "batch_first", "=", "True", ",", "dropout", "=", "self", ".", "dropout", "if", "self", ".", "nlayers", ">", "1", "else", "0", ")", "\n", "self", ".", "lexicon_encoder", "=", "nn", ".", "LSTM", "(", "self", ".", "emb_dim", ",", "self", ".", "enc_hidden_dim", ",", "self", ".", "nlayers", ",", "bidirectional", "=", "True", ",", "batch_first", "=", "True", ",", "dropout", "=", "self", ".", "dropout", "if", "self", ".", "nlayers", ">", "1", "else", "0", ")", "\n", "self", ".", "decoder", "=", "LSTMDoubleAttention", "(", "self", ".", "emb_dim", ",", "self", ".", "dec_hidden_dim", ",", "batch_first", "=", "True", ",", "attn_type", "=", "self", ".", "args", "[", "'attn_type'", "]", ")", "\n", "self", ".", "hn_linear", "=", "nn", ".", "Linear", "(", "self", ".", "enc_hidden_dim", "*", "4", ",", "self", ".", "enc_hidden_dim", "*", "2", ")", "\n", "self", ".", "cn_linear", "=", "nn", ".", "Linear", "(", "self", ".", "enc_hidden_dim", "*", "4", ",", "self", ".", "enc_hidden_dim", "*", "2", ")", "\n", "self", ".", "h_in_linear", "=", "nn", ".", "Linear", "(", "self", ".", "enc_hidden_dim", "*", "2", ",", "self", ".", "enc_hidden_dim", ")", "\n", "self", ".", "dec2vocab", "=", "nn", ".", "Linear", "(", "self", ".", "dec_hidden_dim", ",", "self", ".", "vocab_size", ")", "\n", "if", "self", ".", "edit", ":", "\n", "            ", "edit_hidden", "=", "self", ".", "dec_hidden_dim", "//", "2", "\n", "self", ".", "edit_clf", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "dec_hidden_dim", ",", "edit_hidden", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "edit_hidden", ",", "self", ".", "num_edit", ")", ")", "\n", "\n", "", "self", ".", "SOS_tensor", "=", "torch", ".", "LongTensor", "(", "[", "constant", ".", "SOS_ID", "]", ")", "\n", "self", ".", "SOS_tensor", "=", "self", ".", "SOS_tensor", ".", "cuda", "(", ")", "if", "self", ".", "use_cuda", "else", "self", ".", "SOS_tensor", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.init_weights": [[354, 374], ["isinstance", "seq2seq_model.Seq2SeqModelCombined.embedding.weight.data.copy_", "seq2seq_model.Seq2SeqModelCombined.embedding.weight.data.uniform_", "print", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "seq2seq_model.Seq2SeqModelCombined.emb_matrix.size", "print", "seq2seq_model.Seq2SeqModelCombined.embedding.weight.register_hook", "print", "lexenlem.models.common.utils.keep_partial_grad"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.keep_partial_grad"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "# initialize embeddings", "\n", "        ", "init_range", "=", "constant", ".", "EMB_INIT_RANGE", "\n", "if", "self", ".", "emb_matrix", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "emb_matrix", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "self", ".", "emb_matrix", "=", "torch", ".", "from_numpy", "(", "self", ".", "emb_matrix", ")", "\n", "", "assert", "self", ".", "emb_matrix", ".", "size", "(", ")", "==", "(", "self", ".", "vocab_size", ",", "self", ".", "emb_dim", ")", ",", "\"Input embedding matrix must match size: {} x {}\"", ".", "format", "(", "self", ".", "vocab_size", ",", "self", ".", "emb_dim", ")", "\n", "self", ".", "embedding", ".", "weight", ".", "data", ".", "copy_", "(", "self", ".", "emb_matrix", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "embedding", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "init_range", ",", "init_range", ")", "\n", "# decide finetuning", "\n", "", "if", "self", ".", "top", "<=", "0", ":", "\n", "            ", "print", "(", "\"Do not finetune embedding layer.\"", ")", "\n", "self", ".", "embedding", ".", "weight", ".", "requires_grad", "=", "False", "\n", "", "elif", "self", ".", "top", "<", "self", ".", "vocab_size", ":", "\n", "            ", "print", "(", "\"Finetune top {} embeddings.\"", ".", "format", "(", "self", ".", "top", ")", ")", "\n", "self", ".", "embedding", ".", "weight", ".", "register_hook", "(", "lambda", "x", ":", "utils", ".", "keep_partial_grad", "(", "x", ",", "self", ".", "top", ")", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Finetune all embeddings.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.encode": [[375, 384], ["seq2seq_model.Seq2SeqModelCombined.zero_state", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "encoder", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.zero_state"], ["", "", "def", "encode", "(", "self", ",", "encoder", ",", "enc_inputs", ",", "lens", ")", ":", "\n", "        ", "\"\"\" Encode source sequence. \"\"\"", "\n", "self", ".", "h0", ",", "self", ".", "c0", "=", "self", ".", "zero_state", "(", "enc_inputs", ")", "\n", "packed_inputs", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "enc_inputs", ",", "lens", ",", "batch_first", "=", "True", ",", "enforce_sorted", "=", "False", ")", "\n", "packed_h_in", ",", "(", "hn", ",", "cn", ")", "=", "encoder", "(", "packed_inputs", ",", "(", "self", ".", "h0", ",", "self", ".", "c0", ")", ")", "\n", "h_in", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed_h_in", ",", "batch_first", "=", "True", ")", "\n", "hn", "=", "torch", ".", "cat", "(", "(", "hn", "[", "-", "1", "]", ",", "hn", "[", "-", "2", "]", ")", ",", "1", ")", "\n", "cn", "=", "torch", ".", "cat", "(", "(", "cn", "[", "-", "1", "]", ",", "cn", "[", "-", "2", "]", ")", ",", "1", ")", "\n", "return", "h_in", ",", "(", "hn", ",", "cn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.decode": [[385, 395], ["seq2seq_model.Seq2SeqModelCombined.decoder", "h_out.contiguous().view", "seq2seq_model.Seq2SeqModelCombined.dec2vocab", "decoder_logits.view.view.view", "seq2seq_model.Seq2SeqModelCombined.get_log_prob", "h_out.size", "h_out.size", "h_out.contiguous", "h_out.size", "h_out.size"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.get_log_prob", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "decode", "(", "self", ",", "dec_inputs", ",", "hn", ",", "cn", ",", "src_ctx", ",", "lex_ctx", "=", "None", ",", "ctx_mask", "=", "None", ",", "lex_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\" Decode a step, based on context encoding and source context states.\"\"\"", "\n", "dec_hidden", "=", "(", "hn", ",", "cn", ")", "\n", "h_out", ",", "dec_hidden", ",", "attn", "=", "self", ".", "decoder", "(", "dec_inputs", ",", "dec_hidden", ",", "src_ctx", ",", "lex_ctx", ",", "ctx_mask", ",", "lex_mask", ")", "\n", "\n", "h_out_reshape", "=", "h_out", ".", "contiguous", "(", ")", ".", "view", "(", "h_out", ".", "size", "(", "0", ")", "*", "h_out", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "decoder_logits", "=", "self", ".", "dec2vocab", "(", "h_out_reshape", ")", "\n", "decoder_logits", "=", "decoder_logits", ".", "view", "(", "h_out", ".", "size", "(", "0", ")", ",", "h_out", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "log_probs", "=", "self", ".", "get_log_prob", "(", "decoder_logits", ")", "\n", "return", "log_probs", ",", "dec_hidden", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.forward": [[396, 444], ["seq2seq_model.Seq2SeqModelCombined.emb_drop", "seq2seq_model.Seq2SeqModelCombined.emb_drop", "list", "seq2seq_model.Seq2SeqModelCombined.encode", "seq2seq_model.Seq2SeqModelCombined.emb_drop", "list", "seq2seq_model.Seq2SeqModelCombined.encode", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seq2seq_model.Seq2SeqModelCombined.hn_linear", "seq2seq_model.Seq2SeqModelCombined.cn_linear", "seq2seq_model.Seq2SeqModelCombined.decode", "seq2seq_model.Seq2SeqModelCombined.embedding", "seq2seq_model.Seq2SeqModelCombined.embedding", "src_mask.data.eq().long().sum", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "lem_hide.cuda.cuda.any", "seq2seq_model.Seq2SeqModelCombined.embedding", "lem_mask.narrow.narrow.data.eq().long().sum", "lem_mask.narrow.narrow.size", "max().item", "lem_mask.narrow.narrow.narrow", "seq2seq_model.Seq2SeqModelCombined.edit_clf", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "lem_hide.cuda.cuda.cuda", "lem_stump.cuda.cuda.cuda", "lem_mask_stump.cuda.cuda.cuda", "lem_stump.cuda.cuda.repeat", "lem_mask_stump.cuda.cuda.repeat", "max().item", "src_mask.data.eq().long", "lem[].size", "lem_mask[].size", "lem_mask.narrow.narrow.data.eq().long", "max", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "max", "src_mask.data.eq", "lem.size", "lem.size", "lem.size", "lem_mask.narrow.narrow.data.eq"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.encode", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.encode", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.decode", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "forward", "(", "self", ",", "src", ",", "src_mask", ",", "tgt_in", ",", "lem", "=", "None", ",", "lem_mask", "=", "None", ")", ":", "\n", "# prepare for encoder/decoder", "\n", "        ", "enc_inputs", "=", "self", ".", "emb_drop", "(", "self", ".", "embedding", "(", "src", ")", ")", "\n", "dec_inputs", "=", "self", ".", "emb_drop", "(", "self", ".", "embedding", "(", "tgt_in", ")", ")", "\n", "src_lens", "=", "list", "(", "src_mask", ".", "data", ".", "eq", "(", "constant", ".", "PAD_ID", ")", ".", "long", "(", ")", ".", "sum", "(", "1", ")", ")", "\n", "\n", "h_in", ",", "(", "hn", ",", "cn", ")", "=", "self", ".", "encode", "(", "self", ".", "encoder", ",", "enc_inputs", ",", "src_lens", ")", "\n", "\n", "# Replace the word from the lexicon with UNK with the probability of lexicon_dropout", "\n", "if", "self", ".", "is_lexicon", "and", "self", ".", "lexicon_dropout", ">", "0", ":", "\n", "            ", "lem_stump", "=", "torch", ".", "tensor", "(", "\n", "[", "constant", ".", "SOS_ID", ",", "constant", ".", "EOS_ID", "]", "+", "\n", "[", "constant", ".", "PAD_ID", "]", "*", "(", "lem", ".", "size", "(", "1", ")", "-", "2", ")", "\n", ")", "\n", "lem_mask_stump", "=", "torch", ".", "tensor", "(", "[", "0", "]", "*", "3", "+", "[", "1", "]", "*", "(", "lem", ".", "size", "(", "1", ")", "-", "3", ")", ",", "dtype", "=", "lem_mask", ".", "dtype", ")", "\n", "lem_hide", "=", "torch", ".", "FloatTensor", "(", "lem", ".", "size", "(", "0", ")", ")", ".", "uniform_", "(", ")", "<", "self", ".", "lexicon_dropout", "\n", "if", "self", ".", "use_cuda", ":", "\n", "                ", "lem_hide", "=", "lem_hide", ".", "cuda", "(", ")", "\n", "lem_stump", "=", "lem_stump", ".", "cuda", "(", ")", "\n", "lem_mask_stump", "=", "lem_mask_stump", ".", "cuda", "(", ")", "\n", "", "if", "lem_hide", ".", "any", "(", ")", ":", "\n", "                ", "lem", "[", "lem_hide", "]", "=", "lem_stump", ".", "repeat", "(", "lem", "[", "lem_hide", "]", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "lem_mask", "[", "lem_hide", "]", "=", "lem_mask_stump", ".", "repeat", "(", "lem_mask", "[", "lem_hide", "]", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "\n", "", "", "lem_inputs", "=", "self", ".", "emb_drop", "(", "self", ".", "embedding", "(", "lem", ")", ")", "\n", "\n", "lem_lens", "=", "list", "(", "lem_mask", ".", "data", ".", "eq", "(", "constant", ".", "PAD_ID", ")", ".", "long", "(", ")", ".", "sum", "(", "1", ")", ")", "\n", "\n", "# Make the mask elements have the same size as encoder outputs", "\n", "if", "lem_mask", ".", "size", "(", "1", ")", "!=", "max", "(", "lem_lens", ")", ".", "item", "(", ")", ":", "\n", "            ", "lem_mask", "=", "lem_mask", ".", "narrow", "(", "1", ",", "0", ",", "max", "(", "lem_lens", ")", ".", "item", "(", ")", ")", "\n", "\n", "", "h_in1", ",", "(", "hn1", ",", "cn1", ")", "=", "self", ".", "encode", "(", "self", ".", "lexicon_encoder", ",", "lem_inputs", ",", "lem_lens", ")", "\n", "\n", "hn", "=", "torch", ".", "cat", "(", "(", "hn", ",", "hn1", ")", ",", "1", ")", "\n", "cn", "=", "torch", ".", "cat", "(", "(", "cn", ",", "cn1", ")", ",", "1", ")", "\n", "\n", "hn", "=", "self", ".", "hn_linear", "(", "hn", ")", "\n", "cn", "=", "self", ".", "cn_linear", "(", "cn", ")", "\n", "\n", "if", "self", ".", "edit", ":", "\n", "            ", "edit_logits", "=", "self", ".", "edit_clf", "(", "hn", ")", "\n", "", "else", ":", "\n", "            ", "edit_logits", "=", "None", "\n", "\n", "", "log_probs", ",", "_", ",", "attn", "=", "self", ".", "decode", "(", "dec_inputs", ",", "hn", ",", "cn", ",", "h_in", ",", "h_in1", ",", "src_mask", ",", "lem_mask", ")", "\n", "\n", "return", "log_probs", ",", "edit_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.predict_greedy": [[445, 506], ["seq2seq_model.Seq2SeqModelCombined.embedding", "seq2seq_model.Seq2SeqModelCombined.size", "list", "seq2seq_model.Seq2SeqModelCombined.encode", "seq2seq_model.Seq2SeqModelCombined.emb_drop", "list", "seq2seq_model.Seq2SeqModelCombined.encode", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seq2seq_model.Seq2SeqModelCombined.hn_linear", "seq2seq_model.Seq2SeqModelCombined.cn_linear", "seq2seq_model.Seq2SeqModelCombined.embedding", "seq2seq_model.Seq2SeqModelCombined.expand", "src_mask.data.eq().long().sum", "seq2seq_model.Seq2SeqModelCombined.embedding", "lem_mask.data.eq().long().sum", "seq2seq_model.Seq2SeqModelCombined.edit_clf", "seq2seq_model.Seq2SeqModelCombined.size", "seq2seq_model.Seq2SeqModelCombined.size", "seq2seq_model.Seq2SeqModelCombined.decode", "log_probs.squeeze().max", "seq2seq_model.Seq2SeqModelCombined.embedding", "attns.append", "range", "range", "range", "log_probs.size", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "src_mask.data.eq().long", "lem_mask.data.eq().long", "log_probs.squeeze", "x.tolist", "[].item", "src.tolist", "lem.tolist", "output_seqs[].append", "src_mask.data.eq", "lem_mask.data.eq"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.encode", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.encode", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.decode", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "predict_greedy", "(", "self", ",", "src", ",", "src_mask", ",", "lem", ",", "lem_mask", ",", "log_attn", ")", ":", "\n", "        ", "\"\"\" Predict with greedy decoding. \"\"\"", "\n", "enc_inputs", "=", "self", ".", "embedding", "(", "src", ")", "\n", "batch_size", "=", "enc_inputs", ".", "size", "(", "0", ")", "\n", "src_lens", "=", "list", "(", "src_mask", ".", "data", ".", "eq", "(", "constant", ".", "PAD_ID", ")", ".", "long", "(", ")", ".", "sum", "(", "1", ")", ")", "\n", "\n", "# encode source", "\n", "h_in", ",", "(", "hn", ",", "cn", ")", "=", "self", ".", "encode", "(", "self", ".", "encoder", ",", "enc_inputs", ",", "src_lens", ")", "\n", "\n", "lem_inputs", "=", "self", ".", "emb_drop", "(", "self", ".", "embedding", "(", "lem", ")", ")", "\n", "lem_lens", "=", "list", "(", "lem_mask", ".", "data", ".", "eq", "(", "constant", ".", "PAD_ID", ")", ".", "long", "(", ")", ".", "sum", "(", "1", ")", ")", "\n", "h_in1", ",", "(", "hn1", ",", "cn1", ")", "=", "self", ".", "encode", "(", "self", ".", "lexicon_encoder", ",", "lem_inputs", ",", "lem_lens", ")", "\n", "\n", "hn", "=", "torch", ".", "cat", "(", "(", "hn", ",", "hn1", ")", ",", "1", ")", "\n", "cn", "=", "torch", ".", "cat", "(", "(", "cn", ",", "cn1", ")", ",", "1", ")", "\n", "\n", "hn", "=", "self", ".", "hn_linear", "(", "hn", ")", "\n", "cn", "=", "self", ".", "cn_linear", "(", "cn", ")", "\n", "\n", "if", "self", ".", "edit", ":", "\n", "            ", "edit_logits", "=", "self", ".", "edit_clf", "(", "hn", ")", "\n", "", "else", ":", "\n", "            ", "edit_logits", "=", "None", "\n", "\n", "# greedy decode by step", "\n", "", "dec_inputs", "=", "self", ".", "embedding", "(", "self", ".", "SOS_tensor", ")", "\n", "dec_inputs", "=", "dec_inputs", ".", "expand", "(", "batch_size", ",", "dec_inputs", ".", "size", "(", "0", ")", ",", "dec_inputs", ".", "size", "(", "1", ")", ")", "\n", "\n", "done", "=", "[", "False", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "total_done", "=", "0", "\n", "max_len", "=", "0", "\n", "output_seqs", "=", "[", "[", "]", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "attns", "=", "[", "]", "\n", "while", "total_done", "<", "batch_size", "and", "max_len", "<", "self", ".", "max_dec_len", ":", "\n", "            ", "log_probs", ",", "(", "hn", ",", "cn", ")", ",", "attn", "=", "self", ".", "decode", "(", "dec_inputs", ",", "hn", ",", "cn", ",", "h_in", ",", "h_in1", ",", "src_mask", ",", "lem_mask", ")", "\n", "assert", "log_probs", ".", "size", "(", "1", ")", "==", "1", ",", "\"Output must have 1-step of output.\"", "\n", "_", ",", "preds", "=", "log_probs", ".", "squeeze", "(", "1", ")", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "dec_inputs", "=", "self", ".", "embedding", "(", "preds", ")", "# update decoder inputs", "\n", "max_len", "+=", "1", "\n", "attns", ".", "append", "(", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "attn", "]", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "if", "not", "done", "[", "i", "]", ":", "\n", "                    ", "token", "=", "preds", ".", "data", "[", "i", "]", "[", "0", "]", ".", "item", "(", ")", "\n", "if", "token", "==", "constant", ".", "EOS_ID", ":", "\n", "                        ", "done", "[", "i", "]", "=", "True", "\n", "total_done", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "output_seqs", "[", "i", "]", ".", "append", "(", "token", ")", "\n", "\n", "", "", "", "", "if", "log_attn", ":", "\n", "            ", "log_attns", "=", "{", "\n", "'src'", ":", "np", ".", "array", "(", "src", ".", "tolist", "(", ")", ")", ",", "\n", "'lem'", ":", "np", ".", "array", "(", "lem", ".", "tolist", "(", ")", ")", ",", "\n", "'attns'", ":", "np", ".", "array", "(", "attns", ")", ",", "\n", "'all_hyp'", ":", "np", ".", "array", "(", "output_seqs", ")", "\n", "}", "\n", "", "else", ":", "\n", "            ", "log_attns", "=", "None", "\n", "\n", "", "return", "output_seqs", ",", "edit_logits", ",", "log_attns", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.predict": [[507, 596], ["seq2seq_model.Seq2SeqModelCombined.embedding", "seq2seq_model.Seq2SeqModelCombined.size", "list", "seq2seq_model.Seq2SeqModelCombined.encode", "seq2seq_model.Seq2SeqModelCombined.emb_drop", "list", "seq2seq_model.Seq2SeqModelCombined.encode", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seq2seq_model.Seq2SeqModelCombined.hn_linear", "seq2seq_model.Seq2SeqModelCombined.cn_linear", "range", "range", "seq2seq_model.Seq2SeqModelCombined.predict_greedy", "src_mask.repeat.repeat.data.eq().long().sum", "seq2seq_model.Seq2SeqModelCombined.embedding", "lem_mask.repeat.repeat.data.eq().long().sum", "seq2seq_model.Seq2SeqModelCombined.edit_clf", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "h_in.data.repeat.data.repeat.data.repeat", "src_mask.repeat.repeat.repeat", "h_in1.data.repeat.data.repeat.data.repeat", "lem_mask.repeat.repeat.repeat", "hn.data.repeat.data.repeat.data.repeat", "cn.data.repeat.data.repeat.data.repeat", "lexenlem.models.common.beam.Beam", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "seq2seq_model.Seq2SeqModelCombined.embedding", "seq2seq_model.Seq2SeqModelCombined.decode", "log_probs.view().transpose().contiguous.view().transpose().contiguous.view().transpose().contiguous", "attns.append", "range", "beam[].sort_best", "beam[].get_hyp", "lexenlem.models.common.utils.prune_hyp", "print", "json.dump", "range", "e.size", "s.data.copy_", "beam[].advance", "seq2seq_model.Seq2SeqModelCombined.predict.update_state"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.encode", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.encode", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.predict_greedy", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModelCombined.decode", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.sort_best", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.get_hyp", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.prune_hyp", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.beam.Beam.advance"], ["", "def", "predict", "(", "self", ",", "src", ",", "src_mask", ",", "lem", "=", "None", ",", "lem_mask", "=", "None", ",", "beam_size", "=", "5", ",", "log_attn", "=", "False", ")", ":", "\n", "        ", "\"\"\" Predict with beam search. \"\"\"", "\n", "if", "beam_size", "==", "1", ":", "\n", "            ", "return", "self", ".", "predict_greedy", "(", "src", ",", "src_mask", ",", "lem", ",", "lem_mask", ",", "log_attn", ")", "\n", "\n", "", "enc_inputs", "=", "self", ".", "embedding", "(", "src", ")", "\n", "batch_size", "=", "enc_inputs", ".", "size", "(", "0", ")", "\n", "src_lens", "=", "list", "(", "src_mask", ".", "data", ".", "eq", "(", "constant", ".", "PAD_ID", ")", ".", "long", "(", ")", ".", "sum", "(", "1", ")", ")", "\n", "\n", "# (1) encode source", "\n", "h_in", ",", "(", "hn", ",", "cn", ")", "=", "self", ".", "encode", "(", "self", ".", "encoder", ",", "enc_inputs", ",", "src_lens", ")", "\n", "\n", "lem_inputs", "=", "self", ".", "emb_drop", "(", "self", ".", "embedding", "(", "lem", ")", ")", "\n", "lem_lens", "=", "list", "(", "lem_mask", ".", "data", ".", "eq", "(", "constant", ".", "PAD_ID", ")", ".", "long", "(", ")", ".", "sum", "(", "1", ")", ")", "\n", "h_in1", ",", "(", "hn1", ",", "cn1", ")", "=", "self", ".", "encode", "(", "self", ".", "lexicon_encoder", ",", "lem_inputs", ",", "lem_lens", ")", "\n", "\n", "hn", "=", "torch", ".", "cat", "(", "(", "hn", ",", "hn1", ")", ",", "1", ")", "\n", "cn", "=", "torch", ".", "cat", "(", "(", "cn", ",", "cn1", ")", ",", "1", ")", "\n", "\n", "hn", "=", "self", ".", "hn_linear", "(", "hn", ")", "\n", "cn", "=", "self", ".", "cn_linear", "(", "cn", ")", "\n", "\n", "if", "self", ".", "edit", ":", "\n", "            ", "edit_logits", "=", "self", ".", "edit_clf", "(", "hn", ")", "\n", "", "else", ":", "\n", "            ", "edit_logits", "=", "None", "\n", "\n", "# (2) set up beam", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "h_in", "=", "h_in", ".", "data", ".", "repeat", "(", "beam_size", ",", "1", ",", "1", ")", "# repeat data for beam search", "\n", "src_mask", "=", "src_mask", ".", "repeat", "(", "beam_size", ",", "1", ")", "\n", "h_in1", "=", "h_in1", ".", "data", ".", "repeat", "(", "beam_size", ",", "1", ",", "1", ")", "# repeat data for beam search", "\n", "lem_mask", "=", "lem_mask", ".", "repeat", "(", "beam_size", ",", "1", ")", "\n", "# repeat decoder hidden states", "\n", "hn", "=", "hn", ".", "data", ".", "repeat", "(", "beam_size", ",", "1", ")", "\n", "cn", "=", "cn", ".", "data", ".", "repeat", "(", "beam_size", ",", "1", ")", "\n", "", "beam", "=", "[", "Beam", "(", "beam_size", ",", "self", ".", "use_cuda", ")", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "def", "update_state", "(", "states", ",", "idx", ",", "positions", ",", "beam_size", ")", ":", "\n", "            ", "\"\"\" Select the states according to back pointers. \"\"\"", "\n", "for", "e", "in", "states", ":", "\n", "                ", "br", ",", "d", "=", "e", ".", "size", "(", ")", "\n", "s", "=", "e", ".", "contiguous", "(", ")", ".", "view", "(", "beam_size", ",", "br", "//", "beam_size", ",", "d", ")", "[", ":", ",", "idx", "]", "\n", "s", ".", "data", ".", "copy_", "(", "s", ".", "data", ".", "index_select", "(", "0", ",", "positions", ")", ")", "\n", "\n", "", "", "attns", "=", "[", "]", "\n", "# (3) main loop", "\n", "for", "i", "in", "range", "(", "self", ".", "max_dec_len", ")", ":", "\n", "            ", "dec_inputs", "=", "torch", ".", "stack", "(", "[", "b", ".", "get_current_state", "(", ")", "for", "b", "in", "beam", "]", ")", ".", "t", "(", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "dec_inputs", "=", "self", ".", "embedding", "(", "dec_inputs", ")", "\n", "log_probs", ",", "(", "hn", ",", "cn", ")", ",", "attn", "=", "self", ".", "decode", "(", "dec_inputs", ",", "hn", ",", "cn", ",", "h_in", ",", "h_in1", ",", "src_mask", ",", "lem_mask", ")", "\n", "log_probs", "=", "log_probs", ".", "view", "(", "beam_size", ",", "batch_size", ",", "-", "1", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "# [batch, beam, V]", "\n", "\n", "attns", ".", "append", "(", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "attn", "]", ")", "\n", "# advance each beam", "\n", "done", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "is_done", "=", "beam", "[", "b", "]", ".", "advance", "(", "log_probs", ".", "data", "[", "b", "]", ")", "\n", "if", "is_done", ":", "\n", "                    ", "done", "+=", "[", "b", "]", "\n", "# update beam state", "\n", "", "update_state", "(", "(", "hn", ",", "cn", ")", ",", "b", ",", "beam", "[", "b", "]", ".", "get_current_origin", "(", ")", ",", "beam_size", ")", "\n", "\n", "", "if", "len", "(", "done", ")", "==", "batch_size", ":", "\n", "                ", "break", "\n", "\n", "# back trace and find hypothesis", "\n", "", "", "all_hyp", ",", "all_scores", "=", "[", "]", ",", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "scores", ",", "ks", "=", "beam", "[", "b", "]", ".", "sort_best", "(", ")", "\n", "all_scores", "+=", "[", "scores", "[", "0", "]", "]", "\n", "k", "=", "ks", "[", "0", "]", "\n", "hyp", "=", "beam", "[", "b", "]", ".", "get_hyp", "(", "k", ")", "\n", "hyp", "=", "utils", ".", "prune_hyp", "(", "hyp", ")", "\n", "hyp", "=", "[", "i", ".", "item", "(", ")", "for", "i", "in", "hyp", "]", "\n", "all_hyp", "+=", "[", "hyp", "]", "\n", "\n", "", "if", "log_attn", ":", "\n", "            ", "print", "(", "'[Logging attention scores...]'", ")", "\n", "log_attn", "=", "{", "\n", "'src'", ":", "src", ".", "tolist", "(", ")", ",", "\n", "'lem'", ":", "lem", ".", "tolist", "(", ")", ",", "\n", "'attns'", ":", "attns", ",", "\n", "'all_hyp'", ":", "[", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "hyp", "]", "for", "hyp", "in", "all_hyp", "]", "\n", "}", "\n", "json", ".", "dump", "(", "log_attn", ",", "open", "(", "'log_attn.json'", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", ")", "\n", "\n", "", "return", "all_hyp", ",", "edit_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Document.__init__": [[15, 19], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "text", ")", ":", "\n", "        ", "self", ".", "_text", "=", "text", "\n", "self", ".", "_conll_file", "=", "None", "\n", "self", ".", "_sentences", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Document.conll_file": [[25, 29], ["None"], "methods", ["None"], ["", "@", "conll_file", ".", "setter", "\n", "def", "conll_file", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the document's CoNLLFile value. \"\"\"", "\n", "self", ".", "_conll_file", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Document.text": [[35, 39], ["None"], "methods", ["None"], ["", "@", "text", ".", "setter", "\n", "def", "text", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the document's text value. Example: 'This is a sentence.'\"\"\"", "\n", "self", ".", "_text", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Document.sentences": [[45, 49], ["None"], "methods", ["None"], ["", "@", "sentences", ".", "setter", "\n", "def", "sentences", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the list of tokens for this document. \"\"\"", "\n", "self", ".", "_sentences", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Document.load_annotations": [[50, 53], ["doc.Sentence"], "methods", ["None"], ["", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\" Integrate info from the CoNLLFile instance. \"\"\"", "\n", "self", ".", "_sentences", "=", "[", "Sentence", "(", "token_list", ")", "for", "token_list", "in", "self", ".", "conll_file", ".", "sents", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Document.write_conll_to_file": [[54, 57], ["doc.Document.conll_file.write_conll"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.write_conll"], ["", "def", "write_conll_to_file", "(", "self", ",", "file_path", ")", ":", "\n", "        ", "\"\"\" Write conll contents to file. \"\"\"", "\n", "self", ".", "conll_file", ".", "write_conll", "(", "file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.__init__": [[60, 68], ["doc.Sentence._process_tokens", "doc.Sentence.build_dependencies"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence._process_tokens", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.build_dependencies"], ["    ", "def", "__init__", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "self", ".", "_tokens", "=", "[", "]", "\n", "self", ".", "_words", "=", "[", "]", "\n", "self", ".", "_process_tokens", "(", "tokens", ")", "\n", "self", ".", "_dependencies", "=", "[", "]", "\n", "# check if there is dependency info", "\n", "if", "self", ".", "words", "[", "0", "]", ".", "dependency_relation", "is", "not", "None", ":", "\n", "            ", "self", ".", "build_dependencies", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence._process_tokens": [[69, 85], ["multi_word_token_line.match", "doc.Sentence._tokens.append", "doc.Word", "doc.Sentence._words.append", "int", "int", "int", "doc.Token", "doc.Sentence._tokens[].words.append", "doc.Sentence.tokens.append", "multi_word_token_line.match.group", "multi_word_token_line.match.group", "doc.Token"], "methods", ["None"], ["", "", "def", "_process_tokens", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "st", ",", "en", "=", "-", "1", ",", "-", "1", "\n", "for", "tok", "in", "tokens", ":", "\n", "            ", "m", "=", "multi_word_token_line", ".", "match", "(", "tok", "[", "CONLLU_FIELD_TO_IDX", "[", "'id'", "]", "]", ")", "\n", "if", "m", ":", "\n", "                ", "st", ",", "en", "=", "int", "(", "m", ".", "group", "(", "1", ")", ")", ",", "int", "(", "m", ".", "group", "(", "2", ")", ")", "\n", "self", ".", "_tokens", ".", "append", "(", "Token", "(", "tok", ")", ")", "\n", "", "else", ":", "\n", "                ", "new_word", "=", "Word", "(", "tok", ")", "\n", "self", ".", "_words", ".", "append", "(", "new_word", ")", "\n", "idx", "=", "int", "(", "tok", "[", "CONLLU_FIELD_TO_IDX", "[", "'id'", "]", "]", ")", "\n", "if", "idx", "<=", "en", ":", "\n", "                    ", "self", ".", "_tokens", "[", "-", "1", "]", ".", "words", ".", "append", "(", "new_word", ")", "\n", "new_word", ".", "parent_token", "=", "self", ".", "_tokens", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                    ", "self", ".", "tokens", ".", "append", "(", "Token", "(", "tok", ",", "words", "=", "[", "new_word", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.dependencies": [[91, 95], ["None"], "methods", ["None"], ["", "@", "dependencies", ".", "setter", "\n", "def", "dependencies", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the list of dependencies for this sentence. \"\"\"", "\n", "self", ".", "_dependencies", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.tokens": [[101, 105], ["None"], "methods", ["None"], ["", "@", "tokens", ".", "setter", "\n", "def", "tokens", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the list of tokens for this sentence. \"\"\"", "\n", "self", ".", "_tokens", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.words": [[111, 115], ["None"], "methods", ["None"], ["", "@", "words", ".", "setter", "\n", "def", "words", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the list of words for this sentence. \"\"\"", "\n", "self", ".", "_words", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.build_dependencies": [[116, 125], ["doc.Sentence.dependencies.append", "doc.Word"], "methods", ["None"], ["", "def", "build_dependencies", "(", "self", ")", ":", "\n", "        ", "for", "word", "in", "self", ".", "words", ":", "\n", "            ", "if", "word", ".", "governor", "==", "0", ":", "\n", "# make a word for the ROOT", "\n", "                ", "governor", "=", "Word", "(", "[", "\"0\"", ",", "\"ROOT\"", ",", "\"_\"", ",", "\"_\"", ",", "\"_\"", ",", "\"_\"", ",", "\"-1\"", ",", "\"_\"", ",", "\"_\"", ",", "\"_\"", ",", "\"_\"", ",", "\"_\"", "]", ")", "\n", "", "else", ":", "\n", "# id is index in words list + 1", "\n", "                ", "governor", "=", "self", ".", "words", "[", "word", ".", "governor", "-", "1", "]", "\n", "", "self", ".", "dependencies", ".", "append", "(", "(", "governor", ",", "word", ".", "dependency_relation", ",", "word", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.print_dependencies": [[126, 129], ["print"], "methods", ["None"], ["", "", "def", "print_dependencies", "(", "self", ",", "file", "=", "None", ")", ":", "\n", "        ", "for", "dep_edge", "in", "self", ".", "dependencies", ":", "\n", "            ", "print", "(", "(", "dep_edge", "[", "2", "]", ".", "text", ",", "dep_edge", "[", "0", "]", ".", "index", ",", "dep_edge", "[", "1", "]", ")", ",", "file", "=", "file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.dependencies_string": [[130, 134], ["io.StringIO", "doc.Sentence.print_dependencies", "io.StringIO.getvalue().strip", "io.StringIO.getvalue"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.print_dependencies"], ["", "", "def", "dependencies_string", "(", "self", ")", ":", "\n", "        ", "dep_string", "=", "io", ".", "StringIO", "(", ")", "\n", "self", ".", "print_dependencies", "(", "file", "=", "dep_string", ")", "\n", "return", "dep_string", ".", "getvalue", "(", ")", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.print_tokens": [[135, 138], ["print"], "methods", ["None"], ["", "def", "print_tokens", "(", "self", ",", "file", "=", "None", ")", ":", "\n", "        ", "for", "tok", "in", "self", ".", "tokens", ":", "\n", "            ", "print", "(", "tok", ",", "file", "=", "file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.tokens_string": [[139, 143], ["io.StringIO", "doc.Sentence.print_tokens", "io.StringIO.getvalue().strip", "io.StringIO.getvalue"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.print_tokens"], ["", "", "def", "tokens_string", "(", "self", ")", ":", "\n", "        ", "toks_string", "=", "io", ".", "StringIO", "(", ")", "\n", "self", ".", "print_tokens", "(", "file", "=", "toks_string", ")", "\n", "return", "toks_string", ".", "getvalue", "(", ")", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.print_words": [[144, 147], ["print"], "methods", ["None"], ["", "def", "print_words", "(", "self", ",", "file", "=", "None", ")", ":", "\n", "        ", "for", "word", "in", "self", ".", "words", ":", "\n", "            ", "print", "(", "word", ",", "file", "=", "file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.words_string": [[148, 152], ["io.StringIO", "doc.Sentence.print_words", "io.StringIO.getvalue().strip", "io.StringIO.getvalue"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Sentence.print_words"], ["", "", "def", "words_string", "(", "self", ")", ":", "\n", "        ", "wrds_string", "=", "io", ".", "StringIO", "(", ")", "\n", "self", ".", "print_words", "(", "file", "=", "wrds_string", ")", "\n", "return", "wrds_string", ".", "getvalue", "(", ")", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Token.__init__": [[156, 163], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "token_entry", ",", "words", "=", "None", ")", ":", "\n", "        ", "self", ".", "_index", "=", "token_entry", "[", "CONLLU_FIELD_TO_IDX", "[", "'id'", "]", "]", "\n", "self", ".", "_text", "=", "token_entry", "[", "CONLLU_FIELD_TO_IDX", "[", "'word'", "]", "]", "\n", "if", "words", "is", "None", ":", "\n", "            ", "self", ".", "words", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "words", "=", "words", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Token.words": [[169, 175], ["None"], "methods", ["None"], ["", "@", "words", ".", "setter", "\n", "def", "words", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set this token's list of underlying syntactic words. \"\"\"", "\n", "self", ".", "_words", "=", "value", "\n", "for", "w", "in", "self", ".", "_words", ":", "\n", "            ", "w", ".", "parent_token", "=", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Token.index": [[181, 185], ["None"], "methods", ["None"], ["", "@", "index", ".", "setter", "\n", "def", "index", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the token's index value. \"\"\"", "\n", "self", ".", "_index", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Token.text": [[191, 195], ["None"], "methods", ["None"], ["", "@", "text", ".", "setter", "\n", "def", "text", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the token's text value. Example: 'The'\"\"\"", "\n", "self", ".", "_text", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Token.__repr__": [[196, 198], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "f\"<{self.__class__.__name__} index={self.index};words={self.words}>\"", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.__init__": [[201, 223], ["int"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "word_entry", ")", ":", "\n", "        ", "self", ".", "_index", "=", "word_entry", "[", "CONLLU_FIELD_TO_IDX", "[", "'id'", "]", "]", "\n", "self", ".", "_text", "=", "word_entry", "[", "CONLLU_FIELD_TO_IDX", "[", "'word'", "]", "]", "\n", "self", ".", "_lemma", "=", "word_entry", "[", "CONLLU_FIELD_TO_IDX", "[", "'lemma'", "]", "]", "\n", "if", "self", ".", "_lemma", "==", "'_'", ":", "\n", "            ", "self", ".", "_lemma", "=", "None", "\n", "", "self", ".", "_upos", "=", "word_entry", "[", "CONLLU_FIELD_TO_IDX", "[", "'upos'", "]", "]", "\n", "self", ".", "_xpos", "=", "word_entry", "[", "CONLLU_FIELD_TO_IDX", "[", "'xpos'", "]", "]", "\n", "self", ".", "_feats", "=", "word_entry", "[", "CONLLU_FIELD_TO_IDX", "[", "'feats'", "]", "]", "\n", "if", "self", ".", "_upos", "==", "'_'", ":", "\n", "            ", "self", ".", "_upos", "=", "None", "\n", "self", ".", "_xpos", "=", "None", "\n", "self", ".", "_feats", "=", "None", "\n", "", "self", ".", "_governor", "=", "word_entry", "[", "CONLLU_FIELD_TO_IDX", "[", "'head'", "]", "]", "\n", "self", ".", "_dependency_relation", "=", "word_entry", "[", "CONLLU_FIELD_TO_IDX", "[", "'deprel'", "]", "]", "\n", "self", ".", "_parent_token", "=", "None", "\n", "# check if there is dependency information", "\n", "if", "self", ".", "_dependency_relation", "!=", "'_'", ":", "\n", "            ", "self", ".", "_governor", "=", "int", "(", "self", ".", "_governor", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_governor", "=", "None", "\n", "self", ".", "_dependency_relation", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.dependency_relation": [[229, 233], ["None"], "methods", ["None"], ["", "@", "dependency_relation", ".", "setter", "\n", "def", "dependency_relation", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the word's dependency relation value. Example: 'nmod'\"\"\"", "\n", "self", ".", "_dependency_relation", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.lemma": [[239, 243], ["None"], "methods", ["None"], ["", "@", "lemma", ".", "setter", "\n", "def", "lemma", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the word's lemma value. \"\"\"", "\n", "self", ".", "_lemma", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.governor": [[249, 253], ["None"], "methods", ["None"], ["", "@", "governor", ".", "setter", "\n", "def", "governor", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the word's governor value. \"\"\"", "\n", "self", ".", "_governor", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.pos": [[259, 263], ["None"], "methods", ["None"], ["", "@", "pos", ".", "setter", "\n", "def", "pos", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the word's (treebank-specific) part-of-speech value. Example: 'NNP'\"\"\"", "\n", "self", ".", "_xpos", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.text": [[269, 273], ["None"], "methods", ["None"], ["", "@", "text", ".", "setter", "\n", "def", "text", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the word's text value. Example: 'The'\"\"\"", "\n", "self", ".", "_text", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.xpos": [[279, 283], ["None"], "methods", ["None"], ["", "@", "xpos", ".", "setter", "\n", "def", "xpos", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the word's treebank-specific part-of-speech value. Example: 'NNP'\"\"\"", "\n", "self", ".", "_xpos", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.upos": [[289, 293], ["None"], "methods", ["None"], ["", "@", "upos", ".", "setter", "\n", "def", "upos", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the word's universal part-of-speech value. Example: 'DET'\"\"\"", "\n", "self", ".", "_upos", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.feats": [[299, 303], ["None"], "methods", ["None"], ["", "@", "feats", ".", "setter", "\n", "def", "feats", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set this word's morphological features. Example: 'Gender=Fem'\"\"\"", "\n", "self", ".", "_feats", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.parent_token": [[309, 313], ["None"], "methods", ["None"], ["", "@", "parent_token", ".", "setter", "\n", "def", "parent_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set this word's parent token. \"\"\"", "\n", "self", ".", "_parent_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.index": [[319, 323], ["None"], "methods", ["None"], ["", "@", "index", ".", "setter", "\n", "def", "index", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\" Set the word's index value. \"\"\"", "\n", "self", ".", "_index", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.__repr__": [[324, 329], ["getattr", "getattr"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "features", "=", "[", "'index'", ",", "'text'", ",", "'lemma'", ",", "'upos'", ",", "'xpos'", ",", "'feats'", ",", "'governor'", ",", "'dependency_relation'", "]", "\n", "feature_str", "=", "\";\"", ".", "join", "(", "[", "\"{}={}\"", ".", "format", "(", "k", ",", "getattr", "(", "self", ",", "k", ")", ")", "for", "k", "in", "features", "if", "getattr", "(", "self", ",", "k", ")", "is", "not", "None", "]", ")", "\n", "\n", "return", "f\"<{self.__class__.__name__} {feature_str}>\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.get_wordvec_file": [[16, 25], ["shorthand.split", "os.path.join"], "function", ["None"], ["def", "get_wordvec_file", "(", "wordvec_dir", ",", "shorthand", ")", ":", "\n", "    ", "\"\"\" Lookup the name of the word vectors file, given a directory and the language shorthand.\n    \"\"\"", "\n", "lcode", ",", "tcode", "=", "shorthand", ".", "split", "(", "'_'", ")", "\n", "lang", "=", "lcode2lang", "[", "lcode", "]", "if", "lcode", "!=", "'no'", "else", "lcode2lang", "[", "shorthand", "]", "\n", "if", "lcode", "==", "'zh'", ":", "\n", "        ", "lang", "=", "'ChineseT'", "\n", "", "return", "os", ".", "path", ".", "join", "(", "wordvec_dir", ",", "lang", ",", "'{}.vectors.xz'", ".", "format", "(", "lcode", "if", "lcode", "!=", "'no'", "else", "(", "shorthand", "if", "shorthand", "!=", "'no_nynorsklia'", "else", "'no_nynorsk'", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.get_adaptive_eval_interval": [[27, 37], ["round"], "function", ["None"], ["", "def", "get_adaptive_eval_interval", "(", "cur_dev_size", ",", "thres_dev_size", ",", "base_interval", ")", ":", "\n", "    ", "\"\"\" Adjust the evaluation interval adaptively.\n    If cur_dev_size <= thres_dev_size, return base_interval;\n    else, linearly increase the interval (round to integer times of base interval).\n    \"\"\"", "\n", "if", "cur_dev_size", "<=", "thres_dev_size", ":", "\n", "        ", "return", "base_interval", "\n", "", "else", ":", "\n", "        ", "alpha", "=", "round", "(", "cur_dev_size", "/", "thres_dev_size", ")", "\n", "return", "base_interval", "*", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.ud_scores": [[39, 45], ["lexenlem.load_conllu_file", "lexenlem.load_conllu_file", "lexenlem.evaluate"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.load_conllu_file", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.utils.conll18_ud_eval.load_conllu_file", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.models.lemmatizer_cmb.evaluate"], ["", "", "def", "ud_scores", "(", "gold_conllu_file", ",", "system_conllu_file", ")", ":", "\n", "    ", "gold_ud", "=", "ud_eval", ".", "load_conllu_file", "(", "gold_conllu_file", ")", "\n", "system_ud", "=", "ud_eval", ".", "load_conllu_file", "(", "system_conllu_file", ")", "\n", "evaluation", "=", "ud_eval", ".", "evaluate", "(", "gold_ud", ",", "system_ud", ")", "\n", "\n", "return", "evaluation", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.harmonic_mean": [[46, 55], ["any", "len", "len", "len", "len", "len", "sum", "sum", "sum", "zip"], "function", ["None"], ["", "def", "harmonic_mean", "(", "a", ",", "weights", "=", "None", ")", ":", "\n", "    ", "if", "any", "(", "[", "x", "==", "0", "for", "x", "in", "a", "]", ")", ":", "\n", "        ", "return", "0", "\n", "", "else", ":", "\n", "        ", "assert", "weights", "is", "None", "or", "len", "(", "weights", ")", "==", "len", "(", "a", ")", ",", "'Weights has length {} which is different from that of the array ({}).'", ".", "format", "(", "len", "(", "weights", ")", ",", "len", "(", "a", ")", ")", "\n", "if", "weights", "is", "None", ":", "\n", "            ", "return", "len", "(", "a", ")", "/", "sum", "(", "[", "1", "/", "x", "for", "x", "in", "a", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "sum", "(", "weights", ")", "/", "sum", "(", "w", "/", "x", "for", "x", ",", "w", "in", "zip", "(", "a", ",", "weights", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.get_optimizer": [[57, 68], ["torch.optim.SGD", "torch.optim.Adagrad", "torch.optim.Adam", "torch.optim.Adamax", "Exception"], "function", ["None"], ["", "", "", "def", "get_optimizer", "(", "name", ",", "parameters", ",", "lr", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ")", ":", "\n", "    ", "if", "name", "==", "'sgd'", ":", "\n", "        ", "return", "torch", ".", "optim", ".", "SGD", "(", "parameters", ",", "lr", "=", "lr", ")", "\n", "", "elif", "name", "==", "'adagrad'", ":", "\n", "        ", "return", "torch", ".", "optim", ".", "Adagrad", "(", "parameters", ",", "lr", "=", "lr", ")", "\n", "", "elif", "name", "==", "'adam'", ":", "\n", "        ", "return", "torch", ".", "optim", ".", "Adam", "(", "parameters", ",", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ")", "\n", "", "elif", "name", "==", "'adamax'", ":", "\n", "        ", "return", "torch", ".", "optim", ".", "Adamax", "(", "parameters", ")", "# use default lr", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "\"Unsupported optimizer: {}\"", ".", "format", "(", "name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.change_lr": [[69, 72], ["None"], "function", ["None"], ["", "", "def", "change_lr", "(", "optimizer", ",", "new_lr", ")", ":", "\n", "    ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "param_group", "[", "'lr'", "]", "=", "new_lr", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.flatten_indices": [[73, 79], ["enumerate", "range", "flat.append"], "function", ["None"], ["", "", "def", "flatten_indices", "(", "seq_lens", ",", "width", ")", ":", "\n", "    ", "flat", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "seq_lens", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "l", ")", ":", "\n", "            ", "flat", ".", "append", "(", "i", "*", "width", "+", "j", ")", "\n", "", "", "return", "flat", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.set_cuda": [[80, 84], ["var.cuda"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.seq2seq_model.Seq2SeqModel.cuda"], ["", "def", "set_cuda", "(", "var", ",", "cuda", ")", ":", "\n", "    ", "if", "cuda", ":", "\n", "        ", "return", "var", ".", "cuda", "(", ")", "\n", "", "return", "var", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.keep_partial_grad": [[85, 92], ["grad.data[].zero_", "grad.size"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "keep_partial_grad", "(", "grad", ",", "topk", ")", ":", "\n", "    ", "\"\"\"\n    Keep only the topk rows of grads.\n    \"\"\"", "\n", "assert", "topk", "<", "grad", ".", "size", "(", "0", ")", "\n", "grad", ".", "data", "[", "topk", ":", "]", ".", "zero_", "(", ")", "\n", "return", "grad", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.ensure_dir": [[94, 99], ["os.path.exists", "os.makedirs", "print"], "function", ["None"], ["", "def", "ensure_dir", "(", "d", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "d", ")", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "\"Directory {} do not exist; creating...\"", ".", "format", "(", "d", ")", ")", "\n", "", "os", ".", "makedirs", "(", "d", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.save_config": [[100, 106], ["open", "json.dump", "print"], "function", ["None"], ["", "", "def", "save_config", "(", "config", ",", "path", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'w'", ")", "as", "outfile", ":", "\n", "        ", "json", ".", "dump", "(", "config", ",", "outfile", ",", "indent", "=", "2", ")", "\n", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Config saved to file {}\"", ".", "format", "(", "path", ")", ")", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.load_config": [[107, 113], ["open", "json.load", "print"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.lemma.trainer.TrainerCombined.load"], ["", "def", "load_config", "(", "path", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "with", "open", "(", "path", ")", "as", "f", ":", "\n", "        ", "config", "=", "json", ".", "load", "(", "f", ")", "\n", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Config loaded from file {}\"", ".", "format", "(", "path", ")", ")", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.print_config": [[114, 120], ["config.items", "print", "str"], "function", ["None"], ["", "def", "print_config", "(", "config", ")", ":", "\n", "    ", "info", "=", "\"Running with the following configs:\\n\"", "\n", "for", "k", ",", "v", "in", "config", ".", "items", "(", ")", ":", "\n", "        ", "info", "+=", "\"\\t{} : {}\\n\"", ".", "format", "(", "k", ",", "str", "(", "v", ")", ")", "\n", "", "print", "(", "\"\\n\"", "+", "info", "+", "\"\\n\"", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.normalize_text": [[121, 123], ["unicodedata.normalize"], "function", ["None"], ["", "def", "normalize_text", "(", "text", ")", ":", "\n", "    ", "return", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.unmap_with_copy": [[124, 139], ["zip", "words.append", "words.append"], "function", ["None"], ["", "def", "unmap_with_copy", "(", "indices", ",", "src_tokens", ",", "vocab", ")", ":", "\n", "    ", "\"\"\"\n    Unmap a list of list of indices, by optionally copying from src_tokens.\n    \"\"\"", "\n", "result", "=", "[", "]", "\n", "for", "ind", ",", "tokens", "in", "zip", "(", "indices", ",", "src_tokens", ")", ":", "\n", "        ", "words", "=", "[", "]", "\n", "for", "idx", "in", "ind", ":", "\n", "            ", "if", "idx", ">=", "0", ":", "\n", "                ", "words", ".", "append", "(", "vocab", ".", "id2word", "[", "idx", "]", ")", "\n", "", "else", ":", "\n", "                ", "idx", "=", "-", "idx", "-", "1", "# flip and minus 1", "\n", "words", ".", "append", "(", "tokens", "[", "idx", "]", ")", "\n", "", "", "result", "+=", "[", "words", "]", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.prune_decoded_seqs": [[140, 152], ["s.index"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.index"], ["", "def", "prune_decoded_seqs", "(", "seqs", ")", ":", "\n", "    ", "\"\"\"\n    Prune decoded sequences after EOS token.\n    \"\"\"", "\n", "out", "=", "[", "]", "\n", "for", "s", "in", "seqs", ":", "\n", "        ", "if", "constant", ".", "EOS", "in", "s", ":", "\n", "            ", "idx", "=", "s", ".", "index", "(", "constant", ".", "EOS_TOKEN", ")", "\n", "out", "+=", "[", "s", "[", ":", "idx", "]", "]", "\n", "", "else", ":", "\n", "            ", "out", "+=", "[", "s", "]", "\n", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.prune_hyp": [[153, 162], ["hyp.index"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.doc.Word.index"], ["", "def", "prune_hyp", "(", "hyp", ")", ":", "\n", "    ", "\"\"\"\n    Prune a decoded hypothesis\n    \"\"\"", "\n", "if", "constant", ".", "EOS_ID", "in", "hyp", ":", "\n", "        ", "idx", "=", "hyp", ".", "index", "(", "constant", ".", "EOS_ID", ")", "\n", "return", "hyp", "[", ":", "idx", "]", "\n", "", "else", ":", "\n", "        ", "return", "hyp", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.prune": [[163, 169], ["zip", "len", "len", "nl.append"], "function", ["None"], ["", "", "def", "prune", "(", "data_list", ",", "lens", ")", ":", "\n", "    ", "assert", "len", "(", "data_list", ")", "==", "len", "(", "lens", ")", "\n", "nl", "=", "[", "]", "\n", "for", "d", ",", "l", "in", "zip", "(", "data_list", ",", "lens", ")", ":", "\n", "        ", "nl", ".", "append", "(", "d", "[", ":", "l", "]", ")", "\n", "", "return", "nl", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.sort": [[170, 179], ["tuple", "isinstance", "list", "list", "isinstance", "isinstance", "zip", "range", "len", "sorted", "zip"], "function", ["None"], ["", "def", "sort", "(", "packed", ",", "ref", ",", "reverse", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Sort a series of packed list, according to a ref list.\n    Also return the original index before the sort.\n    \"\"\"", "\n", "assert", "(", "isinstance", "(", "packed", ",", "tuple", ")", "or", "isinstance", "(", "packed", ",", "list", ")", ")", "and", "isinstance", "(", "ref", ",", "list", ")", "\n", "packed", "=", "[", "ref", "]", "+", "[", "range", "(", "len", "(", "ref", ")", ")", "]", "+", "list", "(", "packed", ")", "\n", "sorted_packed", "=", "[", "list", "(", "t", ")", "for", "t", "in", "zip", "(", "*", "sorted", "(", "zip", "(", "*", "packed", ")", ",", "reverse", "=", "reverse", ")", ")", "]", "\n", "return", "tuple", "(", "sorted_packed", "[", "1", ":", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.unsort": [[180, 187], ["len", "len", "list", "zip", "sorted", "zip"], "function", ["None"], ["", "def", "unsort", "(", "sorted_list", ",", "oidx", ")", ":", "\n", "    ", "\"\"\"\n    Unsort a sorted list, based on the original idx.\n    \"\"\"", "\n", "assert", "len", "(", "sorted_list", ")", "==", "len", "(", "oidx", ")", ",", "\"Number of list elements must match with original indices.\"", "\n", "_", ",", "unsorted", "=", "[", "list", "(", "t", ")", "for", "t", "in", "zip", "(", "*", "sorted", "(", "zip", "(", "oidx", ",", "sorted_list", ")", ")", ")", "]", "\n", "return", "unsorted", "\n", "\n"]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.utils.tensor_unsort": [[188, 195], ["sorted_tensor.size", "len", "sorted", "enumerate"], "function", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size"], ["", "def", "tensor_unsort", "(", "sorted_tensor", ",", "oidx", ")", ":", "\n", "    ", "\"\"\"\n    Unsort a sorted tensor on its 0-th dimension, based on the original idx.\n    \"\"\"", "\n", "assert", "sorted_tensor", ".", "size", "(", "0", ")", "==", "len", "(", "oidx", ")", ",", "\"Number of list elements must match with original indices.\"", "\n", "backidx", "=", "[", "x", "[", "0", "]", "for", "x", "in", "sorted", "(", "enumerate", "(", "oidx", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "]", "\n", "return", "sorted_tensor", "[", "backidx", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.__init__": [[21, 30], ["vocab.BaseVocab.build_vocab"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.build_vocab"], ["", "", "return", "new", "\n", "\n", "", "", "class", "FeatureVocab", "(", "CompositeVocab", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "data", "=", "None", ",", "lang", "=", "\"\"", ",", "idx", "=", "0", ",", "sep", "=", "\"|\"", ",", "keyed", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "data", ",", "lang", ",", "idx", "=", "idx", ",", "sep", "=", "sep", ",", "keyed", "=", "keyed", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.build_vocab": [[31, 33], ["NotImplementedError"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.state_dict": [[34, 42], ["collections.OrderedDict", "hasattr", "getattr"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.load_state_dict": [[43, 50], ["cls", "state_dict.items", "setattr"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.normalize_unit": [[51, 55], ["unit.lower"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.unit2id": [[56, 62], ["vocab.BaseVocab.normalize_unit"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.normalize_unit"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.id2unit": [[63, 65], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.map": [[66, 68], ["vocab.BaseVocab.unit2id"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.unit2id"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.unmap": [[69, 71], ["vocab.BaseVocab.id2unit"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.id2unit"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.__len__": [[72, 74], ["len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.__getitem__": [[75, 82], ["isinstance", "vocab.BaseVocab.unit2id", "isinstance", "isinstance", "vocab.BaseVocab.id2unit", "TypeError"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.unit2id", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.id2unit"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.__contains__": [[83, 85], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseVocab.size": [[86, 89], ["len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.__init__": [[104, 109], ["vocab.BaseVocab.__init__"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.unit2parts": [[110, 126], ["unit.split", "dict", "dict", "x.split", "len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.unit2id": [[127, 134], ["vocab.CompositeVocab.unit2parts", "vocab.CompositeVocab._unit2id[].get", "vocab.CompositeVocab._unit2id[].get", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.unit2parts", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.conll.CoNLLFile.get"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.id2unit": [[135, 147], ["zip", "vocab.CompositeVocab.sep.join", "vocab.CompositeVocab._id2unit.keys", "items.append", "items.append"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.build_vocab": [[148, 193], ["collections.OrderedDict", "dict", "dict", "max", "vocab.CompositeVocab.unit2parts", "len", "copy.copy.copy", "vocab.CompositeVocab.unit2parts", "enumerate", "len", "copy.copy.copy", "allunits.append", "len", "sorted", "enumerate", "logging.debug", "copy.copy.copy", "vocab.CompositeVocab._id2unit[].append", "copy.copy.copy", "vocab.CompositeVocab._id2unit[].append", "vocab.CompositeVocab._id2unit.keys", "len"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.unit2parts", "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.unit2parts"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.CompositeVocab.lens": [[194, 196], ["len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__init__": [[202, 210], ["collections.OrderedDict", "all", "vocab_dict.items", "isinstance", "vocab_dict.values"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__setitem__": [[211, 213], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.__getitem__": [[214, 216], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.state_dict": [[217, 223], ["collections.OrderedDict", "vocab.BaseMultiVocab._vocabs.items", "v.state_dict"], "methods", ["home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.state_dict"], []], "home.repos.pwc.inspect_result.501Good_lexicon-enhanced-lemmatization.common.vocab.BaseMultiVocab.load_state_dict": [[224, 228], ["None"], "methods", ["None"], []]}