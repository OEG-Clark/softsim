{"home.repos.pwc.inspect_result.uclmr_fakenewschallenge.None.util.FNCData.__init__": [[68, 86], ["util.FNCData.read", "util.FNCData.read", "int", "len", "int"], "methods", ["home.repos.pwc.inspect_result.uclmr_fakenewschallenge.None.util.FNCData.read", "home.repos.pwc.inspect_result.uclmr_fakenewschallenge.None.util.FNCData.read"], ["def", "__init__", "(", "self", ",", "file_instances", ",", "file_bodies", ")", ":", "\n", "\n", "# Load data", "\n", "        ", "self", ".", "instances", "=", "self", ".", "read", "(", "file_instances", ")", "\n", "bodies", "=", "self", ".", "read", "(", "file_bodies", ")", "\n", "self", ".", "heads", "=", "{", "}", "\n", "self", ".", "bodies", "=", "{", "}", "\n", "\n", "# Process instances", "\n", "for", "instance", "in", "self", ".", "instances", ":", "\n", "            ", "if", "instance", "[", "'Headline'", "]", "not", "in", "self", ".", "heads", ":", "\n", "                ", "head_id", "=", "len", "(", "self", ".", "heads", ")", "\n", "self", ".", "heads", "[", "instance", "[", "'Headline'", "]", "]", "=", "head_id", "\n", "", "instance", "[", "'Body ID'", "]", "=", "int", "(", "instance", "[", "'Body ID'", "]", ")", "\n", "\n", "# Process bodies", "\n", "", "for", "body", "in", "bodies", ":", "\n", "            ", "self", ".", "bodies", "[", "int", "(", "body", "[", "'Body ID'", "]", ")", "]", "=", "body", "[", "'articleBody'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uclmr_fakenewschallenge.None.util.FNCData.read": [[87, 110], ["open", "csv.DictReader", "rows.append"], "methods", ["None"], ["", "", "def", "read", "(", "self", ",", "filename", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Read Fake News Challenge data from CSV file\n\n        Args:\n            filename: str, filename + extension\n\n        Returns:\n            rows: list, of dict per instance\n\n        \"\"\"", "\n", "\n", "# Initialise", "\n", "rows", "=", "[", "]", "\n", "\n", "# Process file", "\n", "with", "open", "(", "filename", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "table", ":", "\n", "            ", "r", "=", "DictReader", "(", "table", ")", "\n", "for", "line", "in", "r", ":", "\n", "                ", "rows", ".", "append", "(", "line", ")", "\n", "\n", "", "", "return", "rows", "\n", "\n"]], "home.repos.pwc.inspect_result.uclmr_fakenewschallenge.None.util.pipeline_train": [[113, 214], ["enumerate", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "sklearn.feature_extraction.text.TfidfTransformer().fit", "TfidfTransformer().fit.transform().toarray", "sklearn.feature_extraction.text.TfidfVectorizer().fit", "tfreq[].reshape", "tfreq[].reshape", "numpy.squeeze", "train_set.append", "train_stances.append", "heads.append", "bodies.append", "body_ids.append", "test_heads.append", "test_bodies.append", "test_body_ids.append", "sklearn.feature_extraction.text.TfidfTransformer", "TfidfTransformer().fit.transform", "sklearn.feature_extraction.text.TfidfVectorizer", "TfidfVectorizer().fit.transform().toarray", "TfidfVectorizer().fit.transform().toarray", "[].reshape", "TfidfVectorizer().fit.transform", "TfidfVectorizer().fit.transform", "sklearn.metrics.pairwise.cosine_similarity"], "function", ["None"], ["", "", "def", "pipeline_train", "(", "train", ",", "test", ",", "lim_unigram", ")", ":", "\n", "\n", "    ", "\"\"\"\n\n    Process train set, create relevant vectorizers\n\n    Args:\n        train: FNCData object, train set\n        test: FNCData object, test set\n        lim_unigram: int, number of most frequent words to consider\n\n    Returns:\n        train_set: list, of numpy arrays\n        train_stances: list, of ints\n        bow_vectorizer: sklearn CountVectorizer\n        tfreq_vectorizer: sklearn TfidfTransformer(use_idf=False)\n        tfidf_vectorizer: sklearn TfidfVectorizer()\n\n    \"\"\"", "\n", "\n", "# Initialise", "\n", "heads", "=", "[", "]", "\n", "heads_track", "=", "{", "}", "\n", "bodies", "=", "[", "]", "\n", "bodies_track", "=", "{", "}", "\n", "body_ids", "=", "[", "]", "\n", "id_ref", "=", "{", "}", "\n", "train_set", "=", "[", "]", "\n", "train_stances", "=", "[", "]", "\n", "cos_track", "=", "{", "}", "\n", "test_heads", "=", "[", "]", "\n", "test_heads_track", "=", "{", "}", "\n", "test_bodies", "=", "[", "]", "\n", "test_bodies_track", "=", "{", "}", "\n", "test_body_ids", "=", "[", "]", "\n", "head_tfidf_track", "=", "{", "}", "\n", "body_tfidf_track", "=", "{", "}", "\n", "\n", "# Identify unique heads and bodies", "\n", "for", "instance", "in", "train", ".", "instances", ":", "\n", "        ", "head", "=", "instance", "[", "'Headline'", "]", "\n", "body_id", "=", "instance", "[", "'Body ID'", "]", "\n", "if", "head", "not", "in", "heads_track", ":", "\n", "            ", "heads", ".", "append", "(", "head", ")", "\n", "heads_track", "[", "head", "]", "=", "1", "\n", "", "if", "body_id", "not", "in", "bodies_track", ":", "\n", "            ", "bodies", ".", "append", "(", "train", ".", "bodies", "[", "body_id", "]", ")", "\n", "bodies_track", "[", "body_id", "]", "=", "1", "\n", "body_ids", ".", "append", "(", "body_id", ")", "\n", "\n", "", "", "for", "instance", "in", "test", ".", "instances", ":", "\n", "        ", "head", "=", "instance", "[", "'Headline'", "]", "\n", "body_id", "=", "instance", "[", "'Body ID'", "]", "\n", "if", "head", "not", "in", "test_heads_track", ":", "\n", "            ", "test_heads", ".", "append", "(", "head", ")", "\n", "test_heads_track", "[", "head", "]", "=", "1", "\n", "", "if", "body_id", "not", "in", "test_bodies_track", ":", "\n", "            ", "test_bodies", ".", "append", "(", "test", ".", "bodies", "[", "body_id", "]", ")", "\n", "test_bodies_track", "[", "body_id", "]", "=", "1", "\n", "test_body_ids", ".", "append", "(", "body_id", ")", "\n", "\n", "# Create reference dictionary", "\n", "", "", "for", "i", ",", "elem", "in", "enumerate", "(", "heads", "+", "body_ids", ")", ":", "\n", "        ", "id_ref", "[", "elem", "]", "=", "i", "\n", "\n", "# Create vectorizers and BOW and TF arrays for train set", "\n", "", "bow_vectorizer", "=", "CountVectorizer", "(", "max_features", "=", "lim_unigram", ",", "stop_words", "=", "stop_words", ")", "\n", "bow", "=", "bow_vectorizer", ".", "fit_transform", "(", "heads", "+", "bodies", ")", "# Train set only", "\n", "\n", "tfreq_vectorizer", "=", "TfidfTransformer", "(", "use_idf", "=", "False", ")", ".", "fit", "(", "bow", ")", "\n", "tfreq", "=", "tfreq_vectorizer", ".", "transform", "(", "bow", ")", ".", "toarray", "(", ")", "# Train set only", "\n", "\n", "tfidf_vectorizer", "=", "TfidfVectorizer", "(", "max_features", "=", "lim_unigram", ",", "stop_words", "=", "stop_words", ")", ".", "fit", "(", "heads", "+", "bodies", "+", "test_heads", "+", "test_bodies", ")", "# Train and test sets", "\n", "\n", "# Process train set", "\n", "for", "instance", "in", "train", ".", "instances", ":", "\n", "        ", "head", "=", "instance", "[", "'Headline'", "]", "\n", "body_id", "=", "instance", "[", "'Body ID'", "]", "\n", "head_tf", "=", "tfreq", "[", "id_ref", "[", "head", "]", "]", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "body_tf", "=", "tfreq", "[", "id_ref", "[", "body_id", "]", "]", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "if", "head", "not", "in", "head_tfidf_track", ":", "\n", "            ", "head_tfidf", "=", "tfidf_vectorizer", ".", "transform", "(", "[", "head", "]", ")", ".", "toarray", "(", ")", "\n", "head_tfidf_track", "[", "head", "]", "=", "head_tfidf", "\n", "", "else", ":", "\n", "            ", "head_tfidf", "=", "head_tfidf_track", "[", "head", "]", "\n", "", "if", "body_id", "not", "in", "body_tfidf_track", ":", "\n", "            ", "body_tfidf", "=", "tfidf_vectorizer", ".", "transform", "(", "[", "train", ".", "bodies", "[", "body_id", "]", "]", ")", ".", "toarray", "(", ")", "\n", "body_tfidf_track", "[", "body_id", "]", "=", "body_tfidf", "\n", "", "else", ":", "\n", "            ", "body_tfidf", "=", "body_tfidf_track", "[", "body_id", "]", "\n", "", "if", "(", "head", ",", "body_id", ")", "not", "in", "cos_track", ":", "\n", "            ", "tfidf_cos", "=", "cosine_similarity", "(", "head_tfidf", ",", "body_tfidf", ")", "[", "0", "]", ".", "reshape", "(", "1", ",", "1", ")", "\n", "cos_track", "[", "(", "head", ",", "body_id", ")", "]", "=", "tfidf_cos", "\n", "", "else", ":", "\n", "            ", "tfidf_cos", "=", "cos_track", "[", "(", "head", ",", "body_id", ")", "]", "\n", "", "feat_vec", "=", "np", ".", "squeeze", "(", "np", ".", "c_", "[", "head_tf", ",", "body_tf", ",", "tfidf_cos", "]", ")", "\n", "train_set", ".", "append", "(", "feat_vec", ")", "\n", "train_stances", ".", "append", "(", "label_ref", "[", "instance", "[", "'Stance'", "]", "]", ")", "\n", "\n", "", "return", "train_set", ",", "train_stances", ",", "bow_vectorizer", ",", "tfreq_vectorizer", ",", "tfidf_vectorizer", "\n", "\n"]], "home.repos.pwc.inspect_result.uclmr_fakenewschallenge.None.util.pipeline_test": [[216, 268], ["numpy.squeeze", "test_set.append", "bow_vectorizer.transform().toarray", "[].reshape", "tfidf_vectorizer.transform().toarray().reshape", "bow_vectorizer.transform().toarray", "[].reshape", "tfidf_vectorizer.transform().toarray().reshape", "[].reshape", "bow_vectorizer.transform", "tfidf_vectorizer.transform().toarray", "bow_vectorizer.transform", "tfidf_vectorizer.transform().toarray", "tfreq_vectorizer.transform().toarray", "tfreq_vectorizer.transform().toarray", "sklearn.metrics.pairwise.cosine_similarity", "tfidf_vectorizer.transform", "tfidf_vectorizer.transform", "tfreq_vectorizer.transform", "tfreq_vectorizer.transform"], "function", ["None"], ["", "def", "pipeline_test", "(", "test", ",", "bow_vectorizer", ",", "tfreq_vectorizer", ",", "tfidf_vectorizer", ")", ":", "\n", "\n", "    ", "\"\"\"\n\n    Process test set\n\n    Args:\n        test: FNCData object, test set\n        bow_vectorizer: sklearn CountVectorizer\n        tfreq_vectorizer: sklearn TfidfTransformer(use_idf=False)\n        tfidf_vectorizer: sklearn TfidfVectorizer()\n\n    Returns:\n        test_set: list, of numpy arrays\n\n    \"\"\"", "\n", "\n", "# Initialise", "\n", "test_set", "=", "[", "]", "\n", "heads_track", "=", "{", "}", "\n", "bodies_track", "=", "{", "}", "\n", "cos_track", "=", "{", "}", "\n", "\n", "# Process test set", "\n", "for", "instance", "in", "test", ".", "instances", ":", "\n", "        ", "head", "=", "instance", "[", "'Headline'", "]", "\n", "body_id", "=", "instance", "[", "'Body ID'", "]", "\n", "if", "head", "not", "in", "heads_track", ":", "\n", "            ", "head_bow", "=", "bow_vectorizer", ".", "transform", "(", "[", "head", "]", ")", ".", "toarray", "(", ")", "\n", "head_tf", "=", "tfreq_vectorizer", ".", "transform", "(", "head_bow", ")", ".", "toarray", "(", ")", "[", "0", "]", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "head_tfidf", "=", "tfidf_vectorizer", ".", "transform", "(", "[", "head", "]", ")", ".", "toarray", "(", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "heads_track", "[", "head", "]", "=", "(", "head_tf", ",", "head_tfidf", ")", "\n", "", "else", ":", "\n", "            ", "head_tf", "=", "heads_track", "[", "head", "]", "[", "0", "]", "\n", "head_tfidf", "=", "heads_track", "[", "head", "]", "[", "1", "]", "\n", "", "if", "body_id", "not", "in", "bodies_track", ":", "\n", "            ", "body_bow", "=", "bow_vectorizer", ".", "transform", "(", "[", "test", ".", "bodies", "[", "body_id", "]", "]", ")", ".", "toarray", "(", ")", "\n", "body_tf", "=", "tfreq_vectorizer", ".", "transform", "(", "body_bow", ")", ".", "toarray", "(", ")", "[", "0", "]", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "body_tfidf", "=", "tfidf_vectorizer", ".", "transform", "(", "[", "test", ".", "bodies", "[", "body_id", "]", "]", ")", ".", "toarray", "(", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "bodies_track", "[", "body_id", "]", "=", "(", "body_tf", ",", "body_tfidf", ")", "\n", "", "else", ":", "\n", "            ", "body_tf", "=", "bodies_track", "[", "body_id", "]", "[", "0", "]", "\n", "body_tfidf", "=", "bodies_track", "[", "body_id", "]", "[", "1", "]", "\n", "", "if", "(", "head", ",", "body_id", ")", "not", "in", "cos_track", ":", "\n", "            ", "tfidf_cos", "=", "cosine_similarity", "(", "head_tfidf", ",", "body_tfidf", ")", "[", "0", "]", ".", "reshape", "(", "1", ",", "1", ")", "\n", "cos_track", "[", "(", "head", ",", "body_id", ")", "]", "=", "tfidf_cos", "\n", "", "else", ":", "\n", "            ", "tfidf_cos", "=", "cos_track", "[", "(", "head", ",", "body_id", ")", "]", "\n", "", "feat_vec", "=", "np", ".", "squeeze", "(", "np", ".", "c_", "[", "head_tf", ",", "body_tf", ",", "tfidf_cos", "]", ")", "\n", "test_set", ".", "append", "(", "feat_vec", ")", "\n", "\n", "", "return", "test_set", "\n", "\n"]], "home.repos.pwc.inspect_result.uclmr_fakenewschallenge.None.util.load_model": [[270, 283], ["tensorflow.train.Saver", "tf.train.Saver.restore"], "function", ["None"], ["", "def", "load_model", "(", "sess", ")", ":", "\n", "\n", "    ", "\"\"\"\n\n    Load TensorFlow model\n\n    Args:\n        sess: TensorFlow session\n\n    \"\"\"", "\n", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "saver", ".", "restore", "(", "sess", ",", "'./model/model.checkpoint'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uclmr_fakenewschallenge.None.util.save_predictions": [[285, 304], ["open", "csv.DictWriter", "csv.DictWriter.writeheader", "csv.DictWriter.writerow"], "function", ["None"], ["", "def", "save_predictions", "(", "pred", ",", "file", ")", ":", "\n", "\n", "    ", "\"\"\"\n\n    Save predictions to CSV file\n\n    Args:\n        pred: numpy array, of numeric predictions\n        file: str, filename + extension\n\n    \"\"\"", "\n", "\n", "with", "open", "(", "file", ",", "'w'", ")", "as", "csvfile", ":", "\n", "        ", "fieldnames", "=", "[", "'Stance'", "]", "\n", "writer", "=", "DictWriter", "(", "csvfile", ",", "fieldnames", "=", "fieldnames", ")", "\n", "\n", "writer", ".", "writeheader", "(", ")", "\n", "for", "instance", "in", "pred", ":", "\n", "            ", "writer", ".", "writerow", "(", "{", "'Stance'", ":", "label_ref_rev", "[", "instance", "]", "}", ")", "\n", "", "", "", ""]]}