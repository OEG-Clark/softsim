{"home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_utils.InputFeature.__init__": [[20, 26], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "init_ids", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "masked_lm_labels", ")", ":", "\n", "        ", "self", ".", "init_ids", "=", "init_ids", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "masked_lm_labels", "=", "masked_lm_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_utils.get_entity_pos": [[28, 57], ["range", "len", "pos1.append", "pos1.append", "len", "pos1.append", "pos1.append", "pos2.append", "pos2.append", "len", "pos2.append", "pos2.append"], "function", ["None"], ["", "", "def", "get_entity_pos", "(", "sentence_token_list", ")", ":", "\n", "    ", "pos1", "=", "[", "]", "\n", "pos2", "=", "[", "]", "\n", "e1_flag", "=", "0", "\n", "e2_flag", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "sentence_token_list", ")", ")", ":", "\n", "        ", "if", "sentence_token_list", "[", "i", "]", "==", "\">\"", "and", "sentence_token_list", "[", "i", "-", "1", "]", "==", "\"##1\"", "and", "sentence_token_list", "[", "i", "-", "3", "]", "==", "\"<\"", ":", "\n", "            ", "e1_flag", "=", "1", "\n", "if", "e1_flag", "==", "1", "and", "e2_flag", "==", "0", ":", "\n", "                ", "pos1", ".", "append", "(", "i", "+", "1", "-", "4", ")", "\n", "", "else", ":", "\n", "                ", "pos1", ".", "append", "(", "i", "+", "1", "-", "13", ")", "\n", "", "", "if", "sentence_token_list", "[", "i", "]", "==", "\"<\"", "and", "i", "+", "3", "<", "len", "(", "sentence_token_list", ")", "and", "sentence_token_list", "[", "i", "+", "1", "]", "==", "\"/\"", "and", "sentence_token_list", "[", "i", "+", "3", "]", "==", "\"##1\"", ":", "\n", "            ", "if", "e1_flag", "==", "1", "and", "e2_flag", "==", "0", ":", "\n", "                ", "pos1", ".", "append", "(", "i", "-", "1", "-", "4", ")", "\n", "", "else", ":", "\n", "                ", "pos1", ".", "append", "(", "i", "-", "1", "-", "13", ")", "\n", "", "", "if", "sentence_token_list", "[", "i", "]", "==", "\">\"", "and", "sentence_token_list", "[", "i", "-", "1", "]", "==", "\"##2\"", "and", "sentence_token_list", "[", "i", "-", "3", "]", "==", "\"<\"", ":", "\n", "            ", "e2_flag", "=", "1", "\n", "if", "e1_flag", "==", "1", "and", "e2_flag", "==", "1", ":", "\n", "                ", "pos2", ".", "append", "(", "i", "+", "1", "-", "13", ")", "\n", "", "else", ":", "\n", "                ", "pos2", ".", "append", "(", "i", "+", "1", "-", "4", ")", "\n", "", "", "if", "sentence_token_list", "[", "i", "]", "==", "\"<\"", "and", "i", "+", "3", "<", "len", "(", "sentence_token_list", ")", "and", "sentence_token_list", "[", "i", "+", "1", "]", "==", "\"/\"", "and", "sentence_token_list", "[", "i", "+", "3", "]", "==", "\"##2\"", ":", "\n", "            ", "if", "e1_flag", "==", "1", "and", "e2_flag", "==", "1", ":", "\n", "                ", "pos2", ".", "append", "(", "i", "-", "1", "-", "13", ")", "\n", "", "else", ":", "\n", "                ", "pos2", ".", "append", "(", "i", "-", "1", "-", "4", ")", "\n", "", "", "", "return", "pos1", ",", "pos2", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_utils.create_masked_lm_predictions": [[59, 103], ["tokens.index", "range", "rng.shuffle", "len", "list", "min", "set", "len", "cand_indexes.append", "max", "set.add", "masked_lm_positions.append", "int", "len", "rng.random", "tokenizer.convert_tokens_to_ids", "round", "rng.random", "len", "range", "len", "range", "len", "rng.randint"], "function", ["None"], ["", "def", "create_masked_lm_predictions", "(", "tokens", ",", "pos1", ",", "pos2", ",", "masked_lm_probs", ",", "masked_lm_labels", ",", "\n", "max_predictions_per_seq", ",", "rng", ")", ":", "\n", "    ", "\"\"\"Creates the predictions for the masked LM objective.\"\"\"", "\n", "\n", "#vocab_words = list(tokenizer.vocab.keys())", "\n", "mask_start_pos", "=", "tokens", ".", "index", "(", "'[SEP]'", ")", "\n", "\n", "cand_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "mask_start_pos", ",", "len", "(", "tokens", ")", ")", ":", "\n", "        ", "if", "tokens", "[", "i", "]", "==", "\"[CLS]\"", "or", "tokens", "[", "i", "]", "==", "\"[SEP]\"", "or", "(", "len", "(", "pos1", ")", "==", "2", "and", "i", "in", "range", "(", "pos1", "[", "0", "]", ",", "pos1", "[", "1", "]", "+", "1", ")", ")", "or", "(", "len", "(", "pos2", ")", "==", "2", "and", "i", "in", "range", "(", "pos2", "[", "0", "]", ",", "pos2", "[", "1", "]", "+", "1", ")", ")", ":", "\n", "            ", "continue", "\n", "", "cand_indexes", ".", "append", "(", "i", ")", "\n", "\n", "", "rng", ".", "shuffle", "(", "cand_indexes", ")", "\n", "len_cand", "=", "len", "(", "cand_indexes", ")", "\n", "output_tokens", "=", "list", "(", "tokens", ")", "\n", "num_to_predict", "=", "min", "(", "max_predictions_per_seq", ",", "\n", "max", "(", "1", ",", "int", "(", "round", "(", "len", "(", "tokens", ")", "*", "masked_lm_probs", ")", ")", ")", ")", "\n", "\n", "masked_lm_positions", "=", "[", "]", "\n", "covered_indexes", "=", "set", "(", ")", "\n", "for", "index", "in", "cand_indexes", ":", "\n", "        ", "if", "len", "(", "masked_lm_positions", ")", ">=", "num_to_predict", ":", "\n", "            ", "break", "\n", "", "if", "index", "in", "covered_indexes", ":", "\n", "            ", "continue", "\n", "", "covered_indexes", ".", "add", "(", "index", ")", "\n", "\n", "masked_token", "=", "None", "\n", "## 80% of the time, replace with [MASK]", "\n", "if", "rng", ".", "random", "(", ")", "<", "0.8", ":", "\n", "            ", "masked_token", "=", "\"[MASK]\"", "\n", "", "else", ":", "\n", "## 10% of the time, keep original", "\n", "            ", "if", "rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                ", "masked_token", "=", "tokens", "[", "index", "]", "\n", "## 10% of the time, replace with random word", "\n", "", "else", ":", "\n", "                ", "masked_token", "=", "tokens", "[", "cand_indexes", "[", "rng", ".", "randint", "(", "0", ",", "len_cand", "-", "1", ")", "]", "]", "\n", "\n", "", "", "masked_lm_labels", "[", "index", "]", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "tokens", "[", "index", "]", "]", ")", "[", "0", "]", "\n", "output_tokens", "[", "index", "]", "=", "masked_token", "\n", "masked_lm_positions", ".", "append", "(", "index", ")", "\n", "", "return", "output_tokens", ",", "masked_lm_positions", ",", "masked_lm_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_utils.extract_features": [[105, 145], ["tokenizer.convert_tokens_to_ids", "random.Random", "cbert_utils.get_entity_pos", "cbert_utils.create_masked_lm_predictions", "tokenizer.convert_tokens_to_ids", "len", "len", "len", "len", "tokenizer.convert_tokens_to_ids.append", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_entity_pos", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_utils.create_masked_lm_predictions"], ["", "def", "extract_features", "(", "tokens", ",", "max_seq_length", ")", ":", "\n", "    ", "\"\"\"extract features from tokens\"\"\"", "\n", "\n", "if", "len", "(", "tokens", ")", ">", "max_seq_length", ":", "\n", "        ", "tokens", "=", "tokens", "[", "0", ":", "max_seq_length", "]", "\n", "\n", "## construct init_ids for each example", "\n", "", "init_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "## construct input_ids for each example, we replace the word_id using ", "\n", "## the ids of masked words (mask words based on original sentence)", "\n", "masked_lm_probs", "=", "0.15", "\n", "max_predictions_per_seq", "=", "20", "\n", "rng", "=", "random", ".", "Random", "(", "12345", ")", "\n", "original_masked_lm_labels", "=", "[", "-", "100", "]", "*", "max_seq_length", "\n", "\n", "pos1", ",", "pos2", "=", "get_entity_pos", "(", "tokens", ")", "\n", "(", "output_tokens", ",", "masked_lm_positions", ",", "\n", "masked_lm_labels", ")", "=", "create_masked_lm_predictions", "(", "\n", "tokens", ",", "pos1", ",", "pos2", ",", "masked_lm_probs", ",", "original_masked_lm_labels", ",", "max_predictions_per_seq", ",", "rng", ")", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "output_tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "        ", "init_ids", ".", "append", "(", "0", ")", "\n", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "init_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "\n", "return", "tokens", ",", "init_ids", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "masked_lm_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_utils.convert_examples_to_features": [[147, 178], ["enumerate", "tokenizer.tokenize", "cbert_utils.extract_features", "features.append", "cbert_utils.InputFeature", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_utils.extract_features"], ["", "def", "convert_examples_to_features", "(", "examples", ",", "max_seq_length", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of 'InputBatch's.\"\"\"", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "# The convention in BERT is:", "\n", "# tokens:   [CLS] is this jack ##son ##ville ? [SEP]", "\n", "# type_ids: 0     0  0    0    0     0       0 0    ", "\n", "        ", "tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ")", "\n", "tokens", ",", "init_ids", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "masked_lm_labels", "=", "extract_features", "(", "tokens", ",", "max_seq_length", ")", "\n", "\n", "\"\"\"consturct features\"\"\"", "\n", "features", ".", "append", "(", "\n", "InputFeature", "(", "\n", "init_ids", "=", "init_ids", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "masked_lm_labels", "=", "masked_lm_labels", ")", ")", "\n", "\n", "\"\"\"print examples\"\"\"", "\n", "if", "ex_index", "<", "5", ":", "\n", "            ", "logger", ".", "info", "(", "\"[mlm_tune] *** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"[mlm_tune] tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[mlm_tune] init_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "init_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[mlm_tune] input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[mlm_tune] input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[mlm_tune] segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[mlm_tune] masked_lm_labels: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "masked_lm_labels", "]", ")", ")", "\n", "", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_utils.construct_train_dataloader": [[180, 199], ["cbert_utils.convert_examples_to_features", "int", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.RandomSampler", "torch.utils.data.DataLoader", "len"], "function", ["home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_utils.convert_examples_to_features"], ["", "def", "construct_train_dataloader", "(", "train_examples", ",", "max_seq_length", ",", "train_batch_size", ",", "num_train_epochs", ",", "device", ")", ":", "\n", "    ", "\"\"\"construct dataloader for training data\"\"\"", "\n", "\n", "num_train_steps", "=", "None", "\n", "train_features", "=", "convert_examples_to_features", "(", "\n", "train_examples", ",", "max_seq_length", ")", "\n", "num_train_steps", "=", "int", "(", "len", "(", "train_features", ")", "/", "train_batch_size", "*", "num_train_epochs", ")", "\n", "\n", "all_init_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "init_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "all_masked_lm_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "masked_lm_labels", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "tensor_dataset", "=", "TensorDataset", "(", "all_init_ids", ",", "all_input_ids", ",", "all_input_mask", ",", "\n", "all_segment_ids", ",", "all_masked_lm_labels", ")", "\n", "train_sampler", "=", "RandomSampler", "(", "tensor_dataset", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "tensor_dataset", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "train_batch_size", ")", "\n", "return", "train_features", ",", "num_train_steps", ",", "train_dataloader", "", "", ""]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.networks.BertForSequenceClassificationUserDefined.__init__": [[8, 19], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "networks.BertForSequenceClassificationUserDefined.init_weights"], "methods", ["home.repos.pwc.inspect_result.thu-bpm_gradlre.src.networks.LabelGeneration.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "classifier_2", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "config", ".", "num_labels", ")", "\n", "#self.classifier_3 = nn.Linear(config.hidden_size//2, self.config.num_labels)", "\n", "#self.classifier = nn.Linear(2 * config.hidden_size, self.config.num_labels)", "\n", "self", ".", "init_weights", "(", ")", "\n", "self", ".", "output_emebedding", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.networks.BertForSequenceClassificationUserDefined.forward": [[20, 53], ["networks.BertForSequenceClassificationUserDefined.bert", "range", "torch.stack", "networks.BertForSequenceClassificationUserDefined.dropout", "networks.BertForSequenceClassificationUserDefined.classifier", "networks.BertForSequenceClassificationUserDefined.classifier_2", "len", "torch.cat", "e_pos_outputs.append", "e1_pos[].item", "e2_pos[].item"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "inputs_embeds", "=", "None", ",", "labels", "=", "None", ",", "e1_pos", "=", "None", ",", "e2_pos", "=", "None", ")", ":", "\n", "\n", "        ", "outputs", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", ")", "# sequence_output, pooled_output, (hidden_states), (attentions)", "\n", "\n", "e_pos_outputs", "=", "[", "]", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "e1_pos", ")", ")", ":", "\n", "            ", "e1_pos_output_i", "=", "sequence_output", "[", "i", ",", "e1_pos", "[", "i", "]", ".", "item", "(", ")", ",", ":", "]", "\n", "e2_pos_output_i", "=", "sequence_output", "[", "i", ",", "e2_pos", "[", "i", "]", ".", "item", "(", ")", ",", ":", "]", "\n", "e_pos_output_i", "=", "torch", ".", "cat", "(", "(", "e1_pos_output_i", ",", "e2_pos_output_i", ")", ",", "dim", "=", "0", ")", "\n", "e_pos_outputs", ".", "append", "(", "e_pos_output_i", ")", "\n", "\n", "", "e_pos_output", "=", "torch", ".", "stack", "(", "e_pos_outputs", ")", "\n", "\n", "self", ".", "output_emebedding", "=", "e_pos_output", "#e1&e2 cancat output", "\n", "\n", "e_pos_output", "=", "self", ".", "dropout", "(", "e_pos_output", ")", "\n", "#logits = self.classifier(e_pos_output)", "\n", "hidden", "=", "self", ".", "classifier", "(", "e_pos_output", ")", "\n", "#hidden_1 = self.classifier_2(hidden)", "\n", "logits", "=", "self", ".", "classifier_2", "(", "hidden", ")", "\n", "\n", "outputs", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "# add hidden states and attention if they are here", "\n", "\n", "return", "outputs", "+", "(", "self", ".", "output_emebedding", ",", ")", "# (loss), logits, (hidden_states), (attentions), (self.output_emebedding)", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.networks.RelationClassification.__init__": [[57, 59], ["networks.BertForSequenceClassificationUserDefined.__init__"], "methods", ["home.repos.pwc.inspect_result.thu-bpm_gradlre.src.networks.LabelGeneration.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.networks.LabelGeneration.__init__": [[63, 65], ["networks.BertForSequenceClassificationUserDefined.__init__"], "methods", ["home.repos.pwc.inspect_result.thu-bpm_gradlre.src.networks.LabelGeneration.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_finetune.add_space": [[25, 33], ["sentence_train_new.append", "sentence.replace().replace().replace().replace", "sentence.replace().replace().replace", "sentence.replace().replace", "sentence.replace"], "function", ["None"], ["def", "add_space", "(", "sentence_train", ",", "dataset_name", ")", ":", "\n", "    ", "if", "dataset_name", "==", "\"tacred\"", ":", "\n", "        ", "return", "sentence_train", "\n", "", "else", ":", "\n", "        ", "sentence_train_new", "=", "[", "]", "\n", "for", "sentence", "in", "sentence_train", ":", "\n", "            ", "sentence_train_new", ".", "append", "(", "sentence", ".", "replace", "(", "\"<e1>\"", ",", "\"<e1> \"", ")", ".", "replace", "(", "\"</e1>\"", ",", "\" </e1>\"", ")", ".", "replace", "(", "\"<e2>\"", ",", "\"<e2> \"", ")", ".", "replace", "(", "\"</e2>\"", ",", "\" </e2>\"", ")", ")", "\n", "", "return", "sentence_train_new", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_finetune.train_mlm": [[35, 121], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "random.seed", "numpy.random.seed", "torch.manual_seed", "transformers.BertForMaskedLM.from_pretrained", "cbert_utils.construct_train_dataloader", "BertForMaskedLM.from_pretrained.cuda", "logger.info", "logger.info", "logger.info", "logger.info", "list", "transformers.AdamW", "BertForMaskedLM.from_pretrained.train", "os.makedirs", "tqdm.trange", "len", "BertForMaskedLM.from_pretrained.named_parameters", "int", "enumerate", "tuple", "transformers.AdamW.zero_grad", "BertForMaskedLM.from_pretrained.", "loss.backward", "loss.item", "transformers.AdamW.step", "print", "any", "t.cuda", "any"], "function", ["home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_utils.construct_train_dataloader"], ["", "", "def", "train_mlm", "(", "train_examples", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "default", "=", "\"datasets\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "default", "=", "\"aug_data\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output dir for augmented dataset.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_model_dir\"", ",", "default", "=", "\"cbert_model\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The cache dir for saved model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "\"bert-base-uncased\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The path of pretrained bert model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "default", "=", "128", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequence longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this wille be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "default", "=", "32", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "default", "=", "10.0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for.\"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_every_epoch\"", ",", "default", "=", "True", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--use_aug'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'whether to use data aug'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "## leveraging lastest bert module in Transformers to load pre-trained model (weights)", "\n", "masked_model", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "args", ".", "bert_model", ")", "\n", "\n", "train_features", ",", "num_train_steps", ",", "train_dataloader", "=", "cbert_utils", ".", "construct_train_dataloader", "(", "train_examples", ",", "args", ".", "max_seq_length", ",", "\n", "args", ".", "train_batch_size", ",", "args", ".", "num_train_epochs", ",", "device", ")", "\n", "\n", "## if you have a GPU, put everything on cuda", "\n", "masked_model", ".", "cuda", "(", ")", "\n", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_features", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "\n", "## in Transformers, optimizer and schedules are splitted and instantiated like this:", "\n", "param_optimizer", "=", "list", "(", "masked_model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'gamma'", ",", "'beta'", "]", "\n", "optimizer_grounded_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay_rate'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay_rate'", ":", "0.0", "}", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grounded_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "correct_bias", "=", "False", ")", "\n", "masked_model", ".", "train", "(", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "save_model_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "for", "e", "in", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "        ", "avg_loss", "=", "0.", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "train_dataloader", ")", ":", "\n", "# print(step)", "\n", "            ", "batch", "=", "tuple", "(", "t", ".", "cuda", "(", ")", "for", "t", "in", "batch", ")", "\n", "_", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "masked_ids", "=", "batch", "\n", "\"\"\"train generator at each batch\"\"\"", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "outputs", "=", "masked_model", "(", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "labels", "=", "masked_ids", ")", "\n", "loss", "=", "outputs", "[", "0", "]", "\n", "# print(loss)", "\n", "loss", ".", "backward", "(", ")", "\n", "avg_loss", "+=", "loss", ".", "item", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "if", "(", "step", "+", "1", ")", "%", "50", "==", "0", ":", "\n", "                ", "print", "(", "\"avg_loss: {}\"", ".", "format", "(", "avg_loss", "/", "50", ")", ")", "\n", "avg_loss", "=", "0", "\n", "\n", "", "", "", "return", "masked_model", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.flat_accuracy": [[52, 60], ["numpy.argmax().flatten", "labels.flatten", "numpy.sum", "len", "numpy.argmax"], "function", ["None"], ["def", "flat_accuracy", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "pred_flat", "=", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "labels", ".", "flatten", "(", ")", "\n", "non_zero_idx", "=", "(", "labels_flat", "!=", "0", ")", "\n", "# if len(labels_flat[non_zero_idx])==0:", "\n", "#     print(\"error occur: \", labels_flat)", "\n", "#     return 0", "\n", "return", "np", ".", "sum", "(", "pred_flat", "[", "non_zero_idx", "]", "==", "labels_flat", "[", "non_zero_idx", "]", ")", "/", "len", "(", "labels_flat", "[", "non_zero_idx", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.format_time": [[63, 68], ["int", "str", "round", "datetime.timedelta"], "function", ["None"], ["", "def", "format_time", "(", "elapsed", ")", ":", "\n", "# Round to the nearest second.", "\n", "    ", "elapsed_rounded", "=", "int", "(", "round", "(", "(", "elapsed", ")", ")", ")", "\n", "# Format as hh:mm:ss", "\n", "return", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "elapsed_rounded", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.score": [[71, 112], ["collections.Counter", "collections.Counter", "collections.Counter", "range", "print", "print", "print", "print", "len", "print", "sum", "sum", "collections.Counter.values", "float", "float", "collections.Counter.values", "float", "float", "sum", "sum", "sum", "sum", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values"], "function", ["None"], ["", "def", "score", "(", "key", ",", "prediction", ",", "verbose", "=", "True", ",", "NO_RELATION", "=", "0", ")", ":", "\n", "    ", "correct_by_relation", "=", "Counter", "(", ")", "\n", "guessed_by_relation", "=", "Counter", "(", ")", "\n", "gold_by_relation", "=", "Counter", "(", ")", "\n", "\n", "# Loop over the data to compute a score", "\n", "for", "row", "in", "range", "(", "len", "(", "key", ")", ")", ":", "\n", "        ", "gold", "=", "key", "[", "row", "]", "\n", "guess", "=", "prediction", "[", "row", "]", "\n", "\n", "if", "gold", "==", "NO_RELATION", "and", "guess", "==", "NO_RELATION", ":", "\n", "            ", "pass", "\n", "", "elif", "gold", "==", "NO_RELATION", "and", "guess", "!=", "NO_RELATION", ":", "\n", "            ", "guessed_by_relation", "[", "guess", "]", "+=", "1", "\n", "", "elif", "gold", "!=", "NO_RELATION", "and", "guess", "==", "NO_RELATION", ":", "\n", "            ", "gold_by_relation", "[", "gold", "]", "+=", "1", "\n", "", "elif", "gold", "!=", "NO_RELATION", "and", "guess", "!=", "NO_RELATION", ":", "\n", "            ", "guessed_by_relation", "[", "guess", "]", "+=", "1", "\n", "gold_by_relation", "[", "gold", "]", "+=", "1", "\n", "if", "gold", "==", "guess", ":", "\n", "                ", "correct_by_relation", "[", "guess", "]", "+=", "1", "\n", "\n", "# Print the aggregate score", "\n", "", "", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Final Score:\"", ")", "\n", "", "prec_micro", "=", "1.0", "\n", "if", "sum", "(", "guessed_by_relation", ".", "values", "(", ")", ")", ">", "0", ":", "\n", "        ", "prec_micro", "=", "float", "(", "sum", "(", "correct_by_relation", ".", "values", "(", ")", ")", ")", "/", "float", "(", "\n", "sum", "(", "guessed_by_relation", ".", "values", "(", ")", ")", ")", "\n", "", "recall_micro", "=", "0.0", "\n", "if", "sum", "(", "gold_by_relation", ".", "values", "(", ")", ")", ">", "0", ":", "\n", "        ", "recall_micro", "=", "float", "(", "sum", "(", "correct_by_relation", ".", "values", "(", ")", ")", ")", "/", "float", "(", "\n", "sum", "(", "gold_by_relation", ".", "values", "(", ")", ")", ")", "\n", "", "f1_micro", "=", "0.0", "\n", "if", "prec_micro", "+", "recall_micro", ">", "0.0", ":", "\n", "        ", "f1_micro", "=", "2.0", "*", "prec_micro", "*", "recall_micro", "/", "(", "prec_micro", "+", "recall_micro", ")", "\n", "", "print", "(", "\"SET NO_RELATION ID: \"", ",", "NO_RELATION", ")", "\n", "print", "(", "\"Precision (micro): {:.3%}\"", ".", "format", "(", "prec_micro", ")", ")", "\n", "print", "(", "\"   Recall (micro): {:.3%}\"", ".", "format", "(", "recall_micro", ")", ")", "\n", "print", "(", "\"       F1 (micro): {:.3%}\"", ".", "format", "(", "f1_micro", ")", ")", "\n", "return", "prec_micro", ",", "recall_micro", ",", "f1_micro", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.pre_processing": [[116, 170], ["print", "transformers.BertTokenizer.from_pretrained", "range", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "len", "BertTokenizer.from_pretrained.encode_plus", "[].item", "[].item", "torch.tensor.append", "torch.tensor.append", "torch.cat().to.append", "torch.cat().to.append", "torch.tensor.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "pre_processing", "(", "sentence_train", ",", "sentence_train_label", ")", ":", "\n", "    ", "input_ids", "=", "[", "]", "\n", "attention_masks", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "e1_pos", "=", "[", "]", "\n", "e2_pos", "=", "[", "]", "\n", "# index_list = []", "\n", "\n", "# Load tokenizer.", "\n", "print", "(", "'Loading BERT tokenizer...'", ")", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "do_lower_case", "=", "True", ")", "\n", "# tokenizer.add_special_tokens({'additional_special_tokens':[\"<e1>\",\"</e1>\",\"<e2>\",\"</e2>\"]})", "\n", "\n", "# pre-processing sentenses to BERT pattern", "\n", "for", "i", "in", "range", "(", "len", "(", "sentence_train", ")", ")", ":", "\n", "        ", "encoded_dict", "=", "tokenizer", ".", "encode_plus", "(", "\n", "sentence_train", "[", "i", "]", ",", "# Sentence to encode.", "\n", "add_special_tokens", "=", "False", ",", "# Add '[CLS]' and '[SEP]'", "\n", "max_length", "=", "args", ".", "max_length", ",", "# Pad & truncate all sentences.", "\n", "pad_to_max_length", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "# Construct attn. masks.", "\n", "return_tensors", "=", "'pt'", ",", "# Return pytorch tensors.", "\n", "truncation", "=", "True", "\n", ")", "\n", "try", ":", "\n", "# Find e1(id:2487) and e2(id:2475) position", "\n", "            ", "pos1", "=", "(", "encoded_dict", "[", "'input_ids'", "]", "==", "2487", ")", ".", "nonzero", "(", ")", "[", "0", "]", "[", "1", "]", ".", "item", "(", ")", "\n", "pos2", "=", "(", "encoded_dict", "[", "'input_ids'", "]", "==", "2475", ")", ".", "nonzero", "(", ")", "[", "0", "]", "[", "1", "]", ".", "item", "(", ")", "\n", "e1_pos", ".", "append", "(", "pos1", ")", "\n", "e2_pos", ".", "append", "(", "pos2", ")", "\n", "# Add the encoded sentence to the list.", "\n", "input_ids", ".", "append", "(", "encoded_dict", "[", "'input_ids'", "]", ")", "\n", "# And its attention mask (simply differentiates padding from non-padding).", "\n", "attention_masks", ".", "append", "(", "encoded_dict", "[", "'attention_mask'", "]", ")", "\n", "labels", ".", "append", "(", "sentence_train_label", "[", "i", "]", ")", "\n", "# index_list.append(i)", "\n", "", "except", ":", "\n", "            ", "pass", "\n", "#print(sent)", "\n", "\n", "# Convert the lists into tensors.", "\n", "", "", "input_ids", "=", "torch", ".", "cat", "(", "input_ids", ",", "dim", "=", "0", ")", ".", "to", "(", "device", ")", "\n", "attention_masks", "=", "torch", ".", "cat", "(", "attention_masks", ",", "dim", "=", "0", ")", ".", "to", "(", "device", ")", "\n", "labels", "=", "torch", ".", "tensor", "(", "labels", ",", "device", "=", "'cuda'", ")", "\n", "e1_pos", "=", "torch", ".", "tensor", "(", "e1_pos", ",", "device", "=", "'cuda'", ")", "\n", "e2_pos", "=", "torch", ".", "tensor", "(", "e2_pos", ",", "device", "=", "'cuda'", ")", "\n", "# index_list = torch.tensor(index_list, device='cuda')", "\n", "# w = torch.ones(len(e1_pos), device='cuda')", "\n", "\n", "# Combine the training inputs into a TensorDataset.", "\n", "# train_dataset = TensorDataset(input_ids, attention_masks, labels, e1_pos, e2_pos, w)", "\n", "train_dataset", "=", "TensorDataset", "(", "input_ids", ",", "attention_masks", ",", "labels", ",", "e1_pos", ",", "e2_pos", ")", "\n", "\n", "return", "train_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.stratified_sample": [[172, 188], ["range", "data_dict.values", "len", "data_dict[].append", "random.shuffle", "torch.utils.data.Subset", "torch.utils.data.Subset", "data_dict.get", "[].item", "int", "int", "int", "[].item", "[].item", "len", "len", "len"], "function", ["None"], ["", "def", "stratified_sample", "(", "dataset", ",", "ratio", ")", ":", "\n", "    ", "data_dict", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "dataset", ")", ")", ":", "\n", "        ", "if", "not", "data_dict", ".", "get", "(", "dataset", "[", "i", "]", "[", "2", "]", ".", "item", "(", ")", ")", ":", "\n", "            ", "data_dict", "[", "dataset", "[", "i", "]", "[", "2", "]", ".", "item", "(", ")", "]", "=", "[", "]", "\n", "", "data_dict", "[", "dataset", "[", "i", "]", "[", "2", "]", ".", "item", "(", ")", "]", ".", "append", "(", "i", ")", "\n", "", "sampled_indices", "=", "[", "]", "\n", "rest_indices", "=", "[", "]", "\n", "for", "indices", "in", "data_dict", ".", "values", "(", ")", ":", "\n", "        ", "random", ".", "shuffle", "(", "indices", ")", "\n", "sampled_indices", "+=", "indices", "[", "0", ":", "int", "(", "len", "(", "indices", ")", "*", "ratio", ")", "]", "\n", "rest_indices", "+=", "indices", "[", "int", "(", "len", "(", "indices", ")", "*", "ratio", ")", ":", "int", "(", "len", "(", "indices", ")", "*", "(", "ratio", "+", "args", ".", "unlabel_of_train", ")", ")", "]", "\n", "# print(\"****************************************************\")", "\n", "# print(sampled_indices)", "\n", "# print(\"****************************************************\")", "\n", "", "return", "[", "Subset", "(", "dataset", ",", "sampled_indices", ")", ",", "Subset", "(", "dataset", ",", "rest_indices", ")", ",", "sampled_indices", ",", "rest_indices", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.add_space": [[190, 198], ["sentence_train_new.append", "sentence.replace().replace().replace().replace", "sentence.replace().replace().replace", "sentence.replace().replace", "sentence.replace"], "function", ["None"], ["", "def", "add_space", "(", "sentence_train", ",", "dataset_name", ")", ":", "\n", "    ", "if", "dataset_name", "==", "\"tacred\"", ":", "\n", "        ", "return", "sentence_train", "\n", "", "else", ":", "\n", "        ", "sentence_train_new", "=", "[", "]", "\n", "for", "sentence", "in", "sentence_train", ":", "\n", "            ", "sentence_train_new", ".", "append", "(", "sentence", ".", "replace", "(", "\"<e1>\"", ",", "\"<e1> \"", ")", ".", "replace", "(", "\"</e1>\"", ",", "\" </e1>\"", ")", ".", "replace", "(", "\"<e2>\"", ",", "\"<e2> \"", ")", ".", "replace", "(", "\"</e2>\"", ",", "\" </e2>\"", ")", ")", "\n", "", "return", "sentence_train_new", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.aug_data": [[200, 259], ["json.load", "range", "open", "len", "range", "mask_words_predict.get_enhance_result", "batch_aug_texts.append", "batch_aug_texts_ids.append", "range", "mask_words_predict.get_enhance_result", "batch_aug_texts.append", "batch_aug_texts_ids.append", "range", "range", "mask_words_predict.get_enhance_result", "batch_aug_texts.append", "batch_aug_texts_ids.append", "mask_words_predict.get_enhance_result", "batch_aug_texts.append", "batch_aug_texts_ids.append", "range", "mask_words_predict.get_enhance_result", "batch_aug_texts.append", "batch_aug_texts_ids.append", "len", "mask_words_predict.get_enhance_result", "batch_aug_texts.append", "batch_aug_texts_ids.append", "range", "range", "len", "mask_words_predict.get_enhance_result", "batch_aug_texts.append", "batch_aug_texts_ids.append", "mask_words_predict.get_enhance_result", "batch_aug_texts.append", "batch_aug_texts_ids.append", "len"], "function", ["home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_enhance_result", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_enhance_result", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_enhance_result", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_enhance_result", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_enhance_result", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_enhance_result", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_enhance_result", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_enhance_result"], ["", "", "def", "aug_data", "(", "sentence_train", ",", "sentence_train_label", ",", "b_index_list", ",", "masked_model", ")", ":", "\n", "    ", "batch_aug_texts", "=", "[", "]", "\n", "batch_aug_texts_ids", "=", "[", "]", "\n", "relation2id", "=", "json", ".", "load", "(", "open", "(", "'../data/'", "+", "args", ".", "dataset", "+", "'/relation2id.json'", ",", "'r'", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "b_index_list", ")", ")", ":", "\n", "        ", "enhance_result", "=", "[", "]", "\n", "cls_dist", "=", "0.0", "\n", "if", "args", ".", "label_of_train", "==", "0.05", ":", "\n", "            ", "for", "j", "in", "range", "(", "10", ")", ":", "\n", "                ", "enhance_result", ",", "cls_dist", "=", "get_enhance_result", "(", "sentence_train", "[", "b_index_list", "[", "i", "]", "]", ",", "masked_model", ")", "\n", "aug_text", "=", "\" \"", ".", "join", "(", "enhance_result", ")", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "batch_aug_texts", ".", "append", "(", "aug_text", ")", "\n", "batch_aug_texts_ids", ".", "append", "(", "sentence_train_label", "[", "b_index_list", "[", "i", "]", "]", ")", "\n", "", "", "elif", "args", ".", "label_of_train", "==", "0.10", ":", "\n", "            ", "for", "j", "in", "range", "(", "5", ")", ":", "\n", "                ", "enhance_result", ",", "cls_dist", "=", "get_enhance_result", "(", "sentence_train", "[", "b_index_list", "[", "i", "]", "]", ",", "masked_model", ")", "\n", "aug_text", "=", "\" \"", ".", "join", "(", "enhance_result", ")", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "batch_aug_texts", ".", "append", "(", "aug_text", ")", "\n", "batch_aug_texts_ids", ".", "append", "(", "sentence_train_label", "[", "b_index_list", "[", "i", "]", "]", ")", "\n", "", "", "elif", "args", ".", "label_of_train", "==", "0.15", ":", "\n", "            ", "if", "i", "<", "(", "2", "*", "len", "(", "b_index_list", ")", ")", "/", "3", ":", "\n", "                ", "for", "j", "in", "range", "(", "4", ")", ":", "\n", "                    ", "enhance_result", ",", "cls_dist", "=", "get_enhance_result", "(", "sentence_train", "[", "b_index_list", "[", "i", "]", "]", ",", "masked_model", ")", "\n", "aug_text", "=", "\" \"", ".", "join", "(", "enhance_result", ")", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "batch_aug_texts", ".", "append", "(", "aug_text", ")", "\n", "batch_aug_texts_ids", ".", "append", "(", "sentence_train_label", "[", "b_index_list", "[", "i", "]", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "for", "j", "in", "range", "(", "2", ")", ":", "\n", "                    ", "enhance_result", ",", "cls_dist", "=", "get_enhance_result", "(", "sentence_train", "[", "b_index_list", "[", "i", "]", "]", ",", "masked_model", ")", "\n", "aug_text", "=", "\" \"", ".", "join", "(", "enhance_result", ")", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "batch_aug_texts", ".", "append", "(", "aug_text", ")", "\n", "batch_aug_texts_ids", ".", "append", "(", "sentence_train_label", "[", "b_index_list", "[", "i", "]", "]", ")", "\n", "", "", "", "elif", "args", ".", "label_of_train", "==", "0.30", ":", "\n", "            ", "if", "i", "<", "(", "2", "*", "len", "(", "b_index_list", ")", ")", "/", "3", ":", "\n", "                ", "for", "j", "in", "range", "(", "2", ")", ":", "\n", "                    ", "enhance_result", ",", "cls_dist", "=", "get_enhance_result", "(", "sentence_train", "[", "b_index_list", "[", "i", "]", "]", ",", "masked_model", ")", "\n", "aug_text", "=", "\" \"", ".", "join", "(", "enhance_result", ")", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "batch_aug_texts", ".", "append", "(", "aug_text", ")", "\n", "batch_aug_texts_ids", ".", "append", "(", "sentence_train_label", "[", "b_index_list", "[", "i", "]", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "enhance_result", ",", "cls_dist", "=", "get_enhance_result", "(", "sentence_train", "[", "b_index_list", "[", "i", "]", "]", ",", "masked_model", ")", "\n", "aug_text", "=", "\" \"", ".", "join", "(", "enhance_result", ")", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "batch_aug_texts", ".", "append", "(", "aug_text", ")", "\n", "batch_aug_texts_ids", ".", "append", "(", "sentence_train_label", "[", "b_index_list", "[", "i", "]", "]", ")", "\n", "", "", "elif", "args", ".", "label_of_train", "==", "0.03", ":", "\n", "            ", "if", "i", "<", "(", "2", "*", "len", "(", "b_index_list", ")", ")", "/", "3", ":", "\n", "                ", "for", "j", "in", "range", "(", "20", ")", ":", "\n", "                    ", "enhance_result", ",", "cls_dist", "=", "get_enhance_result", "(", "sentence_train", "[", "b_index_list", "[", "i", "]", "]", ",", "masked_model", ")", "\n", "aug_text", "=", "\" \"", ".", "join", "(", "enhance_result", ")", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "batch_aug_texts", ".", "append", "(", "aug_text", ")", "\n", "batch_aug_texts_ids", ".", "append", "(", "sentence_train_label", "[", "b_index_list", "[", "i", "]", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "for", "j", "in", "range", "(", "10", ")", ":", "\n", "                    ", "enhance_result", ",", "cls_dist", "=", "get_enhance_result", "(", "sentence_train", "[", "b_index_list", "[", "i", "]", "]", ",", "masked_model", ")", "\n", "aug_text", "=", "\" \"", ".", "join", "(", "enhance_result", ")", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "batch_aug_texts", ".", "append", "(", "aug_text", ")", "\n", "batch_aug_texts_ids", ".", "append", "(", "sentence_train_label", "[", "b_index_list", "[", "i", "]", "]", ")", "\n", "\n", "", "", "", "", "return", "batch_aug_texts", ",", "batch_aug_texts_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.cos_dist": [[262, 264], ["numpy.dot", "numpy.linalg.norm", "numpy.linalg.norm"], "function", ["None"], ["", "def", "cos_dist", "(", "x", ",", "y", ")", ":", "\n", "    ", "return", "np", ".", "dot", "(", "x", ",", "y", ")", "/", "(", "np", ".", "linalg", ".", "norm", "(", "x", ")", "*", "np", ".", "linalg", ".", "norm", "(", "y", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.main": [[267, 968], ["json.load", "json.load", "train.add_space", "train.pre_processing", "torch.nn.CrossEntropyLoss", "train.stratified_sample", "print", "print", "range", "torch.utils.data.DataLoader", "range", "json.load", "json.load", "train.pre_processing", "torch.utils.data.DataLoader", "networks.LabelGeneration.from_pretrained", "torch.nn.DataParallel", "model_teacher.to.to", "transformers.AdamW", "range", "transformers.get_linear_schedule_with_warmup", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "time.time", "range", "range", "numpy.array", "numpy.mean", "range", "open", "open", "len", "len", "print", "print", "print", "json.load", "cbert_finetune.train_mlm", "print", "print", "print", "train.aug_data", "train.pre_processing", "train.stratified_sample", "unlabeled_dataset_list.append", "sample_index_list.append", "torch.utils.data.DataLoader", "unlabeled_dataloader_list.append", "open", "open", "model_teacher.to.parameters", "len", "len", "len", "range", "print", "print", "time.time", "model_teacher.to.train", "numpy.array", "numpy.array", "enumerate", "numpy.array", "np.array.append", "print", "print", "time.time", "model_teacher.to.eval", "numpy.array", "numpy.array", "numpy.array", "train.format_time", "train.score", "print", "print", "print", "print", "time.time", "model_teacher.to.eval", "numpy.array", "numpy.array", "train.format_time", "train.score", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "print", "print", "print", "print", "time.time", "model_teacher.to.train", "numpy.array", "enumerate", "train.format_time", "print", "print", "print", "print", "time.time", "model_teacher.to.eval", "numpy.array", "numpy.array", "numpy.array", "train.format_time", "train.score", "print", "print", "print", "time.time", "model_teacher.to.eval", "numpy.array", "numpy.array", "train.format_time", "train.score", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.DataLoader", "print", "print", "print", "time.time", "model_teacher.to.train", "numpy.array", "numpy.array", "enumerate", "train.format_time", "print", "print", "print", "print", "time.time", "model_teacher.to.eval", "numpy.array", "numpy.array", "numpy.array", "train.format_time", "train.score", "open", "sentence_train[].replace", "train_examples.append", "torch.utils.data.RandomSampler", "torch.utils.data.SequentialSampler", "len", "numpy.array", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "model_teacher.to.zero_grad", "model_teacher.to.", "nn.CrossEntropyLoss.", "criterion.sum().item", "criterion.sum().backward", "model_teacher.to.parameters", "np.array.append", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "logits.detach().cpu().numpy.detach().cpu().numpy", "torch.tensor().to.to().numpy", "b_labels.to().numpy.flatten", "numpy.concatenate", "numpy.mean", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "nn.CrossEntropyLoss.", "criterion.sum().item", "logits.detach().cpu().numpy.detach().cpu().numpy", "torch.tensor().to.to().numpy", "numpy.argmax().flatten", "b_labels.to().numpy.flatten", "numpy.concatenate", "numpy.concatenate", "len", "len", "train.cos_dist", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "np.concatenate.append", "nn.CrossEntropyLoss.", "criterion.sum().item", "logits.detach().cpu().numpy.detach().cpu().numpy", "torch.tensor().to.to().numpy", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "numpy.argmax().flatten", "b_labels.to().numpy.flatten", "numpy.concatenate", "numpy.concatenate", "len", "len", "range", "model_teacher.to.zero_grad", "sum", "sum.sum().item", "sum.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "len", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "nn.CrossEntropyLoss.", "criterion.sum().item", "logits.detach().cpu().numpy.detach().cpu().numpy", "torch.tensor().to.to().numpy", "numpy.argmax().flatten", "b_labels.to().numpy.flatten", "numpy.concatenate", "numpy.concatenate", "len", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "np.concatenate.append", "nn.CrossEntropyLoss.", "criterion.sum().item", "logits.detach().cpu().numpy.detach().cpu().numpy", "torch.tensor().to.to().numpy", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "numpy.argmax().flatten", "b_labels.to().numpy.flatten", "numpy.concatenate", "numpy.concatenate", "len", "torch.utils.data.TensorDataset", "len", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "model_teacher.to.zero_grad", "model_teacher.to.", "nn.CrossEntropyLoss.", "criterion.sum().item", "criterion.sum().backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "logits.detach().cpu().numpy.detach().cpu().numpy", "torch.tensor().to.to().numpy", "b_labels.to().numpy.flatten", "numpy.concatenate", "len", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "nn.CrossEntropyLoss.", "criterion.sum().item", "logits.detach().cpu().numpy.detach().cpu().numpy", "torch.tensor().to.to().numpy", "numpy.argmax().flatten", "b_labels.to().numpy.flatten", "numpy.concatenate", "numpy.concatenate", "len", "list", "torch.utils.data.RandomSampler", "train.format_time", "logits.detach().cpu().numpy.view", "batch[].view", "model_teacher.to.parameters", "len", "numpy.concatenate", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model_teacher.to.", "logits.detach().cpu().numpy.view", "batch[].view", "len", "numpy.concatenate", "time.time", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model_teacher.to.", "logits.detach().cpu().numpy.detach", "logits.detach().cpu().numpy.view", "batch[].view", "time.time", "torch.utils.data.SequentialSampler", "len", "train.format_time", "len", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "model_teacher.to.zero_grad", "model_teacher.to.", "nn.CrossEntropyLoss.", "criterion.sum().backward", "model_teacher.to.parameters", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.softmax", "torch.argmax", "torch.argmax", "torch.argmax", "policy_loss.append", "model_teacher.to.parameters", "time.time", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model_teacher.to.", "logits.detach().cpu().numpy.view", "batch[].view", "len", "numpy.concatenate", "time.time", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model_teacher.to.", "logits.detach().cpu().numpy.detach", "logits.detach().cpu().numpy.view", "batch[].view", "time.time", "torch.utils.data.RandomSampler", "train.format_time", "logits.detach().cpu().numpy.view", "batch[].long().view", "model_teacher.to.parameters", "len", "numpy.concatenate", "time.time", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model_teacher.to.", "logits.detach().cpu().numpy.view", "batch[].view", "len", "numpy.concatenate", "time.time", "json.load.keys", "list().index", "criterion.sum", "criterion.sum", "param.grad.cpu().numpy().flatten", "logits.detach().cpu().numpy.detach().cpu", "torch.tensor().to.to", "criterion.sum", "logits.detach().cpu().numpy.detach().cpu", "torch.tensor().to.to", "numpy.argmax", "criterion.sum", "logits.detach().cpu().numpy.detach().cpu", "torch.tensor().to.to", "numpy.argmax", "logits.detach().cpu().numpy.view", "torch.tensor().to.long().view", "model_teacher.to.parameters", "sum.sum", "criterion.sum", "logits.detach().cpu().numpy.detach().cpu", "torch.tensor().to.to", "numpy.argmax", "criterion.sum", "logits.detach().cpu().numpy.detach().cpu", "torch.tensor().to.to", "numpy.argmax", "criterion.sum", "criterion.sum", "logits.detach().cpu().numpy.detach().cpu", "torch.tensor().to.to", "criterion.sum", "logits.detach().cpu().numpy.detach().cpu", "torch.tensor().to.to", "numpy.argmax", "time.time", "len", "param.data.cpu().numpy().flatten", "param_x.append", "param_y.append", "numpy.concatenate", "range", "range", "time.time", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "criterion.sum", "param.grad.cpu().numpy().flatten", "criterion.sum", "time.time", "batch[].long", "list", "param.grad.cpu().numpy", "logits.detach().cpu().numpy.detach", "logits.detach().cpu().numpy.detach", "logits.detach().cpu().numpy.detach", "len", "torch.tensor().to.long", "len", "train.cos_dist", "rewards.append", "logits.detach().cpu().numpy.detach", "logits.detach().cpu().numpy.detach", "logits.detach().cpu().numpy.detach", "logits.detach().cpu().numpy.detach", "json.load.values", "param.data.cpu().numpy", "[].cpu().numpy", "[].cpu().numpy", "param.grad.cpu().numpy", "param.grad.cpu", "param.data.cpu", "[].cpu", "[].cpu", "param.grad.cpu", "len", "len"], "function", ["home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.add_space", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.pre_processing", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.stratified_sample", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.pre_processing", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.cbert_finetune.train_mlm", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.aug_data", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.pre_processing", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.stratified_sample", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.format_time", "home.repos.pwc.inspect_result.thu-bpm_gradlre.data.data_analysis.score", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.format_time", "home.repos.pwc.inspect_result.thu-bpm_gradlre.data.data_analysis.score", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.format_time", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.format_time", "home.repos.pwc.inspect_result.thu-bpm_gradlre.data.data_analysis.score", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.format_time", "home.repos.pwc.inspect_result.thu-bpm_gradlre.data.data_analysis.score", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.format_time", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.format_time", "home.repos.pwc.inspect_result.thu-bpm_gradlre.data.data_analysis.score", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.cos_dist", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.format_time", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.format_time", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.train.format_time", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.cos_dist"], ["", "def", "main", "(", "argv", "=", "None", ")", ":", "\n", "# Load the dataset.", "\n", "    ", "sentence_train", "=", "json", ".", "load", "(", "open", "(", "'../data/'", "+", "args", ".", "dataset", "+", "'/train_sentence.json'", ",", "'r'", ")", ")", "\n", "sentence_train_label", "=", "json", ".", "load", "(", "open", "(", "'../data/'", "+", "args", ".", "dataset", "+", "'/train_label_id.json'", ",", "'r'", ")", ")", "\n", "sentence_train", "=", "add_space", "(", "sentence_train", ",", "args", ".", "dataset", ")", "\n", "\n", "train_dataset", "=", "pre_processing", "(", "sentence_train", ",", "sentence_train_label", ")", "\n", "\n", "# define the loss function ", "\n", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n", "# split training data to labeled set and unlabeled set", "\n", "# labeled_dataset, unlabeled_dataset_total = random_split(train_dataset, [int(LABEL_OF_TRAIN * len(train_dataset)),", "\n", "#                                                                         len(train_dataset) -", "\n", "#                                                                         int(LABEL_OF_TRAIN * len(train_dataset))])", "\n", "labeled_dataset", ",", "unlabeled_dataset_total", ",", "labeled_indices", ",", "unlabeled_indices", "=", "stratified_sample", "(", "train_dataset", ",", "args", ".", "label_of_train", ")", "\n", "print", "(", "len", "(", "labeled_dataset", ")", ")", "\n", "print", "(", "len", "(", "unlabeled_dataset_total", ")", ")", "\n", "\n", "if", "args", ".", "use_aug", ":", "\n", "        ", "print", "(", "\"##################################\"", ")", "\n", "print", "(", "\"fine tune MLM model with labeled data\"", ")", "\n", "print", "(", "\"##################################\"", ")", "\n", "relation2id", "=", "json", ".", "load", "(", "open", "(", "'../data/'", "+", "args", ".", "dataset", "+", "'/relation2id.json'", ",", "'r'", ")", ")", "\n", "train_examples", "=", "[", "]", "\n", "for", "index", "in", "labeled_indices", ":", "\n", "            ", "relation", "=", "list", "(", "relation2id", ".", "keys", "(", ")", ")", "[", "list", "(", "relation2id", ".", "values", "(", ")", ")", ".", "index", "(", "sentence_train_label", "[", "index", "]", ")", "]", "\n", "sentence_relation", "=", "sentence_train", "[", "index", "]", ".", "replace", "(", "\"[CLS]\"", ",", "\"[CLS] \"", "+", "relation", "+", "\" [SEP]\"", ")", "\n", "train_examples", ".", "append", "(", "sentence_relation", ")", "\n", "", "masked_model", "=", "cbert_finetune", ".", "train_mlm", "(", "train_examples", ")", "\n", "\n", "print", "(", "\"##################################\"", ")", "\n", "print", "(", "\"perform augmentation for labeled data...\"", ")", "\n", "print", "(", "\"##################################\"", ")", "\n", "sentence_labeled_aug", ",", "sentence_labeled_aug_labels", "=", "aug_data", "(", "sentence_train", ",", "sentence_train_label", ",", "labeled_indices", ",", "masked_model", ")", "\n", "unlabeled_dataset_total", "=", "pre_processing", "(", "sentence_labeled_aug", ",", "sentence_labeled_aug_labels", ")", "\n", "\n", "", "unlabeled_dataset_list", "=", "[", "]", "\n", "sample_index_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "args", ".", "total_epochs", ")", ":", "\n", "        ", "unlabeled_dataset_now", ",", "_", ",", "index", ",", "_", "=", "stratified_sample", "(", "unlabeled_dataset_total", ",", "args", ".", "unlabel_of_train", "*", "2", "/", "args", ".", "total_epochs", ")", "\n", "unlabeled_dataset_list", ".", "append", "(", "unlabeled_dataset_now", ")", "\n", "sample_index_list", ".", "append", "(", "index", ")", "\n", "\n", "# Create the DataLoaders for our label and unlabel sets.", "\n", "", "labeled_dataloader", "=", "DataLoader", "(", "\n", "labeled_dataset", ",", "# The training samples.", "\n", "sampler", "=", "RandomSampler", "(", "labeled_dataset", ")", ",", "# Select batches randomly", "\n", "batch_size", "=", "args", ".", "batch_size", "# Trains with this batch size.", "\n", ")", "\n", "\n", "unlabeled_dataloader_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "args", ".", "total_epochs", ")", ":", "\n", "        ", "unlabeled_dataloader_now", "=", "DataLoader", "(", "\n", "unlabeled_dataset_list", "[", "i", "]", ",", "# The training samples.", "\n", "sampler", "=", "RandomSampler", "(", "unlabeled_dataset_list", "[", "i", "]", ")", ",", "# Select batches randomly", "\n", "batch_size", "=", "args", ".", "batch_size", "# Trains with this batch size.", "\n", ")", "\n", "unlabeled_dataloader_list", ".", "append", "(", "unlabeled_dataloader_now", ")", "\n", "\n", "", "sentence_val", "=", "json", ".", "load", "(", "open", "(", "'../data/'", "+", "args", ".", "dataset", "+", "'/test_sentence.json'", ",", "'r'", ")", ")", "\n", "sentence_val_label", "=", "json", ".", "load", "(", "open", "(", "'../data/'", "+", "args", ".", "dataset", "+", "'/test_label_id.json'", ",", "'r'", ")", ")", "\n", "val_dataset", "=", "pre_processing", "(", "sentence_val", ",", "sentence_val_label", ")", "\n", "\n", "validation_dataloader", "=", "DataLoader", "(", "\n", "val_dataset", ",", "# The validation samples.", "\n", "sampler", "=", "SequentialSampler", "(", "val_dataset", ")", ",", "# Pull out batches sequentially.", "\n", "batch_size", "=", "args", ".", "batch_size", "# Evaluate with this batch size.", "\n", ")", "\n", "# Load models", "\n", "model_teacher", "=", "LabelGeneration", ".", "from_pretrained", "(", "\n", "\"bert-base-uncased\"", ",", "# Use the 12-layer BERT model, with an uncased vocab.", "\n", "num_labels", "=", "args", ".", "num_labels", ",", "# The number of output labels--2 for binary classification.", "\n", "# You can increase this for multi-class tasks.", "\n", "output_attentions", "=", "False", ",", "# Whether the model returns attentions weights.", "\n", "output_hidden_states", "=", "False", ",", "# Whether the model returns all hidden-states.", "\n", ")", "\n", "model_teacher", "=", "nn", ".", "DataParallel", "(", "model_teacher", ")", "\n", "model_teacher", "=", "model_teacher", ".", "to", "(", "device", ")", "\n", "\n", "optimizer", "=", "AdamW", "(", "model_teacher", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "args", ".", "initial_lr", ",", "# args.learning_rate - default is 5e-5, our notebook had 2e-5", "\n", "eps", "=", "args", ".", "initial_eps", "# args.adam_epsilon  - default is 1e-8.", "\n", ")", "\n", "# Total number of training steps is [number of batches] x [number of epochs]. ", "\n", "# (Note that this is not the same as the number of training samples).", "\n", "# total_steps = len(labeled_dataloader) * EPOCHS", "\n", "total_steps1", "=", "len", "(", "labeled_dataloader", ")", "*", "(", "args", ".", "epochs", ")", "\n", "total_steps2", "=", "0", "\n", "for", "i", "in", "range", "(", "1", ",", "args", ".", "total_epochs", "+", "1", ")", ":", "\n", "        ", "total_steps2", "+=", "len", "(", "unlabeled_dataloader_list", "[", "i", "-", "1", "]", ")", "\n", "total_steps2", "+=", "len", "(", "labeled_dataloader", ")", "\n", "for", "j", "in", "range", "(", "i", ")", ":", "\n", "            ", "total_steps2", "+=", "len", "(", "unlabeled_dataloader_list", "[", "j", "]", ")", "\n", "# total_steps = len(labeled_dataloader) * EPOCHS + (1 * len(labeled_dataloader) + 1 * len(unlabeled_dataloader)) * TOTAL_EPOCHS", "\n", "", "", "total_steps", "=", "total_steps1", "+", "total_steps2", "\n", "# Create the learning rate scheduler.", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "\n", "num_warmup_steps", "=", "0", ",", "\n", "num_training_steps", "=", "total_steps", ")", "\n", "# Set the seed value all over the place to make this reproducible.", "\n", "random", ".", "seed", "(", "args", ".", "seed_val", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed_val", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed_val", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed_val", ")", "\n", "\n", "# validation accuracy, and timings.", "\n", "training_stats", "=", "[", "]", "\n", "# Measure the total training time for the whole run.", "\n", "total_t0", "=", "time", ".", "time", "(", ")", "\n", "\n", "# For each epoch...", "\n", "grad_vector", "=", "[", "]", "\n", "param_x", "=", "[", "]", "\n", "param_y", "=", "[", "]", "\n", "for", "epoch_i", "in", "range", "(", "0", ",", "args", ".", "epochs", ")", ":", "\n", "\n", "# ========================================", "\n", "#               Training", "\n", "# ========================================", "\n", "\n", "# Perform one full pass over the training set.", "\n", "        ", "print", "(", "'======== Epoch {:} / {:} ========'", ".", "format", "(", "epoch_i", "+", "1", ",", "args", ".", "epochs", ")", ")", "\n", "print", "(", "'Training...'", ")", "\n", "# Measure how long the training epoch takes.", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "# Reset the total loss for this epoch.", "\n", "total_train_loss", "=", "0", "\n", "# Put the model into training mode.", "\n", "model_teacher", ".", "train", "(", ")", "\n", "\n", "# save mixup features", "\n", "all_logits", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "\n", "# For each batch of training data...", "\n", "epoch_params", "=", "[", "]", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "labeled_dataloader", ")", ":", "\n", "            ", "batch_params", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "if", "step", "%", "40", "==", "0", "and", "not", "step", "==", "0", ":", "\n", "# Calculate elapsed time in minutes.", "\n", "                ", "elapsed", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "# Report progress.", "\n", "# print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(labeled_dataloader), elapsed))", "\n", "# Unpack this training batch from our dataloader.", "\n", "", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "# b_index_list = batch[5].to(device)", "\n", "\n", "model_teacher", ".", "zero_grad", "(", ")", "\n", "\n", "# Perform a forward pass (evaluate the model on this training batch)", "\n", "logits", ",", "_", "=", "model_teacher", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ")", "\n", "loss", "=", "criterion", "(", "logits", ".", "view", "(", "-", "1", ",", "args", ".", "num_labels", ")", ",", "batch", "[", "2", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "total_train_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Perform a backward pass to calculate the gradients.", "\n", "loss", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "for", "param", "in", "model_teacher", ".", "parameters", "(", ")", ":", "\n", "                ", "if", "param", ".", "grad", "is", "not", "None", ":", "\n", "                    ", "grad_nmp", "=", "param", ".", "grad", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "# print(len(grad_nmp))", "\n", "if", "len", "(", "grad_nmp", ")", "==", "args", ".", "num_labels", "*", "768", ":", "\n", "                        ", "param_data", "=", "param", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "# print(param_data[-2])", "\n", "# print(param_data[-1])", "\n", "param_x", ".", "append", "(", "param_data", "[", "-", "2", "]", ")", "\n", "param_y", ".", "append", "(", "param_data", "[", "-", "1", "]", ")", "\n", "batch_params", "=", "np", ".", "concatenate", "(", "(", "batch_params", ",", "grad_nmp", ")", ",", "axis", "=", "None", ")", "\n", "# print(\"the total param num is: \" + str(len(batch_params)))", "\n", "", "", "", "epoch_params", ".", "append", "(", "batch_params", ")", "\n", "# Clip the norm of the gradients to 1.0.", "\n", "# This is to help prevent the \"exploding gradients\" problem.", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model_teacher", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# Update the learning rate.", "\n", "scheduler", ".", "step", "(", ")", "\n", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "b_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "if", "len", "(", "all_logits", ")", "==", "0", ":", "\n", "                ", "all_logits", "=", "logits", "\n", "", "else", ":", "\n", "                ", "all_logits", "=", "np", ".", "concatenate", "(", "(", "all_logits", ",", "logits", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# compute grad cos similarity for each batch", "\n", "# for i in range(len(epoch_params)-1):", "\n", "#     print(cos_dist(epoch_params[i], epoch_params[i+1]))", "\n", "", "", "epoch_params", "=", "np", ".", "array", "(", "epoch_params", ")", "\n", "grad_vector", ".", "append", "(", "np", ".", "mean", "(", "epoch_params", ",", "axis", "=", "0", ")", ")", "\n", "\n", "# ========================================", "\n", "#               Validation", "\n", "# ========================================", "\n", "# After the completion of each training epoch, measure our performance on", "\n", "# our validation set.", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"Running Validation...\"", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "# Put the model in evaluation mode", "\n", "model_teacher", ".", "eval", "(", ")", "\n", "\n", "# Tracking variables", "\n", "total_eval_accuracy", "=", "0", "\n", "total_eval_loss", "=", "0", "\n", "nb_eval_steps", "=", "0", "\n", "all_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_logits", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "\n", "# Evaluate data for one epoch", "\n", "for", "batch", "in", "validation_dataloader", ":", "\n", "# Unpack this training batch from our dataloader.", "\n", "            ", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "(", "logits", ",", "_", ")", "=", "model_teacher", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ")", "\n", "", "loss", "=", "criterion", "(", "logits", ".", "view", "(", "-", "1", ",", "args", ".", "num_labels", ")", ",", "batch", "[", "2", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "# Accumulate the validation loss.", "\n", "total_eval_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Move logits and labels to CPU", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "b_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "\n", "pred_flat", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "all_prediction", "=", "np", ".", "concatenate", "(", "(", "all_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "if", "len", "(", "all_logits", ")", "==", "0", ":", "\n", "                ", "all_logits", "=", "logits", "\n", "", "else", ":", "\n", "                ", "all_logits", "=", "np", ".", "concatenate", "(", "(", "all_logits", ",", "logits", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "", "avg_val_loss", "=", "total_eval_loss", "/", "len", "(", "validation_dataloader", ")", "\n", "# Measure how long the validation run took.", "\n", "validation_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "score", "(", "all_ground_truth", ",", "all_prediction", ")", "\n", "\n", "# compute grad cos similarity for each epoch", "\n", "", "for", "i", "in", "range", "(", "len", "(", "grad_vector", ")", "-", "1", ")", ":", "\n", "        ", "print", "(", "cos_dist", "(", "grad_vector", "[", "i", "]", ",", "grad_vector", "[", "i", "+", "1", "]", ")", ")", "\n", "", "grad_vector", "=", "np", ".", "array", "(", "grad_vector", ")", "\n", "grad_vector_average", "=", "np", ".", "mean", "(", "grad_vector", ",", "axis", "=", "0", ")", "\n", "# grad_vector_average = grad_vector[-1]", "\n", "\n", "# # Displays final plot", "\n", "# plt.savefig(\"param_data_change.png\")", "\n", "\n", "train_dataloader", "=", "labeled_dataloader", "\n", "for", "epoch", "in", "range", "(", "0", ",", "args", ".", "total_epochs", ")", ":", "\n", "        ", "print", "(", "\"##################################\"", ")", "\n", "print", "(", "\"teacher model generate pseudo label\"", ")", "\n", "print", "(", "\"##################################\"", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "# Put the model in evaluation mode", "\n", "model_teacher", ".", "eval", "(", ")", "\n", "\n", "# Tracking variables", "\n", "total_pseudo_loss", "=", "0", "\n", "all_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_logits", "=", "[", "]", "\n", "\n", "input_ids", "=", "[", "]", "\n", "input_mask", "=", "[", "]", "\n", "gold_labels", "=", "[", "]", "\n", "e1_pos", "=", "[", "]", "\n", "e2_pos", "=", "[", "]", "\n", "\n", "# Evaluate data for one epoch", "\n", "for", "batch", "in", "unlabeled_dataloader_list", "[", "epoch", "]", ":", "\n", "# Unpack this training batch from our dataloader.", "\n", "            ", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "(", "logits", ",", "_", ")", "=", "model_teacher", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ")", "\n", "", "all_logits", ".", "append", "(", "logits", ".", "detach", "(", ")", ")", "\n", "loss", "=", "criterion", "(", "logits", ".", "view", "(", "-", "1", ",", "args", ".", "num_labels", ")", ",", "batch", "[", "2", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "# Accumulate the validation loss.", "\n", "total_pseudo_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Move logits and labels to CPU", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "b_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "\n", "input_ids", ".", "append", "(", "b_input_ids", ")", "\n", "input_mask", ".", "append", "(", "b_input_mask", ")", "\n", "gold_labels", ".", "append", "(", "b_labels", ")", "\n", "e1_pos", ".", "append", "(", "b_e1_pos", ")", "\n", "e2_pos", ".", "append", "(", "b_e2_pos", ")", "\n", "\n", "pred_flat", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "all_prediction", "=", "np", ".", "concatenate", "(", "(", "all_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "avg_pseudo_loss", "=", "total_pseudo_loss", "/", "len", "(", "unlabeled_dataloader_list", "[", "epoch", "]", ")", "\n", "# Measure how long the validation run took.", "\n", "generate_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "score", "(", "all_ground_truth", ",", "all_prediction", ")", "\n", "\n", "input_ids", "=", "torch", ".", "cat", "(", "input_ids", ",", "dim", "=", "0", ")", "\n", "input_mask", "=", "torch", ".", "cat", "(", "input_mask", ",", "dim", "=", "0", ")", "\n", "gold_labels", "=", "torch", ".", "cat", "(", "gold_labels", ",", "dim", "=", "0", ")", "\n", "e1_pos", "=", "torch", ".", "cat", "(", "e1_pos", ",", "dim", "=", "0", ")", "\n", "e2_pos", "=", "torch", ".", "cat", "(", "e2_pos", ",", "dim", "=", "0", ")", "\n", "\n", "# unlabeled_gold_labels = all_ground_truth", "\n", "# unlabeled_first_pseudo_labels = all_prediction", "\n", "\n", "pseudo_labels", "=", "torch", ".", "tensor", "(", "all_prediction", ",", "device", "=", "'cuda'", ")", "\n", "\n", "unlabeled_dataset_new", "=", "TensorDataset", "(", "input_ids", ",", "input_mask", ",", "pseudo_labels", ",", "e1_pos", ",", "e2_pos", ")", "\n", "unlabeled_dataloader_new", "=", "DataLoader", "(", "\n", "unlabeled_dataset_new", ",", "# The training samples.", "\n", "sampler", "=", "SequentialSampler", "(", "unlabeled_dataset_new", ")", ",", "# Select batches randomly", "\n", "batch_size", "=", "args", ".", "batch_size", "# Trains with this batch size.", "\n", ")", "\n", "print", "(", "len", "(", "unlabeled_dataset_new", ")", ")", "\n", "\n", "##########################################", "\n", "print", "(", "\"conduct the RL learning process...\"", ")", "\n", "##########################################", "\n", "\n", "# Perform one full pass over the training set.", "\n", "print", "(", "'======== Epoch {:} / {:} ========'", ".", "format", "(", "epoch", "+", "1", ",", "args", ".", "total_epochs", ")", ")", "\n", "print", "(", "'Training...'", ")", "\n", "# Measure how long the training epoch takes.", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "# Reset the total loss for this epoch.", "\n", "total_train_loss", "=", "0", "\n", "# Put the model into training mode.", "\n", "model_teacher", ".", "train", "(", ")", "\n", "\n", "# save mixup features", "\n", "all_logits", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "\n", "# For each batch of training data...", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "unlabeled_dataloader_new", ")", ":", "\n", "            ", "if", "(", "step", "+", "1", ")", "*", "args", ".", "batch_size", "<=", "len", "(", "all_ground_truth", ")", ":", "\n", "                ", "gold_labels_batch", "=", "[", "all_ground_truth", "[", "i", "]", "for", "i", "in", "range", "(", "step", "*", "args", ".", "batch_size", ",", "(", "step", "+", "1", ")", "*", "args", ".", "batch_size", ")", "]", "\n", "", "else", ":", "\n", "                ", "gold_labels_batch", "=", "[", "all_ground_truth", "[", "i", "]", "for", "i", "in", "range", "(", "step", "*", "args", ".", "batch_size", ",", "len", "(", "all_ground_truth", ")", ")", "]", "\n", "# model_teacher.zero_grad()", "\n", "", "saved_log_probs", "=", "[", "]", "\n", "rewards", "=", "[", "]", "\n", "policy_loss", "=", "[", "]", "\n", "R", "=", "0", "\n", "returns", "=", "[", "]", "\n", "if", "step", "%", "40", "==", "0", "and", "not", "step", "==", "0", ":", "\n", "# Calculate elapsed time in minutes.", "\n", "                ", "elapsed", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "# Report progress.", "\n", "# print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(labeled_dataloader), elapsed))", "\n", "\n", "# Unpack this training batch from our dataloader.", "\n", "", "for", "i", "in", "range", "(", "len", "(", "batch", "[", "0", "]", ")", ")", ":", "\n", "                ", "b_input_ids", "=", "torch", ".", "tensor", "(", "[", "batch", "[", "0", "]", "[", "i", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "]", ")", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "torch", ".", "tensor", "(", "[", "batch", "[", "1", "]", "[", "i", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "]", ")", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "torch", ".", "tensor", "(", "[", "batch", "[", "2", "]", "[", "i", "]", "]", ")", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "torch", ".", "tensor", "(", "[", "batch", "[", "3", "]", "[", "i", "]", "]", ")", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "torch", ".", "tensor", "(", "[", "batch", "[", "4", "]", "[", "i", "]", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "model_teacher", ".", "zero_grad", "(", ")", "\n", "\n", "# Perform a forward pass (evaluate the model on this training batch)", "\n", "logits", ",", "_", "=", "model_teacher", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ")", "\n", "loss", "=", "criterion", "(", "logits", ".", "view", "(", "-", "1", ",", "args", ".", "num_labels", ")", ",", "b_labels", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ")", "\n", "# Perform a backward pass to calculate the gradients.", "\n", "loss", ".", "sum", "(", ")", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "# Clip the norm of the gradients to 1.0.", "\n", "# This is to help prevent the \"exploding gradients\" problem.", "\n", "for", "param", "in", "model_teacher", ".", "parameters", "(", ")", ":", "\n", "                    ", "if", "param", ".", "grad", "is", "not", "None", ":", "\n", "                        ", "grad_nmp", "=", "param", ".", "grad", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "if", "len", "(", "grad_nmp", ")", "==", "args", ".", "num_labels", "*", "768", ":", "\n", "# grad_nmp_part = [grad_nmp[-3], grad_nmp[-2], grad_nmp[-1]]", "\n", "                            ", "reward", "=", "cos_dist", "(", "grad_vector_average", ",", "grad_nmp", ")", "\n", "if", "reward", ">=", "0.5", ":", "\n", "                                ", "grad_vector_average", "=", "(", "grad_vector_average", "*", "len", "(", "grad_vector", ")", "+", "grad_nmp", ")", "/", "len", "(", "grad_vector", ")", "+", "1", "\n", "", "if", "reward", ">", "0", ":", "\n", "                                ", "reward", "+=", "1", "\n", "", "else", ":", "\n", "# reward = 0", "\n", "                                ", "reward", "+=", "0.5", "\n", "# print(reward)", "\n", "", "rewards", ".", "append", "(", "reward", ")", "\n", "", "", "", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model_teacher", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "probs", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", "\n", "action", "=", "torch", ".", "argmax", "(", "probs", ")", "\n", "# m = Categorical(probs)", "\n", "# action = m.sample()", "\n", "# saved_log_probs.append(m.log_prob(action))", "\n", "# policy_loss.append(-m.log_prob(action) * reward)", "\n", "policy_loss", ".", "append", "(", "loss", ".", "sum", "(", ")", "*", "reward", ")", "\n", "\n", "", "model_teacher", ".", "zero_grad", "(", ")", "\n", "# loss_rl = torch.cat(policy_loss).sum()", "\n", "loss_rl", "=", "sum", "(", "policy_loss", ")", "\n", "total_train_loss", "+=", "loss_rl", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "loss_rl", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model_teacher", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "# Update the learning rate.", "\n", "scheduler", ".", "step", "(", ")", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "avg_train_loss", "=", "total_train_loss", "/", "len", "(", "unlabeled_dataloader_new", ")", "\n", "# Measure how long this epoch took.", "\n", "training_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "\n", "print", "(", "\"  Average training loss: {0:.2f}\"", ".", "format", "(", "avg_train_loss", ")", ")", "\n", "print", "(", "\"  Training epcoh took: {:}\"", ".", "format", "(", "training_time", ")", ")", "\n", "\n", "# ========================================", "\n", "#               Validation", "\n", "# ========================================", "\n", "# After the completion of each training epoch, measure our performance on", "\n", "# our validation set.", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"Running Validation...\"", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "# Put the model in evaluation mode", "\n", "model_teacher", ".", "eval", "(", ")", "\n", "\n", "# Tracking variables", "\n", "total_eval_accuracy", "=", "0", "\n", "total_eval_loss", "=", "0", "\n", "nb_eval_steps", "=", "0", "\n", "all_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_logits", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "\n", "# Evaluate data for one epoch", "\n", "for", "batch", "in", "validation_dataloader", ":", "\n", "# Unpack this training batch from our dataloader.", "\n", "            ", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "(", "logits", ",", "_", ")", "=", "model_teacher", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ")", "\n", "", "loss", "=", "criterion", "(", "logits", ".", "view", "(", "-", "1", ",", "args", ".", "num_labels", ")", ",", "batch", "[", "2", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "# Accumulate the validation loss.", "\n", "total_eval_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Move logits and labels to CPU", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "b_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "\n", "pred_flat", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "all_prediction", "=", "np", ".", "concatenate", "(", "(", "all_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "if", "len", "(", "all_logits", ")", "==", "0", ":", "\n", "                ", "all_logits", "=", "logits", "\n", "", "else", ":", "\n", "                ", "all_logits", "=", "np", ".", "concatenate", "(", "(", "all_logits", ",", "logits", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "", "avg_val_loss", "=", "total_eval_loss", "/", "len", "(", "validation_dataloader", ")", "\n", "# Measure how long the validation run took.", "\n", "validation_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "score", "(", "all_ground_truth", ",", "all_prediction", ")", "\n", "\n", "print", "(", "\"##################################\"", ")", "\n", "print", "(", "\"teacher model generate pseudo label\"", ")", "\n", "print", "(", "\"##################################\"", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "# Put the model in evaluation mode", "\n", "model_teacher", ".", "eval", "(", ")", "\n", "\n", "# Tracking variables", "\n", "total_pseudo_loss", "=", "0", "\n", "all_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_logits", "=", "[", "]", "\n", "\n", "input_ids", "=", "[", "]", "\n", "input_mask", "=", "[", "]", "\n", "gold_labels", "=", "[", "]", "\n", "e1_pos", "=", "[", "]", "\n", "e2_pos", "=", "[", "]", "\n", "\n", "# Evaluate data for one epoch", "\n", "for", "batch", "in", "unlabeled_dataloader_list", "[", "epoch", "]", ":", "\n", "# Unpack this training batch from our dataloader.", "\n", "            ", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "(", "logits", ",", "_", ")", "=", "model_teacher", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ")", "\n", "", "all_logits", ".", "append", "(", "logits", ".", "detach", "(", ")", ")", "\n", "loss", "=", "criterion", "(", "logits", ".", "view", "(", "-", "1", ",", "args", ".", "num_labels", ")", ",", "batch", "[", "2", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "# Accumulate the validation loss.", "\n", "total_pseudo_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Move logits and labels to CPU", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "b_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "\n", "input_ids", ".", "append", "(", "b_input_ids", ")", "\n", "input_mask", ".", "append", "(", "b_input_mask", ")", "\n", "gold_labels", ".", "append", "(", "b_labels", ")", "\n", "e1_pos", ".", "append", "(", "b_e1_pos", ")", "\n", "e2_pos", ".", "append", "(", "b_e2_pos", ")", "\n", "\n", "pred_flat", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "all_prediction", "=", "np", ".", "concatenate", "(", "(", "all_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "avg_pseudo_loss", "=", "total_pseudo_loss", "/", "len", "(", "unlabeled_dataloader_list", "[", "epoch", "]", ")", "\n", "# Measure how long the validation run took.", "\n", "generate_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "score", "(", "all_ground_truth", ",", "all_prediction", ")", "\n", "\n", "input_ids", "=", "torch", ".", "cat", "(", "input_ids", ",", "dim", "=", "0", ")", "\n", "input_mask", "=", "torch", ".", "cat", "(", "input_mask", ",", "dim", "=", "0", ")", "\n", "gold_labels", "=", "torch", ".", "cat", "(", "gold_labels", ",", "dim", "=", "0", ")", "\n", "e1_pos", "=", "torch", ".", "cat", "(", "e1_pos", ",", "dim", "=", "0", ")", "\n", "e2_pos", "=", "torch", ".", "cat", "(", "e2_pos", ",", "dim", "=", "0", ")", "\n", "\n", "# unlabeled_gold_labels = all_ground_truth", "\n", "# unlabeled_first_pseudo_labels = all_prediction", "\n", "\n", "pseudo_labels", "=", "torch", ".", "tensor", "(", "all_prediction", ",", "device", "=", "'cuda'", ")", "\n", "\n", "train_add_dataset", "=", "train_dataloader", ".", "dataset", "+", "TensorDataset", "(", "input_ids", ",", "input_mask", ",", "pseudo_labels", ",", "e1_pos", ",", "e2_pos", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "\n", "train_add_dataset", ",", "# The training samples.", "\n", "sampler", "=", "RandomSampler", "(", "train_add_dataset", ")", ",", "# Select batches randomly", "\n", "batch_size", "=", "args", ".", "batch_size", "# Trains with this batch size.", "\n", ")", "\n", "print", "(", "len", "(", "train_add_dataset", ")", ")", "\n", "\n", "# Perform one full pass over the training set.", "\n", "print", "(", "'======== Epoch {:} / {:} ========'", ".", "format", "(", "epoch", "+", "1", ",", "args", ".", "total_epochs", ")", ")", "\n", "print", "(", "'Training...'", ")", "\n", "# Measure how long the training epoch takes.", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "# Reset the total loss for this epoch.", "\n", "total_train_loss", "=", "0", "\n", "# Put the model into training mode.", "\n", "model_teacher", ".", "train", "(", ")", "\n", "\n", "# save mixup features", "\n", "all_logits", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "\n", "# For each batch of training data...", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "train_dataloader", ")", ":", "\n", "            ", "if", "step", "%", "40", "==", "0", "and", "not", "step", "==", "0", ":", "\n", "# Calculate elapsed time in minutes.", "\n", "                ", "elapsed", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "# Report progress.", "\n", "# print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(labeled_dataloader), elapsed))", "\n", "# Unpack this training batch from our dataloader.", "\n", "", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "\n", "model_teacher", ".", "zero_grad", "(", ")", "\n", "\n", "# Perform a forward pass (evaluate the model on this training batch)", "\n", "logits", ",", "_", "=", "model_teacher", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ")", "\n", "loss", "=", "criterion", "(", "logits", ".", "view", "(", "-", "1", ",", "args", ".", "num_labels", ")", ",", "batch", "[", "2", "]", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ")", "\n", "total_train_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Perform a backward pass to calculate the gradients.", "\n", "loss", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "# Clip the norm of the gradients to 1.0.", "\n", "# This is to help prevent the \"exploding gradients\" problem.", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model_teacher", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# Update the learning rate.", "\n", "scheduler", ".", "step", "(", ")", "\n", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "b_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "if", "len", "(", "all_logits", ")", "==", "0", ":", "\n", "                ", "all_logits", "=", "logits", "\n", "", "else", ":", "\n", "                ", "all_logits", "=", "np", ".", "concatenate", "(", "(", "all_logits", ",", "logits", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "", "avg_train_loss", "=", "total_train_loss", "/", "len", "(", "train_dataloader", ")", "\n", "# Measure how long this epoch took.", "\n", "training_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "\n", "print", "(", "\"  Average training loss: {0:.2f}\"", ".", "format", "(", "avg_train_loss", ")", ")", "\n", "print", "(", "\"  Training epcoh took: {:}\"", ".", "format", "(", "training_time", ")", ")", "\n", "\n", "# ========================================", "\n", "#               Validation", "\n", "# ========================================", "\n", "# After the completion of each training epoch, measure our performance on", "\n", "# our validation set.", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"Running Validation...\"", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "# Put the model in evaluation mode", "\n", "model_teacher", ".", "eval", "(", ")", "\n", "\n", "# Tracking variables", "\n", "total_eval_accuracy", "=", "0", "\n", "total_eval_loss", "=", "0", "\n", "nb_eval_steps", "=", "0", "\n", "all_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_logits", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "\n", "# Evaluate data for one epoch", "\n", "for", "batch", "in", "validation_dataloader", ":", "\n", "# Unpack this training batch from our dataloader.", "\n", "            ", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "(", "logits", ",", "_", ")", "=", "model_teacher", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ")", "\n", "", "loss", "=", "criterion", "(", "logits", ".", "view", "(", "-", "1", ",", "args", ".", "num_labels", ")", ",", "batch", "[", "2", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "# Accumulate the validation loss.", "\n", "total_eval_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Move logits and labels to CPU", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "b_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "\n", "pred_flat", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "all_prediction", "=", "np", ".", "concatenate", "(", "(", "all_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "if", "len", "(", "all_logits", ")", "==", "0", ":", "\n", "                ", "all_logits", "=", "logits", "\n", "", "else", ":", "\n", "                ", "all_logits", "=", "np", ".", "concatenate", "(", "(", "all_logits", ",", "logits", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "", "avg_val_loss", "=", "total_eval_loss", "/", "len", "(", "validation_dataloader", ")", "\n", "# Measure how long the validation run took.", "\n", "validation_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "score", "(", "all_ground_truth", ",", "all_prediction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.dist": [[39, 43], ["numpy.sqrt"], "function", ["None"], ["def", "dist", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\"calculate the cos distance between two vectors\n    \"\"\"", "\n", "return", "np", ".", "sqrt", "(", "(", "(", "x", "-", "y", ")", "**", "2", ")", ".", "sum", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.cos_dist": [[44, 46], ["numpy.dot", "numpy.linalg.norm", "numpy.linalg.norm"], "function", ["None"], ["", "def", "cos_dist", "(", "x", ",", "y", ")", ":", "\n", "    ", "return", "np", ".", "dot", "(", "x", ",", "y", ")", "/", "(", "np", ".", "linalg", ".", "norm", "(", "x", ")", "*", "np", ".", "linalg", ".", "norm", "(", "y", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_entity_pos": [[47, 76], ["range", "len", "pos1.append", "pos1.append", "len", "pos1.append", "pos1.append", "pos2.append", "pos2.append", "len", "pos2.append", "pos2.append"], "function", ["None"], ["", "def", "get_entity_pos", "(", "sentence_token_list", ")", ":", "\n", "    ", "pos1", "=", "[", "]", "\n", "pos2", "=", "[", "]", "\n", "e1_flag", "=", "0", "\n", "e2_flag", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "sentence_token_list", ")", ")", ":", "\n", "        ", "if", "sentence_token_list", "[", "i", "]", "==", "\">\"", "and", "sentence_token_list", "[", "i", "-", "1", "]", "==", "\"##1\"", "and", "sentence_token_list", "[", "i", "-", "3", "]", "==", "\"<\"", ":", "\n", "            ", "e1_flag", "=", "1", "\n", "if", "e1_flag", "==", "1", "and", "e2_flag", "==", "0", ":", "\n", "                ", "pos1", ".", "append", "(", "i", "+", "1", "-", "4", ")", "\n", "", "else", ":", "\n", "                ", "pos1", ".", "append", "(", "i", "+", "1", "-", "13", ")", "\n", "", "", "if", "sentence_token_list", "[", "i", "]", "==", "\"<\"", "and", "i", "+", "3", "<", "len", "(", "sentence_token_list", ")", "and", "sentence_token_list", "[", "i", "+", "1", "]", "==", "\"/\"", "and", "sentence_token_list", "[", "i", "+", "3", "]", "==", "\"##1\"", ":", "\n", "            ", "if", "e1_flag", "==", "1", "and", "e2_flag", "==", "0", ":", "\n", "                ", "pos1", ".", "append", "(", "i", "-", "1", "-", "4", ")", "\n", "", "else", ":", "\n", "                ", "pos1", ".", "append", "(", "i", "-", "1", "-", "13", ")", "\n", "", "", "if", "sentence_token_list", "[", "i", "]", "==", "\">\"", "and", "sentence_token_list", "[", "i", "-", "1", "]", "==", "\"##2\"", "and", "sentence_token_list", "[", "i", "-", "3", "]", "==", "\"<\"", ":", "\n", "            ", "e2_flag", "=", "1", "\n", "if", "e1_flag", "==", "1", "and", "e2_flag", "==", "1", ":", "\n", "                ", "pos2", ".", "append", "(", "i", "+", "1", "-", "13", ")", "\n", "", "else", ":", "\n", "                ", "pos2", ".", "append", "(", "i", "+", "1", "-", "4", ")", "\n", "", "", "if", "sentence_token_list", "[", "i", "]", "==", "\"<\"", "and", "i", "+", "3", "<", "len", "(", "sentence_token_list", ")", "and", "sentence_token_list", "[", "i", "+", "1", "]", "==", "\"/\"", "and", "sentence_token_list", "[", "i", "+", "3", "]", "==", "\"##2\"", ":", "\n", "            ", "if", "e1_flag", "==", "1", "and", "e2_flag", "==", "1", ":", "\n", "                ", "pos2", ".", "append", "(", "i", "-", "1", "-", "13", ")", "\n", "", "else", ":", "\n", "                ", "pos2", ".", "append", "(", "i", "-", "1", "-", "4", ")", "\n", "", "", "", "return", "pos1", ",", "pos2", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_random_mask_pos": [[77, 106], ["range", "max", "min", "set", "len", "int", "len", "len", "numpy.random.choice", "min", "numpy.random.choice", "sentence_token_list[].startswith", "range", "text_index_list.append", "len", "numpy.random.choice", "set.add", "range", "range", "len", "len", "len", "len"], "function", ["None"], ["", "def", "get_random_mask_pos", "(", "sentence_token_list", ",", "pos1", ",", "pos2", ",", "p", ")", ":", "\n", "# mask_start = sentence_token_list.index('[SEP]')", "\n", "    ", "mask_start", "=", "0", "\n", "text_index_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "mask_start", ",", "len", "(", "sentence_token_list", ")", ")", ":", "\n", "        ", "if", "sentence_token_list", "[", "i", "]", "not", "in", "stop_words", "and", "i", "not", "in", "range", "(", "pos1", "[", "0", "]", ",", "pos1", "[", "1", "]", "+", "1", ")", "and", "i", "not", "in", "range", "(", "pos2", "[", "0", "]", ",", "pos2", "[", "1", "]", "+", "1", ")", ":", "\n", "            ", "text_index_list", ".", "append", "(", "i", ")", "\n", "", "", "mask_num", "=", "max", "(", "1", ",", "int", "(", "p", "*", "len", "(", "sentence_token_list", ")", ")", ")", "# whether text_index_list or sentence_token_list ??", "\n", "mask_num", "=", "min", "(", "mask_num", ",", "len", "(", "text_index_list", ")", ")", "\n", "# print(\"#####\")", "\n", "# print(mask_num)", "\n", "# print(\"#####\")", "\n", "# print(text_index_list)", "\n", "random_mask_pos", "=", "set", "(", ")", "\n", "while", "len", "(", "random_mask_pos", ")", "<", "mask_num", ":", "\n", "        ", "span_len", "=", "np", ".", "random", ".", "choice", "(", "lens", ",", "p", "=", "len_distrib", ")", "\n", "# print(\"----------------\")", "\n", "# print(span_len)", "\n", "# print(\"----------------\")", "\n", "span_len", "=", "min", "(", "span_len", ",", "len", "(", "text_index_list", ")", ")", "\n", "start", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "text_index_list", ")", "-", "(", "span_len", "-", "1", ")", ")", "\n", "while", "sentence_token_list", "[", "text_index_list", "[", "start", "]", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "            ", "start", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "text_index_list", ")", "-", "(", "span_len", "-", "1", ")", ")", "\n", "", "for", "i", "in", "range", "(", "start", ",", "start", "+", "span_len", ")", ":", "\n", "            ", "if", "len", "(", "random_mask_pos", ")", ">=", "mask_num", ":", "\n", "                ", "break", "\n", "", "random_mask_pos", ".", "add", "(", "text_index_list", "[", "i", "]", ")", "\n", "# random_mask_pos = random.sample(text_index_list, mask_num)", "\n", "", "", "return", "random_mask_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.add_point": [[107, 111], ["sentence.split", "sentence.split.insert", "len"], "function", ["None"], ["", "def", "add_point", "(", "sentence", ")", ":", "\n", "    ", "words_list", "=", "sentence", ".", "split", "(", "\" \"", ")", "\n", "words_list", ".", "insert", "(", "len", "(", "words_list", ")", "-", "1", ",", "\".\"", ")", "\n", "return", "\" \"", ".", "join", "(", "words_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.add_entity_flag": [[112, 128], ["None"], "function", ["None"], ["", "def", "add_entity_flag", "(", "tokenized_text_without_mask", ",", "pos1", ",", "pos2", ")", ":", "\n", "    ", "e1_start", "=", "[", "'<e1>'", "]", "\n", "e1_end", "=", "[", "'</e1>'", "]", "\n", "e2_start", "=", "[", "'<e2>'", "]", "\n", "e2_end", "=", "[", "'</e2>'", "]", "\n", "if", "pos1", "[", "0", "]", "<", "pos2", "[", "0", "]", ":", "\n", "        ", "tokenized_text_without_mask", "=", "tokenized_text_without_mask", "[", ":", "pos1", "[", "0", "]", "]", "+", "e1_start", "+", "tokenized_text_without_mask", "[", "pos1", "[", "0", "]", ":", "]", "\n", "tokenized_text_without_mask", "=", "tokenized_text_without_mask", "[", ":", "pos1", "[", "1", "]", "+", "1", "+", "1", "]", "+", "e1_end", "+", "tokenized_text_without_mask", "[", "pos1", "[", "1", "]", "+", "1", "+", "1", ":", "]", "\n", "tokenized_text_without_mask", "=", "tokenized_text_without_mask", "[", ":", "pos2", "[", "0", "]", "+", "2", "]", "+", "e2_start", "+", "tokenized_text_without_mask", "[", "pos2", "[", "0", "]", "+", "2", ":", "]", "\n", "tokenized_text_without_mask", "=", "tokenized_text_without_mask", "[", ":", "pos2", "[", "1", "]", "+", "1", "+", "3", "]", "+", "e2_end", "+", "tokenized_text_without_mask", "[", "pos2", "[", "1", "]", "+", "1", "+", "3", ":", "]", "\n", "", "else", ":", "\n", "        ", "tokenized_text_without_mask", "=", "tokenized_text_without_mask", "[", ":", "pos2", "[", "0", "]", "]", "+", "e2_start", "+", "tokenized_text_without_mask", "[", "pos2", "[", "0", "]", ":", "]", "\n", "tokenized_text_without_mask", "=", "tokenized_text_without_mask", "[", ":", "pos2", "[", "1", "]", "+", "1", "+", "1", "]", "+", "e2_end", "+", "tokenized_text_without_mask", "[", "pos2", "[", "1", "]", "+", "1", "+", "1", ":", "]", "\n", "tokenized_text_without_mask", "=", "tokenized_text_without_mask", "[", ":", "pos1", "[", "0", "]", "+", "2", "]", "+", "e1_start", "+", "tokenized_text_without_mask", "[", "pos1", "[", "0", "]", "+", "2", ":", "]", "\n", "tokenized_text_without_mask", "=", "tokenized_text_without_mask", "[", ":", "pos1", "[", "1", "]", "+", "1", "+", "3", "]", "+", "e1_end", "+", "tokenized_text_without_mask", "[", "pos1", "[", "1", "]", "+", "1", "+", "3", ":", "]", "\n", "", "return", "tokenized_text_without_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_entity_dist": [[129, 175], ["range", "tokenizer.convert_tokens_to_ids", "torch.tensor", "torch.tensor", "tokens_tensor.to.to", "segments_tensors.to.to", "sequence_output[].mean", "range", "tokenizer.convert_tokens_to_ids", "torch.tensor", "torch.tensor", "tokens_tensor.to.to", "segments_tensors.to.to", "sequence_output[].mean", "mask_words_predict.cos_dist", "len", "torch.no_grad", "model", "len", "torch.no_grad", "model", "sequence_output[].mean.cpu", "sequence_output[].mean.cpu"], "function", ["home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.cos_dist"], ["", "def", "get_entity_dist", "(", "tokenized_text_without_mask", ",", "pos1", ",", "pos2", ")", ":", "\n", "# Mask Entity1", "\n", "    ", "for", "masked_index", "in", "range", "(", "pos1", "[", "0", "]", ",", "pos1", "[", "1", "]", "+", "1", ")", ":", "\n", "        ", "tokenized_text_without_mask", "[", "masked_index", "]", "=", "'[MASK]'", "\n", "# print(tokenized_text_without_mask)", "\n", "# Convert tokens into vocabulary indexes", "\n", "", "indexed_tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenized_text_without_mask", ")", "\n", "segments_ids", "=", "[", "0", "]", "*", "len", "(", "tokenized_text_without_mask", ")", "\n", "# Convert the input into Pytorch tensors", "\n", "tokens_tensor", "=", "torch", ".", "tensor", "(", "[", "indexed_tokens", "]", ")", "\n", "segments_tensors", "=", "torch", ".", "tensor", "(", "[", "segments_ids", "]", ")", "\n", "\n", "# put data to GPU", "\n", "tokens_tensor", "=", "tokens_tensor", ".", "to", "(", "device", ")", "\n", "segments_tensors", "=", "segments_tensors", ".", "to", "(", "device", ")", "\n", "\n", "# get emb_vector for entity1", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "outputs", "=", "model", "(", "tokens_tensor", ",", "segments_tensors", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "# print(sequence_output.shape)       # torch.Size([1, 17, 768])", "\n", "\n", "", "entity1_emb", "=", "sequence_output", "[", "0", ",", "pos1", "[", "0", "]", ":", "pos1", "[", "1", "]", "+", "1", "]", ".", "mean", "(", "0", ")", "\n", "\n", "# Mask Entity2", "\n", "for", "masked_index", "in", "range", "(", "pos2", "[", "0", "]", ",", "pos2", "[", "1", "]", "+", "1", ")", ":", "\n", "        ", "tokenized_text_without_mask", "[", "masked_index", "]", "=", "'[MASK]'", "\n", "# print(tokenized_text_without_mask)", "\n", "\n", "", "indexed_tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenized_text_without_mask", ")", "\n", "segments_ids", "=", "[", "0", "]", "*", "len", "(", "tokenized_text_without_mask", ")", "\n", "\n", "tokens_tensor", "=", "torch", ".", "tensor", "(", "[", "indexed_tokens", "]", ")", "\n", "segments_tensors", "=", "torch", ".", "tensor", "(", "[", "segments_ids", "]", ")", "\n", "\n", "tokens_tensor", "=", "tokens_tensor", ".", "to", "(", "device", ")", "\n", "segments_tensors", "=", "segments_tensors", ".", "to", "(", "device", ")", "\n", "\n", "# get emb_vector for entity1", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "outputs", "=", "model", "(", "tokens_tensor", ",", "segments_tensors", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "# print(sequence_output.shape)       # torch.Size([1, 17, 768])", "\n", "\n", "", "entity1_emb_2", "=", "sequence_output", "[", "0", ",", "pos1", "[", "0", "]", ":", "pos1", "[", "1", "]", "+", "1", "]", ".", "mean", "(", "0", ")", "\n", "return", "cos_dist", "(", "entity1_emb", ".", "cpu", "(", ")", ",", "entity1_emb_2", ".", "cpu", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_cls_emb": [[177, 193], ["tokenizer.convert_tokens_to_ids", "torch.tensor", "torch.tensor", "tokens_tensor.to.to", "segments_tensors.to.to", "len", "torch.no_grad", "model"], "function", ["None"], ["", "def", "get_cls_emb", "(", "tokenized_text_without_mask", ")", ":", "\n", "    ", "indexed_tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenized_text_without_mask", ")", "\n", "segments_ids", "=", "[", "0", "]", "*", "len", "(", "tokenized_text_without_mask", ")", "\n", "tokens_tensor", "=", "torch", ".", "tensor", "(", "[", "indexed_tokens", "]", ")", "\n", "segments_tensors", "=", "torch", ".", "tensor", "(", "[", "segments_ids", "]", ")", "\n", "\n", "# put data to cuda", "\n", "tokens_tensor", "=", "tokens_tensor", ".", "to", "(", "device", ")", "\n", "segments_tensors", "=", "segments_tensors", ".", "to", "(", "device", ")", "\n", "\n", "# get emb_vector for entity1", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "outputs", "=", "model", "(", "tokens_tensor", ",", "segments_tensors", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "# print(sequence_output.shape)       # torch.Size([1, 17, 768])", "\n", "", "return", "sequence_output", "[", "0", ",", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_enhance_result": [[197, 256], ["masked_model.eval", "masked_model.to", "mask_words_predict.add_point", "tokenizer.tokenize", "mask_words_predict.get_entity_pos", "text.replace().replace().replace().replace.replace().replace().replace().replace", "tokenizer.tokenize", "copy.deepcopy", "mask_words_predict.get_cls_emb", "mask_words_predict.get_entity_dist", "mask_words_predict.get_random_mask_pos", "copy.deepcopy", "tokenizer.convert_tokens_to_ids", "torch.tensor", "torch.tensor", "tokens_tensor.to.to", "segments_tensors.to.to", "copy.deepcopy", "mask_words_predict.add_entity_flag", "mask_words_predict.get_cls_emb", "mask_words_predict.get_entity_dist", "mask_words_predict.cos_dist", "copy.deepcopy", "copy.deepcopy", "len", "torch.no_grad", "masked_model", "torch.argmax().item", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "get_cls_emb.cpu", "get_cls_emb.cpu", "text.replace().replace().replace().replace.replace().replace().replace", "tokenizer.convert_ids_to_tokens", "torch.argmax", "text.replace().replace().replace().replace.replace().replace", "text.replace().replace().replace().replace.replace"], "function", ["home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.add_point", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_entity_pos", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_cls_emb", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_entity_dist", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_random_mask_pos", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.add_entity_flag", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_cls_emb", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.get_entity_dist", "home.repos.pwc.inspect_result.thu-bpm_gradlre.src.mask_words_predict.cos_dist"], ["", "def", "get_enhance_result", "(", "text", ",", "masked_model", ")", ":", "\n", "    ", "masked_model", ".", "eval", "(", ")", "\n", "masked_model", ".", "to", "(", "device", ")", "\n", "text", "=", "add_point", "(", "text", ")", "\n", "tokenized_text_temp", "=", "tokenizer", ".", "tokenize", "(", "text", ")", "\n", "pos1", ",", "pos2", "=", "get_entity_pos", "(", "tokenized_text_temp", ")", "\n", "\n", "text", "=", "text", ".", "replace", "(", "\"<e1>\"", ",", "\"\"", ")", ".", "replace", "(", "\"</e1>\"", ",", "\"\"", ")", ".", "replace", "(", "\"<e2>\"", ",", "\"\"", ")", ".", "replace", "(", "\"</e2>\"", ",", "\"\"", ")", "\n", "text_enhance_list", "=", "{", "}", "\n", "\n", "tokenized_text", "=", "tokenizer", ".", "tokenize", "(", "text", ")", "\n", "# text_enhance_list.append(copy.deepcopy(tokenized_text))", "\n", "text_enhance_list", "[", "\"before mask\"", "]", "=", "copy", ".", "deepcopy", "(", "tokenized_text", ")", "\n", "\n", "cls_emb1", "=", "get_cls_emb", "(", "copy", ".", "deepcopy", "(", "tokenized_text", ")", ")", "\n", "entity_effect1", "=", "get_entity_dist", "(", "copy", ".", "deepcopy", "(", "tokenized_text", ")", ",", "pos1", ",", "pos2", ")", "\n", "text_enhance_list", "[", "\"entity_effect1\"", "]", "=", "entity_effect1", "\n", "\n", "random_mask_pos", "=", "get_random_mask_pos", "(", "tokenized_text", ",", "pos1", ",", "pos2", ",", "0.15", ")", "\n", "\n", "# print(random_mask_pos)", "\n", "# Mask text tokens except entity", "\n", "for", "masked_index", "in", "random_mask_pos", ":", "\n", "        ", "tokenized_text", "[", "masked_index", "]", "=", "'[MASK]'", "\n", "# print(tokenized_text)", "\n", "# text_enhance_list.append(copy.deepcopy(tokenized_text))", "\n", "", "text_enhance_list", "[", "\"after mask\"", "]", "=", "copy", ".", "deepcopy", "(", "tokenized_text", ")", "\n", "\n", "indexed_tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenized_text", ")", "\n", "segments_ids", "=", "[", "0", "]", "*", "len", "(", "tokenized_text", ")", "\n", "\n", "tokens_tensor", "=", "torch", ".", "tensor", "(", "[", "indexed_tokens", "]", ")", "\n", "segments_tensors", "=", "torch", ".", "tensor", "(", "[", "segments_ids", "]", ")", "\n", "\n", "tokens_tensor", "=", "tokens_tensor", ".", "to", "(", "device", ")", "\n", "segments_tensors", "=", "segments_tensors", ".", "to", "(", "device", ")", "\n", "\n", "# predict all the masked token", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "outputs", "=", "masked_model", "(", "tokens_tensor", ",", "token_type_ids", "=", "segments_tensors", ")", "\n", "predictions", "=", "outputs", "[", "0", "]", "\n", "# print(predictions.shape)       # torch.Size([1, 17, 30522])", "\n", "\n", "", "for", "i", "in", "random_mask_pos", ":", "\n", "        ", "predicted_index", "=", "torch", ".", "argmax", "(", "predictions", "[", "0", ",", "i", "]", ")", ".", "item", "(", ")", "\n", "predicted_token", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "[", "predicted_index", "]", ")", "[", "0", "]", "\n", "tokenized_text", "[", "i", "]", "=", "predicted_token", "\n", "# print(tokenized_text)", "\n", "\n", "# text_enhance_list.append(copy.deepcopy(tokenized_text))", "\n", "", "text_enhance_list", "[", "\"mask predict\"", "]", "=", "copy", ".", "deepcopy", "(", "tokenized_text", ")", "\n", "text_enhance_list", "[", "\"mask predict add flag\"", "]", "=", "add_entity_flag", "(", "copy", ".", "deepcopy", "(", "tokenized_text", ")", ",", "pos1", ",", "pos2", ")", "\n", "\n", "cls_emb2", "=", "get_cls_emb", "(", "copy", ".", "deepcopy", "(", "tokenized_text", ")", ")", "\n", "entity_effect2", "=", "get_entity_dist", "(", "copy", ".", "deepcopy", "(", "tokenized_text", ")", ",", "pos1", ",", "pos2", ")", "\n", "text_enhance_list", "[", "\"entity_effect2\"", "]", "=", "entity_effect2", "\n", "text_enhance_list", "[", "\"cls_dist\"", "]", "=", "cos_dist", "(", "cls_emb1", ".", "cpu", "(", ")", ",", "cls_emb2", ".", "cpu", "(", ")", ")", "\n", "# return text_enhance_list", "\n", "return", "text_enhance_list", "[", "\"mask predict add flag\"", "]", ",", "text_enhance_list", "[", "\"cls_dist\"", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.data.data_prepare.Read_SemEval_data": [[3, 21], ["open", "open", "line.split.split", "line[].strip", "interval.join.insert", "interval.join.insert", "interval.join.insert", "interval.join", "sentence.append", "line.split.split", "line[].strip", "label_name.append"], "function", ["None"], ["def", "Read_SemEval_data", "(", "CATEGORY", ")", ":", "\n", "    ", "sentence", "=", "[", "]", "\n", "label_name", "=", "[", "]", "\n", "interval", "=", "' '", "\n", "for", "line", "in", "open", "(", "'data/SemEval/%s/%s.txt'", "%", "(", "CATEGORY", ",", "CATEGORY", ")", ")", ":", "\n", "        ", "token", "=", "[", "]", "\n", "line", "=", "line", ".", "split", "(", "\"\t\"", ")", "\n", "line", "[", "1", "]", "=", "line", "[", "1", "]", ".", "strip", "(", "'\\n'", ")", "\n", "token", ".", "insert", "(", "0", ",", "'[CLS]'", ")", "\n", "token", ".", "insert", "(", "1", ",", "line", "[", "1", "]", ")", "\n", "token", ".", "insert", "(", "2", ",", "'[SEP]'", ")", "\n", "token", "=", "interval", ".", "join", "(", "token", ")", "\n", "sentence", ".", "append", "(", "token", ")", "\n", "", "for", "line", "in", "open", "(", "'data/SemEval/%s/%s_result_full.txt'", "%", "(", "CATEGORY", ",", "CATEGORY", ")", ")", ":", "\n", "        ", "line", "=", "line", ".", "split", "(", "\"\t\"", ")", "\n", "line", "[", "1", "]", "=", "line", "[", "1", "]", ".", "strip", "(", "'\\n'", ")", "\n", "label_name", ".", "append", "(", "line", "[", "1", "]", ")", "\n", "", "return", "sentence", ",", "label_name", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.data.data_prepare.realtion2id": [[30, 38], ["range", "open", "json.load", "len", "label_id.append"], "function", ["None"], ["", "def", "realtion2id", "(", "dataset_name", ",", "label_name", ")", ":", "\n", "    ", "label_id", "=", "[", "]", "\n", "with", "open", "(", "'data/%s/relation2id.json'", "%", "dataset_name", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "label_name", ")", ")", ":", "\n", "        ", "id", "=", "data", "[", "label_name", "[", "i", "]", "]", "\n", "label_id", ".", "append", "(", "id", ")", "\n", "", "return", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.data.data_prepare.Read_TACRED_data": [[49, 73], ["range", "open", "f.readline", "json.loads", "len", "interval.join.insert", "interval.join.insert", "interval.join.insert", "interval.join.insert", "interval.join.insert", "interval.join.insert", "interval.join", "sentence.append", "label_name.append"], "function", ["None"], ["", "def", "Read_TACRED_data", "(", "CATEGORY", ")", ":", "\n", "    ", "sentence", "=", "[", "]", "\n", "label_name", "=", "[", "]", "\n", "interval", "=", "' '", "\n", "with", "open", "(", "'data/tacred/data/json/%s.json'", "%", "CATEGORY", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "line", "=", "f", ".", "readline", "(", ")", "\n", "data", "=", "json", ".", "loads", "(", "line", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "        ", "tokens", "=", "data", "[", "i", "]", "[", "\"token\"", "]", "\n", "subj_start", "=", "data", "[", "i", "]", "[", "\"subj_start\"", "]", "\n", "subj_end", "=", "data", "[", "i", "]", "[", "\"subj_end\"", "]", "\n", "obj_start", "=", "data", "[", "i", "]", "[", "\"obj_start\"", "]", "\n", "obj_end", "=", "data", "[", "i", "]", "[", "\"obj_end\"", "]", "\n", "relation", "=", "data", "[", "i", "]", "[", "\"relation\"", "]", "\n", "tokens", ".", "insert", "(", "subj_start", ",", "'<e1>'", ")", "\n", "tokens", ".", "insert", "(", "subj_end", "+", "2", ",", "'</e1>'", ")", "\n", "tokens", ".", "insert", "(", "obj_start", "+", "2", ",", "'<e2>'", ")", "\n", "tokens", ".", "insert", "(", "obj_end", "+", "4", ",", "'</e2>'", ")", "\n", "tokens", ".", "insert", "(", "0", ",", "'[CLS]'", ")", "\n", "tokens", ".", "insert", "(", "-", "1", ",", "'[SEP]'", ")", "\n", "tokens", "=", "interval", ".", "join", "(", "tokens", ")", "\n", "sentence", ".", "append", "(", "tokens", ")", "\n", "label_name", ".", "append", "(", "relation", ")", "\n", "", "return", "sentence", ",", "label_name", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-bpm_gradlre.data.data_analysis.score": [[14, 101], ["collections.Counter", "collections.Counter", "collections.Counter", "range", "print", "print", "print", "print", "len", "print", "collections.Counter.keys", "sorted", "print", "print", "sum", "sum", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "collections.Counter.values", "float", "float", "collections.Counter.values", "float", "float", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sum", "sum", "sum", "sum", "float", "float", "float", "float", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "str"], "function", ["None"], ["def", "score", "(", "key", ",", "prediction", ",", "verbose", "=", "True", ",", "NO_RELATION", "=", "NO_RELATION", ")", ":", "\n", "\n", "    ", "correct_by_relation", "=", "Counter", "(", ")", "\n", "guessed_by_relation", "=", "Counter", "(", ")", "\n", "gold_by_relation", "=", "Counter", "(", ")", "\n", "\n", "# Loop over the data to compute a score", "\n", "for", "row", "in", "range", "(", "len", "(", "key", ")", ")", ":", "\n", "        ", "gold", "=", "key", "[", "row", "]", "\n", "guess", "=", "prediction", "[", "row", "]", "\n", "\n", "if", "gold", "==", "NO_RELATION", "and", "guess", "==", "NO_RELATION", ":", "\n", "            ", "pass", "\n", "", "elif", "gold", "==", "NO_RELATION", "and", "guess", "!=", "NO_RELATION", ":", "\n", "            ", "guessed_by_relation", "[", "guess", "]", "+=", "1", "\n", "", "elif", "gold", "!=", "NO_RELATION", "and", "guess", "==", "NO_RELATION", ":", "\n", "            ", "gold_by_relation", "[", "gold", "]", "+=", "1", "\n", "", "elif", "gold", "!=", "NO_RELATION", "and", "guess", "!=", "NO_RELATION", ":", "\n", "            ", "guessed_by_relation", "[", "guess", "]", "+=", "1", "\n", "gold_by_relation", "[", "gold", "]", "+=", "1", "\n", "if", "gold", "==", "guess", ":", "\n", "                ", "correct_by_relation", "[", "guess", "]", "+=", "1", "\n", "\n", "# Print verbose information", "\n", "", "", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Per-relation statistics:\"", ")", "\n", "relations", "=", "gold_by_relation", ".", "keys", "(", ")", "\n", "longest_relation", "=", "0", "\n", "\n", "# for relation in sorted(relations):", "\n", "#     longest_relation = max(len(relation), longest_relation)", "\n", "for", "relation", "in", "sorted", "(", "relations", ")", ":", "\n", "# (compute the score)", "\n", "            ", "correct", "=", "correct_by_relation", "[", "relation", "]", "\n", "guessed", "=", "guessed_by_relation", "[", "relation", "]", "\n", "gold", "=", "gold_by_relation", "[", "relation", "]", "\n", "prec", "=", "1.0", "\n", "if", "guessed", ">", "0", ":", "\n", "                ", "prec", "=", "float", "(", "correct", ")", "/", "float", "(", "guessed", ")", "\n", "", "recall", "=", "0.0", "\n", "if", "gold", ">", "0", ":", "\n", "                ", "recall", "=", "float", "(", "correct", ")", "/", "float", "(", "gold", ")", "\n", "", "f1", "=", "0.0", "\n", "if", "prec", "+", "recall", ">", "0", ":", "\n", "                ", "f1", "=", "2.0", "*", "prec", "*", "recall", "/", "(", "prec", "+", "recall", ")", "\n", "# (print the score)", "\n", "", "sys", ".", "stdout", ".", "write", "(", "(", "\"{:<\"", "+", "str", "(", "longest_relation", ")", "+", "\"}\"", ")", ".", "format", "(", "relation", ")", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"  P: \"", ")", "\n", "if", "prec", "<", "0.1", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "", "if", "prec", "<", "1.0", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "", "sys", ".", "stdout", ".", "write", "(", "\"{:.2%}\"", ".", "format", "(", "prec", ")", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"  R: \"", ")", "\n", "if", "recall", "<", "0.1", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "", "if", "recall", "<", "1.0", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "", "sys", ".", "stdout", ".", "write", "(", "\"{:.2%}\"", ".", "format", "(", "recall", ")", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"  F1: \"", ")", "\n", "if", "f1", "<", "0.1", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "", "if", "f1", "<", "1.0", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "", "sys", ".", "stdout", ".", "write", "(", "\"{:.2%}\"", ".", "format", "(", "f1", ")", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"  #: %d\"", "%", "gold", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"\\n\"", ")", "\n", "", "print", "(", "\"\"", ")", "\n", "\n", "# Print the aggregate score", "\n", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Final Score:\"", ")", "\n", "", "prec_micro", "=", "1.0", "\n", "if", "sum", "(", "guessed_by_relation", ".", "values", "(", ")", ")", ">", "0", ":", "\n", "        ", "prec_micro", "=", "float", "(", "sum", "(", "correct_by_relation", ".", "values", "(", ")", ")", ")", "/", "float", "(", "\n", "sum", "(", "guessed_by_relation", ".", "values", "(", ")", ")", ")", "\n", "", "recall_micro", "=", "0.0", "\n", "if", "sum", "(", "gold_by_relation", ".", "values", "(", ")", ")", ">", "0", ":", "\n", "        ", "recall_micro", "=", "float", "(", "sum", "(", "correct_by_relation", ".", "values", "(", ")", ")", ")", "/", "float", "(", "\n", "sum", "(", "gold_by_relation", ".", "values", "(", ")", ")", ")", "\n", "", "f1_micro", "=", "0.0", "\n", "if", "prec_micro", "+", "recall_micro", ">", "0.0", ":", "\n", "        ", "f1_micro", "=", "2.0", "*", "prec_micro", "*", "recall_micro", "/", "(", "prec_micro", "+", "recall_micro", ")", "\n", "", "print", "(", "\"SET NO_RELATION ID: \"", ",", "NO_RELATION", ")", "\n", "print", "(", "\"Precision (micro): {:.3%}\"", ".", "format", "(", "prec_micro", ")", ")", "\n", "print", "(", "\"   Recall (micro): {:.3%}\"", ".", "format", "(", "recall_micro", ")", ")", "\n", "print", "(", "\"       F1 (micro): {:.3%}\"", ".", "format", "(", "f1_micro", ")", ")", "\n", "\n"]]}